\fatihh{\subsection{Selection of Features and ML Approaches}}
\eat{If several models have similar classification performance, it is recommended to select the least complex model as it is easier to not only use in practice but also to debug if something goes wrong. To this end, we compare our proposed model with the state-of-the-art ML approaches.}
In our quest to determine the best-performing model and feature sets, Table~\ref{tab:results_baseline} presents the results for three different feature sets using the five different ML approaches used by state-of-the-art methods:

\begin{itemize}[leftmargin=*]
\itemsep0em
    \item Feature-based supervised learning: The classical machine learning approach, as employed in various studies~\cite{Notos_Antonakakis2010, bilge:2014:Exposure,  lexical2015, page:2019:mal, practicalattacks:SP:2024}, involves tabulating features and applying classical supervised learners. As representative algorithms, we employ Random Forest~\cite{Leistner:2009:SSLRandomForest}, XGBoost~\cite{chen2016xgboost}, LightGBM~\cite{ke2017lightgbm}, known for their superior performance.
    \item Label propagation: Has been applied in various studies~\cite{polonium:SIAM:2011, nazca:NDSS:2014, BPPhishingCCS:2022, bp_mal2:2020, marmite:CCS:2017}, iteratively assigns labels to unlabeled nodes based on seed node labels in their neighborhoods. In our research, we adopt one of the most promising label propagation approaches, namely belief propagation~\cite{Yedidia:2003:BP}. 
    \item Shallow embedding (unsupervised) + supervised learning: Shallow embedding approaches learn a unique embedding for each node based on the graph topology. We use Node2Vec \cite{node2vec:sigkdd:2016}, a popular transductive shallow embedding generator. We concatenate the generated embeddings with features and perform the downstream classification using a feature-based supervised model. 
    \item Deep embedding (unsupervised) + supervised learning: We employ an inductive unsupervised GraphSAGE~\cite{graphsage:nips:2017} to create deep embeddings for the domain nodes, followed by feature-based supervised classification.
    \item Semi-supervised GNN: We employ a semi-supervised GraphSAGE to classify unlabeled nodes based on the graph structure and node features.
\end{itemize}

With no domain features, deep embedding outperforms the other methods. However, all models result in a high FPR ranging from 12.8\% to 21.1\%. This shows that the network structure alone is insufficient to detect malicious domains. This is consistent with many malicious domains being hosted on shared hosting infrastructures where benign domains are also hosted. Thus, one needs distinct features to effectively differentiate between malicious and benign domains within such infrastructures. In the second set of experiments, we observe that all six models, which utilize lexical features only, exhibit improved performance compared to the experiments with no features. We attribute this improvement to the tendency of attacks to create lexically similar domains in the same infrastructure. In the third set of experiments focusing on hosting features, we observe that the models demonstrate superior classification performance compared to those without features and those utilizing only lexical features. Since, unlike benign domains, many malicious domains do not have consistent traffic and the hosting features capture these differences to differentiate between the two classes. The last set of experiments utilizes both lexical and hosting features. Not surprisingly, we obtained the overall best result compared to all the categories. Out of all the models, both GNN models (deep embedding and semi-supervised GNN) achieve high classification performance as they take into account both topology and node features simultaneously to learn to discriminate embeddings for the two classes. Out of the two GNN models, we consistently get slightly better classification performance for the semi-supervised model and thus utilize it for daily blocklist generation. \ccsupdate{You can find further details on our GNN hyperparameter search in Appendix~\ref{app:differentgnn}. Based on the grid search results we use GNN that comprises three layers with embedding dimensions of 256, employ a final layer that aggregates all embeddings from the preceding layers. }

\begin{table}
\centering
\caption{\centering Performance comparison of various ML approaches and feature sets \textcolor{black}{at 0.5 classification threshold}.}
\footnotesize
\resizebox{\columnwidth}{!}{
\begin{tabular}{c|c|c|c|c|c}
    \toprule
     & \multirow{2}{*}{\textbf{Model Type}} & \multicolumn{4}{c}{\textbf{Metrics}} \\ 
    \cline{3-6} 
    & & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{FPR} \\
    \midrule
    \multirow{4}{*}{\rotatebox[origin=c]{90}{Without}}
    \multirow{4}{*}{\rotatebox[origin=c]{90}{Features}}
    & Belief Propagation & 0.757 & 0.711 & 0.715 & 0.211  \\
    & Node2Vec & 0.807 & 0.801 & 0.798 & 0.148   \\
    & Unsup. GNN & 0.871 & 0.842 & 0.872 & 0.128  \\
    & Semisup. GNN & 0.815 & 0.767 & 0.815 & 0.185  \\
    \midrule
    \multirow{7}{*}{\rotatebox[origin=c]{90}{Lexical}} 
    \multirow{7}{*}{\rotatebox[origin=c]{90}{Features}} 
    & Random Forest & 0.864 & 0.880 & 0.801 & 0.085  \\
    & LightGBM & 0.868 & 0.897 & 0.790 & 0.071  \\
    & XGBoost & 0.861 & 0.882 & 0.788 & 0.082  \\
    & Node2Vec & 0.909 & 0.933 & 0.855 & 0.048  \\
    & Unsup. GNN & 0.913 & 0.927 & 0.871 & 0.053  \\
    & Semisup. GNN & 0.938 & 0.946 & 0.904 & 0.037  \\
    \midrule
    \multirow{7}{*}{\rotatebox[origin=c]{90}{Hosting}}
    \multirow{7}{*}{\rotatebox[origin=c]{90}{Features}}
    & Random Forest & 0.944 & 0.958 & 0.914 & 0.031  \\
    & LightGBM & 0.945 & 0.959 & 0.915 & 0.030  \\
    & XGBoost & 0.944 & 0.957 & 0.914 & 0.032  \\
    & Node2Vec & 0.953 & 0.962 & 0.930 & 0.028 \\
    & Unsup. GNN & 0.968 & 0.971 & \textbf{0.954} & 0.021  \\
    & Semisup. GNN & 0.967 & 0.970 & 0.951 & 0.021  \\
    \midrule
    \multirow{7}{*}{\rotatebox[origin=l]{90}{Lex. \& Hosting}}
    \multirow{7}{*}{\rotatebox[origin=c]{90}{Features}}
    & Random Forest & 0.952 & 0.971 & 0.920 & 0.021  \\
    & LightGBM & 0.956 & 0.974 & 0.925 & 0.019  \\
    & XGBoost & 0.957 & 0.971 & 0.931 & 0.021  \\
    & Node2Vec & 0.962 & 0.977 & 0.935 &  0.017 \\
    & Unsup. GNN & 0.965 & 0.970 & 0.949 & 0.022  \\
    & Semisup. GNN & \textbf{0.982} & \textbf{0.984} & 0.941 & \textbf{0.012}  \\
    \bottomrule
\end{tabular}
}
\label{tab:results_baseline}
\end{table}

\subsection{PDNS Expansion}
\label{subsec:pdnsexpansion}




In this section, we empirically estimate the impact of two key parameters of the PDNS expansion algorithm: the number of hops and the number of recently hosted domains considered in each hop. 
We perform Level 1 (Domain-IP-Domain), Level 2 (Domain-IP-Domain-IP), and Level 3 (Domain-IP-Domain-IP-Domain) expansions for each day from July 1st to 7th, 2022. Fig.~\ref{fig:expansion_types} shows the F1-score for each day for different expansion levels. We observe that the Level 2 expansion consistently yields a 1\% improvement compared to Level 1. However, the gain in F1-score for Level 3 is less than 0.4\% in general. As Level 3 domains are farther away from the seed nodes, they have less influence from the seed domains. Additionally, the graph size exponentially increases from Level 2 (around 0.5 million nodes on average) to Level 3 (around 3 million nodes on average), incurring a huge computational cost. Based on these findings, we empirically fix the graph expansion to Level 2.
While keeping the expansion to Level 2, we perform experiments to identify the optimal number of recent domains hosted on each IP (i.e. expansion rate) that yields a high F1-score and a low FPR. Fig.~\ref{fig:exp_expansionrateacc} and~\ref{fig:exp_expansionratefpr} show the F1-score and FPR for different expansion rates from 50 to 250, respectively. It shows that as the expansion increases, F1-score falls slowly while FPR falls rapidly and plateaus at 200-250. Since our primary goal is to minimize false positives and reduce the burden on security operations teams, we set the expansion rate to 200 in our experiments. 
\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{images/expansionrate_c.png}
\caption{Graph structure comparison. \eat{\ik{Make "Mantis" capital in the legend}} \eat{\nabeel{Fatih: Would you be able to repeat the experiment for another level? i.e.. D-IP-D-IP-D. One possible issue is the scale of the graph.}}}
\label{fig:expansion_types}
\end{figure}



\eat{With the first layer of IP resolutions, \system detects domains that share at least one common IP address with one of the malicious seed domains. The interrelationship between the second-layer domains is formed by the second-layer IP resolutions. Also, due to the expansion rate limit at the first layer, if some  relationships between seed domains to second-layer domains could not be achieved, they will also be included with these resolutions. We especially observe these lacking connections when seed domains are resolved to public firewall addresses. On average, the second-hop domain to seed domain ratio is 48.2 fold. 

If we are to expand for example, two rounds and reach out to third-hop domains, to capture the direct connections between the seed and other potential malicious ones, the graph grows exponentially and the associations become weaker. Seed domains form an important part of the malicious ground truth, and for the expanded part, since the graph size increases exponentially, their effect also diminishes. That is why we needed a smarter way of dealing with it. As shown in Fig.~\ref{fig:overlapping_ip_july}, around 80\% of the daily IPs were seen at least once during the past week. That is why by combining the weekly records we include additional connections to the leaf IPs and at the same time, preserve the strong association of the expanded graph with the seed domains, as all the nodes are at most one-hop away from at least one seed domain. As discussed in Section~\ref{subsec:windowsize}, this significantly improves  the performance of the model. 

The expansion rate is one of the interesting and tricky parameters. We can see the average performance results for three graphs according to different expansion rates in Fig.~\ref{fig:exp_expansionrate}. As can be seen, increasing the expansion rate decreases both the false positive rate and the F1 score. Thus, a threshold should be set to a point that produces a high precision and recall value while keeping the false positive rate at an acceptable rate.  Since the decline in the false positive rate seems exponential, whereas the F1 score seems more linear, a value in the range [100-200] can be considered as a reasonable expansion rate. For \system, we set this value to 200.}




\begin{figure}
\centering
\begin{subfigure}[t]{0.45\linewidth}
    \includegraphics[width=\linewidth]{images/trainingsize_a.png}
    \caption{}
    \label{fig:exp_trainingsizefpr}
  \end{subfigure}
\begin{subfigure}[t]{0.45\linewidth}
    \includegraphics[width=\linewidth]{images/trainingsize_b.png}
    \caption{}
    \label{fig:exp_trainingsizeacc}
  \end{subfigure}  \hfill
  \begin{subfigure}[t]{0.45\linewidth}
    \includegraphics[width=\linewidth]{images/expansionrate_a.png}
    \caption{}
    \label{fig:exp_expansionrateacc}
  \end{subfigure}
  \begin{subfigure}[t]{0.45\linewidth}
    \includegraphics[width=\linewidth]{images/expansionrate_b.png}
    \caption{}
    \label{fig:exp_expansionratefpr}
  \end{subfigure}\hfill
    \caption{(a) F1 score, (b) FPR for different seed domain counts. (c) F1 score, (d) FPR for different expansion rates.}
  \vspace{-2mm}
  
  \label{fig:exp_trainingsize}
\end{figure}

\subsection{Daily Blocklist and Training Window Size}
\label{subsec:datasize}

Collecting labeled training data is an expensive and timely process. Therefore, it is important to identify the minimum training data size for the best empirical performance. \spfinal{Fig.~\ref{fig:exp_trainingsize} shows the performance for different seed domain counts from 100 to 3000 (i.e., different training data sizes), averaged over three temporally disjoint graphs. We observe that the F1-score rapidly increases as the seed size increases and starts to plateau when the seed size reaches 2000. Based on this experiment, we recommend using at least 2,000 seed domains for optimal results with high recall and very low false positive rate. In our daily blocklist generation pipeline, we employ on average 3,000 domains per day. However, it should be noted that one may use our approach with small seeds with only hundreds of malicious domains and still achieve an F1-score above 90\% at the expense of slightly lower recall. One needs to select different classification thresholds depending on the seed size and the desired false positive rate.} 


As explained in Section~\ref{subsec:pdnsexpansion}, we generate a daily passive DNS graph, and the window size determines the number of consecutive days combined in each graph. This feature sheds light on the duration of hosting infrastructure reuse by attackers, as empirically analyzed in Fig.~\ref{fig:overlapping_ip_july}. By combining daily graphs, we add more connections to the leaf IP nodes, consequently enhancing the overall performance of our trained models. Our model is trained using data from different window sizes ranging from 1 to 15 days. 
Notably, as the window size increases, FPR gradually converges to a value closer to 1\%, particularly from day 7 onwards. 
Considering the graph size (i.e. computational cost detailed in Appendix~\ref{app:differentgnn}) and FPR, we empirically fix our window size to 7 days.  


\eat{To ensure consistently high performance, we train \system with different training data sizes and try to find the best performing one. Besides the size of the training window, which specifies the number of daily graphs to be combined, the sizes of individual graphs are affected by two factors: the number of seed domains and the rate of expansion around those domains. In this section, we perform the experiments on seed domain counts and evaluate PDNS expansion parameters in the following section. Fig.~\ref{fig:exp_trainingsize} shows the performance for different seed domain counts from 500 to 3000 for three temporally different graphs. We observe that seed count and \system's performance are positively correlated and in order to ensure consistently high performance one needs at least 2000 seed nodes. Considering our daily average seed count of 2877 domains and 7-day training window size, we use much more than 2000 seed nodes, on average, to train our models.}


\subsection{Generalizability of the Model and Predictive Performance over Time}
\label{subsec:generalizability}

\review{Maintaining consistent high performance over temporally different datasets is crucial for practical machine learning models. Since its deployment, \system has consistently delivered outstanding performance. Fig.~\ref{fig:exp_logitivity} illustrates daily precision and recall metrics for different FPR values, each calculated within a 7-day training window. 
\system provides an excellent trade-off between FPR and recall and can substantially reduce FPR while making some trade-offs in recall: On average, \system can achieve FPR 0.5\%, with recall 95.6\%, FPR 0.1\% with recall 86.9\%.
}


The ability to employ a trained model for a specific duration without the need for re-training offers several practical advantages, including reduced training time, optimized resource utilization, and lower labeling costs. To evaluate the predictive capability of our system, we train a model using one-week window data and predict unseen malicious domains in the days following the training window. This experiment is repeated at the beginning of three months, and as illustrated in Fig.~\ref{fig:predictiveperf} our system maintains its performance within a 3\% precision margin on average even after three weeks for unseen data. Given that the retraining cost falls within our budget and we prioritize achieving the highest precision feasible, our current deployment of the system opts for daily retraining. However, if daily retraining is not feasible, we recommend retraining the system at least bi-weekly, as the decline in precision becomes noticeable after 14 days.

\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{images/exp_logitivity_2.png}
\caption{Generalizability of the model.\eat{ across temporally different datasets from 2022-06-10 to 2022-08-10.}}
\label{fig:exp_logitivity}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.80\columnwidth]{images/9.predictiveperf.png}
\caption{Predictive performance of the model.}
\label{fig:predictiveperf}
\end{figure}

\subsection{SoTA Comparison and Optimizations}\label{subsec:ablation}

\ccsupdate{
We compared our approach with the recent state-of-the-art methods. 
\spupdate{Galloway et al.~\cite{practicalattacks:SP:2024} integrate different feature sets with features extracted from the network structure to train a Random Forest model.}
Nabeel et al.~\cite{bp_mal2:2020} apply belief propagation and Kim et al.
~\cite{BPPhishingCCS:2022} employ loopy belief propagation. Other graph-based approach ~\cite{ringer:ICCS:2020, handom:wang:2023, blocklist_raid2024} leverage the power of GNNs on graphs containing client information. Ringer~\cite{ringer:ICCS:2020} employs a dynamic GCN model with lexical features and calculates attention weights based on the shared neighborhood, while HanDom~\cite{handom:wang:2023} utilizes an attention-based HAN model with time-based features. Blocklist-forecast~\cite{blocklist_raid2024} builds a heterogeneous graph and uses HinSAGE embeddings along with a Random Forest classifier. We implemented these approaches to the best of our knowledge, and their optimal results on our datasets are showcased in Table~\ref{tab:sota}. The intuition behind existing graph-based approaches is that a client associated with a malicious domain is likely to have connections with other malicious domains. However, our results suggest that these approaches are optimized for benign environments. As the results indicate, by focusing on recent attacks, we can efficiently identify other attacks. 




\begin{table}
\centering
\caption{\centering Performance comparison with SoTA Methods.}
\footnotesize
\resizebox{\linewidth}{!}{
\begin{tabular}{l|l|c|c|c|c}
    \toprule
        &\textbf{Model} & \textbf{F1} & \textbf{Prec.} & \textbf{Recall} & \textbf{FPR} \\ 
    \midrule
        Nabeel et. al.~\cite{bp_mal2:2020} & BP & 71.3 & 71.1 & 71.5 &  21.1 \\
    Kim et. al.~\cite{BPPhishingCCS:2022} & BP & 79.1 & 80.4 & 77.8 &  17.8 \\
    Galloway et. al.~\cite{practicalattacks:SP:2024} & RF & 92.4 & 92.5 & 92.3 &  8.7 \\     Liu et. al.~\cite{ringer:ICCS:2020} & GNN & 94.0 & 95.5 & 92.6 &  3.8 \\     Wang et. al.~\cite{handom:wang:2023} & GNN & 94.8 & 96.9 & 92.8 &  3.4 \\ 
        Kumarasinghe et. al.~\cite{blocklist_raid2024} & GNN & 94.3 & 94.2 & 94.4 & 5.9 \\
    \system (ours) & GNN & \textbf{97.3} & \textbf{99.0} & \textbf{95.6} & \textbf{0.5}  \\
    \bottomrule
\end{tabular}
}
\label{tab:sota}
\end{table}

We further conducted an experiment to validate our decision to distinguish between attacker-owned and compromised domains, and exclude web hosting domains.
By making such decisions, we achieve higher precision (99.0\%) and recall (95.6\%) with a lower FPR of 0.5\%. In contrast, when compromised and webhosting domains are included: lower precision (93.69\%) and recall (89.80\%) with a higher FPR (4.55\%), reflecting a notable decrease of 4\% in each metric. 
Note that even in this setting, our approach still outperforms recent state-of-the-art approaches such as~\cite{BPPhishingCCS:2022} and~\cite{practicalattacks:SP:2024}.
In addition to assessing the implications of our decisions, we conducted a thorough analysis of our computational performance, which can be found in Appendix~\ref{app:differentgnn}.
}

\input{robustness}

\subsection{Meta-Learner Analysis}~\label{ss:metaanalysis}
In this section, we empirically evaluate our design choices with the meta-learner for the on-demand classifier.


\begin{table}[!ht]
\centering
\caption{On-demand classifier testing results. \eat{\nabeel{Let's create the domain dataset from GSB for Oct and add another row to this table. Also, another one for Tranco.}}}
\footnotesize
\label{tab:results_baseline_meta}
\resizebox{\linewidth}{!}{
\begin{tabular}{l|c|c|c|c|c}
    \toprule
    \multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{\#Domain}} & \multicolumn{2}{c|}{\textbf{Baseline RF}} & \multicolumn{2}{c}{\textbf{On-Demand Classifier}} \\ 
    \cline{3-6} 
    & & \textbf{Accuracy} & \textbf{F1} & \textbf{Accuracy} & \textbf{F1} \\
    \midrule
    Alexa & 19767 & 96.8 & - & 99.1 & - \\ 
    Tranco & 10000 & 98.6 & - & 99.4 & - \\ 
    CrUX & 10000 & 98.4 & - & 99.6  & -\\ 
    \midrule
    VT & 19408 & 95.2 & 97.5 & 96.8  & 98.4\\ 
    GSB & 1000 & 73.6 & 84.8 & 94.1  & 97.0 \\ 
    OpenPhish & 2115 & 89.6 & 94.5 & 96.3  & 98.1\\ 
    PhishTank & 2215 & 93.1 & 96.4 & 97.1  & 98.5\\ 
    
    \bottomrule
\end{tabular}
}
\label{tab:dataset_real}
\end{table}

{\bf On-Demand Performance on Different Datasets:} We also measure \system's on-demand performance on testing datasets from different sources. 
To mitigate temporal bias, we perform these tests on previously unseen data collected from the following month.
In addition to VT, we also rigorously test our approach on diverse datasets, including OpenPhish, PhishTank, GSB, and benign datasets. 
\eat{Malicious domains are selected from the first seen, i.e. newly observed domains, feeds on October 2022 from the respective sources. }In order to have labels with high confidence, we select only manually verified URLs by PhishTank and OpenPhish and consider the top 100k domains from the top lists. 
\eat{Since the voting process used by those intelligence feeds could be compromised, we do sanity-checking on the voting results by actively querying VT and filtering out URLs that receive less than three positives.} Additionally, we actively query all the domains in VT and exclude those from malicious sources (PhishTank, OpenPhish, VT) that are not marked by VT as malicious, and exclude those from the benign sources marked as malicious. Our evaluation results, as presented in Table~\ref{tab:dataset_real}, consistently demonstrate high classification performance and inductive nature of our approach. 

{\bf Number of Ensemble Models:} An important aspect of the meta-learner is to identify the best number of ensemble models. We measure the accuracy and FPRs for varying numbers of ensemble models on the Alexa top 100K domains using non-overlapping weekly trained GNN models from  2022-07-01, and 2022-09-30. 
As shown in Fig.~\ref{fig:exp_ensemblecount}, the performance improves as the number of models increases and peaks at 4 models. Hence, we fix the number of GNN encoders in our on-demand classifier to 4. 
\begin{figure}
\centering
\begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{images/8a.ensemblemodel.png}
    \caption{}
    \label{fig:exp_ensemblecountacc}
  \end{subfigure}
  \begin{subfigure}[t]{0.49\linewidth}
    \includegraphics[width=\linewidth]{images/8b.ensemblemodel.png}
    \caption{}
    \label{fig:exp_ensemblecountfpr}
  \end{subfigure}\hfill
  \caption{(a) Acc., (b) FPR for different ensemble counts.}
    \label{fig:exp_ensemblecount}
\end{figure}






