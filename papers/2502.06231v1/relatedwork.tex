\section{Related Works}
%Sensitivity analysis
When discussing the validity of causal assumptions, sensitivity analysis might come to mind. In sensitivity analysis, one hypothesizes departures from the assumption of no unmeasured confounding and investigates how different biases would arise depending on the hypothesized confounder's relationship with treatment and outcome~\citep{cornfield1959smoking, tan2006distributional, vanderweele2017sensitivity}. However, while sensitivity analysis probes `what-if' scenarios regarding potential unmeasured confounding (a process that can always be undertaken), falsification aims to empirically test whether assumptions are violated, based on the real-world implications of those assumptions (which is not always feasible). An example of the latter is the instrumental variable inequalities from~\citet{pearl1995testability}. In this way, sensitivity analysis and falsification are complementary: the former explores possible scenarios, while the latter seeks direct empirical evidence for these scenarios. Despite this, falsification has received comparatively less attention in the~literature.

%Falsification/identification in multi-environment data
One line of research on falsification in observational causal inference assumes that certain transportability conditions hold, allowing causal effects to be transferred between different environments~\citep{dahabreh2020benchmarking,hussain2022falsification,hussain2023falsification}. The basic premise is that, under transportability conditions, comparing treatment effect estimates from multiple observational studies, or from a single observational study and a randomized one, should yield consistent results. If inconsistencies are found, this can be used to falsify the identifiability conditions, assuming the transportability assumptions hold. This idea has been further extended to time-to-event outcomes with censoring~\citep{demirel2024benchmarking}, as well as for quantifying bias from unmeasured confounding~\citep{de2024detecting, de2024hidden}. In contrast, our approach assumes independence of causal mechanisms, which does not require transportable treatment effects or access to randomized data. 

Testing for independence of causal mechanisms has been applied in previous work to falsify causal assumptions, such as detecting hidden confounding~\citep{karlsson2023detecting} or testing the validity of instrumental variables~\citep{burauel2023evaluating}. Most similar to our work is that of \citet{karlsson2023detecting}, though their method relies on conditional independence testing which is a notoriously difficult statistical problem in itself~\citep{shah2020hardness}. To avoid the challenges of conditional independence testing--for instance, losing statistical power as the adjustment set becomes larger--we address this problem by proposing an alternative method that does not rely on conditional independence~testing.


% ICM and causal discovery
Our work investigates the implications of the principle of independent causal mechanisms, which has a rich literature in causal discovery, particularly in multi-environment settings~\citep{ huang2020causal, perry2022causal,guo2024causal,mameche2024learning}. However, rather than aiming to learn the entire causal structure as most previous work, our approach focuses on verifying specific aspects of a partially known structure that is relevant for treatment effect estimation. Recently,~\citet{guo2024finetti} examined how independent causal mechanisms can lead to identification of certain treatment effects, though they did not address scenarios where causal assumptions are violated, such as in the presence of unmeasured confounders, which we study~here.