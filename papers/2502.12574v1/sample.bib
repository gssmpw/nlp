@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2018}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{vzak1983turing,
  title={A Turing machine time hierarchy},
  author={{\v{Z}}{\'a}k, Stanislav},
  journal={Theoretical Computer Science},
  volume={26},
  number={3},
  pages={327--333},
  year={1983},
  publisher={Elsevier}
}

@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@inproceedings{narayanan2019pipedream,
  title={PipeDream: Generalized pipeline parallelism for DNN training},
  author={Narayanan, Deepak and Harlap, Aaron and Phanishayee, Amar and Seshadri, Vivek and Devanur, Nikhil R and Ganger, Gregory R and Gibbons, Phillip B and Zaharia, Matei},
  booktitle={Proceedings of the 27th ACM Symposium on Operating Systems Principles},
  pages={1--15},
  year={2019}
}

@article{zhao2023pytorch,
  title={Pytorch FSDP: experiences on scaling fully sharded data parallel},
  author={Zhao, Yanli and Gu, Andrew and Varma, Rohan and Luo, Liang and Huang, Chien-Chin and Xu, Min and Wright, Less and Shojanazeri, Hamid and Ott, Myle and Shleifer, Sam and others},
  journal={arXiv preprint arXiv:2304.11277},
  year={2023}
}

@inproceedings{ren2021zero,
  title={$\{$ZeRO-Offload$\}$: Democratizing $\{$Billion-Scale$\}$ model training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={551--564},
  year={2021}
}

@article{shoeybi2019megatron,
  title={Megatron-lm: Training multi-billion parameter language models using model parallelism},
  author={Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:1909.08053},
  year={2019}
}

@article{li2020pytorch,
  title={Pytorch distributed: Experiences on accelerating data parallel training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={arXiv preprint arXiv:2006.15704},
  year={2020}
}

@article{bugnion1997disco,
  title={Disco: Running commodity operating systems on scalable multiprocessors},
  author={Bugnion, Edouard and Devine, Scott and Govil, Kinshuk and Rosenblum, Mendel},
  journal={ACM Transactions on Computer Systems (TOCS)},
  volume={15},
  number={4},
  pages={412--447},
  year={1997},
  publisher={ACM New York, NY, USA}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={International Conference on Machine Learning},
  pages={7480--7512},
  year={2023},
  organization={PMLR}
}


@misc{a100_80G,
  title = {{NVIDIA A100 Tensor Core GPU}},
  author = {{nvidia}},
  howpublished = {\url{https://www.nvidia.com/en-us/data-center/a100/}}
}


@misc{Nsight,
  title = {{NVIDIA Nsight Compute Profiling Tool}},
  author = {{nvidia}},
  howpublished = {\url{https://docs.nvidia.com/ nsight-compute/NsightCompute/index.html}}
}

@misc{In-place_Operations,
  title = {{In-place Operations}},
  author = {{nvidia}},
  howpublished = {\url{https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/inplace.html}}
}

@article{sergeev2018horovod,
  title={Horovod: fast and easy distributed deep learning in TensorFlow},
  author={Sergeev, Alexander and Del Balso, Mike},
  journal={arXiv preprint arXiv:1802.05799},
  year={2018}
}

@inproceedings{liberty2020elastic,
  title={Elastic machine learning algorithms in amazon sagemaker},
  author={Liberty, Edo and Karnin, Zohar and Xiang, Bing and Rouesnel, Laurence and Coskun, Baris and Nallapati, Ramesh and Delgado, Julio and Sadoughi, Amir and Astashonok, Yury and Das, Piali and others},
  booktitle={Proceedings of the 2020 ACM SIGMOD International Conference on Management of Data},
  pages={731--737},
  year={2020}
}

@article{imambi2021pytorch,
  title={PyTorch},
  author={Imambi, Sagar and Prakash, Kolla Bhanu and Kanagachidambaresan, GR},
  journal={Programming with TensorFlow: Solution for Edge Computing Applications},
  pages={87--104},
  year={2021},
  publisher={Springer}
}

@article{singh2020introduction,
  title={Introduction to tensorflow 2.0},
  author={Singh, Pramod and Manure, Avinash and Singh, Pramod and Manure, Avinash},
  journal={Learn TensorFlow 2.0: Implement Machine Learning and Deep Learning Models with Python},
  pages={1--24},
  year={2020},
  publisher={Springer}
}

@article{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}


@article{gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{kim2020torchgpipe,
  title={torchgpipe: On-the-fly pipeline parallelism for training giant models},
  author={Kim, Chiheon and Lee, Heungsub and Jeong, Myungryong and Baek, Woonhyuk and Yoon, Boogeon and Kim, Ildoo and Lim, Sungbin and Kim, Sungwoong},
  journal={arXiv preprint arXiv:2004.09910},
  year={2020}
}


@article{pipetransformer,
  title={Pipetransformer: Automated elastic pipelining for distributed training of transformers},
  author={He, Chaoyang and Li, Shen and Soltanolkotabi, Mahdi and Avestimehr, Salman},
  journal={arXiv preprint arXiv:2102.03161},
  year={2021}
}


@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@inproceedings{sourouri2014effective,
  title={Effective multi-GPU communication using multiple CUDA streams and threads},
  author={Sourouri, Mohammed and Gillberg, Tor and Baden, Scott B and Cai, Xing},
  booktitle={2014 20th IEEE International Conference on Parallel and Distributed Systems (ICPADS)},
  pages={981--986},
  year={2014},
  organization={IEEE}
}

@article{harmes2008flyweight,
  title={The Flyweight Pattern},
  author={Harmes, Ross and Diaz, Dustin},
  journal={Pro JavaScript Design Patterns},
  pages={179--195},
  year={2008},
  publisher={Springer}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017}
}

@inproceedings{you2018imagenet,
  title={Imagenet training in minutes},
  author={You, Yang and Zhang, Zhao and Hsieh, Cho-Jui and Demmel, James and Keutzer, Kurt},
  booktitle={Proceedings of the 47th International Conference on Parallel Processing},
  pages={1--10},
  year={2018}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{anil2023palm,
  title={Palm 2 technical report},
  author={Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
  journal={arXiv preprint arXiv:2305.10403},
  year={2023}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{korthikanti2023reducing,
  title={Reducing activation recomputation in large transformer models},
  author={Korthikanti, Vijay Anand and Casper, Jared and Lym, Sangkug and McAfee, Lawrence and Andersch, Michael and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  year={2023}
}

@article{dean2008mapreduce,
  title={MapReduce: simplified data processing on large clusters},
  author={Dean, Jeffrey and Ghemawat, Sanjay},
  journal={Communications of the ACM},
  volume={51},
  number={1},
  pages={107--113},
  year={2008},
  publisher={ACM New York, NY, USA}
}

@article{zaharia2016apache,
  title={Apache spark: a unified engine for big data processing},
  author={Zaharia, Matei and Xin, Reynold S and Wendell, Patrick and Das, Tathagata and Armbrust, Michael and Dave, Ankur and Meng, Xiangrui and Rosen, Josh and Venkataraman, Shivaram and Franklin, Michael J and others},
  journal={Communications of the ACM},
  volume={59},
  number={11},
  pages={56--65},
  year={2016},
  publisher={ACM New York, NY, USA}
}

@inproceedings{rhu2016vdnn,
  title={vDNN: Virtualized deep neural networks for scalable, memory-efficient neural network design},
  author={Rhu, Minsoo and Gimelshein, Natalia and Clemons, Jason and Zulfiqar, Arslan and Keckler, Stephen W},
  booktitle={2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={1--13},
  year={2016},
  organization={IEEE}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{shazeer2018mesh,
  title={Mesh-tensorflow: Deep learning for supercomputers},
  author={Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and others},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{liu2023ring,
  title={Ring Attention with Blockwise Transformers for Near-Infinite Context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}

@article{li2021sequence,
  title={Sequence parallelism: Long sequence training from system perspective},
  author={Li, Shenggui and Xue, Fuzhao and Baranwal, Chaitanya and Li, Yongbin and You, Yang},
  journal={arXiv preprint arXiv:2105.13120},
  year={2021}
}

@inproceedings{soundararajan2021pdede,
  title={Pdede: Partitioned, deduplicated, delta branch target buffer},
  author={Soundararajan, Niranjan K and Braun, Peter and Khan, Tanvir Ahmed and Kasikci, Baris and Litz, Heiner and Subramoney, Sreenivas},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={779--791},
  year={2021}
}

@inproceedings{lee2015memscope,
  title={Memscope: Analyzing memory duplication on android systems},
  author={Lee, Byeoksan and Kim, Seong Min and Park, Eru and Han, Dongsu},
  booktitle={Proceedings of the 6th Asia-Pacific Workshop on Systems},
  pages={1--7},
  year={2015}
}

@techreport{Condie:EECS-2009-136,
    Author = {Condie, Tyson and Conway, Neil and Alvaro, Peter and Hellerstein, Joseph M. and Elmeleegy, Khaled and Sears, Russell},
    Title = {MapReduce Online},
    Institution = {EECS Department, University of California, Berkeley},
    Year = {2009},
    Month = {Oct},
    URL = {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-136.html},
    Number = {UCB/EECS-2009-136},
    Abstract = {MapReduce is a popular framework for data-intensive distributed computing of batch jobs. To simplify fault tolerance, the output of each MapReduce task and job is materialized to disk before it is consumed. In this paper, we propose a modified MapReduce architecture that allows data to be pipelined between operators. This extends the MapReduce programming model beyond batch processing, and can reduce completion times and improve system utilization for batch jobs as well.  We present a modified version of the Hadoop MapReduce framework that supports online aggregation, which allows users to see "early returns" from a job as it is being computed. Our Hadoop Online Prototype (HOP) also supports continuous queries, which enable MapReduce programs to be written for applications such as event monitoring and stream processing. HOP retains the fault tolerance properties of Hadoop, and can run unmodified user-defined MapReduce programs.}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{phang2022eleutherai,
  title={EleutherAI: Going Beyond" Open Science" to" Science in the Open"},
  author={Phang, Jason and Bradley, Herbie and Gao, Leo and Castricato, Louis and Biderman, Stella},
  journal={arXiv preprint arXiv:2210.06413},
  year={2022}
}

@Misc{FairScale2021,
  author =       {{FairScale authors}},
  title =        {FairScale:  A general purpose modular PyTorch library for high performance and large scale training},
  howpublished = {\url{https://github.com/facebookresearch/fairscale}},
  year =         {2021}
}

@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging $\{$AI$\}$ applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I and others},
  booktitle={13th USENIX symposium on operating systems design and implementation (OSDI 18)},
  pages={561--577},
  year={2018}
}

@article{chen2018tvm,
  title={TVM: end-to-end optimization stack for deep learning},
  author={Chen, Tianqi and Moreau, Thierry and Jiang, Ziheng and Shen, Haichen and Yan, Eddie Q and Wang, Leyuan and Hu, Yuwei and Ceze, Luis and Guestrin, Carlos and Krishnamurthy, Arvind},
  journal={arXiv preprint arXiv:1802.04799},
  volume={11},
  number={20},
  year={2018},
  publisher={CoRR}
}

@inproceedings{ma2020rammer,
  title={Rammer: Enabling holistic deep learning compiler optimizations with $\{$rTasks$\}$},
  author={Ma, Lingxiao and Xie, Zhiqiang and Yang, Zhi and Xue, Jilong and Miao, Youshan and Cui, Wei and Hu, Wenxiang and Yang, Fan and Zhang, Lintao and Zhou, Lidong},
  booktitle={14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  pages={881--897},
  year={2020}
}

@article{kisous2022and,
  title={The what, the from, and the to: The migration games in deduplicated systems},
  author={Kisous, Roei and Kolikant, Ariel and Duggal, Abhinav and Sheinvald, Sarai and Yadgar, Gala},
  journal={ACM Transactions on Storage},
  volume={18},
  number={4},
  pages={1--29},
  year={2022},
  publisher={ACM New York, NY}
}

@article{shah2020memory,
  title={Memory optimization for deep networks},
  author={Shah, Aashaka and Wu, Chao-Yuan and Mohan, Jayashree and Chidambaram, Vijay and Kr{\"a}henb{\"u}hl, Philipp},
  journal={arXiv preprint arXiv:2010.14501},
  year={2020}
}

@article{chen2015mxnet,
  title={Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems},
  author={Chen, Tianqi and Li, Mu and Li, Yutian and Lin, Min and Wang, Naiyan and Wang, Minjie and Xiao, Tianjun and Xu, Bing and Zhang, Chiyuan and Zhang, Zheng},
  journal={arXiv preprint arXiv:1512.01274},
  year={2015}
}

@inproceedings{rajbhandari2021zero,
  title={Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning},
  author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--14},
  year={2021}
}

@article{copeland1998super,
  title={Super turing-machines},
  author={Copeland, B Jack},
  journal={Complexity},
  volume={4},
  number={1},
  pages={30--32},
  year={1998}
}

@inproceedings{clements2009decentralized,
  title={Decentralized Deduplication in SAN Cluster File Systems.},
  author={Clements, Austin T and Ahmad, Irfan and Vilayannur, Murali and Li, Jinyuan and others},
  booktitle={USENIX annual technical conference},
  volume={9},
  pages={101--114},
  year={2009}
}

@article{deng2017memory,
  title={Memory deduplication: An effective approach to improve the memory system},
  author={Deng, Yuhui and Huang, Xinyu and Song, Liangshan and Zhou, Yongtao and Wang, Frank Z},
  journal={Journal of Information Science and Engineering},
  volume={33},
  number={5},
  pages={1103--1120},
  year={2017},
  publisher={Academia Sinica}
}

@inproceedings{gao2020estimating,
  title={Estimating gpu memory consumption of deep learning models},
  author={Gao, Yanjie and Liu, Yu and Zhang, Hongyu and Li, Zhengxian and Zhu, Yonghao and Lin, Haoxiang and Yang, Mao},
  booktitle={Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
  pages={1342--1352},
  year={2020}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{zhang2022dhen,
  title={DHEN: A deep and hierarchical ensemble network for large-scale click-through rate prediction},
  author={Zhang, Buyun and Luo, Liang and Liu, Xi and Li, Jay and Chen, Zeliang and Zhang, Weilin and Wei, Xiaohan and Hao, Yuchen and Tsang, Michael and Wang, Wenjun and others},
  journal={arXiv preprint arXiv:2203.11014},
  year={2022}
}

@article{egli2023chatgpt,
  title={ChatGPT, GPT-4, and other large language models: The next revolution for clinical microbiology?},
  author={Egli, Adrian},
  journal={Clinical Infectious Diseases},
  volume={77},
  number={9},
  pages={1322--1328},
  year={2023},
  publisher={Oxford University Press US}
}

@article{boiarsky2023deep,
  title={A Deep Dive into Single-Cell RNA Sequencing Foundation Models},
  author={Boiarsky, Rebecca and Singh, Nalini M and Buendia, Alejandro and Getz, Gad and Sontag, David},
  journal={bioRxiv},
  pages={2023--10},
  year={2023},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{cheng2023towards,
  title={Towards GPU Memory Efficiency for Distributed Training at Scale},
  author={Cheng, Runxiang and Cai, Chris and Yilmaz, Selman and Mitra, Rahul and Bag, Malay and Ghosh, Mrinmoy and Xu, Tianyin},
  booktitle={Proceedings of the 2023 ACM Symposium on Cloud Computing},
  pages={281--297},
  year={2023}
}

@misc{FlopsProfiler,
  title = {{Flops Profiler - Deepspeed}},
  author={deepspeed.ai},
  howpublished = {\url{https://www.deepspeed.ai/tutorials/flops-profiler/}}
}

@misc{llm-analysis-chengli,
  author = {Cheng Li},
  title = {LLM-Analysis: Latency and Memory Analysis of Transformer Models for Training and Inference},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/cli99/llm-analysis}},
}

@inproceedings{hwang2023ark,
  title={$\{$ARK$\}$:$\{$GPU-driven$\}$ Code Execution for Distributed Deep Learning},
  author={Hwang, Changho and Park, KyoungSoo and Shu, Ran and Qu, Xinyuan and Cheng, Peng and Xiong, Yongqiang},
  booktitle={20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
  pages={87--101},
  year={2023}
}

@article{nagrecha2023systems,
  title={Systems for Parallel and Distributed Large-Model Deep Learning Training},
  author={Nagrecha, Kabir},
  journal={arXiv preprint arXiv:2301.02691},
  year={2023}
}

@article{dehghani2023distributed,
  title={From distributed machine to distributed deep learning: a comprehensive survey},
  author={Dehghani, Mohammad and Yazdanparast, Zahra},
  journal={Journal of Big Data},
  volume={10},
  number={1},
  pages={158},
  year={2023},
  publisher={Springer}
}

@inproceedings{song2023optimus,
  title={Optimus-CC: Efficient Large NLP Model Training with 3D Parallelism Aware Communication Compression},
  author={Song, Jaeyong and Yim, Jinkyu and Jung, Jaewon and Jang, Hongsun and Kim, Hyung-Jin and Kim, Youngsok and Lee, Jinho},
  booktitle={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={560--573},
  year={2023}
}


@misc{Fine-tuning,
  author = {Sourab, Mangrulkar and Sylvain, Gugger and Lewis, Tunstall and Philipp, Schmid},
  title = {Fine-tuning Llama 2 70B using PyTorch FSDP},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/huggingface/blog/blob/main/ram-efficient-pytorch-fsdp.md}},
}


@article{zhang2019efficient,
  title={Efficient memory management for gpu-based deep learning systems},
  author={Zhang, Junzhe and Yeung, Sai Ho and Shu, Yao and He, Bingsheng and Wang, Wei},
  journal={arXiv preprint arXiv:1903.06631},
  year={2019}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{lee2020biobert,
  title={BioBERT: a pre-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  volume={36},
  number={4},
  pages={1234--1240},
  year={2020},
  publisher={Oxford University Press}
}

@article{walker1996mpi,
  title={MPI: a standard message passing interface},
  author={Walker, David W and Dongarra, Jack J},
  journal={Supercomputer},
  volume={12},
  pages={56--68},
  year={1996},
  publisher={ASFRA BV}
}

@inproceedings{jangda2022breaking,
  title={Breaking the computation and communication abstraction barrier in distributed machine learning workloads},
  author={Jangda, Abhinav and Huang, Jun and Liu, Guodong and Sabet, Amir Hossein Nodehi and Maleki, Saeed and Miao, Youshan and Musuvathi, Madanlal and Mytkowicz, Todd and Saarikivi, Olli},
  booktitle={Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={402--416},
  year={2022}
}

@inproceedings{wang2022tesseract,
  title={Tesseract: Parallelize the tensor parallelism efficiently},
  author={Wang, Boxiang and Xu, Qifan and Bian, Zhengda and You, Yang},
  booktitle={Proceedings of the 51st International Conference on Parallel Processing},
  pages={1--11},
  year={2022}
}

@inproceedings{lattner2021mlir,
  title={MLIR: Scaling compiler infrastructure for domain specific computation},
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)},
  pages={2--14},
  year={2021},
  organization={IEEE}
}

@inproceedings{pan2011hypervisor,
  title={Hypervisor support for efficient memory de-duplication},
  author={Pan, Ying-Shiuan and Chiang, Jui-Hao and Li, Han-Lin and Tsao, Po-Jui and Lin, Ming-Fen and Chiueh, Tzi-cker},
  booktitle={2011 IEEE 17th International Conference on Parallel and Distributed Systems},
  pages={33--39},
  year={2011},
  organization={IEEE}
}

@inproceedings{miller2012ksm++,
  title={KSM++: Using I/O-based hints to make memory-deduplication scanners more efficient},
  author={Miller, Konrad and Franz, Fabian and Groeninger, Thorsten and Rittinghaus, Marc and Hillenbrand, Marius and Bellosa, Frank},
  booktitle={Proceedings of the ASPLOS Workshop on Runtime Environments, Systems, Layering and Virtualized Environments (RESoLVEâ€™12)},
  year={2012}
}

@article{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}

@inproceedings{szegedy2015going,
  title={Going deeper with convolutions},
  author={Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--9},
  year={2015}
}

@article{rumelhart1986learning,
  title={Learning representations by back-propagating errors},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  journal={nature},
  volume={323},
  number={6088},
  pages={533--536},
  year={1986},
  publisher={Nature Publishing Group UK London}
}

@article{023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}


@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{anthropic2024claude,
  title={The claude 3 model family: Opus, sonnet, haiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  volume={1},
  year={2024}
}

@article{yang2024qwen2,
  title={Qwen2 technical report},
  author={Yang, An and Yang, Baosong and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Zhou, Chang and Li, Chengpeng and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and others},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@misc{gradientlongcontextllama3,
  title={Llama 3 Gradient: A series of long context models},
  author={Leonid Pekelis and Michael Feil and Forrest Moret and Mark Huang and Tiffany Peng},
  year={2024},
  url = {https://gradient.ai/blog/scaling-rotational-embeddings-for-long-context-language-models}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{zhang2024h2o,
  title={H2o: Heavy-hitter oracle for efficient generative inference of large language models},
  author={Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\'e}, Christopher and Barrett, Clark and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@inproceedings{lee2024infinigen,
  title={$\{$InfiniGen$\}$: Efficient generative inference of large language models with dynamic $\{$KV$\}$ cache management},
  author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
  booktitle={18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  pages={155--172},
  year={2024}
}

@article{wu2024retrieval,
  title={Retrieval head mechanistically explains long-context factuality},
  author={Wu, Wenhao and Wang, Yizhong and Xiao, Guangxuan and Peng, Hao and Fu, Yao},
  journal={arXiv preprint arXiv:2404.15574},
  year={2024}
}

@article{zheng2024attention,
  title={Attention heads of large language models: A survey},
  author={Zheng, Zifan and Wang, Yezhaohui and Huang, Yuxin and Song, Shichao and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
  journal={arXiv preprint arXiv:2409.03752},
  year={2024}
}

@article{xiao2024duoattention,
  title={DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads},
  author={Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song},
  journal={arXiv preprint arXiv:2410.10819},
  year={2024}
}

@article{fu2024not,
  title={Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning},
  author={Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen},
  journal={arXiv preprint arXiv:2410.19258},
  year={2024}
}

@article{tang2024razorattention,
  title={Razorattention: Efficient kv cache compression through retrieval heads},
  author={Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi},
  journal={arXiv preprint arXiv:2407.15891},
  year={2024}
}

@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}


@inproceedings{aminabadi2022deepspeed,
  title={Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale},
  author={Aminabadi, Reza Yazdani and Rajbhandari, Samyam and Awan, Ammar Ahmad and Li, Cheng and Li, Du and Zheng, Elton and Ruwase, Olatunji and Smith, Shaden and Zhang, Minjia and Rasley, Jeff and others},
  booktitle={SC22: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2022},
  organization={IEEE}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

 @article{peng2023rwkv,
  title={Rwkv: Reinventing rnns for the transformer era},
  author={Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and others},
  journal={arXiv preprint arXiv:2305.13048},
  year={2023}
}

@article{de2024griffin,
  title={Griffin: Mixing gated linear recurrences with local attention for efficient language models},
  author={De, Soham and Smith, Samuel L and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and others},
  journal={arXiv preprint arXiv:2402.19427},
  year={2024}
}

@article{hooper2024kvquant,
  title={Kvquant: Towards 10 million context length llm inference with kv cache quantization},
  author={Hooper, Coleman and Kim, Sehoon and Mohammadzadeh, Hiva and Mahoney, Michael W and Shao, Yakun Sophia and Keutzer, Kurt and Gholami, Amir},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}

@article{jiang2024minference,
  title={Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention},
  author={Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others},
  journal={arXiv preprint arXiv:2407.02490},
  year={2024}
}