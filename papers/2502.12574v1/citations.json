[
  {
    "index": 0,
    "papers": [
      {
        "key": "agrawal2024taming",
        "author": "Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav and Tumanov, Alexey and Ramjee, Ramachandran",
        "title": "Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "zhang2024h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient Streaming Language Models with Attention Sinks"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wu2024retrieval",
        "author": "Wu, Wenhao and Wang, Yizhong and Xiao, Guangxuan and Peng, Hao and Fu, Yao",
        "title": "Retrieval head mechanistically explains long-context factuality"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "xiao2024duoattention",
        "author": "Xiao, Guangxuan and Tang, Jiaming and Zuo, Jingwei and Guo, Junxian and Yang, Shang and Tang, Haotian and Fu, Yao and Han, Song",
        "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "fu2024not",
        "author": "Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen",
        "title": "Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "tang2024razorattention",
        "author": "Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi",
        "title": "Razorattention: Efficient kv cache compression through retrieval heads"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jiang2024minference",
        "author": "Jiang, Huiqiang and LI, YUCHENG and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others",
        "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "xiong2024layerkv",
        "author": "Xiong, Yi and Wu, Hao and Shao, Changxu and Wang, Ziqing and Zhang, Rui and Guo, Yuhong and Zhao, Junping and Zhang, Ke and Pan, Zhenxuan",
        "title": "Layerkv: Optimizing large language model serving with layer-wise kv cache management"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "he2024fastdecode",
        "author": "He, Jiaao and Zhai, Jidong",
        "title": "FastDecode: High-Throughput GPU-Efficient LLM Serving using Heterogeneous Pipelines"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "jiang2024neo",
        "author": "Jiang, Xuanlin and Zhou, Yang and Cao, Shiyi and Stoica, Ion and Yu, Minlan",
        "title": "Neo: Saving gpu memory crisis with cpu offloading for online llm inference"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "sun2024shadowkv",
        "author": "Sun, Hanshi and Chang, Li-Wen and Bao, Wenlei and Zheng, Size and Zheng, Ningxin and Liu, Xin and Dong, Harry and Chi, Yuejie and Chen, Beidi",
        "title": "Shadowkv: Kv cache in shadows for high-throughput long-context llm inference"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xu2024vtensor",
        "author": "Xu, Jiale and Zhang, Rui and Guo, Cong and Hu, Weiming and Liu, Zihan and Wu, Feiyang and Feng, Yu and Sun, Shixuan and Shao, Changxu and Guo, Yuhong and others",
        "title": "vtensor: Flexible virtual tensor management for efficient llm serving"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "lee2024infinigen",
        "author": "Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong",
        "title": "InfiniGen: Efficient generative inference of large language models with dynamic KV cache management"
      }
    ]
  }
]