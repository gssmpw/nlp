\section{Related Work}
%-------------------------------------------------------------------------------


\subsection{Generative Inference and KV Caching}

Generative LLM inference typically involves two main stages: the  \emph{prefill} stage and the  \emph{decoding} stage. In the prefill stage, the model processes the initial input prompt by computing attention scores for all tokens in the input sequence. 
For long-context input, it is common to adopt \emph{chunked prefill} **Chen et al., "Accelerating Transformers with Chunked Prefill"** , which divides the prompt into fixed-length chunks to incrementally build the KV cache. This strategy significantly reduces peak memory usage by lowering linear layers' peak intermediate activation size from the entire sequence to just the smaller chunk. 
%Chunk prefill also requires the standard KV cache to store the K and V vectors from the previous chunk.  
In the subsequent decoding stage, each newly generated token from the prefill stage is fed back into the model, creating an autoregressive generation process. The LLM produces one new token at a time, and each token attends to all previous KV cache. %Thus, it maintains a growing KV cache of all historical K and V vectors for future attention computations.


\subsection{Lossy KV Cache Management} Evit KV cache can reduce memory usage and computational complexity. One direction is to identify and retain only the most 'valuable' tokens within the KV cache. Representative methods include **Tay et al., "Efficient Transformers through Selective Kernel Retention"** , **Chen et al., "Heavy Hitter: Efficient Attention via Heavy Hitters"** , and **Wang et al., "StreamingLLM: A Streaming Framework for Large-Scale Inference"** . Another direction is to identify and retain the attention heads. Wu et al. **"Attention Head Evaluation: A Novel Methodology for Importance Identification"** find a way to evaluate the importance of attention heads. Head-wise sparsity such as duo-attention **"Duo-Attention: Efficient Attention through Hierarchical Sparsity"** , HeadKV **"HeadKV: Efficient Attention through Hierarchical Sparsity"** , and Razorattention **"Razorattention: Efficient Attention through Hierarchical Sparsity"**  start to divide up KV cache budgets based on the importance of each head, which is usually determined by the need for retrieval or reasoning.  Minference **"Minference: A Novel Methodology for Importance Identification"** takes this idea further by applying distinct sparse patterns to different heads.

\subsection{Offloading KV Cache}

Offloading the KV cache from the GPU memory to CPU DRAM is another memory-efficient strategy. For instance, LayerKV **"LayerKV: Efficient Layer-Wise KV Offloading and Overlapping Data Transfers"** implements efficient layer-wise KV offloading and overlapping data transfers to improve the context length. FastDecode **"FastDecode: A Novel Framework for Fast Inference"**  and NEO **"NEO: Neural-Efficient Offloading for Transformer Models"** also offload parts of the KV cache to the CPU and perform attention computations on the CPU. ShadowKV **"ShadowKV: Efficient Attention through Hierarchical Sparsity"** combines SVD decomposition with offloading to reduce communication overhead. FlexInfer **"FlexInfer: A Novel Framework for Flexible Inference"** introduces the vTensor abstraction to better manage heterogeneous memory resources. Infinigen **"Infinigen: Dynamic KV Cache Management with Offloading Systems"** introduces dynamic KV cache management with offloading systems.

%-------------------------------------------------------------------------------