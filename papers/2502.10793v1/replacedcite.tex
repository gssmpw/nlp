\section{Related Works}
Estimating training sample influence is crucial for model optimization and interpretability. While LOO provides ground truth, it is computationally intensive. Influence functions____ and recent extensions ____  offer a faster alternative, estimating the impact of removing a single training sample on model performance at convergence. However, their effectiveness is limited in non-convex and non-convergent scenarios____. 

Shapley Value-based approaches ____ provide a robust, equitable valuation of individual sample contributions by considering all possible subsets of training data. Efficient approximation algorithms____ and domain-specific extensions____ have improved scalability, but remain computationally expensive for large-scale problems.

Data cleansing and pruning focus on removing noisy or irrelevant data. SGD-influence ____ analyzes the gradient descent process and estimates sample influence across the entire training trajectory. Our proposed DIT extends this approach, enabling influence estimation within arbitrary time windows and providing detailed experimental evaluation. Forgetting events____ and early-training scores____ enable efficient data pruning. MOSO ____ identifies less informative samples via gradient deviations, and YOCO ____ enables flexible resizing of condensed datasets.

Despite these advancements, analyzing sample influence within arbitrary time windows during training remains challenging. DIT addresses this gap by providing a computationally efficient method for dynamic influence tracking without relying on convexity and convergence assumptions. It enables multidimensional influence measurement in a single training process, offering a comprehensive understanding of sample importance.