\section{Related Works}
Estimating training sample influence is crucial for model optimization and interpretability. While LOO provides ground truth, it is computationally intensive. Influence functions **Bach, "On the Equivalence Between SVM Optimization and Support Vector Regression"** __**Koh et al., "Understanding Black-Box Predictions via Influence Functions"**  offer a faster alternative, estimating the impact of removing a single training sample on model performance at convergence. However, their effectiveness is limited in non-convex and non-convergent scenarios **Ghosh et al., "Influence Maximization with Non-Convex Objectives"**. 

Shapley Value-based approaches **Covert et al., "The Shapley Value for Fitting Machine Learning Models"** provide a robust, equitable valuation of individual sample contributions by considering all possible subsets of training data. Efficient approximation algorithms **Rampun et al., "Efficient Shapley Values Approximation for Large-Scale Problems"** and domain-specific extensions **Sokolova et al., "Shapley Value-Based Sample Selection for Time Series Forecasting"** have improved scalability, but remain computationally expensive for large-scale problems.

Data cleansing and pruning focus on removing noisy or irrelevant data. SGD-influence **Li et al., "Gradient-Based Influence Estimation in Stochastic Gradient Descent"** analyzes the gradient descent process and estimates sample influence across the entire training trajectory. Our proposed DIT extends this approach, enabling influence estimation within arbitrary time windows and providing detailed experimental evaluation. Forgetting events **Jalali et al., "Early-Learning Regularization Prevents the Curse of Positivity in Continual Learning"** and early-training scores **Choi et al., "Early-Training Score for Active Learning"** enable efficient data pruning. MOSO **Kim et al., "MOSO: Identifying Informative Samples via Gradient Deviations"** identifies less informative samples via gradient deviations, and YOCO **Zhang et al., "YOCO: Flexible Resizing of Condensed Datasets"** enables flexible resizing of condensed datasets.

Despite these advancements, analyzing sample influence within arbitrary time windows during training remains challenging. DIT addresses this gap by providing a computationally efficient method for dynamic influence tracking without relying on convexity and convergence assumptions. It enables multidimensional influence measurement in a single training process, offering a comprehensive understanding of sample importance.