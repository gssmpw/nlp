\section{Related Works}
Estimating training sample influence is crucial for model optimization and interpretability. While LOO provides ground truth, it is computationally intensive. Influence functions~\cite{koh2017understanding} and recent extensions ~\cite{guo2021fastif,schioppa2022scaling,choe2024your}  offer a faster alternative, estimating the impact of removing a single training sample on model performance at convergence. However, their effectiveness is limited in non-convex and non-convergent scenarios~\cite{basu2021influence}. 

Shapley Value-based approaches ~\cite{ghorbani2019data} provide a robust, equitable valuation of individual sample contributions by considering all possible subsets of training data. Efficient approximation algorithms~\cite{jia2019towards,jia2021scalability,xu2021validation} and domain-specific extensions~\cite{schoch2022cs, sun2023shapleyfl,fan2022improving} have improved scalability, but remain computationally expensive for large-scale problems.

Data cleansing and pruning focus on removing noisy or irrelevant data. SGD-influence ~\cite{hara2019data} analyzes the gradient descent process and estimates sample influence across the entire training trajectory. Our proposed DIT extends this approach, enabling influence estimation within arbitrary time windows and providing detailed experimental evaluation. Forgetting events~\cite{toneva2018empirical} and early-training scores~\cite{paul2021deep} enable efficient data pruning. MOSO ~\cite{tan2024data} identifies less informative samples via gradient deviations, and YOCO ~\cite{he2023you} enables flexible resizing of condensed datasets.

Despite these advancements, analyzing sample influence within arbitrary time windows during training remains challenging. DIT addresses this gap by providing a computationally efficient method for dynamic influence tracking without relying on convexity and convergence assumptions. It enables multidimensional influence measurement in a single training process, offering a comprehensive understanding of sample importance.