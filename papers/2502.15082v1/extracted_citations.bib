@inproceedings{NEURIPS2023_3abe23bf,
 author = {Tan, Haoru and Wu, Sitong and Du, Fei and Chen, Yukang and Wang, Zhibin and Wang, Fan and Qi, Xiaojuan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {18251--18262},
 publisher = {Curran Associates, Inc.},
 title = {Data Pruning via Moving-one-Sample-out},
 volume = {36},
 year = {2023}
}

@article{chen2023unlearn,
  title={Unlearn what you want to forget: Efficient unlearning for llms},
  author={Chen, Jiaao and Yang, Diyi},
  journal={arXiv preprint arXiv:2310.20150},
  year={2023}
}

@inproceedings{dai2022knowledge,
  title={Knowledge Neurons in Pretrained Transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={8493--8502},
  year={2022}
}

@inproceedings{de-cao-etal-2021-editing,
    title = "Editing Factual Knowledge in Language Models",
    author = "De Cao, Nicola  and
      Aziz, Wilker  and
      Titov, Ivan",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.522/",
    doi = "10.18653/v1/2021.emnlp-main.522",
    pages = "6491--6506",
    abstract = "The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix {\textquoteleft}bugs' or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor`s efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a {\textquoteleft}probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at \url{https://github.com/nicola-decao/KnowledgeEditor}"
}

@inproceedings{debenedetti2024privacy,
  title={Privacy side channels in machine learning systems},
  author={Debenedetti, Edoardo and Severi, Giorgio and Carlini, Nicholas and Choquette-Choo, Christopher A and Jagielski, Matthew and Nasr, Milad and Wallace, Eric and Tram{\`e}r, Florian},
  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},
  pages={6861--6848},
  year={2024}
}

@article{eldan2023s,
  title={Who's Harry Potter? Approximate Unlearning in LLMs},
  author={Eldan, Ronen and Russinovich, Mark},
  journal={arXiv preprint arXiv:2310.02238},
  year={2023}
}

@inproceedings{killamsetty2021glister,
  title={Glister: Generalization based data subset selection for efficient and robust learning},
  author={Killamsetty, Krishnateja and Sivasubramanian, Durga and Ramakrishnan, Ganesh and Iyer, Rishabh},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={9},
  pages={8110--8118},
  year={2021}
}

@inproceedings{killamsetty2021grad,
  title={Grad-match: Gradient matching based data subset selection for efficient deep model training},
  author={Killamsetty, Krishnateja and Durga, Sivasubramanian and Ramakrishnan, Ganesh and De, Abir and Iyer, Rishabh},
  booktitle={International Conference on Machine Learning},
  pages={5464--5474},
  year={2021},
  organization={PMLR}
}

@inproceedings{maini2024tofu,
  title={TOFU: A Task of Fictitious Unlearning for LLMs},
  author={Maini, Pratyush and Feng, Zhili and Schwarzschild, Avi and Lipton, Zachary Chase and Kolter, J Zico},
  booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
  year={2024}
}

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{wei2015submodularity,
  title={Submodularity in data subset selection and active learning},
  author={Wei, Kai and Iyer, Rishabh and Bilmes, Jeff},
  booktitle={International conference on machine learning},
  pages={1954--1963},
  year={2015},
  organization={PMLR}
}

@article{wen2024know,
  title={Know your limits: A survey of abstention in large language models},
  author={Wen, Bingbing and Yao, Jihan and Feng, Shangbin and Xu, Chenjun and Tsvetkov, Yulia and Howe, Bill and Wang, Lucy Lu},
  journal={arXiv preprint arXiv:2407.18418},
  year={2024}
}

@article{yao2023large,
  title={Large language model unlearning},
  author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  journal={arXiv preprint arXiv:2310.10683},
  year={2023}
}

@article{zhang2024right,
  title={Right to be forgotten in the era of large language models: Implications, challenges, and solutions},
  author={Zhang, Dawen and Finckenberg-Broman, Pamela and Hoang, Thong and Pan, Shidong and Xing, Zhenchang and Staples, Mark and Xu, Xiwei},
  journal={AI and Ethics},
  pages={1--10},
  year={2024},
  publisher={Springer}
}

