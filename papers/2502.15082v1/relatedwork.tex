\section{Background and Related Work}
\paragraph{Unlearning Methods for LLMs.} Machine unlearning is broadly categorized into \emph{exact unlearning}, which ensures the model is indistinguishable from one retrained without the forget data, and \emph{approximate unlearning}, which efficiently modifies existing model parameters to approximate this effect without full retraining. 
Due to the cost of retraining LLMs, most unlearning applied to LLMs (including ours) falls into the second category. 
One such approach trains the model to output an uninformative response instead of knowledge in the ``forget set'' via RLHF \citep{ouyang2022training}, maximizing the probability of a predefined response like, \emph{``I don't know''} \citep{wen2024know}. 
\citet{yao2023large} introduce a gradient ascent-based method for unlearning harmful content, outputting whitespace for harmful prompts. 
While effective in reducing harmful responses, this approach leads to notable performance degradation on normal prompts, underscoring the need for balance. 
\citet{chen2023unlearn} propose an unlearning framework that introduces an unlearning layer, demonstrating success in both classification and generation tasks with less extreme performance degradation. 
\citet{eldan2023s} explore a novel network architecture designed to specifically unlearn copyrighted content in LLMs. 
On the evaluation side, \citet{maini2024tofu} present a benchmark for evaluating unlearning, which we use in our evaluation. 
Despite these advancements, balancing the trade-off between forgetting accuracy and preserving model utility remains an open challenge, motivating the need for more data-driven solutions. 
Our work addresses this gap by proposing a principled framework that focuses on minimizing collateral damage through data selection.

\paragraph{Model Editing for Unlearning.} Model editing provides an alternative approach to unlearning by directly modifying model weights to forget target facts \citep{de-cao-etal-2021-editing, dai2022knowledge, mitchell2022fast, meng2022locating}. This method aligns with privacy requirements \citep{zhang2024right}, avoids data-side interventions \citep{debenedetti2024privacy}, and protects against white-box extraction attacks. Following model editing work like \citet{patil2024unlearning}, our framework employs LoRA-based weight updates for controlled unlearning via standard unlearning objectives (See \cref{app:editing} for more details). 





\paragraph{Coreset Selection.} Coreset selection identifies representative subsets that preserve key dataset properties, improving computational efficiency. Given the NP-hard complexity of the exhaustive search, methods have focused on optimizing coverage, diversity, or importance \citep{sener2018active, NEURIPS2023_3abe23bf}. By recognizing unequal contributions of data points, coreset selection has proven effective in supervised learning \citep{wei2015submodularity, killamsetty2021glister, killamsetty2021grad}, enabling efficient performance. 
Our work forms new connections between these methods and the problem of unlearning in LLMs, where preserving utility and minimizing collateral damage are critical.