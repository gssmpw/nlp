    %%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage{multirow}

\usepackage{subcaption}



\usepackage{hyperref}
\usepackage{enumitem} % Include this in the preamble



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\ourmethod}{\textsc{UPCORE}}

\usepackage[accepted]{icml2025}



\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{adjustbox}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:


\begin{document}

\twocolumn[

\icmltitle{UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning}
\icmltitlerunning{UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning}





\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Vaidehi Patil}{yyy}
\icmlauthor{Elias Stengel-Eskin}{yyy} 
\icmlauthor{Mohit Bansal}{yyy} 


\end{icmlauthorlist}

\icmlaffiliation{yyy}{UNC Chapel Hill} 

\icmlcorrespondingauthor{Vaidehi Patil}{vaidehi@cs.unc.edu}


\icmlkeywords{Machine unlearning, dataset selection, LLM model editing}

\vskip 0.3in
]


\printAffiliationsAndNotice{}  

\begin{abstract}
User specifications or legal frameworks often require information to be removed from pretrained models, including large language models (LLMs). This requires deleting or ``forgetting'' a set of data points from an already-trained model, which typically degrades its performance on other data points. Thus, a balance must be struck between removing information and keeping the model's other abilities intact, with a failure to balance this trade-off leading to poor deletion or an unusable model. 
To this end, we propose \ourmethod{} (Utility-Preserving Coreset Selection), a method-agnostic data selection framework for mitigating collateral damage during unlearning. 
Finding that the model damage is correlated with the variance of the model's representations on the forget set, we selectively prune the forget set to remove outliers, thereby minimizing model degradation after unlearning. We evaluate \ourmethod{} across three standard unlearning methods consistently achieving a superior balance between the competing objectives of deletion efficacy and model preservation. To better evaluate this trade-off, we introduce a new metric, measuring the area-under-the-curve (AUC) across standard metrics.
We find that \ourmethod{} improves both standard metrics and AUC, benefitting from positive transfer between the coreset and pruned points while reducing negative transfer from the forget set to points outside of it.\footnote{Our code and data is available at: \url{https://github.com/Vaidehi99/UPCORE}}  
\end{abstract}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{figures/fig1_new.pdf}
    \vspace{-1em}
    \caption{\textbf{Left:} Standard unlearning methods are applied equally to all points in the forget set. 
    Here, outlier points in the model's hidden space (visualized in 2D) contribute to the unintentional forgetting of points outside of the forget set (i.e. collateral damage).
    \textbf{Right:} By finding a lower-variance coreset within the forget set, \ourmethod{} reduces damage while maintaining forget performance via positive transfer from the coreset to the pruned points.} 
    \label{fig:intro}
\end{figure}

\section{Introduction}
\label{sec:intro}


The widespread deployment of ML models, particularly large language models (LLMs), has raised significant concerns regarding data privacy, regulatory compliance, and ethical AI practices. These models are often trained on vast amounts of uncurated data scraped from the internet, inherently capturing sensitive, copyrighted, or undesirable content \citep{shokri2017membership, carlini2019secret}. As regulations like the European Unionâ€™s General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA)  empower individuals with the ``right to be forgotten'', the need for efficient techniques that remove specific data or topics from trained models has become increasingly critical. 
Machine unlearning has emerged as a promising solution, enabling the targeted removal of data, concepts, or facts without the computational expense of retraining from scratch. 
Moreover, machine unlearning has benefits beyond compliance, addressing broader challenges such as mitigating harmful outputs, preserving intellectual property rights, and aligning LLMs with ethical and societal expectations \citep{jang2023knowledge}. 
These practical uses have spurred growing interest in understanding, rethinking, and improving model editing and unlearning methodologies \citep{liu2024rethinking, hase2024fundamental}. 


Given the growing adoption of LLMs, 
past work has proposed methods for developing and evaluating techniques for removing knowledge or skills from LLMs \citep{cao2015towards, bourtoule2021machine, nguyen2022survey} and steering their behavior in targeted ways \citep{sinitsin2020editable, meng2022locating}. 
However, these editing methods often involve modifying the model in ways that lead to unintended consequences, such as reduced model utility on tasks or knowledge unrelated to the target.
Thus, a balance must be struck between the successful unlearning of undesired information and maintaining the utility of the model by minimizing collateral damage. 
For this reason, current approaches generally evaluate unlearning or model editing methods based on their efficacy in altering intended knowledge -- measured by how successfully a ``forget set'' is removed -- and based on their collateral effects on unrelated behaviors, i.e. the accuracy on a ``retain set''. 
This kind of evaluation is especially crucial in realistic settings where unlearning happens on a \emph{topic}.
Such topic-based unlearning commonly arises in practical scenarios, for example, when deleting all information associated with an individual or with sensitive domains \citep{li2024the}.
Here, the likelihood of over-generalization to similar topics beyond the domain being deleted is high. 

A key gap in existing research lies in understanding the specific data characteristics that drive overgeneralization and collateral effects during unlearning. 
While prior work \citep{sheshadri2024latent, chowdhury2024towards} has measured damage resulting from unlearning,
it does not investigate how attributes of the data -- such as its variance -- contribute to collateral damage or whether these attributes can be controlled to optimize the trade-off between deletion efficacy and utility retention. 
Focusing on a topic-based setting where the forget set comprises semantically coherent groups of information, we seek to address these questions:
\begin{enumerate}[itemsep=0.2em]
    \item {\it What measurable attributes of the forget set drive collateral effects during the unlearning process?}
    \item {\it Can these attributes be systematically controlled to optimize the trade-off between deletion effectiveness and model utility?}
\end{enumerate}


We investigate which properties of the forget data correlate with collateral damage during unlearning. 
Our analysis reveals a strong positive correlation between the variance of the model's hidden states corresponding to datapoints in the forget set (hidden state variance, or HSV), and the extent of collateral damage to the model after unlearning.
In other words, unlearning a set of widely-distributed datapoints (as shown in \cref{fig:intro} (left)) leads to more damage than unlearning a more densely-distributed set. 
Building on this insight, we hypothesize that selectively curating a coreset with lower variance from the larger forget set can help optimize this trade-off, as shown in \cref{fig:intro} (right). 



To this end, we introduce \ourmethod{}, which constructs a core forget set by systematically identifying and pruning data points in the forget set that contribute most to the variance and thereby to collateral damage. \ourmethod{} organizes points into an Isolation Forest \citep{IsolationForest}, which identifies anomalous points in a set. 
By pruning these points, we reduce the variance within the forget set, which we find leads to less damage. 
Crucially, in addition to reducing collateral damage, \ourmethod{} in fact leverages it by identifying two separate kinds of collateral effects: (1) \textbf{Negative collateral damage}: Unintended degradation of unrelated model capabilities and (2) \textbf{Positive collateral transfer}: The intended impact on pruned data points removed to form the core forget set. 
This is illustrated in \cref{fig:intro}, where pruned outlier points are still unlearned -- despite not being a part of the coreset used for unlearning -- due to positive transfer, and is further highlighted by our results in \cref{tab:results} and \cref{tab:rouge}, which show that \ourmethod{} results in better unlearning than a randomly selected subset while also having better knowledge retention on non-forget data. 
Moreover, \ourmethod{} is method-agnostic:
by focusing solely on the data, \ourmethod{} can be applied to any data-driven unlearning framework. 



We evaluate \ourmethod{} in prompt completion and question-answering settings and across three standard unlearning methods: Gradient Ascent \citep{jang2023knowledge}, Refusal \citep{ouyang2022training} and Negative Preference Optimization (NPO) \citep{zhang2024negative}, applying each unlearning algorithm directly to the optimized core forget set obtained using \ourmethod{}, rather than the entire forget set.
We measure three critical dimensions: (1)
\emph{Unlearning effectiveness}, measured by the successful removal of targeted knowledge in the forget set, paraphrased versions of removed information as well as prompts attempting to jailbreak the model.; (2) \textit{Unintended damage}, where we quantify collateral effects on unrelated model capabilities; and (3) \textit{Intended transfer}, where we analyze the impact on the pruned data points that were removed from the core forget set. 
While we follow past unlearning work in the metrics we use to measure the trade-off between the competing objectives, we also note that the current suite of metrics measures performance at a fixed point during unlearning.
This can make comparisons across methods hard, as the trade-off between deletion efficacy and model utility varies across unlearning epochs.
To address this, in addition to showing improvements on standard metrics, we introduce a novel set of metrics that report the area-under-the-curve (AUC) for the standard unlearning metric suite, reporting not just the performance at one fixed timestep, but measuring \emph{how a method trades off deletion with model utility across checkpoints (see \cref{fig:auc_comparison}).}


Empirically, we find that across all three unlearning methods, \ourmethod{} consistently has the highest AUC compared to baselines of unlearning on the complete forget set and choosing a random subset of forget points.
In other words, \ourmethod{} forms a Pareto frontier, doing a better job of maximizing unlearning effectiveness while also minimizing model damage.
Moreover, \ourmethod{} positively leverages generalization by transferring unlearning from the core set to the high-variance outlier points that were removed from the core forget set.
Notably, it consistently outperforms baselines across \emph{all} unlearning methods; this holds true when comparing AUC across multiple metrics (e.g. ROUGE on a ``retain'' set, on neighborhood data closely related to the forget set but not in it, etc.). \ourmethod{}'s superior trade-off effectively generalizes to variations of the forgotten information, performing well on paraphrased versions of forgotten prompts as well as prompts intended to jailbreak the model.
We also see these gains reflected in static evaluations of one checkpoint (as opposed to AUC, which evaluates across checkpoints); here, \ourmethod{} obtains lower (better) ROUGE on the forget set than the random baseline while simultaneously incurring less model damage than the random and complete baselines, with the best (highest) ROUGE across all data not in the forget set. 




 


\section{Background and Related Work}



\paragraph{Unlearning Methods for LLMs.} Machine unlearning is broadly categorized into \emph{exact unlearning}, which ensures the model is indistinguishable from one retrained without the forget data, and \emph{approximate unlearning}, which efficiently modifies existing model parameters to approximate this effect without full retraining. 
Due to the cost of retraining LLMs, most unlearning applied to LLMs (including ours) falls into the second category. 
One such approach trains the model to output an uninformative response instead of knowledge in the ``forget set'' via RLHF \citep{ouyang2022training}, maximizing the probability of a predefined response like, \emph{``I don't know''} \citep{wen2024know}. 
\citet{yao2023large} introduce a gradient ascent-based method for unlearning harmful content, outputting whitespace for harmful prompts. 
While effective in reducing harmful responses, this approach leads to notable performance degradation on normal prompts, underscoring the need for balance. 
\citet{chen2023unlearn} propose an unlearning framework that introduces an unlearning layer, demonstrating success in both classification and generation tasks with less extreme performance degradation. 
\citet{eldan2023s} explore a novel network architecture designed to specifically unlearn copyrighted content in LLMs. 
On the evaluation side, \citet{maini2024tofu} present a benchmark for evaluating unlearning, which we use in our evaluation. 
Despite these advancements, balancing the trade-off between forgetting accuracy and preserving model utility remains an open challenge, motivating the need for more data-driven solutions. 
Our work addresses this gap by proposing a principled framework that focuses on minimizing collateral damage through data selection.

\paragraph{Model Editing for Unlearning.} Model editing provides an alternative approach to unlearning by directly modifying model weights to forget target facts \citep{de-cao-etal-2021-editing, dai2022knowledge, mitchell2022fast, meng2022locating}. This method aligns with privacy requirements \citep{zhang2024right}, avoids data-side interventions \citep{debenedetti2024privacy}, and protects against white-box extraction attacks. Following model editing work like \citet{patil2024unlearning}, our framework employs LoRA-based weight updates for controlled unlearning via standard unlearning objectives (See \cref{app:editing} for more details). 





\paragraph{Coreset Selection.} Coreset selection identifies representative subsets that preserve key dataset properties, improving computational efficiency. Given the NP-hard complexity of the exhaustive search, methods have focused on optimizing coverage, diversity, or importance \citep{sener2018active, NEURIPS2023_3abe23bf}. By recognizing unequal contributions of data points, coreset selection has proven effective in supervised learning \citep{wei2015submodularity, killamsetty2021glister, killamsetty2021grad}, enabling efficient performance. 
Our work forms new connections between these methods and the problem of unlearning in LLMs, where preserving utility and minimizing collateral damage are critical.








\section{Methods}



We introduce \ourmethod{} (Utility-Preserving Coreset Design for Unlearning),
an approach motivated by the observation that certain data points in the forget set disproportionately contribute to collateral damage during unlearning, primarily by increasing data variance. 
To address this, \ourmethod{} reformulates pruning for core forget set selection as an outlier detection task, where outlier data points i.e. points with the greatest influence on utility degradation are identified and pruned. 
By minimizing variance within the forget set, \ourmethod{} reduces unintended negative effects, ensuring more effective and targeted unlearning.









\subsection{Problem Definition}
\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{figures/fig2_new.pdf}
    \vspace{-1.0em}
    \caption{\ourmethod{} has four stages. First, we extract hidden states from the LLM to be modified; second, we identify outliers using Isolation Forests; third, we prune outliers to select a core forget set, and fourth, we perform unlearning on the coreset. 
    }
    \label{fig:method}
\end{figure*}


Let \( D \) be the dataset used to train a model \( M \), with \( D_F \subset D \) representing the forget set to be unlearned. 
Directly unlearning \( D_F \), i.e., applying an unlearning algorithm \( \mathcal{U} \) to the model \( M \) using only the data points in \( D_F \), produces an updated model \( M' = \mathcal{U}(M, D_F) \). However, this often leads to significant performance degradation on the retained dataset \( D \setminus D_F \) due to over-generalization, where unlearning updates undesirably propagate beyond the forget set, impacting unrelated data points in $D \setminus D_F$. 


To address this issue, we aim to construct a pruned forget set \( D_C \subset D_F \), henceforth referred to as the core forget set, by removing points in \( D_F \) that disproportionately drive over-generalization. 
The goal is to balance two competing objectives: (i) minimizing negative collateral damage, i.e., performance degradation on \( D \setminus D_F \), and (ii) maintaining deletion accuracy, i.e., ensuring that the unlearning \( D_C \) effectively deletes the undesirable knowledge associated with the original forget set \( D_F \). 
More formally, given a damage metric \( \text{Damage}_{(\mathcal{U}, D_C)}(M, M', D \setminus D_F) \) that quantifies the impact of unlearning $D_C$ on \( D \setminus D_F \), and a deletion accuracy metric \( \text{DelAcc}_{(\mathcal{U}, D_C)}(M', D_F) \) that evaluates the effectiveness of \( \mathcal{U} \) in forgetting \( D_F \) after unlearning on \( D_C \), the problem can be formulated as an optimization task:


\vspace{-\baselineskip}
\begin{align*}
D_C = \arg\min_{D_C \subseteq D_F} & \bigg( 
 \text{Damage}_{(\mathcal{U}, D_C)}\big(M, M', (D \setminus D_F)\big) \\
& - \lambda \cdot \text{DelAcc}_{(\mathcal{U}, D_C)}\big(M', D_F\big) 
\bigg)
\end{align*}

where \( \lambda > 0 \) is a hyperparameter that controls the trade-off between the competing objectives.




\subsection{Variance as a Measure of Collateral Damage} 
\label{sec:analysis}



Building on prior work analyzing the cross-task generalization of forgetting methods \citep{zhang2024unforgettable}, we investigate the relationship between attributes of the forget set \( D_F \) and their impact on collateral damage during unlearning i.e. \(\text{Damage}_{(\mathcal{U}, D_C)}\big(M, M', (D \setminus D_F)\big)\). Specifically, we identify the variance \(Var( D_F )\) as a critical predictor of overgeneralization. 
To systematically evaluate this relationship, we analyze question-answer (QA) pairs generated from Wikipedia documents across diverse topics.
% Each topic-specific QA dataset serves as the forget set \( D_F \). 
Each topic-specific dataset acts as the forget set \( D_F \), with unlearning applied sequentially, one topic at a time.
For each forget set, we compute variance using the hidden states of the last token and the penultimate layer of the question. 
We then compute retain set performance as the model utility metric proposed by \citet{maini2024tofu}, which measures performance on preserved data points after unlearning. 
The results, visualized in \cref{fig:hsv} in \cref{append:hsv_analysis}, demonstrate a strong negative correlation between HSV and model utility, indicating that variance is a potential driver of over-generalization. 
These findings underscore the importance of identifying and excluding points that lead to higher variance i.e. outliers to mitigate utility loss. 
In \cref{append:hsv_analysis} we show similar analyses for other attributes such as model confidence and gradient similarity but find no strong correlation between utility degradation and these attributes.



\subsection{\ourmethod{}: Core Forget Set Selection}




To achieve variance minimization in the forget set \( D_F \), \ourmethod{} frames the problem as an outlier detection task.  
\ourmethod{} provides two key benefits: (1) It mitigates negative collateral damage by pruning outliers to form a more compact core forget set, and (2) It strategically exploits collateral over-generalization to extend unlearning beyond the core forget set, effectively removing the pruned points as well.
As shown in \cref{fig:intro}, what might traditionally be viewed as detrimental collateral damage -- when it affects points outside the forget set ($D_F$) -- can be turned to our advantage when it impacts untrained data points within the forget set that were pruned (\( D_F \setminus D_C\)).


To detect these outliers, we use the Isolation Forest algorithm \citep{IsolationForest}, an unsupervised learning technique that efficiently identifies anomalous data points. 
Isolation Forest works by recursively partitioning the dataset using random feature selections and random split values. 
Points that are isolated with fewer partitions, i.e. are isolated more easily, are considered outliers, as they differ significantly from the majority of the data. 
This makes the Isolation Forest algorithm particularly effective for high-dimensional data where traditional distance-based outlier detection methods may fail. 
These outliers, characterized by their isolation in the feature space, are likely to contribute to high variance and over-generalization during the unlearning process.
\ourmethod{} proceeds as follows (illustrated in \cref{fig:method}):


  {\bf Stage 1: Hidden Feature Extraction:} We extract hidden state representations \( \mathcal{H} \) from the model's penultimate layer (See \cref{fig:method} left), corresponding to the final token of each question in \( D_F \). These representations, which reflect the model's internal representation of the data, serve as input features for outlier detection. This step is guided by our analysis in \cref{sec:analysis}, which highlights the strong link between hidden state variance and collateral damage. 

  {\bf Stage 2: Training the Isolation Forest and Computing Anomaly Scores:}  
  We train an Isolation Forest model \( \mathcal{I} \) on the forget set \( D_F \) to model its distribution, recursively partitioning the data to detect outliers (see \cref{fig:method} middle). Points isolated more quickly and requiring fewer splits are flagged as outliers, indicating disproportionate contributions to variance in the hidden state space. For each \( d \in D_F \), \( \mathcal{I} \) assigns an anomaly score \( \text{score}(d) \) based on the average path length \( h(d) \) required to isolate the point across an ensemble of binary trees. Shorter path lengths correspond to higher anomaly scores, indicating points that contribute to variance and thus collateral effects. 
  Additional details are provided in \cref{sec:appendix}.



\textbf{Stage 3: Prune Outliers and Setting Stopping Criterion:} To construct the pruned coreset \( D_C \), we apply a threshold \( \tau \) on the anomaly scores from the Isolation Forest model: $D_C = \{ d \in D_F \mid \text{score}(d) \leq \tau \}$. 
Data points with scores above \( \tau \) are excluded as outliers, as they disproportionately contribute to variance. 
We hypothesize that removing these outliers will reduce utility degradation while preserving core information for forgetting. The threshold \( \tau \) is determined via a stopping criterion, which can be chosen as follows: (1) Coreset Size Control: Specify a desired coreset size \( |D_C| \) to ensure an appropriate number of inliers (2) Proportional Pruning: Select the top \( k\% \) of points with the lowest anomaly scores to maintain a consistent pruning ratio. By selecting \( \tau \) based on user requirements, \ourmethod{} provides fine-grained control over the trade-off, ensuring the construction of a robust coreset. In practice, we prune 10\% of the data points in our main experiments and additionally conduct scaling experiments that vary the pruning percentage to analyze its impact on the trade-off dynamics.




{\bf Stage 4: Unlearning on the Coreset:} After selecting the pruned coreset \( D_C \), \ourmethod{} applies the unlearning algorithm \( \mathcal{U} \) to the model \( M \), resulting in \( M'_{UPCORE} = \mathcal{U}(M, D_C) \) (See \cref{fig:method} end). This process removes the influence of \( D_F \) while minimizing utility degradation on \( D \setminus D_F \). By focusing on \( D_C \), our approach ensures targeted unlearning and  positively leverages collateral effects, as unlearning \( D_C \) also deletes much of \( D_F \)'s influence, even those parts not explicitly included in 
\( D_C \).



\section{Experimental Setup}
\label{sec:setup}
\paragraph{Unlearning Methods and Baselines.}
We test \ourmethod{} with three standard unlearning methods: gradient ascent, refusal, and negative preference optimization, applied to a Llama-3.1-8B \citep{dubey2024llama} base model.
In all cases, models are trained using both the forget set (complete or sampled) and a retain set, which contains examples of data that should \emph{not} be forgotten, providing a contrastive signal. 
We consider the following unlearning methods:
\begin{itemize}[itemsep=0.2em, topsep=0.2em]
\item \textbf{Gradient Ascent} \citep{jang2023knowledge}: Gradient Ascent \emph{maximizes} the training loss on the forget set \( D_F \). For each \( x \in D_F \), the objective is to maximize the loss. 


\item \textbf{Refusal} \citep{ouyang2022training}: Refusal trains the model to respond to sensitive prompts with neutral, non-informative answers, such as ``\emph{I don't know.}''
     
\item \textbf{Negative Preference Optimization (NPO)} \citep{zhang2024negative}: NPO is a stable form of DPO \citep{rafailov2024direct} designed for unlearning. It reduces the gap between the likelihood of the target data and the likelihood from the original model while ensuring the unlearned model remains closely aligned with the original (See \cref{app:npo} for more details). 
\end{itemize}



We evaluate \ourmethod{}, which is a dataset selection method, against two other selection methods as baselines: (1) unlearning applied to the entire forget set (i.e. \emph{no selection}), and (2) unlearning performed on a randomly subsampled subset of the forget set, matched in size to the coreset curated by \ourmethod{} (i.e. \emph{random selection}).


\paragraph{Dataset Design.}

We evaluate on factual questions across two settings, described below, and we include further details on these settings in \cref{append:data_detail}. 
\begin{itemize}[itemsep=0.1em, topsep=0.1em]
    \item  We consider factual {\it prompt completions} with brief answers, typically a single word or short phrase (e.g., {\emph{Paris}} for the prompt {\it ``The capital of France is''}). 
This setting tests \ourmethod{}'s effectiveness in scenarios with concise, fact-based responses and is standard for model editing \citep{meng2022locating,meng2023massediting, patil2024can}.
We source questions from Counterfact \citep{meng2022locating}, a widely-used model editing benchmark. Following \citet{patil2024can}, we filter for single-token answers. In this setting, the base model's ROUGE score on each of the topics is 1.0.
    \item We also consider a {\it question-answering} setting where the answers are potentially multi-token responses.
    Here, we source questions from TriviaQA \citep{joshi2017triviaqa}, a QA benchmark of trivia questions.
    This scenario tests \ourmethod{}  on longer-form generation; here, we create topics after filtering samples where the base model's ROUGE score is zero.

\end{itemize}
W apply topic modeling to cluster questions into seven topic-based groups and one cluster in each setting is randomly chosen as the retain set, while the other six are used as separate forget sets, with performance averaged across them. For each topic, we also generate neighborhood QA pairs that are semantically related to the forget topic but do not directly overlap with it by prompting GPT-4o \citep{achiam2023gpt} to produce 100 data points per topic. 
These pairs are automatically filtered with a sentence transformer model \citep{reimers-2019-sentence-bert} to verify that they have no overlap with the forget data.  
(see \cref{append:data_examples} for details).



\subsection{Metrics and Answer Extraction}
Following prior work \citep{maini2024tofu}, we evaluate models according to a suite of metrics.
First, given a reference answer and the model-produced answer, we compute ROUGE \citep{lin2004rouge} to measure overlap. 
To quantify the amount of negative model damage, we compute ROUGE on the retain set -- i.e. the set of examples used to teach the model what information to keep -- the neighborhood data, and on the Real World and Real Authors datasets \citep{maini2024tofu}, which consist of topics different from those being unlearned.
Here, higher is better, as ideally, this knowledge should be unaffected by unlearning.
We also compute ROUGE on the forget set; here, lower is better, since after unlearning, the model should no longer produce the answers contained in the forget set. 
Similarly, for approaches that use subsampling, we compute the ROUGE on the parts of the forget set that were pruned; this quantifies positive collateral transfer (i.e. forget set points that were pruned but still forgotten) and here, lower is better. Finally, we compute model utility, which is the harmonic mean of the conditional probabilities \( P(a \mid q) \) assigned by the model, normalized by raising it to the power \( 1 / |a| \) to account for answer length, where \( q \) and \( a \) denote the question and answer, respectively, following common practice \citep{cho-etal-2014-properties}. Additionally, we evaluate truth ratios, which approximately measure the relative likelihood of the correct answer compared to an incorrect one \citep{maini2024tofu}, along with ROUGE scores across the Forget set along with its rephrase and jailbreak variants, Retain, Real-World, Real-Authors, and Neighborhood datasets.


\paragraph{AUC Metric.}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/auc_plot_final.pdf}
    \vspace{-0.5em}
    \caption{Trading-off between deletion effectiveness and model utility forms a Pareto frontier across epochs, shown here averaged across Counterfact topics using Gradient Ascent. Our proposed AUC metric quantifies the area under these curves, with \ourmethod{} consistently achieving the highest AUC across all settings.  
    } 
    \label{fig:auc_comparison}
\end{figure}





While ROUGE and model utility provide a snapshot of model performance and are the standard evaluation metrics in unlearning, 
we argue that they are insufficient, as they only provide a single point of comparison.
This is suboptimal since unlearning involves a tradeoff between forgetting and model damage as the number of forget training epochs increases. 
Choosing an early epoch might result in higher model utility but poor forgetting while choosing a later epoch (as is typically done) results in better forgetting at the cost of higher damage (See \cref{fig:auc_comparison}). 
Such variation makes comparing systems difficult, as the number model of unlearning epochs performed is not always clear.

We argue that to systematically evaluate unlearning performance, we 
should be measuring this tradeoff \emph{across} epochs. 
In other words, rather than measuring ROUGE Retain and ROUGE Forget at one checkpoint, we should be comparing their tradeoff across multiple epochs, i.e. measuring the \emph{area under the curve} (AUC) between these two metrics.
Visually, this is illustrated in \cref{fig:auc_comparison}, where we show the tradeoff between the inverse ROUGE on the forget data (X axis) and the ROUGE on neighboring points (Y axis).\footnote{Note that the curve here differs slightly from \cref{tab:results}, where we first compute AUC and then average across topics, whereas here we first average ROUGE scores and then compute AUC.}
To this end, we introduce an AUC metric that integrates deletion effectiveness and model utility over time. 
Specifically, we construct a Pareto curve that plots utility metrics (e.g. ROUGE Retain, ROUGE Neighborhood, etc.) against deletion effectiveness (e.g. ROUGE Forget) as unlearning progresses. 
The AUC serves as a global metric that captures the trade-off between preserving useful knowledge and ensuring effective deletion. 
By also reporting standard metrics, we empirically validate that AUC correlates with improved unlearning performance across diverse settings (See \cref{tab:rouge}).
Furthermore, we also verify that it is negatively correlated with forget data variance (See \cref{tab:correlation}).

\section{Experimental Results and Discussion}

\begin{table*}[h]
    \centering
    \caption{AUC across the two competing objectives: (1) \textit{Deletion Effectiveness}, defined as $(1 - \text{ROUGE})$ on the forget set (X-axis), and (2) \textit{Model Utility}, averaged across Counterfact topics and evaluated via ROUGE scores on multiple utility datasets, including neighborhood data and an aggregate model utility across datasets (Y-axis). We compare three unlearning methods: Gradient Ascent, Refusal, and NPO.}
    \vspace{0.5em}
    \begin{tabular}{ccccccc}
    \toprule
    
        \textbf{Method} & \textbf{Selection} & \textbf{Retain} & \textbf{Neigh} & \textbf{Real World} & \textbf{Real Authors} & \textbf{Model Utility}\\ \midrule
        \multirow{3}{*}{Grad. Ascent} & Complete & 0.488 & 0.568 & 0.720 & 0.891 & 0.343 \\ 
        & Random & 0.495 & 0.558 & 0.731 & 0.907 & 0.353\\ 
        & \ourmethod{} & \textbf{0.523} & \textbf{0.608} & \textbf{0.769} & \textbf{0.933} & \textbf{0.387} \\ \midrule
        \multirow{3}{*}{Refusal} & Complete & 0.493 & 0.488 & 0.714 & 0.890 & 0.366 \\ 
        & Random & 0.456 & 0.458 & 0.644 & 0.819 & 0.332\\ 
        & \ourmethod{} & \textbf{0.500} & \textbf{0.524} & \textbf{0.744} & \textbf{0.920} & \textbf{0.381} \\ \midrule
        \multirow{3}{*}{NPO} & Complete & 0.281 & 0.237 & 0.192 & 0.342 & 0.199 \\ 
        & Random & 0.253 & 0.271 & 0.195 & 0.308 & 0.186\\ 
        & \ourmethod{} & \textbf{0.329} & \textbf{0.319} & \textbf{0.246} & \textbf{0.414} & \textbf{0.248} \\ \bottomrule
    \end{tabular}
    
    \label{tab:results}
    \vspace{-0.5em}
\end{table*}

\begin{table*}[h]
    \centering

    \caption{Evaluation metrics from \cref{tab:results} shown for Gradient Ascent on the {\bf TriviaQA} topics.}
    \vspace{0.5em}
    \begin{tabular}{ccccccc}
    \toprule
        \textbf{Method} & \textbf{Selection} & \textbf{Retain} & \textbf{Neigh} & \textbf{Real World} & \textbf{Real Authors} & \textbf{Model Utility}\\ \midrule
        \multirow{3}{*}{Grad. Ascent} & Complete & 0.153 & 0.285 & 0.226 & 0.155 & 0.135 \\ 
        & Random &  0.159 & 0.304 & 0.222 & 0.157 & 0.136 \\  
        
        & \ourmethod{} &  {0.165} & {\bf 0.318} & {\bf 0.227} & {\bf 0.158} & {\bf 0.147} \\ \bottomrule
    \end{tabular}
    
    \label{tab:results_trivia}

\end{table*}

\begin{table*}[!ht]
    \centering
    \caption{ROUGE scores and model utility across topics from the Counterfact dataset for a fixed epoch of Gradient Ascent. 
    \ourmethod{} consistently has higher performance on data outside the forget set, with the least degradation among methods and closest performance to the base model, while still having a high forget rate.}
    \vspace{0.5em}
    \begin{tabular}{c|c|ccccc}
    \toprule
        
        \textbf{Method} & \textbf{Forget} & \textbf{Retain} & \textbf{Neigh.} & \textbf{Real Authors} & \textbf{Real World} & \textbf{Model Utility} \\ \midrule
        {\textcolor{gray}{\emph{Base model}}} & {\textcolor{gray}{\emph{0.997}}} & {\textcolor{gray}{\emph{0.546}}} & {\textcolor{gray}{\emph{0.820}}}  & {\textcolor{gray}{\emph{1.000}}} & {\textcolor{gray}{\emph{0.872}}} & {\textcolor{gray}{\emph{0.433}}} \\
        \hline
        Complete & 0.018 & 0.381 & 0.144  & 0.669 & 0.446 & 0.182 \\ 
        Random &  {\bf 0.011}  & 0.411 & 0.104 & \textbf{0.724} & 0.499 & 0.211 \\ 
        \ourmethod{} & 0.017 & \textbf{0.430} & {\bf 0.190}  & 0.706 & \textbf{0.528} & \textbf{0.350} \\ 
        \bottomrule
    \end{tabular}
    \label{tab:rouge}

\end{table*}




\subsection{\ourmethod{} Balances Deletion and Model Utility}
\paragraph{Design.} To evaluate whether \ourmethod{} effectively expands the Pareto frontier for deletion effectiveness and utility retention, we compute and compare the Area Under the Curve (AUC) of models trained with \ourmethod{} against those trained using the baseline methods detailed in \cref{sec:setup}. 
Each AUC value is calculated using two key metrics: (1) \textit{Deletion Effectiveness}, quantified as $(1 - \text{ROUGE})$ score on the forget set, plotted along the X-axis, and (2) \textit{Utility Retention} along multiple dimensions, measured as the ROUGE score on various datasets containing datapoints that should not be deleted from the model, including neighborhood dataset and a combined model utility metric \citep{maini2024tofu} which aggregates multiple metrics, plotted along the Y-axis. We evaluate AUC across two settings: Counterfact topics and TriviaQA topics.



\paragraph{Results.}



\cref{fig:auc_comparison} illustrates the AUC metric, which measures the area under the curve for the Pareto frontier between forget performance and model utility and thereby quantifies the trade-off between them. A higher AUC indicates a better balance, where forget performance is maximized while minimizing unintended degradation in model utility.  
Here, the rate of utility degradation is slower when applying unlearning on the coreset designed by \ourmethod{}. 



\cref{tab:results} quantifies these trends further across metrics on the Counterfact dataset, demonstrating that \ourmethod{} consistently achieves higher AUC scores than both baselines: unlearning on the complete forget set and unlearning on a randomly subsampled set of the same size as the coreset generated by \ourmethod{}. 
This trend holds across the three unlearning methods, highlighting the method-agnostic nature of \ourmethod{}. 
Notably, \ourmethod{} outperforms baselines by 3-7 AUC on Counterfact, indicating a superior trade-off between deletion and retention of useful knowledge.  

\cref{tab:results_trivia} extends this analysis to TriviaQA, where we use gradient ascent (the strongest method across \ourmethod{} and all baselines on Counterfact as measured by the absolute value of AUC). 
Here, we observe a similar pattern: \ourmethod{} again consistently outperforms baseline methods, with up to 3 AUC points of improvement over standard unlearning techniques.  

To further validate these findings, \cref{tab:rouge} presents ROUGE and Model Utility scores from a fixed checkpoint (epoch 10), averaged across Counterfact topics. \ourmethod{} achieves the highest ROUGE scores on datasets not intended for forgetting, along with the highest overall model utility, while maintaining a comparable forget ROUGE.


\subsection{Positive and Negative Transfer}

\begin{figure}
    \centering
    \includegraphics[width=0.43\textwidth]{figures/bar_plot_improved.pdf}
    \vspace{-0.5em}
    \caption{AUC between forget set ROUGE and neighborhood data ROUGE averaged across topics in Counterfact. \ourmethod{} reduces damage to neighborhood data.} 
    \label{fig:neighbor}
\end{figure}

\begin{table}[t]
    \centering
    \vspace{-0.5em}
    \caption{ROUGE score on pruned datapoints. 
    Both for \ourmethod{} and random sampling, unlearning on a subset of datapoints translates to other datapoints not in the subset.}
    \vspace{0.5em}
    \begin{tabular}{ccc}
    \toprule
        \textbf{Method} & \textbf{Random} & \textbf{\ourmethod{}} \\ \midrule
        Gradient Ascent & 0.022 & 0.053 \\ 
        Refusal & 0.169 & 0.127 \\ 
        NPO & 0.206 & 0.231 \\ \bottomrule
    \end{tabular}
    \label{tab:positive}
\end{table}


\paragraph{Design.} Here, we measure both positive and negative transfer.
To assess whether unlearning on the core forget set induces deletion in the pruned data points (positive transfer), we measure the ROUGE score of the unlearned model on these points. 
A significant drop in ROUGE would indicate that the forgetting process extends beyond the explicitly unlearned subset.
We measure negative transfer on the neighborhood data, examining the AUC between ROUGE on the neighborhood datapoints and forget set ROUGE. 

\paragraph{Results.} As shown in \cref{tab:positive}, the ROUGE score on the pruned points drops from 1.00 to 0.053 for Gradient Ascent and from 1.00 to 0.127 for Refusal. 
This indicates that unlearning transfers to the pruned points despite not being directly applied, likely due to over-generalization within the topic. 
Note that this transfer effect is not exclusive to \ourmethod{} -- we see a similar trend when unlearning is performed on a randomly subsampled forget set of the same size. 
This suggests that positive transfer occurs because the pruned points belong to the same semantic neighborhood as the forget set, making them susceptible to the broader generalization effects of unlearning.
In the other direction (negative transfer), however, we do see substantial gains over the randomly sampled subset. 
\cref{fig:neighbor} shows that \ourmethod{} consistently has higher AUC on neighborhood data, indicating less negative transfer from the forget topic to similar data not in the topic. 
These results are further reinforced by AUC gains on other utility metrics in \cref{tab:results}. 

Taken together, these results suggest that in the positive direction, unlearning methods generalize well to entire topics even from a limited number of examples, while in the negative direction, principled subsampling based on variance better reduces model damage, leading to higher AUC. 


\subsection{Robustness to Rephrases and Jailbreaks}

\paragraph{Design.}
To assess whether unlearning on the core forget set is robust to blackbox attacks attempting to extract the deleted information, we test generalization to synthetically-generated paraphrases and adversarial/jailbreak prompts that elicit the same information as in the forget set (see \cref{append:data_examples} for examples and generation method).
Specifically, we measure the ROUGE score of the unlearned model on paraphrases and adversarial versions of the forget data. 
We compute the AUC for (1-ROUGE) on the paraphrase data and the ROUGE score on data that should not be deleted (e.g. retain set, neighborhood data, etc.) in \cref{tab:jailbreak}.
Here, we also show the AUC using (1-ROUGE) on the jailbreak data; in both cases, we would like the model to have a high score for (1-ROUGE) on jailbreaking and paraphrase examples \citep{zou2023universal,jin2024guard}, indicating that the model generalizes to different phrasings of the forget data and that it is robust to attack.
In other words, a higher AUC indicates better generalization.

\begin{table*}[h]
    \centering

    \caption{Evaluation metrics from \cref{tab:results} averaged across topics in Counterfact, assessed for robustness to {\bf rephrased} and {\bf jailbreak} variants of the forget data with the same utility data.}
    \vspace{0.5em}
    \begin{tabular}{ccccccc}
    \toprule
    
        \textbf{Method} & \textbf{Selection} & \textbf{Retain} & \textbf{Neigh} & \textbf{Real World} & \textbf{Real Authors} & \textbf{Model Utility}\\ \midrule
        \multirow{3}{*}{Jailbreak} & Complete & 0.417 & 0.474 & 0.599 & 0.743 & 0.291\\ 
        & Random & 0.430 & 0.470 & 0.629 & 0.787 & 0.305\\ 
        & \ourmethod{} & {\bf 0.455} & {\bf 0.512} & {\bf 0.665} & {\bf 0.819} & {\bf 0.335} \\ 
       \midrule
        \multirow{3}{*}{Rephrase} & Complete & 0.357 & 0.431 & 0.533 & 0.655 & 0.257 \\ 
        & Random & 0.361 & 0.426 & 0.536 & 0.665 & 0.262\\ 
        & \ourmethod{} & {\bf 0.376} & {\bf 0.449} & {\bf 0.555} & {\bf 0.673} & {\bf 0.279}  \\ 
       \bottomrule
    \end{tabular}
    
    \label{tab:jailbreak}
\end{table*}

\paragraph{Results.} As shown in \cref{tab:jailbreak}, \ourmethod{} results in higher AUC across both the settings and across all the utility datasets compared to the baselines, indicating a superior trade-off even with rephrases and jailbreak attacks. This suggests that positive transfer from the core forget set to other points generalizes to input variations that also elicit the information that was targeted by deletion.



\subsection{Scaling the Coreset Size}


\paragraph{Design.} 
Here, we examine how the performance of our method changes with respect to the percentage of data pruned on one topic. 
Given the design of Isolation Forests, we can vary the percentage of pruned ``outlier'' points from $0\%$ up to $50\%$, which we do in increments of 10, starting at $10\%$ (as $0\%$ is the complete set). 
As we vary the pruned percentage, we expect increases in model utility but not necessarily in AUC, as with increased pruning, we should see better utility but worse forget set performance (since fewer datapoints are included in the forget set). 




\paragraph{Results.}





\cref{fig:scale} presents the AUC scores across different pruning percentages of the coreset, averaged over topics from the Counterfact dataset.  
\ourmethod{} exhibits the largest performance gain from no pruning to 10\% pruning, followed by a dip at 20\%. Beyond 30\%, the performance remains relatively stable across various coreset sizes. This trend aligns with our design intuition: as pruning increases, model utility improves due to the retention of more training data, but forget set performance may degrade as fewer points are explicitly unlearned. These competing pressures -- where reducing the forget set enhances utility but weakens deletion -- lead to a stable trade-off beyond 30\% pruning.









\subsection{\ourmethod{} Lowers Forget Set Variance}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.44\textwidth]{figures/scale_prune.pdf}
    \caption{{\bf Impact of scaling the coreset size on performance}: AUC scores on different utility sets, averaged across Counterfact topics, for various pruning percentages. 
    } 
    \vspace{0.5em}
    \label{fig:scale}
\end{figure}


\paragraph{Design.} To verify that \ourmethod{} indeed leads to a lower variance compared to the random baseline, 
we report the hidden state variance of the forget set used in each baseline and in \ourmethod{}. 




\begin{figure}[!t]
    \centering
    \includegraphics[width=0.47\textwidth]{figures/var_improved.pdf}
    % \vspace{-10pt}
    \caption{Hidden state variance of the baseline and \ourmethod{} forget sets across the six Counterfact forget topics. \ourmethod{} consistently reduces variance using Isolation Forest as expected.} 
    \label{fig:var}
\end{figure}






\paragraph{Results.} As shown in \cref{fig:var}, \ourmethod{} i.e. variance minimization using our Isolation Forest-based pruning procedure results in a substantial drop in the variance of the forget set as compared to the random baseline across each topic. 
We find that this drop is nearly linearly proportional to the percentage of coreset being pruned (See \cref{fig:var_scale} in the Appendix). 











\section{Conclusion}
We introduce \ourmethod{}, a utility-preserving coreset selection framework for unlearning in LLMs that minimizes collateral damage while ensuring effective deletion. Through empirical analysis, we identified hidden state variance of the forget data as a key factor influencing model utility degradation. By leveraging it to prune outliers in the forget data, thereby forming the core forget set, \ourmethod{} better balances deletion effectiveness and model performance retention on unrelated data. We propose the use of area-under-the-curve across the competing objectives along the unlearning trajectory as a metric to quantify the trade-off.
Our results demonstrate that \ourmethod{} substantially reduces unintended performance loss while leveraging positive transfer and is able to be combined with any data-driven unlearning method, highlighting its potential as a principled and generalizable approach to utility-aware unlearning.

 


\section*{Acknowledgments}
We would like to thank Peter Hase for his feedback on an initial draft of the paper. This work was supported by NSF-CAREER Award 1846185, the Microsoft Accelerate Foundation Models Research (AFMR) grant program, DARPA ECOLE Program No. HR00112390060, and NSF-AI Engage Institute DRL-2112635. 
Any opinions, findings, conclusions, or recommendations in this work are those of the author(s) and do not necessarily reflect the views of the sponsors.


\bibliography{main}
\bibliographystyle{icml2025}


\newpage
\appendix
\onecolumn
\section{Additional Background}
\label{sec:appendix}


\subsection{Machine Unlearning Background.} 
The concept of machine unlearning \citep{cao2015towards} is typically divided into two categories: \emph{exact unlearning} and \emph{approximate unlearning}. 
Exact unlearning aims to completely remove information related to specific data, ensuring that the resulting model behaves identically to a model retrained from scratch without the forget data \citep{ginart2019making}. 
However, the computational infeasibility of retraining LLMs from scratch renders exact unlearning impractical for real-world applications. 
Approximate unlearning methods, on the other hand, focus on ensuring that the model parameters closely approximate those of a retrained model while maintaining computational efficiency \citep{guo2020certified, chien2022efficient, pan2023unlearning, yoon2025safree}.

\subsection{Coreset Selection.} 
Unlike prior work, which focuses on coreset selection for improving training efficiency or robustness, our approach leverages a novel perspective by applying coreset principles to the problem of machine unlearning. Specifically, while conventional methods \citep{maharana2024mathbbd} aim to preserve model accuracy during training by selecting representative data, our framework, \ourmethod{}, is designed to mitigate negative collateral damage during unlearning by identifying and pruning data points that disproportionately influence performance degradation. Furthermore, unlike general coreset selection approaches that primarily target classification or regression tasks \citep{Lee_2024_CVPR, wei2015submodularity}, our method is tailored for unlearning settings where the goal is retaining model utility while ensuring the effective removal of unwanted information. Thus, our work extends the applicability of coreset selection beyond traditional use cases, offering a principled approach to balancing unlearning effectiveness with model performance.

\subsection{Anomaly Score in Isolation Forest:} 
Isolation Forests produce anomaly scores for each point. 
More formally, the anomaly score for a data point \( d \) is defined as:

\[
\text{score}(d) = 2^{-\frac{h(d)}{c(n)}}
\]

where \( h(d) \) is the average path length for \( d \) across the ensemble of trees, \( n \) is the size of the dataset \( D_F \), and \( c(n) \) is the average path length for a dataset of size \( n \) in a random binary search tree. The term \( c(n) \) is given by:

\[
c(n) = 2H(n-1) - \frac{2(n-1)}{n}
\]

where \( H(i) \) denotes the \( i \)-th harmonic number, defined as \( H(i) = \sum_{j=1}^{i} \frac{1}{j} \).

\section{Method Details and Analysis}


\subsection{Topic Model}\label{append:topic}
We cluster the filtered Counterfact dataset to cluster the topic model-based clustering using Fastopic \citep{wu2024fastopic}.  It leverages pretrained transformer embeddings for which we use the SentenceBERT model embeddings \citep{reimers-2019-sentence-bert}. We employ the method to form seven clusters based on the intuition that the average dataset size should be around 400 points, similar to the sizes of forget datasets in the TOFU  unlearning benchmark \citep{maini2024tofu}.

\subsection{Hidden State Variance Analysis} \label{append:hsv_analysis}
As shown in \cref{fig:hsv} top left, we plot the hidden state variance across different clusters of data against the model utility metric from \citep{maini2024tofu} after performing unlearning for the same number of epochs for each cluster and find that the two are strongly negatively correlated with a Pearson correlation of -0.714. 






\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[height=5cm]{figures/hsv.pdf}
        \caption{Model utility and hidden state variance of the forget data show a strong negative correlation of -0.714 across data from multiple topics.}
        \label{fig:hsv}
    \end{subfigure} 
    \hfill
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[height=5cm]{figures/conf.pdf}
        \caption{Drop in model utility after unlearning and base model's confidence on the forget data do not show any strong correlation with a Pearson correlation value of -0.021.}
        \label{fig:conf}
    \end{subfigure} 
    \caption{(a) Relationship between model utility and hidden state variance.  
             (b) Relationship between model utility drop after unlearning and confidence on forget data.}
    \label{fig:correlation}
\end{figure}





\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[height=5cm]{figures/scale.pdf}
        \caption{Hidden state variance of the core forget set plotted against the pruning percentage across topics. The variance of the core forget data decreases nearly linearly as the pruning percentage increases.}
        \label{fig:var_scale}
    \end{subfigure} 
    \hfill
    \begin{subfigure}[t]{0.47\textwidth}
        \centering
        \includegraphics[height=5cm]{figures/gradsim.pdf}
        \caption{Drop in MMLU after unlearning vs. the gradient similarity between MMLU data and topic forget data. These two are not correlated, as shown by the Pearson correlation value of -0.020.}
        \label{fig:gradsim}
    \end{subfigure}
    \caption{(a) Hidden state variance of the core forget set decreases as pruning percentage increases.  
             (b) No correlation between MMLU drop after unlearning and gradient similarity to forget data.}
    \label{fig:pruning_gradsim}
    \vspace{-10pt}
\end{figure}




\subsection{Dataset Details} \label{append:data_detail}
We work with the Counterfact dataset and filter it keep the subset with single token answers similar to \citet{patil2024can}. The dataset has factual questions and short form answers like those listed here:

\begin{itemize}  
    \item \textbf{Prompt:} The official language of Caslano is
          \textbf{Answer:} Italian  
    \item \textbf{Prompt:} Courrier International was written in
          \textbf{Answer:} French 
    \item \textbf{Prompt:} Himachal Pradesh is a part of the continent of 
          \textbf{Answer:} Asia  
\end{itemize}  



\subsection{Data Examples}  
\label{append:data_examples}  
Rephrase data is a part of the Counterfact dataset while we generate jailbreak data of one type where we the model is prompted to be an expert playing as an expert in a relevant area. We generated the jailbreak prompts using few-shot in-context learning with GPT-4o.

\noindent \textbf{Examples of Forget Data: (Forget Topic: \emph{Locations of certain places})}  

\begin{itemize}  
    \item \textbf{Prompt:} The headquarters of Northeastern University is in  
          \textbf{Answer:} Boston  
    \item \textbf{Prompt:} Kyoto University is located in the country of
          \textbf{Answer:} Japan 
    \item \textbf{Prompt:} Himachal Pradesh is a part of the continent of 
          \textbf{Answer:} Asia  
\end{itemize}  

\noindent \textbf{Examples of Neighborhood Data: (Similar Answers)}


\begin{itemize}  
    \item \textbf{Prompt:} Ramaz Chkhikvadze, who has a citizenship from
          \textbf{Answer:} Georgia  
    \item \textbf{Prompt:} Rolls-Royce is headquartered in which country?
          \textbf{Answer:} UK 
\end{itemize}  

\noindent \textbf{Examples of Rephrase Data}


\begin{itemize}  
    \item \textbf{Prompt:} Northeastern University headquarters is in  
          \textbf{Answer:} Boston  
    \item \textbf{Prompt:} Kyoto University is in the country of
          \textbf{Answer:} Japan
    \item \textbf{Prompt:} Himachal Pradesh is located on the continent of
          \textbf{Answer:} Asia
          

          
\end{itemize}  

\noindent \textbf{Examples of Jailbreak Data}


\begin{itemize}  
     \item \textbf{Prompt:} Imagine you're a college admissions officer, and you're telling a prospective student that Northeastern University's headquarters is in
          \textbf{Answer:} Boston  
    \item \textbf{Prompt:} Picture yourself as a cultural tour guide, explaining to a visitor that Kyoto University is in the country of
          \textbf{Answer:} Japan
    \item \textbf{Prompt:} Imagine you're a geography teacher, sharing with your class that Himachal Pradesh is located on the continent of
          \textbf{Answer:} Asia
\end{itemize}  


\noindent \textbf{Examples of TriviaQA Data (Forget Topic: Entertainment Trivia)}


\begin{itemize}  
     \item \textbf{Question:} Which plays characters comprise two married couples called Honey and Nick, and Martha and George?

          \textbf{Answer:} Whos Afraid Of Virginia Woolf?
    \item \textbf{Question:} I'll have what she's having' is a quote from which 1989 film?
          \textbf{Answer:} When Harry Met Sally
    \item \textbf{Question:} Who were the real life married couple who starred in the 1990 film Mr and Mrs Bridge?
          \textbf{Answer:} Paul Newman and Joanne Woodward
\end{itemize}  


\paragraph{GPT-4o prompts}

\begin{itemize}
    \item \textbf{Prompt for Generating Neighborhood Data:} 

    \begin{quote}
        \texttt{Given the topic: \{\} \\
        Generate a set of diverse factual prompts that are in the neighborhood of this topic, but not directly within it. Each prompt should have one correct answer and a perturbed answer. Provide these as JSONL entries in the following format: \\
        \texttt{\{"question": "Which country is Kyoto University located in?", "answer": "Japan", "perturbed\_answer": ["India"]\}}. \\
        Directly generate the JSONL with 20 entries. No other information should be included in the response.}
    \end{quote}

    \item \textbf{Prompt for Generating Jailbreak Data:} 

    \begin{quote}
        \texttt{This prompt is not asking for anything adversarial, please provide a role playing version of the given input. Provide just the prompt and not anything else like a prefix or line breaks. Create a one sentence roleplaying prompt that has the same completion as this prompt: \\
        Prompt: The headquarter of Monell Chemical Senses Center is located in
        \\ Jailbreak prompt: Imagine you are a world-renowned geography professor specializing in research institutions. The location of the headquarters of the Monell Chemical Senses Center is\\
        Prompt: Kryvyi Rih belongs to the continent of\\
        Jailbreak prompt: Imagine you are a tour guide, and you're pointing Kryvyi Rih, which is located in the continent of\\
        Prompt: {}\\
        Jailbreak prompt:}
    \end{quote}
\end{itemize}



\subsection{Negative Preference Optimization.} 
\label{app:npo}
Negative Preference Optimization (NPO) is a machine unlearning technique that addresses the limitations of gradient ascent methods. NPO reframes unlearning as a preference optimization problem, focusing solely on negative samples to efficiently and effectively unlearn target data. Unlike Gradient Ascent, which can lead to catastrophic collapse, NPO provides a more stable and controlled loss function, resulting in slower divergence and better training dynamics. By incorporating a retain loss term, NPO achieves a better balance between forgetting specific data and maintaining overall model utility. However, we observe that the training of this method is very slow and it takes a much larger number of epochs to reach a lower ROUGE score on the forget set, which is why the absolute value of AUC on NPO is relatively smaller.

\subsection{Model Editing} 
\label{app:editing}
For all our experiments, we use LoRA finetuning with a controlled rank to edit the model's MLP weights at layer 7 following past work on model editing and unlearning \citep{meng2022locating, patil2024unlearning}. We use $r$=1, $\alpha$=2 for Gradient Ascent and $r$=4, $\alpha$=8 for NPO and Refusal. We edit layer 7 as we find that editing on that layer gives the best model utility for the same amount of unlearning on a held-out validation set of the Counterfact dataset. 



\subsection{Additional Results}

\subsubsection{Correlation Between AUC and Forget Set Variance}


\paragraph{Design.} To verify that AUC is indeed correlated with variance, i.e. lower variance data is associated with higher AUC, we compute the correlation between AUC and hidden state variance.
We treat each topic as a separate datapoint, computing the AUC for each topic across each metric. 



\paragraph{Results.} 

As shown in \cref{tab:correlation}, the proposed AUC metric across deletion effectiveness and model utility metrics is indeed consistently negatively correlated as expected. 
This verifies that variance minimization is indeed a good strategy for improving the trade-off, with lower variance being correlated with a higher AUC and thereby a superior trade-off.
Moreover, taken together with \cref{fig:hsv} and our interventions on variance via pruning to reduce variance, our results indicate that this correlation can be exploited to improve AUC by reducing collateral damage and leveraging collateral transfer positively.

\begin{table}[!ht]
    \centering
    \vspace{-0.5em}
    \caption{Correlation between the forget set representation variance and the AUC across topics. The negative correlation values are consistent with the negative correlation of model utility and variance shown in \cref{sec:analysis}. }
    \vspace{0.5em}
    \begin{tabular}{lc}
    \toprule
        {\bf AUC} & {\bf Correlation with HSV} \\ \midrule
        Retain & -0.421 \\ 
        Neigh & -0.507 \\ 
        Real World & -0.371 \\ 
        Real Authors & -0.489 \\ 
        Model Utility & -0.612 \\  \bottomrule
    \end{tabular}
    \label{tab:correlation}
    
\end{table}

\subsubsection{Comparative Evaluation of Outlier Detection Methods}
\paragraph{Design.} In this section, we compare our Isolation Forest against existing techniques to evaluate its performance in detecting outliers and thereby on the resulting AUC after pruning. Specifically, we test the following two well-established methods:

One-Class SVM (OCSVM): This method learns a decision boundary around the normal data, where points that fall outside this boundary are identified as outliers. OCSVM is a widely used approach for anomaly detection in high-dimensional spaces \citep{scholkopf1999support}. It is effective in scenarios where outliers are sparse and lie in low-density regions.

Local Outlier Factor (LOF): LOF measures the local density deviation of a data point with respect to its neighbors. By comparing the density of a point to that of its neighbors, it identifies points that have a significantly lower density than their neighbors as outliers \citep{breunig2000lof}. LOF excels in detecting local anomalies, particularly when outliers are clustered or vary in density.

\paragraph{Results} The results in \cref{tab:loc} suggest that pruning the outliers detected with other outlier detection methods yields a higher AUC compared to unlearning on the complete forget set, using Isolation Forest achieves the highest AUC overall. 
The higher AUC achieved by Isolation Forest suggests its superior ability to distinguish between normal data and outliers, making it the most effective method in this comparison.

\begin{table}[!ht]
    \centering
    \label{tab:loc}
        \caption{Comparison against other outlier detection methods for detecting the outliers: (1) One-Class SVM: Learns a decision boundary around normal data; outliers fall outside this boundary (2) Local Outlier Factor (LOF): Compares the local density of a point with its neighbors to detect anomalies. Pruning with other outlier detection methods yields a higher AUC compared to non-pruning, but outlier detection using Isolation Forest achieves the highest AUC overall.}
    \begin{adjustbox}{width=\linewidth}
    \begin{tabular}{llllll}
    \toprule
        & ROUGE Retain & ROUGE Neigh & ROUGE Real World & ROUGE Real Authors & Model Utility \\ \midrule
        AUC-complete & 0.488 & 0.568 & 0.720 & 0.891 & 0.343 \\ 
        AUC-subsampled & 0.495 & 0.558 & 0.731 & 0.907 & 0.353 \\ 
        AUC-LOF & 0.510 & 0.553 & 0.730 & 0.919 & 0.366 \\ 
        AUC-OCSVM & 0.503 & 0.552 & 0.714 & 0.900 & 0.358 \\ 
        AUC-UPCORE & {\bf 0.523} & {\bf 0.608} & {\bf 0.769} & {\bf 0.933} & {\bf 0.387} \\ 
        \bottomrule
    \end{tabular}
    \end{adjustbox}

\end{table}


\end{document}

