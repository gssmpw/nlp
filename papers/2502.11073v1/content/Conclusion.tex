%We introduced \textsf{IntMeme}, an innovative multimodal framework that harnesses the power of L-VLMs to produce explicit meme interpretations for a more explainable harmful meme detection approach. Specifically, \textsf{IntMeme} first prompts a frozen L-VLM to generate an informative interpretation of a given meme. Subsequently, \textsf{IntMeme} utilizes distinct modules to adeptly encode both the meme and explicit meme interpretation independently before conditioning the detection of harmful memes on the combined encoded data. Our experimental results across three widely-used harmful meme datasets demonstrated the robustness and effectiveness of our proposed framework. Furthermore, we demonstrated the utility of meme interpretation in the detection of harmful memes through a case study and LIME analysis.

% We have proposed \textsf{IntMeme}, a novel multimodal framework leverages the strong multimodal reasoning capabilities of LMMs to generate quality meme interpretations for the classification of hateful memes, improving the transparency of the classification process. The generated meme interpretations help users better comprehend how \textsf{IntMeme} distinguishes between hateful and non-hateful content. \textsf{IntMeme} also introduces two distinct modules to efficiently encode both the meme and its corresponding interpretation for the classification of hateful memes. Through rigorous testing on three widely-used hateful meme datasets, we demonstrated the resilience and efficiency of our framework. Moreover, the ablation study comprises of a case study analysis and a LIME analysis have substantiated the instrumental role of generated meme interpretation in identifying hateful memes.

%The applications of LMMs in downstream tasks have garnered significant attention \cite{yang2023dawn,fu2023mme}. However, using LMMs for such tasks presents several challenges. Firstly, fine-tuning LMMs is computationally expensive and time-consuming. Secondly, extracting answers from the generated text can be difficult. Lastly, the issue of hallucinations remains a persistent challenge \cite{ji2023survey}. Hence, we introduce a more efficient approach in using LMMs, specifically to produce high-quality interpretations of memes for the purpose of classifying hateful memes.

%Nevertheless, our human evaluation study and manual examination uncovered numerous challenges in generating interpretations with LMMs. Although most interpretations are of good quality, a significant portion needs improvements to become more informative and helpful to end-users. We have pinpointed three main issues with these problematic interpretations: Firstly, inaccuracies in identifying visual elements cause confusion. Secondly, an inability to recognize sarcasm and wordplay results in literal interpretations. Thirdly, interpretations can be incomplete due to premature termination from LMMs. These challenges highlight the limitations of using open-source LMMs for meme interpretation and suggest avenues for future exploration in LMMs.

%We introduce \textsf{IntMeme}, a novel framework that leverages the robust multimodal reasoning capabilities of LMMs to produce high-quality interpretations of memes for hateful meme classification. These interpretations provide insight into how \textsf{IntMeme} differentiates between hateful and non-hateful content, enhancing the explainability of \textsf{IntMeme}'s decisions. Furthermore, \textsf{IntMeme} employs two unique modules to independently encode the meme and its interpretation, leading to improved performance. Our extensive evaluation on three widely-used hateful meme datasets has demonstrated the robustness and effectiveness of \textsf{IntMeme}. Additionally, our empirical analysis, which includes a human evaluation study, case study analysis, and LIME analysis, affirms the critical contribution of the generated meme interpretations to an explainable hateful meme detection system.

% Despite its potential, the deployment of LMMs for multimodal downstream tasks is fraught with challenges: the fine-tuning process is resource-intensive, deriving specific answers from generated texts can be cumbersome, and the models are prone to generating hallucinated content \cite{ji2023survey}. In response, this study proposes \textsf{IntMeme}, an innovative framework designed to efficiently utilize LMMs for generating insightful interpretations of memes, particularly to aid in the classification of hateful content.

The deployment of LMMs for multimodal downstream tasks is fraught with challenges: the fine-tuning process is resource-intensive, deriving specific answers from generated texts can be cumbersome, and the models are prone to hallucinating content \cite{ji2023survey}. In response, this study proposes \textsf{IntMeme}, an innovative framework designed to efficiently utilize LMMs for generating insightful interpretations of memes, particularly to aid in classifying hateful content.

Our empirical investigation, which includes a human evaluation study and a manual examination, reveals that while LMMs can produce quality interpretations, there remains a considerable margin for improvement. Specifically, we identified three recurrent issues: inaccuracies in visual element identification leading to confusion, failures in detecting sarcasm or wordplay resulting in overly literal interpretations, and the issue of incomplete interpretations due to premature model termination. These findings underscore the limitations of open-source LMMs in meme interpretation and open pathways for further research advancement.

In conclusion, \textsf{IntMeme} presents a novel framework that uses LMM to improve the performance and explainability of hateful meme classification. \textsf{IntMeme} prompts LMM in a zero-shot manner before using separate modules to encode the meme and the LMM-generated interpretation efficiently. Our comprehensive experiments on three popular harmful meme datasets demonstrate the framework's effectiveness. Despite the high quality and utility of most generated meme interpretations, our study identifies key areas for improvement. Future research can focus on enhancing models' ability to more accurately capture visual elements and interpret figurative language. 