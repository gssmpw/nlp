\begin{figure}[t!]
\centering
\includegraphics[width=0.95\linewidth]{images/intmeme-teaser.pdf}
\caption{Overview of the proposed \textsf{IntMeme}'s approach and its advantages in a content moderation process.}
\label{fig:intmeme-teaser}
\end{figure}

The rise of internet memes has significantly influenced modern communication and culture, blending humour and satire. However, the emergence of \textit{hateful} memes\footnote{\color{red}\textbf{WARNING}: \textit{This paper contains violence and discriminatory content that may be disturbing to some readers.}} reveals a darker side, contributing to social tensions, stereotyping, and misinformation. This phenomenon underscores the urgency of developing effective classification strategies to curb their negative societal impacts, highlighting the importance of promoting a harmonious and inclusive online environment.

% The classification of hateful memes requires a nuanced understanding of multimodal content \cite{kiela2021hateful}. Research has extensively utilized pre-trained vision-language transformer models (PT-VLMs) \cite{lu2019vilbert,li2019visualbert,singh2022flava}. Despite their ability to learn subtle multimodal interactions, PT-VLMs face significant limitations. The complex attention mechanisms obscure the decision-making processes, impacting their explainability and societal trust. Moreover, fine-tuned PT-VLM's bias and sensitivity to multimodal nuances often misclassify non-hateful content, raising concerns about their efficacy in accurately identifying hateful memes \cite{cuo2022understanding,hee2022explaining}.



% The emergence of Large Multimodal Models (LMMs), such as GPT-4(V)~\cite{openai2023gpt4v}, mPLUG-Owl~\cite{ye2023mplug}, and InstructBLIP~\cite{dai2023instructblip}, has shown promising generative capabilities and an improved understanding of multimodal data. These models can improve hateful meme detection by generating text-based classification responses. However, they also pose challenges, including the need for significant computational resources, deviations in the responses generated from intended queries, and difficulties in extracting classification outcomes from generated text. These challenges underscore the critical need for new strategies that effectively leverage the strengths of LMMs in the classification of hateful memes, addressing both their inherent limitations and those associated with PT-VLM approaches.
The classification of hateful memes requires a nuanced understanding of visual and textual elements \cite{kiela2021hateful}, as well as the context behind the implied message \cite{hee2023decoding}. To address this challenge, previous research has used pre-trained vision-language transformer models (PT-VLMs) \cite{lu2019vilbert,li2019visualbert} for hateful meme classification, often enhancing these models with additional inputs like image captions \cite{velioglu2020detecting, zhu2020enhance}. While PT-VLMs can learn the interactions between visual and textual modalities, they face several limitations. Their performance heavily depends on the implicit knowledge acquired during pre-training and complex attention mechanisms, which can make it difficult to explain their decisions—an important factor for building trust in meme classification. The reliance on implicit knowledge complicates tracing the reasoning behind classifications, and the attention mechanisms make it hard to identify which features influence decisions.  Recent studies suggest that PT-VLMs might be too sensitive to subtle multimodal nuances, leading to the misclassification of non-hateful content \cite{cuo2022understanding, hee2022explaining}. These findings raise concerns about whether these models truly capture the deeper meanings that memes often convey. As a result, there is a growing need for more interpretable and efficient approaches to accurately classify hateful memes while providing clear justifications for their predictions, thereby fostering greater transparency and accountability in automated content moderation systems.

The emergence of Large Multimodal Models (LMMs), such as GPT-4(V)~\cite{openai2023gpt4v}, mPLUG-Owl~\cite{ye2023mplug}, and InstructBLIP~\cite{dai2023instructblip}, has shown promising generative capabilities. These models demonstrate strong multimodal understanding and text generation skills, which can be seen in their ability to deliver accurate and relevant text responses for complex vision-language tasks like visual question answering and visual commonsense reasoning \cite{xu2023lvlm, yang2023dawn}. Consequently, LMMs are becoming a promising solution for detecting hateful memes, providing insightful explanations into the implicit meaning hidden within memes. However, these models face several challenges. First, when used in a zero-shot setting, LMMs often perform less effectively compared to smaller models fine-tuned specifically for hateful meme classification \cite{lin2024goat}. Additionally, their zero-shot responses can sometimes deviate from the intended query, and extracting classification decisions from their generated text can be difficult, as the relevant information may be scattered throughout the output. Most importantly, fine-tuning these models demands significant computational resources and may reduce their generalizability, raising concerns about their feasibility and scalability in real-world applications. These limitations have led us to explore new methods for leveraging LMMs in hateful meme classification while maintaining their ability to produce high-quality text-based responses.

% In this work, we present \textsf{IntMeme}, a novel framework that utilizes LMMs to improve the classification and interpretation of hateful memes. \textsf{IntMeme} significantly enhances the explainability and accuracy of hateful meme detection by grounding the classification decision on LMM-generated textual interpretations. This approach reduces reliance on implicit model understanding and provides a better understanding of classification decisions, as illustrated in Figure 1. Demonstrated through our human evaluation study that mimicks a hate content moderation process, \textsf{IntMeme} represents a explainable, interpretable, and human-centric methodology, emphasizing the importance of aligning AI development with ethical standards and human-centric engineering principles. Our framework showcases the potential to promote a more ethical and responsible human-AI collaboration to online content moderation, underlining its contribution to the broader discourse on AI's role in society.

In this paper, we introduce \textsf{IntMeme}\footnote{https://github.com/Social-AI-Studio/IntMeme}, a new framework that leverages the generative abilities of large multimodal models (LMMs) to generate high-quality interpretations of memes for classifying hateful content. \textsf{IntMeme} prompts LMMs to generate these interpretations, thereby enhances the explainability of the classification process and reduces the dependence on the model's implicit knowledge. The framework then encodes both the meme and its interpretation using separate modules, which are subsequently used for final classification. This method of grounding classification decisions in meme interpretations significantly improves the accuracy and explainability of hateful meme detection while also providing clearer insights into the reasoning behind classification decisions. Figure\ref{fig:intmeme-teaser} illustrates the benefits of the \textsf{IntMeme} framework in a content moderation process involving a human moderator.

To demonstrate the effectiveness of \textsf{IntMeme} in classifying hateful memes, we conducted comprehensive experiments on three well-known datasets containing hateful memes. Our comparisons with leading PT-VLMs showed that \textsf{IntMeme} outperformed state-of-the-art baselines across all three datasets. Additionally, our ablation studies highlighted the importance of generating high-quality interpretations and utilizing distinct encoding modules, both of which significantly improved the detection of hateful memes. Furthermore, our detailed case analysis and human evaluation study underscored the effectiveness and practical utility of \textsf{IntMeme} within a simulated content moderation process. The high-quality meme interpretations used in our method enhance the explainability of the classification process, providing clearer insights into how \textsf{IntMeme} differentiates between hateful and non-hateful content. Overall, our framework promotes effective collaboration between humans and AI in online content moderation, emphasizing its contribution to the ongoing discussion about AI's role in society.


We summarize our contributions as follows: (i) We introduced \textsf{IntMeme}, a novel multimodal framework leveraging LMMs to generate insightful meme interpretations. This approach improves model explainability and provides deeper insights into the decision-making process, aiding in the distinction between hateful and non-hateful content; (ii) \textsf{IntMeme} addresses the limitations inherent in fine-tuning PT-VLMs by employing separate modules for efficient encoding of memes and their interpretations; (iii) Our comprehensive experiments on three popular harmful meme datasets validate both the efficacy and explainability of \textsf{IntMeme} when compared against similarity sized state-of-the-art harmful meme detection models. % The empirical analysis also showcases the quality and utility of interpretation in explaining classification decisions.

%We validated \textsf{IntMeme}'s effectiveness through comprehensive experiments on three major hateful meme datasets, comparing its performance against leading PT-VLMs. Our findings reveal that \textsf{IntMeme} consistently outperforms state-of-the-art models across all datasets. Ablation studies highlight the critical role of high-quality meme interpretations and the use of distinct encoding modules in enhancing detection accuracy. Detailed case analyses further demonstrate our method's explainability and pinpoint opportunities for further refinement.




%In this paper, we introduce \textsf{IntMeme}\footnote{code be released in camera-ready}, a novel framework designed to leverage the generative strengths of LMMs for the nuanced task of hateful meme classification. \textsf{IntMeme} employs LMMs to generate insightful interpretations of memes, thereby enhancing the explainability of the classification process and reducing the model's dependency on implicit, often opaque knowledge. This approach involves prompting LMMs to produce detailed meme interpretations, followed by the application of specialized modules for encoding both the meme and its generated interpretation. This encoded information is then used for classification, addressing the limitations associated with fine-tuning pre-trained vision-language transformer models (PT-VLMs) and improving vision-language alignment and meme understanding.

%Our framework significantly enhances explainability in the hateful meme classification process by basing predictions on these detailed interpretations, and it demonstrates notable performance improvements. Figure \ref{fig:intmeme-teaser} showcases the \textsf{IntMeme} framework's benefits in a content moderation scenario, featuring a human moderator for illustrative purposes. Ultimately, our research champions a more transparent and interpretable approach to combating hateful memes. By grounding classification decisions in understandable features, \textsf{IntMeme} clarifies the distinction between hateful and non-hateful content, improving the classification process's transparency and aiding in the broader understanding of effective content moderation strategies.

%In recent years, the proliferation of internet memes dissemination has become an integral part of modern communication and culture. These visual and textual artifacts, often characterised by humor and satire, possess the power to shape and influence public opinions and perceptions. While many memes contribute to light-hearted amusement and social cohesion, a darker aspect of this phenomenon has emerged: the propagation of \textit{hateful} memes. The dissemination of these hateful memes has been associated with the amplification of social tensions, reinforcement of stereotypes, and the spread of misinformation. These issues pose substantial challenges to the creation of a harmonious and inclusive society. Thus, the circulation of hateful memes underscores the pressing need to develop effective detection methods to mitigate their adverse societal impacts and foster a healthy online environment.



%In recent years, the proliferation of internet memes has become an integral part of modern communication and culture. These visual and textual artifacts, often characterized by humor and satire, possess the power to shape public opinions and perceptions. While many memes contribute to light-hearted amusement and social cohesion, a darker aspect of this phenomenon has emerged: the propagation of \textit{hateful} memes. The dissemination of these hateful memes has been linked to the amplification of social tensions, the reinforcement of stereotypes, and the spread of misinformation. These issues present significant challenges to creating a harmonious and inclusive society. Thus, the circulation of hateful memes underscores the pressing need to develop effective classification methods that mitigate their adverse societal impacts and promote a healthy online environment.

%The detection of hateful memes presents a multifaceted challenge that demands comprehensive and nuanced multimodal understanding. The primary aspects of effective hateful meme detection require understanding the intricate interplay of diverse modalities and interpreting the implied meaning concealed within the memes. To address these challenges, previous works have fine-tuned attention-based small vision-language models (S-VLMs) on curated hateful meme datasets with supplementary information (i.e., image captioning). These augmented approaches strive to capture the nuanced modality relationships and comprehend the concealed implied meaning. Nevertheless, these models exhibit limitations. Firstly, the intricate attention mechanism within these models hinders explainability, impeding the transparency crucial for building societal trust in the detection process. While recent studies have established the attention-based multimodal models' ability to acquire new vision-language alignments indicative of multimodal understanding, it remains unclear whether these models can genuinely comprehend the implied meaning conveyed in the meme. Secondly, these S-VLMs are pre-trained on image-text alignment tasks, such as image captioning and object recognition, which raises concerns about their suitability for comprehending multimodal memes with supplementary information.

%The classification of hateful memes is a multifaceted endeavor, demanding a profound and nuanced understanding across multiple modalities \cite{kiela2021hateful}. The essence of this classification process lies in deciphering the complex interplay between various modalities and comprehending the hidden meanings within the memes. To tackle this, prior research has adapted pre-trained vision-language transformer models (PT-VLMs) \cite{lu2019vilbert,li2019visualbert,singh2022flava} for hateful meme classification tasks \cite{pramanick2021harmemes,mathias2021fhmfg,fersini2022mami}, often supplemented with additional information like image captions \cite{velioglu2020detecting,zhu2020enhance}. While these refined approaches aim to tease out the subtle relationships between modalities and understand the underlying message, they aren't without limitations. A significant drawback is the complex attention mechanism within these models, which clouds explainability — a transparency that's vital to earn societal trust in the classification mechanism. Recent studies underscore the prowess of these PT-VLMs in forming new vision-language alignments, a sign of their capability to grasp multimodal nuances \cite{cuo2022understanding, hee2022explaining}. However, these studies also show that PT-VLMs can be overly sensitive to these multimodal nuances and may misclassify non-hateful examples, causing one to wonder whether these models can truly fathom the depth of meaning memes often carry. Moreover, the initial training of these PT-VLMs on tasks like image captioning and object recognition casts doubts on their aptness for processing hateful memes that come with additional context.

% Recent advancements in the field have given rise to Large Vision-Language Models (L-VLMs) with notable generative capabilities, exemplified by models such as GPT-4(V) \cite{openai2023gpt4v}, mPLUG-Owl \cite{ye2023mplug} and InstructBLIP \cite{dai2023instructblip}. These models have showcased profound multimodal comprehension and text generation prowess, as evidenced by their ability to produce pertinent text-based responses for intricate visual question-answering tasks. As a result, L-VLMs are emerging as a more comprehensive solution for detecting hateful memes and providing insightful explanations. However, several challenges accompany these models. Firstly, their refinement demands considerable computational resources, raising concerns about financial feasibility and scalability. Secondly, these generative models might produce responses that diverge from the intended query. Lastly, extracting classification responses from the generated text presents a challenge, given that the relevant information could be dispersed throughout the output.

%Recent advancements in artificial intelligence have led to the development of Large Multimodal Models (LMMs) that exhibit impressive generative capabilities, exemplified by models like GPT-4(V) \cite{openai2023gpt4v}, mPLUG-Owl \cite{ye2023mplug}, and InstructBLIP \cite{dai2023instructblip}. These models demonstrate profound multimodal comprehension and text generation prowess, as evidenced by their ability to produce accurate and relevant text-based responses for complex vision-language tasks \cite{xu2023lvlm,yang2023dawn}, such as visual question answering and visual commonsense reasoning. As a result, LMMs are emerging as a more viable solution for detecting hateful memes and providing insightful explanations. However, these models come with several challenges. Firstly, their refinement requires substantial computational resources, posing concerns about feasibility and scalability in real-world deployment. Secondly, there is a risk that generative models may produce responses diverging from the intended query. Lastly, extracting classification decisions from the generated text is challenging, given that the relevant output information could be dispersed throughout the text. These limitations inspired us to explore new approaches in leveraging these LMMs for hateful meme classification while retaining their ability to produce quality text-based responses.

%In this paper, we present \textsf{IntMeme}~\footnote{code be released in camera-ready}, a novel framework that capitalizes on the generative capabilities of LMMs to produce quality meme interpretations for the classification of hateful memes. Specifically, \textsf{IntMeme} prompts LMMs to generate quality meme interpretations, enhancing the explainability of the hateful meme classification process and alleviating the heavy reliance on implicit knowledge within the model. It then uses distinct modules to effectively encode the meme and its interpretation before performing the classification on the encoded information. This methodology not only overcomes issues tied to fine-tuning PT-VLMs with auxiliary data but also bolsters the specificity of vision-language alignment and meme comprehension. As a result, \textsf{IntMeme} improves the explainability of the hateful meme classification process by grounding the predictions on these quality interpretations while achieving some performance improvement. Figure \ref{fig:intmeme-teaser} illustrates the advantages of the IntMeme framework in a content moderation process involving a human moderator.

% To substantiate \textsf{IntMeme}'s efficacy in hateful meme classification, we perform extensive experiments on three predominant hateful meme datasets, contrasting its performance with leading PT-VLMs. Our findings indicate that \textsf{IntMeme} outperformed the state-of-the-art baselines in three real-world hateful meme datasets. Ablation studies further underscored the utility of generating quality interpretations and employing distinct encoding modules significantly improve hateful meme detection performance. Through detailed case analysis, we elucidated our method's explainability and identified areas for refinement. Collectively, our studies advocate for a more lucid and robust strategy to curtail the dissemination of hateful memes.

%To substantiate the efficacy of \textsf{IntMeme} in classifying hateful memes, we conducted extensive experiments on three prominent datasets containing hateful memes. We compared its performance with leading PT-VLMs, revealing that \textsf{IntMeme} surpassed state-of-the-art baselines across three real-world datasets. Our ablation studies further emphasized the significance of generating high-quality interpretations and utilizing distinct encoding modules to significantly enhance the detection performance of hateful memes. Through detailed case analysis, we elucidated the explainability of our method and identified areas for refinement. Overall, our research advocates for a more explainable approach to mitigate the proliferation of hateful memes, emphasizing the grounding of classification decisions on interpretable features. The quality meme interpretation used in our method improves the transparency of the classification process, enabling a better understanding of how \textsf{IntMeme} distinguishes between hateful and non-hateful content.

%We summarize our contributions as follows: (i) We developed \textsf{IntMeme}, a novel multimodal framework that capitalizes on LMMs to generate quality interpretations before using them for hateful meme classification. The generated meme interpretation not only improves transparency in the classification process but also provides insights into how \textsf{IntMeme} makes decisions, contributing to a better understanding of the distinction between hateful and non-hateful content. (ii) \textsf{IntMeme} addresses the challenges inherent in fine-tuning PT-VLMs by employing distinct modules for the independent encoding of memes and their interpretations; (iii) Our rigorous experimentation validates both the efficacy and explainability of \textsf{IntMeme} in comparison to state-of-the-art hateful meme classification methodologies.
