
\begin{figure*}[t!]
\centering
\includegraphics[width=\linewidth]{images/IntMeme-architecture.pdf}
% \caption{Overview of the IntMeme framework for hateful meme classification. The framework consists of two distinct modules for information encoding: (1) the Vision-Language Alignment module, meant to capture nuanced cross-modality interactions, and (2) the Meme Interpretation Encoding module, which efficiently encodes the generated meme interpretations.}
\caption{Overview of the IntMeme framework for hateful meme classification, comprising two modules: (1) Vision-Language Alignment and (2) Meme Interpretation Encoding.}
\label{fig:framework}
\end{figure*}

\label{sec:related}
\subsection{Hate Speech Detection}
Hate speech is an increasingly prevalent issue worldwide, spreading rapidly through digital platforms and fostering social divisions. It poses a significant risk, as it not only perpetuates discriminatory attitudes but also has the potential to escalate into offline hate crimes \cite{lupu2023offline,muller2021fanning,muller2023hashtag}, resulting in severe consequences for affected communities. Researchers tackle this issue by creating datasets and developing new approaches to detect hate speech \cite{davidson2017automated,founta2018large,yoder2022hate} and explain the underlying implicit messages \cite{sap2019social,elsherief2021latent}. More recently, several studies have highlighted concerns in HS detection systems, introducing functional tests for evaluating HS detection models \cite{ng2024sghatecheck,rottger2020hatecheck,rottger2022multilingual}. Such efforts are crucial for promoting transparency and accountability, creating a safer and inclusive online environment.

Internet memes play a crucial role in online communication, serving both as sources of humor and as vehicles for disseminating hateful messages \cite{hee2024recent,hee2024understanding,uyheng2020visualizing}. This dual nature has attracted considerable attention from both industry and academia \cite{kiela2020hateful, pramanick2021harmemes, fersini2022mami, lim2024aisg, thapa2024ruhate}. The negative impacts of these hateful memes have led to the development of classification models aimed at identifying them \cite{pramanick2021momenta, thakur2022multimodal, lee2021disentangling, hee2024bridging}. For instance, \citet{pramanick2021momenta} introduced a model that detects hate by capturing complex multimodal interactions through the integration of local and global information, while also utilizing object proposals and extracted entity data. Similarly, \citet{cao2022prompting} developed a prompt-based transformer model that incorporates examples of both hateful and non-hateful memes, along with an unseen meme for inference. More recent studies have employed large multimodal models (LMMs) to generate explanations based on the true classification labels in the prompt, followed by fine-tuning a classification model using knowledge distillation \cite{lin2023beneath} or multimodal debate mechanisms \cite{lin2024towards}. In contrast, IntMeme prompts LMMs to produce meaningful interpretations of memes without relying on knowledge of the true labels and introduces separate modules to efficiently encode both the meme and its interpretation.

\subsection{Large Multimodal Models}
The emergence of large multimodal models (LMMs) represents a significant advancement in artificial intelligence, demonstrating impressive generative capabilities \cite{ye2023mplug,dai2023instructblip,openai2023gpt4v,deitke2024molmo}. These models typically utilize a pre-trained large language model (LLM) as their foundational base, incorporating a vision projection module that includes an image encoder (e.g., ViT \cite{dosovitskiy2020image}, EVA \cite{fang2023eva}) and several image projection layers to convert images into the text embedding space. This configuration enables LMMs to effectively interpret visual inputs while leveraging the robust language modeling capabilities of the foundational LLM.

LMMs have demonstrated excellent performance across various multimodal tasks, including understanding the subtleties of humor within visual and textual content \cite{yang2023dawn,zheng2023judging,han2023evaluate}. Recent research on LMMs has also explored reasoning capabilities related to humor, focusing specifically on jokes and humorous memes. For instance, \citet{ye2023mplug} demonstrate that their mPLUG-Owl LMM exhibits strong vision-language understanding, allowing it to grasp visually driven jokes. Additionally, \citet{yang2023dawn} highlight the impressive ability of GPT-4V to extract information from both visual and textual modalities, facilitating the comprehension of humor within memes. Drawing inspiration from Socratic models \cite{zeng2022socratic} that combine various large pre-trained models to address new multimodal challenges, our approach employs LMMs to generate meaningful interpretations prior to their use in classifying hateful memes. This strategy enhances both model performance and explainability.


% Despite these advances, existing models rely heavily on implicit knowledge and attention mechanisms for meme interpretation. Our proposed method aims to mitigate this reliance by leveraging LMMs to generate high-quality interpretations of memes, offering a novel approach that enhances both the explicitness and explainability of meme content classification.