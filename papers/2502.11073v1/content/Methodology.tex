% The \textsf{IntMeme} framework utilizes the powerful multimodal reasoning capabilities of Large Language Models (LLMs) to generate high-quality interpretations of memes, which aids in classifying hateful memes in a manner similar to the human cognitive process of meme interpretation before assessing their hatefulness. Moreover, the interpretations generated by \textsf{IntMeme} enhance the explainability of the classification process, allowing end-users to review the interpretations to understand better how \textsf{IntMeme} differentiates between hateful and non-hateful content. Figure 2 presents an overview of our proposed \textsf{IntMeme} framework.

The \textsf{IntMeme} framework uses the strong multimodal reasoning capabilities of LMMs to generate high-quality interpretations of memes, aiding in the classification of hateful memes. This approach mirrors the human process of understanding memes before assessing their potential for hatefulness. Additionally, this approach enables end-users to review and comprehend the generated interpretations, enhancing the explainability of the classification process. Figure \ref{fig:framework} presents an overview of the \textsf{IntMeme} framework.

% The \textsf{IntMeme} framework leverages the strong multimodal reasoning capabilities of LMMs to generate quality meme interpretations for the classification of hateful memes, improving the transparency of the classification process. The generated meme interpretations help users better comprehend how \textsf{IntMeme} distinguishes between hateful and non-hateful content. We start by explaining the zero-shot inference approach to generate meme interpretations using a frozen pre-trained LMM, which involves carefully crafted system instructions and human prompt design. Following that, we elaborate on the distinct modules used to efficiently encode both the meme and its interpretation, leading to improved classification performance. Figure 2 shows the overview of our proposed \textsf{IntMeme} framework.

\subsection{Generating Meme Interpretation}

\paragraph{Zero-Shot Inference using LMMs.} Our methodology uses a pre-trained language model in a zero-shot setting. Internet memes, created and shared by diverse netizens, cover numerous topics, tones, and cultural contexts. \cite{kiela2020hateful,fersini2022mami}. This variety makes it challenging to adequately address all the variations with a limited set of demonstration examples. A limited set of examples may introduce bias, hindering the model's ability to interpret diverse content accurately and increasing computational resources and processing time. On the other hand, recent studies also highlighted the strong zero-shot reasoning capabilities of LMMs across various multimodal tasks \cite{yang2023dawn,fu2023mme}. Hence, our approach leverages and explores the limits of LMMs in a zero-shot setting.

% Our methodology proposes capitalizing on a frozen pre-trained LMM through a zero-shot inference approach, analogous to the human cognitive process of interpreting memes before determining their hatefulness. While few-shot demonstrations have proven effective in closed-book scenarios with limited demonstrations from a predefined set of classes, applying them to Internet meme interpretation faces significant challenges. Internet memes, created and shared by netizens with diverse backgrounds, span a wide spectrum of topics, tones, and cultural contexts. This diversity makes it challenging to adequately cover the various meme variations with a limited set of few-shot demonstrations. Such a limited set of few-shot demonstrations can introduce potential bias and restrict the model's ability to accurately interpret diverse content. It is also worth noting that using few-shot demonstrations demands considerably more computational resources and processing time for LMMs, rendering it impractical for real-world content moderation. Furthermore, recent studies have highlighted the strong reasoning capabilities of LMMs across various multimodal tasks when used in a zero-shot inference setting \cite{yang2023dawn,fu2023mme}. Hence, our approach capitalizes on and explores the limits of the pre-trained knowledge within the LMM in a zero-shot inference setting, eliminating the need for additional resource-intensive demonstrations.

\definecolor{text_placeholder}{HTML}{8A9DB8}
\definecolor{length_control_measure}{HTML}{1B9E77}
\definecolor{caption_placeholder}{HTML}{C77975}

\begin{table}[t]
\small
\centering
\begin{tabular}{p{\linewidth}}
\toprule
\textbf{System Instructions*} \\
\midrule
The following is a conversation between a human content moderator, who works on meme moderation, and an AI assistant. The assistant provides an informative interpretation of memes, including details about the underlying message and any potential prejudice (i.e. towards individuals or communities) within the memes. It is important that the interpretation utilizes both the visual and linguistic elements of the memes. \\ 
\bottomrule \\
\toprule
\textbf{Human Prompt} \\ 
\midrule
Given that the generated caption for the meme is ``\textcolor{caption_placeholder}{\{caption\}}'' and the overlaid text on this meme is ``\textcolor{text_placeholder}{\{text\}}'', interpret the conveyed message and any potential bias conveyed in the meme \textcolor{length_control_measure}{using a paragraph containing three sentences}. \\
\bottomrule
\end{tabular}
\caption{Example system instructions and prompts for generating \textsl{meme interpretation}. The prompt input includes the caption placeholder tag (in \textcolor{caption_placeholder}{orange}), text placeholder tag (in \textcolor{text_placeholder}{blue}), and the length control measure (in \textcolor{length_control_measure}{green}). \textit{*InstructBLIP does not customization of model behaviour via system instructions.}}
\label{tab:lvlm_settings}
\end{table}


\paragraph{System Instructions.} 
% To ensure the generation of accurate and quality meme interpretations through a zero-shot approach, we used an thorough process involving customizing the behaviour of a LMM and crafting a meaningful human prompt. We used custom system instructions to adjust the behaviour of these large instruction-tuned models, facilitating a purposeful adaptation of the model's responses and aligning the responses with the goals of meme interpretability. Our custom system instructions guide the models in producing informative interpretations of a meme, while adeptly identifying potential visual and textual nuances within the meme that may encompass social prejudice. The custom system instruction can be found in Table \ref{tab:lvlm_settings}.
To ensure the generation of accurate and high-quality meme interpretations using a zero-shot approach, we implemented a careful process that involved customizing the behavior of a large multimodal model (LMM) and creating a meaningful human prompt. By employing custom system instructions, we were able to adjust the responses of these instruction-tuned models, aligning them with the objectives of meme interpretability. These instructions guide the models to produce informative interpretations while effectively identifying potential visual and textual nuances within the memes that may reflect social prejudice. The system instruction can be found in Table~\ref{tab:lvlm_settings}.

\paragraph{Prompt Design.} 
We designed a model prompt to guide the LMMs in producing clear and informative meme interpretations. The objetive of this prompt is to generate a high-quality interpretation that not only captures the meme's underlying message but also identifies any potential bias it may convey. To achieve this, we designed the prompt to include both the text overlay from the meme and an image caption generated by the same LMM, encouraging the model to focus on reasoning. However, recent studies also show that instruction-tuned LMMs often produce lengthy responses, which can lead to performance and encoding challenges, particularly with pre-trained encoding modules. To address this, we incorporated explicit length control measures into the model prompt. The details of the human prompt are provided in Table \ref{tab:lvlm_settings}.

\subsection{Information Encoding \& Classification}
% The IntMeme framework uses a model architecture comprising two distinct modules: the \textsl{vision-language alignment module} and the \textsl{meme interpretation encoding} module. The vision-language alignment module captures the meme and its cross-modality information, while the meme interpretation encoding module efficiently encodes the generated meme interpretation. Subsequently, these encoded representations are concatenated and processed through a classification layer to facilitate the classification of potentially hateful memes. In this configuration, the meme interpretation encoding module captures the primary interpretation of the meme, while the vision-language understanding module supplements aditional meme information for any uncaptured cross-modality information. Notably, our approach enhances explainability and provides insights into the model's classification decisions by conditioning meme classification on nuanced cross-modality information and the generated meme interpretation.
The IntMeme framework uses two distinct modules to encode information efficiently: the \textsl{meme interpretation encoding} (MIE) module and the \textsl{vision-language alignment} (VLA) module. The MIE module is responsible for learning the semantic meanings of meme interpretations, whereas the VLA module focuses on capturing both the inter- and intra-modality information present within memes. Subsequently, these encoded representations are combined to classify potentially hateful memes. This approach improves the model's explainability by providing insights into the model's decisions through the conditioned meme interpretation.

\subsubsection{Meme Interpretation Encoding Module}
% While it is possible to encode the meme interpretation alongside the meme inputs using PT-VLMs, the PT-VLMs are typically pre-trained on a combination of Masked-Language Modeling (MLM), Masked-Image Modeling (MIM) and Image-Text Matching (ITM) objectives to align specific parts of images with text. This endows them with efficiency in vision-language encoding but limits their capability to process additional supplementary information. To overcome this limitation, we used a separate text encoder to capture and align the information in the meme interpretation. Specifically, we utilized the RoBERTa model, a robust pre-trained language model renowned for its proficiency in diverse language modeling tasks. Formally, we feed the generated meme interpretation $\mathcal{G}$ into the RoBERTa model to generate the hidden states $\mathbf{I}$:
We use a separate text encoder module to learn the semantic meaning of the meme interpretation. Formally, we feed the generated meme interpretation $\mathcal{G}$ into the text encoder model to generate the hidden states $\mathbf{I}$:

\begin{equation*}
    \mathbf{I} = \text{Encoder}_\text{text}(\mathcal{G})
\end{equation*}

From the hidden states $\mathbf{I}$, we use the hidden state from the $\mathbf{[CLS]}$ token ($\mathbf{I}_{CLS}$). This token has demonstrated effectiveness in sentence understanding tasks, as evidenced in \cite{reimers2019sentence,liu2019roberta}.

\subsubsection{Vision Language Alignment Module.} 
% While using a zero-shot approach to generate meme interpretations offers practicality, LMMs can generate irrelevant interpretations of the meme \cite{ji2023survey}. To mitigate the severity of providing irrelevant interpretation, the vision-language alignment module uses a pre-trained PT-VLM to process the meme and its intricate cross-modality interactions for hateful meme classification. This supplementary meme information allows the model to rely on the vision and language information within the meme, alleviating the model’s dependency on the generated meme interpretation. In this study, we used the \textit{Foundational Language And Vision Alignment} (\textbf{FLAVA}) model \cite{singh2022flava}, chosen for its exceptional performance across various multimodal understanding tasks. Importantly, FLAVA has also been pre-trained on the hateful memes detection task, providing it a nuanced perspective on meme comprehension. 
% To obtain the meme representation, we feed the the meme image $\mathcal{V}$ and meme text $\mathcal{T}$ into the FLAVA model to generate the hidden states $\mathbf{M}$:
While generating meme interpretations in a zero-shot approach offers practical advantages, these interpretations can be misleading or contain inaccuracies \cite{ji2023survey}. To reduce the severity of misleading interpretation, the vision-language alignment module processes and supplements the intricate inter- and intra-modality interactions within the meme. This supplementary meme information allows the model to rely on the vision and language information within the meme, alleviating the model’s dependency on the generated meme interpretation. Formally, we feed the the meme image $\mathcal{V}$ and meme text $\mathcal{T}$ into a vision-language model to generate the hidden states $\mathbf{M}$:

\begin{equation*}
    \mathbf{M} = \text{Encoder}_\text{vision-language}([\mathcal{V}, \mathcal{T}])
\end{equation*}

From the hidden states $\mathbf{M}$, we use the hidden state from the $\mathbf{[CLS]}$ token ($\mathbf{M}_{CLS}$). This token has been included to facilitate the multimodal understanding tasks during pre-training, serving as an ideal representation of the meme context.

% \subsubsection{Hateful Meme Classification Layer. \textcolor{red}{unedited}} 
% % To perform the detection of the hateful classification task, we utilise both the $h_{CLS,m}$ hidden state and $h_{CLS,i}$ hidden state from FLAVA and RoBERTa model respectively. This allows the model’s prediction to be conditioned on the explicit meme interpretation and the joint multimodal representation, making it more robust to potential inaccuracies in the interpretation. We first concatenate the $h_{CLS,m}$ and $h_{CLS,i}$ before performing a linear projection for harmful meme classification. 

% To perform the classification of the hateful memes, we feed the $\mathbf{M}_{CLS}$ and $\mathbf{I}_{CLS}$ hidden states into a linear layer for hateful meme classification. 

% \begin{equation}
%     \nonumber
%     \mathbf{prediction} = \text{Sigmoid}(\mathbf{W}^\text{T}[\mathbf{M}_{CLS}, \mathbf{I}_{CLS}]+\mathbf{b}),
% \end{equation}

% where $[\cdot, \cdot]$ represents concatenation, $\mathbf{W} \in \mathbb{R}^{d \times 2}$ are learnable weights and $\mathbf{b} \in \mathbb{R}^{d\times 2}$ are learnable bias. This allow the model to condition its prediction on the encoded meme representation and the encoded meme interpretation representation, making it more robust to potential inaccuracies in the generated interpretation.


\subsection{Classification Layer}
% After obtaining the meme and interpretation representations, we experimented with two methods for combining these representations: gated fusion and concatenation. Concatenation preserves the original information from each representation, while the gated fusion mechanism controls the flow of information from the different representations. The features derived from these two approaches are calculated as follows:

% \begin{equation}
%     \mathbf{F}_{concat} = [\mathbf{M}_{CLS}, \mathbf{I}_{CLS}] \\
% \end{equation}

% \begin{gather*}
%     \mathbf{I}_{transformed} = \mathbf{ReLU}(\mathbf{W}_{i} \mathbf{I}_{CLS} + b_i) \\
%     \mathbf{M}_{transformed} = \mathbf{ReLU}(\mathbf{W}_{m} \mathbf{M}_{CLS} + b_m) \\
%     \mathbf{F}_{gate} = \sigma(\mathbf{W}_{M} \mathbf{M}_{CLS} + \mathbf{W}_I \mathbf{I}_{CLS}+\mathbf{b_g})
% \end{gather*}

% where $\textbf{W}_{L}$, $\textbf{W}_{L}$ and $\textbf{b}_{g}$ are learnable parameters. The representations, $\textbf{F}_{concat}$ and $\textbf{F}_{gate}$, will have a $\mathbb{R}^{d}$ shape with varying d size.

% \begin{equation*}
%     \mathbf{F}_{concat} = [\mathbf{M}_{CLS}, \mathbf{I}_{CLS}] \\
% \end{equation*}

% \begin{equation*}
%     \mathbf{F}_{gate} = \sigma(\mathbf{W}_{M} \mathbf{M}_{CLS} + \mathbf{W}_I \mathbf{I}_{CLS}+\mathbf{b_g})
% \end{equation*}

After obtaining the representations ($\mathbf{M}_{CLS}$ and $\mathbf{I}_{CLS}$), we concatenate and feed them into a classification layer. The classification layer consists of a single-layer perception followed by a softmax layer for normalization.

\begin{equation}
    \nonumber
    O = \text{Sigmoid}(W^\text{T}[\mathbf{M}_{CLS}, \mathbf{I}_{CLS}]+ b),
\end{equation}

where $[\cdot, \cdot]$ represents concatenation, $W \in \mathbb{R}^{d \times 2}$ are learnable weights and $b \in \mathbb{R}^{2}$ are learnable bias. The final prediction, $O \in \mathbb{R}^{2}$, represents the logits for each class.