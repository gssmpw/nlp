% \begin{table*}[t]
% \centering
%   \caption{Model comparison \textbf{without} any augmented image tags.}
% \label{tab:exp-results-wo}
%   \begin{tabular}{c|cc|cc|cc}
%     \hline
%     \textbf{Dataset} &\multicolumn{2}{c|}{\textbf{FHM}}&\multicolumn{2}{c|}{\textbf{MAMI}}&\multicolumn{2}{c}{\textbf{HarM}}\\
%     \textbf{Model} & \textbf{AUC.} & \textbf{Acc.}& \textbf{AUC.} & \textbf{Acc.} & \textbf{AUC.} & \textbf{Acc.}\\
%     \hline\hline
%     Text BERT & 66.10$_{\pm0.55}$& 57.12$_{\pm0.49}$   & 74.48$_{\pm0.60}$ & 67.37$_{\pm0.57}$  & 81.39$_{\pm0.91}$& 75.68$_{\pm1.59}$\\
%     Image-Region & 56.69$_{\pm1.05}$ &52.34$_{\pm1.39}$  & 70.20$_{\pm0.63}$ & 64.18$_{\pm0.81}$ &76.46$_{\pm0.47}$ &73.05$_{\pm1.80}$ \\
%     \hline\hline
%     VisualBERT COCO & 68.71$_{\pm1.02}$& 61.48$_{\pm1.19}$  &78.71$_{\pm0.59}$ &71.06$_{\pm0.94}$  &80.46$_{\pm1.04}$ &75.31$_{\pm1.44}$ \\
%     ViLBERT CC& 73.05$_{\pm0.62}$&64.70$_{\pm1.12}$  &77.71$_{\pm1.20}$ &69.48$_{\pm1.00}$  &84.11$_{\pm0.88}$ &78.70$_{\pm1.17}$  \\
%     MMBT-Region  & 72.86$_{\pm0.64}$&65.06$_{\pm1.76}$  & 79.17$_{\pm0.91}$& 70.46$_{\pm0.76}$ & 85.48$_{\pm0.75}$& 79.83$_{\pm2.00}$ \\
%     \hline
%     CLIP-BERT  & 66.97$_{\pm0.34}$&58.28$_{\pm0.63}$  & 77.66$_{\pm0.64}$& 68.44$_{\pm1.07}$ &82.63$_{\pm3.83}$ &80.48$_{\pm1.95}$  \\
%     DisMultiHate & 69.11$_{\pm0.84}$& 62.42$_{\pm0.72}$ &78.21$_{\pm0.61}$ & 70.58$_{\pm1.13}$ & 83.69$_{\pm1.33}$& 78.05$_{\pm0.73}$ \\
%     PromptHate & 76.76$_{\pm0.95}$&67.82$_{\pm1.23}$  &76.21$_{\pm1.05}$ &68.08$_{\pm0.58}$  &87.51$_{\pm0.74}$ & 79.38$_{\pm1.72}$ \\
%     \hline
%     FLAVA & ~77.40 & ~69.0 & & & & \\
%     \hline\hline
%     MIME$_{FLAVA}$ & ~77.40 & ~69.0 & & & & \\
%     \hline
% \end{tabular}
% \end{table*}


\begin{table}[t]
\centering
  % \small
  \begin{tabular}{c|cc|cc}
    \hline
     & \multicolumn{2}{c|}{\textbf{Train}} & \multicolumn{2}{c}{\textbf{Test}}\\
    Dataset & \# H & \# Non-H & \# H & \# Non-H\\
    \hline\hline
    FHM-FG & 3,007 & 5,493 & 246 & 254 \\
    HarMeme & 1,064 & 1,949 & 124 & 230\\
    MAMI & 5,004 & 4,996 & 500 & 500 \\
    \hline
\end{tabular}
\caption{Statistical distributions of datasets, where "H" represents harmful and "Non-H" represents non-harmful }
  \label{tab:dataset}
\end{table} 

\subsection{Evaluation Datasets} 
We evaluated \textsf{IntMeme} against the state-of-the-art PT-VLMs across three widely-used hateful meme datasets, showcasing its robustness and generalizability. 
\textit{Facebookâ€™s Fine-Grained Hateful Memes} (\textbf{FHM-FG}) dataset \cite{mathias2021fhmfg} is a synthetic memes dataset containing hateful memes with five distinct types of incitement to hatred: gender, racial, religious, nationality and disability-based. \textit{Multimedia Automatic Misogyny Identification} (\textbf{MAMI}) dataset \cite{fersini2022mami} consists of misogynous memes collected from popular social media platforms and websites dedicated to meme creation. Evaluating our models on this dataset provides insight into the performance of hateful meme detection models in a natural environment. \textit{Harmful Meme} (\textbf{HarMeme}) dataset \cite{pramanick2021harmemes} consists of crowdsourced memes primarily collected from Google Image Search and publicly available groups on popular social media websites. These memes contains \textit{harmless}, \textit{partially harmful}, and \textit{very harmful} memes related to the COVID-19 topic. Following \citeauthor{pramanick2021harmemes}, we merge \textit{partially harmful}, and \textit{very harmful} into a single \textit{harmful} category.
A summary of the distribution of the three datasets is presented in Table \ref{tab:dataset}.

\subsection{Models}
% We conducted an evaluation of \textsf{IntMeme} against six state-of-the-art PT-VLMs. The \textbf{VisualBERT} \cite{li2019visualbert} model uses a single-stream transformer-based approach that concurrently processes textual and visual inputs using a single Transformer module. In contrast, the \textbf{ViLBERT} \cite{lu2019vilbert} uses a dual-stream transformer-based approach that independently processes textual and visual inputs before utilizing Transformer modules to capture inter-modality interactions. More recently, the \textit{Bootstrapping Language-Image Pre-training} (\textbf{BLIP}) \cite{li2022blip} model is pre-trained on a mixture of multimodal encoder-decoder models using a dataset bootstrapped from large-scale noisy image-text pairs. The \textit{Foundational Language and Vision Alignment} (\textbf{FLAVA}) \cite{singh2022flava} model is pre-trained on multimodal and unimodal data with unpaired images and text. Moving into models designed for hateful memes detection, the \textbf{MOMENTA} \cite{pramanick2021momenta} model utilizes both local and global multimodal fusion mechanisms to exploit interactions for detecting harmful memes. The \textbf{DisMultiHate} \cite{lee2021disentangling} model adopts a disentanglement approach to separate target information from memes, crucial for identifying hateful content. Our implementation in this paper uses the VisualBERT and ViLBERT model pre-trained on the MS-COCO dataset \cite{lin2014microsoft} and Conceptual Captions \cite{sharma2018conceptual} respectively.
We evaluated \textsf{IntMeme} against seven state-of-the-art models. The \textbf{VisualBERT} \cite{li2019visualbert} model uses a single-stream transformer-based approach that concurrently processes textual and visual inputs using a single Transformer module. In contrast, the \textbf{ViLBERT} \cite{lu2019vilbert} uses a dual-stream transformer-based approach that independently processes textual and visual inputs before using Transformer modules to capture inter-modality interactions. More recently, the \textbf{BLIP} \cite{li2022blip} model is pre-trained on a mixture of multimodal encoder-decoder models using a dataset bootstrapped from large-scale noisy image-text pairs. The \textbf{FLAVA} \cite{singh2022flava} model is pre-trained on multimodal and unimodal data with unpaired images and text. Moving into models designed for hateful memes detection, the \textbf{MOMENTA} \cite{pramanick2021momenta} model utilizes both local and global multimodal fusion mechanisms to exploit interactions for detecting harmful memes. The \textbf{DisMultiHate} \cite{lee2021disentangling} model adopts a disentanglement approach to separate target information from memes, crucial for identifying hateful content. Lastly, the \textbf{PromptHate} \cite{cao2022prompting} model uses a prompt-based approach with few-shot demonstrations to classify memes.


\subsection{Evaluation Metrics}
% As hateful meme classification is primarily a binary classification task, we employed two widely adopted metrics to evaluate the performance of the various models: Accuracy (Acc.) and Area Under the Receiver Operating Characteristics curve (AUROC). All the experimental results are aggregated across five random seeds, with the average results and standard deviation reported. All the models use the same set of random seeds to ensure a fair comparison.

We employed two widely adopted metrics to evaluate the performance of the various models: Accuracy (Acc.) and Area Under the Receiver Operating Characteristics curve (AUROC). All the experimental results are aggregated across five random seeds, with the average results and standard deviation reported. All the models use the same set of random seeds to ensure a fair comparison.


\subsection{Implementation Details}
\paragraph{Large Multimodal Models.} 
% We use two recently introduced LMMs known for their strong generation capabilities: mPLUG-Owl \cite{ye2023mplug} and InstructBLIP \cite{dai2023instructblip}. These LMMs have demonstrated impressive overall visual perception and cognition abilities, as evidenced by their high rankings on the MME benchmark leaderboards \cite{fu2023mme}. Furthermore, the open-source code implementation of these LMMs allows for their unrestricted use in real-world applications. We first prompt the frozen pre-trained LMMs to generate the image captions, which is required for the interpretation prompt, before prompting the model to generate the meme interpretation. To facilitate reproducibility, we use greedy decoding. Additionally, to prevent lengthy and repetitive responses, we set no\_repeat\_ngram\_size = 2 and max\_new\_tokens = 256 for the additional decoding configuration.

We compare two open-source LMMs with robust multimodal reasoning capabilities: mPLUG-Owl \cite{ye2023mplug} and InstructBLIP \cite{dai2023instructblip}. These LMMs have shown impressive overall visual perception and cognition abilities, as evidenced by their high rankings on the MME benchmark leaderboards \cite{fu2023mme}. We prompt the pre-trained LMMs to generate the image captions before prompting them to generate the meme interpretation. For reproducibility, we use greedy decoding. Moreover, to minimize the occurrence of lengthy and repetitive responses, we configure the decoding settings to use no\_repeat\_ngram\_size = 2 and max\_new\_tokens = 256.

\paragraph{IntMeme Encoders.} 
The MIE module uses RoBERTa as its text encoder, while the VLA module employs FLAVA as the vision-language encoder. The RoBERTa model has shown proficiency across various language modelling tasks. The FLAVA model, trained on the hateful meme detection task during pre-training, is well-suited for modelling the complex inter- and intra-modality interactions within memes.

\paragraph{IntMeme Training.} 
We use a learning rate of 2e-5 and a batch size of 32 to fine-tune \textsf{IntMeme} on 1 A100 GPU over 30 epochs with early stopping (i.e., patience = 5)\footnote{The model typically converges within 10 epochs}. As for the selection of the models, we base our choices on the average of their Acc. and AUROC scores. We optimized these models using Adam optimizer \cite{kingma2015adam} and are implemented in PyTorch using the Huggingface's \texttt{Transformers}\footnote{https://huggingface.co/docs/transformers} library.



\begin{table*}[t!]
    % \small
    \centering
    \begin{tabular}{ccccccc}
        \toprule
         &\multicolumn{2}{c}{\textbf{FHM}}&\multicolumn{2}{c}{\textbf{MAMI}}&\multicolumn{2}{c}{\textbf{HarMeme}}\\
         \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        \textbf{Model} & \textbf{AUROC} & \textbf{Acc.}& \textbf{AUROC} & \textbf{Acc.} & \textbf{AUROC} & \textbf{Acc.}\\
        \midrule
        VisualBERT & 68.71$_{\pm1.02}$& 61.48$_{\pm1.19}$  &78.71$_{\pm0.59}$ &71.06$_{\pm0.94}$  &80.46$_{\pm1.04}$ &75.31$_{\pm1.44}$ \\
        ViLBERT & 73.05$_{\pm0.62}$&64.70$_{\pm1.12}$  &77.71$_{\pm1.20}$ &69.48$_{\pm1.00}$  &84.11$_{\pm0.88}$ &78.70$_{\pm1.17}$  \\
        MOMENTA$^*$ & 69.17$_{\pm4.71}$ & 61.34$_{\pm4.89}$  &81.68$_{\pm2.80}$ &72.10$_{\pm2.90}$   & 86.32$_{\pm3.83}$& 80.48$_{\pm1.95}$\\
        DisMultiHate & 69.11$_{\pm0.84}$& 62.42$_{\pm0.72}$ &78.21$_{\pm0.61}$ & 70.58$_{\pm1.13}$ & 83.69$_{\pm1.33}$& 78.05$_{\pm0.73}$ \\
        PromptHate & 76.76$_{\pm0.95}$&67.82$_{\pm1.23}$  &76.21$_{\pm1.05}$ &68.08$_{\pm0.58}$  &87.51$_{\pm0.74}$ & 79.38$_{\pm1.72}$ \\
        BLIP & 76.80$_{\pm2.37}$ &69.20$_{\pm1.84}$  & 80.59$_{\pm0.87}$&71.84$_{\pm1.11}$  &87.09$_{\pm1.46}$ &81.81$_{\pm1.74}$  \\
        FLAVA & 78.51$_{\pm0.70}$ & 70.28$_{\pm1.03}$ & 80.69$_{\pm0.84}$ & 71.72$_{\pm0.36}$ & 88.34$_{\pm1.15}$ & 81.58$_{\pm1.40}$ \\
        \midrule
        IntMeme$_\text{InstructBLIP}$ & 81.05$_{\pm0.81}$ & \textbf{71.48$_{\pm1.71}$} & 81.59$_{\pm0.65}$ & \textbf{72.44$_{\pm0.88}$} & 88.00$_{\pm0.84}$ & \textbf{82.66$_{\pm1.33}$} \\
        IntMeme$_\text{mPLUG-Owl}$ & \textbf{81.50$_{\pm1.11}$} & 71.52$_{\pm1.49}$ & \textbf{81.89$_{\pm1.15}$} & 72.30$_{\pm1.79}$ & \textbf{89.35}$_{\pm1.22}$ & 81.92$_{\pm2.47}$ \\
        \bottomrule
    \end{tabular}
    \caption{Evaluation results of hateful meme detection models on three benchmark datasets \textbf{without} any augmented image tags. These results have been aggregated over 5 random seeds and are reported along with their corresponding standard deviations.}
    \label{tab:experimental-results}
\end{table*}


% \begin{table*}[t!]
%     \small
%     \centering
%     \begin{tabular}{ccccccc}
%         \toprule
%          &\multicolumn{2}{c}{\textbf{FHM}}&\multicolumn{2}{c}{\textbf{MAMI}}&\multicolumn{2}{c}{\textbf{HarMeme}}\\
%          \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
%         \textbf{Model} & \textbf{AUROC} & \textbf{Acc.}& \textbf{AUROC} & \textbf{Acc.} & \textbf{AUROC} & \textbf{Acc.}\\
%         \midrule
%         VisualBERT \shortcite{li2019visualbert} & 68.71$_{\pm1.02}$& 61.48$_{\pm1.19}$  &78.71$_{\pm0.59}$ &71.06$_{\pm0.94}$  &80.46$_{\pm1.04}$ &75.31$_{\pm1.44}$ \\
%         ViLBERT \shortcite{lu2019vilbert} & 73.05$_{\pm0.62}$&64.70$_{\pm1.12}$  &77.71$_{\pm1.20}$ &69.48$_{\pm1.00}$  &84.11$_{\pm0.88}$ &78.70$_{\pm1.17}$  \\
%         MOMENTA \shortcite{pramanick2021momenta}$^*$ & 69.17$_{\pm4.71}$ & 61.34$_{\pm4.89}$  &81.68$_{\pm2.80}$ &72.10$_{\pm2.90}$   & 86.32$_{\pm3.83}$& 80.48$_{\pm1.95}$\\
%         DisMultiHate \shortcite{lee2021disentangling} & 69.11$_{\pm0.84}$& 62.42$_{\pm0.72}$ &78.21$_{\pm0.61}$ & 70.58$_{\pm1.13}$ & 83.69$_{\pm1.33}$& 78.05$_{\pm0.73}$ \\
%         PromptHate \shortcite{cao2022prompting} & 76.76$_{\pm0.95}$&67.82$_{\pm1.23}$  &76.21$_{\pm1.05}$ &68.08$_{\pm0.58}$  &87.51$_{\pm0.74}$ & 79.38$_{\pm1.72}$ \\
%         BLIP \shortcite{li2022blip} & 76.80$_{\pm2.37}$ &69.20$_{\pm1.84}$  & 80.59$_{\pm0.87}$&71.84$_{\pm1.11}$  &87.09$_{\pm1.46}$ &81.81$_{\pm1.74}$  \\
%         FLAVA \shortcite{singh2022flava} & 78.51$_{\pm0.70}$ & 70.28$_{\pm1.03}$ & 80.69$_{\pm0.84}$ & 71.72$_{\pm0.36}$ & 88.34$_{\pm1.15}$ & 81.58$_{\pm1.40}$ \\
%         \midrule
%         IntMeme$_\text{InstructBLIP}$ & 81.05$_{\pm0.81}$ & \textbf{71.48$_{\pm1.71}$} & 81.59$_{\pm0.65}$ & \textbf{72.44$_{\pm0.88}$} & 88.00$_{\pm0.84}$ & \textbf{82.66$_{\pm1.33}$} \\
%         IntMeme$_\text{mPLUG-Owl}$ & \textbf{81.50$_{\pm1.11}$} & 71.52$_{\pm1.49}$ & \textbf{81.89$_{\pm1.15}$} & 72.30$_{\pm1.79}$ & \textbf{89.35}$_{\pm1.22}$ & 81.92$_{\pm2.47}$ \\
%         \bottomrule
%     \end{tabular}
%     \caption{Evaluation results of state-of-the-art vision-language models on three benchmark datasets. These results have been aggregated over 5 random seeds and are reported along with their corresponding standard deviations. \textsuperscript{*}denotes the models that use supplementary information (i.e. image caption, object proposals and etc).}
%     \label{tab:experimental-results}
% \end{table*}


\begin{table*}[t]
  % \small
  \centering
  \begin{tabular}{lcccccc}
    \toprule
     &\multicolumn{2}{c}{\textbf{FHM}}&\multicolumn{2}{c}{\textbf{MAMI}}&\multicolumn{2}{c}{\textbf{HarMeme}} \\
     \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
    \textbf{Model} &\textbf{AUC.}&\textbf{Acc.} &\textbf{AUC.}&\textbf{Acc.} &\textbf{AUC.}&\textbf{Acc.} \\
    \midrule
    IntMeme$_\text{InstructBLIP}$ & & \\
    $-$  w/ \textsc{INTPN (MIE Module)} & 75.49$_{\pm1.46}$ & 68.64$_{\pm1.56}$ & 75.22$_{\pm1.56}$ & 66.50$_{\pm2.26}$ & 83.04$_{\pm1.96}$ & 77.12$_{\pm2.14}$  \\
    $-$  w/ \textsc{Meme (VLA Module)} & 78.51$_{\pm0.70}$ & 70.28$_{\pm1.03}$ & 80.69$_{\pm0.84}$ & 71.72$_{\pm0.36}$ & \textbf{88.34$_{\pm1.15}$} & 81.58$_{\pm1.40}$ \\
    $-$  w/ \textsc{Both (MIE + VLA Module)} & \textbf{81.05$_{\pm0.81}$} & \textbf{71.48$_{\pm1.71}$} & \textbf{81.59$_{\pm0.65}$} & \textbf{72.44$_{\pm0.88}$} & 88.00$_{\pm0.84}$ & \textbf{82.66$_{\pm1.33}$} \\
    \midrule
    IntMeme$_\text{mPLUG-Owl}$ & & \\
    $-$  w/ \textsc{INTPN (MIE Module)} & 77.26$_{\pm0.66}$ & 68.24$_{\pm2.42}$ & 77.61$_{\pm0.91}$ & 70.18$_{\pm0.72}$ & 88.74$_{\pm1.77}$ & 78.81$_{\pm2.32}$  \\
    $-$  w/ \textsc{Meme (VLA Module)} & 78.51$_{\pm0.70}$ & 70.28$_{\pm1.03}$ & 80.69$_{\pm0.84}$ & 71.72$_{\pm0.36}$ & 88.34$_{\pm1.15}$ & 81.58$_{\pm1.40}$ \\
    $-$  w/ \textsc{Both (MIE + VLA Module)} & \textbf{81.50$_{\pm1.11}$} & \textbf{71.52$_{\pm1.49}$} & \textbf{81.89$_{\pm1.15}$} & \textbf{72.30$_{\pm1.79}$} & \textbf{89.35$_{\pm1.22}$} & \textbf{81.92$_{\pm2.47}$} \\
    \midrule
    FLAVA \\
    $-$ \textsc{ Vanilla} & 78.51$_{\pm0.70}$ & 70.28$_{\pm1.03}$ & 80.69$_{\pm0.84}$ & 71.72$_{\pm0.36}$ & 88.34$_{\pm1.15}$ & 81.58$_{\pm1.40}$ \\
    $-$ w/ \textsc{INTPN}$_\text{InstructBLIP}$ \textsc{(CONCAT)} & 78.98$_{\pm0.79}$ & \textbf{70.52}$_{\pm0.87}$ & \textbf{81.23}$_{\pm1.28}$ & \textbf{71.22}$_{\pm2.59}$ & 88.63$_{\pm0.78}$ & 80.73$_{\pm2.79}$ \\
    $-$ w/ \textsc{INTPN}$_\text{mPLUG-Owl}$ \textsc{(CONCAT)} & \textbf{79.45}$_{\pm0.85}$ & 70.44$_{\pm1.58}$ & 81.20$_{\pm1.03}$  & 70.84$_{\pm2.22}$ & \textbf{89.10}$_{\pm1.16}$ & \textbf{81.53}$_{\pm2.32}$  \\
    \bottomrule
\end{tabular}
\caption{Ablation study w.r.t \textsf{IntMeme} and its distinct modules. The top scores across the variations are highlighted in \textbf{bold}.}
\label{tab:ablation-modules}
\end{table*}
