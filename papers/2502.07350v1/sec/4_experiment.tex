\section{Experiments}

In this section, we detail the experimental setup, present the main results, and provide an in-depth analysis of KABB.

\subsection{Experimental Setup}
\label{sec:setup}
\textbf{Models.} To construct the default configuration of KABB, we use 6 open-source models\footnote{Inference was conducted using the Together Inference Endpoint: \href{https://api.together.ai/playground/chat}{https://api.together.ai/playground/chat}.} 
% to ensure high performance while maintaining accessibility. The selected models 
% include
including Qwen2-72B-Instruct \cite{bai2023qwen}, LLaMa-3-70B-Instruct \cite{adams2024llama}, WizardLM-2-8x22B \cite{xu2024wizardlm}, Gemma-2-27B \cite{team2024gemma}, Deepseek-V3 \cite{liu2024deepseek}, and Deepseek-R1 \cite{guo2025deepseek}. 12 knowledge concepts and 24 experts are defined, and the models are evenly distributed across these experts using tailored prompts to specialize their expertise, resulting in a straightforward yet effective multi-agent system. By default, the system dynamically routes queries to top-3 experts from top-2 knowledge concepts. Following the insights from MoA \cite{wang2024mixture}, we designated Qwen2-72B-Instruct as the aggregator. Two variants are also developed: KABB w/o Deepseek, which excludes the Deepseek-V3 and Deepseek-R1 models from the system, and KABB-Single-LLaMa3, which employs only LLaMa-3-70B-Instruct as both the experts and the aggregator.

\textbf{Benchmarks.} The evaluation mainly uses AlpacaEval 2.0 \cite{dubois2024length} with 805 instructions that reflect real-world cases. The model outputs are directly compared to those of the GPT-4 Preview (11/06), with a GPT-4-based evaluator determining the preference probabilities. The length-controlled (LC) win rate is adopted to eliminate potential length biases\footnote{This metric closely approximates human judgment, boasting a Spearman correlation of 0.98 when compared to actual human evaluations \cite{dubois2024length}.}. We also assess performance on MT-Bench \cite{zheng2023judging} and FLASK-Hard \cite{ye2023flask}. FLASK-Hard, the 89 most difficult instances in FLASK, provides a detailed evaluation of 12 skill-specific categories. 
For reasoning and problem-solving tasks, the results on Arena-Hard, MATH, and BBH are reported in Appendix \ref{app:reasoning}.

\subsection{Main Results}

We analyze the performance of KABB and its variants across AlpacaEval 2.0, MT-Bench, and FLAS-Hard. A detailed comparison with baseline models and their ablations provides insights into its effectiveness and robustness.

\textbf{AlpacaEval 2.0} focuses on measuring alignment with human preferences. The results, as shown in \cref{tab:model_comparison}, highlight that KABB achieves a leading LC win rate of 77.9\%, marking a 9.8\% improvement over MoA under the same configuration. It is noteworthy that KABB selects only 2 experts to respond to instruction, while MoA requires 6 proposers, which shows the cost efficiency of KABB. Although KABB does not surpass Deepseek-R1 (80.1\%), this is expected, as not all responses in the system involve Deepseek contributions. Importantly, KABB w/o Deepseek outperforms both the open-source models inside the system and proprietary models including GPT-4 Omni. Similarly, KABB-Single-LLaMa3 surpasses LLaMa-3-70B-Instruct, illustrating that collaboration and specialization in KABB enhance overall performance. 
These results confirm that its ability to dynamically route queries to specialized experts and aggregate their responses effectively contributes to this strong alignment.

\textbf{MT-Bench.} KABB achieves a state-of-the-art average score of 9.60, maintaining top-tier performance in multi-turn dialogue. KABB w/o Deepseek (9.47) exceeds GPT-4 Turbo (9.31). While individual models already perform exceptionally well on this benchmark, KABB’s collaborative design with dynamic expert routing secures a leadership position, reinforcing its robustness in multi-turn interactions.

\textbf{FLASK-Hard.} KABB demonstrates strong performance in 12 skill-specific metrics (see \cref{falsk-hard}), surpassing or matching MoA and GPT-4 in two-thirds of the categories, particularly robustness, correctness, common sense, insight, metacognition, and readability. Notably, KABB outperforms MoA in metacognition, reflecting its ability to reason and adapt effectively. However, KABB lags slightly in conciseness, producing more detailed outputs. This trade-off highlights KABB's emphasis on thoroughness over brevity.

\begin{figure}[h]
% \vskip -0.15in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{fig/flask-hard-moa.pdf}}
\caption{Results on FLASK-Hard where we use the default KABB setup with 6 models and Qwen2-70B-Instruct as the aggregator. We include the results of GPT-4, Qwen2-72B-Instruct, and MoA with the same 6 proposers and aggregator for comparison.}
\label{falsk-hard}
\end{center}
\vskip -0.3in
\end{figure}

\begin{table*}[t]
    \centering
    \small
    \sisetup{
        table-format=2.1,
        separate-uncertainty=true,
        detect-weight=true,
        detect-inline-weight=math
    }
    \renewcommand{\arraystretch}{1.2} % 增加行间距
    \setlength{\tabcolsep}{12pt} % 增加列间距
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
         & \multicolumn{2}{c}{AlpacaEval 2.0} & \multicolumn{3}{c}{MT-Bench} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-6}
        Model & \textbf{LC win. (\%)} & \textbf{win. (\%)} & \textbf{Avg.} & \textbf{1st turn} & \textbf{2nd turn} \\
        \midrule
        \rowcolor[gray]{0.95} KABB & \underline{77.9} & \underline{72.3} & \textbf{9.65} & \textbf{9.85} & \textbf{9.45} \\
        MoA & 68.1 & 65.4 & 9.41 & 9.53 & 9.29 \\
        \rowcolor[gray]{0.95} KABB w/o Deepseek & 62.4 & 66.7 & 9.47 & 9.58 & 9.35 \\
        GPT-4 Omni (05/13) & 57.5 & 51.3 & 9.19 & 9.31 & 9.07 \\
        GPT-4 Turbo (04/09) & 55.0 & 46.1 & 9.31 & 9.35 & 9.28 \\
        GPT-4 Preview (11/06) & 50.0 & 50.0 & 9.20 & 9.38 & 9.03 \\
        GPT-4 (03/14) & 35.3 & 36.1 & 8.84 & 9.08 & 8.61 \\
        Qwen2-72B-Instruct & 38.1 & 29.9 & 9.15 & 9.25 & 9.05 \\
        Gemma-2-27B & 44.9 & 33.2 & 9.09 & 9.23 & 8.95 \\
        WizardLM-2-8x22B & 51.3 & 62.3 & 8.78 & 8.96 & 8.61 \\
        \rowcolor[gray]{0.95} KABB-Single-LLaMa3 & 34.7 & 36.2 & 9.16 & 9.10 & 9.23 \\
        LLaMa-3-70B-Instruct & 34.4 & 33.2 & 8.94 & 9.20 & 8.68 \\
        Deepseek-V3 & 67.2 & 69.3 & 9.51 & 9.59 & 9.42 \\
        Deepseek-R1 & \textbf{80.1} & \textbf{75.4} & 9.30 & 9.40 & 9.20 \\
        \bottomrule
    \end{tabular}
    }
    \caption{Comparison of different models on AlpacaEval 2.0 and MT-Bench. MoA (with 2 layers) shares the same model configuration with KABB, where 6 different proposers are in the first layer and 1 aggregator in the second. For AlpacaEval 2.0, the performance of GPT-4 variants, LLaMa-3-70B-Instruct, and Qwen2-72B-Instruct on AlpacaEval 2.0 are sourced from public leaderboards; WizardLM-2-8x22B results come from \cite{wang2024mixture}. We reproduced results for Deepseek-V3, Deepseek-R1, and Gemma-2-27B on AlpacaEval 2.0. For MT-Bench, we conducted evaluations to obtain turn-based scores, except for the results of GPT-4 variants, LLaMa-3-70B-Instruct, and WizardLM-2-8x22B, which are from \cite{wang2024mixture}.}
    \label{tab:model_comparison}
\end{table*}
\subsection{WHAT MAKES KABB EFFECTIVE?}
% In this subsection, we uncover the factors that contribute to the superior performance of KABB.
% Key insights are summarized below.
% \textbf{Comparison with Different Methods.} 
We analyze KABB's effectiveness by comparing different routing strategies. 

We replaced our Knowledge-Aware (KA) routing mechanism with a classifier-based routing (CL) approach. To be specific, We replaced our Knowledge-Aware (KA) routing mechanism with a classifier-based routing (CL) approach. The CL mechanism uses Sentence-BERT to encode both the instruction and the expert’s knowledge concept into vector representations. Cosine similarity is then calculated between these vectors, and the expert with the highest similarity score is selected. 

Several optimization algorithms including PPO \cite{schulman2017proximalpolicyoptimizationalgorithms}, MCTS \cite{_wiechowski_2022}, and A2C \cite{DBLP:journals/corr/MnihBMGLHSK16} are also compared with our MAB algorithms. 

For a more nuanced evaluation that considers both the human preference for routing decisions and the relative performance advantage of the chosen experts, we introduce two new metrics: Routing Alignment Score (RAS) for human annotation consistency and Preference-Weighted Routing Score (PWRS) incorporating output quality with human preference. Detailed definitions are provided in \cref{app:comparison}.
As shown in Table \ref{tab:method_comparison}, the KA mechanism with MAB achieves the best overall performance, demonstrating strong alignment with human preferences and expert output quality. Among optimization methods, MAB consistently outperforms PPO, MCTS, and A2C, underscoring its effectiveness in balancing exploration and exploitation. KA with MAB also outperforms CL by a notable margin. This demonstrates that incorporating knowledge-awareness is critical for achieving optimal alignment with human preferences and expert output quality.
\begin{table}[t]
    \centering
    % \small
    \begin{tabular}{lccc}
        \toprule
        Method & LC win. & RAS & PWRS \\
        \midrule
        \textbf{KA (MAB) (Ours)} & \textbf{62.4} & \textbf{94.16} & \textbf{60.19} \\
        CL (MAB) & 60.9 & 92.92 & 57.34 \\
        KA (A2C) & 60.2 & 91.61 & 54.38 \\
        KA (PPO) & 57.3 & 90.43 & 56.07 \\
        KA (MCTS) & 54.8 & 87.95 & 51.74 \\
        \bottomrule
    \end{tabular}
    \caption{Comparison of different methods on LC win rate of AlpacaEval 2.0, RAS, and PWRS metrics. All experiments were conducted on AlpacaEval 2.0. The system dynamically routes queries to the top-2 experts derived from the top-2 knowledge concepts. All model configurations align with the KABB w/o Deepseek (see \cref{sec:setup}).}
    \label{tab:method_comparison}
\end{table}
\subsection{Budget and Consumption Analysis}
\begin{figure}[h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{fig/cost5.pdf}}
\caption{Performance trade-off versus cost. Our experiments use default configurations to evaluate KABB’s average cost per instruction on AlpacaEval 2.0, calculated from expert routing statistics and public API pricing\protect\footnotemark. By routing instructions to specific experts rather than all models, KABB effectively lowers costs. For instance, even with expensive models like DeepSeek-R1, unsuitable instructions are directed to cheaper experts, optimizing both cost and performance.}
\label{cost}
\end{center}
% \vskip -0.2in
\end{figure}

\textbf{Cost Effectiveness.} In Figure 4, we plot the LC Win Rate of KABB and several baseline models on AlpacaEval 2.0 against their inference costs. The chart shows the trade-off between cost and performance across models. Our plots depict a Pareto frontier that optimally balances performance and cost. We demonstrate that the KABB systems are positioned along or close to this frontier. Our experiment illustrates that KABB, by dynamically adjusting the number of experts, is significantly more cost-effective than other models. Compared to GPT-4o, GPT-4 Turbo, and GPT-4 (11/06) Preview, KABB achieves higher LC Win Rates at lower costs. With 3, 5, or 6 experts, KABB performs similarly to DeepSeek-R1, and with 6 experts, it achieves the highest LC Win Rate at the lowest cost in that tier. For cost-sensitive scenarios, KABB with fewer experts offers better quality than GPT-4o at lower prices. With just one expert, KABB improves LC Win Rate by about approximately 10\% over GPT-4o at half the cost. Compared to the previous MoA model, KABB provides a much better cost-performance balance, requiring only 1/7 of the cost to achieve a similar LC Win Rate. 
\footnotetext{For open source models, the price information is from \href{https://www.together.ai/pricing}{https://www.together.ai/pricing}; for GPT-4 models, we use \href{https://openai.com/api/pricing/}{https://openai.com/api/pricing/} as price details. API prices are obtained on January 20, 2025.}
% \cref{cost}

\begin{figure}[h]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\linewidth]{fig/flops2.pdf}}
\caption{The trade-off between performance and computational cost (average TFLOPS, also used as a proxy for latency). The actual tflops of GPT-4 are unknown, so we use the rumored size from the community of an 8x220B architecture. The precise TFLOPS for GPT-4 remains undisclosed; therefore, we estimate it based on community speculation suggesting an 8x220B architecture.}
\label{tflops}
\end{center}
% \vskip -0.2in
\end{figure}

\textbf{Tflops Consumption.} \cref{tflops} shows that KABB excels at maintaining high performance while keeping computational demands relatively low, even as the model scales with more experts or larger architectures. Unlike MoA models, which encounter diminishing scalability due to increased TFLOPS, KABB demonstrates efficient resource utilization. This highlights the scalability and cost-effectiveness of our approach relative to alternative architectures. Additionally, by using TFLOPS as an approximate indicator of latency, we highlight the efficiency of our approach. While inference endpoint latency isn't solely determined by TFLOPS -- factors like batching strategies and server load also play a role -- we leverage TFLOPS as a reasonable proxy for gauging the inherent computational burden of each model. It provides a valuable, albeit theoretical, measure of the resources a model demands, allowing for a relative comparison of computational intensity between different architectures.



