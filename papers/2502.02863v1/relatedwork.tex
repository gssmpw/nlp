\section{Related Work}
As sustainability challenges become increasingly urgent, researchers have turned to interactive AI systems—especially those powered by large language models (LLMs)—as potentially powerful drivers of pro-environmental behavior. Below, we situate our work within two key areas of prior research: the emerging role of LLMs in persuasive interactions for (pro-environmental) behavior change and methodological considerations that incorporate both real and synthetic participants.

\subsection{LLMs and Persuasive Interactions for Sustainability}
The rapid growth of large language models (LLMs) has not only transformed how individuals interact with AI—exemplified by platforms like OpenAI’s ChatGPT—but has also opened new avenues for influencing environmental knowledge, attitudes, and behaviors. By leveraging principles from behavioral science and persuasive design, these conversational systems can adapt messages in real-time to address individual misconceptions and motivations through personalized interactions \cite{Fogg2003, Giudici2024, Giudici2024b}. In parallel, advancements in behavioral psychology have revealed promising strategies for fostering sustainable behavior. Psychological methods such as leveraging dynamic social norms, goal setting, and emotionally resonant storytelling have demonstrated the potential to motivate individuals towards adopting pro-environmental habits \cite{Breiter2024, Vlasceanu2024, Gifford2011a, Aavik2022UsingChoices}. When integrated into AI-driven interfaces, these approaches can be deployed at scale to offer tailored suggestions that strengthen users’ sense of self-efficacy and commitment to environmental goals, thereby contributing to broader sustainability efforts.

Empirical research increasingly demonstrates these systems’ ability to shift beliefs across diverse contexts. For instance, LLM-powered agents have been shown to counteract misinformation and conspiracy theories, guide users toward healthier behaviors, and enhance education effectiveness \cite{Wang2023, Aggarwal2023, Costello2024}. Through iterative dialogue, chatbots can tailor feedback to users’ specific barriers—such as time constraints or skepticism—thereby enhancing both the persuasiveness and relevance of pro-environmental messages \cite{Aggarwal2023, Giudici2024b}. 

Building on these insights, \textit{OceanChat} explores how LLM-driven conversations might create richer, more empathic connections with marine ecosystems than static or one-way interventions. While prior studies confirm the efficacy of conversational agents for changing attitudes, less is known about how fully \textit{embodied}, character-based narratives might deepen engagement with—and commitment to—sustainability goals.

Recent experiments underscore the importance of multi-strategy approaches. For instance, Costello et al. found that short, tailored interventions generated through GPT-4 Turbo significantly diminished conspiracy-theory beliefs over time, illustrating how fact-based appeals can complement empathy-building dialogues \cite{Costello2024}. Meanwhile, Giudici’s chatbot delivering energy-saving tips combined multiple persuasive cues (e.g., social comparisons, immediate feedback) to improve user engagement, though the translation to actual behavior varied \cite{Giudici2024}.

Taken together, these findings suggest that effective sustainability interventions often blend factual content with emotional resonance and interactive affordances. In the marine conservation domain, such hybrid approaches could be particularly beneficial, as users’ emotional connection to non-human life forms is a key driver of behavior change. With \textit{OceanChat}, we extend this line of inquiry by using first-person, AI-generated marine characters to evoke empathy, frame scientific consensus on climate impacts, and deliver context-aware guidance on sustainability practices.

\subsection{Methodological Considerations: Real and Synthetic Participants}
Designing and testing novel AI interventions at scale raises unique methodological questions. While human-subject experiments remain the gold standard for measuring authentic behavioral and attitudinal change, LLM-based synthetic pilot studies have emerged as a complementary technique \cite{SyntheticStudy2023, Long2024}. By simulating participants from different demographic backgrounds, researchers can pre-test interventions for potential flaws, refine prompts for clarity, and evaluate initial user flows before deploying a system to a larger pool of human participants.

We adopt this dual approach, integrating a synthetic pilot study with our eventual human-subject trial. In doing so, we first tested \textit{OceanChat}’s conversational prompts, ensuring the characters’ personalities, narrative tone, and factual accuracy met our design goals. Synthetic participants flagged potential pitfalls—such as confusing transitions or underexplained technical terms—that might reduce the intervention’s effectiveness. After refining these elements, we proceeded to test the system with real participants, allowing us to focus on genuine emotional engagement and reported behavior change rather than debugging user-flow issues.

By combining LLM-based research techniques, best practices in persuasive design, and real-user testing, \textit{OceanChat} investigates how conversational AI agents can drive deeper, more lasting pro-environmental attitudes and behaviors. This dual-method strategy ensures robust study design while capitalizing on the rapid iteration possible through LLM-driven synthetic pilots.