
@phdthesis{Kylasa,
  title={Higher order optimization techniques for machine learning},
  author={Kylasa, Sudhir B},
  year={2019},
  school={Purdue University}
}

@article{kant1908critique,
  title={Critique of pure reason. 1781},
  author={Kant, Immanuel},
  journal={Modern Classical Philosophers, Cambridge, MA: Houghton Mifflin},
  pages={370--456},
  year={1908}
}


@article{henrion2009gloptipoly,
  title={GloptiPoly 3: moments, optimization and semidefinite programming},
  author={Henrion, Didier and Lasserre, Jean-Bernard and L{\"o}fberg, Johan},
  journal={Optimization Methods \& Software},
  volume={24},
  number={4-5},
  pages={761--779},
  year={2009},
  publisher={Taylor \& Francis}
}


@article{papachristodoulou2013sostools,
  title={SOSTOOLS version 4.00 sum of squares optimization toolbox for MATLAB},
  author={Papachristodoulou, Antonis and Anderson, James and Valmorbida, Giorgio and Prajna, Stephen and Seiler, Pete and Parrilo, Pablo and Peet, Matthew M and Jagt, Declan},
  journal={arXiv preprint arXiv:1310.4716},
  year={2013}
}

@article{muennighoff2025s1,
  title={s1: Simple test-time scaling},
  author={Muennighoff, Niklas and Yang, Zitong and Shi, Weijia and Li, Xiang Lisa and Fei-Fei, Li and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Liang, Percy and Cand{\`e}s, Emmanuel and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2501.19393},
  year={2025}
}



@article{ahmadi2023sums,
  title={Sums of separable and quadratic polynomials},
  author={Ahmadi, Amir Ali and Dibek, Cemil and Hall, Georgina},
  journal={Mathematics of Operations Research},
  volume={48},
  number={3},
  pages={1316--1343},
  year={2023},
  publisher={INFORMS}
}


@article{gvozdenovic2007semidefinite,
  title={Semidefinite bounds for the stability number of a graph via sums of squares of polynomials},
  author={Gvozdenovi{\'c}, Neboj{\v{s}}a and Laurent, Monique},
  journal={Mathematical Programming},
  volume={110},
  pages={145--173},
  year={2007},
  publisher={Springer}
}

@article{doherty2002distinguishing,
  title={Distinguishing separable and entangled states},
  author={Doherty, Andrew C and Parrilo, Pablo A and Spedalieri, Federico M},
  journal={Physical Review Letters},
  volume={88},
  number={18},
  pages={187904},
  year={2002},
  publisher={APS}
}

@article{alfarano2024global,
  title={Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers},
  author={Alfarano, Alberto and Charton, Fran{\c{c}}ois and Hayat, Amaury},
  journal={arXiv preprint arXiv:2410.08304},
  year={2024}
}

@article{motzkin1967arithmetic,
  title={The arithmetic-geometric inequality},
  author={Motzkin, Theodore Samuel},
  journal={Inequalities (Proc. Sympos. Wright-Patterson Air Force Base, Ohio, 1965)},
  volume={205},
  pages={54},
  year={1967}
}


@article{ahmadi2024convex,
  title={Convex ternary quartics are sos-convex},
  author={Ahmadi, Amir Ali and Blekherman, Grigoriy and Parrilo, Pablo A},
  journal={arXiv preprint arXiv:2404.14440},
  year={2024}
}

@article{hilbert1893ternare,
  title={{\"U}ber tern{\"a}re definite Formen},
  author={Hilbert, David},
  journal={Journal},
  year={1893}
}



@article{parrilo2001minimizing,
  title={Minimizing polynomial functions},
  author={Parrilo, Pablo A and Sturmfels, Bernd},
  journal={arXiv preprint math/0103170},
  year={2001}
}


@article{helton2010semidefinite,
  title={Semidefinite representation of convex sets},
  author={Helton, J William and Nie, Jiawang},
  journal={Mathematical Programming},
  volume={122},
  pages={21--64},
  year={2010},
  publisher={Springer}
}

@article{schweighofer2005optimization,
  title={Optimization of polynomials on compact semialgebraic sets},
  author={Schweighofer, Markus},
  journal={SIAM Journal on Optimization},
  volume={15},
  number={3},
  pages={805--825},
  year={2005},
  publisher={SIAM}
}

@article{lasserre2007sum,
  title={A sum of squares approximation of nonnegative polynomials},
  author={Lasserre, Jean B},
  journal={SIAM review},
  volume={49},
  number={4},
  pages={651--669},
  year={2007},
  publisher={SIAM}
}

@article{nie2013certifying,
  title={Certifying convergence of Lasserre’s hierarchy via flat truncation},
  author={Nie, Jiawang},
  journal={Mathematical Programming},
  volume={142},
  pages={485--510},
  year={2013},
  publisher={Springer}
}

@article{nie2013exact,
  title={An exact Jacobian SDP relaxation for polynomial optimization},
  author={Nie, Jiawang},
  journal={Mathematical Programming},
  volume={137},
  pages={225--255},
  year={2013},
  publisher={Springer}
}

@incollection{reznick2000some,
  author    = {Reznick, Bruce},
  title     = {Some Concrete Aspects of Hilbert's 17th Problem},
  booktitle = {Contemporary Mathematics},
  volume    = {253},
  pages     = {251--272},
  publisher = {American Mathematical Society},
  year      = {2000}
}


@article{lasserre2006convergent,
  title={Convergent SDP-relaxations in polynomial optimization with sparsity},
  author={Lasserre, Jean-Bernard},
  journal={SIAM Journal on Optimization},
  volume={17},
  number={3},
  pages={822--843},
  year={2006},
  publisher={SIAM}
}

@article{nie2014optimality,
  title={Optimality conditions and finite convergence of Lasserre’s hierarchy},
  author={Nie, Jiawang},
  journal={Mathematical programming},
  volume={146},
  pages={97--121},
  year={2014},
  publisher={Springer}
}

@article{molzahn2015sparsity,
  title={Sparsity-Exploiting Moment-Based Relaxations of the Optimal Power Flow Problem},
  author={Molzahn, Daniel K and Hiskens, Ian K},
  journal={IEEE Transactions on Power Systems},
  volume={30},
  year={2015},
  pages={3168--3180}
}


@incollection{camps2017interplay,
  author    = {Camps, Octavia and Sznaier, Mario},
  title     = {The Interplay between Big-data and Sparsity in Systems Identification},
  booktitle = {Geometric and Numerical Foundations of Movements},
  editor    = {Laumond, Jean-Paul and Mansard, Nicolas and Lasserre, Jean-Bernard},
  publisher = {Springer},
  year      = {2017},
  pages     = {133--159},
  series    = {Springer Tracts in Advanced Robotics},
  number    = {117},
  address   = {New York}
}


@article{lasserre2000optimisation,
  title={Optimisation globale et th{\'e}orie des moments},
  author={Lasserre, Jean Bernard},
  journal={Comptes Rendus de l'Acad{\'e}mie des Sciences-Series I-Mathematics},
  volume={331},
  number={11},
  pages={929--934},
  year={2000},
  publisher={Elsevier}
}

@book{
,
  title={Moments, positive polynomials and their applications},
  author={Lasserre, Jean Bernard},
  volume={1},
  year={2009},
  publisher={World Scientific}
}

@article{scheiderer2006distinguished,
  title={Non-existence of degree bounds for weighted sums of squares representations},
  author={Scheiderer, Claus},
  journal={Journal of Complexity},
  year={2006},
}


@article{ahmadi2013np,
  title={NP-hardness of deciding convexity of quartic polynomials and related problems},
  author={Ahmadi, Amir Ali and Olshevsky, Alex and Parrilo, Pablo A and Tsitsiklis, John N},
  journal={Mathematical programming},
  volume={137},
  pages={453--476},
  year={2013},
  publisher={Springer}
}

@inproceedings{berg1987multidimensional,
  title={The multidimensional moment problem and semigroups},
  author={Berg, Christian},
  booktitle={Proc. Symp. Appl. Math},
  volume={37},
  pages={110--124},
  year={1987}
}

@inproceedings{lasserre2018moment,
  title={The moment-SOS hierarchy},
  author={Lasserre, Jean B},
  booktitle={Proceedings of the International Congress of Mathematicians: Rio de Janeiro 2018},
  pages={3773--3794},
  year={2018},
  organization={World Scientific}
}


@article{lasserre2008representation,
  title={Representation of nonnegative convex polynomials},
  author={Lasserre, Jean B},
  journal={Archiv der Mathematik},
  volume={91},
  number={2},
  pages={126--130},
  year={2008},
  publisher={Springer}
}

@article{henrion2003gloptipoly,
  title={GloptiPoly: Global optimization over polynomials with MATLAB and SeDuMi},
  author={Henrion, Didier and Lasserre, Jean-Bernard},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={29},
  number={2},
  pages={165--194},
  year={2003},
  publisher={ACM}
}

@incollection{henrion2005detecting,
  title={Detecting global optimality and extracting solutions in GloptiPoly},
  author={Henrion, Didier and Lasserre, Jean-Bernard},
  booktitle={Positive polynomials in control},
  pages={293--310},
  year={2005},
  publisher={Springer},
  series={Lecture Notes in Control and Information Sciences},
  volume={312},
  address={Berlin}
}


@book{lasserre2009moments,
  title={Moments, positive polynomials and their applications},
  author={Lasserre, Jean Bernard},
  volume={1},
  year={2009},
  publisher={World Scientific}
}

@article{lasserre2009convexity,
  title={Convexity in semialgebraic geometry and polynomial optimization},
  author={Lasserre, Jean B},
  journal={SIAM Journal on Optimization},
  volume={19},
  number={4},
  pages={1995--2014},
  year={2009},
  publisher={SIAM}
}

@book{kojima2003sums,
  title={Sums of squares relaxations of polynomial semidefinite programs},
  author={Kojima, Masakazu},
  year={2003},
  publisher={Inst. of Technology}
}

@article{martinez2017cubic,
  title={Cubic-regularization counterpart of a variable-norm trust-region method for unconstrained minimization},
  author={Mart{\'\i}nez, Jos{\'e} Mario and Raydan, Marcos},
  journal={Journal of Global Optimization},
  volume={68},
  pages={367--385},
  year={2017},
  publisher={Springer}
}

@article{de2000multilinear,
  title={A multilinear singular value decomposition},
  author={De Lathauwer, Lieven and De Moor, Bart and Vandewalle, Joos},
  journal={SIAM journal on Matrix Analysis and Applications},
  volume={21},
  number={4},
  pages={1253--1278},
  year={2000},
  publisher={SIAM}
}

@article{zhu2023cubic,
  title={Cubic-quartic regularization models for solving polynomial subproblems in third-order tensor methods},
  author={Zhu, Wenqi and Cartis, Coralia},
  journal={arXiv preprint arXiv:2312.10283},
  year={2023}
}

@inproceedings{de1996independent,
  title={Independent component analysis based on higher-order statistics only},
  author={De Lathauwer, Lieven and De Moor, Bart and Vandewalle, Joos},
  booktitle={Proceedings of 8th Workshop on Statistical Signal and Array Processing},
  pages={356--359},
  year={1996},
  organization={IEEE}
}

@book{nocedal1999numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen J},
  year={1999},
  publisher={Springer}
}

@article{de2000best,
  title={On the best rank-1 and rank-(r 1, r 2,..., rn) approximation of higher-order tensors},
  author={De Lathauwer, Lieven and De Moor, Bart and Vandewalle, Joos},
  journal={SIAM journal on Matrix Analysis and Applications},
  volume={21},
  number={4},
  pages={1324--1342},
  year={2000},
  publisher={SIAM}
}


@article{vandenberghe1996semidefinite,
  title={Semidefinite programming},
  author={Vandenberghe, Lieven and Boyd, Stephen},
  journal={SIAM review},
  volume={38},
  number={1},
  pages={49--95},
  year={1996},
  publisher={SIAM}
}

@inproceedings{zhu2022quartic,
  title={Quartic Polynomial Sub-problem Solutions in Tensor Methods for Nonconvex Optimization},
  author={Zhu, Wenqi and Cartis, Coralia},
  booktitle={NeurIPS 2022 Workshop},
  year={2022}
}


@article{cartis2023second,
  title={Second-order methods for quartically-regularised cubic polynomials, with applications to high-order tensor methods},
  author={Cartis, Coralia and Zhu, Wenqi},
  journal={arXiv preprint arXiv:2308.15336},
  year={2023}
}

@article{ahmadi2022complexity,
  title={On the complexity of finding a local minimizer of a quadratic function over a polytope},
  author={Ahmadi, Amir Ali and Zhang, Jeffrey},
  journal={Mathematical Programming},
  volume={195},
  number={1-2},
  pages={783--792},
  year={2022},
  publisher={Springer}
}

@article{murty1987some,
  title={Some NP-complete problems in quadratic and nonlinear programming},
  author={Murty, Katta G and Kabadi, Santosh N},
  journal={Mathematical Programming: Series A and B},
  volume={39},
  number={2},
  pages={117--129},
  year={1987},
  publisher={Springer-Verlag Berlin, Heidelberg}
}

@article{gould2012updating,
    author = {Gould, Nicholas IM and Porcelli, Margherita and Toint, Philippe L},
    doi = {10.1007/s10589-011-9446-7},
    journal = {Computational optimization and applications},
    pages = {1--22},
    publisher = {Springer},
    title = {{Updating the regularization parameter in the adaptive cubic regularization algorithm}},
    volume = {53},
    year = {2012}
}

@article{cartis2011adaptive,
  title={Adaptive cubic regularisation methods for unconstrained optimization. Part I: motivation, convergence, and numerical results},
  author={Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
  journal={Mathematical Programming},
  volume={127},
  number={2},
  pages={245--295},
  year={2011},
  publisher={Springer}
}

@article{martinez2015separable,
  title={Separable cubic modeling and a trust-region strategy for unconstrained minimization with impact in global optimization},
  author={Mart{\'\i}nez, Jos{\'e} Mario and Raydan, Marcos},
  journal={Journal of Global Optimization},
  volume={63},
  number={2},
  pages={319--342},
  year={2015},
  publisher={Springer}
}

@article{wang2009practical,
  title={A practical method for computing the largest M-eigenvalue of a fourth-order partially symmetric tensor},
  author={Wang, Yiju and Qi, Liqun and Zhang, Xinzhen},
  journal={Numerical Linear Algebra with Applications},
  volume={16},
  number={7},
  pages={589--601},
  year={2009},
  publisher={Wiley Online Library}
}

@book{horst2013global,
  title={Global optimization: Deterministic approaches},
  author={Horst, Reiner and Tuy, Hoang},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{neumaier2004complete,
  title={Complete search in continuous global optimization and constraint satisfaction},
  author={Neumaier, Arnold},
  journal={Acta numerica},
  volume={13},
  pages={271--369},
  year={2004},
  publisher={Cambridge University Press}
}

@article{kvasov2009univariate,
  title={A univariate global search working with a set of Lipschitz constants for the first derivative},
  author={Kvasov, Dmitri E and Sergeyev, Yaroslav D},
  journal={Optimization Letters},
  volume={3},
  number={2},
  pages={303--318},
  year={2009},
  publisher={Springer}
}

@article{qi2019spectral,
  title={Spectral norm and nuclear norm of a third order tensor},
  author={Qi, Liqun and Hu, Shenglong},
  journal={arXiv preprint arXiv:1909.01529},
  year={2019}
}

@book{nesterov1994interior,
  title={Interior-point polynomial algorithms in convex programming},
  author={Nesterov, Yurii and Nemirovskii, Arkadii},
  year={1994},
  publisher={SIAM}
}

@book{lasserre2015introduction,
  title={An introduction to polynomial and semi-algebraic optimization},
  author={Lasserre, Jean Bernard},
  volume={52},
  year={2015},
  publisher={Cambridge University Press}
}

@article{lasserre2001global,
  title={Global optimization with polynomials and the problem of moments},
  author={Lasserre, Jean B},
  journal={SIAM Journal on optimization},
  volume={11},
  number={3},
  pages={796--817},
  year={2001},
  publisher={SIAM}
}

@article{laurent2009sums,
  title={Sums of squares, moment matrices and optimization over polynomials},
  author={Laurent, Monique},
  journal={Emerging applications of algebraic geometry},
  pages={157--270},
  year={2009},
  publisher={Springer}
}

@book{conn2000trust,
  title={Trust region methods},
  author={Conn, Andrew R and Gould, Nicholas IM and Toint, Philippe L},
  year={2000},
  publisher={SIAM}
}


@article{chen2018semidefinite,
  title={A semidefinite program approach for computing the maximum eigenvalue of a class of structured tensors and its applications in hypergraphs and copositivity test},
  author={Chen, Haibin and Chen, Yannan and Li, Guoyin and Qi, Liqun},
  journal={Numerical Linear Algebra with Applications},
  volume={25},
  number={1},
  pages={e2125},
  year={2018},
  publisher={Wiley Online Library}
}

@inproceedings{doikov2020inexact,
  title={Inexact Tensor Methods with Dynamic Accuracies.},
  author={Doikov, Nikita and Nesterov, Yurii E},
  booktitle={ICML},
  pages={2577--2586},
  year={2020}
}


@article{dussault2018arcq,
  title={ARCq: a new adaptive regularization by cubics},
  author={Dussault, Jean-Pierre},
  journal={Optimization Methods and Software},
  volume={33},
  number={2},
  pages={322--335},
  year={2018},
  publisher={Taylor \& Francis}
}




@article{dvurechensky2019near,
  title={Near-optimal tensor methods for minimizing the gradient norm of convex function},
  author={Dvurechensky, Pavel and Gasnikov, Alexander and Ostroukhov, Petr and Uribe, C{\'e}sar A and Ivanova, Anastasiya},
  journal={arXiv preprint arXiv:1912.03381},
  year={2019}
}


@techreport{nesterov2003random,
  title={Random walk in a simplex and quadratic optimization over convex polytopes},
  author={Nesterov, Yurii and others},
  year={2003},
  institution={CORE}
}

@article{luo2010semidefinite,
  title={A semidefinite relaxation scheme for multivariate quartic polynomial optimization with quadratic constraints},
  author={Luo, Zhi-Quan and Zhang, Shuzhong},
  journal={SIAM Journal on Optimization},
  volume={20},
  number={4},
  pages={1716--1736},
  year={2010},
  publisher={SIAM}
}



@article{arikan2020steklov,
  title={Steklov regularization and trajectory methods for univariate global optimization},
  author={Ar{\i}kan, Orhan and Burachik, Regina S and Kaya, C Yal{\c{c}}{\i}n},
  journal={Journal of Global Optimization},
  volume={76},
  number={1},
  pages={91--120},
  year={2020},
  publisher={Springer}
}

@article{burachik2021steklov,
  title={Steklov convexification and a trajectory method for global optimization of multivariate quartic polynomials},
  author={Burachik, Regina S and Kaya, C Yal{\c{c}}{\i}n},
  journal={Mathematical Programming},
  volume={189},
  number={1},
  pages={187--216},
  year={2021},
  publisher={Springer}
}

@article{parrilo2003semidefinite,
  title={Semidefinite programming relaxations for semialgebraic problems},
  author={Parrilo, Pablo A},
  journal={Mathematical programming},
  volume={96},
  pages={293--320},
  year={2003},
  publisher={Springer}
}

@article{dvurechensky2019near,
  title={Near-optimal tensor methods for minimizing the gradient norm of convex function},
  author={Dvurechensky, Pavel and Gasnikov, Alexander and Ostroukhov, Petr and Uribe, C{\'e}sar A and Ivanova, Anastasiya},
  journal={arXiv preprint arXiv:1912.03381},
  year={2019}
}

@article{nesterov2006cubic,
  title={Cubic regularization of Newton method and its global performance},
  author={Nesterov, Yurii and Polyak, Boris T},
  journal={Mathematical Programming},
  volume={108},
  number={1},
  pages={177--205},
  year={2006},
  publisher={Springer}
}


@article{grapiglia2021inexact,
  title={On inexact solution of auxiliary problems in tensor methods for convex optimization},
  author={Grapiglia, Geovani Nunes and Nesterov, Yu},
  journal={Optimization Methods and Software},
  volume={36},
  number={1},
  pages={145--170},
  year={2021},
  publisher={Taylor \& Francis}
}

@article{grapiglia2020tensor,
  title={Tensor methods for finding approximate stationary points of convex functions},
  author={Grapiglia, Geovani Nunes and Nesterov, Yurii},
  journal={Optimization Methods and Software},
  pages={1--34},
  year={2020},
  publisher={Taylor \& Francis}
}



@article{grapiglia2019accelerated,
  title={Accelerated regularized Newton methods for minimizing composite convex functions},
  author={Grapiglia, Geovani N and Nesterov, Yurii},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={1},
  pages={77--99},
  year={2019},
  publisher={SIAM}
}


@article{vavasis1993black,
  title={Black-box complexity of local minimization},
  author={Vavasis, Stephen A},
  journal={SIAM Journal on Optimization},
  volume={3},
  number={1},
  pages={60--80},
  year={1993},
  publisher={SIAM}
}



@article{nesterov2022quartic,
  title={Quartic Regularity},
  author={Nesterov, Yurii},
  journal={arXiv preprint arXiv:2201.04852},
  year={2022}
}

@article{grapiglia2022adaptive,
  title={Adaptive Third-Order Methods for Composite Convex Optimization},
  author={Grapiglia, Geovani Nunes and Nesterov, Yurii},
  journal={arXiv preprint arXiv:2202.12730},
  year={2022}
}

@article{huang2022finding,
  title={Finding the global optimum of a class of quartic minimization problem},
  author={Huang, Pengfei and Yang, Qingzhi and Yang, Yuning},
  journal={Computational Optimization and Applications},
  pages={1--32},
  year={2022},
  publisher={Springer}
}

@article{biglari2011new,
  title={New quasi-Newton methods via higher order tensor models},
  author={Biglari, Fahimeh and Hassan, Malik Abu and Leong, Wah June},
  journal={Journal of computational and applied mathematics},
  volume={235},
  number={8},
  pages={2412--2422},
  year={2011},
  publisher={Elsevier}
}

@article{emmenegger2021oracle,
  title={On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum Optimization},
  author={Emmenegger, Nicolas and Kyng, Rasmus and Zehmakan, Ahad N},
  journal={arXiv preprint arXiv:2103.05138},
  year={2021}
}


@article{cartis2022evaluation,
  title={Evaluation complexity of algorithms for nonconvex optimization},
  author={Cartis, Coralia and Gould, Nicholas Ian Mark and Toint, Ph L},
  journal={MOS-SIAM Series on Optimization.},
  year={2022}
}

@article{gould2015cutest,
  title={CUTEst: A Constrained and Unconstrained Testing Environment with Safe Threads for Mathematical Optimization},
  author={Gould, Nicholas IM and Orban, Dominique and Toint, Philippe L},
  journal={Computational Optimization and Applications},
  volume={60},
  number={3},
  pages={545--557},
  year={2015},
  publisher={Springer}
}

@article{more1981testing,
  title={Testing Unconstrained Optimization Software},
  author={Mor{\'e}, Jorge J and Garbow, Burton S and Hillstrom, Kenneth E},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={7},
  number={1},
  pages={17--41},
  year={1981},
  publisher={ACM New York, NY, USA}
}

@article{schnabel1984tensor,
  title={Tensor methods for nonlinear equations},
  author={Schnabel, Robert B and Frank, Paul D},
  journal={SIAM Journal on Numerical Analysis},
  volume={21},
  number={5},
  pages={815--843},
  year={1984},
  publisher={SIAM}
}

@inproceedings{kohler2017sub,
  title={Sub-sampled cubic regularization for non-convex optimization},
  author={Kohler, Jonas Moritz and Lucchi, Aurelien},
  booktitle={International Conference on Machine Learning},
  pages={1895--1904},
  year={2017},
  organization={PMLR}
}

@article{cartis2019universal,
  title={Universal regularization methods: varying the power, the smoothness and the accuracy},
  author={Cartis, Coralia and Gould, Nick I and Toint, Philippe L},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={1},
  pages={595--615},
  year={2019},
  publisher={SIAM}
}

@article{8,
  title={Second-order optimality and beyond: characterization and evaluation complexity in nonconvex convexly-constrained optimization},
  author={Cartis, C and Gould, NIM and Toint, Ph L},
  journal={Preprint RAL-P-2016-008, Rutherford Appleton Laboratory, Chilton, Oxfordshire, England},
  year={2016}
}

@article{zhu2020adaptive,
  title={An adaptive high order method for finding third-order critical points of nonconvex optimization},
  author={Zhu, Xihua and Han, Jiangze and Jiang, Bo},
  journal={arXiv preprint arXiv:2008.04191},
  year={2020}
}



@article{cartis2020sharp,
  title={Sharp worst-case evaluation complexity bounds for arbitrary-order nonconvex optimization with inexpensive constraints},
  author={Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={1},
  pages={513--541},
  year={2020},
  publisher={SIAM}
}

@article{cartis2020concise,
  title={A concise second-order complexity analysis for unconstrained optimization using high-order regularized models},
  author={Cartis, Coralia and Gould, Nick IM and Toint, Ph L},
  journal={Optimization Methods and Software},
  volume={35},
  number={2},
  pages={243--256},
  year={2020},
  publisher={Taylor \& Francis}
}


@article{nesterov2021implementable,
  title={Implementable tensor methods in unconstrained convex optimization},
  author={Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={186},
  number={1},
  pages={157--183},
  year={2021},
  publisher={Springer}
}

@article{carmon2020lower,
  title={Lower bounds for finding stationary points I},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={184},
  number={1},
  pages={71--120},
  year={2020},
  publisher={Springer}
}

@article{nesterov2020inexact,
  title={Inexact accelerated high-order proximal-point methods},
  author={Nesterov, Yurii},
  journal={Mathematical Programming},
  pages={1--26},
  year={2021},
  publisher={Springer}
}


@article{nesterov2021inexact,
  title={Inexact high-order proximal-point methods with auxiliary search procedure},
  author={Nesterov, Yurii},
  journal={SIAM Journal on Optimization},
  volume={31},
  number={4},
  pages={2807--2828},
  year={2021},
  publisher={SIAM}
}

@article{nesterov2008accelerating,
  title={Accelerating the cubic regularization of Newton’s method on convex problems},
  author={Nesterov, Yu},
  journal={Mathematical Programming},
  volume={112},
  number={1},
  pages={159--181},
  year={2008},
  publisher={Springer}
}

@article{weiser2007affine,
  title={Affine conjugate adaptive Newton methods for nonlinear elastomechanics},
  author={Weiser, Martin and Deuflhard, Peter and Erdmann, Bodo},
  journal={Optimisation Methods and Software},
  volume={22},
  number={3},
  pages={413--431},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{lieder2020solving,
  title={Solving large-scale cubic regularization by a generalized eigenvalue problem},
  author={Lieder, Felix},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={4},
  pages={3345--3358},
  year={2020},
  publisher={SIAM}
}

@article{lucidi1998some,
  title={On some properties of quadratic programs with a convex quadratic constraint},
  author={Lucidi, Stefano and Palagi, Laura and Roma, Massimo},
  journal={SIAM Journal on Optimization},
  volume={8},
  number={1},
  pages={105--122},
  year={1998},
  publisher={SIAM}
}

@article{martinez1994local,
  title={Local minimizers of quadratic functions on Euclidean balls and spheres},
  author={Mart{\'\i}nez, Jos{\'e} Mario},
  journal={SIAM Journal on Optimization},
  volume={4},
  number={1},
  pages={159--176},
  year={1994},
  publisher={SIAM}
}

@techreport{griewank1981modification,
  title={The modification of Newton’s method for unconstrained optimization by bounding cubic terms},
  author={Griewank, Andreas},
  year={1981},
  institution={Technical report NA/12}
}

@article{hsia2017theory,
  title={Theory and application of p-regularized subproblems for p> 2},
  author={Hsia, Yong and Sheu, Ruey-Lin and Yuan, Ya-xiang},
  journal={Optimization Methods and Software},
  volume={32},
  number={5},
  pages={1059--1077},
  year={2017},
  publisher={Taylor \& Francis}
}

@article{carmon2021lower,
  title={Lower bounds for finding stationary points II: first-order methods},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={Mathematical Programming},
  volume={185},
  number={1},
  pages={315--355},
  year={2021},
  publisher={Springer}
}

@article{birgin2017worst,
  title={Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order regularized models},
  author={Birgin, Ernesto G and Gardenghi, JL and Mart{\'\i}nez, Jos{\'e} Mario and Santos, Sandra Augusta and Toint, Ph L},
  journal={Mathematical Programming},
  volume={163},
  number={1},
  pages={359--368},
  year={2017},
  publisher={Springer}
}


@article{arc1,
  title={Complexity bounds for second-order optimality in unconstrained optimization},
  author={Cartis, Coralia and Gould, Nicholas IM and Toint, Ph L},
  journal={Journal of Complexity},
  volume={28},
  number={1},
  pages={93--108},
  year={2012},
  publisher={Elsevier}
}

@article{birgin2018fortran,
  title={Fortran routines for testing unconstrained optimization software with derivatives up to third-order},
  author={Birgin, EG and Gardenghi, JL and Mart{\'\i}nez, JM and Santos, SA},
  year={2018}
}


@article{birgin2020use,
  title={On the use of third-order models with fourth-order regularization for unconstrained optimization},
  author={Birgin, Ernesto G and Gardenghi, JL and Mart{\'\i}nez, Jos{\'e} Mario and Santos, Sandra A},
  journal={Optimization Letters},
  volume={14},
  number={4},
  pages={815--838},
  year={2020},
  publisher={Springer}
}

@article{gould2017higher,
  title={A higher order method for solving nonlinear least-squares problems},
  author={Gould, NIM and Rees, T and Scott, JA},
  journal={RAL Preprint RAL-P-2017--010, STFC Rutherford Appleton Laboratory},
  year={2017}
}

@article{schnabel1991tensor,
  title={Tensor methods for unconstrained optimization using second derivatives},
  author={Schnabel, Robert B and Chow, Ta-Tung},
  journal={SIAM Journal on Optimization},
  volume={1},
  number={3},
  pages={293--315},
  year={1991},
  publisher={SIAM}
}

@article{more1980software,
  title={A software package for solving systems of nonlinear equations and nonlinear least-squares problems},
  author={More, Jorge J and Garbow, Burton S and Hillstrom, Kenneth E},
  journal={ACM Transactions on Mathematical Software (TOMS)},
  volume={7},
  number={2},
  pages={105--116},
  year={1980},
  publisher={ACM}
}


@article{zhou2016tensor,
  title={Tensor methods for large, sparse nonlinear least squares problems},
  author={Zhou, Guangming and Qi, Yingyong and Zhou, Xiaofeng and Zhang, Tonghua},
  journal={SIAM Journal on Scientific Computing},
  volume={38},
  number={5},
  pages={A2843--A2866},
  year={2016},
  publisher={SIAM}
}


@article{cartis2021scalable,
  title={Scalable Subspace Methods for Derivative-Free Nonlinear Least-Squares Optimization},
  author={Cartis, Coralia and Roberts, Lindon},
  journal={arXiv preprint arXiv:2102.12016},
  year={2021}
}

@article{grapiglia2022tensor,
  title={Tensor methods for finding approximate stationary points of convex functions},
  author={Grapiglia, Geovani Nunes and Nesterov, Yurii},
  journal={Optimization Methods and Software},
  volume={37},
  number={2},
  pages={605--638},
  year={2022},
  publisher={Taylor \& Francis}
}

@article{cartis2007adaptive,
  title={Adaptive cubic overestimation methods for unconstrained optimization},
  journal={ora.ox.ac.uk},
  author={Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
  year={2007},
  
}

@misc{wright2006numerical,
  title={Numerical optimization},
  author={Wright, Jorge Nocedal Stephen J},
  year={2006},
  publisher={springer publication}
} 


@article{li2020hyperfast,
  title={Hyperfast second-order local solvers for efficient statistically preconditioned distributed optimization},
  author={Li, Jiawei and Zhang, Ji and Chen, Weinan and Chen, Zhiqiang and Yang, Wotao},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={87},
  pages={1--39},
  year={2020}
}

@article{schnabel1971tensor,
  title={Tensor methods for unconstrained optimization using second derivatives},
  author={Schnabel, Robert B},
  journal={Mathematics of Computation},
  volume={25},
  number={114},
  pages={295--315},
  year={1971},
  publisher={American Mathematical Society}
}

@phdthesis{chow1989derivative,
title={Derivative and Secant Tensor Methods for Unconstrained Optimization},
author={Chow, T.},
year={1989},
school={University of California, Berkeley}
}

@article{welzel2023generalizing,
  title={Generalizing Quasi-Newton Updates to Higher-Order Derivative Tensors},
  author={Welzel, Karl and Hauser, Raphael A},
  journal={arXiv preprint arXiv:2301.11678},
  year={2023}
}

@techreport{birgin2018third,
  title={Third-order derivatives of the Mor{\'e}, Garbow, and Hillstrom test set problems},
  author={Birgin, EG and Gardenghi, JL and Mart{\'\i}nez, JM and Santos, SA},
  year={2018},
  institution={Tech. Rep. MCDO010418, Department of Computer Science, University of Sao~…}
}

@article{birgin2017use,
  title={On the use of third-order models with fourth-order regularization for unconstrained optimization},
  author={Birgin, Eduardo G and Martinez, Jos{\'e} M and Raydan, Marcos},
  journal={Computational Optimization and Applications},
  volume={68},
  number={3},
  pages={599--622},
  year={2017},
  publisher={Springer}
}

@article{fowkes2013branch,
  title={A branch and bound algorithm for the global optimization of Hessian Lipschitz continuous functions},
  author={Fowkes, Jaroslav M and Gould, Nicholas IM and Farmer, Chris L},
  journal={Journal of Global Optimization},
  volume={56},
  number={4},
  pages={1791--1815},
  year={2013},
  publisher={Springer}
}

@article{cartis2015branching,
  title={Branching and bounding improvements for global optimization algorithms with Lipschitz continuity properties},
  author={Cartis, Coralia and Fowkes, Jaroslav M and Gould, Nicholas IM},
  journal={Journal of Global Optimization},
  volume={61},
  number={3},
  pages={429--457},
  year={2015},
  publisher={Springer}
}

@article{yang2018cubically,
  title={A cubically convergent method for solving the largest eigenvalue of a nonnegative irreducible tensor},
  author={Yang, Wei-wei and Ni, Qin},
  journal={Numerical Algorithms},
  volume={77},
  number={4},
  pages={1183--1197},
  year={2018},
  publisher={Springer}
}



@article{lasserre2021connecting,
  title={Connecting optimization with spectral analysis of tri-diagonal matrices},
  author={Lasserre, Jean B},
  journal={Mathematical Programming},
  volume={190},
  number={1},
  pages={795--809},
  year={2021},
  publisher={Springer}
}

@inproceedings{carmon2017convex,
  title={“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={654--663},
  year={2017},
  organization={PMLR}
}

@article{steihaugcomputing,
  title={On computing with general sparse third derivatives in unconstrained optimization Geir Gundersen},
  author={Steihaug, Trond},
  publisher={Citeseer}
}

@article{doikov2022affine,
  title={Affine-invariant contracting-point methods for Convex Optimization},
  author={Doikov, Nikita and Nesterov, Yurii},
  journal={Mathematical Programming},
  pages={1--23},
  year={2022},
  publisher={Springer}
} %side project

@article{halko2011randomized,
  title={Randomized low-rank approximation for symmetric indefinite matrices},
  author={Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={35},
  number={4},
  pages={1255--1305},
  year={2011},
  publisher={SIAM}
}

@article{adachi2017solving,
  title={Solving the trust-region subproblem by a generalized eigenvalue problem},
  author={Adachi, Satoru and Iwata, Satoru and Nakatsukasa, Yuji and Takeda, Akiko},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={1},
  pages={269--291},
  year={2017},
  publisher={SIAM}
}

@article{zhu2023convergence,
  title={Convergence and Near-optimal Sampling for Multivariate Function Approximations in Irregular Domains via Vandermonde with Arnoldi},
  author={Zhu, Wenqi and Nakatsukasa, Yuji},
  journal={arXiv preprint arXiv:2301.12241},
  year={2023}
}

@article{park2022randomized,
  title={Randomized low-rank approximation for symmetric indefinite matrices},
  author={Park, Taejun and Nakatsukasa, Yuji},
  journal={arXiv preprint arXiv:2212.01127},
  year={2022}
}

@article{meier2023sketch,
  title={Are sketch-and-precondition least squares solvers numerically stable?},
  author={Meier, Maike and Nakatsukasa, Yuji and Townsend, Alex and Webb, Marcus},
  journal={arXiv preprint arXiv:2302.07202},
  year={2023}
}

@article{che2020computation,
  title={The computation of low multilinear rank approximations of tensors via power scheme and random projection},
  author={Che, Maolin and Wei, Yimin and Yan, Hong},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={41},
  number={2},
  pages={605--636},
  year={2020},
  publisher={SIAM}
}

@article{hazan2012sketch,
  title={Are sketch-and-precondition least squares solvers numerically stable?},
  author={Hazan, Elad},
  journal={Foundations of Computational Mathematics},
  volume={12},
  number={6},
  pages={805--826},
  year={2012},
  publisher={Springer}
}

@article{doikov2020contracting,
  title={Contracting proximal methods for smooth convex optimization},
  author={Doikov, Nikita and Nesterov, Yurii},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={4},
  pages={3146--3169},
  year={2020},
  publisher={SIAM}
} %side project


@article{gould2019convergence,
  title={Convergence and evaluation-complexity analysis of a regularized tensor-Newton method for solving nonlinear least-squares problems},
  author={Gould, Nicholas IM and Rees, Tyrone and Scott, Jennifer A},
  journal={Computational Optimization and Applications},
  volume={73},
  number={1},
  pages={1--35},
  year={2019},
  publisher={Springer}
} % least squares

@inproceedings{lfberg2004toolbox, 
  title={A toolbox for modeling and optimization in MATLAB},
  author={Lfberg, J},
  booktitle={Proceedings of the Conference on Computer-Aided Control System Design (CACSD) p},
  volume={284289},
  year={2004}
}


@inproceedings{prajna2002introducing,
  title={Introducing SOSTOOLS: A general purpose sum of squares programming solver},
  author={Prajna, Stephen and Papachristodoulou, Antonis and Parrilo, Pablo A},
  booktitle={Proceedings of the 41st IEEE Conference on Decision and Control, 2002.},
  volume={1},
  pages={741--746},
  year={2002},
  organization={IEEE}
}

@article{lasserre2017bounded,
  title={A bounded degree SOS hierarchy for polynomial optimization},
  author={Lasserre, JeanB and Toh, Kim-Chuan and Yang, Shouguang},
  journal={EURO Journal on Computational Optimization},
  volume={5},
  number={1-2},
  pages={87--117},
  year={2017},
  publisher={Elsevier}
}

@article{zhu2024global,
  title={Global Convergence of High-Order Regularization Methods with Sums-of-Squares Taylor Models},
  author={Zhu, Wenqi and Cartis, Coralia},
  journal={arXiv preprint arXiv:2404.03035},
  year={2024}
}


@article{weisser2018sparse,
  title={Sparse-BSOS: a bounded degree SOS hierarchy for large scale polynomial optimization with sparsity},
  author={Weisser, Tillmann and Lasserre, Jean B and Toh, Kim-Chuan},
  journal={Mathematical Programming Computation},
  volume={10},
  pages={1--32},
  year={2018},
  publisher={Springer}
}

@article{waki2006sums,
  title={Sums of squares and semidefinite program relaxations for polynomial optimization problems with structured sparsity},
  author={Waki, Hayato and Kim, Sunyoung and Kojima, Masakazu and Muramatsu, Masakazu},
  journal={SIAM Journal on Optimization},
  volume={17},
  number={1},
  pages={218--242},
  year={2006},
  publisher={SIAM}
}

@inproceedings{zheng2019sparse,
  title={Sparse sum-of-squares (SOS) optimization: A bridge between DSOS/SDSOS and SOS optimization for sparse polynomials},
  author={Zheng, Yang and Fantuzzi, Giovanni and Papachristodoulou, Antonis},
  booktitle={2019 American Control Conference (ACC)},
  pages={5513--5518},
  year={2019},
  organization={IEEE}
}

@article{ahmadi2019dsos,
  title={DSOS and SDSOS optimization: more tractable alternatives to sum of squares and semidefinite optimization},
  author={Ahmadi, Amir Ali and Majumdar, Anirudha},
  journal={SIAM Journal on Applied Algebra and Geometry},
  volume={3},
  number={2},
  pages={193--230},
  year={2019},
  publisher={SIAM}
}

@article{sturm1999using,
  title={Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones},
  author={Sturm, Jos F},
  journal={Optimization methods and software},
  volume={11},
  number={1-4},
  pages={625--653},
  year={1999},
  publisher={Taylor \& Francis}
}

@phdthesis{parrilo2000structured, 
  title={Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization},
  author={Parrilo, Pablo A},
  year={2000},
  school={California Institute of Technology}
}
@inproceedings{lofberg2004yalmip,
  title={YALMIP: A toolbox for modeling and optimization in MATLAB},
  author={Lofberg, Johan},
  booktitle={2004 IEEE international conference on robotics and automation (IEEE Cat. No. 04CH37508)},
  pages={284--289},
  year={2004},
  organization={IEEE}
}

@article{cartis2012evaluation,
  title={Evaluation complexity of adaptive cubic regularization methods for convex unconstrained optimization},
  author={Cartis, Coralia and Gould, Nicholas IM and Toint, Philippe L},
  journal={Optimization Methods and Software},
  volume={27},
  number={2},
  pages={197--219},
  year={2012},
  publisher={Taylor \& Francis}
}


@article{toint1978some,
  title={Some numerical results using a sparse matrix updating formula in unconstrained optimization},
  author={Toint, Ph L},
  journal={Mathematics of Computation},
  volume={32},
  number={143},
  pages={839--851},
  year={1978}
} %halley method

@article{qi2003multivariate,
  title={Multivariate polynomial minimization and its application in signal processing},
  author={Qi, Liqun and Lay Teo, Kok},
  journal={Journal of Global Optimization},
  volume={26},
  number={4},
  pages={419--433},
  year={2003},
  publisher={Springer}
}

@article{qi2004global,
  title={Global minimization of normal quartic polynomials based on global descent directions},
  author={Qi, Liqun and Wan, Zhong and Yang, Yu-Fei},
  journal={SIAM Journal on Optimization},
  volume={15},
  number={1},
  pages={275--302},
  year={2004},
  publisher={SIAM}
}

@article{qi2004extrema,
  title={Extrema of a real polynomial},
  author={Qi, Liqun},
  journal={Journal of Global Optimization},
  volume={30},
  number={4},
  pages={405--433},
  year={2004},
  publisher={Springer}
}




@article{nesterov2021superfast,
  title={Superfast second-order methods for unconstrained convex optimization},
  author={Nesterov, Yurii},
  journal={Journal of Optimization Theory and Applications},
  volume={191},
  number={1},
  pages={1--30},
  year={2021},
  publisher={Springer}
}

%%Tensors-------------------------------------
@article{qi2005eigenvalues,
  title={Eigenvalues of a real supersymmetric tensor},
  author={Qi, Liqun},
  journal={Journal of Symbolic Computation},
  volume={40},
  number={6},
  pages={1302--1324},
  year={2005},
  publisher={Elsevier}
}

@article{he2015inhomogeneous,
  title={Inhomogeneous polynomial optimization over a convex set: An approximation approach},
  author={He, Simai and Li, Zhening and Zhang, Shuzhong},
  journal={Mathematics of Computation},
  volume={84},
  number={292},
  pages={715--741},
  year={2015}
}

@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{de2004first,
  title={First-order perturbation analysis of the best rank-(R1, R2, R3) approximation in multilinear algebra},
  author={De Lathauwer, Lieven},
  journal={Journal of Chemometrics: A Journal of the Chemometrics Society},
  volume={18},
  number={1},
  pages={2--11},
  year={2004},
  publisher={Wiley Online Library}
}

@article{zhang2012best,
  title={The best rank-1 approximation of a symmetric tensor and related spherical optimization problems},
  author={Zhang, Xinzhen and Ling, Chen and Qi, Liqun},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={33},
  number={3},
  pages={806--821},
  year={2012},
  publisher={SIAM}
}

@article{ni2007best,
  title={On the best rank-1 approximation to higher-order symmetric tensors},
  author={Ni, Guyan and Wang, Yiju},
  journal={Mathematical and Computer Modelling},
  volume={46},
  number={9-10},
  pages={1345--1352},
  year={2007},
  publisher={Elsevier}
}

@article{kofidis2002best,
  title={On the best rank-1 approximation of higher-order supersymmetric tensors},
  author={Kofidis, Eleftherios and Regalia, Phillip A},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={23},
  number={3},
  pages={863--884},
  year={2002},
  publisher={SIAM}
}

@article{chen2021c,
  title={The C-eigenvalue of third order tensors and its application in crystals},
  author={Chen, Yannan and J{\'a}kli, Antal and Qi, Liqun},
  journal={Journal of Industrial \& Management Optimization},
  year={2021},
  publisher={American Institute of Mathematical Sciences}
}

@article{kolda2020stochastic,
  title={Stochastic gradients for large-scale tensor decomposition},
  author={Kolda, Tamara G and Hong, David},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1066--1095},
  year={2020},
  publisher={SIAM}
}

@inproceedings{lim2005singular,
  title={Singular values and eigenvalues of tensors: a variational approach},
  author={Lim, Lek-Heng},
  booktitle={1st IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing, 2005.},
  pages={129--132},
  year={2005},
  organization={IEEE}
}

@article{woodruff2014sketching,
  title={Sketching as a tool for numerical linear algebra},
  author={Woodruff, David P},
  journal={arXiv preprint arXiv:1411.4357},
  year={2014}
}

@incollection{gelfand1994hyperdeterminants,
  title={Hyperdeterminants},
  author={Gelfand, Israel M and Kapranov, Mikhail M and Zelevinsky, Andrei V},
  booktitle={Discriminants, Resultants, and Multidimensional Determinants},
  pages={444--479},
  year={1994},
  publisher={Springer}
}

@article{kolda2009tensor,
  title={Tensor decompositions and applications},
  author={Kolda, Tamara G and Bader, Brett W},
  journal={SIAM review},
  volume={51},
  number={3},
  pages={455--500},
  year={2009},
  publisher={SIAM}
}


@article{doikov2022local,
  title={Local convergence of tensor methods},
  author={Doikov, Nikita and Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={193},
  number={1},
  pages={315--336},
  year={2022},
  publisher={Springer}
}

@article{ahmadi2013complete,
  title={A complete characterization of the gap between convexity and SOS-convexity},
  author={Ahmadi, Amir Ali and Parrilo, Pablo A},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={2},
  pages={811--833},
  year={2013},
  publisher={SIAM}
}

@article{chen2022efficient,  
  title={An efficient alternating minimization method for fourth degree polynomial optimization},
  author={Chen, Haibin and He, Hongjin and Wang, Yiju and Zhou, Guanglu},
  journal={Journal of Global Optimization},
  volume={82},
  number={1},
  pages={83--103},
  year={2022},
  publisher={Springer}
}

@article{kolda2011shifted,
  title={Shifted power method for computing tensor eigenpairs},
  author={Kolda, Tamara G and Mayo, Jackson R},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={32},
  number={4},
  pages={1095--1124},
  year={2011},
  publisher={SIAM}
}

@article{wen2018proximal,
  title={A proximal difference-of-convex algorithm with extrapolation},
  author={Wen, Bo and Chen, Xiaojun and Pong, Ting Kei},
  journal={Computational optimization and applications},
  volume={69},
  pages={297--324},
  year={2018},
  publisher={Springer}
}

@article{kolda2014adaptive, 
  title={An adaptive shifted power method for computing generalized tensor eigenpairs},
  author={Kolda, Tamara G and Mayo, Jackson R},
  journal={SIAM Journal on Matrix Analysis and Applications},
  volume={35},
  number={4},
  pages={1563--1581},
  year={2014},
  publisher={SIAM}
}

@article{ahmadi2023higher,
  title={Higher-Order Newton Methods with Polynomial Work per Iteration},
  author={Ahmadi, Amir Ali and Chaudhry, Abraar and Zhang, Jeffrey},
  journal={arXiv preprint arXiv:2311.06374},
  year={2023}
}

@article{Cartis2024Efficient,
  title={Efficient implementation of third-order tensor methods with adaptive regularization for unconstrained optimization},
  author={Cartis, Coralia and Hauser, Raphael and Liu, Yang and Welzel, Karl and Zhu, Wenqi},
  journal={Draft available on request},
  year={2024}
}


@article{welleck2024decoding,
  title={From decoding to meta-generation: Inference-time algorithms for large language models},
  author={Welleck, Sean and Bertsch, Amanda and Finlayson, Matthew and Schoelkopf, Hailey and Xie, Alex and Neubig, Graham and Kulikov, Ilia and Harchaoui, Zaid},
  journal={arXiv preprint arXiv:2406.16838},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@misc{aime,
    title = {AIME},
    url = {https://artofproblemsolving.com/wiki/index.php/AIME_Problems_and_Solutions/},
    author = {Mathematical Association of America },
    month = {February},
    year = {2024}
}


@book{qi2018tensor,
  title={Tensor eigenvalues and their applications},
  author={Qi, Liqun and Chen, Haibin and Chen, Yannan},
  volume={39},
  year={2018},
  publisher={Springer}
}

@inproceedings{zheng2024llamafactory,
  title={LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Zhangchi Feng and Yongqiang Ma},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations)},
  address={Bangkok, Thailand},
  publisher={Association for Computational Linguistics},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}

@article{larsen2020practical,
  title={Practical leverage-based sampling for low-rank tensor decomposition},
  author={Larsen, Brett W and Kolda, Tamara G},
  journal={arXiv preprint arXiv:2006.16438},
  year={2020}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@article{ye2025limo,
  title={LIMO: Less is More for Reasoning},
  author={Ye, Yixin and Huang, Zhen and Xiao, Yang and Chern, Ethan and Xia, Shijie and Liu, Pengfei},
  journal={arXiv preprint arXiv:2502.03387},
  year={2025}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}


@article{ahn2024large,
  title={Large language models for mathematical reasoning: Progresses and challenges},
  author={Ahn, Janice and Verma, Rishu and Lou, Renze and Liu, Di and Zhang, Rui and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2402.00157},
  year={2024}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{huang2022towards,
  title={Towards reasoning in large language models: A survey},
  author={Huang, Jie and Chang, Kevin Chen-Chuan},
  journal={arXiv preprint arXiv:2212.10403},
  year={2022}
}

@article{yoshikawa2023large,
  title={Large language models for chemistry robotics},
  author={Yoshikawa, Naruki and Skreta, Marta and Darvish, Kourosh and Arellano-Rubach, Sebastian and Ji, Zhi and Bj{\o}rn Kristensen, Lasse and Li, Andrew Zou and Zhao, Yuchi and Xu, Haoping and Kuramshin, Artur and others},
  journal={Autonomous Robots},
  volume={47},
  number={8},
  pages={1057--1086},
  year={2023},
  publisher={Springer}
}

@article{cai2023large,
  title={Large language models as tool makers},
  author={Cai, Tianle and Wang, Xuezhi and Ma, Tengyu and Chen, Xinyun and Zhou, Denny},
  journal={arXiv preprint arXiv:2305.17126},
  year={2023}
}

@article{singhal2023large,
  title={Large language models encode clinical knowledge},
  author={Singhal, Karan and Azizi, Shekoofeh and Tu, Tao and Mahdavi, S Sara and Wei, Jason and Chung, Hyung Won and Scales, Nathan and Tanwani, Ajay and Cole-Lewis, Heather and Pfohl, Stephen and others},
  journal={Nature},
  volume={620},
  number={7972},
  pages={172--180},
  year={2023},
  publisher={Nature Publishing Group}
}

@book{qi2017tensor,
  title={Tensor analysis: spectral theory and special tensors},
  author={Qi, Liqun and Luo, Ziyan},
  year={2017},
  publisher={SIAM}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{bradley-terry,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334029},
 author = {Ralph Allan Bradley and Milton E. Terry},
 journal = {Biometrika},
 number = {3/4},
 pages = {324--345},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons},
 urldate = {2024-10-31},
 volume = {39},
 year = {1952}
}

@misc{xin2024deepseekproveradvancingtheoremproving,
      title={DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data}, 
      author={Huajian Xin and Daya Guo and Zhihong Shao and Zhizhou Ren and Qihao Zhu and Bo Liu and Chong Ruan and Wenda Li and Xiaodan Liang},
      year={2024},
      eprint={2405.14333},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.14333}, 
}

@misc{xu2025redstardoesscalinglongcot,
      title={RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?}, 
      author={Haotian Xu and Xing Wu and Weinong Wang and Zhongzhi Li and Da Zheng and Boyuan Chen and Yi Hu and Shijia Kang and Jiaming Ji and Yingying Zhang and Zhijiang Guo and Yaodong Yang and Muhan Zhang and Debing Zhang},
      year={2025},
      eprint={2501.11284},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.11284}, 
}

@misc{gandhi2024streamsearchsoslearning,
      title={Stream of Search (SoS): Learning to Search in Language}, 
      author={Kanishk Gandhi and Denise Lee and Gabriel Grand and Muxin Liu and Winson Cheng and Archit Sharma and Noah D. Goodman},
      year={2024},
      eprint={2404.03683},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.03683}, 
}

@misc{su2024brightrealisticchallengingbenchmark,
      title={BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval}, 
      author={Hongjin Su and Howard Yen and Mengzhou Xia and Weijia Shi and Niklas Muennighoff and Han-yu Wang and Haisu Liu and Quan Shi and Zachary S. Siegel and Michael Tang and Ruoxi Sun and Jinsung Yoon and Sercan O. Arik and Danqi Chen and Tao Yu},
      year={2024},
      eprint={2407.12883},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12883}, 
}

@misc{kapfer_2025_14751899,
  author       = {Kapfer, Craig and
                  Stine, Kurt and
                  Narasimhan, Balasubramanian and
                  Mentzel, Christopher and
                  Candes, Emmanuel},
  title        = {Marlowe: Stanford's GPU-based Computational
                   Instrument
                  },
  month        = jan,
  year         = 2025,
  publisher    = {Zenodo},
  version      = {0.1},
  doi          = {10.5281/zenodo.14751899},
  url          = {https://doi.org/10.5281/zenodo.14751899},
}

@misc{zelikman2022starbootstrappingreasoningreasoning,
      title={STaR: Bootstrapping Reasoning With Reasoning}, 
      author={Eric Zelikman and Yuhuai Wu and Jesse Mu and Noah D. Goodman},
      year={2022},
      eprint={2203.14465},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2203.14465}, 
}

@misc{zelikman2024quietstarlanguagemodelsteach,
      title={Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking}, 
      author={Eric Zelikman and Georges Harik and Yijia Shao and Varuna Jayasiri and Nick Haber and Noah D. Goodman},
      year={2024},
      eprint={2403.09629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.09629}, 
}

@misc{r1,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{wall,
      title={Pre-training as we know it will end}, 
      author={Ilya Sutskever},
      year={2024},
      url={https://x.com/_jasonwei/status/1867696401830096970}, 
}

@misc{modelcompression,
author = {Bucila, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
title = {Model compression},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150464},
doi = {10.1145/1150402.1150464},
abstract = {Often the best performing supervised learning models are ensembles of hundreds or thousands of base-level classifiers. Unfortunately, the space required to store this many classifiers, and the time required to execute them at run-time, prohibits their use in applications where test sets are large (e.g. Google), where storage space is at a premium (e.g. PDAs), and where computational power is limited (e.g. hea-ring aids). We present a method for "compressing" large, complex ensembles into smaller, faster models, usually without significant loss in performance.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {535–541},
numpages = {7},
keywords = {supervised learning, model compression},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@misc{k1.5,
      title={Kimi k1.5: Scaling Reinforcement Learning with LLMs}, 
      author={Kimi Team and Angang Du and Bofei Gao and Bowei Xing and Changjiu Jiang and Cheng Chen and Cheng Li and Chenjun Xiao and Chenzhuang Du and Chonghua Liao and Chuning Tang and Congcong Wang and Dehao Zhang and Enming Yuan and Enzhe Lu and Fengxiang Tang and Flood Sung and Guangda Wei and Guokun Lai and Haiqing Guo and Han Zhu and Hao Ding and Hao Hu and Hao Yang and Hao Zhang and Haotian Yao and Haotian Zhao and Haoyu Lu and Haoze Li and Haozhen Yu and Hongcheng Gao and Huabin Zheng and Huan Yuan and Jia Chen and Jianhang Guo and Jianlin Su and Jianzhou Wang and Jie Zhao and Jin Zhang and Jingyuan Liu and Junjie Yan and Junyan Wu and Lidong Shi and Ling Ye and Longhui Yu and Mengnan Dong and Neo Zhang and Ningchen Ma and Qiwei Pan and Qucheng Gong and Shaowei Liu and Shengling Ma and Shupeng Wei and Sihan Cao and Siying Huang and Tao Jiang and Weihao Gao and Weimin Xiong and Weiran He and Weixiao Huang and Wenhao Wu and Wenyang He and Xianghui Wei and Xianqing Jia and Xingzhe Wu and Xinran Xu and Xinxing Zu and Xinyu Zhou and Xuehai Pan and Y. Charles and Yang Li and Yangyang Hu and Yangyang Liu and Yanru Chen and Yejie Wang and Yibo Liu and Yidao Qin and Yifeng Liu and Ying Yang and Yiping Bao and Yulun Du and Yuxin Wu and Yuzhi Wang and Zaida Zhou and Zhaoji Wang and Zhaowei Li and Zhen Zhu and Zheng Zhang and Zhexu Wang and Zhilin Yang and Zhiqi Huang and Zihao Huang and Ziyao Xu and Zonghan Yang},
      year={2025},
      eprint={2501.12599},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.12599}, 
}

@misc{hou2025advancinglanguagemodelreasoning,
      title={Advancing Language Model Reasoning through Reinforcement Learning and Inference Scaling}, 
      author={Zhenyu Hou and Xin Lv and Rui Lu and Jiajie Zhang and Yujiang Li and Zijun Yao and Juanzi Li and Jie Tang and Yuxiao Dong},
      year={2025},
      eprint={2501.11651},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.11651}, 
}

@misc{cesista2024multimodalstructuredgenerationcvprs,
      title={Multimodal Structured Generation: CVPR's 2nd MMFM Challenge Technical Report}, 
      author={Franz Louis Cesista},
      year={2024},
      eprint={2406.11403},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.11403}, 
}

@misc{phan2025humanity,
  title={Humanity's Last Exam},
  author={Phan, Long and Gatti, Alice and Han, Ziwen and Li, Nathaniel and Hu, Josephina and Zhang, Hugh and Shi, Sean and Choi, Michael and Agrawal, Anish and Chopra, Arnav and others},
  year={2025},
  eprint={2501.14249},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2501.14249}, 
}

@misc{sky_t1,
  author       = {NovaSky Team},
  title        = {Sky-T1: Fully open-source reasoning model with o1-preview performance in \$450 budget},
  url = {https://novasky-ai.github.io/posts/sky-t1},
  note         = {Accessed: 2025-01-09},
  year         = {2025}
}

@misc{yang2024syntheticcontinuedpretraining,
      title={Synthetic continued pretraining}, 
      author={Zitong Yang and Neil Band and Shuangping Li and Emmanuel Candès and Tatsunori Hashimoto},
      year={2024},
      eprint={2409.07431},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2409.07431}, 
}

@misc{wang-etal-2024-math,
      title={Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations}, 
      author={Peiyi Wang and Lei Li and Zhihong Shao and R. X. Xu and Damai Dai and Yifei Li and Deli Chen and Y. Wu and Zhifang Sui},
      year={2024},
      eprint={2312.08935},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2312.08935}, 
}

@misc{lightman2023letsverifystepstep,
      title={Let's Verify Step by Step}, 
      author={Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
      year={2023},
      eprint={2305.20050},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.20050}, 
}

@misc{zhuo2024bigcodebenchbenchmarkingcodegeneration,
      title={BigCodeBench: Benchmarking Code Generation with Diverse Function Calls and Complex Instructions}, 
      author={Terry Yue Zhuo and Minh Chien Vu and Jenny Chim and Han Hu and Wenhao Yu and Ratnadira Widyasari and Imam Nur Bani Yusuf and Haolan Zhan and Junda He and Indraneil Paul and Simon Brunner and Chen Gong and Thong Hoang and Armel Randy Zebaze and Xiaoheng Hong and Wen-Ding Li and Jean Kaddour and Ming Xu and Zhihan Zhang and Prateek Yadav and Naman Jain and Alex Gu and Zhoujun Cheng and Jiawei Liu and Qian Liu and Zijian Wang and David Lo and Binyuan Hui and Niklas Muennighoff and Daniel Fried and Xiaoning Du and Harm de Vries and Leandro Von Werra},
      year={2024},
      eprint={2406.15877},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2406.15877}, 
}

@misc{lee2025evolvingdeeperllmthinking,
      title={Evolving Deeper LLM Thinking}, 
      author={Kuang-Huei Lee and Ian Fischer and Yueh-Hua Wu and Dave Marwood and Shumeet Baluja and Dale Schuurmans and Xinyun Chen},
      year={2025},
      eprint={2501.09891},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.09891}, 
}

@misc{yuan2025agentrtraininglanguagemodel,
      title={Agent-R: Training Language Model Agents to Reflect via Iterative Self-Training}, 
      author={Siyu Yuan and Zehui Chen and Zhiheng Xi and Junjie Ye and Zhengyin Du and Jiecao Chen},
      year={2025},
      eprint={2501.11425},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.11425}, 
}

@misc{wang2024drto1optimizeddeepreasoning,
      title={DRT-o1: Optimized Deep Reasoning Translation via Long Chain-of-Thought}, 
      author={Jiaan Wang and Fandong Meng and Yunlong Liang and Jie Zhou},
      year={2024},
      eprint={2412.17498},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.17498}, 
}


@misc{kim2024llmasaninterviewerstatictestingdynamic,
      title={LLM-as-an-Interviewer: Beyond Static Testing Through Dynamic LLM Evaluation}, 
      author={Eunsu Kim and Juyoung Suk and Seungone Kim and Niklas Muennighoff and Dongkwan Kim and Alice Oh},
      year={2024},
      eprint={2412.10424},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.10424}, 
}

@misc{yao2023reactsynergizingreasoningacting,
      title={ReAct: Synergizing Reasoning and Acting in Language Models}, 
      author={Shunyu Yao and Jeffrey Zhao and Dian Yu and Nan Du and Izhak Shafran and Karthik Narasimhan and Yuan Cao},
      year={2023},
      eprint={2210.03629},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.03629}, 
}

@misc{luo2025wizardmathempoweringmathematicalreasoning,
      title={WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct}, 
      author={Haipeng Luo and Qingfeng Sun and Can Xu and Pu Zhao and Jianguang Lou and Chongyang Tao and Xiubo Geng and Qingwei Lin and Shifeng Chen and Yansong Tang and Dongmei Zhang},
      year={2025},
      eprint={2308.09583},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.09583}, 
}

@misc{ankner2024critiqueoutloudrewardmodels,
      title={Critique-out-Loud Reward Models}, 
      author={Zachary Ankner and Mansheej Paul and Brandon Cui and Jonathan D. Chang and Prithviraj Ammanabrolu},
      year={2024},
      eprint={2408.11791},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.11791}, 
}

@misc{xie2024self,
      title={Self-Evaluation Guided Beam Search for Reasoning}, 
      author={Yuxi Xie and Kenji Kawaguchi and Yiran Zhao and Xu Zhao and Min-Yen Kan and Junxian He and Qizhe Xie},
      year={2023},
      eprint={2305.00633},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.00633}, 
}


@misc{choi2023kcts,
      title={KCTS: Knowledge-Constrained Tree Search Decoding with Token-Level Hallucination Detection}, 
      author={Sehyun Choi and Tianqing Fang and Zhaowei Wang and Yangqiu Song},
      year={2023},
      eprint={2310.09044},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.09044}, 
}

@misc{zhou2024languageagenttreesearch,
      title={Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models}, 
      author={Andy Zhou and Kai Yan and Michal Shlapentokh-Rothman and Haohan Wang and Yu-Xiong Wang},
      year={2024},
      eprint={2310.04406},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2310.04406}, 
}

@misc{zhang2023planninglargelanguagemodels,
      title={Planning with Large Language Models for Code Generation}, 
      author={Shun Zhang and Zhenfang Chen and Yikang Shen and Mingyu Ding and Joshua B. Tenenbaum and Chuang Gan},
      year={2023},
      eprint={2303.05510},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.05510}, 
}


@misc{liu2024dontthrowawayvalue,
      title={Don't throw away your value model! Generating more preferable text with Value-Guided Monte-Carlo Tree Search decoding}, 
      author={Jiacheng Liu and Andrew Cohen and Ramakanth Pasunuru and Yejin Choi and Hannaneh Hajishirzi and Asli Celikyilmaz},
      year={2024},
      eprint={2309.15028},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.15028}, 
}

@misc{snell2024scalingllmtesttimecompute,
      title={Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters}, 
      author={Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
      year={2024},
      eprint={2408.03314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.03314}, 
}

@misc{zhang2023cumulative,
      title={Cumulative Reasoning with Large Language Models}, 
      author={Yifan Zhang and Jingqin Yang and Yang Yuan and Andrew Chi-Chih Yao},
      year={2024},
      eprint={2308.04371},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.04371}, 
}

@misc{fu2022complexity,
      title={Complexity-Based Prompting for Multi-Step Reasoning}, 
      author={Yao Fu and Hao Peng and Ashish Sabharwal and Peter Clark and Tushar Khot},
      year={2023},
      eprint={2210.00720},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.00720}, 
}

@misc{bi2024program,
      title={When Do Program-of-Thoughts Work for Reasoning?}, 
      author={Zhen Bi and Ningyu Zhang and Yinuo Jiang and Shumin Deng and Guozhou Zheng and Huajun Chen},
      year={2023},
      eprint={2308.15452},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.15452}, 
}

@misc{hu2024visual,
      title={Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models}, 
      author={Yushi Hu and Weijia Shi and Xingyu Fu and Dan Roth and Mari Ostendorf and Luke Zettlemoyer and Noah A Smith and Ranjay Krishna},
      year={2024},
      eprint={2406.09403},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2406.09403}, 
}

@misc{yao2024tree,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10601}, 
}


@misc{wei2023chainofthoughtpromptingelicitsreasoning,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2201.11903}, 
}

@misc{yu2023metamath,
      title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}, 
      author={Longhui Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
      year={2024},
      eprint={2309.12284},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.12284}, 
}

@misc{muennighoff2024olmoeopenmixtureofexpertslanguage,
      title={OLMoE: Open Mixture-of-Experts Language Models}, 
      author={Niklas Muennighoff and Luca Soldaini and Dirk Groeneveld and Kyle Lo and Jacob Morrison and Sewon Min and Weijia Shi and Pete Walsh and Oyvind Tafjord and Nathan Lambert and Yuling Gu and Shane Arora and Akshita Bhagia and Dustin Schwenk and David Wadden and Alexander Wettig and Binyuan Hui and Tim Dettmers and Douwe Kiela and Ali Farhadi and Noah A. Smith and Pang Wei Koh and Amanpreet Singh and Hannaneh Hajishirzi},
      year={2024},
      eprint={2409.02060},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.02060}, 
}

@misc{wu2024thinkingllmsgeneralinstruction,
      title={Thinking LLMs: General Instruction Following with Thought Generation}, 
      author={Tianhao Wu and Janice Lan and Weizhe Yuan and Jiantao Jiao and Jason Weston and Sainbayar Sukhbaatar},
      year={2024},
      eprint={2410.10630},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.10630}, 
}

@misc{xiang20252reasoningllmslearning,
      title={Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought}, 
      author={Violet Xiang and Charlie Snell and Kanishk Gandhi and Alon Albalak and Anikait Singh and Chase Blagden and Duy Phung and Rafael Rafailov and Nathan Lile and Dakota Mahan and Louis Castricato and Jan-Philipp Franken and Nick Haber and Chelsea Finn},
      year={2025},
      eprint={2501.04682},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2501.04682}, 
}

@misc{glazer2024frontiermathbenchmarkevaluatingadvanced,
      title={FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI}, 
      author={Elliot Glazer and Ege Erdil and Tamay Besiroglu and Diego Chicharro and Evan Chen and Alex Gunning and Caroline Falkman Olsson and Jean-Stanislas Denain and Anson Ho and Emily de Oliveira Santos and Olli Järviniemi and Matthew Barnett and Robert Sandler and Matej Vrzala and Jaime Sevilla and Qiuyu Ren and Elizabeth Pratt and Lionel Levine and Grant Barkley and Natalie Stewart and Bogdan Grechuk and Tetiana Grechuk and Shreepranav Varma Enugandla and Mark Wildon},
      year={2024},
      eprint={2411.04872},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2411.04872}, 
}


@misc{irvine2023rewardingchatbotsrealworldengagement,
      title={Rewarding Chatbots for Real-World Engagement with Millions of Users}, 
      author={Robert Irvine and Douglas Boubert and Vyas Raina and Adian Liusie and Ziyi Zhu and Vineet Mudupalli and Aliaksei Korshuk and Zongyi Liu and Fritz Cremer and Valentin Assassi and Christie-Carol Beauchamp and Xiaoding Lu and Thomas Rialan and William Beauchamp},
      year={2023},
      eprint={2303.06135},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.06135}, 
}

@misc{wu2024inference,
      title={Inference Scaling Laws: An Empirical Analysis of Compute-Optimal Inference for Problem-Solving with Language Models}, 
      author={Yangzhen Wu and Zhiqing Sun and Shanda Li and Sean Welleck and Yiming Yang},
      year={2024},
      eprint={2408.00724},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.00724}, 
}


%%% Big Corps

@misc{qwq-32b-preview,
    title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
    url = {https://qwenlm.github.io/blog/qwq-32b-preview/},
    author = {Qwen Team},
    month = {November},
    year = {2024}
}

@misc{r1-lite-preview,
    title = {DeepSeek R1},
    url = {https://x.com/deepseek_ai/status/1859200141355536422},
    author = {DeepSeek Team},
    month = {November},
    year = {2024}
}

@misc{o1,
    title = {Learning to Reason with LLMs},
    url = {https://openai.com/index/learning-to-reason-with-llms/},
    author = {OpenAI},
    month = {September},
    year = {2024}
}


@misc{geminithinking,
    title = {Gemini 2.0 Flash Thinking Mode (gemini-2.0-flash-thinking-exp-1219)},
    url = {https://cloud.google.com/vertex-ai/generative-ai/docs/thinking-mode},
    author = {Google},
    month = {December},
    year = {2024}
}

@misc{guan2024deliberativealignmentreasoningenables,
      title={Deliberative Alignment: Reasoning Enables Safer Language Models}, 
      author={Melody Y. Guan and Manas Joglekar and Eric Wallace and Saachi Jain and Boaz Barak and Alec Heylar and Rachel Dias and Andrea Vallone and Hongyu Ren and Jason Wei and Hyung Won Chung and Sam Toyer and Johannes Heidecke and Alex Beutel and Amelia Glaese},
      year={2024},
      eprint={2412.16339},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.16339}, 
}

%%% Datasets

@misc{huang2024olympicarenabenchmarkingmultidisciplinecognitive,
      title={OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI}, 
      author={Zhen Huang and Zengzhi Wang and Shijie Xia and Xuefeng Li and Haoyang Zou and Ruijie Xu and Run-Ze Fan and Lyumanshan Ye and Ethan Chern and Yixin Ye and Yikai Zhang and Yuqing Yang and Ting Wu and Binjie Wang and Shichao Sun and Yang Xiao and Yiyuan Li and Fan Zhou and Steffi Chern and Yiwei Qin and Yan Ma and Jiadi Su and Yixiu Liu and Yuxiang Zheng and Shaoting Zhang and Dahua Lin and Yu Qiao and Pengfei Liu},
      year={2024},
      eprint={2406.12753},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.12753}, 
}

@misc{zhong2023agievalhumancentricbenchmarkevaluating,
      title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
      author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and Yaobo Liang and Shuai Lu and Yanlin Wang and Amin Saied and Weizhu Chen and Nan Duan},
      year={2023},
      eprint={2304.06364},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.06364}, 
}

@misc{ling2017programinductionrationalegeneration,
      title={Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems}, 
      author={Wang Ling and Dani Yogatama and Chris Dyer and Phil Blunsom},
      year={2017},
      eprint={1705.04146},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1705.04146}, 
}

@misc{liu2020logiqachallengedatasetmachine,
      title={LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning}, 
      author={Jian Liu and Leyang Cui and Hanmeng Liu and Dandan Huang and Yile Wang and Yue Zhang},
      year={2020},
      eprint={2007.08124},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2007.08124}, 
}

@misc{gao2024omnimathuniversalolympiadlevel,
      title={Omni-MATH: A Universal Olympiad Level Mathematic Benchmark For Large Language Models}, 
      author={Bofei Gao and Feifan Song and Zhe Yang and Zefan Cai and Yibo Miao and Qingxiu Dong and Lei Li and Chenghao Ma and Liang Chen and Runxin Xu and Zhengyang Tang and Benyou Wang and Daoguang Zan and Shanghaoran Quan and Ge Zhang and Lei Sha and Yichang Zhang and Xuancheng Ren and Tianyu Liu and Baobao Chang},
      year={2024},
      eprint={2410.07985},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.07985}, 
}

@misc{jain2024livecodebenchholisticcontaminationfree,
      title={LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code}, 
      author={Naman Jain and King Han and Alex Gu and Wen-Ding Li and Fanjia Yan and Tianjun Zhang and Sida Wang and Armando Solar-Lezama and Koushik Sen and Ion Stoica},
      year={2024},
      eprint={2403.07974},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2403.07974}, 
}

@misc{zhong2019jecqalegaldomainquestionanswering,
      title={JEC-QA: A Legal-Domain Question Answering Dataset}, 
      author={Haoxi Zhong and Chaojun Xiao and Cunchao Tu and Tianyang Zhang and Zhiyuan Liu and Maosong Sun},
      year={2019},
      eprint={1911.12011},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.12011}, 
}

@misc{wang2021lsatprogresschallengescomplex,
      title={From LSAT: The Progress and Challenges of Complex Reasoning}, 
      author={Siyuan Wang and Zhongkun Liu and Wanjun Zhong and Ming Zhou and Zhongyu Wei and Zhumin Chen and Nan Duan},
      year={2021},
      eprint={2108.00648},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2108.00648}, 
}



@misc{arora2023llmsadvancedenoughchallenging,
      title={Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models}, 
      author={Daman Arora and Himanshu Gaurav Singh and Mausam},
      year={2023},
      eprint={2305.15074},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.15074}, 
}

@misc{he2024olympiadbenchchallengingbenchmarkpromoting,
      title={OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems}, 
      author={Chaoqun He and Renjie Luo and Yuzhuo Bai and Shengding Hu and Zhen Leng Thai and Junhao Shen and Jinyi Hu and Xu Han and Yujie Huang and Yuxiang Zhang and Jie Liu and Lei Qi and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2402.14008},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14008}, 
}

@misc{sun2024scievalmultilevellargelanguage,
      title={SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research}, 
      author={Liangtai Sun and Yang Han and Zihan Zhao and Da Ma and Zhennan Shen and Baocai Chen and Lu Chen and Kai Yu},
      year={2024},
      eprint={2308.13149},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.13149}, 
}

@misc{chen2023theoremqatheoremdrivenquestionanswering,
      title={TheoremQA: A Theorem-driven Question Answering dataset}, 
      author={Wenhu Chen and Ming Yin and Max Ku and Pan Lu and Yixin Wan and Xueguang Ma and Jianyu Xu and Xinyi Wang and Tony Xia},
      year={2023},
      eprint={2305.12524},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.12524}, 
}

@misc{qwen2024qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2024},
      eprint={2412.15115},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.15115}, 
}

@misc{numina_math_datasets,
  author = {Jia LI and Edward Beeching and Lewis Tunstall and Ben Lipkin and Roman Soletskyi and Shengyi Costa Huang and Kashif Rasul and Longhui Yu and Albert Jiang and Ziju Shen and Zihan Qin and Bin Dong and Li Zhou and Yann Fleureau and Guillaume Lample and Stanislas Polu},
  title = {NuminaMath},
  year = {2024},
  publisher = {Numina},
  journal = {Hugging Face repository},
  url = {https://github.com/project-numina/aimo-progress-prize/blob/main/report/numina_dataset.pdf}
}


@misc{rein2023gpqagraduatelevelgoogleproofqa,
      title={GPQA: A Graduate-Level Google-Proof Q\&A Benchmark}, 
      author={David Rein and Betty Li Hou and Asa Cooper Stickland and Jackson Petty and Richard Yuanzhe Pang and Julien Dirani and Julian Michael and Samuel R. Bowman},
      year={2023},
      eprint={2311.12022},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.12022}, 
}

@misc{shi2024languagemodelssolveolympiad,
      title={Can Language Models Solve Olympiad Programming?}, 
      author={Quan Shi and Michael Tang and Karthik Narasimhan and Shunyu Yao},
      year={2024},
      eprint={2404.10952},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.10952}, 
}

%%% Other o1 replication approaches

@misc{gao2024interpretablecontrastivemontecarlo,
      title={Interpretable Contrastive Monte Carlo Tree Search Reasoning}, 
      author={Zitian Gao and Boye Niu and Xuzheng He and Haotian Xu and Hongzhang Liu and Aiwei Liu and Xuming Hu and Lijie Wen},
      year={2024},
      eprint={2410.01707},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.01707}, 
}

@misc{qin2024o1replicationjourneystrategic,
      title={O1 Replication Journey: A Strategic Progress Report -- Part 1}, 
      author={Yiwei Qin and Xuefeng Li and Haoyang Zou and Yixiu Liu and Shijie Xia and Zhen Huang and Yixin Ye and Weizhe Yuan and Hector Liu and Yuanzhi Li and Pengfei Liu},
      year={2024},
      eprint={2410.18982},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2410.18982}, 
}

@misc{huang2024o1replicationjourney,
      title={O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?}, 
      author={Zhen Huang and Haoyang Zou and Xuefeng Li and Yixiu Liu and Yuxiang Zheng and Ethan Chern and Shijie Xia and Yiwei Qin and Weizhe Yuan and Pengfei Liu},
      year={2024},
      eprint={2411.16489},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.16489}, 
}

@misc{huang2025o1replicationjourney,
      title={O1 Replication Journey -- Part 3: Inference-time Scaling for Medical Reasoning}, 
      author={Zhongzhen Huang and Gui Geng and Shengyi Hua and Zhen Huang and Haoyang Zou and Shaoting Zhang and Pengfei Liu and Xiaofan Zhang},
      year={2025},
      eprint={2501.06458},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.06458}, 
}

@misc{zhang2024o1codero1replicationcoding,
      title={o1-Coder: an o1 Replication for Coding}, 
      author={Yuxiang Zhang and Shangxi Wu and Yuqi Yang and Jiangming Shu and Jinlin Xiao and Chao Kong and Jitao Sang},
      year={2024},
      eprint={2412.00154},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2412.00154}, 
}

%%% Other o1 evaluations

@misc{zhang_o1_inference_scaling_laws,
  author = {Hugh Zhang and Celia Chen},
  title = {Test-Time Compute Scaling Laws},
  year = {2024},
  URL = {https://github.com/hughbzhang/o1_inference_scaling_laws},
}

@misc{zhong2024evaluationopenaio1opportunities,
      title={Evaluation of OpenAI o1: Opportunities and Challenges of AGI}, 
      author={Tianyang Zhong and Zhengliang Liu and Yi Pan and Yutong Zhang and Yifan Zhou and Shizhe Liang and Zihao Wu and Yanjun Lyu and Peng Shu and Xiaowei Yu and Chao Cao and Hanqi Jiang and Hanxu Chen and Yiwei Li and Junhao Chen and Huawen Hu and Yihen Liu and Huaqin Zhao and Shaochen Xu and Haixing Dai and Lin Zhao and Ruidong Zhang and Wei Zhao and Zhenyuan Yang and Jingyuan Chen and Peilong Wang and Wei Ruan and Hui Wang and Huan Zhao and Jing Zhang and Yiming Ren and Shihuan Qin and Tong Chen and Jiaxi Li and Arif Hassan Zidan and Afrar Jahin and Minheng Chen and Sichen Xia and Jason Holmes and Yan Zhuang and Jiaqi Wang and Bochen Xu and Weiran Xia and Jichao Yu and Kaibo Tang and Yaxuan Yang and Bolun Sun and Tao Yang and Guoyu Lu and Xianqiao Wang and Lilong Chai and He Li and Jin Lu and Lichao Sun and Xin Zhang and Bao Ge and Xintao Hu and Lian Zhang and Hua Zhou and Lu Zhang and Shu Zhang and Ninghao Liu and Bei Jiang and Linglong Kong and Zhen Xiang and Yudan Ren and Jun Liu and Xi Jiang and Yu Bao and Wei Zhang and Xiang Li and Gang Li and Wei Liu and Dinggang Shen and Andrea Sikora and Xiaoming Zhai and Dajiang Zhu and Tianming Liu},
      year={2024},
      eprint={2409.18486},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.18486}, 
}

@misc{latif2024openaio1outperformhumans,
      title={Can OpenAI o1 outperform humans in higher-order cognitive thinking?}, 
      author={Ehsan Latif and Yifan Zhou and Shuchen Guo and Lehong Shi and Yizhu Gao and Matthew Nyaaba and Arne Bewerdorff and Xiantong Yang and Xiaoming Zhai},
      year={2024},
      eprint={2412.05753},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2412.05753}, 
}

@misc{latif2024systematicassessmentopenaio1preview,
      title={A Systematic Assessment of OpenAI o1-Preview for Higher Order Thinking in Education}, 
      author={Ehsan Latif and Yifan Zhou and Shuchen Guo and Yizhu Gao and Lehong Shi and Matthew Nayaaba and Gyeonggeon Lee and Liang Zhang and Arne Bewersdorff and Luyang Fang and Xiantong Yang and Huaqin Zhao and Hanqi Jiang and Haoran Lu and Jiaxi Li and Jichao Yu and Weihang You and Zhengliang Liu and Vincent Shung Liu and Hui Wang and Zihao Wu and Jin Lu and Fei Dou and Ping Ma and Ninghao Liu and Tianming Liu and Xiaoming Zhai},
      year={2024},
      eprint={2410.21287},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2410.21287}, 
}

@misc{ristea2024aicyberriskbenchmark,
      title={AI Cyber Risk Benchmark: Automated Exploitation Capabilities}, 
      author={Dan Ristea and Vasilios Mavroudis and Chris Hicks},
      year={2024},
      eprint={2410.21939},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2410.21939}, 
}

%%% Common papers

@misc{kwon2023efficientmemorymanagementlarge,
      title={Efficient Memory Management for Large Language Model Serving with PagedAttention}, 
      author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
      year={2023},
      eprint={2309.06180},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.06180}, 
}

@misc{lewkowycz2022solvingquantitativereasoningproblems,
      title={Solving Quantitative Reasoning Problems with Language Models}, 
      author={Aitor Lewkowycz and Anders Andreassen and David Dohan and Ethan Dyer and Henryk Michalewski and Vinay Ramasesh and Ambrose Slone and Cem Anil and Imanol Schlag and Theo Gutman-Solo and Yuhuai Wu and Behnam Neyshabur and Guy Gur-Ari and Vedant Misra},
      year={2022},
      eprint={2206.14858},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2206.14858}, 
}

@misc{welleck2024decodingmetagenerationinferencetimealgorithms,
      title={From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models}, 
      author={Sean Welleck and Amanda Bertsch and Matthew Finlayson and Hailey Schoelkopf and Alex Xie and Graham Neubig and Ilia Kulikov and Zaid Harchaoui},
      year={2024},
      eprint={2406.16838},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.16838}, 
}

@article{DBLP:journals/corr/abs-2009-07118,
  author    = {Timo Schick and
               Hinrich Sch{\"{u}}tze},
  title     = {It's Not Just Size That Matters: Small Language Models Are Also Few-Shot
               Learners},
  journal   = {CoRR},
  volume    = {abs/2009.07118},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.07118},
  eprinttype = {arXiv},
  eprint    = {2009.07118},
  timestamp = {Fri, 18 Sep 2020 15:17:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-07118.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kasneci2023chatgpt,
  title={ChatGPT for good? On opportunities and challenges of large language models for education},
  author={Kasneci, Enkelejda and Se{\ss}ler, Kathrin and K{\"u}chemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and G{\"u}nnemann, Stephan and H{\"u}llermeier, Eyke and others},
  journal={Learning and individual differences},
  volume={103},
  pages={102274},
  year={2023},
  publisher={Elsevier}
}

@article{tay2021scale,
  title={Scale efficiently: Insights from pre-training and fine-tuning transformers},
  author={Tay, Yi and Dehghani, Mostafa and Rao, Jinfeng and Fedus, William and Abnar, Samira and Chung, Hyung Won and Narang, Sharan and Yogatama, Dani and Vaswani, Ashish and Metzler, Donald},
  journal={arXiv preprint arXiv:2109.10686},
  year={2021}
}

@article{alabdulmohsin2022revisiting,
  title={Revisiting neural scaling laws in language and vision},
  author={Alabdulmohsin, Ibrahim M and Neyshabur, Behnam and Zhai, Xiaohua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22300--22312},
  year={2022}
}

@misc{li2023starcoder,
      title={StarCoder: may the source be with you!}, 
      author={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and others},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{wenzek2019ccnet,
  title={CCNet: Extracting high quality monolingual datasets from web crawl data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Joulin, Armand and Grave, Edouard},
  journal={arXiv preprint arXiv:1911.00359},
  year={2019}
}

@misc{touvron2023llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yang2021tuning,
  title={Tuning large neural networks via zero-shot hyperparameter transfer},
  author={Yang, Ge and Hu, Edward and Babuschkin, Igor and Sidor, Szymon and Liu, Xiaodong and Farhi, David and Ryder, Nick and Pachocki, Jakub and Chen, Weizhu and Gao, Jianfeng},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17084--17097},
  year={2021}
}

@article{aghajanyan2023scaling,
  title={Scaling Laws for Generative Mixed-Modal Language Models},
  author={Aghajanyan, Armen and Yu, Lili and Conneau, Alexis and Hsu, Wei-Ning and Hambardzumyan, Karen and Zhang, Susan and Roller, Stephen and Goyal, Naman and Levy, Omer and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2301.03728},
  year={2023}
}

@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2021},
  number={12},
  pages={124003},
  year={2021},
  publisher={IOP Publishing}
}

@misc{neelakantan2022text,
      title={Text and Code Embeddings by Contrastive Pre-Training}, 
      author={Arvind Neelakantan and Tao Xu and Raul Puri and Alec Radford and Jesse Michael Han and Jerry Tworek and Qiming Yuan and Nikolas Tezak and Jong Wook Kim and Chris Hallacy and Johannes Heidecke and Pranav Shyam and Boris Power and Tyna Eloundou Nekoul and Girish Sastry and Gretchen Krueger and David Schnurr and Felipe Petroski Such and Kenny Hsu and Madeleine Thompson and Tabarak Khan and Toki Sherbakov and Joanne Jang and Peter Welinder and Lilian Weng},
      year={2022},
      eprint={2201.10005},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023angleoptimized,
      title={AnglE-optimized Text Embeddings}, 
      author={Xianming Li and Jing Li},
      year={2023},
      eprint={2309.12871},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023deelm,
      title={DeeLM: Dependency-enhanced Large Language Model for Sentence Embeddings}, 
      author={Xianming Li and Jing Li},
      year={2023},
      eprint={2311.05296},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2023scaling,
      title={Scaling Sentence Embeddings with Large Language Models}, 
      author={Ting Jiang and Shaohan Huang and Zhongzhi Luan and Deqing Wang and Fuzhen Zhuang},
      year={2023},
      eprint={2307.16645},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{arya1998optimal,
  title={An optimal algorithm for approximate nearest neighbor searching fixed dimensions},
  author={Arya, Sunil and Mount, David M and Netanyahu, Nathan S and Silverman, Ruth and Wu, Angela Y},
  journal={Journal of the ACM (JACM)},
  volume={45},
  number={6},
  pages={891--923},
  year={1998},
  publisher={ACM New York, NY, USA},
  url={https://graphics.stanford.edu/courses/cs468-06-fall/Papers/03%20AMNSW%20-%20JACM.pdf}
}

@misc{douze2024faiss,
      title={The Faiss library}, 
      author={Matthijs Douze and Alexandr Guzhva and Chengqi Deng and Jeff Johnson and Gergely Szilvasy and Pierre-Emmanuel Mazaré and Maria Lomeli and Lucas Hosseini and Hervé Jégou},
      year={2024},
      eprint={2401.08281},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{johnson2017billionscale,
      title={Billion-scale similarity search with GPUs}, 
      author={Jeff Johnson and Matthijs Douze and Hervé Jégou},
      year={2017},
      eprint={1702.08734},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{Filippova2013OvercomingTL,
  title={Overcoming the Lack of Parallel Data in Sentence Compression},
  author={Katja Filippova and Yasemin Altun},
  booktitle={Conference on Empirical Methods in Natural Language Processing},
  year={2013},
  url={https://api.semanticscholar.org/CorpusID:9751546}
}

@misc{Coster2011SimpleEW,
  title={Simple English Wikipedia: A New Text Simplification Task},
  author={William Coster and David Kauchak},
  booktitle={Annual Meeting of the Association for Computational Linguistics},
  year={2011},
  url={https://api.semanticscholar.org/CorpusID:9128245}
}

@misc{fabbri2021summeval,
      title={SummEval: Re-evaluating Summarization Evaluation}, 
      author={Alexander R. Fabbri and Wojciech Kryściński and Bryan McCann and Caiming Xiong and Richard Socher and Dragomir Radev},
      year={2021},
      eprint={2007.12626},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shen2022multilexsum,
      title={Multi-LexSum: Real-World Summaries of Civil Rights Lawsuits at Multiple Granularities}, 
      author={Zejiang Shen and Kyle Lo and Lauren Yu and Nathan Dahlberg and Margo Schlanger and Doug Downey},
      year={2022},
      eprint={2206.10883},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Hamborg2017newspleaseA,
  title={news-please - A Generic News Crawler and Extractor},
  author={Felix Hamborg and Norman Meuschke and Corinna Breitinger and Bela Gipp},
  booktitle={Intelligence and Security Informatics},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:5830937}
}

@misc{lewis2021paq,
      title={PAQ: 65 Million Probably-Asked Questions and What You Can Do With Them}, 
      author={Patrick Lewis and Yuxiang Wu and Linqing Liu and Pasquale Minervini and Heinrich Küttler and Aleksandra Piktus and Pontus Stenetorp and Sebastian Riedel},
      year={2021},
      eprint={2102.07033},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{levy2017zeroshot,
      title={Zero-Shot Relation Extraction via Reading Comprehension}, 
      author={Omer Levy and Minjoon Seo and Eunsol Choi and Luke Zettlemoyer},
      year={2017},
      eprint={1706.04115},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{keung2020multilingual,
      title={The Multilingual Amazon Reviews Corpus}, 
      author={Phillip Keung and Yichao Lu and György Szarvas and Noah A. Smith},
      year={2020},
      eprint={2010.02573},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{StackExchangeDataset,
  author = {Flax Sentence Embeddings Team},
  title = {Stack Exchange question pairs},
  year = {2021},
  url = {https://hf.co/datasets/flax-sentence-embeddings/}
}



@misc{NPR,
  author = {Sentence Transformers Team},
  title = {(Title, Body) pairs from the npr.org website},
  year = {2021},
  url = {https://hf.co/datasets/sentence-transformers/embedding-training-data}
}

@misc{Reddit,
  author = {Sentence Transformers Team},
  title = {Reddit Title Body},
  year = {2021},
  url = {https://hf.co/datasets/sentence-transformers/reddit-title-body}
}


@misc{chen2015microsoft,
      title={Microsoft COCO Captions: Data Collection and Evaluation Server}, 
      author={Xinlei Chen and Hao Fang and Tsung-Yi Lin and Ramakrishna Vedantam and Saurabh Gupta and Piotr Dollar and C. Lawrence Zitnick},
      year={2015},
      eprint={1504.00325},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{jin2019pubmedqa,
      title={PubMedQA: A Dataset for Biomedical Research Question Answering}, 
      author={Qiao Jin and Bhuwan Dhingra and Zhengping Liu and William W. Cohen and Xinghua Lu},
      year={2019},
      eprint={1909.06146},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{young2014image,
  title={From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
  author={Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
  journal={Transactions of the Association for Computational Linguistics},
  volume={2},
  pages={67--78},
  year={2014},
  publisher={MIT Press},
  url={https://aclanthology.org/Q14-1006}
}

@misc{ElSahar2018TRExAL,
  title={T-REx: A Large Scale Alignment of Natural Language with Knowledge Base Triples},
  author={Hady ElSahar and Pavlos Vougiouklis and Arslen Remaci and Christophe Gravier and Jonathon S. Hare and Fr{\'e}d{\'e}rique Laforest and Elena Paslaru Bontas Simperl},
  booktitle={International Conference on Language Resources and Evaluation},
  year={2018},
  url={https://api.semanticscholar.org/CorpusID:4612975}
}

@misc{cohan2020specter,
      title={SPECTER: Document-level Representation Learning using Citation-informed Transformers}, 
      author={Arman Cohan and Sergey Feldman and Iz Beltagy and Doug Downey and Daniel S. Weld},
      year={2020},
      eprint={2004.07180},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Fader2014OpenQA,
  title={Open question answering over curated and extracted knowledge bases},
  author={Anthony Fader and Luke Zettlemoyer and Oren Etzioni},
  journal={Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  year={2014},
  url={https://api.semanticscholar.org/CorpusID:207214527}
}

@misc{zhang2016characterlevel,
      title={Character-level Convolutional Networks for Text Classification}, 
      author={Xiang Zhang and Junbo Zhao and Yann LeCun},
      year={2016},
      eprint={1509.01626},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{khashabi2021gooaq,
      title={GooAQ: Open Question Answering with Diverse Answer Types}, 
      author={Daniel Khashabi and Amos Ng and Tushar Khot and Ashish Sabharwal and Hannaneh Hajishirzi and Chris Callison-Burch},
      year={2021},
      eprint={2104.08727},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{narayan2018dont,
      title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization}, 
      author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
      year={2018},
      eprint={1808.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hidey-mckeown-2016-identifying,
    title = "Identifying Causal Relations Using Parallel {W}ikipedia Articles",
    author = "Hidey, Christopher  and
      McKeown, Kathy",
    editor = "Erk, Katrin  and
      Smith, Noah A.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1135",
    doi = "10.18653/v1/P16-1135",
    pages = "1424--1433",
}

@misc{cachola2020tldr,
      title={TLDR: Extreme Summarization of Scientific Documents}, 
      author={Isabel Cachola and Kyle Lo and Arman Cohan and Daniel S. Weld},
      year={2020},
      eprint={2004.15011},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{DBLP:journals/corr/DunnSHGCC17,
    author    = {Matthew Dunn and
                Levent Sagun and
                Mike Higgins and
                V. Ugur G{"{u}}ney and
                Volkan Cirik and
                Kyunghyun Cho},
    title     = {SearchQA: {A} New Q{\&}A Dataset Augmented with Context from a
                Search Engine},
    journal   = {CoRR},
    volume    = {abs/1704.05179},
    year      = {2017},
    url       = {http://arxiv.org/abs/1704.05179},
    archivePrefix = {arXiv},
    eprint    = {1704.05179},
    timestamp = {Mon, 13 Aug 2018 16:47:09 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/DunnSHGCC17.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gupta2019amazonqa,
      title={AmazonQA: A Review-Based Question Answering Task}, 
      author={Mansi Gupta and Nitish Kulkarni and Raghuveer Chanda and Anirudha Rayasam and Zachary C Lipton},
      year={2019},
      eprint={1908.04364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{graff2003english,
  title={English gigaword},
  author={Graff, David and Kong, Junbo and Chen, Ke and Maeda, Kazuaki},
  journal={Linguistic Data Consortium, Philadelphia},
  volume={4},
  number={1},
  pages={34},
  year={2003},
  url={https://catalog.ldc.upenn.edu/LDC2011T07}
}

@misc{rush2015neural,
      title={A Neural Attention Model for Abstractive Sentence Summarization}, 
      author={Alexander M. Rush and Sumit Chopra and Jason Weston},
      year={2015},
      eprint={1509.00685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{koupaee2018wikihow,
      title={WikiHow: A Large Scale Text Summarization Dataset}, 
      author={Mahnaz Koupaee and William Yang Wang},
      year={2018},
      eprint={1810.09305},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pal2022medmcqa,
      title={MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering}, 
      author={Ankit Pal and Logesh Kumar Umapathi and Malaikannan Sankarasubbu},
      year={2022},
      eprint={2203.14371},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{qiu2022dureaderretrieval,
      title={DuReader\_retrieval: A Large-scale Chinese Benchmark for Passage Retrieval from Web Search Engine}, 
      author={Yifu Qiu and Hongyu Li and Yingqi Qu and Ying Chen and Qiaoqiao She and Jing Liu and Hua Wu and Haifeng Wang},
      year={2022},
      eprint={2203.10232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{fan2019eli5,
      title={ELI5: Long Form Question Answering}, 
      author={Angela Fan and Yacine Jernite and Ethan Perez and David Grangier and Jason Weston and Michael Auli},
      year={2019},
      eprint={1907.09190},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jiang2024mixtral,
      title={Mixtral of Experts}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Antoine Roux and Arthur Mensch and Blanche Savary and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Emma Bou Hanna and Florian Bressand and Gianna Lengyel and Guillaume Bour and Guillaume Lample and Lélio Renard Lavaud and Lucile Saulnier and Marie-Anne Lachaux and Pierre Stock and Sandeep Subramanian and Sophia Yang and Szymon Antoniak and Teven Le Scao and Théophile Gervet and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2024},
      eprint={2401.04088},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{nogueira2020passage,
      title={Passage Re-ranking with BERT}, 
      author={Rodrigo Nogueira and Kyunghyun Cho},
      year={2020},
      eprint={1901.04085},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{su2017chatbot,
  title={A chatbot using LSTM-based multi-layer embedding for elderly care},
  author={Su, Ming-Hsiang and Wu, Chung-Hsien and Huang, Kun-Yi and Hong, Qian-Bei and Wang, Hsin-Min},
  booktitle={2017 International Conference on Orange Technologies (ICOT)},
  pages={70--74},
  year={2017},
  organization={IEEE},
  url={https://ieeexplore.ieee.org/document/8336091}
}

@misc{kaplan2020scalinglawsneurallanguage,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2001.08361}, 
}

@misc{singh2022flava,
      title={FLAVA: A Foundational Language And Vision Alignment Model}, 
      author={Amanpreet Singh and Ronghang Hu and Vedanuj Goswami and Guillaume Couairon and Wojciech Galuba and Marcus Rohrbach and Douwe Kiela},
      year={2022},
      eprint={2112.04482},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{cai2024surveymixtureexperts,
      title={A Survey on Mixture of Experts}, 
      author={Weilin Cai and Juyong Jiang and Fan Wang and Jing Tang and Sunghun Kim and Jiayi Huang},
      year={2024},
      eprint={2407.06204},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.06204}, 
}

@misc{duquenne2023sonar,
      title={SONAR: Sentence-Level Multimodal and Language-Agnostic Representations}, 
      author={Paul-Ambroise Duquenne and Holger Schwenk and Benoît Sagot},
      year={2023},
      eprint={2308.11466},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rubin2022learning,
      title={Learning To Retrieve Prompts for In-Context Learning}, 
      author={Ohad Rubin and Jonathan Herzig and Jonathan Berant},
      year={2022},
      eprint={2112.08633},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{min2022rethinking,
      title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?}, 
      author={Sewon Min and Xinxi Lyu and Ari Holtzman and Mikel Artetxe and Mike Lewis and Hannaneh Hajishirzi and Luke Zettlemoyer},
      year={2022},
      eprint={2202.12837},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xia2023sheared,
      title={Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning}, 
      author={Mengzhou Xia and Tianyu Gao and Zhiyuan Zeng and Danqi Chen},
      year={2023},
      eprint={2310.06694},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{no_robots,
  author = {Nazneen Rajani and Lewis Tunstall and Edward Beeching and Nathan Lambert and Alexander M. Rush and Thomas Wolf},
  title = {No Robots},
  year = {2023},
  publisher = {Hugging Face},
  journal = {Hugging Face repository},
  url ={https://huggingface.co/datasets/HuggingFaceH4/no_robots}
}

@article{zheng2024opencodeinterpreter,
  title={Opencodeinterpreter: Integrating code generation with execution and refinement},
  author={Zheng, Tianyu and Zhang, Ge and Shen, Tianhao and Liu, Xueling and Lin, Bill Yuchen and Fu, Jie and Chen, Wenhu and Yue, Xiang},
  journal={arXiv preprint arXiv:2402.14658},
  year={2024}
}
@misc{yu2024metamathbootstrapmathematicalquestions,
      title={MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models}, 
      author={Longhui Yu and Weisen Jiang and Han Shi and Jincheng Yu and Zhengying Liu and Yu Zhang and James T. Kwok and Zhenguo Li and Adrian Weller and Weiyang Liu},
      year={2024},
      eprint={2309.12284},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.12284}, 
}

@misc{rafailov2023direct,
      title={Direct Preference Optimization: Your Language Model is Secretly a Reward Model}, 
      author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Stefano Ermon and Christopher D. Manning and Chelsea Finn},
      year={2023},
      eprint={2305.18290},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hinton2015distilling,
      title={Distilling the Knowledge in a Neural Network}, 
      author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
      year={2015},
      eprint={1503.02531},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1503.02531}, 
}

@misc{xiao2022retromae,
      title={RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder}, 
      author={Shitao Xiao and Zheng Liu and Yingxia Shao and Zhao Cao},
      year={2022},
      eprint={2205.12035},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xiao2022retromae2,
      title={RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models}, 
      author={Shitao Xiao and Zheng Liu},
      year={2022},
      eprint={2211.08769},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2024unimoescalingunifiedmultimodal,
      title={Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts}, 
      author={Yunxin Li and Shenyuan Jiang and Baotian Hu and Longyue Wang and Wanqi Zhong and Wenhan Luo and Lin Ma and Min Zhang},
      year={2024},
      eprint={2405.11273},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.11273}, 
}

@misc{radford2022robustspeechrecognitionlargescale,
      title={Robust Speech Recognition via Large-Scale Weak Supervision}, 
      author={Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
      year={2022},
      eprint={2212.04356},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2212.04356}, 
}

@misc{driess2023palmeembodiedmultimodallanguage,
      title={PaLM-E: An Embodied Multimodal Language Model}, 
      author={Danny Driess and Fei Xia and Mehdi S. M. Sajjadi and Corey Lynch and Aakanksha Chowdhery and Brian Ichter and Ayzaan Wahid and Jonathan Tompson and Quan Vuong and Tianhe Yu and Wenlong Huang and Yevgen Chebotar and Pierre Sermanet and Daniel Duckworth and Sergey Levine and Vincent Vanhoucke and Karol Hausman and Marc Toussaint and Klaus Greff and Andy Zeng and Igor Mordatch and Pete Florence},
      year={2023},
      eprint={2303.03378},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.03378}, 
}

@misc{bisk2019piqareasoningphysicalcommonsense,
      title={PIQA: Reasoning about Physical Commonsense in Natural Language}, 
      author={Yonatan Bisk and Rowan Zellers and Ronan Le Bras and Jianfeng Gao and Yejin Choi},
      year={2019},
      eprint={1911.11641},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.11641}, 
}

@misc{gu2024olmesstandardlanguagemodel,
      title={OLMES: A Standard for Language Model Evaluations}, 
      author={Yuling Gu and Oyvind Tafjord and Bailey Kuehl and Dany Haddad and Jesse Dodge and Hannaneh Hajishirzi},
      year={2024},
      eprint={2406.08446},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.08446}, 
}

@misc{magnusson2023palomabenchmarkevaluatinglanguage,
      title={Paloma: A Benchmark for Evaluating Language Model Fit}, 
      author={Ian Magnusson and Akshita Bhagia and Valentin Hofmann and Luca Soldaini and Ananya Harsh Jha and Oyvind Tafjord and Dustin Schwenk and Evan Pete Walsh and Yanai Elazar and Kyle Lo and Dirk Groeneveld and Iz Beltagy and Hannaneh Hajishirzi and Noah A. Smith and Kyle Richardson and Jesse Dodge},
      year={2023},
      eprint={2312.10523},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10523}, 
}

@misc{shen2023scalingvisionlanguagemodelssparse,
      title={Scaling Vision-Language Models with Sparse Mixture of Experts}, 
      author={Sheng Shen and Zhewei Yao and Chunyuan Li and Trevor Darrell and Kurt Keutzer and Yuxiong He},
      year={2023},
      eprint={2303.07226},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.07226}, 
}

@misc{lin2024moellavamixtureexpertslarge,
      title={MoE-LLaVA: Mixture of Experts for Large Vision-Language Models}, 
      author={Bin Lin and Zhenyu Tang and Yang Ye and Jiaxi Cui and Bin Zhu and Peng Jin and Jinfa Huang and Junwu Zhang and Yatian Pang and Munan Ning and Li Yuan},
      year={2024},
      eprint={2401.15947},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.15947}, 
}

@misc{geminiteam2023gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Yonghui Wu and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and others},
      year={2023},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pradeep2023rankvicuna,
      title={RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models}, 
      author={Ronak Pradeep and Sahel Sharifymoghaddam and Jimmy Lin},
      year={2023},
      eprint={2309.15088},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{pradeep2023rankzephyr,
      title={RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!}, 
      author={Ronak Pradeep and Sahel Sharifymoghaddam and Jimmy Lin},
      year={2023},
      eprint={2312.02724},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{ma2023zeroshot,
      title={Zero-Shot Listwise Document Reranking with a Large Language Model}, 
      author={Xueguang Ma and Xinyu Zhang and Ronak Pradeep and Jimmy Lin},
      year={2023},
      eprint={2305.02156},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{sun2023chatgpt,
      title={Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents}, 
      author={Weiwei Sun and Lingyong Yan and Xinyu Ma and Shuaiqiang Wang and Pengjie Ren and Zhumin Chen and Dawei Yin and Zhaochun Ren},
      year={2023},
      eprint={2304.09542},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ding2023enhancing,
      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, 
      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
      year={2023},
      eprint={2305.14233},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gao2021scaling,
      title={Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup}, 
      author={Luyu Gao and Yunyi Zhang and Jiawei Han and Jamie Callan},
      year={2021},
      eprint={2101.06983},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2023chatgpts,
      title={How is ChatGPT's behavior changing over time?}, 
      author={Lingjiao Chen and Matei Zaharia and James Zou},
      year={2023},
      eprint={2307.09009},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{christiano2023deep,
      title={Deep reinforcement learning from human preferences}, 
      author={Paul Christiano and Jan Leike and Tom B. Brown and Miljan Martic and Shane Legg and Dario Amodei},
      year={2023},
      eprint={1706.03741},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and others},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{luukkonen2023fingpt,
      title={FinGPT: Large Generative Models for a Small Language}, 
      author={Risto Luukkonen and Ville Komulainen and Jouni Luoma and Anni Eskelinen and Jenna Kanerva and Hanna-Mari Kupari and Filip Ginter and Veronika Laippala and Niklas Muennighoff and Aleksandra Piktus and Thomas Wang and Nouamane Tazi and Teven Le Scao and Thomas Wolf and Osma Suominen and Samuli Sairanen and Mikko Merioksa and Jyrki Heinonen and Aija Vahtola and Samuel Antao and Sampo Pyysalo},
      year={2023},
      eprint={2311.05640},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hernandez2022scaling,
  title={Scaling Laws and Interpretability of Learning from Repeated Data},
  author={Hernandez, Danny and Brown, Tom and Conerly, Tom and DasSarma, Nova and Drain, Dawn and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Henighan, Tom and Hume, Tristan and others},
  journal={arXiv preprint arXiv:2205.10487},
  year={2022}
}

@article{DBLP:journals/corr/abs-2001-07676,
  author    = {Timo Schick and
               Hinrich Sch{\"{u}}tze},
  title     = {Exploiting Cloze Questions for Few-Shot Text Classification and Natural
               Language Inference},
  journal   = {CoRR},
  volume    = {abs/2001.07676},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.07676},
  eprinttype = {arXiv},
  eprint    = {2001.07676},
  timestamp = {Fri, 24 Jan 2020 15:00:57 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2001-07676.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{sanh2022multitask,
      title={Multitask Prompted Training Enables Zero-Shot Task Generalization}, 
      author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and others},
      year={2022},
      eprint={2110.08207},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{DBLP:journals/corr/abs-2202-01279,
  author    = {Stephen H. Bach and
               Victor Sanh and
               Zheng Xin Yong and
               Albert Webson and
               Colin Raffel and
               Nihal V. Nayak and
               Abheesht Sharma and
               Taewoon Kim and
               M. Saiful Bari and
               Thibault F{\'{e}}vry and
               Zaid Alyafeai and
               Manan Dey and
               Andrea Santilli and
               Zhiqing Sun and
               Srulik Ben{-}David and
               Canwen Xu and
               Gunjan Chhablani and
               Han Wang and
               Jason Alan Fries and
               Maged Saeed AlShaibani and
               Shanya Sharma and
               Urmish Thakker and
               Khalid Almubarak and
               Xiangru Tang and
               Dragomir Radev and
               Mike Tian{-}Jian Jiang and
               Alexander M. Rush},
  title     = {PromptSource: An Integrated Development Environment and Repository
               for Natural Language Prompts},
  journal   = {CoRR},
  volume    = {abs/2202.01279},
  year      = {2022},
  url       = {https://arxiv.org/abs/2202.01279},
  eprinttype = {arXiv},
  eprint    = {2202.01279},
  timestamp = {Fri, 18 Feb 2022 11:05:41 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2202-01279.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{unifiedqa,
  author    = {Daniel Khashabi and
               Tushar Khot and
               Ashish Sabharwal and
               Oyvind Tafjord and
               Peter Clark and
               Hannaneh Hajishirzi},
  title     = {UnifiedQA: Crossing Format Boundaries With a Single {QA} System},
  journal   = {CoRR},
  volume    = {abs/2005.00700},
  year      = {2020},
  url       = {https://arxiv.org/abs/2005.00700},
  eprinttype = {arXiv},
  eprint    = {2005.00700},
  timestamp = {Fri, 08 May 2020 15:04:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2005-00700.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ye2021crossfit,
  title={CrossFit: A Few-shot Learning Challenge for Cross-task Generalization in NLP},
  author={Ye, Qinyuan and Lin, Bill Yuchen and Ren, Xiang},
  journal={arXiv preprint arXiv:2104.08835},
  year={2021},
  url={https://arxiv.org/abs/2104.08835}
}

@misc{flan,
      title={Finetuned Language Models Are Zero-Shot Learners},
      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
      year={2021},
      eprint={2109.01652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{Vania2021ComparingTS,
  title={Comparing Test Sets with Item Response Theory},
  author={Clara Vania and Phu Mon Htut and William R. Huang and Dhara Mungra and Richard Yuanzhe Pang and Jason Phang and Haokun Liu and Kyunghyun Cho and Sam Bowman},
  booktitle={ACL/IJCNLP},
  year={2021}
}

@article{DBLP:journals/corr/abs-2104-04670,
  author    = {Ruiqi Zhong and
               Kristy Lee and
               Zheng Zhang and
               Dan Klein},
  title     = {Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections},
  journal   = {CoRR},
  volume    = {abs/2104.04670},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.04670},
  eprinttype = {arXiv},
  eprint    = {2104.04670},
  timestamp = {Mon, 19 Apr 2021 16:45:47 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-04670.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2009-03300,
  author    = {Dan Hendrycks and
               Collin Burns and
               Steven Basart and
               Andy Zou and
               Mantas Mazeika and
               Dawn Song and
               Jacob Steinhardt},
  title     = {Measuring Massive Multitask Language Understanding},
  journal   = {CoRR},
  volume    = {abs/2009.03300},
  year      = {2020},
  url       = {https://arxiv.org/abs/2009.03300},
  eprinttype = {arXiv},
  eprint    = {2009.03300},
  timestamp = {Thu, 17 Sep 2020 12:49:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2009-03300.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019},
  url={https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf}
}

@article{t5,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}

@article{DBLP:journals/corr/LebretGA16,
  author    = {R{\'{e}}mi Lebret and
               David Grangier and
               Michael Auli},
  title     = {Generating Text from Structured Data with Application to the Biography
               Domain},
  journal   = {CoRR},
  volume    = {abs/1603.07771},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.07771},
  archivePrefix = {arXiv},
  eprint    = {1603.07771},
  timestamp = {Mon, 13 Aug 2018 16:48:30 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LebretGA16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{warstadt2018neural,
    title={Neural Network Acceptability Judgments},
    author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
    journal={arXiv preprint arXiv:1805.12471},
    year={2018}
}

@article{allenai:quoref,
      author    = {Pradeep Dasigi and Nelson F. Liu and Ana Marasovic and Noah A. Smith and  Matt Gardner},
      title     = {Quoref: A Reading Comprehension Dataset with Questions Requiring Coreferential Reasoning},
      journal   = {arXiv:1908.05803v2 },
      year      = {2019},
}

@incollection{levesque_winograd_2012,
  abstract = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
  added-at = {2019-01-10T12:03:51.000+0100},
  address = {Rome, Italy},
  author = {Levesque, Hector J. and Davis, Ernest and Morgenstern, Leora},
  biburl = {https://www.bibsonomy.org/bibtex/2f2adaaa66a83d35ce30618142dcfdbd9/lepsky},
  booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Principles} of {Knowledge} {Representation} and {Reasoning}},
  interhash = {bf16118d357c0acb5b4463889e76beb4},
  intrahash = {f2adaaa66a83d35ce30618142dcfdbd9},
  isbn = {978-1-57735-560-1},
  keywords = {kuenstliche_intelligenz},
  pages = {552--561},
  publisher = {AAAI Press},
  series = {{KR}'12},
  timestamp = {2019-01-10T12:05:20.000+0100},
  title = {The {Winograd} {Schema} {Challenge}},
  url = {https://cs.nyu.edu/faculty/davise/papers/WSKR2012.pdf},
  urldate = {2019-01-06},
  year = 2012
}

@article{Narayan2018DontGM,
  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Shashi Narayan and Shay B. Cohen and Mirella Lapata},
  journal={ArXiv},
  year={2018},
  volume={abs/1808.08745}
}

@article{deMarneffe_Simons_Tonhauser_2019,
    title={The CommitmentBank: Investigating projection in naturally occurring discourse}, volume={23}, url={https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601}, DOI={10.18148/sub/2019.v23i2.601},
    abstractNote={&lt;p&gt;This paper describes a new resource, the CommitmentBank, developed for the empirical investigation of the projection of finite clausal complements. A clausal complement is said to project when its content is understood as a commitment of the speaker even though the clause occurs under the scope of an entailment canceling operator such as negation or a question. The study of projection is therefore part of the study of commitments expressed by speakers to non-asserted sentence content. The content of clausal complements has been a central case for the study of projection, as there is a long-standing claim that clause-taking predicates fall into two classes—factives and nonfactives—distinguished on the basis of whether the contents of their complements project. This claim identifies the embedding predicate as the primary determinant of the projection behavior of these contents. The CommitmentBank is a corpus of naturally occurring discourses whose final sentence contains a clause-embedding predicate under an entailment canceling operator. In this paper, we describe the CommitmentBank and present initial results of analyses designed to evaluate the factive/nonfactive distinction and to investigate additional factors which affect the projectivity of clausal complements.&lt;/p&#38;gt;},
    number={2},
    journal={Proceedings of Sinn und Bedeutung},
    author={de Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
    year={2019},
    month={Jul.},
    pages={107-124}
}

@misc{zellers2019hellaswagmachinereallyfinish,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?}, 
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.07830}, 
}

@misc{sakaguchi2019winograndeadversarialwinogradschema,
      title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
      author={Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
      year={2019},
      eprint={1907.10641},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.10641}, 
}

@misc{merity2016pointersentinelmixturemodels,
      title={Pointer Sentinel Mixture Models}, 
      author={Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
      year={2016},
      eprint={1609.07843},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1609.07843}, 
}

@misc{liang2023holisticevaluationlanguagemodels,
      title={Holistic Evaluation of Language Models}, 
      author={Percy Liang and Rishi Bommasani and Tony Lee and Dimitris Tsipras and Dilara Soylu and Michihiro Yasunaga and Yian Zhang and Deepak Narayanan and Yuhuai Wu and Ananya Kumar and Benjamin Newman and Binhang Yuan and Bobby Yan and Ce Zhang and Christian Cosgrove and Christopher D. Manning and Christopher Ré and Diana Acosta-Navas and Drew A. Hudson and Eric Zelikman and Esin Durmus and Faisal Ladhak and Frieda Rong and Hongyu Ren and Huaxiu Yao and Jue Wang and Keshav Santhanam and Laurel Orr and Lucia Zheng and Mert Yuksekgonul and Mirac Suzgun and Nathan Kim and Neel Guha and Niladri Chatterji and Omar Khattab and Peter Henderson and Qian Huang and Ryan Chi and Sang Michael Xie and Shibani Santurkar and Surya Ganguli and Tatsunori Hashimoto and Thomas Icard and Tianyi Zhang and Vishrav Chaudhary and William Wang and Xuechen Li and Yifan Mai and Yuhui Zhang and Yuta Koreeda},
      year={2023},
      eprint={2211.09110},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.09110}, 
}

@misc{reid2022m2d2massivelymultidomainlanguage,
      title={M2D2: A Massively Multi-domain Language Modeling Dataset}, 
      author={Machel Reid and Victor Zhong and Suchin Gururangan and Luke Zettlemoyer},
      year={2022},
      eprint={2210.07370},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.07370}, 
}

@inproceedings{zellers2018swagaf,
    title={SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference},
    author={Zellers, Rowan and Bisk, Yonatan and Schwartz, Roy and Choi, Yejin},
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year={2018}
}

@article{lhoest2021datasets,
      title={Datasets: A Community Library for Natural Language Processing},
      author={Quentin Lhoest and Albert Villanova del Moral and Yacine Jernite and Abhishek Thakur and Patrick von Platen and Suraj Patil and Julien Chaumond and Mariama Drame and Julien Plu and Lewis Tunstall and Joe Davison and Mario Šaško and Gunjan Chhablani and Bhavitvya Malik and Simon Brandeis and Teven Le Scao and Victor Sanh and Canwen Xu and Nicolas Patry and Angelina McMillan-Major and Philipp Schmid and Sylvain Gugger and Clément Delangue and Théo Matussière and Lysandre Debut and Stas Bekman and Pierric Cistac and Thibault Goehringer and Victor Mustar and François Lagunas and Alexander M. Rush and Thomas Wolf},
      year={2021},
      eprint={2109.02846},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      journal={emnlp}
}

@article{DBLP:journals/ml/Caruana97,
  author    = {Rich Caruana},
  title     = {Multitask Learning},
  journal   = {Mach. Learn.},
  volume    = {28},
  number    = {1},
  pages     = {41--75},
  year      = {1997},
  url       = {https://doi.org/10.1023/A:1007379606734},
  doi       = {10.1023/A:1007379606734},
  timestamp = {Mon, 02 Mar 2020 16:28:56 +0100},
  biburl    = {https://dblp.org/rec/journals/ml/Caruana97.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/icml/CollobertW08,
  author    = {Ronan Collobert and
               Jason Weston},
  editor    = {William W. Cohen and
               Andrew McCallum and
               Sam T. Roweis},
  title     = {A unified architecture for natural language processing: deep neural
               networks with multitask learning},
  booktitle = {Machine Learning, Proceedings of the Twenty-Fifth International Conference
               {(ICML} 2008), Helsinki, Filnand, June 5-9, 2008},
  series    = {{ACM} International Conference Proceeding Series},
  volume    = {307},
  pages     = {160--167},
  publisher = {{ACM}},
  year      = {2008},
  url       = {https://doi.org/10.1145/1390156.1390177},
  doi       = {10.1145/1390156.1390177},
  timestamp = {Wed, 28 Nov 2018 12:57:16 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/CollobertW08.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HashimotoXTS16,
  author    = {Kazuma Hashimoto and
               Caiming Xiong and
               Yoshimasa Tsuruoka and
               Richard Socher},
  title     = {A Joint Many-Task Model: Growing a Neural Network for Multiple {NLP}
               Tasks},
  journal   = {CoRR},
  volume    = {abs/1611.015collin87},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.01587},
  eprinttype = {arXiv},
  eprint    = {1611.01587},
  timestamp = {Mon, 13 Aug 2018 16:46:50 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HashimotoXTS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1806-08730,
  author    = {Bryan McCann and
               Nitish Shirish Keskar and
               Caiming Xiong and
               Richard Socher},
  title     = {The Natural Language Decathlon: Multitask Learning as Question Answering},
  journal   = {CoRR},
  volume    = {abs/1806.08730},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.08730},
  eprinttype = {arXiv},
  eprint    = {1806.08730},
  timestamp = {Mon, 13 Aug 2018 16:49:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-08730.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{master-translator,
  author    = {Laria Reynolds and
               Kyle McDonell},
  title     = {Prompt Programming for Large Language Models: Beyond the Few-Shot
               Paradigm},
  journal   = {CoRR},
  volume    = {abs/2102.07350},
  year      = {2021},
  url       = {https://arxiv.org/abs/2102.07350},
  eprinttype = {arXiv},
  eprint    = {2102.07350},
  timestamp = {Thu, 18 Feb 2021 15:26:00 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2102-07350.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2104-14690,
  author    = {Sinong Wang and
               Han Fang and
               Madian Khabsa and
               Hanzi Mao and
               Hao Ma},
  title     = {Entailment as Few-Shot Learner},
  journal   = {CoRR},
  volume    = {abs/2104.14690},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.14690},
  eprinttype = {arXiv},
  eprint    = {2104.14690},
  timestamp = {Tue, 04 May 2021 15:12:43 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-14690.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{kim2021changes,
  title={What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers},
  author={Kim, Boseop and Kim, HyoungSeok and Lee, Sang-Woo and Lee, Gichang and Kwak, Donghyun and Jeon, Dong Hyeon and Park, Sunghyun and Kim, Sungju and Kim, Seonhoon and Seo, Dongpil and others},
  journal={arXiv preprint arXiv:2109.04650},
  year={2021}
}

@article{dodge2021documenting,
  title={Documenting the english colossal clean crawled corpus},
  author={Dodge, Jesse and Sap, Maarten and Marasovic, Ana and Agnew, William and Ilharco, Gabriel and Groeneveld, Dirk and Gardner, Matt},
  journal={arXiv preprint arXiv:2104.08758},
  year={2021}
}

@article{bandy2021addressing,
  title={Addressing" Documentation Debt" in Machine Learning Research: A Retrospective Datasheet for BookCorpus},
  author={Bandy, Jack and Vincent, Nicholas},
  journal={arXiv preprint arXiv:2105.05241},
  year={2021}
}

@article{gao2020pile,
  title={The {P}ile: An 800{GB} Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{black2021gpt,
  title={GPT-Neo: Large scale autoregressive language modeling with mesh-tensorflow},
  author={Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  journal={If you use this software, please cite it using these metadata},
  volume={58},
  year={2021}
}

@misc{wang2021gpt,
  title={GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model},
  author={Wang, Ben and Komatsuzaki, Aran},
  year={2021},
  url={https://github.com/kingoflolz/mesh-transformer-jax}
}

@misc{longpre2023dataprovenanceinitiativelarge,
      title={The Data Provenance Initiative: A Large Scale Audit of Dataset Licensing \& Attribution in AI}, 
      author={Shayne Longpre and Robert Mahari and Anthony Chen and Naana Obeng-Marnu and Damien Sileo and William Brannon and Niklas Muennighoff and Nathan Khazam and Jad Kabbara and Kartik Perisetla and Xinyi Wu and Enrico Shippole and Kurt Bollacker and Tongshuang Wu and Luis Villa and Sandy Pentland and Sara Hooker},
      year={2023},
      eprint={2310.16787},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.16787}, 
}

@misc{son2024kmmlumeasuringmassivemultitask,
      title={KMMLU: Measuring Massive Multitask Language Understanding in Korean}, 
      author={Guijin Son and Hanwool Lee and Sungdong Kim and Seungone Kim and Niklas Muennighoff and Taekyoon Choi and Cheonbok Park and Kang Min Yoo and Stella Biderman},
      year={2024},
      eprint={2402.11548},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.11548}, 
}

@misc{yong2023bloom1addinglanguagesupport,
      title={BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting}, 
      author={Zheng-Xin Yong and Hailey Schoelkopf and Niklas Muennighoff and Alham Fikri Aji and David Ifeoluwa Adelani and Khalid Almubarak and M Saiful Bari and Lintang Sutawika and Jungo Kasai and Ahmed Baruwa and Genta Indra Winata and Stella Biderman and Edward Raff and Dragomir Radev and Vassilina Nikoulina},
      year={2023},
      eprint={2212.09535},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.09535}, 
}

@misc{albalak2024surveydataselectionlanguage,
      title={A Survey on Data Selection for Language Models}, 
      author={Alon Albalak and Yanai Elazar and Sang Michael Xie and Shayne Longpre and Nathan Lambert and Xinyi Wang and Niklas Muennighoff and Bairu Hou and Liangming Pan and Haewon Jeong and Colin Raffel and Shiyu Chang and Tatsunori Hashimoto and William Yang Wang},
      year={2024},
      eprint={2402.16827},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16827}, 
}

@misc{mustafa2022multimodalcontrastivelearninglimoe,
      title={Multimodal Contrastive Learning with LIMoE: the Language-Image Mixture of Experts}, 
      author={Basil Mustafa and Carlos Riquelme and Joan Puigcerver and Rodolphe Jenatton and Neil Houlsby},
      year={2022},
      eprint={2206.02770},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2206.02770}, 
}

@misc{hong2024orpomonolithicpreferenceoptimization,
      title={ORPO: Monolithic Preference Optimization without Reference Model}, 
      author={Jiwoo Hong and Noah Lee and James Thorne},
      year={2024},
      eprint={2403.07691},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.07691}, 
}

@misc{meng2024simposimplepreferenceoptimization,
      title={SimPO: Simple Preference Optimization with a Reference-Free Reward}, 
      author={Yu Meng and Mengzhou Xia and Danqi Chen},
      year={2024},
      eprint={2405.14734},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.14734}, 
}

@misc{xu2024benchmarkdatacontaminationlarge,
      title={Benchmark Data Contamination of Large Language Models: A Survey}, 
      author={Cheng Xu and Shuhao Guan and Derek Greene and M-Tahar Kechadi},
      year={2024},
      eprint={2406.04244},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.04244}, 
}

@misc{peS2o,
    author = {Luca Soldaini and Kyle Lo},
    year = 2023,
    title = {{peS2o (Pretraining Efficiently on S2ORC) Dataset}},
    institution = {{Allen Institute for AI}},
    url = {https://github.com/allenai/pes2o}
}

@misc{Luccioni_2024, 
   series={FAccT ’24},
   title={Power Hungry Processing: Watts Driving the Cost of AI Deployment?},
   url={http://dx.doi.org/10.1145/3630106.3658542},
   DOI={10.1145/3630106.3658542},
   booktitle={The 2024 ACM Conference on Fairness, Accountability, and Transparency},
   publisher={ACM},
   author={Luccioni, Sasha and Jernite, Yacine and Strubell, Emma},
   year={2024},
   month=jun, collection={FAccT ’24} }


@misc{sukhbaatar2024branchtrainmixmixingexpertllms,
      title={Branch-Train-MiX: Mixing Expert LLMs into a Mixture-of-Experts LLM}, 
      author={Sainbayar Sukhbaatar and Olga Golovneva and Vasu Sharma and Hu Xu and Xi Victoria Lin and Baptiste Rozière and Jacob Kahn and Daniel Li and Wen-tau Yih and Jason Weston and Xian Li},
      year={2024},
      eprint={2403.07816},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.07816}, 
}

@misc{li2022branchtrainmergeembarrassinglyparalleltraining,
      title={Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models}, 
      author={Margaret Li and Suchin Gururangan and Tim Dettmers and Mike Lewis and Tim Althoff and Noah A. Smith and Luke Zettlemoyer},
      year={2022},
      eprint={2208.03306},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.03306}, 
}

@misc{zadouri2023pushingmixtureexpertslimit,
      title={Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning}, 
      author={Ted Zadouri and Ahmet Üstün and Arash Ahmadian and Beyza Ermiş and Acyr Locatelli and Sara Hooker},
      year={2023},
      eprint={2309.05444},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.05444}, 
}

@misc{ren2023pangusigmatrillionparameterlanguage,
      title={PanGu-Sigma: Towards Trillion Parameter Language Model with Sparse Heterogeneous Computing},
      author={Xiaozhe Ren and Pingyi Zhou and Xinfan Meng and Xinjing Huang and Yadao Wang and Weichao Wang and Pengfei Li and Xiaoda Zhang and Alexander Podolskiy and Grigory Arshinov and Andrey Bout and Irina Piontkovskaya and Jiansheng Wei and Xin Jiang and Teng Su and Qun Liu and Jun Yao},
      year={2023},
      eprint={2303.10845},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10845}, 
}

@misc{muqeeth2024softmergingexpertsadaptive,
      title={Soft Merging of Experts with Adaptive Routing}, 
      author={Mohammed Muqeeth and Haokun Liu and Colin Raffel},
      year={2024},
      eprint={2306.03745},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.03745}, 
}

@misc{wu2024omnismolaboostinggeneralistmultimodal,
      title={Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts}, 
      author={Jialin Wu and Xia Hu and Yaqing Wang and Bo Pang and Radu Soricut},
      year={2024},
      eprint={2312.00968},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.00968}, 
}

@misc{mckinzie2024mm1methodsanalysis,
      title={MM1: Methods, Analysis \& Insights from Multimodal LLM Pre-training}, 
      author={Brandon McKinzie and Zhe Gan and Jean-Philippe Fauconnier and Sam Dodge and Bowen Zhang and Philipp Dufter and Dhruti Shah and Xianzhi Du and Futang Peng and Floris Weers and Anton Belyi and Haotian Zhang and Karanjeet Singh and Doug Kang and Ankur Jain and Hongyu Hè and Max Schwarzer and Tom Gunter and Xiang Kong and Aonan Zhang and Jianyu Wang and Chong Wang and Nan Du and Tao Lei and Sam Wiseman and Guoli Yin and Mark Lee and Zirui Wang and Ruoming Pang and Peter Grasch and Alexander Toshev and Yinfei Yang},
      year={2024},
      eprint={2403.09611},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.09611}, 
}

@misc{wu2024yuan20m32mixtureexperts,
      title={Yuan 2.0-M32: Mixture of Experts with Attention Router}, 
      author={Shaohua Wu and Jiangang Luo and Xi Chen and Lingjun Li and Xudong Zhao and Tong Yu and Chao Wang and Yue Wang and Fei Wang and Weixu Qiao and Houbo He and Zeru Zhang and Zeyu Sun and Junxiong Mao and Chong Shen},
      year={2024},
      eprint={2405.17976},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2405.17976}, 
}

@misc{pan2024densetrainingsparseinference,
      title={Dense Training, Sparse Inference: Rethinking Training of Mixture-of-Experts Language Models}, 
      author={Bowen Pan and Yikang Shen and Haokun Liu and Mayank Mishra and Gaoyuan Zhang and Aude Oliva and Colin Raffel and Rameswar Panda},
      year={2024},
      eprint={2404.05567},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.05567}, 
}

@misc{bengio2016conditionalcomputationneuralnetworks,
      title={Conditional Computation in Neural Networks for faster models}, 
      author={Emmanuel Bengio and Pierre-Luc Bacon and Joelle Pineau and Doina Precup},
      year={2016},
      eprint={1511.06297},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1511.06297}, 
}

@misc{eigen2014learningfactoredrepresentationsdeep,
      title={Learning Factored Representations in a Deep Mixture of Experts}, 
      author={David Eigen and Marc'Aurelio Ranzato and Ilya Sutskever},
      year={2014},
      eprint={1312.4314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1312.4314}, 
}

@misc{MosaicML2023Introducing,
    title={Introducing MPT-7B: A New Standard for Open-Source, Commercially Usable LLMs},
    author={MosaicML NLP Team},
    year={2023},
    url={https://mosaicml.com/blog/mpt-7b}
}


@misc{tao2024scalinglawsvocabularylarger,
      title={Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies}, 
      author={Chaofan Tao and Qian Liu and Longxu Dou and Niklas Muennighoff and Zhongwei Wan and Ping Luo and Min Lin and Ngai Wong},
      year={2024},
      eprint={2407.13623},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.13623}, 
}

@misc{faysse2024croissantllmtrulybilingualfrenchenglish,
      title={CroissantLLM: A Truly Bilingual French-English Language Model}, 
      author={Manuel Faysse and Patrick Fernandes and Nuno M. Guerreiro and António Loison and Duarte M. Alves and Caio Corro and Nicolas Boizard and João Alves and Ricardo Rei and Pedro H. Martins and Antoni Bigata Casademunt and François Yvon and André F. T. Martins and Gautier Viaud and Céline Hudelot and Pierre Colombo},
      year={2024},
      eprint={2402.00786},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.00786}, 
}

@misc{enevoldsen2024scandinavianembeddingbenchmarkscomprehensive,
      title={The Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding}, 
      author={Kenneth Enevoldsen and Márton Kardos and Niklas Muennighoff and Kristoffer Laigaard Nielbo},
      year={2024},
      eprint={2406.02396},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.02396}, 
}

@misc{lovenia2024seacrowdmultilingualmultimodaldata,
      title={SEACrowd: A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages}, 
      author={Holy Lovenia and Rahmad Mahendra and Salsabil Maulana Akbar and Lester James V. Miranda and Jennifer Santoso and Elyanah Aco and Akhdan Fadhilah and Jonibek Mansurov and Joseph Marvin Imperial and Onno P. Kampman and Joel Ruben Antony Moniz and Muhammad Ravi Shulthan Habibi and Frederikus Hudi and Railey Montalan and Ryan Ignatius and Joanito Agili Lopo and William Nixon and Börje F. Karlsson and James Jaya and Ryandito Diandaru and Yuze Gao and Patrick Amadeus and Bin Wang and Jan Christian Blaise Cruz and Chenxi Whitehouse and Ivan Halim Parmonangan and Maria Khelli and Wenyu Zhang and Lucky Susanto and Reynard Adha Ryanda and Sonny Lazuardi Hermawan and Dan John Velasco and Muhammad Dehan Al Kautsar and Willy Fitra Hendria and Yasmin Moslem and Noah Flynn and Muhammad Farid Adilazuarda and Haochen Li and Johanes Lee and R. Damanhuri and Shuo Sun and Muhammad Reza Qorib and Amirbek Djanibekov and Wei Qi Leong and Quyet V. Do and Niklas Muennighoff and Tanrada Pansuwan and Ilham Firdausi Putra and Yan Xu and Ngee Chia Tai and Ayu Purwarianti and Sebastian Ruder and William Tjhi and Peerat Limkonchotiwat and Alham Fikri Aji and Sedrick Keh and Genta Indra Winata and Ruochen Zhang and Fajri Koto and Zheng-Xin Yong and Samuel Cahyawijaya},
      year={2024},
      eprint={2406.10118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.10118}, 
}

@misc{bommasani2023foundationmodeltransparencyindex,
      title={The Foundation Model Transparency Index}, 
      author={Rishi Bommasani and Kevin Klyman and Shayne Longpre and Sayash Kapoor and Nestor Maslej and Betty Xiong and Daniel Zhang and Percy Liang},
      year={2023},
      eprint={2310.12941},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.12941}, 
}

@misc{longpre2024consentcrisisrapiddecline,
      title={Consent in Crisis: The Rapid Decline of the AI Data Commons}, 
      author={Shayne Longpre and Robert Mahari and Ariel Lee and Campbell Lund and Hamidah Oderinwale and William Brannon and Nayan Saxena and Naana Obeng-Marnu and Tobin South and Cole Hunter and Kevin Klyman and Christopher Klamm and Hailey Schoelkopf and Nikhil Singh and Manuel Cherep and Ahmad Anis and An Dinh and Caroline Chitongo and Da Yin and Damien Sileo and Deividas Mataciunas and Diganta Misra and Emad Alghamdi and Enrico Shippole and Jianguo Zhang and Joanna Materzynska and Kun Qian and Kush Tiwary and Lester Miranda and Manan Dey and Minnie Liang and Mohammed Hamdy and Niklas Muennighoff and Seonghyeon Ye and Seungone Kim and Shrestha Mohanty and Vipul Gupta and Vivek Sharma and Vu Minh Chien and Xuhui Zhou and Yizhi Li and Caiming Xiong and Luis Villa and Stella Biderman and Hanlin Li and Daphne Ippolito and Sara Hooker and Jad Kabbara and Sandy Pentland},
      year={2024},
      eprint={2407.14933},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.14933}, 
}

@misc{yang2024sweagentagentcomputerinterfacesenable,
      title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering}, 
      author={John Yang and Carlos E. Jimenez and Alexander Wettig and Kilian Lieret and Shunyu Yao and Karthik Narasimhan and Ofir Press},
      year={2024},
      eprint={2405.15793},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2405.15793}, 
}

@misc{wang2024opendevinopenplatformai,
      title={OpenDevin: An Open Platform for AI Software Developers as Generalist Agents}, 
      author={Xingyao Wang and Boxuan Li and Yufan Song and Frank F. Xu and Xiangru Tang and Mingchen Zhuge and Jiayi Pan and Yueqi Song and Bowen Li and Jaskirat Singh and Hoang H. Tran and Fuqiang Li and Ren Ma and Mingzhang Zheng and Bill Qian and Yanjun Shao and Niklas Muennighoff and Yizhe Zhang and Binyuan Hui and Junyang Lin and Robert Brennan and Hao Peng and Heng Ji and Graham Neubig},
      year={2024},
      eprint={2407.16741},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2407.16741}, 
}

@misc{liu2024regmixdatamixtureregression,
      title={RegMix: Data Mixture as Regression for Language Model Pre-training}, 
      author={Qian Liu and Xiaosen Zheng and Niklas Muennighoff and Guangtao Zeng and Longxu Dou and Tianyu Pang and Jing Jiang and Min Lin},
      year={2024},
      eprint={2407.01492},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.01492}, 
}

@article{wolf2019huggingface,
  title={Huggingface's transformers: State-of-the-art natural language processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@misc{eval-harness,
  author       = {Gao, Leo and
                  Tow, Jonathan and
                  Biderman, Stella and
                  Black, Sid and
                  DiPofi, Anthony and
                  Foster, Charles and
                  Golding, Laurence and
                  Hsu, Jeffrey and
                  McDonell, Kyle and
                  Muennighoff, Niklas and
                  Phang, Jason and
                  Reynolds, Laria and
                  Tang, Eric and
                  Thite, Anish and
                  Wang, Ben and
                  Wang, Kevin and
                  Zou, Andy},
  title        = {A framework for few-shot language model evaluation},
  month        = sep,
  year         = 2021,
  publisher    = {Zenodo},
  version      = {v0.0.1},
  doi          = {10.5281/zenodo.5371628},
  url          = {https://doi.org/10.5281/zenodo.5371628}
}

@misc{webson-pavlick-2021,
      title={Do Prompt-Based Models Really Understand the Meaning of their Prompts?}, 
      author={Albert Webson and Ellie Pavlick},
      year={2021},
      eprint={2109.01247},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2109.01247}
}


%    url = {https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-of-reasoning-distillation},  
@misc{bespoke_stratos,  
    author = {Bespoke Labs},  
    title = {Bespoke-Stratos: The unreasonable effectiveness of reasoning distillation},
    url = {https://hf.co/bespokelabs/Bespoke-Stratos-32B},
    note = {Accessed: 2025-01-22},  
    year = {2025}
}

@misc{wang2024helpsteer2opensourcedatasettraining,
      title={HelpSteer2: Open-source dataset for training top-performing reward models}, 
      author={Zhilin Wang and Yi Dong and Olivier Delalleau and Jiaqi Zeng and Gerald Shen and Daniel Egert and Jimmy J. Zhang and Makesh Narsimhan Sreedhar and Oleksii Kuchaiev},
      year={2024},
      eprint={2406.08673},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.08673}, 
}


@misc{wikidotgpt,
  author = {Matt Brockman},
  title = {Wikidot - GPT Propmts - Summarization},
  howpublished = "\url{http://gptprompts.wikidot.com/prompt:summarization}",
  year = {2020},
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@article{tay2022unifying,
  title={Unifying Language Learning Paradigms},
  author={Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Houlsby, Neil and Metzler, Donald},
  journal={arXiv preprint arXiv:2205.05131},
  year={2022}
}

@article{true-zero-shot,
  author    = {Ethan Perez and
               Douwe Kiela and
               Kyunghyun Cho},
  title     = {True Few-Shot Learning with Language Models},
  journal   = {CoRR},
  volume    = {abs/2105.11447},
  year      = {2021},
  url       = {https://arxiv.org/abs/2105.11447},
  eprinttype = {arXiv},
  eprint    = {2105.11447},
  timestamp = {Tue, 01 Jun 2021 18:07:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2105-11447.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lester_prompt,
  author    = {Brian Lester and
               Rami Al{-}Rfou and
               Noah Constant},
  title     = {The Power of Scale for Parameter-Efficient Prompt Tuning},
  journal   = {CoRR},
  volume    = {abs/2104.08691},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08691},
  eprinttype = {arXiv},
  eprint    = {2104.08691},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08691.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{yao2021,
  author    = {Yao Lu and
               Max Bartolo and
               Alastair Moore and
               Sebastian Riedel and
               Pontus Stenetorp},
  title     = {Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot
               Prompt Order Sensitivity},
  journal   = {CoRR},
  volume    = {abs/2104.08786},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08786},
  eprinttype = {arXiv},
  eprint    = {2104.08786},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08786.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mishra2021,
  author    = {Swaroop Mishra and
               Daniel Khashabi and
               Chitta Baral and
               Hannaneh Hajishirzi},
  title     = {Natural Instructions: Benchmarking Generalization to New Tasks from
               Natural Language Instructions},
  journal   = {CoRR},
  volume    = {abs/2104.08773},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.08773},
  eprinttype = {arXiv},
  eprint    = {2104.08773},
  timestamp = {Mon, 26 Apr 2021 17:25:10 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-08773.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhong2021adapting,
  author    = {Ruiqi Zhong and
               Kristy Lee and
               Zheng Zhang and
               Dan Klein},
  title     = {Meta-tuning Language Models to Answer Prompts Better},
  journal   = {CoRR},
  volume    = {abs/2104.04670},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.04670},
  eprinttype = {arXiv},
  eprint    = {2104.04670},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2104-04670.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bender2021dangers,
  title={On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  author={Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
  pages={610--623},
  year={2021}
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@inproceedings{strubell2019energy,
  title={Energy and Policy Considerations for Deep Learning in {NLP}},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={3645--3650},
  year={2019}
}

@misc{fewshot_var,
    title={Evaluating Different Fewshot Description Prompts on GPT-3}, 
    howpublished = {EleutherAI Blog},
    url={https://blog.eleuther.ai/prompts-gpt-fewshot/}, journal={EleutherAI Blog},
    note = {test},
    author={Gao, Leo},
    year={2021},
    }

@misc{downstream_finetuning,
title={Finetuning Models on Downstream Tasks}, url={https://blog.eleuther.ai/tuning-on-eval-harness/}, journal={EleutherAI Blog}, author={Gao, Leo}, year={2021}, month={May}}

@article{logan2021cutting,
  title={Cutting Down on Prompts and Parameters: Simple Few-Shot Learning with Language Models},
  author={Logan, Robert L and Bala{\v{z}}evi{\'c}, Ivana and Wallace, Eric and Petroni, Fabio and Singh, Sameer and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2106.13353},
  year={2021}
}

@article{sotala_superintelligence_2017,
  title={Superintelligence as a cause or cure for risks of astronomical suffering},
  author={Sotala, Kaj and Gloor, Lukas},
  journal={Informatica},
  volume={41},
  number={4},
  year={2017}
}



@article{amodei_concrete_2016,
  title={Concrete problems in {AI} safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016},
  url={https://arxiv.org/abs/1606.06565}
}


@article{IEM,
  title={Intelligence explosion microeconomics},
  author={Yudkowsky, Eliezer},
  journal={Machine Intelligence Research Institute, accessed online October},
  volume={23},
  pages={2015},
  year={2013},
  publisher={Citeseer}
}


@article{bostrom2014ethics,
  title={The ethics of artificial intelligence},
  author={Bostrom, Nick and Yudkowsky, Eliezer},
  journal={The Cambridge handbook of artificial intelligence},
  volume={1},
  pages={316--334},
  year={2014},
  publisher={Cambridge University Press Cambridge}
}

@book{bostrom2014superintelligence,
  title={Superintelligence: Paths, Dangers, Strategies},
  author={Bostrom, Nick},
  year={2014},
  publisher={Oxford University Press, Inc.}
}

@book{russell2019human,
  title={Human Compatible: Artificial Intelligence and the Problem of Control},
  author={Russell, S.},
  isbn={9780525558620},
  lccn={2019029689},
  url={https://books.google.de/books?id=M1eFDwAAQBAJ},
  year={2019},
  publisher={Penguin Publishing Group}
}

@article{monster,
  title={Sharing the World with Digital Minds},
  author={Shulman, Carl and Bostrom, Nick},
  journal={preprint},
  year={2020}
}

@book{christian2020alignment,
  title={The Alignment Problem: Machine Learning and Human Values},
  author={Christian, Brian},
  year={2020},
  publisher={WW Norton \& Company}
}

@article{learningsum,
  title={Learning to summarize with human feedback},
  author={Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{critch2020ai, 
title={{AI Research Considerations for Human Existential Safety (ARCHES)}},
author={Critch, Andrew and Krueger, David}, 
year={2020}, 
journal={Preprint at \href{http://acritch.com/arches}{acritch.com/arches}}}

@article{taylor2016alignment,
  title={Alignment for advanced machine learning systems},
  author={Taylor, Jessica and Yudkowsky, Eliezer and LaVictoire, Patrick and Critch, Andrew},
  year={2016}
}

@inproceedings{levenshtein1966binary,
  title={Binary codes capable of correcting deletions, insertions, and reversals},
  author={Levenshtein, Vladimir I and others},
  booktitle={Soviet physics doklady},
  volume={10},
  pages={707--710},
  year={1966},
  organization={Soviet Union}
}

@article{grace_when_2018,
  title={When will {AI} exceed human performance? Evidence from {AI} experts},
  author={Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
  journal={Journal of Artificial Intelligence Research},
  volume={62},
  pages={729--754},
  year={2018}
}

@misc{instruct,
  title={{OpenAI API}: Instruct Series (beta},
  author={OpenAI},
  url={https://beta.openai.com/docs/engines/instruct-series-beta},
  year={2021}
}

@article{schwartz2020green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={Communications of the ACM},
  volume={63},
  number={12},
  pages={54--63},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{lacoste2019quantifying,
  title={Quantifying the carbon emissions of machine learning},
  author={Lacoste, Alexandre and Luccioni, Alexandra and Schmidt, Victor and Dandres, Thomas},
  journal={arXiv preprint arXiv:1910.09700},
  year={2019}
}

@misc{min2022metaicl,
      title={MetaICL: Learning to Learn In Context}, 
      author={Sewon Min and Mike Lewis and Luke Zettlemoyer and Hannaneh Hajishirzi},
      year={2022},
      eprint={2110.15943},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2022finetuned,
      title={Finetuned Language Models Are Zero-Shot Learners}, 
      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
      year={2022},
      eprint={2109.01652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hendrycks2022unsolved,
      title={Unsolved Problems in ML Safety}, 
      author={Dan Hendrycks and Nicholas Carlini and John Schulman and Jacob Steinhardt},
      year={2022},
      eprint={2109.13916},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kenton2021alignment,
  title={Alignment of Language Agents},
  author={Kenton, Zachary and Everitt, Tom and Weidinger, Laura and Gabriel, Iason and Mikulik, Vladimir and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2103.14659},
  year={2021}
}

@article{leahy2021hard,
  title={The hard problem of aligning AI to human values},
  author={Leahy, Connor and Biderman, Stella},
  journal={The State of AI Ethics Report},
  volume={4},
  pages={180--183},
  year={2021}
}

@article{solaiman2019release,
  title={Release strategies and the social impacts of language models},
  author={Solaiman, Irene and Brundage, Miles and Clark, Jack and Askell, Amanda and Herbert-Voss, Ariel and Wu, Jeff and Radford, Alec and Krueger, Gretchen and Kim, Jong Wook and Kreps, Sarah and others},
  journal={arXiv preprint arXiv:1908.09203},
  year={2019}
}

@misc{release-strat,
    title={Why Release a Large Language Model?},
    author={Leahy, Connor},
    journal={EleutherAI Blog},
    url={https://blog.eleuther.ai/why-release-a-large-language-model/},
    year={2021},
}

@article{williams2020anlizing,
      title={ANLIzing the Adversarial Natural Language Inference Dataset}, 
      author={Adina Williams and Tristan Thrush and Douwe Kiela},
      year={2020},
      journal={arXiv preprint arXiv:2010.12729}
}

@misc{zhao2021calibrate,
      title={Calibrate Before Use: Improving Few-Shot Performance of Language Models}, 
      author={Tony Z. Zhao and Eric Wallace and Shi Feng and Dan Klein and Sameer Singh},
      year={2021},
      eprint={2102.09690},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@online{WinNT,
  author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornel},
  title = {First Quora Dataset Release: Question Pairs},
  year = 2017,
  url = {https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs},
  urldate = {2019-04-03}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{xia2022training,
  title={Training Trajectories of Language Models Across Scales},
  author={Xia, Mengzhou and Artetxe, Mikel and Zhou, Chunting and Lin, Xi Victoria and Pasunuru, Ramakanth and Chen, Danqi and Zettlemoyer, Luke and Stoyanov, Ves},
  journal={arXiv preprint arXiv:2212.09803},
  year={2022}
}

@article{xia2021metaxl,
  title={MetaXL: Meta representation transformation for low-resource cross-lingual learning},
  author={Xia, Mengzhou and Zheng, Guoqing and Mukherjee, Subhabrata and Shokouhi, Milad and Neubig, Graham and Awadallah, Ahmed Hassan},
  journal={arXiv preprint arXiv:2104.07908},
  year={2021}
}

@misc{muennighoff2023octopack,
      title={OctoPack: Instruction Tuning Code Large Language Models}, 
      author={Niklas Muennighoff and Qian Liu and Armel Zebaze and Qinkai Zheng and Binyuan Hui and Terry Yue Zhuo and Swayam Singh and Xiangru Tang and Leandro von Werra and Shayne Longpre},
      year={2023},
      eprint={2308.07124},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{madani2020progen,
  title={Progen: Language modeling for protein generation},
  author={Madani, Ali and McCann, Bryan and Naik, Nikhil and Keskar, Nitish Shirish and Anand, Namrata and Eguchi, Raphael R and Huang, Po-Ssu and Socher, Richard},
  journal={arXiv preprint arXiv:2004.03497},
  year={2020}
}

@article{gupta2023instruction,
  title={Instruction Tuned Models are Quick Learners},
  author={Gupta, Himanshu and Sawant, Saurabh Arjun and Mishra, Swaroop and Nakamura, Mutsumi and Mitra, Arindam and Mashetty, Santosh and Baral, Chitta},
  journal={arXiv preprint arXiv:2306.05539},
  year={2023}
}

@misc{zhou2023lima,
      title={LIMA: Less Is More for Alignment}, 
      author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
      year={2023},
      eprint={2305.11206},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.11206}, 
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

@article{henderson2022pile,
  title={Pile of law: Learning responsible data filtering from the law and a 256gb open-source legal dataset},
  author={Henderson, Peter and Krass, Mark and Zheng, Lucia and Guha, Neel and Manning, Christopher D and Jurafsky, Dan and Ho, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={29217--29234},
  year={2022}
}

@article{wang2022self,
  title={Self-Instruct: Aligning Language Model with Self Generated Instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@misc{dao2022flashattention,
      title={FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness}, 
      author={Tri Dao and Daniel Y. Fu and Stefano Ermon and Atri Rudra and Christopher Ré},
      year={2022},
      eprint={2205.14135},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dao2023flashattention,
      title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}, 
      author={Tri Dao},
      year={2023},
      eprint={2307.08691},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhao2023pytorch,
      title={PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel}, 
      author={Yanli Zhao and Andrew Gu and Rohan Varma and Liang Luo and Chien-Chin Huang and Min Xu and Less Wright and Hamid Shojanazeri and Myle Ott and Sam Shleifer and Alban Desmaison and Can Balioglu and Pritam Damania and Bernard Nguyen and Geeta Chauhan and Yuchen Hao and Ajit Mathews and Shen Li},
      year={2023},
      eprint={2304.11277},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@misc{köpf2023openassistant,
      title={OpenAssistant Conversations -- Democratizing Large Language Model Alignment}, 
      author={Andreas Köpf and Yannic Kilcher and Dimitri von Rütte and Sotiris Anagnostidis and Zhi-Rui Tam and Keith Stevens and Abdullah Barhoum and Nguyen Minh Duc and Oliver Stanley and Richárd Nagyfi and Shahul ES and Sameer Suri and David Glushkov and Arnav Dantuluri and Andrew Maguire and Christoph Schuhmann and Huu Nguyen and Alexander Mattick},
      year={2023},
      eprint={2304.07327},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2024improving,
      title={Improving Text Embeddings with Large Language Models}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Linjun Yang and Rangan Majumder and Furu Wei},
      year={2024},
      eprint={2401.00368},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2023far,
      title={How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources}, 
      author={Yizhong Wang and Hamish Ivison and Pradeep Dasigi and Jack Hessel and Tushar Khot and Khyathi Raghavi Chandu and David Wadden and Kelsey MacMillan and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
      eprint={2306.04751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{prabhumoye2023adding,
  title={Adding Instructions during Pretraining: Effective Way of Controlling Toxicity in Language Models},
  author={Prabhumoye, Shrimai and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal={arXiv preprint arXiv:2302.07388},
  year={2023}
}

@article{penedo2023refinedweb,
  title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only},
  author={Penedo, Guilherme and Malartic, Quentin and Hesslow, Daniel and Cojocaru, Ruxandra and Cappelli, Alessandro and Alobeidli, Hamza and Pannier, Baptiste and Almazrouei, Ebtesam and Launay, Julien},
  journal={arXiv preprint arXiv:2306.01116},
  year={2023}
}

@article{lin2019choosing,
  title={Choosing transfer languages for cross-lingual learning},
  author={Lin, Yu-Hsiang and Chen, Chian-Yu and Lee, Jean and Li, Zirui and Zhang, Yuyan and Xia, Mengzhou and Rijhwani, Shruti and He, Junxian and Zhang, Zhisong and Ma, Xuezhe and others},
  journal={arXiv preprint arXiv:1905.12688},
  year={2019}
}

@misc{shazeer2018adafactoradaptivelearningrates,
      title={Adafactor: Adaptive Learning Rates with Sublinear Memory Cost}, 
      author={Noam Shazeer and Mitchell Stern},
      year={2018},
      eprint={1804.04235},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1804.04235}, 
}

@article{tay2022transformer,
  title={Transformer memory as a differentiable search index},
  author={Tay, Yi and Tran, Vinh Q and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and others},
  journal={arXiv preprint arXiv:2202.06991},
  year={2022}
}

@misc{zhang2024bamjustlikethat,
      title={BAM! Just Like That: Simple and Efficient Parameter Upcycling for Mixture of Experts}, 
      author={Qizhen Zhang and Nikolas Gritsch and Dwaraknath Gnaneshwar and Simon Guo and David Cairuz and Bharat Venkitesh and Jakob Foerster and Phil Blunsom and Sebastian Ruder and Ahmet Ustun and Acyr Locatelli},
      year={2024},
      eprint={2408.08274},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.08274}, 
}

@misc{jambateam2024jamba15hybridtransformermambamodels,
      title={Jamba-1.5: Hybrid Transformer-Mamba Models at Scale}, 
      author={Jamba Team and Barak Lenz and Alan Arazi and Amir Bergman and Avshalom Manevich and Barak Peleg and Ben Aviram and Chen Almagor and Clara Fridman and Dan Padnos and Daniel Gissin and Daniel Jannai and Dor Muhlgay and Dor Zimberg and Edden M Gerber and Elad Dolev and Eran Krakovsky and Erez Safahi and Erez Schwartz and Gal Cohen and Gal Shachaf and Haim Rozenblum and Hofit Bata and Ido Blass and Inbal Magar and Itay Dalmedigos and Jhonathan Osin and Julie Fadlon and Maria Rozman and Matan Danos and Michael Gokhman and Mor Zusman and Naama Gidron and Nir Ratner and Noam Gat and Noam Rozen and Oded Fried and Ohad Leshno and Omer Antverg and Omri Abend and Opher Lieber and Or Dagan and Orit Cohavi and Raz Alon and Ro'i Belson and Roi Cohen and Rom Gilad and Roman Glozman and Shahar Lev and Shaked Meirom and Tal Delbari and Tal Ness and Tomer Asida and Tom Ben Gal and Tom Braude and Uriya Pumerantz and Yehoshua Cohen and Yonatan Belinkov and Yuval Globerson and Yuval Peleg Levy and Yoav Shoham},
      year={2024},
      eprint={2408.12570},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.12570}, 
}

@misc{muennighoff2023crosslingual,
      title={Crosslingual Generalization through Multitask Finetuning}, 
      author={Niklas Muennighoff and Thomas Wang and Lintang Sutawika and Adam Roberts and Stella Biderman and Teven Le Scao and M Saiful Bari and Sheng Shen and Zheng-Xin Yong and Hailey Schoelkopf and Xiangru Tang and Dragomir Radev and Alham Fikri Aji and Khalid Almubarak and Samuel Albanie and Zaid Alyafeai and Albert Webson and Edward Raff and Colin Raffel},
      year={2023},
      eprint={2211.01786},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{iyer2016summarizing,
  title={Summarizing source code using a neural attention model},
  author={Iyer, Srinivasan and Konstas, Ioannis and Cheung, Alvin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2073--2083},
  year={2016}
}

@inproceedings{allamanis2015suggesting,
  title={Suggesting accurate method and class names},
  author={Allamanis, Miltiadis and Barr, Earl T and Bird, Christian and Sutton, Charles},
  booktitle={Proceedings of the 2015 10th joint meeting on foundations of software engineering},
  pages={38--49},
  year={2015}
}

@article{nijkamp2022codegen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@article{virtanen2019multilingual,
  title={Multilingual is not enough: BERT for Finnish},
  author={Virtanen, Antti and Kanerva, Jenna and Ilo, Rami and Luoma, Jouni and Luotolahti, Juhani and Salakoski, Tapio and Ginter, Filip and Pyysalo, Sampo},
  journal={arXiv preprint arXiv:1912.07076},
  year={2019}
}

@article{gu2021domain,
  title={Domain-specific language model pretraining for biomedical natural language processing},
  author={Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  journal={ACM Transactions on Computing for Healthcare (HEALTH)},
  volume={3},
  number={1},
  pages={1--23},
  year={2021},
  publisher={ACM New York, NY}
}

@article{orlanski2023measuring,
  title={Measuring The Impact Of Programming Language Distribution},
  author={Orlanski, Gabriel and Xiao, Kefan and Garcia, Xavier and Hui, Jeffrey and Howland, Joshua and Malmaud, Jonathan and Austin, Jacob and Singh, Rishah and Catasta, Michele},
  journal={arXiv preprint arXiv:2302.01973},
  year={2023}
}

@article{bian2021colossal,
  title={Colossal-AI: A unified deep learning system for large-scale parallel training},
  author={Bian, Zhengda and Liu, Hongxin and Wang, Boxiang and Huang, Haichen and Li, Yongbin and Wang, Chuanrui and Cui, Fan and You, Yang},
  journal={arXiv preprint arXiv:2110.14883},
  year={2021}
}

@article{huo2021wenlan,
  title={WenLan: Bridging vision and language by large-scale multi-modal pre-training},
  author={Huo, Yuqi and Zhang, Manli and Liu, Guangzhen and Lu, Haoyu and Gao, Yizhao and Yang, Guoxing and Wen, Jingyuan and Zhang, Heng and Xu, Baogui and Zheng, Weihao and others},
  journal={arXiv preprint arXiv:2103.06561},
  year={2021}
}

@article{zeng2021pangu,
  title={PanGu-alpha: Large-scale Autoregressive Pretrained Chinese Language Models with Auto-parallel Computation},
  author={Zeng, Wei and Ren, Xiaozhe and Su, Teng and Wang, Hui and Liao, Yi and Wang, Zhiwei and Jiang, Xin and Yang, ZhenZhang and Wang, Kaisheng and Zhang, Xiaoda and others},
  journal={arXiv preprint arXiv:2104.12369},
  year={2021}
}

@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}

@article{eichenberg2021magma,
  title={MAGMA--Multimodal Augmentation of Generative Models through Adapter-based Finetuning},
  author={Eichenberg, Constantin and Black, Sidney and Weinbach, Samuel and Parcalabescu, Letitia and Frank, Anette},
  journal={arXiv preprint arXiv:2112.05253},
  year={2021}
}

@article{henighan2020scaling,
  title={Scaling laws for autoregressive generative modeling},
  author={Henighan, Tom and Kaplan, Jared and Katz, Mor and Chen, Mark and Hesse, Christopher and Jackson, Jacob and Jun, Heewoo and Brown, Tom B and Dhariwal, Prafulla and Gray, Scott and others},
  journal={arXiv preprint arXiv:2010.14701},
  year={2020}
}

@article{soltan2022alexatm,
  title={Alexatm 20b: Few-shot learning using a large-scale multilingual seq2seq model},
  author={Soltan, Saleh and Ananthakrishnan, Shankar and FitzGerald, Jack and Gupta, Rahul and Hamza, Wael and Khan, Haidar and Peris, Charith and Rawls, Stephen and Rosenbaum, Andy and Rumshisky, Anna and others},
  journal={arXiv preprint arXiv:2208.01448},
  year={2022}
}

@article{carlini2022quantifying,
  title={Quantifying memorization across neural language models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  journal={arXiv preprint arXiv:2202.07646},
  year={2022}
}

@misc{srivastava2023imitation,
      title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models}, 
      author={Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adrià Garriga-Alonso and others},
      year={2023},
      eprint={2206.04615},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and others},
  journal={arXiv preprint arXiv:2204.14198},
  year={2022}
}

@article{longpre2023flan,
  title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning},
  author={Longpre, Shayne and Hou, Le and Vu, Tu and Webson, Albert and Chung, Hyung Won and Tay, Yi and Zhou, Denny and Le, Quoc V and Zoph, Barret and Wei, Jason and others},
  journal={arXiv preprint arXiv:2301.13688},
  year={2023}
}

@misc{dubois2024lengthcontrolledalpacaevalsimpleway,
      title={Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators}, 
      author={Yann Dubois and Balázs Galambosi and Percy Liang and Tatsunori B. Hashimoto},
      year={2024},
      eprint={2404.04475},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.04475}, 
}

@misc{longpre2023pretrainers,
      title={A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, \& Toxicity}, 
      author={Shayne Longpre and Gregory Yauney and Emily Reif and Katherine Lee and Adam Roberts and Barret Zoph and Denny Zhou and Jason Wei and Kevin Robinson and David Mimno and Daphne Ippolito},
      year={2023},
      eprint={2305.13169},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@misc{kiela2021hateful,
  title={The hateful memes challenge: Competition report},
  author={Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Fitzpatrick, Casey A and Bull, Peter and Lipstein, Greg and Nelli, Tony and Zhu, Ron and others},
  booktitle={NeurIPS 2020 Competition and Demonstration Track},
  pages={344--360},
  year={2021},
  organization={PMLR},
  url={https://proceedings.mlr.press/v133/kiela21a.html}
}

@misc{dehghani2019universaltransformers,
      title={Universal Transformers}, 
      author={Mostafa Dehghani and Stephan Gouws and Oriol Vinyals and Jakob Uszkoreit and Łukasz Kaiser},
      year={2019},
      eprint={1807.03819},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1807.03819}, 
}

@article{biderman2023emergent,
  title={Emergent and Predictable Memorization in Large Language Models},
  author={Biderman, Stella and Prashanth, USVSN Sai and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raf, Edward},
  journal={arXiv preprint arXiv:2304.11158},
  year={2023}
}

@misc{allal2024SmolLM,
      title={SmolLM - blazingly fast and remarkably powerful}, 
      author={Loubna Ben Allal and Anton Lozhkov and Elie Bakouch and Leandro von Werra and Thomas Wolf},
      year={2024},
      url={https://huggingface.co/blog/smollm}
}

@misc{zhou2024brainformerstradingsimplicityefficiency,
      title={Brainformers: Trading Simplicity for Efficiency}, 
      author={Yanqi Zhou and Nan Du and Yanping Huang and Daiyi Peng and Chang Lan and Da Huang and Siamak Shakeri and David So and Andrew Dai and Yifeng Lu and Zhifeng Chen and Quoc Le and Claire Cui and James Laudon and Jeff Dean},
      year={2024},
      eprint={2306.00008},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.00008}, 
}

@misc{rajbhandari2022deepspeedmoeadvancingmixtureofexpertsinference,
      title={DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale}, 
      author={Samyam Rajbhandari and Conglong Li and Zhewei Yao and Minjia Zhang and Reza Yazdani Aminabadi and Ammar Ahmad Awan and Jeff Rasley and Yuxiong He},
      year={2022},
      eprint={2201.05596},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2201.05596}, 
}

@misc{gemmateam2024gemma2improvingopen,
      title={Gemma 2: Improving Open Language Models at a Practical Size}, 
      author={Gemma Team and Morgane Riviere and Shreya Pathak and Pier Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and Léonard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ramé and Johan Ferret and Peter Liu and Pouya Tafti and Abe Friesen and others},
      year={2024},
      eprint={2408.00118},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.00118}, 
}

@misc{cai2024internlm2,
      title={InternLM2 Technical Report},
      author={Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},
      year={2024},
      eprint={2403.17297},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{biderman2023pythiasuiteanalyzinglarge,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01373}, 
}

@misc{huang2018musictransformer,
      title={Music Transformer}, 
      author={Cheng-Zhi Anna Huang and Ashish Vaswani and Jakob Uszkoreit and Noam Shazeer and Ian Simon and Curtis Hawthorne and Andrew M. Dai and Matthew D. Hoffman and Monica Dinculescu and Douglas Eck},
      year={2018},
      eprint={1809.04281},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.04281}, 
}

@article{weston2015towards,
  title={Towards ai-complete question answering: A set of prerequisite toy tasks},
  author={Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M and Van Merri{\"e}nboer, Bart and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1502.05698},
  year={2015}
}

@misc{Khrushchev_YaLM_100B_2022,
author={Khrushchev, Mikhail and Vasilev, Ruslan and Petrov, Alexey and Zinov, Nikolay},
month={6},
title={{YaLM 100B}},
url={https://github.com/yandex/YaLM-100B},
year={2022},
journal={GitHub}
}

@article{grave2018learning,
  title={Learning word vectors for 157 languages},
  author={Grave, Edouard and Bojanowski, Piotr and Gupta, Prakhar and Joulin, Armand and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1802.06893},
  year={2018}
}

@article{deduplicatinglee2021,
      title={Deduplicating Training Data Makes Language Models Better}, 
      author={Katherine Lee and Daphne Ippolito and Andrew Nystrom and Chiyuan Zhang and Douglas Eck and Chris Callison-Burch and Nicholas Carlini},
      journal={arXiv preprint arXiv:2107.06499},
      year={2021},
}

@article{baevski2019cloze,
  title={Cloze-driven pretraining of self-attention networks},
  author={Baevski, Alexei and Edunov, Sergey and Liu, Yinhan and Zettlemoyer, Luke and Auli, Michael},
  journal={arXiv preprint arXiv:1903.07785},
  year={2019}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{DBLP:journals/corr/abs-2108-07258,
  author    = {Rishi Bommasani and
               Drew A. Hudson and
               Ehsan Adeli and
               Russ Altman and
               Simran Arora and
               Sydney von Arx and
               Michael S. Bernstein and
               Jeannette Bohg and
               Antoine Bosselut and
               Emma Brunskill and
               Erik Brynjolfsson and
               Shyamal Buch and
               Dallas Card and
               Rodrigo Castellon and
               Niladri Chatterji and
               Annie S. Chen and
               Kathleen Creel and
               Jared Quincy Davis and
               Dorottya Demszky and
               Chris Donahue and
               Moussa Doumbouya and
               Esin Durmus and
               Stefano Ermon and
               John Etchemendy and
               Kawin Ethayarajh and
               Li Fei{-}Fei and
               Chelsea Finn and
               Trevor Gale and
               Lauren Gillespie and
               Karan Goel and
               Noah D. Goodman and
               Shelby Grossman and
               Neel Guha and
               Tatsunori Hashimoto and
               Peter Henderson and
               John Hewitt and
               Daniel E. Ho and
               Jenny Hong and
               Kyle Hsu and
               Jing Huang and
               Thomas Icard and
               Saahil Jain and
               Dan Jurafsky and
               Pratyusha Kalluri and
               Siddharth Karamcheti and
               Geoff Keeling and
               Fereshte Khani and
               Omar Khattab and
               Pang Wei Koh and
               Mark S. Krass and
               Ranjay Krishna and
               Rohith Kuditipudi and
               et al.},
  title     = {On the Opportunities and Risks of Foundation Models},
  journal   = {CoRR},
  volume    = {abs/2108.07258},
  year      = {2021},
  url       = {https://arxiv.org/abs/2108.07258},
  eprinttype = {arXiv},
  eprint    = {2108.07258},
  timestamp = {Fri, 20 Aug 2021 13:55:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2108-07258.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1908-09203,
  author    = {Irene Solaiman and
               Miles Brundage and
               Jack Clark and
               Amanda Askell and
               Ariel Herbert{-}Voss and
               Jeff Wu and
               Alec Radford and
               Jasmine Wang},
  title     = {Release Strategies and the Social Impacts of Language Models},
  journal   = {CoRR},
  volume    = {abs/1908.09203},
  year      = {2019},
  url       = {http://arxiv.org/abs/1908.09203},
  eprinttype = {arXiv},
  eprint    = {1908.09203},
  timestamp = {Tue, 24 Sep 2019 10:04:06 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1908-09203.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{go2009twitter,
  title={Twitter sentiment classification using distant supervision},
  author={Go, Alec and Bhayani, Richa and Huang, Lei},
  journal={CS224N project report, Stanford},
  volume={1},
  number={12},
  pages={2009},
  year={2009}
}

@inproceedings{gibert2018hate,
    title = "{Hate Speech Dataset from a White Supremacy Forum}",
    author = "de Gibert, Ona  and
      Perez, Naiara  and
      Garcia-Pablos, Aitor  and
      Cuadros, Montse",
    booktitle = "Proceedings of the 2nd Workshop on Abusive Language Online ({ALW}2)",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5102",
    doi = "10.18653/v1/W18-5102",
    pages = "11--20",
}

@article{wang2019superglue,
  author    = {Alex Wang and
               Yada Pruksachatkun and
               Nikita Nangia and
               Amanpreet Singh and
               Julian Michael and
               Felix Hill and
               Omer Levy and
               Samuel R. Bowman},
  title     = {SuperGLUE: {A} Stickier Benchmark for General-Purpose Language Understanding
               Systems},
  journal   = {CoRR},
  volume    = {abs/1905.00537},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.00537},
  eprinttype = {arXiv},
  eprint    = {1905.00537},
  timestamp = {Mon, 27 May 2019 13:15:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-00537.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Ngoc-Quan and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1525--1534},
  year={2016}
}

@InProceedings{rudinger-EtAl:2018:N18,
  author    = {Rudinger, Rachel  and  Naradowsky, Jason  and  Leonard, Brian  and  {Van Durme}, Benjamin},
  title     = {Gender Bias in Coreference Resolution},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  month     = {June},
  year      = {2018},
  address   = {New Orleans, Louisiana},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{nangia2020crows,
    title = "{CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models}",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics"
}

@article{schick2020as,
  title={It's not just size that matters: Small language models are also few-shot learners},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2009.07118},
  year={2020}
}

@article{nllb2022,
  doi = {10.48550/ARXIV.2207.04672},
  url = {https://arxiv.org/abs/2207.04672},
  author = {{NLLB Team} and Costa-jussà, Marta R. and Cross, James and Çelebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and Sun, Anna and Wang, Skyler and Wenzek, Guillaume and Youngblood, Al and Akula, Bapi and Barrault, Loic and Gonzalez, Gabriel Mejia and Hansanti, Prangthip and Hoffman, John and Jarrett, Semarley and Sadagopan, Kaushik Ram and Rowe, Dirk and Spruit, Shannon and Tran, Chau and Andrews, Pierre and Ayan, Necip Fazil and Bhosale, Shruti and Edunov, Sergey and Fan, Angela and Gao, Cynthia and Goswami, Vedanuj and Guzmán, Francisco and Koehn, Philipp and Mourachko, Alexandre and Ropers, Christophe and Saleem, Safiyyah and Schwenk, Holger and Wang, Jeff},
  title = {No Language Left Behind: Scaling Human-Centered Machine Translation},
  journal = {arXiv preprint 2207.04672},
  year = {2022},
  copyright = {Creative Commons Attribution Share Alike 4.0 International}
}

@article{bertsch2023unlimiformer,
  title={Unlimiformer: Long-range transformers with unlimited length input},
  author={Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew R},
  journal={arXiv preprint arXiv:2305.01625},
  year={2023}
}

@misc{du2021all,
  title={All nlp tasks are generation tasks: A general pretraining framework},
  author={Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10360},
  volume={18},
  year={2021},
  publisher={Mar},
  url={https://arxiv.org/abs/2103.10360v1}
}

@misc{izacard2022atlas,
      title={Atlas: Few-shot Learning with Retrieval Augmented Language Models}, 
      author={Gautier Izacard and Patrick Lewis and Maria Lomeli and Lucas Hosseini and Fabio Petroni and Timo Schick and Jane Dwivedi-Yu and Armand Joulin and Sebastian Riedel and Edouard Grave},
      year={2022},
      eprint={2208.03299},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lin2023radit,
      title={RA-DIT: Retrieval-Augmented Dual Instruction Tuning}, 
      author={Xi Victoria Lin and Xilun Chen and Mingda Chen and Weijia Shi and Maria Lomeli and Rich James and Pedro Rodriguez and Jacob Kahn and Gergely Szilvasy and Mike Lewis and Luke Zettlemoyer and Scott Yih},
      year={2023},
      eprint={2310.01352},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{alpaca_eval,
  author = {Xuechen Li and Tianyi Zhang and Yann Dubois and Rohan Taori and Ishaan Gulrajani and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {AlpacaEval: An Automatic Evaluator of Instruction-following Models},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  url = {https://github.com/tatsu-lab/alpaca_eval}
}

@misc{dubois2023alpacafarm,
  title={AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback}, 
  author={Yann Dubois and Xuechen Li and Rohan Taori and Tianyi Zhang and Ishaan Gulrajani and Jimmy Ba and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto},
  year={2023},
  eprint={2305.14387},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@misc{yu2022coca,
      title={CoCa: Contrastive Captioners are Image-Text Foundation Models}, 
      author={Jiahui Yu and Zirui Wang and Vijay Vasudevan and Legg Yeung and Mojtaba Seyedhosseini and Yonghui Wu},
      year={2022},
      eprint={2205.01917},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{cui2023ultrafeedback,
      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, 
      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2310.01377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{muennighoff2023mteb,
      title={MTEB: Massive Text Embedding Benchmark}, 
      author={Niklas Muennighoff and Nouamane Tazi and Loïc Magne and Nils Reimers},
      year={2023},
      eprint={2210.07316},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2022language,
      title={What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?}, 
      author={Thomas Wang and Adam Roberts and Daniel Hesslow and Teven Le Scao and Hyung Won Chung and Iz Beltagy and Julien Launay and Colin Raffel},
      year={2022},
      eprint={2204.05832},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{xu2020clue,
  title={CLUE: A Chinese language understanding evaluation benchmark},
  author={Xu, Liang and Hu, Hai and Zhang, Xuanwei and Li, Lu and Cao, Chenjie and Li, Yudong and Xu, Yechen and Sun, Kai and Yu, Dian and Yu, Cong and others},
  journal={arXiv preprint arXiv:2004.05986},
  year={2020}
}

@article{tikhonov2021s,
  title={It's All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning},
  author={Tikhonov, Alexey and Ryabinin, Max},
  journal={arXiv preprint arXiv:2106.12066},
  year={2021}
}

@misc{devlin2018bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mishra2022crosstask,
      title={Cross-Task Generalization via Natural Language Crowdsourcing Instructions}, 
      author={Swaroop Mishra and Daniel Khashabi and Chitta Baral and Hannaneh Hajishirzi},
      year={2022},
      eprint={2104.08773},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{raffel2023exploring,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{laurencconbigscience,
  title={The BigScience ROOTS Corpus: A 1.6 TB Composite Multilingual Dataset},
  author={Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Wang, Thomas and Akiki, Christopher and del Moral, Albert Villanova and Le Scao, Teven and Von Werra, Leandro and Mou, Chenghao and Ponferrada, Eduardo Gonz{\'a}lez and Nguyen, Huu and others},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022}
}

@misc{promptsource,
      title={PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts},
      author={Stephen H. Bach and Victor Sanh and Zheng-Xin Yong and Albert Webson and Colin Raffel and Nihal V. Nayak and Abheesht Sharma and Taewoon Kim and M Saiful Bari and Thibault Fevry and Zaid Alyafeai and Manan Dey and Andrea Santilli and Zhiqing Sun and Srulik Ben-David and Canwen Xu and Gunjan Chhablani and Han Wang and Jason Alan Fries and Maged S. Al-shaibani and Shanya Sharma and Urmish Thakker and Khalid Almubarak and Xiangru Tang and Xiangru Tang and Mike Tian-Jian Jiang and Alexander M. Rush},
      year={2022},
      eprint={2202.01279},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{lin2021few,
  title={Few-shot learning with multilingual language models},
  author={Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and others},
  journal={arXiv preprint arXiv:2112.10668},
  year={2021}
}
@article{shliazhko2022mgpt,
  title={mGPT: Few-Shot Learners Go Multilingual},
  author={Shliazhko, Oleh and Fenogenova, Alena and Tikhonova, Maria and Mikhailov, Vladislav and Kozlova, Anastasia and Shavrina, Tatiana},
  journal={arXiv preprint arXiv:2204.07580},
  year={2022}
}

@article{bahri2021explaining,
  title={Explaining neural scaling laws},
  author={Bahri, Yasaman and Dyer, Ethan and Kaplan, Jared and Lee, Jaehoon and Sharma, Utkarsh},
  journal={arXiv preprint arXiv:2102.06701},
  year={2021}
}

@article{hernandez2021scaling,
  title={Scaling laws for transfer},
  author={Hernandez, Danny and Kaplan, Jared and Henighan, Tom and McCandlish, Sam},
  journal={arXiv preprint arXiv:2102.01293},
  year={2021}
}

@article{ghorbani2021scaling,
  title={Scaling laws for neural machine translation},
  author={Ghorbani, Behrooz and Firat, Orhan and Freitag, Markus and Bapna, Ankur and Krikun, Maxim and Garcia, Xavier and Chelba, Ciprian and Cherry, Colin},
  journal={arXiv preprint arXiv:2109.07740},
  year={2021}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{xue2020mt5,
  title={mT5: A massively multilingual pre-trained text-to-text transformer},
  author={Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
  journal={arXiv preprint arXiv:2010.11934},
  year={2020}
}

@article{liu2020multilingual,
  title={Multilingual denoising pre-training for neural machine translation},
  author={Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={726--742},
  year={2020},
  publisher={MIT Press}
}

@article{yong2022bloom+,
  title={BLOOM+ 1: Adding Language Support to BLOOM for Zero-Shot Prompting},
  author={Yong, Zheng-Xin and Schoelkopf, Hailey and Muennighoff, Niklas and Aji, Alham Fikri and Adelani, David Ifeoluwa and Almubarak, Khalid and Bari, M Saiful and Sutawika, Lintang and Kasai, Jungo and Baruwa, Ahmed and others},
  journal={arXiv preprint arXiv:2212.09535},
  year={2022}
}

@article{ExT5,
  author    = {Vamsi Aribandi and
               Yi Tay and
               Tal Schuster and
               Jinfeng Rao and
               Huaixiu Steven Zheng and
               Sanket Vaibhav Mehta and
               Honglei Zhuang and
               Vinh Q. Tran and
               Dara Bahri and
               Jianmo Ni and
               Jai Prakash Gupta and
               Kai Hui and
               Sebastian Ruder and
               Donald Metzler},
  title     = {ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning},
  journal   = {CoRR},
  volume    = {abs/2111.10952},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.10952},
  eprinttype = {arXiv},
  eprint    = {2111.10952},
  timestamp = {Tue, 15 Mar 2022 13:23:14 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-10952.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{fan2021beyond,
  title={Beyond English-Centric Multilingual Machine Translation.},
  author={Fan, Angela and Bhosale, Shruti and Schwenk, Holger and Ma, Zhiyi and El-Kishky, Ahmed and Goyal, Siddharth and Baines, Mandeep and Celebi, Onur and Wenzek, Guillaume and Chaudhary, Vishrav and others},
  journal={J. Mach. Learn. Res.},
  volume={22},
  number={107},
  pages={1--48},
  year={2021}
}

@misc{lample2019,
  doi = {10.48550/ARXIV.1901.07291},
  
  url = {https://arxiv.org/abs/1901.07291},
  
  author = {Lample, Guillaume and Conneau, Alexis},
  
  keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Cross-lingual Language Model Pretraining},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{conneau2019unsupervised,
  title={Unsupervised Cross-lingual Representation Learning at Scale},
  author={Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzm{\'a}n, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1911.02116},
  year={2019}
}

@article{goyal2021larger,
  title={Larger-Scale Transformers for Multilingual Masked Language Modeling},
  author={Goyal, Naman and Du, Jingfei and Ott, Myle and Anantharaman, Giri and Conneau, Alexis},
  journal={arXiv preprint arXiv:2105.00572},
  year={2021}
}

@article{kosec2021packing,
  title={Packing: Towards 2x nlp bert acceleration},
  author={Kosec, Matej and Fu, Sheng and Krell, Mario Michael},
  journal={arXiv preprint arXiv:2107.02027},
  year={2021}
}

@inproceedings{wu-dredze-2019-beto,
    title = "Beto, Bentz, Becas: The Surprising Cross-Lingual Effectiveness of {BERT}",
    author = "Wu, Shijie  and
      Dredze, Mark",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1077",
    doi = "10.18653/v1/D19-1077",
    pages = "833--844",
    abstract = "Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.",
}

@article{patel2022bidirectional,
  title={Bidirectional Language Models Are Also Few-shot Learners},
  author={Patel, Ajay and Li, Bryan and Rasooli, Mohammad Sadegh and Constant, Noah and Raffel, Colin and Callison-Burch, Chris},
  journal={arXiv preprint arXiv:2209.14500},
  year={2022}
}

@misc{suzgun2022challenging,
      title={Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them}, 
      author={Mirac Suzgun and Nathan Scales and Nathanael Schärli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V. Le and Ed H. Chi and Denny Zhou and Jason Wei},
      year={2022},
      eprint={2210.09261},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{saleh2022,
  doi = {10.48550/ARXIV.2208.01448},
  url = {https://arxiv.org/abs/2208.01448},
  author = {Soltan, Saleh and Ananthakrishnan, Shankar and FitzGerald, Jack and Gupta, Rahul and Hamza, Wael and Khan, Haidar and Peris, Charith and Rawls, Stephen and Rosenbaum, Andy and Rumshisky, Anna and Prakash, Chandana Satya and Sridhar, Mukund and Triefenbach, Fabian and Verma, Apurv and Tur, Gokhan and Natarajan, Prem},
  keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {AlexaTM 20B: Few-Shot Learning Using a Large-Scale Multilingual Seq2Seq Model},
  publisher = {arXiv},
  year = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{fries2022bigbio,
  title={BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing},
  author={Fries, Jason Alan and Weber, Leon and Seelam, Natasha and Altay, Gabriel and Datta, Debajyoti and Garda, Samuele and Kang, Myungsun and Su, Ruisi and Kusa, Wojciech and Cahyawijaya, Samuel and others},
  journal={arXiv preprint arXiv:2206.15076},
  year={2022}
}

@article{wang2022zemi,
  title={Zemi: Learning Zero-Shot Semi-Parametric Language Models from Multiple Tasks},
  author={Wang, Zhenhailong and Pan, Xiaoman and Yu, Dian and Yu, Dong and Chen, Jianshu and Ji, Heng},
  journal={arXiv preprint arXiv:2210.00185},
  year={2022}
}

@article{scialom2022continual,
  title={Continual-T0: Progressively Instructing 50+ Tasks to Language Models Without Forgetting},
  author={Scialom, Thomas and Chakrabarty, Tuhin and Muresan, Smaranda},
  journal={arXiv preprint arXiv:2205.12393},
  year={2022}
}

@article{schick2020exploiting,
  title={Exploiting cloze questions for few shot text classification and natural language inference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2001.07676},
  year={2020}
}

@misc{shoeybi2020megatronlmtrainingmultibillionparameter,
      title={Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism}, 
      author={Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
      year={2020},
      eprint={1909.08053},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.08053}, 
}

@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{smith2022using,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@misc{chowdhery2022palm,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and others},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis \& Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and others},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{su2022welm,
  title={WeLM: A Well-Read Pre-trained Language Model for Chinese},
  author={Su, Hui and Zhou, Xiao and Yu, Houjing and Chen, Yuwen and Zhu, Zilin and Yu, Yang and Zhou, Jie},
  journal={arXiv preprint arXiv:2209.10372},
  year={2022}
}

@article{chalkidis2021multieurlex,
  title={MultiEURLEX--A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer},
  author={Chalkidis, Ilias and Fergadiotis, Manos and Androutsopoulos, Ion},
  journal={arXiv preprint arXiv:2109.00904},
  year={2021}
}

@article{ding2022delta,
  title={Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={arXiv preprint arXiv:2203.06904},
  year={2022}
}

@misc{chameleonteam2024chameleon,
      title={Chameleon: Mixed-Modal Early-Fusion Foundation Models}, 
      author={Chameleon Team},
      year={2024},
      eprint={2405.09818},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{geminiteam2024gemini,
      title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context}, 
      author={Gemini Team and Petko Georgiev and Ving Ian Lei and Ryan Burnell and Libin Bai and Anmol Gulati and Garrett Tanzer and Damien Vincent and Zhufeng Pan and Shibo Wang and Soroosh Mariooryad and Yifan Ding and Xinyang Geng and Fred Alcober and Roy Frostig and Mark Omernick and Lexi Walker and Cosmin Paduraru and Christina Sorokin and Andrea Tacchetti and Colin Gaffney and Samira Daruki and Olcan Sercinoglu and Zach Gleicher and Juliette Love and Paul Voigtlaender and Rohan Jain and others},
      year={2024},
      eprint={2403.05530},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{bai2023qwen,
      title={Qwen Technical Report}, 
      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
      year={2023},
      eprint={2309.16609},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{grok,
    title={Open Release of Grok-1},
    url={https://x.ai/blog/grok-os},
    author={xAI},
    year={2024}
}

@misc{arctic,
    title={Snowflake Arctic: The Best LLM for Enterprise AI — Efficiently Intelligent, Truly Open},
    url={https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/},
    author={Snowflake},
    year={2024}
}

@misc{arcticcookbook,
    title={Snowflake Arctic Cookbook Series: Exploring Mixture of Experts (MoE)},
    url={https://medium.com/snowflake/snowflake-arctic-cookbook-series-exploring-mixture-of-experts-moe-c7d6b8f14d16},
    author={Snowflake},
    year={2024}
}

@misc{ainslie2023gqatraininggeneralizedmultiquery,
      title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints}, 
      author={Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrón and Sumit Sanghai},
      year={2023},
      eprint={2305.13245},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.13245}, 
}

@misc{artetxe2022efficientlargescalelanguage,
      title={Efficient Large Scale Language Modeling with Mixtures of Experts}, 
      author={Mikel Artetxe and Shruti Bhosale and Naman Goyal and Todor Mihaylov and Myle Ott and Sam Shleifer and Xi Victoria Lin and Jingfei Du and Srinivasan Iyer and Ramakanth Pasunuru and Giri Anantharaman and Xian Li and Shuohui Chen and Halil Akin and Mandeep Baines and Louis Martin and Xing Zhou and Punit Singh Koura and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Mona Diab and Zornitsa Kozareva and Ves Stoyanov},
      year={2022},
      eprint={2112.10684},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.10684}, 
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and others},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@misc{he2024mixturemillionexperts,
      title={Mixture of A Million Experts}, 
      author={Xu Owen He},
      year={2024},
      eprint={2407.04153},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.04153}, 
}

@misc{dua2021trickstrainingsparsetranslation,
      title={Tricks for Training Sparse Translation Models}, 
      author={Dheeru Dua and Shruti Bhosale and Vedanuj Goswami and James Cross and Mike Lewis and Angela Fan},
      year={2021},
      eprint={2110.08246},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.08246}, 
}

@misc{jaszczur2021sparsescalingtransformers,
      title={Sparse is Enough in Scaling Transformers}, 
      author={Sebastian Jaszczur and Aakanksha Chowdhery and Afroz Mohiuddin and Łukasz Kaiser and Wojciech Gajewski and Henryk Michalewski and Jonni Kanerva},
      year={2021},
      eprint={2111.12763},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2111.12763}, 
}

@misc{gross2017hardmixturesexpertslarge,
      title={Hard Mixtures of Experts for Large Scale Weakly Supervised Vision}, 
      author={Sam Gross and Marc'Aurelio Ranzato and Arthur Szlam},
      year={2017},
      eprint={1704.06363},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1704.06363}, 
}

@misc{zuo2022tamingsparselyactivatedtransformer,
      title={Taming Sparsely Activated Transformer with Stochastic Experts}, 
      author={Simiao Zuo and Xiaodong Liu and Jian Jiao and Young Jin Kim and Hany Hassan and Ruofei Zhang and Tuo Zhao and Jianfeng Gao},
      year={2022},
      eprint={2110.04260},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.04260}, 
}

@misc{roller2021hashlayerslargesparse,
      title={Hash Layers For Large Sparse Models}, 
      author={Stephen Roller and Sainbayar Sukhbaatar and Arthur Szlam and Jason Weston},
      year={2021},
      eprint={2106.04426},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.04426}, 
}

@misc{lewis2021baselayerssimplifyingtraining,
      title={BASE Layers: Simplifying Training of Large, Sparse Models}, 
      author={Mike Lewis and Shruti Bhosale and Tim Dettmers and Naman Goyal and Luke Zettlemoyer},
      year={2021},
      eprint={2103.16716},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.16716}, 
}

@misc{su2023roformerenhancedtransformerrotary,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2023},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.09864}, 
}

@misc{shazeer2019fasttransformerdecodingwritehead,
      title={Fast Transformer Decoding: One Write-Head is All You Need}, 
      author={Noam Shazeer},
      year={2019},
      eprint={1911.02150},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1911.02150}, 
}
@misc{yang2024qwen2technicalreport,
      title={Qwen2 Technical Report}, 
      author={An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and Guanting Dong and Haoran Wei and Huan Lin and Jialong Tang and Jialin Wang and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Ma and Jianxin Yang and Jin Xu and Jingren Zhou and Jinze Bai and Jinzheng He and Junyang Lin and Kai Dang and Keming Lu and Keqin Chen and Kexin Yang and Mei Li and Mingfeng Xue and Na Ni and Pei Zhang and Peng Wang and Ru Peng and Rui Men and Ruize Gao and Runji Lin and Shijie Wang and Shuai Bai and Sinan Tan and Tianhang Zhu and Tianhao Li and Tianyu Liu and Wenbin Ge and Xiaodong Deng and Xiaohuan Zhou and Xingzhang Ren and Xinyu Zhang and Xipin Wei and Xuancheng Ren and Xuejing Liu and Yang Fan and Yang Yao and Yichang Zhang and Yu Wan and Yunfei Chu and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zhifang Guo and Zhihao Fan},
      year={2024},
      eprint={2407.10671},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.10671}, 
}

@misc{lieber2024jambahybridtransformermambalanguage,
      title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
      author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
      year={2024},
      eprint={2403.19887},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19887}, 
}

@misc{ai2024yiopenfoundationmodels,
      title={Yi: Open Foundation Models by 01.AI}, 
      author={01. AI and : and Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Tao Yu and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai},
      year={2024},
      eprint={2403.04652},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.04652}, 
}

@misc{dey2023cerebrasgptopencomputeoptimallanguage,
      title={Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster}, 
      author={Nolan Dey and Gurpreet Gosal and Zhiming and Chen and Hemant Khachane and William Marshall and Ribhu Pathria and Marvin Tom and Joel Hestness},
      year={2023},
      eprint={2304.03208},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.03208}, 
}

@misc{anil2023palm2technicalreport,
      title={PaLM 2 Technical Report}, 
      author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and Eric Chu and Jonathan H. Clark and Laurent El Shafey and Yanping Huang and Kathy Meier-Hellstern and Gaurav Mishra and Erica Moreira and Mark Omernick and Kevin Robinson and Sebastian Ruder and Yi Tay and Kefan Xiao and Yuanzhong Xu and Yujing Zhang and Gustavo Hernandez Abrego and Junwhan Ahn and Jacob Austin and Paul Barham and Jan Botha and James Bradbury and Siddhartha Brahma and Kevin Brooks and Michele Catasta and Yong Cheng and Colin Cherry and Christopher A. Choquette-Choo and Aakanksha Chowdhery and Clément Crepy and Shachi Dave and Mostafa Dehghani and Sunipa Dev and Jacob Devlin and Mark Díaz and Nan Du and Ethan Dyer and Vlad Feinberg and Fangxiaoyu Feng and Vlad Fienber and Markus Freitag and Xavier Garcia and Sebastian Gehrmann and Lucas Gonzalez and Guy Gur-Ari and Steven Hand and Hadi Hashemi and Le Hou and Joshua Howland and Andrea Hu and Jeffrey Hui and Jeremy Hurwitz and Michael Isard and Abe Ittycheriah and Matthew Jagielski and Wenhao Jia and Kathleen Kenealy and Maxim Krikun and Sneha Kudugunta and Chang Lan and Katherine Lee and Benjamin Lee and Eric Li and Music Li and Wei Li and YaGuang Li and Jian Li and Hyeontaek Lim and Hanzhao Lin and Zhongtao Liu and Frederick Liu and Marcello Maggioni and Aroma Mahendru and Joshua Maynez and Vedant Misra and Maysam Moussalem and Zachary Nado and John Nham and Eric Ni and Andrew Nystrom and Alicia Parrish and Marie Pellat and Martin Polacek and Alex Polozov and Reiner Pope and Siyuan Qiao and Emily Reif and Bryan Richter and Parker Riley and Alex Castro Ros and Aurko Roy and Brennan Saeta and Rajkumar Samuel and Renee Shelby and Ambrose Slone and Daniel Smilkov and David R. So and Daniel Sohn and Simon Tokumine and Dasha Valter and Vijay Vasudevan and Kiran Vodrahalli and Xuezhi Wang and Pidong Wang and Zirui Wang and Tao Wang and John Wieting and Yuhuai Wu and Kelvin Xu and Yunhan Xu and Linting Xue and Pengcheng Yin and Jiahui Yu and Qiao Zhang and Steven Zheng and Ce Zheng and Weikang Zhou and Denny Zhou and Slav Petrov and Yonghui Wu},
      year={2023},
      eprint={2305.10403},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10403} 
}



@misc{lozhkov2024starcoder2stackv2,
      title={StarCoder 2 and The Stack v2: The Next Generation}, 
      author={Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2024},
      eprint={2402.19173},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2402.19173}, 
}

@misc{gemmateam2024gemmaopenmodelsbased,
      title={Gemma: Open Models Based on Gemini Research and Technology}, 
      author={Gemma Team and Thomas Mesnard and Cassidy Hardin and Robert Dadashi and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Léonard Hussenot and Pier Giuseppe Sessa and Aakanksha Chowdhery and Adam Roberts and Aditya Barua and Alex Botev and Alex Castro-Ros and Ambrose Slone and Amélie Héliou and Andrea Tacchetti and Anna Bulanova and Antonia Paterson and Beth Tsai and Bobak Shahriari and Charline Le Lan and Christopher A. Choquette-Choo and Clément Crepy and Daniel Cer and Daphne Ippolito and David Reid and Elena Buchatskaya and Eric Ni and Eric Noland and Geng Yan and George Tucker and George-Christian Muraru and Grigory Rozhdestvenskiy and Henryk Michalewski and Ian Tenney and Ivan Grishchenko and Jacob Austin and James Keeling and Jane Labanowski and Jean-Baptiste Lespiau and Jeff Stanway and Jenny Brennan and Jeremy Chen and Johan Ferret and Justin Chiu and Justin Mao-Jones and Katherine Lee and Kathy Yu and Katie Millican and Lars Lowe Sjoesund and Lisa Lee and Lucas Dixon and Machel Reid and Maciej Mikuła and Mateo Wirth and Michael Sharman and Nikolai Chinaev and Nithum Thain and Olivier Bachem and Oscar Chang and Oscar Wahltinez and Paige Bailey and Paul Michel and Petko Yotov and Rahma Chaabouni and Ramona Comanescu and Reena Jana and Rohan Anil and Ross McIlroy and Ruibo Liu and Ryan Mullins and Samuel L Smith and Sebastian Borgeaud and Sertan Girgin and Sholto Douglas and Shree Pandya and Siamak Shakeri and Soham De and Ted Klimenko and Tom Hennigan and Vlad Feinberg and Wojciech Stokowiec and Yu-hui Chen and Zafarali Ahmed and Zhitao Gong and Tris Warkentin and Ludovic Peran and Minh Giang and Clément Farabet and Oriol Vinyals and Jeff Dean and Koray Kavukcuoglu and Demis Hassabis and Zoubin Ghahramani and Douglas Eck and Joelle Barral and Fernando Pereira and Eli Collins and Armand Joulin and Noah Fiedel and Evan Senter and Alek Andreev and Kathleen Kenealy},
      year={2024},
      eprint={2403.08295},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.08295}, 
}

@misc{li2023textbooksneediiphi15,
      title={Textbooks Are All You Need II: phi-1.5 technical report}, 
      author={Yuanzhi Li and Sébastien Bubeck and Ronen Eldan and Allie Del Giorno and Suriya Gunasekar and Yin Tat Lee},
      year={2023},
      eprint={2309.05463},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.05463}, 
}

@misc{gunasekar2023textbooksneed,
      title={Textbooks Are All You Need}, 
      author={Suriya Gunasekar and Yi Zhang and Jyoti Aneja and Caio César Teodoro Mendes and Allie Del Giorno and Sivakanth Gopi and Mojan Javaheripi and Piero Kauffmann and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Harkirat Singh Behl and Xin Wang and Sébastien Bubeck and Ronen Eldan and Adam Tauman Kalai and Yin Tat Lee and Yuanzhi Li},
      year={2023},
      eprint={2306.11644},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.11644}, 
}

@misc{zhang2022optopenpretrainedtransformer,
      title={OPT: Open Pre-trained Transformer Language Models}, 
      author={Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin and Todor Mihaylov and Myle Ott and Sam Shleifer and Kurt Shuster and Daniel Simig and Punit Singh Koura and Anjali Sridhar and Tianlu Wang and Luke Zettlemoyer},
      year={2022},
      eprint={2205.01068},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2205.01068}, 
}

@misc{abdin2024phi3technicalreporthighly,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah and Hany Awadalla and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and Sébastien Bubeck and Qin Cai and Martin Cai and Caio César Teodoro Mendes and Weizhu Chen and Vishrav Chaudhary and Dong Chen and Dongdong Chen and Yen-Chun Chen and Yi-Ling Chen and Parul Chopra and Xiyang Dai and Allie Del Giorno and Gustavo de Rosa and Matthew Dixon and Ronen Eldan and Victor Fragoso and Dan Iter and Mei Gao and Min Gao and Jianfeng Gao and Amit Garg and Abhishek Goswami and Suriya Gunasekar and Emman Haider and Junheng Hao and Russell J. Hewett and Jamie Huynh and Mojan Javaheripi and Xin Jin and Piero Kauffmann and Nikos Karampatziakis and Dongwoo Kim and Mahoud Khademi and Lev Kurilenko and James R. Lee and Yin Tat Lee and Yuanzhi Li and Yunsheng Li and Chen Liang and Lars Liden and Ce Liu and Mengchen Liu and Weishung Liu and Eric Lin and Zeqi Lin and Chong Luo and Piyush Madan and Matt Mazzola and Arindam Mitra and Hardik Modi and Anh Nguyen and Brandon Norick and Barun Patra and Daniel Perez-Becker and Thomas Portet and Reid Pryzant and Heyang Qin and Marko Radmilac and Corby Rosset and Sambudha Roy and Olatunji Ruwase and Olli Saarikivi and Amin Saied and Adil Salim and Michael Santacroce and Shital Shah and Ning Shang and Hiteshi Sharma and Swadheen Shukla and Xia Song and Masahiro Tanaka and Andrea Tupini and Xin Wang and Lijuan Wang and Chunyu Wang and Yu Wang and Rachel Ward and Guanhua Wang and Philipp Witte and Haiping Wu and Michael Wyatt and Bin Xiao and Can Xu and Jiahang Xu and Weijian Xu and Sonali Yadav and Fan Yang and Jianwei Yang and Ziyi Yang and Yifan Yang and Donghan Yu and Lu Yuan and Chengruidong Zhang and Cyril Zhang and Jianwen Zhang and Li Lyna Zhang and Yi Zhang and Yue Zhang and Yunan Zhang and Xiren Zhou},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}

@misc{dbrx,
    title={DBRX},
    url={https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm},
    author={Databricks},
    year={2024}
}

@misc{together,
    title={Together Inference Llama3},
    url={https://www.together.ai/blog/together-inference-engine-2},
    author={Together AI},
    year={2024}
}



@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{bellagente2024stablelm216b,
      title={Stable LM 2 1.6B Technical Report}, 
      author={Marco Bellagente and Jonathan Tow and Dakota Mahan and Duy Phung and Maksym Zhuravinskyi and Reshinth Adithyan and James Baicoianu and Ben Brooks and Nathan Cooper and Ashish Datta and Meng Lee and Emad Mostaque and Michael Pieler and Nikhil Pinnaparju and Paulo Rocha and Harry Saini and Hannah Teufel and Niccolo Zanichelli and Carlos Riquelme},
      year={2024},
      eprint={2402.17834},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.17834}, 
}

@misc{rekateam2024rekacoreflashedge,
      title={Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models}, 
      author={Reka Team and Aitor Ormazabal and Che Zheng and Cyprien de Masson d'Autume and Dani Yogatama and Deyu Fu and Donovan Ong and Eric Chen and Eugenie Lamprecht and Hai Pham and Isaac Ong and Kaloyan Aleksiev and Lei Li and Matthew Henderson and Max Bain and Mikel Artetxe and Nishant Relan and Piotr Padlewski and Qi Liu and Ren Chen and Samuel Phua and Yazheng Yang and Yi Tay and Yuqi Wang and Zhongkai Zhu and Zhihui Xie},
      year={2024},
      eprint={2404.12387},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.12387}, 
}

@misc{gpt4moe,
    title={GPT-4 MoE},
    url={https://x.com/soumithchintala/status/1671267150101721090},
    author={Soumith Chintala},
    year={2024}
}

@misc{parmar2024nemotron415btechnicalreport,
      title={Nemotron-4 15B Technical Report}, 
      author={Jupinder Parmar and Shrimai Prabhumoye and Joseph Jennings and Mostofa Patwary and Sandeep Subramanian and Dan Su and Chen Zhu and Deepak Narayanan and Aastha Jhunjhunwala and Ayush Dattagupta and Vibhu Jawa and Jiwei Liu and Ameya Mahabaleshwarkar and Osvald Nitski and Annika Brundyn and James Maki and Miguel Martinez and Jiaxuan You and John Kamalu and Patrick LeGresley and Denys Fridman and Jared Casper and Ashwath Aithal and Oleksii Kuchaiev and Mohammad Shoeybi and Jonathan Cohen and Bryan Catanzaro},
      year={2024},
      eprint={2402.16819},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.16819}, 
}

@misc{nvidia2024nemotron4340btechnicalreport,
      title={Nemotron-4 340B Technical Report}, 
      author={Nvidia and : and Bo Adler and Niket Agarwal and Ashwath Aithal and Dong H. Anh and Pallab Bhattacharya and Annika Brundyn and Jared Casper and Bryan Catanzaro and Sharon Clay and Jonathan Cohen and Sirshak Das and Ayush Dattagupta and Olivier Delalleau and Leon Derczynski and Yi Dong and Daniel Egert and Ellie Evans and Aleksander Ficek and Denys Fridman and Shaona Ghosh and Boris Ginsburg and Igor Gitman and Tomasz Grzegorzek and Robert Hero and Jining Huang and Vibhu Jawa and Joseph Jennings and Aastha Jhunjhunwala and John Kamalu and Sadaf Khan and Oleksii Kuchaiev and Patrick LeGresley and Hui Li and Jiwei Liu and Zihan Liu and Eileen Long and Ameya Sunil Mahabaleshwarkar and Somshubra Majumdar and James Maki and Miguel Martinez and Maer Rodrigues de Melo and Ivan Moshkov and Deepak Narayanan and Sean Narenthiran and Jesus Navarro and Phong Nguyen and Osvald Nitski and Vahid Noroozi and Guruprasad Nutheti and Christopher Parisien and Jupinder Parmar and Mostofa Patwary and Krzysztof Pawelec and Wei Ping and Shrimai Prabhumoye and Rajarshi Roy and Trisha Saar and Vasanth Rao Naik Sabavat and Sanjeev Satheesh and Jane Polak Scowcroft and Jason Sewall and Pavel Shamis and Gerald Shen and Mohammad Shoeybi and Dave Sizer and Misha Smelyanskiy and Felipe Soares and Makesh Narsimhan Sreedhar and Dan Su and Sandeep Subramanian and Shengyang Sun and Shubham Toshniwal and Hao Wang and Zhilin Wang and Jiaxuan You and Jiaqi Zeng and Jimmy Zhang and Jing Zhang and Vivienne Zhang and Yian Zhang and Chen Zhu},
      year={2024},
      eprint={2406.11704},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.11704}, 
}


@misc{yang2023baichuan2openlargescale,
      title={Baichuan 2: Open Large-scale Language Models}, 
      author={Aiyuan Yang and Bin Xiao and Bingning Wang and Borong Zhang and Ce Bian and Chao Yin and Chenxu Lv and Da Pan and Dian Wang and Dong Yan and Fan Yang and Fei Deng and Feng Wang and Feng Liu and Guangwei Ai and Guosheng Dong and Haizhou Zhao and Hang Xu and Haoze Sun and Hongda Zhang and Hui Liu and Jiaming Ji and Jian Xie and JunTao Dai and Kun Fang and Lei Su and Liang Song and Lifeng Liu and Liyun Ru and Luyao Ma and Mang Wang and Mickel Liu and MingAn Lin and Nuolan Nie and Peidong Guo and Ruiyang Sun and Tao Zhang and Tianpeng Li and Tianyu Li and Wei Cheng and Weipeng Chen and Xiangrong Zeng and Xiaochuan Wang and Xiaoxi Chen and Xin Men and Xin Yu and Xuehai Pan and Yanjun Shen and Yiding Wang and Yiyu Li and Youxin Jiang and Yuchen Gao and Yupeng Zhang and Zenan Zhou and Zhiying Wu},
      year={2023},
      eprint={2309.10305},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.10305}, 
}

@misc{glm2024chatglm,
      title={ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools}, 
      author={Team GLM and Aohan Zeng and Bin Xu and Bowen Wang and Chenhui Zhang and Da Yin and Diego Rojas and Guanyu Feng and Hanlin Zhao and Hanyu Lai and Hao Yu and Hongning Wang and Jiadai Sun and Jiajie Zhang and Jiale Cheng and Jiayi Gui and Jie Tang and Jing Zhang and Juanzi Li and Lei Zhao and Lindong Wu and Lucen Zhong and Mingdao Liu and Minlie Huang and Peng Zhang and Qinkai Zheng and Rui Lu and Shuaiqi Duan and Shudan Zhang and Shulin Cao and Shuxun Yang and Weng Lam Tam and Wenyi Zhao and Xiao Liu and Xiao Xia and Xiaohan Zhang and Xiaotao Gu and Xin Lv and Xinghan Liu and Xinyi Liu and Xinyue Yang and Xixuan Song and Xunkai Zhang and Yifan An and Yifan Xu and Yilin Niu and Yuantao Yang and Yueyan Li and Yushi Bai and Yuxiao Dong and Zehan Qi and Zhaoyu Wang and Zhen Yang and Zhengxiao Du and Zhenyu Hou and Zihan Wang},
      year={2024},
      eprint={2406.12793},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow},
  year         = {2021},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}


@misc{longpre2023flancollectiondesigningdata,
      title={The Flan Collection: Designing Data and Methods for Effective Instruction Tuning}, 
      author={Shayne Longpre and Le Hou and Tu Vu and Albert Webson and Hyung Won Chung and Yi Tay and Denny Zhou and Quoc V. Le and Barret Zoph and Jason Wei and Adam Roberts},
      year={2023},
      eprint={2301.13688},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2301.13688}, 
}

@misc{azerbayev2023llemma,
      title={Llemma: An Open Language Model For Mathematics}, 
      author={Zhangir Azerbayev and Hailey Schoelkopf and Keiran Paster and Marco Dos Santos and Stephen McAleer and Albert Q. Jiang and Jia Deng and Stella Biderman and Sean Welleck},
      year={2023},
      eprint={2310.10631},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{together2023redpajama,
  author={Together Computer},
  title={RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  year={2023},
  url={https://github.com/togethercomputer/RedPajama-Data}
}

@misc{paster2023openwebmath,
      title={OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text}, 
      author={Keiran Paster and Marco Dos Santos and Zhangir Azerbayev and Jimmy Ba},
      year={2023},
      eprint={2310.06786},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}



@misc{hendrycks2021measuringmathematicalproblemsolving,
      title={Measuring Mathematical Problem Solving With the MATH Dataset}, 
      author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2103.03874},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2103.03874}, 
}

@misc{liu2024routersvisionmixtureexperts,
      title={Routers in Vision Mixture of Experts: An Empirical Study}, 
      author={Tianlin Liu and Mathieu Blondel and Carlos Riquelme and Joan Puigcerver},
      year={2024},
      eprint={2401.15969},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2401.15969}, 
}

@misc{xu2024benchmarkingbenchmarkleakagelarge,
      title={Benchmarking Benchmark Leakage in Large Language Models}, 
      author={Ruijie Xu and Zengzhi Wang and Run-Ze Fan and Pengfei Liu},
      year={2024},
      eprint={2404.18824},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.18824}, 
}

@misc{muennighoff2024generativerepresentationalinstructiontuning,
      title={Generative Representational Instruction Tuning}, 
      author={Niklas Muennighoff and Hongjin Su and Liang Wang and Nan Yang and Furu Wei and Tao Yu and Amanpreet Singh and Douwe Kiela},
      year={2024},
      eprint={2402.09906},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09906}, 
}

@misc{bai2022constitutionalaiharmlessnessai,
      title={Constitutional AI: Harmlessness from AI Feedback}, 
      author={Yuntao Bai and Saurav Kadavath and Sandipan Kundu and Amanda Askell and Jackson Kernion and Andy Jones and Anna Chen and Anna Goldie and Azalia Mirhoseini and Cameron McKinnon and Carol Chen and Catherine Olsson and Christopher Olah and Danny Hernandez and Dawn Drain and Deep Ganguli and Dustin Li and Eli Tran-Johnson and Ethan Perez and Jamie Kerr and Jared Mueller and Jeffrey Ladish and Joshua Landau and Kamal Ndousse and Kamile Lukosuite and Liane Lovitt and Michael Sellitto and Nelson Elhage and Nicholas Schiefer and Noemi Mercado and Nova DasSarma and Robert Lasenby and Robin Larson and Sam Ringer and Scott Johnston and Shauna Kravec and Sheer El Showk and Stanislav Fort and Tamera Lanham and Timothy Telleen-Lawton and Tom Conerly and Tom Henighan and Tristan Hume and Samuel R. Bowman and Zac Hatfield-Dodds and Ben Mann and Dario Amodei and Nicholas Joseph and Sam McCandlish and Tom Brown and Jared Kaplan},
      year={2022},
      eprint={2212.08073},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.08073}, 
}

@misc{penedo2023refinedwebdatasetfalconllm,
      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, 
      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
      year={2023},
      eprint={2306.01116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.01116}, 
}

@misc{dauphin2017languagemodelinggatedconvolutional,
      title={Language Modeling with Gated Convolutional Networks}, 
      author={Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
      year={2017},
      eprint={1612.08083},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1612.08083}, 
}

@misc{groeneveld2023catwalkunifiedlanguagemodel,
      title={Catwalk: A Unified Language Model Evaluation Framework for Many Datasets}, 
      author={Dirk Groeneveld and Anas Awadalla and Iz Beltagy and Akshita Bhagia and Ian Magnusson and Hao Peng and Oyvind Tafjord and Pete Walsh and Kyle Richardson and Jesse Dodge},
      year={2023},
      eprint={2312.10253},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10253}, 
}

@misc{su2020vlbertpretraininggenericvisuallinguistic,
      title={VL-BERT: Pre-training of Generic Visual-Linguistic Representations}, 
      author={Weijie Su and Xizhou Zhu and Yue Cao and Bin Li and Lewei Lu and Furu Wei and Jifeng Dai},
      year={2020},
      eprint={1908.08530},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1908.08530}, 
}

@misc{bai2023qwenvlversatilevisionlanguagemodel,
      title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}, 
      author={Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2023},
      eprint={2308.12966},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2308.12966}, 
}

@misc{llmsize,
    title={LLM model size competition is intensifying… backwards!},
    url={https://x.com/karpathy/status/1814038096218083497},
    author={Andrej Karpathy},
    year={2024}
}

@misc{mixtralupcycle,
    title={Mixtral from Mistral},
    url={https://x.com/tianle_cai/status/1734188749117153684},
    author={Tianle Cai},
    year={2023}
}



@misc{li2024datacomplm,
      title={DataComp-LM: In search of the next generation of training sets for language models}, 
      author={Jeffrey Li and Alex Fang and Georgios Smyrnis and Maor Ivgi and Matt Jordan and Samir Gadre and Hritik Bansal and Etash Guha and Sedrick Keh and Kushal Arora and Saurabh Garg and Rui Xin and Niklas Muennighoff and Reinhard Heckel and Jean Mercat and Mayee Chen and Suchin Gururangan and Mitchell Wortsman and Alon Albalak and Yonatan Bitton and Marianna Nezhurina and Amro Abbas and Cheng-Yu Hsieh and Dhruba Ghosh and Josh Gardner and Maciej Kilian and Hanlin Zhang and Rulin Shao and Sarah Pratt and Sunny Sanyal and Gabriel Ilharco and Giannis Daras and Kalyani Marathe and Aaron Gokaslan and Jieyu Zhang and Khyathi Chandu and Thao Nguyen and Igor Vasiljevic and Sham Kakade and Shuran Song and Sujay Sanghavi and Fartash Faghri and Sewoong Oh and Luke Zettlemoyer and Kyle Lo and Alaaeldin El-Nouby and Hadi Pouransari and Alexander Toshev and Stephanie Wang and Dirk Groeneveld and Luca Soldaini and Pang Wei Koh and Jenia Jitsev and Thomas Kollar and Alexandros G. Dimakis and Yair Carmon and Achal Dave and Ludwig Schmidt and Vaishaal Shankar},
      year={2024},
      eprint={2406.11794},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{dehghani2023scaling,
      title={Scaling Vision Transformers to 22 Billion Parameters}, 
      author={Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Steiner and Mathilde Caron and Robert Geirhos and Ibrahim Alabdulmohsin and Rodolphe Jenatton and Lucas Beyer and Michael Tschannen and Anurag Arnab and Xiao Wang and Carlos Riquelme and Matthias Minderer and Joan Puigcerver and Utku Evci and Manoj Kumar and Sjoerd van Steenkiste and Gamaleldin F. Elsayed and Aravindh Mahendran and Fisher Yu and Avital Oliver and Fantine Huot and Jasmijn Bastings and Mark Patrick Collier and Alexey Gritsenko and Vighnesh Birodkar and Cristina Vasconcelos and Yi Tay and Thomas Mensink and Alexander Kolesnikov and Filip Pavetić and Dustin Tran and Thomas Kipf and Mario Lučić and Xiaohua Zhai and Daniel Keysers and Jeremiah Harmsen and Neil Houlsby},
      year={2023},
      eprint={2302.05442},
      archivePrefix={arXiv},
      primaryClass={id='cs.CV' full_name='Computer Vision and Pattern Recognition' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers image processing, computer vision, pattern recognition, and scene understanding. Roughly includes material in ACM Subject Classes I.2.10, I.4, and I.5.'}
}

@misc{mehta2024openelm,
      title={OpenELM: An Efficient Language Model Family with Open Training and Inference Framework}, 
      author={Sachin Mehta and Mohammad Hossein Sekhavat and Qingqing Cao and Maxwell Horton and Yanzi Jin and Chenfan Sun and Iman Mirzadeh and Mahyar Najibi and Dmitry Belenko and Peter Zatloukal and Mohammad Rastegari},
      year={2024},
      eprint={2404.14619},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{komatsuzaki2023sparse,
      title={Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints}, 
      author={Aran Komatsuzaki and Joan Puigcerver and James Lee-Thorp and Carlos Riquelme Ruiz and Basil Mustafa and Joshua Ainslie and Yi Tay and Mostafa Dehghani and Neil Houlsby},
      year={2023},
      eprint={2212.05055},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{almazrouei2023falcon,
      title={The Falcon Series of Open Language Models}, 
      author={Ebtesam Almazrouei and Hamza Alobeidli and Abdulaziz Alshamsi and Alessandro Cappelli and Ruxandra Cojocaru and Mérouane Debbah and Étienne Goffinet and Daniel Hesslow and Julien Launay and Quentin Malartic and Daniele Mazzotta and Badreddine Noune and Baptiste Pannier and Guilherme Penedo},
      year={2023},
      eprint={2311.16867},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{glorioso2024zambacompact7bssm,
      title={Zamba: A Compact 7B SSM Hybrid Model}, 
      author={Paolo Glorioso and Quentin Anthony and Yury Tokpanov and James Whittington and Jonathan Pilault and Adam Ibrahim and Beren Millidge},
      year={2024},
      eprint={2405.16712},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16712}, 
}

@misc{shazeer2020glu,
      title={GLU Variants Improve Transformer}, 
      author={Noam Shazeer},
      year={2020},
      eprint={2002.05202},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chung2022scaling,
      title={Scaling Instruction-Finetuned Language Models}, 
      author={Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and others},
      year={2022},
      eprint={2210.11416},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{workshop2023bloom,
      title={BLOOM: A 176B-Parameter Open-Access Multilingual Language Model}, 
      author={BigScience Workshop and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ilić and Daniel Hesslow and Roman Castagné and Alexandra Sasha Luccioni and François Yvon and Matthias Gallé and Jonathan Tow and Alexander M. Rush and Stella Biderman and Albert Webson and Pawan Sasanka Ammanamanchi and Thomas Wang and Benoît Sagot and Niklas Muennighoff and others},
      year={2023},
      eprint={2211.05100},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ortiz-suarez-etal-2020-monolingual,
    title = "A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages",
    author = "Ortiz Su{'a}rez, Pedro Javier  and
      Romary, Laurent  and
      Sagot, Benoit",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.156",
    pages = "1703--1714",
    abstract = "We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures.",
}

@inproceedings{OrtizSuarezSagotRomary2019,
  author    = {Pedro Javier {Ortiz Su{'a}rez} and Benoit Sagot and Laurent Romary},
  title     = {Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures},
  series = {Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019. Cardiff, 22nd July 2019},
  editor    = {Piotr Bański and Adrien Barbaresi and Hanno Biber and Evelyn Breiteneder and Simon Clematide and Marc Kupietz and Harald L{"u}ngen and Caroline Iliadi},
  publisher = {Leibniz-Institut f{"u}r Deutsche Sprache},
  address   = {Mannheim},
  doi       = {10.14618/ids-pub-9021},
  url       = {http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215},
  pages     = {9 -- 16},
  year      = {2019},
  abstract  = {Common Crawl is a considerably large, heterogeneous multilingual corpus comprised of crawled documents from the internet, surpassing 20TB of data and distributed as a set of more than 50 thousand plain text files where each contains many documents written in a wide variety of languages. Even though each document has a metadata block associated to it, this data lacks any information about the language in which each document is written, making it extremely difficult to use Common Crawl for monolingual applications. We propose a general, highly parallel, multithreaded pipeline to clean and classify Common Crawl by language; we specifically design it so that it runs efficiently on medium to low resource infrastructures where I/O speeds are the main constraint. We develop the pipeline so that it can be easily reapplied to any kind of heterogeneous corpus and so that it can be parameterised to a wide range of infrastructures. We also distribute a 6.3TB version of Common Crawl, filtered, classified by language, shuffled at line level in order to avoid copyright issues, and ready to be used for NLP applications.},
  language  = {en}
}

@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359},
  year={2021}
}

@misc{kocetkov2022stack3tbpermissively,
      title={The Stack: 3 TB of permissively licensed source code}, 
      author={Denis Kocetkov and Raymond Li and Loubna Ben Allal and Jia Li and Chenghao Mou and Carlos Muñoz Ferrandis and Yacine Jernite and Margaret Mitchell and Sean Hughes and Thomas Wolf and Dzmitry Bahdanau and Leandro von Werra and Harm de Vries},
      year={2022},
      eprint={2211.15533},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2211.15533}, 
}

@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}

@misc{scao2022language,
      title={What Language Model to Train if You Have One Million GPU Hours?}, 
      author={Teven Le Scao and Thomas Wang and Daniel Hesslow and Lucile Saulnier and Stas Bekman and M Saiful Bari and Stella Biderman and Hady Elsahar and Niklas Muennighoff and Jason Phang and Ofir Press and Colin Raffel and Victor Sanh and Sheng Shen and Lintang Sutawika and Jaesung Tae and Zheng Xin Yong and Julien Launay and Iz Beltagy},
      year={2022},
      eprint={2210.15424},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ladhak2020wikilingua,
  title={WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization},
  author={Ladhak, Faisal and Durmus, Esin and Cardie, Claire and McKeown, Kathleen},
  journal={arXiv preprint arXiv:2010.03093},
  year={2020}
}

@inproceedings{castro-ferreira20:bilin-bi-direc-webnl-shared,
  title={The 2020 Bilingual, Bi-Directional WebNLG+ Shared Task Overview and Evaluation Results (WebNLG+ 2020)},
  author={Castro Ferreira, Thiago and
                  Gardent, Claire and
          Ilinykh, Nikolai and
          van der Lee, Chris and
          Mille, Simon and
          Moussallem, Diego and
          Shimorina, Anastasia},
  booktitle = {Proceedings of the 3rd WebNLG Workshop on Natural Language Generation from the Semantic Web (WebNLG+ 2020)},
    pages = "55--76",
  year = 	 2020,
  address = 	 {Dublin, Ireland (Virtual)},
  publisher = {Association for Computational Linguistics}}

@article{gehrmann2021gem,
  title={The gem benchmark: Natural language generation, its evaluation and metrics},
  author={Gehrmann, Sebastian and Adewumi, Tosin and Aggarwal, Karmanya and Ammanamanchi, Pawan Sasanka and Anuoluwapo, Aremu and Bosselut, Antoine and Chandu, Khyathi Raghavi and Clinciu, Miruna and Das, Dipanjan and Dhole, Kaustubh D and others},
  journal={arXiv preprint arXiv:2102.01672},
  year={2021}
}

@InProceedings{xsum-emnlp,
  author =      "Shashi Narayan and Shay B. Cohen and Mirella Lapata",
  title =       "Don't Give Me the Details, Just the Summary! {T}opic-Aware Convolutional Neural Networks for Extreme Summarization",
  booktitle =   "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing ",
  year =        "2018",
  address =     "Brussels, Belgium",
}


@article{dusek.etal2020:csl,
  title = {Evaluating the {{State}}-of-the-{{Art}} of {{End}}-to-{{End Natural Language Generation}}: {{The E2E NLG Challenge}}},
  author = {Du{\v{s}}ek, Ond\v{r}ej and Novikova, Jekaterina and Rieser, Verena},
  year = {2020},
  month = jan,
  volume = {59},
  pages = {123--156},
  doi = {10.1016/j.csl.2019.06.009},
  archivePrefix = {arXiv},
  eprint = {1901.11528},
  eprinttype = {arxiv},
  journal = {Computer Speech \& Language}
}


@inproceedings{mostafazadeh2017lsdsem,
  title={Lsdsem 2017 shared task: The story cloze test},
  author={Mostafazadeh, Nasrin and Roth, Michael and Louis, Annie and Chambers, Nathanael and Allen, James},
  booktitle={Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics},
  pages={46--51},
  year={2017}
}

@article{driess2023palm,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{bonifacio2022inpars,
  title={Inpars: Data augmentation for information retrieval using large language models},
  author={Bonifacio, Luiz and Abonizio, Hugo and Fadaee, Marzieh and Nogueira, Rodrigo},
  journal={arXiv preprint arXiv:2202.05144},
  year={2022}
}

@article{welbl2017crowdsourcing,
  title={Crowdsourcing multiple choice science questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  journal={arXiv preprint arXiv:1707.06209},
  year={2017}
}


@inproceedings{roemmele2011choice,
  title={Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning.},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={AAAI spring symposium: logical formalizations of commonsense reasoning},
  pages={90--95},
  year={2011}
}

@inproceedings{de2019commitmentbank,
  title={The commitmentbank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  booktitle={proceedings of Sinn und Bedeutung},
  volume={23},
  pages={107--124},
  year={2019}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@article{komatsuzaki2019one,
  title={One epoch is all you need},
  author={Komatsuzaki, Aran},
  journal={arXiv preprint arXiv:1906.06669},
  year={2019}
}

@misc{chen2020generative,
  title={Generative pretraining from pixels},
  author={Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
  year={2020},
  url={https://cdn.openai.com/papers/Generative_Pretraining_from_Pixels_V2.pdf}
}

@article{tay2022scaling,
  title={Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Chung, Hyung Won and Fedus, William and Rao, Jinfeng and Narang, Sharan and Tran, Vinh Q and Yogatama, Dani and Metzler, Donald},
  journal={arXiv preprint arXiv:2207.10551},
  year={2022}
}

@article{nostalgebraist, 
    title={chinchilla’s wild implications},
    year={2022},
    author={nostalgebraist},
    journal={lesswrong}
}

@article{shin2022effect,
  title={On the effect of pretraining corpora on in-context learning by a large-scale language model},
  author={Shin, Seongjin and Lee, Sang-Woo and Ahn, Hwijeen and Kim, Sungdong and Kim, HyoungSeok and Kim, Boseop and Cho, Kyunghyun and Lee, Gichang and Park, Woomyoung and Ha, Jung-Woo and others},
  journal={arXiv preprint arXiv:2204.13509},
  year={2022}
}

@article{sorscher2022beyond,
  title={Beyond neural scaling laws: beating power law scaling via data pruning},
  author={Sorscher, Ben and Geirhos, Robert and Shekhar, Shashank and Ganguli, Surya and Morcos, Ari S},
  journal={arXiv preprint arXiv:2206.14486},
  year={2022}
}

@misc{clark2019boolqexploringsurprisingdifficulty,
      title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}, 
      author={Christopher Clark and Kenton Lee and Ming-Wei Chang and Tom Kwiatkowski and Michael Collins and Kristina Toutanova},
      year={2019},
      eprint={1905.10044},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.10044}, 
}

@inproceedings{dagan2006pascal,
  title={The pascal recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers},
  pages={177--190},
  year={2006},
  organization={Springer}
}

@misc{welbl2017crowdsourcingmultiplechoicescience,
      title={Crowdsourcing Multiple Choice Science Questions}, 
      author={Johannes Welbl and Nelson F. Liu and Matt Gardner},
      year={2017},
      eprint={1707.06209},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/1707.06209}, 
}

@misc{gordon2012semeval,
    title = "{S}em{E}val-2012 Task 7: Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning",
    author = "Gordon, Andrew  and
      Kozareva, Zornitsa  and
      Roemmele, Melissa",
    year = "2012",
    url="https://aclanthology.org/S12-1052",
}

@misc{mihaylov2018suitarmorconductelectricity,
      title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, 
      author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
      year={2018},
      eprint={1809.02789},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1809.02789}, 
}

@misc{talmor2019commonsenseqaquestionansweringchallenge,
      title={CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge}, 
      author={Alon Talmor and Jonathan Herzig and Nicholas Lourie and Jonathan Berant},
      year={2019},
      eprint={1811.00937},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1811.00937}, 
}

@misc{sap2019socialiqacommonsensereasoningsocial,
      title={SocialIQA: Commonsense Reasoning about Social Interactions}, 
      author={Maarten Sap and Hannah Rashkin and Derek Chen and Ronan LeBras and Yejin Choi},
      year={2019},
      eprint={1904.09728},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1904.09728}, 
}

@misc{hendrycks2021measuringmassivemultitasklanguage,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url={https://arxiv.org/abs/2009.03300}, 
}

@misc{clark2018thinksolvedquestionanswering,
      title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
      author={Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
      year={2018},
      eprint={1803.05457},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1803.05457}, 
}

@misc{lin2024momaefficientearlyfusionpretraining,
      title={MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware Experts}, 
      author={Xi Victoria Lin and Akshat Shrivastava and Liang Luo and Srinivasan Iyer and Mike Lewis and Gargi Gosh and Luke Zettlemoyer and Armen Aghajanyan},
      year={2024},
      eprint={2407.21770},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21770}, 
}

@misc{zoph2022stmoe,
      title={ST-MoE: Designing Stable and Transferable Sparse Expert Models}, 
      author={Barret Zoph and Irwan Bello and Sameer Kumar and Nan Du and Yanping Huang and Jeff Dean and Noam Shazeer and William Fedus},
      year={2022},
      eprint={2202.08906},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{tan2023sparse,
      title={Sparse Universal Transformer}, 
      author={Shawn Tan and Yikang Shen and Zhenfang Chen and Aaron Courville and Chuang Gan},
      year={2023},
      eprint={2310.07096},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{gale2022megablocksefficientsparsetraining,
      title={MegaBlocks: Efficient Sparse Training with Mixture-of-Experts}, 
      author={Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
      year={2022},
      eprint={2211.15841},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.15841}, 
}

@misc{zhang2024tinyllamaopensourcesmalllanguage,
      title={TinyLlama: An Open-Source Small Language Model}, 
      author={Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
      year={2024},
      eprint={2401.02385},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.02385}, 
}

@misc{open_lm,
  author = {Gururangan, Suchin and Wortsman, Mitchell and Gadre, Samir Yitzhak and Dave, Achal and Kilian, Maciej and Shi, Weijia and Mercat, Jean and Smyrnis, Georgios and Ilharco, Gabriel and Jordan, Matt and Heckel, Reinhard and Dimakis, Alex and Farhadi, Ali and Shankar, Vaishaal and Schmidt, Ludwig},
  title = {{open\_lm}:  a minimal but performative language modeling (LM) repository},
  year = {2023},
  url = {https://github.com/mlfoundations/open_lm/}
}
%  note = {GitHub repository},

@misc{vanlaarhoven2017l2regularizationversusbatch,
      title={L2 Regularization versus Batch and Weight Normalization}, 
      author={Twan van Laarhoven},
      year={2017},
      eprint={1706.05350},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.05350}, 
}

@misc{qwen_moe,
    title = {Qwen1.5-MoE: Matching 7B Model Performance with 1/3 Activated Parameters"},
    url = {https://qwenlm.github.io/blog/qwen-moe/},
    author = {Qwen Team},
    month = {February},
    year = {2024}
}

@misc{zhang2019rootmeansquarelayer,
      title={Root Mean Square Layer Normalization}, 
      author={Biao Zhang and Rico Sennrich},
      year={2019},
      eprint={1910.07467},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1910.07467}, 
}

@misc{ott2019fairseq,
      title={fairseq: A Fast, Extensible Toolkit for Sequence Modeling}, 
      author={Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},
      year={2019},
      eprint={1904.01038},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{deepseekai2024deepseek,
      title={DeepSeek LLM: Scaling Open-Source Language Models with Longtermism}, 
      author={DeepSeek-AI and : and Xiao Bi and Deli Chen and Guanting Chen and Shanhuang Chen and Damai Dai and Chengqi Deng and Honghui Ding and Kai Dong and Qiushi Du and Zhe Fu and Huazuo Gao and Kaige Gao and Wenjun Gao and Ruiqi Ge and Kang Guan and Daya Guo and Jianzhong Guo and Guangbo Hao and Zhewen Hao and Ying He and Wenjie Hu and Panpan Huang and Erhang Li and Guowei Li and Jiashi Li and Yao Li and Y. K. Li and Wenfeng Liang and Fangyun Lin and A. X. Liu and Bo Liu and Wen Liu and Xiaodong Liu and Xin Liu and Yiyuan Liu and Haoyu Lu and Shanghao Lu and Fuli Luo and Shirong Ma and Xiaotao Nie and Tian Pei and Yishi Piao and Junjie Qiu and Hui Qu and Tongzheng Ren and Zehui Ren and Chong Ruan and Zhangli Sha and Zhihong Shao and Junxiao Song and Xuecheng Su and Jingxiang Sun and Yaofeng Sun and Minghui Tang and Bingxuan Wang and Peiyi Wang and Shiyu Wang and Yaohui Wang and Yongji Wang and Tong Wu and Y. Wu and Xin Xie and Zhenda Xie and Ziwei Xie and Yiliang Xiong and Hanwei Xu and R. X. Xu and Yanhong Xu and Dejian Yang and Yuxiang You and Shuiping Yu and Xingkai Yu and B. Zhang and Haowei Zhang and Lecong Zhang and Liyue Zhang and Mingchuan Zhang and Minghua Zhang and Wentao Zhang and Yichao Zhang and Chenggang Zhao and Yao Zhao and Shangyan Zhou and Shunfeng Zhou and Qihao Zhu and Yuheng Zou},
      year={2024},
      eprint={2401.02954},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{black2022gptneox20b,
      title={GPT-NeoX-20B: An Open-Source Autoregressive Language Model}, 
      author={Sid Black and Stella Biderman and Eric Hallahan and Quentin Anthony and Leo Gao and Laurence Golding and Horace He and Connor Leahy and Kyle McDonell and Jason Phang and Michael Pieler and USVSN Sai Prashanth and Shivanshu Purohit and Laria Reynolds and Jonathan Tow and Ben Wang and Samuel Weinbach},
      year={2022},
      eprint={2204.06745},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{peng2023rwkv,
      title={RWKV: Reinventing RNNs for the Transformer Era}, 
      author={Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Stella Biderman and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and Xuzheng He and Haowen Hou and Jiaju Lin and Przemyslaw Kazienko and Jan Kocon and Jiaming Kong and Bartlomiej Koptyra and Hayden Lau and Krishna Sri Ipsit Mantri and Ferdinand Mom and Atsushi Saito and Guangyu Song and Xiangru Tang and Bolun Wang and Johan S. Wind and Stanislaw Wozniak and Ruichong Zhang and Zhenyuan Zhang and Qihang Zhao and Peng Zhou and Qinghua Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2023},
      eprint={2305.13048},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{peng2024eagle,
      title={Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence}, 
      author={Bo Peng and Daniel Goldstein and Quentin Anthony and Alon Albalak and Eric Alcaide and Stella Biderman and Eugene Cheah and Xingjian Du and Teddy Ferdinan and Haowen Hou and Przemysław Kazienko and Kranthi Kiran GV and Jan Kocoń and Bartłomiej Koptyra and Satyapriya Krishna and Ronald McClelland Jr. au2 and Niklas Muennighoff and Fares Obeid and Atsushi Saito and Guangyu Song and Haoqin Tu and Stanisław Woźniak and Ruichong Zhang and Bingchen Zhao and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
      year={2024},
      eprint={2404.05892},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{llama-moe-2023,
  title={LLaMA-MoE: Building Mixture-of-Experts from LLaMA with Continual Pre-training},
  author={LLaMA-MoE Team},
  year={2023},
  month={Dec},
  url={https://github.com/pjlab-sys4nlp/llama-moe}
}

@misc{csordás2024moeut,
      title={MoEUT: Mixture-of-Experts Universal Transformers}, 
      author={Róbert Csordás and Kazuki Irie and Jürgen Schmidhuber and Christopher Potts and Christopher D. Manning},
      year={2024},
      eprint={2405.16039},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{gadre2024language,
      title={Language models scale reliably with over-training and on downstream tasks}, 
      author={Samir Yitzhak Gadre and Georgios Smyrnis and Vaishaal Shankar and Suchin Gururangan and Mitchell Wortsman and Rulin Shao and Jean Mercat and Alex Fang and Jeffrey Li and Sedrick Keh and Rui Xin and Marianna Nezhurina and Igor Vasiljevic and Jenia Jitsev and Luca Soldaini and Alexandros G. Dimakis and Gabriel Ilharco and Pang Wei Koh and Shuran Song and Thomas Kollar and Yair Carmon and Achal Dave and Reinhard Heckel and Niklas Muennighoff and Ludwig Schmidt},
      year={2024},
      eprint={2403.08540},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{chiang2024chatbot,
      title={Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference}, 
      author={Wei-Lin Chiang and Lianmin Zheng and Ying Sheng and Anastasios Nikolas Angelopoulos and Tianle Li and Dacheng Li and Hao Zhang and Banghua Zhu and Michael Jordan and Joseph E. Gonzalez and Ion Stoica},
      year={2024},
      eprint={2403.04132},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@misc{taylor2022galactica,
      title={Galactica: A Large Language Model for Science}, 
      author={Ross Taylor and Marcin Kardas and Guillem Cucurull and Thomas Scialom and Anthony Hartshorn and Elvis Saravia and Andrew Poulton and Viktor Kerkez and Robert Stojnic},
      year={2022},
      eprint={2211.09085},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{ziems2024large,
      title={Can Large Language Models Transform Computational Social Science?}, 
      author={Caleb Ziems and William Held and Omar Shaikh and Jiaao Chen and Zhehao Zhang and Diyi Yang},
      year={2024},
      eprint={2305.03514},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{fedus2022review,
      title={A Review of Sparse Expert Models in Deep Learning}, 
      author={William Fedus and Jeff Dean and Barret Zoph},
      year={2022},
      eprint={2209.01667},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{du2022glam,
      title={GLaM: Efficient Scaling of Language Models with Mixture-of-Experts}, 
      author={Nan Du and Yanping Huang and Andrew M. Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and Barret Zoph and Liam Fedus and Maarten Bosma and Zongwei Zhou and Tao Wang and Yu Emma Wang and Kellie Webster and Marie Pellat and Kevin Robinson and Kathleen Meier-Hellstern and Toju Duke and Lucas Dixon and Kun Zhang and Quoc V Le and Yonghui Wu and Zhifeng Chen and Claire Cui},
      year={2022},
      eprint={2112.06905},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@InProceedings{nie2019adversarial,
    title={Adversarial NLI: A New Benchmark for Natural Language Understanding},
    author={Nie, Yixin
                and Williams, Adina
                and Dinan, Emily
                and Bansal, Mohit
                and Weston, Jason
                and Kiela, Douwe},
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    publisher = "Association for Computational Linguistics",
}

@article{lieber2021jurassic,
  title={Jurassic-1: Technical details and evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal={White Paper. AI21 Labs},
  volume={1},
  year={2021}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@inproceedings{bansal2022data,
  title={Data scaling laws in NMT: The effect of noise and architecture},
  author={Bansal, Yamini and Ghorbani, Behrooz and Garg, Ankush and Zhang, Biao and Cherry, Colin and Neyshabur, Behnam and Firat, Orhan},
  booktitle={International Conference on Machine Learning},
  pages={1466--1482},
  year={2022},
  organization={PMLR}
}

@misc{clark2022unifiedscalinglawsrouted,
      title={Unified Scaling Laws for Routed Language Models}, 
      author={Aidan Clark and Diego de las Casas and Aurelia Guy and Arthur Mensch and Michela Paganini and Jordan Hoffmann and Bogdan Damoc and Blake Hechtman and Trevor Cai and Sebastian Borgeaud and George van den Driessche and Eliza Rutherford and Tom Hennigan and Matthew Johnson and Katie Millican and Albin Cassirer and Chris Jones and Elena Buchatskaya and David Budden and Laurent Sifre and Simon Osindero and Oriol Vinyals and Jack Rae and Erich Elsen and Koray Kavukcuoglu and Karen Simonyan},
      year={2022},
      eprint={2202.01169},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2202.01169}, 
}

@article{villalobos2022will,
  title={Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning},
  author={Villalobos, Pablo and Sevilla, Jaime and Heim, Lennart and Besiroglu, Tamay and Hobbhahn, Marius and Ho, Anson},
  journal={arXiv preprint arXiv:2211.04325},
  year={2022}
}

@article{geiping2022cramming,
  title={Cramming: Training a Language Model on a Single GPU in One Day},
  author={Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2212.14034},
  year={2022}
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.15556}, 
}

@article{vu2022overcoming,
  title={Overcoming Catastrophic Forgetting in Zero-Shot Cross-Lingual Generation},
  author={Vu, Tu and Barua, Aditya and Lester, Brian and Cer, Daniel and Iyyer, Mohit and Constant, Noah},
  journal={arXiv preprint arXiv:2205.12647},
  year={2022}
}


@article{fried2022incoder,
  title={Incoder: A generative model for code infilling and synthesis},
  author={Fried, Daniel and Aghajanyan, Armen and Lin, Jessy and Wang, Sida and Wallace, Eric and Shi, Freda and Zhong, Ruiqi and Yih, Wen-tau and Zettlemoyer, Luke and Lewis, Mike},
  journal={arXiv preprint arXiv:2204.05999},
  year={2022}
}

@article{roberts2022t5x,
  url = {https://arxiv.org/abs/2203.17189},
  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},
  title = {Scaling Up Models and Data with $\texttt{t5x}$ and $\texttt{seqio}$},
  
  publisher = {arXiv},
  journal={arXiv preprint arXiv:2203.17189},

  year = {2022},
}

@misc{scao2021data,
      title={How Many Data Points is a Prompt Worth?}, 
      author={Teven Le Scao and Alexander M. Rush},
      year={2021},
      eprint={2103.08493},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{tay2022transcending,
  title={Transcending scaling laws with 0.1\% extra compute},
  author={Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q and So, David R and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and others},
  journal={arXiv preprint arXiv:2210.11399},
  year={2022}
}

@article{zhang2019paws,
  title={PAWS: Paraphrase adversaries from word scrambling},
  author={Zhang, Yuan and Baldridge, Jason and He, Luheng},
  journal={arXiv preprint arXiv:1904.01130},
  year={2019}
}

@article{perez2021true,
  title={True few-shot learning with language models},
  author={Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={11054--11070},
  year={2021}
}

@article{yang2019paws,
  title={PAWS-X: A cross-lingual adversarial dataset for paraphrase identification},
  author={Yang, Yinfei and Zhang, Yuan and Tar, Chris and Baldridge, Jason},
  journal={arXiv preprint arXiv:1908.11828},
  year={2019}
}

@misc{liu2022fewshot,
      title={Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning}, 
      author={Haokun Liu and Derek Tam and Mohammed Muqeeth and Jay Mohta and Tenghao Huang and Mohit Bansal and Colin Raffel},
      year={2022},
      eprint={2205.05638},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{muennighoff2022sgpt,
      title={SGPT: GPT Sentence Embeddings for Semantic Search}, 
      author={Niklas Muennighoff},
      year={2022},
      eprint={2202.08904},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{yong2022adapting,
  title={Adapting BigScience Multilingual Model to Unseen Languages},
  author={Yong, Zheng-Xin and Nikoulina, Vassilina},
  journal={arXiv preprint arXiv:2204.04873},
  year={2022}
}

@misc{morris2023text,
      title={Text Embeddings Reveal (Almost) As Much As Text}, 
      author={John X. Morris and Volodymyr Kuleshov and Vitaly Shmatikov and Alexander M. Rush},
      year={2023},
      eprint={2310.06816},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{asai2022taskaware,
      title={Task-aware Retrieval with Instructions}, 
      author={Akari Asai and Timo Schick and Patrick Lewis and Xilun Chen and Gautier Izacard and Sebastian Riedel and Hannaneh Hajishirzi and Wen-tau Yih},
      year={2022},
      eprint={2211.09260},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2020sbertwk,
      title={SBERT-WK: A Sentence Embedding Method by Dissecting BERT-based Word Models}, 
      author={Bin Wang and C. -C. Jay Kuo},
      year={2020},
      eprint={2002.06652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{dhole2022nlaugmenter,
      title={NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation}, 
      author={Kaustubh D. Dhole and Varun Gangal and Sebastian Gehrmann and Aadesh Gupta and Zhenhao Li and Saad Mahamood and Abinaya Mahendiran and Simon Mille and Ashish Shrivastava and Samson Tan and others},
      year={2022},
      eprint={2112.02721},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zaken2022bitfit,
      title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models}, 
      author={Elad Ben Zaken and Shauli Ravfogel and Yoav Goldberg},
      year={2022},
      eprint={2106.10199},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2022supernaturalinstructions,
      title={Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks}, 
      author={Yizhong Wang and Swaroop Mishra and Pegah Alipoormolabashi and Yeganeh Kordi and Amirreza Mirzaei and Anjana Arunkumar and Arjun Ashok and Arut Selvan Dhanasekaran and Atharva Naik and David Stap and others},
      year={2022},
      eprint={2204.07705},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{phang2020english,
  title={English intermediate-task training improves zero-shot cross-lingual transfer too},
  author={Phang, Jason and Calixto, Iacer and Htut, Phu Mon and Pruksachatkun, Yada and Liu, Haokun and Vania, Clara and Kann, Katharina and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2005.13013},
  year={2020}
}

@inproceedings{kudo-richardson-2018-sentencepiece,
    title = "{S}entence{P}iece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing",
    author = "Kudo, Taku  and
      Richardson, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D18-2012",
    doi = "10.18653/v1/D18-2012",
    pages = "66--71",
    abstract = "This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at \url{https://github.com/google/sentencepiece}.",
}

@inproceedings{heafield-2011-kenlm,
    title = "{K}en{LM}: Faster and Smaller Language Model Queries",
    author = "Heafield, Kenneth",
    booktitle = "Proceedings of the Sixth Workshop on Statistical Machine Translation",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-2123",
    pages = "187--197",
}

@misc{kandpal2022deduplicating,
      title={Deduplicating Training Data Mitigates Privacy Risks in Language Models}, 
      author={Nikhil Kandpal and Eric Wallace and Colin Raffel},
      year={2022},
      eprint={2202.06539},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}


@inproceedings{NakkiranNS21,
  author       = {Preetum Nakkiran and
                  Behnam Neyshabur and
                  Hanie Sedghi},
  title        = {The Deep Bootstrap Framework: Good Online Learners are Good Offline
                  Generalizers},
  booktitle    = {9th International Conference on Learning Representations, {ICLR} 2021,
                  Virtual Event, Austria, May 3-7, 2021},
  publisher    = {OpenReview.net},
  year         = {2021},
  url          = {https://openreview.net/forum?id=guetrIHLFGI},
  timestamp    = {Wed, 23 Jun 2021 17:36:39 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/NakkiranNS21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ivison2023camels,
      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, 
      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
      eprint={2311.10702},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{muennighoff2023scaling,
      title={Scaling Data-Constrained Language Models}, 
      author={Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
      year={2023},
      eprint={2305.16264},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{reed2022generalist,
      title={A Generalist Agent}, 
      author={Scott Reed and Konrad Zolna and Emilio Parisotto and Sergio Gomez Colmenarejo and Alexander Novikov and Gabriel Barth-Maron and Mai Gimenez and Yury Sulsky and Jackie Kay and Jost Tobias Springenberg and Tom Eccles and Jake Bruce and Ali Razavi and Ashley Edwards and Nicolas Heess and Yutian Chen and Raia Hadsell and Oriol Vinyals and Mahyar Bordbar and Nando de Freitas},
      year={2022},
      eprint={2205.06175},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{allal2023santacoder,
      title={SantaCoder: don't reach for the stars!}, 
      author={Loubna Ben Allal and Raymond Li and Denis Kocetkov and Chenghao Mou and Christopher Akiki and Carlos Munoz Ferrandis and Niklas Muennighoff and Mayank Mishra and Alex Gu and Manan Dey and others},
      year={2023},
      eprint={2301.03988},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{kamath2019deep,
  title={Deep learning for NLP and speech recognition},
  author={Kamath, Uday and Liu, John and Whitaker, James},
  volume={84},
  year={2019},
  publisher={Springer},
  url={https://link.springer.com/book/10.1007/978-3-030-14596-5}
}

@misc{min2020knowledge,
      title={Knowledge Guided Text Retrieval and Reading for Open Domain Question Answering}, 
      author={Sewon Min and Danqi Chen and Luke Zettlemoyer and Hannaneh Hajishirzi},
      year={2020},
      eprint={1911.03868},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{asai2023retrieval,
  title={Retrieval-based Language Models and Applications},
  author={Asai, Akari and Min, Sewon and Zhong, Zexuan and Chen, Danqi},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts)},
  pages={41--46},
  year={2023},
  url={https://aclanthology.org/2023.acl-tutorials.6/}
}

@misc{gao2024retrievalaugmented,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Qianyu Guo and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhu2024large,
      title={Large Language Models for Information Retrieval: A Survey}, 
      author={Yutao Zhu and Huaying Yuan and Shuting Wang and Jiongnan Liu and Wenhan Liu and Chenlong Deng and Haonan Chen and Zhicheng Dou and Ji-Rong Wen},
      year={2024},
      eprint={2308.07107},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hossain2018comprehensive,
      title={A Comprehensive Survey of Deep Learning for Image Captioning}, 
      author={Md. Zakir Hossain and Ferdous Sohel and Mohd Fairuz Shiratuddin and Hamid Laga},
      year={2018},
      eprint={1810.04020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{radford2021learning,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{iyer2023optiml,
      title={OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization}, 
      author={Srinivasan Iyer and Xi Victoria Lin and Ramakanth Pasunuru and Todor Mihaylov and Daniel Simig and Ping Yu and Kurt Shuster and Tianlu Wang and Qing Liu and Punit Singh Koura and Xian Li and Brian O'Horo and Gabriel Pereyra and Jeff Wang and Christopher Dewan and Asli Celikyilmaz and Luke Zettlemoyer and Ves Stoyanov},
      year={2023},
      eprint={2212.12017},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{luo2023sail,
      title={SAIL: Search-Augmented Instruction Learning}, 
      author={Hongyin Luo and Yung-Sung Chuang and Yuan Gong and Tianhua Zhang and Yoon Kim and Xixin Wu and Danny Fox and Helen Meng and James Glass},
      year={2023},
      eprint={2305.15225},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{asai2023selfrag,
      title={Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection}, 
      author={Akari Asai and Zeqiu Wu and Yizhong Wang and Avirup Sil and Hannaneh Hajishirzi},
      year={2023},
      eprint={2310.11511},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{feng2022languageagnostic,
      title={Language-agnostic BERT Sentence Embedding}, 
      author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},
      year={2022},
      eprint={2007.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2024bge,
      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, 
      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2402.03216},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mccann2018natural,
      title={The Natural Language Decathlon: Multitask Learning as Question Answering}, 
      author={Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard Socher},
      year={2018},
      eprint={1806.08730},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yasunaga2023retrievalaugmented,
      title={Retrieval-Augmented Multimodal Language Modeling}, 
      author={Michihiro Yasunaga and Armen Aghajanyan and Weijia Shi and Rich James and Jure Leskovec and Percy Liang and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
      year={2023},
      eprint={2211.12561},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{borgeaud2022improving,
      title={Improving language models by retrieving from trillions of tokens}, 
      author={Sebastian Borgeaud and Arthur Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George van den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and others},
      year={2022},
      eprint={2112.04426},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shi2023replug,
      title={REPLUG: Retrieval-Augmented Black-Box Language Models}, 
      author={Weijia Shi and Sewon Min and Michihiro Yasunaga and Minjoon Seo and Rich James and Mike Lewis and Luke Zettlemoyer and Wen-tau Yih},
      year={2023},
      eprint={2301.12652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ethayarajh2024kto,
      title={KTO: Model Alignment as Prospect Theoretic Optimization}, 
      author={Kawin Ethayarajh and Winnie Xu and Niklas Muennighoff and Dan Jurafsky and Douwe Kiela},
      year={2024},
      eprint={2402.01306},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{zhuo2024astraios,
      title={Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models}, 
      author={Terry Yue Zhuo and Armel Zebaze and Nitchakarn Suppattarachai and Leandro von Werra and Harm de Vries and Qian Liu and Niklas Muennighoff},
      year={2024},
      eprint={2401.00788},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{schick2023toolformer,
      title={Toolformer: Language Models Can Teach Themselves to Use Tools}, 
      author={Timo Schick and Jane Dwivedi-Yu and Roberto Dessì and Roberta Raileanu and Maria Lomeli and Luke Zettlemoyer and Nicola Cancedda and Thomas Scialom},
      year={2023},
      eprint={2302.04761},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{üstün2024aya,
      title={Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model}, 
      author={Ahmet Üstün and Viraat Aryabumi and Zheng-Xin Yong and Wei-Yin Ko and Daniel D'souza and Gbemileke Onilude and Neel Bhandari and Shivalika Singh and Hui-Lee Ooi and Amr Kayid and Freddie Vargus and Phil Blunsom and Shayne Longpre and Niklas Muennighoff and Marzieh Fadaee and Julia Kreutzer and Sara Hooker},
      year={2024},
      eprint={2402.07827},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{child2019generating,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{singh2024aya,
      title={Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning}, 
      author={Shivalika Singh and Freddie Vargus and Daniel Dsouza and Börje F. Karlsson and Abinaya Mahendiran and Wei-Yin Ko and Herumb Shandilya and Jay Patel and Deividas Mataciunas and Laura OMahony and Mike Zhang and Ramith Hettiarachchi and Joseph Wilson and Marina Machado and Luisa Souza Moura and Dominik Krzemiński and Hakimeh Fadaei and Irem Ergün and Ifeoma Okoh and Aisha Alaagib and Oshan Mudannayake and Zaid Alyafeai and Vu Minh Chien and Sebastian Ruder and Surya Guthikonda and Emad A. Alghamdi and Sebastian Gehrmann and Niklas Muennighoff and Max Bartolo and Julia Kreutzer and Ahmet Üstün and Marzieh Fadaee and Sara Hooker},
      year={2024},
      eprint={2402.06619},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{soldaini2024dolma,
      title={Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research}, 
      author={Luca Soldaini and Rodney Kinney and Akshita Bhagia and Dustin Schwenk and David Atkinson and Russell Authur and Ben Bogin and Khyathi Chandu and Jennifer Dumas and Yanai Elazar and Valentin Hofmann and Ananya Harsh Jha and Sachin Kumar and Li Lucy and Xinxi Lyu and Nathan Lambert and Ian Magnusson and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Abhilasha Ravichander and Kyle Richardson and Zejiang Shen and Emma Strubell and Nishant Subramani and Oyvind Tafjord and Pete Walsh and Luke Zettlemoyer and Noah A. Smith and Hannaneh Hajishirzi and Iz Beltagy and Dirk Groeneveld and Jesse Dodge and Kyle Lo},
      year={2024},
      eprint={2402.00159},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{guu2020realm,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{cho2021unifying,
      title={Unifying Vision-and-Language Tasks via Text Generation}, 
      author={Jaemin Cho and Jie Lei and Hao Tan and Mohit Bansal},
      year={2021},
      eprint={2102.02779},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{tay2020long,
  title={Long range arena: A benchmark for efficient transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint arXiv:2011.04006},
  year={2020}
}

@misc{huang2020embedding,
  title={Embedding-based retrieval in facebook search},
  author={Huang, Jui-Ting and Sharma, Ashish and Sun, Shuying and Xia, Li and Zhang, David and Pronin, Philip and Padmanabhan, Janani and Ottaviano, Giuseppe and Yang, Linjun},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2553--2561},
  year={2020},
  url={https://arxiv.org/abs/2006.11632}
}

@misc{jaegle2021perceiver,
      title={Perceiver: General Perception with Iterative Attention}, 
      author={Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman and Oriol Vinyals and Joao Carreira},
      year={2021},
      eprint={2103.03206},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{kaiser2017model,
      title={One Model To Learn Them All}, 
      author={Lukasz Kaiser and Aidan N. Gomez and Noam Shazeer and Ashish Vaswani and Niki Parmar and Llion Jones and Jakob Uszkoreit},
      year={2017},
      eprint={1706.05137},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lo2020s2orc,
      title={S2ORC: The Semantic Scholar Open Research Corpus}, 
      author={Kyle Lo and Lucy Lu Wang and Mark Neumann and Rodney Kinney and Dan S. Weld},
      year={2020},
      eprint={1911.02782},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  publisher={OpenAI}
}

@misc{thakur2021beir,
      title={BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models}, 
      author={Nandan Thakur and Nils Reimers and Andreas Rücklé and Abhishek Srivastava and Iryna Gurevych},
      year={2021},
      eprint={2104.08663},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{lewis2021retrievalaugmented,
      title={Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks}, 
      author={Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rocktäschel and Sebastian Riedel and Douwe Kiela},
      year={2021},
      eprint={2005.11401},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ba2016layer,
      title={Layer Normalization}, 
      author={Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
      year={2016},
      eprint={1607.06450},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{paszke2019pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{clark2020tydi,
      title={TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages}, 
      author={Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki},
      year={2020},
      eprint={2003.05002},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  url={https://aclanthology.org/Q19-1026/}
}

@misc{chen2020simple,
      title={A Simple Framework for Contrastive Learning of Visual Representations}, 
      author={Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
      year={2020},
      eprint={2002.05709},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{anil2023palm,
      title={PaLM 2 Technical Report}, 
      author={Rohan Anil and Andrew M. Dai and Orhan Firat and Melvin Johnson and Dmitry Lepikhin and Alexandre Passos and Siamak Shakeri and Emanuel Taropa and Paige Bailey and Zhifeng Chen and others},
      year={2023},
      eprint={2305.10403},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wang2022text,
      title={Text Embeddings by Weakly-Supervised Contrastive Pre-training}, 
      author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
      year={2022},
      eprint={2212.03533},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yin2016neural,
      title={Neural Generative Question Answering}, 
      author={Jun Yin and Xin Jiang and Zhengdong Lu and Lifeng Shang and Hang Li and Xiaoming Li},
      year={2016},
      eprint={1512.01337},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{sutskever2014sequence,
      title={Sequence to Sequence Learning with Neural Networks}, 
      author={Ilya Sutskever and Oriol Vinyals and Quoc V. Le},
      year={2014},
      eprint={1409.3215},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ni2021sentencet5,
      title={Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models}, 
      author={Jianmo Ni and Gustavo Hernández Ábrego and Noah Constant and Ji Ma and Keith B. Hall and Daniel Cer and Yinfei Yang},
      year={2021},
      eprint={2108.08877},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mikolov2013distributed,
      title={Distributed Representations of Words and Phrases and their Compositionality}, 
      author={Tomas Mikolov and Ilya Sutskever and Kai Chen and Greg Corrado and Jeffrey Dean},
      year={2013},
      eprint={1310.4546},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{pennington2014glove,
  title={Glove: Global vectors for word representation},
  author={Pennington, Jeffrey and Socher, Richard and Manning, Christopher D},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={1532--1543},
  year={2014},
  url={https://aclanthology.org/D14-1162/}
}

@misc{tunstall2023zephyr,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
      eprint={2310.16944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gao2022simcse,
      title={SimCSE: Simple Contrastive Learning of Sentence Embeddings}, 
      author={Tianyu Gao and Xingcheng Yao and Danqi Chen},
      year={2022},
      eprint={2104.08821},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bigcode-evaluation-harness,
  author={Ben Allal, Loubna and
          Muennighoff, Niklas and
          Kumar Umapathi, Logesh and
          Lipkin, Ben and
          von Werra, Leandro},
  title={A framework for the evaluation of code generation models},
  publisher={GitHub},
  journal={GitHub repository},
  url={https://github.com/bigcode-project/bigcode-evaluation-harness},
  year=2022,
}

@misc{lin2022truthfulqa,
      title={TruthfulQA: Measuring How Models Mimic Human Falsehoods}, 
      author={Stephanie Lin and Jacob Hilton and Owain Evans},
      year={2022},
      eprint={2109.07958},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhou2023instructionfollowingevaluationlargelanguage,
      title={Instruction-Following Evaluation for Large Language Models}, 
      author={Jeffrey Zhou and Tianjian Lu and Swaroop Mishra and Siddhartha Brahma and Sujoy Basu and Yi Luan and Denny Zhou and Le Hou},
      year={2023},
      eprint={2311.07911},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.07911}, 
}

@misc{röttger2024xstest,
      title={XSTest: A Test Suite for Identifying Exaggerated Safety Behaviours in Large Language Models}, 
      author={Paul Röttger and Hannah Rose Kirk and Bertie Vidgen and Giuseppe Attanasio and Federico Bianchi and Dirk Hovy},
      year={2024},
      eprint={2308.01263},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Remez, Tal and Rapin, J{\'e}r{\'e}my and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@misc{xiao2023cpack,
      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, 
      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},
      year={2023},
      eprint={2309.07597},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric P. Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{muennighoff2020vilio,
      title={Vilio: State-of-the-art Visio-Linguistic Models applied to Hateful Memes}, 
      author={Niklas Muennighoff},
      year={2020},
      eprint={2012.07788},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@misc{yong2023bloom1,
      title={BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting}, 
      author={Zheng-Xin Yong and Hailey Schoelkopf and Niklas Muennighoff and Alham Fikri Aji and David Ifeoluwa Adelani and Khalid Almubarak and M Saiful Bari and Lintang Sutawika and Jungo Kasai and Ahmed Baruwa and Genta Indra Winata and Stella Biderman and Edward Raff and Dragomir Radev and Vassilina Nikoulina},
      year={2023},
      eprint={2212.09535},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{petroni2021kilt,
      title={KILT: a Benchmark for Knowledge Intensive Language Tasks}, 
      author={Fabio Petroni and Aleksandra Piktus and Angela Fan and Patrick Lewis and Majid Yazdani and Nicola De Cao and James Thorne and Yacine Jernite and Vladimir Karpukhin and Jean Maillard and Vassilis Plachouras and Tim Rocktäschel and Sebastian Riedel},
      year={2021},
      eprint={2009.02252},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ouyang2022traininglanguagemodelsfollow,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.02155}, 
}

@misc{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems}, 
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{bajaj2018ms,
      title={MS MARCO: A Human Generated MAchine Reading COmprehension Dataset}, 
      author={Payal Bajaj and Daniel Campos and Nick Craswell and Li Deng and Jianfeng Gao and Xiaodong Liu and Rangan Majumder and Andrew McNamara and Bhaskar Mitra and Tri Nguyen and Mir Rosenberg and Xia Song and Alina Stoica and Saurabh Tiwary and Tong Wang},
      year={2018},
      eprint={1611.09268},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{quora-question-pairs,
    author = {DataCanary and hilfialkaff and Lili Jiang, Meg Risdal and Nikhil Dandekar and tomtung},
    title = {Quora Question Pairs},
    publisher = {Kaggle},
    year = {2017},
    url = {https://kaggle.com/competitions/quora-question-pairs}
}

@misc{xie2023t2ranking,
      title={T2Ranking: A large-scale Chinese Benchmark for Passage Ranking}, 
      author={Xiaohui Xie and Qian Dong and Bingning Wang and Feiyang Lv and Ting Yao and Weinan Gan and Zhijing Wu and Xiangsheng Li and Haitao Li and Yiqun Liu and Jin Ma},
      year={2023},
      eprint={2304.03679},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{thorne2018fever,
      title={FEVER: a large-scale dataset for Fact Extraction and VERification}, 
      author={James Thorne and Andreas Vlachos and Christos Christodoulopoulos and Arpit Mittal},
      year={2018},
      eprint={1803.05355},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2021mr,
      title={Mr. TyDi: A Multi-lingual Benchmark for Dense Retrieval}, 
      author={Xinyu Zhang and Xueguang Ma and Peng Shi and Jimmy Lin},
      year={2021},
      eprint={2108.08787},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhang2022making,
      title={Making a MIRACL: Multilingual Information Retrieval Across a Continuum of Languages}, 
      author={Xinyu Zhang and Nandan Thakur and Odunayo Ogundepo and Ehsan Kamalloo and David Alfonso-Hermelo and Xiaoguang Li and Qun Liu and Mehdi Rezagholizadeh and Jimmy Lin},
      year={2022},
      eprint={2210.09984},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{rajpurkar2016squad,
      title={SQuAD: 100,000+ Questions for Machine Comprehension of Text}, 
      author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
      year={2016},
      eprint={1606.05250},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{karpukhin2020dense,
      title={Dense Passage Retrieval for Open-Domain Question Answering}, 
      author={Vladimir Karpukhin and Barlas Oğuz and Sewon Min and Patrick Lewis and Ledell Wu and Sergey Edunov and Danqi Chen and Wen-tau Yih},
      year={2020},
      eprint={2004.04906},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yang2018hotpotqa,
      title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering}, 
      author={Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},
      year={2018},
      eprint={1809.09600},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{conneau2018senteval,
      title={SentEval: An Evaluation Toolkit for Universal Sentence Representations}, 
      author={Alexis Conneau and Douwe Kiela},
      year={2018},
      eprint={1803.05449},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{conneau2018supervised,
      title={Supervised Learning of Universal Sentence Representations from Natural Language Inference Data}, 
      author={Alexis Conneau and Douwe Kiela and Holger Schwenk and Loic Barrault and Antoine Bordes},
      year={2018},
      eprint={1705.02364},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{su2023embedder,
      title={One Embedder, Any Task: Instruction-Finetuned Text Embeddings}, 
      author={Hongjin Su and Weijia Shi and Jungo Kasai and Yizhong Wang and Yushi Hu and Mari Ostendorf and Wen-tau Yih and Noah A. Smith and Luke Zettlemoyer and Tao Yu},
      year={2023},
      eprint={2212.09741},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{reimers2019sentencebert,
      title={Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}, 
      author={Nils Reimers and Iryna Gurevych},
      year={2019},
      eprint={1908.10084},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ni2021large,
      title={Large Dual Encoders Are Generalizable Retrievers}, 
      author={Jianmo Ni and Chen Qu and Jing Lu and Zhuyun Dai and Gustavo Hernández Ábrego and Ji Ma and Vincent Y. Zhao and Yi Luan and Keith B. Hall and Ming-Wei Chang and Yinfei Yang},
      year={2021},
      eprint={2112.07899},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@article{jha2023limit,
  title={LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms},
  author={Jha, Aditi and Havens, Sam and Dohmann, Jeremey and Trott, Alex and Portes, Jacob},
  journal={arXiv preprint arXiv:2311.13133},
  year={2023}
}

@misc{zhang2023language,
      title={Language Models are Universal Embedders}, 
      author={Xin Zhang and Zehan Li and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang and Min Zhang},
      year={2023},
      eprint={2310.08232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{günther2023jina,
      title={Jina Embeddings: A Novel Set of High-Performance Sentence Embedding Models}, 
      author={Michael Günther and Louis Milliken and Jonathan Geuter and Georgios Mastrapas and Bo Wang and Han Xiao},
      year={2023},
      eprint={2307.11224},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{ma2023finetuning,
      title={Fine-Tuning LLaMA for Multi-Stage Text Retrieval}, 
      author={Xueguang Ma and Liang Wang and Nan Yang and Furu Wei and Jimmy Lin},
      year={2023},
      eprint={2310.08319},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@misc{günther2024jina,
      title={Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long Documents}, 
      author={Michael Günther and Jackmin Ong and Isabelle Mohr and Alaeddine Abdessalem and Tanguy Abel and Mohammad Kalim Akram and Susana Guzman and Georgios Mastrapas and Saba Sturua and Bo Wang and Maximilian Werk and Nan Wang and Han Xiao},
      year={2024},
      eprint={2310.19923},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{raposo2024mixtureofdepthsdynamicallyallocatingcompute,
      title={Mixture-of-Depths: Dynamically allocating compute in transformer-based language models}, 
      author={David Raposo and Sam Ritter and Blake Richards and Timothy Lillicrap and Peter Conway Humphreys and Adam Santoro},
      year={2024},
      eprint={2404.02258},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02258}, 
}

@misc{press2017usingoutputembeddingimprove,
      title={Using the Output Embedding to Improve Language Models}, 
      author={Ofir Press and Lior Wolf},
      year={2017},
      eprint={1608.05859},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1608.05859}, 
}

@misc{gao2020pile800gbdatasetdiverse,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.00027}, 
}

@misc{lieber2024jamba,
      title={Jamba: A Hybrid Transformer-Mamba Language Model}, 
      author={Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
      year={2024},
      eprint={2403.19887},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{wei2023skywork,
      title={Skywork: A More Open Bilingual Foundation Model}, 
      author={Tianwen Wei and Liang Zhao and Lichang Zhang and Bo Zhu and Lijie Wang and Haihua Yang and Biye Li and Cheng Cheng and Weiwei Lü and Rui Hu and Chenxia Li and Liu Yang and Xilin Luo and Xuejie Wu and Lunan Liu and Wenjun Cheng and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Lei Lin and Xiaokun Wang and Yutuan Ma and Chuanhai Dong and Yanqi Sun and Yifu Chen and Yongyi Peng and Xiaojuan Liang and Shuicheng Yan and Han Fang and Yahui Zhou},
      year={2023},
      eprint={2310.19341},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{wei2024skyworkmoe,
      title={Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models}, 
      author={Tianwen Wei and Bo Zhu and Liang Zhao and Cheng Cheng and Biye Li and Weiwei Lü and Peng Cheng and Jianhao Zhang and Xiaoyu Zhang and Liang Zeng and Xiaokun Wang and Yutuan Ma and Rui Hu and Shuicheng Yan and Han Fang and Yahui Zhou},
      year={2024},
      eprint={2406.06563},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{krajewski2024scaling,
      title={Scaling Laws for Fine-Grained Mixture of Experts}, 
      author={Jakub Krajewski and Jan Ludziejewski and Kamil Adamczewski and Maciej Pióro and Michał Krutul and Szymon Antoniak and Kamil Ciebiera and Krystian Król and Tomasz Odrzygóźdź and Piotr Sankowski and Marek Cygan and Sebastian Jaszczur},
      year={2024},
      eprint={2402.07871},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{allenzhu2024physics,
      title={Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2024},
      eprint={2404.05405},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{transformer-math-eleutherai,
  title = {Transformer Math 101},
  author = {Anthony, Quentin and Biderman, Stella and Schoelkopf, Hailey},
  howpublished = {\url{https://blog.eleuther.ai/transformer-math/}},
  year = {2023}
}

@misc{groeneveld2024olmo,
      title={OLMo: Accelerating the Science of Language Models}, 
      author={Dirk Groeneveld and Iz Beltagy and Pete Walsh and Akshita Bhagia and Rodney Kinney and Oyvind Tafjord and Ananya Harsh Jha and Hamish Ivison and Ian Magnusson and Yizhong Wang and Shane Arora and David Atkinson and Russell Authur and Khyathi Raghavi Chandu and Arman Cohan and Jennifer Dumas and Yanai Elazar and Yuling Gu and Jack Hessel and Tushar Khot and William Merrill and Jacob Morrison and Niklas Muennighoff and Aakanksha Naik and Crystal Nam and Matthew E. Peters and Valentina Pyatkin and Abhilasha Ravichander and Dustin Schwenk and Saurabh Shah and Will Smith and Emma Strubell and Nishant Subramani and Mitchell Wortsman and Pradeep Dasigi and Nathan Lambert and Kyle Richardson and Luke Zettlemoyer and Jesse Dodge and Kyle Lo and Luca Soldaini and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2402.00838},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{biderman2024lessons,
      title={Lessons from the Trenches on Reproducible Evaluation of Language Models}, 
      author={Stella Biderman and Hailey Schoelkopf and Lintang Sutawika and Leo Gao and Jonathan Tow and Baber Abbasi and Alham Fikri Aji and Pawan Sasanka Ammanamanchi and Sidney Black and Jordan Clive and Anthony DiPofi and Julen Etxaniz and Benjamin Fattori and Jessica Zosa Forde and Charles Foster and Jeffrey Hsu and Mimansa Jaiswal and Wilson Y. Lee and Haonan Li and Charles Lovering and Niklas Muennighoff and Ellie Pavlick and Jason Phang and Aviya Skowron and Samson Tan and Xiangru Tang and Kevin A. Wang and Genta Indra Winata and François Yvon and Andy Zou},
      year={2024},
      eprint={2405.14782},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shen2023mixtureofexperts,
      title={Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models}, 
      author={Sheng Shen and Le Hou and Yanqi Zhou and Nan Du and Shayne Longpre and Jason Wei and Hyung Won Chung and Barret Zoph and William Fedus and Xinyun Chen and Tu Vu and Yuexin Wu and Wuyang Chen and Albert Webson and Yunxuan Li and Vincent Zhao and Hongkun Yu and Kurt Keutzer and Trevor Darrell and Denny Zhou},
      year={2023},
      eprint={2305.14705},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{fedus2022switch,
      title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity}, 
      author={William Fedus and Barret Zoph and Noam Shazeer},
      year={2022},
      eprint={2101.03961},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shazeer2017outrageously,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
      eprint={1701.06538},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shen2024jetmoe,
      title={JetMoE: Reaching Llama2 Performance with 0.1M Dollars}, 
      author={Yikang Shen and Zhen Guo and Tianle Cai and Zengyi Qin},
      year={2024},
      eprint={2404.07413},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xue2024openmoe,
      title={OpenMoE: An Early Effort on Open Mixture-of-Experts Language Models}, 
      author={Fuzhao Xue and Zian Zheng and Yao Fu and Jinjie Ni and Zangwei Zheng and Wangchunshu Zhou and Yang You},
      year={2024},
      eprint={2402.01739},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{micikevicius2018mixed,
      title={Mixed Precision Training}, 
      author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
      year={2018},
      eprint={1710.03740},
      archivePrefix={arXiv},
      primaryClass={id='cs.AI' full_name='Artificial Intelligence' is_active=True alt_name=None in_archive='cs' is_general=False description='Covers all areas of AI except Vision, Robotics, Machine Learning, Multiagent Systems, and Computation and Language (Natural Language Processing), which have separate subject areas. In particular, includes Expert Systems, Theorem Proving (although this may overlap with Logic in Computer Science), Knowledge Representation, Planning, and Uncertainty in AI. Roughly includes material in ACM Subject Classes I.2.0, I.2.1, I.2.3, I.2.4, I.2.8, and I.2.11.'}
}

@misc{rajbhandari2020zero,
      title={ZeRO: Memory Optimizations Toward Training Trillion Parameter Models}, 
      author={Samyam Rajbhandari and Jeff Rasley and Olatunji Ruwase and Yuxiong He},
      year={2020},
      eprint={1910.02054},
      archivePrefix={arXiv},
      primaryClass={id='cs.LG' full_name='Machine Learning' is_active=True alt_name=None in_archive='cs' is_general=False description='Papers on all aspects of machine learning research (supervised, unsupervised, reinforcement learning, bandit problems, and so on) including also robustness, explanation, fairness, and methodology. cs.LG is also an appropriate primary category for applications of machine learning methods.'}
}

@misc{liu2023llm360,
      title={LLM360: Towards Fully Transparent Open-Source LLMs}, 
      author={Zhengzhong Liu and Aurick Qiao and Willie Neiswanger and Hongyi Wang and Bowen Tan and Tianhua Tao and Junbo Li and Yuqi Wang and Suqi Sun and Omkar Pangarkar and Richard Fan and Yi Gu and Victor Miller and Yonghao Zhuang and Guowei He and Haonan Li and Fajri Koto and Liping Tang and Nikhil Ranjan and Zhiqiang Shen and Xuguang Ren and Roberto Iriondo and Cun Mu and Zhiting Hu and Mark Schulze and Preslav Nakov and Tim Baldwin and Eric P. Xing},
      year={2023},
      eprint={2312.06550},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{deepseekai2024deepseekv2,
      title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Hanwei Xu and Hao Yang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jin Chen and Jingyang Yuan and Junjie Qiu and Junxiao Song and Kai Dong and Kaige Gao and Kang Guan and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruizhe Pan and Runxin Xu and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Size Zheng and T. Wang and Tian Pei and Tian Yuan and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Liu and Xin Xie and Xingkai Yu and Xinnan Song and Xinyi Zhou and Xinyu Yang and Xuan Lu and Xuecheng Su and Y. Wu and Y. K. Li and Y. X. Wei and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Zheng and Yichao Zhang and Yiliang Xiong and Yilong Zhao and Ying He and Ying Tang and Yishi Piao and Yixin Dong and Yixuan Tan and Yiyuan Liu and Yongji Wang and Yongqiang Guo and Yuchen Zhu and Yuduan Wang and Yuheng Zou and Yukun Zha and Yunxian Ma and Yuting Yan and Yuxiang You and Yuxuan Liu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhewen Hao and Zhihong Shao and Zhiniu Wen and Zhipeng Xu and Zhongyu Zhang and Zhuoshu Li and Zihan Wang and Zihui Gu and Zilin Li and Ziwei Xie},
      year={2024},
      eprint={2405.04434},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}

@misc{lepikhin2020gshardscalinggiantmodels,
      title={GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding}, 
      author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
      year={2020},
      eprint={2006.16668},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.16668}, 
}

@misc{dai2024deepseekmoeultimateexpertspecialization,
      title={DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models}, 
      author={Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang},
      year={2024},
      eprint={2401.06066},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.06066}, 
}

@misc{shao2024deepseekmathpushinglimitsmathematical,
      title={DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models}, 
      author={Zhihong Shao and Peiyi Wang and Qihao Zhu and Runxin Xu and Junxiao Song and Xiao Bi and Haowei Zhang and Mingchuan Zhang and Y. K. Li and Y. Wu and Daya Guo},
      year={2024},
      eprint={2402.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.03300}, 
}

@misc{zhou2022mixtureofexperts,
      title={Mixture-of-Experts with Expert Choice Routing}, 
      author={Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Zhao and Andrew Dai and Zhifeng Chen and Quoc Le and James Laudon},
      year={2022},
      eprint={2202.09368},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhong2024lory,
      title={Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training}, 
      author={Zexuan Zhong and Mengzhou Xia and Danqi Chen and Mike Lewis},
      year={2024},
      eprint={2405.03133},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hu2024minicpm,
      title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies}, 
      author={Shengding Hu and Yuge Tu and Xu Han and Chaoqun He and Ganqu Cui and Xiang Long and Zhi Zheng and Yewei Fang and Yuxiang Huang and Weilin Zhao and Xinrong Zhang and Zheng Leng Thai and Kaihuo Zhang and Chongyi Wang and Yuan Yao and Chenyang Zhao and Jie Zhou and Jie Cai and Zhongwu Zhai and Ning Ding and Chao Jia and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2404.06395},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{polo2024tinybenchmarksevaluatingllmsfewer,
      title={tinyBenchmarks: evaluating LLMs with fewer examples}, 
      author={Felipe Maia Polo and Lucas Weber and Leshem Choshen and Yuekai Sun and Gongjun Xu and Mikhail Yurochkin},
      year={2024},
      eprint={2402.14992},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.14992}, 
}

@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}

@software{togetherai,
  title = {Together AI},
  url = {https://www.together.ai/}
}

@software{fireworksai,
    title = {Fireworks AI},
    url = {https://fireworks.ai/}
}


@misc{zhang2024mapneohighlycapabletransparent,
      title={MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series}, 
      author={Ge Zhang and Scott Qu and Jiaheng Liu and Chenchen Zhang and Chenghua Lin and Chou Leuang Yu and Danny Pan and Esther Cheng and Jie Liu and Qunshu Lin and Raven Yuan and Tuney Zheng and Wei Pang and Xinrun Du and Yiming Liang and Yinghao Ma and Yizhi Li and Ziyang Ma and Bill Lin and Emmanouil Benetos and Huan Yang and Junting Zhou and Kaijing Ma and Minghao Liu and Morry Niu and Noah Wang and Quehry Que and Ruibo Liu and Sine Liu and Shawn Guo and Soren Gao and Wangchunshu Zhou and Xinyue Zhang and Yizhi Zhou and Yubo Wang and Yuelin Bai and Yuhan Zhang and Yuxiang Zhang and Zenith Wang and Zhenzhu Yang and Zijian Zhao and Jiajun Zhang and Wanli Ouyang and Wenhao Huang and Wenhu Chen},
      year={2024},
      eprint={2405.19327},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.19327}, 
}

@article{xin2024deepseek,
  title={DeepSeek-Prover: Advancing Theorem Proving in LLMs through Large-Scale Synthetic Data},
  author={Xin, Huajian and Guo, Daya and Shao, Zhihong and Ren, Zhizhou and Zhu, Qihao and Liu, Bo and Ruan, Chong and Li, Wenda and Liang, Xiaodan},
  journal={arXiv preprint arXiv:2405.14333},
  year={2024}
}

@article{brown2024large,
  title={Large language monkeys: Scaling inference compute with repeated sampling},
  author={Brown, Bradley and Juravsky, Jordan and Ehrlich, Ryan and Clark, Ronald and Le, Quoc V and R{\'e}, Christopher and Mirhoseini, Azalia},
  journal={arXiv preprint arXiv:2407.21787},
  year={2024}
}

@misc{yun2024inferenceoptimalmixtureofexpertlargelanguage,
      title={Toward Inference-optimal Mixture-of-Expert Large Language Models}, 
      author={Longfei Yun and Yonghao Zhuang and Yao Fu and Eric P Xing and Hao Zhang},
      year={2024},
      eprint={2404.02852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.02852}, 
}