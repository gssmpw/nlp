\section{Related Work}
\label{sec:related}
\subsection{RGBT Object Detection}
Object detection is a fundamental task in computer vision. However, general object detection based on RGB images is vulnerable under unsatisfactory illumination conditions. The introduction of a thermal modality provides rich information around the clock and greatly improves the ability against low-light conditions. Therefore, extensive research has been conducted on RGBT object detection. 

\textbf{Fusion-based methods.} Some~\cite{iafrcnn,guan2019fusion,zhang2023tinet} propose to utilize the illumination condition to determine a reliable modality. Some researchers~\cite{kim2021uncertainty,sun2022drone,li2023multiscale} introduce uncertainty or confidence to fuse RGBT features accordingly. Others~\cite{zhang2019cross,zhou2020improving,shen2024icafusion,yuan2024c} resort to designing attention-based fusion networks to achieve adaptive fusion. Progress has been made in multi-modal feature fusion to combine the strengths of different modalities.

\textbf{Alignment-based methods.} 
However, these methods all assume that the multispectral image pair is geometrically aligned, which is impractical in the real world. 
\textit{(1) Weakly misalignment.} Zhang \etal~\cite{zhang2019weakly} first study the impact of the position shift problem in RGBT object detection and propose AR-CNN to align RoI region features. TSRA~\cite{yuan2022translation,yuan2024improving} extends AR-CNN to oriented object detection, taking into account angle and scale deviations. These methods manage to predict the shift pattern for each instance but demand paired multi-modal annotations, which is labor-intensive. C$^2$Former~\cite{yuan2024c} utilizes the pairwise correlation modeling capability of the Transformer to adaptively obtain calibrated and complementary features. OAFA~\cite{chen2024weakly} predicts the offset of features based on the common subspace learning of RGBT data. The above two methods do not require paired annotations, but rely on implicit calibration. 
\textit{(2) Large misalignment.} Napat \etal~\cite{napat2021misalignment} point out that the previous works are limited in weak misalignment and propose a modal-wise regression and a multi-modal IoU to tackle the large misalignment situation. However, they study large misalignments only under manually generated position shifts instead of real-world RGBT shifts, which consist of different directions and distances even in one image pair. 
\textit{(3) Prominent position shift.} This problem is first proposed by drone-based RGBT tiny person detection~\cite{zhang2023drone}. Different from vehicle-viewed RGBT misalignment, where most objects remain overlapped in different modalities, for example, a 5-pixel position shift will cause the reference GT box to drift away from the sensed tiny objects in drone view. QFDet~\cite{zhang2023drone} simply relies on the pooling layer to alleviate the impact of position shift on the RGBT features, which is insufficient.
In our work, CBC-Head only requires annotations for the reference modality to perform RGBT detection and automatically yields better annotations for the sensed modality. With explicit supervision, our method manages to learn informative RGBT representations.

\subsection{Learning with Noisy Annotations}
To improve the ability of deep neural networks against noise in annotations has been studied in various computer vision tasks. In image classification, various methods~\cite{yi2019probabilistic,zheng2021meta} are developed to identify noisy labels and further correct them. In object detection, the impact of label noise is more severe and complex since this task performs label classification and bounding box regression simultaneously. Chadwick \etal~\cite{chadwick2019training} firstly study the impact of noisy labels in object detection and propose a co-teaching method to mitigate the effects of label noise in object detection. Li \etal~\cite{li2020learning} propose a cleanliness score to mitigate the influence brought by noisy anchors. Liu \etal~\cite{liu2022robust} introduce multiple instance learning (MIL) to address the inaccurate bounding boxes problem. Wu \etal~\cite{wu2023spatial} also utilize the MIL technique but propose a spatial self-distillation based object detector to exploit spatial and category information simultaneously. 

These methods effectively improve the robustness of detectors under real-world noisy environments and simulated noisy settings. Nevertheless, a fundamental assumption of these methods is that there are many reliable and high-quality annotations to supervise the network. In our condition, we view the shifted bounding boxes as noisy annotations for the sensed modality. In this way, all of the annotations are noisy, making the problem distinct and challenging. 

\subsection{Unsupervised Domain Adaptation}
Unsupervised domain adaptation~\cite{li2022cross,he2022cross,kennerley20232pcnet,chen2023confidence,zhai2024maximizing} aims to learn transferable features to reduce the discrepancy between a labeled source and an unlabeled target domain. The task is more challenging in object detection because of the discrimination between objects and backgrounds. Mean Teacher~\cite{tarvainen2017mean} is proposed for semi-supervised learning. AT~\cite{li2022cross}, TDD~\cite{he2022cross}, and 2PCNet~\cite{kennerley20232pcnet} introduce the Mean Teacher framework into the UDA setting to improve the performance of cross-domain detection. In our problem setting, the reference modality is labeled while the sensed modality is unlabeled. Therefore, we take the reference modality as the source domain and the sensed as the target domain.