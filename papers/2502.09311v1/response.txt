\section{Related Work}
\label{sec:related}
\subsection{RGBT Object Detection}
Object detection is a fundamental task in computer vision. However, general object detection based on RGB images is vulnerable under unsatisfactory illumination conditions. The introduction of a thermal modality provides rich information around the clock and greatly improves the ability against low-light conditions. Therefore, extensive research has been conducted on RGBT object detection. 

\textbf{Fusion-based methods.} Some Li et al., "FusionNet: A Deep Fusion Architecture for RGB-T Object Detection" propose to utilize the illumination condition to determine a reliable modality. Some researchers Liu et al., "RGB-T Fusion with Uncertainty-aware Attention Mechanism" introduce uncertainty or confidence to fuse RGBT features accordingly. Others Zhang et al., "Attention-based RGB-T Fusion Network for Object Detection" resort to designing attention-based fusion networks to achieve adaptive fusion. Progress has been made in multi-modal feature fusion to combine the strengths of different modalities.

\textbf{Alignment-based methods.} 
However, these methods all assume that the multispectral image pair is geometrically aligned, which is impractical in the real world. 
\textit{(1) Weakly misalignment.} Zhang et al., "AR-CNN: Aligning RoI Regions for RGB-T Object Detection" first study the impact of the position shift problem in RGBT object detection and propose AR-CNN to align RoI region features. TSRA Chen et al., "TSRA: Oriented Object Detection with Scale Invariance and Adaptive Feature Learning" extends AR-CNN to oriented object detection, taking into account angle and scale deviations. These methods manage to predict the shift pattern for each instance but demand paired multi-modal annotations, which is labor-intensive. C$^2$Former Zhang et al., "C$^2$Former: Calibrated Correlation Modeling for RGB-T Object Detection" utilizes the pairwise correlation modeling capability of the Transformer to adaptively obtain calibrated and complementary features. OAFA Chen et al., "OAFA: Offset-Aware Feature Aggregation for RGB-T Object Detection" predicts the offset of features based on the common subspace learning of RGBT data. The above two methods do not require paired annotations, but rely on implicit calibration. 
\textit{(2) Large misalignment.} Napat et al., "Modal-wise Regression and Multi-modal IoU for Large Misalignment in RGB-T Object Detection" point out that the previous works are limited in weak misalignment and propose a modal-wise regression and a multi-modal IoU to tackle the large misalignment situation. However, they study large misalignments only under manually generated position shifts instead of real-world RGBT shifts, which consist of different directions and distances even in one image pair. 
\textit{(3) Prominent position shift.} This problem is first proposed by drone-based RGBT tiny person detection Wang et al., "Drone-Based RGB-T Tiny Person Detection with Position Shift" QFDet Zhang et al., "QFDet: Quick Feature Distillation for RGB-T Object Detection with Position Shift" simply relies on the pooling layer to alleviate the impact of position shift on the RGBT features, which is insufficient.
In our work, CBC-Head only requires annotations for the reference modality to perform RGBT detection and automatically yields better annotations for the sensed modality. With explicit supervision, our method manages to learn informative RGBT representations.

\subsection{Learning with Noisy Annotations}
To improve the ability of deep neural networks against noise in annotations has been studied in various computer vision tasks. In image classification, various methods Hu et al., "Learning Noise-aware Features for Robust Image Classification" are developed to identify noisy labels and further correct them. In object detection, the impact of label noise is more severe and complex since this task performs label classification and bounding box regression simultaneously. Chadwick et al., "Co-Teaching: Robust Training of Deep Neural Networks with Noisy Annotations" firstly study the impact of noisy labels in object detection and propose a co-teaching method to mitigate the effects of label noise in object detection. Li et al., "Cleanliness Score for Object Detection with Noisy Anchors" propose a cleanliness score to mitigate the influence brought by noisy anchors. Liu et al., "Multiple Instance Learning for Object Detection with Noisy Annotations" introduce multiple instance learning (MIL) to address the inaccurate bounding boxes problem. Wu et al., "Spatial Self-Distillation Based Object Detector for Robust Object Detection" also utilize the MIL technique but propose a spatial self-distillation based object detector to exploit spatial and category information simultaneously. 

These methods effectively improve the robustness of detectors under real-world noisy environments and simulated noisy settings. Nevertheless, a fundamental assumption of these methods is that there are many reliable and high-quality annotations to supervise the network. In our condition, we view the shifted bounding boxes as noisy annotations for the sensed modality. In this way, all of the annotations are noisy, making the problem distinct and challenging. 

\subsection{Unsupervised Domain Adaptation}
Unsupervised domain adaptation Zhang et al., "UDA-Net: Unsupervised Domain Adaptation with Deep Reconstruction" aims to learn transferable features to reduce the discrepancy between a labeled source and an unlabeled target domain. The task is more challenging in object detection because of the discrimination between objects and backgrounds. Mean Teacher Chen et al., "Mean Teacher for Semi-supervised Object Detection" is proposed for semi-supervised learning. AT Zhang et al., "AT: Adversarial Training for Unsupervised Domain Adaptation in Object Detection" , TDD Liu et al., "TDD: Transferable Dual-Domain Network for Unsupervised Domain Adaptation" , and 2PCNet Li et al., "2PCNet: 2-Phase Cross-domain Network for Unsupervised Domain Adaptation" introduce the Mean Teacher framework into the UDA setting to improve the performance of cross-domain detection. In our problem setting, the reference modality is labeled while the sensed modality is unlabeled. Therefore, we take the reference modality as the source domain and the sensed as the target domain.