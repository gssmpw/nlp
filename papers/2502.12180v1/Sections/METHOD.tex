\section{METHOD}
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\textwidth]{Figures/Method_Overview.png} % 使用 \textwidth 来指定图片宽度为双栏宽度
    \caption{Overview of ClusMFL. In this figure, PET-only instances are used as examples of single-modality instances in local training. Different patterns represent different modalities, and different colors indicate different labels.}  
    \label{fig:overview}
\end{figure*}
\subsection{Preliminary}
\label{sec: preliminary}
In this study, we adopt a typical architecture for multimodal models, which includes two encoders—one for each modality—and a classifier. Let \( f_P \) and \( f_M \) denote the encoders for PET modality and MRI modality, respectively. The encoders \( f_P \) and \( f_M \) are responsible for extracting the relevant features from each modality. The model also includes a classifier, denoted as \( g \), which concatenates the embeddings from the encoders and performs the final prediction.

During the inference stage, for instances containing both modalities, the input data \( \mathbf{x}_P \) and \( \mathbf{x}_M \) are processed through their respective encoders \( f_P \) and \( f_M \). Specifically, the feature embeddings are obtained as \( \mathbf{z}_P = f_P(\mathbf{x}_P) \) and \( \mathbf{z}_M = f_M(\mathbf{x}_M)\), which are subsequently concatenated and passed to the classifier \( g \) for the final prediction, \( \hat{y} = g(\mathbf{z}_P, \mathbf{z}_M) \). For instances with only a single modality, the feature embedding of the missing modality is replaced with a tensor of zeros, denoted as \( \mathbf{0} \). 
% For example, in the case of PET-only instances, the feature embedding \( \mathbf{z}_M \) is replaced by a tensor of zeros, \textit{i.e.}, \( \mathbf{z}_M = \mathbf{0} \).

The federated training process repeats the construction of the pool of cluster centers and cluster sizes (\textit{i.e.}, \( \mathbf{C}^{\text{global}} \) and \( \mathbf{S}^{\text{global}} \)), followed by local training and aggregation in each round, which are explained in detail in the following sections. We provide an overview of the construction of  \( \mathbf{C}^{\text{global}} \) and \( \mathbf{S}^{\text{global}} \) and local training in \Cref{fig:overview}.

\subsection{Constructing The Pool of Cluster Centers}
Unlike traditional prototype learning methods, which use the mean of features as the prototype and result in a shift between individual samples and the prototype, this study adopts the FINCH \cite{FINCH} algorithm to construct a pool of cluster centers, thereby more effectively representing the clients' data distribution.
% Unlike traditional prototype learning methods, which fail to adequately represent the data distribution due to the significant shift between individual samples and the prototype, this study employs the FINCH algorithm to cluster feature embeddings and more accurately capture the underlying data distribution.

On each client, we begin by applying the FINCH clustering algorithm to calculate the local cluster centers for feature embeddings of each pair of modality and label. The resulting cluster centers, along with cluster sizes, are then uploaded to the server. For instance, consider the modality PET. The feature embeddings associated with modality PET are extracted as \( \mathbf{Z}_P = f_P(\mathbf{X}_P) \), where $\mathbf{X}_P$ represents all PET data on this client. The FINCH algorithm is then applied to \( \mathbf{Z}_P \) for clustering, yielding the corresponding cluster centers and cluster sizes. Specifically, for label \( j \) in modality PET, FINCH identifies the cluster centers as:
\begin{equation}
    (\mathbf{C}_{P,j},\mathbf{S}_{P,j})= \text{FINCH}(\mathbf{Z}_{P,j}),
\end{equation}
where \( \mathbf{Z}_{P,j} \) denotes the feature embeddings with modality PET corresponding to label \( j \), and \( \mathbf{C}_{P,j} = ( \mathbf{c}_{P,j}^{1}, \mathbf{c}_{P,j}^{2}, \dots, \mathbf{c}_{P,j}^{K_{P,j}} )\) represents the \( K_{P,j} \) cluster centers obtained from the FINCH algorithm for label \( j \). The set \( \mathbf{S}_{P,j} =( s_{P,j}^{1}, s_{P,j}^{2}, \dots, s_{P,j}^{K_{P,j}} ) \) represents the sizes of the corresponding clusters, where each \( s_{P,j}^{k} \) denotes the number of feature embeddings assigned to the \( k \)-th cluster for label \( j \), with \( \mathbf{c}_{P,j}^{k} \) being the cluster center. $K_{P,j}$ represents the number of clusters, which is determined automatically by FINCH.

For client \(i\), the cluster centers and sizes associated with modality PET and label \(j\) are denoted as \(\mathbf{C}_{P,j}^i\) and \(\mathbf{S}_{P,j}^i\), respectively. If client \(i\) lacks data for modality PET, we set \(\mathbf{C}_{P,j}^i = \varnothing\) and \(\mathbf{S}_{P,j}^i = \varnothing\).

Once the cluster centers and cluster sizes for each client are computed, they are sent to the server, where they are collected to form a global pool of cluster centers. Specifically, the global pool of cluster centers for label \( j \) in modality PET, denoted as \( \mathbf{C}_{P,j}^{\text{global}} \), is constructed by concatenating the cluster centers from all clients:

\begin{equation}
    \mathbf{C}_{P,j}^{\text{global}} = \bigoplus_{i=1}^N \mathbf{C}_{P,j}^i.
\end{equation}
Similarly, the corresponding cluster sizes for label \( j \) are cancatenated into a global pool, denoted as \( \mathbf{S}_{P,j}^{\text{global}} \):

\begin{equation}
    \mathbf{S}_{P,j}^{\text{global}} = \bigoplus_{i=1}^N \mathbf{S}_{P,j}^i.
\end{equation}

The global pools \( \mathbf{C}_{P,j}^{\text{global}} \) and \( \mathbf{S}_{P,j}^{\text{global}} \) encapsulate the aggregated cluster centers and their respective sizes for each label \( j \) across all clients, enabling the model to leverage a comprehensive representation of the data distribution. These global pools are then distributed back to each client, allowing them to utilize the global information during local training.

The same process is performed for MRI modality. Let \( \mathbf{C}_{M,j}^i \) and \( \mathbf{S}_{M,j}^i \) represent the cluster centers and cluster sizes, respectively, for MRI and label \( j \) on client \( i \). The global pools for MRI are constructed as:

\begin{equation}
    \mathbf{C}_{M,j}^{\text{global}} = \bigoplus_{i=1}^N \mathbf{C}_{M,j}^i, \quad \mathbf{S}_{M,j}^{\text{global}} = \bigoplus_{i=1}^N \mathbf{S}_{M,j}^i.
\end{equation}

These global pools for MRI are also distributed back to the clients to ensure that the global knowledge from both modalities is available for further training and optimization.

\subsection{Feature Alignment}

To ensure that the encoders \( f_M \) and \( f_P \) extract the correct modality-specific features and mitigate overfitting under severe modality incompleteness, we combine global cluster centers and local feature embeddings and apply supervised contrastive loss for feature alignment.

For any multimodal client with dataset $\mathcal{D}_B$, let \( \mathbf{X}_P \) and \( \mathbf{X}_M \) represent all PET and MRI data on this client, respectively, along with their corresponding labels \( \mathbf{y}_P \) and \( \mathbf{y}_M \). The feature embeddings for PET modality and MRI modality are computed as \( \mathbf{Z}_P = f_P(\mathbf{X}_P) \) and  \( \mathbf{Z}_M = f_M(\mathbf{X}_M) \) respectively. To incorporate global information, the feature embeddings \( \mathbf{Z}_P \) are concatenated with the global cluster centers \( \mathbf{C}_{P}^{\text{global}} \):
\begin{equation}
    \mathbf{Z}_{P,G} = \mathbf{Z}_P \oplus \bigoplus_{j=1}^J \mathbf{C}_{P,j}^{\text{global}},
\end{equation}
where \( J \) is the total number of unique labels. Similarly, the label \( \mathbf{y}_P \) is extended as:
\begin{equation}
    \mathbf{y}_{P,G} = \mathbf{y}_P \oplus \bigoplus_{j=1}^J (j )^{|\mathbf{C}_{P,j}^{\text{global}}|},
\end{equation}
where \( ( j )^{|\mathbf{C}_{P,j}^{\text{global}}|} \) denotes a list containing label \( j \) repeated \( |\mathbf{C}_{P,j}^{\text{global}}| \) times.

We compute the supervised contrastive loss over \( \mathbf{Z}_{P,G} \). Let \( \mathcal{I} = \{ 1, 2, \dots, |\mathbf{y}_{P,G}| \} \) denote the index set, and define \( \mathcal{A}(i) = \mathcal{I} \backslash \{ i \} \) as the set of all indices excluding \( i \). The supervised contrastive loss for PET modality is defined as:
\begin{equation}
    \mathcal{L}_{\text{ctr}}^P(\mathbf{X}_P) = \mathbb{E}_{i \in \mathcal{I}} \left[ -\frac{1}{|\mathcal{P}(i)|} \sum_{p \in \mathcal{P}(i)} \log \frac{v_{i,p}}{\sum_{a \in \mathcal{A}(i)} v_{i,a}} \right],
\end{equation}
where:
\begin{itemize}
    \item \( \mathcal{P}(i) = \{ p \in \mathcal{A}(i) \mid \mathbf{y}_{P,G}^{(p)} = \mathbf{y}_{P,G}^{(i)} \} \) represents the set of indices corresponding to positive examples for \( \mathbf{Z}_{P,G}^{(i)} \).
    \item \( v_{i,p} = \exp(\text{sim}(\mathbf{Z}_{P,G}^{(i)}, \mathbf{Z}_{P,G}^{(p)}) / \tau) \), where \( \text{sim}(\cdot, \cdot) \) denotes the cosine similarity between two embeddings, and \( \tau \) is a temperature parameter.
\end{itemize}

The similar process is applied to calculate \( \mathcal{L}_{\text{ctr}}^M(\mathbf{X}_M) \) and the overall supervised contrastive loss for $\mathcal{D}_B$ is then computed as:
\begin{equation}
    \mathcal{L}_{\text{CTR}} (\mathcal{D}_B)= 
    \frac{
        |\mathbf{y}_P| \cdot \mathcal{L}_{\text{ctr}}^P(\mathbf{X}_P) 
        + 
        |\mathbf{y}_M| \cdot \mathcal{L}_{\text{ctr}}^M(\mathbf{X}_M)
    }{
        |\mathbf{y}_P| + |\mathbf{y}_M|
    }.
\end{equation}

For clients with data from only one modality, the loss is computed solely for the corresponding modality, without involving the other modality. 
% For example, for a PET-only client, the overall supervised contrastive loss \( \mathcal{L}_{\text{CTR}}(\mathcal{D}_P) \) is simply given by \( \mathcal{L}_{\text{ctr}}^P(\mathbf{X}_P) \).

\subsection{Modality Completion Loss}
To further enhance the model's classification performance in the presence of missing modalities, we use $\mathbf{C}^{\mathrm{global}}$ as approximations for feature embeddings of the missing modality. Specifically, for a PET-only instance with data \( d_P = (\mathbf{x}_P, \varnothing, y) \) or an MRI-only instance with data \( d_M = (\varnothing, \mathbf{x}_M, y) \), we first pass the available modality through the corresponding encoder to obtain the feature embedding \( \mathbf{z}_P \) and \( \mathbf{z}_M \). We then approximate the missing modality using the \( i \)-th cluster center from the corresponding global cluster centers. The prediction for each instance is given by:
\begin{equation}
\hat{y}_c^i = \begin{cases}
g(\mathbf{z}_P, \mathbf{C}_{M,y}^{\text{global},(i)}) & \text{for PET-only instance}, \\
g(\mathbf{C}_{P,y}^{\text{global},(i)}, \mathbf{z}_M) & \text{for MRI-only instance},
\end{cases}
\end{equation}
where \( \mathbf{C}_{M,y}^{\text{global},(i)} \) and \( \mathbf{C}_{P,y}^{\text{global},(i)} \) represents the \( i \)-th cluster center from \( \mathbf{C}_{M,y}^{\text{global}} \) and \( \mathbf{C}_{P,y}^{\text{global}} \) respectively.

The loss for a PET-only instance is then computed as:
\begin{equation}
\mathcal{L}_{\text{mc}}(d_P) =  \frac{1}{T_{M,y}} \sum_{i=1}^{|\mathbf{S}_{M,y}^{\text{global}}|} \mathbf{S}_{M,y}^{\text{global},(i)} \cdot \mathcal{L}_{\text{CE}}(\hat{y}_c^i, y),
\end{equation}
where \( \mathcal{L}_{\text{CE}} \) denotes the cross-entropy loss function, and \( T_{M,y} = \sum_{i=1}^{|\mathbf{S}_{M,y}^{\text{global}}|} \mathbf{S}_{M,y}^{\text{global},(i)} \).

Similarly, for an MRI-only instance, the loss is computed as:
\begin{equation}
\mathcal{L}_{\text{mc}}(d_M) =  \frac{1}{T_{P,y}} \sum_{i=1}^{|\mathbf{S}_{P,y}^{\text{global}}|} \mathbf{S}_{P,y}^{\text{global},(i)} \cdot \mathcal{L}_{\text{CE}}(\hat{y}_c^i, y),
\end{equation}
where \( T_{P,y} = \sum_{i=1}^{|\mathbf{S}_{P,y}^{\text{global}}|} \mathbf{S}_{P,y}^{\text{global},(i)} \).

For any client $i$, let \( \mathcal{D}_i^{\text{single}} = \{ d \mid d \in \mathcal{D}_i, \, d \text{ is a single modality instance} \} \) be the subset of instances with a single modality (either PET-only or MRI-only). The overall modality completion loss is calculated as:
\begin{equation}
    \mathcal{L}_{\text{MC}}(\mathcal{D}_i)=\mathbb{E}_{d\in\mathcal{D}_{\text{single}}}\left[\mathcal{L}_{\text{mc}}(d)\right].
\end{equation}

In this manner, we use the global cluster centers of the available modality to approximate the missing modality, allowing the model to still make meaningful predictions in the presence of incomplete data.



\subsection{Overall Loss Function}

For client \( i \) with dataset \( \mathcal{D}_i \), the prediction \( \hat{y}_i \) is generated as described in \Cref{sec: preliminary} for each instance \( d_i \). Let \( \hat{\mathbf{y}} = (\hat{y}_1, \hat{y}_2, \dots, \hat{y}_{n_i}) \) denote the vector of predicted labels for the \( m \) instances in the client's dataset, and \( \mathbf{y} = (y_1, y_2, \dots, y_{n_i}) \) denote the corresponding true labels. The overall loss is computed as:

\begin{equation}
\mathcal{L}(\mathcal{D}_i) = \mathcal{L}_{\text{CE}}(\hat{\mathbf{y}}, \mathbf{y}) + \lambda_1 \mathcal{L}_{\text{CTR}}(\mathcal{D}_i) + \lambda_2 \mathcal{L}_{\text{MC}}(\mathcal{D}_i),
\end{equation}
where \( \lambda_1 \) and \( \lambda_2 \) are the regularization coefficients that balance the contributions of the contrastive loss and the modality completion loss, respectively.



\subsection{Modality-Aware Aggregation}

In this section, we describe the modality-aware aggregation (MAA) method adopted in our framework. Specifically, we use different aggregation weights for different modules within the model, based on the number of instances for each modality at each client.

Let \( n_P^i \) represent the number of instances with the PET modality at client \( i \), and \( n_M^i \) represent the number of instances with the MRI modality at client \( i \). The total numbers of instances with PET and MRI modalities across all clients are denoted as \( n^{\text{total}}_P \) and \( n^{\text{total}}_M \), and are computed as:
\begin{equation}
n^{\text{total}}_P = \sum_{i=1}^{N} n_P^i, \quad n^{\text{total}}_M = \sum_{i=1}^{N} n_M^i.    
\end{equation}

For client \( i \), the aggregation weights for the encoders are computed as:
\begin{equation}
w_P^i = \frac{n_P^i}{n_{\text{total}}^P}, \quad w_M^i = \frac{n_M^i}{n_{\text{total}}^M},
\end{equation}
where \( w_P^i \) and \( w_M^i \) are the aggregation weights for encoder $f_P$ and encoder $f_M$, respectively. 

The aggregation processes for encoders $f_P$ and $f_M$ are as follows:
\begin{equation}
    \mathbf{f}_P^{\text{global}} = \sum_{i=1}^{N} w_P^i \mathbf{f}_P^i, \quad
\mathbf{f}_M^{\text{global}} = \sum_{i=1}^{N} w_M^i \mathbf{f}_M^i,
\end{equation}
where \( \mathbf{f}_P^i \), \( \mathbf{f}_M^i \) are the model parameters of the PET encoder and MRI encoder, respectively.


For the classifier \( g \), the aggregation process is given by:
\begin{equation}
    \mathbf{g}^{\text{global}} = \frac{1}{\sum_{i=1}^{N}n_i}\sum_{i=1}^{N} n_i\cdot\mathbf{g}^i,
\end{equation}
where \( \mathbf{g}^i \) denotes the model parameters for classifier $g$.

% This approach ensures that the model updates are more accurately representative of the data distribution at each client, while assigning different importance to the PET and MRI modalities based on the number of instances available at each client.

% \subsection{Communication Complexity Analysis}
               