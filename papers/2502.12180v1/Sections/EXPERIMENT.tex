\section{EXPERIMENT}
\subsection{Experimental Setup}
\input{Tables/main_result}
\begin{figure*}[h]
    \centering
    \includegraphics[width=\linewidth]{Figures/convergence.png}
    \caption{Training curves of different methods.}
    \label{fig:training curve}
\end{figure*}
% dataset description
% In this study, we use AV45-PET and VBM-MRI modality.
The brain imaging dataset used in this study is sourced from the Alzheimer's Disease Neuroimaging Initiative (ADNI) public repository~\cite{mueller2005alzheimer} and comprises 915 participants stratified into three diagnostic categories: 297 healthy controls (HC), 451 mild cognitive impairment (MCI), and 167 Alzheimer's disease (AD) patients. Multimodal neuroimaging acquisitions encompass structural Magnetic Resonance Imaging (VBM-MRI) and 18 F-florbetapir PET (AV45-PET), enabling examination of brain structure and amyloid plaque deposition, respectively.

Consistent with established neuroimaging processing pipelines \cite{barshan2015stage,zhu2010graphene}, we preprocess neuroimaging data to region-of-interest (ROI) features from each participant’s images. 
First, the multi-modality imaging scans are aligned to each participant's same visit. All imaging scans are aligned to a T1-weighted template image. 
Subsequently, the images are segmented into gray matter (GM), white matter (WM) and cerebrospinal fluid (CSF) maps. 
They are normalized to the standard Montreal Neurological Institute (MNI) space as $2 \times 2 \times 2$ mm$^3$ voxels, being smoothed with an $8$ mm full-width at half-maximum (FWHM) Gaussian kernel.
We preprocess the structural MRI scans with voxel-based morphometry (VBM) by using the SPM software \cite{ashburner2000voxel}, and register the AV45-PET scans to the MNI space by SPM. 
For both MRI and PET scans, we parcellate the brain into 90 ROIs (excluding the cerebellum and vermis) based on the AAL-90 atlas \cite{tzourio2002automated}, and computed ROI-level measures by averaging voxel-wise values within each region.

In the original dataset, each instance contains both modalities. We first split the dataset into a training set and a test set with a ratio of 1:4. For the test set, we ensure that the proportions of the three types of instances are equal, \textit{i.e.}, each type accounts for $\frac{1}{3}$ of the total. 
In the federated learning setup, we distribute the training instances equally across clients while preserving label distribution. Each instance is then assigned a type (multimodal or single-modality) based on the MFL settings controlled by $\alpha_1$, $\alpha_2$, $\beta_1$, and $\beta_2$ as described in \Cref{setting}. Single-modality instances drop the corresponding modality. For simplicity, we set $\alpha_1 = \alpha_2 = \alpha$ and $\beta_1 = \beta_2 = \beta$.

% baselinese
To evaluate the effectiveness of our proposed method, we compare it against several baseline approaches, including FedAvg \cite{FedAvg}, FedProx \cite{FedProx}, FedMed-GAN \cite{FedMed-GAN}, FedMI \cite{FedMI}, MFCPL \cite{MFCPL}, and PmcmFL \cite{PmcmFL}. FedAvg and FedProx are traditional federated learning algorithms, with FedAvg employing simple parameter averaging, while FedProx introduces a proximal term to the objective to mitigate client heterogeneity. FedMed-GAN employs CycleGAN \cite{cyclegan} to complete missing modalities, enhancing diagnosis accuracy. FedMI, MFCPL, and PmcmFL leverage prototype learning to model class distributions and align features, effectively addressing incomplete modalities and heterogeneous data across clients. All methods are evaluated under the same experimental setup to ensure a fair comparison.

% inplementation details
In our experiment, we set the number of clients to $N=10$, with the number of communication rounds fixed at 30 and each client performing 10 local training epochs. For optimization, we employ the Adam \cite{adam} optimizer with an initial learning rate of 0.01. To dynamically adjust the learning rate during training, we use cosine annealing strategy. We conduct 5-fold cross-validation and report the results as the mean $\pm$ standard deviation across all folds. For evaluation metrics, we adopt a weighted average for precision, F1-score, and ROC-AUC, while using a macro average for recall.

\subsection{Main Results}
\Cref{main result} presents a comprehensive comparison of the performance of our proposed method against several baseline approaches. The experiments were conducted under varying configurations of $\beta$ and $\alpha$, which denote modality incompleteness and client diversity, respectively.

As indicated in \Cref{main result}, our method consistently outperforms the baseline approaches across different settings of modality incompleteness, achieving superior results in terms of precision, recall, F1 score, accuracy, and AUC. Notably, certain algorithms specifically designed for modality-incomplete MFL fail to outperform traditional federated learning methods, such as FedProx, in some scenarios.

Moreover, as the values of $\alpha$ and $\beta$ increase—corresponding to a higher proportion of instances with only a single modality—most of the baseline algorithms exhibit a noticeable decline in performance. In contrast, our proposed method demonstrates stable and robust performance, highlighting its effectiveness and resilience in handling varying degrees of modality incompleteness.



\subsection{Ablation Study}

 In order to assess the contributions of each component, we conduct an ablation study under the setting of $\alpha = 0.4$ and $\beta = 0.2$, as shown in Table \ref{table:ablation study}. The results demonstrate that applying modality-aware aggregation (MAA) alone yields lower performance across all metrics. Incorporating the contrastive loss ($\mathcal{L}_{\text{CTR}}$) improves the results significantly, while the modality completion loss ($\mathcal{L}_{\text{MC}}$) also leads to moderate gains. Combining both losses without MAA further enhances performance. The full method, which includes both MAA and the two loss functions, achieves the highest performance, with precision, recall, F1-score, accuracy, and AUC all showing notable improvements. These findings highlight the effectiveness of combining modality-aware aggregation with the contrastive and modality completion losses in addressing modality incompleteness in MFL.
\input{Tables/ablation_study}
\subsection{Convergence Efficiency}

To better analyze the convergence behavior of different algorithms in modality-incompleteness scenarios, we conduct experiments with four different random seeds in the setting of the first fold, with $\alpha = 0.4$ and $\beta = 0.2$. The training curves, displayed in \Cref{fig:training curve}, represent the mean values across these random seeds, while the shaded areas indicate the standard deviation. From \Cref{fig:training curve}, we make the following observations:

\begin{itemize}
    \item \textbf{F1 Score and AUC:} Our method exhibits a rapid and consistent increase in both F1 score and AUC during the initial communication rounds, and subsequently stabilizes at higher values compared to the baseline methods. This indicates its superior ability to quickly capture relevant patterns. Furthermore, the consistent improvement in both metrics suggests that our approach enhances class discrimination more effectively as training progresses.
    \item \textbf{Test Loss:} Our method achieves convergence with fewer communication rounds compared to the baseline methods. Specifically, it converges around the 11th communication round, whereas most baseline methods require approximately 20 rounds to reach convergence, highlighting the superior efficiency of our approach.
    \item \textbf{Time:} Our method achieves significant performance improvements without a substantial increase in training time. Furthermore, the training time of our method remains significantly lower than that of FedMed-GAN, which requires adversarial training for GANs.
\end{itemize}
