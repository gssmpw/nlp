\documentclass[11pt,a4paper]{article}
\usepackage{macros}
\usepackage{comment}
\usepackage[textsize=tiny]{todonotes}
\setlength{\marginparwidth}{2cm}

\title{Conformal Prediction under Lévy-Prokhorov Distribution Shifts: Robustness to Local and Global Perturbations}


\author[$1$]{Liviu Aolaritei}
\author[$1$]{Michael I.~Jordan}
\author[$2$]{Youssef Marzouk}
\author[$2$]{Zheyu Oliver Wang}
\author[$2$]{Julie Zhu}
\affil[$1$]{Department of Electrical Engineering and Computer Sciences, UC Berkeley, USA \protect\\ \texttt{liviu.aolaritei@berkeley.edu, jordan@cs.berkeley.edu}}
\affil[$2$]{Department of Aeronautics and Astronautics, MIT, USA \protect\\
\texttt{\{olivrw, ymarz, qianyu\_z\}@mit.edu}}
\date{}

\begin{document}
\maketitle

%-------------------------------------
%-------------------------------------

\begin{abstract}
Conformal prediction provides a powerful framework for constructing prediction intervals with finite-sample guarantees, yet its robustness under distribution shifts remains a significant challenge. This paper addresses this limitation by modeling distribution shifts using Lévy-Prokhorov (LP) ambiguity sets, which capture both local and global perturbations. We provide a self-contained overview of LP ambiguity sets and their connections to popular metrics such as Wasserstein and Total Variation. We show that the link between conformal prediction and LP ambiguity sets is a natural one: by propagating the LP ambiguity set through the scoring function, we reduce complex high-dimensional distribution shifts to manageable one-dimensional distribution shifts, enabling exact quantification of worst-case quantiles and coverage. Building on this analysis, we construct robust conformal prediction intervals that remain valid under distribution shifts, explicitly linking LP parameters to interval width and confidence levels. Experimental results on real-world datasets demonstrate the effectiveness of the proposed approach.
\end{abstract}

%--------------------------------------------------------------------------
%--------------------------------------------------------------------------

\section{Introduction}
\label{sec:introduction}
%-------------------------------------
%- speak about conformal (why important, applications, etc)
%- need for robustness
%- mention existing approaches ensuring robustness
%- why optimal transport
%- reason for LP! (why is this needed given the previous approaches)
%- a possible reason: noise + contamination (link to robust statistics) or adversarial + TV
%- why more general costs are challenging
%- explain contributions (bullet points)
%-------------------------------------

\begin{comment}
{\color{blue}Quantification of predictive uncertainty for neural network models is critical to their success in high-stake problems such as autonomous vehicles, cancer diagnosis, presidential election forecast, and etc. Conformal prediction (split)~\cite{Vovk_2005, Angelopoulos_2024} is the canonical algorithm for this task. Its effectiveness is particularly notable, as it requires only minimal assumptions about the model and the underlying data distribution, making it both practical and versatile. 
Concretely, let $(X_i, Y_i) \sim P(\mathcal{X}\times\mathcal{Y}), ~i=1, ..., n$ be a set of \textit{calibration} data points exchangeable with the training data. Let us denote each data point $Z_i = (X_i, Y_i)$ as $Z_i$ and the data space as $\mathcal{Z} = \mathcal{X}\times\mathcal{Y}$. Then given a trained neural network predictor $\mu_\theta$ on some i.i.d. training data from $P$ and a desired coverage $1-\alpha$, conformal prediction seeks set-valued prediction bands $C:\mathcal{X} \to \{\text{a subset of }\mathcal{Y}\}$ such that 
\begin{equation}\label{eq: validity}
\mathbb{P}\Bigl(Y_{n+1} \in C(X_{n+1})\Bigr) \geq 1-\alpha
\end{equation}
for a new test point $(X_{n+1}, Y_{n+1})$. Specifically, $C$ is constructed as
\begin{equation}\label{eq: sc_pred_band}
C(X\in\mathcal{X}) := \left\{Y\in\mathcal{Y}: s(X, Y)\leq \qt\left(\frac{\lceil(n+1)(1-\alpha)\rceil}{n}; \frac{1}{n}\sum^n_{i=1}\delta_{Y_i}\right)\right\}
\end{equation}
though the definition of a score function $s: \mathcal{Z} \to \R$. The score function is defined as
\begin{equation}
    s(x, y) := d\bigl(\mu_\theta(x),y\bigr)
\end{equation}
where $d$ is some measure of discrepancy between the predicted label and the true label. 

The validity of conformal prediction, defined as achieving a coverage greater than $1-\alpha$, requires exchangeability between the calibration and test data, which mathematically writes 
\begin{equation}
\left(Z_1,...,Z_{n+1}\right) \overset{d}{=} \left(Z_{\sigma(1)}, ..., Z_{\sigma(n+1)}\right).
\end{equation}
In real-world scenarios, this assumption of exchangeability is often violated due to \textit{distribution shift}, rendering split conformal prediction invalid~\cite{tibshirani2019conformal}. Common types of distribution shift include covariate shift of feature distribution $P(X)$~\cite{tibshirani2019conformal}, temporal correlation~\cite{Gibbs_2021}, real-world distribution shift~\cite{Koh_2021}, and so on.}
% Our metric captures both localized noise and global contamination within a unified framework.
% , with distribution $\mu$ representing an arbitrary \emph{local perturbation} supported on the $\varepsilon$-ball in $\mathbb R^{d+1}$ and distribution $\nu$ representing an arbitrary \emph{global contamination}, 
\end{comment}

Conformal prediction has emerged as a versatile framework for constructing prediction intervals with finite-sample coverage guarantees \cite{papadopoulos2002inductive, Vovk_2005, angelopoulos2024theoretical}. By leveraging the concept of nonconformity, it provides valid confidence sets for predictions, regardless of the underlying data distribution. This framework has gained significant traction in fields such as medicine \cite{lu2022fair, vazquez2022conformal}, bioinformatics \cite{fannjiang2022conformal}, finance \cite{wisniewski2020application}, and autonomous systems \cite{lindemann2023safe, lindemann2023conformal}, where decision-making under uncertainty is critical. However, the standard conformal prediction framework relies on the assumption of exchangeability between training and test data \cite{Barber_2023}. When this assumption is violated due to distribution shifts, the coverage guarantees of conformal prediction may break down, limiting its applicability in real-world scenarios \cite{tibshirani2019conformal}.

Distribution shifts—systematic changes between the training and test distributions—are ubiquitous in practice. Examples include covariate shift in medical diagnostics, where the population characteristics evolve over time \cite{rahmani2023assessing}, or adversarial perturbations in image classification, where small, targeted changes to inputs can drastically alter predictions \cite{moosavi2017universal}. Addressing such shifts is essential for ensuring the reliability of predictive models, particularly in high-stakes applications.

Existing extensions to conformal prediction under distribution shifts often rely on specific assumptions about the nature of the shift, such as covariate or label shift, or require access to likelihood ratios between training and test distributions \cite{tibshirani2019conformal, Podkopaev_2021}. While effective in certain settings, these approaches can struggle with more complex shifts that involve both local perturbations (e.g., small, pixel-level changes in images) and global perturbations (e.g., population-wide shifts in feature distributions) \cite{Beery_2020}. To bridge this gap, we propose a novel framework based on Lévy–Prokhorov (LP) ambiguity sets, a class of optimal transport-based discrepancy measures that simultaneously capture local and global perturbations.

LP ambiguity sets offer a flexible and interpretable way to model distributional uncertainty. Unlike $f$-divergences, which are limited to absolutely continuous shifts, LP metrics naturally handle broader scenarios, including discrete and transport-based perturbations \cite{Bennouna_2023}. For example, LP metrics can capture local shifts such as minor variations in image textures or sensor readings, as well as global shifts like changes in population demographics. This dual capability makes LP metrics particularly suited for robust prediction in dynamic and heterogeneous environments.

In this paper, we leverage the LP ambiguity set to develop a distributionally robust extension of conformal prediction. By propagating LP ambiguity sets through the scoring function, we simplify high-dimensional shifts into one-dimensional shifts in the score space, enabling exact quantification of worst-case quantiles and coverage. This approach leads to interpretable and robust prediction intervals, with explicit control over how the local and global LP parameters influence interval width and confidence levels. Specifically, the main contributions of this paper can be summarized as follows.
\begin{enumerate}%[label=\roman*.]
    \item[1.] {\bf Distribution shifts as LP ambiguity sets.} We provide a brief, yet self-contained, overview of Lévy-Prokhorov (LP) ambiguity sets, highlighting their connection to well-known ambiguity sets derived from Wasserstein and Total Variation (TV) distances.
    \item[2.] {\bf Propagation of LP ambiguity sets.} We show that the propagation through the scoring function $s$ of the LP ambiguity set around the training distribution $\mathbb P$ is captured by another LP ambiguity set around the scoring distribution $s_\# \mathbb P$ (the pushforward of $\mathbb P$ via the map $s$). This naturally translates the LP distribution shift in the input-label pair $(X,Y)$ into a much simpler one-dimensional LP distribution shift in $s(X,Y)$.
    \item[3.] {\bf Distributionally robust conformal prediction.} The propagated LP ambiguity set around $s_\# \mathbb{P}$ enables precise quantification of both the \emph{worst-case quantile} and \emph{worst-case coverage}. These quantities are essential for constructing valid conformal prediction intervals under LP distribution shifts. Our approach is interpretable, elucidating how the local and global parameters of the LP ambiguity set influence the width of the interval and the confidence level.
\end{enumerate}

Finally, we validate the proposed approach on three benchmark datasets: MNIST~\cite{Lecun_1998}, ImageNet~\cite{Li_2009}, and iWildCam~\cite{Beery_2020}, the latter of which captures real-world distribution shifts, demonstrating its empirical coverage guarantees and efficiency in terms of prediction set size.

\subsection{Related Work}
\label{subsec:related:work}

Under train-test distribution shifts that violate exchangeability, conformal prediction often fails to maintain valid coverage guarantees~\cite{tibshirani2019conformal}. Extensions to conformal prediction under such shifts can be summarized into three main categories: sample reweighting, ambiguity sets, and sequential learning.

\medskip

\noindent\emph{Sample Reweighting.} This approach assigns weights to calibration samples based on their relevance to the test data. For instance, \cite{tibshirani2019conformal} proposed weighted conformal prediction for covariate shift, where the marginal distribution $\mathbb P_X$ changes while the conditional distribution $\mathbb P_{Y|X}$ remains fixed. Likelihood ratios are used to adjust for compositional differences, enabling valid predictions. Subsequent extensions address label shift~\cite{Podkopaev_2021}, causal inference~\cite{Lei_2021}, and survival analysis~\cite{Candes_2023, Gui_2022}. However, these methods rely on the accurate estimation of likelihood ratios, which may be challenging in practice. For spatial data, \cite{Mao_2022} proposed weighting samples based on proximity to test points. Compared to these approaches, our method handles distribution shifts in the \textit{joint} distribution $\mathbb P$ of $(X,Y)$, without requiring likelihood ratios, and remains effective under more complex local and global perturbations.

\medskip

\noindent\emph{Ambiguity Sets.} Ambiguity sets provide a flexible framework for modeling uncertainty in the data distribution. For instance, \cite{cauchois2024robust} used an $f$-divergence ambiguity set around the training distribution to derive worst-case coverage guarantees and adjusted prediction sets. This work is most closely related to ours, and while their analysis inspired our approach, we rely on fundamentally different tools, particularly drawing on optimal transport techniques. A key limitation of $f$-divergences is that they are restricted to distribution shifts that are absolutely continuous with respect to the training distribution. Differently, \cite{Gendler_2022} proposed robust score functions based on randomized smoothing~\cite{Cohen_2019, Kumar_2020}, which ensure valid predictions under adversarial perturbations within $\ell_2$-norm balls. While adversarial methods tend to produce overly conservative uncertainty sets, recent works~\cite{Yan_2024, Ghosh_2023, Clarkson_2024} have refined prediction sets by considering specific perturbation structures. Other extensions have incorporated poisoning attacks and non-continuous data types such as graphs~\cite{Zargarbashi_2024}. However, these methods often assume very specific types of distribution shifts or require solving complex optimization problems. In contrast, our method employs a unified discrepancy measure that captures both local and global perturbations, imposes no assumptions on the score distribution, and provides a computationally efficient way to construct prediction sets.

\medskip

\noindent\emph{Sequential Learning.} While most methods assume i.i.d.\ or exchangeable training data, several works have explored sequential conformal prediction. These methods include updating nonconformity scores~\cite{Xu_2021}, leveraging correlation structures~\cite{chernozhukov_2018}, reweighting samples~\cite{Xu_2023, Barber_2023}, and monitoring rolling coverage~\cite{Gibbs_2021, Gibbs_2022, Zaffran_2022, Bastani_2022}. Although our method does not address sequential settings, extending it to this context is a promising avenue for future research.





%--------------------------------------------------------------------------
%--------------------------------------------------------------------------




\subsection{Mathematical Notation} 
\label{subsec:math:preliminaries}

We denote by $\mathcal P(\mathcal Z)$ the space of Borel probability distributions on $\mathcal Z:= \mathcal X \times \mathcal Y \subseteq \mathbb R^{d}\times \mathbb R$. Given $\mathbb P \in \mathcal P(\mathcal Z)$, we denote by $Z \sim \mathbb P$ the fact that the random variable $Z$ is distributed according to $\mathbb P$. Projection maps are denoted by $\pi$, and the indicator function of a set $\mathcal A$ is denoted by $\mathds{1}\{\mathcal A\}$. We implicitly assume that all maps $s:\mathcal Z \to \mathbb R$ are Borel. We denote by $s_\# \mathbb P$ the pushforward of $\mathbb P$ via the map $s$, defined as $(s_\# \mathbb P)(\mathcal A) := \mathbb P(s^{-1}(\mathcal A))$, for all Borel sets $\mathcal A \subseteq \mathcal Z$. Given $\mathbb P, \mathbb Q \in \mathcal P(\mathcal Z)$, the \emph{$\infty$-Wasserstein distance} is defined as
\begin{align}
\label{eq:W:infty}
    \text{W}_\infty(\mathbb P, \mathbb Q) := \inf\left\{\varepsilon \geq 0: \inf_{\gamma \in \Gamma(\mathbb P, \mathbb Q)} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > \varepsilon\} \, \mathrm{d} \gamma(z_1,z_2) \leq 0\right\},
\end{align}
where $\Gamma(\mathbb P, \mathbb Q)$ is the set of all joint probability distributions over $\mathcal Z \times \mathcal Z$, with marginals $\mathbb P$ and $\mathbb Q$, often called transportation plans or couplings \cite{villani2009optimal}. Moreover, the \emph{Total Variation (TV) distance} is defined as
\begin{align}
\label{eq:TV}
    \text{TV}(\mathbb P, \mathbb Q) := \inf_{\gamma \in \Gamma(\mathbb P, \mathbb Q)} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \gamma(z_1,z_2).
\end{align}
At first sight, definition~\eqref{eq:TV} might seem different from the more classical definition $\text{TV}(\mathbb P, \mathbb Q) = \sup\{|\mathbb P(\mathcal A) - \mathbb Q(\mathcal A)|: \mathcal A \subseteq \mathcal Z \text{ is a Borel set}\}$. We refer to \cite[Proposition~2.24]{kuhn2024distributionally} for a proof of their equivalence. Here, we prefer definition~\eqref{eq:TV}, as it demonstrates that the TV distance is a special case of an optimal transport discrepancy, enabling us to leverage the extensive literature on optimal transport \cite{villani2009optimal}. Finally, we denote the $\alpha$-quantile of a distribution $\mathbb P$ by
\begin{align}
\label{eq:Quant}
\text{Quant}(\alpha; \mathbb{P}) := \inf \{s \in \mathbb{R} : \mathbb{P}(S \leq s) \geq \alpha\}.
\end{align}






%--------------------------------------------------------------------------
%--------------------------------------------------------------------------





\subsection{Preliminaries in Conformal Prediction}
\label{subsec:CP:preliminaries}

In what follows, we provide a brief introduction to \emph{split conformal prediction}. Consider a predictive model $f: \mathcal{X} \to \mathcal{Y}$ and a calibration dataset $\mathcal{D} = \{(X_i, Y_i)\}_{i=1}^n \subseteq \mathcal{X} \times \mathcal{Y}$, where the points in $\mathcal{D}$, along with any test sample $(X_{n+1}, Y_{n+1}) \in \mathcal{X} \times \mathcal{Y}$, are assumed to be exchangeable and distributed according to $\mathbb{P}$. Without additional assumptions on the predictive model or the data-generating process, conformal prediction constructs a prediction set $C^{1-\alpha}(X_{n+1})$ that satisfies the finite-sample coverage guarantee:
\begin{align}
\label{eq:basic:CP:guarantee}
    \text{Prob}\left\{Y_{n+1} \in C^{1-\alpha}(X_{n+1})\right\} \geq 1 - \alpha,
\end{align}
where the probability is taken over both the calibration dataset $\mathcal{D}$ and the test point $(X_{n+1}, Y_{n+1})$. 

To achieve this, conformal prediction relies on a scoring function $s: \mathcal{X} \times \mathcal{Y} \to \mathbb{R}$, which quantifies the nonconformity of a label $y \in \mathcal{Y}$ for a given input $x \in \mathcal{X}$. The predictive model $f$ is typically used to define the scoring function $s$, where $f(x)$ represents the model's prediction. In regression, $f(x)$ might return a point estimate of $y$, with $s(x, y)$ defined as the absolute error $|f(x) - y|$. In classification, $f(x)$ might output class probabilities, and $s(x, y)$ could be the negative log-probability of the true label $y$. For each calibration point $(X_i, Y_i) \in \mathcal{D}$, the nonconformity score $s(X_i, Y_i)$ is computed. The scores are then used to estimate the empirical $(1-\alpha)$-quantile, with a \emph{finite-sample correction}:
\begin{align*}
    \widehat{q}_\alpha \coloneqq \text{Quant}\left(\frac{\lceil (1-\alpha)(n+1) \rceil}{n}; s_\#{\widehat{\mathbb{P}}_n}\right),
\end{align*}
where $s_\#{\widehat{\mathbb{P}}_n}$ is the empirical distribution of the calibration scores $\{s(X_i, Y_i)\}_{i=1}^n$. Finally, the prediction set for a new label $Y_{n+1}$ is defined as
\begin{align*}
    C^{1-\alpha}(X_{n+1}) = \left\{y \in \mathcal{Y} : s(X_{n+1}, y) \leq \widehat{q}_\alpha \right\}.
\end{align*}
By construction, the prediction set $C^{1-\alpha}(X_{n+1})$ satisfies the coverage guarantee in \eqref{eq:basic:CP:guarantee}, provided the data is exchangeable. With the conformal prediction framework in place, we now shift our focus to the challenge of distribution shifts. Specifically, we consider scenarios where the test data $(X_{n+1}, Y_{n+1})$ is drawn from a distribution that differs from the distribution $\mathbb P$, with this shift captured by the Lévy–Prokhorov ambiguity set around $\mathbb P$. Such shifts introduce additional complexities in ensuring the robustness of the prediction intervals.






%--------------------------------------------------------------------------
%--------------------------------------------------------------------------





\section{Lévy–Prokhorov Distribution Shifts}
\label{sec:LP:shifts}

We model distribution shifts as an \emph{ambiguity set}, i.e., a ball of probability distributions
\begin{align}
\label{eq:LP:ambiguity:set}
    \mathbb B_{\varepsilon,\rho}(\mathbb P) :=\left\{\mathbb Q\in P(\mathcal{Z}): \text{LP}_\varepsilon(\mathbb P, \mathbb Q)\leq \rho\right\},
\end{align}
around the training distribution $\mathbb P$, constructed using the Lévy-Prokhorov (LP) pseudo-metric
\begin{align}
\label{eq:LP:distance}
    \text{LP}_{\varepsilon}(\mathbb P, \mathbb Q) := \inf_{\gamma\in\Gamma(\mathbb P,\mathbb Q)}\int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1-z_2\|>\varepsilon\}\mathrm{d}\gamma(z_1,z_2).
\end{align}
Note that the LP pseudo-metric belongs to the general class of optimal transport discrepancies, with the particular choice of transportation cost $c(z_1,z_2) := \mathds{1}\{\|z_1-z_2\|>\varepsilon\}$ \cite{Bennouna_2023}. In this section, we provide a detailed exposition of the LP pseudo-metric and explore its expressivity in modeling significant distribution shifts. The section culminates with Proposition~\ref{prop:propagation:LP}, where we study the propagation of the ambiguity set $\mathbb B_{\varepsilon,\rho}(\mathbb P)$ thorough the scoring function $s$, showing that the LP distribution shift can be directly considered in the one-dimensional nonconformity scores.

To provide more insights into the LP ambiguity set, we begin by presenting an alternative representation that decomposes it in terms of the $\infty$-Wasserstein distance and the TV distance.

\begin{proposition}[Decomposition of the LP ambiguity set]
\label{prop:LP:decomposition}
The LP ambiguity set can be equivalently rewritten as 
\begin{align}
\label{eq:LP:decomposition}
    \mathbb{B}_{\varepsilon,\rho}(\mathbb P) = \bigcup_{\widetilde{\mathbb P}:\, \text{W}_\infty(\mathbb P, \widetilde{\mathbb P})\leq \varepsilon} \left\{\mathbb Q\in P(\mathcal{Z}):\, \text{TV}(\widetilde{\mathbb P},\mathbb Q)\leq \rho\right\}.
\end{align}
\end{proposition}
\begin{proof}
We start by proving the ``$\supseteq$'' direction. Let $\mathbb Q$ belong to the right-hand side in \eqref{eq:LP:decomposition}, and we want to prove that $\mathbb Q \in \mathbb{B}_{\varepsilon,\rho}(\mathbb P)$. From the right-hand side in \eqref{eq:LP:decomposition}, we know that there exists $\widetilde{\mathbb P}$ such that $\text{W}_\infty(\mathbb P, \widetilde{\mathbb P})\leq \varepsilon$ and $\text{TV}(\widetilde{\mathbb P}, \mathbb Q)\leq \rho$. Using the definition of the $\text{W}
_\infty$ distance in~\eqref{eq:W:infty}, we note that $\text{W}_\infty(\mathbb P, \widetilde{\mathbb P})\leq \varepsilon$ is equivalent to
\begin{align}
\label{lemma:LP:decomposition:1}
    \inf_{\gamma \in \Gamma(\mathbb P, \widetilde{\mathbb P})} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > \varepsilon\} \mathrm{d} \gamma(z_1,z_2) \leq 0.
\end{align}

Now, since $\mathds{1}\{\|z_1 - z_2\| > \varepsilon\}$ is a lower semicontinuous function, by \cite[Theorem 4.1]{villani2009optimal} we know that there exists a coupling $\gamma_{12}^\star\in\Gamma(\mathbb P, \widetilde{\mathbb P})$ which attains the infimum in \eqref{lemma:LP:decomposition:1}. Analogously, since $\mathds{1}\{\|z_1 - z_2\| > 0\}$ is lower semicontinuous, the same result ensures that there exists a coupling $\gamma_{23}^\star\in\Gamma(\widetilde{\mathbb P},\mathbb Q)$ which attains the infimum in $\text{TV}(\widetilde{\mathbb P}, \mathbb Q)\leq \rho$. Since 
\begin{align*}
    (\pi_2)_\# \gamma_{12}^\star = (\pi_1)_\# \gamma_{23}^\star = \widetilde{\mathbb P},
\end{align*}
where $\pi_1: \mathcal Z_1 \times \mathcal Z_2 \to \mathcal Z_1$ and $\pi_2: \mathcal Z_1 \times \mathcal Z_2 \to \mathcal Z_2$ are the canonical projections, the Gluing lemma \cite[pp.\ 11--12]{villani2009optimal} guarantees that there exists a distribution $\gamma_{123} \in \mathcal P(\mathcal Z \times \mathcal Z \times \mathcal Z)$ such that $(\pi_{12})_\# \gamma_{123} = \gamma_{12}^\star$ and $(\pi_{23})_\# \gamma_{123} = \gamma_{23}^\star$. We now construct $\gamma_{13}:= (\pi_{13})_\# \gamma_{123}$, which can be easily shown to be a coupling of $\mathbb P$ and $\mathbb Q$. Then, we have that
\begin{align*}
    \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_3\| > \varepsilon\} &\mathrm{d} \gamma_{13}(z_1,z_3) = \int_{\mathcal Z \times \mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_3\| > \varepsilon\} \mathrm{d} \gamma_{123}(z_1,z_2,z_3)
    \\& = \int_{\mathcal Z \times \mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2 + z_2 - z_3\| > \varepsilon\} \mathrm{d} \gamma_{123}(z_1,z_2,z_3)
    \\& \leq \int_{\mathcal Z \times \mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| + \|z_2 - z_3\| > \varepsilon\} \mathrm{d} \gamma_{123}(z_1,z_2,z_3)
    \\& \leq \int_{\mathcal Z \times \mathcal Z \times \mathcal Z} \left(\mathds{1}\{\|z_1 - z_2\|>\varepsilon\} + \mathds{1}\{\|z_2 - z_3\| > 0\}\right) \mathrm{d} \gamma_{123}(z_1,z_2,z_3)
    \\& = \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\|>\varepsilon\} \mathrm{d} \gamma_{12}^\star(z_1,z_2) + \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_2 - z_3\|>0\} \mathrm{d} \gamma_{23}^\star(z_2,z_3)
    \\& \leq 0 + \rho = \rho,
\end{align*}
where the first inequality is a consequence of the triangle inequality, and the second inequality follows by noticing that the event $\{\|z_1 - z_2\| + \|z_2 - z_3\| > \varepsilon\}$ is contained in $\{\|z_1 - z_2\|>\varepsilon\} \cup \{\|z_2 - z_3\| > 0\}$. Therefore, 
\begin{align*}
    \inf_{\gamma\in\Gamma(\mathbb P,\mathbb Q)}\int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1-z_3\|>\varepsilon\}\mathrm{d}\gamma(z_1,z_3) \leq \rho,
\end{align*}
showing that $\text{LP}_\varepsilon(\mathbb P, \mathbb Q) \leq \rho$, and therefore $\mathbb Q \in \mathbb{B}_{\varepsilon,\rho}(\mathbb P)$.

We now prove the ``$\subseteq$'' direction. Let $\mathbb Q \in \mathbb{B}_{\varepsilon,\rho}(\mathbb P)$. In what follows, we will construct a distribution $\widetilde{\mathbb P}$ such that $\text{W}_\infty(\mathbb P, \widetilde{\mathbb P})\leq \varepsilon$ and $\text{TV}(\widetilde{\mathbb P}, \mathbb Q)\leq \rho$, showing that $\mathbb Q$ belongs to the right-hand side in \eqref{eq:LP:decomposition}. Since $\mathds{1}\{\|z_1-z_2\|>\varepsilon\}$ is lower semicontinuous, again by \cite[Theorem 4.1]{villani2009optimal}, we know that there exists a coupling $\gamma^\star\in\Gamma(\mathbb P, \mathbb Q)$ which attains the infimum in $\text{LP}_\varepsilon(\mathbb P,\mathbb Q) \leq \rho$. Therefore, $\gamma^\star(\|z_1-z_2\|>\varepsilon) = \bar{\rho}$ and $\gamma^\star(\|z_1-z_2\|\leq\varepsilon) = 1-\bar{\rho}$, for some $\bar{\rho} \leq \rho$. We define the event $\mathcal A := \{\|z_1-z_2\|\leq\varepsilon\}$, and its complement $\mathcal A^c = \{\|z_1-z_2\|>\varepsilon\}$, and denote by $\gamma^\star|_{\mathcal A}$ and $\gamma^\star|_{\mathcal A^c}$ the restrictions of the distribution $\gamma^\star$ to $\mathcal A$ and $\mathcal A^c$, respectively. We now construct the distribution $\widetilde{\mathbb P}$ as follows
\begin{align*}
    \widetilde{\mathbb P} := (\pi_1)_\#\gamma^\star|_{\mathcal A^c} + (\pi_2)_\# \gamma^\star|_{\mathcal A}.
\end{align*}
note that ${\widetilde{\gamma}} = \gamma^*|_\mathcal{A} + (\text{Id} \times \text{Id})_\# \left((\pi_1)_\# \gamma^\star|_{\mathcal A^c}\right)$ is a coupling between $\mathbb P$ and $\widetilde{\mathbb P}$. Then, we immediately have that
\begin{align*}
    &\inf_{\gamma \in \Gamma(\mathbb P, \widetilde{\mathbb P})} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > \varepsilon\} \mathrm{d} \gamma(z_1,z_2) \leq \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > \varepsilon\} \mathrm{d} \widetilde{\gamma}(z_1,z_2)
    \\& = \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > \varepsilon\} \mathrm{d} \gamma^*|_\mathcal{A}(z_1,z_2) + \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > \varepsilon\} \mathrm{d} (\text{Id} \times \text{Id})_\# \left((\pi_1)_\# \gamma^\star|_{\mathcal A^c}\right)(z_1,z_2),
\end{align*}
which is clearly equal to zero, showing that $\text{W}_\infty(\mathbb P, \widetilde{\mathbb P})\leq \varepsilon$. Moreover,
% \begin{align*}
%     \text{TV}(\widetilde{\mathbb P}, \mathbb Q) &= \text{TV}\left(\bar{\rho}\, \mathbb P + (\pi_2)_\# \gamma^\star|_{\mathcal A}, (\pi_2)_\# \gamma^\star|_{\mathcal A} + (\pi_2)_\#\gamma^\star|_{\mathcal A^c}\right)
%     \\&= \inf_{\gamma \in \Gamma(\bar{\rho}\, \mathbb P + (\pi_2)_\# \gamma^\star|_{\mathcal A}, (\pi_2)_\# \gamma^\star|_{\mathcal A} + (\pi_2)_\#\gamma^\star|_{\mathcal A^c})} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \gamma(z_1,z_2)
%     \\& \leq \inf_{\widehat{\gamma} \in \Gamma \left(\mathbb P, \frac{1}{\bar{\rho}}(\pi_2)_\#\gamma^\star|_{\mathcal A^c} \right)} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \left( \bar{\rho}\, \widehat{\gamma} + (\text{Id} \times \text{Id})_\# \left((\pi_2)_\# \gamma^\star|_{\mathcal A}\right) \right)(z_1,z_2)
%     \\&= \inf_{\widehat{\gamma} \in \Gamma \left(\mathbb P, \frac{1}{\bar{\rho}}(\pi_2)_\#\gamma^\star|_{\mathcal A^c} \right)} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \left( \bar{\rho}\, \widehat{\gamma} \right)(z_1,z_2)
%     \\& = \bar{\rho} \left( \inf_{\widehat{\gamma} \in \Gamma \left(\mathbb P, \frac{1}{\bar{\rho}}(\pi_2)_\#\gamma^\star|_{\mathcal A^c} \right)} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \widehat{\gamma}(z_1,z_2) \right)
%     \\& = \bar{\rho}.
% \end{align*}

\begin{align*}
    \text{TV}(\widetilde{\mathbb P}, \mathbb Q) &= \text{TV}\Big( (\pi_1)_\#\gamma^\star|_{\mathcal A^c} + (\pi_2)_\# \gamma^\star|_{\mathcal A},(\pi_2)_\#\gamma^\star|_{\mathcal A^c} + (\pi_2)_\# \gamma^\star|_{\mathcal A}\Big)\\
    &=\inf_{\gamma\in\Gamma\big( (\pi_1)_\#\gamma^\star|_{\mathcal A^c} + (\pi_2)_\# \gamma^\star|_{\mathcal A}, ~(\pi_2)_\# \gamma^\star|_{\mathcal A} + (\pi_2)_\#\gamma^\star|_{\mathcal A^c}\big)}\int_{\mathcal Z \times \mathcal Z}\mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \gamma(z_1,z_2)\\
    &\leq \inf_{\widehat{\gamma}\in\Gamma\big(\frac{1}{\bar{\rho}}(\pi_1)_\#\gamma^\star|_{\mathcal A^c}, ~\frac{1}{\bar{\rho}}(\pi_2)_\#\gamma^\star|_{\mathcal A^c}\big)}\int_{\mathcal Z \times \mathcal Z}\mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \big(\bar{\rho}\,\widehat{\gamma}+(\text{Id} \times \text{Id})_\# \left((\pi_2)_\# \gamma^\star|_{\mathcal A}\right) \big)(z_1,z_2)\\
    &=\inf_{\widehat{\gamma}\in\Gamma\big(\frac{1}{\bar{\rho}}(\pi_1)_\#\gamma^\star|_{\mathcal A^c}, ~\frac{1}{\bar{\rho}}(\pi_2)_\#\gamma^\star|_{\mathcal A^c}\big)}\int_{\mathcal Z \times \mathcal Z}\mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d}(\bar{\rho}\,\widehat{\gamma})(z_1,z_2)\\
    &=\bar{\mathbb \rho}\Bigg(\inf_{\widehat{\gamma}\in\Gamma\big(\frac{1}{\bar{\rho}}(\pi_1)_\#\gamma^\star|_{\mathcal A^c}, ~\frac{1}{\bar{\rho}}(\pi_2)_\#\gamma^\star|_{\mathcal A^c}\big)}\int_{\mathcal Z \times \mathcal Z}\mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d}\widehat{\gamma}(z_1,z_2)\Bigg)\\
    &= \bar{\rho}.
\end{align*}

Here, the first inequality holds since $\bar{\rho}\, \widehat{\gamma} + (\text{Id} \times \text{Id})_\# \left((\pi_2)_\# \gamma^\star|_{\mathcal A}\right)$, with $\widehat{\gamma}\in\Gamma\big(\frac{1}{\bar{\rho}}(\pi_1)_\#\gamma^\star|_{\mathcal A^c}, ~\frac{1}{\bar{\rho}}(\pi_2)_\#\gamma^\star|_{\mathcal A^c}\big)$, is a coupling of $(\pi_1)_\#\gamma^\star|_{\mathcal A^c} + (\pi_2)_\# \gamma^\star|_{\mathcal A}$ and $(\pi_2)_\# \gamma^\star|_{\mathcal A} + (\pi_2)_\#\gamma^\star|_{\mathcal A^c}$. Moreover, the third equality follows from the fact that 
\begin{align*}
    \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \left((\text{Id} \times \text{Id})_\# \left((\pi_2)_\# \gamma^\star|_{\mathcal A}\right) \right)(z_1,z_2) = 0. 
\end{align*}
Finally, the last equality follows from the fact that $\mathcal A^c = \{\|z_1-z_2\|>\varepsilon\}$. This shows that $\text{TV}(\widetilde{\mathbb P}, \mathbb Q) \leq \rho$, and concludes the proof.
\end{proof}

The decomposition in equation~\eqref{eq:LP:decomposition} reveals that each distribution $\mathbb Q \in \mathbb{B}_{\varepsilon,\rho}(\mathbb P)$ can be constructed through a two-step procedure. First, the center distribution $\mathbb P$ undergoes a \emph{local perturbation}, resulting in an intermediate distribution $\widetilde{\mathbb P}$ that lies within a $W_\infty$ distance of at most $\varepsilon$ from $\mathbb P$. This implies that each unit of mass in $\mathbb P$ can be arbitrarily relocated within a radius of $\varepsilon$ in $\mathcal Z$. Secondly, $\widetilde{\mathbb P}$ is subjected to a \emph{global perturbation}, producing the final distribution $\mathbb Q$, which lies within a TV distance of at most $\rho$ from $\widetilde{\mathbb P}$. Specifically, this step entails displacing up to a fraction $\rho$ of $\widetilde{\mathbb P}$'s total mass to any location in the space $\mathcal Z$. This decomposition in~\eqref{eq:LP:decomposition} immediately implies that other well-known distribution shifts can be recovered as extreme cases of the LP ambiguity set $\mathbb B_{\varepsilon,\rho}(\mathbb P)$.

\begin{corollary}[Relationship to other metrics]~
\label{cor:relationship:other:metrics}
\begin{itemize}
    \item[(i)] $\mathbb B_{0,\rho}(\mathbb P)$ recovers the TV ambiguity set $\left\{\mathbb Q\in P(\mathcal{Z}): \text{TV}(\mathbb P, \mathbb Q)\leq \rho\right\}$.
    
    \item[(ii)] $\mathbb B_{\varepsilon,0}(\mathbb P)$ recovers the $\infty$-Wasserstein ambiguity set $\left\{\mathbb Q\in P(\mathcal{Z}): \text{W}_\infty(\mathbb P, \mathbb Q)\leq \rho\right\}$.
\end{itemize}
\end{corollary}
\begin{proof}
Assertion~(i) follows from \eqref{eq:LP:decomposition} by setting $\varepsilon$ to zero, resulting in $\widetilde{\mathbb P} = \mathbb P$. Moreover, assertion~(ii) follows from \eqref{eq:LP:decomposition} by setting $\rho = 0$, resulting in $\widetilde{\mathbb P} = \mathbb Q$.
\end{proof}

The decomposition in equation~\eqref{eq:LP:decomposition} can also be expressed in terms of random variables, which may offer a clearer understanding of the distribution shifts represented by the LP ambiguity set. We state this in the following proposition, which recovers \cite[Theorem~2.1]{Bennouna_2023} using a different approach.

\begin{proposition}[Local and Global Perturbation]
\label{prop:local:global}
Let $Z_1 \sim \mathbb P$. Then $\mathbb Q \in \mathbb B_{\varepsilon,\rho}(\mathbb P)$ if and only if there exists a random variable $Z_2 \sim \mathbb Q$ of the form
\begin{align}
\label{eq:local:global}
    Z_2 \overset{d}{=} (Z_1 + N)\mathds{1}\{B = 0\} + C \mathds{1}\{B = 1\},
\end{align}
where the random variables $N, B, C$ are as follows:
\begin{itemize}
    \item $N$ represents the local perturbation, whose distribution is supported on $\{n \in \mathcal Z: \|n\| \leq \varepsilon\}$,

    \item $B$ indicates whether the sample is globally perturbed or not, with $\text{Prob}(B = 1) \leq \rho$, and

    \item $C$ represents the global perturbation, following an arbitrary distribution on $\mathcal Z$.
\end{itemize}
In particular, $Z_1, N, B$, and $C$ can all be correlated.
\end{proposition}
\begin{proof}
We first prove that any distribution $\mathbb Q \in \mathbb B_{\varepsilon,\rho}(\mathbb P)$ admits a random variable decomposition $Z_2$ as described in \eqref{eq:local:global}. Since $\mathds{1}\{\|z_1-z_2\|>\varepsilon\}$ is lower semicontinuous, by \cite[Theorem 4.1]{villani2009optimal} there exists a coupling $\gamma^\star\in\Gamma(\mathbb P, \mathbb Q)$ which attains the infimum in $\text{LP}_\varepsilon(\mathbb P,\mathbb Q) \leq \rho$. Furthermore, given $Z_1 \sim \mathbb P$, consider the conditional distribution $Z_2|Z_1\sim\gamma^*_{Z_1}$, and define the (random) event $\mathcal{A}_{Z_1} := \{\|z_2-Z_1\|\leq \epsilon\}$. Moreover, we denote by $\gamma^*_{Z_1}|_{\mathcal{A}_{Z_1}}$ the restriction of $\gamma^*_{Z_1}$ to the event $\mathcal{A}_{Z_1}$, and by $\overline{\gamma^*_{Z_1}|_{\mathcal{A}_{Z_1}}}$ its normalized version. Similarly, $\overline{\gamma^*_{Z_1}|_{\mathcal{A}_{Z_1}^c }}$ is the normalized version of the restriction to the complement $\mathcal{A}_{Z_1}^c$. We then construct the random variables $B$, $N$, and $C$ as follows:
\begin{equation}
\begin{aligned}
    B|Z_1&\sim\text{Bern}\left(\gamma^*_{Z_1} \left(\|z_2-Z_1\|>\varepsilon \right) \right),\\
    N|Z_1 &= \mathds{1}(B=1)\cdot 0+\mathds{1}(B=0)\cdot (Z'_2-Z_1)|Z_1, \text{ and}\\
    C|Z_1 &= \mathds{1}(B=1)\cdot Z''_2|Z_1+\mathds{1}(B=0)\cdot 0,\label{eq:decomposition:BNC}
\end{aligned}
\end{equation}
where $Z'_2|Z_1$ and $Z''_2|Z_1$ follow the probability distributions $\overline{\gamma^*_{Z_1}|_{\mathcal{A}_{Z_1}}}$ and $\overline{\gamma^*_{Z_1}|_{\mathcal{A}_{Z_1}^c}}$, respectively. Here $B$, $N$, $C$ are dependent with marginals satisfying the properties in the statement of the proposition. We now define $\widetilde{Z}_2 := (Z_1 + N)\mathds{1}\{B = 0\} + C \mathds{1}\{B = 1\}$, and aim to show that $Z_2\overset{d}{=}\widetilde{Z}_2$. Following the construction in \eqref{eq:decomposition:BNC}, conditioning $\widetilde{Z}_2$ on $Z_1$ yields
\begin{align*}
    \widetilde{Z}_2|Z_1& = (Z_1 + N|Z_1)\cdot\mathds{1}\{B|Z_1 = 0\}+C|Z_1\cdot\mathds{1}\{B|Z_1 = 1\}\\
    &=Z_2'|Z_1\cdot\mathds{1}\{B|Z_1 = 0\}+Z''_2|Z_1 \cdot \mathds{1}(B|Z_1=1).
\end{align*}
Now recall from \eqref{eq:decomposition:BNC} that the conditional random variable $B|Z_1$ follows a Bernoulli distribution with parameter $\gamma^*_{Z_1} \left(\|z_2-Z_1\|>\varepsilon \right) = \gamma^*_{Z_1} (\mathcal A_{Z_1}^c)$. Thus, the distribution of $\widetilde{Z}_2|Z_1$ becomes $\gamma^*_{Z_1} (\mathcal A_{Z_1}) \cdot \overline{\gamma^*_{Z_1}|_{\mathcal A_{Z_1}}} + \gamma^*_{Z_1} (\mathcal A_{Z_1}^c) \cdot \overline{\gamma^*_{Z_1}|_{\mathcal A_{Z_1}^c}}$. Moreover, since $\gamma^*_{Z_1} (\mathcal A_{Z_1}) \cdot \overline{\gamma^*_{Z_1}|_{\mathcal A_{Z_1}}} = \gamma^*_{Z_1}|_{\mathcal A_{Z_1}}$ and $\gamma^*_{Z_1} (\mathcal A_{Z_1}^c) \cdot \overline{\gamma^*_{Z_1}|_{\mathcal A_{Z_1}^c}} = \gamma^*_{Z_1}|_{\mathcal A_{Z_1}^c}$, we have that $ \widetilde{Z}_2|Z_1\sim \gamma^*_{Z_1}.$
Therefore, the distribution of $\widetilde{Z}_2$ is is equal to
\begin{equation*}
    \widetilde{Z}_2 = \mathbb E_{Z_1}[\widetilde{Z}_2|Z_1] \sim(\pi_2)_\#\gamma^* = \mathbb Q,
\end{equation*}
which concludes the proof of the first direction.

We now prove the converse: any random variable $Z_2$ of the form \eqref{eq:local:global} is distributed according to some distribution $\mathbb Q$ belonging to the LP ambiguity set $\mathbb B_{\varepsilon,\rho}(\mathbb P)$. To show this, we employ Proposition~\ref{prop:LP:decomposition}, which reduces the problem to showing that $\mathbb Q$ belongs to the union on the right-hand side in \eqref{eq:LP:decomposition}. We start by defining the random variable
\begin{equation*}
    Z_3 \coloneqq (Z_1+N)\mathds{1}\{B = 0\}+Z_1\mathds{1}\{B = 1\},
\end{equation*}
where $Z_1, N$, and $B$ are the same random variables as in the definition of $Z_2$ in \eqref{eq:local:global}. Let $\widetilde{\mathbb{P}}$ denote the distribution of $Z_3$. Then, the pair $(Z_1, Z_3)$ induces a coupling $\gamma_{13} \in \Gamma(\mathbb P, \widetilde{\mathbb P})$. By construction we have $\gamma_{13}(\|z_1-z_3\|>\varepsilon) = 0$, implying that
\begin{align*}
    \inf_{\gamma \in \Gamma(\mathbb P, \widetilde{\mathbb P})} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_3\| > \varepsilon\} \mathrm{d} \gamma(z_1,z_3) \leq \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_3\| > \varepsilon\} \mathrm{d} \gamma_{13}(z_1,z_3)\leq 0.
\end{align*}
Using the definition of the $\text{W}
_\infty$ distance in~\eqref{eq:W:infty}, this is equivalent to $\text{W}_\infty(\mathbb P, \widetilde{\mathbb P})\leq \varepsilon$. Next, we verify that $\text{TV}(\widetilde{\mathbb P},\mathbb Q)\leq \rho$. Note that $(Z_3, Z_2)$ induces a coupling $\gamma_{32} \in \Gamma(\widetilde{\mathbb P}, \mathbb Q)$ satisfying
\begin{align*}
    \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_3 - z_2\| > 0\} \mathrm{d} \gamma_{32}(z_3,z_2) &\leq 0\cdot \text{Prob}(B = 0) + 1\cdot\text{Prob}(B=1) \leq \rho,
\end{align*}
where the equality follows from $\|(Z_3-Z_2) | (B=0)\| = 0$ and the fact that the indicator function is bounded by $1$. Therefore, 
\begin{equation*}
    \text{TV}(\widetilde{\mathbb P}, \mathbb Q) := \inf_{\gamma \in \Gamma(\widetilde{\mathbb P}, \mathbb Q)} \int_{\mathcal Z \times \mathcal Z} \mathds{1}\{\|z_1 - z_2\| > 0\} \mathrm{d} \gamma(z_1,z_2)\leq \rho,
\end{equation*}
Putting everything together, we have that that $\mathbb Q\in \bigcup_{\widetilde{\mathbb P}:\, \text{W}_\infty(\mathbb P, \widetilde{\mathbb P})\leq \varepsilon} \left\{\mathbb Q\in P(\mathcal{Z}):\, \text{TV}(\widetilde{\mathbb P},\mathbb Q)\leq \rho\right\}$, which completes the proof.
\end{proof}

Propositions~\ref{prop:LP:decomposition} and \ref{prop:local:global} readily imply that the LP ambiguity set allows for distributions $\mathbb Q$ which are significantly different from $\mathbb P$, as the following remark explains.


\begin{remark}[Absolute continuity]
\label{remark:absolute:continuity}
The representation \eqref{eq:LP:decomposition} of the LP ambiguity set implies, in particular, that $\mathbb{B}_{\varepsilon,\rho}(\mathbb P)$ contains test distributions which are not absolutely continuous with respect to the training distribution $\mathbb P$. This is an important feature of our work, which is not present in previous approaches which model distribution shifts via an $f$-divergence ambiguity set \cite{cauchois2024robust}, or by a bounded likelihood between the test and train distributions \cite{tibshirani2019conformal}.
\end{remark}

So far, we have considered the distribution shift modeled via an LP ambiguity set in the space $\mathcal Z = \mathcal X \times \mathcal Y$. This is in line with supervised learning tasks, where it is more natural to consider distribution shifts in \emph{data-space} $\mathcal X \times \mathcal Y$, as opposed to a distribution shift in the \emph{score-space} $s(\mathcal X, \mathcal Y)$. Nonetheless, from a technical point of view, it is much easier to deal with an LP ambiguity set in the one-dimensional scores, due to its immediate relationship with the cumulative distribution functions and quantiles. The following proposition shows that the result of the propagation of $\mathbb{B}_{\varepsilon,\rho}(\mathbb P)$ through the scoring function $s$ is again captured by a an LP ambiguity set, allowing us to effectively restrict the analysis to a distribution shift on the scores.

\begin{prop}[Propagation of the LP ambiguity set]
\label{prop:propagation:LP}
Let the scoring function $s:\mathcal Z \to \mathbb R$ be $k$-Lipschitz over $\mathcal Z$, for some $k \in \mathbb R_+$. Then,
\begin{align}
\label{eq:propagation:LP}
    s_\# \mathbb{B}_{\varepsilon,\rho}(\mathbb P) \subseteq \mathbb{B}_{k \varepsilon, \rho}(s_\# \mathbb P).
\end{align}
%where the ambiguity set $\mathbb{B}_{k \varepsilon, \rho}(s_\# \mathbb P)$ is supported on $s(\mathcal Z)$, i.e., the image of the set $\mathcal Z$ through $s$.
\end{prop}
\begin{proof}
Let $\mathbb Q \in \mathbb{B}_{\varepsilon,\rho}(\mathbb P)$. We will show that $s_\# \mathbb Q$ belongs to the LP ambiguity set $\mathbb{B}_{k \varepsilon, \rho}(s_\# \mathbb P)$.
\begin{align*}
    \text{LP}_{k\varepsilon}(s_\# \mathbb P, s_\# \mathbb Q) &=\inf_{\tilde\gamma\in\Gamma(s_\# \mathbb P, s_\# \mathbb Q)}\int_{\mathbb R \times \mathbb R}\mathds{1}\{|\tilde{z}_1-\tilde{z}_2|>k\varepsilon\}\mathrm{d}\tilde{\gamma}(\tilde{z}_1, \tilde{z}_2)\\
    &= \inf_{\tilde\gamma\in(s\times s)_\#\Gamma(\mathbb P, \mathbb Q)}\int_{\mathbb R \times \mathbb R} \mathds{1}\{|\tilde{z}_1-\tilde{z}_2|>k\varepsilon\}\mathrm{d}\tilde{\gamma}(\tilde{z}_1, \tilde{z}_2)\\
    &= \inf_{\gamma\in\Gamma(\mathbb P, \mathbb Q)}\int_{\mathbb R \times \mathbb R}\mathds{1}\{|\tilde{z}_1 - \tilde{z}_2|>k\varepsilon\}\mathrm{d}((s\times s)_\# \gamma)(\tilde{z}_1, \tilde{z}_2)\\
    &=\inf_{\gamma\in\Gamma(\mathbb P, \mathbb Q)}\int_{\mathcal Z \times \mathcal Z} \mathds{1}(|s(z_1)-s(z_2)|>k\varepsilon)\mathrm{d}\gamma(z_1,z_2)\\
    &\leq\inf_{\gamma\in\Gamma(\mathbb P, \mathbb Q)}\int_{\mathcal Z \times \mathcal Z}\mathds{1}(\|z_1-z_2\|>\varepsilon)\mathrm{d}\gamma(z_1,z_2) \\
    &= \text{LP}_{\varepsilon}(\mathbb P, \mathbb Q),
\end{align*}
where the second equality follows from the equality $\Gamma(s_\# \mathbb P, s_\# \mathbb Q) = (s\times s)_\#\Gamma(\mathbb P, \mathbb Q)$ (see \cite[Lemma~2]{Aolaritei_2023}), and the inequality follows from the fact that $s$ is $k$-Lipschitz, i.e., $|s(z_1)-s(z_2)| \leq k \|z_1-z_2\|$.
\end{proof}

Proposition~\ref{prop:propagation:LP} requires the scoring function $s$ to be Lipschitz continuous over $\mathcal{Z}$. This condition is trivially satisfied if, for instance, $s$ is continuous and $\mathcal{Z}$ is compact. In light of the inclusion~\eqref{eq:propagation:LP}, we focus, for the remainder of the paper, on distribution shifts over the nonconformity scores. These shifts are modeled via an LP ambiguity set $\mathbb{B}_{\varepsilon, \rho}(\mathbb{P})$, where, for simplicity, we omit the Lipschitz constant $k$ from the notation and consider $\mathbb P$ to be directly the distribution of $s(Z)$. Note that, in this case, all distributions inside $\mathbb{B}_{\varepsilon, \rho}(\mathbb{P})$ are supported on $\mathbb R$.





%--------------------------------------------------------------------------
%--------------------------------------------------------------------------






\section{Worst-Case Quantile and Coverage}
\label{sec:worst:case}

In this section we introduce and analyze the two key quantities which allow us to construct a robust prediction interval with the right coverage level for any test distribution in the LP ambiguity set. The first quantity is the \emph{worst-case quantile}, defined below.

\begin{definition}[Worst-case quantile]
\label{def:Quant:WC}
For $\beta \in [0,1]$, the worst-case $\beta$-quantile in $\mathbb{B}_{\varepsilon, \rho}(\mathbb P)$ is defined as
\begin{align}
\label{eq:Quant:WC}
    \text{Quant}_{\varepsilon,\rho}^{\text{WC}}(\beta;\mathbb P) := \sup_{\mathbb Q \in \mathbb{B}_{\varepsilon,\rho}(\mathbb P)} \text{Quant}(\beta;\mathbb Q).
\end{align}
\end{definition}

Equation~\eqref{eq:Quant:WC} defines the worst-case quantile through a distributionally robust optimization problem, which quantifies the largest $\beta$-quantile for all the test distributions in the LP ambiguity set. In other words, $\text{Quant}_{\varepsilon,\rho}^{\text{WC}}(\beta;\mathbb P)$ represents the worst-case impact of the distribution shift on the value of the $\beta$-quantile. This, in turn, affects the size of the confidence interval, as we will show in Section~\ref{sec:robust:conformal}. The second quantity is the \emph{worst-case coverage}, defined next.

\begin{definition}[Worst-case coverage]
\label{def:Cov:WC}
For $q \in \mathbb R$, the worst-case coverage in $\mathbb{B}_{\varepsilon, \rho}(\mathbb P)$ at $q$ is defined as
\begin{align}
\label{eq:Cov:WC}
    \text{Cov}_{\varepsilon,\rho}^{\text{WC}}(q;\mathbb P) = \inf_{\mathbb Q \in \mathbb{B}_{\varepsilon,\rho}(\mathbb P)} F_{\mathbb Q}(q),
\end{align}
where $F_{\mathbb Q}:\mathbb R \to [0,1]$ is the cumulative distribution function of $\mathbb Q$.
\end{definition}

Equation~\eqref{def:Cov:WC} defines the worst-case coverage as the lowest value among the cumulative distribution functions in the LP ambiguity set evaluated at $q \in \mathbb R$. For example, if $q = \text{Quant}(1-\alpha;\mathbb P)$, $\text{Cov}_{\varepsilon,\rho}^{\text{WC}}(q;\mathbb P)$ represents the worst-case impact of the distribution shift on the true confidence level when the confidence level for $\mathbb P$ is $1-\alpha$. In the remainder of this section, we will show that both $\text{Quant}_{\varepsilon,\rho}^{\text{WC}}(\beta;\mathbb P)$ and $\text{Cov}_{\varepsilon,\rho}^{\text{WC}}(q;\mathbb P)$ can be quantified in closed-form, as a function of the training distribution $\mathbb P$ and the two robustness parameters $\varepsilon,\rho$. Before doing so, we note that a high value of $\rho$, i.e., the global perturbation parameter, renders the worst-case quantile trivial. We show this in the following remark.

\begin{remark}[Case $\rho\geq 1 - \beta$]
\label{remark:trivial:bound}
If $\rho\geq 1 - \beta$, then $\text{Quant}_{\varepsilon,\rho}^{\text{WC}}(\beta;\mathbb P) = \text{Quant}(1;\mathbb P)$. Intuitively, the LP ambiguity set $\mathbb{B}_{\varepsilon, \rho}(\mathbb P)$ allows to displace $\rho$ mass from the distribution $\mathbb P$ and move it arbitrarily in $\mathbb R$. Since $\rho\geq 1 - \beta$, this implies that we can construct a sequence of distributions $\mathbb Q_n \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$ for which $\text{Quant}(\beta;\mathbb Q_n) \to \infty$. We exemplify this intuition in the following example. Let $\mathbb P = \mathcal U([0,1])$, and let $\mathbb Q_n := \mathcal U([0,1-\rho]) + \rho \delta_{n}$. Then, clearly $\text{LP}_{\varepsilon}(\mathbb P, \mathbb Q_n) = \rho$, and $\text{Quant}(\beta;\mathbb Q_n) \geq n$.
\end{remark}

Following Remark~\ref{remark:trivial:bound}, in the rest of the paper, we restrict our attention to the case $\rho < 1 - \beta$ in the quantity $\text{Quant}_{\varepsilon,\rho}^{\text{WC}}(\beta;\mathbb P)$. We are now prepared to present the first result of this section.

\begin{proposition}[Worst-case quantile in the LP ambiguity set]
\label{prop:WC:Quant}
The following holds
\begin{align}
\label{eq:WC:Quant}
    \text{Quant}_{\varepsilon,\rho}^{\text{WC}}(\beta;\mathbb P) = \text{Quant}(\beta+\rho; \mathbb P) + \varepsilon.
\end{align}
\end{proposition}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/Quant_Cov_WC.pdf}
    \caption{(Left) Worst-case quantile; (Right) Worst-case coverage.}
    \label{fig:Quant_Cov}
\end{figure}
\begin{proof}
We prove the proposition in two steps. First, we show that the right-hand side in \eqref{eq:WC:Quant} is an upper bound on the $\beta$-quantile of any distribution in $\mathbb{B}_{\varepsilon, \rho}(\mathbb P)$. Second, we prove that there exists a sequence of distributions $\mathbb Q
_n \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$, whose $\beta$-quantiles converge to it. 

\medskip

\noindent\emph{Step 1.} We prove, by contradiction, that $\text{Quant}(\beta;\mathbb Q) \leq \text{Quant}(\beta+\rho; \mathbb P) + \varepsilon$, for all $\mathbb Q \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$. Suppose there exists $\widetilde{\mathbb Q}$ satisfying 
\begin{align}
\label{prop:WC:Quant:1}
    \widetilde{\mathbb Q} \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P) \quad \text{and} \quad \text{Quant}(\beta; \widetilde{\mathbb Q}) > \text{Quant}(\beta+\rho; \mathbb P) + \varepsilon.
\end{align} 
We will show that this leads to
\begin{align*}
    \text{LP}_\varepsilon(\mathbb P, \widetilde{\mathbb Q}) = \inf_{\gamma\in\Gamma(\mathbb P, \widetilde{\mathbb Q})}\int_{\R\times\R} \mathds{1}\{|z_1 - z_2| > \varepsilon\}\mathrm{d}\gamma(z_1,z_2) > \rho.
\end{align*}
To simplify notation, we define $a:=\text{Quant}(\beta+\rho; \mathbb P)$ and $b:=\text{Quant}(\beta+\rho; \mathbb P)+\varepsilon$. Following \eqref{prop:WC:Quant:1}, $b$ must satisfy $F_{\widetilde{\mathbb Q}}(b)< \beta$. Hence, there exists $\Delta >0$ such that
\begin{align*}
    F_{\mathbb P}(a)-F_{\widetilde{\mathbb Q}}(b) \geq \rho + \Delta.
\end{align*}
Now, for an arbitrary coupling $\gamma \in \Gamma(\mathbb P, \widetilde{\mathbb Q})$, we have 
\begin{align*}
    F_{\mathbb P}(a) - F_{\widetilde{\mathbb Q}}(b) &= \int_{-\infty}^{a}\int_{-\infty}^\infty \mathrm{d}\gamma(z_1,z_2) - \int_{-\infty}^\infty\int_{-\infty}^{b}d\gamma(z_1,z_2)\\
    &=\int_{-\infty}^{a}\int_{-\infty}^b d\gamma(z_1,z_2)+\int_{-\infty}^{a}\int_{b+}^\infty d\gamma(z_1,z_2)-\int_{-\infty}^a\int_{-\infty}^{b} \mathrm{d}\gamma(z_1,z_2)-\int_{a+}^\infty\int_{-\infty}^{b}d\gamma(z_1,z_2)\\
    &\leq\int_{-\infty}^{a}\int_{b+}^\infty \mathds{1}_{\{|z_1-z_2|> \varepsilon\}} \mathrm{d}\gamma(z_1,z_2)\\
    &\leq\int_{-\infty}^\infty\int_{-\infty}^\infty \mathds{1}_{\{|z_1-z_2|> \varepsilon\}} \mathrm{d}\gamma(z_1,z_2),
\end{align*}
Since the above holds for every $\gamma\in\Gamma(\mathbb P, \widetilde{\mathbb Q})$, we conclude that
\begin{align*}
    \inf_{\gamma\in\Gamma(\mathbb P, \widetilde{\mathbb Q}
    )}\int_{\R\times\R} \mathds{1}_{\{|z_1-z_2|> \varepsilon\}} \mathrm{d}\gamma(z_1,z_2) \geq  \rho+\Delta,
\end{align*}
which contradicts the fact that $\widetilde{\mathbb Q} \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$. This proves that $\text{Quant}(\beta;\mathbb Q) \leq \text{Quant}(\beta+\rho; \mathbb P) + \varepsilon$, for all $\mathbb Q \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$.

\medskip

\noindent\emph{Step 2.} We construct a sequence of distributions $\mathbb Q_n \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$ satisfying, as $n \to \infty$,
\begin{align*}
    \text{Quant}(\beta; \mathbb Q_n) \to \text{Quant}(\beta+\rho; \mathbb P)+\varepsilon.
\end{align*}
We define the sequence of distributions $\mathbb Q_n$ through their cumulative distribution functions as
\begin{align}
\label{prop:WC:Quant:2}
\begin{split}
    F_{\mathbb Q_n}(q) = \begin{cases}
    F_{\mathbb P}(q-\varepsilon), & q < \text{Quant}\left(\beta-\frac{1}{n}; \mathbb P \right) +\varepsilon\\
    \beta-\frac{1}{n}, &\text{Quant} \left(\beta-\frac{1}{n}; \mathbb P \right) +\varepsilon \leq q < \text{Quant} \left(\beta-\frac{1}{n}+\rho; \mathbb P \right)+\varepsilon\\
    F_{\mathbb P}(q-\varepsilon), & q \geq \text{Quant} \left(\beta-\frac{1}{n}+\rho; \mathbb P \right)+\varepsilon.
    \end{cases}
\end{split}
\end{align}
To simplify notation, for the rest of the proof, we define $q^{(1)}_n := \text{Quant}(\beta-\frac{1}{n}; \mathbb P)+\varepsilon$ and $q^{(2)}_n := \text{Quant}(\beta-\frac{1}{n}+\rho; \mathbb P)+\varepsilon$. The intuition behind the construction of $\mathbb Q_n$ is as follows: first, $\mathbb{Q}_n$ is obtained by translating the distribution $\mathbb P$ to the right by $\varepsilon$, and then, the mass between $[q^{(1)}_n,q^{(2)}_n)$ is moved to the point $q^{(2)}_n$. We refer to the illustration on the left in Figure~\ref{fig:Quant_Cov} for a visualization of this intuition. From this construction, it is clear that the $\text{LP}_\varepsilon(\mathbb P, \mathbb{Q}_n)$ is bounded by
\begin{align*}
    F_{\mathbb{Q}_n} \left(q^{(2)}_n \right)-F_{\mathbb{Q}_n} \left(q^{(1)}_n \right) &= F_{\mathbb{Q}_n} \left(\text{Quant} \left(\beta-\frac{1}{n}+\rho; \mathbb P \right)+\varepsilon \right)-F_{\mathbb{Q}_n} \left(\text{Quant} \left(\beta-\frac{1}{n}; \mathbb P \right)+\varepsilon \right) 
    \\& = F_{\mathbb P}\left(\text{Quant} \left(\beta-\frac{1}{n}+\rho; \mathbb P \right) \right) - \left( \beta - \frac{1}{n} \right)
    = \rho,
\end{align*}
showing that the sequence $\mathbb Q_n$ belongs to the LP ambiguity set $\mathbb{B}_{\varepsilon, \rho}(\mathbb P)$. Finally, we prove that the sequence of $\beta$-quantiles of $\mathbb{Q}_n$ converges to $\text{Quant}(\beta+\rho; \mathbb P)+\varepsilon$ from below. From the construction in \eqref{prop:WC:Quant:2}, we know that the following two properties hold:
\begin{itemize}
    \item $F_{\mathbb{Q}_n}(q)  < \beta, \qquad\forall~q<q^{(2)}_n$;
    \item $F_{\mathbb{Q}_n}(q)  \geq \beta, \qquad\forall~q\geq q^{(2)}_n, ~~n\geq \frac{1}{\rho}$.
\end{itemize}
Combining these two inequalities, we have that $\text{Quant}(\beta; \mathbb{Q}_n) = q^{(2)}_n$, which admits a limit as $n$ goes to infinity:
\begin{align*}
    q^{(2)}_n = \text{Quant} \left(\beta-\frac{1}{n}+\rho;\mathbb P \right)+\varepsilon
    \stackrel{n\rightarrow\infty}{\longrightarrow} \text{Quant}(\beta+\rho; \mathbb P)+\epsilon
\end{align*}
where the convergence follows from the left-continuity of the quantile function, which follows from the right-continuity of the cumulative distribution function. This concludes the proof.
\end{proof}



In words, the worst-case quantile in the LP ambiguity set $\mathbb{B}_{\varepsilon, \rho}(\mathbb{P})$ corresponds to a quantile of $\mathbb{P}$ that is shifted by the local parameter $\varepsilon$ and adjusted by the global parameter $\rho$. We will now present the second result of this section.

\begin{proposition}[Worst-case coverage in the LP ambiguity set]
\label{prop:WC:Cov}
The following holds
\begin{align}
\label{eq:WC:Cov}
    \text{Cov}_{\varepsilon,\rho}^{\text{WC}}(q;\mathbb P) = F_{\mathbb P}(q - \varepsilon) - \rho.
\end{align}
\end{proposition}
\begin{proof}
Similarly to Proposition~\ref{prop:WC:Quant}, we prove this in two steps. First, we show that the right-hand side in \eqref{eq:WC:Cov} is a lower bound on the coverage at $q$ of any distribution in $\mathbb{B}_{\varepsilon, \rho}(\mathbb P)$. Second, we prove that there exists a sequence of distributions $\mathbb Q
_n \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$, whose coverage at $q$ converges to it. 

\medskip

\noindent\emph{Step 1.} We prove, by contradiction, that $F_{\mathbb Q}(q) \geq F_{\mathbb P}(q - \varepsilon) - \rho$, for all $\mathbb Q \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$. Suppose there exists $\widetilde{\mathbb Q}$ satisfying 
\begin{align}
\label{prop:WC:Cov:1}
    \widetilde{\mathbb Q} \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P) \quad \text{and} \quad F_{\widetilde{\mathbb Q}}(q) < F_{\mathbb P}(q - \varepsilon) - \rho.
\end{align} 
We will show that this leads to
\begin{align*}
    \text{LP}_\varepsilon(\mathbb P, \widetilde{\mathbb Q}) = \inf_{\gamma\in\Gamma(\mathbb P, \widetilde{\mathbb Q})}\int_{\R\times\R} \mathds{1}\{|z_1 - z_2| > \varepsilon\}\mathrm{d}\gamma(z_1,z_2) > \rho.
\end{align*}
From the inequality in \eqref{prop:WC:Cov:1}, we know that there exists $\Delta > 0$ such that
\begin{align*}
    F_{\widetilde{\mathbb Q}}(q) \leq F_{\mathbb P}(q - \varepsilon)-(\rho+\Delta).
\end{align*}
Meanwhile, for any coupling $\gamma\in\Gamma(\mathbb P, \widetilde{\mathbb Q})$, we have
\begin{align*}
    \rho+\Delta &\leq F_{\mathbb P}(q-\varepsilon) - F_{\widetilde{\mathbb Q}}(q)
     = \int_{-\infty}^{q-\varepsilon}\int_{-\infty}^\infty d\gamma(z_1,z_2) - \int_{-\infty}^\infty \int_{-\infty}^{q} d\gamma(z_1,z_2)\\
    &=\int_{-\infty}^{q-\varepsilon}\int_{-\infty}^{q} d\gamma(z_1,z_2)+\int_{-\infty}^{q-\varepsilon}\int_{q+}^\infty d\gamma(z_1,z_2)-\int_{-\infty}^{q-\varepsilon} \int_{-\infty}^{q} d\gamma(z_1,z_2)-\int_{(q-\varepsilon)+}^\infty \int_{-\infty}^{q} d\gamma(z_1,z_2)\\
    &\leq\int_{-\infty}^{q-\varepsilon}\int_{q+}^\infty \mathds{1}_{\{|z_1-z_2|> \varepsilon\}}~d\gamma(z_1,z_2)\\
    &\leq\int_{\mathbb{R}\times\mathbb{R}} \mathds{1}_{\{|z_1-z_2|> \varepsilon\}}~d\gamma(z_1,z_2).
\end{align*}
Taking an infimum over $\gamma\in\Gamma(\mathbb P, \widetilde{\mathbb Q})$, we obtain that the $\text{LP}_\varepsilon(\mathbb P, \widetilde{\mathbb Q})>\rho$, which contradicts the fact that $\widetilde{\mathbb Q} \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$. This proves that $F_{\mathbb Q}(q) \geq F_{\mathbb P}(q - \varepsilon) - \rho$, for all $\mathbb Q \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$.

\medskip

\noindent\emph{Step 2.} We construct a sequence of distributions $\mathbb Q_n \in \mathbb{B}_{\varepsilon, \rho}(\mathbb P)$ satisfying, as $n \to \infty$,
\begin{align*}
    F_{\mathbb Q_n}(q) \to F_{\mathbb P}(q - \varepsilon) - \rho.
\end{align*}
We define the sequence of distributions $\mathbb Q_n$ through their cumulative distribution functions as
\begin{equation*}
        F_{\mathbb Q_n}(\gamma) = \begin{cases}
        F_\mathbb{P}(\gamma-\varepsilon), & \gamma<\text{Quant}\left(F_\mathbb{P}(q-\varepsilon)-\rho+\frac{1}{n}; \mathbb P \right)+\varepsilon\\
        F_\mathbb{P}(q-\varepsilon)-\rho+\frac{1}{n}, & \text{Quant} \left(F_\mathbb{P}(q-\varepsilon)-\rho+\frac{1}{n};\mathbb P \right)+\varepsilon\leq \gamma <  \text{Quant} \left(F_\mathbb{P}(q-\varepsilon)+\frac{1}{n};\mathbb P \right)+\varepsilon\\
        F_\mathbb{P}(\gamma-\varepsilon), & \gamma \geq \text{Quant} \left(F_\mathbb{P}(q-\varepsilon)+\frac{1}{n};\mathbb P \right)+\varepsilon.
        \end{cases}
    \end{equation*}
To simplify notation, for the rest of the proof, we define $q^{(1)}_n = \text{Quant}(F_{\mathbb P}(q-\varepsilon)-\rho+\frac{1}{n}; \mathbb P)+\varepsilon$ and $q^{(2)}_n = \text{Quant}(F_{\mathbb P}(q-\varepsilon)+\frac{1}{n}; \mathbb P)+\varepsilon$.  The intuition behind the construction of $\mathbb Q_n$ is as follows: first, $\mathbb{Q}_n$ is obtained by translating the distribution $\mathbb P$ to the right by $\varepsilon$, and then, the mass between $[q^{(1)}_n,q^{(2)}_n)$ is moved to the point $q^{(2)}_n$. We refer to the illustration on the right in Figure~\ref{fig:Quant_Cov} for a visualization of this intuition. From this construction, it is clear that the $\text{LP}_\varepsilon(\mathbb P, \mathbb{Q}_n)$ is bounded by
\begin{align*}
    F_{\mathbb{Q}_n} \left(q^{(2)}_n \right)-F_{\mathbb{Q}_n} \left(q^{(1)}_n \right) &= F_{\mathbb P}\left(\text{Quant} \left(F_{\mathbb P}(q-\varepsilon)+\frac{1}{n}; \mathbb P \right) \right) - F_{\mathbb P} \left( \text{Quant} \left(F_{\mathbb P}(q-\varepsilon)-\rho+\frac{1}{n}; \mathbb P \right) \right)
    \\ &= F_{\mathbb P}(q-\varepsilon)+\frac{1}{n} - \left( F_{\mathbb P}(q-\varepsilon)-\rho+\frac{1}{n} \right) = \rho,
\end{align*}
showing that the sequence $\mathbb Q_n$ belongs to the LP ambiguity set $\mathbb{B}_{\varepsilon, \rho}(\mathbb P)$. Moreover, when $n\geq\frac{1}{\rho}$, we have that $q\in[q^{(1)}_n, q^{(2)}_n)$ holds, and therefore
\begin{align*}
    F_{\mathbb Q_n}(q) = F_{\mathbb P}(q-\varepsilon)-\rho+\frac{1}{n} \stackrel{n\rightarrow\infty}{\longrightarrow} F_{\mathbb P}(q-\varepsilon)-\rho.
\end{align*}
This concludes the proof.
\end{proof}

Similarly to the worst-case quantile, the worst-case coverage in the LP ambiguity set $\mathbb{B}_{\varepsilon, \rho}(\mathbb{P})$ corresponds to the coverage of $\mathbb{P}$ shifted by the local parameter $\varepsilon$ and adjusted by the global parameter $\rho$. The proofs of Propositions~\ref{prop:WC:Quant} and \ref{prop:WC:Cov} are constructive, in the sense that we propose two sequences of distributions which attain, in the limit, the two quantities $\text{Quant}_{\varepsilon,\rho}^{\text{WC}}(\beta;\mathbb P)$ and $\text{Cov}_{\varepsilon,\rho}^{\text{WC}}(q;\mathbb P)$, respectively. The intuition for both constructions stems from Proposition~\ref{prop:LP:decomposition}, which allows us to construct every distribution in $\mathbb{B}_{\varepsilon, \rho}(\mathbb{P})$ using a two-step procedure that decouples the local and global perturbations. This intuition is illustrated in Figure~\ref{fig:Quant_Cov}.





%--------------------------------------------------------------------------
%--------------------------------------------------------------------------






\section{Distributionally Robust Conformal Prediction}
\label{sec:robust:conformal}

In this section, we demonstrate how the worst-case quantile and coverage introduced earlier enable the construction of a confidence interval and its worst-case coverage for all distributions in the LP ambiguity set. We start by defining the prediction set
\begin{align}
\label{eq:WC:prediction:set}
    C_{\varepsilon,\rho}^{1-\alpha}(x;\mathbb P) := \left\{y\in\mathcal{Y}:\,s(x,y)\leq \text{Quant}_{\varepsilon,\rho}^{\text{WC}}(1 - \alpha;\mathbb P) \right\},
\end{align}
where, as noted in Proposition~\ref{prop:WC:Quant}, $\text{Quant}_{\varepsilon,\rho}^{\text{WC}}(1 - \alpha;\mathbb P) = \text{Quant}(1 - \alpha +\rho; \mathbb P) + \varepsilon$. Observe that $C_{\varepsilon,\rho}(x;\mathbb P)$ depends on the training distribution $\mathbb P$, which is unknown. Instead, we assume access to $n$ exchangeable data points $\{s(X_i,Y_i)\}_{i=1}^n \sim \mathbb P$. Based on this, we define the empirical distribution
\begin{align*}
    \widehat{\mathbb P}_n := \frac{1}{n} \sum_{i=1}^n \delta_{s(X_i,Y_i)}
\end{align*}
and consider the empirical confidence set $C_{\varepsilon,\rho}^{1-\alpha}(x;\widehat{\mathbb P}_n)$. We now state the main result of this paper.

\begin{theorem}[Conformal Prediction under LP distribution shifts]
\label{thm:robust:CP}
Let $s(X_{n+1}, Y_{n+1}) \sim \mathbb P_{\text{test}}$ be independent of $\{s(X_i, Y_i)\}_{i=1}^n \sim \mathbb P$. Moreover, let $\text{LP}_\varepsilon(\mathbb P, \mathbb P_{\text{test}}) \leq \rho$. Then, 
\begin{align}
\label{eq:robust:CP}
    \text{Prob}\left\{ Y_{n+1} \in C_{\varepsilon,\rho}^{1-\alpha}\left(X_{n+1};\widehat{\mathbb P}_n \right) \right\} \geq \frac{\lceil n(1-\alpha+\rho)\rceil}{n+1}-\rho.
\end{align}
\end{theorem}
\begin{proof}
By conditioning on $\{(X_i,Y_i)\}_{i=1}^n$, we obtain
\begin{align*}
    \text{Prob}\left\{Y_{n+1}\in C_{\varepsilon,\rho}(X_{n+1};\widehat{\mathbb P}_n)|\{(X_i,Y_i)\}_{i=1}^n\right\} & = F_{\mathbb P_{\text{test}}}\left(\text{Quant}_{\varepsilon,\rho}^{\text{WC}} \left(1 - \alpha;\widehat{\mathbb P}_n \right)\right)
    \\& = F_{\mathbb P_{\text{test}}}\left(\text{Quant} \left(1-\alpha +\rho; \widehat{\mathbb P}_n \right)+\varepsilon\right)
    \\& \geq F_{\mathbb P}\left(\text{Quant}\left(1-\alpha +\rho; \widehat{\mathbb P}_n\right)+\varepsilon -\varepsilon\right) -\rho\\
    &= F_{\mathbb P}\left(\text{Quant}\left(1-\alpha +\rho, \widehat{\mathbb P}_n\right)\right)-\rho,
\end{align*}
where the first equality follows from Definition~\ref{eq:WC:prediction:set}, the second equality follows from Proposition~\ref{prop:WC:Quant}, and the first inequality is a consequence of Proposition~\ref{prop:WC:Cov}. Now, taking the expectation with respect to $\{(X_i,Y_i)\}_{i=1}^n$, we obtain
\begin{align*}
    \text{Prob}\left\{Y_{n+1}\in C_{\varepsilon,\rho}^{1-\alpha}\left(X_{n+1};\widehat{\mathbb P}_n \right)\right\} \geq \mathbb E \left[ F_{\mathbb P}\left(\text{Quant}\left(1-\alpha +\rho, \widehat{\mathbb P}_n\right)\right) \right] -\rho \geq \frac{\lceil n(1-\alpha+\rho)\rceil}{n+1}-\rho,
\end{align*}
where the second inequality follows from the guarantee $\mathbb{E}\left[F_{\mathbb P}(\text{Quant}(\beta; \widehat{\mathbb P}_n))\right] \geq {\lceil n\beta\rceil}/{(n+1)}$ (see \cite[Lemma~D.3]{cauchois2024robust}). This concludes the proof.
\end{proof}

A few remarks are in order. First, the local parameter $\varepsilon$ affects only the size of the confidence interval, but not its coverage. This is expected, given the construction of the two sequences of distributions that achieve the worst-case quantile and coverage in Propositions~\ref{prop:WC:Quant} and \ref{prop:WC:Cov}, respectively (see also the illustration in Figure~\ref{fig:Quant_Cov}). Each distribution in the sequence was obtained by translating $\mathbb P$ to the right by $\varepsilon$, a transformation that clearly does not affect the confidence level. Second, as expected, the distribution shift reduces the coverage below the desired $1-\alpha$ level. The following corollary provides an adjusted coverage for the worst-case quantile, ensuring a $1-\alpha$ confidence level in \eqref{eq:robust:CP}.

\begin{corollary}
\label{cor:robust:CP}
Let $\beta = \alpha + {(\alpha-\rho-2)}/{n}$. Under the same conditions as in Theorem~\ref{thm:robust:CP}, we have
\begin{align}
\label{eq:cor:robust:CP}
    \text{Prob}\left\{ Y_{n+1} \in C_{\varepsilon,\rho}^{1-\beta}\left(X_{n+1};\widehat{\mathbb P}_n \right) \right\} \geq 1 - \alpha.
\end{align}
\end{corollary}
\begin{proof}
Note that ${\lceil n(1-\beta+\rho)\rceil}/{(n+1)} - \rho \geq 1-\alpha$ is guaranteed by $n(1-\beta+\rho) \geq (n+1)(1-\alpha+\rho) + 1$, which is further guaranteed by $\beta \leq \alpha + {(\alpha-\rho-2)}/{n}$. This concludes the proof.
\end{proof}

Finally, recall from Proposition~\ref{prop:LP:decomposition} that the LP pseudo-metric recovers the Total Variation and the $\infty$-Wasserstein distances if $\varepsilon =0$ and $\rho = 0$, respectively. As a consequence, the guarantee in Corollary~\ref{cor:robust:CP} can be immediately specialized to these additional types of distribution shifts. We do this in the following corollary.

\begin{corollary}[TV and $\text{W}_{\infty}$ distribution shifts]
\label{cor:TV:Winf}
Under the same conditions as in Corollary~\ref{cor:robust:CP},
\begin{itemize}
    \item[(i)] \emph{Total Variation distance.} If $\varepsilon = 0$, then guarantee \eqref{eq:cor:robust:CP} holds with
    \begin{align}
    \label{eq:guarantee:TV}
        C_{0,\rho}^{1-\beta}(X_{n+1};\widehat{\mathbb P}_n) = \left\{y\in\mathcal{Y}:\,s(x,y)\leq \text{Quant} \left(1 - \frac{(\alpha-\rho) (n+1) -2}{n}; \widehat{\mathbb P}_n \right)
        \right\}.
    \end{align}
    \item[(ii)] \emph{$\infty$-Wasserstein distance.} If $\rho = 0$, then guarantee \eqref{eq:cor:robust:CP} holds with
    \begin{align}
    \label{eq:guarantee:Winf}
        C_{\varepsilon, 0}^{1-\beta}(X_{n+1};\widehat{\mathbb P}_n) = \left\{y\in\mathcal{Y}:\,s(x,y)\leq \text{Quant} \left(1 - \frac{\alpha (n+1)-2}{n}; \widehat{\mathbb P}_n \right) + \varepsilon
        \right\}.
    \end{align}
\end{itemize}
\end{corollary}

We note that a guarantee similar to \eqref{eq:guarantee:TV} was previously established in \cite{cauchois2024robust}, which addressed $f$-divergence distribution shifts, by recognizing the TV distance as a special case of the $f$-divergence.



%--------------------------------------------------------------------------
%--------------------------------------------------------------------------




\section{Experiments}
% In this section, we numerically validate the theoretical results obtained in this paper.

We conduct experiments on three classification datasets: MNIST~\cite{Lecun_1998},  ImageNet~\cite{Li_2009}, and iWildCam~\cite{Beery_2020}. The validity of our algorithm is assessed based on \emph{empirical coverage}, defined as 
\begin{align*}\frac{1}{K} \sum^K_{i=1}\mathds{1}\left\{y_i \in C_{\varepsilon,\rho}^{1-\beta}\left(x_i;\widehat{\mathbb P}_n \right)\right\},
\end{align*}
where $\beta$ is defined in Corollary~\ref{cor:robust:CP} to achieve $1-\alpha$ coverage, and $\{(x_i, y_i)\}^K_{i=1}$ is the test set with scores $s(X_i, Y_i)$. For all experiments, $\alpha$ is set to $0.1$ and the score function $s(x, y) = -\log p(y|x)$ is chosen to be the negative log likelihood (NLL). To assess efficiency, we consider the \emph{prediction set size}, which refers to the number of classes included in the prediction set. 

On MNIST and ImageNet, we simulate test-time \textit{data-space} distribution shifts via global and local perturbations. By Proposition \ref{prop:propagation:LP}, these shifts naturally translate to \textit{score-space} distribution shifts in an LP sense. Calibration data points for the conformal prediction procedure are randomly sampled from the two datasets' respective validation sets. In contrast, iWildCam features real-world distribution shift resulting from changes in camera positions (location ID) and recording dates (year). For the classification task, \cite{Koh_2021} provides a training dataset along with an out-of-distribution test dataset, where the distribution has shifted. Additionally, for comparative analysis, ``in-distribution'' validation and test datasets are provided, consisting of images captured from the same camera positions as the training data but recorded on different dates; they are considered to share the same distribution as the training data. We use the in-distribution test set here for calibration. A pre-trained ResNet-152 model is used as the classifier for ImageNet, while a simple ResNet is used for MNIST. For iWildCam, we use a pre-trained ResNet-50 model provided by the authors of \cite{Beery_2020}. All datasets are normalized based on the empirical mean and standard deviation of the images.

We compare our algorithm to standard split conformal prediction (SC) and chi-square ($\chi^2$) divergence robust conformal prediction~\cite{cauchois2024robust}. For a given level $\alpha$ and $n$ calibration data points, the prediction sets for each algorithm are constructed from the following quantiles:
\begin{enumerate}
    \item \textit{Standard Conformal Prediction: } \begin{align*}
    \qt\left({\lceil(n+1)(1-\alpha)\rceil}/{n}; \widehat{\mathbb{P}}_n\right)
    \end{align*}
    \item \textit{LP Robust Conformal Prediction (following Corollary \ref{cor:robust:CP}): }
    \begin{align*}
    \mathrm{Quant}\Bigl(1-\beta+\rho; \widehat{\mathbb{P}}_n\Bigr) + \varepsilon,\qquad \beta = \alpha + (\alpha-\rho-2)/n
    \end{align*}
    \item \textit{$\chi^2$ Robust Conformal Prediction: }
    \begin{align*}
    &\qt\left( g_{f,\rho}^{-1} (1-\alpha_n); \widehat{\mathbb{P}}_n \right), \ \alpha_n = g_{f,\rho} \left( \left(1 + {1}/{n} \right) g_{f,\rho}^{-1} (1 - \alpha) \right), 
%     \\
%     & g_{f,\rho}^{-1}(\tau) = \sup \left\{ \beta \in [0,1] : g_{f,\rho}(\beta) \leq \tau \right\}, ~\tau\in[0, 1], \\
% &g_{f,\rho}(\beta) := \inf \left\{ z \in [0,1] : \beta f \left( \frac{z}{\beta} \right) + (1-\beta) f \left( \frac{1-z}{1-\beta} \right) \leq \rho \right\},~ \beta\in[0, 1].
    \end{align*}
where $\rho$ is the ambiguity set radius, $f(x) = (x-1)^2$, and $g_{f,\rho}$ and $g_{f,\rho}^{-1}$ are as defined in \cite{cauchois2024robust}.
\end{enumerate}
The code to reproduce our results is available at our GitHub repository\footnote{\url{https://github.com/olivrw/LP-robust-conformal.git}}.

\subsection{Data-Space Distribution Shift: MNIST and ImageNet}
\label{sec:datspace_exp}

\begin{figure}[t]
\centering
\includegraphics[width=0.4\linewidth]{figures/score_dist/MNIST_scores_plot.png}
\includegraphics[width=0.4\linewidth]{figures/score_dist/ImageNet_scores_plot.png}
\caption{Score distribution shift plots on MNIST and ImageNet under ($p=0.05, u=1.0$) perturbation. The score distribution obtained from the unperturbed data (red), and from the perturbed data (blue) are plotted in log scale. For ImageNet, we removed 18 negative-valued outliers ranging from -5.5 to -10 for visualization purposes.}
\label{fig:score_shift}
\end{figure}

To simulate local perturbations for MNIST and ImageNet data, we add independent draws of random noise from $\mathcal{U}([-u, u])$ to all the channels of each pixel of each test image. We simulate global perturbations by corrupting a random fraction $p$ of test labels: each corrupted label is replaced with its neighboring label. Such perturbations are realistic as test-time images are often observed with noise, \textit{and} a small fraction of all data may be labeled incorrectly~\cite{Feldman_2023, Zargarbashi_2024}. We refer to Figure~\ref{fig:score_shift} for a visualization of the distribution shifts induced by the pair $(p =0.05, u=1.0)$.

Following the split conformal procedure, calibration NLL scores are computed on unperturbed calibration data points to determine empirical quantiles. Constructing prediction sets is then straightforward for standard conformal prediction. For the robust algorithms, ours naturally models global and local perturbations via the choices of $\rho$ and $\varepsilon$, respectively. 
%
Following Proposition~\ref{prop:propagation:LP}, we simply set $\rho = p$. Proposition~\ref{prop:propagation:LP} also suggests setting $\varepsilon = ku$, where $k$ is the Lipschitz constant of the score function. This constant may be estimated from data, but in practice we find that it yields a rather loose bound, as a global Lipschitz constant may not be representative of the behavior of the score function over the region where the data are concentrated. Instead, we find that setting $k$ to a relatively small value, here $k=2$, suffices to achieve valid coverage across the full range of data-space shifts $u$ for all three datasets. 
%\todo{Empirically, having set $\rho$, we could tune $\varepsilon$ to achieve the smallest possible prediction set size that still maintains valid coverage. But we avoid this detailed tuning here, as it w}

For the $\chi^2$ robust algorithm, \cite{cauchois2024robust} proposes two procedures for estimating the ambiguity set radius $\rho$, based either on an assumed collection of possible shift directions or the solution of an optimization problem. 
%
%that first assumes a collection of possible shift directions, then constructs a family of shifted feature distributions by restraining the original feature distribution onto a smaller subset of the feature space $\cal{X}$ based on the assumed directions. Then, validity is guaranteed by finding the ambiguity set radius $\rho$ via an optimization procedure. 
%
%guaranteed either with high probability over the set of shifted distributions, or for a worst-case shift direction.
%
As these techniques are comparatively computationally complex and involve restrictive assumptions, we instead tune $\rho$ for the $\chi^2$ robust algorithm empirically, for each given $(u, p)$ pair. In principle one would like to find the smallest $\rho$ that achieves the desired predictive coverage, but due to the high sensitivity of this coverage to $\rho$, it is practically challenging to ensure that the selected radius corresponds to a sharp prediction set. In contrast, for the LP robust algorithm, we do not perform such tuning; instead, we fix a single value of $k$ and use it consistently across all simulated shifts. 

\begin{figure}[t]
\centering
\includegraphics[width=0.32\linewidth]{figures/both_figures/mnist_both_3.pdf}
\includegraphics[width=0.32\linewidth]{figures/both_figures/mnist_both_2.pdf}
\includegraphics[width=0.32\linewidth]{figures/both_figures/mnist_both_1.pdf}

\includegraphics[width=0.32\linewidth]{figures/both_figures/imgnet_both_3.pdf}
\includegraphics[width=0.32\linewidth]{figures/both_figures/imgnet_both_2.pdf}
\includegraphics[width=0.32\linewidth]{figures/both_figures/imgnet_both_1.pdf}
\caption{Data-space distribution shift validity and efficiency. Desired $1-\alpha$ coverage (long dark red line); empirical coverage and prediction set size for each split (scattered points); and mean coverage and prediction set size across 30 calibration-test splits (short horizontal lines).}
\label{fig:data_shift_results}
\end{figure}

% describing a simulated shift, we find the smallest $\rho$ that guarantees valid coverage, \todo{OK, this seems not quite true, looking at the figure. We have more coverage than desired, in many examples. Why is that? Is it because we have a limited-resolution grid of candidate $\rho$ values? Or is there some other reason?}
% as this in turn yields a more efficient prediction set. This tuning is thus  favorable to the $\chi^2$ robust algorithm, and is simply for illustrative purposes. Note that for the LP robust algorithm, we do not perform such tuning; rather, we fix a single value of $k$ and use it across all simulated shifts.

% future distribution shift is modeled through the calibration \emph{feature} data~\cite{cauchois2024robust}. In their two-step procedure, a collection of possible shift directions is first assumed, then a family of shifted feature distributions is constructed by restraining the original feature distribution onto a smaller subset of the feature space $\cal{X}$ based on the shift directions. Then, validity is guaranteed either with high-probability over the set of shifted distributions, or for a worst-case shift direction via optimization. Despite having these techniques, we omit them due to the computational complexity and restrictive assumptions introduced. The ambiguity set radius $\rho$ for the $chi^2$ robust algorithm is therefore chosen empirically, \todo{separate choice for each value of $u$ and $p$?}
% to maximize performance in terms of validity and efficiency.  

Figure~\ref{fig:data_shift_results} plots the empirical coverage and prediction set size results (over 30 random calibration-test splits) generated by the three approaches on the three datasets, for three noise-corruption configurations: $(p,u) \in$ \{(0.01, 0.25), (0.025, 0.5), (0.05, 1.0)\}. Standard conformal prediction suffers an increasing loss of coverage as $u$ and $p$ grow, whereas both robust methods achieve valid coverage. 
%
The coverage achieved by our LP approach remains valid across all experiments, with performance similar to that of the $\chi^2$ method. 
%
In terms of efficiency, our algorithm performs similarly to $\chi^2$ on ImageNet but yields significantly smaller prediction set sizes on MNIST. 
%
Across all algorithms and examples, larger coverage probabilities correspond to larger prediction set sizes, as expected. We also note that larger mean prediction set sizes are associated with higher variances in the prediction set size, due to the presence of many possible low-confidence labels; this is particularly apparent in some of the $\chi^2$ results.

This numerical illustration also underscores an important \emph{modeling} point. In the present example, it is not at all clear that an ambiguity set under $\chi^2$ divergence or some other metric captures our shifted distribution, and thus it is not obvious how to choose the ball radius; instead one must resort to some empirical tuning. In contrast, the LP approach provides a natural way of choosing the ambiguity set given the perturbations imposed here.

% This result demonstrates the superior efficiency of our algorithm, particularly for tasks where the prediction problem is relatively simple. 
%

% Bigger coverage yields bigger size. But larger mean prediction set sizes are associated with higher variance in the prediction set size, due to the presence of *many* possible low-confidence labels.

% notes
% - For the chi^2 div, it's not clear that the ball captures the perturbed distribution (especially given AC assumptions)
% - It's not clear how to choose the ball size
% - Hence, we resort to some empirical tuning
%
% - In contrast, if we know something about our distribution shift, then the LP approach provides a natural way of choosing the ambiguity set. The LP model is somehow very compatible with the knowledge we have about the perturbations.

% Additionally, while~\cite{cauchois2024robust} proposed several high-probability estimation techniques, our algorithm is better suited for modeling scenarios involving both local and global perturbations. Overall, our algorithm produces prediction sets that maintain valid coverage while being more efficient than state-of-the-art methods across varying perturbation levels. We further highlight that greater efficiency can be achieved with more principled approach for selecting $\rho$ and $\varepsilon$.

% HIGH VARIANCE FOR CHI^2?

\subsection{Real-world Distribution Shift: iWildCam}
\begin{figure}[t] 
\centering
\includegraphics[width=0.45\linewidth]{figures/wilds_cover_plt.png}
\includegraphics[width=0.45\linewidth]{figures/wilds_size_plt.png}
\caption{iWildCam example: coverage (left) and prediction set size (right) over a range of $(\varepsilon,\rho)$, illustrated with color maps. Desired 90\% coverage is the black dotted line; points above and to the right of this line achieve valid coverage. The point $(0,0)$ represents standard conformal prediction.}
\label{fig:wilds_grid}
\end{figure}
Now we evaluate our algorithm's effectiveness in handling real-world distribution shifts. The iWildCam dataset~\cite{Beery_2020} is a multi-class prediction problem that contains realistic train-test distribution shifts caused by variability in camera trap position and timing, and the induced variability in illumination, color, camera angle, background, vegetation, and relative animal frequencies. 
%
As noted earlier, we use iWildCam's ``out-of-distribution'' test dataset as test dataset.
%
In Figure~\ref{fig:wilds_grid}, we analyze the impact of the LP ambiguity set parameters $\varepsilon$ and $\rho$ by plotting results for empirical coverage and prediction set size over the two-dimensional space $(\varepsilon, \rho) \in [0,2] \times [0, 0.02]$.
%
The coverage plot in Figure \ref{fig:wilds_grid} illustrates a trade-off between $\varepsilon$ and $\rho$, and captures the transition between a regime where local perturbations dominate the ambiguity set and a regime where global perturbations dominate. Generally, larger parameter values achieve valid coverage, but further increases in either parameter yield overly conservative prediction sets. To obtain the sharpest prediction set, one can select parameters from the 90\% coverage curve, which corresponds to a prediction set size of roughly 7 out of 182 possible classes.

The results suggest that LP ambiguity sets are capable of capturing real-world distribution shifts even if there is no \textit{a priori} understanding of the local/global separation of perturbations. In future work, it would be very useful to develop automated tools for estimating the LP ambiguity set parameters $\rho$ and $\varepsilon$ in examples like this one.












\section*{Acknowledgement}

Liviu Aolaritei acknowledges support from the Swiss National Science Foundation through the Postdoc.Mobility Fellowship (grant agreement P500PT\_222215). Youssef Marzouk and Julie Zhu acknowledge support from the US Department of Energy (DOE), Office of Advanced Scientific Computing Research, under grant DE-SC0023188. Youssef Marzouk and Zheyu Oliver Wang acknowledge support from the ExxonMobil Technology and Engineering Company.





%\nocite{*}

\bibliographystyle{myabbrvnat} 
\bibliography{references.bib}


\appendix
\begin{comment}
\section{Estimate the LP Parameters from Data}
{\color{blue}Although LP is a natural selection given noisy and misspecified data, it is critical to determine the scale of each type of noise and find suitable $\varepsilon$ and $\rho$ from finite dataset.

Consider the situation where the bounded local noise perturbed the feature vector $\tilde{X} = X+n_l$ and the global misspecification attack the label $\tilde{Y} = n_g$, and we have access to both the clean data and noisy data. Then the natural estimation for $\varepsilon$ and $\rho$ is 
\begin{align*}
    \hat\varepsilon = \max_{i=1,\dots,n} \|\tilde{X}_i-X_i\|_2,\qquad \hat\rho = \frac{\sum_{i=1}^n\mathds{1}(\tilde{Y}_i\neq Y_i)}{n}.
\end{align*}

To compensate for finite samples in the datasets and find an upper bound for both parameters with confidence, we add a correction term which vanishes as $n\rightarrow\infty$ and provide a high-probability bound:
\begin{prop}
    Assume we have $n$ i.i.d. samples $(X_i, Y_i)_{i=1,\dots,n}$ from the original data distribution, with corresponding noisy realization $(\tilde{X}_i,\tilde{Y}_i)_{i=1,\dots,n}$. We make no assumption on the noise distribution except that $|1-P(\|n_l\|_2 < \varepsilon-\delta)|\notin\mathcal{O}(\delta)$. Then with high probability, we have an upper bound $\hat\varepsilon$, $\hat\rho$ for $\varepsilon$, $\rho$, and any $\tau>0$:
    \begin{align*}
        \hat\varepsilon := \frac{\sqrt{n}+1}{\sqrt{n}}\max_{i=1,\dots,n} \|\tilde{X}_i-X_i\|_2,\qquad \hat\rho := \frac{1}{n}\sum_{i=1}^n\mathds{1}(\tilde{Y}_i\neq Y_i)+n^{-\frac{1}{2+\tau}}.
    \end{align*}
\end{prop}
\begin{proof}
    The assumption $|1-P(\|n_l\|_2 < \varepsilon-\delta)|\notin\mathcal{O}(\delta)$ ensures that the probability for $\|\tilde X_i-X_i\|_2$ to be close to the upper bound $\varepsilon$ does not vanish exponentially.
    
    For any fixed $\delta$ close to 0, the probability that $\hat\varepsilon$ is not an upper bound for $\varepsilon$ is given by
    \begin{align*}
        P\left(\hat\varepsilon<\varepsilon\right) &= P\left(\frac{\sqrt{n}+1}{\sqrt{n}}\max_{i=1,\dots,n} \|\tilde{X}_i-X_i\|_2<\varepsilon\right)\\
        &= \prod_{i=1}^n P\left(\|n_l\|_2 < \frac{\sqrt{n}\varepsilon}{\sqrt{n}+1}\right)\\
        &=\prod_{i=1}^n \left((1-P\left((\|n_l\|_2 \geq \frac{\sqrt{n}\varepsilon}{\sqrt{n}+1}\right)\right)\\
        &\leq \left(1-\frac{c}{\sqrt{n}+1}\right)^n\\
        &\approx e^{-c\sqrt{n}}
    \end{align*}
    where $c$ is a constant depending only on the distribution of $n_l$ at $\varepsilon^-$. It follows that $\hat\varepsilon\geq\varepsilon$ with probability at least $1-e^{-c\sqrt{n}}$.

    note that $\mathds{1}(\tilde{Y}_i\neq Y_i)$ is a Bernoilli distribution with parameter $\rho$. By Hoeffding's inequality,
    \begin{align*}
        P\left(|\sum_{i=1}^n \mathds{1}(\tilde{Y}_i\neq Y_i)-p|\geq\delta\right) \leq 2\exp\left(-2n\delta^2\right).
    \end{align*}
    Therefore, for any $\tau>0$, $\hat{\rho} = \frac{1}{n}\sum_{i=1}^n\mathds{1}(\tilde{Y}_i\neq Y_i)+n^{-\frac{1}{2+\tau}}$ is a valid upper bound for the true $\rho$ with probability at least $1-2e^{-2n^{\frac{\tau}{2+\tau}}}$. 

    The estimation $\hat\varepsilon$ and $\hat\rho$ converge to the true $\varepsilon$ and $\rho$ asymptotically when $n\rightarrow\infty$.
\end{proof}
The Theorem above gives an upper bound for general noise distribution $n_l$. A tighter bound can be derived if the effective noise is smaller than a threshold most of the time, and we categorized the noise beound this threshold into misspecification error.

In practice one would not have access to the clean data, and the local and global contamination would apply to the whole data instead of influencing two components separately. However, it is still possible to determine $\varepsilon$, $\rho$ such that the noisy distribution is contained in the $LP_\varepsilon$ ball of radius $\rho$. However, there would be multiple pairs of $\varepsilon, \rho$  due to the trade off between local and global noise. One can cross-check and find the optimal pair by testing on the calibration set such that the prediction set remains sharp and the coverage rate is beyond $1-\alpha$.}
%---------------------------------------------------------------------

\end{comment}


\begin{comment}
\section{Additional Experiments}

To further validate our method's performance, we repeat the experiment from \Cref{sec:datspace_exp}, this time with increased noise levels $u \in (2.0, 2.5, 3.0)$. The results show that the robust methods maintain validity, with LP outperforming $\chi^2$ in terms of efficiency on the MNIST problem. When the noise level is extremely high, achieving valid coverage requires including the entire label set. While this outcome is not ideal, we note that such extreme local perturbations are unlikely in realistic settings.

\begin{figure*}[ht]\label{fig: additional_0.01}
\centering
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.01_mnist_newboth_3.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.01_mnist_newboth_2.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.01_mnist_newboth_1.pdf}

\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.01_imgnet_newboth_3.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.01_imgnet_newboth_2.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.01_imgnet_newboth_1.pdf}

\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.025_mnist_newboth_3.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.025_mnist_newboth_2.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.025_mnist_newboth_1.pdf}

\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.025_imgnet_newboth_3.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.025_imgnet_newboth_2.pdf}
\includegraphics[width=0.32\linewidth]
{figures/additional_figures/add_fig_pdfs/0.025_imgnet_newboth_1.pdf}

\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.05_mnist_newboth_3.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.05_mnist_newboth_2.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.05_mnist_newboth_1.pdf}

\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.05_imgnet_newboth_3.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.05_imgnet_newboth_2.pdf}
\includegraphics[width=0.32\linewidth]{figures/additional_figures/add_fig_pdfs/0.05_imgnet_newboth_1.pdf}
\caption{Data-space distribution shift validity and efficiency. Desired $1-\alpha$ coverage (long dark red line); empirical coverage and prediction set size for each split (scattered points); and mean coverage and prediction set size across 30 calibration-test splits (short horizontal lines).}
\end{figure*}
\end{comment}









\begin{comment}

\subsection{Additional experiments TBD}
There are several experiments/additional plots that we should make and can start right after the submission:
\begin{itemize}
    \item MNIST and ImageNet: To be more systematic about all of these numerical explorations. For the $\chi^2$ results, one could make a line/scatter plot for any given $(p, u)$ where $\rho$ is the horizontal axis and there are two color-coded vertical axes, one for coverage and one for size. The plot could then show the trend versus $\rho$, where the line shows the mean (coverage or size) and the scatter points illustrate the variability!
    
    \item Plot the pdf of the score, under calibration $\mathbb{P}$ and also under the shifted $\mathbb{P}_{test}$. Maybe we can also quantify the $\chi^2$-distance and LP numerically?
    
    \item Try CIFAR 10 and CIFAR 100, or other wilds datasets
\end{itemize}

\end{comment}


\end{document}