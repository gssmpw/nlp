\section{Related Work}
\label{subsec:related:work}

Under train-test distribution shifts that violate exchangeability, conformal prediction often fails to maintain valid coverage guarantees~\cite{tibshirani2019conformal}. Extensions to conformal prediction under such shifts can be summarized into three main categories: sample reweighting, ambiguity sets, and sequential learning.

\medskip

\noindent\emph{Sample Reweighting.} This approach assigns weights to calibration samples based on their relevance to the test data. For instance, \cite{tibshirani2019conformal} proposed weighted conformal prediction for covariate shift, where the marginal distribution $\mathbb P_X$ changes while the conditional distribution $\mathbb P_{Y|X}$ remains fixed. Likelihood ratios are used to adjust for compositional differences, enabling valid predictions. Subsequent extensions address label shift~\cite{Podkopaev_2021}, causal inference~\cite{Lei_2021}, and survival analysis~\cite{Candes_2023, Gui_2022}. However, these methods rely on the accurate estimation of likelihood ratios, which may be challenging in practice. For spatial data, \cite{Mao_2022} proposed weighting samples based on proximity to test points. Compared to these approaches, our method handles distribution shifts in the \textit{joint} distribution $\mathbb P$ of $(X,Y)$, without requiring likelihood ratios, and remains effective under more complex local and global perturbations.

\medskip

\noindent\emph{Ambiguity Sets.} Ambiguity sets provide a flexible framework for modeling uncertainty in the data distribution. For instance, \cite{cauchois2024robust} used an $f$-divergence ambiguity set around the training distribution to derive worst-case coverage guarantees and adjusted prediction sets. This work is most closely related to ours, and while their analysis inspired our approach, we rely on fundamentally different tools, particularly drawing on optimal transport techniques. A key limitation of $f$-divergences is that they are restricted to distribution shifts that are absolutely continuous with respect to the training distribution. Differently, \cite{Gendler_2022} proposed robust score functions based on randomized smoothing~\cite{Cohen_2019, Kumar_2020}, which ensure valid predictions under adversarial perturbations within $\ell_2$-norm balls. While adversarial methods tend to produce overly conservative uncertainty sets, recent works~\cite{Yan_2024, Ghosh_2023, Clarkson_2024} have refined prediction sets by considering specific perturbation structures. Other extensions have incorporated poisoning attacks and non-continuous data types such as graphs~\cite{Zargarbashi_2024}. However, these methods often assume very specific types of distribution shifts or require solving complex optimization problems. In contrast, our method employs a unified discrepancy measure that captures both local and global perturbations, imposes no assumptions on the score distribution, and provides a computationally efficient way to construct prediction sets.

\medskip

\noindent\emph{Sequential Learning.} While most methods assume i.i.d.\ or exchangeable training data, several works have explored sequential conformal prediction. These methods include updating nonconformity scores~\cite{Xu_2021}, leveraging correlation structures~\cite{chernozhukov_2018}, reweighting samples~\cite{Xu_2023, Barber_2023}, and monitoring rolling coverage~\cite{Gibbs_2021, Gibbs_2022, Zaffran_2022, Bastani_2022}. Although our method does not address sequential settings, extending it to this context is a promising avenue for future research.





%--------------------------------------------------------------------------
%--------------------------------------------------------------------------