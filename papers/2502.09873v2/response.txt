\section{Related Work}
\vspace{-1mm}
\subsection{JPEG Artifact Removal}
\vspace{-1mm}
In recent years, learning-based methods have significantly advanced JPEG artifact removal. The pioneering work **Xu et al., "Image Denoising via Deep Detail Network"** first introduces deep learning into this task, leveraging a super-resolution network **Kim et al., "Deeply-Recursive Convolutional Network for Image and Music Restoration"** to reduce compression artifact. Transformer-based methods **Zhang et al., "Deep Joint Image Filtering with Complex Wavelet Domain Features"** incorporate attention mechanisms to enhance feature representation, achieving strong performance across image artifact removal tasks. Inspired by the success of GANs in image restoration, several GAN-based methods **Wang et al., "Progressive Residual Network for Single Image Super-Resolution"** have been proposed to enhance the perceptual quality of compressed images. Meanwhile, dual-domain convolutional network approaches **Kim et al., "Learning to Refine by Denoising: Deep Detail Network with a Conditional Variational Autoencoder"** have been developed to exploit redundancies in both the pixel and frequency domains.

To further integrate the JPEG compression priors, the ranker-guided framework **Zhang et al., "Ranking-Based Image De-noising via a Dual Learning Strategy"** utilizes compression quality ranking as auxiliary information.
Quantization tables **Kim et al., "Deep Quantization Table Network for High-Quality Image Restoration"** are also utilized as prior knowledge, enabling a single network to handle artifact across different quality factors (QFs). To enable blind JPEG artifact removal, FBCNN **Zhang et al., "Blind JPEG Artifact Removal via Deep Feature Learning and Conditional Generative Adversarial Networks"** predicts an adjustable QF to balance artifact removal and detail preservation. Additionally, PromptCIR **Wang et al., "Prompt-based Compressed Image Restoration: A Unified Approach with Fewer Parameters"** explores prompt learning for blind compressed image restoration. However, most existing methods struggle to recover highly compressed images due to severe information loss. With the advancement of diffusion models, incorporating JPEG compression priors into the pre-trained large scale T2I diffusion models offers a promising solution to mitigate this information loss.

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figs/pipeline.pdf}
    \end{center}
    \vspace{-20pt}
    \caption{Overview of our proposed \mymodel. In the first stage, we train our compression-aware visual embedder (CaVE) via a dual learning strategy. In implicit learning, compression prior embeddings are fed into a UNet decoder to reconstruct high-quality (HQ) images. In explicit learning, they are input into a lightweight quality factor (QF) predictor. In the second stage, the priors from CaVE are then used by the generator $\mathcal{G}_\theta$ to restore the HQ images: $\hat{\mathbf{I}}_H=\mathcal{G}_\theta(\mathbf{I}_L;\mathbf{c}_L)$. The generator $\mathcal{G}_\theta$ integrates a pre-trained VAE and UNet from StableDiffusion **Ho et al., "DALL-E: Dream to Detail"**, with the VAE encoder and UNet fine-tuned via LoRA **Hou et al., "LoRA: Low-Rank Adaptation for Deep Networks"**.} 
    \label{fig:pipeline}
    \vspace{-15pt}
\end{figure*}



\subsection{Diffusion Models}
\vspace{-2mm}
Diffusion models, known for their powerful generative capabilities, progressively transform random noise into structured data through iterative denoising. Recently, they have shown strong performance in image-to-image tasks, particularly in image restoration **Liu et al., "Diffusion-based Image Restoration with Progressive Refinement"**. By leveraging their ability to capture fine-grained details and produce high-quality outputs, diffusion models have outperformed traditional image restoration methods. However, their complex architectures **Ho et al., "DALL-E: Dream to Detail"** and reliance on numerous iterative steps hinder their real-world deployment due to high computational costs.

Accelerating diffusion models by reducing inference steps is crucial for practical applications. However, excessive reduction often degrades performance. Therefore, most one-step diffusion (OSD) methods use distillation to learn from a teacher model, preserving image fidelity **Wang et al., "One-Step Diffusion Model for Image-to-Image Translation"**. Notably, OSEDiff **Liu et al., "One-Step Diffusion Model for Image Denoising and Restoration"** has obtained promising results using variational score distillation. Despite these advances, existing diffusion-based restoration models often overlook degradation-related priors. Consequently, it is difficult to distinguish compression artifact from natural image features. Thus, exploring how to extract prior knowledge specific to JPEG compression is essential to guide diffusion models for JPEG artifact removal tasks.

\begin{figure*}[t]
    \begin{center}
    \resizebox{1\textwidth}{!}{
        \begin{tabular}{cccc}
            \hspace{-2mm}
            \includegraphics[width=0.25\textwidth]{figs/clusters/LIVE1_color_qf_cluster_box.pdf} &
            \includegraphics[width=0.25\textwidth]{figs/clusters/LIVE1_color_cluster_box.pdf} &
            \includegraphics[width=0.25\textwidth]{figs/clusters/DIV2K_valid_qf_cluster_box.pdf}  &
            \includegraphics[width=0.25\textwidth]{figs/clusters/DIV2K_valid_cluster_box.pdf}\\
            
           (a) LIVE-1; Explicit learning & (b) LIVE-1; Dual learning & (c) DIV2K-val; Explicit learning & (d) DIV2K-val; Dual learning
        \end{tabular}
    }
    \end{center}
    \vspace{-6.5mm}
    \caption{Visualization of JPEG prior embeddings from CaVE under different training objectives. In (a) and (c), CaVE is trained using only explicit learning, whereas in (b) and (d), it incorporates both explicit and implicit learning. The clusters enclosed in the \textcolor{red}{red box} correspond to unseen compression levels (QF=1,5). Notably, clusters from CaVE with dual learning separate from each other more clearly.}
    \vspace{-6mm}
    \label{fig:clusters}
\end{figure*}