\section{Related Work}
\vspace{-1mm}
\subsection{JPEG Artifact Removal}
\vspace{-1mm}
In recent years, learning-based methods have significantly advanced JPEG artifact removal. The pioneering work ARCNN____ first introduces deep learning into this task, leveraging a super-resolution network____ to reduce compression artifact. Transformer-based methods ____ incorporate attention mechanisms to enhance feature representation, achieving strong performance across image artifact removal tasks. Inspired by the success of GANs in image restoration, several GAN-based methods____ have been proposed to enhance the perceptual quality of compressed images. Meanwhile, dual-domain convolutional network approaches____ have been developed to exploit redundancies in both the pixel and frequency domains. 

To further integrate the JPEG compression priors, the ranker-guided framework____ utilizes compression quality ranking as auxiliary information.
Quantization tables____ are also utilized as prior knowledge, enabling a single network to handle artifact across different quality factors (QFs). To enable blind JPEG artifact removal, FBCNN____ predicts an adjustable QF to balance artifact removal and detail preservation. Additionally, PromptCIR____ explores prompt learning for blind compressed image restoration. However, most existing methods struggle to recover highly compressed images due to severe information loss. With the advancement of diffusion models, incorporating JPEG compression priors into the pre-trained large scale T2I diffusion models offers a promising solution to mitigate this information loss. 

\begin{figure*}[t]
    \begin{center}
        \includegraphics[width=\textwidth]{figs/pipeline.pdf}
    \end{center}
    \vspace{-20pt}
    \caption{Overview of our proposed \mymodel. In the first stage, we train our compression-aware visual embedder (CaVE) via a dual learning strategy. In implicit learning, compression prior embeddings are fed into a UNet decoder to reconstruct high-quality (HQ) images. In explicit learning, they are input into a lightweight quality factor (QF) predictor. In the second stage, the priors from CaVE are then used by the generator $\mathcal{G}_\theta$ to restore the HQ images: $\hat{\mathbf{I}}_H=\mathcal{G}_\theta(\mathbf{I}_L;\mathbf{c}_L)$. The generator $\mathcal{G}_\theta$ integrates a pre-trained VAE and UNet from StableDiffusion____, with the VAE encoder and UNet fine-tuned via LoRA____.} 
    \label{fig:pipeline}
    \vspace{-15pt}
\end{figure*}



\subsection{Diffusion Models}
\vspace{-2mm}
Diffusion models, known for their powerful generative capabilities, progressively transform random noise into structured data through iterative denoising. Recently, they have shown strong performance in image-to-image tasks, particularly in image restoration____. By leveraging their ability to capture fine-grained details and produce high-quality outputs, diffusion models have outperformed traditional image restoration methods. However, their complex architectures____ and reliance on numerous iterative steps hinder their real-world deployment due to high computational costs.

Accelerating diffusion models by reducing inference steps is crucial for practical applications. However, excessive reduction often degrades performance. Therefore, most one-step diffusion (OSD) methods use distillation to learn from a teacher model, preserving image fidelity____. Notably, OSEDiff____ has obtained promising results using variational score distillation. Despite these advances, existing diffusion-based restoration models often overlook degradation-related priors. Consequently, it is difficult to distinguish compression artifact from natural image features. Thus, exploring how to extract prior knowledge specific to JPEG compression is essential to guide diffusion models for JPEG artifact removal tasks.

\begin{figure*}[t]
    \begin{center}
    \resizebox{1\textwidth}{!}{
        \begin{tabular}{cccc}
            \hspace{-2mm}
            \includegraphics[width=0.25\textwidth]{figs/clusters/LIVE1_color_qf_cluster_box.pdf} &
            \includegraphics[width=0.25\textwidth]{figs/clusters/LIVE1_color_cluster_box.pdf} &
            \includegraphics[width=0.25\textwidth]{figs/clusters/DIV2K_valid_qf_cluster_box.pdf}  &
            \includegraphics[width=0.25\textwidth]{figs/clusters/DIV2K_valid_cluster_box.pdf}\\
            
           (a) LIVE-1; Explicit learning & (b) LIVE-1; Dual learning & (c) DIV2K-val; Explicit learning & (d) DIV2K-val; Dual learning
        \end{tabular}
    }
    \end{center}
    \vspace{-6.5mm}
    \caption{Visualization of JPEG prior embeddings from CaVE under different training objectives. In (a) and (c), CaVE is trained using only explicit learning, whereas in (b) and (d), it incorporates both explicit and implicit learning. The clusters enclosed in the \textcolor{red}{red box} correspond to unseen compression levels (QF=1,5). Notably, clusters from CaVE with dual learning separate from each other more clearly.}
    \vspace{-6mm}
    \label{fig:clusters}
\end{figure*}


% \vspace{-4mm}