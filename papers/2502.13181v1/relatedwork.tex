\section{Related Work}
\label{2.2.pe transformer}
To address the challenge of requiring extensive hardware resources for large Transformer models, researchers have explored various methods to enhance efficiency.

One approach is related to pruning of Transformer model layers, which involves removing less important layers or weights to streamline the model. It was found that many deep layers in large language models are redundant \cite{gromov2024unreasonable}, and by pruning up to half of these layers, it was possible to significantly reduce the model size with minimal accuracy degradation. 

Another strategy is sharing parameters across different layers or components in Transformers, reducing the model's complexity and memory usage. The Universal Transformers \cite{dehghani2019universal} introduces a model where parameters are shared across layers using a recurrent mechanism with layer-dependent positional encoding, which maintains good performance in various NLP tasks while reducing the number of parameters. People have also proposed sequence and cycle strategies for sharing parameters across layers~\cite{takase2021lessons}, improving efficiency and performance in tasks like machine translation and speech recognition. Similarly, Subformer \cite{reid2021subformer} and One Wide Feedforward \cite{pires2023wide} investigate partial weight sharing within layers, showing that significant parameter reductions can be achieved with little accuracy sacrifice. These models demonstrate that shared parameters can lead to efficient and effective Transformer architectures.

To investigate recurrence-based models, we performed a layer representation similarity analysis using the common CKA (centered kernel alignment) \cite{kornblith2019similarity} method and mean attention distance (MAD) \cite{dosovitskiy2020image} analysis, and we found that the layer representations and internal attention behavior of the previously proposed fully recurrence-based Transformer model \cite{dehghani2019universal} are considerably different compared to those of the original Transformer model.

We hypothesized that the difference in model behavior, especially in attention module, might be the main cause for the gap in performance, and if we can simulate the behavior of the original model using a recurrent model with adaptive level signals, we can also maintain higher performance. Our proposed methodology is focused on addressing this difference, narrowing the gap of the model behavior, and in turn the model performance.