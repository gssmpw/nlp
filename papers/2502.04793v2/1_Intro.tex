%\vspace{-3ex}
%\vspace{-2ex}
\section{Introduction \& Motivation}
Contemporary technology companies have embraced continuous experimentation practices to guide their research and product development cycles.
Designing, running, monitoring and subsequently interpreting A/B-tests has become part of the daily routine for many industry practitioners~\cite{kohavi2020trustworthy, Larsen2024}.
The A/B-testing framework is generally well understood, due to a long history of scientific literature on randomised controlled experiments~\cite{Fisher1921,Rubin1974}.
Other recent works highlight pitfalls that often occur in modern-day settings~\cite{Kohavi2022, Dmitriev2017,Jeunen2023_Forum, Kohavi2024}, or focus on ensuring that the metrics under consideration have sufficient statistical power~\cite{Baweja2024,Jeunen2024_Learning,Jeunen2024_RecSysIndustry,Tripuraneni2024, Deng2024, Guo2021}.

When all data has been collected in a properly randomised manner, the next step is typically to estimate Confidence Intervals (CIs) for Average Treatment Effects (ATEs) across a range of user behaviour metrics, and to perform statistical hypothesis tests to ensure observed effects are not simply the result of noise~\cite{Greenland2016}.
The null hypothesis is often a ``\emph{zero effect}'' hypothesis, and hence the procedure involves computing sample means and standard deviations to construct normal CIs and checking whether they contain zero.
More generally, one can construct a test statistic from the CI to inform a $p$-value.
The reason that normal CIs are an appropriate choice is that we are estimating the \emph{average} treatment effect.
According to the CLT, we are guaranteed that as the sample size increases, the sampling distribution of the ATE will converge to normality~\cite{Durret2019, Fischer2010}.
Whilst asymptotic convergence to a normal distribution is clearly a desirable property, practitioners are left with little guidance as to when the sample size is considered ``\emph{sufficiently large}''.
To complicate things further, the underlying distribution of the event under consideration non-trivially impacts the distribution of its mean, and hence, the validity of the outlined approach.

In this work, we describe a practical approach to empirically validate whether we can trust the CIs and resulting $p$-values.
When the null hypothesis holds true by design, we expect a CI with confidence level $100\cdot(1-\alpha)\%$ to \emph{exclude} zero in $\alpha\%$ of cases.
Furthermore, the distribution of $p$-values should be expected to be uniform.
We can emulate such situations by repeatedly resampling synthetic A/A-tests from the available data, leveraging the Kolmogorov-Smirnov test to ascertain whether the resulting $p$-value distribution is likely to be uniform~\cite[p.~188]{kohavi2020trustworthy}.
This provides an efficient and effective approach to empirically validate the assumptions that permeate common practice, allowing us to pinpoint problematic experimental designs where the false positive rate deviates from $\alpha$~\cite{Kohavi2024}.
This, in turn, enforces statistical rigour and increases the trust we can place in A/B-testing estimands that do pass the test.

We note that the statistical hypothesis testing framework and the use of $p$-values itself is a topic of ongoing debate, with several works providing guidance on alternative communication about estimated effects and their associated uncertainty~\cite{Wasserstein2019,McShane2019}.
Other works suggest the use of Bayesian alternatives to handle uncertainty estimation of A/B-testing results~\cite{Deng2015,Gronau2021}.
Either line of work suggests to put more emphasis on confidence (or credible) intervals rather than a $p$-value or binary ``\emph{significant}'' label.
The validity of both frequentist and Bayesian intervals---i.e. whether they fulfil the meaning that is ascribed to them---is a primary concern of our work, and covered by our analysis and methodology.

The following sections formalise the problem statement and our proposed approach, providing empirical results and insights from its implementation in a real-world system.