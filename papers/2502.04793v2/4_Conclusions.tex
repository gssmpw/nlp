\section{Conclusions \& Outlook}
A/B-tests are omnipresent in modern technology companies, often seen as the ``gold standard'' of experimentation practices.
The default estimand is typically the ATE on a metric of interest, and statistical uncertainty on the estimate is handled through standard methods. 
It is often forgotten that these methods rely on implicit assumptions that might be violated, invalidating the estimates.

Most commonly, we assume that the distribution of the ATE has converged to normality due to the CLT. 
If this is false, the CIs and $p$-values we use to assess A/B-testing outcomes become misleading.
In this work, we propose an efficient and effective manner to validate this assumption empirically, through the use of repeatedly resampled A/A-tests and a Kolmogorov-Smirnov test on the uniformity of the resulting $p$-value distribution.
We describe our approach, present empirical results on real-world data that both elucidate the methods and highlight that potential proxies (like sample size or skewness) are promising but imperfect.
This provides a practical framework to assess A/B-testing estimands and avoid situations where improper estimation methods are applied, in the hope that it will help the community to enforce statistical rigour and avoid inflated false positive risk going forward.