% ****** Start of file aipsamp.tex ******
%
%   This file is part of the AIP files in the AIP distribution for REVTeX 4.
%   Version 4.1 of REVTeX, October 2009
%
%   Copyright (c) 2009 American Institute of Physics.
%
%   See the AIP README file for restrictions and more information.
%
% TeX'ing this file requires that you have AMS-LaTeX 2.0 installed
% as well as the rest of the prerequisites for REVTeX 4.1
% 
% It also requires running BibTeX. The commands are as follows:
%
%  1)  latex  aipsamp
%  2)  bibtex aipsamp
%  3)  latex  aipsamp
%  4)  latex  aipsamp
%
% Use this file as a source of example code for your aip document.
% Use the file aiptemplate.tex as a template for your document.
\documentclass[%
 aip,
% jmp,
% bmf,
% sd,
% rsi,
 amsmath,amssymb,
%preprint,%
 reprint,%
%author-year,%
%author-numerical,%
% Conference Proceedings
]{revtex4-1}

\usepackage{graphicx}% Include figure files
\usepackage{dcolumn}% Align table columns on decimal point
\usepackage{bm}% bold math
%\usepackage[mathlines]{lineno}% Enable numbering of text and display math
%\linenumbers\relax % Commence numbering lines

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage{etoolbox}
\usepackage{color}
%% Apr 2021: AIP requests that the corresponding 
%% email to be moved after the affiliations
\makeatletter
\def\@email#1#2{%
 \endgroup
 \patchcmd{\titleblock@produce}
  {\frontmatter@RRAPformat}
  {\frontmatter@RRAPformat{\produce@RRAP{*#1\href{mailto:#2}{#2}}}\frontmatter@RRAPformat}
  {}{}
}%
\makeatother
\begin{document}

\preprint{AIP/123-QED}

\title[Sample title]{Revealing Bias Formation in Deep Neural Networks Through the Geometric Mechanisms of Human Visual Decoupling}
% Force line breaks with \\
\author{Yanbiao Ma}
\email{ybmamail@stu.xidian.edu.cn}
 %\altaffiliation[Also at ]{School of Artificial Intelligence, Xidian University, Xi’an, 710071, China}%Lines break automatically or can be forced with \\
\author{Bowei Liu}%
\affiliation{ 
School of Artificial Intelligence, Xidian University, Xi’an, 710071, China%\\This line break forced with \textbackslash\textbackslash
}%

\author{Boyuan Gao}%
\affiliation{ 
Xidian University, Xi’an, 710071, China%\\This line break forced with \textbackslash\textbackslash
}%


\author{Wei Dai}%
\affiliation{ 
School of Artificial Intelligence, Xidian University, Xi’an, 710071, China%\\This line break forced with \textbackslash\textbackslash
}%

\author{Jiayi Chen}%
\affiliation{ 
School of Artificial Intelligence, Xidian University, Xi’an, 710071, China%\\This line break forced with \textbackslash\textbackslash
}%

\author{Shuo Li}%
\affiliation{ 
School of Artificial Intelligence, Xidian University, Xi’an, 710071, China%\\This line break forced with \textbackslash\textbackslash
}%

\date{\today}% It is always \today, today,
             %  but any date may be explicitly specified

\begin{abstract}
Deep neural networks (DNNs) often exhibit biases toward certain categories during object recognition, even under balanced training data conditions. The intrinsic mechanisms underlying these biases remain unclear. Inspired by the human visual system, which decouples object manifolds through hierarchical processing to achieve object recognition, we propose a geometric analysis framework linking the geometric complexity of class-specific perceptual manifolds in DNNs to model bias. Our findings reveal that differences in geometric complexity can lead to varying recognition capabilities across categories, introducing biases. To support this analysis, we present the Perceptual-Manifold-Geometry library, designed for calculating the geometric properties of perceptual manifolds.
\end{abstract}

\maketitle

\begin{quotation}
Bias formation in deep neural networks (DNNs) remains a critical yet poorly understood challenge, influencing both fairness and reliability in artificial intelligence systems. While previous studies have primarily focused on statistical imbalances in training data, recent advances suggest that the intrinsic geometry of feature representations may play a fundamental role in shaping classification biases. Motivated by findings in cognitive science and geometric deep learning, this study introduces a theoretical framework to analyze how perceptual manifold geometry affects class-wise recognition performance. By quantifying manifold complexity through curvature, dimensionality, and topological measures, this work reveals a strong correlation between geometric structure and classification bias. These findings contribute to a deeper theoretical foundation for understanding chaos and complexity in modern learning systems, offering new perspectives at the intersection of machine learning, neuroscience, and nonlinear dynamics.
\end{quotation}

\section{Introduction}



Deep neural networks (DNNs), with their powerful learning and generalization capabilities, have been widely applied in visual tasks such as image classification and object detection \cite{paper1,paper2}. However, the biases exhibited by DNNs toward different categories significantly limit their fairness and reliability in real-world applications \cite{paper5,paper6}. Traditional theories mainly attribute these biases to the long-tailed distribution of training samples \cite{paper10,paper11}. Nonetheless, research and practical observations suggest that even with balanced datasets, DNNs still show substantially better recognition performance for certain categories over others \cite{paper15,paper17}. This indicates that the mechanisms underlying such biases are more complex and remain poorly understood.

The human visual system provides critical insights into understanding the origins of biases in DNNs. When neurons in the visual cortex are stimulated by different physical attributes of objects belonging to the same category, they form Object manifolds \cite{paper20,paper21}. As shown in Figure.\ref{fig1}a, the human visual cortex gradually disentangles and reduces the dimensionality of complex Object manifolds layer by layer, making them easier to distinguish in deeper cortical areas, thereby achieving object recognition \cite{paper21,paper25}. This process suggests that the geometric complexity of Object manifolds influences the difficulty of disentanglement and recognition performance \cite{paper21,paper26,chaos1}.

Considering that the architecture of DNNs mimics this multi-layer disentanglement mechanism \cite{paper34,chaos2}, and recent studies \cite{paper19,paper29,paper31} demonstrate that the responses of DNNs to images exhibit manifold-like properties similar to those of the human visual system, we refer to the point-cloud manifolds formed by the embeddings of data in the feature space of a trained DNN's representation network as class-specific perceptual manifolds.
Formally, given a dataset \( X = [x_1, \dots, x_m] \) belonging to a specific class and a trained deep neural network \( \text{Model} = \{f(x, \theta_1), g(z, \theta_2)\} \), where \( f(x, \theta_1) \) represents the representation network and \( g(z, \theta_2) \) denotes the classifier, we extract the \( p \)-dimensional embeddings \( Z = [z_1, \dots, z_m] \in \mathbb{R}^{p \times m} \) using the representation network, with each \( z_i = f(x_i, \theta_1) \in \mathbb{R}^p \). The point-cloud manifold formed by \( Z \) is referred to as the \emph{class-specific perceptual manifold} within the DNN.  
If the recognition process of DNNs can be viewed as the representation network disentangling, reducing the dimensionality of, and separating class-specific perceptual manifolds, followed by the classifier making decisions (as illustrated in Figure.\ref{fig1}b), we hypothesize that: 
(1) The higher the geometric complexity of a class-specific perceptual manifold produced by the representation network, the more challenging it becomes for the classifier to decode and recognize the corresponding class. 
(2) Differences in geometric complexity across classes lead to inconsistent recognition capabilities, thus introducing biases. 

\section*{Results}
To validate this hypothesis, we conducted experiments on widely used image datasets with balanced class samples, including CIFAR-10, CIFAR-100 \cite{cifar10}, Mini-ImageNet \cite{imagenet}, and Caltech-101 \cite{caltech}, to eliminate the influence of sample quantity. We employed deep neural networks with convolutional architectures, such as the ResNet \cite{resnet} series, and Transformer-based architectures, including ViT-B/16 and ViT-B/32 \cite{vit}. All models were trained following standard configurations (see Table 1) \cite{resnet,vit}. Each model was trained $10$ times using different random seeds, and the experimental results are reported as the average performance across these independent runs.
To systematically and efficiently quantify the geometric complexity of class-specific perceptual manifolds in DNNs, we developed a Python toolkit named \emph{Perceptual-Manifold-Geometry}. This toolkit offers comprehensive functionality for geometric analysis, including intrinsic dimensionality estimation, curvature analysis, and topological characteristics (the number of holes) quantification. Figure.\ref{fig1}c provides an example of the toolkit’s usage, see the Methods section for the relevant theory. Detailed documentation, tutorials, example datasets, and contribution guidelines are available online at \url{https://pypi.org/project/perceptual-manifold-geometry/}.

\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{fig1.pdf}
\caption{\textbf{a}, When stimulated by objects of the same category, the human visual system forms object manifolds. As the visual cortex progresses through its layers, the object manifolds of different objects become more distinguishable and flatter.
\textbf{b}, Deep neural networks use representation networks to disentangle and reduce the dimensionality of data manifolds into class-specific perceptual manifolds, which are then used for object recognition by the classifier.
\textbf{c}, Example calculations of Gaussian curvature, the number of topological holes, and intrinsic dimensionality for perceptual manifolds using the \emph{Perceptual-Manifold-Geometry} toolkit.}
\label{fig1}
\end{figure}


\begin{table}
\caption{\label{tab:table1}Summary of the experiments and results. LR denotes the learning rate and Mn denotes the momentum of the optimizer. If not specifically identified, the details of the experimental hyperparameters are shared in the Settings column. \textcolor[RGB]{0,201,87}{Green} and \textcolor{red}{red text} denote the hyperparameters of the convolutional neural network and vision transformer, respectively.}
\begin{ruledtabular}
\begin{tabular}{lll}
 \textbf{\normalsize Dataset} & \textbf{\normalsize DNN} & \textbf{\normalsize Settings} \\

CIFAR-10 & ViT-B/32, ViT-B/16  &  epoch: \textcolor[RGB]{0,201,87}{200}, \textcolor{red}{300} \\
CIFAR-100 & VGG-19  & Optimizer: SGD  \\
Mini-ImageNet  & SeNet-50   & Mm: 0.9 \\
Caltech-101  & ResNet-18, ResNet-34, ResNet-50  & LR: \textcolor[RGB]{0,201,87}{0.05}, \textcolor{red}{0.001} \\
\end{tabular}
\end{ruledtabular}
\end{table}


\begin{figure}[!t]
\centering
\includegraphics[width=1\linewidth]{fig2_1.pdf}
\caption{\textbf{a}, Using the representation network of a DNN to extract image embeddings for each class and calculating the geometric complexity of class-specific perceptual manifolds based on these embeddings.
\textbf{b}, Bar charts showing Pearson correlation coefficients (PCCs) between class accuracy and various geometric complexities of class-specific perceptual manifolds across different datasets.}
\label{fig2}
\end{figure}

We first measured the recognition accuracy of fully trained DNNs on each class. As shown in Figure.\ref{fig2}a, we then extracted the embeddings of images for each class from the representation network of the DNN and stored them separately. Subsequently, we estimated the geometric complexity of the perceptual manifolds corresponding to each class’s embeddings, including intrinsic dimensionality, Gaussian curvature, and the number of topological holes. We calculated the Pearson correlation coefficients between geometric complexity and class-wise recognition accuracy.
As illustrated in Figure.\ref{fig2}b, the experimental results on different datasets and DNN architectures reveal a significant negative correlation between the geometric complexity of class-specific perceptual manifolds and recognition accuracy. These findings confirm our hypothesis: differences in geometric complexity among class-specific perceptual manifolds contribute to inconsistent recognition performance across classes, thereby inducing bias. Furthermore, the higher the geometric complexity of a perceptual manifold, the more challenging it is for the model to recognize the corresponding class.

This finding can also be understood from the perspective of model optimization. Intrinsic dimensionality reflects the complexity of a manifold's embedding. Higher dimensionality indicates more intricate manifold structures in high-dimensional space, requiring a classifier with greater capacity to effectively distinguish these samples. High Gaussian curvature typically signifies that the data distribution in the embedding space is more twisted and complex, leading to unstable decision boundaries and increasing classification difficulty. A higher number of topological holes implies more complex decision boundaries, making classifiers more prone to overfitting and resulting in poorer generalization performance.  
Our findings also offer new insights into mitigating model bias. By constraining optimization objectives, models can be encouraged to learn class-specific perceptual manifolds with lower and more balanced geometric complexity.  

Inspired by human visual behavior, this study establishes a universal geometric analysis framework to explain the pervasive bias in DNNs—an achievement that was previously difficult to realize through algorithmic research alone. Experimental results demonstrate that DNNs not only structurally resemble the human visual system but also exhibit similar characteristics in their internal data compression and processing mechanisms. This discovery reinforces the confidence of researchers in brain-inspired artificial intelligence.  
As the \emph{Perceptual-Manifold-Geometry} toolkit gains broader adoption, it is poised to provide valuable tools for artificial intelligence and neuroscience research. This work exemplifies the successful integration of neuroscience and computer science, highlighting the immense potential of interdisciplinary collaboration.  


\section*{Methods}
To facilitate the study of perceptual manifolds in DNNs, we developed the Perceptual-Manifold-Geometry toolkit, which provides a comprehensive framework for analyzing the geometric characteristics of class-specific perceptual manifolds. The toolkit includes modules for intrinsic dimensionality estimation, Gaussian curvature calculation, and topological quantification, focusing particularly on the number of topological holes. These features enable systematic analysis of the relationships between manifold geometry and recognition performance.

\subsection*{Estimation of the Intrinsic Dimension}

Given a set of embeddings $Z=[z_1,\dots,z_n]\in \mathbb{R}^{p \times n}$ corresponding to an image dataset, $Z$ is typically distributed near a low-dimensional perceptual manifold $M$ embedded in the $p$-dimensional space, akin to a two-dimensional plane in three-dimensional space. The intrinsic dimension $ID(M)$ of the perceptual manifold is such that $d<p$. A higher intrinsic dimension indicates a more complex perceptual manifold. The following describes how to use TLE to estimate the intrinsic dimension of the perceptual manifold formed by $Z=[z_1,\dots,z_n]\in \mathbb{R}^{p \times n}$.

The primary method for estimating intrinsic dimension involves analyzing the distribution of distances between each point in the dataset and its neighboring points, and then estimating the dimensionality of the local space based on the rate of growth of distances or other statistic. Assuming that the distribution of samples is uniform within a small neighborhood, and then uses a Poisson process to simulate the number of points discovered by random sampling within neighborhoods of a given radius around each sample \cite{ID_MLE}. Subsequently, by constructing a likelihood function, the rate of growth in quantity is associated with the surface area of a sphere. Given any embedding $z_i$ in the dataset and its set of $k$ nearest neighbors $V$, the Maximum Likelihood Estimator (MLE) of the local intrinsic dimensiona at $z_i$ is given by: $$ID_{MLE}(z_i)=-\Bigg(\frac{1}{k}\sum_{j=1}^{k}ln\frac{r_j(z_i)}{r_k(z_i)}\Bigg)^{-1},$$ where $r_j(z_i)$ represents the distance between $z_i$ and its $j$-th nearest neighbor. TLE \cite{ID_TLE} no longer assumes uniformity of sample distribution in local neighborhoods, thus closer to the true data distribution. It assumes the local intrinsic dimension to be continuous, thereby utilizing nearby sample points to stabilize estimates at $z_i$. Specifically, the estimate of the intrinsic dimension at $z_i$ using TLE is given by 
\begin{equation}
ID_{TLE}(z_i) = -\Bigg(\frac{1}{\left | V_* \right |^2}\sum_{\substack{v,w\in V_*\\ v\neq w}}\Bigg[\ln\frac{d_{z_i}(v,w)}{r_k(z_i)} 
+\ln\frac{d_{z_i}(2z_i-v,w)}{r_k(z_i)}\Bigg]\Bigg)^{-1},
\end{equation}
where $V_* = V \cup \{z_i\}$, and $d_{z_i}(v,w)$ is defined as $(r_k(z_i)(w-v) \cdot (w-v)) / (2(z_i-v) \cdot (w-v))$. Furthermore, the global intrinsic dimensionality of the perceptual manifold is estimated as the average of local intrinsic dimensionalities: $$ID_{TLE}(Z)=\frac{1}{n}\sum_{i=1}^{n}ID_{TLE}(z_i).$$

\subsection*{Estimation of the Gaussian curvature}
Given a point cloud perceptual manifold $M$, which consists of a $p$-dimensional point set $Z=[z_1,\dots,z_n]\in \mathbb{R}^{p \times n}$, our goal is to calculate the Gauss curvature at each point. First, the normal vector at each point on $M$ is estimated by the neighbor points. Denote by $z_i^j$ the $j$-th neighbor point of $z_i$ and $u_i$ the normal vector at $z_i$. We solve for the normal vector by minimizing the inner product of $z_i^j-c_i,j=1,\dots,k$ and $u_i$ \cite{asao2021curvature}, i.e., $$\min {\textstyle \sum_{j=1}^{k}}((z_i^j-c_i)^Tu_i)^2,$$ where $c_i=\frac{1}{k}{\textstyle \sum_{j=1}^{k}}z_i^j$ and $k$ is the number of neighbor points. Let $y_j=z_i^j-c_i$, then the optimization objective is converted to
$$\min{\textstyle \sum_{j=1}^{k}}(y_j^Tu_i)^2 =\min {\textstyle \sum_{j=1}^{k}}u_i^Ty_jy_j^Tu_i =\min(u_i^T( {\textstyle \sum_{j=1}^{k}}y_jy_j^T)u_i).$$
${\textstyle \sum_{j=1}^{k}}y_jy_j^T$ is the covariance matrix of $k$ neighbors of $z_i$. Therefore, let $Y=[y_1,\dots,y_k]\in \mathbb{R}^{p\times k}$ and ${\textstyle \sum_{j=1}^{k}}y_jy_j^T=YY^T$. The optimization objective is further equated to
$$\begin{cases} f(u_i)=u_i^TYY^Tu_i,YY^T\in \mathbb{R}^{p\times p},
 \\ min(f(u_i)),
 \\ s.t. u_i^Tu_i=1.
\end{cases} $$
Construct the Lagrangian function $L(u_i,\lambda)=f(u_i)-\lambda (u_i^Tu_i-1)$ for the above optimization objective, where $\lambda$ is a parameter. The first-order partial derivatives of $L(u_i,\lambda)$ with respect to $u_i$ and $\lambda$ are
$$\frac{\partial L(u_i,\lambda)}{\partial u_i} =\frac{\partial}{\partial u_i}f(u_i)-\lambda\frac{\partial}{\partial u_i}(u_i^Tu_i-1) 
 =2(YY^Tu_i-\lambda u_i), $$ 
$$\frac{\partial L(u_i,\lambda)}{\partial \lambda}  =u_i^Tu_i-1.$$
Let $\frac{\partial L(u_i,\lambda)}{\partial u_i}$ and $\frac{\partial L(u_i,\lambda)}{\partial \lambda}$ be $0$, we can get $YY^Tu_i=\lambda u_i,u_i^Tu_i=1$. It is obvious that solving for $u_i$ is equivalent to calculating the eigenvectors of the covariance matrix $YY^T$, but the eigenvectors are not unique. From $\left \langle YY^Tu_i,u_i \right \rangle =\left \langle \lambda u_i,u_i \right \rangle$ we can get $\lambda=\left \langle YY^Tu_i,u_i \right \rangle=u_i^TYY^Tu_i $, so the optimization problem is equated to $\mathop{\arg\min}_{u_i}(\lambda)$. Performing the eigenvalue decomposition on the matrix $YY^T$ yields $p$ eigenvalues $\lambda_1,\dots,\lambda_p$ and the corresponding $p$-dimensional eigenvectors $[\xi_1,\dots,\xi_p]\in \mathbb{R}^{p\times p}$, where $\lambda_1\ge \dots \ge \lambda_p\ge 0$, $\left \| \xi_i \right \| _2=1,i=1,\dots,p$, $\left \langle \xi_a,\xi_b \right \rangle =0(a\neq b)$. The eigenvector $\xi_{m+1}$ corresponding to the smallest non-zero eigenvalue of the matrix $YY^T$ is taken as the normal vector $u_i$ of $M$ at $z_i$.

Consider an $m$-dimensional affine space with center $z_i$, which is spanned by $\xi_1,\dots,\xi_m$. This affine space approximates the tangent space at $z_i$ on $M$. We estimate the curvature of $M$ at $z_i$ by fitting a quadratic hypersurface in the tangent space utilizing the neighbor points of $z_i$. The $k$ neighbors of $z_i$ are projected into the affine space $z_i+\left \langle \xi_1,\dots,\xi_m \right \rangle$ and denoted as 
$$o_j=[(z_i^j-z_i)\cdot \xi_1,\dots,(z_i^j-z_i)\cdot \xi_m]^T\in \mathbb{R}^m,j=1,\dots,k.$$
Denote by $o_j[m]$ the $m$-th component $(z_i^j-z_i)\cdot \xi_m$ of $o_j$. We use $z_i$ and $k$ neighbor points to fit a quadratic hypersurface $f(\theta)$ with parameter $\theta \in \mathbb{R}^{m\times m}$. The hypersurface equation is denoted as
$$f(o_j,\theta)=\frac{1}{2} {\textstyle \sum_{a,b}}\theta_{a,b}o_j\left [ a \right ] o_j\left [ b \right ] ,j\in \left \{ 1,\dots,k \right \}, $$
further, minimize the squared error
$$E(\theta)= {\textstyle \sum_{j=1}^{k}}( \frac{1}{2} {\textstyle \sum_{a,b}}\theta_{a,b}o_j\left [ a \right ] o_j\left [ b \right ] -(z_i^j-z_i)\cdot u_i)^2.$$
Let $\frac{\partial E(\theta)}{\partial \theta_{a,b}}=0,a,b\in \left \{ 1,\dots,m \right \}$ yield a nonlinear system of equations, but it needs to be solved iteratively. Here, we propose an ingenious method to fit the hypersurface and \textbf{give the analytic solution of the parameter $\theta$} directly. Expand the parameter $\theta$ of the hypersurface into the column vector
$$\theta=\left [ \theta_{1,1},\dots,\theta_{1,m},\theta_{2,1},\dots,\theta_{m,m} \right]^T\in \mathbb{R}^{m^2}.$$
Organize the $k$ neighbor points $\left \{ o_j \right \}_{j=1}^k $ of $z_i$ according to the following form:
$$ O(z_i)=\begin{bmatrix} o_1\left [ 1 \right ] o_1\left [ 1 \right ] 
  &  o_1\left [ 1 \right ] o_1\left [ 2 \right ]  & \cdots  &  o_1\left [ m \right ] o_1\left [ m \right ]  \\
  o_2\left [ 1 \right ] o_2\left [ 1 \right ]  & o_2\left [ 1 \right ] o_2\left [ 2 \right ] & \cdots &o_2\left [ m \right ] o_2\left [ m \right ] \\
 \vdots  & \vdots  & \ddots  & \vdots  \\
 o_k\left [ 1 \right ] o_k\left [ 1 \right] & o_k\left [ 1 \right ] o_k\left [ 2 \right] & \cdots  &o_k\left [ m \right ] o_k\left [ m \right]
\end{bmatrix}\in \mathbb{R}^{k\times m^2}.$$
The target value is
$$T=\left [ (z_i^1-z_i)\cdot u_i,(z_i^2-z_i)\cdot u_i,\dots,(z_i^k-z_i)\cdot u_i \right ]^T \in \mathbb{R}^{k}.$$
We minimize the squared error
$$E(\theta)=\frac{1}{2}tr\left [ \left ( O(z_i)\theta-T \right)^T(O(z_i)\theta-T) \right ],$$
and find the partial derivative of $E(\theta)$ for $\theta$:
$$\frac{\partial E(\theta)}{\partial \theta} =\frac{1}{2} \left ( \frac{\partial tr(\theta^TO(z_i)^TO(z_i)\theta)}{\partial \theta}-\frac{\partial tr(\theta^TO(z_i)^TT)}{\partial \theta}   \right )$$
$$=O(z_i)^TO(z_i)\theta-O(z_i)^TT.$$
Let $\frac{\partial E(\theta)}{\partial \theta}=0$, we can get
$$\theta = (O(z_i)^TO(z_i))^{-1}O(z_i)^TT.$$
Thus, the Gauss curvature of the perceptual manifold $M$ at $z_i$ can be calculated as
$$G(z_i)=det(\theta)=det((O(z_i)^TO(z_i))^{-1}O(z_i)^TT).$$


Up to this point, we provide an approximate solution of the Gauss curvature at any point on the point cloud perceptual manifold $M$. Recent research \cite{balestriero2021learning} shows that on a high-dimensional dataset, almost all samples lie on convex locations, and thus the complexity of the perceptual manifold is defined as the average $\frac{1}{n} {\textstyle \sum_{i=1}^{n}}G(z_i)$ of the Gauss curvatures at all points on $M$. Our approach does not require iterative optimization and can be quickly deployed in a deep neural network to calculate the Gauss curvature of the perceptual manifold. 



\subsection*{Estimation of the Number of Holes}

To analyze the geometric complexity of perceptual manifolds, this study employs Persistent Homology, a technique from Topological Data Analysis (TDA), to quantify the topological hole features in data manifolds. This method reveals topological properties, such as connectivity, loops, and void distributions in high-dimensional data embeddings, offering a geometric perspective for understanding the feature spaces learned by Deep Neural Networks (DNNs) \cite{carlsson2009topology}.

Specifically, given a dataset \( X \subset \mathbb{R}^d \), the Ripser algorithm is used to compute its Persistence Diagram, capturing the evolution of topological structures at different scales. The persistence diagram represents the topological complexity of the manifold by recording the birth and death times of each topological feature. In this study, we focus on 1-dimensional homology \( H_1 \), which corresponds to loop structures, where the total number of loops reflects the number of holes in the data manifold.

To enhance computational robustness, a persistence threshold \( \tau \) is introduced to filter out low-persistence features caused by noise. Let \( \text{Persistence} = \text{Death} - \text{Birth} \); only loops with \( \text{Persistence} > \tau \) are retained as significant features. The number of holes is then defined as the total count of these significant loops. Additionally, to characterize the spatial distribution of holes comprehensively, we compute their total persistence, average persistence, and persistence density (defined as the ratio of total persistence to the time span of features).
This method, implemented efficiently using tools such as Ripser \cite{bauer2021ripser}, extracts key topological attributes of the data without significantly increasing computational complexity. These topological metrics not only reveal the geometric complexity of perceptual manifolds but also provide a theoretical foundation for understanding their influence on classifier decision boundaries.




\section*{Conflict of Interest Statement}
The author (authors) has (have) no conflicts to disclose.

\section*{Acknowledgements}
This work was supported in part by the Postdoctoral Fellowship Program of China Postdoctoral Science Foundation (CPSF) (No.GZC20232033), in part by the China Postdoctoral Science Foundation (No.2023M742738), in part by the National Natural Science Foundation of China (No.62406231)

\section*{Author contributions}
Yanbiao Ma: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Writing – original
draft. Bowei Liu: Conceptualization, Data curation, Formal
analysis, Investigation, Methodology, Software. Wei Dai:
Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software. Jiayi Chen: Resources,
Writing – review \& editing. Shuo Li: Funding acquisition,
Validation. All authors have read and agreed to the published
version of the manuscript.

\section*{Data availability}
All deep neural networks used in this study have been appropriately cited in the main text. All datasets utilized in this research are open access and have been referenced in the paper. The web links to the datasets are as follows: CIFAR-10/100 (\url{https://www.cs.toronto.edu/~kriz/cifar.html}), Caltech-101 (\url{https://www.vision.caltech.edu/datasets/}). The Mini-ImageNet dataset is derived from ImageNet and can be generated using the MLclf package (\url{https://github.com/tiger2017/MLclf}). Additionally, the toolkit developed for calculating the geometric properties of perceptual manifolds has been officially released and is available at \url{https://github.com/mayanbiao1234/Geometric-metrics-for-perceptual-manifolds}.














\iffalse

This sample document demonstrates proper use of REV\TeX~4.1 (and
\LaTeXe) in manuscripts prepared for submission to AIP
journals. Further information can be found in the documentation included in the distribution or available at
\url{http://authors.aip.org} and in the documentation for 
REV\TeX~4.1 itself.

When commands are referred to in this example file, they are always
shown with their required arguments, using normal \TeX{} format. In
this format, \verb+#1+, \verb+#2+, etc. stand for required
author-supplied arguments to commands. For example, in
\verb+\section{#1}+ the \verb+#1+ stands for the title text of the
author's section heading, and in \verb+\title{#1}+ the \verb+#1+
stands for the title text of the paper.

Line breaks in section headings at all levels can be introduced using
\textbackslash\textbackslash. A blank input line tells \TeX\ that the
paragraph has ended. 

\subsection{\label{sec:level2}Second-level heading: Formatting}

This file may be formatted in both the \texttt{preprint} (the default) and
\texttt{reprint} styles; the latter format may be used to 
mimic final journal output. Either format may be used for submission
purposes; however, for peer review and production, AIP will format the
article using the \texttt{preprint} class option. Hence, it is
essential that authors check that their manuscripts format acceptably
under \texttt{preprint}. Manuscripts submitted to AIP that do not
format correctly under the \texttt{preprint} option may be delayed in
both the editorial and production processes.

The \texttt{widetext} environment will make the text the width of the
full page, as on page~\pageref{eq:wideeq}. (Note the use the
\verb+\pageref{#1}+ to get the page number right automatically.) The
width-changing commands only take effect in \texttt{twocolumn}
formatting. It has no effect if \texttt{preprint} formatting is chosen
instead.

\subsubsection{\label{sec:level3}Third-level heading: Citations and Footnotes}

Citations in text refer to entries in the Bibliography;
they use the commands \verb+\cite{#1}+ or \verb+\onlinecite{#1}+. 
Because REV\TeX\ uses the \verb+natbib+ package of Patrick Daly, 
its entire repertoire of commands are available in your document;
see the \verb+natbib+ documentation for further details.
The argument of \verb+\cite+ is a comma-separated list of \emph{keys};
a key may consist of letters and numerals. 

By default, citations are numerical; \cite{feyn54} author-year citations are an option. 
To give a textual citation, use \verb+\onlinecite{#1}+: (Refs.~\onlinecite{witten2001,epr,Bire82}). 
REV\TeX\ ``collapses'' lists of consecutive numerical citations when appropriate. 
REV\TeX\ provides the ability to properly punctuate textual citations in author-year style;
this facility works correctly with numerical citations only with \texttt{natbib}'s compress option turned off. 
To illustrate, we cite several together \cite{feyn54,witten2001,epr,Berman1983}, 
and once again (Refs.~\onlinecite{epr,feyn54,Bire82,Berman1983}). 
Note that, when numerical citations are used, the references were sorted into the same order they appear in the bibliography. 

A reference within the bibliography is specified with a \verb+\bibitem{#1}+ command,
where the argument is the citation key mentioned above. 
\verb+\bibitem{#1}+ commands may be crafted by hand or, preferably,
generated by using Bib\TeX. 
The AIP styles for REV\TeX~4 include Bib\TeX\ style files
\verb+aipnum.bst+ and \verb+aipauth.bst+, appropriate for
numbered and author-year bibliographies,
respectively. 
REV\TeX~4 will automatically choose the style appropriate for 
the document's selected class options: the default is numerical, and
you obtain the author-year style by specifying a class option of \verb+author-year+.

This sample file demonstrates a simple use of Bib\TeX\ 
via a \verb+\bibliography+ command referencing the \verb+aipsamp.bib+ file.
Running Bib\TeX\ (in this case \texttt{bibtex
aipsamp}) after the first pass of \LaTeX\ produces the file
\verb+aipsamp.bbl+ which contains the automatically formatted
\verb+\bibitem+ commands (including extra markup information via
\verb+\bibinfo+ commands). If not using Bib\TeX, the
\verb+thebibiliography+ environment should be used instead.

\paragraph{Fourth-level heading is run in.}%
Footnotes are produced using the \verb+\footnote{#1}+ command. 
Numerical style citations put footnotes into the 
bibliography\footnote{Automatically placing footnotes into the bibliography requires using BibTeX to compile the bibliography.}.
Author-year and numerical author-year citation styles (each for its own reason) cannot use this method. 
Note: due to the method used to place footnotes in the bibliography, \emph{you
must re-run BibTeX every time you change any of your document's
footnotes}. 

\section{Math and Equations}
Inline math may be typeset using the \verb+$+ delimiters. Bold math
symbols may be achieved using the \verb+bm+ package and the
\verb+\bm{#1}+ command it supplies. For instance, a bold $\alpha$ can
be typeset as \verb+$\bm{\alpha}$+ giving $\bm{\alpha}$. Fraktur and
Blackboard (or open face or double struck) characters should be
typeset using the \verb+\mathfrak{#1}+ and \verb+\mathbb{#1}+ commands
respectively. Both are supplied by the \texttt{amssymb} package. For
example, \verb+$\mathbb{R}$+ gives $\mathbb{R}$ and
\verb+$\mathfrak{G}$+ gives $\mathfrak{G}$

In \LaTeX\ there are many different ways to display equations, and a
few preferred ways are noted below. Displayed math will center by
default. Use the class option \verb+fleqn+ to flush equations left.

Below we have numbered single-line equations, the most common kind: 
\begin{eqnarray}
\chi_+(p)\alt{\bf [}2|{\bf p}|(|{\bf p}|+p_z){\bf ]}^{-1/2}
\left(
\begin{array}{c}
|{\bf p}|+p_z\\
px+ip_y
\end{array}\right)\;,
\\
\left\{%
 \openone234567890abc123\alpha\beta\gamma\delta1234556\alpha\beta
 \frac{1\sum^{a}_{b}}{A^2}%
\right\}%
\label{eq:one}.
\end{eqnarray}
Note the open one in Eq.~(\ref{eq:one}).

Not all numbered equations will fit within a narrow column this
way. The equation number will move down automatically if it cannot fit
on the same line with a one-line equation:
\begin{equation}
\left\{
 ab12345678abc123456abcdef\alpha\beta\gamma\delta1234556\alpha\beta
 \frac{1\sum^{a}_{b}}{A^2}%
\right\}.
\end{equation}

When the \verb+\label{#1}+ command is used [cf. input for
Eq.~(\ref{eq:one})], the equation can be referred to in text without
knowing the equation number that \TeX\ will assign to it. Just
use \verb+\ref{#1}+, where \verb+#1+ is the same name that used in
the \verb+\label{#1}+ command.

Unnumbered single-line equations can be typeset
using the \verb+\[+, \verb+\]+ format:
\[g^+g^+ \rightarrow g^+g^+g^+g^+ \dots ~,~~q^+q^+\rightarrow
q^+g^+g^+ \dots ~. \]

\subsection{Multiline equations}

Multiline equations are obtained by using the \verb+eqnarray+
environment.  Use the \verb+\nonumber+ command at the end of each line
to avoid assigning a number:
\begin{eqnarray}
{\cal M}=&&ig_Z^2(4E_1E_2)^{1/2}(l_i^2)^{-1}
\delta_{\sigma_1,-\sigma_2}
(g_{\sigma_2}^e)^2\chi_{-\sigma_2}(p_2)\nonumber\\
&&\times
[\epsilon_jl_i\epsilon_i]_{\sigma_1}\chi_{\sigma_1}(p_1),
\end{eqnarray}
\begin{eqnarray}
\sum \vert M^{\text{viol}}_g \vert ^2&=&g^{2n-4}_S(Q^2)~N^{n-2}
        (N^2-1)\nonumber \\
 & &\times \left( \sum_{i<j}\right)
  \sum_{\text{perm}}
 \frac{1}{S_{12}}
 \frac{1}{S_{12}}
 \sum_\tau c^f_\tau~.
\end{eqnarray}
\textbf{Note:} Do not use \verb+\label{#1}+ on a line of a multiline
equation if \verb+\nonumber+ is also used on that line. Incorrect
cross-referencing will result. Notice the use \verb+\text{#1}+ for
using a Roman font within a math environment.

To set a multiline equation without \emph{any} equation
numbers, use the \verb+\begin{eqnarray*}+,
\verb+\end{eqnarray*}+ format:
\begin{eqnarray*}
\sum \vert M^{\text{viol}}_g \vert ^2&=&g^{2n-4}_S(Q^2)~N^{n-2}
        (N^2-1)\\
 & &\times \left( \sum_{i<j}\right)
 \left(
  \sum_{\text{perm}}\frac{1}{S_{12}S_{23}S_{n1}}
 \right)
 \frac{1}{S_{12}}~.
\end{eqnarray*}
To obtain numbers not normally produced by the automatic numbering,
use the \verb+\tag{#1}+ command, where \verb+#1+ is the desired
equation number. For example, to get an equation number of
(\ref{eq:mynum}),
\begin{equation}
g^+g^+ \rightarrow g^+g^+g^+g^+ \dots ~,~~q^+q^+\rightarrow
q^+g^+g^+ \dots ~. \tag{2.6$'$}\label{eq:mynum}
\end{equation}

A few notes on \verb=\tag{#1}=. \verb+\tag{#1}+ requires
\texttt{amsmath}. The \verb+\tag{#1}+ must come before the
\verb+\label{#1}+, if any. The numbering set with \verb+\tag{#1}+ is
\textit{transparent} to the automatic numbering in REV\TeX{};
therefore, the number must be known ahead of time, and it must be
manually adjusted if other equations are added. \verb+\tag{#1}+ works
with both single-line and multiline equations. \verb+\tag{#1}+ should
only be used in exceptional case - do not use it to number all
equations in a paper.

Enclosing single-line and multiline equations in
\verb+\begin{subequations}+ and \verb+\end{subequations}+ will produce
a set of equations that are ``numbered'' with letters, as shown in
Eqs.~(\ref{subeq:1}) and (\ref{subeq:2}) below:
\begin{subequations}
\label{eq:whole}
\begin{equation}
\left\{
 abc123456abcdef\alpha\beta\gamma\delta1234556\alpha\beta
 \frac{1\sum^{a}_{b}}{A^2}
\right\},\label{subeq:1}
\end{equation}
\begin{eqnarray}
{\cal M}=&&ig_Z^2(4E_1E_2)^{1/2}(l_i^2)^{-1}
(g_{\sigma_2}^e)^2\chi_{-\sigma_2}(p_2)\nonumber\\
&&\times
[\epsilon_i]_{\sigma_1}\chi_{\sigma_1}(p_1).\label{subeq:2}
\end{eqnarray}
\end{subequations}
Putting a \verb+\label{#1}+ command right after the
\verb+\begin{subequations}+, allows one to
reference all the equations in a subequations environment. For
example, the equations in the preceding subequations environment were
Eqs.~(\ref{eq:whole}).

\subsubsection{Wide equations}
The equation that follows is set in a wide format, i.e., it spans
across the full page. The wide format is reserved for long equations
that cannot be easily broken into four lines or less:
\begin{widetext}
\begin{equation}
{\cal R}^{(\text{d})}=
 g_{\sigma_2}^e
 \left(
   \frac{[\Gamma^Z(3,21)]_{\sigma_1}}{Q_{12}^2-M_W^2}
  +\frac{[\Gamma^Z(13,2)]_{\sigma_1}}{Q_{13}^2-M_W^2}
 \right)
 + x_WQ_e
 \left(
   \frac{[\Gamma^\gamma(3,21)]_{\sigma_1}}{Q_{12}^2-M_W^2}
  +\frac{[\Gamma^\gamma(13,2)]_{\sigma_1}}{Q_{13}^2-M_W^2}
 \right)\;. \label{eq:wideeq}
\end{equation}
\end{widetext}
This is typed to show the output is in wide format.
(Since there is no input line between \verb+\equation+ and
this paragraph, there is no paragraph indent for this paragraph.)
\section{Cross-referencing}
REV\TeX{} will automatically number sections, equations, figure
captions, and tables. In order to reference them in text, use the
\verb+\label{#1}+ and \verb+\ref{#1}+ commands. To reference a
particular page, use the \verb+\pageref{#1}+ command.

The \verb+\label{#1}+ should appear in a section heading, within an
equation, or in a table or figure caption. The \verb+\ref{#1}+ command
is used in the text where the citation is to be displayed.  Some
examples: Section~\ref{sec:level1} on page~\pageref{sec:level1},
Table~\ref{tab:table1},%
\begin{table}
\caption{\label{tab:table1}This is a narrow table which fits into a
text column when using \texttt{twocolumn} formatting. Note that
REV\TeX~4 adjusts the intercolumn spacing so that the table fills the
entire width of the column. Table captions are numbered
automatically. This table illustrates left-aligned, centered, and
right-aligned columns.  }
\begin{ruledtabular}
\begin{tabular}{lcr}
Left\footnote{Note a.}&Centered\footnote{Note b.}&Right\\
\hline
1 & 2 & 3\\
10 & 20 & 30\\
100 & 200 & 300\\
\end{tabular}
\end{ruledtabular}
\end{table}
and Fig.~\ref{fig:epsart}.

\section{Figures and Tables}
Figures and tables are typically ``floats''; \LaTeX\ determines their
final position via placement rules. 
\LaTeX\ isn't always successful in automatically placing floats where you wish them.

Figures are marked up with the \texttt{figure} environment, the content of which
imports the image (\verb+\includegraphics+) followed by the figure caption (\verb+\caption+).
The argument of the latter command should itself contain a \verb+\label+ command if you
wish to refer to your figure with \verb+\ref+.

Import your image using either the \texttt{graphics} or
\texttt{graphix} packages. These packages both define the
\verb+\includegraphics{#1}+ command, but they differ in the optional
arguments for specifying the orientation, scaling, and translation of the figure.
Fig.~\ref{fig:epsart}%
\begin{figure}
\includegraphics{fig_1}% Here is how to import EPS art
\caption{\label{fig:epsart} A figure caption. The figure captions are
automatically numbered.}
\end{figure}
is small enough to fit in a single column, while
Fig.~\ref{fig:wide}%
\begin{figure*}
\includegraphics{fig_2}% Here is how to import EPS art
\caption{\label{fig:wide}Use the \texttt{figure*} environment to get a wide
figure, spanning the page in \texttt{twocolumn} formatting.}
\end{figure*}
is too wide for a single column,
so instead the \texttt{figure*} environment has been used.

The analog of the \texttt{figure} environment is \texttt{table}, which uses
the same \verb+\caption+ command.
However, you should type your caption command first within the \texttt{table}, 
instead of last as you did for \texttt{figure}.

The heart of any table is the \texttt{tabular} environment,
which represents the table content as a (vertical) sequence of table rows,
each containing a (horizontal) sequence of table cells. 
Cells are separated by the \verb+&+ character;
the row terminates with \verb+\\+. 
The required argument for the \texttt{tabular} environment
specifies how data are displayed in each of the columns. 
For instance, a column
may be centered (\verb+c+), left-justified (\verb+l+), right-justified (\verb+r+),
or aligned on a decimal point (\verb+d+). 
(Table~\ref{tab:table4}%
\begin{table}
\caption{\label{tab:table4}Numbers in columns Three--Five have been
aligned by using the ``d'' column specifier (requires the
\texttt{dcolumn} package). 
Non-numeric entries (those entries without
a ``.'') in a ``d'' column are aligned on the decimal point. 
Use the
``D'' specifier for more complex layouts. }
\begin{ruledtabular}
\begin{tabular}{ccddd}
One&Two&\mbox{Three}&\mbox{Four}&\mbox{Five}\\
\hline
one&two&\mbox{three}&\mbox{four}&\mbox{five}\\
He&2& 2.77234 & 45672. & 0.69 \\
C\footnote{Some tables require footnotes.}
  &C\footnote{Some tables need more than one footnote.}
  & 12537.64 & 37.66345 & 86.37 \\
\end{tabular}
\end{ruledtabular}
\end{table}
illustrates the use of decimal column alignment.)

Extra column-spacing may be be specified as well, although
REV\TeX~4 sets this spacing so that the columns fill the width of the
table.
Horizontal rules are typeset using the \verb+\hline+
command.
The doubled (or Scotch) rules that appear at the top and
bottom of a table can be achieved by enclosing the \texttt{tabular}
environment within a \texttt{ruledtabular} environment.
Rows whose columns span multiple columns can be typeset using \LaTeX's
\verb+\multicolumn{#1}{#2}{#3}+ command
(for example, see the first row of Table~\ref{tab:table3}).%
\begin{table*}
\caption{\label{tab:table3}This is a wide table that spans the page
width in \texttt{twocolumn} mode. It is formatted using the
\texttt{table*} environment. It also demonstrates the use of
\textbackslash\texttt{multicolumn} in rows with entries that span
more than one column.}
\begin{ruledtabular}
\begin{tabular}{ccccc}
 &\multicolumn{2}{c}{$D_{4h}^1$}&\multicolumn{2}{c}{$D_{4h}^5$}\\
 Ion&1st alternative&2nd alternative&lst alternative
&2nd alternative\\ \hline
 K&$(2e)+(2f)$&$(4i)$ &$(2c)+(2d)$&$(4f)$ \\
 Mn&$(2g)$\footnote{The $z$ parameter of these positions is $z\sim\frac{1}{4}$.}
 &$(a)+(b)+(c)+(d)$&$(4e)$&$(2a)+(2b)$\\
 Cl&$(a)+(b)+(c)+(d)$&$(2g)$\footnote{This is a footnote in a table that spans the full page
width in \texttt{twocolumn} mode. It is supposed to set on the full width of the page, just as the caption does. }
 &$(4e)^{\text{a}}$\\
 He&$(8r)^{\text{a}}$&$(4j)^{\text{a}}$&$(4g)^{\text{a}}$\\
 Ag& &$(4k)^{\text{a}}$& &$(4h)^{\text{a}}$\\
\end{tabular}
\end{ruledtabular}
\end{table*}

The tables in this document illustrate various effects.
Tables that fit in a narrow column are contained in a \texttt{table}
environment.
Table~\ref{tab:table3} is a wide table, therefore set with the
\texttt{table*} environment.
Lengthy tables may need to break across pages.
A simple way to allow this is to specify
the \verb+[H]+ float placement on the \texttt{table} or
\texttt{table*} environment.
Alternatively, using the standard \LaTeXe\ package \texttt{longtable} 
gives more control over how tables break and allows headers and footers 
to be specified for each page of the table.
An example of the use of \texttt{longtable} can be found
in the file \texttt{summary.tex} that is included with the REV\TeX~4
distribution.

There are two methods for setting footnotes within a table (these
footnotes will be displayed directly below the table rather than at
the bottom of the page or in the bibliography).
The easiest
and preferred method is just to use the \verb+\footnote{#1}+
command. This will automatically enumerate the footnotes with
lowercase roman letters.
However, it is sometimes necessary to have
multiple entries in the table share the same footnote.
In this case,
create the footnotes using
\verb+\footnotemark[#1]+ and \verb+\footnotetext[#1]{#2}+.
\texttt{\#1} is a numeric value.
Each time the same value for \texttt{\#1} is used, 
the same mark is produced in the table. 
The \verb+\footnotetext[#1]{#2}+ commands are placed after the \texttt{tabular}
environment. 
Examine the \LaTeX\ source and output for Tables~\ref{tab:table1} and 
\ref{tab:table2}%
\begin{table}
\caption{\label{tab:table2}A table with more columns still fits
properly in a column. Note that several entries share the same
footnote. Inspect the \LaTeX\ input for this table to see
exactly how it is done.}
\begin{ruledtabular}
\begin{tabular}{cccccccc}
 &$r_c$ (\AA)&$r_0$ (\AA)&$\kappa r_0$&
 &$r_c$ (\AA) &$r_0$ (\AA)&$\kappa r_0$\\
\hline
Cu& 0.800 & 14.10 & 2.550 &Sn\footnotemark[1]
& 0.680 & 1.870 & 3.700 \\
Ag& 0.990 & 15.90 & 2.710 &Pb\footnotemark[2]
& 0.450 & 1.930 & 3.760 \\
Au& 1.150 & 15.90 & 2.710 &Ca\footnotemark[3]
& 0.750 & 2.170 & 3.560 \\
Mg& 0.490 & 17.60 & 3.200 &Sr\footnotemark[4]
& 0.900 & 2.370 & 3.720 \\
Zn& 0.300 & 15.20 & 2.970 &Li\footnotemark[2]
& 0.380 & 1.730 & 2.830 \\
Cd& 0.530 & 17.10 & 3.160 &Na\footnotemark[5]
& 0.760 & 2.110 & 3.120 \\
Hg& 0.550 & 17.80 & 3.220 &K\footnotemark[5]
&  1.120 & 2.620 & 3.480 \\
Al& 0.230 & 15.80 & 3.240 &Rb\footnotemark[3]
& 1.330 & 2.800 & 3.590 \\
Ga& 0.310 & 16.70 & 3.330 &Cs\footnotemark[4]
& 1.420 & 3.030 & 3.740 \\
In& 0.460 & 18.40 & 3.500 &Ba\footnotemark[5]
& 0.960 & 2.460 & 3.780 \\
Tl& 0.480 & 18.90 & 3.550 & & & & \\
\end{tabular}
\end{ruledtabular}
\footnotetext[1]{Here's the first, from Ref.~\onlinecite{feyn54}.}
\footnotetext[2]{Here's the second.}
\footnotetext[3]{Here's the third.}
\footnotetext[4]{Here's the fourth.}
\footnotetext[5]{And etc.}
\end{table}
for an illustration. 

All AIP journals require that the initial citation of
figures or tables be in numerical order.
\LaTeX's automatic numbering of floats is your friend here:
just put each \texttt{figure} environment immediately following 
its first reference (\verb+\ref+), as we have done in this example file. 

\begin{acknowledgments}
We wish to acknowledge the support of the author community in using
REV\TeX{}, offering suggestions and encouragement, testing new versions,
\dots.
\end{acknowledgments}

\section*{Data Availability Statement}

AIP Publishing believes that all datasets underlying the conclusions of the paper should be available to readers. Authors are encouraged to deposit their datasets in publicly available repositories or present them in the main manuscript. All research articles must include a data availability statement stating where the data can be found. In this section, authors should add the respective statement from the chart below based on the availability of data in their paper.

\begin{center}
\renewcommand\arraystretch{1.2}
\begin{tabular}{| >{\raggedright\arraybackslash}p{0.3\linewidth} | >{\raggedright\arraybackslash}p{0.65\linewidth} |}
\hline
\textbf{AVAILABILITY OF DATA} & \textbf{STATEMENT OF DATA AVAILABILITY}\\  
\hline
Data available on request from the authors
&
The data that support the findings of this study are available from the corresponding author upon reasonable request.
\\\hline
Data available in article or supplementary material
&
The data that support the findings of this study are available within the article [and its supplementary material].
\\\hline
Data openly available in a public repository that issues datasets with DOIs
&
The data that support the findings of this study are openly available in [repository name] at http://doi.org/[doi], reference number [reference number].
\\\hline
Data openly available in a public repository that does not issue DOIs
&
The data that support the findings of this study are openly available in [repository name], reference number [reference number].
\\\hline
Data sharing not applicable – no new data generated
&
Data sharing is not applicable to this article as no new data were created or analyzed in this study.
\\\hline
Data generated at a central, large scale facility
&
Raw data were generated at the [facility name] large scale facility. Derived data supporting the findings of this study are available from the corresponding author upon reasonable request.
\\\hline
Embargo on data due to commercial restrictions
&
The data that support the findings will be available in [repository name] at [DOI link] following an embargo from the date of publication to allow for commercialization of research findings.
\\\hline
Data available on request due to privacy/ethical restrictions
&
The data that support the findings of this study are available on request from the corresponding author. The data are not publicly available due [state restrictions such as privacy or ethical restrictions].
\\\hline
Data subject to third party restrictions
&
The data that support the findings of this study are available from [third party]. Restrictions apply to the availability of these data, which were used under license for this study. Data are available from the authors upon reasonable request and with the permission of [third party].
\\\hline
\end{tabular}
\end{center}

\appendix

\section{Appendixes}

To start the appendixes, use the \verb+\appendix+ command.
This signals that all following section commands refer to appendixes
instead of regular sections. Therefore, the \verb+\appendix+ command
should be used only once---to set up the section commands to act as
appendixes. Thereafter normal section commands are used. The heading
for a section can be left empty. For example,
\begin{verbatim}
\appendix
\section{}
\end{verbatim}
will produce an appendix heading that says ``APPENDIX A'' and
\begin{verbatim}
\appendix
\section{Background}
\end{verbatim}
will produce an appendix heading that says ``APPENDIX A: BACKGROUND''
(note that the colon is set automatically).

If there is only one appendix, then the letter ``A'' should not
appear. This is suppressed by using the star version of the appendix
command (\verb+\appendix*+ in the place of \verb+\appendix+).

\section{A little more on appendixes}

Observe that this appendix was started by using
\begin{verbatim}
\section{A little more on appendixes}
\end{verbatim}

Note the equation number in an appendix:
\begin{equation}
E=mc^2.
\end{equation}

\subsection{\label{app:subsec}A subsection in an appendix}

You can use a subsection or subsubsection in an appendix. Note the
numbering: we are now in Appendix~\ref{app:subsec}.

\subsubsection{\label{app:subsubsec}A subsubsection in an appendix}
Note the equation numbers in this appendix, produced with the
subequations environment:
\begin{subequations}
\begin{eqnarray}
E&=&mc, \label{appa}
\\
E&=&mc^2, \label{appb}
\\
E&\agt& mc^3. \label{appc}
\end{eqnarray}
\end{subequations}
They turn out to be Eqs.~(\ref{appa}), (\ref{appb}), and (\ref{appc}).

\fi
%\nocite{*}
\bibliography{aipsamp}% Produces the bibliography via BibTeX.

\end{document}
%
% ****** End of file aipsamp.tex ******