

\begin{table*}[t!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}llc|cccccccc|cc@{}}
\toprule
\multirow{3}{*}{Methods} & \multirow{3}{*}{Venue} & \multirow{3}{*}{}%
& \multicolumn{8}{c|}{ImageNet-1k $\rightarrow$ OOD Datasets} & \multicolumn{2}{c}{\multirow{2}{*}{Average}} \\
 & & & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c|}{Texture} &  \\ \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
 &  & & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ \\
\midrule
MOS$\dagger$~\cite{huang_mos_2021} (BiT) & CVPR'21  &  & 9.28 & 98.15 & 40.63 & 92.01 & 49.54 & 89.06 & 60.43 & 81.23 & 39.97 & 90.11 \\
Fort \etal$\dagger$~\cite{fort_exploring_2021} (ViT-B) & NeurIPS'21 & & 15.07 & 96.64 & 54.12 & 86.37 & 57.99 & 85.24 & 53.32 & 84.77 & 45.12 & 88.25 \\
Energy$\dagger$~\cite{liu_energy-based_2020} (ViT-B)& NeurIPS'20 & & 21.59 & 95.99 & 34.28 & 93.15 & 36.64 & 91.82 & 51.18 & 88.09 & 35.92 & 92.26 \\
MSP$\dagger$~\cite{hendrycks_baseline_2017} (ViT-B) & ICLR'17 & & 40.89 & 88.63 & 65.81 & 81.24 & 67.90 & 80.14 & 64.96 & 78.16 & 59.89 & 82.04 \\
ODIN$\ddagger$~\cite{liang_enhancing_2020} & ICLR'18 & & 30.22 & 94.65 & 54.04 & 87.17 & 55.06 & 85.54 & 51.67 & 87.85 & 47.75 & 88.80\\
ViM$\ddagger$~\cite{Wang_2022_CVPR} & CVPR'22 & & 32.19 & 93.16 & 54.01 & 87.19 & 60.67 & 83.75 & 53.94 & 87.18 & 50.20 & 87.82\\
KNN$\ddagger$~\cite{sun_out--distribution_2022} & ICML'22 & & 29.17 & 94.52 & 35.62 & 92.67 & 39.61 & 91.02 & 64.35 & 85.67 & 42.19 & 90.97\\
\rowcolor[gray]{.9} \multicolumn{13}{c}{VLM-based OOD Detection (CLIP of VIT-B/16)}  \\
MCM~\cite{ming2022delving} & NeurIPS'22  && 30.91 & 94.61 & 37.59 & 92.57 & 44.69 & 89.77 & 57.77 & 86.11 & 42.74 & 90.77 \\
CoOp*~\cite{zhou_learning_2022} & IJCV & & 29.47 & 94.89 & 31.34 & 93.36 & 40.28 & 90.07 & 54.25 & 87.58 & 38.83 & 91.47\\
CoCoOp*~\cite{zhou2022cocoop} & CVPR'22 & & 30.74 & 94.73 & 31.18 & 93.15 & 38.75 & 90.63 & 53.84 & 87.92 & 38.63 & 91.61 \\
NPOS~\cite{tao_non-parametric_2023} & ICLR'23 & & 16.58 & 96.19 & 43.77 & 90.44 & 45.27 & 89.44 & 46.12 & 88.80 & 37.93 & 91.22 \\
LoCoOp~\cite{miyai_locoop_2023} & NeurIPS'23 & & 16.05 & {96.86} & 23.44 & 95.07 & 32.87 & 91.98 & 42.28 & 90.19 & 28.66 & 93.52 \\
CATEX~\cite{liu_category-extensible_2023} & NeurIPS'23 & & 10.18 & 97.88 & 33.87 & 92.83 & 41.43 & 90.48 & {33.17} & {92.73} & 29.66 & 93.48 \\
CLIPN~\cite{wang_clipn_2023} & CVPR'23 &  &23.94 & 95.27 & 26.17 & 93.93 & 33.45 & 92.28 & 40.83 & 90.93 & 31.10 & 93.10 \\
NegLabel~\cite{jiang2024negative} & ICLR'24 & & {\textbf{1.91}} & {\textbf{99.49}} & 20.53 & 95.49 & 35.59 & 91.64 & 43.56 & 90.22 & 25.40 & 94.21 \\
LSN~\cite{nie_out--distribution_2024} & ICLR'24 & & 21.56 & 95.83 & 26.32 & 94.35 & 34.48 & 91.25 & 38.54 & 90.42 & 30.22 & 92.96 \\
EOE~\cite{cao_envisioning_2024} & ICML'24 &  &12.29 & 97.52 & 20.40 & 95.73 & 30.16 & 92.95 & 57.53 & 85.64 & 30.09 & 92.96 \\
ID-Like (4-shots)~\cite{bai_id-like_2024} & CVPR'24 & & 8.98 & 98.19 & 42.03 & 91.64 & 44.00 & 90.57 & \textbf{25.27} & 94.32 & 30.07 & 93.68 \\
NegPrompt~\cite{li_learning_2024} & CVPR'24  && 6.32 & 98.73 & 22.89 & 95.55 & 27.60 & 93.34 & {35.21} & {91.60} & 23.01 & {94.81} \\
\midrule
\rowcolor{green!10} \ours &  &  & 8.27 & 98.29 & \textbf{19.40} & \textbf{95.84} & \textbf{26.69} & \textbf{93.56} & {26.77} & \textbf{94.45} & \textbf{20.28} & \textbf{95.54} \\
\bottomrule
\end{tabular}%
}
\vspace{-1em}
\caption{OOD detection results with ID data of ImageNet-1k and four OOD datasets using CLIP (VIT-B/16). Best in \textbf{Bold}. 
``*'' is cited from LSN~\cite{nie_out--distribution_2024}. 
``$\dagger$'' represents results from MCM~\cite{ming2022delving}.
``$\ddagger$'' represents results from NPOS~\cite{tao_non-parametric_2023}. 
}
\label{tab: main}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}l|cccccccc|cc@{}}
\toprule
\multirow{3}{*}{Methods} & \multicolumn{8}{c|}{ImageNet-100 $\rightarrow$ OOD Datasets} & \multicolumn{2}{c}{\multirow{2}{*}{Average}} \\
 & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c|}{Texture} &  \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
 & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ \\
\midrule
MSP$\dagger$ & 23.55 & 95.92 & 37.02 & 92.45 & 40.76 & 91.23 & 24.40 & 94.90 & 31.43 & 93.63 \\
ViM$\dagger$ & 20.11 & 96.22 & 38.56 & 93.12 & 44.01 & 87.33 & 33.12 & 93.24 & 33.95 & 92.48 \\
MCM & 18.13 & 96.77 & 36.45 & 94.54 & 34.52 & 94.36 & 41.22 & 92.25 & 32.58 & 94.48 \\
CoOp$\dagger$& 9.30 & 97.95 & 11.64 & 97.61 & 17.45 & 96.53 & 15.94 & 96.90 & 13.58 & 97.25  \\
CoCoOp$\dagger$& 11.76 & 97.84 & 14.28 & 97.13 & 15.16 & 96.73 & 18.27 & 96.54 & 14.86 & 97.06 \\
LSN & 4.93 & 98.92 & 8.23 & \textbf{98.98} & 12.82 & \textbf{97.19} & \textbf{8.26} & \textbf{98.11} & 8.56 & 98.05 \\
\midrule
\rowcolor{green!10}\ours & \textbf{2.54} & \textbf{99.21} & \textbf{8.08} & {98.28} & \textbf{12.17} & {97.15} & {9.98} & {97.69} & \textbf{8.19} & \textbf{98.08} \\
\bottomrule
\end{tabular}%
}
\vspace{-1em}
\caption{OOD detection results with ID data of ImageNet-100 and four OOD datasets using CLIP (VIT-B/16). Best in \textbf{Bold}. 
``$\dagger$'' represents results from LSN~\cite{nie_out--distribution_2024}.
}
\label{tab: main: imagenet100}
\vspace{-2em}
\end{table*}


\section{Experiments}

\subsection{Experimental Details}
\textbf{Datasets and benchmarks.} 
Our experiments primarily use the ImageNet-1k~\cite{dengImageNetLargescaleHierarchical2009} and ImageNet-100~\cite{dengImageNetLargescaleHierarchical2009} dataset as ID data. 
Aligning with standards from prior works~\cite{ming2022delving,wang_clipn_2023}, we evaluate on four diverse OOD datasets: iNaturalist~\cite{van_horn_inaturalist_2018}, SUN~\cite{xiao_sun_2010}, Places~\cite{zhou_places_2018}, and Texture~\cite{cimpoi_describing_2014}. 
Additionally, we test on ImageNet-10~\cite{dengImageNetLargescaleHierarchical2009} and ImageNet-20~\cite{dengImageNetLargescaleHierarchical2009} for near OOD.  
Details are in the Appendix. 

\noindent
\textbf{Baselines methods.} 
We compare \ours with 19 baseline methods, including traditional OOD detection methods~\cite{huang_mos_2021,fort_exploring_2021,liu_energy-based_2020,hendrycks_baseline_2017,liang_enhancing_2020,Wang_2022_CVPR,sun_out--distribution_2022}, VLM (CLIP)-based OOD methods~\cite{bai_id-like_2024,ming2022delving,cao_envisioning_2024,wang_clipn_2023,tao_non-parametric_2023,miyai_locoop_2023,liu_category-extensible_2023,nie_out--distribution_2024,li_learning_2024,jiang2024negative}, and visual prompt tuning methods~\cite{zhou_learning_2022,zhou2022cocoop}. 

\noindent
\textbf{Implementation details.}
All the images in the official training set are used to obtain ID image prototypes. 
For each class, 16 images are used for few-shot tuning. 
We use the image and text encoders of VIT-B/16 pre-trained by CLIP~\cite{radford_learning_2021} for all the experiments. 
The parameters of CLIP are frozen.
We use SGD to optimize other parameters, \eg, estimated image domain bias and learnable context, with a momentum of $0.9$. 
Training epochs, learning rate, batch size, and learnable context length are set to $50, 0.002, 32$, and $16$, respectively. 
We set $\alpha = 0.005$ and $\beta=0.1$. 
Experiments are conducted on a single Nvidia V100 with 32 GB memory. 
All the reported results for \ours are the average of three trials.

\noindent
\textbf{Metrics.} 
We employ two widely accepted metrics: FPR95, the false positive rate of OOD images when the true positive rate of ID images is at 95\%, and AUROC, the area under the receiver operating characteristic curve.

\subsection{Main Results}

\textbf{ImageNet-1k and ImageNet-100 as the ID dataset}. 
In \cref{tab: main}, we present a comprehensive analysis on ImageNet-1k (ID) across four OOD datasets (iNaturalist, SUN, Places, Texture) using CLIP (VIT-B/16). 
\ours demonstrates superior performance, achieving the lowest FPR95 and highest AUROC scores across most datasets. 
Notably, \ours achieves an average FPR95 of 20.28 and an average AUROC of 95.54. 
In \cref{tab: main: imagenet100}, we observe consistent trends using ImageNet-100 as the ID dataset. 
\ours achieves the best average results, with FPR95 of 8.19 and AUROC of 98.08, showing substantial improvements over other methods. 
These results underscore our method's robustness across varied OOD benchmark datasets, emphasizing its effectiveness in enhancing model reliability for real-world applications. 
However, we note that these OOD datasets are mostly semantically different from the ID data (ImageNet), making this problem easier. 
To evaluate \ours under more challenging conditions (near OOD), we follow previous work~\cite{ming2022delving,jiang2024negative} and conduct experiments on ImageNet-10 and ImageNet-20.

\begin{table}[t]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{l|cc|cc}
\toprule
\multirow{2}{*}{Methods}    & \multicolumn{2}{c|}{ImageNet-10 $\rightarrow$ ImageNet-20}& \multicolumn{2}{c}{ImageNet-20 $\rightarrow$ ImageNet-10} \\
& FPR95 (\%) $\downarrow$   & AUROC (\%) $\uparrow$ & FPR95 (\%) $\downarrow$   & AUROC (\%) $\uparrow$  \\
\midrule
Energy$\dagger$ & 10.30 & 97.94 & 16.40 & 97.37\\
CLIPN$\dagger$ & 7.80 & 98.07 & 13.67 & 97.47\\
MCM$\dagger$        &         5.00 & 98.71    &     17.40 & 98.87       \\
LoCoOp* & 5.60 & 98.47     & 5.40 & 98.92      \\
NegLabel & 5.10 & 98.86& 4.60 & 98.81 \\
\midrule
\rowcolor{green!10}\ours & \textbf{4.00} & \textbf{98.93}  & \textbf{3.80} & \textbf{98.98}\\
\bottomrule
\end{tabular}%
}
\vspace{-1em}
\caption{
Comparison on ImageNet-10 $\leftrightarrow$ ImageNet-20. 
We use CLIP-B/16 for all the experiments. 
Best in \textbf{bold}. 
``*'' represents our reproduction. 
``$\dagger$'' refers to results from EOE~\cite{cao_envisioning_2024}.
}\label{tab: main: imagenet10 20}
\vspace{-0.7em}
\end{table}
\begin{table}[t!]
\begin{minipage}{.52\columnwidth}
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{l|cc}
    \toprule
    Methods  & FPR95 $\downarrow$ & AUROC $\uparrow$ \\
    \midrule
    \rowcolor{green!10} \ours & \textbf{20.28} & \textbf{95.54} \\
    \midrule
    - BPG & 27.51&	93.71 \\
    - ITC & 28.59&	93.38 \\
    - $S_{\textit{GMP}}$ & 25.03 & 94.80 \\
    \bottomrule
    \end{tabular}
    }
    \vspace{-0.7em}
    \caption{Effectiveness of different modules and the proposed OOD score $S_{\textit{GMP}}$.}
    \label{tab: ab: modules}
\end{minipage}%
\quad
\begin{minipage}{.43\columnwidth}
  \centering
  \resizebox{\linewidth}{!}{
    \begin{tabular}{lc}
    \toprule
    Methods              & Top-1 Acc. (\%) \\
    \midrule
    MCM & 67.0 \\
    CLIPN & 68.5 \\
    ID-like & 68.3 \\
    \midrule
    \rowcolor{green!10} \ours & \textbf{71.8}               \\
    \bottomrule
    \end{tabular}%
    }
    \vspace{-0.7em}
    \caption{Comparison in ID accuracy on ImageNet-1K val set.}
    \label{tab: ab: ID accuracy}
\end{minipage} 
\vspace{-1em}
\end{table}

\noindent
\textbf{ImageNet-10 $\leftrightarrow$ ImageNet-20}. 
In \cref{tab: main: imagenet10 20}, 
we compare OOD detection performance tested with ImageNet-10 as the ID dataset and ImageNet-20 as the OOD dataset (ImageNet-10 $\rightarrow$ ImageNet-20), and vice versa (ImageNet-20 $\rightarrow$ ImageNet-10).
\ours achieves the best performance, with the lowest FPR95 and highest AUROC scores in both scenarios.
These results underscore its robustness and reliability in near OOD.

\subsection{Ablation Study}
To understand the effectiveness of \ours, we present the average results of an ablation study using CLIP (ViT-B/16) on ImageNet-1k $\rightarrow$ four OOD datasets. 
The detailed results are given in the Appendix.

\noindent
\textbf{Effectiveness of the proposed modules (BPG and ITC) and scores ($S_{\textit{GMP}}$).} 
In \cref{tab: ab: modules}, we evaluate the effectiveness of BPG and ITC, as well as the proposed score $S_{\textit{GMP}}$, for enhancing OOD detection. 
We see that excluding BPG (-BPG) raises FPR95 to 27.51 and reduces AUROC to 93.71, highlighting BPG’s importance in reducing false positives. 
Similarly, removing ITC (-ITC) significantly degrades performance, increasing FPR95 to 28.59 and lowering AUROC to 93.38. 
Furthermore, when the model uses $S_{\textit{MCM}}$ instead of $S_{\textit{GMP}}$, the performance also declines, with FPR95 at 25.03 and AUROC at 94.80. 
Our full model, incorporating ITC, BPG, and $S_{\textit{GMP}}$, achieves the best overall performance.
This demonstrates the combined efficacy of these modules and the proposed OOD score $S_{\textit{GMP}}$.

\noindent
\textbf{\ours improves the ID performance.} 
OOD detection acts as a preliminary step before models make predictions, aiming to perform a binary classification to distinguish between ID and OOD samples. 
However, it is also important to understand how OOD methods work with ID classification. 
In \cref{tab: ab: ID accuracy}, we compare the Top-1 accuracy of different methods on the ImageNet-1K validation set. 
\ours achieves the highest accuracy, reaching 71.8\%.
Compared to other OOD detection methods, \eg, CLIPN (68.5\%) and ID-like (68.3\%), \ours shows a considerable gain, underscoring its effectiveness in maintaining high ID accuracy while also excelling in OOD detection. 


\begin{figure}[t!]
\centering
\centering
\includegraphics[width=0.9\columnwidth]{pics/number_of_prompts.eps}
\vspace{-1.8em}
\caption{Performance comparison on the length of prompts.}
\label{fig: ab: length of prompts}
\vspace{-0.2em}
\includegraphics[width=0.9\columnwidth]{pics/number_of_Image.eps}
\vspace{-1.8em}
\caption{Performance comparison on the number of based images used for obtaining image prototypes. ``1281'' corresponds to using all the training images as the base images.}
\label{fig: ab: size of base images}
\vspace{-0.5em}
\end{figure}


\noindent
\textbf{Impact of the length of prompts.}
In the main experiments, we follow previous works~\cite{miyai_locoop_2023,zhou2022cocoop,zhou_learning_2022} and set the length of prompts to $L = 16$. 
Here, we would like to validate that $16$ is the best choice for \ours. 
We see in \cref{fig: ab: length of prompts} that among the choices from $1$ to $16$, $16$ obtains the lowest FPR95 and highest AUROC. 
Increasing the length of prompts improves the OOD detection performance.

\noindent
\textbf{Impact of base image size for constructing ID image prototypes.}
In $S_{\textit{GMP}}$, we use a set of base images to construct image prototypes $\{P_{i,c}\}_{c \in [C]}$. 
To understand how the size of base images affects performance, we conduct experiments varying its size. 
The results in \cref{fig: ab: size of base images} show that by increasing the number of images for each class, the performance of \ours is improved, as image prototypes are closer to the real distribution. 
FPR95 and AUROC reach the best when using all the images. 


\noindent
\textbf{Effectiveness of $S_{\textit{GMP}}$.} 
To empirically show the effectiveness of $S_{\textit{GMP}}$, we visualize the empirical cumulative distribution of $S_{\textit{MCM}}$ and our $S_{\textit{GMP}}$ in \cref{fig: ablations: scores}. 
We use our best model trained on ImageNet-1k (ID), with iNaturalist being the OOD dataset. 
$S_{\textit{GMP}}$ clearly improves the gap between ID and OOD data in the empirical CDF figure. 
Moreover, the statistics of Kolmogorov–Smirnov test is enlarged from $0.7270$ ($S_{\textit{MCM}}$) to $0.8724$ ($S_{\textit{GMP}}$), indicating that using $S_{\textit{GMP}}$ has more separable scores compared to $S_{\textit{MCM}}$.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MCM_imagenet_iNaturalist_ecdf.pdf}
        \vspace{-1.7em}
        \caption{$S_{\textit{MCM}}, \textit{KS}=0.7270$~\cite{ming2022delving}.}
    \end{subfigure}%
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MMO_imagenet_iNaturalist_ecdf.pdf}
        \vspace{-1.7em}
        \caption{$S_{\textit{GMP}}, \textit{KS}=0.8724$.}
    \end{subfigure}%
    \vspace{-1em}
    \caption{Comparison between $S_{\textit{MCM}}$~\cite{ming2022delving} and our $S_{\textit{GMP}}$ on ImageNet-1k (ID) to iNaturalist (OOD). 
    The scores are multiplied by $100$ for better illustration. 
    ``KS'' is the statistic from the Kolmogorov–Smirnov test. 
    Higher KS statistic values indicate a greater difference between two distributions.
    Best viewed in color. 
    }
    \label{fig: ablations: scores}
    \vspace{-0.5em}
\end{figure}
