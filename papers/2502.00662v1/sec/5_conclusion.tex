\section{Conclusion}
In this paper, we introduced \ours, a novel VLM-based few-shot tuning OOD detection framework designed to minimize the modality gap between text and image in CLIP. 
\ours incorporates two key modules: BPG, which enhances image-text fusion, and ITC, which minimizes the inter- and intra-modal distances. 
We also proposed the generalized multi-modal prototype OOD score, $S_{\textit{GMP}}$, using multi-modal prototypes and embeddings to improve robustness and reduce false positives. 
Experimental results show that \ours consistently outperforms existing VLM-based OOD detection methods. 

\noindent
\textbf{Limitations}. 
The efficiency of \ours partially relies on the introduction of multi-modal prototypes, which may not always be feasible in scenarios with strict data constraints. 
Exploring more advanced methods such as image generation or retrieval from a web-scale dataset could help alleviate the dependency on the ID data. 
Moreover, our \ours and previous VLM-based methods use CLIP as the base model. 
It would be interesting to explore the efficacy of using advanced VLMs, such as ImageBind~\cite{girdhar_imagebind_2023}, in OOD detection.
Finally, in this work we have only explored the effectiveness of multi-modal positive/ID prototypes. Introducing negative/OOD prototypes may further enhance our framework's versatility.
