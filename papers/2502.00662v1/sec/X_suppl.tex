\clearpage
\setcounter{page}{1}
\maketitlesupplementary

In this Appendix, we present the details and complete results corresponding to \cref{tab: MMA} in \cref{Appendix: Sec: full MMA}, additional ablation studies in \cref{Appendix: Sec: More ablations}, benchmark dataset specifications in \cref{Appendix: Sec: datasets}, and the proof of \cref{the: image anchor helps} in \cref{Appendix: Sec: Proof}.

\section{Experiments}

\subsection{Empirical Evidence for RQ1}
\label{Appendix: Sec: full MMA}

\begin{table}[h!]
\centering
\begin{tabular}{l|cc}
\toprule
Methods    & FPR95 (\%) $\downarrow$   & AUROC (\%) $\uparrow$  \\
\midrule
\rowcolor[gray]{.9} \multicolumn{3}{c}{ImageNet-100 $\rightarrow$ iNaturalist} \\
$S_{\textit{MCM}}$~\cite{ming2022delving}        &    18.13 & 96.77          \\
$S_{\textit{MMP}}$ (Ours, \cref{eq: MMA})        &    \textbf{14.76} & \textbf{97.36}         \\
\rowcolor[gray]{.9} \multicolumn{3}{c}{ImageNet-100 $\rightarrow$ SUN} \\
$S_{\textit{MCM}}$~\cite{ming2022delving}        &    36.45 & \textbf{94.54}        \\
$S_{\textit{MMP}}$ (Ours, \cref{eq: MMA})        &    \textbf{30.28} & 92.51       \\
\rowcolor[gray]{.9} \multicolumn{3}{c}{ImageNet-100 $\rightarrow$ Places} \\
$S_{\textit{MCM}}$~\cite{ming2022delving}        &    34.52 & \textbf{94.36}        \\
$S_{\textit{MMP}}$ (Ours, \cref{eq: MMA})        &    \textbf{34.04} &    93.92      \\
\rowcolor[gray]{.9} \multicolumn{3}{c}{ImageNet-100 $\rightarrow$ Texture} \\
$S_{\textit{MCM}}$~\cite{ming2022delving}        &    41.22 & 92.25        \\
$S_{\textit{MMP}}$ (Ours, \cref{eq: MMA})        &    \textbf{17.66} & \textbf{96.66}        \\
\bottomrule
\end{tabular}%
\caption{
Comparison of OOD detection results on four OOD tasks. 
We use CLIP-B/16 for $S_{\textit{MCM}}$ and $S_{\textit{MMP}}$ (ours, \cref{eq: MMA}). 
FPR95 represents the false positive rate of OOD images when the true positive rate of ID images is at 95\% while AUROC is the area under the
receiver operating characteristic curve.
Best in \textbf{bold}. 
}\label{tab: MMA full}
\end{table}
\begin{table*}[t!]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}ccc|cccccccc|cc@{}}
\toprule
\multirow{3}{*}{ITC} & \multirow{3}{*}{BPG} & \multirow{3}{*}{$S_{\textit{GMP}}$} & \multicolumn{8}{c}{ImageNet-1k $\rightarrow$ OOD Datasets} & \multicolumn{2}{|c}{\multirow{2}{*}{Average}} \\
 & & & \multicolumn{2}{c}{iNaturalist} & \multicolumn{2}{c}{SUN} & \multicolumn{2}{c}{Places} & \multicolumn{2}{c|}{Texture} &  \\ \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9} \cmidrule(lr){10-11}
 &  & & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ \\
\midrule
&&& 30.91 & 94.61 & 37.59 & 92.57 & 44.69 & 89.77 & 57.77 & 86.11 & 42.74 & 90.77 \\
\checkmark&  &&14.64&	96.76&	22.92&	95.01&	33.05&	91.86&	42.55&	90.00&	28.29&	93.40\\
& \checkmark &&18.75&	96.15&	24.47&	94.43&	33.36&	91.80&	42.96&	89.92&	29.89&	93.07 \\
\checkmark& \checkmark &&20.00&	96.02&	22.35&	95.65&	28.19&	93.82&	29.57&	93.82&	25.03&	94.80\\
\checkmark& & \checkmark &12.73&	97.27&	23.86&	94.80&	33.65&	91.67&	39.80&	91.10&	27.51&	93.71\\
&\checkmark&\checkmark&15.78&	96.79&	24.56&	94.23&	33.35&	91.55&	40.66&	90.93&	28.59&	93.38\\
\rowcolor{green!10}\checkmark & \checkmark  & \checkmark  & \textbf{8.27} & \textbf{98.29} & \textbf{19.40} & \textbf{95.84} & \textbf{26.69} & \textbf{93.56} & \textbf{26.77} & \textbf{94.45} & \textbf{20.28} & \textbf{95.54} \\
\bottomrule
\end{tabular}%
}
\caption{
Effectiveness of different modules and the proposed OOD score $S_{\textit{GMP}}$. 
When $S_{\textit{GMP}}$ is not combined with BPG, it degenerates to $S_{\textit{MMP}}$.
}
\label{tab: ablations: modules full}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{cc|cccccccc|cc}
\toprule
\multirow{3}{*}{ReAct} & \multirow{3}{*}{$S_{\textit{EOE}}$} & \multicolumn{8}{c|}{ImageNet-1k $\rightarrow$ OOD Datasets}                                                                                                   & \multicolumn{2}{c}{\multirow{2}{*}{Average}} \\
                       &                           & \multicolumn{2}{c}{iNaturalist}       & \multicolumn{2}{c}{SUN}               & \multicolumn{2}{c}{Places}            & \multicolumn{2}{c|}{Texture}          & \multicolumn{2}{c}{}                         \\
                       &                           & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$ & AUROC $\uparrow$ & FPR95 $\downarrow$     & AUROC $\uparrow$    \\
\midrule
                       &                      & 8.27               & 98.29            & 19.40              & 95.84            & 26.69              & \textbf{93.56}   & \textbf{26.77}     & \textbf{94.45}   & 20.28                  & \textbf{95.54}               \\
                \checkmark &                      & 10.73              & 97.32            & 14.22              & 96.23            & \textbf{24.12}     & 93.03            & 35.76              & 90.83            & 21.21                  & 94.35               \\
                       &   \checkmark                    & 4.83               & 98.89            & \textbf{13.02}     & \textbf{96.91}   & 24.58              & 93.22            & 34.57              & 92.34            & 19.25                  & 95.34               \\
             \checkmark           &    \checkmark                   & \textbf{4.28}      & \textbf{99.00}   & 13.63              & 96.72            & 24.40              & 93.51            & 34.24              & 92.20            & \textbf{19.14}         & {95.36}       \\
\bottomrule
\end{tabular}%
}
\caption{Performance of \ours combined with post-hoc methods, \ie, ReAct~\cite{sun_react_2021} and $S_{\textit{EOE}}$~\cite{cao_envisioning_2024}.}
\label{Appendix: tab: post-hoc}
\end{table*}

\begin{table}[]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lcccc}
\toprule
Methods   & Parameters                      & Trainable Parameters & Image Prototypes & Inference \\
\midrule
MCM    & 124,323,841                     & 0                    & -                             &    $O(1)$  \\
\ours & 124,891,683 ($\uparrow$567,842) & 567,842              & $O(N_{base})$                           &   $O(1)$  \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of (trainable) parameters and inference efficiency between MCM and \ours with the base model as CLIP-B/16.}
\label{Appendix: Tab: Parameters}
\end{table}

To empirically justify the effectiveness of multi-modal (image and text) prototypes, we present experiments using $S_{\textit{MCM}}$ and $S_{\textit{MMP}}$ on four OOD datasets (\ie, iNaturalist, SUN, Places, and Texture). 
We use CLIP-B/16 as the base model for $S_{\textit{MCM}}$ and $S_{\textit{MMP}}$. 
All the training images from ImageNet-100 are used for generating image prototypes.

Our method, $S_{\textit{MMP}}$, consistently demonstrates superior performance in terms of FPR95 across all four tasks, achieving the lowest values, with notable improvements on iNaturalist (14.76 vs. 18.13) and Texture (17.66 vs. 41.22), highlighting its robustness in reducing false positives. 
For AUROC, $S_{\textit{MMP}}$ achieves the highest score on iNaturalist (97.36) and Texture (96.66). 
However, in SUN and Places datasets, while $S_{\textit{MMP}}$ maintains competitive performance, it slightly underperforms $S_{\textit{MCM}}$ in AUROC (92.51 vs. 94.54 on SUN and 93.92 vs. 94.36 on Places).

Overall, the results underscore the effectiveness of $S_{\textit{MMP}}$ and multi-modal prototypes in reducing false positives and the performance of VLM-based OOD detection.
    
\subsection{Ablations}
\label{Appendix: Sec: More ablations}

To understand the effectiveness of \ours, we present the results of several ablation studies using CLIP (ViT-B/16) on ImageNet-1k $\rightarrow$ four OOD datasets (\ie, iNaturalist, SUN, Places, and Texture). Our experiments examine the effectiveness of the proposed components (\ie, ITC, BPG, and $S_{\textit{GMP}}$), the impact of combining \ours with post-hoc methods, training data efficiency (number of shots), the statistical evaluation of $S_{\textit{GMP}}$, and sensitivity to hyperparameters.

\noindent
\textbf{Evaluating the effectiveness of the proposed modules (BPG and ITC) and $\pmb{S_{\textit{GMP}}}$ (complete results).}
To evaluate the effectiveness of the proposed modules and $S_{\textit{GMP}}$, we present detailed results of a combinatorial ablation study in \cref{tab: ablations: modules full}. 
Performance is evaluated across four datasets (\ie, iNaturalist, SUN, Places, and Texture). 

Compared with the base model (MCM~\cite{ming2022delving}), introducing ITC considerably reduces FPR95 to 28.29 while boosting AUROC to 93.40. 
Similarly, BPG improves performance, though to a slightly lesser extent, achieving an average FPR95 of 29.89 and AUROC of 93.07. 
When ITC and BPG are combined, the performance improves further, reaching an average FPR95 of 25.03 and AUROC of 94.80. 
These results show that both ITC and BPG individually contribute substantial improvements over the base model, while their combination yields even greater performance.

For the proposed score, $S_{\textit{GMP}}$, 
when combining it with either ITC or BPG, the performance shows incremental gains compared to using ITC or BPG alone. 
The best results are obtained when all three components (ITC, BPG, and $S_{\textit{GMP}}$) are combined, yielding an average FPR95 of 20.28 and AUROC of 95.54. This configuration achieves the lowest FPR95 and highest AUROC across all configurations and datasets.

\noindent
\textbf{Combining \ours with post-hoc methods.} 
To understand how \ours combines with existing representative post-hoc methods, \ie, ReAct~\cite{sun_react_2021} and $S_{\textit{EOE}}$~\cite{cao_envisioning_2024}, we present a systematic study in \cref{Appendix: tab: post-hoc}. 
We directly apply ReAct and $S_{\textit{EOE}}$ on our best model as post-hoc methods, which means we do not use them during training. 
For ReAct, we set the threshold at 0.95, while for $S_{\textit{EOE}}$, we follow the standard parameters. 
We see that using ReAct alone makes the performance worse with average FPR95 of 21.21 and AUROC of 94.35. 
On the other hand, $S_{\textit{EOE}}$ boosts average FPR95 from 20.28 to 19.25. 
When combining ReAct and $S_{\textit{EOE}}$, the FPR95 is boosted to 19.14 on average. 
However, using ReAct and/or $S_{\textit{EOE}}$ decreases the average AUROC compared to vanilla \ours, showing that these post-hoc methods might only help reduce false positives. 
The experiments show that \ours is compatible with these post-hoc methods without performance drops.

\noindent
\textbf{Computational cost of the training and inference.}
Table~\ref{Appendix: Tab: Parameters} compares parameter efficiency and inference scalability between the baseline MCM method and our proposed \ours, using CLIP-B/16 as the base model. 
While both methods exhibit comparable total parameter sizes, \ours introduces 567,842 trainable parameters to achieve its enhanced functionality, representing a negligible increase of 0.46\% in total parameters. 
\ours obtains image prototypes with a complexity of $O(N_{base})$, while maintaining inference complexity at $O(1)$ given one test input. 
Also, the calculation of image prototypes can be conducted before inference, and image prototypes can be shared among all the test samples, which avoids introducing overhead during inference compared with MCM. 
It demonstrates the scalability of \ours without computational overhead during inference, effectively addressing challenges of learnable flexibility and inference efficiency.







\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{pics/number_of_shots.pdf}
    \vspace{-3em}
    \caption{The impact of different sizes of fine-tuning data.}
    \label{fig: ab: shots}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{pics/alpha.pdf}
    \vspace{-1.5em}
    \includegraphics[width=\columnwidth]{pics/beta.pdf}
    \vspace{-0.7em}
    \caption{Hyperparameter sensitivity ($\alpha$ and $\beta$). We report the average FPR95 and AUROC.}
    \label{fig: ab: alpha}
\end{figure}

\begin{figure*}
    \centering
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics//score_comparison/MCM_imagenet_iNaturalist_ecdf.pdf}
        \caption{$S_{\textit{MCM}}, \textit{KS}=0.7270$~\cite{ming2022delving}.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MCM_imagenet_SUN_ecdf.pdf}
        \caption{$S_{\textit{MCM}}, \textit{KS}=0.7182$~\cite{ming2022delving}.}
    \end{subfigure}%
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MCM_imagenet_Places_ecdf.pdf}
        \caption{$S_{\textit{MCM}}, \textit{KS}=0.6650$~\cite{ming2022delving}.}
    \end{subfigure}%
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MCM_imagenet_Texture_ecdf.pdf}
        \caption{$S_{\textit{MCM}}, \textit{KS}=0.6760$~\cite{ming2022delving}.}
    \end{subfigure}%
    
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MMO_imagenet_iNaturalist_ecdf.pdf}
        \caption{$S_{\textit{GMP}}, \textit{KS}=0.8724$.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MMO_imagenet_SUN_ecdf.pdf}
        \caption{$S_{\textit{GMP}}, \textit{KS}=0.7798$.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MMO_imagenet_Places_ecdf.pdf}
        \caption{$S_{\textit{GMP}}, \textit{KS}=0.7219$.}
    \end{subfigure}
    \begin{subfigure}[t]{0.48\columnwidth}
        \centering
        \includegraphics[width=\linewidth]{pics/score_comparison/MMO_imagenet_Texture_ecdf.pdf}
        \caption{$S_{\textit{GMP}}, \textit{KS}=0.6874$.}
    \end{subfigure}
    
    \caption{The comparison between $S_{\textit{MCM}}$~\cite{ming2022delving} and our proposed $S_{\textit{GMP}}$ score (\cref{eq: MMO}) on ImageNet-1k (ID) to four OOD datasets, \ie, iNaturalist, SUN, Places, and Texture.
    Best viewed in color.
    }
    \label{fig: ablations: scores full}
\end{figure*}

\noindent
\textbf{How much data do we need for few-shot tuning?}
We evaluate the effect of the number of shots for few-shot tuning on the model's performance in \cref{fig: ab: shots}. 
As the number of shots increases from \(1\) to \(16\), we observe a consistent improvement in both metrics. Specifically, FPR95 decreases from 28.73 at \(1\)-shot to 20.28 at \(16\)-shot, indicating that more shots lead to better separability between ID and OOD data. 
Similarly, AUROC improves from 92.13 at \(1\)-shot to a peak of 95.54 at \(16\)-shot, demonstrating enhanced overall detection performance. 
These results highlight the robustness of \ours in leveraging additional labeled examples, with diminishing returns on FPR95 as data size increases. 
While higher shot numbers provide the best performance, the model performs reasonably well even with a limited number of shots, showcasing its flexibility in low-data scenarios.

\noindent
\textbf{Hyperparameter ($\alpha$ and $\beta$) sensitivity}.  
We analyze the effect of the trade-off hyperparameters $\alpha$ and $\beta$ (\cref{sec: our method}) on the model's performance, as shown in \cref{fig: ab: alpha}. 
For $\alpha$, we observe a trade-off between FPR95 and AUROC as its value increases. 
Specifically, as $\alpha$ grows from 0.001 to 0.01, FPR95 decreases initially from 20.82 to 20.28 at $\alpha = 0.005$, indicating improved separability, but then rises to 25.56 at $\alpha = 0.01$. 
Meanwhile, AUROC peaks at 95.54 for $\alpha = 0.005$ before declining, indicating that excessively large values of $\alpha$ may overemphasize certain features, leading to diminished overall performance.  
Similarly, $\beta$ demonstrates a comparable trend. The FPR95 achieves its lowest value of 20.28 at $\beta = 0.1$, while AUROC peaks at 95.54. 
The results of both hyperparameters suggest that \ours is not sensitive to the parameters and that moderate values ($\alpha = 0.005, \beta = 0.1$) balance the trade-off between false positive rate and overall discriminative ability.

\noindent
\textbf{Full statistical evaluation of $\pmb{S_{\textit{GMP}}}$.} 
We visualize the empirical cumulative distribution (empirical CDF) and the Kolmogorov–Smirnov (KS) test statistics for \( S_{\textit{MCM}} \) and our proposed \( S_{\textit{GMP}} \) in \cref{fig: ablations: scores full}.  
The evaluation uses our best model trained on the ImageNet-1k (ID) dataset, with iNaturalist, SUN, Places, and Texture serving as OOD datasets.  
The empirical CDF clearly shows that \( S_{\textit{GMP}} \) improves the gap between ID and OOD data more effectively.  
Furthermore, the KS test statistics improve considerably from \( 0.7270, 0.7182, 0.6650, \) and \( 0.6760 \) (\( S_{\textit{MCM}} \)) to \( 0.8724, 0.7798, 0.7219, \) and \( 0.6874 \) (\( S_{\textit{GMP}} \)) across the four OOD datasets. 
These results indicate that \( S_{\textit{GMP}} \) achieves better separation between ID and OOD scores compared to \( S_{\textit{MCM}} \).





\subsection{Details of Benchmarking Datasets}
\label{Appendix: Sec: datasets}

\textbf{ImageNet-100, ImageNet-10, and ImageNet-20}. 
Following MCM~\cite{ming2022delving}, we choose 100/10/20 classes from ImageNet-1k~\cite{krizhevskyImagenetClassificationDeep2012} to form ImageNet-100, ImageNet-10, and ImageNet-20. 
The chosen classes for each dataset are as follows:  
\begin{itemize}
    \item 
    \textbf{ImageNet-100}: n03877845, n03000684, n03110669, n03710721, n02825657, n02113186, n01817953, n04239074, n02002556, n04356056, n03187595, n03355925, n03125729, n02058221, n01580077, n03016953, n02843684, n04371430, n01944390, n03887697, n04037443, n02493793, n01518878, n03840681, n04179913, n01871265, n03866082, n03180011, n01910747, n03388549, n03908714, n01855032, n02134084, n03400231, n04483307, n03721384, n02033041, n01775062, n02808304, n13052670, n01601694, n04136333, n03272562, n03895866, n03995372, n06785654, n02111889, n03447721, n03666591, n04376876, n03929855, n02128757, n02326432, n07614500, n01695060, n02484975, n02105412, n04090263, n03127925, n04550184, n04606251, n02488702, n03404251, n03633091, n02091635, n03457902, n02233338, n02483362, n04461696, n02871525, n01689811, n01498041, n02107312, n01632458, n03394916, n04147183, n04418357, n03218198, n01917289, n02102318, n02088364, n09835506, n02095570, n03982430, n04041544, n04562935, n03933933, n01843065, n02128925, n02480495, n03425413, n03935335, n02971356, n02124075, n07714571, n03133878, n02097130, n02113799, n09399592, n03594945.
    
    \item
    \textbf{ImageNet-10}: n04552348, n04285008, n01530575, n02123597, n02422699, n02107574, n01641577, n03417042, n02389026, n03095699.

    \item
    \textbf{ImageNet-20}: n04147183, n02951358, n02782093, n04389033, n03773504, n02917067, n02317335, n01632458, n01630670, n01631663, n02391049, n01693334, n01697457, n02120079, n02114367, n02132136, n03785016, n04310018, n04266014, n04252077.
    
\end{itemize}

\noindent
\textbf{Other datasets}. 
Similarly, we use subsets from iNaturalist~\cite{van_horn_inaturalist_2018}, SUN~\cite{xiao_sun_2010}, Places~\cite{zhou_places_2018}, and Texture~\cite{cimpoi_describing_2014} as OOD datasets, which are created by Huang and Li~\cite{huang_mos_2021}. 
\begin{itemize}
    \item 
        \textbf{iNaturalist} contains images from the natural world with images from 5089 classes, belonging to 13 super-categories, such as Plantae (Plant), Insecta (Insect), Aves (Bird), Mammalia (Mammal).
        The subset containing 110 plant classes not present in ImageNet-1k is chosen as the OOD test set. 
        The classes are as follows, \textit{Coprosma lucida, Cucurbita foetidissima, Mitella diphylla, Selaginella bigelovii, Toxicodendron vernix, Rumex obtusifolius, Ceratophyllum demersum, Streptopus amplexifolius, Portulaca oleracea, Cynodon dactylon, Agave lechuguilla, Pennantia corymbosa, Sapindus saponaria, Prunus serotina, Chondracanthus exasperatus, Sambucus racemosa, Polypodium vulgare, Rhus integrifolia, Woodwardia areolata, Epifagus virginiana, Rubus idaeus, Croton setiger, Mammillaria dioica, Opuntia littoralis, Cercis canadensis, Psidium guajava, Asclepias exaltata, Linaria purpurea, Ferocactus wislizeni, Briza minor, Arbutus menziesii, Corylus americana, Pleopeltis polypodioides, Myoporum laetum, Persea americana, Avena fatua, Blechnum discolor, Physocarpus capitatus, Ungnadia speciosa, Cercocarpus betuloides, Arisaema dracontium, Juniperus californica, Euphorbia prostrata, Leptopteris hymenophylloides, Arum italicum, Raphanus sativus, Myrsine australis, Lupinus stiversii, Pinus echinata, Geum macrophyllum, Ripogonum scandens, Echinocereus triglochidiatus, Cupressus macrocarpa, Ulmus crassifolia, Phormium tenax, Aptenia cordifolia, Osmunda claytoniana, Datura wrightii, Solanum rostratum, Viola adunca, Toxicodendron diversilobum, Viola sororia, Uropappus lindleyi, Veronica chamaedrys, Adenocaulon bicolor, Clintonia uniflora, Cirsium scariosum, Arum maculatum, Taraxacum officinale officinale, Orthilia secunda, Eryngium yuccifolium, Diodia virginiana, Cuscuta gronovii, Sisyrinchium montanum, Lotus corniculatus, Lamium purpureum, Ranunculus repens, Hirschfeldia incana, Phlox divaricata laphamii, Lilium martagon, Clarkia purpurea, Hibiscus moscheutos, Polanisia dodecandra, Fallugia paradoxa, Oenothera rosea, Proboscidea louisianica, Packera glabella, Impatiens parviflora, Glaucium flavum, Cirsium andersonii, Heliopsis helianthoides, Hesperis matronalis, Callirhoe pedata, Crocosmia × crocosmiiflora, Calochortus albus, Nuttallanthus canadensis, Argemone albiflora, Eriogonum fasciculatum, Pyrrhopappus pauciflorus, Zantedeschia aethiopica, Melilotus officinalis, Peritoma arborea, Sisyrinchium bellum, Lobelia siphilitica, Sorghastrum nutans, Typha domingensis, Rubus laciniatus, Dichelostemma congestum, Chimaphila maculata, Echinocactus texensis.}
    \item 
        \textbf{SUN} contains 899 classes that cover indoor, urban, and natural places. 
        We use the subset that contains 50 natural objects that do not overlap with ImageNet-1k. 
        The classes we use are \textit{badlands, bamboo forest, bayou, botanical garden, canal (natural), canal (urban), catacomb, cavern (indoor), corn field, creek, crevasse, desert (sand), desert (vegetation), field (cultivated), field (wild), fishpond, forest (broadleaf), forest (needleleaf), forest path, forest road, hayfield, ice floe, ice shelf, iceberg, islet, marsh, ocean, orchard, pond, rainforest, rice paddy, river, rock arch, sky, snowfield, swamp, tree farm, trench, vineyard, waterfall (block), waterfall (fan), waterfall (plunge), wave, wheat field, herb garden, putting green, ski slope, topiary garden, vegetable garden, formal garden.}
    \item 
        \textbf{Places} contains photos labeled with scene semantic categories from three macro-classes: Indoor, Nature, and Urban. 
        We use a subset sampled from 50 categories that are not present in ImageNet-1k. 
        The classes we use are 
        \textit{badlands, bamboo forest, canal (natural), canal (urban), corn field, creek, crevasse, desert (sand), desert (vegetation), desert road, field (cultivated), field (wild), field road, forest (broadleaf), forest path, forest road, formal garden, glacier, grotto, hayfield, ice floe, ice shelf, iceberg, igloo, islet, japanese garden, lagoon, lawn, marsh, ocean, orchard, pond, rainforest, rice paddy, river, rock arch, ski slope, sky, snowfield, swamp, swimming hole, topiary garden, tree farm, trench, tundra, underwater (ocean deep), vegetable garden, waterfall, wave, wheat field}.
    \item 
        \textbf{Texture} contains images of textures and abstracted patterns. As no categories overlap with ImageNet1k, we use the entire dataset.
\end{itemize}
































































































































\section{Proof of \cref{the: image anchor helps}}
\label{Appendix: Sec: Proof}

We first introduce three necessary assumptions before proceeding with the proof of our theorem.

\begin{assumption}[ID image embeddings follow the distribution centered at ID image prototypes]\label{ass: ID from image prototypes}
Image embeddings $\mathbf{x}_{c}$ are drawn from the symmetric distribution $\mathcal{D}_c$ which is centered at the image prototypes $P_{i,c}$ in the embedding space. 
\end{assumption}

\begin{assumption}[The image-text modality gap exists]\label{ass: modality gap}
The image embeddings are closer to the ground truth class's image prototypes than the text prototypes due to the modality gap.
\end{assumption}

\begin{remark}
    These two assumptions are the basic assumptions about the multi-modal prototypes and the modality gap. 
    Based on the findings from a recent study~\cite{liang2022mind}, these two hold true for most real-world scenarios.
\end{remark}

\begin{assumption}[Models are well trained]\label{ass: well trained}
Cosine similarity between embeddings from the same class distribution $\mathcal{D}_i$ is higher on average than between embeddings from different distributions. 
And the image embeddings from the $c$-th class are equally dissimilar to other classes.
\end{assumption}

\begin{remark}
    Here we only consider the optimal representation extractors without any other assumptions on the models.
\end{remark}

\begin{assumption}[OOD embeddings are not sampled from any ID distribution.]\label{ass: OOD condition}
OOD image embeddings $\mathbf{x}_{\textit{OOD}}$ are drawn from a distribution $\mathcal{D}_{\text{\textit{OOD}}}$ different from any $\mathcal{D}_c^{*}, \forall c \in [C], * \in [\textit{text}, \textit{image}]$. 
Besides, the OOD image embeddings are equally similar to all ID text/image classes.
\end{assumption}

\begin{remark}
    This assumption is a general basic assumption on the OOD data, which provides us with an overall characteristic of the OOD data.
\end{remark}

Under the above assumptions, we first would like to prove the following inequality holds,
\begin{equation}
    \begin{aligned}
        &  \Delta_{\text{image}}=\mathbb{E}\left[\max_{i \in [C]} \sigma_i^{\text{image}}(\mathbf{x}_{\textit{ID}}) - \max_{i \in [C]} \sigma_i^{\text{image}}(\mathbf{x}_{\textit{OOD}})\right] \\
        \geq &  \Delta_{\text{text}}=\mathbb{E}\left[\max_{i \in [C]} \sigma_i^{\text{text}}(\mathbf{x}_{\textit{ID}}) - \max_{i \in [C]} \sigma_i^{\text{text}}(\mathbf{x}_{\textit{OOD}})\right]\,, \\
        & \text{s.t. } \sigma_i^{*} = \frac{\exp(s_{*,i})}{\sum_{k=1}^{N} \exp(s_k)}, s_{*,i} = cos(\mathbf{x}, P_{*,i}), \\
        & \ \ \ * \in [\textit{image}, \textit{text}] \,,
    \end{aligned}
\end{equation}
where $\mathbf{x}_{\textit{ID}}$ and $\mathbf{x}_{\textit{OOD}}$ are samples from any ID distributions and OOD samples.

First, for ID samples, with Assumption~\ref{ass: well trained}, we have
\begin{equation}
    \begin{aligned}
        \mathbb{E}\left[\max_{c\in[C]}\sigma_{c}^{*}(\mathbf{x}_{\textit{ID}})\right] &= \frac{\exp\left(\mu_{\text{intra}}^{*}\right)}{\exp\left(\mu_{\text{intra}}^{*}\right) + (C-1) \exp\left(\mu_{\text{inter}}^{*}\right)}, \\
        &\forall * \in \{\textit{text}, \textit{image}\}\,,
    \end{aligned}
\end{equation}
where $\mu_{\textit{intra}}^{*} = \max_{c\in[C]}\mathbb{E}[cos(\mathbf{x}_{\textit{ID}}, P_{*,c})]$, $\mu_{iter}^{*} = \mathbb{E}[cos(\mathbf{x}_{\textit{ID}}, I_{c_j})], \forall c_j \in [C] \setminus \{c_{\textit{ID}}\}$, and $C_{\textit{ID}}$ is the ground-truth class for the input $\mathbf{x}_{\textit{ID}}$.

With assumption~\ref{ass: OOD condition}, for OOD samples, we have,
\begin{equation}
    \mathbb{E}\left[\max_{c \in [C]} \sigma_c^{\text{image}}(\mathbf{x}_{\textit{OOD}})\right] = \mathbb{E}\left[\max_{c \in [C]} \sigma_c^{\text{text}}(\mathbf{x}_{\textit{OOD}})\right] = \frac{1}{C}\,.  
\end{equation}

Then we have,
\begin{equation}
    \Delta_{\text{image}} = \mathbb{E}\left[\max_{c\in[C]}\sigma_{c}^{\text{image}}(\mathbf{x}_i)\right] - \frac{1}{C}\,.
\end{equation}

Similarly, we have,
\begin{equation}
    \Delta_{\text{text}} = \mathbb{E}\left[\max_{c\in[C]}\sigma_{c}^{\text{text}}(\mathbf{x}_i)\right] - \frac{1}{C}\,.    
\end{equation}

Since $\mu_{\text{intra}}^{\text{image}} > \mu_{\text{intra}}^{\text{text}}$ (Assumption~\ref{ass: modality gap}), we have,
\begin{equation}
    \begin{aligned}
        \Delta_{\text{image}} & > \Delta_{\text{text}}  \,.        
    \end{aligned}
\end{equation}

By simple algebra, we have,
\begin{equation}
    \begin{aligned}
        &  \mathbb{E}\left[\frac{\max_{i \in [C]} \sigma_i^{\text{image}}(\mathbf{x}_{\textit{ID}}) + \max_{i \in [C]} \sigma_i^{\text{text}}(\mathbf{x}_{\textit{ID}})}{2} \right.\\
        & \left. - \frac{\max_{i \in [C]} \sigma_i^{\text{image}}(\mathbf{x}_{\textit{OOD}}) + \max_{i \in [C]} \sigma_i^{\text{text}}(\mathbf{x}_{\textit{OOD}})}{2}\right] \\
        \geq &  \mathbb{E}\left[\max_{i \in [C]} \sigma_i^{\text{text}}(\mathbf{x}_{\textit{ID}}) - \max_{i \in [C]} \sigma_i^{\text{text}}(\mathbf{x}_{\textit{OOD}})\right]\,, \\
        & \mathbb{E}\left[ S_{\textit{MMP}}(\mathbf{x}_{\textit{ID}}) - S_{\textit{MMP}}(\mathbf{x}_{\textit{OOD}}) \right] \\
        \geq & \mathbb{E}\left[ S_{\textit{MCM}}(\mathbf{x}_{ID}) - S_{\textit{MCM}}(\mathbf{x}_{\textit{OOD}}) \right]\,.
    \end{aligned}
\end{equation}

Now, we complete the proof.
