\section{Introduction}
\label{sec:intro}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{pics/teaser.pdf}
    \vspace{-0.7em}
    \caption{
    Standard VLM-based OOD detection methods~\cite{ming2022delving,miyai_locoop_2023,li_learning_2024} only utilize ID text prototypes ($\Diamond$) for identifying OOD samples. 
    In comparison, \ours employs ID image prototypes ($\square$) to complement ID text prototypes, reducing the impact of the image-text modality gap~\cite{liang2022mind} and sharpening the boundary between ID and OOD data. 
    ``Img.'' and ``Emb.'' represent image and embedding.
    }
    \label{fig: teaser}
\end{figure}


Detecting out-of-distribution (OOD) samples~\cite{hendrycks_baseline_2017,liang_enhancing_2020,liu_energy-based_2020,huang_mos_2021,fort_exploring_2021,Wang_2022_CVPR,sun_out--distribution_2022} is essential for the real-world deployment of machine learning models~\cite{heDeepResidualLearning2016,radford_learning_2021,yang_openood_2022}, as novel samples may emerge and should be flagged for careful consideration. 
Recently, inspired by the power of vision-language foundation models (VLMs)~\cite{radford_learning_2021,girdhar_imagebind_2023,Hess_2024_WACV}, novel approaches to OOD detection using VLMs~\cite{fort_exploring_2021,esmaeilpour_zero-shot_2022,ming2022delving,adaloglou_adapting_2023,liu_category-extensible_2023,miyai_locoop_2023,park_powerfulness_2023,wang_clipn_2023,bai_id-like_2024,cao_envisioning_2024,jiang2024negative,li_learning_2024,zhang_lapt_2024} have gained significant attention. 
Early VLM-based OOD detection works~\cite{ming2022delving,wang_clipn_2023,jiang2024negative,cao_envisioning_2024} mainly focus on using CLIP~\cite{radford_learning_2021} in a zero-shot setting, where only the in-distribution (ID) class names are utilized. 
For example, Maximum Concept Matching (MCM)~\cite{ming2022delving} 
measures the similarity between input images and text embeddings of ID classes, also referred to as ID \emph{text prototypes}, in the joint vision-language representation space of CLIP.
MCM then uses this similarity score to differentiate OOD and ID samples, on the basis that ID samples should have higher similarity scores. 

Following MCM, other zero-shot methods~\cite{wang_clipn_2023,jiang2024negative,cao_envisioning_2024} aim at generating OOD text prototypes by querying large language models (LLMs)~\cite{khattab_colbert_2020,openai_gpt-4_2023} or WordNet~\cite{DBLP:journals/cacm/Miller95}. 
These methods obtain OOD scores by evaluating the similarity difference between ID and OOD text prototypes.
To further improve performance, various few-shot tuning OOD detection methods~\cite{liu_category-extensible_2023,li_learning_2024,zhang_lapt_2024,bai_id-like_2024,nie_out--distribution_2024} have been proposed. 
These methods mainly target learning OOD (negative) text prototypes. 
For example, NegPrompt~\cite{li_learning_2024} learns OOD text prototypes by minimizing the similarity between OOD text prototypes and ID data with contrastive learning.

While these approaches have shown promise using CLIP, they use only text prototypes and thus are limited by the modality gap~\cite{liang2022mind} between image and text modalities. 
Liang \etal~\cite{liang2022mind} demonstrate that CLIPâ€™s image and text embeddings are located in two clearly separate regions of the embedding space.
As a result, OOD images may also exhibit high similarity to ID text prototypes, since high similarity can stem from either true semantic similarity or mere spatial closeness to the ID text prototypes, causing a risk of increased false positives. 
This observation raises a critical question about the impact of the modality gap on VLM-based OOD detection, leading to our first research question: 
    \begin{center}
        \textit{RQ1: What is the impact of the modality gap on VLM-based OOD detection methods?}
    \end{center}

In response to RQ1, we hypothesize that the modality gap negatively impacts performance and that incorporating image and text (multi-modal) prototypes, as opposed to only text prototypes, could alleviate this effect. We illustrate this in~\cref{fig: teaser}.
To explore this, we compute class-specific ID image prototypes by averaging embeddings extracted from the ID data 
within each class. 
We extend the MCM score $S_{\textit{MCM}}$~\cite{ming2022delving} to incorporate the image prototypes alongside the text prototypes. 
Compared to $S_{\textit{MCM}}$, our empirical analysis shows that, without any training, using the extension of $S_{\textit{MCM}}$ with multi-modal prototypes improves average FPR95 and AUROC from 32.4 and 94.5 to 24.2 and 95.8, respectively, on Imagenet-100~\cite{dengImageNetLargescaleHierarchical2009} (ID) across four OOD datasets (\cref{tab: MMA}).
For theoretical support, in \cref{the: image anchor helps}, we demonstrate that incorporating image prototypes increases the score separation between ID and OOD data, leading to improved performance. 

Given these findings, \ie, that the modality gap significantly affects VLM-based OOD detection, we conclude that it is necessary to explore approaches mitigating the gap to improve reliability. 
This leads us to our second research question:
\begin{center} 
    \textit{RQ2: How can we mitigate the impact of modality gap to improve VLM-based OOD detection?} 
\end{center}
To address RQ2, we propose a novel few-\textbf{\textsc{s}}hot tuning, m\textbf{\textsc{u}}lti-modal \textbf{\textsc{pr}}ototyp\textbf{\textsc{e}}-based \textbf{\textsc{me}}thod for OOD detection with CLIP, termed \ours. 
\ours comprises \BPG (BPG) and \ITC (ITC) modules. 
BPG introduces a Gaussian-based image domain bias to enhance image-text fusion and improve generalization.
ITC reduces the modality gap by minimizing inter- and intra-modal distances. 
\ours also makes use of a new OOD score, which we call the Generalized Multi-modal Prototype OOD score, denoted $S_{\textit{GMP}}$. 

\noindent
\textbf{BPG}. 
To improve multi-modal fusion and generalization, in contrast to previous OOD detection methods~\cite{li_learning_2024,miyai_locoop_2023,bai_id-like_2024} that only utilize learnable contexts for generating text prototypes, BPG conditions text prototypes on three components: learnable contexts, the image embedding, and the ID image domain bias. 
The image embedding and ID image domain bias facilitate image-text fusion, enhancing cross-modal alignment. 
The third component, the Gaussian-based ID image domain bias, %
captures the distribution of ID image embeddings to improve generalization. 

\noindent
\textbf{ITC}. 
To minimize the modality gap, we first map the image embedding $I$ to the text domain as $I^{'} = f_{i2t}(I)$ with the image-to-text mapping $f_{i2t}(\cdot)$. 
To ensure it aligns with ID text prototypes, we introduce the inter-modal loss $\ell_{\textit{inter}}$. 
Additionally, to avoid information loss, the intra-modal loss $\ell_{\textit{intra}}$ is applied between the original embedding $I$ and the reconstructed image embedding $\hat{I}=f_{t2i}(I^{'})$ using the text-to-image mapping $f_{t2i}(\cdot)$. 

\noindent
$\pmb{S_{\textit{GMP}}}$. 
Building on our findings from RQ1%
, we introduce a new OOD score, %
$S_{\textit{GMP}}$, which integrates uni- and cross-modal similarities. 
While previous methods rely solely on the similarity between ID text prototypes and input image embedding, $S_{\textit{GMP}}$ incorporates the similarity between multi-modal input embeddings---the image embedding $I$ and the mapped image embedding $f_{i2t}(I)$---and ID multi-modal (image and text) prototypes, respectively. 
This allows $S_{\textit{GMP}}$ to balance inter- and intra-modal similarities, enhancing robustness and performance.


Our contributions are summarized as follows:
\begin{itemize}
    \item 
        (RQ1) 
        We empirically and theoretically demonstrate that multi-modal (image and text) prototypes reduce the negative impact of the modality gap, resulting in performance improvements without additional training.
        

    \item 
        (RQ2) To further mitigate the modality gap, we propose a novel few-shot tuning framework, \ours, comprising \BPG (BPG) and \ITC (ITC) modules.
        
    \item 
        (RQ2) Building on our empirical and theoretical results, we design a new OOD score, $S_{\textit{GMP}}$, exploiting ID multi-modal prototypes and multi-modal input embeddings to enhance performance and robustness. 
        
    \item
        (RQ2) %
        Extensive experiments across multiple benchmarks, including ImageNet-1k~\cite{dengImageNetLargescaleHierarchical2009}, ImageNet-100~\cite{dengImageNetLargescaleHierarchical2009}, ImageNet-10~\cite{dengImageNetLargescaleHierarchical2009}, and ImageNet-20~\cite{dengImageNetLargescaleHierarchical2009}, demonstrate that \ours outperforms existing OOD detection methods and achieves new state-of-the-art performance. 
        We plan to release our code upon publication.
\end{itemize}
