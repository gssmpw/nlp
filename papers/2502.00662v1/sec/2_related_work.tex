\section{Related Work}

\textbf{Out-of-distribution (OOD) detection.}
OOD detection~\cite{hendrycks_baseline_2017,huang_importance_2021,liu_energy-based_2020,yang_openood_2022,zhang_openood_2023} aims to discriminate ID samples and OOD samples. 
Traditional methods include post-hoc~\cite{sun_react_2021,lee_simple_2018,park_nearest_2023,sun_out--distribution_2022}, confidence enhancement~\cite{devries_learning_2018,hein_why_2019,wei_mitigating_2022,xu_vra_2023,narasimhan_plugin_2023}, and outlier exposure methods~\cite{hendrycks_deep_2018,jiang_dos_2023,du_dream_2023,kieuOutlierDetectionMultidimensional2018,huang_mos_2021,liu_residual_2023,zhou_learning_2021,fort_exploring_2021}, by estimating the OOD probability using logits~\cite{hendrycks_baseline_2017,huang_importance_2021,liu_energy-based_2020,liang_enhancing_2020} or feature representations~\cite{Wang_2022_CVPR,sun_react_2021}. 
Recently, the field has shifted towards exploiting large-scale pre-trained VLMs~\cite{radford_learning_2021,girdhar_imagebind_2023,Hess_2024_WACV}, \ie, CLIP~\cite{radford_learning_2021}, to enhance OOD detection, transitioning from uni-modal to multi-modal approaches. 
MCM~\cite{ming2022delving}
employs CLIP and explores the effects of softmax and temperature scaling in a zero-shot manner. 
Subsequent works can be roughly categorized into zero-shot and few-shot tuning methods. 
Zero-shot methods~\cite{ming2022delving,jiang2024negative,cao_envisioning_2024,wang_clipn_2023} primarily focus on identifying negative classes by querying LLMs~\cite{khattab_colbert_2020,openai_gpt-4_2023} or WordNet~\cite{DBLP:journals/cacm/Miller95}. 
In parallel, few-shot tuning methods~\cite{liu_category-extensible_2023,li_learning_2024,zhang_lapt_2024,bai_id-like_2024,nie_out--distribution_2024} also mainly target finding negative text prompts (OOD text prototypes). 
For example, CATEX~\cite{liu_category-extensible_2023}, LSN~\cite{nie_out--distribution_2024}, and NegPrompt~\cite{li_learning_2024} learn negative prompts by minimizing the similarity between negative prompts and ID training data. 

In contrast to previous VLM-based methods that use only text prototypes, \ours enhances OOD detection by employing multi-modal prototypes and encouraging cross-modal alignment
to reduce the modality gap. 

\noindent
\textbf{Prompt tuning}. 
Prompt tuning originates in Natural Language Processing~\cite{lester_power_2021,li_prefix-tuning_2021,shin_autoprompt_2020} as a method for automating template/prompt creation in models such as BERT~\cite{devlinBERTPretrainingDeep2019} and GPT~\cite{openai_gpt-4_2023}. 
For example, AutoPrompt~\cite{shin_autoprompt_2020} is a gradient-based approach for identifying ``optimal'' prompts, replacing manually designed prompts. 
By adapting prompts in the input embedding space based on downstream data, prompt tuning offers a parameter-efficient alternative to fine-tuning.
Recently, prompt tuning has been applied in computer vision models~\cite{khattak_self-regulating_2023,zhou2022cocoop,zhou_learning_2022}.
Notably, CoOp~\cite{zhou_learning_2022} and CoCoOp~\cite{zhou2022cocoop}, as representative methods in visual prompt tuning (VPT), employ learnable prompts optimized by minimizing classification loss to improve CLIP's performance. 

Different from previous VPT works~\cite{zhou2022cocoop,zhou_learning_2022,miyai_locoop_2023,liu_category-extensible_2023,bai_id-like_2024,nie_out--distribution_2024} that generate text prompts with text and individual image data, \ours generates text prompts/prototypes with a novel Gaussian-based image domain bias for estimating the distribution of image data, which enables enhanced image-text fusion with improved generalization ability.
