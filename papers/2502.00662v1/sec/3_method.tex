\section{Preliminaries}

\subsection{Problem Setup}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{pics/main.pdf}
    \vspace{-2.3em}
    \caption{
    Overview of \ours. 
    The two novel modules, \ie, BPG (\BPG) and ITC (\ITC), are designed to minimize the modality gap. 
    1) During the few-shot fine-tuning stage, BPG generates image domain-biased prompts, conditioned on the estimated image domain bias and the mapped image embedding $m(I)$ for better image-text fusion.
    ITC minimizes the modality gap directly by the intra- and inter-modal losses ($\ell_{\textit{intra}}$ and $\ell_{\textit{inter}}$) with the text-to-image mapping $f_{t2i}(\cdot)$ and the image-to-text mapping $f_{i2t}(\cdot)$. 
    2) During inference, image prototypes are obtained by averaging each class's base ID image embeddings (\cref{eq: image anchor}). 
    The proposed $S_{\textit{GMP}}$ (\cref{eq: MMO}) is calculated based on the maximum similarity between the multi-modal embeddings (the image embedding $I_t$ and mapped image embedding $I_t^{'}$) and ID multi-modal prototypes (\ie, text prototypes $\{P_{t,c}\}_{c\in[C]}$ and image prototypes $\{P_{i,c}\}_{c\in[C]}$). 
    $S_{\textit{MCM}}$ refers to the MCM score~\cite{ming2022delving}.
    }
    \label{fig: main figure}
\end{figure*}

Let $\mathcal{X}$ and $\mathcal{Y}=\{y_{1}, \ldots, y_{C}\}$ represent the feature space and ID label space, respectively, where $C$ is the number of ID classes. 

\noindent \textbf{OOD detection}. 
In real-world applications, AI models are trained on ID data and may misclassify OOD data into ID classes with high confidence~\cite{joseph_towards_2021}. 
To tackle this problem, OOD detection~\cite{yang_generalized_2024} is proposed to identify OOD samples using a score function $S(\cdot)$~\cite{li_rethinking_2023,li_moodv2_2024}. 
A sample is classified as OOD if $S(\mathbf{x}) \leq \gamma$, where $\gamma$ is a predefined threshold. 

\noindent \textbf{Few-shot tuning}. 
In contrast to existing works that either utilize the entire ID training dataset~\cite{li_tagood_2024,Wang_2022_CVPR,lee_simple_2018} or avoid tuning entirely~\cite{wang_clipn_2023,ming2022delving,cao_envisioning_2024,jiang2024negative}, we study the scenario where the model is fine-tuned using only a subset of the ID training data (16 images per class) {without} access to OOD data or other additional data. 

\subsection{Revisiting CLIP, MCM, and Prompt Tuning}

\textbf{CLIP~\cite{radford_learning_2021}} is a foundational VLM pre-trained on a web-scale image-text dataset using self-supervised contrastive learning~\cite{DBLP:conf/icml/ChenK0H20}, which achieves strong zero-shot classification results on numerous benchmark datasets~\cite{krizhevskyImagenetClassificationDeep2012,li_caltech_2022,bossardFood101MiningDiscriminative2014,majiFineGrainedVisualClassification2013,parkhi_cats_2012}. 
Specifically, for classification, CLIP first incorporates class labels $\mathcal{Y} = \{y_{c}\}_{c\in[C]}$, \eg, ``cat'' and ``dog'', into fixed pre-designed, rather than learned, text prompts, \eg, ``\texttt{a photo of a [CLASS]}''.
These prompts combined with class names %
are processed by CLIPâ€™s text encoder $f_{\textit{text}}(\cdot)$ to generate text embeddings $\{P_{t,c}\}_{c\in[C]}$, where $P_{t,c} = f_{\textit{text}}(\texttt{a photo of a }y_c)$. 
Given an image $\mathbf{x}$, the image embedding $I$ is obtained by the image encoder $f_{\textit{image}}(\cdot)$ as $I = f_{\textit{image}}(\mathbf{x})$. 
The predicted class label is $\hat{y} = \argmax_{c \in [C]} \operatorname{cos}(I, P_{t,c})$, where $\operatorname{cos}(\cdot, \cdot)$ denotes the cosine similarity. 
The zero-shot classification score is calculated as $p = \stmx([\operatorname{cos}(I, P_{t,1}), \ldots, \operatorname{cos}(I, P_{t,C}])$, where $\stmx(\cdot)$ is the softmax function. 

\noindent
\textbf{CLIP for OOD Detection (MCM)~\cite{ming2022delving}}.
Beyond its strong classification capabilities, MCM demonstrates that pre-trained CLIP models also exhibit robust zero-shot OOD detection capabilities. 
Specifically, MCM defines the maximum classification score as the OOD score $S_{\textit{MCM}}$,
\begin{equation}
\label{eq: MCM}
    S_{\textit{MCM}} (I ,\{P_{t,c}\}_{c\in[C]}) = \max_{c\in [C]} \frac{\exp(\operatorname{cos}(I, P_{t,c}) / \tau)}{\sum_{j\in[C]} \exp(\operatorname{cos}(I, P_{t,j}) / \tau)},
\end{equation}
where $\tau$ is the temperature, and $\exp(\cdot)$ is the exponential function.

\noindent
\textbf{Visual prompt tuning~\cite{zhou_learning_2022,zhou2022cocoop,guo_pfedprompt_2023,yao_visual-language_2023}}. 
To improve CLIP's performance when target training data is accessible, CoOp~\cite{zhou_learning_2022} replaces manually designed prompt templates with learnable (soft) prompts as $\textit{CoOp}_c=[V_1, \ldots, V_L, y_c]$, where $c \in [C]$, $L$ is the length of learnable prompts, and each $V_*$ represents a learnable vector. 
The class-wise text embedding is then generated as $P^{\textit{CoOp}}_{t, c}=f_{\textit{text}}(\textit{CoOp}_c), \forall c \in [C]$. 
With CLIP's text and image encoder parameters frozen, the learnable prompts are optimized using the cross-entropy loss,
\begin{equation}
    \ell_{\textit{ID}} = -\log \frac{\exp({\operatorname{cos}(I, P_{t, y}^{\textit{CoOp}} })/ \tau)}{\sum_{c \in [C]} \exp({\operatorname{cos}(I, P_{t, c}^{\textit{CoOp}}})/\tau)}\,,
\end{equation}
where $y$ is the ground truth label for the input image. 
Following CoOp, to avoid potential overfitting on the training data, CoCoOp~\cite{zhou2022cocoop} 
conditions prompts on the image embedding $I$ as $\textit{CoCoOp}_c=[V_1 + m(I),\ldots, V_L+m(I), y_c]$, 
where $m(\cdot)$ is a two-layer multi-layer perception (MLP, Linear-ReLU-Linear).

\section{Methodology}

In this section, we first answer RQ1 by presenting our hypothesis and findings in \cref{Sec: MMA}, where we demonstrate that the modality gap negatively impacts performance and that incorporating image prototypes alleviates this effect.
Then, to mitigate the modality gap (RQ2), we introduce \ours in \cref{sec: our method} with two modules and a novel OOD score, $S_{\textit{GMP}}$, as shown in \cref{fig: main figure}. 
Note that in this section, we denote the input image embedding by $I_t$.

\subsection{RQ1: Multi-modal Prototypes}
\label{Sec: MMA}

CLIP has been widely applied in different areas~\cite{tschannen_clippo_2023,gao_clip2tv_2022}. 
However, a recent study~\cite{liang2022mind} demonstrates that representations of images and texts are clearly separated, creating what is known as the modality gap. 
It remains unclear, however, whether this modality gap positively or negatively impacts OOD detection.

We hypothesize that the modality gap negatively impacts OOD detection performance by causing misalignment between image and text embeddings, and that this misalignment can lead to increased false positives, as OOD images may exhibit high similarity to ID text prototypes due to either semantic similarity or small spatial distance. 

To test our hypothesis, we demonstrate that incorporating multi-modal (image and text) prototypes can mitigate the negative impact of the modality gap and reduce false positives. 
We obtain ID text prototypes $\{P_{t,c}\}_{c\in[C]}$ as in MCM~\cite{ming2022delving}. 
For ID image prototypes $\{P_{i,c}\}_{c\in[C]}$, we collect ID base images $\mathcal{X}_{\textit{base}} = \{\mathbf{x}_i\}_{i \in [N_{\textit{base}}]}$ with corresponding labels $\{y_{\textit{base}, i}\}_{i \in [N_{\textit{base}}]}$, where $N_{\textit{base}}$ is the number of base images. 
ID image prototypes are then calculated by averaging the image embeddings for each class,
\begin{equation}
\small
    \label{eq: image anchor}
    P_{i,c} = \frac{\sum_{i \in [N_{\textit{base}}]} \mathbb{I}(y_{\textit{base},i}=c) f_{\textit{image}}(\mathbf{x}_{i})}{\sum_{j \in [N_{\textit{base}}]} \mathbb{I}(y_{\textit{base},j}=c)} , \forall c \in [C]\,,
\end{equation}
where $\mathbb{I}(\cdot)$ is the indicator function and $f_{\textit{image}}(\cdot)$ is CLIP's image encoder. 
With these image prototypes, we extend $S_{\textit{MCM}}$ (\cref{eq: MCM})~\cite{ming2022delving} to a new OOD score, termed the Multi-Modal Prototypes score, $S_{\textit{MMP}}$, defined as,
\begin{equation}\label{eq: MMA}
\small
    S_{\textit{MMP}} %
    = \frac{S_{\textit{MCM}} (I_t, \{P_{i,c}\}_{c\in[C]}) + S_{\textit{MCM}} (I_t ,\{P_{t,c}\}_{c\in[C]})}{2} \,.
\end{equation}
This approach balances contributions from both modalities, potentially mitigating the impact of the modality gap.

\begin{table}[t!]
\centering
\begin{tabular}{l|cc}
\toprule
Methods    & FPR95 (\%) $\downarrow$   & AUROC (\%) $\uparrow$  \\
\midrule
\rowcolor[gray]{.9} \multicolumn{3}{c}{ImageNet-100 $\rightarrow$ 4 Datasets (Average)} \\
$S_{\textit{MCM}}$~\cite{ming2022delving}        & 32.58                & 94.48             \\
\rowcolor{green!10} $S_{\textit{MMP}}$ (Ours, \cref{eq: MMA})        & \textbf{24.18}                & \textbf{95.79}            \\
\bottomrule
\end{tabular}%
\vspace{-1em}
\caption{
Comparison of OOD detection results on ImageNet-100 (ID) $\rightarrow$ 4 OOD datasets (OOD). 
We use CLIP-B/16 for $S_{\textit{MCM}}$ and $S_{\textit{MMP}}$. 
FPR95 represents the false positive rate of OOD images when the true positive rate of ID images is at 95\%. 
AUROC is the area under the
receiver operating characteristic curve.
Best in \textbf{bold}. Full results are in the Appendix.
}\label{tab: MMA}
\vspace{-0.6em}
\end{table}

\noindent
\textbf{Empirical evidence}. 
We begin by empirically validating the effectiveness of image prototypes and $S_{\textit{MMP}}$. 
We use ImageNet-100~\cite{dengImageNetLargescaleHierarchical2009} as the ID dataset, with four OOD datasets:  iNaturalist~\cite{van_horn_inaturalist_2018}, SUN~\cite{xiao_sun_2010}, Places~\cite{zhou_places_2018}, and Texture~\cite{cimpoi_describing_2014}. 
With all training images of ImageNet-100 as the base set, we observe in \cref{tab: MMA} that, on average, $S_{\textit{MMP}}$ outperforms $S_{\textit{MCM}}$ in both FPR95 and AUROC, demonstrating the effectiveness of image prototypes and $S_{\textit{MMP}}$.

\noindent
\textbf{Theoretical evidence.} 
To further demonstrate why image prototypes improve performance, we present a theoretical justification in \cref{the: image anchor helps}. 
The theorem shows that $S_{\textit{MMP}}$ increases the expected score separation between ID and OOD samples compared to using text prototypes alone as in $S_{\textit{MCM}}$. 
The proof is presented in the Appendix.
\begin{theorem}[Multi-modal Prototypes Increase Score Separation between ID and OOD Data]\label{the: image anchor helps} 
    Assuming that the %
    OOD data is not drawn from any ID distribution, we have,
    \begin{equation}
    \small
        \begin{aligned}
            & \mathbb{E}\left[ S_{\textit{MMP}}(I_{\textit{ID}}) - S_{\textit{MMP}}(I_{\textit{OOD}}) \right]             \geq \mathbb{E}\left[ S_{\textit{MCM}}(I_{\textit{ID}}) - S_{\textit{MCM}}(I_{\textit{OOD}}) \right]\,,
        \end{aligned}
    \end{equation}
    where $I_{\textit{ID}}$ and $I_{\textit{OOD}}$ are the image embeddings of ID and OOD samples. We omit multi-modal prototypes for clarity. %
\end{theorem}

While incorporating image prototypes improves performance both empirically and theoretically, inspired by recent research~\cite{miyai_locoop_2023,li_learning_2024}, we further fine-tune CLIP 
in a few-shot tuning manner to mitigate the impact of the modality gap (RQ2). The details are presented in the following section.

\subsection{RQ2: \ours}\label{sec: our method}

To alleviate the negative impact of the image-text modality gap~\cite{liang2022mind} in CLIP (RQ2), we propose \ours, which is summarized in \cref{fig: main figure}. 
\ours incorporates two novel modules, \ie, the {\BPG} module (BPG) and the {\ITC} module (ITC), along with a new OOD score. 
BPG enhances image-text fusion using image domain-biased prompts, while ITC minimizes both intra- and inter-modal distances. 
\ours's novel OOD score, $S_{\textit{GMP}}$, leverages uni- and cross-modal similarities for improving robustness and performance. 

\noindent
\paragraph{Biased Prompts Generation (BPG).}

Drawing inspiration from CoCoOp~\cite{zhou_learning_2022}, we employ a set of learnable contexts $\{V_i\}_{i \in [L]}$ and the mapped image embedding $m(I_t)$ to generate the text prototypes, where $L$ is the length of learnable contexts and $m(\cdot)$ is a two-layer MLP.

To enhance image-text fusion and improve generalization, we introduce a third component, a Gaussian-based estimated image domain bias $\mathcal{N}(\mathbf{\mu}, \Sigma)$, for generating the prompts and ID text prototypes.
This bias models the distribution of the mapped ID image embeddings, enabling the model to better distinguish between ID and OOD samples during inference. 
After that, we generate image domain-biased prompts (IDBP) conditioned on these three components. 
We next introduce the details of estimating the bias and generating IDBP.

\noindent 
\textbf{Estimating image domain bias}. 
In each training iteration, we sample $\mathbf{b}$ from the Gaussian-based image domain bias $\mathcal{N}(\mathbf{\mu}, \Sigma)$ as,
\begin{equation}
    \small
    \mathbf{b} = \mu + \sigma n, n\sim \mathcal{N}(0, \mathcal{I}),
\end{equation}
where $\Sigma = \sigma \sigma^\top$ (based on the Cholesky decomposition) and $\mathcal{N}(0, \mathcal{I})$ represents the standard Gaussian distribution, given identity matrix $\mathcal{I}$.
To align the domain bias with the distribution of training ID image embeddings, we employ the following loss,
\begin{equation}
\small
\label{loss: bias}
    \ell_{\textit{bias}} = \| \mu - m(I_t)\|_1 + \|\mathbf{b} - m(I_t)\|_1\,,
\end{equation}
where $\|\cdot\|_1$ is the $\ell_1$ distance.

\noindent 
\textbf{Generating image domain-biased prompts (IDBP)}. 
We condition our IDBP on three components: the learnable contexts $\{V_{i}\}_{i\in[L]}$, mapped image embedding $m(I_t)$, and a sample $\mathbf{b}$ from the estimated domain bias $\mathcal{N}(\mathbf{\mu}, \Sigma)$ as,
\begin{equation}
    \small
    \textit{IDBP}_c = [V_1 + m(I_t) + \mathbf{b}, \ldots, V_L + m(I_t) + \mathbf{b}, y_c]\,.
\end{equation}
The final ID text prototype $P_{t, c}$ for the $c$-th class is obtained as $P_{t, c} = f_{\textit{text}} (\textit{IDBP}_c)$.

We thus establish a foundation for cross-modal fusion by conditioning prompts on learned contexts, image embeddings, and the estimated image domain bias. 

\noindent
\paragraph{Image-Text Consistency (ITC).}
While BPG reduces the gap by image-text fusion, image-text consistency (ITC) directly reduces the modality gap by aligning image and text embeddings through inter- and intra-modal distances at a fine-grained level.
It consists of two mappings, \ie, the image-to-text mapping $f_{i2t}(\cdot)$ and text-to-image mapping $f_{t2i}(\cdot)$, which are used for mapping embeddings into another domain/modality. 

The first component of ITC is the \emph{inter-modal loss}. 
It is designed to ensure that mapped text/image embeddings ($f_{t2i}(P_{t,c}), \forall c \in [C]$ and $f_{i2t}(I_t)$) remain close to their original counterparts, thus enabling effective alignment. 
It is designed to minimize the cross-entropy loss,
\begin{align}
\small
\label{loss: inter}
    \ell_{\textit{inter}} = & -\log \frac{\exp({\operatorname{cos}(I_t, f_{t2i}(P_{t,y})) / \tau})}{\sum_{c \in [C]} \exp({\operatorname{cos}(I_t, f_{t2i}(P_{t,c})) / \tau})} &\\
    &- \log \frac{\exp({\operatorname{cos}(f_{i2t}(I_t), P_{t,y}) / \tau})}{\sum_{c \in [C]} \exp({\operatorname{cos}(f_{i2t}(I_t), P_{t,c}) / \tau})} \,,\nonumber
\end{align}
where $y$ is the ground truth label for the input image, $C$ is the number of classes, and $\tau$ is the temperature. 

Next, to prevent information loss, we introduce \emph{intra-modal loss}. 
Specifically, we reconstruct the original embedding by mapping the mapped embedding back to its initial modality.
Then, we minimize the $\ell_1$ distance between the reconstructed and original embeddings as follows,
\begin{multline}
\small
\label{loss: intra}
\ell_{\textit{intra}} = \|I_t - f_{t2i}(f_{i2t}(I_t)) \|_1 \\
    + \sum_{c \in [C]} \| P_{t,c} - f_{i2t}(f_{t2i} (P_{t,c})) \|_1 \,.
\end{multline}

The inter- and intra-modal losses serve to reduce the modality gap between image and text, and to preserve all information during mapping. 
To avoid overfitting, instead of using deep MLPs, we employ linear transformations without bias as the mappings ($f_{i2t}(\cdot)$ and $f_{t2i}(\cdot)$). 

Combining BPG with ITC, \ours mitigates the modality gap by enhancing image-text fusion (BPG) as well as reducing inter- and intra-modal distances (ITC).





\noindent
\paragraph{Training, Inference, and $S_{\textit{GMP}}$.}
Our overall training objectives are a linear combination of the four losses: $\mathcal{L} = \ell_{\textit{ID}} + \alpha( \ell_{\textit{intra}} + \ell_{\textit{inter}}) + \beta \ell_{\textit{bias}}$,
where $\alpha$ and $\beta$ are trade-off parameters. 
During inference, we use the mean $\mu$ of the image domain bias as the sample $\mathbf{b}$ to generate image domain-biased prompts. 


\noindent
\textbf{Generalized multi-modal prototypes OOD score $S_{\textit{GMP}}$.}
Building on our empirical and theoretical findings in \cref{Sec: MMA}, with the image-to-text mapping $f_{i2t}(\cdot)$, we introduce the generalized multi-modal prototypes OOD score $S_{\textit{GMP}}$. 
$S_{\textit{GMP}}$ uses the average of the maximum similarities between the multi-modal input embedding (the image embedding $I_t$ and mapped image embedding $I_t^{'} = f_{t2i}(I_t)$) and multi-modal prototypes:
    \begin{align}
    \small
        & S_{\textit{GMP}} (I_t, I_t^{'}, \{P_{t, c}\}_{c\in[C] }, \{P_{i, c}\}_{c\in[C]}) \label{eq: MMO} \\
        = & \frac{1}{4} (S_{\textit{MCM}}(I_t,\{P_{t, c}\}_{c\in[C]}) + S_{\textit{MCM}}(I_t,\{P_{i, c}\}_{c\in[C]}) \nonumber \\
        + & S_{\textit{MCM}}(I_t^{'},\{P_{t, c}\}_{c\in[C]}) + S_{\textit{MCM}}(I_t^{'},\{P_{i, c}\}_{c\in[C]})) \nonumber \,.
    \end{align}

\begin{remark}
    $S_{\textit{GMP}}$ extends $S_{\textit{MCM}}$ by incorporating both uni- and cross-modal similarities. 
    $S_{\textit{GMP}}$ balances contributions from both modalities, thus mitigating the negative impact of the modality gap. 
    This approach enhances the separation between ID and OOD samples, improving the OOD detection performance of VLM-based methods as shown in \cref{fig: ablations: scores}.
\end{remark}


