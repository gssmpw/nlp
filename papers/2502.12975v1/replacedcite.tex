\section{Related Work}
\label{sec:related}

\noindent\textbf{Image-based moving object segmentation} \Fix{aims to segment moving objects from video frames. 
It is a challenging task in computer vision with various applications}
____. 
FgSegNet____ propose to extract multi-scale features for foreground segmentation. 
____ propose to jointly solve video deblurring, scene flow estimation, and MOS in a unified framework. 
GraphMOS____ put forward the concepts of graph signal processing for MOS. 
MOVE____ proposes to segment movable objects from a single image without any form of supervision.
____ present an auto-encoder-based framework for unsupervised object discovery.
____ incorporate weakly-supervised contrastive learning to enhance texture information.
However, these methods only separate the AMOs from the background, while some recent methods also segment IMOs, \ie, address the InsMOS task.
____ propose to segment IMOs by predicting instance-specific 3D scene flow maps and instance masks from stereo videos.
____ introduce the expectation maximization framework to unsupervised motion segmentation from optical flow. 
ZBS____ proposes an instance-level background model based on zero-shot object detection. 
____ propose dual-stream transformers to segment IMOs by fusing image and optical flow. 
DeepFTSG____ incorporates single and multi-stream network blocks to discriminate salient moving objects. 
Despite the promising results, these methods usually cannot accurately model pixel-level movements, \eg, simultaneous multi-object and camera motion, and thus are not robust to complex multi-object dynamics. 


\Fix{\noindent\textbf{Image-based video object segmentation.} 
Image-based video object segmentation (VOS) and video instance segmentation (VIS) have been studied extensively over the years____. 
The tasks of zero-shot VOS (ZVOS, also known as unsupervised VOS) and VIS are also closely related to our InsMOS task. 
The goal of ZVOS is to segment all objects in a video without any guidance (\eg, initial mask or text prompt). 
Early efforts directly rank segments and regress foreground objects____, with subsequent pixel-level embedding____ and long-term context encoding____ becoming popular. 
}
MATNet____ attempts to incorporate object appearance and motion information. 
Isomer____ explores the benefit of the long-range dependency modeling capacity of the transformer. 
____ presents a multi-source predictor with RGB, optical flow, depth, and saliency map as inputs. 
\Fix{
However, these methods typically only segment salient foreground areas without identifying individual objects, which are not suitable for video analysis that requires fine-grained analysis of varying numbers of objects. 
}
\Fix{Furthermore, the VIS task is more challenging as it requires not only segmenting foreground regions but also separating different object instances. 
RVOS____ takes a recurrent neural network to recover frame-by-frame instances and associate cross-frame instances.
}
VisTR____ proposes a direct end-to-end framework built upon transformers. 
____ explore the generalization of the instance segmentation structure Mask2Former____ for VIS.
IDOL____ develops an online framework to reduce the performance gap with the offline paradigm. 
\Fix{Recently, UVIS____ explores the superior generalization and open-set transfer capabilities that visual language models and self-supervised vision models bring to VIS. 
Despite these advances, these methods typically deal with all foreground objects in a scene and lack the direct discriminative ability of whether an object is moving or not. 
In video analysis of complex dynamic scenes including cases where a static object is still accompanied by the same pixel displacements corresponding to the camera, it remains an open challenge to exclude camera motion interference and accurately recognize the attribute of whether an object is moving or not, which is exactly the problem that instance-level moving object segmentation aims to solve. 
}



\noindent\textbf{Event-based motion estimation.} This task is developed to extract motion information from brightness changes in event data____.
Some early optimization-based methods estimate the motion with events only____, but their results mainly focus on the moving boundary rather than the whole motion field because event data are spatially sparse. 
For the same reason, recent learning-based methods____ that use only events to directly regress the optical flow are difficult to produce reliable dense results____. 
Therefore, there is a new trend to combine event data with other modalities, such as images____, 
depth____ and point clouds____, \Fix{to take advantage of} complementary multimodal data. 
Given these successes in introducing events for motion estimation, we explore and fully illustrate the role of combining image and event data for InsMOS. 


\begin{figure*}[tbp]
    \centering
    \includegraphics[width=\textwidth]{2_figure_framework.pdf}
    \caption{ 
    \textbf{Our proposed InsMOS framework combines a single image with its subsequent events}. 
    The network pipeline is divided into three parts:
    1) The cross-modal masked attention augmentation (CMA) module interactively augments texture and motion representations with an additional contrastive feature learning mechanism applied in training. 
    2) Masks and motion embeddings are decoded separately, allowing for thresholding instance-level segmentation outputs. 
    Since the training loss is applied on full embeddings, this thresholding step only needs to be performed during inference. 
    3) The flow-guided motion feature enhancement module is designed to enhance motion feature learning during training. 
    }  
    \label{fig:framework}
\end{figure*}

\noindent\textbf{Event-based moving object segmentation.} This task aims to segment IMOs from events overlaid with object and camera motion____. 
The contrast maximization framework____ separates camera motion by maximizing the contrast in warped event images and has been extended to achieve globally optimal motion estimation____ and multiple object segmentation____. 
EMSGC____ utilize a spatiotemporal graph representation of events and employ graph cuts for segmentation.
\FixCite{____} introduce a progressive motion segmentation framework that jointly optimizes motion estimation and event denoising. 
Recently, learning-based methods usually directly regress the AMOs____, so they can only determine which positions are moving but cannot separate different IMOs. 
____ propagate object motion through tracklet decomposition and cluster merging. 
RENet____ fuses images and events to detect moving objects, predicting object bounding boxes rather than pixel-level segmentation. 
JSTR____ proposes to incorporate IMU data and treat events as spatial and temporal 3D point clouds represented by 2D coordinates and timestamps. 
____ propose incorporating semantic and 3D depth information for MOS, while ____ present a multi-stage divide-and-conquer framework based on ego-motion and optical flow estimation. 



Previous event-based MOS methods only utilize events as input, potentially leading to the incorrect identification of nearby multiple objects as a single large object due to the absence of texture information in events.
In contrast, we propose for the first time an InsMOS framework to segment all IMOs by exploiting the texture and motion information in a single image and events.