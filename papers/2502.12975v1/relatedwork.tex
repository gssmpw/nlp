\section{Related Work}
\label{sec:related}

\noindent\textbf{Image-based moving object segmentation} \Fix{aims to segment moving objects from video frames. 
It is a challenging task in computer vision with various applications}
~\citep{mos:tokmakov_learning2seg_IJCV_2019}. 
FgSegNet~\citep{cd:lim_fgsegnet_PRL_2018, cd:lim_fgsegnetv2_PAA_2020} propose to extract multi-scale features for foreground segmentation. 
\cite{mos:pan_jointstereomos_TIP_2020} propose to jointly solve video deblurring, scene flow estimation, and MOS in a unified framework. 
GraphMOS~\citep{mos:giraldo_graphmos_TPAMI_2020} put forward the concepts of graph signal processing for MOS. 
MOVE~\citep{mos:bielski_move_NeurIPS_2022} proposes to segment movable objects from a single image without any form of supervision.
~\cite{objdis:bao_discovering_CVPR_2022} present an auto-encoder-based framework for unsupervised object discovery.
~\cite{objdis:lv_weaklycontrastive_arXiv_2023} incorporate weakly-supervised contrastive learning to enhance texture information.
However, these methods only separate the AMOs from the background, while some recent methods also segment IMOs, \ie, address the InsMOS task.
\cite{mos:cao_independent_stereoscopic_CVPR_2019} propose to segment IMOs by predicting instance-specific 3D scene flow maps and instance masks from stereo videos.
\cite{mos:Meunier_EMMOS_TPAMI_2023} introduce the expectation maximization framework to unsupervised motion segmentation from optical flow. 
ZBS~\citep{cd:an_zbs_CVPR_2023} proposes an instance-level background model based on zero-shot object detection. 
\cite{mos:homeyer_mostrans_ICCVW_2023} propose dual-stream transformers to segment IMOs by fusing image and optical flow. 
DeepFTSG~\citep{cd:rahmon_deepftsg_IJCV_2024} incorporates single and multi-stream network blocks to discriminate salient moving objects. 
Despite the promising results, these methods usually cannot accurately model pixel-level movements, \eg, simultaneous multi-object and camera motion, and thus are not robust to complex multi-object dynamics. 


\Fix{\noindent\textbf{Image-based video object segmentation.} 
Image-based video object segmentation (VOS) and video instance segmentation (VIS) have been studied extensively over the years~\citep{zhou_SurveyVideoSeg_TPAMI_2022, gao_deepvosreview_AIR_2023}. 
The tasks of zero-shot VOS (ZVOS, also known as unsupervised VOS) and VIS are also closely related to our InsMOS task. 
The goal of ZVOS is to segment all objects in a video without any guidance (\eg, initial mask or text prompt). 
Early efforts directly rank segments and regress foreground objects~\cite{vos:fragkiadaki2015learning}, with subsequent pixel-level embedding~\citep{vos:li2018instance} and long-term context encoding~\citep{vos:lu2019see} becoming popular. 
}
MATNet~\citep{vos:zhou_motiontrans_AAAI_2020} attempts to incorporate object appearance and motion information. 
Isomer~\citep{vos:yuan_isomer_ICCV_2023} explores the benefit of the long-range dependency modeling capacity of the transformer. 
~\cite{vos:zhao_adaptivezvos_ijcv_2024} presents a multi-source predictor with RGB, optical flow, depth, and saliency map as inputs. 
\Fix{
However, these methods typically only segment salient foreground areas without identifying individual objects, which are not suitable for video analysis that requires fine-grained analysis of varying numbers of objects. 
}
\Fix{Furthermore, the VIS task is more challenging as it requires not only segmenting foreground regions but also separating different object instances. 
RVOS~\citep{vos:ventura2019rvos} takes a recurrent neural network to recover frame-by-frame instances and associate cross-frame instances.
}
VisTR~\citep{vis:wang_endtransformer_CVPR_2021} proposes a direct end-to-end framework built upon transformers. 
~\cite{vis:cheng_mask2vis_arXiv_2021} explore the generalization of the instance segmentation structure Mask2Former~\citep{vis:cheng_mask2former_CVPR_2022} for VIS.
IDOL~\citep{vis:wu_defenseIDOL_ECCV_2022} develops an online framework to reduce the performance gap with the offline paradigm. 
\Fix{Recently, UVIS~\citep{vos:huang_uvis_CVPR_2024} explores the superior generalization and open-set transfer capabilities that visual language models and self-supervised vision models bring to VIS. 
Despite these advances, these methods typically deal with all foreground objects in a scene and lack the direct discriminative ability of whether an object is moving or not. 
In video analysis of complex dynamic scenes including cases where a static object is still accompanied by the same pixel displacements corresponding to the camera, it remains an open challenge to exclude camera motion interference and accurately recognize the attribute of whether an object is moving or not, which is exactly the problem that instance-level moving object segmentation aims to solve. 
}



\noindent\textbf{Event-based motion estimation.} This task is developed to extract motion information from brightness changes in event data~\citep{eventflow:Bardow_simultaneousflowintensity_2016}.
Some early optimization-based methods estimate the motion with events only~\citep{eventmos:Gallego_Unifyingcontrastmax_cvpr_2018, eventapp:gehrig_eklt_IJCV_2020}, but their results mainly focus on the moving boundary rather than the whole motion field because event data are spatially sparse. 
For the same reason, recent learning-based methods~\citep{eventflow:Zhu_EVFlowNet_CVPR_2019,eventflow:Gehrig_DenseRAFTFlow_3DV_2021, eventflow:ponghiran_denceflow_ICCV_2023, eventflow:gehrig_densetimeflow_TPAMI_2024} that use only events to directly regress the optical flow are difficult to produce reliable dense results~\citep{eventflow:Pan_SingleImageFlow_CVPR_2020}. 
Therefore, there is a new trend to combine event data with other modalities, such as images~\citep{eventflow:Lee_Fusion_FlowNet_ICRA_2022, eventflow:Wan_DCEIFlow_TIP_2022}, 
depth~\citep{eventflow:ieng_event3dflow4dsubspace_2017} and point clouds~\citep{eventflow:Wan_RPEFlow_ICCV_2023}, \Fix{to take advantage of} complementary multimodal data. 
Given these successes in introducing events for motion estimation, we explore and fully illustrate the role of combining image and event data for InsMOS. 


\begin{figure*}[tbp]
    \centering
    \includegraphics[width=\textwidth]{2_figure_framework.pdf}
    \caption{ 
    \textbf{Our proposed InsMOS framework combines a single image with its subsequent events}. 
    The network pipeline is divided into three parts:
    1) The cross-modal masked attention augmentation (CMA) module interactively augments texture and motion representations with an additional contrastive feature learning mechanism applied in training. 
    2) Masks and motion embeddings are decoded separately, allowing for thresholding instance-level segmentation outputs. 
    Since the training loss is applied on full embeddings, this thresholding step only needs to be performed during inference. 
    3) The flow-guided motion feature enhancement module is designed to enhance motion feature learning during training. 
    }  
    \label{fig:framework}
\end{figure*}

\noindent\textbf{Event-based moving object segmentation.} This task aims to segment IMOs from events overlaid with object and camera motion~\citep{eventdatasets:mitrokhin_EVIMO_IROS_2019, eventmos:mitrokhin_modt_iros_2018}. 
The contrast maximization framework~\citep{eventmos:Gallego_Unifyingcontrastmax_cvpr_2018} separates camera motion by maximizing the contrast in warped event images and has been extended to achieve globally optimal motion estimation~\citep{eventmos:peng_globally_TPAMI_2021} and multiple object segmentation~\citep{eventmos:stoffregen_eventcompensation_ICCV_2019}. 
EMSGC~\citep{eventmos:zhou_EMSGC_TNNLS_2021} utilize a spatiotemporal graph representation of events and employ graph cuts for segmentation.
\FixCite{\cite{eventmos:chen_progressivemotionseg_AAAI_2022}} introduce a progressive motion segmentation framework that jointly optimizes motion estimation and event denoising. 
Recently, learning-based methods usually directly regress the AMOs~\citep{eventdatasets:mitrokhin_EVIMO_IROS_2019, eventmos:mitrokhin_learningms_CVPR_2020, eventmos:wang_evumoseg_arXiv_2023}, so they can only determine which positions are moving but cannot separate different IMOs. 
\cite{eventmos:parameshwara_0mms_ICRA_2021} propagate object motion through tracklet decomposition and cluster merging. 
RENet~\citep{eventmos:zhou_MOD_ICRA_2023} fuses images and events to detect moving objects, predicting object bounding boxes rather than pixel-level segmentation. 
JSTR~\citep{eventmos:zhou_jstr_ICRA_2024} proposes to incorporate IMU data and treat events as spatial and temporal 3D point clouds represented by 2D coordinates and timestamps. 
\cite{eventmos:jiang_semantic_VISAPP_2024} propose incorporating semantic and 3D depth information for MOS, while \cite{eventmos:Stamatios_GeneralMOS_3DV_2024} present a multi-stage divide-and-conquer framework based on ego-motion and optical flow estimation. 



Previous event-based MOS methods only utilize events as input, potentially leading to the incorrect identification of nearby multiple objects as a single large object due to the absence of texture information in events.
In contrast, we propose for the first time an InsMOS framework to segment all IMOs by exploiting the texture and motion information in a single image and events.