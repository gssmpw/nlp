\section{Related Work}
\label{sec:related-work}
\paragraph{Markov chain Monte Carlo (MCMC)}
Markov chain Monte Carlo methods are a classical approach for sampling from unnormalized target densities. 
The key idea is to construct a Markov chain whose stationary distribution matches the target distribution \citep{brooks2012handbookmcmc}. 
Prominent examples include the Metropolis-Hastings algorithm \citep{metropolis1953equation, hastings1970monte}, Gibbs sampling \citep{geman1984gibbs}, and Langevin dynamics \citep{rossky1978langevin, parisi1981langevin}. 
By exploiting geometric structure in the target distribution, Hamiltonian Monte Carlo \citep{duance1987hmc, mackay2003mcmcbook, brooks2012handbookmcmc, chen2014stochastic} often leads to more efficient exploration. 
To address scalability challenges in high-dimensional or large-dataset scenarios, stochastic gradient MCMC variants \citep{welling2011langevin, chen2014stochastic, zhang2020amagold, zhang2019cyclical} have been introduced. 
Although these MCMC methods reduce per-step computational costs or improve mixing, they remain inherently iterative, requiring many transitions to yield high-quality samples.

\paragraph{Learning-Based Samplers}
Amortized inference shifts the computational overhead from test-time sampling to a training phase, allowing for faster inference \citep{gershman2014amortized}. 
Approaches such as amortized MCMC~\citep{li2017amortizedmcmc} train a neural network to mimic the distribution of samples obtained after $T$ transitions of a traditional MCMC process. 
Similarly, GFlowNets \citep{bengio2021gflownets, bengio2023foundations} learn to sequentially construct complex discrete objects, effectively learning a sampling strategy. 
While GFlowNets amortize the computational challenges of lengthy stochastic searches and mode-mixing
during training, their sampling process remains sequential, as objects are constructed step-by-step
through a series of constructive steps.

An alternative viewpoint casts the sampling problem as an optimal control task \citep{zhang2022pis, berner2024dis, richter2024improved}, where one trains a controlled stochastic differential equation to transport an initial distribution to the target via a Schr√∂dinger bridge \citep{schrodinger1931umkehrung, schrodinger1932relativiste}. 
This perspective motivates recent efforts to use diffusion-based samplers \citep{geffner2023langevin, vargas2023dds, zhang2024dgfs, phillips2024particle, chen2025SCLD}. 
While such diffusion and flow-based frameworks have advanced the state of the art, they require numerical solvers operating on dense time discretizations.

\paragraph{Consistent Generative Models}
Recent work in generative modeling has explored the concept of consistency: ensuring that large transitions between observed distributions are consistent with sequences of incremental transformations. 
Consistency models \citep{song2023consistency, song2023improved, lu2025simplifying} 
learn a direct mapping from any point in time to the terminal state. 
Progressive distillation \citep{salimans2022progdist, meng2023distillation} incrementally distills a trained diffusion model into a more efficient version that takes half as many until a single-step model is achieved.
Similarly, shortcut models \citep{liu2023flowstraight, frans2025shortcut} leverage progressive self-distillation during training to achieve accelerated inference without relying on a pre-trained teacher model.

These methods focus on generative modeling tasks and assume access to a dataset drawn from the target distribution.
Our work introduces the notion of consistency into the setting of sampling from unnormalized densities. 
We assume access only to an unnormalized pointwise oracle $\rho$ for the target density, without requiring any pre-collected samples.