\section{Consistency Distillation Proof}\label{sec:a-proof}
\renewcommand{\thetheorem}{4.\arabic{theorem}} 
\begin{theorem}
    Let $\vf_\vtheta(\rvx_t, t)$ be a consistency function parameterized by $\vtheta$, and let $\vf(\rvx_t, t; u)$ denote the consistency function of the PF ODE defined by the control $u$. 
    Assume that $\vf_\vtheta$ satisfies a Lipschitz condition with constant $L > 0$, such that for all $t \in [0, T]$ and for all $\rvx_t, \rvy_t$,
    $$
        \| \vf_\vtheta(\rvx_t, t) - \vf_\vtheta(\rvy_t, t)\|_2 \leq L \|\rvx_t - \rvy_t\|_2.
    $$
    Additionally, assume that for each step $n \in \{1, 2, \ldots, N-1\}$, the ODE solver called at $t_{n}$ has a local error bounded by $O((t_{n+1} - t_n)^{p+1})$ for some $p \geq 1$.
    
    If, additionally, $\mathcal{L}_\text{CD}(\vtheta, \vtheta; u) = 0$, then:
    $$
    \sup_{n,\rvx_{t_n}} \|\vf_\vtheta(\rvx_{t_n}, t_n) -  \vf(\rvx_{t_n}, t_n; u)\|_2 = O((\Delta t)^p),
    $$
    where $\Delta t := \max_{n \in \{1, 2, \ldots, N-1\}} |t_{n+1} - t_n|$.
\end{theorem}
\begin{proof}
    The proof is similar to the one presented by \citet{song2023consistency}, with the key difference that we must account for the global integration error introduced by the ODE solver.

    If the ODE solver, when called at $t_{n+1}$, has a local error uniformly bounded by $O((t_{n} - t_{n-1})^{p+1})$, then the cumulative error across all steps is approximately the sum of $n+1$ local errors and is bounded by $O((\Delta t)^p)$.

    We are interested in $\ve_n$, the error between the learned consistency function and the consistency function of the PF ODE defined by the control $u$ at $\rvx_{t_n} \sim p_{t_n}(\rvx_{t_n})$,
    $$
    \ve_n:=\vf_\vtheta(\rvx_{t_n}, t_n) - \vf(\rvx_{t_n}, t_n; u). 
    $$
        
    If $\mathcal{L}(\vtheta, \vtheta; u) = 0$, we deduce that
    $$
    \lambda(t_n) d(f_{\vtheta}(\hat{\rvx}_{t_{n+1}}, t_{n+1}), f_{\vtheta}(\hat{\rvx}_{t_{n}}, t_n)) = 0.
    $$
    Since $\lambda(t_n) > 0$, this implies:
    \begin{equation}\label{eq:learned-cf}
        \vf_\vtheta(\hat \rvx_{t_{n+1}}, t_{n+1}) = \vf_\vtheta(\hat \rvx_{t_{n}}, t_{n}).
    \end{equation}

    We can derive a recurrence relation for $\ve_n$:
    \begin{align*}
        \ve_n &\overset{(i)}{=}  \vf_\vtheta(\rvx_{t_n}, t_n) - \vf_\vtheta(\hat\rvx_{t_n}, t_n) + \vf_\vtheta(\hat\rvx_{t_n}, t_n) - \vf(\rvx_{t_{n+1}}, t_{n+1}; u)\\
        &\overset{(ii)}{=} \vf_\vtheta(\rvx_{t_n}, t_n) - \vf_\vtheta(\hat\rvx_{t_n}, t_n) + \vf_\vtheta(\hat\rvx_{t_{n+1}}, t_{n+1}) - \vf(\rvx_{t_{n+1}}, t_{n+1}; u)\\
        &= \vf_\vtheta(\rvx_{t_n}, t_n) - \vf_\vtheta(\hat\rvx_{t_n}, t_n) + \vf_\vtheta(\hat\rvx_{t_{n+1}}, t_{n+1}) - \vf_\vtheta(\rvx_{t_{n+1}}, t_{n+1})\\
                &\quad\quad\quad + \vf_\vtheta(\rvx_{t_{n+1}}, t_{n+1}) - \vf(\rvx_{t_{n+1}}, t_{n+1}; u)\\
         &= \vf_\vtheta(\rvx_{t_n}, t_n) - \vf_\vtheta(\hat\rvx_{t_n}, t_n) + \vf_\vtheta(\hat\rvx_{t_{n+1}}, t_{n+1}) - \vf_\vtheta(\rvx_{t_{n+1}}, t_{n+1}) + \ve_{n+1}\\
         &\ldots\\
        &\overset{(iii)}{=} \vf_\vtheta(\rvx_{t_n}, t_n) - \vf_\vtheta(\hat\rvx_{t_n}, t_n) + \vf_\vtheta(\rvx_{T}, T) - \vf_\vtheta(\hat\rvx_{T}, T) + \ve_T.
    \end{align*}
    Here, step $(i)$ follows from the definition of the consistency function, step $(ii)$ is due to Eq. (\ref{eq:learned-cf}), and step $(iii)$ leverages the telescoping nature of the sum.

    Furthermore, since $\vf_\vtheta$ is parameterized such that $\vf_\vtheta(\rvx_{T}, T) = \rvx_{T}$, we have
    \begin{align*}
        \ve_T &= \vf_\vtheta(\rvx_{T}, T) - \vf(\rvx_{T}, T; u) \\ 
        &= \rvx_{T} - \rvx_{T}\\
        &= 0.
    \end{align*}

    Finally, given that $\vf_\vtheta$ is Lipschitz and considering the bound on the global error of the ODE solver:
    \begin{equation*}
        \|\ve_n\|_2 \leq \|\ve_T\|_2 + L\|\rvx_{t_n} - \hat\rvx_{t_n} \|_2 + L\|\rvx_{T} - \hat\rvx_{T} \|_2
        = O((\Delta t)^p).
    \end{equation*}
\end{proof}


\section{Experimental Details}\label{sec:details}
\subsection{Target Distributions}
\paragraph{GMM.}
Here we discuss the parameterization for the Gaussian mixture model with well separated modes. 
We follow the same setting as \citet{zhang2022pis, berner2024dis}, defining the target distribution as follows: 
\begin{equation*}
    \rho(\rvx) = \sum_{m=1}^{M} \alpha_m \mathcal{N}(\rvx; \mu_m, \Sigma_m)
\end{equation*}
Following their prarameterization, we set $M=9$, $\sigma_m = .3 I$, and $(\mu)_{m=1}^M = \{-5, -, 5\} \times \{-5, 0, 5\} \subset \mathbb{R}^2$.


\paragraph{Image.}
We use a normalized grayscale image to create a two-dimensional probability density, following the setup from \citet{wu2020snf}. 

\paragraph{Funnel.}
Following the methodology of \citet{berner2024dis}, we use the funnel distribution introduced from \citet{neal2003slice}. The distribution is defined as follows: 
\begin{equation*}
    \rho(\rvx) = \mathcal{N}(x_1; 0, v^2) \prod_{i=1}^d \mathcal{N}(x_i; 0, e^{x_1})
\end{equation*}
We set $d=10, v=3$. 

We include this benchmark as this is a canonical distribution used for comparing MCMC methods and has been used extensively within the growing field of learned diffusion samplers \citep{berner2024dis, zhang2022pis, vargas2023dds, richter2024improved}.

\paragraph{Many-Well.}
We use the many-well target distribution following the methodology of \citet{berner2024dis}:
\begin{equation*}
\rho(\rvx) = \exp \left(-\sum_{i=1}^m (x_i^2 - \delta) - \frac{1}{2} \sum_{i=m+1}^d x_i^2 \right).
\end{equation*}
For the target distribution labeled as MW-54, we set $d=5$, $m=5$, and $\delta=4$; for the target distribution labeled as MW-52, we set $d=50, m=5, \delta=2$. 

\paragraph{Log Cox Gaussian Process (LGCP).}
The log cox Gaussian process is a popular target distribution for benchmarking sampling methods due to its complexity and high-dimensionality. 
As discussed in \citet{zhang2022pis, chen2025SCLD}, the LGCP distribution is defined as follows: 
\begin{align*}
    \rho(x) = \mathcal{N} (x; \mu, \Sigma) \prod_{i=1}^d \exp \left(x_i y_i -\frac{\exp(x_i)}{d}\right).
\end{align*}
Here, $y$ is a given dataset, and $\mu, \Sigma$ are mean and covariance for some given prior. 
We follow the methodology of \citet{zhang2022pis, arbel2021annealed} for both the dataset and prior distribution. 


\subsection{Training Details}
For GMM, image, funnel, and MW54, we train all diffusion samplers until convergence or for 30,000 training iterations. 
For MW52d, we train all samplers for 10,000 training iterations. For LGCP, we train all samplers for 5,000 training iterations. 

For a complete specification of sampler details, see Table \ref{tab:sampler_comparison}. 
For details on the global configurations used across all samplers, see Table \ref{tab:details}.

\begin{table*} % Use table* for spanning both columns in a two-column format
    \centering
    \renewcommand{\arraystretch}{1.3}  % Adjust row height for better spacing
    \begin{minipage}[t]{.45\textwidth}
    \vspace{0pt} 
        \begin{tabular}{p{4cm} p{4cm}}  
            \toprule
            \multicolumn{2}{c}{\textbf{SCDS}} \\  
            \midrule
            Terminal Time & 1 \\
            SDE & VP SDE \\ 
            Terminal Time & 1 \\
            Time Schedule & Linear \\ 
            Initial Distribution & $\mathcal{N}(0, I)$ with Truncation Quartile of $1e-4$ \\
            Loss Function & Log-Variance, Time Reversal \citep{berner2024dis, richter2024improved}, Self-Consistency\\ 
            \midrule 
            \multicolumn{2}{c}{\textbf{CDDS}} \\  
            \midrule
            Pretrained Generative Ctrl & DIS \\ 
            Consistency Model Train Timesteps & 18 \\ 
            Loss Function & Equation \eqref{eq:cdloss} \\
            \midrule 
            \multicolumn{2}{c}{\textbf{DIS} \citep{berner2024dis}}  \\
            \midrule 
            SDE & VP SDE \\ 
            Loss Function & Log Variance, Time Reversal \\ 
            Terminal Time & 1 \\
            Time Schedule & Linear \\ 
            Initial Distribution & $\mathcal{N}(0, I)$ with Truncation Quartile of $1e-4$ \\
            \midrule
            \multicolumn{2}{c}{\textbf{PIS} \citep{zhang2022pis}}  \\
            \midrule 
            SDE & VE SDE \\ 
            Loss Function & Log Variance \citep{richter2024improved}\\ 
            Terminal Time & 1 \\
            Time Schedule & Linear \\ 
            Initial Distribution & Dirac-Delta  \\  
            \midrule
            \multicolumn{2}{c}{\textbf{DDS} \citep{vargas2023dds}}  \\
            \midrule 
            SDE & VP SDE \\ 
            Loss Function & Log Variance \\ 
            SDE & Exponential SDE \\
            Time Schedule & Cosine \\ 
            Terminal T & 12.8 \\ 
            $\Delta t$ & $.1$ \\
            Initial Distribution & $\mathcal{N}(0, I)$ with Truncation Quartile of $1e-4$ \\
            \bottomrule
        \end{tabular}
        \caption{Diffusion Sampler Configurations}
        \label{tab:sampler_comparison}
    \end{minipage} \hfill
    \begin{minipage}[t]{.45\textwidth}
    \vspace{0pt} 
        \begin{tabular}{p{4cm} p{4cm}}  
            \toprule
            \multicolumn{2}{c}{\textbf{Optimizer Settings}} \\  
            \midrule
            Optimizer & Adam \\ 
            Learning Rate & $.005$ \\ 
            Weight Decay & $1e-7$ \\
            Gradient Clipping & 1 \\
            $\beta_1, \beta_2$ & $.9, .999$ \\
            \midrule 
            \multicolumn{2}{c}{\textbf{Training Settings}} \\  
            \midrule
            Total Iterations & GMM, Image, Funnel, MW54=30,000; MW52=10,000; LGCP=5,000 \\
            Train Time Steps & 128 \\ 
            Batch Size & 2048 \\
            \midrule 
            \multicolumn{2}{c}{\textbf{Model Settings}} \\
            \midrule 
            Number of Layers & 4 \\ 
            Channels & 64 \\
            Time Conditioning & Fourier Time Embeddings \citet{tancik2020fourier} \\ 
            Activation & GeLU \\
            \multicolumn{2}{c}{\textbf{Evaluation Settings}} \\
            \midrule
            Batch Size & $10000$ \\
            Weight Decay & $1e-7$ \\ 
            \bottomrule
        \end{tabular}
        \caption{Global Configurations}
        \label{tab:details}
    \end{minipage}
\end{table*}


\begin{figure}[tb!]
\centering
\includegraphics[width=1.0\linewidth]{figures/training.pdf}
\caption{Loss curves for the samplers studied in this paper. 
SCDS and CDDS exhibit stable learning across most settings, except for the image target distribution, where all samplers—except CDDS—show instability. 
Notably, the self-consistency loss and the sampling loss remain relatively independent.}
\label{fig:training}
\end{figure}