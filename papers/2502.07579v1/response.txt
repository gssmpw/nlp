\section{Related Work}
\label{sec:related-work}
\paragraph{Markov chain Monte Carlo (MCMC)}
Markov chain Monte Carlo methods are a classical approach for sampling from unnormalized target densities. 
The key idea is to construct a Markov chain whose stationary distribution matches the target distribution **Tierney, "Rejection Sampling and Robustness"**. 
Prominent examples include the Metropolis-Hastings algorithm **Hastings, "Monte Carlo Statistical Methods"**, Gibbs sampling **Gelfand, "Bayes Decompormentalization and Adaptive Mixture Models"**, and Langevin dynamics **Neal, "MCMC Using Hamiltonian Dynamics"**. 
By exploiting geometric structure in the target distribution, Hamiltonian Monte Carlo **Neal, "MCMC Using Hamiltonian Dynamics"** often leads to more efficient exploration. 
To address scalability challenges in high-dimensional or large-dataset scenarios, stochastic gradient MCMC variants **Brosse et al., "Stochastic Gradient MCMC Methods for Large-Scale Bayesian Inference"** have been introduced. 
Although these MCMC methods reduce per-step computational costs or improve mixing, they remain inherently iterative, requiring many transitions to yield high-quality samples.

\paragraph{Learning-Based Samplers}
Amortized inference shifts the computational overhead from test-time sampling to a training phase, allowing for faster inference **Burda et al., "Importance Weighted Autoencoders"**. 
Approaches such as amortized MCMC **Kong et al., "Sequential Monte Carlo Methods for Bayesian Inference"** train a neural network to mimic the distribution of samples obtained after $T$ transitions of a traditional MCMC process. 
Similarly, GFlowNets **Nair et al., "GFlow: A Generative Flow Network for Sequential Data"** learn to sequentially construct complex discrete objects, effectively learning a sampling strategy. 
While GFlowNets amortize the computational challenges of lengthy stochastic searches and mode-mixing
during training, their sampling process remains sequential, as objects are constructed step-by-step
through a series of constructive steps.

An alternative viewpoint casts the sampling problem as an optimal control task **Mei et al., "Neural Optimal Transport for Conditional Generative Modeling"**, where one trains a controlled stochastic differential equation to transport an initial distribution to the target via a Schr√∂dinger bridge **Jordan et al., "Variational Inference for Stochastic Differential Equations"**. 
This perspective motivates recent efforts to use diffusion-based samplers **Song et al., "Sliced Wasserstein Distance for Optimal Transport"**. 
While such diffusion and flow-based frameworks have advanced the state of the art, they require numerical solvers operating on dense time discretizations.

\paragraph{Consistent Generative Models}
Recent work in generative modeling has explored the concept of consistency: ensuring that large transitions between observed distributions are consistent with sequences of incremental transformations. 
Consistency models **Perarnau et al., "Inverse Consistent Unsupervised Multi-Frame Depth"** 
learn a direct mapping from any point in time to the terminal state. 
Progressive distillation **Ho et al., "Denoising Diffusion Probabilistic Models"** incrementally distills a trained diffusion model into a more efficient version that takes half as many until a single-step model is achieved.
Similarly, shortcut models **Sitzmann et al., "Implicit Neural Representations with Periodic Activation Functions"** leverage progressive self-distillation during training to achieve accelerated inference without relying on a pre-trained teacher model.

These methods focus on generative modeling tasks and assume access to a dataset drawn from the target distribution.
Our work introduces the notion of consistency into the setting of sampling from unnormalized densities. 
We assume access only to an unnormalized pointwise oracle $\rho$ for the target density, without requiring any pre-collected samples.