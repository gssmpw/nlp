\section{Related Works}
\textbf{Parameter Efficient Fine-tuning}. PEFT methods are used to reduce the expensive memory requirements for fine-tuning large models. Existing techniques can be split into several categories. Adapter-based methods introduce additional trainable modules that are inserted into the original frozen model \citep{houlsby2019parameterefficienttransferlearningnlp,pfeiffer2021adapterfusionnondestructivetaskcomposition,he2022unifiedviewparameterefficienttransfer,mahabadi2021parameterefficientmultitaskfinetuningtransformers}. However, these approaches may increase latency during inference. Prompt tuning, on the other hand, adapts a model by adding learnable prefix tokens to the input \citep{li2021prefixtuningoptimizingcontinuousprompts,lester2021powerscaleparameterefficientprompt, liu2022ptuningv2prompttuning}. Despite their simplicity, these methods have structural limitations since they only train additional input tokens. LoRA is a widely used PEFT method that does not introduce additional inference latency \citep{hu2021loralowrankadaptationlarge}. LoRA employs low-rank matrices to approximate the updates in the parameters during fine-tuning. Several variants of LoRA have been developed to either improve performance or further reduce the number of trainable parameters \citep{zhang2023adaloraadaptivebudgetallocation,xia2024chainloraefficientfinetuning,liu2024doraweightdecomposedlowrankadaptation,kopiczko2024veravectorbasedrandommatrix}. Due to LoRA's popularity, extensive research has been conducted on both its theoretical foundations and empirical performance \citep{jang2024loratrainingntkregime,hayou2024loraefficientlowrank,mao2024surveyloralargelanguage}. Additionally, quantization-based methods have been proposed to further reduce memory overhead \cite{dettmers2023qloraefficientfinetuningquantized,qin2024accuratelorafinetuningquantizationllms}. 

\textbf{Gradient Compression}. An area that has been relatively underexplored but is now gaining attention is gradient compression \citep{zhao2024galorememoryefficientllmtraining,hao2024floralowrankadapterssecretly,liang2024memoryefficientllmtrainingonline,wu2024cgfedllmcompressgradientsfederated,song2024sparsefinetuningpretrainedlarge}. These approaches selectively compress gradient information to reduce the size of optimizer states during training. One category of methods uses projection matrices to obtain a lower-rank gradients \citep{zhao2024galorememoryefficientllmtraining,hao2024floralowrankadapterssecretly,liang2024memoryefficientllmtrainingonline}. For instance, GaLore uses singular value decomposition (SVD) to obtain projection matrices \citep{zhao2024galorememoryefficientllmtraining}, while FLoRA utilizes random projection matrices \citep{hao2024floralowrankadapterssecretly}. \citet{liang2024memoryefficientllmtrainingonline} propose a method that updates the projection matrix in an online fashion using principal component analysis. Alongside projection matrices, gradient sparsity is another emerging factor. SIFT shows that gradients are approximately sparse, and achieves efficient fine-tuning by selecting parameters corresponding to the largest gradient magnitudes \citep{song2024sparsefinetuningpretrainedlarge}. However, a significant limitation of this approach is that the selected parameters remain static, failing to fully capture the dynamic nature of gradient sparsity patterns during training.