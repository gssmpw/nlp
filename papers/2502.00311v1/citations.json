[
  {
    "index": 0,
    "papers": [
      {
        "key": "houlsby2019parameterefficienttransferlearningnlp",
        "author": "Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly",
        "title": "Parameter-Efficient Transfer Learning for NLP"
      },
      {
        "key": "pfeiffer2021adapterfusionnondestructivetaskcomposition",
        "author": "Jonas Pfeiffer and Aishwarya Kamath and Andreas R\u00fcckl\u00e9 and Kyunghyun Cho and Iryna Gurevych",
        "title": "AdapterFusion: Non-Destructive Task Composition for Transfer Learning"
      },
      {
        "key": "he2022unifiedviewparameterefficienttransfer",
        "author": "Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig",
        "title": "Towards a Unified View of Parameter-Efficient Transfer Learning"
      },
      {
        "key": "mahabadi2021parameterefficientmultitaskfinetuningtransformers",
        "author": "Rabeeh Karimi Mahabadi and Sebastian Ruder and Mostafa Dehghani and James Henderson",
        "title": "Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "li2021prefixtuningoptimizingcontinuousprompts",
        "author": "Xiang Lisa Li and Percy Liang",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      },
      {
        "key": "lester2021powerscaleparameterefficientprompt",
        "author": "Brian Lester and Rami Al-Rfou and Noah Constant",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      },
      {
        "key": "liu2022ptuningv2prompttuning",
        "author": "Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang",
        "title": "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hu2021loralowrankadaptationlarge",
        "author": "Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "LoRA: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhang2023adaloraadaptivebudgetallocation",
        "author": "Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao",
        "title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning"
      },
      {
        "key": "xia2024chainloraefficientfinetuning",
        "author": "Wenhan Xia and Chengwei Qin and Elad Hazan",
        "title": "Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning"
      },
      {
        "key": "liu2024doraweightdecomposedlowrankadaptation",
        "author": "Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen",
        "title": "DoRA: Weight-Decomposed Low-Rank Adaptation"
      },
      {
        "key": "kopiczko2024veravectorbasedrandommatrix",
        "author": "Dawid J. Kopiczko and Tijmen Blankevoort and Yuki M. Asano",
        "title": "VeRA: Vector-based Random Matrix Adaptation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "jang2024loratrainingntkregime",
        "author": "Uijeong Jang and Jason D. Lee and Ernest K. Ryu",
        "title": "LoRA Training in the NTK Regime has No Spurious Local Minima"
      },
      {
        "key": "hayou2024loraefficientlowrank",
        "author": "Soufiane Hayou and Nikhil Ghosh and Bin Yu",
        "title": "LoRA+: Efficient Low Rank Adaptation of Large Models"
      },
      {
        "key": "mao2024surveyloralargelanguage",
        "author": "Yuren Mao and Yuhang Ge and Yijiang Fan and Wenyi Xu and Yu Mi and Zhonghao Hu and Yunjun Gao",
        "title": "A Survey on LoRA of Large Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dettmers2023qloraefficientfinetuningquantized",
        "author": "Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer",
        "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
      },
      {
        "key": "qin2024accuratelorafinetuningquantizationllms",
        "author": "Haotong Qin and Xudong Ma and Xingyu Zheng and Xiaoyang Li and Yang Zhang and Shouda Liu and Jie Luo and Xianglong Liu and Michele Magno",
        "title": "Accurate LoRA-Finetuning Quantization of LLMs via Information Retention"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zhao2024galorememoryefficientllmtraining",
        "author": "Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian",
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"
      },
      {
        "key": "hao2024floralowrankadapterssecretly",
        "author": "Yongchang Hao and Yanshuai Cao and Lili Mou",
        "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors"
      },
      {
        "key": "liang2024memoryefficientllmtrainingonline",
        "author": "Kaizhao Liang and Bo Liu and Lizhang Chen and Qiang Liu",
        "title": "Memory-Efficient LLM Training with Online Subspace Descent"
      },
      {
        "key": "wu2024cgfedllmcompressgradientsfederated",
        "author": "Huiwen Wu and Xiaohan Li and Deyi Zhang and Xiaogang Xu and Jiafei Wu and Puning Zhao and Zhe Liu",
        "title": "CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models"
      },
      {
        "key": "song2024sparsefinetuningpretrainedlarge",
        "author": "Weixi Song and Zuchao Li and Lefei Zhang and Hai Zhao and Bo Du",
        "title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhao2024galorememoryefficientllmtraining",
        "author": "Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian",
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"
      },
      {
        "key": "hao2024floralowrankadapterssecretly",
        "author": "Yongchang Hao and Yanshuai Cao and Lili Mou",
        "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors"
      },
      {
        "key": "liang2024memoryefficientllmtrainingonline",
        "author": "Kaizhao Liang and Bo Liu and Lizhang Chen and Qiang Liu",
        "title": "Memory-Efficient LLM Training with Online Subspace Descent"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhao2024galorememoryefficientllmtraining",
        "author": "Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian",
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hao2024floralowrankadapterssecretly",
        "author": "Yongchang Hao and Yanshuai Cao and Lili Mou",
        "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liang2024memoryefficientllmtrainingonline",
        "author": "Kaizhao Liang and Bo Liu and Lizhang Chen and Qiang Liu",
        "title": "Memory-Efficient LLM Training with Online Subspace Descent"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "song2024sparsefinetuningpretrainedlarge",
        "author": "Weixi Song and Zuchao Li and Lefei Zhang and Hai Zhao and Bo Du",
        "title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models"
      }
    ]
  }
]