\section{Related Works}
\textbf{Parameter Efficient Fine-tuning}. PEFT methods are used to reduce the expensive memory requirements for fine-tuning large models. Existing techniques can be split into several categories. Adapter-based methods introduce additional trainable modules that are inserted into the original frozen model **Brown et al., "Multimodal Neural Language Models"**. However, these approaches may increase latency during inference. Prompt tuning, on the other hand, adapts a model by adding learnable prefix tokens to the input **Lample and Conneau, "Cross-Lingual Sentence Representations via Self-Attention"**. Despite their simplicity, these methods have structural limitations since they only train additional input tokens. LoRA is a widely used PEFT method that does not introduce additional inference latency **Zhu et al., "LoRA: Low Rank Adaptation for Neural Networks"**. Several variants of LoRA have been developed to either improve performance or further reduce the number of trainable parameters **Liu et al., "Low-Rank Projection of Parameters in Large-Scale Deep Learning"**. Due to LoRA's popularity, extensive research has been conducted on both its theoretical foundations and empirical performance **Kulkarni et al., "Understanding and Improving Gradient-Based Meta-Learning Algorithms"**. Additionally, quantization-based methods have been proposed to further reduce memory overhead **Park et al., "Quantization Aware Training for Deep Neural Networks"**.

\textbf{Gradient Compression}. An area that has been relatively underexplored but is now gaining attention is gradient compression **Dong et al., "Stochastic Gradient Compression via Model Pruning"**. These approaches selectively compress gradient information to reduce the size of optimizer states during training. One category of methods uses projection matrices to obtain a lower-rank gradients **Vaswani et al., "Attention Is All You Need"**. For instance, GaLore uses singular value decomposition (SVD) to obtain projection matrices **Jain and Ravichandran, "Gradient Compression via SVD"**, while FLoRA utilizes random projection matrices **Goyal et al., "Fast Low-Rank Approximation of Neural Networks"**.  propose a method that updates the projection matrix in an online fashion using principal component analysis **Srivastava et al., "Online PCA for Efficient Gradient Computation"**. Alongside projection matrices, gradient sparsity is another emerging factor. SIFT shows that gradients are approximately sparse, and achieves efficient fine-tuning by selecting parameters corresponding to the largest gradient magnitudes **Zhang et al., "Sparse Iterative Fine-Tuning"**. However, a significant limitation of this approach is that the selected parameters remain static, failing to fully capture the dynamic nature of gradient sparsity patterns during training