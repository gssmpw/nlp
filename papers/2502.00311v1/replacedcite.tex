\section{Related Works}
\textbf{Parameter Efficient Fine-tuning}. PEFT methods are used to reduce the expensive memory requirements for fine-tuning large models. Existing techniques can be split into several categories. Adapter-based methods introduce additional trainable modules that are inserted into the original frozen model ____. However, these approaches may increase latency during inference. Prompt tuning, on the other hand, adapts a model by adding learnable prefix tokens to the input ____. Despite their simplicity, these methods have structural limitations since they only train additional input tokens. LoRA is a widely used PEFT method that does not introduce additional inference latency ____. LoRA employs low-rank matrices to approximate the updates in the parameters during fine-tuning. Several variants of LoRA have been developed to either improve performance or further reduce the number of trainable parameters ____. Due to LoRA's popularity, extensive research has been conducted on both its theoretical foundations and empirical performance ____. Additionally, quantization-based methods have been proposed to further reduce memory overhead ____. 

\textbf{Gradient Compression}. An area that has been relatively underexplored but is now gaining attention is gradient compression ____. These approaches selectively compress gradient information to reduce the size of optimizer states during training. One category of methods uses projection matrices to obtain a lower-rank gradients ____. For instance, GaLore uses singular value decomposition (SVD) to obtain projection matrices ____, while FLoRA utilizes random projection matrices ____. ____ propose a method that updates the projection matrix in an online fashion using principal component analysis. Alongside projection matrices, gradient sparsity is another emerging factor. SIFT shows that gradients are approximately sparse, and achieves efficient fine-tuning by selecting parameters corresponding to the largest gradient magnitudes ____. However, a significant limitation of this approach is that the selected parameters remain static, failing to fully capture the dynamic nature of gradient sparsity patterns during training.