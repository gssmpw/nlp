@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{liu2024doraweightdecomposedlowrankadaptation,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09353}, 
}

@misc{kopiczko2024veravectorbasedrandommatrix,
      title={VeRA: Vector-based Random Matrix Adaptation}, 
      author={Dawid J. Kopiczko and Tijmen Blankevoort and Yuki M. Asano},
      year={2024},
      eprint={2310.11454},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11454}, 
}

@misc{houlsby2019parameterefficienttransferlearningnlp,
      title={Parameter-Efficient Transfer Learning for NLP}, 
      author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
      year={2019},
      eprint={1902.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.00751}, 
}

@misc{li2021prefixtuningoptimizingcontinuousprompts,
      title={Prefix-Tuning: Optimizing Continuous Prompts for Generation}, 
      author={Xiang Lisa Li and Percy Liang},
      year={2021},
      eprint={2101.00190},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.00190}, 
}

@misc{zhao2024galorememoryefficientllmtraining,
      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, 
      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
      year={2024},
      eprint={2403.03507},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.03507}, 
}

@misc{hao2024floralowrankadapterssecretly,
      title={Flora: Low-Rank Adapters Are Secretly Gradient Compressors}, 
      author={Yongchang Hao and Yanshuai Cao and Lili Mou},
      year={2024},
      eprint={2402.03293},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.03293}, 
}

@misc{song2024sparsefinetuningpretrainedlarge,
      title={Sparse is Enough in Fine-tuning Pre-trained Large Language Models}, 
      author={Weixi Song and Zuchao Li and Lefei Zhang and Hai Zhao and Bo Du},
      year={2024},
      eprint={2312.11875},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.11875}, 
}

@misc{liang2024memoryefficientllmtrainingonline,
      title={Memory-Efficient LLM Training with Online Subspace Descent}, 
      author={Kaizhao Liang and Bo Liu and Lizhang Chen and Qiang Liu},
      year={2024},
      eprint={2408.12857},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.12857}, 
}

@misc{dettmers2023qloraefficientfinetuningquantized,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14314}, 
}

@misc{qin2024accuratelorafinetuningquantizationllms,
      title={Accurate LoRA-Finetuning Quantization of LLMs via Information Retention}, 
      author={Haotong Qin and Xudong Ma and Xingyu Zheng and Xiaoyang Li and Yang Zhang and Shouda Liu and Jie Luo and Xianglong Liu and Michele Magno},
      year={2024},
      eprint={2402.05445},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05445}, 
}

@misc{jang2024loratrainingntkregime,
      title={LoRA Training in the NTK Regime has No Spurious Local Minima}, 
      author={Uijeong Jang and Jason D. Lee and Ernest K. Ryu},
      year={2024},
      eprint={2402.11867},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.11867}, 
}

@misc{hayou2024loraefficientlowrank,
      title={LoRA+: Efficient Low Rank Adaptation of Large Models}, 
      author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
      year={2024},
      eprint={2402.12354},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12354}, 
}

@misc{mao2024surveyloralargelanguage,
      title={A Survey on LoRA of Large Language Models}, 
      author={Yuren Mao and Yuhang Ge and Yijiang Fan and Wenyi Xu and Yu Mi and Zhonghao Hu and Yunjun Gao},
      year={2024},
      eprint={2407.11046},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.11046}, 
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and et al},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@misc{dubey2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and et. al},
      year={2024},
      eprint={2407.21783},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2407.21783}, 
}

@inproceedings{pati1993orthogonal,
  title={Orthogonal matching pursuit: Recursive function approximation with applications to wavelet decomposition},
  author={Pati, Yagyensh Chandra and Rezaiifar, Ramin and Krishnaprasad, Perinkulam Sambamurthy},
  booktitle={Proceedings of 27th Asilomar conference on signals, systems and computers},
  pages={40--44},
  year={1993},
  organization={IEEE}
}

@article{zhu2020efficient,
  title={Efficient implementations for orthogonal matching pursuit},
  author={Zhu, Hufei and Chen, Wen and Wu, Yanpeng},
  journal={Electronics},
  volume={9},
  number={9},
  pages={1507},
  year={2020},
  publisher={MDPI}
}

@misc{zaken2022bitfitsimpleparameterefficientfinetuning,
      title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models}, 
      author={Elad Ben Zaken and Shauli Ravfogel and Yoav Goldberg},
      year={2022},
      eprint={2106.10199},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.10199}, 
}

@misc{hu2023llmadaptersadapterfamilyparameterefficient,
      title={LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models}, 
      author={Zhiqiang Hu and Lei Wang and Yihuai Lan and Wanyu Xu and Ee-Peng Lim and Lidong Bing and Xing Xu and Soujanya Poria and Roy Ka-Wei Lee},
      year={2023},
      eprint={2304.01933},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2304.01933}, 
}

@misc{zhang2023adaloraadaptivebudgetallocation,
      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2303.10512},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10512}, 
}

@misc{xia2024chainloraefficientfinetuning,
      title={Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning}, 
      author={Wenhan Xia and Chengwei Qin and Elad Hazan},
      year={2024},
      eprint={2401.04151},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.04151}, 
}

@misc{chowdhery2022palmscalinglanguagemodeling,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and et. al},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2204.02311}, 
}

@misc{bai2023qwentechnicalreport,
      title={Qwen Technical Report}, 
      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and et. al},
      year={2023},
      eprint={2309.16609},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.16609}, 
}

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1412.6980}, 
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@article{marques2018review,
  title={A review of sparse recovery algorithms},
  author={Marques, Elaine Crespo and Maciel, Nilson and Naviner, Lirida and Cai, Hao and Yang, Jun},
  journal={IEEE access},
  volume={7},
  pages={1300--1322},
  year={2018},
  publisher={IEEE}
}

@misc{pfeiffer2021adapterfusionnondestructivetaskcomposition,
      title={AdapterFusion: Non-Destructive Task Composition for Transfer Learning}, 
      author={Jonas Pfeiffer and Aishwarya Kamath and Andreas Rücklé and Kyunghyun Cho and Iryna Gurevych},
      year={2021},
      eprint={2005.00247},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.00247}, 
}

@misc{he2022unifiedviewparameterefficienttransfer,
      title={Towards a Unified View of Parameter-Efficient Transfer Learning}, 
      author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
      year={2022},
      eprint={2110.04366},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.04366}, 
}

@misc{mahabadi2021parameterefficientmultitaskfinetuningtransformers,
      title={Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks}, 
      author={Rabeeh Karimi Mahabadi and Sebastian Ruder and Mostafa Dehghani and James Henderson},
      year={2021},
      eprint={2106.04489},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.04489}, 
}

@misc{lester2021powerscaleparameterefficientprompt,
      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
      author={Brian Lester and Rami Al-Rfou and Noah Constant},
      year={2021},
      eprint={2104.08691},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08691}, 
}

@misc{liu2022ptuningv2prompttuning,
      title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks}, 
      author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},
      year={2022},
      eprint={2110.07602},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.07602}, 
}

@misc{wu2024cgfedllmcompressgradientsfederated,
      title={CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models}, 
      author={Huiwen Wu and Xiaohan Li and Deyi Zhang and Xiaogang Xu and Jiafei Wu and Puning Zhao and Zhe Liu},
      year={2024},
      eprint={2405.13746},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.13746}, 
}

@misc{candes2004robustuncertaintyprinciplesexact,
      title={Robust Uncertainty Principles: Exact Signal Reconstruction from Highly Incomplete Frequency Information}, 
      author={Emmanuel Candes and Justin Romberg and Terence Tao},
      year={2004},
      eprint={math/0409186},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/math/0409186}, 
}

@article{donoho2006compressed,
  title={Compressed sensing},
  author={Donoho, David L},
  journal={IEEE Transactions on information theory},
  volume={52},
  number={4},
  pages={1289--1306},
  year={2006},
  publisher={IEEE}
}

@article{candes2008restricted,
  title={The restricted isometry property and its implications for compressed sensing},
  author={Candes, Emmanuel J},
  journal={Comptes rendus. Mathematique},
  volume={346},
  number={9-10},
  pages={589--592},
  year={2008}
}

@misc{candes2005decodinglinearprogramming,
      title={Decoding by Linear Programming}, 
      author={Emmanuel Candes and Terence Tao},
      year={2005},
      eprint={math/0502327},
      archivePrefix={arXiv},
      primaryClass={math.MG},
      url={https://arxiv.org/abs/math/0502327}, 
}

@misc{clark2019boolqexploringsurprisingdifficulty,
      title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions}, 
      author={Christopher Clark and Kenton Lee and Ming-Wei Chang and Tom Kwiatkowski and Michael Collins and Kristina Toutanova},
      year={2019},
      eprint={1905.10044},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1905.10044}, 
}

@article{ding2023parameter,
  title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={Nature Machine Intelligence},
  volume={5},
  number={3},
  pages={220--235},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@misc{han2024parameterefficientfinetuninglargemodels,
      title={Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey}, 
      author={Zeyu Han and Chao Gao and Jinyang Liu and Jeff Zhang and Sai Qian Zhang},
      year={2024},
      eprint={2403.14608},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.14608}, 
}

@misc{stich2018sparsifiedsgdmemory,
      title={Sparsified SGD with Memory}, 
      author={Sebastian U. Stich and Jean-Baptiste Cordonnier and Martin Jaggi},
      year={2018},
      eprint={1809.07599},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1809.07599}, 
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}