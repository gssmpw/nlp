@misc{dettmers2023qloraefficientfinetuningquantized,
      title={QLoRA: Efficient Finetuning of Quantized LLMs}, 
      author={Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
      year={2023},
      eprint={2305.14314},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.14314}, 
}

@misc{hao2024floralowrankadapterssecretly,
      title={Flora: Low-Rank Adapters Are Secretly Gradient Compressors}, 
      author={Yongchang Hao and Yanshuai Cao and Lili Mou},
      year={2024},
      eprint={2402.03293},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.03293}, 
}

@misc{hayou2024loraefficientlowrank,
      title={LoRA+: Efficient Low Rank Adaptation of Large Models}, 
      author={Soufiane Hayou and Nikhil Ghosh and Bin Yu},
      year={2024},
      eprint={2402.12354},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12354}, 
}

@misc{he2022unifiedviewparameterefficienttransfer,
      title={Towards a Unified View of Parameter-Efficient Transfer Learning}, 
      author={Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
      year={2022},
      eprint={2110.04366},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.04366}, 
}

@misc{houlsby2019parameterefficienttransferlearningnlp,
      title={Parameter-Efficient Transfer Learning for NLP}, 
      author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
      year={2019},
      eprint={1902.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.00751}, 
}

@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}

@misc{jang2024loratrainingntkregime,
      title={LoRA Training in the NTK Regime has No Spurious Local Minima}, 
      author={Uijeong Jang and Jason D. Lee and Ernest K. Ryu},
      year={2024},
      eprint={2402.11867},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.11867}, 
}

@misc{kopiczko2024veravectorbasedrandommatrix,
      title={VeRA: Vector-based Random Matrix Adaptation}, 
      author={Dawid J. Kopiczko and Tijmen Blankevoort and Yuki M. Asano},
      year={2024},
      eprint={2310.11454},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.11454}, 
}

@misc{lester2021powerscaleparameterefficientprompt,
      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
      author={Brian Lester and Rami Al-Rfou and Noah Constant},
      year={2021},
      eprint={2104.08691},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2104.08691}, 
}

@misc{li2021prefixtuningoptimizingcontinuousprompts,
      title={Prefix-Tuning: Optimizing Continuous Prompts for Generation}, 
      author={Xiang Lisa Li and Percy Liang},
      year={2021},
      eprint={2101.00190},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2101.00190}, 
}

@misc{liang2024memoryefficientllmtrainingonline,
      title={Memory-Efficient LLM Training with Online Subspace Descent}, 
      author={Kaizhao Liang and Bo Liu and Lizhang Chen and Qiang Liu},
      year={2024},
      eprint={2408.12857},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.12857}, 
}

@misc{liu2022ptuningv2prompttuning,
      title={P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks}, 
      author={Xiao Liu and Kaixuan Ji and Yicheng Fu and Weng Lam Tam and Zhengxiao Du and Zhilin Yang and Jie Tang},
      year={2022},
      eprint={2110.07602},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2110.07602}, 
}

@misc{liu2024doraweightdecomposedlowrankadaptation,
      title={DoRA: Weight-Decomposed Low-Rank Adaptation}, 
      author={Shih-Yang Liu and Chien-Yi Wang and Hongxu Yin and Pavlo Molchanov and Yu-Chiang Frank Wang and Kwang-Ting Cheng and Min-Hung Chen},
      year={2024},
      eprint={2402.09353},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.09353}, 
}

@misc{mahabadi2021parameterefficientmultitaskfinetuningtransformers,
      title={Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks}, 
      author={Rabeeh Karimi Mahabadi and Sebastian Ruder and Mostafa Dehghani and James Henderson},
      year={2021},
      eprint={2106.04489},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.04489}, 
}

@misc{mao2024surveyloralargelanguage,
      title={A Survey on LoRA of Large Language Models}, 
      author={Yuren Mao and Yuhang Ge and Yijiang Fan and Wenyi Xu and Yu Mi and Zhonghao Hu and Yunjun Gao},
      year={2024},
      eprint={2407.11046},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.11046}, 
}

@misc{pfeiffer2021adapterfusionnondestructivetaskcomposition,
      title={AdapterFusion: Non-Destructive Task Composition for Transfer Learning}, 
      author={Jonas Pfeiffer and Aishwarya Kamath and Andreas Rücklé and Kyunghyun Cho and Iryna Gurevych},
      year={2021},
      eprint={2005.00247},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.00247}, 
}

@misc{qin2024accuratelorafinetuningquantizationllms,
      title={Accurate LoRA-Finetuning Quantization of LLMs via Information Retention}, 
      author={Haotong Qin and Xudong Ma and Xingyu Zheng and Xiaoyang Li and Yang Zhang and Shouda Liu and Jie Luo and Xianglong Liu and Michele Magno},
      year={2024},
      eprint={2402.05445},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.05445}, 
}

@misc{song2024sparsefinetuningpretrainedlarge,
      title={Sparse is Enough in Fine-tuning Pre-trained Large Language Models}, 
      author={Weixi Song and Zuchao Li and Lefei Zhang and Hai Zhao and Bo Du},
      year={2024},
      eprint={2312.11875},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.11875}, 
}

@misc{wu2024cgfedllmcompressgradientsfederated,
      title={CG-FedLLM: How to Compress Gradients in Federated Fune-tuning for Large Language Models}, 
      author={Huiwen Wu and Xiaohan Li and Deyi Zhang and Xiaogang Xu and Jiafei Wu and Puning Zhao and Zhe Liu},
      year={2024},
      eprint={2405.13746},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.13746}, 
}

@misc{xia2024chainloraefficientfinetuning,
      title={Chain of LoRA: Efficient Fine-tuning of Language Models via Residual Learning}, 
      author={Wenhan Xia and Chengwei Qin and Elad Hazan},
      year={2024},
      eprint={2401.04151},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2401.04151}, 
}

@misc{zhang2023adaloraadaptivebudgetallocation,
      title={AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning}, 
      author={Qingru Zhang and Minshuo Chen and Alexander Bukharin and Nikos Karampatziakis and Pengcheng He and Yu Cheng and Weizhu Chen and Tuo Zhao},
      year={2023},
      eprint={2303.10512},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10512}, 
}

@misc{zhao2024galorememoryefficientllmtraining,
      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, 
      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
      year={2024},
      eprint={2403.03507},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.03507}, 
}

