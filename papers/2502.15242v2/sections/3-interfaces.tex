\section{Four Paradigmatic Interfaces for Image Generation Interactions}
\label{paradigms-interfaces}


\begin{figure}[]
    \centering
    % Top left
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[height=6cm]{assets/interface-demos/demo-baseline.pdf}
        \phantomsubcaption
        \caption*{(a) \baselinebox}
        \label{fig:image1}
    \end{subfigure}
    \hfill
    % Top right
    \begin{subfigure}[b]{0.64\textwidth}
        \centering
        \includegraphics[height=6cm]{assets/interface-demos/demo-reformulative.pdf}
        \phantomsubcaption
        \caption*{(c) \reformulativebox}
        \label{fig:image3}
    \end{subfigure}
    \vspace{4mm}
    % Bottom left
    \begin{subfigure}[b]{0.35\textwidth}
        \centering
        \includegraphics[height=6cm]{assets/interface-demos/demo-diverse.pdf}
        \phantomsubcaption
        \caption*{(b) \diversebox}
        \label{fig:image2}
    \end{subfigure}
    \hfill
    % Bottom right
    \begin{subfigure}[b]{0.64\textwidth}
        \centering
        \includegraphics[height=6cm]{assets/interface-demos/demo-agonistic.pdf}
        \phantomsubcaption
        \caption*{(d) \agonisticbox}
        \label{fig:image4}
    \end{subfigure}
    \caption{Annotated screenshots from each of the paradigmatic image generation interfaces evaluated in our study. See \S\ref{paradigms-interfaces} for details.}
    \label{fig:interface-screenshots}
\end{figure}


To holistically evaluate how agonistic design might encourages reflection (\S\ref{hai-reflection}), we create \textbf{four image generation interfaces}: \baseline, which accepts the user's prompt and outputs generated images with no further interaction, and three \textit{paradigmatic interfaces} each embodying a dominant approach or value --- \diverse~for diversity, \reformulative~for intention actualization, and \agonistic~for agonism.
In this section, we describe the important frontend and backend elements of each interface, with more details in Appendix \S\ref{detailed-view-interfaces}. 
Annotated screenshots are displayed in Figure~\ref{fig:interface-screenshots}.


% \andre{@andrew: will need to design a single figure which illustrates how each of these interfaces works. can also be a $2 \times 2$ multifigure. good option is to design in google slides and then export as .pdf}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{assets/Four Interfaces Mockup.pdf}
%     \caption{Screenshots of each of the interfaces, taken from the interview with participant 18. For a more detailed visual walkthrough of each interface, please see the supplementary \S\ref{detailed-view-interfaces}. \andre{Final version will be prettier}}
%     \label{fig:interface-screenshots}
% \end{figure}




% \S\ref{paradigms-interfaces}

% \assign[Andrew]{...}

% \subsection{\baseline}
\textbf{\baselinebox}~$\cdot$~
We design a baseline interface as a control for the study. In this interface, users enter a prompt and are displayed four images generated from the prompt.
We use a lightweight open-source image generation model (Black Forest Labs, \verb|flux-schnell|) throughout all interfaces in the study, to avoid relying on interfaces that perform under-the-hood prompt rewriting (like the DALL-E API).

% \subsection{\diverse}
\textbf{\diversebox}~$\cdot$~
\diverse~represents the paradigm of image generation that explicitly corrects for diversity issues by rewriting prompts in more diverse ways, as was reported in the Gemini case. 
To create this interface, we use a leaked alleged DALL-E prompt as reported by \cite{milmoandkern2024gemini}, which instructs the model to ``\textit{diversify depictions of ALL images with people to include DESCENT and GENDER for EACH person using direct term.}''
We instruct GPT-3.5 to rewrite the user prompt with the above instructions and use the rewritten prompt to generate four images that are then displayed to the user.
Although we recognize that the prompt may not match the exact approach used in the Gemini case, it anecdotally yields similar results to reported images for a variety of historical prompts.
Moreover, this approach makes \diverse~a realistic interface to compare against, since it uses the (alleged) prompt for the DALL-E API, which is widely in use.



% \andre{Add comment that even though B is supposed to be similar to the Gemini paradigm, we're taking it from DALL-E, and actually it's a pretty reasonable prompt, or at least it seems so, and is widely in use -- so it's a practical think to be comparing}

% \subsection{\reformulative}
\textbf{\reformulativebox}~$\cdot$~
% \reformulative~represents an approach that attempts to resolve controversies over image generation diversity by delegating control to individual users.
The \reformulative~interface embodies the value of intention actualization and is inspired by \S\ref{current-work}; it supports users by producing prompt reformulations that add detail to the prompt (e.g., adding useful semantics or stylistic indicators) in ways that are likely to result in more aesthetic or preferred images.
This interface is similar to interfaces like Promptify, which allows users to iteratively refine their prompts based on a variety of AI-generated reformulations \cite{brade2023promptify}.
Skilton and Cardinal propose using AI to generate a list of ``suggested descriptors'' that the user can then choose from and modify to create images that depart from stereotypical representations \cite{skiltonandcardinal2024inclusivepromptengineering}. 
We instruct GPT-4o to generate a diverse set of detail-adding reformulations of the user prompt with in-context examples from or inspired by Brade et al. 
% \amy{we should include all the prompts in our study in the appendix}
To aid visual navigation of reformulations, we generate a sample image (``thumbnail'') for each reformulation and display both in a list of suggested reformulations to the user.
The user can choose a suggestion and modify the suggestion before using it to generate images.
This interface is designed to have a comparable interaction level and similar features where applicable to \agonistic~for fair comparison.

% \subsection{\agonistic}
\textbf{\agonisticbox}~$\cdot$~
The \agonistic~interface attempts to forefront ambiguities and controversies over visual interpretation to the user, drawing from \S\ref{agonistic-democracy} and \S\ref{hai-reflection}.
We implement \agonistic~by creating a multi-step retrieval-augmented workflow that researches controversies about the user prompt from Wikipedia and presents them to the user.
Our work builds on insights from Cox et. al., who find that presenting users with maximally distant phrases (measured by language embeddings) is effective at enhancing diversity in creative ideation \cite{coxetal2021directeddiversity}.
Because Wikipedia allows public contributions, we believe it is more likely to capture relevant prompt controversies.% 
~$\cdot$~
We identify the main subject of the prompt with GPT-3.5 and perform a Wikipedia search for 50 pages related to the main subject. 
We then use GPT-4o to filter for 40 pages relevant to the user prompt based on their titles (excluding, for example, soccer player Gabriel Jesus from a search for ``Jesus'').
Next, we compute a controversy score for each page and rank pages by controversy, selecting the top 20 most controversial pages. 
The controversy score is calculated by dividing the total number of edits by the number of unique editors, following findings from Kittur et. al. that controversy is positively correlated with total number of edits and negatively correlated with the number of unique editors \cite{kitturetal2007conflictwikipedia}.
We chose this particular controversy metric for its efficiency and strong qualitative performance.
% to keep generation time relatively low for study interviews.
~$\cdot$~
% From this set of 40 pages, we take a random subset of 20 pages to generate interpretations of the user prompt.
We then instruct GPT-4o to generate 5 mental images an ``average person'' might have of the main subject and provide this in-context to the interpretation generation call.
For each page, GPT-4o then selects 4 sections based on their titles to read and produces an \textit{interpretation} with 4 fields: 1) a \textit{section summary} explaining the main points of the cited page section; 2) a \textit{description} of what the user's main subject looks like, taking into account the user's full prompt; 3) a \textit{source} with the referenced page and section; 4) a \textit{justification} explaining how the section content justifies the description.
The section summary is not shown to the user but is generated first to reduce model hallucinations.
~$\cdot$~
We instruct GPT-4o to generate descriptions that challenge the provided mental images, and phrase justifications in the form ``you may assume..., but... .'' These decisions were made to increase the likelihood that users would encounter discomforting suggestions, creating opportunities for critical reflection. 
Finally, like in \reformulative, we generate a ``thumbnail'' for each interpretation.
Users are shown a list of interpretations with descriptions, thumbnails, and sources; when users click on an interpretation, the card expands and the justification becomes visible.
After a 3-second wait period to encourage users to read the justification, the user can ``accept'' the interpretation and generate images, as well as edit the description text and re-generate.
The design iteration of this interface is documented in \S\ref{design-iteration}.