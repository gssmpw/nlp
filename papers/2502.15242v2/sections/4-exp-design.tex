\section{Experimental Design}
\label{experiment}

To evaluate all four interfaces with respect to  reflection, we conduct a within-subjects comparative lab study and
ask 
\textbf{(RQ1):} \textit{How} each interface induces reflection using a variety of measures for user reflection; and
\textbf{(RQ2):} \textit{Why} each interface induces reflection by examining how other  variables could explain the reflection observations in RQ1.
% We describe the user task and interview structure (\S\ref{task-structure}), the recruiting process (\S\ref{recruiting-procedure}), and our measurements (\S\ref{measures}).
% \amy{wonder if you want to start out with some RQs/summarize them here, as it was unclear at this point what the experiment is testing}
% We begin by describing the user task and the interview structure as well as the data we collect~(\S\ref{task-structure}) for each interviewee.
% Then, we describe the recruiting process and give more information about the interviewees~(\S\ref{recruiting-procedure}).
% Lastly, we generally describe the measures we take during the study to answer RQ1 and RQ2~(\S\ref{measures}).

\subsection{User Task and Interview Structure}
% \assign[Andre]{...}}
\label{task-structure}

Each lab study session was conducted 1-on-1, lasted around one hour, and consisted of the \textbf{setup} and \textbf{user task}.

\textbf{Setup ($\sim$15 mins).}
% Each interface was referenced by a letter (A, B, C, D) in participant-facing communication to avoid prejudicing participants based on interface names.
% , where \baseline~$\to$ A, \diverse~$\to$ B, \reformulative~$\to$ C, and \agonistic~$\to$ D. \amy{prob too much detail? not necessary to know letters?}
Each participant was randomly assigned to an ordering of interfaces, with \baseline~fixed as the first interface (e.g., [\baseline~\agonistic, \diverse, \reformulative] --- 6 unique orderings), to account for learning effects between non-\baseline~interfaces.
The interviewer received consent to record the interview and use anonymized data.
% collect artifacts produced during the interview, and use anonymized quotes from interview.
% Participants were asked to share their screen.
Then, to familiarize the participants with the interfaces, participants were guided through using each interface in their assigned order with the prompt ``a person.''
Afterwards, participants selected a subject (prompt) for their task; they were told to choose one of interest to them and encouraged to choose one in a randomly pre-assigned category.
Subjects all featured people and were divided into three categories --- identity  / demographics, history, and political issues. 
This choice of categories allowed for potential reflection on issues of visual representation.
% Participants were encouraged to choose a subject in a randomly pre-assigned category.
% (such that $\approx \sfrac{1}{3}$ of participants fall in each category) 
% but allowed to choose a subject in a different category if strongly desired.
Participants were provided with examples in each category for inspiration but also allowed come up with their own subjects. 
See \S\ref{participant-background} for participants' chosen prompts and \S\ref{topic-list} for example prompts.
% \amy{starts out past tense and then weirdly shifts to present tense}

\textbf{User Task ($\sim$45 mins).}
After setup, participants were informed of their task: to create a collage of ten images which represented all relevant aspects of the subject.
This task forced participants to make nuanced choices about what to include and exclude in visual representation of their chosen subject.
% for example by including two images which represent different aspects of the subject.
Participants constructed an initial collage of ten images with \baseline.
Then, they \textit{improved} their collage by interacting with the other interfaces (\diverse, \reformulative, \agonistic) in their assigned order. 
If participants produced an image they wanted to add to their collage, they picked one of their existing images to replace.
% They were also allowed not to replace any images at all.
% They were allowed to forgo replacing images if they did not produce better ones with the interface at hand.
The task design of accumulating collage improvement gave permanence to users' decisions about visual representation, provided a holistic picture of how users interacted with interfaces across different stages of exploration, and was a more natural and engaging task for participants compared with other experiment designs tested.
% (e.g., using \diverse, \reformulative, and \agonistic~to independently improve upon \baseline's collage).
% Participants interacted with each of the four interfaces for approximately ten minutes.
Participants were instructed to ``think out loud,'' commenting on their choices to include or exclude images in their collage.
Figure~\ref{fig:example-collage-progression} displays an example collage progression from the study.

% maybe note somewhere that participants were instructed to disregard accuracy of text in image generations


\subsection{Recruiting Procedure and Study Chronology}
% \assign[Andre]{...}}
\label{recruiting-procedure}

After receiving IRB approval from our university, we sent recruiting materials to undergraduate and graduate students in university departments and student groups via online communication channels.
We aimed to represent a variety of backgrounds, experiences, and interests in our participant pool.
We piloted an initial version of our study with 11 individuals and revised the task to its current form (as described in \S\ref{task-structure}) in response to time constraints and perceived confusion about the task.
Then, the two co-first authors individually conducted interviews with 29 individuals over Zoom.
Participants were compensated $\$20$ per hour.
More information about participants is available in \S\ref{participant-background}.
% \andre{This section is kind of short. Do we need it as standalone or maybe we can integrate into previous section?}
% \amy{some other details can be dropped but we should keep at least some participant info in the main body.}



\subsection{Measures}
\label{measures}

Given the work discussed in \S\ref{hai-reflection},
we measured reflection by looking quantitative and qualitative signals of how much individuals questioned or changed prior assumptions after using our interfaces. 
While it is more common to use purely qualitative metrics when measuring reflection \cite{bentvelzenetal2022revisitingreflection}, we believed a mixed-methods approach offered unique benefits for our study because quantitative metrics would allow us to more precisely compare reflection across interfaces.  
We drew from common metrics in mixed methods studies of reflection such as interviews, questionnaires, and self-reports \cite{bentvelzenetal2022revisitingreflection}. 

\textbf{Survey.}
After participants interacted with each interface, they provided responses on a 5-point Likert-style scale
% (1 $=$ ``not at all'', 3 $=$ ``somewhat'', 5 = ``entirely'')
to measure perceived rethinking (how much their mental image changed), satisfaction, appropriateness, and control (see \S\ref{survey-questions}). 
For the \reformulative~and \agonistic~interfaces, we also collected responses about how interesting the suggestions/interpretations were.
Quantitatively measuring self-reported rethinking helped capture reflection not have been recorded by other artifacts like collages (for instance, when an image caused reflection but was not added). 
We measured non-reflection variables (e.g., control) to explain why interfaces might have different reflection.

\textbf{Interview Coding.}
After conducing interviews, the two co-first authors coded interview transcripts to systematically extract further information about user experience. 
They independently coded a small subset of interviews before coming together to build a coding ontology. 
We divided codes into \textit{intents} (how user intents changed while adding an image, roughly correlated to different degrees of reflection) and \textit{values} (reasons users give for adding an image). 
Values are not mutually exclusive, because a user can invoke multiple values when adding an image.
% We decided to code for each image individually to be able to perform more rigorous quantitative coding analyses.
% We therefore note that the reflection captured by our coding ontology is a subset of all reflection during interviews, because reflection at times does not correlate to any particular image or is caused by images that were ultimately rejected by the participant.
The two first co-authors then independently coded a subset of three interviews to compute inter-rater reliability (IRR), revising the coding ontology and independently re-coding interviews as needed to resolve disagreements. 
The final IRR was 0.67 based on a weighted average of Cohen's Kappa scores across value codes. 
After coding all interviews, the two co-first authors reviewed all images marked with non-\direct~intent together to establish more agreement on intent codes.
See \S\ref{irr-methodology} for our IRR methodology and \S\ref{adding-images}, \S\ref{why-add} for a delination of intents and values.

% \andrew{TODO: move to findings}
% \amy{hmm, often codebooks are in an appendix, do you think you need it here for ppl to understand the findings?}
% \amy{after reading the rest of the paper, I'd move these to the relevant part of the findings, maybe as a table along with results counts/quotes. can do a longer codebook in appendix}




% \amy{I feel like something that's missing is something like "Measures" - basically what outcome measures are we trying to capture and how are we capturing it? The big thing I want to be able to do here is to put forth a definition of "reflection" and how we measure it grounded in prior lit. need to write defensively with R2 in mind as this is tricky to measure.}