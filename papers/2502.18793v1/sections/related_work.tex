\section{Related Work}


\subsection{Large Language Model}

The advancement of pre-training technology has significantly advanced code generation in both academia and industry ~\cite{li2022competition,shen2022incorporating,nijkamp2022codegen,fried2023incoder}. 
This has led to the emergence of numerous Large Language Models (LLMs) that have made substantial strides in code generation, including ChatGPT~\cite{openai2022chatgpt}, Magicoder~\cite{wei2023magicoder}, CodeLlama~\cite{roziere2023code}, and Qwen~\cite{bai2023qwen}, DeepSeek-Coder~\cite{deepseekcoder} and OpenCodeInterpreter~\cite{zheng2024opencodeinterpreter}.

To optimize LLMs for various code generation scenarios, some previous studies focus on enhancing prompt engineering by introducing specific patterns, such as Structured Chain-of-Thought~\cite{yin2024thinkrepair,li2025structured}, Self-planning~\cite{jiang2024self}, Self-debug~\cite{chen2023teaching,xia2023keep}, and Self-collaboration~\cite{dong2024self,yin2024rectifier}. 
However, these efforts primarily address mainstream programming languages (e.g., Java, Python, and C++)~\cite{yin2024rectifier,yin2024you,xia2023keep}.


\subsection{Code Generation Benchmark}
Existing benchmarks predominantly focus on mainstream programming languages (e.g., Python, Java), giving insufficient attention to Solidity language.


For mainstream languages, HumanEval is a widely recognized benchmark for evaluating code generation models on the functional correctness of code generated from docstrings~\cite{chen2021evaluating}. 
It consists of 164 hand-crafted programming problems, each with a corresponding docstring, solution in Python, function signature, body, and multiple unit tests. 
Following HumanEval, AiXBench~\cite{hao2022aixbench} was introduced to benchmark code generation models for Java. 
AiXBench contains 175 problems for automated evaluation and 161 problems for manual evaluation.
The authors propose a new metric to automatically assess the correctness of generated code and a set of criteria for manually evaluating the overall quality of the generated code. 
MultiPL-E~\cite{cassano2023multipl} is the first multi-language parallel benchmark for text-to-code generation. It extends HumanEval and MBPP~\cite{austin2021program} to support 18 programming languages. 


While all the aforementioned benchmarks focus on standalone functions, DS-1000~\cite{lai2023ds} introduces non-standalone functions. 
It includes 1000 problems, covering seven widely used Python data science libraries, including NumPy, Pandas, TensorFlow, PyTorch, Scipy, Scikit-learn, and Matplotlib.
To mitigate data leakage, the authors manually modify functions and emphasize the use of real development data in DS-1000.


Concode~\cite{iyer2018mapping} is a large dataset containing over 100,000 problems from Java classes in open-source projects.
The authors collect Java functions with at least one contextual dependency from approximately 33,000 GitHub repositories. 
These functions are paired with natural language annotations (e.g., Javadoc-style method descriptions) and code.
The dataset is split at the repository level rather than the function level, and while it includes contextual dependencies, it uses BLEU as the sole evaluation metric and does not evaluate the correctness of the generated functions. 
Additionally, none of the above benchmarks supports Solidity.


For Solidity language, BenchSol~\cite{benchmark2024sol} is the only available benchmark for Solidity smart contract generation. 
It contains 15 use cases of varying difficulty levels and utilizes Slither and Hardhat.
However, BenchSol is hand-crafted, poorly aligned with real-world code repositories, and extremely limited in scale, only supporting the evaluation of standalone functions (i.e., Non-repository-level generation) for LLMs.