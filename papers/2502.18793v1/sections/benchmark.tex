\section{Benchmark - \datasetname}


In this section, we describe the features of \mytitle and how to use the benchmark, including the task description, evaluation process, and metrics.


\subsection{Overview}

\datasetname contains 1,125 samples from 9 real-world code repositories. 
As shown in Fig.~\ref{fig:overview}, each sample is made up of seven components. 
\ding{182} Function Signature: The signature of the function. 
\ding{183} Requirement: The description of the function, also referred to as `comment' in the following sections. 
\ding{184} Repository: Code contexts (e.g., interfaces, functions, variables) defined outside the target code in the current repository. 
\ding{185} Context Dependency: The context dependencies that are invoked in the reference code.
\ding{186} Generated Code: The LLM-generated codes of the function. 
This code may invoke context dependencies defined in the current repository. 
\ding{187} Test Cases: Test cases are used to check the functional correctness of the smart contract. 
\ding{188} Quality: The quality attributes of the function (i.e., vulnerability type and gas fee).


Based on \mytitle, we propose a repository-level smart contract generation task. 
As shown in Fig.~\ref{fig:overview}, the task can seen as a Question (\ding{182}\ding{183}\ding{184}\ding{185}) to Answer (\ding{186}) process. 
In this task, the model is provided with a function signature, a requirement, and repository dependencies.
The model is then tasked with generating a function that satisfies the given requirement. 
Afterwards, the generated function is inserted into the repository, and its functional correctness and quality attributes are evaluated (\ding{187}\ding{188}).


\subsection{Evaluation Process}
% Since publicly available contracts can have unknown intent and generated contracts are often incorrect, executing these contracts poses a security risk~\cite{rokon2020sourcefinder}. 
% Therefore, we choose Linux Docker as the sandbox environment to safely execute untrusted contracts against unit tests.


To evaluate generated Solidity contracts, we need to set up environments for all projects. 
Since different projects often use different versions of Solidity compilers and the distribution of unit test files varies, we follow \citet{britikov2024soltg} and use the Forge test framework~\cite{Foundry} to handle these differences and execute these unit tests. 
Unlike existing study~\cite{benchmark2024sol} that uses hardhat to write JavaScript unit test scripts, we employ native Solidity unit tests to avoid introducing other programming languages. 
Then we use the Forge test framework to pipelinely compile all the projects and perform unit tests on the repositories to validate the runtime environment.


With the runtime environment ready and given an LLM to evaluate, we craft a program to inject the generated function into a contract. 
After that, the corresponding test cases will be executed and the test outputs (e.g., exceptions, panic, reversion) will be captured.
We then compare the actual output with the ground truth output, which process is automatically completed by the Forge. 


Finally, we use the testing results to compute the evaluation metrics when all the generated samples are tested. 
The detailed computation process of evaluation metrics for \mytitle is in Section~\ref{sec:eval_metrics}.


\subsection{Evaluation Metrics}
\label{sec:eval_metrics}
\textbf{Pass@k (Functional Correctness).}
Pass@K measures the percentage of the problems for which at least one correctly (judged based on executing the corresponding test cases) generated solution among the top K samples generated by the LLM. 
To avoid the issue of high sampling variance, we use the unbiased estimator of Pass@K implemented in HumanEval~\cite{chen2021evaluating}.


\noindent
\textbf{Compile@k (Functional Compilation Correctness).}
We propose the Compile@K metric to measure the percentage of the problems for which at least one correctly compiled among the top K samples generated by the LLM. 
Similarly to Pass@K, we count the number of samples \( c' \geq n \) that pass the compilation stage, and then we have


\vspace{-0.5cm}
\begin{equation}
\text{Compile@}k := \mathop{\mathbb{E}}\limits_{\text{Problems}} \left[ 1 - \frac{\binom{n-c'}{k}}{\binom{n}{k}} \right].
\end{equation}


\noindent
\textbf{Gas Fee (Gas Consumption).}
For each sample, we execute the corresponding test cases and use Forge to calculate the gas fee, denoted as \( f'_i \). 
Then, we also calculate the gas fee of the original function from the repository, denoted as \( f_i \).
Finally, for each function sample \( s \), the number of samples per function \( k \), and the base LLM \( l \), the intermediate gas fee is calculated by accumulating the difference \( (f_i - f'_i) \) for \( k \) samples per function.
This result is then accumulated for all function samples \( s \). 
Given that different LLMs can only generate the correct contract for a portion of \mytitle, and that the correctly generated functions of different LLMs often do not fully intersect, we calculate gas fees only for functions in the intersection. 
For example, consider LLM A and LLM B: LLM A can solve problems \( x \) and \( y \), while LLM B can solve problems \( y \) and \( z \). 
The capabilities intersection \( \mathcal{C}_{\text{intersect}} \) of LLM A and LLM B only includes problem \( y \), as this is the only problem both models can handle. 
Thus, we restrict our gas fee calculations to the functions within this intersection, ensuring a fair comparison across the models. 
The total gas fee for an LLM is


\vspace{-0.5cm}
\begin{equation}
\text{Gas}_{l} = \sum_{s=1}^{S} \sum_{i=1}^{k} (f_i - f'_i) \quad \text{for} \quad s \in \mathcal{C}_{\text{intersect}}.
\end{equation}


\noindent
\textbf{Vul (Vulnerability Rate).}  
We calculate the Vulnerability Rate for each LLM by using Slither to analyze the generated code for vulnerabilities flagged as `high risk' and `high confidence'. 
Functions flagged with these criteria are considered vulnerable. 
We exclude vulnerabilities with lower risk or confidence levels, as Slither often generates false positives in these categories, leading to inaccurate security assessments. 
For example, in a set of 100 functions, if 35 patches are vulnerable and top-1 samples are evaluated, the rate is 35\%.


\begin{figure}[htbp!]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/collection.pdf}
    \caption{The process of constructing \datasetname.}
    \label{fig:collection}
\end{figure}


\section{Benchmark Construction}


As shown in Fig.~\ref{fig:collection}, the construction process of \datasetname includes five phases:


\textbf{Phase \ding{182}: Project Selection.}
To ensure \datasetname's practicality and diversity, we follow best practices~\cite{chen2021evaluating,yu2024codereval,liu2024your} and select functions from different open-source projects through four steps. 
First, we manually select six popular GitHub organizations, such as OpenZeppelin, that host Solidity projects.
We crawl all their public repositories, sort them by star count in descending order, and filter out projects lacking test cases or containing fewer than 10\% files written in Solidity language.


We manually select popular GitHub organizations first to enable \datasetname to assess a modelâ€™s ability to create smart contracts that the blockchain community is more likely to use. 
We select functions that may be used in real scenarios based on the following three criteria: 
(1) We exclude trivial functions with fewer than five lines of code (LOC), as previous studies do~\cite{tse24gassmell}; 
(2) We exclude functions that are rarely deployed in real-world scenarios, as assessed by five software engineers. 
Given that developers may have varying preferences regarding frequently used functions, the inclusion of a diverse set of preferences helps mitigate potential bias; 
and (3) We exclude test functions or deprecated functions.


\textbf{Phase \ding{183}: Function Parsing.}
We extract all functions from the selected projects.
Since native Tree-sitter~\cite{treesitter} support for Solidity is inadequate for use, we design a Solidity version of Tree-sitter to accurately parse Solidity contracts and extract relevant information (e.g., function identifiers, bodies, and requirements).
From the extracted functions, we filter out tests, interfaces, and functions with LOC smaller than five, and retain those functions invoked by test functions, successfully compiled and passed the original test cases.
This process results in 1,125 function samples from different Solidity projects.


\textbf{Phase \ding{184}: Test Construction.}
To enhance the reliability of the evaluation, we take meticulous steps to ensure the correctness and completeness of the tests.
We analyze and collect the unit tests included in the project. 
For tests that did not provide sufficient line or branch coverage, we manually wrote additional test cases to ensure full line and branch coverage for the functions in \datasetname.


To further ensure the correctness of the model-generated functions, we employ advanced testing techniques such as Fuzz Testing, Invariant Testing, and Differential Testing using Forge~\cite{Foundry_Invariant_Test}. 
To maintain the reproducibility of the gas fee results, we set the fuzzing seed to a fixed value (i.e., 666).


To establish a mapping between the focal functions and their corresponding test cases, we follow \citet{nie2023learning} and select the last function call before the first assertion from the test case.
Therefore, we identify the test cases for each focal function. 
This method minimizes the number of test cases per function. 
Evaluating the correctness of a function typically requires executing all test cases, which can be time-consuming. 
Consequently, in our experiment, we execute only the test cases that directly or indirectly call the target function, thereby reducing the testing time while maintaining comprehensive test coverage.


\textbf{Phase \ding{185}: Human Annotation.}
Prompts play a crucial role in the performance of LLMs~\cite{jang2023can,sarkar2022like,shrivastava2023repository,zhou2022learning,zhou2022large}. 
In code generation tasks, the quality of the generated code is significantly influenced by the input requirements. 
Function-level comments serve multiple purposes, including explaining internal logic, describing behaviour and external usage, and stating effects and precautions~\cite{yu2024codereval}.


We recruit five software engineers with at least three years of Solidity experience to provide double-checked, manually annotated function descriptions. 
There are two reasons for incorporating manually annotated comments into \datasetname: (1) to reduce the LLMs' memorization effects, as original comments are highly likely to have been encountered during the pre-training phase, and (2) to provide high-quality comments for the functions in \datasetname. 
To ensure the quality and consistency of the annotated function descriptions, we perform an inter-annotator agreement analysis using Cohen's Kappa~\cite{mchugh2012interrater}. 
This metric assesses the level of agreement among the five annotators, accounting for the potential chance agreement and quantifying the true consistency of the annotations. 
Given that multiple annotators may interpret function descriptions differently, Cohen's Kappa provides a reliable measure of annotation reliability.


\textbf{Phase \ding{186}: Context Parse.}
One of the key differences between \mytitle and BenchSol~\cite{benchmark2024sol} is our consideration of contextual dependencies. 
In repository-level code generation, a token undefined error often occurs when the necessary context is missing, leading to compilation errors~\cite{A3CodGen}. 
Therefore, providing relevant contextual information (e.g., function signatures) is essential to help \mytitle validate the model's understanding of context.


To maintain efficiency and avoid unnecessary costs or performance degradation, it is crucial to ensure that the contextual information is concise~\cite{A3CodGen}. 
Following~\cite{yu2024codereval}, we define the context code (e.g., functions, variables, and interfaces) required by a function to execute as its contextual dependencies.
We identify the contextual dependencies of a function through a two-step program analysis of the entire project.
First, given a function to analyze, we retrieve the corresponding source file from the database, and then parse it to obtain a list of type, function, variable, and constant definitions. 
Next, we use static program analysis to identify all external invocations defined outside the current function, retrieving the signatures of these invocations. 
We then store these invocation signatures along with other relevant information about the function sample.