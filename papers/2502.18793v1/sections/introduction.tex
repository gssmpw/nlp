\section{Introduction}
The rapid expansion of blockchain technology and Decentralized Finance (DeFi) has led to a significant surge in smart contract deployments. This growth brings about increased development pressures and elevated security demands, highlighting the critical need for efficient and reliable Solidity code generation tools. As the cornerstone of Ethereum smart contracts, Solidity plays a fundamental role in enabling the decentralized applications that are driving the blockchain revolution.


\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figs/example}
\caption{Examples of standalone and non-standalone functions in Solidity with highlighted context dependencies. 
Repository-level code generation usually contains non-standalone function generation.}
\label{fig:example}
\end{figure}


\begin{table*}[htbp]
  \centering
  \caption{Comparison of existing benchmarks and \mytitle. Task: number of code generation tasks. SA Ratio: ratio of standalone functions. Dependency: number of dependencies (e.g., cross-file invocations). Avg. Token: average tokens in function requirements. Repo-Level: whether the benchmark is repository-level or not.}
  \small
    \begin{tabular}{lrrrrrrc}
    \toprule
    Benchmark & Task & SA Ratio & Dependency & File & Avg. Token & Language & Repo-Level \\
    \midrule
    CoNaLa~\cite{yin2018learning} & 500   & 100\% & 0     & 0     & 13.1  & Python & \ding{55} \\
    HumanEval~\cite{chen2021evaluating} & 164   & 100\% & 0     & 0     & 58.8  & Python & \ding{55} \\
    MBPP~\cite{austin2021program}  & 974   & 100\% & 0     & 0     & 16.1  & Python & \ding{55} \\
    PandasEval~\cite{zan2022cert} & 101   & 100\% & 0     & 0     & 29.7  & Python & \ding{55} \\
    NumpyEval~\cite{zan2022cert} & 101   & 100\% & 0     & 0     & 30.5  & Python & \ding{55} \\
    AixBench~\cite{hao2022aixbench} & 175   & 100\% & 0     & 0     & 34.5  & Java  & \ding{55} \\
    ClassEval~\cite{du2023classeval} & 100   & 100\% & 0     & 0     & / & Python & \ding{55} \\
    Concode~\cite{iyer2018mapping} & 2,000 & 20\%  & 2,455 & 0     & 16.8  & Java  & \ding{51} \\
    CoderEval~\cite{yu2024codereval} & 230   & 36\%  & 256   & 71    & 41.5  & Python, Java & \ding{51} \\
    DevEval~\cite{li2024deveval} & 1,825 & 27\%  & 4,448 & 164   & 101.6 & Python & \ding{51} \\
    \midrule
    BenchSol~\cite{benchmark2024sol} & 15    & 100\% & 0     & 0     & 41.7 & Solidity & \ding{55} \\
    \cellcolor{lightgray}\textbf{SolEval} & \cellcolor{lightgray}1,125 & \cellcolor{lightgray}89\% & \cellcolor{lightgray}822   & \cellcolor{lightgray}81    & \cellcolor{lightgray}176.4 & \cellcolor{lightgray}Solidity & \cellcolor{lightgray}\ding{51} \\
    \bottomrule
    \end{tabular}
  \label{tab:Statistics_other_benchmarks}
\end{table*}


Recently, methods based on large language models (LLMs) have become the dominant approach to code generation~\cite{radford2018improving,brown2020language,yu2024codereval}. 
These methods can generate the corresponding functions according to descriptions in natural language.
To assess the code generation capabilities of models, researchers have proposed a series of benchmarks~\cite{du2023classeval,yu2024codereval,li2024deveval,benchmark2024sol}.
As shown in Table~\ref{tab:Statistics_other_benchmarks}, most of these benchmarks focus on mainstream programming languages such as Python and Java, with little attention paid to the Solidity language.
Different from the high flexibility of programming languages like Python, Solidity's operation is constrained by gas fee (costs of executing operations on a blockchain) and blockchain immutability, making Solidity code generation more challenging than general programming languages.
To evaluate the coding abilities of LLMs in Solidity, \citet{benchmark2024sol} propose the first Solidity benchmark BenchSol.
However, BenchSol is entirely generated by GPT-4, distinct from real-world scenarios. 
Moreover, this benchmark is severely limited in scale, featuring only 15 use cases, and is restricted to evaluating LLMs on standalone functions (i.e., Non-repository-level generation).


To fill the gap in Solidity benchmarks aligned with the real world, we propose~\mytitle, the first benchmark that supports repository-level smart contract generation.
As shown in Figure~\ref{fig:example}, \mytitle contains non-standalone functions that invoke context dependencies from other files, which is absent in the existing Solidity benchmark.
\ding{182} \mytitle contains 1,125 samples from 9 real-world repositories, covering 6 popular domains (e.g., security, economics, and games).
\ding{183} \mytitle is manually annotated by 5 master's students with Solidity experience. 
\mytitle contains detailed requirements, repositories, codes, context information, and test cases.
\ding{184} To evaluate secure and cost-effective smart contract generation, we incorporate gas fee and vulnerability rate attributes into \mytitle.

We evaluate 10 popular LLMs on \mytitle, including closed-source models (e.g., GPT-4o and GPT-4o-mini) and open-source models (e.g., CodeLlama and DeepSeek).
The results reveal a striking performance gap: these models achieve a Pass@10 ranging from 5.91\% to 26.29\%, indicating that their performance in Solidity code generation is far from optimal, with significant room for improvement. The generated smart contracts exhibit varying gas fees and vulnerability rates, highlighting the dilemma of balancing cost efficiency with security in contract generation. We also have an interesting finding: DeepSeek-V3 ranks highest in Pass@10 but generates contracts with high gas fees, while DeepSeek-R1-Distill-Qwen-7B ranks lowest but generates the cheapest contracts. This contrast highlights a fundamental challenge in Solidity code generation: balancing functional correctness with gas efficiency. LLMs excelling in generating correct code may struggle with optimizing gas costs, while models focused on optimizing gas efficiency may sacrifice the quality or correctness of the generated code. Additionally, we discover that the inclusion of Retrieval-Augmented Generation (RAG) and contextual information improves model performance, highlighting the importance of incorporating contextual awareness in Solidity code generation tasks.


In summary, our contributions are as follows:


\begin{itemize}[leftmargin=*]
    \item We point out the limitations of the existing benchmark for Solidity smart contract generation, highlighting its insufficient scale and misalignment with real-world applications. 
    \item We introduce the first repository-level benchmark for Solidity smart contract generation, including a large and diverse set of 1,125 samples from 9 real-world repositories, covering 6 popular domains (i.e., security, finance, gaming, test suite, community, and gas optimization).
    This benchmark incorporates essential attributes such as gas fees and vulnerability rates, which are critical for smart contract development.
    \item We conduct an extensive evaluation of 10 state-of-the-art LLMs on \mytitle, revealing their performance gaps when generating smart contracts. We also find that LLMs can generate better contracts when using RAG and context information.
\end{itemize}


\begin{figure*}[htbp!]
    \centering
    \includegraphics[width=.9\linewidth]{figs/overview.pdf}
    \caption{Overview of the \datasetname benchmark for Solidity code generation.}
    \label{fig:overview}
\end{figure*}
% \vspace{-0.2cm}