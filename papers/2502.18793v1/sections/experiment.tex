\section{Experimental Setup}


We conduct the first study to evaluate existing LLMs on repository-level Solidity code generation by answering the following research questions:

\begin{itemize}[leftmargin=*]
\item \textbf{RQ-1 Overall Correctness.} {\em How do LLMs perform on Solidity code generation?}

\item \textbf{RQ-2 Sensitivity Analysis.} {\em How do different configurations affect the effectiveness of LLMs?}

\end{itemize}


\subsection{Studied LLMs}

We select the 10 state-of-the-art LLMs widely used in recent code generation studies~\cite{khan2023xcodeeval,yan2023codescope,A3CodGen,yu2024codereval,li2024deveval}. 
In particular, we focus on recent models released since 2022, and we exclude the small models (with less than 1B parameters) due to their limited efficacy. 
Table~\ref{tab:studied_llm} presents the 10 state-of-the-art LLMs studied in our experiments with their sizes and types. 
Our study includes a wide scope of LLMs that are diverse in multiple dimensions, such as (i) being both closed-source and open-source, (ii) covering a range of model sizes from 6.7B to 671B, (iii) being trained for general or code-specific purposes. 
For detailed descriptions of each model, refer to \S\ref{sec:base_llms}.


\begin{table}[htbp]
    \centering
    \caption{Overview of the studied LLMs}
    \resizebox{0.8\linewidth}{!}
    {
        \begin{tabular}{llc}
        \toprule
        \textbf{Type} & \textbf{Name} & \textbf{Size} \\
        \midrule
        \multirow{4}[0]{*}{\textbf{General LLM}}& DeepSeek-V3 & 671B (API) \\
        & DeepSeek-R1-Distill-Qwen & 7B / 32B  \\
        & GPT-4o & - \\
        & GPT-4o-mini & - \\
        \midrule
        \multirow{6}[0]{*}{\textbf{Code LLM}} & CodeLlama & 7B / 34B \\
        & DeepSeek-Coder & 6.7B / 33B \\
        & DeepSeek-Coder-V2-Lite & 16B \\
        & Magicoder-S-DS & 6.7B \\
        & OpenCodeInterpreter-DS & 6.7B \\
        & Qwen2.5-Coder & 7B / 32B \\
        \bottomrule
        \end{tabular}
    }
  \label{tab:studied_llm}
\end{table}
% \vspace{-0.5cm}


\subsection{Evaluation Methodology and Metrics}
We adopt the Pass@K and propose the Compile@K. 
The detailed explanations of the metrics are in \S\ref{sec:eval_metrics}.
We set the total number (denoted as n) of samples generated by an LLM to 10, and then calculate Pass@K for the LLM with Kâ€™s value of 1, 5, and 10, respectively, which is also the case for Compile@K.
When k = 1, we use the greedy search and generate a single program per requirement. 
When k > 1, we use the nucleus sampling with a temperature of 1 and sample k programs per requirement.
We set the top-p to 0.95 and the max generation length to 512.
We also use the Vul (i.e., Vulnerability Rate) and Gas Fee metrics.
The detail of these metrics is illustrated in \S\ref{sec:eval_metrics}.
We follow \citet{parvez2021retrieval,chen2024code,yin2024thinkrepair} and use RAG to select the best examples and collect a database from our projects for RAG based on the functions that are excluded from \mytitle. 
For detailed descriptions of RAG, refer to \S\ref{RAG attributes}. Note that all experimental results are averaged over five independent runs. 