% \vspace{0.8cm}
\begin{abstract}
Large language models (LLMs) have transformed code generation. 
However, most existing approaches focus on mainstream languages such as Python and Java, neglecting the Solidity language, the predominant programming language for Ethereum smart contracts.
Due to the lack of adequate benchmarks for Solidity, LLMs' ability to generate secure, cost-effective smart contracts remains unexplored.
To fill this gap, we construct \mytitle, the first repository-level benchmark designed for Solidity smart contract generation, to evaluate the performance of LLMs on Solidity. 
\mytitle consists of 1,125 samples from 9 different repositories, covering 6 popular domains, providing LLMs with a comprehensive evaluation benchmark. 
Unlike the existing Solidity benchmark, \mytitle not only includes complex function calls but also reflects the real-world complexity of the Ethereum ecosystem by incorporating gas fee and vulnerability rate.
We evaluate 10 LLMs on \mytitle, and our results show that the best-performing LLM achieves only 26.29\% Pass@10, highlighting substantial room for improvement in Solidity code generation by LLMs.
We release our data and code at \url{https://anonymous.4open.science/r/SolEval-1C06/}.
\end{abstract}