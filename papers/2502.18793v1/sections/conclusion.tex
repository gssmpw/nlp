\section{Conclusion and Future Work}


This paper presents a new benchmark named \datasetname to evaluate LLMs' effectiveness in Solidity smart contract generation scenarios.
Compared with BenchSol~\cite{benchmark2024sol}, \mytitle supports repository-level smart contract generation and excels in scale (75 times in number of tasks) and real-world code alignment.
Meanwhile, our benchmark takes vulnerability rate and gas fee into consideration, both of which are crucial for secure and cost-effective smart contract development.
The experimental results show that SolEval can reveal the weaknesses of 10 state-of-the-art LLMs, highlighting the limitations of these LLMs in generating non-standalone Solidity functions.


In the future, there are two main directions for extending \mytitle. 
Firstly, we will look for more high-quality code repositories from GitHub and enlarge our dataset with more projects and test cases. 
Secondly, we plan to support more programming languages to make it a multilingual benchmark.