\subsection{RQ-1 How do LLMs perform on Solidity code generation?}
\label{sec:rq1}


\begin{table*}[htbp!]
    \centering
    \caption{Performance of LLMs on \datasetname, evaluated using Pass@k, Compile@k, Gas fee (Fee), and Vulnerability Rate (Vul).
    The table presents results under the one-shot setting with RAG and Context. 
    Bold values indicate the highest performance in each respective column.}
    \resizebox{\linewidth}{!}
    {
        \begin{tabular}{lc|ccc|ccc|cc}
        \toprule
        LLMs & Size & Pass@1 & Pass@5 & Pass@10 & Compile@1 & Compile@5 & Compile@10 & Fee & Vul \\
        \midrule
        \multicolumn{10}{c}{\cellcolor{lightgray}6.7B to 16B} \\
        \midrule
        DeepSeek-R1-Distill-Qwen & 7B & 2.08\% & 4.50\% & 5.91\% & 6.37\% & 18.27\% & 26.29\% & -3472& \textbf{10.59\%}\\
        DeepSeek-Coder-Lite & 16B &   \textbf{10.10\%}&   14.94\%&   16.79\%&   \textbf{39.44\%}&   \textbf{54.21\%}& \textbf{57.55\%}& -8199& 26.91\%\\
        DeepSeek-Coder & 6.7B & 8.39\% & 14.25\% & 16.68\% & 32.45\% & 50.74\% & 54.59\% & -7195& 23.17\% \\
        CodeLlama & 7B & 5.15\% & 11.38\% & 14.26\% & 19.88\% & 43.05\% & 49.95\% & +18267& 25.00\% \\
        Magicoder-S-DS & 6.7B & 7.26\% & 13.80\% & 16.68\% & 26.81\% & 48.77\% & 53.64\% & -8427& 24.33\% \\
        OpenCodeInterpreter-DS & 6.7B & 7.05\% & 12.96\% & 15.66\% & 27.05\% & 48.71\% & 53.76\% & -8802& 27.08\%\\
        Qwen2.5-Coder & 7B & 9.13\% & \textbf{15.28\%}& \textbf{17.44\%}& 33.31\% & 50.34\% & 54.44\% & -9791& 29.26\% \\
        GPT-4o-mini & - & 7.18\% & 12.37\% & 14.69\% & 38.04\% & 53.18\% & 56.66\% & \textbf{-9964}& 34.01\%\\
        \midrule
        \multicolumn{10}{c}{\cellcolor{lightgray}32B to 671B} \\
        \midrule
        DeepSeek-V3 & 671B & \textbf{21.72\%}& \textbf{24.99\%}& \textbf{26.29\%}& \textbf{53.35\%}& 57.57\% & 58.61\% & -7525& 26.61\%\\
        DeepSeek-R1-Distill-Qwen & 32B & 10.19\% & 17.06\% & 19.77\% & 31.99\% & 55.31\% & 61.31\% & -7894& 23.84\%\\
        DeepSeek-Coder & 33B & 8.32\% & 15.57\% & 18.92\% & 29.35\% & 50.08\% & 55.39\% & -8706& 23.08\%\\
        CodeLlama & 34B & 6.80\% & 13.52\% & 16.47\% & 24.59\% & 48.68\% & 54.80\% & -8412& 25.47\%\\
        Qwen2.5-Coder & 32B & 13.46\% & 19.28\% & 21.44\% & 44.03\% &  55.53\% & 57.87\% & -7959& 24.52\% \\
        GPT-4o & - & 12.96\% & 20.79\% & 23.70\% & 47.04\% & \textbf{58.45\%}& \textbf{60.74\%} & \textbf{-9640}& \textbf{21.50\%}\\
        \bottomrule
        \end{tabular}%
    }
    \label{tab:rq1}%
\end{table*}%
% \vspace{-0.2cm}

\noindent 
\textbf{Evaluation of Pass@k and Compile@k for generated code.}
Table~\ref{tab:rq1} presents the overall performance of state-of-the-art LLMs on \datasetname. 
Among the 6.7B-to-16B models, DeepSeek-Coder-Lite achieves the highest Pass@1 and Compile@1, surpassing other models. 
Notably, DeepSeek-R1-Distill-Qwen-7B, which claims comparable performance to ChatGPT-o1-mini on benchmarks such as LiveCodeBench and CodeForces~\cite{deepseekr1}, underperforms compared to CodeLlama-7B. 
This discrepancy is likely due to DeepSeek-R1-Distill's lack of knowledge of Solidity, highlighting the importance of a specialized benchmark like \mytitle. 
Among the 32B-to-34B models, Qwen2.5-Coder outperforms others in both Pass@k and Compile@k.
Overall, DeepSeek-V3 performs best with a 26.29\% Pass@10.
It is noteworthy that DeepSeek-R1-Distill-Qwen-32B significantly outperforms its 7B counterpart, maintaining most of its Solidity code generation capabilities.


\noindent 
\textbf{Evaluation of Gas Fee and Vulnerability Rate for generated code.}
As shown in Table~\ref{tab:rq1}, there is a significant variation in gas fee and vulnerability rate across various LLMs. 
DeepSeek-V3 ranks first in Pass@k but generates the most gas-inefficient contracts among the 32B-to-671B models.
Additionally, GPT-4o-mini, while being outperformed by GPT-4o in Pass@k and vulnerability rate, excels in generating contracts with lower gas fee.