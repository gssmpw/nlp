\section{Benchmark - \datasetname}

\subsection{Overview}

\datasetname contains 1,125 samples from 9 real-world code repositories (see \S\ref{sec:stats}), covering 6 popular domains (e.g., security, economics, and games). 

\datasetname is designed for benchmarking LLMs on repository-level smart contract generation, which consists of two key phases: (1) LLM-based Solidity Code Generation (\S\ref{sec:llm_cg}) and (2) Post-Generation Evaluation (\S\ref{sec:eval_metrics}).


As illustrated in Fig.~\ref{fig:overview}, the first phase involves the evaluated LLM taking a function signature, requirements, and repository dependencies as input (\ding{182}\ding{183}\ding{184}\ding{185}). 
The LLM then generates a function (\ding{186}) that satisfies the specified requirements.
In the Post-Generation Evaluation phase, the generated function is integrated into the repository to get the generated smart contract, and its functional correctness (\ding{187}) and quality attributes (\ding{188}) are evaluated.

% \vspace{-0.3cm}

\subsection{LLM-based Solidity Code Generation}
\label{sec:llm_cg}

The evaluated LLM receives the following inputs: \ding{182} \textbf{Function Signature}: The function's signature. \ding{183} \textbf{Requirement}: A natural language description of the function, also referred to as `comment' in later sections. \ding{184} + \ding{185} \textbf{Repository Context}: Code contexts (e.g., interfaces, functions, variables) defined outside the target code and invoked in the reference code. 
The LLM is then prompted (see \S\ref{sec:prompt} for details) to generate a desired function, which is subsequently injected into the repository to get the smart contract for real-world code evaluation.

\subsection{Post-Generation Evaluation}
\label{sec:eval_metrics}

Following \citet{britikov2024soltg}, we utilize Forge, which handles differences across Solidity compilers and the distribution of unit test files, to execute the test cases. We evaluate functional correctness (\ding{187}) using Pass@k and Compile@k, and assess quality attributes (\ding{188}) with Gas Fee and Vul.

\paragraph{Pass@k (Functional Correctness).}
Pass@K measures the percentage of the problems for which at least one correctly (judged based on executing the corresponding test cases) generated solution among the top K samples generated by the LLM. 
To avoid the issue of high sampling variance, we use the same unbiased estimator of Pass@K implemented in HumanEval~\cite{chen2021evaluating} (see \S\ref{sec:passk} for details).


\noindent
\paragraph{Compile@k (Functional Compilation Correctness).}
We propose the Compile@K metric to measure the percentage of the problems for which at least one correctly compiled among the top K samples generated by the LLM. 
Similarly to Pass@K, we count the number of samples \( c' \leq n \) that pass the compilation stage and calculate the unbiased estimator


% \vspace{-0.3cm}
\begin{equation}
\text{Compile@}k := \mathop{\mathbb{E}}\limits_{\text{Problems}} \left[ 1 - \frac{\binom{n-c'}{k}}{\binom{n}{k}} \right].
\end{equation}


\noindent
\paragraph{Gas Fee (Gas Consumption).}
For each sample, we use Forge to execute the corresponding test cases and calculate the gas fee, denoted as \( f'_i \). 
Then, we also calculate the gas fee of the original function from the repository, denoted as \( f_i \).
Finally, for each function sample \( s \), the number of samples per function \( k \), and the base LLM \( l \), the intermediate gas fee is calculated by accumulating the difference \( (f_i - f'_i) \) for \( k \) samples per function.
This result is then accumulated for all function samples \( s \). 
Given that different LLMs can only generate the correct contract for a portion of \mytitle, and that the correctly generated functions of different LLMs often do not fully intersect, we calculate gas fees only for functions in the intersection. 
For example, consider LLM A and LLM B: LLM A can solve problems \( x \) and \( y \), while LLM B can solve problems \( y \) and \( z \). 
The capabilities intersection \( \mathcal{C}_{\text{intersect}} \) of LLM A and LLM B only includes problem \( y \), as this is the only problem both models can handle. 
Thus, we restrict our gas fee calculations to the functions within this intersection, ensuring a fair comparison across the models. 
The total gas fee for an LLM is


% \vspace{-0.3cm}
\begin{equation}
\text{Gas}_{l} = \sum_{s=1}^{S} \sum_{i=1}^{k} (f_i - f'_i) \quad \text{for} \quad s \in \mathcal{C}_{\text{intersect}}.
\end{equation}


\noindent
\paragraph{Vul (Vulnerability Rate).}  
We calculate the Vulnerability Rate for each LLM with Slither to analyze the generated code for `high risk' flagged with `high confidence'. 
Functions flagged with these criteria are considered vulnerable.
For example, in a set of 100 functions, if 35 patches are vulnerable and top-1 samples are evaluated, the rate is 35\%.


\section{Benchmark Construction}

As shown in Fig.~\ref{fig:collection}, the construction of \datasetname involves five key phases, each designed to ensure the robustness and diversity of the dataset. These phases are carefully structured to handle the complexities of smart contract generation, covering everything from project selection to context parsing.


% \begin{figure}[htbp!]
\begin{figure}
    \centering
    \includegraphics[width=.9\linewidth]{figs/collection.pdf}
    \caption{The process of constructing \datasetname.}
    \label{fig:collection}
\end{figure}
% \vspace{-0.5cm}


\subsection{Project Selection}
\label{sec:project_select}
% \textbf{Phase \ding{182}: Project Selection.}
To ensure \datasetname's practicality and diversity, we follow best practices~\cite{chen2021evaluating,yu2024codereval,liu2024your} and select functions from different open-source projects through four steps. 
First, we manually select six popular GitHub organizations, such as OpenZeppelin, that host Solidity projects.
We crawl all their public repositories, sort them by star count in descending order, and filter out low-star (i.e., with fewer than 40 stars) projects lacking test cases or containing fewer than 10\% files written in Solidity language. By manually selecting popular GitHub projects, we ensure that \datasetname assesses a modelâ€™s ability to generate smart contracts that are more likely to be used within the blockchain community.

We then select functions that may be used in real scenarios based on three criteria: 
(1) We exclude trivial functions with fewer than five lines of code (LOC), following previous studies \citep{tse24gassmell}; 
(2) We exclude functions that are rarely deployed in real-world scenarios, as assessed by five master's students. 
Given that developers may have varying preferences regarding frequently used functions, the inclusion of a diverse set of preferences helps mitigate potential bias; 
and (3) We exclude test functions or deprecated functions.


\subsection{Function Parsing}
% \textbf{Phase \ding{183}: Function Parsing.}
We extract all functions from the selected projects.
Since native Tree-sitter~\cite{treesitter} support for Solidity is inadequate for use, we design a Solidity version of Tree-sitter to accurately parse Solidity contracts and extract relevant information (e.g., function identifiers, bodies, and requirements).
From the extracted functions, we filter out tests, interfaces, and functions with LOC smaller than five, and retain those functions invoked by test functions, successfully compiled and passed the original test cases.
This process results in 1,125 function samples from different Solidity projects.

\subsection{Test Construction}
% \textbf{Phase \ding{184}: Test Construction.}
To enhance the reliability of the evaluation, we take meticulous steps to ensure the correctness and completeness of the tests.
First, We analyze and collect the unit tests included in the project. 
For tests that did not provide sufficient line or branch coverage, we manually wrote additional test cases to ensure full line and branch coverage for the functions.


To further ensure the correctness of the assessment of the generated functions, we employ advanced testing techniques (i.e., Fuzz, Invariant, and Differential Testing) using Forge~\cite{Foundry_Invariant_Test}. 
To maintain result reproducibility, we set the fuzzing seed to a fixed value (i.e., 666).


To establish a mapping between the focal functions and their corresponding test cases, we follow \citet{nie2023learning} and select the last function call before the first assertion from the test case.
Therefore, we identify the test cases for each focal function. 
This method minimizes the number of test cases per function. 
Evaluating the correctness of a function typically requires executing all test cases, which can be time-consuming. 
Consequently, in our experiment, we execute only the test cases that directly or indirectly call the target function, thereby reducing the testing time while maintaining comprehensive test coverage.

\subsection{Human Annotation}
% \textbf{Phase \ding{185}: Human Annotation.}
Prompts play a crucial role in the performance of LLMs~\cite{jang2023can,sarkar2022like,shrivastava2023repository,zhou2022learning,zhou2022large}. 
In code generation tasks, the quality of the generated code is significantly influenced by the input requirements. 
Function-level comments serve multiple purposes, including explaining internal logic, describing behaviour and external usage, and stating effects and precautions~\cite{yu2024codereval}.


We recruit five master's students with at least three years of Solidity experience to provide double-checked, manually annotated function descriptions. 
There are two reasons for incorporating manually annotated comments into \datasetname: (1) to reduce the LLMs' memorization effects, as original comments are highly likely to have been encountered during the pre-training phase, and (2) to provide high-quality comments for the functions in \datasetname. 
To ensure the quality and consistency of the annotated function descriptions, we perform an inter-annotator agreement analysis using Fleiss' Kappa~\cite{fleiss1971measuring}. We classify the annotated comments into four categories (i.e., intact, partially intact, unclear, and unlabeled). By calculating the observed agreement (\(P_o\)) and the expected agreement (\(P_e\)) under the assumption of independent classifications, Fleiss' Kappa serves as a reliable indicator of annotator alignment, ranging from complete agreement (\(\kappa = 1\)) to random agreement (\(\kappa = 0\)). We consider \(0.75\ \leq \kappa \leq 1\) an excellent level of agreement, indicating that the annotators' decisions are highly consistent.


\subsection{Context Parsing}
% \textbf{Phase \ding{186}: Context Parse.}
One of the key differences between \mytitle and BenchSol~\cite{benchmark2024sol} is our consideration of contextual dependencies. 
In repository-level code generation, a token undefined error often occurs when the necessary context is missing, leading to compilation errors~\cite{A3CodGen}. 
Therefore, providing relevant context (e.g., function signatures) is essential to help \mytitle validate the model's understanding of the requirement.


To maintain efficiency and avoid unnecessary costs or performance degradation, it is crucial to ensure that the contextual information is concise~\cite{A3CodGen}. 
Following~\cite{yu2024codereval}, we define the context code (e.g., functions, variables, and interfaces) required by a function to execute as its contextual dependencies.
We identify the contextual dependencies of a function through a two-step program analysis of the entire project.
First, given a function to analyze, we retrieve the corresponding source file from the database, and then parse it to obtain a list of type, function, variable, and constant definitions. 
Next, we use static program analysis to identify all external invocations defined outside the current function, retrieving the signatures of these invocations. 
We then store these invocation signatures along with other relevant information about the function sample.