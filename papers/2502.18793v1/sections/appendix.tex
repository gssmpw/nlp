\newpage


\appendix
% \section{Appendix}
% \label{sec:appendix}


\section{Statistics of \datasetname}
\label{sec:stats}


The statistics for the 9 projects are shown in Table~\ref{tab:dataset-statistics}. 
The functions that are filtered out can still serve as knowledge databases for RAG to select examples.
% 作为rag选取example的知识库

\begin{table}[htbp!]
\centering
\caption{The statistics of the 9 projects. 
Fi.: Filtered Functions with filter rules defined in Section~\ref{sec:project_select}.
% Func.: Total Functions. Func. (Filter): Filtered Functions.
}
\label{tab:dataset-statistics}
\resizebox{\linewidth}{!}
{
\begin{tabular}{lccc}
\toprule
\textbf{Project} & \textbf{Function} & \textbf{Test Case} & \textbf{LOC} \\
\midrule
\href{https://github.com/Vectorized/solady}{Solady} & 4,570 &  1,389 & 9.68 \\
% Openzeppelin-contracts
\href{https://github.com/OpenZeppelin/openzeppelin-contracts}{Contracts}
& 2,453  & 217 & 7.39 \\
\href{https://github.com/OpenZeppelin/ethernaut}{Ethernaut} & 445 & 86 & 6.10 \\
% Openzeppelin-foundry-upgrades
\href{https://github.com/OpenZeppelin/openzeppelin-foundry-upgrades}{foundry-upgrades} & 5,317 & 70 & 4.70 \\
\href{https://github.com/0xethsign/Account2}{Account2} & 13 & 2 & 6.93 \\
% Openzeppelin-community-contracts
\href{https://github.com/OpenZeppelin/openzeppelin-community-contracts}{community-contracts} & 1,372 & 12 & 3.77 \\
% Openzeppelin-contracts-upgradeable
\href{https://github.com/OpenZeppelin/openzeppelin-contracts-upgradeable}{contracts-upgradeable}& 1,663 & 161 & 4.53 \\
% Uniswap-solidity-hooks-template
\href{https://github.com/OpenZeppelin/uniswap-solidity-hooks-template}{Uniswap-solidity} & 39 & 10 & 15.8 \\
\href{https://github.com/foundry-rs/forge-std}{Forge-std} & 1,951 & 270 & 8.66 \\
\midrule
Total & 17,823 (Fi.: 1,125) & 2,217 & 6.76 \\
\bottomrule
\end{tabular}
}
\end{table}


\section{Experimental Details}
\subsection{Base LLMs}
\label{sec:base_llms}
In this paper, we select 10 popular LLMs as base LLMs and evaluate them on \mytitle.
The details of these LLMs are described as follows.


\begin{itemize}[leftmargin=*]
    \item GPT-4o mini~\cite{gpt4o_mini} is OpenAI's most cost-effective small model, designed to make AI technology more accessible. 
    It offers enhanced performance at a significantly reduced cost, making it over 60\% cheaper than GPT-3.5 Turbo. 
    GPT-4o mini supports both text and vision inputs and outputs. It features a context window of 128,000 tokens and can handle up to 16,000 output tokens per request. The model's knowledge base is current up to October 2023, and it utilizes an improved tokenizer for more cost-effective handling of non-English text.
    
    \item GPT-4o~\cite{openai_access_gpt4} is OpenAI's flagship model, designed to process and generate text, images, and audio inputs and outputs. Trained end-to-end across text, vision, and audio, GPT-4o is capable of handling a wide range of multimodal tasks. It delivers enhanced performance across various benchmarks, particularly excelling in voice, multilingual, and vision tasks, setting new records in audio speech recognition and translation. The model features a context window of 128,000 tokens and can handle up to 16,000 output tokens per request. Additionally, GPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average response time of 320 milliseconds, closely matching human conversation speed. While it matches GPT-4 Turbo in performance for English text and code, GPT-4o offers significant improvements in handling non-English text. Moreover, it is faster and 50\% more cost-effective in the API, with notable advancements in vision and audio understanding compared to existing models.

    \item DeepSeek-R1~\cite{deepseekr1} is a series of reasoning-focused large language models developed by DeepSeek, a Chinese AI company founded in 2023. These models are trained using large-scale reinforcement learning (RL) without prior supervised fine-tuning (SFT), enabling them to develop advanced reasoning capabilities such as self-verification, reflection, and extended chain-of-thought generation. DeepSeek-R1 has demonstrated performance comparable to OpenAI's o1 model across various tasks, including mathematics, code generation, and general reasoning. The models are available in sizes ranging from 1.5 billion to 70 billion parameters, offering flexibility for different applications. Notably, DeepSeek has open-sourced these models, allowing the research community to access and build upon their advancements. We evaluated DeepSeek-R1-Distill-Qwen-{7B, 32B} on \mytitle.

    \item CodeLlama~\cite{roziere2023code} is a family of large language models developed by Meta AI, specializing in code generation and understanding tasks. Based on the Llama 2 architecture, CodeLlama has been fine-tuned on extensive code datasets to enhance its performance in various programming languages. The models are available in sizes ranging from 7 billion to 70 billion parameters, offering flexibility to meet diverse application needs. CodeLlama supports infilling capabilities, allowing it to generate code snippets based on surrounding context, and can handle input contexts up to 100,000 tokens, making it suitable for complex code generation tasks. The family includes different variants: CodeLlama for General-purpose code synthesis and understanding, CodeLlama-Python for Python programming tasks, and CodeLlama-Instruct Fine-tuned for instruction-following tasks. These models have demonstrated state-of-the-art performance on various code-related benchmarks, including Python, C++, Java, PHP, C\#, TypeScript, and Bash. They are designed to assist in code completion, bug fixing, and other code-related tasks, thereby improving developer productivity. We evaluated CodeLlama-{7B, 34B} on \mytitle.

    \item Qwen~\cite{bai2023qwen} is a series of large language models developed by Alibaba Cloud, designed to handle a wide range of natural language processing tasks. The models are based on the Llama architecture and have been fine-tuned with techniques like supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) to enhance their performance. Qwen models are available in various sizes, ranging from 0.5 billion to 72 billion parameters, and support multilingual capabilities, including English, Chinese, Spanish, French, German, Arabic, Russian, Korean, Japanese, Thai, Vietnamese, and more. They have demonstrated competitive performance on benchmarks such as MMLU, HumanEval, and GSM8K, showcasing their proficiency in language understanding, code generation, and mathematical reasoning. We evaluated Qwen2.5-Coder-{7B, 32B} on \mytitle.

    \item Magicoder~\cite{wei2023magicoder} is a series of large language models developed by the Institute for Software Engineering at the University of Illinois Urbana-Champaign. These models are specifically designed to enhance code generation capabilities by leveraging open-source code data. Magicoder has demonstrated substantial improvements over existing code models, achieving state-of-the-art performance on various coding benchmarks, including Python text-to-code generation, multilingual coding, and data science program completion. Notably, MagicoderS-CL-7B, based on CodeLlama, surpasses prominent models like ChatGPT on the HumanEval+ benchmark, achieving a pass@1 score of 66.5 compared to ChatGPT's 65.9. This advancement underscores the effectiveness of utilizing open-source code data for instruction tuning in code generation tasks. We evaluated Magicoder-S-DS-{6.7B} on \mytitle.

    \item OpenCodeInterpreter~\cite{zheng2024opencodeinterpreter} is an open-source suite of code generation systems developed to bridge the gap between large language models and advanced proprietary systems like the GPT-4 Code Interpreter. It significantly enhances code generation capabilities by integrating execution and iterative refinement, enabling models to refine their output based on real-time execution feedback. This iterative process improves the accuracy and efficiency of generated code. The system is designed to work seamlessly with multiple programming languages and has been benchmarked against various coding tasks, demonstrating considerable improvements in code generation performance.
    
    \item DeepSeek-V3~\cite{liu2024deepseek} is a large-scale language model developed by DeepSeek, featuring 671 billion parameters with 37 billion activated for each token. It employs a Mixture-of-Experts (MoE) architecture, utilizing Multi-head Latent Attention (MLA) and DeepSeekMoE frameworks to achieve efficient inference and cost-effective training. The model was pre-trained on 14.8 trillion diverse tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to enhance its capabilities. DeepSeek-V3 has demonstrated performance comparable to leading closed-source models, while requiring only 2.788 million H800 GPU hours for full training.
        
    \item DeepSeek-Coder~\cite{deepseekcoder} is a series of code language models developed by DeepSeek, trained from scratch on 2 trillion tokens comprising 87\% code and 13\% natural language data in both English and Chinese. These models are available in sizes ranging from 1.3 billion to 33 billion parameters, offering flexibility to meet various requirements. They have demonstrated state-of-the-art performance among publicly available code models on benchmarks such as HumanEval, MultiPL-E, MBPP, DS-1000, and APPS. Additionally, DeepSeek-Coder models support project-level code completion and infilling tasks, thanks to their 16,000-token context window and fill-in-the-blank training objective. We evaluated DeepSeek-Coder-{6.7B, 33B} on \mytitle.

    \item DeepSeek-Coder-V2~\cite{deepseekcoderv2} is an open-source Mixture-of-Experts (MoE) code language model developed by DeepSeek. It builds upon the DeepSeek-V2 model, undergoing further pre-training on an additional 6 trillion tokens to enhance its coding and mathematical reasoning capabilities. This model supports an extended context length of up to 128,000 tokens, accommodating complex code generation tasks. DeepSeek-Coder-V2 has demonstrated performance comparable to leading closed-source models, including GPT-4 Turbo, in code-specific tasks. It also offers support for 338 programming languages, significantly expanding its applicability across diverse coding environments. We evaluated DeepSeek-Coder-V2-Lite-Instruct-{16B} on \mytitle.

\end{itemize}


\subsection{Experimental Settings}

We develop the generation pipeline in Python, utilizing PyTorch~\cite{paszke2019pytorch} implementations of models such as DeepSeek-Coder, CodeLlama, Qwen, and Magicoder. We load model weights and generate outputs using the Huggingface library~\cite{huggingface}. 

We select models with parameter sizes ranging from 7B to 34B, including DeepSeek-Coder 6.7B, CodeLlama 7B, Qwen2.5-Coder 7B, and a 671B DeepSeek-V3 (accessed via the online API). The constraint on model size is determined by our available computing resources. 

The evaluation is conducted on a 16-core workstation equipped with an Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz, 192GB RAM, and 8 NVIDIA RTX A8000 GPUs, running Ubuntu 20.04.1 LTS. For reproduction of the experiment in Table~\ref{tab:rq1}, approximately one week of computational time on a machine with the above configuration is required. For the experiment in Table~\ref{tab:ablation}, reproduction is estimated to take about 24 hours. The computational budget, including GPU hours, the number of GPUs, and the total parallelism across them, is crucial for understanding the computational requirements to replicate this work.

% Note that 7B models can be evaluated (with 1-shot Pass@5) on a PC equipped with a single NVIDIA RTX 4090 and 32GB RAM.


\subsection{Pass@k Calculation and Its Necessity for Estimation}
\label{sec:passk}

In this study, we adopt the Pass@k metric to evaluate the functional correctness of the generated Solidity code. The Pass@k metric has been widely used to assess the success rate of models in generating code that meets specified requirements~\cite{chen2021evaluating,yu2024codereval,benchmark2024sol}. Specifically, for each task, the model generates \( k \) code samples per problem, and a problem is considered solved if at least one of the generated samples passes the unit tests. The overall Pass@k score is then calculated by evaluating the fraction of problems for which at least one sample passes.

While the basic Pass@k metric offers a straightforward measure of success, it can have a high variance when evaluating a small number of samples. To reduce this variance, we follow a more robust approach, as outlined by \citet{kulal2019spoc}. Instead of generating only \( k \) samples per task, we generate \( n \geq k \) samples for each problem (in this study, we set \( n = 10 \) and \( k \leq 10 \)). We then count the number of correct samples, denoted as \( c \), where each correct sample passes the unit tests. The unbiased estimator for Pass@k is computed as:

\begin{equation}
\text{Pass@}k := \mathop{\mathbb{E}}\limits_{\text{Requirements}} \left[ 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} \right],
\end{equation}

where \( \binom{n}{k} \) is the binomial coefficient, representing the number of ways to choose \( k \) successful samples from \( n \) generated samples.

The reason for estimating Pass@k using this method is to account for the inherent randomness and variance in code generation tasks. Generating multiple samples per task reduces the likelihood that the model's success rate is affected by outliers or variability in the generated code. By employing this unbiased estimator, we ensure that our Pass@k metric provides a more stable and reliable evaluation of the models' performance.

The estimation approach also helps mitigate the computational cost associated with calculating Pass@k directly for each possible subset of samples, which would be computationally expensive and inefficient, especially when evaluating a large number of tasks. Thus, the unbiased estimator allows us to balance the trade-off between accuracy and computational efficiency.


\section{Benchmark Format}
\label{sec:prompt}
\subsection{Few-shot Learning}
Following previous studies~\cite{brown2020language}, few-shot learning will greatly improve the effectiveness of language models. 
Therefore, our benchmark supports prompts from one-shot to three-shot.
Theoretically, you can set n with a very large number, but that will bring serious performance issues~\cite{vaswani2017attention}. 
Here we recommend setting n below 3 for a better trade-off.


\subsection{Prompt Template}

As shown in Fig.~\ref{fig:prompt_template}, there are three parts in this prompt template.


\ding{182} Role Designation: 
We start a role for LLM with an instruction like \texttt{``// IMPLEMENT THE FUNCTIONALITY BASED ON THE PROVIDED REQUIREMENT''}.


\ding{183} Requirement: the human-written requirement for the function sample. 
We add the \texttt{``// START\_OF\_REQUIREMENT''} and \texttt{``// END\_OF\_REQUIREMENT''} instructions to help LLMs formalize their predictions.


\ding{184} Function Signature: In Fig.~\ref{fig:prompt_template}, the first function between line 4 to line 7 is for the LLM to understand the input format. 
The function signature in line 34 is provided for the LLM as a hint. 
As for Fig.~\ref{fig:LLM_output_1-shot}, the LLM generates the whole function body for \texttt{``function pack\_1\_1''} and ends the prediction with an \texttt{``// END\_OF\_FUNCTION''}.


\ding{185} Context (Optional): When a function sample has context dependency, we include the context in the prompt.
We add the \texttt{``// START\_OF\_CONTEXT''} and \texttt{``// END\_OF\_CONTEXT''} as instructions to help LLMs distinguish between context and focal function.


\subsection{Dataset Attributes}
\label{RAG attributes}
We have three data files that are required for Solidity smart contract generation.

1. \texttt{dataset.json}

2. \texttt{example.json}

3. \texttt{raw.json}

The \texttt{dataset.json} contains the detailed information (e.g., signature, function body, comment) of the to-be-generate function. 
While the \texttt{example.json} contains the functions that will be leveraged at the RAG stage. 
These functions are without test cases, but with curated comments that are useful as a part of the prompt. 
Note that when generating functions without RAG, \datasetname will randomly choose k (k-shot generation) examples from \texttt{example.json} to formulate a prompt.


In the following subsections, We will define each data attribute of \datasetname, with Fig.~\ref{fig:dataset.json} as an example.




\subsection{Source Information}

The source information that is needed to generate smart contracts is in the \texttt{dataset.json} file. We link this data source to the specific use cases by matching the \texttt{file\_path} and \texttt{identifier} columns for each function.

1. \texttt{file\_path}: This field specifies the location of the target function within the project directory. 

2. \texttt{identifier}: The identifier of the function. For the example in Fig.~\ref{fig:dataset.json}, the corresponding identifier is \texttt{pack\_1\_1}. 

3. \texttt{parameters}: The input parameters of the function.

4. \texttt{modifiers}: The function uses the \texttt{pure} modifier, indicating that it does not alter the state of the blockchain and performs computations based solely on the input parameters.

5. \texttt{return}: The function returns a single \texttt{bytes2} value. This return type signifies that the result of the operation is a 2-byte value combining the two 1-byte values.

6. \texttt{body}: The whole function body.

7. \texttt{start}: The line in the file where the \texttt{pack\_1\_1} function begins is at line 39. This value is used for locating and patching the function.

8. \texttt{end}: The function's implementation ends at line 45 in the file.

9. \texttt{class}: The function is part of the \texttt{Packing} class.

10. \texttt{signature}: The function's signature, which is used to define the function's external API, succinctly describes the function's input parameters and return type.

11. \texttt{full\_signature}: The full signature clearly indicates the function's internal visibility and pure nature. This attribute is useful when prompting the LLMs to generate the whole function.

12. \texttt{class\_method\_signature}: This identifies the function within its class and shows the types of parameters it accepts.

13. \texttt{comment}: The original comment of the target function, without any human labor.

14. \texttt{sol\_version}: The function is compatible with Solidity version \texttt{\^0.8.20}, as indicated in the pragma statement. Many contracts behave differently between different solidity compiler versions, sometimes they may even fail to compile.

15. \texttt{import\_directive}: This function has no import dependency.

16. \texttt{context}: The context dependency of a focal function.

17. \texttt{human\_labeled\_comment}: The human-labeled comment.



\section{The License For Artifacts}
\label{sec:license}
The benchmark dataset presented in this work is released under the MIT License, a permissive open-source license that grants users unrestricted rights to utilize, modify, and distribute the resource for both academic and commercial purposes. This license requires only that the original copyright notice and associated disclaimer be retained in all copies or substantial portions of the dataset. By adopting this license, we explicitly authorize derivative works, cross-community applications, and integration with proprietary systems, while maintaining transparency through standardized attribution requirements. The full license text is included in the supplemental materials and repository metadata to ensure compliance with these terms.


\section{Human Annotations}
We recruit five master's students with at least three years of Solidity experience to manually annotate the function descriptions in \datasetname. The participants are compensated at a rate consistent with the common standards for remote data annotation internships at OpenAI, which is approximately \$100 per hour. This payment rate is considered fair given the participants' demographic and their expertise in Solidity. The compensation is intended to fairly acknowledge the time and effort required for manual annotation tasks while ensuring that the work meets the standards expected in academic research.


\subsection{Instructions Given to Participants}

For the annotation of function descriptions in \datasetname, detailed instructions were provided to all participants to ensure clarity and consistency in the annotation process. These instructions outlined the specific tasks to be completed, the scope of the data involved, and the expected format for the annotations. The instructions included the following key points:
\begin{itemize}[leftmargin=*]
    \item A clear explanation of the purpose of the annotation task: participants were informed that their role was to provide accurate, manually annotated descriptions for Solidity function definitions to support research on code generation models.
    \item Guidelines for how to annotate the functions: Participants were instructed on how to write concise and informative comments, ensuring that these comments explained the internal logic, usage, and any potential effects or precautions associated with the functions.
    \item Ethical considerations: Participants were reminded to ensure that no private, sensitive, or proprietary information was included in their annotations, and that their annotations should not contain offensive or harmful content.
    \item Data usage and confidentiality: Participants were explicitly informed that their annotations would be used in a publicly available benchmark for academic research purposes. Their identities were kept confidential, and they were reassured that the data would be stored securely.
    \item Risk Disclaimer: Although no direct risks were associated with the task, participants were informed about the potential for their annotations to be included in publicly available datasets, thereby contributing to research in the field of Solidity code generation.
\end{itemize}


The full text of the instructions, including disclaimers, was made available to all participants prior to their involvement, and they were asked to confirm their understanding and agreement to these terms before proceeding with the annotation task.




\subsection{Consent for Data Usage}

In this study, all data used for \datasetname was collected from publicly available open-source Solidity smart contract repositories. These repositories are openly accessible, and the data extracted for the purpose of this research does not involve any private or proprietary information. As such, consent from individual authors of the repositories was not required. For the manual annotation of function descriptions, the participating master's students were fully informed about the scope and use of the data. Prior to their involvement, detailed instructions were provided, clarifying how the data would be used for the sole purpose of evaluating code generation models and advancing research in Solidity code generation. Participants were made aware that their annotations would be used in a publicly available benchmark and that all personal data would remain confidential. 

Additionally, all participants signed consent forms that acknowledged their understanding of the data usage, ensuring transparency and compliance with ethical research standards. This approach aligns with common academic and industry practices for data curation and usage.


\section{Artifact Use Consistentency}

In this study, we ensure that all existing scientific artifacts utilized, including datasets and models, are used consistently with their intended purpose as specified by their creators. For instance, datasets and tools used for code generation and evaluation in Solidity were sourced and implemented following the terms set by the original authors. We strictly adhered to the licensing agreements and usage restrictions outlined for each artifact. Any modifications made to the artifacts, such as the adaptation of existing datasets for Solidity smart contract generation, were performed within the bounds of academic research and in compliance with the access conditions (\S\ref{sec:license}).

For the artifacts we created, including the \datasetname benchmark and related tools, we clearly define their intended use within the context of this research. These artifacts are designed for evaluating large language models (LLMs) on Solidity code generation tasks and should only be used within the scope of academic or research purposes. Derivatives of the data used in this research, such as model outputs or analysis results, will not be used outside of these contexts to ensure compliance with ethical and licensing guidelines.


\section{Data Containing Personally Identifying Information or Offensive Content}

To ensure the ethical integrity of our research, we carefully examined the data collected for \datasetname to verify that it does not contain any personally identifying information (PII) or offensive content. The data used in our benchmark consists of Solidity smart contracts sourced from publicly available repositories, with no inclusion of private or sensitive personal information. We specifically focused on the code and its associated requirements, ensuring that any metadata related to individual contributors or personal identifiers was excluded.

Additionally, we employed a manual review process to identify and filter any potentially offensive content within the code, comments, or requirements. We worked with our annotators to establish clear guidelines for identifying content that could be deemed inappropriate or offensive, ensuring that all samples in \datasetname adhered to a high standard of professionalism and respectfulness. This process helps maintain the privacy and safety of individuals and ensures the ethical use of the data in our research. Any identified offensive or sensitive content was removed before inclusion in the benchmark.


\section{Potential Risks}

While the research presented in this paper contributes to advancing Solidity code generation using large language models (LLMs), several potential risks associated with this work must be considered. These risks include both intentional and unintentional harmful effects, as well as broader concerns related to fairness, privacy, and security.

\begin{enumerate}[leftmargin=*]
    \item \textbf{Malicious or Unintended Harmful Effects:} 
    The generation of smart contracts through LLMs may inadvertently lead to the creation of faulty or insecure contracts that, if deployed in production environments, could be exploited by malicious actors. These contracts might not only be prone to security vulnerabilities but could also be misused for illicit purposes, such as financial fraud or exploitation of blockchain systems. This highlights the importance of integrating robust security evaluation mechanisms like gas fee analysis and vulnerability detection into the evaluation pipeline, as we have done in this study.
    
    \item \textbf{Environmental Impact:} 
    The computational resources required for training and fine-tuning large-scale models, such as the ones used in this research, contribute to the environmental impact of AI research. Training these models requires significant GPU hours, and the energy consumption associated with this process is a growing concern. Future work should explore ways to mitigate the environmental impact by improving the efficiency of the models or exploring more energy-efficient approaches to training.
    
    \item \textbf{Fairness Considerations:} 
    One potential risk of deploying these technologies is the possibility of exacerbating existing biases or inequalities in the blockchain space. If the models are trained on a narrow set of data sources, there is a risk that they could generate code that is biased or not applicable to the needs of diverse or marginalized groups. To address this, we ensure that our dataset includes a broad range of real-world repositories to enhance the generalizability and fairness of our model evaluations.
    
    \item \textbf{Privacy and Security Considerations:} 
    Since the data used in this research comes from publicly available smart contract repositories, there are minimal privacy concerns. However, security risks are inherent in the generation of smart contracts, particularly when models are not fully vetted for safety or are used to create contracts that interact with real assets. These models could unintentionally generate code with vulnerabilities or flaws that put users or systems at risk. We address this by using static analysis tools like Slither to detect vulnerabilities in the generated contracts.
    
    \item \textbf{Dual Use:} 
    The technology presented in this research, although intended for advancing smart contract generation for legitimate use cases, could be misused. For example, the ability to generate smart contracts quickly might be exploited to create malicious contracts or to automate the creation of fraudulent systems. Moreover, incorrect or insecure code generated by the models could result in unintended consequences if it is used in production environments.
    
    \item \textbf{Exclusion of Certain Groups:} 
    While the research focuses on Solidity, it is important to consider that smart contract technology is not equally accessible or relevant to all communities. There is a risk that focusing on Ethereum-based contracts could inadvertently exclude developers or communities working on other blockchain ecosystems. We advocate for future research to expand the capabilities of such models to support multiple blockchain platforms, ensuring inclusivity in the adoption of LLM-generated code.
\end{enumerate}

In conclusion, while our research aims to contribute positively to the development of secure and efficient Solidity code generation, it is crucial to acknowledge these potential risks and actively work toward mitigating them. Future work can build upon these findings to improve model robustness, security, and fairness in the context of blockchain technologies.


\section{AI Assistants in Research and Writing}

Yes, we did utilize AI assistants in certain aspects of our research and writing process. Specifically, we employed generative AI tools, such as ChatGPT, to assist with writing portions of the Python code and in drafting parts of the appendix, as well as for polishing and refining sections of the paper. The AI tools were particularly helpful for enhancing clarity, improving grammatical structure, and ensuring a more concise presentation of our ideas.

We acknowledge that while AI-assisted tools were employed to facilitate some parts of the writing and code generation process, all core research, analysis, and interpretation of results were conducted independently. The use of AI tools was limited to supporting tasks that did not impact the integrity or originality of the research. Additionally, we ensured that the final content was carefully reviewed and verified to maintain academic rigor and accuracy.


\begin{figure*}[htbp]
\centering
\begin{lstlisting}[language=json, firstnumber=1, xleftmargin=5.0ex]
// IMPLEMENT THE FUNCTIONALITY BASED ON THE PROVIDED REQUIREMENT.

// START_OF_REQUIREMENT
/**
 * @notice Packs a uint160 value into a DynamicBuffer.
 * Steps:
 * 1. Deallocate the memory of the result buffer to ensure it is clean.
 * 2. Pack the uint160 value into the buffer using the `p` function, ensuring the data is treated as a 20-byte value.
 * 3. Return the updated buffer.
 */
// END_OF_REQUIREMENT

// START_OF_FUNCTION
function pUint160(DynamicBuffer memory buffer, uint160 data)
    internal pure returns (DynamicBuffer memory result) {
        _deallocate(result);
        result = p(buffer, _single(data, 20));
}
// END_OF_FUNCTION

// IMPLEMENT THE FUNCTIONALITY BASED ON THE PROVIDED REQUIREMENT.

// START_OF_REQUIREMENT
/**
 * @notice Packs two `bytes1` values into a single `bytes2` value.
 * @param left The first `bytes1` value to be packed.
 * @param right The second `bytes1` value to be packed.
 * @return result The resulting `bytes2` value after packing.
 * Steps:
 * 1. Clear the higher bits of the `left` value to ensure it fits within the lower 8 bits.
 * 2. Clear the higher bits of the `right` value to ensure it fits within the lower 8 bits.
 * 3. Combine the `left` and `right` values into a single `bytes2` value by shifting and OR-ing them.
 * @dev This function uses inline assembly to perform bitwise operations for efficiency.
 */
// END_OF_REQUIREMENT

// START_OF_FUNCTION
function pack_1_1(bytes1 left, bytes1 right) internal pure returns (bytes2 result) {
\end{lstlisting}
\caption{A 1-shot prompt example}
\label{fig:prompt_template}
\end{figure*}




\begin{figure*}[htbp!]
\centering
\begin{lstlisting}[language=json, firstnumber=1, xleftmargin=2.0ex]
assembly {
    // Clear higher bits of left and right to fit in bytes1
    let clearedLeft := and(left, 0xFF)
    let clearedRight := and(right, 0xFF)
    // Combine the two bytes1 values into a bytes2 value
    // Shift left by 8 bits and OR with the right value
    result := or(shl(8, clearedLeft), clearedRight)
}
}
// END_OF_FUNCTION
\end{lstlisting}
\caption{The LLM output for 1-shot prompt example}
\label{fig:LLM_output_1-shot}
\end{figure*}


% \vspace{-1.5cm}
\begin{figure*}[htbp]
\centering
\begin{lstlisting}[language=json, firstnumber=1, xleftmargin=2.0ex]
{
  "openzeppelin-contracts/contracts/utils/Packing.sol": [
    { "identifier": "pack_1_1",
      "parameters": "bytes1 left, bytes1 right",
      "modifiers": "pure",
      "return": "returns (bytes2 result)",
      "body": "function pack_1_1(bytes1 left, bytes1 right) internal pure returns (bytes2 result) {\n        assembly (\"memory-safe\") {\n            left := and(left, shl(248, not(0)))\n            right := and(right, shl(248, not(0)))\n            result := or(left, shr(8, right))\n        }\n    }",
      "start": "39",
      "end": "45",
      "class": "Packing",
      "signature": "returns (bytes2 result) pack_1_1 bytes1 left, bytes1 right",
      "full_signature": "function pack_1_1(bytes1 left, bytes1 right) internal pure returns (bytes2 result)",
      "class_method_signature": "Packing.pack_1_1 bytes1 left, bytes1 right",
      "testcase": "",
      "constructor": "False",
      "comment": "",
      "visibility": "internal",
      "sol_version": ["pragma solidity ^0.8.20;"],
      "import_directive": "",
      "context": "",
      "human_labeled_comment": "/**\n * @notice Packs two `bytes1` values into a single `bytes2` value.\n *\n * @param left The first `bytes1` value to be packed.\n ......*/",},
    ......
  ],
  ......
}
\end{lstlisting}
\caption{A short example of \texttt{dataset.json}}
\label{fig:dataset.json}
\end{figure*}



\begin{figure*}[htbp]
\centering
\begin{lstlisting}[language=json, firstnumber=1, xleftmargin=5.0ex]
// IMPLEMENT THE FUNCTIONALITY BASED ON THE PROVIDED REQUIREMENT.

// START_OF_REQUIREMENT
(Example Requirement...)
// END_OF_REQUIREMENT

// START_OF_FUNCTION
(Example Function...)
// END_OF_FUNCTION

// IMPLEMENT THE FUNCTIONALITY BASED ON THE PROVIDED REQUIREMENT.

// START_OF_REQUIREMENT
/**
 * @notice Internal function to perform a custom burn operation using low-level assembly.
 * Steps:
 * (Some more lines about details)
 */
// END_OF_REQUIREMENT

// START_OF_FUNCTION
function burn(uint256 x) internal {
    assembly {
        let m := mload(0x40)
        let n := div(x, 17700)
        if lt(x, 18000) { n := 0 }
        mstore(m, or(or(address(), shl(160, origin())), shl(192, timestamp())))
        extcodecopy(address(), add(m, 0x20), keccak256(m, 0x20), 2080)
        let i := 0
        for { } 1 { } {let h := keccak256(m, 0x850)
        mstore(m, h)
        extcodecopy(address(), add(m, add(0x20, and(h, 0x7ff))), keccak256(m, 0x20), 2080)
        mstore(add(m, 2048), not(h))
        if eq(i, n) {n := add(mod(x, 17700), 0x31)
            mstore(add(m, n), h)
            log0(m, add(n, 0x20))
            break
        }
        log0(m, 2080)
        i:= add(i, 1)
        }
    }
// END_OF_FUNCTION
\end{lstlisting}
\caption{LLMs can generate pretty nice contracts (GPT-4o with one-shot)}
\label{fig:LLMs can generate pretty nice contracts}
\end{figure*}

\begin{figure*}[htbp]
\centering
\begin{lstlisting}[language=json, firstnumber=1, xleftmargin=5.0ex]
// IMPLEMENT THE FUNCTIONALITY BASED ON THE PROVIDED REQUIREMENT.

// START_OF_REQUIREMENT
(Example Requirement...)
// END_OF_REQUIREMENT

// START_OF_CONTEXT
No context for this function
// END_OF_CONTEXT

// START_OF_FUNCTION
(Example Function...)
// END_OF_FUNCTION

// IMPLEMENT THE FUNCTIONALITY BASED ON THE PROVIDED REQUIREMENT.

// START_OF_REQUIREMENT
/**
 * @notice Performs a bitwise AND operation on two boolean values using inline assembly.
 *
 * @param x The first boolean value.
 * @param y The second boolean value.
 * @return z The result of the bitwise AND operation between `x` and `y`.
 *
 * Steps:
 * 1. Use inline assembly to perform the bitwise AND operation on `x` and `y`.
 * 2. Store the result in `z` and return it.
 *
 * @dev This function is marked as `internal pure` and uses `memory-safe-assembly` to ensure safety.
 */
// END_OF_REQUIREMENT

// START_OF_FUNCTION
function rawAnd(bool x, bool y) internal pure returns (bool z) {
    using assembly {
        let z := x & y
    }
}
// END_OF_FUNCTION
\end{lstlisting}
\caption{LLMs can generate really dumb contracts (DeepSeek-R1-Distill-Qwen-7B with one-shot)}
\label{fig:LLMs can generate really dumb contracts}
\end{figure*}
