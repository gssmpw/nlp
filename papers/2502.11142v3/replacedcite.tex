\section{Related Work}
\noindent \textbf{Vision-and-Language Navigation (VLN)}____ enables embodied agents to navigate to the destination described by the language instructions.
Early VLN researches focus on discrete environments within 90 scenes of Matterport3D____, which uses a predefined navigation graph, the agent observes panoramic RGB and depth images, teleporting between graph nodes to follow natural language instructions. Under this setting, the datasets include the step-by-step instruction dataset R2R____,  the multilingual instruction dataset RxR____ with longer trajectories, the Remote Embodied Visual Referring Expression (REVERIE)____ dataset, and the Scenario Oriented Object Navigation (SOON)____ task.
Although efficient for training in discrete environments, these datasets lack real-world applicability. To address this, R2R-CE____ introduce continuous environments____ with instructions from the R2R dataset, where agents navigate freely in 3D spaces using low-level actions (e.g., turn 15Â°, move 0.25m) in the Habitat simulator____. %Similarly, RxR-CE____ is also transferred to the continuous environment. 
In this work, we focus on generating large-scale, high-quality navigation instructions, for simplicity and efficiency, our NavRAG is currently validated in the discrete environments, while the annotated data remains easily transferable to continuous settings.

\noindent \textbf{Navigation Instruction Generation} is an effective approach to addressing the scarcity of training data for VLN. Speaker-follower____ and Env-Drop____ use the LSTM-based instruction generator to generate the offline augmented instructions. VLN-Trans____ propose a translator module that enables the navigation agent to generate more concise sub-instructions, leveraging recognizable and distinctive landmarks. 
AutoVLN____, MARVAL____ and ScaleVLN____ leverage multiple foundation models____ and use more 3D scenes to annotate instructions, such as HM3D____ and Gibson____. Recently, more works focus on designing more powerful instruction generator, such as a joint structure for instruction following and generation____, Knowledge enhanced speaker____, LLM instruction generator with chain of thought prompting____, and LLM instruction generator with BEV perception____. However, these methods are limited to identifying landmarks in navigation trajectories and generating low-level instructions, making it difficult to integrate global context, match user demands, and plan high-level tasks. NavRAG will generate navigation instructions better tailored to the application scenario by considering the global context and user demands through scene description trees and retrieval-augmented LLM.

\noindent \textbf{Retrieval-Augmented Generation (RAG)}____  was initially introduced to enhance LLMs by retrieving relevant document chunks, thereby providing domain-specific knowledge for better answer. Over time, several innovations have expanded on this idea, including techniques like iterative knowledge retrieval____, and the incorporation of knowledge graphs____. Furthermore, adapting RAG to the field of robotics, some works____ attempt constructing non-parametric memory or scene graphs for 3D scenes, and utilize retrieval-augmented LLM for question answering or navigation. However, traditional RAG methods for scene graph retrieval struggle to balance global context with local details and interpret the environment layout. NavRAG leverages the scene description tree and hierarchical retrieval strategy, achieve better scene understanding.