\section{Related Work}
\noindent \textbf{Vision-and-Language Navigation (VLN)}~\cite{anderson2018vision,krantz2020beyond,qi2020reverie,zhu2021soon} enables embodied agents to navigate to the destination described by the language instructions.
Early VLN researches focus on discrete environments within 90 scenes of Matterport3D~\cite{chang2017matterport3d}, which uses a predefined navigation graph, the agent observes panoramic RGB and depth images, teleporting between graph nodes to follow natural language instructions. Under this setting, the datasets include the step-by-step instruction dataset R2R~\cite{anderson2018vision},  the multilingual instruction dataset RxR~\cite{ku2020room} with longer trajectories, the Remote Embodied Visual Referring Expression (REVERIE)~\cite{qi2020reverie} dataset, and the Scenario Oriented Object Navigation (SOON)~\cite{zhu2021soon} task.
Although efficient for training in discrete environments, these datasets lack real-world applicability. To address this, R2R-CE~\cite{krantz2020beyond} introduce continuous environments~\cite{savva2019habitat} with instructions from the R2R dataset, where agents navigate freely in 3D spaces using low-level actions (e.g., turn 15Â°, move 0.25m) in the Habitat simulator~\cite{savva2019habitat}. %Similarly, RxR-CE~\cite{ku2020room} is also transferred to the continuous environment. 
In this work, we focus on generating large-scale, high-quality navigation instructions, for simplicity and efficiency, our NavRAG is currently validated in the discrete environments, while the annotated data remains easily transferable to continuous settings.

\noindent \textbf{Navigation Instruction Generation} is an effective approach to addressing the scarcity of training data for VLN. Speaker-follower~\cite{fried2018speaker} and Env-Drop~\cite{tan2019learning} use the LSTM-based instruction generator to generate the offline augmented instructions. VLN-Trans~\cite{zhang2023vln} propose a translator module that enables the navigation agent to generate more concise sub-instructions, leveraging recognizable and distinctive landmarks. 
AutoVLN~\cite{chen2022learning}, MARVAL~\cite{kamath2023new} and ScaleVLN~\cite{wang2023scaling} leverage multiple foundation models~\cite{cheng2022masked,radford2019language,zhaolarge,koh2023simple} and use more 3D scenes to annotate instructions, such as HM3D~\cite{ramakrishnan2habitat} and Gibson~\cite{xia2018gibson}. Recently, more works focus on designing more powerful instruction generator, such as a joint structure for instruction following and generation~\cite{wang2023lana}, Knowledge enhanced speaker~\cite{zeng2023kefa}, LLM instruction generator with chain of thought prompting~\cite{kong2025controllable}, and LLM instruction generator with BEV perception~\cite{fan2025navigation}. However, these methods are limited to identifying landmarks in navigation trajectories and generating low-level instructions, making it difficult to integrate global context, match user demands, and plan high-level tasks. NavRAG will generate navigation instructions better tailored to the application scenario by considering the global context and user demands through scene description trees and retrieval-augmented LLM.

\noindent \textbf{Retrieval-Augmented Generation (RAG)}~\cite{lewis2020retrieval}  was initially introduced to enhance LLMs by retrieving relevant document chunks, thereby providing domain-specific knowledge for better answer. Over time, several innovations have expanded on this idea, including techniques like iterative knowledge retrieval~\cite{shao2023enhancing}, and the incorporation of knowledge graphs~\cite{edge2024local}. Furthermore, adapting RAG to the field of robotics, some works~\cite{xie2024embodied,booker2024embodiedrag} attempt constructing non-parametric memory or scene graphs for 3D scenes, and utilize retrieval-augmented LLM for question answering or navigation. However, traditional RAG methods for scene graph retrieval struggle to balance global context with local details and interpret the environment layout. NavRAG leverages the scene description tree and hierarchical retrieval strategy, achieve better scene understanding.