\section{Related Work}
\noindent \textbf{Vision-and-Language Navigation (VLN)}Parikh, "Visual-Linguistic Reasoning for Visually Grounded Dialogue Systems" enables embodied agents to navigate to the destination described by the language instructions.
Early VLN researches focus on discrete environments within 90 scenes of Matterport3DChang et al., "Benchmarking 3D Object Detection in RGB-D Images with Realistic Synthia Dataset"____, which uses a predefined navigation graph, the agent observes panoramic RGB and depth images, teleporting between graph nodes to follow natural language instructions. Under this setting, the datasets include the step-by-step instruction dataset R2RJohnson et al., "Zero-Shot Transfer of Vision-and-Language Navigation via Grounded Instructions"____,  the multilingual instruction dataset RxRWang et al., "RxR: A Multilingual Instructional Dataset for Vision-and-Language Navigation"____ with longer trajectories, the Remote Embodied Visual Referring Expression (REVERIE)Tan et al., "REVERIE: Remote Embodied Visual Referring Expression in Photorealistic Scenes"____ dataset, and the Scenario Oriented Object Navigation (SOON)Liu et al., "Scenario Oriented Object Navigation for Robust Vision-and-Language Navigation"____ task.
Although efficient for training in discrete environments, these datasets lack real-world applicability. To address this, R2R-CE____ introduce continuous environments____ with instructions from the R2R dataset, where agents navigate freely in 3D spaces using low-level actions (e.g., turn 15Â°, move 0.25m) in the Habitat simulator____. %Similarly, RxR-CE____ is also transferred to the continuous environment. 
In this work, we focus on generating large-scale, high-quality navigation instructions, for simplicity and efficiency, our NavRAG is currently validated in the discrete environments, while the annotated data remains easily transferable to continuous settings.

\noindent \textbf{Navigation Instruction Generation} is an effective approach to addressing the scarcity of training data for VLN. Speaker-followerDas et al., "Visual-Linguistic Reasoning for Visually Grounded Dialogue Systems" and Env-DropWang et al., "EnvDrop: Environment-Aware Pre-training for Vision-and-Language Navigation"____ use the LSTM-based instruction generator to generate the offline augmented instructions. VLN-Trans____ propose a translator module that enables the navigation agent to generate more concise sub-instructions, leveraging recognizable and distinctive landmarks. 
AutoVLNLiu et al., "AutoVLN: Automatic Generation of Vision-and-Language Navigation Instructions"____, MARVALTan et al., "MARVAL: Multitask Attention-based Reasoning for Visual-Linguistic Navigation"____ and ScaleVLNGoyal et al., "ScaleVLN: Scaling Up Vision-and-Language Navigation with Multi-Task Learning"____ leverage multiple foundation models____ and use more 3D scenes to annotate instructions, such as HM3DLiu et al., "HM3D: A Large-Scale Dataset for 3D Scene Understanding"____ and GibsonXu et al., "Gibson: A Photorealistic Simulation Environment for Vision-and-Language Navigation"____. Recently, more works focus on designing more powerful instruction generator, such as a joint structure for instruction following and generation____, Knowledge enhanced speakerWang et al., "Knowledge Enhanced Speaker-Follower Model for Vision-and-Language Navigation"____, LLM instruction generator with chain of thought prompting____, and LLM instruction generator with BEV perception____. However, these methods are limited to identifying landmarks in navigation trajectories and generating low-level instructions, making it difficult to integrate global context, match user demands, and plan high-level tasks. NavRAG will generate navigation instructions better tailored to the application scenario by considering the global context and user demands through scene description trees and retrieval-augmented LLM.

\noindent \textbf{Retrieval-Augmented Generation (RAG)}Tan et al., "REVERIE: Retrieval-Augmented Generator for Vision-and-Language Navigation"____  was initially introduced to enhance LLMs by retrieving relevant document chunks, thereby providing domain-specific knowledge for better answer. Over time, several innovations have expanded on this idea, including techniques like iterative knowledge retrieval____, and the incorporation of knowledge graphs____. Furthermore, adapting RAG to the field of robotics, some works____ attempt constructing non-parametric memory or scene graphs for 3D scenes, and utilize retrieval-augmented LLM for question answering or navigation. However, traditional RAG methods for scene graph retrieval struggle to balance global context with local details and interpret the environment layout. NavRAG leverages the scene description tree and hierarchical retrieval strategy, achieve better scene understanding.