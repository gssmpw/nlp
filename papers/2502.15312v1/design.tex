\section{\sysname Design}
\label{sec-design}
\subsection{Architecture Overview}
\begin{figure}[t]
	\centering
	\includegraphics[width=0.55\textwidth]{fig/CMDPArch.pdf}
	\caption{The architecture of \sysname}
	\label{fig:arch}
\end{figure}
Figure~\ref{fig:arch} illustrates the architecture of \sysname. \sysname takes the computation graph as the general intermediate input and can 
support the inference computation for pre-trained models generated from multiple training frameworks (e.g., PyTorch, MindSpore, Tensorflow, etc). Besides, \sysname also integrates pre-optimization strategies from Xenos~\cite{zhang2023xenos} to optimize computation graph before it is fed into the automatic optimizer of \sysname.

The automatic optimizer consists of two major modules, namely the data-driven cost estimator (CE) and the dynamic partition planner (DPP). CE (\S\ref{sec:gdbt-estimator}) is implemented with a Gradient Boosting Decision Tree (GBDT). The GBDT is pre-trained with more than 330K pieces of trace data, collected from running different model inference workloads under a variety of testbed settings. DPP runs a dynamic programming algorithm (\S\ref{sec:dpp}) to search for the efficient model partition scheme. During runtime, DPP contacts CE to get an estimated time cost for the partition scheme in its consideration. Based on the estimated costs, DPP reserves the promising candidate schemes and continues its search. When the DP algorithm is completed, DPP will output the complete model partition scheme with the lowest estimated time cost. Then the inference engine drives multiple edge devices (nodes) to jointly execute the distributed inference computation according to the partition scheme. % We next describe CE and DPP in more detail.


% Specifically, we run different inference models and collect the inference times of these trials. For each trial, we record the shapes of the model (including the sizes of InW, InH, InC, outC, etc) and the testbed settings (including the number of edge nodes, the inter-edge bandwidth, the topology information of the edge cluster, etc). Together with the inference time, we build a piece of sample data corresponding to this trial. 

%From top to bottom, FlexPie first obtains the model calculation graph through ONNX. Secondly, the calculation graph is optimized by Xenos, the previous work, and the optimized calculation graph will be handed over to the Automated Optimizer (AO) to perform model partitioning and optimization. There are three modules in AO, Dynamic Programming Optimizer (DPO), Feature Expression (FE) and Cost Model (CM). DPO performs a combined optimization based on the model Partition Space and the model Equalizer. During the optimization process, the (Feature Expression) PE of the candidate solution will be generated. Finally, the PE will be sent to the Cost model (CM) for prediction, and it will be continuously iterated to generate the partition plan of the entire model. And finally divide the corresponding model parameters.

\subsection{Data-Driven Cost Estimator (CE)}
\label{sec:gdbt-estimator}




% Specifically, we run different inference models and collect the inference times of these trials. For each trial, we record the shapes of the model (including the sizes of InW, InH, InC, outC, etc) and the testbed settings (including the number of edge nodes, the inter-edge bandwidth, the topology information of the edge cluster, etc). Together with the inference time, we build a piece of sample data corresponding to this trial.


To compare different partition schemes and make the choice between them, we need to decide an indicator to measure the cost of each scheme, so that the cost can serve as the guidance for running the dynamic programming algorithm in DPP. Instead of describing the estimated cost explicitly with some formula, which can be complicated when many variables are involved and fail to generalize, we choose a data-driven method and employ a machine learning model to predict the time cost. We use the GBDT as the cost estimator for \sysname due to its simplicity and interpretability~\cite{chen2016xgboost}. 

The cost estimator takes a group of features as input and outputs an estimated inference time cost to run the distributed inference for a given setting (i.e., running the inference on a specific model layer on a certain testbed). More specifically, we consider three aspects of features (illustrated in Figure~\ref{fig:feature}) to predict the inference time: (1) the shape parameters of the model layer, including InH/OutH, InW/OutW, InC/OutC, K (kernel size), S (stride size), P (padding size),  ConvT (convolution types); (2) the bandwidth between edge devices; (3) the communication architecture adopted by the edge cluster.\footnote{We consider three communication architectures, i.e., ring-based, parameter server (PS)-based, and mesh-based architecture. We have transformed the architecture information into the categorical variable and fed the information to the cost estimator.}

\begin{figure}[t]
    \centering
    \begin{minipage}{0.53\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/feature.pdf}
        \caption{Feature expression}
        \label{fig:feature}
    \end{minipage}\hfill
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/Combination sequence.pdf}
        \caption{Backtracking process in DPP}
        \label{fig:dpp-illustration}
    \end{minipage}
\end{figure}

We have run multiple trials to train the i-Estimator and s-Estimator. Each trace data can be represented as a feature vector of 12 dimensions (Figure~\ref{fig:feature}). While training the i-Estimator, we run the inference computation under different feature settings and use the inference time cost as the label value. While training the s-Estimator, we synchronize the model layer under different feature settings and use the communication time cost as the label value. Each estimator is trained with 330K samples, and the two estimators serve as the oracle to estimate the computation time (inference) and the communication time (synchronization) during the DP process (\S\ref{sec:dpp}).



% The model is trained using runtime measurement data collected during the exploration process and does not require users to input detailed hardware information (footnote: we use the Xenos high-performance library as support for the internal implementation of the operator). We regularly update the model as we explore more configurations during optimization, which also improves accuracy for other related workloads. In this way, the quality of machine learning models can be improved through more experiments. Table 1 summarizes the main differences between automated methods. Machine learning-based cost models balance automatic tuning with predefined cost modeling and benefit from historical performance data for relevant workloads.
% We use the gradient tree boosting model (based on XGBoost) in the machine learning optimizer, which makes predictions based on the features generated from FE; these features can accurately reflect the division mode of the current operator and adjacent operators (it can be inferred that the generated Corresponding communication volume, post-processing volume, etc.), as well as bandwidth and topology that affect communication capabilities. Through AST feature representation, we can quickly complete the prediction of sub-division schemes in the DP process.





% Considering the rich possible combinations of partitioning, our problem is to find the global optimal partitioning scheme for the deep learning model. Here, FlexPie pre-defines operator partition candidate solutions. Compared with random partition schemes, the predefined candidate solutions are more in line with the actual computing logic of the hardware, but the system still needs to perform combination optimization and communication-computation trade-offs between operators. This combination of choices creates a huge search space for each hardware backend. ,To address this challenge, we built an automatic ,optimizer with two main components: a DP optimizer ,that can perform efficient pruning, and a machine learning ,cost model that predicts the performance of a given ,configuration. This section introduces these components and FlexPie's automatic optimization process.
\subsection{Dynamic Partition Planner (DPP)}
\label{sec:dpp}
To find the optimal partitioning scheme, DPP first constructs a search space and then runs a dynamic programming algorithm to search for the optimal partition scheme which can minimize the inference time.

\subsubsection{Search Space}
The left side in Figure~\ref{space} sketches how DPP constructs the under-optimized search space (very large). Given a DNN with $n+1$ layers, denoted as $L_0$-$L_n$, the search starts from $L_n$. For each layer $L_i$, DPP makes the two-step choices, and $L_i$ is tagged with a pair $P_{i}=(p_i, t_i)$, $p_i\in\{\text{InH}, \text{InW}, \text{OutC}, \text{2D-grid}\}$ and $t_i\in \{\text{T}, \text{NT}\}$. DPP aims to find a sequence $S=[P_0, P_1,\cdots,P_n]$.

\Para{Step-1:} DPP needs to choose the dimension to partition $L_i$. We have $k$ different dimensions to consider (e.g. InH-based, InW-based, OutC-based, 2D-grid, etc) and DPP will choose one from them, and then continue to make choices in Step-2.

\begin{figure*}[!t]
	\centering
	\includegraphics[width=1\textwidth]{fig/space.pdf}
	\caption{DPP's search space}
	\label{space}
\end{figure*}

\Para{Step-2:} DPP still needs to decide whether or not the output of $L_i$ needs to be transmitted across edge nodes, in other words, DPP needs to choose between two modes. (1) Transmission (T) mode. The output of $L_i$ is not sufficient to serve as the input of $L_{i+1}$ because the input of $L_{i+1}$ still requires the boundary data (\S\ref{sec:trade-off}). As a result, between the computation of $L_{i}$ and $L_{i+1}$ will be a round of the inter-node communication for nodes to transmit the necessary data between each other. (2) Non-Transmission (NT) mode. The output of $L_i$ is sufficient to serve as the input of $L_{i+1}$ because $L_i$'s inference includes redundant computation. As discussed in \S\ref{sec:trade-off}, we need to consider this trade-off and flexibly decide whether or not to conduct redundant computation for each layer.

Since the typical DNN model can include tens or even hundreds of layers, simply enumerating every combination of partition schemes for each layer can lead to combinatorial explosion(explain in \S\ref{sec-Search Time Comparison}), we resort to the dynamic programming algorithm to search for the optimal partition scheme more efficiently.


\subsubsection{Dynamic Programming Algorithm}
\label{sec:optimal-eval}

Algorithm 1 describes the workflow of the dynamic program process in \sysname. As shown in the right side of Figure~\ref{space}, DPP starts the search from the last layer of the model. In general, DPP is trying to evaluate each optimal substructure during the traversal and finally find out the optimal structure (solution). Specifically, during reverse search (from $L_n$ to $L_0$), every time DPP considers a $P_i$ where $t_i=\text{T}$. At this time, the algorithm will backtrack (from $L_i$ to $L_n$) and generate a combined sequence, which contains multiple subsequences to be evaluated (as shown in Figure~\ref{fig:dpp-illustration}). For each subsequence, the starting point $P_{i}$ and the ending point $P_{j}$ will be T mode (i.e., $t_i=\text{T}$ and $t_j=\text{T}$). DPP will evaluate each subsequence one by one, take the optimal value and save it to the beginning $t_i$.


\Para{Key design-1: Reverse search}. Given the search space, DPP needs to decide the search order. A key point is that the sequence $S$ may contain multiple layers of consecutive NT cases (i.e., using redundant computation to save inter-node communication cost), and the impact of NT cases cascades and will be propagated forward through the layers ($L_i$ to $L_0$).
More specifically, if $P_{i+1}$ has $t_{i+1}=\text{NT}$, the inference of $L_i$ needs to perform redundant computation. Following that, if $P_i$ has $t_i=\text{NT}$, then inference of $L_{i-1}$ also needs to perform redundant computation, and the inference of $L_{i-1}$ needs to perform even more redundant computation.
In such cases, if the dynamic programming algorithm uses searches from $L_0$ to $L_n$, it will lead to a large number of redundant evaluations and cannot establish the optimal substructure. We realize this problem, and decide to adopt the reverse search order to run the dynamic programming algorithm: the search will be performed in order from $L_n$ to $L_0$ (line 19).  DPP first initializes the search space (lines 5-12) and assigns {T, NT} to $t_i$ of each layer $L_i$, $k$ groups per layer (lines 8-9). The last layer is different from previous layers because it must be transmitted after computation (lines 11-12). 

\begin{figure}[t]
	\centering
	\includegraphics[width=1\textwidth]{fig/algo.pdf}
	\label{algo:DPP-algo}
\end{figure}

\Para{Key design-2: Skip all NT states in reverse search}. During reverse search (i.e., $L_{n}\rightarrow L_{i}$), every time DPP only considers a $P_i$ where $t_i=\text{T}$ (line 21) and starts backtracking (line 22). This is because if DPP considers a $P_i$ where $t_i=\text{NT}$, the reverse search becomes a reverse full traversal search. In the process of backtracking, DPP does not evaluate the substructure ended with NT state (as shown in Figure~\ref{fig:dpp-illustration}) and only evaluates the substructure ended with T state (details explained next in Design 3).


\Para{Key design-3: Backtrack and generate combined sequences}. DPP will first consider $P_n$ where $t_n=\text{T}$. Then, DPP extracts its features and sends them to CE for estimation. Finally, DPP can get a total of $k$ costs of $P_n$ where $t_n=\text{T}$ and save to S[$n$][$0\cdots k$] (line 13-17). 

Then, during reverse search (from $L_n$ to $L_{n-1}$), DPP starts backtracking at $P_{n-1}$ (where $t_{n-1}=\text{T}$) and considers fully connecting $P_{n-1}$ (where $t_{n-1}=\text{T}$) and $P_n$ (where $t_n=\text{T}$). Then, DPP generates a combined sequence (a total of $k^2$ subsequences) and sends them to CE for estimation. DPP can obtain $k$ costs of $P_{n-1}$ (where $t_{n-1}=\text{T}$) and add them to the costs in S[$n$][$0\cdots k$] respectively. Based on the comparison between sequences, DPP finally obtains the final $k$ overall costs of $P_{n-1}$ (where $t_{n-1}=\text{T}$) and save to S[$n-1$][$0\cdots k$] (lines 19-36).

Next, DPP performs reverse search again (from $L_{n-1}$ to $L_{n-2}$) and starts backtracking at $P_{n-2}$ (where $t_{n-2}=\text{T}$). We find DPP can estimate the combination of $P_{n-2}$ (where $t_{n-2}=\text{T}$) and $P_{n-1}$ (where $t_{n-1}=\text{T}$), but can not estimate the combination of $P_{n-2}$ (where $t_{n-2}=\text{T}$) and $P_{n-1}$ (where $t_{n-1}=\text{NT}$). This is because we cannot estimate $P_{n-2}$ (where $t_{n-2}=\text{T}$). DPP chooses to save the information in $P_{n-1}$ (where $t_{n-1}=\text{NT}$) until DPP encounters the $P_n$ (where $t_n=\text{T}$). Then, DPP evaluates $P_{n-2}$ (where $t_{n-2}=\text{T}$) and $P_{n-1}$ (where $t_{n-1}=\text{NT}$), and then sums up the cost with $P_n$ (where $t_n=\text{T}$). Based on the comparison between other sequences, DPP finally obtains the final $k$ costs of $P_{n-2}$ (where $t_{n-2}=\text{T}$) and save to S[$n-2$][$0\cdots k$]. 

\Para{Why skip NT states?} In our case, DPP only evaluates the substructure $S_{sub}$ that starts with $P_{i}$ where $t_i=\text{T}$. Otherwise, if $S_{sub}$ that starts with $P_{i}$ where $t_i=\text{NT}$, the time cost of $S_{sub}$ is indeterminate because the computation workload of $L_{i}$ will vary depending on whether the previous layer $L_{i-1}$ chooses the Transmission mode or Non-transmission mode (i.e., whether $t_{i-1}=\text{T}$ or $t_{i-1}=\text{NT}$).\footnote{If $t_{i-1}=\text{NT}$, the inference of $L_{i}$ needs to conduct more computation to construct its input feature map (including the boundary data illustrated in \S\ref{sec:trade-off}). By comparison, if $t_{i-1}=\text{T}$, the inference of $L_{i}$  conducts less computation because the boundary data is obtained from the other edge nodes instead of redundant computation.} Therefore, DPP skips the sub-sequence starting with $P_i$ where $t_i=\text{NT}$. For every $S_{sub}$ that starts with $P_i$ where $t_i=\text{T}$, DPP records the current time cost of $S_{sub}$. The $S_{sub}$ will be used to compare with the other candidate sub-sequences and compose longer sub-sequences. Finally, the optimal structure (solution) will be established with a sequence $S$.

\Para{Piecing together}. In summary, DPP adopts multiple pruning strategies to reduce the search space and further improve the search efficiency. (1) During reverse search, DPP can improve search efficiency by ignoring evaluating $P_i$ where $t_i=\text{NT}$ (line 21). (2) DPP uses S[$j$][$0\cdots k$] to reduce the number of evaluations during the backtracking process (lines 32-34). (3) DPP incrementally generates the combination sequences and avoids much ineffective backtracking in the planning process (lines 22-36, through dynamic thresholds).


% \paragraph{An example of dynamic programming}

% We use an example to help understand the core ideas in Equalizer. The blue line represents the reverse order search, that is, for an n+1 layer model, the search will be performed in the order from $L_n$ to $l_0$. The red line represents the temporary ignoring strategy during the reverse search process, that is, the $NT$ branch is not evaluated during the reverse search process. The black line represents backtracking and generating the combined sequence. That is, if $T$ is encountered during the reverse search process, forward backtracking is performed to find the next $T$. If $NT$ is encountered before $T$ is found in forward backtracking, $NT$ is encountered. No evaluation, and finally a unified evaluation of $T-U-T$. The determination of U is dynamic (not shown in the figure). The green line represents efficient pruning, that is, the $T$ encountered during the forward backtracking process represents the optimal value from the current layer $l_i$ and the current division method $p_x$ to the last layer $l_n$, which can effectively reduce the search time and Assessment time.



% \begin{algorithm}[!]
% 	\caption{Dynamic Programming Algorithm for Model Partition}
% 	\label{algo:DPP-algo}
% 	\KwIn{ 
% 		\\ \texttt{F($l_0:l_n$)} : Native network features\ \ \ 
%             \texttt{Nodes} : The number of devices
%             \\ \texttt{BW} : Inter-node bandwidth\ \ \ \ \ \ \ \ \ 
%             \texttt{Arch} : Communication architecture
% 	}
% 	\KwOut{ \\    \texttt{S} : State sequence
%         \\\textbf{Define:}
%         \\\texttt{P} : Partition scheme \ \ \ \ \ \ 
%         \texttt{CS} : Combined sequence
%         \\\texttt{FE} : Feature expression   \ \ \ 
%         \texttt{F} : Model feature
%         \\\texttt{n} : Number of layers  \ \  \ \ \ \ \
%         \texttt{T} : Transmission mode   \ \
%         \texttt{NT} : Non-Transmission mode
%  }
 
%         \SetKwFunction{FMain}{Function Main}
% 	\SetKwFunction{FInitST}{Function Initialize-Search-Space}
% 	\SetKwFunction{FinitSS}{Function Initialize-State-Sequence}
% 	\SetKwFunction{FRD}{Function DP}
% 	\SetKwProg{Main}{}{}{}
% 	\SetKwProg{Fn}{function}{}{}
	
% 	\Main{\FMain{}}{
%  %        Initialize Search Tree(F($l_0:l_n$), PS($l_0:l_n$)($p_0:p_k$))\\
% 	% Initialize State Sequence(S($l_0:l_n$)($p_0:p_k$), F($l_0:l_n$), Nodes, BW, Arch)\\
% 	% Reverse Dynamic Programming(S($l_0:l_n$)($p_0:p_k$), F($l_0:l_n$), Nodes, BW, Arch)\\
%         \texttt{Initialize-Search-Space\texttt{()}}
        
% 	\texttt{Initialize-State-Sequence\texttt{()}}
 
% 	\texttt{DP\texttt{()}}
% 	} 
% 	\Main{\FInitST{}}
% 	{
%              \For{$i\leftarrow 0$ \KwTo $n$}{
%                 \eIf{\texttt{i != n}}
%                 {
%                     \texttt{P[$i$][$0\cdots k$][$0\cdots 1$] = \{\{T, NT\}, \{T, NT\}, $\cdots$\}}
%                 }{
%                     \texttt{P[$i$][$0\cdots k$][$0\cdots 1$] = \{\{T, T\}, \{T, T\}, $\cdots$\}}
%                 }
%             }    
% 	}
% 	\Main{\FinitSS{}}
% 	{
%             \texttt{FE[$n$][$0\cdots k$] = Extract-Feature(P[$n$][$0\cdots k$])}
            
%             \texttt{S[$n$][$0\cdots k$] = Estimate-Cost(FE[$n$][$0\cdots k$])}
% 	}
%  	\Main{\FRD{}}{

%             \For{$i \leftarrow (n-1)$ \KwTo $0$ \label{line:traverse-start}} 
%             {
%                 \texttt{Traverse P[$i$][$0\cdots k$][$0\cdots 1$]}
                
%     		\If{\texttt{P[$i$][$0\cdots k$][$0\cdots 1$] == T}}
%     		{        
%                     \For{$j\leftarrow (i+1)$ \KwTo $n$}
%                     {
%                          \texttt{Traverse P[$j$][$0\cdots k$][$0\cdots 1$]}
                         
%                         \If{\texttt{P[$j$][$0\cdots k$][$0\cdots 1$] == T}}
%                         {
%                             \texttt{CS[$i\cdots j$][$0\cdots k$][$0\cdots 1$] = Generate-Combined-Sequence()}
         
%                             \texttt{FE[$i\cdots j-1$][$0\cdots k$][$0\cdots 1$] = Extract-Feature(CS[$i\cdots j-1$][$0\cdots k$][$0\cdots 1$])}
        
%                             \eIf{\texttt{(Estimate-Cost(FE[$i\cdots j-1$][$0\cdots k$][$0\cdots 1$]) + S[$j$][$0\cdots k$]) $<$ S[$i$][$0\cdots k$] }}
%                             {
%                                \texttt{ S[$i$][$0\cdots k$] $=$ Estimate-Cost(FE[$i\cdots j-1$][$0\cdots k$][$0\cdots 1$]) + S[$j$][$0\cdots k$]  }
%                             }{
%                                 \textbf{break};
%                             }
%                         }
%                     }
%     		} \label{line:traverse-end}
%             }
% 	} 
% % \vspace{-0.5cm}
% \end{algorithm}





% \begin{algorithm}[!]
% 	\caption{Dynamic Programming Algorithm for Model Partition}
% 	\label{algo:DPP-algo}
% 	\KwIn{ 
% 		\\ \texttt{F($l_0:l_n$)} : Native network features\ \ \ 
%             \\\texttt{Nodes} : The number of devices
%             \\ \texttt{BW} : Inter-node bandwidth\ \ \ \ \ \ \ \ \ 
%             \\\texttt{Arch} : Communication architecture
% 	}
% 	\KwOut{ \\    \texttt{S} : State sequence
%         \\\textbf{Define:}
%         \\\texttt{P} : Partition scheme \ \ \ \ \ \ 
%         \\\texttt{CS} : Combined sequence
%         \\\texttt{FE} : Feature expression   \ \ \ 
%         \\\texttt{F} : Model feature
%         \\\texttt{n} : Number of layers  \ \  \ \ \ \ \
%         \\\texttt{T} : Transmission mode   \ \
%         \\\texttt{NT} : Non-Transmission mode
%  }
 
%         \SetKwFunction{FMain}{Function Main}
% 	\SetKwFunction{FInitST}{Function Initialize-Search-Space}
% 	\SetKwFunction{FinitSS}{Function Initialize-State-Sequence}
% 	\SetKwFunction{FRD}{Function DP}
% 	\SetKwProg{Main}{}{}{}
% 	\SetKwProg{Fn}{function}{}{}
	
% 	\Main{\FMain{}}{
%  %        Initialize Search Tree(F($l_0:l_n$), PS($l_0:l_n$)($p_0:p_k$))\\
% 	% Initialize State Sequence(S($l_0:l_n$)($p_0:p_k$), F($l_0:l_n$), Nodes, BW, Arch)\\
% 	% Reverse Dynamic Programming(S($l_0:l_n$)($p_0:p_k$), F($l_0:l_n$), Nodes, BW, Arch)\\
%         \texttt{Initialize-Search-Space\texttt{()}}
        
% 	\texttt{Initialize-State-Sequence\texttt{()}}
 
% 	\texttt{DP\texttt{()}}
% 	} 
% 	\Main{\FInitST{}}
% 	{
%              \For{$i\leftarrow 0$ \KwTo $n$}{
%                 \eIf{\texttt{i != n}}
%                 {
%                     \texttt{P[$i$][$0\cdots k$][$0\cdots 1$] = \\\{\{T, NT\}, \{T, NT\}, $\cdots$\}}
%                 }{
%                     \texttt{P[$i$][$0\cdots k$][$0\cdots 1$] = \\\{\{T, T\}, \{T, T\}, $\cdots$\}}
%                 }
%             }    
% 	}
% 	\Main{\FinitSS{}}
% 	{
%             \texttt{FE[$n$][$0\cdots k$] = \\Extract-Feature(P[$n$][$0\cdots k$])}
            
%             \texttt{S[$n$][$0\cdots k$] = \\Estimate-Cost(FE[$n$][$0\cdots k$])}
% 	}
%  	\Main{\FRD{}}{

%             \For{$i \leftarrow (n-1)$ \KwTo $0$ \label{line:traverse-start}} 
%             {
%                 \texttt{Traverse P[$i$][$0\cdots k$][$0\cdots 1$]}
                
%     		\If{\texttt{P[$i$][$0\cdots k$][$0\cdots 1$] == T}}
%     		{        
%                     \For{$j\leftarrow (i+1)$ \KwTo $n$}
%                     {
%                          \texttt{Traverse P[$j$][$0\cdots k$][$0\cdots 1$]}
                         
%                         \If{\texttt{P[$j$][$0\cdots k$][$0\cdots 1$] == T}}
%                         {
%                             \texttt{CS[$i\cdots j$][$0\cdots k$][$0\cdots 1$] = Generate-Combined-Sequence()}
         
%                             \texttt{FE[$i\cdots j-1$][$0\cdots k$][$0\cdots 1$] =\\ Extract-Feature(\\CS[$i\cdots j-1$][$0\cdots k$][$0\cdots 1$])}
        
%                             \eIf{\texttt{(Estimate-Cost(\\FE[$i\cdots j-1$][$0\cdots k$][$0\cdots 1$])\\ + S[$j$][$0\cdots k$]) $<$ S[$i$][$0\cdots k$] }}
%                             {
%                                \texttt{ S[$i$][$0\cdots k$] $=$ Estimate-Cost(\\FE[$i\cdots j-1$][$0\cdots k$][$0\cdots 1$])\\ + S[$j$][$0\cdots k$]  }
%                             }{
%                                 \textbf{break};
%                             }
%                         }
%                     }
%     		} \label{line:traverse-end}
%             }
% 	} 
% % \vspace{-0.5cm}
% \end{algorithm}

Based on our algorithm design, we have the following theorem. The proof is included in our technical report~\cite{flexpie-tech}.
\begin{theorem}[Optimality]
    Assuming Cost Estimator always reports the proper time cost for any given partition scheme, then DPP can output the optimal partition scheme for a given DNN model that yields the lowest time cost.
\end{theorem}

% \begin{proof}
% Let $P_{i,\cdots,j}$ denote a partition scheme for layers from $L_i$ to $L_j$. Let \textsc{estimate}($P_{i,\cdots,j}$) denote the time cost of the partition scheme reported by the estimator. The estimator only estimates $P_{i,\cdots,j}$ when $P_i.t_i=\text{T}$ and $P_j.t_j=\text{T}$, i.e., NT mode is skipped by setting such cases an $\inf$ cost.  Let $Cost(i, j)$ denote the optimal (lowest) time cost of $P_{i,\cdots,j}$. Then, based on our traversal algorithm (lines 19-36 in Algorithm 1), $Cost(i, j)$ satisfies the following recurrence.
% \begin{equation*}
%   Cost(i, j)=
%     \begin{cases}
%       \textsc{estimate}(P_{n,\cdots,n}) & \text{if } i=n \text{ and } j=n \\
%       \min\limits_{i\leq k<j}\{ \textsc{estimate}(P_{i,\cdots,k})+\textsc{estimate}(P_{k+1,\cdots,j}) \} & \text{otherwise}\\
%     \end{cases}       
% \end{equation*}
% DPP evaluates the recurrence of $Cost(i,j)$ in a bottom-up way. Note that since we evaluate $Cost(i, j)$ as the sequence length $j-i+1$ increases from 1 to $n$, all values for sub-problems referenced by the recurrence for $Cost(i, j)$ will have already been computed. At the end, DPP returns the optimal (lowest) value for $Cost(0, n)$, as well as its corresponding partition scheme, which in turn is the optimal partition scheme that yields the lowest time cost. 
% \end{proof}