\section{Experimental Evaluation}
\label{sec-exp}
% \subsection{Experiment Setting}
\Para{Testbeds:} Our testbed includes 4 TMS320C6678\cite{tms} devices connected through SRIO. We have evaluated under different communication topologies, including Ring-based, PS-based and Mesh-based topologies.\footnote{Our evaluation shows similar performance results for Ring-based and Mesh-based topologies. Due to space limits, we omit the results of Mesh-based topology.} In addition, we run tests with various settings of inter-device bandwidth: 5Gbps, 1Gbps and 500Mbps.

\Para{Benchmarks:} 
We use four typical models as benchmarks to evaluate \sysname, namely MobileNet~\cite{howard2017mobilenets}, ResNet18~\cite{he2016deep}, ResNet101~\cite{he2016deep} and Bert~\cite{jawahar2019does}.

\Para{Baselines:} We compare \sysname with five baselines. Specifically, we compare \sysname with 3 fixed partition schemes, i.e., Xenos\cite{zhang2023xenos}(One-dim (OutC)), MoDNN\cite{mao2017modnn} and DeepSlicing\cite{zhang2021deepslicing}(One-dim (InH/InW)), DeepThings\cite{zhao2018deepthings}(2D-grid). Besides, we also consider two other baselines used by recent works. We refer to DINA~\cite{mohammed2020distributed} and PartialDI~\cite{dey2019embedded} and implement the layerwise partition, which uses different partition schemes for different model layers. We refer to AOFL~\cite{zhou2019adaptive} and EdgeCI~\cite{chen2024edgeci} and incorporate data fusion into the fixed model partition scheme. 




\Para{Metrics:} We study the inference time cost and the DPP search time cost in our evaluation. We run each inference workload and each generated optimal sequence 1000 times and reported the average. Besides, in order to make an overall comparison between \sysname and the baselines across all the test cases, we define a summative metric called \emph{performance score}, to quantify the performance of each solution. \emph{performance score} is defined as follows. For a given model benchmark and a given testbed setting, we run both \sysname and the four baselines and obtain their inference time cost. Among the five time costs, denoted as $t_1-t_5$, we choose the smallest one $\min\{t_1,t_2,t_3,t_4,t_5\}$ as the reference, and the \emph{performance score} of each solution is computed as 
$Score_{i}=\frac{{\min\{t_1,t_2,t_3,t_4,t_5\}}}{t_i}$.

The value of each score will be a value between 0 and 1.0. If a partition strategy incurs a larger inference time, then its score will be smaller. The best partition strategy that yields the smallest inference time will be scored 1.0. 

% JSA
\iffalse 
\subsection{Profiling of \sysname Components}
\label{sec-Search Time Comparison}

% \begin{figure}[t]
% 	\centering
% 	\includegraphics[width=1\textwidth]{fig/table.pdf}
% 	\label{twotable}
% \end{figure}
\Para{Search efficiency of DPP}.
\begin{table}[t]
\renewcommand\arraystretch{1}
	\centering
	\caption{Search time comparison}
	\label{tab:search-cmp}
\footnotesize
	\begin{tabular}{ccccc}
		\hline
		Network&Layers & Traversal  &  DP-fixed (8) &DP-Dynamic \\
		\hline
            MobieNet & 28   & \textgreater 24 hours & 36.9 minutes& 18.5 seconds   \\
            ResNet18 &  18  & \textgreater 24 hours &  23.4 minutes& 11.6 seconds   \\
            ResNet101&  101 & \textgreater 24 hours &  2.6 hours& 76.3 seconds   \\
            Bert     &  12  &   2.8 seconds         &  0.5 seconds& 0.01 seconds \\
            \hline
	\end{tabular} 
        % \vspace{-0.5cm}
\end{table}
\sysname's DP algorithm can automatically search the optimal division sequence in a large search space, and ensure the efficiency of the search in the process. To verify this, we conduct a comparative experiment. Table~\ref{tab:search-cmp} summarizes the performance comparison between DP algorithm and other alternative search algorithms. One baseline uses full traversal, leading to a very high time complexity. For a model with $n$ layers, if each layer has $p$ partition schemes and there is transmission (T) or non-transmission (NT) between layers, the number of traversal searches must be at least $p^n*2^{n-1}$ times, making the search time ($>$24 hours) become unaffordable for most models with deep networks. For another baseline (DP-fixed), we adopt the same search plan as \sysname, but use a fixed depth ($\alpha=8$) for backtracking, leading to a total number of $p^2*(n-1)+(n-1)*p^\alpha$ times of search. Therefore, DP-fixed also suffers from long runtime due to the costly backtracking process. By comparison, \sysname's algorithm (DP-dynamic) adaptively adjusts $\alpha$ during the runtime, i.e., the backtracking can be early terminated if DP-dynamic realizes the further backtracking leads to higher time cost. Such a pruning approach effectively narrows the search space and in turn reduces the search time.





\Para{Prediction accuracy of CE}
\label{subsec-Prediction Accuracy of CE}
\begin{table}[t]
\renewcommand\arraystretch{1}
	\centering
	\caption{The relative error rate}
	\label{tab:prediction accuracy}
\footnotesize
	\begin{tabular}{ccccc}
		\hline
		Configuration&MobieNet & ResNet18  &  ResNet101&Bert \\
		\hline
            3-node-5G   &   0.37\% & 0.27\% &   0.54\% &  0.27\%  \\
            3-node-1G   &   0.35\% & 0.29\% &   0.57\% &  0.21\%  \\
            3-node-500M &   0.33\% & 0.28\% &   0.53\% &  0.23\%  \\
            4-node-5G   &   0.25\% & 0.23\% &   0.40\% &  0.20\%  \\
            4-node-1G   &   0.20\% & 0.21\% &   0.41\% &  0.20\%  \\
            4-node-500M &   0.26\% & 0.23\% &   0.41\% &  0.24\%  \\
            \hline
	\end{tabular} 
\end{table}

Table~\ref{tab:prediction accuracy} shows the relative errors between estimated execution and actual execution time of our cost estimator. All the relative errors fall within 0.5\%, indicating that CE can reliably predict a model's execution time towards a testing trial expressed as the feature vector (\S\ref{sec:gdbt-estimator}).

\fi

\subsection{Comparison on 4-node Testbed}
\label{4node}





We compare the inference time cost of different partition schemes on the 4-node testbed. Figure ~\ref{Ring-4-node} shows that, the 2D-grid partition performs best among the three baselines (OutC, InH/InW and 2D-grid) without layer-wise and fused-layer strategy. The OutC-based partition introduced non-trivial communication overheads. As illustrated in Figure~\ref{fig:model-partition}(c), with OutC-based partitioning, each node has to fetch the input feature maps from all the other three nodes to suffice its inference computation, so the performance is the lowest. The performance of one-dim.(InH/InW) is between 2D-grid partition and OutC-based partition. However, not all layers perform optimally with 2D-grid partition (as shown in Figure~\ref{fig:micro-bench}), because 2D-grid partitions will cause varying degrees of imbalanced calculation. For example, in MobileNet, when the input feature map is 14x14x512 or 7x7x512, the 2D-grid has unbalanced calculations because neither the InH dimension (14) nor the InW dimension (14) is a multiple of the number of nodes (4). The OutC-based partitioning may not be optimal for overall model partitioning, but it can achieve balanced computation (i.e., dividing the 512 channels evenly among the 4 nodes).



Compared with the One-dim and 2D-grid partition schemes, the layerwise partition introduces more flexibility and adopts different partition schemes for each layer. As a result, the layerwise partition yields better performance than the fixed One-dim and 2D-grid baselines. Different from layerwise partition, the layer fusion optimization accelerates the inference from the other direction. By fusing some model layers, the inference workflow can avoid redundant communication costs and in turn gain speedup. However, the previous works (e.g., DINA, EdgeCI) fail to incorporate both optimizations due to the explosive search space. Therefore, simply adopting layerwise optimization or fusion optimization cannot yield the best performance. By contrast \sysname can efficiently find the proper partitioning scheme through its dynamic optimization strategies (\S\ref{sec:dpp}). Therefore, \sysname distinctly reduces the inference time for a wide range of models under different typologies. 

\Para{Limitation:} \sysname provides little speedup when conducting inference with Bert. With a deeper dive, we realize that Bert model uses much matrix multiplication, but does not involve significant convolution computation as the other three benchmarks. Therefore, Bert already enjoys much easy parallelism in its nature. Neither layerwise flexibility nor inter-layer fusion brings distinct acceleration, leading to very close performance among different partition schemes.



\begin{figure*}[t]
	\centering
	\subfigure[MobileNet, ResNet18 on Ring and PS]{
		\label{MobileNet-ResNet18-4} 
		\includegraphics[width=0.99\textwidth]{fig/4node-Mobile and Res18.pdf}
	}
	\subfigure[ResNet101, Bert on Ring and PS]{
    	\label{ResNet101-Bert-4} 
    	\includegraphics[width=0.99\textwidth]{fig/4node-Res101 and Bert.pdf}
	}
	\caption{Comparison on 4-node testbed}
	\label{Ring-4-node} 
\end{figure*}


\begin{figure*}[t]
	\centering
 	\subfigure[Score on 4node]{
		\label{Score-4} 
		\includegraphics[width=0.35\textwidth]{./fig/Score-4node.pdf}
	}
  	\subfigure[Score on 3node]{
		\label{Score-3} 
		\includegraphics[width=0.35\textwidth]{./fig/Score-3node.pdf}
	}
        \caption{Performance score comparison}
	\label{Score-4 and Score-4} 
\end{figure*}




\Para{Performance score:} \sysname achieves the highest performance score among all 5 solutions, and it outperforms the baselines with a speedup of 1.10-2.21$\times$. 


\subsection{Comparison on 3-node Testbed}
\label{3node}


We then change the testbed setting and only use 3 nodes to run the inference tasks. In the 3-node test, the partition strategies including One-dim (InH/InW), One-dim (OutC) and 2D-grid still lead to imbalanced computation. But different from 4-node test,  the 2D-grid partition strategy becomes the worst case when running on the 3-node testbed. This is because, one node needs to undertake much more (i.e., twice as much) computation workload as the other two nodes. 


Based on the comparative evaluation in both 3-node testbed and 4-node testbed, we also demonstrate that, there is no ``one-size-fits-all'' fixed partition scheme to provide general performance improvement. More specifically, One-dim (OutC) and 2D-grid partition may yield the best performance under different bandwidths or different architectures. Since such statistic partition schemes can only reach their ``sweet spots'' under specific settings, they hardly provide general performance benefits to distributed inference under varying testbed configurations. By contrast, DPP enables \sysname to adapt to different testbeds and automatically search for the appropriate partition strategies to yield high performance, making \sysname more desirable for practical deployment. 

\begin{figure*}[!t]
	\centering
	\subfigure[MobileNet, ResNet18 on Ring and PS]{
		\label{MobileNet-Ring-3} 
		\includegraphics[width=0.99\textwidth]{fig/3node-Mobile and Res18.pdf}
	}
	\subfigure[ResNet101, Bert on Ring and PS]{
    	\label{ResNet18-Ring-3} 
    	\includegraphics[width=0.99\textwidth]{fig/3node-Res101 and Bert.pdf}
	}
	\caption{Comparison on 3-node testbed}
	\label{Ring-3-node} 
\end{figure*}


\Para{Performance score:} \sysname achieves the highest performance score among all 5 solutions, and it outperforms the baselines with a speedup of 1.08-2.39$\times$. 

% \subsection{Other Findings}


% \Para{Flexible partition outweighs fusion optimization in communication-efficient scenarios.}
% Figure~\ref{Ring-4-node} and Figure~\ref{Ring-3-node} also measure the inference time under different bandwidths (5Gbps, 1Gbps, 500Mbps) and different communication topologies (Ring, PS). With a deeper dive into the dynamic planning process of \sysname, we find that \sysname makes different partitioning choices under different configurations. When the bandwidth is high or the Ring architecture is adopted, the communication (parameter synchronization) cost occupies a smaller portion in the overall inference time. In such cases, the flexible layer-wise partition strategy in \sysname brings more acceleration benefits than the other strategies. This is because the flexible partition can produce better load balance in calculations in such cases compared with fixed partition. By contrast, the layer fusion optimization shows less effect on the overall acceleration when the inference workflow is not the bottleneck by communication.

% \Para{Layer fusion optimization is more effective when communication cost dominates the overall inference time cost.}
% After we set a lower bandwidth limit (such as 500Mbps), compared with 5Gbps bandwidth scenarios, One-dim (OutC) partitioning is more disadvantageous than the hybrid partitioning scheme provided by \sysname. Similar communication bottlenecks can also occur when we choose PS architecture instead of Ring architecture. In such cases, the effectiveness of layer fusion shines up because it allows each node to perform more redundant computation to reduce communication between nodes, which can bring more acceleration to the inference workflow.




