\section{Related Work}
\label{sec-related}
\Para{Distributed DNN Inference} Distributed DNN inference can be categorized into \emph{cloud-to-edge devices}~\cite{song2018situ,jeong2018ionn}, \emph{edge server-to-edge device}~\cite{mohammed2020distributed,shan2020collaborative}, \emph{cloud-to-edge server-to-edge device}~\cite{ren2021fine,xue2021eosdnn,lin2019distributed,dey2019offloaded} and \emph{edge device-to-edge device}~\cite{zhao2018deepthings,zhang2021deepslicing,mao2017modnn,zhou2019adaptive}. Among them, \emph{edge device-to-edge device} deploys the DNN on the local terminal device and performs DNN inference entirely in a local collaborative manner. This paradigm focuses on inference latency and energy consumption, but it can be applied to high mobility scenarios or some remote and harsh environments. In this paper, we focus on how to obtain lower inference latency through flexible combinatorial optimization in device-device collaborative inference scenarios.

\Para{Model Partitioning in Distributed Inference}
MoDNN~\cite{mao2017modnn} uses the one-dim to minimize the non-parallel data transmission time, but ignores the computational imbalance problem caused by the one-dim scheme. 
Deepslicing~\cite{zhang2021deepslicing} achieves a trade-off between computation and synchronization through the Proportional Synchronization Scheduler. But it also adopts one-dim scheme, which constrains its optimization opportunities. 
DeepThings\cite{zhao2018deepthings} uses a 2D-grid to perform model partitioning and implements a distributed work-stealing approach to enable dynamic workload distribution and balancing during inference runtime. However, it still suffers from the imbalanced distribution of computation tasks. In addition, many model partition strategies have been developed and applied in distributed training (e.g.,~\cite{vldb14-flexps,icde20-fela,elasticpipe}), and it will be an interesting future direction to study these partition strategies in the scenario of model inference. 

% \Para{Other Frameworks with Different Optimization Goals}
% CoEdge~\cite{zeng2020coedge} aims to minimize the energy consumption during edge-based inference. It tries to utilizes the available computation and communication resources at the edge and dynamically partitions the DNN inference workload according to the edge devicesâ€™ computing power and network conditions. 
% \cite{hadidi2018distributed} proposes a framework to save both energy consumption and memory cost. It leverages the computing power of multiple low-power robots to achieve efficient, dynamic, and real-time recognition. 
% \cite{goel2022efficient} designs a novel method that creates a parallel inference pipeline for computer vision problems that rely on hierarchical DNNs. This approach balances the load between cooperating devices and reduces communication costs to achieve higher throughput of video processing and minimize the inference latency.