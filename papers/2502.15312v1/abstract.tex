\begin{abstract}

%The rapid development of deep learning leads to the emergence of many novel IoT (Internet of Thing) applications. These applications usually deploy the pre-trained DNN (Deep Neural Network) models to multiple (usually $4\sim6$) edge devices to jointly execute the inference tasks. However, the conventional deployment tends to yield undesirable inference time. We have studied the problem and identified that the reason is mainly due to the model partition strategy adopted by the inference engine. Prior inference frameworks either adopt a fixed model partition scheme (e.g., One-dim partition or 2D-grid partition), or integrate limited optimization (e.g., layerwise optimization or simple layer fusion), which fails to achieve the optimal performance towards the different model layers or testbed settings. 


The rapid advancement of deep learning has catalyzed the development of novel IoT applications, which often deploy pre-trained deep neural network (DNN) models across multiple edge devices (typically 4$\sim$6) for collaborative inference. However, conventional deployment methods frequently result in suboptimal inference times. Our study identifies that the inefficiency primarily stems from the model partitioning strategy employed by inference engines. Previous frameworks often rely on fixed partitioning schemes (e.g., one-dimensional or 2D-grid partitions) or limited optimizations (e.g., layer-wise adjustments or simple layer fusion), which fail to deliver optimal performance across varying model layers and testbed configurations. In this paper, we propose \sysname, a solution to accelerate distributed inference on edge devices through \emph{flexible combinatorial optimization}. \sysname integrates an automated optimization procedure based on a data-driven cost model and dynamic programming, which efficiently finds an optimized model partition scheme in huge combinatorial spaces. Our evaluation on four commonly used DNN benchmarks demonstrates that \sysname reduces inference time, achieving up to a 2.39× speedup over state-of-the-art methods.

% In this paper, we propose \sysname to accelerate the distributed inference on edge devices with \emph{flexible and automatic model partition}. Our key idea is to allow the inference engine to choose a flexible partition scheme for the model so as to maximize the utilization of computation power and bandwidth resource. We realize that manually tuning the partition scheme is impractical as it requires much human effort and expertise. Instead, we base on dynamic programming strategy and design an automatic optimization algorithm in \sysname, which takes the model features as input and outputs the efficient model partition scheme for each model layer. We adopt 4 commonly-used DNN models as benchmarks and conduct evaluation on multiple edge devices. Our evaluation shows \sysname can effectively reduce the inference time and outperforms state-of-art solutions with a speedup up to 2.12$\times$.


% 深度学习技术在智能驾驶、智慧城市等新应用场景中得到了快速发展，但其部署需要消耗大量资源。在资源受限的智能物联网（IoT）设备上单独执行推理任务通常难以满足严格的服务延迟要求。推理任务通常被卸载到边缘服务器或云端，但这可能会导致性能不稳定和隐私泄露。为了解决上述挑战，本文旨在设计一个低延迟边缘分布式推理框架\sysname。通过研究现有边缘分布式推理框架，我们发现它们的部署往往会产生不理想的推理时间。其中主要原因是采用了Layer-wise和Fused-layer分离的优化方法。

% 在本文中，我们提出 \sysname 来加速边缘设备上的分布式推理，其具有 \emph{灵活的组合优化}。我们的核心思想是将Layer-wise和Fused-layer进行组合，从而最大限度地利用计算能力和带宽资源。我们意识在组合优化造成的搜索空间中手动获得最优部署方案是不切实际的，因为它需要大量的人力和专业知识。相反，我们基于动态规划策略，在 \sysname 中设计了一种自动寻优算法，可以为每个模型层输出有效的模型分区方案。我们采用 4 个常用的 DNN 模型作为基准，并在多个边缘设备上进行评估。我们的评估表明 \sysname 可以有效地减少推理时间，并且比最先进的解决方案性能更好，加速比高达 2.21$\times$。


% Deep learning has been rapidly developed in application scenarios such as autonomous driving and smart cities, but its deployment consumes a lot of resources. Resource-constrained smart Internet of Things (IoT) devices is often difficult to meet strict service latency requirements. Inference tasks are usually offloaded to the cloud, but this may lead to unstable performance and privacy leakage. To address the above challenges, distributed inference using multiple edge devices becomes a way to efficiently process real-time tasks.  However, existing frameworks tend to yield undesirable inference time. We think the main reason is that they adopt the Layer-wise or Fused-layer separation optimization methods, lacking the necessary combination optimization.

% In this paper, we propose \sysname to accelerate distributed inference on edge devices with \emph{flexible combinatorial optimization}. Our key idea is to combine layer-wise and fused-layer to maximize the use of computing power and bandwidth resources. In real-life deployment, it is impractical to manually try all model partition schemes and decide the optimal solution, which requires a lot of manpower and expertise to search in a very large design space. Instead, we design an automatic optimization algorithm in \sysname based on a dynamic programming strategy to output an effective model partition scheme for every model. We conduct comprehensive evaluation with 4 commonly used DNN models as benchmarks and compare the performance of \sysname with multiple baselines. Our evaluation shows that \sysname can effectively reduce the inference time and outperform the state-of-the-art solutions with a speedup of up to 2.39$\times$.
\end{abstract}

\keywords{Edge Device, Distributed Inference, Model Partition, Combinatorial Optimization, Data-Driven, Dynamic Programming} 