
\section {Background and Motivation}
\label{sec-back}
\subsection{Distributed Inference and Model Partition}

\begin{figure}[!t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/model partition.pdf}
        \caption{Example of parallelizing depthwise separable convolution}
        \label{fig:model-partition}
    \end{minipage}\hfill    
    \begin{minipage}{0.47\textwidth}
        \centering
        \includegraphics[width=\linewidth]{fig/micro-bench.pdf}
        \caption{Micro-bench test}
        \label{fig:micro-bench}
    \end{minipage}
\end{figure}



% \begin{figure}[!t]
%     \centering
%     \begin{minipage}{0.45\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/micro-bench.pdf}
%         \caption{Micro-bench test}
%         \label{fig:micro-bench}
%     \end{minipage}\hfill
%     \begin{minipage}{0.55\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{fig/FCO.pdf}
%         \caption{Illustration of \emph{combinatorial flexible partition} scheme}
%         \label{fig:FCO}
%     \end{minipage}\hfill
%     % \vspace{-0.5cm}
% \end{figure}



%Distributed DNN inference becomes increasingly necessary in edge-based applications because the inference model is growing in size but the applications have stringent real-time latency requirement. By employing multiple edge devices to jointly execute the inference task, the completion time is expected to be reduced to achieve the desirable application responsiveness. 

% While conducting distributed inference, a straightforward question that arises is to decide the model partition scheme across multiple edge devices. 

Figure~\ref{fig:model-partition} illustrates the commonly used partition schemes in practice. Typically in a DNN model, there are two main categories of partition schemes, namely One-dim partition (Figure~\ref{fig:model-partition}(a)-\ref{fig:model-partition}(c)) and 2D-grid partition(Figure~\ref{fig:model-partition}(d)). In One-dim partition, there are three commonly-used sub-categories, i.e., InH-based partition, InW-based partition and OutC-based partition, which indicate that the partition is conducted according to the height of the input feature map (InH), the width of the input feature map (InW), and the channel of the output feature map (OutC). As for the 2D-grid partition, it partitions the model on both the InH dimension and the InW dimension for the sake of load balance. Notably, the InC-based partition is not typically used to partition the model, because it introduces costly gather operations and leads to undesirable performance. 


\subsection{Partition Scheme Affects Performance}

\label{sec:micro}
While One-dim and 2D-grid partition schemes are both generally applicable to inference models, we find that they lead to distinctly different completion times when we conduct the inference with different layers under different testbed settings. Figure~\ref{fig:micro-bench} demonstrates this using a series of micro-bench tests. We conduct inference on MobileNet with different partition schemes and measure the completion inference time on different layers. In the first 3 groups of tests, i.e., 4-Node-L2, 4-Node-L5, 4-Node-L13, we run the inference with 4 edge devices, connected with SRIO at a bandwidth of 5Gb/s. The three groups of tests conduct inference with different layers, i.e., the 2nd layer (L2), the 5th layer (L5) and the 13th layer (L13). Obviously, different layers yield their optimal time with different partition schemes. L2 and L5 prefer the InH/InW-based partition and the 2D-grid partition, whereas L13 prefer the OutC-based partition. Next, we change the testbed settings from using 4 nodes to using 3 nodes, then we get the other three groups of tests, i.e., 3-Node-L2, 3-Node-L5 and 3-Node-L13. Comparing with the first 3 groups we can see that, even towards the same layer, the optimal partition schemes vary under different testbed settings. For example, L5 can yield an optimal inference time with 2D-grid partition in the 4-node setting, but the 2D-grid partition gives the worst inference time for L5 under the 3-node setting. 

From the micro-bench tests, we realize that there are no \emph{one-size-fits-all} partition schemes. Even in the same model, the different layers may prefer different partition schemes. Besides, the optimality of the partition schemes can also vary across different testbed settings. Therefore, we are motivated to design flexible partition schemes for different model layers under various testbed settings.  


% \begin{figure}[!t]
% 	\centering
% 	\includegraphics[width=0.49\textwidth]{fig/cloud-edge-device.pdf}
% 	\caption{cloud-edge-device.}
% 	\label{cloud-edge-device}
% 	% \vspace{-0.5cm}
% \end{figure}

% Distributed DNN inference can make full use of the comprehensive computing power of multiple nodes. According to the offloading method of tasks and the geographical location of the device (such as terminal, edge and cloud), distributed DNN inference can be divided into Cloud-device collaborative inference, Edge-device collaborative inference, Cloud-edge-device collaborative inference and Device-device collaborative inference. Among them, Device-device collaborative inference deploys the DNN model on the local terminal device and performs DNN inference entirely in a local collaborative manner. This means that data will be processed close to the source to obtain decision-making results, without the involvement of edge servers and cloud servers. This paradigm focuses on inference latency and energy consumption, but it can be applied to high mobility scenarios or some remote and harsh environments. In this article, we focus on how to obtain lower model inference latency through flexible model partitioning in device-device collaborative inference scenarios.
% \subsection{Model partitioning}


%For a DNN model, there are usually two types of division methods, One-dim (InH, InW and OutC) and 2D-grid (InH and InW mixed). InC division will bring a large number of gather operations, so usually the performance very bad. In the process of distributed reasoning, a natural question is how to split the model, that is, evenly divide the computing tasks into different computing units. Due to the different structures of neural networks, a single model partitioning method will cause additional hidden overhead (such as unbalanced calculations, redundant synchronization overhead, etc.), thus lengthening the inference delay. Figure~\ref{MP} shows three ways of model partitioning. Among them, One-dim and 2D-grid division will cause data dependence on adjacent layers or even deeper layers, and the relationship between calculation and transmission between layers needs to be further considered.


% \begin{figure}[!t]
% 	\centering
% 	\includegraphics[width=0.5\textwidth]{fig/model partition.pdf}
% 	\caption{model partition}
% 	\label{MP}
% 	% \vspace{-0.5cm}
% \end{figure}





\subsection{Trade-off between Computation and Communication }
\label{sec:trade-off}

% \begin{figure}[t]
% 	\centering
% 	\subfigure[One-dim.(InH)]{
% 		\label{fig:one-dim-trade-off} 
% 	\includegraphics[width=0.2\textwidth]{./fig/one dim_trade-off.pdf}
% 	}
%     \subfigure[2D-grid]{
% 	\label{fig:2d-grid-trade-off} 
%         \includegraphics[width=0.2\textwidth]{./fig/2d-grid_trade-off.pdf}
% 	}
% 	\caption{Trade-off between (redundant) computation and communication}
% 	\label{fig:trade-off} 
%         % \vspace{-0.5cm}
% \end{figure}

One-dim and 2D-grid partitions lead to different overlaps between computation (on each edge device) and communication (across multiple edge devices). Figure~\ref{fig:model-partition}(a) illustrates the One-dim partition scheme (InH-based) when we conduct distributed inference with 4 nodes (Node0-Node3). The inference engine first uses the kernel and input feature maps (In\_Fms) to inference, computing output feature maps (Out\_Fms). Then the Out\_Fms become the In\_Fms for the next layer. From Figure~\ref{fig:model-partition}(a) we can see that Node2 still requires some boundary data to be transferred from other nodes (e.g. Node1 and Node3) to perform a new layer inference. 

% JSA
\iffalse 
\begin{table*}[!t]
\renewcommand\arraystretch{1}
	\centering
	\caption{Comparison with prior frameworks}
	\label{frameworks}
\footnotesize
	\begin{tabular}{ccccc}
		\hline
		&Layer-Wise &  \makecell{Fused-Layer}  & Combinatorial Optimization & Data-Driven \\
		\hline
            MoDNN~\cite{mao2017modnn}                 &  $\times$  (One-dim.) & $\times$    &  $\times$   &  $\times$       \\
            DeepThings~\cite{zhao2018deepthings}      &  $\times$ (2D-grid)   & $\times$    &  $\times$   &  $\times$       \\
            DeepSlicing~\cite{zhang2021deepslicing}   & $\times$ (One-dim.)   & $\times$    &  $\times$   &  $\times$       \\
            AOFL~\cite{zhou2019adaptive}              & $\times$ (2D-grid)    & \checkmark  &  $\times$   &  \checkmark     \\
            EdgeCI~\cite{chen2024edgeci}              & $\times$  (One-dim.)  & \checkmark  &  $\times$   &  $\times$       \\
            DINA~\cite{mohammed2020distributed}       & \checkmark            & $\times$    &  $\times$   &  $\times$       \\
            PartialDI~\cite{dey2019embedded}          & \checkmark            & $\times$    &  $\times$   &  $\times$       \\
            \textbf{\sysname}                         &\checkmark             & \checkmark  &  \checkmark & \checkmark      \\
            \hline
	\end{tabular} 
\end{table*}
\fi


Instead of fetching the data from other nodes, which incurs more communication overheads, an alternative way is to let each node conduct redundant inference computation in the previous layer. In Figure~\ref{fig:model-partition}(a), when Node2 is conducting inference, we let it take not only the input from the dashed black rectangle but also the input from the dashed red rectangle. Therefore, Node2 is conducting more computation workload (i.e., Node2 does duplicate computation as Node1 and Node3), but it saves the communication overheads because the required In\_Fms can be obtained from the local node in the next layer. 

The redundant computation may not always be beneficial. For instance, when some model layers have large sizes and the inter-node bandwidth is high, trading computation overheads for communication efficiency does not help accelerate the inference. Therefore, given a specific testbed, \sysname should \emph{flexibly} decide whether or not to conduct redundant computation for each model layer (\S\ref{sec-design}).

%TODO: where to add this citation \cite{abdel2020image}

%There are two non-overlapping regions in the bottom feature map; to calculate them, the previous output is needed. The required areas of each bottom area are marked with the same color. As we move backward, the size of the overlapping feature maps continues to increase. Figure 2d shows a specific example of using the 2D-grid partition in DeepThings: four workers are responsible for computing four non-overlapping regions in the GoogLe-Net output layer. Layers are numbered in calculation order. Different colors represent the different input ranges required by the 4 workers. Overlapping areas represent redundant computations.

% \begin{figure}[!t]
% 	\centering
% 	\includegraphics[width=0.49\textwidth]{fig/trade-offs.pdf}
% 	\caption{trade-offs}
% 	\label{fig:trade-off}
% 	% \vspace{-0.5cm}
% \end{figure}


% \begin{figure}[!t]
% 	\centering
% 	\subfigure[trade-off]{
% 		\label{fig:trade-off} 
% 		\includegraphics[width=0.48\textwidth]{./fig/one dim_trade-off.pdf}
% 	}\hspace{-0.5cm}
% 	\subfigure[one dim_trade-off]{
% 	\label{fig:one dim_trade-off} 
% 	\includegraphics[width=0.48\textwidth]{./fig/2d-grid_trade-off.pdf}
% 	}
% 	\caption{2d-grid_trade-off}
% 	\label{fig:2d-grid_trade-off} 
% \end{figure}


% \subsection{Comparison with Prior Works}





% Table~\ref{frameworks} summarizes the comparison between \sysname and the state-of-the-art DNN inference frameworks. %At a high level, MoDNN and DeepSlicing use One-dim scheme to partition the model and DeepThings uses 2D-grid scheme to do so. These frameworks use the single \emph{fixed partition scheme}, thus cannot yield desirable performance for different parts of the model and under different testbed settings, as demonstrated in \S\ref{sec:micro}. On the other hand, DINA~\cite{mohammed2020distributed} and PartialDI~\cite{dey2019embedded} enable flexible model partition for every model layer, such \emph{layer-wise partition} bring performance benefits but ignore the optimization between model layers. By contrast, EdgeCI~\cite{chen2024edgeci} and AOFL~\cite{zhou2019adaptive} explore the fusion optimization; they fuse the operators between layers, thus reducing inter-layer communication cost and effectively accelerating the inference. However, EdgeCI and AOFL only support single partition scheme for each layer and lack the partition flexibility compared with DINA and PartialDI. 
% In summary, the prior frameworks either adopt fixed model partitioning schemes (e.g., one-dimensional partition or 2D-grid partition) or integrate limited optimizations (e.g., layer-wise optimization or simple layer fusion), thus cannot yield consistently high performance towards various models and test platform settings. Indeed, it will create a very large design space when considering the multiple optimization aspects in deciding the partition scheme. Prior works lack for an efficient approach to search and pick out the best partition scheme in the large space, thus cannot fully utilize the resource in the edge platforms for inference acceleration. Motivated by existing drawbacks, we develop \sysname that can outperform the prior frameworks in three aspects. 


% 这些现有的框架在实际部署过程中要么采用固定的模型划分方案（例如，一维划分或二维网格划分），要么集成非常有限的优化（例如，逐层优化或简单的操作融合），无法针对不同的模型层或不同的测试平台设置进行灵活的优化方案组合。

% Moreover, all these existing frameworks require much manual tuning effort during practical deployment, which constrains their application in many scenarios. \sysname, on the other hand, can automatically search for the high-performance partition schemes without human involvement.
% 搜索空间，组合了优化


% Table 1 for JSA
% Table~\ref{frameworks} summarizes the comparison between \sysname and the state-of-the-art DNN inference frameworks. \sysname bypasses the existing drawbacks and outperforms the previous frameworks in three main aspects. 


% (1) \sysname develops a \emph{flexible combinatorial partition} strategy, which enables the inference framework to \emph{fully} explore the optimization opportunities during model inference and efficiently find the optimal partition scheme to yield more parallelism for speedy inference.

% (2) \sysname considers the trade-off between computation and communication. Specifically, when bandwidth is limited, \sysname prefers redundant computation on each node. Otherwise, \sysname lets each node fetch the necessary input data through inter-node communication.

% (3) \sysname automates the partition strategy by using the pre-trained Gradient
% Boosting Decision Tree (GDBT) models and an efficient dynamic programming algorithm. \sysname's automatic partition saves the heavy human tuning effort and enables fast product deployment in practice. 

% (1) \sysname develops a \emph{flexible combinatorial optimization} strategy, which supports flexible partition schemes for each model layer (Layer-Wise optimization) and considers the trade-off between computation and communication for model inter-layer (Fused-Layer optimization). Thus, \sysname can \emph{fully} explore the optimization opportunities and efficiently find the optimal partition scheme to yield more parallelism for speedy inference. (2) \sysname evaluates the time cost of every candidate solution using the Data-driven Gradient Boosting Decision Tree (GBDT) approach, which avoids the heavy human tuning and enables fast product deployment in practice (\S\ref{sec:gdbt-estimator}). (3) \sysname automates the search process based on dynamic programming, which can efficiently find the optimal deployment scheme in the huge search space of combinatorial optimization (\S\ref{sec:dpp}).


Motivated by the existing drawbacks, we develop \sysname to (1) support flexible model partition for each layer, (2) conduct inter-layer optimization to yield better trade-off  between computation and communication, and (3) automate the end-to-end optimization workflow to avoid human tuning effort and efficiently generate the optimal deployment scheme from a huge search space.




% To add some content about combinatorial optimization

% 我们在4节点测试平台上比较了不同分区方案的推理时间成本。图~\ref{Ring-4-node}显示，对于没有加入Layer-wise策略的划分方案而言（OutC 、InH/InW和2D-grid），2D-grid在三个基准中表现最佳。基于 OutC 的分区引入了不小的通信开销。如图~\ref{fig:model-partition}(c) 所示，采用基于 OutC 的分区，每个节点都必须从所有其他三个节点获取输入特征图（即从三个节点中的每一个节点获取分区后的特征图）才能进行推理计算，因此性能最低。One dim.(InH/InW)的性能处于两者之间。然而，2D-grid并不是所有的层都是最优的（如图3），因为2D网格分区都会导致不同程度的计算不平衡。例如，在MobileNet中，当输入特征图为14x14x512或7x7x512时，2D-grid因为InH维度（14）和InW维度（14）都不是节点数（4）的倍数，因此产生了计算不均衡的现象。基于OutC的分区在模型分区的整体上可能不是最优，但在上述情况下可以实现平衡计算（即将 512 个通道均匀地划分到 4 个节点上）。因此在整个模型模型的推理性能上，基于Layer-wise的划分方案考虑了多种划分方案的优势使得划分更加均衡，性能优于三种没有加入Layer-wise策略的方案。

% 另一方面，Fused-layer在2D-grid的基础上通过计算代替传输的思想，融合了某些可以优化的层，使得推理性能进一步得到提升。但值得注意的是，Fused-layer并没有利用到Layer-wise的优势，也就是Fused-layer和Layer-wise性能都优于2D-grid，但Fused-layer和Layer-wise两者的优势没有叠加，而是独立的存在。

% 与上述方案相比，\sysname融合Fused-layer和Layer-wise两者的优势，始终能够通过其动态优化策略（\S\ref{sec:dpp}）找到最合适的划分方案。因此，\sysname 明显缩短了不同类型的各种模型的推理时间。


%combines flexible partitioning with computation and communication trade-offs, uses a novel dynamic programming, and combines with the cost model to accurately and efficiently find the optimal partitioning plan for the model.
% \subsection{Comparison with Prior Frameworks}
% As an overview, Table~\ref{frameworks} compares \sysname and the state-of-the-art DNN inference frameworks on multiple aspects. MoDNN, DeepSlicing and AOFL use One-dim scheme to partition the model and DeepThings uses 2D-grid scheme to do so. All of them use the single fixed partition scheme, thus cannot yield desirable performance for different parts of the model and under different testbed settings, as what we have demonstrated in \S\ref{sec:micro}. Besides, MoDNN, DeepThings and DeepSlicing does not consider the trade-off between computation and communication, which is beneficial to the inference acceleration under bandwidth-constrained scenarios. In addition, all these three frameworks require much manual tuning effort during practical deployment; \sysname, on the other hand, can automatically search for the high-performance partition schemes without human involvement.










