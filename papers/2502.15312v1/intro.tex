\section{Introduction}
The past decade has witnessed a rapid development of deep learning, leading to a variety of novel applications in IoT (Internet of Things) scenarios, including image processing~\cite{abdel2020image,hu2017iot}, video analysis~\cite{long2017edge}, wearables~\cite{bi2019ramt} and so on. Due to network congestion and long physical distances, cloud services cannot meet low latency requirements~\cite{liu2023joint}. Therefore, typical IoT applications tend to train the ML model in beefy clusters (e.g., cloud VMs or bare-metal machines), but the inference computation is usually offloaded to the edge device for the sake of real-time responsiveness~\cite{zhou2019adaptive,chen2024edgeci} and data privacy~\cite{dhar2022studying}. 

While edge-based inference offers more timely results than its cloud-based counterpart, it also leads to unique challenges. Because of the growing size of DNN models, deploying a single-edge device to load and process an entire model has become impractical. Consequently, distributed inference is required to enable collaborative execution of inference tasks. Today's edge applications~\cite{zhang2021deepslicing,zhao2018deepthings,mohammed2020distributed,dey2019embedded} usually employ 4$\sim$6 nodes\footnote{Due to the practical constraints such as energy consumption, today's edge applications usually conduct small-scale (4$\sim$6 nodes) distributed inference, which is different from the cloud-based distributed computing that employs 10s--100s nodes.} to jointly execute the inference for efficiency. Commonly, DNN feature maps are partitioned into different edge devices. Each edge device hosts a partial model and exchanges necessary parameters with others throughout the layer-based inference process.

Reviewing the existing distributed inference solutions, we can divide them into two categories. One line of prior works~\cite{zhao2018deepthings,zhou2019adaptive,mao2017modnn} adopt a fixed partition scheme, regardless of model shapes and testbed settings (e.g., the number of edge devices to run the inference). For instance, DeepSlicing~\cite{zhang2021deepslicing} and MoDNN~\cite{mao2017modnn} adopt the One-dim partition scheme (i.e., partition the model by InW or InH or OutC dimension) whereas DeepThings~\cite{zhao2018deepthings} uses the 2D-grid partition scheme to run the distributed inference task. However, we consider the flexible model partition as a necessity due to two observations in our measurement study (details in~\S\ref{sec:micro}): (1) When running inference computation on the same group of edge devices, different DNN layers yield their optimal inference time with different model partition schemes; (2) when running the inference computation for the same layer of DNN, the varying number of edge devices leads to different optimal model partition schemes, i.e., the optimal partition scheme obtained from one testbed will no longer be the optimal after we switch to another testbed. 

The other line of works~\cite{chen2024edgeci,dey2019embedded,mohammed2020distributed,zhou2019adaptive} introduce limited flexibility into the inference framework. For instance, DINA~\cite{mohammed2020distributed} and PartialDI~\cite{dey2019embedded} adopt layerwise optimization and allow the framework to use different partition schemes for every model layer. However, it ignores the inter-layer dependency, which can be further optimized with layer fusion. On the other hand, EdgeCI~\cite{chen2024edgeci} and AOFL~\cite{zhou2019adaptive} explore the opportunities for layer fusion, but its fusion optimization is only applicable for a single partition scheme. While combining both layerwise optimization and fusion-based optimization seems straightforward, such a combination usually leads to a very large search space in practical deployment. The simple exhaustive search can be time-consuming and requires much expertise, which we believe is the main reason that discourages previous work from combining both \emph{flexible layerwise partition} and \emph{opportunistic layer fusion}. 


\Para{Our goal.} We develop \sysname, which targets the scenarios of edge-based inference that employ multiple (4$\sim$6) devices to jointly compute the inference results. Such scenarios have become prevalent in many practical applications~\cite{4orin,2orin}. \sysname fully considers the optimization opportunities of layerwise optimization and inter-layer fusion to generate the desirable model partition scheme. In order to efficiently find the optimal scheme in a large design space, \sysname incorporates an automatic model partition strategy, named \emph{flexible combinatorial optimization} (FCO). In general, FCO runs the dynamic programming process and data-driven cost model to rapidly pick out the best model partition scheme to deploy for a given model and testbed. To implement FCO, \sysname consists of two main components. 

\Para{Data-Driven Cost estimator (CE, \S\ref{sec:gdbt-estimator}).} \sysname employs Gradient Boosting Decision Tree (GBDT) models to estimate the inference time cost for the model layers. We implement the GBDT based on XGBoost~\cite{chen2016xgboost} and collect data traces for edge-based inference under various settings, and then we use these data traces to train two GBDT models (estimators), namely i-Estimator and s-Estimator. The estimators take as input the DNN layer's metadata (including the height, width, and number of input/output channels, etc.) and the testbed information (including the bandwidth, communication topology, etc.) and then output an estimated time cost. The i-estimator estimates the time cost to complete the inference computation on this model layer; the s-estimator estimates the time cost for all the nodes in the testbed to complete the synchronization of the model layer. Estimated time costs provide useful guide information for \sysname to make choices among partition schemes to minimize the overall time cost (we design a dynamic programming algorithm to achieve this). 


\Para{Dynamic partition planner (DPP, \S\ref{sec:dpp}).} Driven by the i-Estimators and s-Estimator, we continue to design and implement a dynamic programming (DP) algorithm to decide the partition scheme for a given model and testbed. Taking the typical DNN model as an example, the DPP starts the DP process from the last layer of the DNN model. By contacting the i-Estimator, DPP knows the estimated inference time cost when it decides to adopt a specific partition scheme (e.g., InW-based, InH-based, OutC-based, 2D-grid, etc.) for one model layer of some specific shape. By contacting the s-Estimator, DPP knows the estimated cost to synchronize one model layer of some specific shape among the nodes of the cluster. Based on the i-Estimator and the s-Estimator, DPP can search and evaluate a series of partition schemes in the DP search space.  DPP will keep the promising candidate partitions which can lead to the lowest overall inference time and keep traversing forwards, and finally output a desirable partition scheme. Besides, during the DP process, we have also incorporated the pruning strategy to reduce the search space and find the optimal partition scheme (i.e., the scheme yielding the lowest overall inference time) more efficiently.

\Para{Evaluation.} We compare \sysname with five partition schemes, i.e., InW/InH-based, OutC-based, 2D-grid, layerwise and fused-layer partition. We conduct inference with four models under different testbed settings. We find that, (1) The 2D-grid partition outperforms One-dim partition (InH/InW-based and OutC-based) when we conduct the distributed inference on the 4-node testbed. However, the InH/InW-based partition outperforms the 2D-grid and OutC-based partition when we switch to the 3-node testbed. Running on different testbeds, these typical partition schemes can all lead to an imbalance of model partition, and thus are not generally applicable to performance improvement. (2) Both layerwise optimization and data fusion can gain better inference speedup than the fixed partition scheme. However, simply using either strategy fails to yield optimal performance. By comparison, \sysname combines both advantages and can adaptively choose the optimal partition scheme for different models under different testbed settings. Therefore, \sysname outperforms all the baselines across multiple benchmarks with a speedup of 1.10-2.39$\times$.
