In this section, we describe the protocol, models used, and results of two experiments intended to validate that our distributed proprioceptive sensor is able to detect and reject external disturbances. In the first experiment, we applied static forces to the fingertip using calibration weights and examined the fingertip position error based on linear regression trained during the experiments of section \ref{sec:experimental_calibration}, and we compared this to a baseline single-sensor finger consisting of a single POF sensor integrated using the same 3D printing method as the multi-sensor fingers. In the second experiment, we trained a data-driven model using the data acquired during the experiments of section \ref{sec:experimental_calibration} to detect between the finger and a light switch based on deviations of the finger sensor readings from those acquired during actuation in the absence of contact. 
\subsection{Protocol}
\subsubsection{External Disturbance Experiment}
To test the accuracy of our multi-sensor approach in the presence of external disturbances, we applied the same testing protocol as in \ref{subsec:protocol} to each finger with calibration weights tied to the fingertip with nylon fishing line (Fig. \ref{fig:disturbances}a). We ran one test with a \qty{20}{g} weight and one test with a \qty{50}{g} weight for each finger. We then compared the results with those coming from a finger of the same geometry with only one fiber optic running along its length and a single pair of FPCs, one emitter and one receiver. We first tested this version of the finger without weights to determine the baseline calibration for pose estimation purposes and then with the \qty{20}{g} and \qty{50}{g} weights, following the same protocol as before. 

\subsubsection{Contact Detection Experiment}
To demonstrate how the finger's multi-sensor architecture can be leveraged to detect contact with an object in the environment without any knowledge of the actuation state, we selected one finger, placed it underneath a light switch, and actuated it until it flipped the switch while collecting data from the sensors at \qty{8}{Hz}. We actuated the finger in steps of \qty{9}{\degree} with \qty{0.25}{\s} pauses to facilitate webcam image acquisition.

\begin{figure}
\centering
\includegraphics[width=0.9\columnwidth]{fingertip_error_bar_chart.pdf}
\caption{(a) Diagram of experimental setup for applying static loads to fingertip. (b) Pose estimations of the highly integrated finger and single-sensor finger shown with marker feedback. The optical intensity measured by the single-sensor finger increases due to the applied force, resulting in a pose estimation pointing the opposite direction, inconsistent with the operating range of the sensor. (c) Average fingertip position error recorded during each trial.}
\label{fig:disturbances}
\vspace{-1em}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\textwidth]{demo_figure.pdf}
    \caption{Contact detection experiment. The $Q$-statistic from the PCA model of the sensors is used against a threshold value shown in red in the chart to detect external contacts.}
    \label{fig:contact_detection_figure}
    \vspace{-1em}
\end{figure*}

\subsection{Pose Estimation and Contact Detection Models}
For pose estimation, we modeled the phalanges as a kinematic chain in two dimensional space with four rigid links of length \qty{28}{\mm} connected by joints with angles estimated by the sensors. For the multi-sensor fingers, joint angles were estimated by inverting the models obtained in section \ref{subsec:sensor_model}. For the single-sensor finger, we employed the same linear regression approach as the multi-sensor finger to train a model predicting each of the joint angles using the data from the single sensor acquired during the test with no weight.

For the contact detection model, we applied principle component analysis (PCA) to the sensor readings acquired from the quasi-static tests utilizing a technique commonly used for fault detection in industrial processes \cite{Yin2014}. We trained an $m$-dimensional PCA model with $m=3$ on the three-dimensional sensor data from the finger. The first component of the PCA model explains 91\% of the data variability, which is consistent with the one-DOF actuation of the finger and allowed us to conclude that the principal subspace of the PCA model has dimensionality $\beta=1$. The model detects contact by projecting online sensor observations onto the residual subspace of the PCA model and calculating the $Q$-statistic:
\begin{equation}\label{eq:SPE}
    Q=z^\top P_\textrm{res} P_\textrm{res}^\top z
\end{equation}
where $z$ is the $3\times 1$ vector of online sensor observations and $P_\textrm{res}$ is the matrix of residual PCA coefficients. We used the threshold for anomaly detection from \cite{Yin2014} determined with confidence level $\alpha=\text{90\%}$:
\begin{equation}\label{eq:J_th}
    J_{{\rm th,} Q} = \vartheta_{1}\left(\frac{c_{\alpha}\sqrt{2\vartheta_{2}h_{0}^{2}}}{\vartheta_{1}} + 1 + \frac{\vartheta_{2}h_{0}(h_{0} - 1)}{ \vartheta_{1}^{2}}\right)^{1/ h_{0}}
\end{equation}
where $c_\alpha=1.282$ is the normal deviate for the upper $1-\alpha$ percentile and $h_0$ and $\vartheta_i$ are parameters calculated from the variances of the last $m-\beta$ principal components $\lambda_i$:
$$h_{0} = 1 - \frac{2\vartheta_{1}\vartheta_{3}}{3\vartheta_{2}^{2}} \qquad \vartheta_{i} = \sum_{j = \beta + 1}^{m}(\lambda_{j})^{i}; \quad i = 1, 2, 3.$$

\subsection{Results}
The results from the external disturbance experiment (Fig. \ref{fig:disturbances}c) show that the multi-sensor finger estimates the fingertip position with lower error than the single-sensor finger. We expected based on the underactuated training data that the single-sensor prediction would produce approximately equal estimates for each joint angle and that the primary source of pose estimation error would be due to poses with unequal angles. However, we also see in Fig. \ref{fig:disturbances}b that the single-sensor pose prediction points in the wrong direction. This is due to the fact that the optical intensity reading of the sensor increases in the presence of external forces to the point that the sensors observe optical intensity gain rather than loss during the experiment. Because the sensor model is based on a linear regression that assumes the sensor will only experience optical intensity loss, if gain is observed, the sensor will predict a negative angle outside of the designed operational range. While this effect is also present in the multi-sensor fingers, it is mitigated by the redundancy afforded by the multi-sensor architecture. We also note that the magnitude of the weight did not directly correlate to the prediction error across the fingers, at least for the range of weights that was tested. 

The results from the contact detection experiment are displayed in Fig. \ref{fig:contact_detection_figure} and the supporting video to the paper. First contact was observed in the trial video at $t=\qty{0.7}{s}$, and the model detected contact at $t=\qty{4.2}{s}$. At $t=\qty{12.2}{s}$, the finger came out of contact with the light switch, and the corresponding $Q$ dropped below $J_{{\rm th,} Q}$ shortly afterwards, at the next sensor reading update. We note that the finger is able to detect coming out of contact online at \qty{8}{Hz}, while the webcam acquisition misses a \qty{0.25}{\s} frame due to motion blur then takes another \qty{0.25}{\s} frame to detect the new pose of the finger, further demonstrating the merits of a low processing-power distributed device. 