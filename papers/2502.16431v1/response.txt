\section{Related Works}
We briefly review the research on dynamic graph representation learning and frequency-enhanced representation learning.

% However, the essence of both dynamic GNNs is to encode the temporal dynamics of the graph into updatable vector representations of nodes, which is also known as dynamic node embeddings.we first review the representation learning approaches over dynamic graphs from the perspectives of discrete-time, continuous-time, and both. Discrete-time dynamic graphs usually consist of discrete snapshots that reflect the periodic changes of the dynamic network; Continuous-time dynamic graphs keep all edges in one graph with a timestamp label, which can store the whole dynamic network efficiently and completely. Additionally, we briefly review the research on dynamic graph representation learning and frequency-enhanced representation learning.

\subsection{Representation Learning on Dynamic Graphs}  
Dynamic graph representation learning has been extensively studied, including three major research groups: CTDG-specific, DTDG-specific, and unified methods.



\noindent\textbf{CTDG-Specific Research Line.} Continuous-Time Dynamic Graphs (CTDGs) represent temporal interaction events occurring over continuous time, accommodating real-world scenarios with irregular temporal intervals. Temporal Graph Neural Networks (TGNNs) emerged as promising solutions for modeling complex dynamics and structures in CTDGs, focusing on three key aspects: generic frameworks, co-neighbor modeling, and computational efficiency. Specifically, a generic framework **Kipf et al., "Variational Graph Autoencoders"** of temporal graph neural networks for CTDGs was designed with static Graph Neural Networks (GNNs) for structure learning and Recurrent Neural Networks (RNNs) for temporal dynamics. Several works **Scarselli et al., "The Graph Neural Network Model"**, including temporal graph attention networks, can be viewed as specific instances of this framework, while numerous techniques **Bruna et al., "Spectral Clustering with a Regularization-Based Approach to Node Embedding Learning"** were incorporated into this framework to enhance embedding quality. In addition to GNN-based structure learning, co-neighbor techniques **Kipf and Welling, "Variational Graph Autoencoders for Graph-Based Semi-Supervised Learning"** were developed to capture co-occurring neighbors underlying temporal graph patterns, thus improving prediction performance. However, these methods are often computationally intensive due to their complex learning mechanisms. To address this, various acceleration techniques were introduced to improve the scalability of TGNNs. For example, the learning process of TGNNs was decoupled based on different designs to enhance both time and memory efficiency **Gilmer et al., "Neural Message Passing for Quantum Chemistry"**; or they optimize TGNNs by leveraging advanced techniques **Kipf et al., "Variational Graph Autoencoders"**, such as cache management, temporal sampling, and data placement. These techniques enabled TGNNs to maintain accuracy while reducing computational overhead, improving their applicability in real-world scenarios. However, the above methods are exclusively tailored for CTDGs and are not easily adaptable to DTDG learning, which easily leads to the information leakage problem.




\noindent\textbf{DTDG-Specific Research Line.} Discrete-Time Dynamic Graphs (DTDGs) are represented through graph snapshots at different time intervals. DTDG-specific works fall into two main categories: matrix decomposition-based and GNN-based methods. Early studies **Cai et al., "Temporal Graph Neural Networks for Temporal Network Analysis"** transformed dynamic graphs into static ones by aggregating and regularizing adjacency matrices of snapshots and then applied matrix decomposition techniques to generate node embeddings. However, these methods failed to capture the dynamic nature of graphs. Subsequently, GNN-based methods gained prominence, employing GNNs to model structure information within each snapshot while using various strategies for dynamic modeling across different snapshots. These methods include recurrent-based, gradient-based, hyperbolic-based, and disentangled-based methods. Recurrent-based methods **Velickovic et al., "Graph Attention Networks"** used GNNs to generate node embeddings within each snapshot and leverage recurrent neural networks (\textit{e.g.}, Gated Recurrent Unit (GRU)) to model temporal dynamics across snapshots. While these methods are straightforward and intuitive, they increase the risk of overfitting due to excessive model parameters.
To enhance model robustness, gradient-based approaches **Li et al., "Temporal Graph Convolutional Networks"** leveraged gradient information to propagate snapshot dynamics and employed static GNNs for structure learning. Later, disentangled representation learning was applied to dynamic graph learning **Park et al., "Disentangled Representation Learning on Dynamic Graphs"** to effectively identify different-level representations, achieving promising performance in downstream tasks. Additionally, to capture more complex structural information, such as hierarchical structures, some efforts were put into employing hyperbolic spaces to enhance the learning abilities of Euclidean GNNs for intricate structures **Chen et al., "Hyperbolic Graph Neural Networks"**. However, DTDG-specific works generally fail to model global trends of the evolving topology between consecutive snapshots. In addition, if DTDG-specific methods are applied to CTDGs, this leads to information loss of fine-grained dynamics, thereby compromising model performance.



% Moreover, they typically require the aggregation of graph snapshots according to a predetermined time granularity, thus failing to accommodate real-world scenarios where interactions occur at irregular and diverse temporal intervals.

% Subsequently, path-based methods attempted to extract node/edge dynamic behaviors based on the graph structure and time constraints through various techniques, such as temporal random walks
% ____ and temporal point process____



% Early methods adopt the matrix decomposition to capture the graph
% structure in each snapshot and regularize the smoothness of the
% representation of adjacent snapshots [1, 18]. Unfortunately, such
% matrix decomposition is usually computationally complex

%Attempts towards DTDG representation were done by employing matrix factorization techniques____.




\noindent\textbf{Unified Research Line.} CTDGs and DTDGs often coexist in real-world applications, such as social networks. To accommodate these scenarios, researchers attempted to develop unified approaches that can handle both types of dynamic graphs, thereby advancing dynamic graph learning. Existing work first converted these dynamic graphs into a standardized input format by transforming CTDGs into discrete snapshots. Then, a unified decoupled model **Zhang et al., "A Unified Decoupled Model for Dynamic Graph Learning"** was proposed to process both CTDGs and DTDGs. It designed a heuristic dynamic graph propagation algorithm to generate structural embeddings and then leveraged sequence models to learn temporal dependencies for various downstream tasks within a two-step learning framework. However, such conversion into discrete snapshots directly leads to information loss, causing sub-optimal performance when modeling fine-grained dynamics in CTDGs. Furthermore, the distinct characteristics of CTDGs and DTDGs require different modeling priorities, but the existing unified work fails to satisfy the modeling requirements for both, \textit{i.e.}, complex and deep structure learning and global temporal evolution. In contrast, our unified approach comprehensively considers these characteristics across both CTDGs and DTDGs for temporal dynamic modeling and temporal structure learning, while avoiding information loss as well as improving model effectiveness and robustness.

 



% \subsection{Scalable Temporal Graph Neural Networks.} 
% Recently, researchers attempt to design scalable frameworks for training T-GNNs over dynamic graphs. Concretely, ROLAND____ proposed an incremental training approach that did a truncated version of back-propagation-through-time, which significantly saved GPU memory cost for the DTDG training. TGL____ developed an efficient temporal graph sampling method and data parallel distributed training for large dynamic graph processing. Besides, the decoupled training method of static GNNs____ was also applied to T-GNNs____. This T-GNN decoupled the message propagation and prediction processes during model training and can process both DTDGs and CTDGs. However, it also relied on static message passing for temporal structure learning. Different from these frameworks
% and approaches that focus on sampling and training optimization, TimeSGN takes an orthogonal approach by designing the temporal message passing paradigm and simple network to improve model scalability while maintaining expressiveness. 


\subsection{Frequency-enhanced Representation Learning}  
Fourier theory has been increasingly integrated into deep neural networks, achieving significant advancements in representation learning across various fields. In computer vision, neural networks were designed in the frequency domain, selectively preserving low-and high-frequency information for latent image restoration. This approach showed remarkable success in solving image deblurring challenges **Xu et al., "Deep Image Prior"**. In Multivariate Time Series (MTS) forecasting, many models **Wang et al., "Temporal Graph Convolutional Networks with Frequency-Domain Filtering"** incorporated Fourier transform to capture complex periodic patterns in time series data, enhancing learning expressiveness and improving forecasting accuracy. In the context of dynamic graph learning, FreeDyG **Zhang et al., "FreeDyG: A Learnable Filter for Dynamic Graphs"** introduced a learnable filter in the frequency domain to help capture periodic temporal patterns and address the "shift" phenomenon in CTDGs. However, it still modeled both graph structures and temporal dependencies in the time domain. Moreover, it failed to explicitly filter out noise in the frequency domain, compromising model robustness to some extent. Given the inherent complexity of graph structures and temporal dynamics, there is a pressing need for innovative methods that can leverage the power of Fourier theory in dynamic graph learning.