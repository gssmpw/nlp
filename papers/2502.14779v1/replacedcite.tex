\section{Related Work}
% \subsection{Text-to-Image Generation}
\noindent\textbf{Image Generation by Diffusion Models.}
Diffusion models____ have proven to be highly effective in a variety of generative tasks, offering significant improvements in semantic relevance and image quality compared to GAN-based models____. Through iterative image denoising, diffusion models excel in generating high-quality images.
The Denoising Diffusion Probabilistic Model (DDPM)____ introduced a probabilistic generative framework using a diffusion process. Building on this, DDIM____ and PLMS____ improved generation efficiency by utilizing an implicit process and a pseudo-likelihood function.
%
The Latent Diffusion Model (LDM)____ incorporated a Variational Autoencoder (VAE)____ to map images into a latent space, and facilitated the generation of larger and higher-quality images.
%This approach allows training at smaller image resolutions, which, in turn, 
Beyond the Unet-based architecture____, many researcher also explored the integration of Transformer architectures into diffusion models for improved performance. Diffusion Transformer (DiT) models____ have demonstrated improved scalability and effectiveness.

%\subsection{Controllable Diffusion Models}
% \textbf{Controllable Diffusion Models.} 
\noindent\textbf{Controllable Generation.} 
To introduce controllability, conditional diffusion models have been developed to guide the diffusion process using additional information, such as categories or text prompts. Models like Guided Diffusion____ and Classifier-Free Guidance (CFG)____ enable more flexible control by adjusting the strength of conditioning.
Going beyond implicit conditioning, ControlNet____ introduced pixel-wise control through explicit mechanisms, using additional control signals like canny edges, depth maps, or OpenPose keypoints to guide the generation process. 
Similarly, T2I-Adapter____ aligned internal knowledge in text-to-image models with external control signals, improving control precision.
IP-Adapter____ offered greater flexibility in controlling image generation by using the image prompt as a condition, allowing for the integration of multiple conditions simultaneously.
% UniControl/UnicontrolNet
UniControlNet____ and UniControl____ introduced a union control framework for controllable condition-to-image with different conditions.
ControlNeXt____ employs Cross-Normalization to align the conditionâ€™s distribution with the main branch, facilitating faster convergence. ControlNet++____ further refined this approach by incorporating reinforcement learning to better align the diffusion model with the given conditions.

Additionally, the layout has been used as a complementary control signal in text-to-image generation____. 
% Layout2Im____ was the first to incorporate layout as a condition, defining it as a set of objects with bounding boxes and categories. 
Layout2Im____ was the first to incorporate layout as a condition, defining it as a set of objects with bounding boxes and categories. 
%
LostGAN-v2____ employed a reconfigurable layout to control individual objects. 
%
GeoDiffusion____ introduced a geometric control approach, encoding location and object descriptions into pre-trained diffusion models. 
DetDiffusion____ proposed a perception-aware loss to improve the generation quality and controllability.
LayoutDiffusion____ treated each image patch as a distinct object, achieving the integration of layout and image in a unified manner. 
HiCo____ achieved spatial disentanglement by hierarchically modeling layouts.
However, these methods restrict layout information to the position defined by bounding boxes and the semantics specified by categories. As a result, they fall short of customizing the specific content within the boxes based on the given conditions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/Control_Edit.pdf}
    \vspace{-1mm}
    \caption{\textbf{Comparison of user interaction between (a) ControlNet____ and (b) our {DC-ControlNet}}. Users can selectively identify and separately control specific elements, thus achieving an efficient controllable generation.}
    \label{fig:edit}
    \vspace{-6mm}
\end{figure}

%%%%%%%%%%%%%%%%% The Framework %%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/LayoutControlNetwork.pdf}
    \caption{\textbf{The overall pipeline of our DC-ControlNet.} The purpose of our DC-ControlNet is to transform the output content features onto the target layout. An Intra-Element Controller consists of 10 layout blocks, where each block receives a content feature and layout feature. To enable model controlling different types of layout \{dot, box, mask\}, we insert a layout-type embedding. Then, if multiple elements are involved, the condition features are further fused through the Inter-Element Controller to resolve conflicts.}
    \label{fig:over-pipeline}
    \vspace{-3mm}
\end{figure*}