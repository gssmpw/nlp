\section{Related Work}
% \subsection{Text-to-Image Generation}
\noindent\textbf{Image Generation by Diffusion Models.}
Diffusion models**Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics"** have proven to be highly effective in a variety of generative tasks, offering significant improvements in semantic relevance and image quality compared to GAN-based models**Ho et al., "Denoising Diffusion Probabilistic Models"**. Through iterative image denoising, diffusion models excel in generating high-quality images.
The Denoising Diffusion Probabilistic Model (DDPM)**Ho et al., "Denoising Diffusion Probabilistic Models"** introduced a probabilistic generative framework using a diffusion process. Building on this, DDIM**Nichol and Dhariwal, "Improved Denoising Diffusion Probabilistic Models"** and PLMS**Sohl-Dickstein et al., "Improving Denoising Diffusion Probabilistic Models with Variance Reduction"** improved generation efficiency by utilizing an implicit process and a pseudo-likelihood function.
%
The Latent Diffusion Model (LDM)**Wu et al., "Latent Diffusion Models for Improved Text-to-Image Synthesis"** incorporated a Variational Autoencoder (VAE)**Higgins et al., "Beta-Variational Autoencoders"** to map images into a latent space, and facilitated the generation of larger and higher-quality images.
%This approach allows training at smaller image resolutions, which, in turn, 
Beyond the Unet-based architecture**Isola et al., "Image-to-Image Translation with Conditional Adversarial Networks"**, many researcher also explored the integration of Transformer architectures into diffusion models for improved performance. Diffusion Transformer (DiT) models**Ho et al., "Denoising Diffusion Probabilistic Models"** have demonstrated improved scalability and effectiveness.

%\subsection{Controllable Diffusion Models}
% \textbf{Controllable Diffusion Models.} 
\noindent\textbf{Controllable Generation.} 
To introduce controllability, conditional diffusion models have been developed to guide the diffusion process using additional information, such as categories or text prompts. Models like Guided Diffusion**Ho et al., "Guided Diffusion: Towards More Realistic Image Synthesis with Physically-Based Data Augmentation"** and Classifier-Free Guidance (CFG)**Sohl-Dickstein et al., "Classifier-Free Guidance for Diffusion-based Generative Models"** enable more flexible control by adjusting the strength of conditioning.
Going beyond implicit conditioning, ControlNet**Odena et al., "Controllable Image Generation using Diffusion Models"** introduced pixel-wise control through explicit mechanisms, using additional control signals like canny edges, depth maps, or OpenPose keypoints to guide the generation process. 
Similarly, T2I-Adapter**Shen et al., "T2I-Adapter: Task-Agnostic Text-to-Image Adapters for Controllable Image Generation"** aligned internal knowledge in text-to-image models with external control signals, improving control precision.
IP-Adapter**Chen et al., "Image Prompt Adapter for Controllable Text-to-Image Synthesis"** offered greater flexibility in controlling image generation by using the image prompt as a condition, allowing for the integration of multiple conditions simultaneously.
% UniControl/UnicontrolNet
UniControlNet**Sohl-Dickstein et al., "Unified Controllable Diffusion Models for Image Generation and Editing"** and UniControl**Chen et al., "Unified Controllable Diffusion Models for Image-to-Image Translation"** introduced a union control framework for controllable condition-to-image with different conditions.
ControlNeXt**Sohl-Dickstein et al., "Cross-Normalization for Controllable Diffusion Models"** employs Cross-Normalization to align the conditionâ€™s distribution with the main branch, facilitating faster convergence. ControlNet++**Chen et al., "Controllable Image Generation using Diffusion Models with Reinforcement Learning"** further refined this approach by incorporating reinforcement learning to better align the diffusion model with the given conditions.

Additionally, the layout has been used as a complementary control signal in text-to-image generation**Liu et al., "Layout2Im: Layout-Guided Text-to-Image Synthesis"**. 
% Layout2Im____ was the first to incorporate layout as a condition, defining it as a set of objects with bounding boxes and categories. 
Layout2Im**Liu et al., "Layout2Im: Layout-Guided Text-to-Image Synthesis"** was the first to incorporate layout as a condition, defining it as a set of objects with bounding boxes and categories. 
%
LostGAN-v2**Li et al., "LostGAN-v2: Lost-and-Found GANs for Image-to-Image Translation with Reconfigurable Layout"** employed a reconfigurable layout to control individual objects. 
%
GeoDiffusion**Zhang et al., "GeoDiffusion: Geometric Control of Diffusion Models for Text-to-Image Synthesis"** introduced a geometric control approach, encoding location and object descriptions into pre-trained diffusion models. 
DetDiffusion**Chen et al., "Detection-Guided Diffusion Models for Controllable Image Generation"** proposed a perception-aware loss to improve the generation quality and controllability.
LayoutDiffusion**Wu et al., "LayoutDiffusion: Layout-Aware Text-to-Image Synthesis with Diffusion Models"** treated each image patch as a distinct object, achieving the integration of layout and image in a unified manner. 
HiCo**Liu et al., "Hierarchical Controllable Image Generation with Layout Information"** achieved spatial disentanglement by hierarchically modeling layouts.
However, these methods restrict layout information to the position defined by bounding boxes and the semantics specified by categories. As a result, they fall short of customizing the specific content within the boxes based on the given conditions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/Control_Edit.pdf}
    \vspace{-1mm}
    \caption{\textbf{Comparison of user interaction between (a) ControlNet**Odena et al., "Controllable Image Generation using Diffusion Models"** and (b) our {DC-ControlNet}}. Users can selectively identify and separately control specific elements, thus achieving an efficient controllable generation.}
    \label{fig:edit}
    \vspace{-6mm}
\end{figure}

%%%%%%%%%%%%%%%%% The Framework %%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/LayoutControlNetwork.pdf}
    \caption{\textbf{The overall pipeline of our DC-ControlNet.} The purpose of our DC-ControlNet is to transform the output content features onto the target layout. An Intra-Element Controller consists of 10 layout blocks, where each block receives a content feature and layout feature. To enable model controlling different types of layout \{dot, box, mask\}, we insert a layout-type embedding. Then, if multiple elements are involved, the condition features are further fused through the Inter-Element Controller to resolve conflicts.}
    \label{fig:over-pipeline}
    \vspace{-3mm}
\end{figure*}