\section{Related Work}
% \subsection{Text-to-Image Generation}
\noindent\textbf{Image Generation by Diffusion Models.}
Diffusion models~\cite{rombach2022high, ho2020denoising} have proven to be highly effective in a variety of generative tasks, offering significant improvements in semantic relevance and image quality compared to GAN-based models~\cite{goodfellow2020generative}. Through iterative image denoising, diffusion models excel in generating high-quality images.
The Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020denoising} introduced a probabilistic generative framework using a diffusion process. Building on this, DDIM~\cite{song2020denoising} and PLMS~\cite{liu2022pseudo} improved generation efficiency by utilizing an implicit process and a pseudo-likelihood function.
%
The Latent Diffusion Model (LDM)~\cite{rombach2022high} incorporated a Variational Autoencoder (VAE)~\cite{kingma2013auto} to map images into a latent space, and facilitated the generation of larger and higher-quality images.
%This approach allows training at smaller image resolutions, which, in turn, 
Beyond the Unet-based architecture~\cite{ronneberger2015u}, many researcher also explored the integration of Transformer architectures into diffusion models for improved performance. Diffusion Transformer (DiT) models~\cite{chen2024pixartalpha, bao2023all, zhang2023adding, peebles2023scalable, chen2024pixart} have demonstrated improved scalability and effectiveness.

%\subsection{Controllable Diffusion Models}
% \textbf{Controllable Diffusion Models.} 
\noindent\textbf{Controllable Generation.} 
To introduce controllability, conditional diffusion models have been developed to guide the diffusion process using additional information, such as categories or text prompts. Models like Guided Diffusion~\cite{nichol2021glide} and Classifier-Free Guidance (CFG)~\cite{ho2021classifierfree} enable more flexible control by adjusting the strength of conditioning.
Going beyond implicit conditioning, ControlNet~\cite{zhang2023adding} introduced pixel-wise control through explicit mechanisms, using additional control signals like canny edges, depth maps, or OpenPose keypoints to guide the generation process. 
Similarly, T2I-Adapter~\cite{mou2024t2i} aligned internal knowledge in text-to-image models with external control signals, improving control precision.
IP-Adapter~\cite{ye2023ip} offered greater flexibility in controlling image generation by using the image prompt as a condition, allowing for the integration of multiple conditions simultaneously.
% UniControl/UnicontrolNet
UniControlNet~\cite{zhao2023uni} and UniControl~\cite{qin2023unicontrol} introduced a union control framework for controllable condition-to-image with different conditions.
ControlNeXt~\cite{peng2024controlnext} employs Cross-Normalization to align the conditionâ€™s distribution with the main branch, facilitating faster convergence. ControlNet++~\cite{li2025controlnet} further refined this approach by incorporating reinforcement learning to better align the diffusion model with the given conditions.

Additionally, the layout has been used as a complementary control signal in text-to-image generation~\cite{karacan2016learning, reed2016learning,wang2022interactive}. 
% Layout2Im~\cite{zhao2020layout2image} was the first to incorporate layout as a condition, defining it as a set of objects with bounding boxes and categories. 
Layout2Im~\cite{zhao2020layout2image} was the first to incorporate layout as a condition, defining it as a set of objects with bounding boxes and categories. 
%
LostGAN-v2~\cite{sun2021learning} employed a reconfigurable layout to control individual objects. 
%
GeoDiffusion~\cite{chen2024geodiffusion} introduced a geometric control approach, encoding location and object descriptions into pre-trained diffusion models. 
DetDiffusion~\cite{wang2024detdiffusion} proposed a perception-aware loss to improve the generation quality and controllability.
LayoutDiffusion~\cite{zheng2023layoutdiffusion} treated each image patch as a distinct object, achieving the integration of layout and image in a unified manner. 
HiCo~\cite{cheng2024hico} achieved spatial disentanglement by hierarchically modeling layouts.
However, these methods restrict layout information to the position defined by bounding boxes and the semantics specified by categories. As a result, they fall short of customizing the specific content within the boxes based on the given conditions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/Control_Edit.pdf}
    \vspace{-1mm}
    \caption{\textbf{Comparison of user interaction between (a) ControlNet~\cite{zhang2023adding} and (b) our {DC-ControlNet}}. Users can selectively identify and separately control specific elements, thus achieving an efficient controllable generation.}
    \label{fig:edit}
    \vspace{-6mm}
\end{figure}

%%%%%%%%%%%%%%%%% The Framework %%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/LayoutControlNetwork.pdf}
    \caption{\textbf{The overall pipeline of our DC-ControlNet.} The purpose of our DC-ControlNet is to transform the output content features onto the target layout. An Intra-Element Controller consists of 10 layout blocks, where each block receives a content feature and layout feature. To enable model controlling different types of layout \{dot, box, mask\}, we insert a layout-type embedding. Then, if multiple elements are involved, the condition features are further fused through the Inter-Element Controller to resolve conflicts.}
    \label{fig:over-pipeline}
    \vspace{-3mm}
\end{figure*}