\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{iccv}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{blindtext}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{bbm}
\usepackage{bbding}
\usepackage[para]{footmisc}
\usepackage[square,comma,numbers]{natbib}
\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}
% Include other packages here, before hyperref.
\usepackage{amssymb}
\usepackage[table]{xcolor}
% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

\iccvfinalcopy % *** Uncomment this line for the final submission

\def\iccvPaperID{****} % *** Enter the ICCV Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
\newcommand{\conv}{Conv}
% \newcommand{\etal}{\textit{et al}.}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ificcvfinal\pagestyle{empty}\fi

\begin{document}


%%%%%%%%% TITLE
\title{DC-ControlNet: Decoupling Inter- and Intra-Element Conditions in \\ 
Image Generation with Diffusion Models}
% \def\spaces{~~~~~}
% \author{Hongji Yang\dag\and Wencheng Han\dag \and Yucheng Zhou \and Jianbing Shen\footnotemark[1]\\\\

\def\spaces{~~~~~}
\author{Hongji Yang\footnotemark[2],\spaces Wencheng Han\textsuperscript{\footnotemark[2]},\spaces Yucheng Zhou,\spaces Jianbing Shen\footnotemark[1]\\\\
SKL-IOTSC, CIS, University of Macau \\ 
{\tt\small yc47942@um.edu.mo, wencheng256@gmail.com}\\ {\tt\small yucheng.zhou@connect.um.edu.mo, jianbingshen@um.edu.mo}
}




% \def\thefootnote{\dag}\footnotetext{These authors contributed equally to this work}

% \author{First Author\\
% Institution1\\
% Institution1 address\\
% {\tt\small firstauthor@i1.org}
% % For a paper whose authors are all at the same institution,
% % omit the following lines up until the closing ``}''.
% % Additional authors and addresses can be added with ``\and'',
% % just like the second author.
% % To save space, use either the email address or home page, not both
% \and
% Second Author\\
% Institution2\\
% First line of institution2 address\\
% {\tt\small secondauthor@i2.org}
% }

% \maketitle
% Remove page # from the first page of camera-ready.
\ificcvfinal\thispagestyle{empty}\fi

\twocolumn[{%
\renewcommand\twocolumn[1][]{#1}%
\maketitle


\begin{center}
    \centering
    \captionsetup{type=figure}
    \includegraphics[width=1.00\textwidth]{figure/LayoutDiffusion_Headv2.pdf}
    \captionof{figure}{Controllable generation using the proposed \textbf{DC-ControlNet} under various conditions. In our pipeline, each element can be controlled by decoupled attributes, while multiple elements can be naturally fused within a given order, enabling occlusion-aware and flexible customized generation. Results are $1024\times1024$.}
    \label{fig1}
\end{center}
}]
\renewcommand{\thefootnote}{\fnsymbol{footnote}} 
\footnotetext[2]{Equal contribution} 
\footnotetext[1]{Corresponding author}


%%%%%%%%% ABSTRACT
\begin{abstract}
In this paper, we introduce DC (Decouple)-ControlNet, a highly flexible and precisely controllable framework for multi-condition image generation. The core idea behind DC-ControlNet is to decouple control conditions, transforming global control into a hierarchical system that integrates distinct elements, contents, and layouts. This enables users to mix these individual conditions with greater flexibility, leading to more efficient and accurate image generation control.
Previous ControlNet-based models rely solely on global conditions, which affect the entire image and lack the ability of element- or region-specific control. This limitation reduces flexibility and can cause condition misunderstandings in multi-conditional image generation. To address these challenges, we propose both intra-element and inter-element Controllers in DC-ControlNet. The Intra-Element Controller handles different types of control signals within individual elements, accurately describing the content and layout characteristics of the object. For interactions between elements, we introduce the Inter-Element Controller, which accurately handles multi-element interactions and occlusion based on user-defined relationships.
Extensive evaluations show that DC-ControlNet significantly outperforms existing ControlNet models and Layout-to-Image generative models in terms of control flexibility and precision in multi-condition control. 

\noindent\textit{Our project website:} \url{https://um-lab.github.io/DC-ControlNet/}.
\end{abstract}

%%%%%%%%% BODY TEXT

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
% \maketitle


\section{Introduction}
% Text 将所有信息耦合到一起，但是没法对生成内容的细节 如appearance、layout等做出准确的控制。ControlNet 使用了一个单独的分支，将pixel-wise的condition提供详细的appearance信息，解决了text无法准确控制的问题。 然而，controlNet将对全图的控制到了一整张的Pixel-wise 的condition中，让创造准确的condition变得exhausting，也让多条件控制变得非常困难。如图所示，当同时给Controlnet多个condition，每个condition描述图像的某一部分内容，controlnet无法做出准确的区分，将多种条件混合在一起导致错误的结果出现。我们将这些问题的出现归咎于这些方法没有对控制条件进行充分的解耦，将不同元素，不同内容，不同布局的信息耦合到一起不仅限制了控制的灵活性，也让产生了ambiguous，让模型难以正确理解用户意图。为了解决这个问题，在这篇论文中，我们对控制条件进行了解耦，将对全图的控制拆分成不同元素，不同内容和不同布局的有机组合，通过简单的交互就可以将多重复杂条件组合成自己想象中的结果。
% 如Fig3 所示，我们首先将图像控制拆分成了互相独立的几个element，包括不同的前景物体，背景等，after每个element单独的控制之后，通过Multi-condition Fusion module，对不同的element进行逻辑性组合，比如考虑不同element之间的交互和遮挡关系。通过这个改进，不同element之间的condition不再会互相影响，用户也可以分开准备不同element的条件，极大地增加了灵活性。在Element内部，我们再次将condition拆解成Conetent Attribute和Layout Attribute。Content关注图像本身的内容如语义、轮廓、颜色等。Layout关注位置、大小等方面。通过解耦element内部的不同attribute，可以实现更细粒度更加精确地控制。更重要的是，这些attribute是可以任意扩展的，用户可以根据自己的要求设计出任意粒度，任意目的的attribute，将其从其他condition中解耦出来。彻底解决condition耦合带来的问题。

% In order to achieve fine-grained image generation control, \citet{zhang2023adding} introduced ControlNet, a framework that uses a control image as input to produce outputs that are tightly aligned, pixel by pixel, with the given image. While ControlNet proves effective in many scenarios, this pixel-wise control approach can limit creative flexibility, particularly when the conditions need to be adjusted. As illustrated in Fig. 1 (a), pixel-wise control requires that users specify both the desired content and its layout, such as scale and position, within a single bitmap. Any adjustment to the control conditions then involves manually reworking the bitmap, a process that can be both tedious and error-prone. Further complications arise when multiple control signals are introduced, as shown in Fig. 1 (b). When multiple conditions are provided without accurately adjusting their spatial relationships in the bitmap, the network struggles to interpret these conflicting inputs, often producing unsatisfactory results. This makes it even more challenging for users to manually refine the control signals.
Condition control plays a crucial role in image generation~\cite{li2024blip,li2024photomaker,mou2024t2i,shi2024instantbooth,wei2023elite,xiao2024fastcomposer}, requiring the output images to closely align with the designer's specifications, including style~\cite{zhang2023inversion,everaert2023diffusion}, content~\cite{zhang2023adding,controlnetplus}, layout~\cite{zheng2023layoutdiffusion,zhao2020layout2image,cheng2024hico} and other attributes. Most image generation methods~\cite{esser2024scaling,flux,sdxl,rombach2022high} rely on text prompts as the primary mechanism for condition control. Due to the expressive and versatile nature of natural language, text prompts integrate multiple aspects of the conditions into a single input, allowing for holistic control over the generated image.
% @Hongji 这里咱直接从layoutdiffusion开始吧 就不讨论传统diffusion了
% However, natural language struggles to precisely define detailed attributes such as the specific shape or position of an object in an image, as shown in Fig.~\ref{fig:shortcoming}(a). Even with highly detailed and complex text descriptions, generation models often fail to produce images that fully meet the designer’s intent.
However, the layout-to-image generative model struggles to control the specific shape of an element in an image, as shown in Fig.~\ref{fig:shortcoming}~(a). It cannot control the detail color and texture of the ``guitar''.

% Even with highly detailed and complex text descriptions, generation models often fail to produce images that fully meet the designer’s intent.

%%%%%%%%%%%%%%%%%% Figure 2 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figure/LayoutControllerComparison.pdf}
    \caption{\textbf{Comparison between existing methods and ours}. (a) \textbf{layout-to-image models}~\cite{zheng2023layoutdiffusion,cheng2024hico} misalign the shape of the user-specified ``guitar'' accurately. Meanwhile, (b) existing pixel-wise conditions \textbf{ControlNet}~\cite{zhang2023adding,li2025controlnet} cause unsatisfactory textures and layouts in conflict areas when fusing multiple conditions. (c) \textbf{DC-ControlNet} offers a more flexible and precise solution for integrating multi-condition control.
    }
    \label{fig:shortcoming}
    \vspace{-2mm}
\end{figure}

To achieve precise detail control, Zhang~\etal~\cite{zhang2023adding} introduced ControlNet, which integrates pixel-wise control conditions into the image generation process. This approach establishes a clear relationship between the output and the conditions, allowing for accurate control over the output by specifying detailed conditions and overcoming the limitations of purely text-based control. However, ControlNet only supports global conditions, resulting in each condition image influencing the entire image and does not allow for element-specific control. This limitation presents significant challenges for multi-condition image generation.  As shown in Fig.\ref{fig:shortcoming}~(b), when multiple conditions are provided to describe different parts of the image, ControlNet struggles to accurately understand the relationships between these conditions. It cannot determine whether the conditions refer to the same element or different elements, resulting in incorrect artifacts.

To address these challenges, this paper introduces a novel approach that decouples control conditions by breaking down global control into a hierarchical integration of individual contents, and layouts, as shown in Fig.\ref{fig:shortcoming}~(c). Using simple interactions to combine multiple conditions, users can generate results that align with their intentions, achieving flexible and precise control.
Specifically, we first decompose the global image control into independent elements, such as different foreground objects and the background. Each element is controlled individually, and an Inter-Element Controller is then used to logically fuse these elements. This module accounts for inter-element relationship processing, such as lighting variations and occlusions, and integrates them based on the user design. This decoupling enables users to prepare conditions for each element independently, greatly enhancing flexibility.

Within each element, we further decouple the control into two types of conditions: Content conditions and Layout conditions. Content conditions focus on the intrinsic properties of the image, such as semantics, edges, and colors, while Layout conditions address spatial properties, such as position and size. By decoupling different conditions within each element, we further enhance the flexibility and accuracy of control. Each condition only focuses on specific aspects of the element, without the need to interact with other conditions or affect other elements, thereby resolving the issue of multi-condition misunderstandings.
% {\color{red}For instance, a color attribute only needs to define the desired color for a given element without requiring the color to be arranged according to the element’s shape.???} 
As shown in Fig.\ref{fig:shortcoming}(c), the color condition specifies only the desired color for an element, without considering its detailed shape. 
% Moreover, these attributes are highly extensible. 
% Users can design attributes with any level of granularity or purpose, tailoring them to their needs and decoupling them from other conditions entirely. This approach effectively resolves the issues caused by coupled conditions, offering a robust framework for flexible and accurate image control. 
To demonstrate the capability of DC-ControlNet in handling complex multi-condition image generation, we propose a new dataset and the corresponding benchmark, named Decoupled Multi-Condition (DMC-120k). 
This dataset includes a total of 120,000 diverse images with multiple conditions, and it will be made publicly available.
The main contributions are summarized as follows:
\begin{itemize}
\item We introduce DC-ControlNet, which decouples global conditions control into a hierarchical integration of individual contents and layouts, facilitating multi-condition image generation.
\item We propose the Intra-Element Controller, enabling independent control of each attribute within an element, without coupling different aspects.
\item {We present the Inter-Element Controller to effectively handle interactions between multiple individual elements, enabling accurate multi-element conditional image generation.} 
% \item We construct a novel dataset containing a wide range of condition combinations commonly required in practical creative tasks. This dataset is designed for training networks capable of multi-condition control. Additionally, we introduce a new benchmark that, by leveraging Large Vision-Language Models (LVLM), provides an accurate evaluation of image control performance.
\item We construct a novel dataset, DMC-120k, which contains a wide range of samples with multi-conditions. Experiments based on this dataset show that our method significantly outperforms previous control methods in terms of flexibility and precision in multi-condition image generation.
\end{itemize}

\section{Related Work}
% \subsection{Text-to-Image Generation}
\noindent\textbf{Image Generation by Diffusion Models.}
Diffusion models~\cite{rombach2022high, ho2020denoising} have proven to be highly effective in a variety of generative tasks, offering significant improvements in semantic relevance and image quality compared to GAN-based models~\cite{goodfellow2020generative}. Through iterative image denoising, diffusion models excel in generating high-quality images.
The Denoising Diffusion Probabilistic Model (DDPM)~\cite{ho2020denoising} introduced a probabilistic generative framework using a diffusion process. Building on this, DDIM~\cite{song2020denoising} and PLMS~\cite{liu2022pseudo} improved generation efficiency by utilizing an implicit process and a pseudo-likelihood function.
%
The Latent Diffusion Model (LDM)~\cite{rombach2022high} incorporated a Variational Autoencoder (VAE)~\cite{kingma2013auto} to map images into a latent space, and facilitated the generation of larger and higher-quality images.
%This approach allows training at smaller image resolutions, which, in turn, 
Beyond the Unet-based architecture~\cite{ronneberger2015u}, many researcher also explored the integration of Transformer architectures into diffusion models for improved performance. Diffusion Transformer (DiT) models~\cite{chen2024pixartalpha, bao2023all, zhang2023adding, peebles2023scalable, chen2024pixart} have demonstrated improved scalability and effectiveness.

%\subsection{Controllable Diffusion Models}
% \textbf{Controllable Diffusion Models.} 
\noindent\textbf{Controllable Generation.} 
To introduce controllability, conditional diffusion models have been developed to guide the diffusion process using additional information, such as categories or text prompts. Models like Guided Diffusion~\cite{nichol2021glide} and Classifier-Free Guidance (CFG)~\cite{ho2021classifierfree} enable more flexible control by adjusting the strength of conditioning.
Going beyond implicit conditioning, ControlNet~\cite{zhang2023adding} introduced pixel-wise control through explicit mechanisms, using additional control signals like canny edges, depth maps, or OpenPose keypoints to guide the generation process. 
Similarly, T2I-Adapter~\cite{mou2024t2i} aligned internal knowledge in text-to-image models with external control signals, improving control precision.
IP-Adapter~\cite{ye2023ip} offered greater flexibility in controlling image generation by using the image prompt as a condition, allowing for the integration of multiple conditions simultaneously.
% UniControl/UnicontrolNet
UniControlNet~\cite{zhao2023uni} and UniControl~\cite{qin2023unicontrol} introduced a union control framework for controllable condition-to-image with different conditions.
ControlNeXt~\cite{peng2024controlnext} employs Cross-Normalization to align the condition’s distribution with the main branch, facilitating faster convergence. ControlNet++~\cite{li2025controlnet} further refined this approach by incorporating reinforcement learning to better align the diffusion model with the given conditions.

Additionally, the layout has been used as a complementary control signal in text-to-image generation~\cite{karacan2016learning, reed2016learning,wang2022interactive}. 
% Layout2Im~\cite{zhao2020layout2image} was the first to incorporate layout as a condition, defining it as a set of objects with bounding boxes and categories. 
Layout2Im~\cite{zhao2020layout2image} was the first to incorporate layout as a condition, defining it as a set of objects with bounding boxes and categories. 
%
LostGAN-v2~\cite{sun2021learning} employed a reconfigurable layout to control individual objects. 
%
GeoDiffusion~\cite{chen2024geodiffusion} introduced a geometric control approach, encoding location and object descriptions into pre-trained diffusion models. 
DetDiffusion~\cite{wang2024detdiffusion} proposed a perception-aware loss to improve the generation quality and controllability.
LayoutDiffusion~\cite{zheng2023layoutdiffusion} treated each image patch as a distinct object, achieving the integration of layout and image in a unified manner. 
HiCo~\cite{cheng2024hico} achieved spatial disentanglement by hierarchically modeling layouts.
However, these methods restrict layout information to the position defined by bounding boxes and the semantics specified by categories. As a result, they fall short of customizing the specific content within the boxes based on the given conditions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/Control_Edit.pdf}
    \vspace{-1mm}
    \caption{\textbf{Comparison of user interaction between (a) ControlNet~\cite{zhang2023adding} and (b) our {DC-ControlNet}}. Users can selectively identify and separately control specific elements, thus achieving an efficient controllable generation.}
    \label{fig:edit}
    \vspace{-6mm}
\end{figure}

%%%%%%%%%%%%%%%%% The Framework %%%%%%%%%%%%%%%%%%%
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/LayoutControlNetwork.pdf}
    \caption{\textbf{The overall pipeline of our DC-ControlNet.} The purpose of our DC-ControlNet is to transform the output content features onto the target layout. An Intra-Element Controller consists of 10 layout blocks, where each block receives a content feature and layout feature. To enable model controlling different types of layout \{dot, box, mask\}, we insert a layout-type embedding. Then, if multiple elements are involved, the condition features are further fused through the Inter-Element Controller to resolve conflicts.}
    \label{fig:over-pipeline}
    \vspace{-3mm}
\end{figure*}


\section{Methodology}
\subsection{Preliminary}
Diffusion Models (DMs) are powerful generative models that have shown remarkable success in image and video generation. By reversing the noise-adding process in latent space, latent diffusion models (LDMs) transform random noise into a target latent variable. The prediction for $z_{t}$ at time step $t$ is determined by $z_{t+1}$ and $t$. The forward adding noise process and reverse de-noise process can be represented as:
\begin{equation}
    q(z_{t}|z_{t-1})=\mathcal N(z_{t};\sqrt{1-\beta_{t}}z_{t-1}I)
    % p_{\theta}(z_{t-1}|z_{t}) = \mathcal N(z_{t-1};\mu_\theta{z_t,t},\sigma_t^2 I)
\end{equation}
\begin{equation}
    % q(z_{t}|z_{t-1})=\mathcal N(z_{t};\sqrt{1-\beta_{t}}z_{t-1}I) \\
    p_{\theta}(z_{t-1}|z_{t}) = \mathcal N(z_{t-1};\mu_\theta{(z_t,t)},\sigma_t^2 I)
\end{equation}
where $\beta_t$ is a coefficient that controls the noise strength in step $t$ and $\theta$ represents the trained model for giving the prediction $\mu_\theta{(z_t,t)}$.
The loss function of diffusion models is the MSE loss that fits the noise $\epsilon$ by the given noise data $z_t$, timesteps $t$, and condition $c$.
\begin{equation}
    \mathcal L_{mse} = \Vert \epsilon - \epsilon_\theta(z_t, t, c) \Vert_2^2
\end{equation}

To provide detailed control, ControlNet~\cite{zhang2023adding} introduces pixel-wise conditions $c_{f}$ as an additional input. The loss function can be expressed as follows:
\begin{equation}
    \mathcal L_{mse} = \Vert \epsilon - \epsilon_\theta(z_t, t, c, c_{f}) \Vert_2^2
\end{equation}


% \subsection{Problem Formulation}
% Precisely aligning generated images with user intentions remains a significant challenge in image generation. Currently, three primary control methods are widely adopted: global text prompt control, layout-based control as implemented in LayoutDiffusion, and global pixel-wise condition control as introduced by ControlNet.
% Global text prompt control is the most commonly used approach, where a diffusion process generates images by transforming random noise into meaningful outputs based on a given text prompt \( T \). This process can be expressed as:
% \begin{equation}
% G = \text{Diffusion}(\eta \mid T),
% \end{equation}
% where \( G \) is the generated image, \( \eta \) represents the initial random noise, and \( T \) is the global text prompt. While this method is simple and versatile, the lack of fine-grained control often results in outputs that deviate from the user's intended design.

% To enhance control specificity, LayoutDiffusion introduces layout information as additional conditions. These conditions include a list of bounding boxes \( L \) and corresponding text descriptions \( LT \), enabling precise control over the position, size, and description of individual instances. This extended diffusion process is represented as:
% \begin{equation}
% G = \text{Diffusion}(\eta \mid T, L, LT),
% \end{equation}
% where \( L \) defines spatial constraints (e.g., bounding boxes), and \( LT \) provides descriptive details for each instance. While this approach improves instance-level control, it still struggles to achieve fine control over details such as texture and shape.

% To provide detailed control, ControlNet introduces pixel-wise conditions \( P \) as an additional input. By associating \( P \) with specific regions of the output, users can influence details such as texture, shape, and structure. The corresponding diffusion process can be expressed as:
% \begin{equation}
% G = \text{Diffusion}(\eta \mid T, P),
% \end{equation}
% where \( P \) represents a pixel-wise condition map. Although this method enables detailed adjustments, its reliance on global pixel-wise conditions often results in conflicts when handling multiple conditions, leading to issues such as the wrong texture problem illustrated in Figure 4.

% After careful analysis, we attribute the challenges in the ControlNet-based control methods to the coupling issues inherent in global pixel-wise conditions with multiple inputs. Specifically, we identify two distinct types of coupling problems within this framework. The first is the inter-element coupling problem, which arises when the global pixel-wise condition combines inputs from multiple elements. This coupling makes it difficult for users to specify which elements their conditions should control. As a result, isolating and controlling individual elements becomes challenging, finally leading to issues such as incorrect textures. 
% The second issue is the intra-element coupling problem, which occurs within a single element. ControlNet enforces alignment between all conditions and the target on a pixel-wise basis, even when some conditions, such as color information, are unrelated to the target’s shape. This rigid coupling reduces control flexibility and ultimately contributes to the previously discussed ambiguity problems.

% To address the coupling issues in ControlNet, we propose a method to decouple conditions into independent components. This approach ensures greater flexibility and precision in control by tackling both inter-element and intra-element coupling.
% The first step is inter-element decoupling, where conditions corresponding to different elements are encoded separately. This separation ensures that conditions within one element do not interfere with those of another. Once encoded, the elements are combined based on their relationships. Importantly, the definition of an “element” in this context is highly flexible and can be adjusted according to the user’s desired control scope. For instance, elements can represent specific aspects like background, foreground, ambient lighting, or any combination thereof.
% The second step is intra-element decoupling, where we independently process different aspects of the conditions within the same element. This ensures that varying aspects, such as shape and color, do not interfere with each other during the control process.
% By implementing these decoupling strategies, we aim to enhance the flexibility and accuracy of ControlNet, allowing users to exert more precise control over the generated outputs.

% % These findings highlight the need for a more flexible approach to handling global pixel-wise conditions to address these limitations effectively.
% % 这一节写一下多条件控制的分析，举几个典型的例子，最终引出条件耦合是最根本的原因。首先，介绍一下diffusion和controlnet，这里用公式表示一下condition和controlnet，output之间的关系。然后从原理和example上分析一下，多element control在controlnet上很难处理，给几个山神效果的例子，放大，分析bad case （符合siggraph对于效果分析的风格）。在同一个element内部的多condition控制，目前的方法也是不灵活的。比如同一个element的多个condition必须 pixel wise对齐的，这就为构建不同aspect的condition增加困难。比如想将颜色condition和轮廓condition分开控制，在现有的controlnet上，无法直接控制（展示bad case）。必须手动将颜色和轮廓进行pixelwise的align，加大了成本。

% % 产生这些问题的原因，归根究底是global pixel-wise 的condition 将多个element，统一element内部多种attribute耦合到了一起，这导致相对其中某些attribute进行精确地控制变得困难，必须同时编辑全局condition中的每一个方面。


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{figure/fig_union_controlnet1.pdf}
%     % \caption{\textbf{Union-ControlNet.} We first train a Union-ControlNet for further fusion. We extend three additional conditions to allow users to generate the corresponding foreground in the specified area according to the prompt.}
%     \caption{\textbf{Union-ControlNet.} To facilitate the decoupling ControlNet experiments, we first train a Union-ControlNet for further inter-element and intra-element control. Note that we expand three extra layout conditions \{dot, mask, box\} to align with the current layout-to-image diffusion models.}
%     \label{fig:union-controlnet}
% \end{figure*}

\subsection{The Proposed Decoupled ControlNet}
% Traditional ControlNet requires the content to be generated in a pixel-aligned manner, which assumes the layout has been provided by users. In fact, this type of generation can be decomposed into content controlling and layout controlling. While some works (i.e., LayoutDiffusion~\cite{zheng2023layoutdiffusion}, HiCo-Net~\cite{,cheng2024hico}) have implemented layout-drive generation, they still fall short in fine-grained local control or occlusion-aware generation. 
The original ControlNet requires users to provide global conditions to control the generation of an entire image. This approach forces users to combine all elements, such as foreground objects, the background, and their relative positions, into one input, significantly reducing control flexibility. This can complicate the editing process, leading to confusion and potential artifacts, as discussed in the introduction. To address this limitation, our pipeline decouples the control conditions by breaking down the global conditions of ControlNet into a hierarchical organization that manages distinct elements and their individual contents and layouts. This approach enhances the flexibility of user editing, allowing for precise, independent control over each element and its internal conditions.

Fig.~\ref{fig:edit} compares our method to ControlNet. In this case, users want to blend multiple conditions, such as the edge of the background and foreground vehicle and the color of the vehicle. ControlNet requires manual pixel-wise editing of the condition image, followed by merging it into the global condition. This process is time-consuming and prone to causing misunderstandings in the model. Additionally, ControlNet does not support element-specific conditions, which limits the user’s ability to control features like vehicle color without affecting the background. In contrast, our method allows for precise condition setting with simple point-and-drag interactions, supporting element-specific control to accurately achieve the user’s desired foreground color adjustment.
%It is worth noting that our method offers a high degree of flexibility in defining elements. 

As illustrated in Fig.~\ref{fig:over-pipeline}, our pipeline consists of four main components: the Diffusion Model, Content Encoder, Intra-Element Controller, and Inter-Element Controller. The Content Encoder is responsible for encoding the content conditions of each element, capturing different aspects within the element. The Intra-Element Controller integrates the content conditions and their corresponding layout for each individual element. The Inter-Element Controller integrates the conditions of multiple elements, merging them into a final unified condition, which is then passed to the Diffusion Model to generate the desired image.
% Our pipeline is a user-friendly layout-content decoupled control network, with a decoupled framework that enables both fine-grained pixel-wise control and layout-wise control. 
% Our pipeline is a user-friendly decoupled control network, with a decoupled framework that enables both fine-grained intra-element control and inter-element control. 
% Therefore, we define a certain element (or object) consisting of $\{Content, Layout\}$ information, both of which need to be generated to realize the controllable generation. Additionally, the order relationship between multiple elements is also need to be considered.
% For an entire framework, we define a quaternion $\{element, background, target image\}$

 % Moreover, since the conditions for each element are independent, different conditions can be used to specify the content and layout of each element individually.

% The role of ControlNet is to translate the pixel-aligned conditions to the target image. So, we define the original ControlNet as one of our content encoders and introduce an additional Layout-ControlNet, thereby achieving a decoupled generation of content and layout. 
% As shown in Tab. \ref{tab:function}, our method achieves layout-content decoupled generation through the combination of layout controller and content controller. 

% Our framework integrates two novel components: the \textbf{Intra-Element Controller} for different conditions for a single element and \textbf{Inter-Element Controller} for fusing multiple elements.

% \subsection{ControlNet-Union}
% In this way, users are not required to isolate and control every element in the image. Instead, as shown in Fig.~\ref{fig:edit}, they can selectively identify and separately control specific elements as desired, while merging other elements into the background. This design ensures that specifying conditions does not become a burden for users, making the process both efficient and user-friendly.

%%%%%%%%%%%%%%%
\begin{figure}
    \centering
\includegraphics[width=1\linewidth]{figure/LayoutContent.pdf}
    \caption{\textbf{The component of our DC-ControlNet.} (a) \textbf{Different Content Encoders} in our intra-element controller to control the generated images across various aspects; (b) \textbf{The structure of the Inter-Element Controller} for adjusting the order of different elements.}
    \label{fig:content-encoders-inter-element-controller}
    \vspace{-4mm}
\end{figure}

% \subsection{Content Encoder}
\subsection{Intra-Element Controller}
%
In practical creative tasks, users often rely on various control conditions to guide different aspects of target generation. 
For instance, edge conditions can regulate image details and textures, while depth and normal conditions define geometric properties, and color conditions control surface colors.
However, since each condition focuses on a distinct aspect, it is challenging for a unified encoder to accurately encode all these conditions. To address this, our pipeline supports a wide range of content encoders, allowing users to integrate the appropriate encoder into the proposed DC-ControlNet framework.

Fig.~\ref{fig:content-encoders-inter-element-controller}~(a) illustrates several content encoders employed in our method, including the ControlNet~\cite{zhang2023adding} Encoder for edge information, the ControlNetPlus~\cite{controlnetplus} Encoder for geometric attributes, the ConvEncoder, a simple convolutional neural network for encoding color information, and BERT~\cite{devlin2018bert} for text information. 
Additionally, we assign the corresponding positional embeddings to each type of condition. Finally, we concatenate the multi-condition contents along the length dimension before passing features into the Intra-Element Controller.
% In addition, when users need to incorporate specific new condition modalities, such as those from specialized domains like X-rays, they can design their own content encoders. By simply fine-tuning our Intra-Element Controller, the new condition can be seamlessly integrated into the pipeline, enabling flexible adaptation to diverse application scenarios.
% To enhance the generalization of our pipeline, we additionally train a Union-Condition ControlNet as the content encoder. 
%
% This provides a more unified ControlNet for different conditions. Specifically, we follow Xin et.al.~\cite{controlnetplus} approach by using control-type embeddings to indicate the type of input conditions and process through a Transformer block.
% As shown in Fig. \ref{fig:union-controlnet}, the Union-ControlNet is capable of generating corresponding outputs within a single network based on varying conditions, eliminating the need to separately training for each condition.

% Beyond the traditional conditions, we further incorporate dot maps, box maps, and mask maps as the conditions of this ControlNet. This allows users to directly use prompts as indications of content while utilizing conditions such as dot maps, box maps, and masks to specify the layout, which is similar to~\cite{zheng2023layoutdiffusion,cheng2024hico}. Note that the weights of Content Encoder are always fixed during subsequent training. 

Once the intra-element content is defined by content encoders, the next step is to consider how to inject these contents into the given layout.
For the layout image, we define it to be a dot map, box map, or mask map, ranging from weak to strong layout control, which can specify where the element is located (i.e., dot map), the approximate size and location of the element (i.e., box map), or the exact shape and location of the element (i.e., mask map).

The core idea of our Intra-Element Controller is to extract pixel-wise spatial relationships from the layout condition and then inject the content feature into the corresponding areas in a learnable manner. During this injection process, the Intra-Element Controller considers the relative relationships between elements and their associations with the environment, ensuring that the overall layout is accurate and harmonious.
{{Specifically, for a layout representation of an element, we first embed it into a layout embedding \( f_{0} \in \mathbbm{R}^{b\times h \times w \times c} \) through several convolution layers and then add it to the  $x$  in the diffusion UNet. Next, we apply a control-type embedding to indicate the type of content condition, which is injected into the ResBlock in a manner similar to time embedding.}}

% \paragraph{Layout injection}
Considering the content feature $\{h_{1}, h_{2}, \cdots, h_{i}\}$ extracted from Content Encoder, our goal is to equip these features with layout information and output them at their original resolution, so that they can still be efficiently injected into the Unet as ControlNet features.
As shown in Fig.~\ref{fig:over-pipeline}, the controller contains multiple fusion blocks to extract the layout features at different resolutions and inject content features into them. 
Specifically, our layout block contains a ResBlock and a Transformer block, where the ResBlock receives time embedding and layout control-type embedding, and the Transformer block employs cross-attention with the content feature as key and value, followed by a convolution layer for output.

Since layout and content are not pixel-aligned, we apply a differential positioning strategy in cross-attention to give the condition a different position embedding than the query to better amplify this difference. We employ the attention with RoPE~\cite{su2024roformer} and add a fixed offset vector $\Delta$~\cite{tan2024ominicontrol} to the positional embedding of the key. 
\begin{equation}
    {(i,j)}_{\text{content}} = {(i,j)}_{\text{layout}} + \Delta
\end{equation}
where $(i,j)$ denotes the position of the image token.

After the ResBlock and transformer block, we obtain the content features with layout information for Unet. Unfortunately, after this fusion module, Layout-ControlNet introduces distortion in the distribution of the original content feature, which causes slower convergence. Therefore, we apply the Cross-Normalization to recover the output feature with the mean and standard deviation of the original content features. In this way, the output features are consistent in distribution with the original ControlNet's features, avoiding sudden collapses due to large distribution differences or the extra cost of using zero-conv to slowly align the Unet.
\begin{equation}
    \begin{aligned}
    & f_{i}^{'} = f_{i}^{'} + \text{SelfAttn}(f_{i}^{'}) \\
    & f_{i}^{'} = f_{i}^{'} + \text{CrossAttn}(f_{i}^{'}, h_{i}) \\
    & f_{i+1} = f_{i}^{'} + \text{FFN}(f_{i}^{'}) \\
    & h_{i}^{'} = \text{Conv}(f_{i+1}) \\
    & h_{i}^{'} = \left( \frac{h_{i}^{'} - \mu_{h_{i}^{'}}}{\sigma_{h_{i}^{'}}} \right) \cdot \sigma_{h_{i}} + \mu_{h_{i}} \\
    \end{aligned}
\end{equation}


% Fusion Block

% Fusion Block

% %% STN, maybe unuseful
% \paragraph{Spatial Transform Network} Essentially, the control of layout is equivalent to transforming elements into the target area. To realize the transformation, we insert a spatial transform network (STN) to spatially transform the features extracted by ControlNet. Specifically, we use several layers of learnable convolution layers to transform the layout embedding into the affine transformation matrix and then employ it to transform the features. 

% \begin{equation}
%     \theta = Reshape(STN(l_{latent})) = \left[ \begin{array}{ccc} a_{11} & a_{12} & tx\\ a_{21} & a_{22} & ty\\ 0 & 0 & 1 \end{array} \right ]
% \end{equation}
% where $tx$ and $ty$ is the translation parameters and $a_{11}, a_{12}, a_{21}, a_{22}$ control the rotation and scaling of the transformation.


% \paragraph{Layout-AdaLN} Given an element feature $e_i$ extracted by ControlNet and a layout feature $l_i$ of the corresponding resolution, we first convert the layout feature $l_i$ into scale $\gamma_i$ and shift $\beta_i$ by two zero-initialized convolution layers:
% \begin{equation}
%     \gamma_i = \conv(l_i), \beta_i = \conv(l_i)
% \end{equation}
% where $e_i$ refers to the $i$-th element feature map from ControlNet.

% Next, we normalize the element feature and inject the layout with scale $\gamma_i$ and shift $\beta_i$, followed by a feed-forward network.


% \begin{equation}
% \begin{aligned}
%     &h_{i} = LayerNorm\left(e_{i}\right),\\
%     &h_{i}^{'} = \left(1 + \gamma_{i}\right)  \cdot h_{i} + \beta_{i},\\
%     &h_{i+1} = e_i + FFN(h_{i}^{'}),\\
% \end{aligned}
% \end{equation}


% %%% Layout Content Cross-attention
% As a result, we obtain features $h_{i+1}$ that incorporate information from the layout. Subsequently, we fuse the original element features $e_{i}$ with $h_{i+1}$ via Cross-Attention.

% \begin{equation}
% \begin{aligned}
%     &h_{layout} = LayerNorm\left(h_{i+1}\right),\\
%     &h_{content} = LayerNorm\left(e_{i}\right),\\
%     &h_{i+1} = h_{i+1} + CrossAttn(h_{layout}, h_{content}),\\
% \end{aligned}
% \end{equation}

% In this way, the layout information can be fully utilized to enable controllable generation of the specified layout. Moreover, as a plug-and-play module, it can be viewed as a simple converter that wraps the ControlNet outputs within a layout, and its output is compatible with the original ControlNet features.

In this way, the layout information can be fully utilized to enable controllable generation of the specified layout. 
Moreover, its output remains fully compatible with the original ControlNet features to maintain its scalability.
% Moreover, as a plug-and-play module, it can be viewed as a simple converter that wraps the ControlNet outputs within a layout, and its output is compatible with the original ControlNet features.



\subsection{Inter-Element Controller}
% UniControlnet，ControlNet++，
% The original ControlNet can realize the controllable generation with multi-type conditions by directly adding the feature. 
% However, the direct addition of multiple conditions can lead to the following two problems, causing the user to have to carefully scale the conditions: 1. the model prefers some conditions and ignores others, resulting in the conditions not being generated. 2. the order of these conditions is not determined, resulting in obvious artifacts in overlapping or conflicting areas.
% A straightforward strategy is to adjust the scale of each control condition through the user. 
The original ControlNet enables multi-type conditions controllable generation by adding the feature directly.
% {\color{red}However, this strategy is too coarse to reasonably fuse the features since individual feature maps spatially share a single strength.} 
However, this strategy results in an unreasonable feature fusion, as individual feature maps spatially share a single strength, leading to unnecessary artifacts.
%


%%%%%%%%%% NEW %%%%%%%%%%%5
% When dealing with occlusion problems, such as when ``object A is in front of object B'', simply adjusting the scale does not achieve such a fusion, as shown in Fig.~\ref{fig:multi-fusion-result}. 
% Our approach enables the fusion of different conditions for multiple elements, even in the case of conflicts.

When dealing with occlusion problems, such as when ``object A is in front of object B'', simply adjusting the scale does not achieve such a fusion. This is because the model cannot infer the relative positioning of elements based on scale alone. However, our approach enables the fusion of different conditions for multiple elements, even in the case of conflicts.


% Our method, however, allows the fusion of multiple conditions, even 
% Note that the multi-condition fusion here differs from the fusion of multiple instances under a single condition (i.e., layout-to-image generation), or the multi-type conditions fusion of the same instance. Our approach enables the fusion of different conditions for multiple instances, even in the case of conflicts.
% 2. The direct addition of multiple conditions causes large values, which can lead to a collapse model. 
%% The original version of our methods
% The inspiration for our fusion modules comes from ControlNeXt \cite{peng2024controlnext}, which aligns the control distribution with the input, thereby facilitating feature fusion. The insight of the multiple control fusion is to make multiple conditions be processed in the same distributions, preventing from ignored conditions or large values due to distribution differences.
% Specifically, our approach requires scaling multiple control signals, including input $x$, to the same normal distribution for summation. 
% For a given input $x$ and a set of $N$ element controls $\{c_{1}, c_{2}, ..., c_{N}\}$ (after the ControlNet for content and layout generation), we begin by adapting them through the linear layer:

% \begin{equation}
%     \begin{aligned}
%         x^{'} &= x\_adapter(x),\\
%         c_{i}^{'} &= c\_adapter(c_{i}),\\
%     \end{aligned}
% \end{equation}
% where $c_i$ denotes the $i$-th condition and we employ a shared weight $c\_adapter$ for different conditions. 

% Then, each feature is normalized to a normal distribution before being summed. The final features are re-scaled with the mean and standard deviation of the original $x$ after an extra projection layer.

% \begin{equation}
%     \begin{aligned}
%         norm\_x = \frac{x^{'}-\mu_{x^{'}}}{\sigma_{x^{'}}}, norm\_c_{i} = \frac{c_{i}^{'}-\mu_{c_{i}}}{\sigma_{c_{i}}}
%     \end{aligned}
% \end{equation}

% \begin{equation}
%     \begin{aligned}
%         x_f = proj(norm\_x + \sum_i^N norm\_{c}_{i})
%     \end{aligned}
% \end{equation}

% \begin{equation}
%     \begin{aligned}
%         final\_x =  \left(\frac{x_f-\mu_{x_f}}{\sigma_{x_f}}\right) \cdot \sigma_{x} + \mu_{x} 
%     \end{aligned}
% \end{equation}

% where $\mu_{x} $ and $\sigma_{x}$ denote the mean and std of the original input $x$. In this way, multiple control features can be fused in the same distribution to achieve equalization.
%% NEW 
% \paragraph{layer positional embedding.}
% \paragraph{layer positional embedding.}
% \paragraph{Order embedding.}
\noindent{{\textbf{Order embedding.}}
We define this occlusion problem as a problem of layer ordering. Specifically, in overlapping regions, features in {upper layers} should have a higher priority and should not be obscured by features at {lower layers.} When inference, \textit{Object A is in front of object B} can be easily realized by simply adjusting the layer order. 
% Inspired by the video diffusion model, 
We treat $L$ elements and concatenate them according to their temporal order to form an input similar to the video latent features. Similarly, to enable the model to perceive the layer order, we provide it with an order embedding to determine the order relationship of these elements. 

% Considering a set of $L$ elements $\{e_{1}, e_{2}, ..., e_{l}\}$, we first perform an order sorting of the occlusion relationships and then concatenate them into a 4-d input:
% \begin{equation}
%     e^{'} = Concat(Sorted(e_0,e_1, ..., e_{l})), e^{'} \in \mathbbm{R}^{B\times L\times N \times C}
% \end{equation}
Considering a set of $L$ elements $\{x_{1}, x_{2}, ..., x_{l}\}$, we first perform an order sorting of the occlusion relationships and then concatenate them into a 4-d input:
\begin{equation}
    x^{'} = [\text{Sorted}(x_0,x_1, ..., x_{l})], x^{'} \in \mathbbm{R}^{B\times L\times N \times C}
\end{equation}
% where the $L$ denotes the condition dimension, while $N$ denotes the spatial dimension.
where the $B$, $L$, $N$, and $C$ denote the batch size, layer dimension, spatial dimension, and channel dimension, respectively. 

After assigning the order embeddings, we obtain a multi-condition representation that describes both the order and spatial relationships. In our implementation, since we use RoPE, the order embedding is applied before the attention, rather than directly adding to the features.
% Inspired by the video diffusion model, we consider multiple features from ControlNets as layers, where the features.

%\vspace{-2mm}
% prior embedding -> Spatial attention -> condition attention -> softmax -> cdot 
%\paragraph{Spatial and Condition reweighing Transformer}
%
\noindent{{\textbf{Spatial and Layer Reweighing Transformer.}}
% The goal of resolving overlap and conflict areas is to decide which layer of the condition should control a specific region. Therefore, we employ spatial transformers and condition transformers to predict spatial and condition weights for reweighing. 
%
To accurately distinguish the position each element occupies in both the spatial and layer dimensions, we employ spatial and layer transformers to predict spatial and layer weights for reweighing. 
The structure of the fusion module is shown in Fig.~\ref{fig:content-encoders-inter-element-controller}~(b). 
%
{Specifically, we apply the spatial transformer and layer transformer to predict the weight of feature at the spatial dimension and layer dimension, respectively.} These transformers are used solely to predict the weights, rather than directly modifying the features. Before the spatial/layer transformer, we first merge the irrelevant dimensions and then feed them into the corresponding transformer. In this step, we apply RoPE to encode the positions of the spatial or layer dimensions, helping the model to perceive the relative spatial positions or the order of elements. The process of spatial transformer and layer transformer can be expressed in Algorithm~\ref{algo:srt}. and Algorithm~\ref{algo:crt}, respectively.

% \begin{equation}
%     \begin{aligned}
%     & x = rearrange(x, (\:b\: l\:) \:n \:c \rightarrow (\:b \:n\:) \:l \:c) \\
%     & x^{'} = x + ConditionSelfAttn(LN(x)) \\
%     & x^{'} = x' + FFN(LN(x')) \\
%     & w_{condition} = Sigmoid(Linear(x^{'})) \\
%     & x = x * w_{condition} \\
%     & x = rearrange(x, (b\: n) \:l \:c \rightarrow b \:l \:n \:c) \\
%     \end{aligned}
% \end{equation}

% where b, l, n, c represent the batch size, number of conditions, number of image tokens and channel dimensions. 

\vspace{-2mm}
\begin{algorithm}[H]
    \caption{Spatial reweighing Transformer}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \label{algo:srt}
    \begin{algorithmic}[1]
        \REQUIRE The representation $x \in \mathbbm{R}^{B\times L\times N \times C} $  %%input
        \ENSURE The spatial-reweighed $x \in \mathbbm{R}^{B\times L\times N \times C} $
        
        $x = \text{rearrange}(x, \:b\: l\: \:n \:c \rightarrow (\:b \:l\:) \:n \:c)$

        $x^{'} = \text{LayerNorm}(x)$

        $q = W_{q}\cdot x^{'} , k = W_{k}\cdot x^{'}  , v = W_{v}\cdot x^{'}   $

        $q^{'} = q\cdot R(\theta), k^{'} = k\cdot  R(\theta)$ \COMMENT{2d pos embedding}

        $x^{'} = x + \text{Attention}(q^{'}, k^{'}, v)$

        $x^{'} = x^{'} + \text{FFN}(\text{LayerNorm}(x^{'}))$

        $w_{\text{spatial}} = \text{Sigmoid}(\text{Linear}(x^{'}))$ \COMMENT{zero-init linear}

        % $x = x * 2 * w_{spatial}$ \COMMENT{reweighing}
        $x = x * w_{\text{spatial}}$ \COMMENT{reweighing}
        
        $x = \text{rearrange}(x, (b\: l\:) \:n \:c \rightarrow b \:l \:n \:c)$
        \end{algorithmic}
\end{algorithm}
\vspace{-3mm}


\begin{algorithm}[H]
    \caption{Layer reweighing Transformer}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \label{algo:crt}
    \begin{algorithmic}[1]
        \REQUIRE The representation $x \in \mathbbm{R}^{B\times L\times N \times C} $  %%input
        \ENSURE The layer-reweighed $x \in \mathbbm{R}^{B\times L\times N \times C} $
        
        $x = \text{rearrange}(x, \:b\: l\: \:n \:c \rightarrow (\:b \:n\:) \:l \:c)$

        $x^{'} = \text{LayerNorm}(x)$

        $q = W_{q}\cdot x^{'} , k = W_{k}\cdot x^{'}  , v = W_{v}\cdot x^{'}   $

        $q^{'} = q\cdot R(\theta), k^{'} = k\cdot  R(\theta)$ \COMMENT{1d order embedding}

        % $x^{'} = x + Softmax(\frac{q^{'}\cdot {k^{'}}^{T}}{\sqrt{d}}) \cdot v$
        $x^{'} = x + \text{Attention}(q^{'}, k^{'}, v)$

        $x^{'} = x^{'} + \text{FFN}(\text{LayerNorm}(x^{'}))$

        $w_{\text{layer}} = \text{Softmax}(\text{Linear}(x^{'}))$ 

        $x = x * w_{\text{layer}}$ \COMMENT{reweighing}
        
        $x = \text{rearrange}(x, (b\: n\:) \:l \:c \rightarrow b \:l \:n \:c)$
        \end{algorithmic}
\end{algorithm}
\vspace{-3mm}
Here, b, l, n, c represent the batch size, number of layers, number of image tokens and channel dimensions. Note that in spatial reweighing transformer, we apply 2d positional embedding. For simplicity of expression, we unify the 2d positional embedding and 1d order embedding under the notation $R(\theta)$.
% % 
% \begin{equation}
%     \begin{aligned}
%     % & x = rearrange(x, (\:b\: l\:) \:n \:c \rightarrow (\:b \:n\:) \:l \:c) \\
%     & Q = W_Q\cdot x, K = W_K\cdot x, V = W_V\cdot x \\
%     & R(\theta) = \[
%     \begin{matrix}
        
%         cos(\theta), -sin(\theta) \\
%         sin(\theta), cos(\theta) \\
        
%     \end{matrix}\] \\
%     & Q^{'} = Q \times R(\theta) \\
%     & K^{'} = K \times R(\theta) \\
%     \end{aligned}
% \end{equation}
The output is then passed through a linear layer and an activate function to obtain the spatial or layer weights. 
The spatial weights serve to emphasize the main content of each element, while the layer weights highlight which element plays a significant role for each image token. 
% sigmoid for spatial dimension / softmax for layer dimension
{As the relationships between different layers of the same pixel are mutually exclusive, we apply softmax to obtain the final weight, which distinguishes our method from the spatial reweighting transformer. When obtaining spatial weights, we use a zero-initialized linear layer followed by a sigmoid activation. With these improvements, we achieve precise control by reweighting the features along both the spatial and layer dimensions. Furthermore, this reweighting approach prevents the model from taking shortcuts by directly modifying the content of specific elements. The final output is obtained by summing along the layer dimension.}




\subsection{Loss Function}

% % Given data sampled from a read data distribution $x_0 \sim q(x_0)$, we follow the DDIM to perform the forward process. 
% The image diffusion model is trained to predict the added noise in latent space. 
% In step $t$, $z_t$ in latent diffusion models is defined as: $z_t = \sqrt{\overline\alpha_t}z_0 + \sqrt{1-\overline\alpha_t}\epsilon$, where $\epsilon\sim \mathcal N (0,1)$ is Gaussian noise and $\overline\alpha_t=\prod_{i=1}^t(1 - \beta_t)$ and $\beta_t$ is a coefficient that controls the noise strength in step $t$. The model achieves the denoising process by fitting the noise by given noised data $z_t$, timesteps $t$, and condition $c$. 

To further facilitate the model to learn the distribution of the foreground, we follow~\cite{chen2024geodiffusion}, employing the re-weighting strategy to assign a greater weight to the foreground. Specifically, we set the weight of the foreground region as inverse proportion to its area relative to the total latent area.
\begin{equation}
m_{i,j} = 
\begin{cases} 
1, & (i,j) \in \text{background} \\
\frac{\text{Aera}_{\text{total}}}{\text{Aera}_{\text{foreground}}}, & (i,j) \in \text{foreground}
\end{cases} 
\end{equation}

Here, the weight of the pixel $(i,j)$ is assigned based on the area of the foreground region, which is set equal to that of the entire image. This implies that the optimization objective for the foreground region is equivalent to that of the background. The smaller the foreground region, the larger its weight, ensuring that the target is not ignored. Note that the loss reverts to the original one when the area of the foreground region equals the total image area. 
Second, a feature-wise L1 loss $L_{transform}$ supervises the spatial transformation of features in the intra-element controller by comparing transformed features $ h^{'}_{i}$ with target feature $\hat{h}^{'}_{i}$ from the ControlNet. Both loss is weighted by $m$.
The final optimization objective can be defined as follows:
% \begin{equation}
%     \mathcal L_{mse} = \Vert \epsilon - \epsilon_\theta(z_t, t, c) \Vert_2^2 \cdot m
% \end{equation}
% To supervise the spatial transformation of the element features in the layout controller, we apply a feature-wise l1 loss for the ControlNet features of ground truth and the outputting features after the layout controller.
% \mathcal L_{transform} = \Vert h_{i+1} - h^{'}_{i+1} \Vert_2^2
\begin{equation}
    \mathcal L_{transform} = \Vert h^{'}_{i} - \hat{h}^{'}_{i} \Vert_1
    \label{l_transform}
\end{equation}
% where the transformed features $h_{i}$ and the target features $h^{'}$ is $i$-th feature in layout controller and in ControlNet, respectively. 
% Finally, the total loss can be expressed as follows:
\begin{equation}
    \mathcal L_{total} = \mathcal L_{mse} \cdot m + \lambda\mathcal L_{transform} \cdot m
\end{equation}
where $\lambda$ is a coefficient to balance the weight of the loss.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figure/Control_Dataset.pdf}
    \caption{\textbf{The proposed DMC-120k dataset}. We first generate the multi-element image with random sampled categories and scenarios and inpaint the elements in order. The conditions are then obtained by corresponding condition extractors.}
    \label{fig:dataset}
    \vspace{-2mm}
\end{figure}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/Control_Illustration.pdf}
    \caption{\textbf{The results of our DC-ControlNet}. Each element can generate pixel-aligned output through given content conditions, as well as the layout conditions. Besides, in overlapping regions, users can resolve conflicts by adjusting the layer order and leveraging our layer fusion module to achieve occlusion-aware generation.}
    \vspace{-1mm}
    \label{fig:total_exp}
\end{figure*}


%%%%%%%%%%%% DATASET %%%%%%%%%%
\section{The Proposed DMC-120k dataset}
\label{sec:dataset}
% \textbf{Dataset.}
% SDXL/FLUX Generation -> GroundingDino SAM -> Inpainting -> Condition Aux
% We construct a large high-quality training dataset with layout conditions and content conditions. 
% To construct such a dataset, we first generate images containing elements using SDXL~\cite{sdxl} or FLUX~\cite{flux} and use GroundingDino~\cite{liu2025grounding} to identify the primary objects within the images. 
% After getting the mask of the generated images, we generate layout conditions like dot, mask, or box maps for the objects and upsample the segmented images. Next, we perform inpainting on the original images to remove the elements. Finally, we extract content conditions like canny, depth, normal map from both the segmented object images and the inpainted images. In this way, we collect a 120k samples dataset, each containing $\{e_{content}, e_{layout}, I_{trg}\}$ for generation. For training, we use only a single object to achieve layout-controllable generation. The size of train/test set is 120,000/1,000. The dataset will be released later.
%
{To fully leverage the potential of our method for multi-conditional image generation, we introduce a new high-quality dataset, DMC-120k, along with a new benchmark.
The core innovation behind this dataset is the idea of decoupling a complex image containing multiple elements into several independent conditions. These decoupled conditions are then used as training data to guide a model in reconstructing the original image.
Specifically, the DMC-120k dataset comprises 120,000 samples, each containing at least two independent elements with their ${e_{\text{content}}, e_{\text{layout}}}$, and a final target image, $I_{\text{trg}}$. The dataset is split into a training set of 120,000 samples and a test set of 1,000 samples. \textit{The entire dataset will be made publicly available.}
The total pipeline of dataset construction is shown in Fig.~\ref{fig:dataset}.

\noindent\textbf{Image Creation.} The first step is to generate complex images composed of multiple distinct elements. To achieve this, we define more than 100 different category labels, including animals, furniture, household items, and \textit{etc.}. We then construct prompts based on randomly selected categories and scenarios. These prompts are further optimized using ChatGPT to improve their suitability for the generation model and to enhance the variety of generated prompts. For each prompt, we use a random seed and generate high-resolution images at $1024 \times 1024$ pixels, employing the open-source SDXL~\cite{sdxl} and FLUX~\cite{flux} models to generate high-quality image data under various conditions.
% \noindent \textbf{Caption sampling.} We first use ChatGPT~\cite{chatgpt} to sample different objects and backgrounds as the final caption. For the objects, we sample 100 categories, including animals, furniture, households items and more. Then, we sample multiple scenes as backgrounds and make ChatGPT provide a brief description for each scene. Finally, we construct the image prompt using the template \textit{``a \{object\} in \{scene\}.''}, and add additional descriptive prompts to assist in generating high-quality and more aesthetically pleasing images.

\noindent\textbf{Element Decoupling.} Next, we use GroundingDino~\cite{liu2025grounding} to detect the location and shape of each object in the image. After obtaining the mask, we crop the foreground object and upsample it to $1024 \times 1024$ pixels, which is then used to extract the foreground conditions.
If two or more foreground element masks overlap, we label them as potentially occluding each other. To address occlusion, we first erase one of the masks and apply SDXL-Inpainting~\cite{sdxl} to inpaint the image. We then redetect the remaining objects to ensure they are intact. This process is repeated for all occluding elements until no occlusion remains. To extract the background conditions, we apply SDXL-Inpainting using the masks of all the foreground objects and inpaint the image.
% image synthesis %%\noindent{{\textbf{Image synthesis.}
% \noindent\textbf{Image Creation.} We randomly select different objects and scenes to form the content to be generated. For each prompt, we employ a random seed to generate a high-resolution image at $1024\times1024$ pixels. Then, we employ GroundingDino~\cite{liu2025grounding} to detect the location and exact shape of the given object. After the mask is obtained, we crop out the object and upsample it.
% To obtain an image with only the background, we apply SDXL-Inpainting~\cite{sdxl} and the mask in the previous step to inpainting the image. This step is repeated several times until GroundingDino can no longer locate the object from the image.

\noindent\textbf{Condition generation.} The final step is to obtain the conditions. We achieve this by using various condition detectors for both foreground and background images. In our dataset, we provide detailed conditions such as canny, {HED}, depth, segmentation, and normal maps for content control. Additionally, we include dot maps, box maps, and mask maps as layout control conditions.
}

% inpainting
% \vspace{-3mm}
% \paragraph{Object remove.} To obtain an image with only the background, we apply SDXL-Inpainting~\cite{sdxl} and the mask in the previous step to inpainting the image. This step is repeated several times until GroundingDino can no longer locate the object from the image.
% generate different conditions
%\vspace{-3mm} \noindent{{\textbf{Condition generation} }
% \begin{figure}
%     \centering
%     % \includegraphics[width=1\linewidth]{figure/prompt-1.pdf}
%     \caption{The prompt of evaluation for element existing and element occlusion with LVLMs. }
%     \label{fig:prompt}
% \end{figure}

%\subsection{Metrics}
% We use our test set for quantitative evaluations of our design choice. Given that our method is a decoupled ControlNet, we consider the following aspects as our metrics: the quality of the generated image, the accuracy of the generated instances, the precision of the position of generated instances, and the sequential relationships between instances. 
% Therefore, we use the IoU, FID, CLIP score, and CLIP-aes metrics used in ControlNet \cite{zhang2023adding} to evaluate the quality and accuracy.

%%%%%%
% \textbf{Metrics.} We use our test set for quantitative evaluations of our design choice. Given that our method is a decoupled ControlNet, we consider the following aspects as our metrics: the accuracy of the generated instances and the sequential relationships between instances. 
% % Therefore, we use the IoU, FID, CLIP score, and CLIP-aes metrics used in ControlNet \cite{zhang2023adding} to evaluate the quality and accuracy.
% In particular, we incorporate VLM (Vision-Language Models) as one of the evaluators due to its powerful image-understanding capabilities. ChatGPT~\cite{chatgpt} is employed to check whether the corresponding instances have been successfully generated and whether the sequential relationships between instances are correct. The prompt we used is shown in Fig.~\ref{fig:prompt}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{table}
%     \centering
%     \setlength{\tabcolsep}{2mm}{
%     \begin{tabular}{c|ccccc}
%     \toprule
%     Methods & $Accuracy_{exist}$  & $Accuracy_{occ}$  \\ \bottomrule
%         MultiControlNet\cite{zhang2023adding} & 21.5\% & 49.2\%\\
%         T2I-Adapter\cite{mou2024t2i} &16.2\% & 51.2\%\\
%         UniControlNet\cite{zhao2023uni} & 81.8\% & 56.5\%\\
%         ControlNet++\cite{li2025controlnet} & 11.2\%& 49.8\%\\ \midrule
%         LayoutDiffusion &  57.2\% &52.1\%  \\
%         HiCo & 69.2\%& 52.5\% \\
%         DC-ControlNet (ours) &  \textbf{92.5\%} & \textbf{91.2\%} \\
%     \bottomrule
%     \end{tabular}}
%     \caption{The results of different methods for multiple elements in our datasets.}
%     \label{tab:qualitative_result}
% \end{table}



\section{Experiment}
\subsection{Experimental Setup}
Our DC-ControlNet is based on SDXL~\cite{sdxl} to achieve controllable generation. All experiments are conducted on eight A100 GPUs with mixed precision training. Our training is divided into three stages for training Union-ControlNet, Intra-Element Controller and Intra-Element Controller, respectively.
For Union-ControlNet, we use the following eight conditions: canny, HED, depth, segmentation map, normal, dot map, mask map, and box map for training. The model is trained using AdamW optimizer with a fixed learning rate of 1e-4 for 50,000 steps. 
%
For Intra-Element Controller, we employ the AdamW optimizer with a fixed learning rate of 1e-4 for 50,000 steps with a batch size of 32 and training for about one day. The dropout rate of the prompt is set to 0.2 in training. All training images, including the target image and the conditioning images, are set to $1024\times1024$ in the model. 
For the Inter-Element Controller, we use the same hyperparameters for training as for the Intra-Element Controller. 
%Note that in each stage, the other modules are frozen.


% \subsection{Qualitative results}
% The methods we compare are mainly the layout-to-image generation and the ControlNet models. We employ the metric in Sec.~\ref{sec:dataset} to evaluate the accuracy of the generated elements and the accuracy of the occlusion relationships. 
% We perform a comparison with existing methods on our newly proposed dataset. 

% As shown in Tab.~\ref{tab:qualitative_result}, our method 

% \begin{table*}
%     \centering
%     \begin{tabular}{c|cccc}
%     \toprule
%     Methods & $IoU_{object}$ & $Accuracy_{exist}$  & $Accuracy_{occ}$ & FID \\ \bottomrule
        
%         LayoutDiffusion &  & 57.2\% &52.1\% &  \\
%         HiCo & & 69.2\%& 52.5\% & \\
%         Ours & & \textbf{92.5\%} & \textbf{91.2\%} & \\
%     \bottomrule
%     \end{tabular}
%     \caption{Caption}
%     \label{tab:my_label}
% \end{table*}


% \begin{table*}
%     \centering
%     \begin{tabular}{c|ccccc}
%     \toprule
%     Methods & $IoU_{object}$ & FID & $Accuracy_{occ}$ & CLIP Score & CLIP-Aes \\ \bottomrule
%         MultiControlNet\cite{zhang2023adding} & & & & \\
%         T2I-Adapter\cite{mou2024t2i} & & & & \\
%         UniControlNet\cite{zhao2023uni} & & & & \\
%         ControlNet++\cite{li2025controlnet} & & & & \\
%         Ours & & & & \\
%     \bottomrule
%     \end{tabular}
%     \caption{The results of multiple elements in our datasets.}
%     \label{tab:my_label}
% \end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}
%     \centering
%     \setlength{\tabcolsep}{2mm}{
%     \begin{tabular}{c|ccccc}
%     \toprule
%     Methods & IoU & FID & $Accuracy_{exist}$  & $Accuracy_{occ}$  \\ \bottomrule
%         MultiControlNet\cite{zhang2023adding} & & 82.51 & 21.5\% & 49.2\%\\
%         T2I-Adapter\cite{mou2024t2i} & & 106.23 &16.2\% & 51.2\%\\
%         UniControlNet\cite{zhao2023uni} &  &45.63 & 81.8\% & 56.5\%\\
%         ControlNet++\cite{li2025controlnet} & & 91.58& 11.2\%& 49.8\%\\ \midrule
%         LayoutDiffusion &  &  & 57.2\% &52.1\%  \\
%         HiCo & &  & 69.2\%& 52.5\% \\
%         Oures & & 23.66 & 92.5\% & 91.2\%\\
%     \bottomrule
%     \end{tabular}}
%     \caption{The results of multiple elements in our datasets.}
%     \label{tab:my_label}
% \end{table*}




\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/Control_Intracontroller_v2.pdf}
    \caption{\textbf{The result of only our intra-element controller}. The intra-element controller can effectively and accurately transform the given condition content to the target layout.}
    \label{fig:layout-controller}
    \vspace{-2mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figure/ControlComparison.pdf}
    \caption{\textbf{Comparisons with ContorlNet~\cite{zhang2023adding}, UniControlNet~\cite{zhao2023uni}, ControlNet++~\cite{li2025controlnet}, Layout Diffusion~\cite{zheng2023layoutdiffusion} and HiCo~\cite{cheng2024hico}.} The prompt we use is ``\textit{A bag and a bear in a cozy living room}''.}
    \label{fig:multi-fusion-result}
    \vspace{-2mm}
\end{figure*}

% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figure/fig_campare2.pdf}
%     \caption{The comparison with ContorlNet, T2I-Adapter, UniControlNet, ControlNet++, Layout Diffusion and HiCo. The prompt we use is ``\textit{A cup and a sandwich in a stage of a theatres}''.}
%     \label{fig:multi-fusion-result1}
%     \vspace{-2mm}
% \end{figure*}

\subsection{Quantitative Results}
In Fig.~\ref{fig:total_exp}, we show the visual results of our method, it takes the layout condition, content condition of the element, and background as inputs.
Unlike ControlNet, which requires the user to provide detailed layout and content information, our approach decouples content and spatial layout control as separate conditions, allowing the user to control independently. 
%
Fig.~\ref{fig:layout-controller} illustrates the result of the intra-element controller, our intra-element controller is capable of controlling the ControlNet outputs of different conditions based on varying layout conditions.

To better compare other controllable image generation models, we give a comparison with existing models.
There are two main classes of approaches we compare, one focusing on customizing the content of different condition types and then fusing them {(e.g., ControlNet~\cite{zhang2023adding}, UniControlNet~\cite{zhao2023uni}, and ControlNet++~\cite{li2025controlnet}).} However, this type of approach ignores the fusion strategy when fusing in the conflict areas, leading to obvious artifacts in the conflict region. For example, as shown in Fig.~\ref{fig:multi-fusion-result}, the ``bag'' and ``bear'' melt in the overlap area. 

Another type of approach focuses on generating a specified object within a particular region based on the given prompt. These methods can only specify approximate objects, not {fine-grained control} of content (e.g., Layout Diffusion~\cite{zheng2023layoutdiffusion}, HiCo~\cite{cheng2024hico}). 
%
Besides, it cannot effectively specify the sequential order of the elements instead relying on prior knowledge from the training datasets. As shown in Fig.~\ref{fig:multi-fusion-result}, the generated content cannot be specified, and the order in which the elements appear is random.

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/fig_compare.pdf}
%     \caption{The comparison with ContorlNet, T2I-Adapter, UniControlNet, ControlNet++, and HiCo. The prompt we use is ``\textit{A bag and a bear in a cozy living room}''.}
%     \label{fig:multi-layer-compare}
% \end{figure*}
As shown in Fig.~\ref{fig:multi-fusion-result}, our proposed module solves the overlap and conflict problem very well. The corresponding image can be generated by simply adjusting the layer order. For example, if conditions are entered in the order of ``a bag'', ``a bear'' and ``a cozy living room'', an image of a living room with a bag in front of a bear can be generated. Similarly, adjusting the conditional order of ``a bag'' and ``a bear'' produces a different image, with the same elements appearing in a different order.

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.8\linewidth]{figure/fig_layers.pdf}
%     \caption{The result of our multi-conditions fusion module.}
%     \label{fig:multi-fusion-result}
% \end{figure}




\subsection{Ablation Studies}

% For the ablation studies, we analyze the two proposed modules, the Layout Controller and Order-based Multi-Condition Fusion modules.

% \subsubsection{Layout Controller}
% The layout controller injects the content feature into the corresponding layout through the Cross-Attention mechanism. This process is the conversion of a pixel-misaligned task to a pixel-aligned task at the feature level. The key design of the layout controller primarily focuses on enabling faster convergence and facilitating the injection of information for misaligned tasks. We present the results in different training steps in Fig. \ref{fig:ablation-conv}. Using CrossNorm allows the model to outline the target structure within 3k training steps and achieve convergence in texture and structure by around 5k steps. This enables the model to focus on the details in the remaining steps. In contrast, models using zero conv require about 20k steps to converge, while variants with standard conv take significantly longer to align the output distribution and half of the training steps are required to produce a reasonable image. 

% Moreover, not utilizing asynchronous RoPE leads to suboptimal performance of the Cross-Attention mechanism in misaligned tasks, particularly when the query and key inputs share the same shape. As shown in the fourth row of Fig. \ref{fig:ablation-conv}, the model fails to generate the target in the corresponding layout, and the details of the condition are also missing.
% Besides, the feature-level transform loss in formula \ref{l_transform}, facilitating the model to transfer conditions to the target layout, which can be observed from the fifth row of Fig. \ref{fig:ablation-conv}.

% \begin{figure*}
%     \centering
%   %  \includegraphics[width=0.9\linewidth]{figure/ablation_conv.pdf}
%     \caption{The ablation study of the Layout Controller. CrossNorm after the convolution output layer achieves faster convergence compared to that using zero conv or regular conv.}
%     \label{fig:ablation-conv}
% \end{figure*}



The Inter-Element Controller solves two following problems: unnaturalness when fusing multiple elements and occlusion when fusing multiple elements. 
To represent the layer ordering relationship between different elements, we assign a 1d order embedding to the sorted element feature, enabling the model to perceive the order relationships. As shown in Fig.~\ref{fig:ablation-fusion}, with the order embedding, the model misinterprets the order of the elements, leading to a blending problem similar to traditional ControlNet across different instances.
The same problem arises in the absence of the layer transformer, the model cannot distinguish which element should appear, thereby failing to execute the user's command of positioning a specific element in the foreground. Additionally, the image quality degrades due to the direct mixing of multiple elements.
Additionally, the absence of the spatial transformer may lead to artifacts or unnatural regions in the generated image, which can also be observed in Fig.~\ref{fig:ablation-fusion}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/ControlAblation.pdf}
    \caption{\textbf{The ablation study of Inter-Element Controller}. Order embedding provides the order information between elements, enabling the model to generate occlusion-aware outputs. The absence of the layer transformer and spatial transformer introduces artifacts.}
    \vspace{-1mm}
    \label{fig:ablation-fusion}
\end{figure}

% \subsection{Model Size}
% The trainable part of our model is the layout controller and the fusion module. We also include the training of Union-ControlNet as part of the overall training process for further refinement. We present the total parameters for the three stages in the Tab. \ref{tab:para}. Note that the roles of different modules are different, so we divided the whole training process into three stages, each focusing on different modules, with unrelated modules frozen in each stage. As the key contributions, the layout controller and layer fusion modules contain only about a fifth of the parameters of ControlNet, yet demonstrate excellent performance.


% \begin{table*}
%     \centering
%     \begin{tabular}{c|cccc}
%     \toprule
%     Module & Union-ControlNet & Layout Controller & Layer Fusion Module & Total\\ \midrule
%     Stage-1 & 1.37B & - & - & 1.37B \\
%     Stage-2 & - & 255.50M & - & 255.50M \\
%     Stage-3 & - & -& 200.20M & 200.20M  \\
%     \bottomrule
%     \end{tabular}
%     \caption{The trainable parameter in different train parameters in our model.}
%     \label{tab:para}
% \end{table*}


% \section{Discussion}
% %
% \paragraph{How does Condition Transformer work?}
% The condition reweighing transformer plays a crucial role in our decouple ControlNet by ensuring that different conditions are processed and fused effectively, particularly when dealing with tasks involving overlapping regions and occlusions. In particular, we apply the softmax function to handle these features in competition. To better illustrate the effectiveness of the fusion, we extract the softmax maps at each layer of the Condition Reweighing Transformer and apply the argmax to visualize which feature dominates. As shown in Fig. \ref{fig:softmax}, the weight map obtained via softmax effectively determines which feature dominates in different regions, especially in overlapping regions.


% \begin{figure}[H]
%     \centering
%     \includegraphics[width=\linewidth]{figure/fig_softmax_map1.pdf}
%     \caption{The softmax map at $64\times64$ and $32\times32$ in our condition reweighing transformer.}
%     \label{fig:softmax}
% \end{figure}

% \paragraph{Why is the layer-order way superior to the scale-way?}
% The original ControlNet \cite{zhang2023adding} enhances or weakens the features by simply adjusting their scale. This method, when applied to multiple instances, inevitably leads to unnatural artifacts or distortions. For example, in Fig. \ref{fig:scale_layer}, with the scale of the feature of ``a bear'' increase, the ControlNet ultimately achieves the goal of placing a ``bear'' in front of the ``bag'', at the expense of degraded image quality and artifacts. In contrast, our approach applies layer embedding to indicate the order relationship, achieving this goal in a more elegant and straightforward manner.

% \begin{figure*}
%     \centering
%     %\includegraphics[width=\linewidth]{figure/fig_scale_layer.pdf}
%     \caption{The comparison between the ControlNet and ours in fusing multiple elements. The ControlNet uses different condition scales to enhance the element ``bear'' on the image to place it in front of the ``bag''.}
%     \label{fig:scale_layer}
% \end{figure*}

\section{Conclusion}
ControlNet-based generation methods typically rely on global conditions to guide image generation. However, these methods face issues of flexibility and precision in multi-condition image generation tasks, often leading to misunderstandings of conditions and artifacts in the generated output. To address this issue, this paper proposed decoupling the global condition into independent elements, contents, and layouts, introducing DC-ControlNet. Specifically, DC-ControlNet includes an Intra-Element Controller to manage the content and layout conditions within individual elements, ensuring that different conditions within an element are independent and do not interfere with each other. On the other hand, the Inter-Element Controller in DC-ControlNet manages the interactions between different elements, accurately handling occlusions and interaction relationships. Additionally, we presented a new dataset designed for training and evaluating multi-condition generation models. Our experimental results demonstrate that DC-ControlNet outperforms existing methods, providing a more refined and flexible solution for controllable image generation.

{\small
\bibliographystyle{ieee_fullname}
\bibliography{arxiv_dc-ControlNet}
}

\end{document}