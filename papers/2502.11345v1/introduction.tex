\section{Introduction}

Documents are usually linked as a graph, e.g., papers cited in a citation graph; news articles linked in a hyperlink graph. Such graph usually exhibits a hierarchical structure:
%Documents are usually linked as a graph, e.g., papers cite others in a citation graph; news articles link to others in a hyperlink graph. Such graph usually shows a hierarchical structure: 
a central document links to others with an exponentially growing connectivity (Fig. \ref{fig:illustration}(a)). For example, an academic paper is extended by follow-up works, which are further cited by others; a news article reporting an event is traced by others with subsequent events. Hyperbolic Graph Neural Networks (HGNNs) \cite{hgcn} capture such \emph{graph hierarchy}. However, when modeling documents, we usually assume latent topics \cite{lda} and model contextualized semantics \cite{transformer}. HGNNs are not designed to capture latent topics or contextualized semantics, leading to inferior document embeddings. Text indicates how documents relate to each other in the latent topic space, and modeling it could capture semantic similarities. %and improve document representations.

Moreover, documents usually discuss topics of different specificity. For instance, some news report the overall Olympic Games, while others focus on specific sports; survey papers summarize a broad area, while regular papers deal with specific problems (Fig. \ref{fig:illustration}(b)). Though topic models \cite{wu2023effective,wu2024fastopic,wu2024survey} capture text semantics, most treat all documents equally and infer flat document representations. They fail to explore \emph{topic hierarchy} to differentiate semantic specificity of documents, resulting in semantic distortion. Hierarchical Topic Models (HTMs) \cite{ncrp} are the first attempt for topic hierarchy, but ignore graph hierarchy, e.g., citations and hyperlinks.
%Hierarchical Topic Models (HTMs) \cite{ncrp} are designed to explore such \emph{topic hierarchy} within documents. However, most of them mainly focus on text within documents, and ignore the graph adjacency across documents, e.g., citations and hyperlinks. %Graph structure reveals topic similarity, and modeling it could allow semantics to span across interlinked documents.

\begin{figure}%{R}{0.6\textwidth}
	\centering
	\includegraphics[width=1\linewidth]{figure/illustration}
	\vspace{-0.4cm}
	\caption{(a) Graph hierarchy, (b) topic hierarchy.}
	\label{fig:illustration}
	\vspace{-0.5cm}
\end{figure}

Graph hierarchy is denoted by edge connectivity across documents, and topic hierarchy appears within text content of documents. %Both hierarchies mutually benefit each other, where a central document on the graph usually contains general content, while surrounding documents tend to focus on specific topics. 
Though some works, e.g., HGTM \cite{hgtm}, consider both hierarchies, they model both of them \emph{separately}, i.e., \emph{first} encoding graph hierarchy, \emph{then} learning topic hierarchy. Such ``cascaded'' method can not well integrate both hierarchies into unified representations, because topic hierarchy is neglected when encoding graph hierarchy. Topic hierarchy could reveal semantic similarity of documents and benefit graph hierarchical learning. Consequently, two hierarchies can not mutually enhance each other, and the representations are biased towards one hierarchy and neglect the other.



% \textbf{Challenges.} \emph{First}, existing HGNNs \cite{hgcn,hgnn} do not model the rich semantics in text corpora, leading to inferior document representations. Text indicates how documents relate to each other in the latent topic space, and modeling it could capture semantic similarities and improve document representations. \emph{Second}, topic models \cite{lda} indeed consider textual semantics, but most do not model \emph{topic hierarchy} within documents. However, hierarchical topic structure indicates how documents are hierarchically organized, and modeling it could preserve different topic specificity and lessen semantic distortion. %While HTMs \cite{ncrp} model topic hierarchy, they still ignore graph hierarchy across documents. 
% \emph{Third}, though some works, e.g., HGTM \cite{hgtm}, consider both topic and graph hierarchies, they model both hierarchies \emph{separately}, i.e., first encoding graph hierarchy, then learning topic hierarchy. Such ``cascaded'' method can not well integrate both hierarchies into unified document representations. %Consequently, the representations may be biased towards one hierarchy and neglect the other.

\textbf{Approach.} %Motivated by these challenges, 
We propose GTFormer, a Hierarchical \textbf{G}raph \textbf{T}opic Modeling Trans\textbf{former}, integrating both topic hierarchy and graph hierarchy into a unified Transformer (Fig. \ref{fig:model}(a)). \emph{First}, to encode topic hierarchy, we design a \emph{topic tree} in latent semantic space and infer a \emph{hierarchical tree embedding} (Fig. \ref{fig:model}(b)). Documents with general content have high probability on root topic, while specific documents focus on leaf topics. \emph{Second}, to derive effective tree embedding, %that better captures topic hierarchy, 
we design topic tree in hyperbolic space, which is more suitable than Euclidean space for hierarchical structure \cite{hgcn}. We design \emph{Hyperbolic Doubly Recurrent Neural Network}, modeling ancestral (parent-to-children) and fraternal (sibling-to-sibling) tree structure to recurrently derive hyperbolic tree embedding. (Fig. \ref{fig:model}(b-c)). In contrast, previous HTMs mainly operate in Euclidean space, leading to topic distortion. \emph{Third}, to deeply unify both topic and graph hierarchies, we insert both tree and graph representations into each Transformer layer. The contextualized modeling allows one to propagate information to the other, and the output representation integrates both hierarchies. %from a nested architecture.

\textbf{Contributions.} \emph{First}, we propose GTFormer to jointly model topic and graph hierarchies into a unified Transformer. To explore topic hierarchy, we design a topic tree and infer hierarchical tree representation. \emph{Second}, to better preserve both topic and graph hierarchies, we design in hyperbolic space and we propose Hyperbolic Doubly Recurrent Network. \emph{Third}, both hierarchies are unified into each Transformer layer for contextualized modeling. %Code and datasets are submitted and will be released upon publication.