\section{Model Architecture}

\begin{figure*}[t]
	\centering
	\includegraphics[width=0.9\linewidth]{figure/model}
	\vspace{-0.2cm}
	\caption{Illustration of (a) our proposed GTFormer, (b) topic tree embedding, and (c) Hyperbolic Doubly Recurrent Neural Network. Hyperbolic operations are omitted for clarity. Best seen in color.}
	\label{fig:model}
	\vspace{-0.4cm}
\end{figure*}

% We first explain topic tree and how we use Hyperbolic Doubly Recurrent Neural Network to infer hierarchical tree representation. We then present hyperbolic graph representation and how both tree and graph representations are unified into Transformer architecture. %Finally, we discuss training objective. 
Fig. \ref{fig:model} shows the overall model with Hyperbolic Doubly Recurrent Network for hierarchical tree representation and a nested Transformer.

\subsection{Tree-structured Topic Hierarchy}
\label{sec:tree_structured_topic_hierarchy}

To preserve topic hierarchy, we construct a topic tree in latent semantic space, illustrated by Fig. \ref{fig:illustration}(b) and Fig. \ref{fig:model}(b). %We follow previous topic models \cite{lda,ncrp} and sample latent topics to generate document content. 
Documents with general concept present high probability on the root topic, while documents focusing on specific content sample topics near leaves. Thus, we calculate topic probability distribution of each document over topic tree, so as to differentiate semantic specificity of different documents and preserve topic hierarchy. Specifically, for each document $ i $, we calculate its path distribution $ \bm{\pi}_i $ and level distribution $ \bm{\delta}_i $, which are then combined to derive its topic distribution over topic tree $ \bm{\theta}_i\in\Delta^T $. $ \Delta^T $ is ($ T-1 $)-dimensional simplex. $ T $ is the number of topics on topic tree.

\textbf{Path distribution.} A path contains a sequence of topics, with increasing topic specificity from root to leaf. Different paths express different semantics. A document with certain themes present high probability on one path and low probabilities on others. In Fig. \ref{fig:illustration}(b), the left path shows reinforcement learning, while the right one is knowledge base. For path $ p $ with topics $ \{t_h\}_{h=1}^H $ where $ H $ is the path length, document $ i $'s path probability is
\begin{equation}
\label{eq:path_dist}
\resizebox{1\columnwidth}{!}{
$ \begin{split}
    \Pr(p=\{t_h\}_{h=1}^H)=&\Pr(t_H|t_{H-1})...\Pr(t_2|t_1)\Pr(t_1),%\\
    %\text{where}\quad&\Pr(t_1)=\Pr(t_{\text{root}})=1.
\end{split} $
}
\end{equation}
where $ \Pr(t_1)=\Pr(t_{\text{root}})=1 $. Topic $ t_h $ is one of the child topics of $ t_{h-1} $. Since a parent topic usually has more than one child, conditional probability $ \Pr(t_h|t_{h-1}) $ is the probability of selecting one of the child topics $ t_h $ given parent $ t_{h-1} $. We define it by tree-based stick-breaking construction:
% \begin{equation}
% \label{eq:path_dist_conditional_prob}
%     \Pr(t_h|t_{h-1})=\text{softmax}(\sigma(t_h,i))=\dfrac{\exp(\sigma(t_h,i))}{\sum_{t_h^\prime\in\text{Child}(t_{h-1})}\exp(\sigma(t_h^\prime,i))},\text{   where   }h=2,3,...,H.
% \end{equation}
\begin{equation}
\label{eq:path_dist_conditional_prob}
\resizebox{1\columnwidth}{!}{
$ \begin{split}
    \Pr(t_h|t_{h-1})=\sigma(t_h,i)&\prod_{t_h^\prime\in\text{LeftSibling}(t_h)}(1-\sigma(t_h^\prime,i)),%\\
    %\text{where}\quad &h=2,3,...,H.
\end{split} $
}
\end{equation}
where $ h=1,2,...,H $. $ \sigma(t_h,i)\in[0,1] $ is the similarity between document $ i $ and topic $ t_h $, to be explained shortly. $ \text{LeftSibling}(t_h) $ contains left siblings of topic $ t_h $. Suppose a parent topic has three children ($ t_A $, $ t_B $, and $ t_C $), the probability of selecting each child topic is respectively $ \sigma(t_A,i) $, $ \sigma(t_B,i)(1-\sigma(t_A,i)) $, and $ (1-\sigma(t_A,i))(1-\sigma(t_B,i)) $, which are summed to one \cite{tsntm}. A document $ i $ with higher similarity to one child topic $ t_h $ than its siblings tends to have higher probability of selecting $ t_h $ on the path. Starting from the root topic, we repeat this selection process until we reach leaf topic, forming a path with Eq. \ref{eq:path_dist} as path probability. Finally, we calculate Eq. \ref{eq:path_dist} for every path and obtain path distribution $ \bm{\pi}_i=[\Pr(p_1),\Pr(p_2),...] $, which is unique to document $ i $, since $ \sigma(t_h,i) $ is document-specific.

\textbf{Hyperbolic doubly recurrent neural network.} We now explain similarity $ \sigma(t_h,i) $, which is calculated by topic $ t_h $'s and document $ i $'s embeddings, using Hyperbolic Doubly Recurrent Neural Network (HypDRNN). HypDRNN consists of two Hyperbolic Recurrent Neural Networks (HypRNNs) that respectively model the ancestral (parent-to-children) and fraternal (sibling-to-sibling) tree structure to preserve topic hierarchy. Specifically, a topic $ t $ has parent and siblings, thus we first use two HypRNNs to respectively calculate its ancestral hidden state $ \textbf{z}_{t,p} $ and fraternal hidden state $ \textbf{z}_{t,s} $, which are then combined to obtain $ t $'s hyperbolic hidden state $ \textbf{z}_t $. See Fig. \ref{fig:model}(b-c).

We first present topic $ t $'s ancestral (parent-to-children) hidden state. We have feature projection,
\begin{equation}
\label{eq:linear_transformation}
    \textbf{z}_{t,p}^\prime=\exp_{\textbf{0}}^K(\textbf{W}_p\log_{\textbf{0}}^K(\textbf{z}_{\text{Parent}(t)}))\in\Bbb H^{n,K}.
\end{equation}
$ \textbf{z}_{\text{Parent}(t)}\in\Bbb H^{n,K} $ is hyperbolic hidden state of $ t $'s parent. $ \textbf{W}_p\in\Bbb R^{(n+1)\times(n+1)} $ is Euclidean parameter. We project $ \textbf{z}_{\text{Parent}(t)} $ to tangent space for linear projection, whose result is mapped back to hyperbolic space. We use the origin $ \textbf{0}=[\sqrt{K},0,...,0]\in\Bbb H^{n,K} $ as projection reference point.

For bias addition, we use parallel transport. We initialize a Euclidean bias $ \textbf{b}_p\in\Bbb R^n $ and concatenate it with 0, i.e., $ \textbf{b}_p^\prime=[0||\textbf{b}_p]\in\mathcal{T}_{\textbf{0}}\Bbb H^{n,K} $. $ \textbf{b}_p^\prime $ is on origin's tangent space, due to $ \langle\textbf{b}_p^\prime,\textbf{0}\rangle_{\mathcal{L}}=0 $. We then transport $ \textbf{b}_p^\prime $ to the tangent space of $ \textbf{z}_{t,p}^\prime $, whose result is mapped back to hyperbolic space.
\begin{equation}
\label{eq:bias_addition}
    \textbf{z}_{t,p}=f_{\text{tanh}}^K\Big(\exp_{\textbf{z}_{t,p}^\prime}^K(\text{PT}^K_{\textbf{0}\rightarrow\textbf{z}_{t,p}^\prime}(\textbf{b}_p^\prime))\Big)\in\Bbb H^{n,K}.
\end{equation}
Here we use hyperbolic tanh activation $ f_{\text{tanh}}^K(\textbf{x})=\exp_{\textbf{0}}^K(\text{tanh}(\log_{\textbf{0}}^K(\textbf{x}))) $ and $ \text{tanh}(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}} $. Summarizing Eqs. \ref{eq:linear_transformation}--\ref{eq:bias_addition}, we have ancestral hidden state $ \textbf{z}_{t,p}=f_{\text{HypRNN}}(\textbf{z}_{\text{Parent}(t)};\textbf{W}_p,\textbf{b}_p) $. Similarly, raternal hidden state is $ \textbf{z}_{t,s}=f_{\text{HypRNN}}(\textbf{z}_{\text{LeftSibling}(t)};\textbf{W}_s,\textbf{b}_s) $. Finally, we obtain topic $ t $'s hyperbolic hidden state $ \textbf{z}_t $ where $ \textbf{W}\in\Bbb R^{(n+1)\times(n+1)} $ is Euclidean parameter.
\begin{equation}
\label{eq:hyperbolic_hidden_state}
\resizebox{1\columnwidth}{!}{
    $ \textbf{z}_t=f^K_{\text{tanh}}\Big(\exp_{\textbf{0}}^K\Big(\textbf{W}\big(\log_{\textbf{0}}^K(\textbf{z}_{t,p})+\log_{\textbf{0}}^K(\textbf{z}_{t,s})\big)\Big)\Big)\in\Bbb H^{n,K}. $
    }
\end{equation}
Integrating Eqs. \ref{eq:linear_transformation}--\ref{eq:hyperbolic_hidden_state}, we have Hyperbolic Doubly RNN, $ \textbf{z}_t=f_{\text{HypDRNN}}(\textbf{z}_{t,p},\textbf{z}_{t,s}) $, which uses two Hyperbolic RNNs to model the information flow of ancestral and fraternal tree structure, hence the name of this module. See Fig. \ref{fig:model}(c). Hidden state $ \textbf{z}_t $ is also topic $ t $'s hyperbolic topic embedding.

Finally, the similarity $ \sigma(t,i) $ is calculated by topic $ t $'s and document $ i $'s hyperbolic embeddings, using Fermi-Dirac function $ \sigma(t,i)=[1+e^{d_{\mathcal{L}}^K(\textbf{z}_t,\textbf{d}_i^{(l)})^2}]^{-1} $. %$ d_{\mathcal{L}}^K(\cdot,\cdot) $ is hyperbolic distance at Eq. \ref{eq:log_map}, and 
$ \textbf{d}_i^{(l)} $ is document $ i $'s hyperbolic embedding, to be explained shortly.
% \begin{equation}
%     \sigma(t,i)=\dfrac{1}{1+e^{d_{\mathcal{L}}^K(\textbf{z}_t,\textbf{d}_i)^2}}.
% \end{equation}
%Two proposed hyperbolic RNNs model the information flow along the tree structure, thus the design of our proposed HypDRNN well preserves topic hierarchy.

\textbf{Level distribution.} Having calculated path distribution, we discuss level distribution. A path contains a sequence of topics, each representing a level. Different levels represent different topic specificity. Root topic denotes the most general concept, while leaf topic focuses on specific sub-concept. %After sampling a path based on path distribution, we now sample a level. 
%Level distribution is similarly defined by tree-based stick-breaking process. 
Specifically, for a document $ i $, its probability at level $ h $ is another tree-based stick-breaking process,
\begin{equation}
    \delta_h=\sigma(h,i)\prod_{h^\prime=1}^{h-1}(1-\sigma(h^\prime,i)),%\text{   where   }h=1,2,...,H.
\end{equation}
where $ h=1,2,...,H $. $ \sigma(h,i)\in[0,1] $ is the similarity between level $ h $ and document $ i $. A general document presents high probability at root $ h=1 $, while a document with specific content falls on bottom $ h=H $. We calculate level similarity $ \sigma(h,i) $ by a separate Hyperbolic RNN, %with different parameters,
\begin{equation}
\begin{split}
    \textbf{z}_h=&f_{\text{HypRNN}}(\textbf{z}_{h-1};\textbf{W}_H,\textbf{b}_H)\in\Bbb H^{n,K},\\
    &\sigma(h,i)=[1+e^{d_{\mathcal{L}}^K(\textbf{z}_h,\textbf{d}_i)^2}]^{-1}.
\end{split}
\end{equation}
$ \textbf{z}_h\in\Bbb H^{n,K} $ is hyperbolic hidden state of level $ h $. $ f_{\text{HypRNN}}(\cdot) $ is a separate hyperbolic RNN with hyperbolic tanh activation (Eqs. \ref{eq:linear_transformation}--\ref{eq:bias_addition}). Level similarity $ \sigma(h,i) $ is Fermi-Dirac function. For document $ i $, we evaluate its probability of each level and obtain level distribution $ \bm{\delta}_i=[\delta_1,\delta_2,...,\delta_H] $.

Having obtained both path and level distributions, we now combine them to derive document $ i $'s topic distribution over topic tree. Specifically, given one path $ p=\{t_h\}_{h=1}^H $ and one level $ h $, we already narrow down to one topic $ t_h $ with probability $ \pi_i(p)\times\delta_i(h) $. Since there are multiple paths going through topic $ t_h $, the overall topic probability is $ \theta_{t_h}=\delta_i(h)\sum_{t_h\in p'}\pi_i(p^\prime) $, i.e., the summation of all paths having topic $ t_h $ at level $ h $. We repeat this process for every topic and obtain document $ i $'s topic distribution $ \bm{\theta}_i=[\theta_{i,t_1},\theta_{i,t_2},...,\theta_{i,T}] $ where $ T $ is the total number of topics on the tree. Documents with general content present high probability on the root topic $ \theta_{t_1} $, while documents focusing on specific content concentrate on leaf topics. Thus, this topic distribution $ \bm{\theta}_i $ preserves topic hierarchy.  % emphasize theta_i is document-specific

\textbf{Hyperbolic tree representation.} We use document $ i $'s hierarchical topic distribution $ \bm{\theta}_i $ and topic embeddings $ \{\textbf{z}_t\}_{t=1}^T $ to obtain \emph{hierarchical tree embedding} $ \textbf{e}_i\in\Bbb H^{n,K} $.
\begin{equation}
\label{eq:tree_embedding}
    \textbf{e}_i=\exp_{\textbf{0}}^K\Big(\sum_{t=1}^T \theta_{i,t}\log_{\textbf{0}}^K(\textbf{z}_t)\Big)\in\Bbb H^{n,K}.
\end{equation}
$ \textbf{z}_t\in\Bbb H^{n,K} $ is $ t $'s topic embedding. %output from Hyperbolic Doubly RNN. 
Tree embedding $ \textbf{e}_i $ preserves topic hierarchy and will later be inserted into Transformer. %for hierarchical topic encoding.

\subsection{Topic and Graph Joint Modeling}
\label{sec:tranformer}

%Having introduced hierarchical tree embedding, 
We show hierarchical graph embedding and integrate both hierarchies into Transformer (Fig. \ref{fig:model}(a)).

\textbf{Hyperbolic graph representation.} We use Hyperbolic Graph Neural Network \cite{hgcn} to capture graph hierarchy. %which has been shown to be better than Euclidean GNN to preserve graph hierarchy. 
For document $ \textbf{d}_i^{(l)} $, we first linearly transform it by %Eq. \ref{eq:hgnn_linear_transformation} where Euclidean bias $ \textbf{b}_g \in\Bbb R^{n} $ is concatenated with 0, $ \textbf{b}_g^\prime=[0||\textbf{b}_g]\in\mathcal{T}_{\textbf{0}}\Bbb H^{n,K} $.
\begin{equation}
\label{eq:hgnn_linear_transformation}
\begin{split}
    \tilde{\textbf{d}}_i^{(l)\prime}=\exp_{\textbf{0}}^K(\textbf{W}_g\log_{\textbf{0}}^K(\textbf{d}_i^{(l)}))\in\Bbb H^{n,K}.%\\
    % \tilde{\textbf{d}}_i^{(l)}=\exp_{\tilde{\textbf{d}}_i^{(l)\prime}}^K(\text{PT}^K_{\textbf{0}\rightarrow\tilde{\textbf{d}}_i^{(l)\prime}}(\textbf{b}_g^\prime))\in\Bbb H^{n,K}.
\end{split}
\end{equation}
We then evaluate its attention w.r.t. its neighbors and aggregate neighbor embeddings by Eq. \ref{eq:hgnn_neighbor_aggregation}. %$ \text{sigmoid}(x)=\frac{1}{1+e^{-x}} $ and 
$ \text{softmax}(x)=\frac{e^x}{\sum_{x^\prime}e^{x^\prime}} $. Here $ \textbf{b}_{\text{att}}\in\Bbb R^{2(n+1)} $ is Euclidean parameter.
% \begin{equation}
% \label{eq:hgnn_neighbor_aggregation}
%     \begin{split}
%     \alpha_{ij}=\text{softmax}&\Big(\text{sigmoid}\big(\textbf{b}_{\text{att}}^\top[\log_{\textbf{0}}^K(\tilde{\textbf{d}}_i^{(l)})||\log_{\textbf{0}}^K(\tilde{\textbf{d}}_j^{(l)})]\big)\Big),\\ 
%     \textbf{g}_i^{(l)}=\exp_{\textbf{0}}^K\Big(\dfrac{1}{2}\big(&\log_{\textbf{0}}^K(\tilde{\textbf{d}}_i^{(l)})+\sum_{j\in\mathcal{N}(i)}\alpha_{ij}\log_{\textbf{0}}^K(\tilde{\textbf{d}}_j^{(l)})\big)\Big)\in\Bbb H^{n,K}.
% \end{split}
% \end{equation}
\begin{equation}
\label{eq:hgnn_neighbor_aggregation}
\resizebox{0.87\columnwidth}{!}{
$ \begin{split}
    &\alpha_{ij}=\text{softmax}\Big(\textbf{b}_{\text{att}}^\top[\log_{\textbf{0}}^K(\tilde{\textbf{d}}_i^{(l)}) \parallel \log_{\textbf{0}}^K(\tilde{\textbf{d}}_j^{(l)})]\Big),\\
    \textbf{g}_i^{(l)}&=\exp_{\textbf{0}}^K\Big(\dfrac{1}{2}\big(\log_{\textbf{0}}^K(\tilde{\textbf{d}}_i^{(l)})+\sum_{j \in \mathcal{N}(i)}\alpha_{ij}\log_{\textbf{0}}^K(\tilde{\textbf{d}}_j^{(l)})\big)\Big).%\in\Bbb H^{n,K}.
\end{split} $
}
\end{equation}
% \begin{equation}
% \label{eq:hgnn_module}
%     \textbf{g}_i^{(l)}=f_{\text{HGNN}}(\textbf{d}_i^{(l)},\textbf{d}_j^{(l)}|j\in\mathcal{N}(i))\in\Bbb H^{n,K}.
% \end{equation}
% Here $ \textbf{d}_i^{(l)}=\textbf{H}_{i,\text{CLS}}^{(l)} $ is document $ i $'s hyperbolic embedding, corresponding to the [CLS] token output from the $ l $-th Transformer layer \cite{transformer}. $ \textbf{d}_j^{(l)}=\textbf{H}_{j,\text{CLS}}^{(l)} $ is similarly defined for neighbor $ j\in\mathcal{N}(i) $. %Since the HGNN module is relatively standard, below we briefly introduce it as a self-contained paper, and defer readers to Appendix \ref{sec:hgnn} for detailed explanation.
After HGNN, we obtain $ i $'s aggregated embedding $ \textbf{g}_i^{(l)}$ , preserving its hierarchical graph structure.

\textbf{Topic- and graph-nested Transformer encoding.} We have obtained both hierarchical tree embedding $ \textbf{e}_i^{(l)} $ at Eq. \ref{eq:tree_embedding} and hierarchical graph embedding $ \textbf{g}_i^{(l)} $ at Eq. \ref{eq:hgnn_neighbor_aggregation}. Both are calculated by document $ i $'s embedding $ \textbf{d}_i^{(l)} $ from the $ l $-th Transformer layer. Now we aim to insert them into the $ (l+1) $-th Transformer layer for hierarchical encoding. Specifically, we let $ \textbf{H}_i^{(l)}=[\textbf{H}_{i,\text{CLS}}^{(l)},\textbf{H}_{i,w_1}^{(l)},\textbf{H}_{i,w_2}^{(l)},...] $ denote the output from the $ l $-th Transformer layer. We concatenate $ \textbf{e}_i^{(l)} $ and $ \textbf{g}_i^{(l)} $ with $ \textbf{H}_i^{(l)} $, i.e., $ \tilde{\textbf{H}}_i^{(l)}=[\textbf{e}_i^{(l)}||\textbf{g}_i^{(l)}||\textbf{H}_i^{(l)}] $. After concatenation, $ \tilde{\textbf{H}}_i^{(l)} $ contains information of both hierarchical topic tree and hierarchical graph structure. To allow all the tokens in $ \tilde{\textbf{H}}_i^{(l)} $ to fully capture both hierarchies, we input it to the next Transformer layer for contextualized modeling.
\begin{equation}
    \textbf{H}_i^{(l+1)}=f_{\text{HypTRM}}(\tilde{\textbf{H}}_i^{(l)}).
\end{equation}
Here $ f_{\text{HypTRM}}(\cdot) $ is a Transformer layer in hyperbolic space. %in order to preserve topic and graph hierarchies and align with the hyperbolic input embedding $ \tilde{\textbf{H}}_i^{(l)} $. 
The building blocks of $ f_{\text{HypTRM}}(\cdot) $ are mostly the same as existing Transformer \cite{transformer}, except that the embeddings are hyperbolic and need to be projected between hyperbolic and tangent spaces. To make this paper self-contained, below we briefly present $ f_{\text{HypTRM}}(\cdot) $. We input $ \tilde{\textbf{H}}_i^{(l)} $ to the $ (l+1) $-th Transformer layer, where $ f_{\text{LN}}(\cdot) $ is layer normalization, and $ f_{\text{MLP}}(\cdot) $ is multi-layer perceptron \cite{transformer}.
% \begin{equation}
% \begin{split}
%     \tilde{\textbf{H}}_i^{(l)\prime}=f_{\text{LN}}&(\log_{\textbf{0}}^K(\textbf{H}_i^{(l)})+\log_{\textbf{0}}^K(f_{\text{AsymMHA}}(\tilde{\textbf{H}}_i^{(l)}))),\\ \textbf{H}_i^{(l+1)}&=\exp_{\textbf{0}}^K(f_{\text{LN}}(\tilde{\textbf{H}}_i^{(l)\prime}+f_{\text{MLP}}(\tilde{\textbf{H}}_i^{(l)\prime}))).
% \end{split}
% \end{equation}
\begin{equation}
\resizebox{0.88\columnwidth}{!}{
$ \begin{split}
    \tilde{\textbf{H}}_i^{(l)\prime}&=f_{\text{LN}}\Big(\log_{\textbf{0}}^K(\textbf{H}_i^{(l)})+\log_{\textbf{0}}^K(f_{\text{AsymMHA}}(\tilde{\textbf{H}}_i^{(l)}))\Big),\\
    &\textbf{H}_i^{(l+1)}=\exp_{\textbf{0}}^K\Big(f_{\text{LN}}(\tilde{\textbf{H}}_i^{(l)\prime}+f_{\text{MLP}}(\tilde{\textbf{H}}_i^{(l)\prime}))\Big).
\end{split} $
}
\end{equation}
We follow \cite{graphformers} and implement an \emph{asymmetric} multi-head attention $ f_{\text{AsymMHA}}(\cdot) $, where $ \textbf{K} $ and $ \textbf{V} $ are augmented with hierarchical topic and graph embeddings, while $ \textbf{Q} $ is not.
% \begin{equation}
% \begin{split}
%     f_{\text{head}}^m(\tilde{\textbf{H}}_i^{(l)})=&f_{\text{softmax}}(\dfrac{\log_{\textbf{0}}^K(\textbf{Q})\log_{\textbf{0}}^K(\textbf{K}^\top)}{\sqrt{n+1}})\log_{\textbf{0}}^K(\textbf{V}),\\ f_{\text{AsymMHA}}&(\tilde{\textbf{H}}_i^{(l)})=\exp_{\textbf{0}}^K\Big(\sideset{}{}\|_{m=1}^{M} f_{\text{head}}^{m}(\tilde{\textbf{H}}_i^{(l)})\Big),\\
%     \textbf{Q}=\exp_{\textbf{0}}^K(\log_{\textbf{0}}^K(\textbf{H}_i^{(l)})\textbf{W}_Q^{(l)}),\;\  \textbf{K}=&\exp_{\textbf{0}}^K(\log_{\textbf{0}}^K(\tilde{\textbf{H}}_i^{(l)})\textbf{W}_K^{(l)}),\;\  \textbf{V}=\exp_{\textbf{0}}^K(\log_{\textbf{0}}^K(\tilde{\textbf{H}}_i^{(l)})\textbf{W}_V^{(l)}).
% \end{split}
% \end{equation}
% \begin{equation}
% \begin{split}
%     f_{\text{head}}^m(\tilde{\textbf{H}}_i^{(l)})=\text{softmax}&\Big(\dfrac{\log_{\textbf{0}}^K(\textbf{Q})\log_{\textbf{0}}^K(\textbf{K}^\top)}{\sqrt{n+1}}\Big)\log_{\textbf{0}}^K(\textbf{V}),\\
%     f_{\text{AsymMHA}}(\tilde{\textbf{H}}_i^{(l)})=&\exp_{\textbf{0}}^K\Big(\sideset{}{}\|_{m=1}^{M} f_{\text{head}}^{m}(\tilde{\textbf{H}}_i^{(l)})\Big),\\
%     \textbf{Q}=\exp_{\textbf{0}}^K(\log_{\textbf{0}}^K(\textbf{H}_i^{(l)})\textbf{W}&_Q^{(l)}),\;\  \textbf{K}=\exp_{\textbf{0}}^K(\log_{\textbf{0}}^K(\tilde{\textbf{H}}_i^{(l)})\textbf{W}_K^{(l)}),\;\  \\
%     \textbf{V}=\exp_{\textbf{0}}^K&(\log_{\textbf{0}}^K(\tilde{\textbf{H}}_i^{(l)})\textbf{W}_V^{(l)}).
% \end{split}
% \end{equation}
\begin{equation}
\resizebox{1\columnwidth}{!}{
$ \begin{split}
    f_{\text{AsyMHA}}(\tilde{\textbf{H}}_i^{(l)})=\text{softmax}&\Big(\dfrac{\log_{\textbf{0}}^K(\textbf{Q})\log_{\textbf{0}}^K(\textbf{K}^\top)}{\sqrt{n+1}}\Big)\log_{\textbf{0}}^K(\textbf{V}),\\
    % f_{\text{AsymMHA}}(\tilde{\textbf{H}}_i^{(l)})=&\exp_{\textbf{0}}^K\Big(\sideset{}{}\|_{m=1}^{M} f_{\text{head}}^{m}(\tilde{\textbf{H}}_i^{(l)})\Big),\\
    \textbf{Q}=\exp_{\textbf{0}}^K(\log_{\textbf{0}}^K(\textbf{H}_i^{(l)})\textbf{W}&_Q^{(l)}),\;\  \textbf{K}=\exp_{\textbf{0}}^K(\log_{\textbf{0}}^K(\tilde{\textbf{H}}_i^{(l)})\textbf{W}_K^{(l)}),\;\  \\
    \textbf{V}=\exp_{\textbf{0}}^K&(\log_{\textbf{0}}^K(\tilde{\textbf{H}}_i^{(l)})\textbf{W}_V^{(l)}).
\end{split} $
}
\end{equation}
%Due to contextualized modeling, 
The output from the $ (l+1) $-th layer $ \textbf{H}_i^{(l+1)} $ preserves both hierarchies. %Its [CLS] token is document $ i $'s hyperbolic embedding. %of the $ (l+1) $-th layer. 
We use [CLS] token to calculate hierarchical tree and graph embeddings for the current layer, which are concatenated and passed to the next layer. We repeat this \emph{layer-wise encoding} for $ L $ layers and obtain $ \textbf{d}_i=\textbf{H}_{i,\text{CLS}}^{(L)}\in\Bbb H^{n,K} $, document $ i $'s hierarchical embedding. %with both hierarchies.

\subsection{Training Objective}
\label{sec:training_objective}

\textbf{Decoding.} Since we preserve both topic and graph hierarchies, we present two decodings. For hierarchical topic decoding, we first use document $ i $'s final-layer embedding $ \textbf{d}_i $ to calculate its topic distribution $ \bm{\theta}_i $ by its path and level distributions. As in LDA, we then construct topic-word distribution for each topic $ t $ by %$ \bm{\beta}_t=\text{softmax}(\frac{\textbf{U}\log_{\textbf{0}}^K(\textbf{z}_t)}{\tau^{1/h}})\in\Delta^{|\mathcal{V}|} $. 
$ \bm{\beta}_t=\text{softmax}(\textbf{U}\log_{\textbf{0}}^K(\textbf{z}_t))\in\Delta^{|\mathcal{V}|} $. Here $ \textbf{U}\in\Bbb R^{|\mathcal{V}|\times(n+1)} $ is a matrix of word embeddings, $ \mathcal{V} $ is vocabulary. %and $ \tau^{1/h} $ is a hyperparameter controlling the sparsity of $ \bm{\beta}_t $. 
The overall topic-word distribution is $ \bm{\beta}=[\bm{\beta}_1,...,\bm{\beta}_T]\in\Bbb R^{|\mathcal{V}|\times T} $. The reconstructed document is $ \hat{\textbf{d}}_i=\bm{\beta}\bm{\theta}_i\in\Delta^{|\mathcal{V}|} $. The topic modeling objective is $ \mathcal{L}_{\text{Topic}} $ where $ d_{i,w} $ is the word count of word $ w $ in document $ i $. For graph decoding, we use contrastive loss in Eq. \ref{eq:decoding}. %with other documents in the same minibatch as negative samples.
\begin{equation}
\label{eq:decoding}
\resizebox{1\columnwidth}{!}{
$ \begin{split}
    \mathcal{L}&_{\text{Topic}}=-\sum_{w\in\mathcal{V}}d_{i,w}\log \hat{d}_{i,w},\\
    \mathcal{L}_{\text{Graph}}=-\log& \dfrac{e^{-d_{\mathcal{L}}^K(\textbf{d}_i,\textbf{d}_j)^2}}{e^{-d_{\mathcal{L}}^K(\textbf{d}_i,\textbf{d}_j)^2}+\sum_{j^\prime\in B}e^{-d_{\mathcal{L}}^K(\textbf{d}_i,\textbf{d}_{j^\prime})^2}}.
\end{split} $
}
\end{equation}
The overall loss is $ \mathcal{L}=\mathcal{L}_{\text{Graph}}+\lambda_{\text{Topic}}\mathcal{L}_{\text{Topic}} $. Hyperparameter $ \lambda_{\text{Topic}} $ controls the weight of $ \mathcal{L}_{\text{Topic}} $.

\textbf{Supervised version.} The above model is \emph{unsupervised}. If we also observe document labels, %such as the categories of news articles or academic papers, 
we design the \emph{supervised} version by adding a classifier $ \hat{\textbf{y}}_i= \text{softmax}(f_{\text{MLP}}(\log_{\textbf{0}}^K(\textbf{d}_i))) $. The supervised loss is $ \mathcal{L}_{\text{Sup}}=-\sum_{y^\prime}y_{i}^\prime\log\hat{y}_{i}^\prime $. The overall loss of the supervised version becomes $ \mathcal{L}=\mathcal{L}_{\text{Graph}}+\lambda_{\text{Topic}}\mathcal{L}_{\text{Topic}}+\lambda_{\text{Sup}}\mathcal{L}_{\text{Sup}} $.

\textbf{Continuously updating topic tree.} Different corpora contain documents of different topic hierarchy. To match the unique topic hierarchy of the given corpus, we update topic tree during training. For each topic $ t $, we calculate the proportion of words belonging to it by $ s_t=\frac{\sum_{d_i\in\mathcal{D}_{\text{train}}}|d_i|\theta_{i,t}}{\sum_{d_i\in\mathcal{D}_{\text{train}}}|d_i|} $ where $ |d_i| $ is the number of words in document $ i $. For a non-leaf topic whose $ s_t $ is greater than the adding threshold $ s_{\text{add}} $, we add a child topic, since it contains overly much semantics, and we split it into sub-concepts. Reversely, if the summation of topic $ t $ and its descendants $ \sum_{t^\prime\in\text{Des}(t)}s_{t^\prime} $ is smaller than the pruning threshold $ s_{\text{prune}} $, we remove topic $ t $ and its descendants, since they contain overly small proportion of semantics, and it is not necessary to keep them. Empirically, we set $ s_{\text{add}}=s_{\text{prune}}=0.05 $.  We summarize our model with Algorithm \ref{algo:training_algorithm}.