\section{Experiments}

\begin{table}%{R}{0.4\textwidth}
	\centering
	\caption{Dataset statistics.}
	\vspace{-0.2cm}
	\resizebox{0.75\columnwidth}{!}{
		\begin{tabular}{cccc}
			\toprule
			Name  & \#Documents & \#Links & \#Labels \\
			\midrule
                DS & 1,703 & 3,234 & 9 \\ 
			ML & 3,087 & 8,573 & 7 \\ 
			PL & 2,597 & 7,754 & 9 \\
                DBLP & 239,026 & 1,071,208 & N.A. \\
			COVID & 1,500 & 5,706 & 5 \\
			Web & 445,657 & 565,502 & N.A. \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.5cm}
	\label{table:dataset_statistics}
\end{table}
\begin{table*}[!h]
	\centering
	\caption{Unsupervised document classification with Micro F1 and Macro F1 scores (in percentage).}
	\vspace{-0.3cm}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|rrrr|rrrr}
			\toprule
			\multirow{2}{*}{Category} & \multirow{2}{*}{Model} & \multicolumn{4}{c|}{Micro F1} & \multicolumn{4}{c}{Macro F1} \\
			\cline{3-10}
			{} & {} & DS & ML & PL & COVID & DS & ML & PL & COVID \\
			\hline
			\multirow{4}{*}{\shortstack{\textbf{Flat topic models}}} & ProdLDA & 51.4$ \pm $1.1 & 65.3$ \pm $1.0 & 49.8$ \pm $2.5 & 72.7$ \pm $1.7 & 40.1$ \pm $4.3 & 67.4$ \pm $1.4 & 48.4$ \pm $1.8 & 73.3$ \pm $1.7 \\
			{} & ETM & 42.2$ \pm $2.4 & 46.2$ \pm $1.2 & 39.8$ \pm $0.8 & 67.2$ \pm $1.8 & 31.1$ \pm $3.4 & 42.8$ \pm $1.6 & 32.1$ \pm $1.2 & 67.4$ \pm $1.7 \\
			{} & GATON & 39.7$ \pm $1.3 & 61.1$ \pm $0.7 & 51.2$ \pm $1.0 & 70.5$ \pm $1.2 & 29.9$ \pm $1.8 & 58.2$ \pm $0.8 & 44.6$ \pm $1.5 & 70.7$ \pm $1.1 \\
            {} & GNTM & 37.0$ \pm $1.3 & 60.2$ \pm $3.1 & 50.0$ \pm $1.9 & 73.2$ \pm $2.2 & 26.4$ \pm $2.1 & 56.4$ \pm $3.4 & 43.0$ \pm $1.6 & 73.2$ \pm $2.1 \\
			\hline
			\multirow{5}{*}{\shortstack{\textbf{Hierarchical topic models}}} & nCRP & 27.5$ \pm $3.3 & 28.6$ \pm $1.7 & 25.2$ \pm $2.5 & 41.7$ \pm $4.4 & 16.7$ \pm $2.2 & 21.6$ \pm $1.8 & 16.7$ \pm $2.5 & 41.5$ \pm $4.5 \\
			{} & TSNTM & 39.5$ \pm $3.7 & 62.0$ \pm $1.0 & 47.8$ \pm $1.3 & 74.1$ \pm $3.2 & 28.9$ \pm $4.1 & 58.8$ \pm $1.0 & 38.9$ \pm $1.6 & 73.9$ \pm $3.2 \\
			{} & HTV & 29.7$ \pm $2.1 & 37.3$ \pm $4.2 & 29.2$ \pm $5.4 & 61.6$ \pm $4.3 & 13.7$ \pm $2.2 & 32.0$ \pm $4.1 & 21.3$ \pm $4.7 & 61.9$ \pm $4.7 \\
            {} & HyperMiner & 41.3$ \pm $1.3 & 53.4$ \pm $0.1 & 45.3$ \pm $0.4 & 50.2$ \pm $4.5 & 30.1$ \pm $1.9 & 43.9$ \pm $2.5 & 38.6$ \pm $1.4 & 48.6$ \pm $2.9 \\
            {} & TraCo & 46.7$ \pm $2.1 & 64.4$ \pm $4.2 & 47.5$ \pm $2.0 & 60.4$ \pm $3.8 & 36.7$ \pm $1.9 & 61.2$ \pm $5.3 & 38.6$ \pm $2.1 & 61.3$ \pm $3.4 \\
			\hline
			\multirow{4}{*}{\shortstack{\textbf{Topic models for document graph}}} & AdjEnc & 58.8$ \pm $1.2 & 72.5$ \pm $1.1 & 61.2$ \pm $1.0 & 74.8$ \pm $2.4 & 54.6$ \pm $1.5 & 68.3$ \pm $1.0 & 49.3$ \pm $0.6 & 69.8$ \pm $2.3 \\
			{} & LANTM & 56.8$ \pm $2.4 & 72.2$ \pm $0.7 & 61.7$ \pm $1.1 & 80.3$ \pm $1.7 & 54.7$ \pm $0.8 & 68.6$ \pm $1.0 & 54.6$ \pm $1.2 & 80.2$ \pm $1.7 \\
			{} & GTNN & 52.9$ \pm $1.4 & 68.1$ \pm $0.7 & 58.8$ \pm $1.2 & 70.9$ \pm $1.0 & 42.8$ \pm $3.3 & 64.7$ \pm $1.3 & 52.4$ \pm $1.3 & 70.8$ \pm $0.9 \\
            {} & HGTM & 65.6$ \pm $1.5 & 82.1$ \pm $0.9 & 68.3$ \pm $0.5 & 81.6$ \pm $0.5 & 62.3$ \pm $1.6 & 80.1$ \pm $0.9 & 63.6$ \pm $0.7 & 81.2$ \pm $0.6 \\
            \hline
			\multirow{5}{*}{\shortstack{\textbf{Text-attributed graph models}}} & BERT+HGCN & 62.5$ \pm $1.4 & 78.9$ \pm $0.9 & 62.7$ \pm $1.5 & 75.5$ \pm $1.3 & 58.6$ \pm $1.3 & 77.0$ \pm $1.1 & 57.0$ \pm $1.3 & 75.1$ \pm $1.6 \\
            {} & LLaMA2+HGCN & 67.3$ \pm $1.6 & 83.0$ \pm $0.5 & 65.8$ \pm $1.1 & 81.3$ \pm $0.3 & 64.4$ \pm $1.2 & 80.9$ \pm $0.6 & 59.4$ \pm $1.9 & 81.0$ \pm $0.6 \\
            {} & Specter & 63.1$ \pm $0.1 & 77.2$ \pm $0.8 & 63.7$ \pm $1.6 & 80.3$ \pm $1.5 & 59.5$ \pm $1.2 & 75.2$ \pm $0.8 & 59.3$ \pm $1.9 & 80.0$ \pm $1.7 \\
            {} & LinkBERT & 47.5$ \pm $2.2 & 61.5$ \pm $3.2 & 47.9$ \pm $0.4 & 72.0$ \pm $1.2 & 40.0$ \pm $2.7 & 59.9$ \pm $3.8 & 39.6$ \pm $0.1 & 76.1$ \pm $0.6 \\
            {} & Patton & 65.1$ \pm $1.8 & 82.4$ \pm $1.0 & 70.0$ \pm $1.6 & 78.6$ \pm $1.5 & 60.0$ \pm $2.4 & 80.4$ \pm $1.1 & 65.5$ \pm $1.9 & 77.9$ \pm $1.7 \\
            %{} & TAPE &  \\
			\hline
			\multirow{1}{*}{\shortstack{\textbf{Hyperbolic graph transformer}}} & FPS-T & 60.9$ \pm $2.7 & 74.7$ \pm $2.6 & 67.2$ \pm $3.9 & 76.0$ \pm $2.4 & 53.4$ \pm $4.7 & 73.4$ \pm $3.0 & 63.0$ \pm $3.2 & 75.7$ \pm $2.4 \\
			\hline
			{\textbf{Our proposed model} (\emph{unsupervised})} & GTFormer & \textbf{69.1}$ \pm $\textbf{0.6} & \textbf{84.5}$ \pm $\textbf{0.9} & \textbf{70.9}$ \pm $\textbf{2.4} & \textbf{82.3}$ \pm $\textbf{1.1} & \textbf{65.8}$ \pm $\textbf{0.8} & \textbf{82.8}$ \pm $\textbf{1.0} & \textbf{65.8}$ \pm $\textbf{2.1} & \textbf{82.2}$ \pm $\textbf{1.1} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.2cm}
	\label{table:unsupervised_classification}
\end{table*}
\begin{table*}[!h]
	\centering
	\caption{Supervised document classification with Micro F1 and Macro F1 scores (in percentage).}
	\vspace{-0.3cm}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|rrrr|rrrr}
			\toprule
			\multirow{2}{*}{Category} & \multirow{2}{*}{Model} & \multicolumn{4}{c|}{Micro F1} & \multicolumn{4}{c}{Macro F1} \\
			\cline{3-10}
			{} & {} & DS & ML & PL & COVID & DS & ML & PL & COVID \\
			\hline
			\multirow{3}{*}{\shortstack{\textbf{\emph{Supervised} version}}} & TSNTM & 54.9$ \pm $2.1 & 72.8$ \pm $1.5 & 63.3$ \pm $0.5 & 84.1$ \pm $1.3 & 50.8$ \pm $2.4 & 68.6$ \pm $1.3 & 56.1$ \pm $0.8 & 84.0$ \pm $1.2 \\
            {} & HGTM & 68.2$ \pm $0.8 & 83.8$ \pm $0.5 & 72.2$ \pm $1.4 & 86.3$ \pm $1.7 & 63.9$ \pm $1.5 & 82.6$ \pm $0.7 & 67.4$ \pm $2.0 & \textbf{86.2}$ \pm $\textbf{1.9} \\
            {} & Patton & 67.8$ \pm $3.5 & 84.1$ \pm $2.4 & 73.5$ \pm $0.5 & 81.5$ \pm $1.2 & 62.9$ \pm $3.0 & 83.2$ \pm $2.3 & 69.5$ \pm $1.7 & 80.8$ \pm $1.6 \\
			\hline
            \multirow{4}{*}{\shortstack{\textbf{Text classification models}}} & TextGCN & 66.8$ \pm $1.0 & 78.3$ \pm $0.7 & 67.5$ \pm $0.7 & 83.7$ \pm $0.5 & 61.6$ \pm $0.4 & 76.0$ \pm $0.8 & 61.4$ \pm $1.1 & 79.6$ \pm $0.5 \\
			{} & HyperGAT & 70.2$ \pm $0.4 & 80.0$ \pm $0.4 & 65.8$ \pm $2.5 & 84.3$ \pm $1.2 & 65.4 $ \pm $0.9 & 78.9$ \pm $0.5 & 60.2$ \pm $2.5 & 81.3$ \pm $0.8 \\
			{} & HINT & 45.7$ \pm $3.5 & 69.5$ \pm $1.1 & 55.4$ \pm $2.3 & 85.7$ \pm $1.5 & 42.1$ \pm $2.6 & 64.8$ \pm $3.9 & 44.3$ \pm $3.2 & 85.8$ \pm $1.5 \\
            {} & G2P2 & 58.0$ \pm $2.2 & 71.7$ \pm $3.2 & 65.3$ \pm $0.4 & 77.5$ \pm $1.2 & 51.1$ \pm $2.7 & 68.7$ \pm $3.8 & 61.1$ \pm $0.1 & 75.6$ \pm $0.6 \\
			\hline
			{\textbf{Our proposed model} (\emph{supervised})} & GTFormer & \textbf{72.2}$ \pm $\textbf{1.1} & \textbf{86.5}$ \pm $\textbf{0.5} & \textbf{74.5}$ \pm $\textbf{1.1} & \textbf{85.3}$ \pm $\textbf{1.0} & \textbf{69.3}$ \pm $\textbf{2.0} & \textbf{85.1}$ \pm $\textbf{0.7} & \textbf{71.4}$ \pm $\textbf{1.0} & \textbf{86.2}$ \pm $\textbf{1.1} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.2cm}
	\label{table:supervised_classification}
\end{table*}
\begin{table*}[!h]
	\centering
	\caption{Topic coherence NPMI (left, in percentage) and perplexity (right).}
	\vspace{-0.3cm}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|rrrrrr|rrrrrr}
			\toprule
			\multirow{2}{*}{Model} & \multicolumn{6}{c|}{Topic Coherence NPMI (higher is better)} & \multicolumn{6}{c}{Perplexity (lower is better)} \\
			\cline{2-13}
			{}  & DS & ML & PL & COVID & DBLP & Web & DS & ML & PL & COVID & DBLP & Web \\
			\hline
			ProdLDA & 10.5$ \pm $0.3 & 10.9$ \pm $0.7 & 12.1$ \pm $0.7 & 12.0$ \pm $0.7 & 9.9$ \pm $0.6 & 21.2$ \pm $0.2 & 7.97$ \pm $0.00 & 7.99$ \pm $0.00 & 7.92$ \pm $0.00 & 7.82$ \pm $0.00 & 8.18$ \pm $0.00 & 8.34$ \pm $0.00 \\
			ETM & 7.3$ \pm $0.2 & 7.1$ \pm $0.2 & 8.7$ \pm $0.1 & 8.2$ \pm $0.7 & 9.5$ \pm $0.5 & 16.4$ \pm $0.6 & 7.92$ \pm $0.00 & 7.96$ \pm $0.00 & 7.94$ \pm $0.00 & 7.80$ \pm $0.00 & 8.66$ \pm $0.00 & 8.52$ \pm $0.00 \\
			GATON & 12.2$ \pm $0.2 & 17.4$ \pm $1.0 & 5.4$ \pm $1.1 & 13.8$ \pm $1.2 & 7.2$ \pm $0.8 & 4.8$ \pm $1.1 & 8.83$ \pm $0.07 & 8.37$ \pm $0.02 & 8.38$ \pm $0.03 & 8.42$ \pm $0.00 & 8.35$ \pm $0.00 & 8.33$ \pm $0.00 \\
			GNTM & 11.6$ \pm $0.5 & 12.1$ \pm $0.3 & 15.4$ \pm $0.7 & 13.8$ \pm $0.8 & 15.2$ \pm $0.2 & 23.8$ \pm $0.3 & 7.18$ \pm $0.01 & 6.91$ \pm $0.01 & 6.83$ \pm $0.01 & 7.69$ \pm $0.01 & 7.52$ \pm $0.00 & 7.79$ \pm $0.00 \\
			\hline
			nCRP & 2.6$ \pm $0.4 & 2.2$ \pm $0.1 & 2.2$ \pm $0.1 & 3.0$ \pm $0.1 & 2.8$ \pm $0.3 & 2.8$ \pm $0.0 & 6.91$ \pm $0.05 & 6.94$ \pm $0.02 & 6.87$ \pm $0.02 & 7.69$ \pm $0.05 & 8.00$ \pm $0.02 & 7.71$ \pm $0.04 \\
			TSNTM & 11.5$ \pm $0.9 & 12.1$ \pm $0.6 & 15.1$ \pm $0.8 & 14.1$ \pm $0.8 & 15.1$ \pm $1.0 & \textbf{26.6}$ \pm $\textbf{2.3} & 7.75$ \pm $0.02 & 6.92$ \pm $0.01 & 6.83$ \pm $0.01 & 7.64$ \pm $0.04 & 7.68$ \pm $0.01 & \textbf{7.35}$ \pm $\textbf{0.03} \\
			HTV & 11.2$ \pm $1.2 & 10.8$ \pm $1.0 & 13.3$ \pm $1.8 & 16.6$ \pm $2.5 & 12.1$ \pm $0.6 & 26.5$ \pm $0.9 & 7.78$ \pm $0.03 & 6.95$ \pm $0.02 & 6.83$ \pm $0.03 & 7.62$ \pm $0.04 & 7.53$ \pm $0.01 & 7.44$ \pm $0.01 \\
            HyperMiner & 12.9$ \pm $0.2 & 15.9$ \pm $0.2 & 20.0$ \pm $1.5 & 9.9$ \pm $0.8 & 17.3$ \pm $0.3 & 15.3$ \pm $0.5 & 7.86$ \pm $0.23 & 7.73$ \pm $0.19 & 7.69$ \pm $0.20 & 8.04$ \pm $0.22 & 9.65$ \pm $0.02 & 8.54$ \pm $0.01 \\
            TraCo & 11.0$ \pm $0.3 & 11.5$ \pm $0.2 & 11.2$ \pm $0.3 & 12.7$ \pm $0.5 & 15.4$ \pm $0.4 & 18.6$ \pm $0.2 & 7.83$ \pm $0.08 & 7.69$ \pm $0.01 & 7.65$ \pm $0.01 & 8.04$ \pm $0.02 & 8.64$ \pm $0.00 & 7.67$ \pm $0.01 \\
			\hline
			AdjEnc & 12.0$ \pm $0.2 & 9.9$ \pm $0.9 & 11.3$ \pm $0.9 & 13.8$ \pm $0.4 & 9.2$ \pm $0.2 & 15.2$ \pm $0.1 & 8.06$ \pm $0.02 & 7.65$ \pm $0.05 & 7.62$ \pm $0.04 & 6.96$ \pm $0.00 & 8.71$ \pm $0.02 & 8.26$ \pm $0.01 \\
			LANTM & 6.4$ \pm $0.5 & 5.4$ \pm $0.3 & 7.2$ \pm $0.8 & 8.6$ \pm $0.3 & N.A. & N.A. & 7.58$ \pm $0.03 & 8.63$ \pm $0.00 & 8.48$ \pm $0.00 & 8.48$ \pm $0.00 & N.A. & N.A. \\
			GTNN & 9.9$ \pm $1.5 & 7.2$ \pm $0.6 & 5.8$ \pm $0.6 & 13.5$ \pm $2.7 & 8.3$ \pm $0.5 & 7.9$ \pm $1.6 & 7.77$ \pm $0.04 & 7.75$ \pm $0.02 & 7.73$ \pm $0.01 & 7.96$ \pm $0.00 & 9.39$ \pm $0.01 & 8.26$ \pm $0.01 \\
            HGTM & 17.1$ \pm $1.4 & 19.0$ \pm $2.6 & 21.9$ \pm $2.8 & 23.3$ \pm $3.1 & 18.5$ \pm $1.2 & 25.0$ \pm $1.7 & 7.46$ \pm $0.03 & 6.89$ \pm $0.02 & 6.81$ \pm $0.00 & 7.60$ \pm $0.01 & 7.77$ \pm $0.02 & 7.71$ \pm $0.01 \\
			\hline
			HINT & 9.3$ \pm $1.3 & 6.6$ \pm $2.2 & 8.6$ \pm $2.4 & 11.6$ \pm $3.0 & N.A. & N.A. & 8.04$ \pm $0.07 & 8.45$ \pm $0.08 & 8.51$ \pm $0.28 & 8.84$ \pm $0.12 & N.A. & N.A. \\
			\hline
			GTFormer & \textbf{19.1}$ \pm $\textbf{1.2} & \textbf{20.6}$ \pm $\textbf{0.4} & \textbf{23.2}$ \pm $\textbf{1.5} & \textbf{24.0}$ \pm $\textbf{1.3} & \textbf{20.2}$ \pm $\textbf{1.0} & 26.2$ \pm $0.8 & \textbf{7.40}$ \pm $\textbf{0.03} & \textbf{6.82}$ \pm $\textbf{0.04} & \textbf{6.68}$ \pm $\textbf{0.04} & \textbf{6.79}$ \pm $\textbf{0.03} & \textbf{7.49}$ \pm $\textbf{0.00} & 7.58$ \pm $0.00 \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.4cm}
	\label{table:topic_analysis}
\end{table*}

\begin{table*}[t]%{R}{0.75\textwidth}
	\centering
	\caption{Link prediction with AUC score (in percentage).}
	\vspace{-0.3cm}
	\resizebox{\linewidth}{!}{
		\begin{tabular}{c|c|rrrrrr}
			\toprule
			\multirow{2}{*}{Category} & \multirow{2}{*}{Model} & \multicolumn{6}{c}{Link prediction AUC} \\
			\cline{3-8}
			{} & {} & DS & ML & PL & COVID & DBLP & Web \\
			\hline
			\multirow{4}{*}{\shortstack{\textbf{Flat topic models}}} & ProdLDA & 76.8$ \pm $0.5 & 82.5$ \pm $0.2 & 78.9$ \pm $0.4 & 80.1$ \pm $0.8 & 92.6$ \pm $0.1 & 82.4$ \pm $0.0 \\
            {} & ETM & 71.0$ \pm $1.3 & 70.3$ \pm $1.6 & 72.5$ \pm $1.1 & 87.2$ \pm $0.9 & 70.3$ \pm $1.2 & 79.4$ \pm $0.0 \\
            {} & GATON & 76.0$ \pm $0.6 & 75.9$ \pm $1.5 & 64.5$ \pm $0.5 & 70.2$ \pm $1.2 & 82.6$ \pm $0.8 & 87.6$ \pm $0.1 \\
            {} & GNTM & 77.0$ \pm $0.9 & 79.3$ \pm $0.8 & 73.2$ \pm $0.3 & 76.8$ \pm $0.4 & 93.7$ \pm $0.0 & 86.3$ \pm $0.0 \\
            \hline
            \multirow{5}{*}{\shortstack{\textbf{Hierarchical topic models}}} & nCRP & 62.2$ \pm $3.3 & 58.0$ \pm $0.9 & 60.1$ \pm $3.1 & 70.8$ \pm $0.8 & 56.3$ \pm $1.1 & 57.2$ \pm $0.0 \\
            {} & TSNTM & 72.7$ \pm $1.9 & 77.8$ \pm $1.5 & 75.5$ \pm $0.9 & 70.8$ \pm $0.8 & 84.6$ \pm $0.8 & 87.4$ \pm $0.8 \\
            {} & HTV & 66.2$ \pm $2.5 & 69.9$ \pm $1.9 & 68.3$ \pm $4.6 & 86.0$ \pm $2.0 & 84.9$ \pm $1.0 & 86.1$ \pm $0.8 \\
            {} & HyperMiner & 64.0$ \pm $1.0 & 63.7$ \pm $1.3 & 62.5$ \pm $2.6 & 71.5$ \pm $2.0 & 75.1$ \pm $2.8 & 78.2$ \pm $1.3 \\
            {} & TraCo & 72.5$ \pm $0.5 & 75.0$ \pm $3.1 & 70.0$ \pm $1.6 & 76.4$ \pm $0.9 & 87.3$ \pm $1.2 & 87.5$ \pm $1.1 \\
            \hline
            \multirow{4}{*}{\shortstack{\textbf{Topic models for document graph}\\(LANTM cannot run on large data DBLP and Web)}} & AdjEnc & 81.7$ \pm $0.4 & 81.0$ \pm $1.1 & 80.8$ \pm $1.7 & 79.8$ \pm $0.6 & 95.5$ \pm $0.0 & 73.2$ \pm $0.0 \\
            {} & LANTM & 78.4$ \pm $0.6 & 78.7$ \pm $0.7 & 82.2$ \pm $0.3 & 93.6$ \pm $0.3 & N.A. & N.A. \\
            {} & GTNN & 71.5$ \pm $1.1 & 76.6$ \pm $0.9 & 73.7$ \pm $1.2 & 84.3$ \pm $1.0 & 95.3$ \pm $0.4 & 74.3$ \pm $0.2 \\
            {} & HGTM & 94.5$ \pm $0.3 & 89.9$ \pm $0.8 & 91.3$ \pm $0.3 & \textbf{95.7}$ \pm $\textbf{0.2} & 96.6$ \pm $0.3 & \textbf{91.3}$ \pm $\textbf{0.1} \\
            \hline
            \multirow{5}{*}{\shortstack{\textbf{Text-attributed graph models}}} & BERT+HGCN & 93.6$ \pm $0.4 & 91.9$ \pm $0.7 & 91.0$ \pm $0.8 & 91.5$ \pm $0.2 & 97.5$ \pm $0.1 & 90.2$ \pm $0.0 \\
            {} & LLaMA2+HGCN & 94.2$ \pm $0.2 & 92.5$ \pm $0.5 & 92.2$ \pm $0.8 & 92.2$ \pm $0.3 & 97.1$ \pm $0.0 & 91.5$ \pm $0.0 \\
            {} & Specter & 90.0$ \pm $1.3 & 88.1$ \pm $0.8 & 87.6$ \pm $0.8 & 87.2$ \pm $1.0 & 97.1$ \pm $0.2 & 86.3$ \pm $0.2 \\
            {} & LinkBERT & 66.9$ \pm $2.1 & 71.2$ \pm $1.5 & 66.3$ \pm $0.7 & 74.9$ \pm $1.2 & 75.3$ \pm $0.8 & 70.4$ \pm $0.5 \\
            {} & Patton & 93.7$ \pm $1.6 & 92.1$ \pm $0.7 & 92.2$ \pm $0.5 & 91.9$ \pm $1.3 & 97.6$ \pm $0.1 & 87.5$ \pm $0.2 \\
            %{} & TAPE & \\
            \hline
            \multirow{1}{*}{\shortstack{\textbf{Hyperbolic transformer} (cannot run on large data)}} & FPS-T & 92.4$ \pm $2.1 & 90.1$ \pm $1.8 & 91.6$ \pm $0.7 & 89.1$ \pm $1.6 & N.A. & N.A. \\
            \hline
            \multirow{4}{*}{\shortstack{\textbf{Text classification models}\\(cannot run on DBLP and Web with no labels)}} & TextGCN & 83.2$ \pm $0.4 & 76.5$ \pm $0.5 & 68.2$ \pm $0.4 & 87.1$ \pm $0.4 & N.A. & N.A. \\
            {} & HyperGAT & 84.5$ \pm $0.2 & 82.0$ \pm $0.8 & 77.5$ \pm $1.0 & 87.1$ \pm $0.4 & N.A. & N.A. \\
			{} & HINT & 72.6$ \pm $2.1 & 71.7$ \pm $1.4 & 69.7$ \pm $1.4 & 86.6$ \pm $0.2 & N.A. & N.A. \\
            {} & G2P2 & 84.5$ \pm $0.7 & 82.9$ \pm $1.0 & 83.3$ \pm $0.1 & 85.0$ \pm $1.3 & N.A. & N.A. \\
            \hline
            {\textbf{Our proposed model}} & GTFormer & \textbf{95.6}$ \pm $\textbf{0.4} & \textbf{93.4}$ \pm $\textbf{0.5} & \textbf{93.4}$ \pm $\textbf{0.6} & 93.2$ \pm $0.7 & \textbf{98.1}$ \pm $\textbf{0.2} & \textbf{91.4}$ \pm $\textbf{0.1} \\
			\bottomrule
		\end{tabular}
	}
	\vspace{-0.2cm}
	\label{table:link_prediction}
\end{table*}

\begin{figure*}%[t]
	\centering
	\includegraphics[width=1\linewidth]{figure/model_analysis}
	\vspace{-0.6cm}
	\caption{Ablation analysis of our model. Best seen in color.}
	\label{fig:model_analysis}
	\vspace{-0.42cm}
\end{figure*}

% We follow previous topic models and evaluate on document classification, link prediction, topic analysis.

\textbf{Datasets.} We use six datasets in Table \ref{table:dataset_statistics}. Cora \cite{cora} is a citation network with abstracts as texts and citations as links. We follow \cite{cora2} and create three independent datasets, \textbf{DS}, \textbf{ML}, and \textbf{PL}. \textbf{DBLP} \cite{aminer} is another citation graph. \textbf{COVID} \cite{hgtm} is a Coronavirus news corpus with hyperlinks among articles. \textbf{Web} \cite{web} is a Webpage hyperlink network. Appendix \ref{sec:dataset_preprocessing} contains dataset preprocessing \cite{wu2024topmost}.

\textbf{Baselines.} Since GTFormer is topic model, we mainly compare to topic models. \emph{i}) \textbf{Flat topic models} do not learn any hierarchy, ProdLDA \cite{prodlda}, ETM \cite{etm}, GATON \cite{gaton}, GNTM \cite{gntm}. \emph{ii}) \textbf{Hierarchical topic models}, nCRP \cite{ncrp}, TSNTM \cite{tsntm}, HTV \cite{htv}, HyperMiner \cite{hyperminer}, TraCo \cite{traco}. \emph{iii}) \textbf{Topic models for document graph} capture text and graph, AdjEnc \cite{adjacent_encoder}, LANTM \cite{lantm}, GTNN \cite{gtnn}, HGTM \cite{hgtm}. HGTM has topic and graph hierarchies in a \emph{cascaded} method. \emph{iv}) \textbf{Text-attributed graph models.} %use language models for text and graph. 
Strictly speaking, they are not topic models, nor baselines. For completeness, we still compare to BERT+HGCN, LLaMA2+HGCN \cite{llama2}, Specter \cite{specter}, LinkBERT \cite{linkbert}, Patton \cite{patton}. For BERT+HGCN, we first use BERT to encode each document, then apply HGCN, in a \emph{cascaded} method. %We use LLaMA2-7B due to limited resource. 
%We exclude GraphFormer \cite{graphformers}, since Patton is its improvement. 
\emph{v}) \textbf{Hyperbolic graph transformer} is designed in hyperbolic space. It is not a topic model, either. We still compare to FPS-T \cite{fps_t}. \emph{vi}) \textbf{Text classification} has label supervision, TextGCN \cite{textgcn}, HyperGAT \cite{hypergat}, HINT \cite{hint}, G2P2 \cite{g2p2}. HINT has a topic tree. Besides, we also convert the best unsupervised baselines to their supervised version (TSNTM, HGTM, Patton).

We set $ \tau=10 $, $ \lambda_{\text{Topic}}=1 $. For the supervised model, $ \lambda_{\text{Sup}}=1 $. %Topic tree structure is first pre-defined before training, then updated during training.
We initialize topic tree with three levels, each non-leaf topic has three children. We initialize model with scientific parameters \cite{scibert} for DS, ML, PL, DBLP, and general ones \cite{bert} for others. Experiments are done on an NVIDIA A40 GPU. Each result is obtained by 5 runs. %Code and data are submitted and will be released upon publication. 
Appendix \ref{sec:implementation_environment} has more details.

\subsection{Quantitative Evaluation}

\textbf{Document classification.} We use document classification for evaluation, as in LDA. We use 80\% documents and links within them for training (of which 10\% are for validation). We infer test document embeddings and classify them. %We have unsupervised and supervised training.

\emph{Unsupervised training} does not involve labels and $ \lambda_{\text{Sup}}=0 $. We %follow previous works \cite{adjacent_encoder} and 
use $ \kappa $NN ($ \kappa=5 $) as classifier. We report Micro and Macro F1 scores at Table \ref{table:unsupervised_classification}.

\emph{Supervised training} uses labels for training %and $ \lambda_{\text{Sup}}=1 $. 
We convert the best baselines to the supervised version by adding classifier $ f_{\text{MLP}}(\cdot) $. Table \ref{table:supervised_classification} shows results.

%Hierarchical topic models tend to outperform flat ones, since they have topic hierarchy to differentiate documents. 
%We model both graph and topic hierarchies, and improve hierarchical topic models. 
HGTM achieves better accuracy than other topic models, due to its modeling of both hierarchies. Our model is better than HGTM, since we capture both hierarchies in a nested manner and better integrate them into representations.

%\subsection{Topic Analysis}

\textbf{Topic coherence.} %A property of topic models is topic interpretability, i.e., each topic is interpreted by a group of keywords. 
To evaluate if keywords of each topic coherently reflect the same meaning, we follow ProdLDA and use Google Web 1T 5-gram Version 1 \cite{google_5gram} as external corpus to evaluate their NPMI score. Only topic models produce topic interpretability and can be evaluated, thus others are excluded. Table \ref{table:topic_analysis}(left) shows that %we further model graph hierarchy and improve NPMI over hierarchical topic models. 
HGTM has higher results than other baselines, due to its topic and graph hierarchies. We unify both hierarchies in a nested way and improve NPMI.

\textbf{Perplexity.} As in LDA, we report perplexity of test documents. Since perplexity, $ e^{-\frac{\log \Pr(\mathcal{D}_{\text{test}})}{\sum_{d\in\mathcal{D}_{\text{test}}}|d|}} $, varies much w.r.t. its power, we report its power at Table \ref{table:topic_analysis}. Lower is better. We model both hierarchies to differentiate documents, thus outperform baselines, except on Web where TSNTM is slightly better, because the text is more informative than its graph. But we are still better than it on other tasks.

\textbf{Link prediction.} %As in RTM, we predict if there is a link between two documents. %We observe 80\% documents and links within them for training. 
We predict links within 20\% test documents. We %follow \cite{hgtm} and 
evaluate the probability of a link for Euclidean models by $ \Pr(e_{ij})\propto e^{-||\textbf{d}_i-\textbf{d}_j||^2}$, and for hyperbolic models by $ \Pr(e_{ij})\propto e^{-d_{\mathcal{L}}^K(\textbf{d}_i,\textbf{d}_j)^2}$. %We compare the predicted links to ground-truth adjacency and 
We report AUC at Table \ref{table:link_prediction}. %LANTM cannot run on large datasets DBLP and Web. %Text classification models cannot run on DBLP and Web with no labels.) 
%Compared to hierarchical models, we verify the advantage of graph hierarchy. 
Among baselines, HGTM predicts links more accurately than others, since it models both topic and graph hierarchies. We achieve better results than it, due to nested modeling. %is better than cascaded modeling. %at unifying both hierarchies in embeddings.

% We also visually plot the topic hierarchy and keywords as a \emph{case study}. 
See Appendix \ref{sec:qualitative_topic_analysis} for topic visualization.

\subsection{Model Analysis}

\textbf{Topic tree structure.} We design two settings: \emph{i}) replacing hierarchical tree with flat topics; \emph{ii}) fixing the tree structure during learning. Fig. \ref{fig:model_analysis}(a) shows that a flat structure decreases the result, since a hierarchical tree captures topic hierarchy to improve the result. A fixed tree decreases the result, since the pre-defined tree is not suitable for the corpus. %causing topic mismatch.

\textbf{Topic and graph hierarchies.} %To validate the effect of topic and graph hierarchies, 
We respectively remove each hierarchy from the model in Fig. \ref{fig:model_analysis}(b). The model with both hierarchies performs the best. The ablated models %without any of the hierarchies 
drop the result. This observation verifies that both hierarchies are useful. %and capturing them improves the performance.

\textbf{Hyperbolic modeling.} %To analyze the effect of hyperbolic space, 
We replace all hyperbolic operations with their Euclidean ones in Fig. \ref{fig:model_analysis}(c). Hyperbolic space is helpful to better preserve hierarchy and improve result than Euclidean space.

\textbf{Different $ \lambda_{\text{Topic}} $.} We vary $ \lambda_{\text{Topic}} $ in Fig. \ref{fig:model_analysis}(d). For classification, an appropriate value maintains result, while a high value hurts result, since a high value influences graph loss. For topic coherence, gradually increasing $ \lambda_{\text{Topic}} $ improves NPMI, %since we focus more on topic modeling, 
while a high value hurts the result. Taking the balance, we set $ \lambda_{\text{Topic}}=1 $.

%More ablation analyses are put in supplementary materials.