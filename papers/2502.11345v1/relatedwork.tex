\section{Related Work}
%\textbf{Flat topic models} are based on graphical models \cite{plsa,lda} and auto-encoders \cite{nvdm,prodlda,etm,nstm}. Some others are GNNs \cite{gcn,graphsage}, including \cite{gaton,graphbtm,gntm}. Recently, they are based on language models \cite{transformer}, e.g., BERTopic \cite{bertopic}, TopicGPT \cite{topicgpt}, GPTopic \cite{gptopic}. They do not model topic or graph hierarchy.

\textbf{Topic models} are first designed with flat topics \cite{lda,nvdm,prodlda,bertopic,topicgpt,nguyen2024topic,wu2019short,wu2020learning,wu2020short,wu2021discovering,wu2022mitigating,wu2023infoctm,wu2024dynamic,wu2024thesis}. HTMs explore topic hierarchy, e.g., graphical \cite{ncrp,dhtg,wedtm,dirbn,pam,dpfa,gbn,hdp} and neural ones \cite{tsntm,htv,hntm,sawetm,traco,hyhtm}, but no one captures graph structure. Though Doubly RNN appears in \cite{drnn,tsntm,htv}, it is in Euclidean, not in hyperbolic space. Hyperbolic space has been shown to be more effective to capture hierarchy.

\textbf{Relational topic models} deal with graph-structured documents \cite{rtm,nrtm,adjacent_encoder,gtnn,lantm}. %They indeed consider both modalities, but no one models topic or graph \emph{hierarchy}. 
The recent HGTM \cite{hgtm} is the only one with both hierarchies, but is a cascaded method and is not effective to integrate both hierarchies. %We insert topic and graph hierarchies into each Transformer layer to unify them.

\textbf{Graph neural networks (GNNs)} are first proposed in Euclidean space \cite{gcn,gat,graphsage}. To model graph hierarchy, hyperbolic GNNs are proposed, e.g., HGNN \cite{hgnn}, HGCN \cite{hgcn}, HAT \cite{hat}, HTGN \cite{htgn}, $\kappa$GCN \cite{kgcn}. However, they mainly focus on graph structure, and do not deal with textual semantics.

\textbf{Text-attributed graph} combines GNNs and language models for both graph and text, e.g., GraphFormer \cite{graphformers}, Patton \cite{patton}, Heterformer \cite{heterformer}, Edgeformers \cite{edgeformers}, TAPE \cite{tape}, Specter \cite{specter}, LinkBERT \cite{linkbert}, etc. They consider both modalities, but no one models topic or graph \emph{hierarchy}.