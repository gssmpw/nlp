\section{Learning Algorithm}
\label{sec:algorithm}

We summarize the learning process of our model and formulate it in Algorithm \ref{algo:training_algorithm}. Code and datasets are submitted as supplementary materials. We will release them upon publication.

\begin{algorithm}%[t]
	\caption{Training Process of GTFormer}
	\label{algo:training_algorithm}
	\begin{flushleft}
		\hspace*{\algorithmicindent}\textbf{Input}: A document graph $ \mathcal{G} $ with documents $ \mathcal{D} $ and graph edges $ \mathcal{E} $, a predefined topic tree structure, hyperparameters $ \lambda_{\text{HTM}} $, $ \tau $, and $ \lambda_{\text{sup}} $. \\
		\hspace*{\algorithmicindent}\textbf{Output}: Document embeddings $ \{\textbf{h}_i\}_{i=1}^N $ and topic-word distribution $ \bm{\beta} $.
	\end{flushleft} 
	\begin{algorithmic}[1] % The number tells where the line numbering should start
		\State Initialize model parameters.
		%\nextnr
		\While{not converged}
        \For{document $ i\in\mathcal{D} $}
        \State Feature initialization %by Eq. \ref{eq:feature_initialization} 
        and obtain hyperbolic embedding $ \textbf{H}^{(l=0)}_i $.
        \Statex \quad\qquad\textit{ // Get the initial hyperbolic token embeddings}
        \State Pass to one hyperbolic Transformer layer and obtain $ \textbf{H}^{(l=1)}_i=f_{\text{HypTRM}}(\textbf{H}^{(l=0)}_i) $.
		\Statex \quad\qquad\textit{ // $ (L-1) $ topic- and graph-Transformer layers}
		\For{$ l=2,...,L $}
		\State Obtain document embedding $ \textbf{d}_i^{(l)}=\textbf{H}^{(l)}_{i,\text{CLS}} $. %at the $ l $-th layer.
        \State Use Hyperbolic Doubly RNN to obtain hyperbolic topic embeddings $ \{\textbf{z}_t\}_{t=1}^T $.
        \State Evaluate path distribution $ \bm{\pi}_i^{(l)} $ and level distribution $ \bm{\delta}_i^{(l)} $ using document embedding $ \textbf{d}_i^{(l)} $ and topic embeddings $ \{\textbf{z}_t\}_{t=1}^T $.
        \Statex \qquad\qquad\textit{ // Hierarchical tree embedding}
        \State Obtain hierarchical tree embedding $ \textbf{e}_i^{(l)} $. %at the $ l $-th layer.
        \Statex \qquad\qquad\textit{ // Capture graph hierarchy by HGNNs}
        \State Obtain hierarchical graph embedding $ \textbf{g}_i^{(l)} $. %at the $ l $-th layer.
        \Statex \qquad\qquad\textit{ // Insert both hierarchies into Transformer}
        \State Obtain the output from the $ l $-th hyperbolic Transformer layer $ \textbf{H}_i^{(l+1)}=f_{\text{HypTRM}}([\textbf{e}_i^{(l)}||\textbf{g}_i^{(l)}||\textbf{H}_i^{(l)}]). $
		\EndFor
        \EndFor
        \Statex \quad\textit{ // Optimization}
		\State Minimize objective function $ \mathcal{L} $ with Adam optimizer.
		\Statex \quad\textit{ // Update topic tree structure}
		\State Update topic tree based on adding threshold $ \lambda_{\text{add}} $ and pruning threshold $ \lambda_{\text{prune}} $.
		\EndWhile
		% %\EndProcedure
	\end{algorithmic}
\end{algorithm}

