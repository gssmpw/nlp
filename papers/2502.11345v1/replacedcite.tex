\section{Related Work}
%\textbf{Flat topic models} are based on graphical models ____ and auto-encoders ____. Some others are GNNs ____, including ____. Recently, they are based on language models ____, e.g., BERTopic ____, TopicGPT ____, GPTopic ____. They do not model topic or graph hierarchy.

\textbf{Topic models} are first designed with flat topics ____. HTMs explore topic hierarchy, e.g., graphical ____ and neural ones ____, but no one captures graph structure. Though Doubly RNN appears in ____, it is in Euclidean, not in hyperbolic space. Hyperbolic space has been shown to be more effective to capture hierarchy.

\textbf{Relational topic models} deal with graph-structured documents ____. %They indeed consider both modalities, but no one models topic or graph \emph{hierarchy}. 
The recent HGTM ____ is the only one with both hierarchies, but is a cascaded method and is not effective to integrate both hierarchies. %We insert topic and graph hierarchies into each Transformer layer to unify them.

\textbf{Graph neural networks (GNNs)} are first proposed in Euclidean space ____. To model graph hierarchy, hyperbolic GNNs are proposed, e.g., HGNN ____, HGCN ____, HAT ____, HTGN ____, $\kappa$GCN ____. However, they mainly focus on graph structure, and do not deal with textual semantics.

\textbf{Text-attributed graph} combines GNNs and language models for both graph and text, e.g., GraphFormer ____, Patton ____, Heterformer ____, Edgeformers ____, TAPE ____, Specter ____, LinkBERT ____, etc. They consider both modalities, but no one models topic or graph \emph{hierarchy}.