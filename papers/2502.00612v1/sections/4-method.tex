\section{Methodology}
\begin{figure*}[t]
    \centering
\includegraphics[width=0.9\textwidth,keepaspectratio]{figures/ccmplus_paper_model.pdf}
    \caption{An overview of the proposed method, which consists of two components: the CCMPlus module and the Backbone Time Series Model. The CCMPlus module identifies causal relationships among web services, generating a CCMPlus representation that enhances the predictive performance of the Backbone Time Series Model. Per iteration refers to the entire method's operations on a single batch of data.}
    \label{fig:model}
\end{figure*}

\begin{algorithm*}[t]

\caption{Algorithmic Procedure for the CCMPlus Module}
\label{alg:ccmplus_module}
\begin{algorithmic}[1]

\Require $\tau = [\tau_1, \ldots, \tau_n]$ : list of time lagged parameters, $\tau_w$ : time window length, 
$L_x$: input time series length,
$N$: the number of web service within the input,
$C_{in}$: the number of input channels for Conv1D,
$C_{out}$: the number of output channels of the Conv1D,
$\hat{\mathbf{X}} \in \mathbb{R}^{B\times N\times L_x}$: a batch of ground truth time series,
$\mathbf{X}\in \mathbb{R}^{B\times N \times L_x \times C_{in}}$: a batch of input time series embedding,
% $\mathbf{Y} \in \mathbb{R}^{B\times N\times L_{out}}$: a batch of ground truth time series,
$\mathbf{M}_{cc} \in \mathbb{R}^{N\times N}$: causal correlation matrix,
$m$: the momentum value used for updating the causal correlation matrix across iterations, constrained within the range \( (0, 1) \),
$num\_iterations$: the number of overall iterations,
$\text{model\_training\_flag} \in \{\text{True}, \text{False}\}$ : training or inference mode.
\Ensure \textbf{Output} $h_{ccm}^{iter}$ and $\mathbf{M}_{cc}^{iter}$
\Statex \textbf{Step 1: Compute $\mathbf{E}$ from $\mathbf{\tau}$ and $\tau_w$.}
\State Initialize $\mathbf{E}$ as an empty list.
\For{each $\tau_i$ in $\tau$}
    \State Compute $E_i \gets \left\lfloor \dfrac{\tau_w}{\tau_i} \right\rfloor$
    \If{$E_i$ is even}
        \State $E_i \gets E_i - 1$
    \EndIf
    \State Append $E_i$ to $\mathbf{E}$
\EndFor

\Statex \textbf{Step 2: calculate the $\mathbf{M}_{cc}^{iter}$ and $h_{ccm}^{iter}$.}
\For{$iter \gets 0, \ldots, num\_iterations-1$}
    \State $\text{multi\_space\_representations} = []$, $\text{multi\_space\_corrs} = []$ 
    \For{$i \gets 1 \ldots n$}
        \State $\tau_i \gets \tau[i-1]$, \quad $E_i \gets \mathbf{E}[i-1]$
        \If{$(L_x - \tau_i \times (E_i - 1)) < 0$}
            \State \textbf{continue} \Comment{Skip if sequence too short}
        \EndIf


        \State Reverse the order of both $\mathbf{X}$ and $\hat{\mathbf{X}}$ along the $L_x$ dimension. Then reshape $\mathbf{X}$ to $\bar{\mathbf{X}} \in \mathbb{R}^{B \cdot N\times C_{in}\times L_x}$
 
        \State $\mathbf{X}_{conv} = \text{Conv1D}\bigl(\bar{\mathbf{X}};\,\text{kernel\_size}=E_i,\text{dilation}=\tau_i,
% \text{in\_channels}=C_{in},\text{out\_channels}=C_{out} 
        \bigr)$, 
        $\mathbf{X}_{conv} \in \mathbb{R}^{B \cdot N\times C_{out}\times  L_{out}}$ 
        % $L_{out} = L_x - \tau_i \cdot (E_i-1))$ 

        \If{$\text{model\_training\_flag} = \text{True}$}
            \State reshape $\mathbf{X}_{conv}$ to $\mathbf{X}_{ccm}\in \mathbb{R}^{B \times N \times L_{out} \times C_{out}}$, $\mathbf{Y} \gets \hat{\mathbf{X}}[:, :,  :L_{out}]$
            
            \State $dist\_matrix \gets \text{PairwiseDistances} (\mathbf{X}_{ccm}, \mathbf{X}_{ccm})$, $nearest\_indices \gets \text{argsort}(dist\_matrix, \text{dim}=-1)$

            \State $nearest\_indices \gets nearest\_indices[:, :, :, 1:(C_{out}+2)]$ 
            
            \State $nearest\_distances \gets \text{Gather}(dist\_matrix, nearest\_indices)$

            \State $u_z \gets \exp\!\Bigl(-\,\dfrac{nearest\_distances}{nearest\_distances[:, :, :, 0{:}1] + \epsilon}\Bigr)$, 
        $w_z \gets \dfrac{u_z}{\sum(u_z) + \epsilon}$

            \State $Y\_nearest \gets \text{GatherTargets}( \mathbf{Y}, nearest\_indices )$, $\hat{Y} \gets \sum (w_z \times Y\_nearest)$

            \State $corr \gets \dfrac{\mathrm{Cov}(\hat{Y},\, \mathbf{Y})}{\sigma(\hat{Y})\,\sigma(\mathbf{Y}) + \epsilon}$,  $\mathbf{M}_{train} \gets corr^\top$ 
            \If{$iter$ >0}
                \State $\mathbf{M}_{train}^{iter} \gets m * \mathbf{M}^{iter-1}_{train} + (1- m) * \mathbf{M}_{train}$
            \Else 
            \State $\mathbf{M}_{train}^{iter} \gets \mathbf{M}_{train}$
            
            \EndIf
             \State $ \widehat{\mathbf{M}_{train}}  \gets \text{Softmax}(\mathbf{M}_{train}^{iter}, \text{dim}=-1)$,
             $\mathbf{M}_{cc}^{iter}(i) \gets \text{Mean}(\widehat{\mathbf{M}_{train}}, dim=0)$
        

        \Else
            \State $\mathbf{M}_{test} \gets \text{Repeat}(\mathbf{M}_{cc}, B)$, $ \widehat{\mathbf{M}_{train}}  \gets \text{Softmax}(\mathbf{M}_{test}, \text{dim}=-1)$, $\mathbf{M}_{cc}^{iter}(i) \gets \mathbf{M}_{cc}$
            
        \EndIf

        \State $conv\_out\_proj \gets \text{LinearProjection}(\mathbf{X}_{conv})$, $ \widehat{\mathbf{X}_{ccm}} \gets \text{Reshape}(conv\_out\_proj, [B,\, N,\, C_{out} ])$
        \State
        $ h_{ccm}^{iter}(i) \gets \widehat{\mathbf{M}_{train}} \times \widehat{\mathbf{X}_{ccm}}$
        

        \State append $h_{ccm}^{iter}(i)$ to $\text{multi\_space\_representations}$, append $\mathbf{M}_{cc}^{iter}(i)$ to $\text{multi\_space\_corrs}$
    \EndFor
    
    
    \State $h_{ccm}^{iter} \gets \text{Mean}(\text{Stack}(\text{multi\_space\_representations}), \text{dim}=0)$
    
    \State $\mathbf{M}_{cc}^{iter} \gets \text{Mean}(\text{Stack}(\text{multi\_space\_corrs}), \text{dim}=0)$
    
\EndFor
\end{algorithmic}
\end{algorithm*}
Inspired by the concept of causal relationships in ecology, as illustrated in Figure~\ref{fig:rabbit}, the population dynamics of rabbits influence the abundance of grass—for instance, a higher rabbit population leads to reduced grass availability. Similarly, we observe causal relationships in the web traffic patterns of different web services, as depicted in Figure~\ref{fig:services_causality}. For example, real-world data from Google Trends indicates that an increase in web traffic for Netflix corresponds to a decrease in traffic for the office software Outlook.

To leverage the latent causality between services for traffic prediction, we extend the ecology CCM theory by integrating into a neural module, referred to as CCMPlus (CCM+). 
As illustrated in Figure~\ref{fig:model}, the CCMPlus module leverages causal relationships among web services to generate the CCMPlus representation~(Section~\ref{sec:ccmplus_module}). This representation is then concatenated with the representation produced by the Backbone Time Series Model~(Section~\ref{sec:backbone_ts_model}), to enhance the accuracy of web service traffic time series prediction.
Finally, we introduce the optimization process in section~\ref{sec:optimize}.
% denoted as \( \hat{t} \). The predictions (\( \hat{t} \)) are compared against the ground truth (\( t \)) using the mean squared error (MSE) loss, which serves as the optimization objective for the entire model.

% In Section~\ref{sec:ccmplus_module}, we detail the construction of the causal correlation matrix by estimating one web service time series using points from the shadow manifold of another web service time series, and then generate the CCMPlus representation using the causal correlation matrix. Additionally, we briefly introduce the two backbone time series models in Section~\ref{sec:backbone_ts_model}. The pseudocode of CCMPlus module is in Algorithm~\ref{alg:ccmplus_module}.
% Finally, we introduce the optimization process in section~\ref{sec:optimize}.

\subsection{CCMPlus Module}
\label{sec:ccmplus_module}
% first we write   \tau and E how come out?  introduce \tau_w       x and target  x.shape    target.shape   
% convolution  then we have conv_out coordinates
% consistent target_ccm length 
% In this section, we aim to derive the feature representation incorporating the causal relationships across web services time series from raw time series, so as to enhance the prediction precision.
In this section, we aim to derive a feature representation that captures causal relationships across web services from raw time series data, thereby improving the precision of traffic prediction.
This process involves constructing the causal correlation matrix by estimating each time point of one web service time series using points from the shadow manifolds of another web service time series, and then generating the CCMPlus representation using the causal correlation matrix.

The first step is to derive the initial time series embedding.
Given the raw time series $\hat{\mathbf{X}} \in \mathbb{R}^{B\times N\times L_x}$. Note that for constructing the shadow manifold, the order of $\hat{\mathbf{X}}$ along the \( L_x \) dimension is reversed.
We first extract date features, e.g., minutes, hours and days, and then combine them with the raw time series $\hat{\mathbf{X}}$ to form the time series embedding $\mathbf{X} \in \mathbb{R}^{B\times N\times L_x\times C_{in}}$.
% The ground truth time series is denoted as $ target \in \mathbb{R}^{B\times N\times L_x}$.
Here, \( L_x \) represents the input time series length, and \( C_{in} \) denotes the representation dimension of the input. 
% Note that for constructing the shadow manifold, the order of $\hat{\mathbf{X}}$ along the \( L_x \) dimension is reversed. 

Next, we employ 1D convolution to construct shadow manifolds for each web service traffic time series.
Traditional CCM algorithm relies expert to set the value of $\tau$, which might introduce human bias.
To alleviate this issue, we extend it to a multi-manifold space, so as to learn the causality from diverse shadow manifold spaces.
Specifically, we initialize two vectors of \( \mathbf{\tau} = [\tau_1, \ldots, \tau_i, \ldots, \tau_n] \) and the corresponding \( \mathbf{E} = [E_1, \ldots, E_i, \ldots, E_n] \).
For simplicity, we use the \( i \)-th shadow manifold, defined by the pair \( (\tau_i, E_i) \), as a representative example to explain the subsequent procedures. The steps for other shadow manifolds differ only in the index \( i \). 
The initialized time lag $\tau_i$ and the corresponding embedding dimension $E_i$ are generated as follows:
\begin{equation}
E_i = 
\begin{cases} 
\left\lfloor \frac{\tau_w}{\tau_i} \right\rfloor - 1, & \text{if } \left\lfloor \frac{\tau_w}{\tau_i} \right\rfloor \bmod 2 = 0, \\[0.5em]
\left\lfloor \frac{\tau_w}{\tau_i} \right\rfloor, & \text{if } \left\lfloor \frac{\tau_w}{\tau_i} \right\rfloor \bmod 2 = 1,\nonumber
\end{cases} 
% \quad \forall \tau_i \in \tau_{\text{list}},\nonumber
\end{equation}
where \( \tau_w \) is set to 100 based on prior empirical observations for shadow manifolds as established in the literature~\cite{kugiumtzis1996state}.
The convolution output can be derived as follows:
\begin{equation}
\mathbf{X}_{conv} = \text{Conv1D}\bigl(\bar{\mathbf{X}};\, \text{kernel\_size}=E_i, \text{dilation}=\tau_i\bigr),\nonumber
\end{equation}
where $\bar{\mathbf{X}} \in \mathbb{R}^{B\cdot N \times C_{in}\times L_x} $ is reshaped from $\mathbf{X}$, the channels of input and output of Conv1D are ${C_{in}}$ and ${C_{out}}$, respectively. 
The trajectory length of the \( i \)-th shadow manifold is: $L_{out} = L_x - \tau_i (E_i - 1)$, which aligns with CCM theory Step 1 in Section~\ref{sec:ccm_procedure}.

After obtaining the convolution output, we can derive the shadow manifold embedding $\mathbf{X}_{ccm} \in \mathbb{R}^{B\times N\times L_{out}\times C_{out}}$ reshaped from $\mathbf{X}_{conv}$, where, for each time point in \( L_{out} \), the corresponding point in the shadow manifold is represented by coordinates in the \( C_{out} \) dimension. To ensure consistency with the trajectory length \( L_{out} \) of the shadow manifolds across different time series, we define the prediction target as \( \mathbf{Y} = \hat{\mathbf{X}}[:, :, :L_{out}] \), where $\mathbf{Y} \in \mathbb{R}^{B\times N\times L_{out}}$.

\subsubsection{Training Mode}
Based on $\mathbf{Y}$ and $\mathbf{X}_{ccm}$, we use the points in the shadow manifold of one web service time series to predict the values of another web service time series. The predictions are then compared with the ground truth values of the target web service time series to compute the correlation coefficient for each pair. 
As a result, we can obtain the causal correlation matrix: $ \mathbf{M}_{train} \in \mathbb{R}^{B\times N\times N}$, where each element represents the causal relationship between a pair of web services. This procedure corresponds to Steps 2–4 in Section~\ref{sec:ccm_procedure}.

The causal correlation matrix $\mathbf{M}_{train}$ can be efficiently computed using matrix operations on a GPU, as outlined in Algorithm~\ref{alg:ccmplus_module} line 18-25. 
To illustrate, we use a specific element of \( \mathbf{M}_{train} \) indexed as \( (l, m, n) \) to demonstrate the process for quantifying causal relationships. The value \( \mathbf{M}_{train}[l, m, n] \) quantifies the causal effect of the service time series \( \mathbf{Y}[l, n, :] \) (denoted with $\mathbf{y}(k)$) on \( \mathbf{Y}[l, m, :] \) (denoted with $x(k)$). 


\( P = \mathbf{X}_{ccm}[l, m, :, :] \), with \( P \in \mathbb{R}^{L_{out} \times C_{out}} \), represents the points on the shadow manifold \( M_x \), where the coordinates of the \( k \)-th point are given by \( \underline{x}(k) = P[k, :] \). The estimation of web service time series $y(k)$ using the shadow manifold $M_x$ of $x(k)$ is denoted by $\hat{y}(k) \mid M_x$.

First, we begin by locating the contemporaneous point on $M_x$, $\underline{x}(k)$, and find its $C_{out}+1$ nearest neighbors. Next, denote the time indices (from closest to farthest) of the nearest neighbors of $\underline{x}(k)$ by $t_{1},...,t_{C_{out}+1}$. These time indices corresponding to nearest neighbors to $\underline{x}(k)$ on $M_{x}$ are used to identify points in time series $y$ to estimate $y(k)$ from a locally weighted mean of the $y(t_i)$ values:
\begin{equation}
    \hat{y}(k) \mid M_x = \sum_{i=1}^{C_{out}+1} w_i y(t_i),\nonumber
\end{equation}
where \( w_i \) represents the weight based on the distance between \( \underline{x}(k) \) and its \( i{\text{-th}} \) nearest neighbor in \( M_x \), and \( y(t_i) \) are the contemporaneous values of time series \( y(k) \). The weights \( w_i \) are determined by:
    \begin{equation}
    w_i = \frac{u_i}{\sum_{j=1}^{C_{out}+1} u_j},\nonumber
    \end{equation}
    where
    \begin{equation}
    u_i = \exp \left\{ -\frac{d[\underline{x}(k), \underline{x}(t_i)]}{d[\underline{x}(k), \underline{x}(t_1)]} \right\}.\nonumber
    \end{equation}
    Here, \( d[\underline{x}(k), \underline{x}(t_i)] \) denotes the Euclidean distance between the two points on the shadow manifold $M_x$.


Finally, we calculate the correlation coefficient \( \mathbf{M}_{train}[l, m, n] \) between \( \hat{y}(k) \) and \( y(k) \), where \( k \) ranges from \( 0 \) to \( L_{out} \):
    \begingroup
\footnotesize
\begin{equation}
    \mathbf{M}_{train}[l, m, n] = \frac{\sum_{t=0}^{L_{out}} \left( y(k) - \overline{y}(k) \right) \left( \hat{y}(k) - \overline{\hat{y}}(k) \right)}
    {\sqrt{\sum_{t=0}^{L_{out}} \left( y(k) - \overline{y}(k) \right)^2 \sum_{t=0}^{L_{out}} \left( \hat{y}(k) - \overline{\hat{y}}(k) \right)^2}}.\nonumber
\end{equation}
\endgroup

% To ensure consistent and stable representations, the $\mathbf{M}_{train}$ is then momentum update with previous iteration $\mathbf{M}^{iter-1}_{train} \in \mathbb{R}^{N \times N}$, and then softmax. 

% $\mathbf{M}_{train} = m * (Repeat into B for first dimension 
%   \mathbf{M}^{iter-1}_{train}) + (1- m) * \mathbf{M}_{train}$

% \widehat{\mathbf{M}_{train}}  \gets \text{Softmax}(\mathbf{M}_{train}, \text{dim}=-1)$,

% then the first batch size is averaged to have the general feature for $\mathbf{M}_{cc}$ for the $i$-th shadow manifold of each web service time series.

% ******

To ensure consistent and stable representations, the $\mathbf{M}_{train}$ matrix is updated using a momentum-based approach~\cite{he2020momentum} with the causal correlation matrix from the previous iteration, denoted as $\mathbf{M}^{iter-1}_{cc} \in \mathbb{R}^{N \times N}$, followed by a softmax operation for normalization:
\begin{align}
\mathbf{M}_{train}^{iter} &= (1 - m) \cdot \mathbf{M}_{train}, + m \cdot \text{Repeat}( \mathbf{M}^{iter-1}_{cc}, \text{dim}=0),\label{eq:causal_matrix_update}\\
{\mathbf{M}_{cc}^{iter}(i)} &= \text{Mean}(\text{Softmax}(\mathbf{M}_{train}^{iter}, \text{dim}=-1), \text{dim}=0),\nonumber
\end{align}
where \( iter \) denotes the current iteration number,
\( \text{Repeat} \) expands \( \mathbf{M}^{iter-1}_{cc} \) into \( \mathbb{R}^{B \times N \times N} \), $m$ is the momentum value in the range $(0,1)$, ${\mathbf{M}_{cc}^{iter}(i)} \in \mathbb{R}^{N \times N}$ corresponds to the \( i \)-th shadow manifold of each web service time series, effectively capturing the causal relationships among the \( N \) web services. 

% Also, $CCMPlus_representation$ incorporating the causal relationship among web services for the \( i \)-th shadow manifold is computed as follows:


% \( \boldsymbol{conv\_out\_ccm} \) with the shape \( (B, N, L_{out}, C_{out}) \), is first reshape to change the last two dimensions, then use Linearlayer to map the dimension $L_{out}$ into 1 , and squeeze it. 
% have intermediate_value

% After that, 

% (\text{Softmax}(\mathbf{M}_{train}, \text{dim}=-1) with shape (B, N, N) is multiply with above intermediate_value, after that, the CCMPlus_representation is generated.

The next step is to compute the feature representation $h_{ccm}^{iter}(i)$ incorporating the causal relationships among web services for the \( i \)-th shadow manifold.
We first transpose the last two dimension of the manifold embedding $\mathbf{X}_{ccm} \in \mathbb{R}^{B\times N\times L_{out}\times C_{out}}$. 
A linear layer is then applied to reduce the dimension $L_{out}$ to \( 1 \). 
This process produces an intermediate value, denoted as $\widehat{\mathbf{X}_{ccm}} \in \mathbb{R}^{B \times N \times C_{out}}$:
\begin{equation}
\widehat{\mathbf{X}_{ccm}} =  \text{LinearLayer} \big( \text{Transpose} (\mathbf{X}_{ccm}) \big).
\label{eq:calculate_conv_val}
\end{equation}
Then,
\begin{equation}
\widehat{\mathbf{M}_{train}} = \mathrm{Softmax}(\mathbf{M}_{train}^{iter}, \, \text{dim} = -1),\nonumber
\end{equation}
\begin{equation}
h_{ccm}^{iter}(i) = \widehat{\mathbf{M}_{train}} \cdot \widehat{\mathbf{X}_{ccm}},
\label{eq:calculate_ccmplus_repre}
\end{equation}
where $h_{ccm}^{iter}(i) \in \mathbb{R}^{B \times N \times C_{out}}$ is the feature representation derived from the \( i \)-th shadow manifold. 
During each training iteration, the CCMPlus representation \( h_{ccm}^{iter} \in \mathbb{R}^{B\times N\times C_{out}} \) and causal correlation matrix \( \mathbf{M}_{cc}^{iter} \in \mathbb{R}^{N \times N}\) are computed by averaging the representations and matrices from all shadow manifolds:
\begin{align}
h_{ccm}^{iter} &= \text{Mean}(\sum_{i=1}^{n }h_{ccm}^{iter}(i)), \label{eq:calcualte_ccmplus_re_iter}\\
\mathbf{M}_{cc}^{iter} &= \text{Mean}(\sum_{i=1}^{n}\mathbf{M}_{cc}^{iter}(i)),\nonumber
\end{align}
where ${\mathbf{M}_{cc}^{iter}(i)} \in \mathbb{R}^{N \times N}$ and ${h_{ccm}^{iter}(i)} \in \mathbb{R}^{B\times N\times C_{out}}$ are the causal correlation matrix and feature representation for the $i$-th manifold space, respectively.
\subsubsection{Testing Mode}
Given \(\mathbf{M}_{cc} \in \mathbb{R}^{N \times N} \) from the output of the training stage, it is expanded along the batch dimension to obtain \( \mathbf{M}_{test} \in \mathbb{R}^{B \times N \times N} \). A softmax normalization is then applied along the last dimension to produce \( \widehat{\mathbf{M}_{train}} \in \mathbb{R}^{B \times N \times N} \):
\begin{align}
\mathbf{M}_{test} &= \text{Repeat}(\mathbf{M}_{cc}, \text{dim}=0), \nonumber\\
\widehat{\mathbf{M}_{train}} &= \text{Softmax}(\mathbf{M}_{test}, \text{dim}=-1).\nonumber
\end{align}

% Based on the $conv\_out\_ccm$ output by the trained moodel, we calculate $\widehat{\mathbf{X}_{ccm}}$ using the Equation~\ref{eq:calculate_conv_val}, then $h_{ccm}$ for the $i$-th shaow manifold of web service time series is calculated by Equation~\ref{eq:calculate_ccmplus_repre}. Since each testing iteration has multiple shadow manifolds, $h_{ccm}^{iter}$ is the average of reprersentations of multiple shadow manifolds with Equation~\ref{eq:calcualte_ccmplus_re_iter}. In the testing stage, the $causal_correlation_matrix_iter$ are keeping constant to be $causal_correlation_matrix$ output by the training stage.

Based on $\mathbf{X}_{ccm} \in \mathbb{R}^{B\times N\times L_{out}\times C_{out}}$ generated by the trained model, \( \widehat{\mathbf{X}_{ccm}} \in \mathbb{R}^{B\times N \times C_{out}} \) is computed using Equation~\ref{eq:calculate_conv_val}. 

Subsequently, the $h_{ccm}^{iter}(i)$ for the \( i \)-th shadow manifold of the web service time series is obtained using Equation~\ref{eq:calculate_ccmplus_repre}. Since each testing iteration involves multiple shadow manifolds, the  $h_{ccm}^{iter}$ is calculated as the average of the representations across all shadow manifolds, as defined in Equation~\ref{eq:calcualte_ccmplus_re_iter}. While ${\mathbf{M}_{cc}^{iter}}$ remains constant and is set to \( \mathbf{M}_{cc} \), which is output by the training stage.

\subsection{Backbone Time Series Model}
\label{sec:backbone_ts_model}
The Backbone Time Series Model, denoted as \( \text{TS\_Model} \), processes the input time series embedding \( \mathbf{X} \in \mathbb{R}^{B \times N \times L_x \times C_{in}} \) and outputs a feature representation \( h_{ts}^{iter} \in \mathbb{R}^{B \times N \times d_{ts}} \):
\begin{equation}
h_{ts}^{iter} = \text{TS\_Model}(\mathbf{X}),\nonumber
\end{equation}
which is concatenated with  \( h_{ccm}^{iter} \), generated by the CCMPlus in training or testing mode, to jointly predict the web service traffic time series.

% Based on the baselines evaluation performances on three real world web service traffic datasets across multiple prediction Granularity in Tables~\ref{tab:30T_performance}, \ref{tab:15T_performance}, \ref{tab:5T_performance}, and \ref{tab:1T_performance}, also considering the research community analysis~\cite{qiu2024tfb,wang2024deep}, the Timesnet and iTransformer are 2 good performing baselines among many SOTA time series models.

% So we use TimesNet and iTransformer as two Backbone Time Series Models to combine with CCMPlus (CCM+) module and verify brooadly the effectiveness of CCMPlus module. The resulting models are called CCM+iTransformer and CCM+TimesNet.


Based on the baseline evaluation performances across three real-world web service traffic datasets at multiple prediction granularities (Tables~\ref{tab:30T_performance}, \ref{tab:15T_performance}, \ref{tab:5T_performance}, and \ref{tab:1T_performance}) and insights from recent research~\cite{qiu2024tfb,wang2024deep}, TimesNet~\cite{wutimesnet} and iTransformer~\cite{liuitransformer} emerge as two strong-performing baselines among state-of-the-art time series models.

TimesNet~\cite{wutimesnet} identifies and utilizes multi-periodicity, decomposing temporal variations into intraperiod and interperiod components. The core module, TimesBlock, adaptively discovers periodicities and extracts features using a parameter-efficient inception block. iTransformer~\cite{liuitransformer} repurposes the Transformer architecture for time series forecasting by applying attention and feed-forward networks on inverted dimensions. It embeds time points as variate tokens, allowing attention to capture multivariate correlations and the feed-forward network to learn nonlinear variate-specific representations.

To broadly verify the effectiveness of the CCMPlus (CCM+) module, we integrate it with these two Backbone Time Series Models, resulting in two variant models, \textit{e.g.,} \textbf{CCM+iTransformer} and \textbf{CCM+TimesNet}.


\subsection{Optimization}
\label{sec:optimize}
For convenient combination with the Backbone Time Series Model and to improve generalization, the CCMPlus representation \( h_{ccm}^{iter} \) is concatenated with the Time Series Model feature representation \( h_{ts}^{iter} \). In general, the whole procedure can be formalized as follows:
{\small
\begin{align}
\hat{t} &= \text{MLP}\left(h_{ccm}^{iter} \mid h_{ts}^{iter}\right),\nonumber\\
\text{MSE Loss} &= \frac{1}{B \cdot N \cdot L_{pred}} \sum_{b=1}^{B} \sum_{n=1}^{N} \sum_{l=1}^{L_{pred}} \left( t_{b, n, l} - \hat{t}_{b, n, l} \right)^2,
\label{eq:mse_loss_overall}
\end{align}
}
where $ h_{ts}^{iter} \in \mathbb{R}^{B\times N\times d_{ts}} $, and both the ground truth \( t \) and predicted values $ \hat{t} \in \mathbb{R}^{B\times N\times L_{pred}} $. \( B \) represents the batch size, \( N \) denotes the number of web services within each batch, \( d_{ts} \) refers to the representation dimension of the Backbone Time Series Models, and \( L_{pred} \) specifies the prediction length of the web services in the time series.















