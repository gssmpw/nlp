\section{Related Work}
\subsection{Web Service Traffic Prediction}
Web service traffic prediction is a task critical to enabling service autoscaling, load balancing, and anomaly detection. As web service traffic is often represented as time series data, existing approaches primarily frame traffic prediction as a time series prediction problem~\cite{pan2023magicscaler,zou2024optscaler,alharthi2024auto}.

Early research focused on statistical prediction methods~\cite{kapgate2014weighted, hu2016autoscaling, kumar2016forecasting} such as Moving Average, Auto-Regression, and Autoregressive Integrated Moving Average. These methods are valued for their simplicity and interpretability but are constrained by strict stationarity requirements. Moreover, they struggle to extend to multi-dimensional or non-linear data, limiting their applicability in dynamic environments.
To overcome these limitations, methods like HOPBLR~\cite{issa2017using}, LLR~\cite{daraghmeh2018linear} and TWRES~\cite{hu2019stream} employed machine learning techniques, including Logistic Regression and Support Vector Regression, respectively, for traffic prediction. However, these approaches were hindered by the limited expressive capacity of their models, resulting in suboptimal prediction accuracy.
More recent advancements have shifted towards deep learning methods. CrystalLP~\cite{ruan2023workload} and GRUWP~\cite{guo2018applying} utilize Long Short-Term Memory networks and Gated Recurrent Units, respectively, to predict service workloads. 
% L-PAW~\cite{} introduces a top-sparse autoencoder to derive sparse representations from highly dynamic service data. 
MagicScaler~\cite{pan2023magicscaler} proposes a novel multi-scale attentive Gaussian process-based predictor, capable of accurately forecasting future demands by capturing scale-sensitive temporal dependencies. 
% It employs a two-stage feature extraction process and Gaussian processes to predict uncertain future demands with high precision.
The Performer~\cite{qi2022performer} integrates the Transformer architecture into an encoder-decoder paradigm for service workload prediction. It leverages the self-attention mechanism to model temporal correlations and learn both global and local representations effectively.
OptScaler~\cite{zou2024optscaler} advances this direction with a proactive prediction module comprising a long-term periodic block and a short-term local block to capture multi-scale temporal dependencies. 
While existing traffic prediction methods demonstrate high accuracy through advanced time series forecasting techniques, they largely neglect the underlying causal relationships within the web services. Exploring these hidden causalities holds significant potential for further improving prediction performance.

\subsection{Time Series Forecasting}
Besides approaches specifically tailored for web service traffic prediction, general time series forecasting methods are also applied to predict web traffic~\cite{alharthi2024auto,zou2024optscaler,10457027}.

Traditional statistical methods such as Prophet~\cite{sean2018forecasting}, and Holt-Winters~\cite{hyndman2018forecasting} assume that time series variations adhere to predefined patterns. However, the inherently complex fluctuations of web service traffic often exceed the scope of these predefined patterns, thereby limiting the practical applicability of such statistical methods~\cite{wutimesnet}.

Recent advancements in neural network architectures have significantly enhanced temporal modeling capabilities. Neural network approaches for time series forecasting can be categorized into five paradigms~\cite{wangtimemixer, tan2024language}: RNN-based, CNN-based, Transformer-based, MLP-based, and large language model (LLM)-based methods. 
% CNN-based methods~\cite{wang2023micn, hewage2020temporal} use convolutional kernels along the temporal dimension to capture sequential patterns, while RNN-based methods~\cite{franceschi2019unsupervised,dudukcu2023temporal,peng2024reservoir} employ recurrent structures to model temporal state transitions. Transformer-based methods~\cite{zhou2022fedformer,liuitransformer,chenpathformer,wang2024timexer} are widely recognized for their effective feature extraction through attention mechanisms. LLM-based methods~\cite{tan2024language,touvron2023llama,jintime} leverage advanced reasoning capabilities, presenting a promising avenue for temporal modeling. Additionally, MLP-based methods~\cite{wang2024timexer,olivares2023neural,daslong,wutimesnet} offer a compelling balance of forecasting accuracy and computational efficiency.

Empirical methods often integrate components from the aforementioned categories, utilizing specific designs to effectively capture critical temporal features~\cite{wangtimemixer}. These specific designs incorporate series decomposition, multi-periodicity analysis, and multi-scale mixing architectures.

For series decomposition, Autoformer~\cite{wu2021autoformer} introduces a decomposition block based on moving averages, enabling the separation of complex temporal variations into seasonal and trend components. Building on this foundation, DLinear~\cite{zeng2023transformers} utilizes series decomposition as a preprocessing step prior to performing linear regression. Crossformer~\cite{zhang2023crossformer} segments time series data into subseries-level patches and employs a Two-Stage Attention layer to effectively model cross-time and cross-variable dependencies within each patch. iTransformer~\cite{liuitransformer} leverages the global representation of entire series and applies attention mechanisms to these series-wise representations, facilitating the capture of multivariate correlations. TimeXer~\cite{wang2024timexer} integrates external information into the Transformer architecture through a carefully designed embedding strategy, allowing the inclusion of external information into patch-wise representations of endogenous series. In the context of multi-periodicity, NBEATS~\cite{oreshkinn} employs multiple trigonometric basis functions to model time series, providing a robust framework for handling periodic patterns. Similarly, TimesNet~\cite{wutimesnet} applies Fourier Transform to decompose time series into components of varying periodic lengths and utilizes a modular architecture to process these decomposed components effectively. With respect to multi-scale mixing architectures, Pathformer~\cite{chenpathformer} adopts multi-scale patch representations and applies dual attention mechanisms across these patches to capture both global correlations and local details, thereby addressing temporal dependencies comprehensively. TimeMixer~\cite{wangtimemixer} captures temporal features by introducing a novel multi-scale mixing architecture, which comprises two key components: Past-Decomposable-Mixing, designed to leverage disentangled series for multi-scale representation learning, and Future-Multipredictor-Mixing, which ensembles complementary forecasting skills across multi-scale series to enhance prediction accuracy.

While general time series forecasting methods have been applied to web service traffic prediction~\cite{pan2023magicscaler, zou2024optscaler,catillo2023survey}, these methods often overlook causal relationships between services. In contrast, our CCMPlus module computes a causal correlation matrix used to generate temporal features incorporating causal relationships, significantly improving web service traffic prediction accuracy.



