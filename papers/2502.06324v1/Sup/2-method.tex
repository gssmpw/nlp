\section{Further details of our Method}
\label{sec:sup_method}



% moiré
This section will showcase the details of the implementation of our UniDemoiré's Moiré Pattern Generator stage and Moiré Image Synthesis stage.

\subsection{Moiré Pattern Generator}
The visualization of moiré pattern patches generated using the Moiré Pattern Generator(MPG) is shown in Figure~\ref{fig:generated_data}. The details of the data preprocessing and networking implementations of MPG are described below.

\input{Tables/alg_data_preprocessing}

\subsubsection{The implementation details of data preprocessing} The details of our data preprocessing method in the Moiré Pattern Generator(MPG) are described in Algorithm~\ref{sup:alg_data_preprocessing}. In Multi-Scale Cropping, ``$\text{Random Crop}(\mathcal{I},w,h)$'' means to randomly crop a patch of size $w\times h$ from $\mathcal{I}$, and ``$\text{Resize}(\mathcal{I},w,h)$'' means to resize the width and height of $\mathcal{I}$ to $w$ and $h$ directly. In Sharpness-Colorfulness selection, ``$\text{RGB\_to\_Gray}(I_{mp})$'' refers to convert $I_{mp}$ to grayscale image $G_{mp}$, while ``$\text{RGB\_to\_LAB}(I_{mp})$'' refers to convert $I_{mp}$ to LAB space and retrieve the corresponding channel matrices $L_{mp}$, $A_{mp}$, and $B_{mp}$ respectively. Moreover, ``$\mathcal{F} \ast G_{mp}$'' denotes the convolution operation on the grayscale image $G_{mp}$ using the Laplace edge detection operator $\mathcal{F}$. 
In the actual training process of MPG, we specify set $n$ to 3 and ($w$, $h$) to (768,768), while $\delta_s$ and $\delta_c$ are set to 15 and 2, respectively.

\subsubsection{The implementation details of Latent Diffusion Model} 
We utilize the Latent Diffusion Model(LDM)~\cite{Rombach2022LDM} as the network component of the Moiré Pattern Generator. 

\begin{figure*}[!p]
  \centering
    \includegraphics[width=1.0\linewidth]{Images/Generated_Data.pdf}
  \caption{Visualization of sampled patches using our Moiré Pattern Generator.}
  \label{fig:generated_data}
\end{figure*}

\begin{figure*}[!t]
  \centering
    \includegraphics[width=1.0\linewidth]{Images/Synthesis_Result_Sup.pdf}
  \caption{Visualization of our intermediate synthetic results. The final synthesis of $I_{trn}$ best resembles the real moiré images in contrast and brightness distortions.} 
  \label{fig:synthesis_result_sup}
\end{figure*}

Firstly, we have trained our autoencoder model for moiré patterns according to the method described in ~\cite{Rombach2022LDM}.
Specifically, given a moiré pattern patch $I_{mp} \in \mathrm{R}^{w \times h \times 3}$ that has gone through Multi-Scale Cropping and Sharpness-Colorfulness selection, we utilize an Encoder $\mathcal{E}$ to convert $I_{mp}$ to the latent space $z=\mathcal{E}(I _{mp})$ through multiple downsampling blocks. Simultaneously, we expect the corresponding Decoder $\mathcal{D}$ to reconstruct the moiré pattern from the latent space variable $z$ : $I_{mp}=\mathcal{D}(z)=\mathcal{D}(\mathcal{E}(I_{mp}))$ by using the same upsampling factor. Note that the overall downsampling factor is denoted as $f=h/h_0=w/w_0$, where  $h_0$ and $w_0$  are hyperparameters chosen to ensure that $f$ is precisely $2^m$, with $m \in \mathrm{N}$.
Our loss function is a combination of a perceptual loss function $\mathcal{L}_{rec}$~\cite{zhang2018unreasonable} and patch-based adversarial targets $\mathcal{L}_{adv}$~\cite{Dosovitskiy2016Generating, Esser2021Taming, Yu2021Vector}, along with a KL-reg regularization term $\mathcal{L}_{reg}$ where the patch-based discriminator $D_{\psi}$ we used is optimized to differentiate between the original moiré pattern $I_{mp}$ and the reconstructed moiré pattern $\mathcal{D}(\mathcal{E}(I_{mp}))$. 
The full objective to train the autoencoder $(\mathcal{E},\mathcal{D})$ is:
\begin{equation}
\begin{split}
&\mathcal{L}=\min_{\mathcal{E}, \mathcal{D}} \max_{\psi}(\mathcal{L}_{rec}(I_{mp}, \mathcal{D}(\mathcal{E}(I_{mp})))+\log D_\psi(I_{mp}) \\
&-\mathcal{L}_{adv}(\mathcal{D}(\mathcal{E}(I_{mp})))+\mathcal{L}_{reg}(I_{mp} ; \mathcal{E}, \mathcal{D}))
\end{split}
\end{equation}

\begin{figure*}[!t]
  \centering
    \includegraphics[width=1.0\linewidth]{Images/Checkerboard_Artifacts.pdf}
  \caption{Examples of the ``Checkerboard Artifacts'' that occur in the $I_{trn}$ when upsampling with Uformer’s transpose convolution~\cite{Wang2022Uformer}.} 
  \label{fig:checkerboard_artifacts}
\end{figure*}

The network structure of our $\mathcal{E}$ and $\mathcal{D}$ are the same as the autoencoder in ~\cite{Rombach2022LDM}.
To compress the moiré pattern as much as possible, we use the downsampling factor $f=\text{32}$ and the number of hidden-space channels 64, which gives the latent variable $z$ a dimension of 64 $\times$ 64 $\times$ 24. 
We adopted 6 downsampling/upsampling blocks in $\mathcal{E}$ and $\mathcal{D}$. Each downsampling/upsampling block contains two layers of ResBlock as well as one layer of multi-head self-attention block, and the list of channel scaling multipliers is [1,1,2,2,4,4]. We trained the autoencoder on 8 NVIDIA A40 GPUs, with a batch size of 2 on each GPU and a learning rate of 4.5e-6. 
Both the autoencoder ($\mathcal{E}$ and $\mathcal{D}$) and the discriminator ($D_{\psi}$) in the loss functions are optimized by Adam~\cite{kingma2014adam} with $\beta_1$ = 0.5 and $\beta_2$ = 0.9.
In total, we trained the autoencoder for 35 Epochs.
% Other hyperparameter settings are shown in Table XXX.

Subsequently, we adopt the diffusion model to modify the complex distribution $p(z)$ obeyed by the latent variable $z$ after the $\mathcal{E}$ transformation with the objective function:
\begin{equation}
\mathcal{L}=\mathbb{E}_{\mathcal{E}\left(I_{m p}\right), \epsilon, t}\left[\| \epsilon-\epsilon_\theta\left(\alpha_t \mathcal{E}\left(I_{m p}\right)+\sigma_t \epsilon, t\right) \|_1\right].
\end{equation}
where $\epsilon\sim\mathcal{N}(0, \mathbb{I})$ is the variable sampled by the standard Gaussian distribution, and $\epsilon_\theta$ is the noisy prediction network parameterized by $\theta$, where we implemented it by using UNet~\cite{ronneberger2015u} which integrates the time-step conditioning variable $t$. The $\alpha_t$ is the value at step $t$ of the signal-to-noise ratio.

The network structures of the diffusion model in latent space are the same as those of the unconditional model in LDM~\cite{Rombach2022LDM}. For the UNet model $\epsilon_\theta$, we set the number of channels to 192, and the encoder and decoder of the UNet contain 4 downsampling/upsampling blocks, and the structure of those blocks are kept the same in the autoencoder. The list of channel scaling multipliers is [1,2,4,8]. 
% We utilize 8 NVIDIA A40 GPUs to train our models for 50 epochs. Each GPU has a batch size of 2, a base learning rate of 1e-4, and a total learning rate that complies with the linear multiplication in the LDM~\cite{Rombach2022LDM} based on the number of GPUs and the batch size.
The model is trained on 8 NVIDIA A40 GPUs for 50 epochs and optimized by AdamW~\cite{loshchilov2019adamw} with $\beta_1$ = 0.9 and $\beta_2$ = 0.999.
The batch size on each GPU is set to 2, and the learning rate is initially set to 1e-4 and scheduled by linear warmup on the first 10,000 steps. The total learning rate complies with the linear multiplication in the LDM~\cite{Rombach2022LDM} based on the number of GPUs and the batch size.
In the training stage, we set the diffusion steps to 1000 and utilized the linear noise schedule to add noise to the latent variable $z$.
We utilize the DDIM sampler~\cite{song2022ddim} to accelerate sampling after training, using 200 sampling steps.
We sampled 100,000 moiré patterns using a single NVIDIA A40 GPU, and some of the samples are shown in Figure~\ref{fig:generated_data}.

% The network parameters of the diffusion model in latent space are the same as those of the unconditional model in LDM~\cite{Rombach2022LDM}. We utilize multiple NVIDIA A40 GPUs to train our models. Each GPU has a batch size of 2, a base learning rate of 1e-4, and a total learning rate that complies with the linear multiplication in the LDM~\cite{Rombach2022LDM} based on the number of GPUs and the batch size


% We use multiple NVIDIA A40 GPUs for training, with each GPU's batch size set to 2, the base learning rate set to 1e-4, and the total learning rate set in compliance with the linear multiplication in the LDM based on the batch size and the number of GPUs.



\subsection{Moiré Image Synthesis}
In this section, we will demonstrate the implementation details of the Moiré Image Synthesis stage that were omitted in our main paper. Additionally, we include more visualizations of the synthesis results in Figure~\ref{fig:synthesis_result_sup}.
% and visualization of comparisons with other synthesis methods in Figure XX.

\subsubsection{Implementations of the Moiré Image Blending} 
For the MIB module, $\omega_m$ in Eq. (5) is randomly selected from [0.65, 0.75], while $\omega_g = 1 - \omega_m$. The $op_m$ and $op_g$ in Eq. (6) are set to 1.0 and 0.8, respectively.
Performance changes resulting from the use of both the Multiply and Grain Merge strategy are detailed in the additional ablation study in Section~\ref{sec:sup_ablation}.

\subsubsection{Implementations of the Tone Refinement Network} 
We implement the backbone of our Tone Refinement Network(TRN) using Uformer-T(Tiny)~\cite{Wang2022Uformer}, where the Transformer Block uses the Locally-enhanced Window (LeWin) Transformer block proposed by Uformer and sets the window size to 8$\times$8. At the same time, we change the encoder depth from \{2,2,2,2\} to \{1,1,1,1\}. 
Performance changes resulting from the use of the Uformer are detailed in the additional ablation study in Section~\ref{sec:sup_ablation}.

In the context of TRN, utilizing the transposed convolutional upsampling block similar to Uformer may lead to the emergence of ``Checkerboard Artifacts'' in the output $I_{trn}$, as illustrated in Figure~\ref{fig:checkerboard_artifacts}.
This issue stems from the uneven overlap when transposed convolution is employed during the upsampling process~\cite{odena2016deconvolution}. 
As a solution, we utilize the CARAFE upsampling operator~\cite{wang2019carafe} to replace transposed convolution in Uformer for upsampling. 
CARAFE effectively addresses the "Checkerboard Artifacts" by predicting diverse up-sampling kernels based on the semantic information of the input feature maps~\cite{wang2019carafe}, thereby contributing to improved feature reorganization within TRN.
Performance changes resulting from the use of CARAFE are detailed in the additional ablation study in Section~\ref{sec:sup_ablation}.

We utilize 2 NVIDIA A40 GPUs to train our Tone Refinement Network on UHDM~\cite{yu2022towards} for 50 epochs, FHDMi~\cite{he2020fhde} for 25 epochs, and TIP~\cite{sun2018moire} for 2 epochs. 
The learning rate is initially set to 1e-5 and scheduled by cyclic cosine annealing~\cite{loshchilov2016sgdr}, and models are optimized by Adam~\cite{kingma2014adam} with $\beta_1=\text{0.9}$ and $\beta_2=\text{0.999}$. 
For the input clean natural images, we set the random crop size to 384 $\times$ 384, and for the moiré patterns sampled by the Moiré Pattern Generator, we resized their resolution to 384 $\times$ 384 as well. 


% We set the batch size to 4. For the input clean natural images, we set the random crop size to $384\times384$, and for the moiré patterns sampled by the Moiré Pattern Generator, we resized their resolution to $384\times384$ as well. The base learning rate is initially set to 0.0001 and scheduled by cyclic cosine annealing~\cite{loshchilov2016sgdr}, and models are optimized by Adam~\cite{kingma2014adam} with $\beta_1=0.9$ and $\beta_2=0.999$. We trained our Tone Refinement Network on UHDM~\cite{yu2022towards}, FHDMi~\cite{he2020fhde}, and TIP~\cite{sun2018moire} using a single NVIDIA A40 GPU card.


\subsubsection{Implementations of loss functions} 
For the loss function Eq. (13) in the main paper, we simply set $\lambda_{per}=\lambda_{color}=\text{1.0}$ and $\lambda_{tv}=\text{0.1}$ to balance the scale of the values during training.

For the perception loss $L_{per}$, to further validate our assumption in the main paper whether computing the content loss between $I_{trn}$ and $I_{mib}$ directly in the pixel space is less effective, we utilize the $\mathcal{L}_1$ loss instead of the $\mathcal{L}_{per}$ loss for our synthesis network in the additional ablation study in Section~\ref{sec:sup_ablation}.

For the color loss $\mathcal{L}_{color}$, we convert $I_{trn}$ and $I_{rm}$ into RGB-uv histogram feature $H(I_{trn})$ and $H(I_{rm})$ from the log-chrominance space followed by prior work on color constancy~\cite{afifi2019sensor, Afifi2019CVPR}, which represents the color distribution of those two images. 
In particular, $u$ and $v$ are used to control the contribution of each color channel in the generated histogram and the smoothness of the histogram bin.
Specifically, given an RGB image $I(\mathrm{x})$ where $\mathrm{x}$ denotes the pixel point index, we first convert it to YUV color space:
\begin{equation}
I_{y}(\mathrm{x})=\sqrt{I_{r}^2(\mathrm{x})+I_{g}^2(\mathrm{x})+I_{b}^2(\mathrm{x})}.
\end{equation}
and:
\begin{flalign}
I_{ur}(i) = \log \frac{I_r(i) + \epsilon}{I_g(i) + \epsilon}  
&\text{;  } I_{vr}(i) = \log \frac{I_r(i) + \epsilon}{I_b(i) + \epsilon} \\
I_{ug}(i) = \log \frac{I_g(i) + \epsilon}{I_r(i) + \epsilon} 
&\text{;  } I_{vg}(i) = \log \frac{I_g(i) + \epsilon}{I_b(i) + \epsilon} \\
I_{ub}(i) = \log \frac{I_b(i) + \epsilon}{I_r(i) + \epsilon} 
&\text{;  } I_{vb}(i) = \log \frac{I_b(i) + \epsilon}{I_g(i) + \epsilon} 
\end{flalign}

where ``$I_r$'', ``$I_g$'', and ``$I_b$'' subscripts refer to the color channels of the image $I$, $\epsilon=\text{10}^{-\text{6}}$ is a small constant added for numerical stability, and ($I_{ur}$, $I_{vr}$), ($I_{ug}$, $I_{vg}$) and ($I_{ub}$, $I_{vb}$) are the $uv$ coordinates of the $I_r$, $I_g$, and $I_b$.

We then generated the unnormalized histogram $H(u,v,c)$ of each color channel $c \in \{r,g,b\}$ according to the HistoGAN~\cite{Afifi2021histogan}, computed as follows:
\begin{equation}
H(u,v,c) \propto \sum_{\mathrm{x}} k\left(I_{uc}(\mathrm{x}), I_{vc}(\mathrm{x}), u, v\right) I_y(\mathrm{x}),
\end{equation}
where $k(\cdot)$ is the inverse-quadratic kernel:
\begin{equation}
\begin{aligned}
k\left(I_{uc}, I_{vc}, u, v\right)=(1 & \left.+\left(\left|I_{uc}-u\right| / \tau\right)^2\right)^{-1} \\
& \times\left(1+\left(\left|I_{vc}-v\right| / \tau\right)^2\right)^{-1}
\end{aligned}
\end{equation}
where $\tau$ is a fall-off parameter to control the smoothness of the histogram's bins. Finally, the histogram features $H(I) \in R^{h \times h \times 3}$ stacked by $H(u,v,c)$ of 3 color channels is normalized to sum to one:
\begin{equation}
H(I) = \frac{[H(u,v,r), H(u,v,g), H(u,v,b)]}{\sum_{u,v,c} H(u, v, c)}.
\end{equation}

Following HistoGAN~\cite{Afifi2021histogan}, we set the histogram bin, $h$, to 64 and set the fall-off parameter of our histogram's bins, $\tau$, to 0.02.