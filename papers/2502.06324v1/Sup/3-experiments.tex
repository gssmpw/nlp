\section{Experiments}
\label{sec:sup_esperiments}
In this section, we will provide a more detailed overview of the experimental setups, present additional visualization results and runtime comparisons, carry out further ablation studies, and address the limitations of our proposed method.

\begin{figure}[!t]
  \centering
    \includegraphics[width=1.0\linewidth]{Images/MHRNID.pdf}
  \caption{Examples of the MHRNID dataset.} 
  \label{fig:mhrnid}
\end{figure}

\subsection{Experimental Setups}
We implement all the experiments using PyTorch Lightning on multiple NVIDIA A40 GPUs. All experiments were conducted once after setting the seed to the same values as~\cite{yu2022towards} and~\cite{undem}.

\subsubsection{Implementation Details of other Comparison Methods}
For Shooting, we migrated their implementation code from Opencv to PyTorch based on the implementation idea provided by~\cite{shooting}. Note that the Shooting method produces a distorted composite image after random projective transformation. We maintain the transformation parameter and adjust the clean image accordingly to ensure that the moiré image aligns with the clean image during the subsequent demoiréing stage.
For UnDeM~\cite{undem}, we directly use their 384$\times$384 moiré image synthesis network trained on UHDM~\cite{yu2022towards} and FHDMi~\cite{he2020fhde} and also train their synthesis network on TIP~\cite{sun2018moire} in their code framework~\cite{undem}.
For MoireSpace~\cite{yang2023doing}, we utilize the moiré patterns provided by their dataset to obtain the synthesis result by deploying their multiply blending strategy. We resize their moiré patterns to 384$\times$384 for a fair comparison.

\input{Tables/tbl_runtime}

\subsubsection{Mixed High-Resolution Natural Image Dataset}
In the Zero-Shot experiments, we collected a comprehensive Mixed High-Resolution Natural Image Dataset (MHRNID) to avoid data overlap between the training and test sets. The MHRNID dataset consists of the super-resolution datasets DF2K-OST~\cite{wang2021real}, the natural image datasets UHD-LOL4K~\cite{wang2023uhdlol4k}, and UHD-IQA~\cite{hosu2024uhdiqa} collated and incorporated, which contains 26,000 high-definition images. We also provide several visual examples of MHRNID, as shown in Figure~\ref{fig:mhrnid}.

\subsubsection{Implementation Details of Demoiréing Models}
For MBCNN~\cite{zheng2020image} and ESDNet-L~\cite{yu2022towards}, we followed the experimental settings from~\cite{yu2022towards} and~\cite{undem}. We trained for 150 epochs on UHDM~\cite{yu2022towards} and FHDMi~\cite{he2020fhde} and 70 epochs on TIP~\cite{sun2018moire}. Additionally, we trained for 50 epochs on the MHRNID dataset.

% \subsection{More Qualitative Comparisons}


\subsection{More Qualitative Comparisons}

\subsubsection{Moiré Image Synthesis}
The visualization results of synthesis moiré images on the MHRNID dataset using Shooting~\cite{shooting}, UnDeM~\cite{undem}, and our UniDemoiré are shown in Figure~\ref{fig:synthesis_compare}. 
The moiré image produced by our UniDemoiré is notably superior to other synthesis methods in terms of diversity and realism. In comparison, the moiré image generated by the Shooting~\cite{shooting} method is excessively distorted, UnDeM's network~\cite{undem} is susceptible to anomalies during image generation, and the moiré pattern dataset provided by MoireSpace~\cite{yang2023doing} is of subpar quality. Additionally, the multiplication strategy results in a darker synthesized image.


\subsubsection{Demoiréing}
Figure~\ref{fig:zero-shot} shows the visualization results of zero-shot demoiréing on UHDM~\cite{yu2022towards}. Additionally, Figures~\ref{fig:cd_fhdmi} and~\ref{fig:cd_tip} illustrate the demoiréing results on FHDMi~\cite{he2020fhde} and TIP~\cite{sun2018moire} using ESDNet-L~\cite{yu2022towards} trained on UHDM~\cite{yu2022towards}. Our method's model effectively removes moiré artifacts and retains high-frequency details, indicating the strong generalization ability of our proposed UniDemoiré.

% \subsubsection{Zero-Shot Demoiréing}
% The visualization results of zero-shot demoiréing results on UHDM and FHDMi are shown in Figure XX, YY. 

% \subsubsection{Cross-Datasets Demoiréing}
% The visualization results of demoiréing results on FHDMi~\cite{he2020fhde} and TIP~\cite{sun2018moire} using ESDNet-L~\cite{yu2022towards} trained on UHDM~\cite{yu2022towards} are shown in Figure ~\ref{fig:cd_fhdmi},~\ref{fig:cd_tip}. The model trained by our method removes moiré artifacts more cleanly and preserves high-frequency details better. This implies that our proposed UniDemoiré has good generalization ability.



\begin{figure}[t]
% \vspace{-3ex}
  \centering
    \includegraphics[width=1.0\linewidth]{Images/limitations.pdf}
\vspace{-1ex}
 \caption{Failure Examples.} 
  \label{fig:limitations}
% \vspace{-3ex}
\end{figure}






\subsection{Runtime Comparisons}
\label{sec:runtime}
Table~\ref{tab:Runtime} shows the comparison of the parameters and the running time of our synthesis module with other methods. 
To ensure fair comparisons, our method and UnDeM use torchinfo for parameter counting, with all methods utilizing 256x256 input images.
Our experimental results indicate that our method, slightly exceeding UnDeM in parameters, achieves a runtime comparable to non-learning algorithms like Shooting and MoireSpace, demonstrating the efficiency of our MIB and TRN.
Furthermore, our model's FLOPs are 5.6266G, significantly lower than UnDeM's 26.7576G, indicating high performance and reduced computational cost.


\subsection{Additional Ablation Study}
\label{sec:sup_ablation}
The results of the additional ablation experiments are in Table~\ref{tab:Exp_ablation_sup}. 
where ``$\mathcal{L}_{per} \rightarrow \mathcal{L}_1$'' denotes replacing the perception loss $\mathcal{L}_{per}$ in the synthesis network with the L1 loss $\mathcal{L}_1$. ``Uformer $\rightarrow$ UNet" denotes switching the entire backbone network of the TRN from Uformer to UNet~\cite{ronneberger2015u}. For a fair comparison, we kept the number of upsampling/downsampling blocks and the base channel in UNet consistent with Uformer, while removing the attention block.



\input{Tables/tbl_exp_ablation_sup}

The results of two sets of ablation experiments on layer blending strategies also show that using only one of them leads to distortion of the synthesis results, which in turn affects the model's generalization ability.
The results of the ``$\mathcal{L}_{per}\rightarrow\mathcal{L}_1$'' show that computing the loss function in this way leads to a degradation of the model performance because moiré patterns can disrupt image structures by generating strip-shaped artifacts. 
The results of the ``$w/o$ CARAFE'' indicate that using the CARAFE upsampling operator~\cite{wang2019carafe} yields better fusion performance than the transposed convolution originally employed by Uformer~\cite{Wang2022Uformer}.
Furthermore, the results from the “Uformer $\rightarrow$ UNet” demonstrate that the LeWin Transformer Block within Uformer is more effective at extracting color features from moiré patterns compared to the original UNet architecture.

% \begin{figure*}[t]
% % \vspace{-3ex}
%   \centering
%     \includegraphics[width=0.7\linewidth]{Images/limitations_3.pdf}
% \vspace{-1ex}
%  \caption{Failure Examples.} 
%   \label{fig:limitations}
% % \vspace{-3ex}
% \end{figure*}

\subsection{Limitations}

In some cases, particularly when the moiré artifacts in the target domain significantly differ from those in the source domain, our solution may struggle to completely remove all artifacts, as Figure~\ref{fig:limitations} shows. However, even in these challenging scenarios, our method tends to perform better at artifact removal compared to the baselines. Our performance can be further refined by generating more diverse moiré patterns and synthesized training data.
In Figure~\ref{fig:limitations}, we show a failure case. When the moiré artifacts in the target domain are too different from those in the source domain, our solution still struggles to produce a completely moiré-free result. However, we still remove the artifacts comparatively better than baselines.
% \yj{Will update based on your fig.}

\begin{figure*}[!p]
  \centering
    \includegraphics[width=1\linewidth]{Images/Synthesis_Result_Compare.pdf}
  \caption{
  Qualitative comparisons of synthesized moire images were obtained using the shooting method, UnDeM, MoireSpace, and our UniDemoiré.
  % Qualitative comparisons of our models with other state-of-the-art methods on the UHDM dataset using ESDNet-L.
  }
  \label{fig:synthesis_compare}
\end{figure*}

\begin{figure*}[!p]
  \centering
    \includegraphics[width=1.0\linewidth]{Images/Zero_Shot_Result.pdf}
  \caption{ Qualitative comparisons of zero-shot evaluation on the UHDM dataset.} 
  \label{fig:zero-shot}
\end{figure*}

\begin{figure*}[!p]
  \centering
    \includegraphics[width=1\linewidth]{Images/FHDMi_Result.pdf}
  \caption{
  Qualitative comparisons of our models with other state-of-the-art methods on the FHDMi dataset.
  % Qualitative comparisons of our models with other state-of-the-art methods on the UHDM dataset using ESDNet-L.
  }
  \label{fig:cd_fhdmi}
\end{figure*}

\begin{figure*}[!p]
  \centering
    \includegraphics[width=1\linewidth]{Images/TIP_Result.pdf}
  \caption{
  Qualitative comparisons of our models with other state-of-the-art methods on the TIP dataset.
  % Qualitative comparisons of our models with other state-of-the-art methods on the TIP dataset using ESDNet-L.
  }
  \label{fig:cd_tip}
\end{figure*}


\newpage
\clearpage
