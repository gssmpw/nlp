%File: anonymous-submission-latex-2025.tex
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in} % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in} % DO NOT CHANGE THIS
%
% These are recommended to typeset algorithms but not required. See the subsubsection on algorithms. Remove them if you don't have algorithms in your paper.
% \usepackage{algorithm}
% \usepackage{algorithmic}
\usepackage[ruled,noline,nofillcomment]{algorithm2e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{float}
\usepackage{subcaption}

% 2.表格内多行
\usepackage{multirow}

% 3.表格内多列
\usepackage{multicol}

% 4.表格内颜色
\usepackage{color, colortbl}
% \definecolor{Gray}{gray}{0.9}
\usepackage{xcolor}

\usepackage{pifont} % support more "x" style
\usepackage{bbding} 
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

%% 自定义命令
\newcommand{\yj}[1]{\textcolor{red}{#1}}        % \yj{x}   批注 1
\newcommand{\red}[1]{\textcolor{red}{#1}}       % \red{x}  会将 x 调整成红色
\newcommand{\blue}[1]{\textcolor{blue}{#1}}     % \blue{x} 会将 x 调整成蓝色
\newcommand{\teal}[1]{\textcolor{teal}{#1}} 
\newcommand{\gray}[1]{\textcolor{gray}{#1}} 
\newcommand{\cyan}[1]{\textcolor{cyan}{#1}} 
\newcommand{\ua}[1]{#1$\uparrow$}               % \u{x} 会在 x 右边加上箭头
\newcommand{\da}[1]{#1$\downarrow$}             % \u{x} 会在 x 右边加下箭头

%% 邮箱
% \newcommand{\emaila}[1]{\texttt{\{#1\}@shanghaitech.edu.cn}}
% \newcommand{\emailb}[1]{\texttt{\{#2\}@cs.hku.hk}}

\usepackage{ragged2e}
\usepackage{blindtext}
% \usepackage{stfloats}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{amssymb}
\usepackage{makecell}
% \usepackage{ulem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
% These are recommended to typeset listings but not required. See the subsubsection on listing. Remove this block if you don't have listings in your paper.
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
%
% Keep the \pdfinfo as shown here. There's no need
% for you to add the /Title and /Author tags.
\pdfinfo{
/TemplateVersion (2025.1)
}

% DISALLOWED PACKAGES
% \usepackage{authblk} -- This package is specifically forbidden
% \usepackage{balance} -- This package is specifically forbidden
% \usepackage{color (if used in text)
% \usepackage{CJK} -- This package is specifically forbidden
% \usepackage{float} -- This package is specifically forbidden
% \usepackage{flushend} -- This package is specifically forbidden
% \usepackage{fontenc} -- This package is specifically forbidden
% \usepackage{fullpage} -- This package is specifically forbidden
% \usepackage{geometry} -- This package is specifically forbidden
% \usepackage{grffile} -- This package is specifically forbidden
% \usepackage{hyperref} -- This package is specifically forbidden
% \usepackage{navigator} -- This package is specifically forbidden
% (or any other package that embeds links such as navigator or hyperref)
% \indentfirst} -- This package is specifically forbidden
% \layout} -- This package is specifically forbidden
% \multicol} -- This package is specifically forbidden
% \nameref} -- This package is specifically forbidden
% \usepackage{savetrees} -- This package is specifically forbidden
% \usepackage{setspace} -- This package is specifically forbidden
% \usepackage{stfloats} -- This package is specifically forbidden
% \usepackage{tabu} -- This package is specifically forbidden
% \usepackage{titlesec} -- This package is specifically forbidden
% \usepackage{tocbibind} -- This package is specifically forbidden
% \usepackage{ulem} -- This package is specifically forbidden
% \usepackage{wrapfig} -- This package is specifically forbidden
% DISALLOWED COMMANDS
% \nocopyright -- Your paper will not be published if you use this command
% \addtolength -- This command may not be used
% \balance -- This command may not be used
% \baselinestretch -- Your paper will not be published if you use this command
% \clearpage -- No page breaks of any kind may be used for the final version of your paper
% \columnsep -- This command may not be used
% \newpage -- No page breaks of any kind may be used for the final version of your paper
% \pagebreak -- No page breaks of any kind may be used for the final version of your paperr
% \pagestyle -- This command may not be used
% \tiny -- This is not an acceptable font size.
% \vspace{- -- No negative value may be used in proximity of a caption, figure, table, section, subsection, subsubsection, or reference
% \vskip{- -- No negative value may be used to alter spacing above or below a caption, figure, table, section, subsection, subsubsection, or reference

\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.

% The file aaai25.sty is the style file for AAAI Press
% proceedings, working notes, and technical reports.
%

% Title

% Your title must be in mixed case, not sentence case.
% That means all verbs (including short verbs like be, is, using,and go),
% nouns, adverbs, adjectives should be capitalized, including both words in hyphenated terms, while
% articles, conjunctions, and prepositions are lower case unless they
% directly follow a colon or long dash
\title{UniDemoiré: Towards Universal Image Demoiréing with Data Generation and Synthesis}
\author{
    %Authors
    % All authors must be in the same font size and format.
    % Written by AAAI Press Staff\textsuperscript{\rm 1}\\
    % AAAI Style Contributions by Pater Patel Schneider,
    % Sunil Issar,\\
    Zemin Yang\textsuperscript{\rm 1,}\equalcontrib,
    Yujing Sun\textsuperscript{\rm 2,}\equalcontrib,
    Xidong Peng\textsuperscript{\rm 1},
    Siu Ming Yiu\textsuperscript{\rm 2},
    Yuexin Ma\textsuperscript{\rm 1,}\thanks{Corresponding author.}
}
\affiliations{
    %Afiliations
    \textsuperscript{\rm 1}ShanghaiTech University \\
    \textsuperscript{\rm 2}The University of Hong Kong\\
    % If you have multiple authors and multiple affiliations
    % use superscripts in text and roman font to identify them.
    % For example,

    % Sunil Issar\textsuperscript{\rm 2},
    % J. Scott Penberthy\textsuperscript{\rm 3},
    % George Ferguson\textsuperscript{\rm 4},
    % Hans Guesgen\textsuperscript{\rm 5}
    % Note that the comma should be placed after the superscript

    % 1101 Pennsylvania Ave, NW Suite 300\\
    % Washington, DC 20004 USA\\
    % email address must be in roman text type, not monospace or sans serif
    % proceedings-questions@aaai.org, 
    \{csyangzm, mayuexin\}@shanghaitech.edu.cn, \{yjsun, smyiu\}@cs.hku.uk
    % \emaila{csyangzm, mayuexin}, \emailb{yjsun, smyiu}
%
% See more examples next
}

%Example, Single Author, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\iffalse
\title{My Publication Title --- Single Author}
\author {
    Author Name
}
\affiliations{
    Affiliation\\
    Affiliation Line 2\\
    name@example.com
}
\fi

\iffalse
%Example, Multiple Authors, ->> remove \iffalse,\fi and place them surrounding AAAI title to use it
\title{My Publication Title --- Multiple Authors}
\author {
    % Authors
    First Author Name\textsuperscript{\rm 1},
    Second Author Name\textsuperscript{\rm 2},
    Third Author Name\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Affiliation 1\\
    \textsuperscript{\rm 2}Affiliation 2\\
    firstAuthor@affiliation1.com, secondAuthor@affilation2.com, thirdAuthor@affiliation1.com
}
\fi


% REMOVE THIS: bibentry
% This is only needed to show inline citations in the guidelines document. You should not need it and can safely delete it.
% \usepackage{bibentry}
% END REMOVE bibentry

\begin{document}

\maketitle

% \input{Contents/0-abstract}
%% Abstract
\begin{abstract}
Image demoiréing poses one of the most formidable challenges in image restoration, primarily due to the unpredictable and anisotropic nature of moiré patterns. Limited by the quantity and diversity of training data, current methods tend to overfit to a single moiré domain, resulting in performance degradation for new domains and restricting their robustness in real-world applications. In this paper, we propose a universal image demoiréing solution, \textbf{UniDemoiré}, which has superior generalization capability. Notably, we propose innovative and effective data generation and synthesis methods that can automatically provide vast high-quality moiré images to train a universal demoiréing model. Our extensive experiments demonstrate the cutting-edge performance and broad potential of our approach for generalized image demoiréing. 
% \textbf{Upon publication of this paper, we will release both our code and dataset.}
% Code and dataset are available at https://github.com/4DVLab/UniDemoire.
% Code: https://github.com/4DVLab/UniDemoire.
% Project page: https://github.com/4DVLab/UniDemoire.
% Project page: https://yizhifengyeyzm.github.io/UniDemoire-page.
\end{abstract}


\begin{links} 
    \link{Code}{https://github.com/4DVLab/UniDemoire} 
\end{links}

%% Introduction
% \input{Contents/1-introduction}
\section{Introduction}
\label{sec:intro}
Digital screens have become essential devices for displaying information in our daily work and life.
However, images captured from screens frequently suffer from frustrating moiré patterns, significantly degrading image quality and hindering content extraction. 
Therefore, it becomes crucial to effectively remove such moiré artifacts to help users obtain high-quality images from their digital imaging devices and to support industries in maintaining high-standard product visual presentation and digital archiving.
However, moiré patterns are characterized as anisotropic and multi-scale, as well as involving considerable shape variations and color distortions~\cite{amidror2009theory}. Such traits are seldom seen in other types of artifacts, like noise, rain streaks, fog, blurring, etc., posing a significant challenge for even the most advanced image restoration methods~\cite{luo2023refusion,zhu2023denoising,fei2023generative}. 

Hence, many methods have been proposed to tackle the problem of demoiréing in recent years~\cite{sun2018moire,liu2020wavelet,luo2020deep,he2019mop,he2020fhde,wang2023coarse,yue2022recaptured,yu2022towards}. Nevertheless, the effectiveness of such supervised methods heavily depends on the volume of training data, consisting of pairs of moiré images and their clean counterparts. As we know, collecting such data is a daunting task and it requires precise calibration between natural images and moiré patterns. The limitations of the data lead to the limitations of the methods, resulting in poor generalization of the network model, which performs poorly on the data containing new moiré patterns or new natural images. In order to expand the quantity and diversity of the training data in a convenient way, some methods have started to explore the synthesis of moiré patterns. LCDMoiré~\cite{yuan2019aim} deigns handcraft mathematical models. However, it could not represent complex features of moiré patterns and leads to a substantial discrepancy between the synthetic data and actual moiré images. To enhance realism, recent studies~\cite{cyclic, undem} extract moiré patterns from existing real images and combine them with clean images for data synthesis. Nevertheless, these methods do not escape from the moiré domains of the existing training data, bringing limited performance improvement on new moiré domains. To develop a universal model for image demoiréing with greater generalization capability and practicality, two critical challenges emerge: \textit{how to generate a vast amount of diverse data, and how to ensure the authenticity of the data?}

\begin{figure*}[ht]
  \centering   
  \includegraphics[width=1.0\linewidth]{Images/Pipeline.pdf}
  \caption{The workflow of our proposed UniDemoiré.}
  \label{fig:pipeline}
\end{figure*}

To address the above challenges, we propose a universal image demoiréing solution, \textbf{UniDemoiré}, capable of generating a vast amount of realistic-looking training data to enhance the generalization capabilities of the image demoiréing model, as Figure~\ref{fig:pipeline} shows. 
First, inspired by the fact that the moiré pattern is unrelated to the content of the image, we introduce a novel, large-scale \textbf{Moiré Pattern Dataset} by capturing moiré patterns against a plain white background. Unlike previous moiré datasets that capture nature images with moiré, our pure moiré patterns can be applied to arbitrary nature images to scale up the data domain automatically. Moreover, our dataset does not need calibrations between the moiré image and the clean image, which can avoid the effect of calibration errors and facilitate the learning process of the model. In particular, our dataset introduces more pattern diversity by considering various previously overlooked factors~\cite{yang2023doing}, including zooming rate, CMOS technology, pixel size, and panel types. Second, building on this real-captured moiré pattern dataset, we propose a diffusion model-based \textbf{Moiré Pattern Generation} method to further increase the diversity of moiré patterns. Specifically, we implement a multi-scale cropping strategy to accommodate different input image sizes and an effective data filtering strategy to ensure the quality of training data for the diffusion model. 
Third, we propose a \textbf{Moiré Image Synthesis} method to create a sufficient amount of diverse and realistic-looking moiré images by blending the generated moiré patterns with clean natural images. In particular, to improve the authenticity of our synthesized data, we develop an effective learnable network and three effective losses to closely mimic the real captured moiré images in terms of color and brightness. Finally, our synthesized abundant moiré images serve to train an \textbf{Image Demoiréing Model} that achieves superior performance and promising generalization capabilities for zero-shot image demoiréing and cross-domain evaluations.

Our contribution can be summarized as follows:
\begin{itemize}
    \item We propose a universal demoiréing solution, which substantially enlarges the knowledge domain and improves the generalization capability of demoiréing models.
    \item We collect a large-scale and high-resolution moiré pattern dataset and develop an effective moiré pattern generator to further increase the diversity of moiré patterns.
    \item We present a novel moiré image synthesis approach, providing a large amount of realistic-looking and high-quality moiré image samples, facilitating the training of a universal image demoiréing model.    
\end{itemize}


%% Related Work
\section{Related Work}
\label{sec:related-work} 

\subsection{Image Restoration and Demoiréing}
The inherent complexity of moiré patterns presents a unique challenge compared to other artifacts such as noise~\cite{xing2021end}, haze~\cite{li2021dehazeflow}, blur~\cite{lee2021iterative}, multiple artifacts in one go~\cite{luo2023refusion,zhu2023denoising,fei2023generative, zhang2023all}, etc. Consequently, these methods may not effectively solve the moiré issue. Current mainstream methods for image demoreing are learning based~\cite{sun2018moire,liu2020wavelet,luo2020deep,he2019mop,he2020fhde,niu2023progressive,wang2023coarse, yue2022recaptured, liu2024video,zheng2020image,zheng2021learning,yu2022towards}, greatly outperforming early handcraft feature based approaches~\cite{ sun2014scanned,liu2015moire,yang2017textured,yang2017demoireing}. However, they exhibit poor generalization capability due to insufficient diverse and realistic training data, and researchers have thus begun exploring the potential of synthesized data.


% Datasets
\begin{table*}[ht]
\centering
\small
\setlength{\tabcolsep}{2.37mm}
\scalebox{0.9}{
\begin{tabular}{cc|ccccccc}
\toprule[1.25pt]
\multicolumn{2}{c|}{Datasets} & \multirow{2.5}{*}{Avg. Resolution} & \multirow{2.5}{*}{Size} & \multicolumn{5}{c}{Capture settings}   \\ 
\cmidrule(lr){1-2} \cmidrule(lr){5-9} 
Type & Name &       &      & Phone  & Screen & Multi-zooming rate & Multi-camera / CMOS  & Screen Panel \\ 
\midrule
\midrule
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}} Moiré Image \\ Dataset\end{tabular}}  
& TIP2018(R) & 256 $\times$ 256   & 135000 & 3     & 3      & \ding{55}(1x-only) & \ding{55}(Main-only) & IPS-only     \\
& FHDMi(R)     & 1024 $\times$ 1024 & 12000  & 3     & 2      & \ding{55}(1x-only) & \ding{55}(Main-only) & IPS-only     \\
& UHDM(R)   & 4328 $\times$ 3248 & 5000   & 3     & 3      & \ding{55}(1x-only) & \ding{55}(Main-only) & IPS-only     \\ 
& LCDMoiré(S) & 1024 $\times$ 1024 & 10200  & -     & -      & -          & -            & -            \\ 
\midrule
\multirow{2.5}{*}{\begin{tabular}[c]{@{}c@{}} Moiré Pattern \\ Dataset\end{tabular}} 
& MoireSpace(R)  & 2160 $\times$ 1286 & 18147  & 3     & 3      & \ding{55}(1x-only) & \ding{55}(Main-only) & IPS-only     \\ 
\cmidrule{2-9}
& \textbf{Ours(R)} & \textbf{3840} $\times$ \textbf{2160} & \textbf{150000} & \textbf{6} & \textbf{6} & \textbf{\checkmark(1x,2x,3x)} & \textbf{\checkmark(Main,Telephoto)} & \textbf{IPS, SVA}  \\ 
\bottomrule[1.25pt]
\end{tabular}
}
\caption{Comparisons of different moiré datasets. The ``R'' denotes the real dataset, and the ``S'' denotes the synthetic dataset.}
\label{tab:datasets-overview}
\end{table*}

\subsection{Moiré Image Synthesis}
An important category focuses on extracting moiré patterns from existing moiré images. Cyclic~\cite{cyclic} and UnDeM~\cite{undem} utilized GAN-based networks to generate moiré images from unpaired real moiré image datasets, resembling moiré patterns found in moiré images while retaining details from moiré-free images. However, they are unstable and constrained by the moiré patterns present in the real image datasets. 
Another category directly simulates moiré patterns on natural images. Shooting~\cite{shooting} simulated the interference of image processing to produce moiré patterns on natural images while Yang et al.~\shortcite{yang2023doing} collected background-independent moiré patterns and then superimposes the natural image with the collected pattern to synthesize moiré images. 
Unfortunately, due to the real-to-synthetic discrepancy, their model performance is limited in real-world applications. 
In contrast, our solution can produce realistic-looking and diverse data to greatly improve demoiréing models' performance.

\subsection{Moiré Dataset} 
TIP18~\cite{sun2018moire}, FHDMi~\cite{he2020fhde}, UHDM~\cite{yu2022towards} are the most widely-used real-world moiré image dataset with increased resolutions 256, 1080P, and 4K, respectively. To lessen the burden of huge human efforts, a synthetic moiré image dataset LCDMoiré~\shortcite{yuan2019aim} has been generated through shooting simulation.
% However, such a synthetic dataset often fails to accurately mimic the real imaging process, resulting in the consequence that demoiréing models trained on synthetic data struggle in real-world situations. 
However, synthetic datasets often fail to accurately replicate real imaging processes, making it difficult for demoiréing models trained on them to perform well in real-world situations.
More recently, MoireSpace~\cite{yang2023doing} collects background-independent moiré pattern data for a different task, moiré detection. Inspired by it, we propose to collect a real moiré pattern dataset for image demoiréing. Taking inspiration from this effort, we introduce a real moiré pattern dataset specifically tailored for image demoiréing. Comparatively, our dataset boasts a larger volume and greater diversity of data.


%% Methods
\section{Method}

\subsection{Overview} 
The generalization ability of SOTA demoiréing models is greatly limited by the scarcity of data. Therefore, we mainly face two challenges to obtain a universal model with improved generalization capability: To obtain a vast amount of 1) diverse and 2) realistic-looking moiré data.
Notice that traditional moiré image datasets contain real data, but continuously expanding their size to involve more diversity is extremely time-consuming and impractical. 
While current synthesized datasets/methods struggle to synthesize realistic-looking moiré images.
Hence, to tackle these challenges, we introduce a universal solution, UniDemoiré (Figure~\ref{fig:pipeline}). The data diversity challenge is solved by collecting a more diverse moiré pattern dataset and presenting a moiré pattern generator to increase further pattern variations. 
Meanwhile, the data realistic-looking challenge is undertaken by a moiré image synthesis module.  
Finally, our solution can produce realistic-looking moiré images of sufficient diversity, substantially enhancing the zero-shot and cross-domain performance of demoiréing models.

\subsection{Moiré Pattern Dataset}
\label{subsec: Moiré_Patterns_Collection}

% \paragraph{Moiré Pattern Dataset V.S. Moiré Image Dataset}
% \textit{Moiré Pattern Dataset V.S. Moiré Image Dataset} \quad 
The traditional demoiréing datasets~\cite{sun2018moire,he2020fhde,yu2022towards} typically exhibit a 1-1 correspondence, 1 clean image corresponds to only 1 moiré-contaminated image. However, in the real world, an image may be affected by various moiré patterns.
Meanwhile, aligning moiré images with clean images often introduces errors because of the non-linear distortions and moiré artifacts within cameras.
Therefore, we propose to collect a moiré pattern dataset rather than a moiré image dataset, with no need for image alignment and can easily synthesize multiple moiré counterparts of a single natural image. 
The collection of such a dataset is inspired by MoireSpace, which is designed to address the problem of detecting the presence of moiré rather than to eliminate moiré artifacts.  

\paragraph{Capturing Process}
% \textit{Capturing Process} \quad  
We capture videos of real-world moiré patterns on a pure white screen with a mobile phone to minimize color distortion in the moiré patterns. After recording, frames are uniformly extracted from each video to constitute our dataset. The setup is shown in Figure~\ref{fig:data_collection}-left.

\paragraph{Data Diversity}
% \textit{Data Diversity} \quad 
To enhance pattern diversity, we build our dataset by considering additional factors that influence moiré formation, which were overlooked in previous moiré datasets, including zooming rate, camera types, CMOS, and screen panel types. Besides, we doubled the number of mobile devices and display screens compared to existing datasets.
% ~\cite{sun2018moire,he2020fhde,yu2022towards}
A detailed comparison of ours and others is shown in Table~\ref{tab:datasets-overview}.
In summary, our dataset showcases an expanded size, 150000 moiré patterns, in standard 4K resolution with increased diversity. More dataset details are in the appendix. 

\begin{figure}[t]
  \centering
    \includegraphics[width=1.0\linewidth]{Images/Data_Collection.pdf}
  \caption{ Data collection setup (left), and examples of moiré patterns in our dataset captured at different zoom rates and screen panel (middle), and our generated patterns (right). } 
  \label{fig:data_collection}
  % \vspace{-1.5ex}
\end{figure}

\subsection{Moiré Pattern Generation}
\label{subsec: Moiré_Pattern_Generator}
 
Although we have collected a large scale of diverse data, it cannot encompass all conceivable moiré patterns.   
Inspired by recent diffusion models, which have been successfully trained towards diverse image generation in many tasks~\cite{dhariwal2021diffusionmodelsbeatgans}, we propose to use diffusion models to further sample more diverse moiré patterns by sufficiently learning the structural, textural, and color representations of real moiré patterns. 
In this stage, we propose a multi-scale cropping strategy and a colorfulness-sharpness selection strategy to filter high-quality real data. Then we learn the distribution of real moiré patterns in the latent space to generate diverse patterns (Figure~\ref{fig:data_collection}-right).



\paragraph{Multi-Scale Cropping}
Demoiréing models typically employ image patches cropped from the entire image for training. However, given the significant variation of image size in different demoiréing datasets, the scale of content in cropped image patches of the same size also varies greatly. 
Hence, to simulate this process and enhance the diversity of the training data, we perform multi-scale cropping (Figure~\ref{fig:generation} up). 
In particular, 4k images are resized to different sizes, from which we extract and randomly select image patches of uniform size as training data.
In this way, the patches extracted from low and high-resolution images emphasize overall patterns and finer details, respectively.

\paragraph{Colorfulness-Sharpness Selection}
We notice that certain patches involve visually invisible patterns (with a ``\ding{55}'' mark in Figure~\ref{fig:generation}). They potentially confuse the generator during training, aiming to generate moiré pattern images rather than to reproduce plain white images. Hence, we filter out such patches based on colorfulness and sharpness. 
As depicted in Figure~\ref{fig:generation} lower-right, an increased sharpness value indicates more visible moiré patterns, while an increased colorfulness value signifies patterns with richer colors.
The sharpness metric is calculated as the standard deviation of grayscaled input image processed with an edge filter, while the colorfulness metric is calculated as the average standard deviations of A and B channels in image LAB color space.


\subsubsection{Learning Moiré Patterns in the Latent Space} 
As shown in Figure~\ref{fig:data_collection}(middle), plenty of pixels in the moiré pattern appear pure white. 
This leads to a polarization in the pixel distribution of the moiré pattern images, where informative data is concentrated in a few pixels with high values while the rest contains little information.
Based on this observation, we choose to compress the moiré pattern into the latent space through an autoencoder for a more compact and efficient representation of its structural, textural, and color information. For better stability and controllability, we utilize the Latent Diffusion Model~\cite{Rombach2022LDM} to effectively model the complex distribution of the moiré pattern in the latent space.
Examples of generated moiré patterns are shown in Figure~\ref{fig:data_collection} right.
More examples are in the appendix.




\subsection{Moiré Image Synthesis}
\label{subsec: Moiré_Image_Synthesis}

Via data collection and generation, we obtain a vast number of diverse moiré patterns. 
Then, we need to composite moiré patterns with clean images $I_{n}$ to form moiré images. To make the synthesized images realistic-looking,
We first create handcraft rules to produce initial moiré images in the Moiré Image Blending (MIB) module, then design a Tone Refinement Network (TRN) to further faithfully replicate the color and brightness variations observed in real scenes that cannot be fully formulated in those handcraft rules. 
The proposed synthesis process is illustrated in Figure~\ref{fig:systhesis}.



\begin{figure}[t]
  \centering
    \includegraphics[width=1.0\linewidth]{Images/Data_Prepocessing.pdf}
  \caption{Data preprocessing for moiré pattern generation. }
  \label{fig:generation}
   % \vspace{-1.5ex}
\end{figure}


\begin{figure*}[ht]
  \centering
    \includegraphics[width=0.95\linewidth]{Images/Moire_Image_Synthesis.pdf}
  \caption{Overview of the Moiré Image Synthesis stage (a). It involves a Moiré Image Blending module (b) for initial moiré image synthesis and a Tone Refinement Network (c) to refine for more realistic results.}
  \label{fig:systhesis}
  % \vspace{-1.5ex}
\end{figure*}


\subsubsection{Moiré Image Blending}
We blend the clean natural image $I_{n}$ (background layer) with the moiré pattern $I_{mp}$ (foreground layer) to form our initial moiré image $I_{mib}$. Notice that MoireSpace~\cite{yang2023doing} synthesized their moiré image $I'_{sm} $via a Multiply Strategy $M(\cdot, \cdot)$,
\begin{equation}
     I'_{sm} = M(I_{mp}, I_{n}) = I_{mp} \odot I_{n},
 \end{equation}
where ``$\odot$'' denotes element-wise multiplication.
However, the result produced by MoireSpace~\cite{yang2023doing} tends to be dark and cannot replicate the desired contrast and color distortion, as shown in Figure~\ref{fig:synthesis_result}. 
% More results are in the appendix. 
Therefore, we design the following handcraft rules to make the blending more realistic (Figure~\ref{fig:systhesis}b). 
we first incorporate an additional blending strategy, Grain Merge ~\cite{LayerModes} $G(\cdot, \cdot)$. Such a brighter strategy can balance the darker result from $M(\cdot, \cdot)$: 
\begin{equation}
    G(I_{mp}, I_{n}) =  I_{mp} + I_{n} - 0.5.
\end{equation} 
Then, we incorporate transparency of the layers using alpha blending~\cite{Porter_Duff_1984} to obtain $I_{comp}^{M}$ and $I_{comp}^{G}$ :
\begin{align}    
I_{comp}^{M} &= r_m \cdot M(I_{mp}, I_{n}) + [1-r_m] \cdot I_{n},\\
I_{comp}^{G} &= r_g \cdot G(I_{mp}, I_{n}) + [1-r_g] \cdot I_{n}.
\end{align}
where $r_m$ and $r_g$ represent the composition ratio parameter of foreground layer $M(I_{mp}, I_{n})$ and $G(I_{mp}, I_{n})$:
\begin{equation}
    r_x = \frac{op_{x}}{op_{x}+(1-op_{x})\cdot op_{n}}, x \in \{m,g\}.
\end{equation}
where $op_m$, $op_g$ represent the opacity of the output layers from the Multiply and the Grain Merge strategies, and $op_{n}$ represent the opacity of the background layer $I_{n}$.
Finally, we perform a weighted (weight $\omega_m$, $\omega_g$) combination of $I_{comp}^{M}$ and $I_{comp}^{G}$ to obtain $I_{mib}$ :
\begin{equation}    
I_{mib} = \omega_m \cdot I_{comp}^{M} + \omega_g \cdot I_{comp}^{G}.
\end{equation}
A visual comparison of MoireSpace result $I'_{sm}$, and our $I_{mib}$ is shown in Figure~\ref{fig:synthesis_result}, showing the superior of $I_{mib}$ over $I'_{sm}$.
Please refer to the appendix for more visual results.



\subsubsection{Tone Refinement Network} 
Though the moiré image blending module creates a preliminary moiré image $I_{mib}$, such a synthesized result based on handcraft rules still struggles to replicate accurate color and brightness changes. 
Comparatively, networks are more powerful in capturing such unknown changes and distortion by progressive learning.
Hence, we present a learnable refinement network to synthesize more realistic results.

The Tone Refinement Network (TRN) proposed here is built on a U-shaped transformer backbone~\cite{Wang2022Uformer} incorporating multiple refine blocks, illustrated in Figure~\ref{fig:systhesis} (c). 
It takes $I_{mib}$ as input, applies pixel-wise tone adjustment to $I_{mib}$, and minimizes the tone gap between the output $I_{trn}$ and the given real moiré images $I_{rm}$. 
To be clear,
TRN firstly applies a 3$\times$3 convolutional layer with LeakyReLU to extract tone features $F_{mib}^{(0)}$, $F_{rm}^{(0)}$. 
Next, the feature maps $F_{mib}^{(0)}$ and $F_{rm}^{(0)}$ are passed through $N$ encoder phases and $N$ decoder phases with skip connections. 
Each phase contains a refine block to capture long-range dependencies, benefiting from the self-attention in Transformer. 

Inspired by research in style transfer and domain generalization~\cite{Ulyanov2016IN, Huang2017Arbitrary, Zhou2021Mixstyle}, we design a tone feature fusion block within each refine block to better fuse the tone feature statistics between $I_{mib}$ and corresponding $I_{rm}$. 
It mixes the feature statistics of two instances with a random convex weight. 
As illustrated in Figure~\ref{fig:systhesis} (c), the computations inside a fusion block module in the $k$-th refine block can be summarized into two steps. 
First, given two sets of feature maps $f^{(k)}$ and $f_r^{(k)}$ for $I_{mib}$ and $I_{rm}$, the fusion block generates a mixture of feature statistics, 
\begin{align}
    \gamma_{mix} &= \lambda \cdot \sigma(f^{(k)}) + (1-\lambda) \cdot \sigma(f^{(k)}_r), \\
    \beta_{mix}  &= \lambda \cdot \mu(f^{(k)})    + (1-\lambda) \cdot \mu(f^{(k)}_r).
\end{align}
where $\mu$ and $\sigma$ represent the mean and variance of feature maps, while $\lambda$ is a random weight sampled from the beta distribution, $\lambda \in \text{Beta}(\alpha, \alpha)$ with $\alpha \in (0, \infty)$ being a hyper-parameter.
Then, the mixture of feature statistics is applied to the tone-normalized $F_{mib}^{(k+1)}$:
\begin{equation}
    F_{mib}^{(k+1)} = \gamma_{mix} \odot \frac{f^{(k)}-\mu(f^{(k)})}{\sigma(f^{(k)})} + \beta_{mix}.
\end{equation}
The fusion block can effectively utilize the moiré feature information of $I_{rm}$ and greatly helps reduce the moiré domain gap between the final synthesized image $I_{trn}$ and real moiré image $I_{rm}$, which is one significant innovation.
After the $N$ decoder stages, we apply a 3$\times$3 convolution layer on feature maps $F_{mib}^{(2N)}$ to obtain a tone refinement matrix $M_{trn}$. 
Finally, the synthetic image is obtained by $I_{trn}=I_{mib} \odot M_{trn}$ after color normalization, where ``$\odot$'' represents element-wise multiplication. 
Notice that the fusion block is solely utilized in the training phase, and $I_{rm}$ is exclusively fed into the network during training. 
% More details of TRN can be found in the appendix.
Figure~\ref{fig:synthesis_result} compares the initial blending result $I_{mib}$ with the final synthesized result $I_{trn}$. Please refer to the appendix for more results.

\begin{figure}[t]
  \centering
    \includegraphics[width=1.0\linewidth]{Images/Synthesis_Result.pdf}
  \caption{Visualization of our intermediate synthetic results.  
  % Final synthesis results $I_{trn}$ best resemble real moiré images.
  } 
  \label{fig:synthesis_result}
  % \vspace{-3ex}
\end{figure}


 





% \input{Tables/tbl_exp_multi_datasets} %% Zero-shot 实验
\begin{table*}[ht]
\small
\centering
\setlength{\tabcolsep}{2.26mm}
\scalebox{0.9}{
\begin{tabular}{cc|ccccc|ccccc}
\toprule[1.25pt]
\multirow{2.5}{*}{\makecell{Test \\ Dataset}} & \multirow{2.5}{*}{Metric} & \multicolumn{5}{c|}{Demoiréing Network: \textbf{MBCNN}}  & \multicolumn{5}{c}{Demoiréing Network:   \textbf{ESDNet-L}}  \\ 
 \cmidrule(lr){3-7}  \cmidrule(lr){8-12}
&           & Shooting & UnDeM$^\dagger$ & UnDeM$^\ddagger$ & MoireSpace  & Ours & Shooting & UnDeM$^\dagger$ & UnDeM$^\ddagger$ & MoireSpace & Ours \\ 
\midrule
\midrule
 \multirow{3}{*}{UHDM}  &\ua{PSNR } & 9.2284  & 13.4256 & 14.5237 & 14.7826 & \textbf{17.9162} & 10.2568 & 15.2269 & 15.2947 & 14.7989 & \textbf{17.2524} \\
                        &\ua{SSIM } & 0.5180  & 0.3973  & 0.4425  & 0.4724  & \textbf{0.6280}  & 0.5664  & 0.5873  & 0.5777  & 0.4859  & \textbf{0.6454}  \\
                        &\da{LPIPS }& 0.6664  & 0.6489  & 0.6332  & 0.5568  & \textbf{0.4162}  & 0.5130  & 0.4190  & 0.4241  & 0.5254  & \textbf{0.3238}  \\ 
\cmidrule{1-12} 
 \multirow{3}{*}{FHDMi} &\ua{PSNR } & 10.6750  & 17.8355 & 18.1652 & 18.5523 & \textbf{19.0094} & 11.6022 & 18.4335 & 18.5390 & 18.0763 & \textbf{19.8128} \\
                        &\ua{SSIM } & 0.4478  & 0.6802  & 0.6999  & 0.7094  & \textbf{0.7137}  & 0.5425  & 0.6900  & 0.6812  & 0.7189  & \textbf{0.7319}  \\
                        &\da{LPIPS }& 0.5978  & 0.2606  & 0.2472  & 0.2742  & \textbf{0.2390}  & 0.4515  & 0.2877  & 0.2986  & 0.2616  & \textbf{0.2134}  \\ 
\bottomrule[1.25pt]
\end{tabular}
}
\caption{Quantitative results of zero-shot demoiréing trained with synthesized data only. 
``$\dagger$'' indicates UnDem uses moiré patterns retrieved from real data in TIP for inference.  ``$\ddagger$''  indicates UnDem uses our generated moiré pattern for inference.
}
\label{tab:Exp_multi_datasets}
\end{table*}


% \input{Tables/tbl_exp_cross_datasets} %% Cross-dataset 实验
\begin{table*}[t]
\small
\centering
\setlength{\tabcolsep}{1.89mm}
\scalebox{0.9}{
\begin{tabular}{ccc|ccccc|ccccc}
\toprule[1.25pt]    % 顶部粗线

\multicolumn{2}{c}{Cross Dataset}& \multirow{2.5}{*}{Metric} & \multicolumn{5}{c|}{Demoiréing Network: \textbf{MBCNN}}  & \multicolumn{5}{c}{Demoiréing Network:   \textbf{ESDNet-L}}  \\ 
\cmidrule(lr){1-2} \cmidrule(lr){4-8}  \cmidrule(lr){9-13}

Source & Target &        & Baseline & Shooting & UnDeM & MoireSpace & Ours  & Baseline & Shooting & UnDeM & MoireSpace   & Ours   \\ 
\midrule
\midrule
\multirow{6.5}{*}{UHDM}    
& \multirow{3}{*}{FHDMi} &\ua{PSNR } & 19.3848 & 19.2032 & 19.4676 & 19.4531 & \textbf{19.8625} & 20.3422 & 20.2407 & 20.4014 & 20.2806 & \textbf{20.7543} \\
&                        &\ua{SSIM } & 0.7436  & 0.7459  & 0.7455  & 0.7496  & \textbf{0.7525}  & 0.7599  & 0.7579  & 0.7510  & 0.7603  & \textbf{0.7653}  \\
&                        &\da{LPIPS }& 0.3002  & 0.2975  & 0.2964  & 0.2993  & \textbf{0.2842}  & 0.2525  & 0.2632  & 0.2509  & 0.2324  & \textbf{0.2136}  \\ 
\cmidrule{2-13} 
& \multirow{3}{*}{TIP}   &\ua{PSNR } & 17.8107 & 18.3730 & 18.6674 & 18.9214 & \textbf{19.3922} & 18.8040 & 18.4543 & 19.3545 & 19.3964 & \textbf{19.5009} \\
&                        &\ua{SSIM } & 0.6627  & 0.6888  & 0.6911  & 0.6996  & \textbf{0.7022}  & 0.6921  & 0.6930  & 0.6998  & 0.7111  & \textbf{0.7149}  \\
&                        &\da{LPIPS }& 0.3580  & 0.3886  & 0.3909  & 0.3829  & \textbf{0.3781}  & 0.3524  & 0.3849  & 0.3601  & 0.3522  & \textbf{0.3495}  \\ 
\midrule
\multirow{6.5}{*}{FHDMi}   
& \multirow{3}{*}{UHDM}  &\ua{PSNR } & 17.1331 & 17.5326 & 17.4870 & 17.6050 & \textbf{18.7931} & 18.0049 & 18.4189 & 17.9574 & 17.9751 & \textbf{18.9240} \\
&                        &\ua{SSIM } & 0.6159  & 0.6334  & 0.6331  & 0.6642  & \textbf{0.7186}  & 0.5755  & 0.5780  & 0.5857  & 0.5548  & \textbf{0.6658}  \\
&                        &\da{LPIPS }& 0.4470  & 0.4350  & 0.4285  & 0.4020  & \textbf{0.3508}  & 0.4420  & 0.4279  & 0.4460  & 0.4579  & \textbf{0.3405}  \\ 
\cmidrule{2-13} 
& \multirow{3}{*}{TIP}   &\ua{PSNR } & 20.2161 & 20.7793 & 20.8261 & 20.1194 & \textbf{21.0694} & 20.6647 & 20.8678 & 20.4663 & 20.8107 & \textbf{21.5786} \\
&                        &\ua{SSIM } & 0.7340  & 0.7304  & 0.7381  & 0.7347  & \textbf{0.7494}  & 0.7504  & 0.7606  & 0.7278  & 0.7582  & \textbf{0.7668}  \\
&                        &\da{LPIPS }& 0.2979  & 0.2884  & 0.2891  & 0.2961  & \textbf{0.2832}  & 0.2459  & 0.2450  & 0.2998  & 0.2468  & \textbf{0.2310}  \\ 
\midrule
\multirow{6.5}{*}{TIP}     
& \multirow{3}{*}{UHDM}  &\ua{PSNR } & 17.3409 & 17.4011 & 17.4407 & 17.4987 & \textbf{18.2937} & 17.4332 & 16.1836 & 16.8402 & 16.6296 & \textbf{18.4978} \\
&                        &\ua{SSIM } & 0.6144  & 0.6062  & 0.6066  & 0.6059  & \textbf{0.6913}  & 0.5523  & 0.5511  & 0.5692  & 0.5748  & \textbf{0.6866}  \\
&                        &\da{LPIPS }& 0.4726  & 0.4487  & 0.4473  & 0.4412  & \textbf{0.3990}  & 0.4987  & 0.4723  & 0.4532  & 0.4387  & \textbf{0.3231}  \\ 
\cmidrule{2-13} 
& \multirow{3}{*}{FHDMi} &\ua{PSNR } & 18.9458 & 19.2731 & 19.0336 & 19.1101 & \textbf{20.1053} & 19.2368 & 18.1936 & 19.2112 & 18.8385 & \textbf{19.9971} \\
&                        &\ua{SSIM } & 0.7369  & 0.7399  & 0.7215  & 0.7321  & \textbf{0.7725}  & 0.7354  & 0.7297  & 0.7499  & 0.7389  & \textbf{0.7580}  \\
&                        &\da{LPIPS }& 0.2494  & 0.2447  & 0.2452  & 0.2382  & \textbf{0.2315}  & 0.2316  & 0.2320  & 0.2130  & 0.2228  & \textbf{0.1915}  \\ 
\bottomrule[1.25pt]  % 底部粗线
\end{tabular}
}
\caption{Quantitative results of cross-dataset evaluations.}
% \vspace{-2ex}
\label{tab:Exp_cross_datasets}
\end{table*}


 % \vspace{-1.5ex}
\subsubsection{Loss Functions}
The tone adjustment network aims to adjust the overall color tone and contrast of $I_{trn}$ in a way that it resembles $I_{rm}$ without affecting moiré pattern $I_{mp}$. 

First, moiré patterns can disrupt image structures by generating strip-shaped artifacts~\cite{yu2022towards}. Therefore, comparing two moiré images directly in pixel space is less effective. 
Thus, we adopt the perceptual loss~\cite{johnson2016perceptual} $\mathcal{L}_{per}$ to 
%ensure the moiré artifacts of the $I_{trn}$ are aligned with the $I_{mib}$ by 
optimize the $\mathcal{L}_1$ distance between the extracted content features of $I_{mib}$ and $I_{trn}$:
\begin{small}
\begin{equation}
\mathcal{L}_{per}(I_{trn}, I_{mib})=\sum_{j=1}^{N_L}\frac{\left\|\phi_j(I_{trn})-\phi_j(I_{mib})\right\|_1}{C_j H_j W_j}, 
\end{equation}
\end{small}
\normalsize 
where $\phi_j(I)$ is the activations of the $j$-th layer of the VGG16 network~\cite{simonyan2014very}, and $N_L$ represents the number of convolutional layers in VGG16.

In addition, to effectively tune the tone of $I_{trn}$, we take advantage of color differentiable RGB-uv histogram features $H(I_{trn})$ and $H(I_{rm})$ in log chromaticity space, inspired by color constancy method~\cite{barron2015ccc, Afifi2021histogan}, 
%and transform $I_{trn}$ and $I_{rm}$ into color differentiable histogram features $H(I_{trn})$ and $H(I_{rm})$ in log chromaticity space, 
as shown in Figure~\ref{fig:systhesis} (a). Such RGB-uv histograms have proven efficient in color transfer tasks~\cite{Eibenberger2012log}. 
We optimize color loss 
using the differentiable Hellinger distances 
%Then, we follow HistoGAN~\cite{Afifi2021histogan} to optimize the RGB-uv histogram features $H(I_{trn})$ and $H(I_{rm})$ to obtain the color loss using the differentiable Hellinger distances:
\begin{equation}
    \mathcal{L}_{color}\left(I_{trn}, I_{rm}\right)=\left\|H(I_{trn})^{1 / 2}-H(I_{rm})^{1 / 2}\right\|_2,
\end{equation}
where $\left\| \cdot \right\|_2$ is the standard Euclidean norm and $\cdot ^{1/2}$ is an element-wise square root. 
%More details of color loss and RGB-uv histogram features can be found in the appendix.

Finally, 
%following prior work on style transfer~\cite{johnson2016perceptual}, and image blending~\cite{Zhang2020deepimageblend}, 
we use total variation regularizer $\mathcal{L}_{tv}$ to remove unwanted details while encouraging spatial smoothness: %in the output image $I_{trn}$:
\begin{small}
\begin{equation}
\mathcal{L}_{tv}(I_{trn})=\sum_{i=1}^H \sum_{j=1}^W\left|I_{trn}^{i+1, j}-I_{trn}^{i, j}\right|+\left|I_{trn}^{i, j+1}-I_{trn}^{i, j}\right|
\end{equation}
\end{small}
\normalsize


Total loss $\mathcal{L}$ is then defined as a weighted compound of $\mathcal{L}_{per}$, $\mathcal{L}_{color}$ and $\mathcal{L}_{tv}$:
\begin{equation}
    \mathcal{L} = \lambda_{per}\mathcal{L}_{per} + \lambda_{color}\mathcal{L}_{color} + \lambda_{tv}\mathcal{L}_{tv}.
\end{equation}

\subsection{Image Demoiréing}
\label{subsec: Image_Dmoiréing}
Our contributions mainly lie in the above three stages. %establishing a novel moiré pattern dataset, developing a moiré pattern generator, and proposing the moiré image synthesis strategy.
Then, diverse and realistic-looking data synthesized by our solution can be seamlessly integrated with demoiréing models to improve their performance.


%% Experiments
% \input{Contents/4-experiments}
\section{Experiments}
\subsection{Experimental Setups} 
For all compared methods, we used their released code. Thorough implementation details are in the appendix. 

\begin{figure}[t]
  \centering
\includegraphics[width=1.0\linewidth]{Images/Demoire_Result.pdf}
  \caption{Comparisons of demoiréing results.} 
  \label{fig:demoiré_result}
  % \vspace{-3ex}
\end{figure}

\subsubsection{Datasets and Metrics.}
\textbf{\emph{1) }Moiré Pattern Dataset} is used to train our moiré pattern generator. 
\textbf{\emph{2) }Real Moiré Image Dataset}, TIP~\cite{sun2018moire}, FHDMi~\cite{he2020fhde}, and UHDM~\cite{yu2022towards}, are used to demonstrate our ability in restoring real moiré images. 
\textbf{\emph{3) }Evaluation Metrics.}  We evaluate demoiréing performance on the Peak-Signal-to-Noise Ratio (PSNR), Structural Similarity Index (SSIM)~\cite{wang2004image}, and LPIPS~\cite{zhang2018unreasonable}.

\subsubsection{Comparison Methods}
We compare UniDemoiré to the SOTA synthesis methods in 3 current modalities: the simulation method ``Shooting"~\cite{shooting}, the implicit moiré synthesis approach ``UnDeM"~\cite{undem}, which employs a neural network, and the explicit synthesis method termed ``MoireSpace"~\cite{yang2023doing}, which utilizes its moiré pattern dataset.

\subsubsection{Demoiréing Models}
We test on the most effective SOTA demoiréing models,  MBCNN~\cite{zheng2020image} and ESDNet-L~\cite{yu2022towards}.

\subsection{Zero-Shot Demoiréing with Synthesized Data Only} 
We first demonstrate demoiréing results on real moiré images trained on purely synthesized data by SOTA moiré synthesis methods. To avoid data overlap in training sets and test sets, we have collected a comprehensive Mixed High-Resolution Natural Image Dataset (MHRNID), based on which, moiré images are synthesized for training demoiréing models. 
Quantitative comparisons can be found in Table~\ref{tab:Exp_multi_datasets}. Visual comparisons on demoiréing real data in UHDM are illustrated in Figure~\ref{fig:demoiré_result}.
Due to that UnDeM relies on existing moiré images in both the training (fusion networks) and inference phase, %and our method also requires real moiré images to guide the tone refinement network, 
we trained their networks on the TIP dataset and showed the result of UnDeM using the real moiré in the TIP dataset (``$\dagger$" in Table  \ref{tab:Exp_multi_datasets}) and our sampled moiré pattern (``$\ddagger$" in Table \ref{tab:Exp_multi_datasets}) during inference, respectively.
% For a fair comparison, we also use real moiré images from TIP dataset to train our tone refinement network.
For a fair comparison, we also use real moiré images from TIP dataset to train our TRN.
Notice that UnDeM and our method only use real moiré images to guide the synthesis, and neither of us uses such real data to train demoiréing models directly.

From the quantitative perspective (Table~\ref{tab:Exp_multi_datasets}), our method substantially outperforms all other approaches, particularly by more than 3.2 dB and 2.0 dB for MBCNN and ESDNet-L on the UHDM dataset,  respectively.
Besides, UnDeM$^\ddagger$ using our generated moiré patterns outperforms UnDeM$^\dagger$ using real moiré patterns in all experiments, proving our effectiveness further. 
From the qualitative perspective (Figure~\ref{fig:demoiré_result}),  our method demonstrates strong capability even when images in the target domain are contaminated by severe moiré patterns, which other synthesis methods fail to address.
We attribute our superiority to the diversity and realism of our synthetic data. Such high-quality data by our UniDemoiré enables the demoiréing model to learn moiré characteristics better, improving performance in removing unseen moiré artifacts. 
More visual results are in the appendix.

\subsection{Cross-Dataset Evaluation} 
We then demonstrate our ability to improve the performance of demoiréing models across domains. 
Quantitative results are shown in Table ~\ref{tab:Exp_cross_datasets}. Note that ``Baseline" means that the demoiréing models (MBCNN and ESDNet-L) are trained with the original source real moiré datasets and tested on the target dataset. 
For each synthesis approach, a demoiréing model is trained with combined original real data in the source dataset and corresponding synthesized data. 

As shown, the Shooting method struggles with real data due to differences between synthetic and real moiré. 
% UnDeM relies on a GAN-based network but can be unscrossdata and quality-dependent. 
UnDeM relies on a GAN network but can be inconsistent depending on the dataset and quality. 
The MoireSpace method performs better than UnDeM but has inferior moiré patterns and synthesis quality, resulting in lower experimental metrics. 
Thanks to the realistic and diverse synthesized data, our method outperforms all previous methods across every experiment.
Visual comparisons in Figure~\ref{fig:demoiré_result} (lower, Source: UHDM, Target: FHDMi) demonstrate our effectiveness. 
% More visualizations are in the appendix.



%% Ablation table
% \input{Tables/tbl_exp_ablation} 
\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{2mm}
% \scalebox{0.9}{
\begin{tabular}{lccc}
\toprule
Components                     &\ua{PSNR}   &\ua{SSIM} &\da{LPIPS} \\ 
\midrule
ALL                          & \textbf{20.7543} & \textbf{0.7653} & \textbf{0.2136} \\

$w/o$ MPG                    & 20.1607          & 0.7326          & 0.2456          \\
$w/o$ TRN                    & 20.1691          & 0.7372          & 0.2544          \\
TRN ($w/o$ $\mathcal{L}_{per}$)    & 20.3076          & 0.7508          & 0.2558          \\
TRN ($w/o$ $\mathcal{L}_{color}$)  & 20.2692          & 0.7406          & 0.2301          \\
TRN ($w/o$ $\mathcal{L}_{tv}$)     & 20.3961          & 0.7451          & 0.2324          \\
TRN ($w/o$ fusion block)           & 20.2868          & 0.7370          & 0.2311          \\ 
\bottomrule
\end{tabular}
% }
\caption{Ablation studies. Source: UHDM, Target: FHDMi.}
% \vspace{-2.5ex}
\label{tab:Exp_ablation}
\end{table}


\subsection{Ablation Study}
We individually ablate submodules in our proposed method to analyze their contribution. All these experiments are trained with the UHDM dataset and validated on the FHDMi dataset. Experimental results in Table~\ref{tab:Exp_ablation} verify that all components in our UniDemoiré solution are crucial for achieving the desired demoiréing performance. Removing any component such as the Moiré Pattern Generator (MPG), Tone Refinement Network (TRN), loss functions, and feature fusion block leads to a significant performance decline. 
More ablation studies are provided in the appendix.

% Conclusion
% \input{Contents/5-conclusions}
\section{Conclusion}
By addressing the issue of data diversity and realism, our universal solution, UniDemoiré, tackles one of the most important bottlenecks in image demoiréing problems. It showcases significant performance in zero-shot demoiréing and demonstrates a strong capability of enhancing the cross-domain performance of existing demoiréing models. 
More importantly, our method holds the potential to generate billions of moiré data and to significantly expand demoiréing models with a vast increase in parameters. 
Our limitations are discussed in the appendix.


% Acknowledgment
\section{Acknowledgments}
This work is supported by NSFC (No.62206173), Shanghai Frontiers Science Center of Human-centered Artificial Intelligence (ShangHAI), MoE Key Laboratory of Intelligent Perception and Human-Machine Collaboration (KLIP-HuMaCo).
This work is also partially supported by HKU-SCF FinTech Academy,
HKRGC Theme-based research scheme project T35-710/20-R, and SZ-HK-Macau Technology Research Programme \#SGDX20210823103537030.

\bibliography{aaai25}

% \newpage
% \clearpage
% \input{checklist}
% \setcounter{secnumdepth}{2}
\newpage
\clearpage
\input{Sup/0-outline}
\input{Sup/1-dataset}
\input{Sup/2-method}
\input{Sup/3-experiments}
% \input{appendix}

\end{document}