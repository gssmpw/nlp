\section{Related Work}
\subsection{Evaluating moral reasoning, political leaning, and values in LLMs}
To date, there has been no systematic evaluation of LLM  alignment to the UDHR. In terms of related evaluation work, one might point to the growing body of literature and tooling that focuses on measuring political ideology and lean expressed in LLM output \cite{agiza2024analyzing}, \cite{santurkar2023whose}, \cite{jiang2022communitylm}, \cite{buyl2024large}. This work suggests that LLMs can be coherently and recognizably aligned to particular political views. 
Another line of work examines the consistency of LLM outputs over value-laden questions \cite{moore2024large}, finding that LLM outputs tend to have higher variance on more controversial values. 
In addition, evaluation research on moral reasoning capacities in LLMs finds that LLMs reflect the same human-like biases in the context of moral and ethical dilemmas \cite{hendrycks2020measuring}, \cite{scherrer2024evaluating}, \cite{schramowski2022large}.  However, neither the work on political lean nor that on moral reasoning considers human rights explicitly. 

Finally, there is relevant normative debate to draw on: Gabriel \cite{gabriel2020artificial} and Kenton \cite{kenton2021alignment} highlight the possibilities of value misalignment, whereby AI systems express values that are at odds with what is expected of them, or what is desirable for the user, a third party, or society at large. Kenton \cite{kenton2021alignment} shows how in the context of LLMs, such misalignment can occur intentionally or unintentionally. Importantly, prior work considers who has the right to make decisions about what to embed \cite{gabriel2020artificial} and how to embed pluralistic values \cite{sorensen2024value}, \cite{kirk2024prism}. While this discussion highlights the need for increased fairness and transparency in determining what LLM outputs should express, this is generally not in reference to universally agreed doctrines such as the UDHR.

\subsection{Hedging}
We further draw on research in linguistics to identify behaviours that express ambiguity or a lack of clear endorsement. \textit{Hedging} is a term that in its everyday usage, is more closely related to the behavior we wished to evaluate here. In everyday usage, hedging can refer to "the act of evading the risk of commitment, especially by leaving open a way of retreat" \cite{mw_hedge}. In linguistics and logic, \textit{hedges} denote fuzzy concepts (those that are neither true nor false) as well as the expressions used to indicate them (\textit{strictly speaking}, \textit{technically speaking}, \textit{sort of})  \cite{lakoff1973hedges}, \cite{meyer1997hedging}. Note that also some machine literature uses  the term \textit{hedging} in a related way but with distinct expressions, to capture speaker uncertainty in the response \cite{yona2024can}.  Similar to past research, we use the term \textit{hedging} to mean that the response avoids fully committing to a singular yes/no view by referencing an opposing point of view. In addition, given that we exclusively use prompts that aim to elicit yes or no responses, we also use a second metric of \textit{non-affirmation} to add depth and support to the first metric.

\subsection{Disparity measures in fairness and bias evaluations}
Fairness is frequently measured via statistical parity, also known as demographic parity or independence. This asserts that in fair models, group membership (e.g. race, gender) should not be predictive of model outputs \cite{hertweck2021moral}, \cite{raz2021group}. Statistical parity and related techniques like positive predictive value parity have been used to study fairness in AI systems for many years, including in supervised systems prior to the recent rise of generative AI such as LLMs. For example, parity-based metrics have been used to measure algorithmic fairness in various types of predictive systems, including those in healthcare and credit risk \cite{li2023survey}, \cite{grabowicz2022marrying}, \cite{pro_publica}. While parity-based metrics have been criticised for failing to take into account relevant context, they are an established reference point in fairness research, and can serve as a first indicator for whether group-based bias may be occurring.

In terms of generative models, there are diverse tasks that have been proposed to measure fairness - ranging from those that measure bias in the semantic space (via semantic similarity tasks or entailment prediction) to those that measure the group fairness of properties of generated text (like toxicity and sentiment) \cite{dev2020measuring}, \cite{dhamala2021bold}, \cite{li2023survey}. There have also been calls for better metrics for evaluation that correspond most strongly to Realistic Use and Tangible Effects (RUTE) evaluations \cite{lum2024bias}. One related work measures disparities in conflict reporting and studies some groups in common with our work \cite{kazenwadel2023user}.

We adopt a parity-based metric to answer the simple question of whether human rights are endorsed equally in reference to different groups. In the discussion, we return to the implications of our findings and how additional fairness evaluation methods may be leveraged to build on this work.

\subsection{Prompt variations}
LLM evaluations have been criticised for lacking robustness, as LLM responses can be highly sensitive to variations in prompt phrasing and structure \cite{reynolds2021prompt}, \cite{wei2022chain}, \cite{lu2021fantastically}. “Prompt engineering” has emerged as a set of techniques to exploit this sensitivity \cite{giray2023prompt}, \cite{mesko2023prompt}, \cite{liu2023jailbreaking}. To ensure that our results are not the artifact of the specific wording of our query, we build on these insights by exploring several axes of variation in our queries (while retaining the same meaning). While the space of potential prompt variations is vast, we select a few key axes of variation to explore in generating prompt variants, including negation, forced choice, and contextual priming \cite{kahneman2013prospect}, \cite{strack1987thinking}.