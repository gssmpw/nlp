\section{Experiments}
Our evaluation investigates the following questions.

\begin{enumerate}[label=RQ\arabic*:]
    \item What are the pass rate, coverage, and failure-detection capability of the test cases generated by the SPAPI-Tester?
    \item To what extent can LLMs overcome the obstacles outlined in Section II.C to achieve end-to-end automated testing?
    \item How efficient is this automated API testing?
    \item How effective is SPAPI-Tester in testing real-world industrial APIs?
\end{enumerate}
Specifically, RQ1-RQ3 focus on ablation studies of SPAPI-Tester, using controlled experiments to evaluate its capabilities and performance. RQ4 examines the application of SPAPI-Tester in the real-world, industrial setting with newly developed (and thus guaranteed to be unseen) APIs to demonstrate the effectiveness of our end-to-end automated testing system.

\subsection{Experimental Setup} 
In this section, we describe our experimental setup. 
\subsubsection{Subjects}
Our research focuses on automating vehicle API testing within an industrial setting, addressing unique challenges such as inconsistencies across documentation and system specifications. As no existing methods directly address these issues in vehicle API automation, we could not compare our approach with general API testing techniques, as they lack the capability to handle the specific requirements of our industrial setting.

We evaluated the quality of generated test cases for 41 truck APIs using metrics such as pass rate and coverage. To assess SPAPI-Tester's error detection capabilities, we annotated an additional 109 APIs developed by a leading vehicle manufacturer. These APIs were supported by system documentation from in-house truck experts, CAN signal protocols from the CAN-bus team, and virtual vehicle documentation from the Virtual Vehicle team.

We tested four LLMs: two classic models—GPT-3.5-turbo (OpenAI, 2023-07-01-preview) and LLaMA3-70B (2024-04-18)—and two recent advancements, GPT-4o (2024-05-13) and LLaMA3.1-70B (2024-07-23). 
To ensure flexibility and reduce maintenance, we opted not to fine-tune these models with company-specific data, allowing seamless adaptation to new models or data without retraining.

\subsubsection{Metrics}
We evaluate our SPAPI-Tester both at the API level and at the test case level.
At API level, we use the \textit{pass rate} of the APIs as our metric. If all generated test cases for a given API pass the tests, we consider that API to have passed. Conversely, if any test case fails, the API is considered to have failed. Therefore, the \textit{pass rate} is defined as the proportion of APIs that pass the tests.

At test case level, we primarily assess the quality and coverage of the generated test cases. For these evaluations, we employ precision and recall as our key metrics. Precision measures the quality of the test cases generated, while recall measures their coverage of API properties.

\begingroup
\setlength{\tabcolsep}{3pt}
\begin{table}[t]
    \centering
    \caption{Pass rate on different types of APIs.}
    \resizebox{0.5\textwidth}{!}{
        \begin{tabular}{|c|c|cccc|}
        \hline
        \multirow{2}{*}{{API Type}} & \multirow{2}{*}{Num.} & \multicolumn{4}{c|}{LLMs} \\
        \cline{3-6}
        & & {GPT-3.5} & {LLaMA3} & {LLaMA3.1} & {GPT-4o} \\
        \hline
        Energy & 8 & 0.88 & 1.0 & 0.88 & 1.0 \\
        Driver Settings & 6 & 0.83 & 0.83 & 1.0 & 0.83 \\
        Visibility Control & 11 & 0.91 & 1.0 & 0.91 & 1.0 \\
        Software Control & 3 & 1.0 & 1.0 & 1.0 & 1.0 \\
        Vehicle Condition & 9 & 1.0 & 1.0 & 1.0 & 1.0 \\
        Other & 4 & 1.0 & 1.0 & 1.0 & 1.0 \\
        \hline
        Total/Average & 41 & 0.93 & 0.98 & 0.95 & 0.98 \\
        \hline
        \end{tabular}
    }
    \label{table:main_results}
    \vspace{-0.5cm}
\end{table}
\endgroup

% \cite{wang2024software}
\subsection{Pass Rate, Coverage, and Failure Detection (RQ1)}
\textit{Pass rate:} Since APIs with similar functions typically call the same electronic control unit (ECU) in embedded systems and, thus, share documentation within the same domain, we grouped 41 truck APIs into 6 categories based on their functions to present the results more clearly. Table~\ref{table:main_results} details the pass rates for each category.

These 41 APIs are online and pre-verified, ensuring that any failures observed during testing were due to issues within the generated test cases or code. Results show that for the majority of categories, all the APIs can pass the tests successfully, with all four LLMs achieving high pass rates. Notably, SPAPI-Tester achieved a 98\% pass rate when using LLaMA3 and GPT-4o, demonstrating the method's accuracy in generating valid test samples. However, GPT-3.5 exhibited slightly lower performance in handling structured input-output, failing in two cases due to improper CAN connection settings. Additionally, a common error across all LLMs stemmed from missing unit descriptions in API specifications. For example, when documentation omitted units for battery power, LLMs incorrectly defaulted to watts (W) instead of kilowatts (kW), leading to test case failures. Broad patterns of errors like this could likely be addressed by further refining the prompts.


\begin{table*}[h!]
    \centering
    \caption{Test case coverage of different types of APIs. 'P' is Precision, 'R' is Recall, and 'F1' is the F1 score.}
    % \resizebox{0.49\textwidth}{!}{
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{\centering API Type} & \multicolumn{3}{|c|}{GPT-3.5} & \multicolumn{3}{|c|}{LLaMA3} & \multicolumn{3}{|c|}{LLaMA3.1} & \multicolumn{3}{|c|}{GPT-4o} \\

            \cline{2-13}
             & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1}\\
            \hline
            Energy & 0.96 & 0.69 & 0.78 & 0.98 & 0.76 & 0.85 & 0.96 & 0.74 & 0.84 & 0.96 & 0.79 & 0.87\\
            Visibility Control & 0.97 & 0.70 & 0.78 & 0.96 & 0.70 & 0.79 & 0.97 & 0.74 & 0.84 & 0.96 & 0.80 & 0.87\\
            Vehicle Condition & 1.0 & 0.95 & 0.97 & 1.0 & 0.9 & 0.95 & 1.0 & 0.95 & 0.97 & 1.0 & 0.95 & 0.97\\
            Other & 1.0 & 0.63 & 0.77 & 1.0 & 0.85 & 0.92 & 1.0 & 0.83 & 0.91 & 1.0 & 0.80 & 0.89\\
            \hline
            \textbf{Average} & \textbf{0.97} & \textbf{0.73} & \textbf{0.80} & \textbf{0.98} & \textbf{0.79} & \textbf{0.88} & \textbf{0.98} & \textbf{0.81} & \textbf{0.89} & \textbf{0.97} & \textbf{0.85} & \textbf{0.90}\\
            \hline
        \end{tabular}
    % }
    \label{table:performance_metrics}
    \vspace{-0.3cm}
\end{table*}

\textit{Coverage:}
In addition to pass rate analysis, we evaluated the coverage of generated test cases to assess whether they adequately test each API. A vehicle expert group was invited to create ground truth test cases for 12 representative APIs, each including 5 to 30 test cases across 4 categories. The results are presented in Table~\ref{table:performance_metrics}. All LLMs demonstrated high precision, with precision rates exceeding 0.97 across the board and reaching 1.0 for half of the APIs, showcasing the high quality of test cases generated by our model. For cases where precision was below perfect, errors originated from limitations in the fuzzy matching step.

However, recall rates did not reach optimal levels primarily due to missing information in the API documentation, such as absent units or variable types for some attributes. To maintain high precision, SPAPI-Tester skips samples that lack sufficient context for accurate matching, resulting in a recall loss of approximately 15 percentage points. All untested attributes are logged in the testing report, allowing developers to trace and address these underlying issues.

\textit{Failure detection:}
To further assess the effectiveness of the generated test cases in detecting failures, vehicle experts labeled 109 additional truck APIs, being developed, identifying 38 as buggy. SPAPI-Tester created test cases that successfully detected all buggy APIs with only four false positives, achieving a 96\% accuracy rate. 
% This high accuracy demonstrates the reliability of the generated test cases.

All models performed comparably, highlighting that our stepwise, structured pipeline design reduces dependence on specific LLM choices.
We seamlessly migrated SPAPI-Tester to different LLMs without requiring additional adaptation. This largely model-agnostic pipeline design allows us to focus on refining the testing process rather than selecting specific LLMs, given the abundance of options.

\subsection{LLMs' ability of overcoming obstacles  (RQ2)}
\textbf{Fuzzy matching} presents a significant challenge in automated API testing. We categorized common fuzzy matching examples into five classes, selecting 20 test samples per class, supplementing with manually written samples if needed. 
The results, shown in Table~\ref{table:fuzzy_matching} (upper part), indicate that all models achieved high precision rates, highlighting the LLMs' capability to accurately recognize and match fuzzy inputs, a key requirement for full automation. For \textit{semantic equivalents, logical equivalents, and similar writing formats}, all the models attained an accuracy of 1.0 or nearly so, demonstrating their strong pattern matching abilities in semantics and logic. However, for \textit{spelling errors}, accuracy slightly dropped as some errors altered word semantics, like mistaking \texttt{date} for \texttt{data}. In the \textit{abbreviations} category, some abbreviations were too short to discern, complicating the matching process. 

For the \textbf{inconsistent units} issue, we selected 200 samples for experiment. The results in Table~\ref{table:fuzzy_matching} (lower part) indicate that while SPAPI-Tester achieves a high precision rate, the recall remains suboptimal. The reason is that some documentation explicitly annotates units for each attribute, while others omit these details. In these cases, it becomes necessary to infer the units based on descriptions or other contextual information, which can affect the performance.

\begin{table*}[h]
    \centering
    \caption{Performance on different types of Fuzzy Matching (upper part) and Inconsistent Units (lower part).}
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
            \hline
            \multirow{2}{*}{\centering API Type} & \multicolumn{3}{|c|}{GPT-3.5} & \multicolumn{3}{|c|}{LLaMA3} & \multicolumn{3}{|c|}{LLaMA3.1} & \multicolumn{3}{|c|}{GPT-4o} \\

            \cline{2-13}
             & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1} & \textbf{P} & \textbf{R} & \textbf{F1}\\
    \hline
    Spelling errors & 0.89 & 0.76 & 0.82 & 0.92 & 0.73 & 0.81 & 0.91 & 0.78 & 0.84 & 0.91 & 0.83 & 0.87\\
    Abbreviations & 0.93 & 0.68 & 0.79 & 0.88 & 0.74 & 0.80 & 0.92 & 0.75 & 0.83 & 0.98 & 0.74 & 0.84\\
    Similar writing formats & 0.95 & 0.95 & 0.95 & 1.0 & 0.95 & 0.97 & 1.0 & 0.95 & 0.97 & 0.95 & 0.95 & 0.95\\
    Logical equivalents & 1.0 & 0.75 & 0.86 & 0.95 & 0.78 & 0.86 & 0.92 & 0.70 & 0.80 & 0.95 & 0.75 & 0.84\\
    Semantic equivalents & 1.0 & 0.70 & 0.82 & 1.0 & 0.73 & 0.84 & 0.94 & 0.73 & 0.82 & 1.0 & 0.70 & 0.82\\
    \hline
    \textbf{Average} & \textbf{0.95} & \textbf{0.77} & \textbf{0.85} & \textbf{0.95} & \textbf{0.79} & \textbf{0.86} & \textbf{0.94} & \textbf{0.78} & \textbf{0.85} & \textbf{0.96} & \textbf{0.80} & \textbf{0.87}\\
    \hline
    \hline
    \textbf{Inconsistent Units} & \textbf{0.95} & \textbf{0.67} & \textbf{0.79} & \textbf{0.95} & \textbf{0.59} & \textbf{0.73} & \textbf{0.95} & \textbf{0.67} & \textbf{0.79} & \textbf{0.98} & \textbf{0.70} & \textbf{0.82}\\
    \hline
    \end{tabular}
    \label{table:fuzzy_matching}
    \vspace{-0.5cm}
\end{table*}
Another notable challenge is \textbf{informal pseudocoded mappings}, where a single test case may correspond to multiple values. We selected 100 representative test cases for this experiment. Each test case consists of two sets with multiple (key, value) pairs, and the goal is to map elements between these sets as accurately and comprehensively as possible. To increase complexity, we intentionally selected test cases where the sets contained different numbers of elements, creating scenarios where matching was non-trivial and recall could fluctuate.
To explore this, we conducted experiments under both strict (precision-focused) and relaxed (recall-focused) matching conditions. Examples reflecting different levels of strictness were included in the prompts, and the strictness level (e.g., strict, moderate, relaxed) was explicitly stated in the prompts. The results are presented in Figure \ref{fig:ambiguous}.

The experimental results indicate that under very strict conditions, precision can reach up to 100\%; however, recall drops significantly, even below 20\%. As the conditions are relaxed, precision slightly decreases, but recall increases substantially, reaching up to 55\%. Under the most relaxed conditions, recall rates for all the models approach 90\%. This demonstrates that our method can achieve high recall rates while maintaining a high level of precision. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{../figures/pr.pdf}
    \caption{Matching performance on informal pseudocoded mappings.}
    \label{fig:ambiguous}
    \vspace{-0.4cm}
\end{figure}

\subsection{Time efficiency (RQ3)} 
% How much time – not effort - does the automatic execution of the LLM-based test generation (steps 1-6 of the test process shown in Figure 1) take? How much took it in the presented case study?
In practical industrial scenarios, time consumption is an important criterion for measuring tool efficiency. Therefore, we measured the total time and the time taken at each stage of the SPAPI-Tester in the testing process. 
Given that the LLaMA model relies on local computational resources and that the processing speeds of GPT-3.5 and GPT-4o do not significantly differ in this pipeline, we report only the results for GPT-3.5.
We calculated the average time spent on testing all APIs. Additionally, we separately computed the time for the two major types of requests, i.e., PUT and GET. The results are shown in Table~\ref{table:time_cost}.

\begin{table}[t]
    \centering
    \caption{Time to generate test cases, per step (seconds). \textbf{DU} is document understanding; \textbf{RI} is retrieval information; \textbf{TSG} is test case generation; \textbf{Run} means running the test cases.}
    \begin{tabular}{|c|c|c|c|c|}
    \hline
    \textbf{Requests} & \textbf{Total} & \textbf{DU \& RI} & \textbf{TSG} & \textbf{Run}  \\
    \hline
    GET & 55.0 & 6.6 & 3.6 & 44.8 \\
    \hline
    PUT & 56.3 & 6.8 & 4.3 & 45.2 \\
    \hline
    \textbf{Average} & 55.7 & 6.7 & 4.0 & 45 \\
    \hline
    \end{tabular}
    \label{table:time_cost}
    \vspace{-0.4cm}
\end{table}


The results indicate that most of the time is consumed during the execution of test cases, with a significant portion dedicated to environment setup. SPAPI's complexity requires the appropriate configuration of embedded system environments, such as setting up the CAN bus for signal transmission. Additionally, the VV system needs to read CAN signals and complete the reading or setting of the virtual vehicle's state, which consumes a large amount of time.

The time required for PUT and GET requests is almost identical, as our approach batch-generates matching results or test cases for these requests, effectively minimizing time differences. In the full pipeline, the DSPy module, which leverages LLM-based capabilities, is called six times: once for documentation comprehension, four times for information matching, and once for test case generation. Additionally, DSPy’s retry mechanism re-calls the module if the output does not adhere to the predefined format. On average, the entire process from initial input to test case generation takes about 11 seconds, which is remarkably fast for automated API testing.

Manual vehicle API testing in the automotive industry is traditionally a time-consuming process, as it requires consideration of numerous conditions. To better understand the time demands of manual testing, we surveyed experts in our industry, including three senior engineers and one technical lead. They estimated that creating test cases for each API takes approximately 0.1 to 3 FTE workdays, with most APIs requiring about two hours. They generate 5 to 30 test cases for each API.

In contrast, our SPAPI-Tester achieves remarkable efficiency improvements. The system generates a complete set of test cases for a single API in just 11 seconds, representing a dramatic reduction in time and effort.
This substantial speedup not only reduces the time and effort required for API testing but also alleviates the traditionally high time burden associated with manual test case creation, greatly enhancing the API testing process.


\subsection{Performance on real-world industry APIs (RQ4)}
To demonstrate the capability of SPAPI-Tester in an real-world setting, we collected 193 newly developed and unverified truck APIs and their corresponding documentation from a leading truck manufacturing facility. We then employed SPAPI-Tester to conduct end-to-end automated testing, aiming to identify issues within these APIs.

SPAPI-Tester identified 23 test failures. The test report indicates that 22 test cases failed due to issues within the API implementation, and one test case failed due to an error while parsing the API documentation. On consultation with the API developers, these were determined to be legitimate bugs in the API implementation. The team has already started addressing these issues upon receiving the checking results.

In addition, this demonstrates that SPAPI-Tester not only has a high accuracy in detecting API errors but also provides detailed reports that help quickly identify the root causes of failures. Even when SPAPI-Tester was unable to generate correct code, the detailed reports can help to identify the failure causes quickly, thereby minimizing misdiagnoses. This capability significantly enhances the practical utility of SPAPI-Tester by providing precise and actionable insights.
In summary, these results underscore the robust practical applicability of SPAPI-Tester in real industrial environments.


\subsection{Performance comparison with manual testing}
To illustrate the advantages of SPAPI-Tester over manual API testing, we conducted a comparative evaluation. As described in Section IV.B, an expert team created ground truth test cases for 12 APIs. To measure the pass rate of manual testing, two additional engineers independently created test cases for these APIs. Results showed that one engineer’s test cases passed 10 APIs, while the other’s passed 11. 
Both engineers missed one or two APIs due to confusion over similar data entries. For instance, attributes like \texttt{reducedWeeklyRestsForCurrentWeek} and \texttt{regularWeeklyRestsForCurrentWeek} proved challenging for human testers to differentiate, whereas SPAPI-Tester’s LLMs handled them effortlessly. This led to an average pass rate of 87.5\% for manual testing at the API level, while SPAPI-Tester, with test cases generated by four different LLMs, achieved pass rates between 93\% and 98\%.

In terms of coverage, the average rate for manually created test cases was 82\%, with human testers occasionally skipping properties due to incomplete API documentation (e.g., missing units). SPAPI-Tester reached 85\% coverage with GPT-4o, while other models ranged between 73\% and 81\%.

To evaluate failure detection, we selected 10 APIs (5 of which contained known bugs) from the 109 APIs mentioned in Section IV.B. Both engineers identified all buggy APIs, although one created a test case that falsely flagged a correct API as erroneous, resulting in a recall rate of 100\% and a precision rate of 91\% for manual testing. Similarly, SPAPI-Tester achieved a recall rate of 100\% with a slightly lower precision of 90\%.

In summary, SPAPI-Tester consistently generates high-quality test cases, demonstrating comparable performance to manual testing in terms of pass rate, coverage, and failure detection.
