\section{Fully automated SPAPI testing with LLMs}
This section presents the details of our automated testing tool, SPAPI-Tester, which can integrate with LLMs to fully automate the entire API testing process.
The overall process, as shown in Figure \ref{fig:spapi-tester-workflow}, can be divided into four main steps. These steps are detailed in Process \ref{alg:spapi_tester_workflow}.

\begin{figure*}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{figures/spapi-workflow.pdf}
    \caption{Architecture and workflow of SPAPI-Tester: The pipeline largely preserves the manual process and selectively uses LLMs to automate discrete steps.}
    \vspace{-5mm}
    \label{fig:spapi-tester-workflow}
\end{figure*}

\begin{algorithm}[htbp]
    \caption{Overall Workflow of SPAPI-Tester} \label{alg:spapi_tester_workflow}
    TestTracker = InitializeTestTracker() \\
    \textbf{For} APISpec \textbf{in} List(APISecifications) \\
    \hspace*{2em} S = ExtractTestObjects(APISpec) \\
    \hspace*{2em} S' = APIToCANMapping(S, CANTable) \\
    \hspace*{2em} S* = CANToVVMapping(S', VVTable) \\
    \hspace*{2em} TestCases = GenerateTestCase(S, S*) \\
    \hspace*{2em} TestCode = WritingTestCode(TestCases) \\    
    \hspace*{2em} TestTracker.analyzeTestRun(TestCode) \\
TestReport = PushToTestRepo(TestTracker)
\end{algorithm}

After initializing SPAPI (\textit{line 1}), the entire testing process is divided into four parts:
\textbf{(1)} Documentation understanding (\textit{line 3}): This part involves identifying test objects based on the API specifications. 
\textbf{(2)} Information matching (\textit{lines 4, 5}): This part entails look up relevant CAN table and virtual vehicle documents to matching all these objects. 
\textbf{(3)} Test case generation (\textit{line 6}): Using the matched data, this step focuses on generating test cases for the API's return results and verifying the virtual vehicle's status.
\textbf{(4)} Executing test cases and generating test reports (\textit{line 7, 8, 9}).


\subsection{Documentation understanding}
The purpose of documentation understanding is to extract the test objects from the API documentation.
Standard API documentation, commonly in YAML or Json format~\cite{openapi2023}, as shown in Figure \ref{fig:spapi-tester-workflow}(a), is structured to list attributes and values associated with various objects. This structured format lends itself well to template-based parsing.
We parse these documents and use predefined templates to extract the relevant attributes and values. Based on existing templates~\cite{openapitemplate2024}, we define a few simple and common rules to ensure the method's general applicability.
These templates focus on fundamental elements, such as endpoint names, attribute names, and data types. Additionally, if sample API calls are provided in the documentation, we extract these directly to test the basic accessibility and functionality of the API.

However, using templates alone is insufficient for determining reasonable attribute values. We have identified the following issues with relying only on templates:

(1) Cannot utilize attribute description: API documentation often includes natural language descriptions of attributes that templates cannot interpret or utilize. These descriptions typically contain constraints on the attributes, which are crucial to prevent generating incorrect values. 


(2) Lack of robustness: API documentation can sometimes be informal or inconsistent. For instance, attributes of enumeration types are usually presented as ["STANDARD", "ECONOMY"], but some documents might incorrectly use "STANDARD or ECONOMY". Only using templates makes it difficult to address these random and informal issues effectively.

To overcome these two issues, we introduce LLMs to enhance the process.
LLMs are utilized to analyze the entire API documentation, leveraging natural language descriptions to understand attribute constraints more effectively. Since LLMs are capable of semantic understanding, they also mitigate the impact of informal formatting or inconsistencies. This allows the system not only to parse API properties but also to map them to CAN signals, which is covered in detail in the subsequent section. 
LLMs further generate constraints based on attribute descriptions, producing reasonable values within these constraints. The contextual insights provided by LLMs help create a broader set of valid test values, thereby improving the coverage and reliability of our test cases.

In practice, to ensure the stability of LLM outputs and reduce the effect of the specific prompt formulations, we employ DSPy \cite{khattab2023dspy} to automate prompt optimization. 
DSPy enables us to write declarative LLM invocations as Python code. Figure \ref{fig:DSPy-Signature} illustrates a simplified example of one of our prompts, along with the DSPy Signature.
This \texttt{APIPropertyToCANSignal} signature outlines the process of converting structured API properties to CAN signals, which automates the time-consuming task of constructing an API property $(k_i, v_i)$ and mapping it to a corresponding CAN signal $(k'_i, v'_i)$.
\input{figures/DSPy_Signature}

To further improve the accuracy and ease of extracting structured data from the LLM, we format the LLM inputs and outputs as dictionaries. We define dictionary-based prompt templates to make tasks more comprehensible for the LLM \cite{openai2023structuredoutputs}, as demonstrated in Figure \ref{fig:template-mapping}. By specifying the expected output fields, the signature directs the LLM to navigate inconsistencies in documentation and accurately associate API properties with CAN signal values. Furthermore, by typing fields in the signature, we enable the use of a \texttt{TypedPredictor} in DSPy, which validates the LLM response. If the response does not conform to the specified types, DSPy re-prompts the LLM, repeating this up to a maximum threshold until compliance is achieved. This structured approach capitalizes on the improved format adherence of LLMs, enhancing consistency and reliability.

\input{figures/template_mapping}

% By first using templates to extract simple objects and then employing LLMs to supplement this extraction, we can effectively identify the test objects $\mathcal{S} = \{(k_i, v_i)\}_{i=1}^N$ from the API documentation.

\subsection{Information Matching}

As illustrated in Figure \ref{fig:mapping}, the mapping of information in our system encompasses two stages: mapping API properties to CAN signals and mapping CAN signals to Virtual Vehicle (VV) signals. These mappings are crucial for enabling signal transmission within the vehicle as well as setting or verifying the vehicle’s state. Since the processes and methods for these two mappings are similar, we will detail the approach for mapping API properties to CAN signals as an example.

First, we retrieve a set of candidate CAN signal key-value pairs \(\{(k'_i, v'_i)\}\) from a CAN signal library through solely matching the name of endpoint. Subsequently, we use the extracted API attributes \(\mathcal{S} = \{(k_i, v_i)\}_{i=1}^N\) and the candidate CAN signals \(\{(k'_i, v'_i)\}\) as input to an LLM, enabling many-to-many matching between API properties and CAN signals. In many cases, attributes may have multiple enumerated values. For instance, as shown in Figure \ref{fig:template-mapping}, an API property `valueFour' might take the values `True' or `False', while the corresponding CAN signal might represent these states as `AA' and `BB'. This type of mapping is common, and to increase the stability of SPAPI-Tester, we utilize a separate DSPy module specifically for matching enumerated values. The input consists of enumerated values from both the API property and the CAN signal, and the output is a mapping of these values.

As discussed in Section II.C, there are several challenges in the mapping process. First, for \textit{fuzzy matching}, the LLM’s strong semantic understanding is well-suited to handle these cases. Second, for \textit{pseudocode mappings}, we enhance template robustness by embedding examples directly into the prompt, as shown in Figure 8. For example, we map ``AAsignal:BB OR PV\_AnotherSignal:CC" to ``can\_value" : ``BB", thereby minimizing document noise while extracting relevant information. Third, for \textit{unit inconsistencies}, we apply a dedicated DSPy module that uses a Chain-of-Thought (CoT) approach to extract and normalize units within values. This module converts units (e.g., `kW' to `Kilowatts'), ensuring unit alignment in the test case generation phase.

The final output is structured as a list, as defined in Figure \ref{fig:DSPy-Signature}, with each element containing a fully matched pair. The same approach is then applied to map information between CAN signals and VV signals, ultimately yielding complete matching results \(\mathcal{S'}\) and \(\mathcal{S*}\).

\subsection{Test case generation}
\input{figures/test-case-gen-prompt}
After matching all the necessary information, we integrate these details into a structured document, as illustrated in Figure \ref{fig:spapi-tester-workflow}(b), which then serves as the basis for generating test cases.

Given the need to address multiple constraints during test case generation—such as unit consistency—we employ a stepwise CoT approach to progressively incorporate these constraints. Specifically, for \textit{inconsistent units}, we prompt the LLM to identify relationships between units and perform any necessary conversions. For \textit{inter-parameter dependencies}, the LLM captures relationships among parameters, ensuring compatibility and avoiding value conflicts. Additionally, the LLM identifies property types and manages specialized formats, such as date-time strings. Finally, we guide the LLM in handling cases common in industrial contexts, such as shared CAN signals among multiple properties or specific constraints on value ranges.

To ensure these constraints are applied consistently, we leverage DSPy’s \texttt{TypedChainOfThought} method, which consolidates all conditions within a single prompt. Figure \ref{fig:test-case-gen-prompt} provides a simplified example of this prompt. For ease of use, we specify that the module outputs test cases in dictionary format, as depicted in Figure \ref{fig:spapi-tester-workflow}(c).

After generating the test cases, we use them to create test code.
The test code generally consists of two sections: a setup section, which includes essential elements such as package imports and requests to enable program execution, and a validation section containing assertions.
Since the setup code remains consistent across tests, we design distinct \texttt{Jinja}\footnote{\url{https://palletsprojects.com/projects/jinja/}} templates for PUT and GET test cases.
Using a simple code renderer, we inject the generated API and VV test objects into the \texttt{Jinja} template to render the \texttt{Pytest} test case. Figure \ref{fig:spapi-tester-workflow}(d) shows an example test case rendered by the test-writing module.

\subsection{Executing test cases and generating test reports}
To ensure the automation of the entire process, the system automatically executes the test code on the test rig \cite{asyraf2019fundamentals}, and then generates a comprehensive test report. This report documents the details of the automated testing process, including the test objects, the matching results, the generated test cases, and the execution logs. Such documentation ensures that our system maintains a high level of transparency, rather than functioning as a black box.

\vspace{0.2cm}