\section{Introduction}
Large Language Models (LLMs) are revolutionizing software engineering. In the past few years, we have witnessed the application of LLMs for assisting or automating numerous software engineering tasks like requirements engineering, software design, coding, and testing \cite{DBLP:conf/fose-ws/FanGHLSYZ23} \cite{DBLP:journals/corr/abs-2308-10620}. Software testing, in particular, is one area where LLMs have been applied with vigor. Facing ever-increasing needs for automation due to the volume and intensity of work involved, testing is rapidly benefiting from the generative capabilities of LLMs. As systematically surveyed in \cite{DBLP:journals/tse/WangHCLWW24}, LLMs have been applied in many testing tasks including system input generation, test case generation, test oracle generation, debugging, and program repair.

While a considerable amount of recent literature has focused on applying LLMs in narrowly scoped tasks \cite{wang2024software} -- such as specific unit tests \cite{chen2023teaching}\hspace{-0.1pt}\cite{yuan2023no}, isolated integration tests~\cite{ajiga2024enhancing}, or individual verification scenarios~\cite{yoon2024intent}\hspace{-0.1pt}\cite{feldt2023towards} -- few have reported on their application to automate a complete test process. Practical testing processes are a diverse mix of steps that are mechanical, creative, and anything in between \cite{fani2023llms}\hspace{-0.1pt}\cite{boukhlif2024llms}. They also involve several (teams of) engineers and tools, whose harmonious cooperation is essential to ensure the quality and cadence of testing. The challenge is only greater when testing automotive embedded systems, where software coexists with mechatronics and other physical systems. Under such heterogeneous conditions, it is not immediately apparent how one can effectively integrate LLMs into a testing process and gain efficiencies. 
In response to these challenges, we present a case study that (1) focuses upon a real-world test process in the automotive industry that is largely performed manually, and (2) automates it using a recipe that seamlessly combines selective use of LLMs with conventional automation.

\input{figures/spapi_test_process.tex}

The focus of this case study -- our system under test -- is SPAPI, a web server that is deployed in trucks made by a leading vehicle manufacturer. SPAPI exposes a set of REST APIs which can be used by clients to read or write selected vehicle states. For example, SPAPI exposes \texttt{/speed} that can be used to read the vehicle speed, and \texttt{/climate} that can be used to change the cabin climate. Essentially, SPAPI serves as a gateway between web clients (like apps on a tablet) on one side, and in-vehicle control and monitoring applications on the other side. More importantly for the purposes of this paper, since SPAPI enables crucial customer-facing applications, considerable effort is spent in ensuring its quality. 

Testing SPAPI requires a dedicated team of 2-3 full-time engineers. As shown in Figure \ref{fig:spapi-test-process} (left), when new APIs are released, the team first reviews the API specifications. They then (2-3) consult multiple documentation sources to understand the associated vehicle states, (4-5) organize this information to determine appropriate mocks and test inputs, and (6-7) write and integrate test cases into a nightly regression suite. Finally, they assess results (8), particularly test failures, to identify valid problems. Notably, as highlighted in Figure 1, most of this process is still performed manually.

These observations prompt the question -- why is such intense manual effort needed to test an arguably simple gateway server? The main reasons are structural. First, as a gateway, SPAPI’s engineering spans multiple teams with overlapping responsibilities. The three core components—the server, vehicle state system, and mocking system—are developed by separate teams, while testing falls to a fourth team that must interpret disparate documentation from each. Second, SPAPI bridges web applications and traditional in-vehicle systems, which differ fundamentally in documentation style. SPAPI APIs are specified in Swagger, making them machine-readable, whereas vehicle states are documented in a mix of natural and formal languages, often requiring human interpretation. Third, SPAPI testers rely heavily on implicit knowledge built over years to manage inconsistencies across systems and teams, leading to highly specialized expertise that intensifies manual effort and complicates team turnover.

In SPAPI testing, the potential for full automation presents two significant benefits: (1) SPAPI testing can be fully automated, increasing the cadence with which APIs can be delivered to customers, and (2) SPAPI testers can be unburdened of their tedious job, allowing their creative talents to be applied elsewhere. Our observations on SPAPI highlight that full automation is not only beneficial but essential under certain test process conditions. 

Specifically, full automation is crucial in scenarios where testers function as a “glue” between tools, systems, and stakeholders in tasks that rely on judgment rather than creativity. Here, automation enhances engineering quality while improving the testers' experience. Additionally, in testing workflows with extensive manual steps, partial automation offers limited gains, reinforcing the need for a comprehensive, all-or-nothing automation approach. Furthermore, when testers navigate legacy processes weighed down by technical debt, partial debt mitigation falls short; complete automation is necessary to address and eliminate debt effectively, benefiting both testers and the organization as a whole.

Recognizing these advantages and the rapid advancements in LLMs for automating manual processes, we explore the central question: can LLMs serve as the key to fully automating a largely manual test process? To address this, we make the following contributions:

\begin{enumerate}
    \item We argue that a test process with clearly decomposed tasks, many of which are executed manually, is a prime candidate for complete automation based on LLMs.
    \item When these criteria are satisfied, we propose a recipe for full automation that involves (a) retaining the test process structure, (b) leveraging LLMs as a general-purpose tool to automate each manual step, and (c) combining LLMs with conventional automation when required.
    \item We present in-vehicle web server testing as a case study, illustrating how a real-world testing process aligns with our criteria and demonstrating its full automation using our proposed recipe.
    \item As the test process structure remains largely intact, we highlight how evaluating the quality of AI-driven automation can be simplified by independently assessing each step where an LLM is applied.
\end{enumerate}

As the following sections will demonstrate, using a real industrial example of in-vehicle embedded software testing, we show that a manual process like SPAPI testing can be fully automated (see Figure \ref{fig:spapi-test-process}) to deliver practical improvements.