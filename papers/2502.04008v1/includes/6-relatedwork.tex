\section{Related Work}

Existing research on API testing mainly focus on black-box and white-box testing, depending on whether the source code of the API is accessible~\cite{golmohammadi2023testing}. White-box testing typically involves generating test cases to thoroughly test the logic within the code~\cite{zhang2019resource}\cite{ zhang2021resource}. For example, EvoMaster~\cite{arcuri2020automated} uses the Many Independent Objective (MIO) evolutionary algorithm to optimize multiple metrics simultaneously, such as line coverage, branch coverage, HTTP status coverage, and the number of errors. Building on this, some studies have employed additional tools for code instrumentation, such as JVM~\cite{arcuri2018test}\cite{arcuri2019restful} and NodeJS programs~\cite{zhang2022javascript}\cite{moller2019model}. Atlidakis et al.~\cite{atlidakis2020pythia} calculate code coverage by pre-configuring basic block locations and use this feedback to guide test generation. 

Currently, most studies focus on black-box API testing, aiming to enhance test case coverage for more comprehensive API testing~\cite{viglianisi2020resttestgen}. Template-based methods, such as fixed test specifications and JSON schemas, are commonly used for generating accurate test cases \cite{benac2014jsongen}\cite{chakrabarti2009test}\cite{fertig2015model}\cite{arcuri2018testmio}\cite{godefroid2020intelligent}. However, these approaches struggle to capture parameter dependencies. To address this, Stallenberg et al. \cite{stallenberg2021improving} proposed a hierarchical clustering method, while Lin et al. \cite{lin2022forest} introduced a tree-based representation of parameter relationships. Martin et al. \cite{martin2020restest} further improved test diversity by integrating external knowledge bases to generate reasonable values. Despite these advancements, traditional methods often fail to achieve robust and comprehensive testing.

Recently, LLMs have emerged as a promising direction for API testing \cite{li2024application}\cite{olasehindeoptimizing}. Kim et al. \cite{kim2024leveraging} demonstrated the utility of LLMs in interpreting natural language API documentation to generate test values. Building on this, Le et al. \cite{le2024kat} proposed constructing dependency graphs from documentation to enhance test coverage. Other studies fine-tuned LLMs using Postman test cases \cite{deepika2024automating} or applied masking techniques to predict test values \cite{decrop2024you}. However, these methods face challenges in ensuring the validity and robustness of generated test cases \cite{pereira2024apitestgenie}.


However, existing methods focus solely on test case generation, which is only one part of the API testing process, and do not address the automation of the entire process. In practical applications, these methods require significant manual verification. For instance, some approaches need to retrieve relevant yet often ambiguous information from external databases. Moreover, these methods lack robustness; if the API specification is missing parameters or contains minor errors, the process may fail. Unlike previous approaches, we are the first to explore the automation of the entire API testing process, focusing on current bottlenecks in API automation and considering how to leverage LLMs to address these challenges robustly.