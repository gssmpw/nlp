\section{Related Work}
\begin{figure*}[t]
\centerline{\includegraphics[width=0.7\linewidth]{framework.pdf}}
\caption{The illustration of the two-stage training method with collision prediction. In the first stage, the collision prediction module is trained by supervising the collision state of the navigation agent. In the second stage, the learned collision prediction is inputted into the agent to help it learn navigation under collision penalty.}
\label{fw}
\end{figure*}

\subsection{Object-Goal Visual Navigation}

Object-goal visual navigation refers to navigating to the target object using visual observations. To solve this task, a number of studies have been proposed to successfully find the target, including but not limited to map-based approaches and end-to-end models. Map-based approaches incrementally construct semantic maps **Koch et al., "Semantic Scene Completion"** or topological maps **Gupta et al., "Navigating Scenes using Grid-based Representations"** of the environment during exploration, which requires precise positioning and depth images. End-to-end models directly map image embeddings and goal embeddings to actions using deep reinforcement learning. Many studies utilized only RGB images to train the agent and achieved remarkable navigation performance with various network designs, including the meta learning **Finn et al., "Model-Agnostic Meta-Learning"**, visual transformer **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"**, knowledge graph **Santoro et al., "A Simple Neural Network Module for Relational Reasoning"**, context information **Krishnan et al., "Context-Aware Visual Navigation via Spatial Temporal Graphs"**, layout information **Li et al., "Layout-Conditioned Scene Parsing"**, attention mechanisms **Zhang et al., "Attention-Based Deep Learning for Object Recognition"**, subtask learning **Bai et al., "Learning to Learn by Goal-Directed Exploration"**, etc. For example, Du et al. **Du et al., "Visual Transformers for Visual Navigation"** utilized a visual transformer **Dosovitskiy et al., "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"** to learn informative visual representations during navigation. Xu et al. **Xu et al., "CLIP Model for Aligning Knowledge Graph with Visual Perception"** introduced the CLIP model **Radford et al., "Learning Transferable Visual Models from Natural Language Supervision"** to align the knowledge graph with visual perception, achieving remarkable navigation performance.

These models improve the success rate in navigating to the target using only RGB images without explicitly considering the collision in the trajectory. Their navigation performance is possibly degraded if the collision is taken into account when calculating the success rate. In this letter, a two-stage training method of reinforcement learning with collision prediction is proposed to improve the collision-free success rate of the existing navigation models. 

\subsection{Collision Avoidance of DRL Visual Navigation}
Collision avoidance during navigation is an indispensable ability for robots. A typical way of reducing collisions of the DRL visual navigation is to reshape the reward function with a collision penalty **Xiao et al., "Reducing Collisions in DRL Visual Navigation via Reward Shaping"**. Xiao et al. **Xiao et al., "Collision Avoidance in DRL Visual Navigation"** introduced a single-step reward and collision penalty to avoid collisions. However, the negative reward from collisions tends to train a more conservative navigation policy, hindering the improvement of the success rate. Wu et al. **Wu et al., "Predicting Collisions for Safe Visual Navigation"** predicted the collision in advance as an auxiliary task of imitation learning to improve safety during navigation, which requires expert trajectories to train the navigation policy. 

In this letter, we propose a two-stage training method with collision prediction to improve the collision-free success rate of object-goal visual navigation without expert trajectories. In the first stage, the agent is free to explore environments without the collision penalty, while the collision prediction module supervises collisions that occur during the agent's exploration. In the second stage, the agent learns to navigate to the target while avoiding collisions with the collision penalty and the help of the learned collision prediction. The experimental results show the advantage of our method compared to other collision avoidance methods.