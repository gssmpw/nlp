\section{Related Work}
\begin{figure*}[t]
\centerline{\includegraphics[width=0.7\linewidth]{framework.pdf}}
\caption{The illustration of the two-stage training method with collision prediction. In the first stage, the collision prediction module is trained by supervising the collision state of the navigation agent. In the second stage, the learned collision prediction is inputted into the agent to help it learn navigation under collision penalty.}
\label{fw}
\end{figure*}

\subsection{Object-Goal Visual Navigation}

Object-goal visual navigation refers to navigating to the target object using visual observations. To solve this task, a number of studies have been proposed to successfully find the target, including but not limited to map-based approaches and end-to-end models. Map-based approaches incrementally construct semantic maps\cite{cow, goat} or topological maps\cite{wu2024voronav} of the environment during exploration, which requires precise positioning and depth images. End-to-end models directly map image embeddings and goal embeddings to actions using deep reinforcement learning. Many studies utilized only RGB images to train the agent and achieved remarkable navigation performance with various network designs, including the meta learning\cite{SAVN}, visual transformer\cite{VTNet, OMT}, knowledge graph\cite{ORG, hoz, akgvp}, context information\cite{mjol, ral, SSNet, Li}, layout information\cite{lstde}, attention mechanisms\cite{SpatialAtt, tdanet}, subtask learning\cite{tro}, etc. For example, Du et al.\cite{VTNet} utilized a visual transformer\cite{transformer} to learn informative visual representations during navigation. Xu et al.\cite{akgvp} introduced the CLIP model\cite{CLIP} to align the knowledge graph with visual perception, achieving remarkable navigation performance.

These models improve the success rate in navigating to the target using only RGB images without explicitly considering the collision in the trajectory. Their navigation performance is possibly degraded if the collision is taken into account when calculating the success rate. In this letter, a two-stage training method of reinforcement learning with collision prediction is proposed to improve the collision-free success rate of the existing navigation models. 

\subsection{Collision Avoidance of DRL Visual Navigation}
Collision avoidance during navigation is an indispensable ability for robots. A typical way of reducing collisions of the DRL visual navigation is to reshape the reward function with a collision penalty\cite{humanoid, drqn, coll1}. Xiao et al.\cite{coll1} introduced a single-step reward and collision penalty to avoid collisions. However, the negative reward from collisions tends to train a more conservative navigation policy, hindering the improvement of the success rate. Wu et al.\cite{dual} predicted the collision in advance as an auxiliary task of imitation learning to improve safety during navigation, which requires expert trajectories to train the navigation policy. 

In this letter, we propose a two-stage training method with collision prediction to improve the collision-free success rate of object-goal visual navigation without expert trajectories. In the first stage, the agent is free to explore environments without the collision penalty, while the collision prediction module supervises collisions that occur during the agent's exploration. In the second stage, the agent learns to navigate to the target while avoiding collisions with the collision penalty and the help of the learned collision prediction. The experimental results show the advantage of our method compared to other collision avoidance methods.