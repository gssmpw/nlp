\section{Datasets \& Evaluation Metrics}
\label{sec:datasets}

\subsection{Data Acquisition}

The datasets for SIRR are a critical aspect of developing effective deep learning models. These datasets vary in size, image source, and the type of annotations provided. In general, they can be categorized into two main types: synthetic datasets and real-world datasets.

\subsubsection{Synthetic Datasets}
Synthetic datasets are created by simulating the reflection phenomenon using computer graphics techniques. This allows for precise control over various factors such as the intensity and blurriness of the reflection, as well as the presence of ghosting effects. Common methods for creating synthetic datasets include:

\textbf{Image Mixing:} Combining two images with different coefficients to represent the background and reflection layers.

\textbf{Reflection Blur:} Applying Gaussian blur to the reflection layer to mimic the out-of-focus effect.

\textbf{Brightness Adjustment:} Adaptively adjusting brightness and contrast to create realistic reflections.


\textbf{Physics-based Rendering:} Using physics-based methods to render reflections.

\subsubsection{Real-world Datasets}

Real-world datasets are captured using cameras in real-world environments. This provides more realistic and diverse data, but it also makes it more challenging to obtain accurate ground truth images. Common methods for creating real-world datasets include:

\textbf{Manual Glass Removal:} Capturing images with and without the glass to obtain ground truth.

\textbf{Raw Data Subtraction:} Subtracting the reflection from the mixed image in the raw data space.

\textbf{Flash/no-flash Pairs:} Capturing images with and without flash to exploit flash-only cues.

\textbf{Polarization:} Using polarization cameras to capture images with different polarization angles.

\textbf{Controlled Environments:} Capturing images in controlled environments with varying lighting, glass thickness, and camera settings.



\subsection{Current Public Datasets}

Several public datasets have been created to facilitate the development and evaluation of deep learning models for SIRR. These datasets vary in size, image source, and the type of annotations provided. Table~\ref{tab:dataset} summarizes the most important datasets for SIRR, including their usage (training, testing, or both), the number of image pairs, average resolution, and whether they collect from real or synthetic images.


\input{Tables/table2}

Some of these datasets, such as Nature and Real, are relatively small and contain only real-world images with ground truth transmission layers. Others, such as SIR$^2$~\cite{wan2017benchmarking} and CEIL~\cite{fan2017generic}, are larger and include both synthetic and real-world images.  The SIR$^2$~\cite{wan2017benchmarking} dataset is particularly notable for its diversity, as it includes images with varying blur levels and glass thicknesses. The CEIL dataset, on the other hand, is designed to be more challenging, as it includes images with strong reflections. 

More recent datasets, such as CDR~\cite{lei2022categorized}, has been created to address the limitations of earlier datasets.  The CDR dataset is categorized according to reflection types and contains images with perfect alignment between the mixed and transmission images includes misaligned raw flash/ambient images. 

The largest and most recent dataset is RRW~\cite{zhu2024revisiting}, which contains over 14,950 high-resolution real-world reflection pairs. This dataset is particularly valuable for training deep learning models, as it provides a large number of images.

The choice of dataset depends on the specific application and the desired properties of the reflection removal algorithm. In general, larger and more diverse datasets are preferred, as they enable the training of more robust and generalizable models.

\subsection{Evaluation Metrics}

In this survey, we provide a comprehensive summary of evaluation metrics commonly employed in deep learning-based reflection removal. These metrics can be broadly classified into two categories: quantitative metrics and qualitative metrics.

\subsubsection{Quantitative Metrics}

Quantitative metrics are used to objectively measure the difference between the predicted transmission layer and the ground-truth transmission layer. Commonly used quantitative metrics include:

\begin{itemize}
\item \textbf{PSNR} (Peak Signal-to-Noise Ratio): Measures the difference between the predicted transmission layer and the ground-truth transmission layer. 
\item \textbf{SSIM} (Structural Similarity): Evaluates the similarity between the predicted and ground-truth transmission layers from three aspects: luminance, contrast, and structure. 
\item \textbf{MSE} (Mean Squared Error): Calculates the average squared difference between the predicted and ground-truth transmission layers. 
\item \textbf{Local MSE} (LMSE): Evaluates the local structure similarity by calculating the similarity of each local patch. 
\item \textbf{Normalized Cross Correlation} (NCC): Measures the correlation between the predicted and ground-truth transmission layers after normalizing their overall intensity. 
\item \textbf{Structure Index} (SI): Evaluates the structural similarity between the predicted and ground-truth transmission layers based on their covariance and variance. 
\end{itemize}

\subsubsection{Qualitative Metrics}

Qualitative metrics, on the other hand, are used to subjectively evaluate the visual quality of the predicted transmission images. One common qualitative metric is the \textbf{perceptual user study} \cite{zhang2018single}, where human users compare the predicted transmission images with the ground-truth images and rate their quality.

The choice of evaluation metrics depends on the specific application and the desired properties of the reflection removal algorithm. In general, a combination of quantitative and qualitative metrics is used to provide a comprehensive evaluation of the algorithm's performance.
