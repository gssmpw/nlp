\input{Tables/table1}

\section{Reflection Removal Approaches}
We first present one-stage SIRR approaches in Section~\ref{sec:single-stage}, followed by two-stage and multi-stage approaches in Sections~\ref{sec:two-stage} and~\ref{sec:multi-stage}. The learning objective is introduced in Section~\ref{sec:learning-objective}, with a comparative analysis of models in Table~\ref{tab:1}.

\subsection{Single-Stage Approaches}
\label{sec:single-stage}

In the field of reflection removal, most academic approaches adopt a multi-stage architecture. However, some studies have also proposed one-stage architectures. Given an image \( I \in [0, 1]^{m \times n \times 3} \) with reflections, these approaches typically decompose \( I \) into a transmission layer \( f_T(I; \theta) \) and/or a reflection layer \( f_R(I; \theta) \), using a single network such that $I \approx f_T(I; \theta) + f_R(I; \theta)$, where \( \theta \) represents the network parameters.
ERRNet~\cite{wei2019single} and RobustSIRR~\cite{song2023robust} take \( I \) as input and output only \( T \), while Zhang et al.~\cite{zhang2018single} and YTMT~\cite{hu2021trash} take \( I \) as input and output both \( R \) and \( T \). Additionally, RobustSIRR~\cite{song2023robust} utilizes multi-resolution inputs alongside to enhance feature extraction and improve reflection removal performance.

Zhang et al.~\cite{zhang2018single} propose utilizing a deep neural network with perceptual losses to address the problem of SIRR. ERRNet~\cite{wei2019single} enhances a fundamental image reconstruction neural network by simplifying residual blocks through the elimination of batch normalization, expanding capacity by widening the network to 256 feature maps, and enriching the input image \( I \)  with hypercolumn features extracted from a pretrained VGG-19 network to incorporate semantic information for better performance. RobustSIRR~\cite{song2023robust} presents a robust transformer-based model for SIRR, integrating cross-scale attention modules, multi-scale fusion modules, and an adversarial image discriminator to improve performance. YTMT~\cite{hu2021trash} introduces a simple yet effective interactive strategy called "Your Trash is My Treasure". This approach constructs dual-stream decomposition networks by facilitating block-wise communication between the streams and transferring deactivated ReLU information from one stream to the other, leveraging the additive property of the components.


\subsection{Two-Stage Approaches}
\label{sec:two-stage}

Due to the inherent ambiguity and complexity of SIRR, solving this problem is very challenging. To address this, researchers typically use deep learning models in a cascade or sequence manner, which helps manage the uncertainty in estimating transmission layer while also simplifying the training of SIRR systems.
Some academic approaches adopt a two-stage architecture, where an intermediate output, such as a reflection layer~\cite{feng2021deep,li2023two,hu2023single,kim2020single,wan2020reflection,zhong2024language}, a coarse transmission layer~\cite{feng2021deep,hu2023single,kim2020single,zhong2024language}, an edge map~\cite{wan2019corrn,fan2017generic,zhu2024revisiting}, and so on~\cite{zheng2021single}, is first estimated, followed by the reconstruction of the final transmission layer and/or reflection layer. Besides, the techniques for fusing features across different stages vary among studies. Some methods use convolutional fusion techniques~\cite{wan2019corrn, feng2021deep, li2023two}, while others apply image-level~\cite{fan2017generic, zheng2021single, zhu2024revisiting} or feature-level concatenation~\cite{zhong2024language}. Additionally, certain approaches~\cite{hu2023single, kim2020single, wan2020reflection} do not employ any fusion strategies.

CoRRN\cite{wan2019corrn} proposes a network that uses feature sharing to tackle the problem within a cooperative framework, combining image context and multi-scale gradient information. 
DMGN~\cite{feng2021deep} presents a unified framework for background restoration, employing the Residual Deep-Masking Cell to progressively refine and control information flow. 
The RAG~\cite{li2023two} module is designed to improve the use of the estimated reflection for more accurate transmission layer prediction. 
CEILNet~\cite{fan2017generic} introduces a cascaded pipeline for edge prediction followed by image reconstruction.
DSRNet~\cite{hu2023single} architecture features two cascaded stages and a learnable residue module (LRM). Stage 1 gathers hierarchical semantic information, while Stage 2 refines the decomposition using the LRM to separate components that break the linear assumption.
The structure in~\cite{kim2020single} includes SP-net, which decomposes an input image into the predicted transmission layer and background reflection layer. The BT-net then eliminates glass and lens effects from the predicted reflection, enhancing image clarity and enabling more accurate error matching.
In this paper~\cite{wan2020reflection}, instead of eliminating reflection components from the mixed image, the goal is to recover the reflection scenes from the mixture.
This paper~\cite{zheng2021single} addresses SIRR by incorporating the absorption effect $(e)$, which is approximated using the average refractive amplitude coefficient map. It proposes a two-step solution: the first step estimates the absorption effect from the reflection-contaminated image, and the second step recovers the transmission image using both the reflection-contaminated image and the estimated absorption effect.
The framework~\cite{zhu2024revisiting} consists of RDNet and RRNet, where RDNet utilizes a pretrained backbone with residual blocks and interpolation to estimate the reflection mask, and RRNet uses this estimate to assist in the reflection removal process.
This paper~\cite{zhong2024language} addresses language-guided reflection separation by using language descriptions to provide layer content. It proposes a unified framework that uses cross-attention and contrastive learning to align language descriptions with image layers, while a gated network and randomized training strategy help resolve layer ambiguity.



\subsection{Multi-Stage Approaches}
\label{sec:multi-stage}

Some studies extend beyond a two-stage architecture by using a multi-stage cascaded structure. Similar to the two-stage design, the multi-stage approach generates intermediate outputs in a recurrent fashion, eventually reconstructing the final transmission and/or reflection layer.
Some methods use convolutional fusion techniques~\cite{prasad2021v}, while others utilize concatenation~\cite{yang2018seeing, li2020single, chang2021single, dong2021location}.

DBN~\cite{yang2018seeing} introduces a cascaded deep neural network that simultaneously estimates both background and reflection components. The network follows a bidirectional approach: first using the estimated background to predict the reflection, and then refining the background prediction using the estimated reflection. This dual-estimation strategy improves reflection removal performance.
IBCLN~\cite{li2020single} is designed for reflection removal by progressively refining the estimates of the transmission and reflection layers, with each iteration improving the prediction of the other. By utilizing LSTM to transfer information between steps and incorporating residual reconstruction loss, IBCLN tackles the vanishing gradient issue and improves training across multiple cascade steps.
The model in~\cite{chang2021single} takes a reflection-contaminated image and separates it into the reflection and transmission layers. To ensure high-quality transmission, three auxiliary techniques are employed: edge guidance, a reflection classifier, and recurrent decomposition.
This paper~\cite{dong2021location} presents a LANet for SIRR. It employs a reflection detection module that generates a probabilistic confidence map using multi-scale Laplacian features. The network, designed as a recurrent model, progressively refines reflection removal, with Laplacian kernel parameters highlighting strong reflection boundaries to improve detection and enhance the quality of the results.
V-DESIRR~\cite{prasad2021v} introduces a lightweight model for reflection removal using an innovative scale-space architecture, which processes the corrupted image in two stages: a Low Scale Sub-network (LSSNet) for the lowest scale and a Progressive Inference (PI) stage for higher scales. To minimize computational complexity, the PI stage sub-networks are significantly shallower than LSSNet, and weight sharing across scales enables the model to generalize to high resolutions without the need for retraining.



\subsection{Learning Objective}
\label{sec:learning-objective}

To train SIRR models, several commonly used loss functions are combined to ensure high-quality reflection removal. These include Reconstruction Loss, Perceptual Loss\cite{johnson2016perceptual}, and Adversarial Loss\cite{goodfellow2014generative}. Each of these loss functions contributes to different aspects of the modelâ€™s learning process:

\textbf{Reconstruction loss} is typically defined using the $L1$ or $L2$ loss, which directly measures the pixel-wise difference between the predicted reflection-free image and the ground truth image. This loss ensures that the output image is as close as possible to the desired reflection-free image in a pixel-wise sense. However, relying solely on this loss can lead to overly smooth results, as it does not consider high-level perceptual differences. The $L1$ loss formulation is as follows:

\begin{equation}
    \mathcal{L}_{\text{rec}} = \| \hat{T} - T \|_1
\end{equation}

where \( \hat{T} \) is the predicted reflection-free image and \( T \) is the ground truth image.

To mitigate the oversmoothing effect of reconstruction loss and preserve important structural details, \textbf{Gradient Consistency Loss} is utilized in~\cite{wei2019single}. This loss ensures that the predicted transmission layer \( \hat{T} \) retains the edge structures of the ground truth \( T \) by minimizing the difference between their gradients along both the \( x \)- and \( y \)-directions:

\begin{equation}
    \mathcal{L}_{\text{grad}} = \|\nabla_x \hat{T} - \nabla_x T\|_1 + \|\nabla_y \hat{T} - \nabla_y T\|_1
\end{equation}

where \( \nabla_x \) and \( \nabla_y \) are the gradient operators along the horizontal and vertical directions, respectively.

By combining these two losses, SIRR achieves a balance between accurate pixel-wise reconstruction and the preservation of structural details.



\textbf{Perceptual loss} utilizes a pre-trained deep neural network (e.g., VGG) to extract high-level feature representations of both the predicted and ground truth images. Instead of measuring pixel-wise differences, this loss compares the differences in feature space, making the generated images more visually realistic and closer to human perception. The perceptual loss can be expressed as:

\begin{equation}
    \mathcal{L}_{\text{per}} = \sum_{i} \| \phi_i(\hat{T}) - \phi_i(T) \|_1
\end{equation}

where \( \phi_i(\cdot) \) represents the feature map extracted from the \( i \)-th layer of the pre-trained network.


\textbf{Adversarial loss} is inspired by Generative Adversarial Networks (GANs) and is used to improve the realism of the generated images. A discriminator \( D \) is introduced to distinguish between real reflection-free images and the generated images. The adversarial loss is formulated as:

\begin{equation}
    \mathcal{L}_{\text{adv}} = \mathbb{E}[\log D(T)] + \mathbb{E}[\log (1 - D(\hat{T}))]
\end{equation}

where \( D(\cdot) \) represents the discriminator network. The generator aims to minimize this loss, making the generated images indistinguishable from real ones.



In addition to reconstruction loss, perceptual loss, and adversarial loss, other loss functions are also utilized to enhance reflection removal performance. One such example is the \textbf{exclusion loss}~\cite{li2023two,zhang2018single}, which encourages the separation of the transmission (\(T\)) and reflection (\(R\)) layers by minimizing their structural correlation. This loss enforces gradient decorrelation at multiple scales. It is formulated as:


\begin{equation}
    \mathcal{L}_{\text{excl}} = \frac{1}{N+1} \sum_{n=0}^{N} \sqrt{\|\Psi(T\downarrow^n, R\downarrow^n)\|_F}
\end{equation}

where \(T\downarrow^n\) and \(R\downarrow^n\) are the downsampled versions at different scales, $\|\cdot\|_F$ is the Frobenius norm, and $\Psi(T, R)$ measures the correlation between their gradients.

% \begin{equation}
%     \Psi(T, R) = \tanh(\lambda_T |\nabla T|) \circ \tanh(\lambda_R |\nabla R|)
% \end{equation}

% measures the correlation between their gradients. The Frobenius norm \(\|\cdot\|_F\) quantifies this correlation. The normalization factors are set as:

% \begin{equation}
%     \lambda_T = \frac{1}{2}, \quad \lambda_R = \frac{\|\nabla T\|_1}{\|\nabla R\|_1}.
% \end{equation}


\textbf{Total Variation Loss} is a regularization technique commonly used in image processing tasks to promote smoothness and reduce noise or artifacts. It encourages spatial continuity by minimizing the differences between neighboring pixels, preventing excessive sharp variations. TVLoss is particularly useful in reflection removal~\cite{zhu2024revisiting}, denoising, and super-resolution tasks, where it helps generate cleaner and more visually appealing results by reducing undesired texture artifacts while preserving important image structures.


\textbf{Contextual Loss} is used in~\cite{prasad2021v} to preserve fine-grained details in image generation tasks by focusing on feature similarity rather than direct pixel-wise differences. The formula for Contextual Loss is typically expressed as:

\[
\mathcal{L}_{\text{CX}}(F, F^*) = - \log \left( \max_j \ \text{CX}_{ij} \right)
\]

where \( F \) and \( F^* \) represent feature activations from a pre-trained network for the generated and target images, respectively. The contextual similarity \( \text{CX}_{ij} \) measures the correlation between feature vectors, ensuring that the generated image retains important structural patterns from the reference. CX Loss is particularly useful in reflection removal, style transfer, and image synthesis, as it helps maintain perceptual consistency while allowing flexibility in pixel arrangements.


In addition to traditional loss functions, some evaluation metrics are also directly used as loss terms. For instance, Zheng et al.~\cite{zheng2021single,wan2019corrn} directly incorporates PSNR, SSIM, and SI. PSNR ensures high-fidelity reconstruction, SSIM preserves structural similarity, and SI enhances overall structural consistency. By combining these metrics, the model improves reflection removal performance.
