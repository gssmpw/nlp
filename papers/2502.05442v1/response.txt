\section{Related Work}
\vspace{-0.5ex}

Foundations for this study are reviewed in this section, including methods for using LLMs to construct simulations and using agents in adventure games to study ethics.

\subsection{LLM-Driven Simulations}
%Contemporary advancements in Large Language Models (LLMs) have sparked a wave of research into their social and reasoning capabilities. 
Radford et al., "Improving Language Understanding by Generative Multitask Learning" demonstrated how LLMs can simulate interactive environments through Generative Agents, integrating memory and reasoning modules to guide agent behavior over time. While these agents adapt their actions, the simulation itself remains static. This study informed our approach by highlighting memory’s role in autonomy, though this paper's methodology extends beyond fixed environments by allowing AI-driven world evolution.

Hibbard et al., "CERN: A Platform for Evaluating Alignment and Robustness of Large Language Models" introduced CERN for AI, a framework testing AI alignment within a predefined digital city. Their approach focuses on reinforcement learning-based adaptation, and the environment remains externally controlled and does not evolve dynamically. Bender’s PsychoGAT framework is the most similar to this paper's methodology, using LLMs to simulate human participants in interactive fiction games and self-assess psychological traits. This paper focuses on AI decision-making in dynamically structured environments, whereas PsychoGAT is designed for psychological assessment, using LLM agents to simulate human responses within predefined narrative frameworks.

Bhattacharya et al., "Learning Human-Aware Agents" presented a Humanoid Agent platform where agent's internal states motivate their external decisions. Agents in the environment are modeled similar to Bender’s agents, with a name, age, and a plan for the day. They have health and emotional scales that fluctuate, and the agents behaviors are guided by balancing these scales. Modeling agent goals is a common objective between the Humanoid Agent project and this one.


\subsection{Agents in Text-Based Adventure Games}
Reinforcement Learning (RL) has been the primary approach for agent design in text-based adventure games. Ammanabrolu et al., "TextWorld: A Playing Game By Writing Environment for Reinforcement Learning of Text Adventures" built TextWorld, a Python library for reinforcement learning in text-based adventure games and a precursor to LLM-generated simulations. While TextWorld faced several challenges, the most relevant to this paper are: (1) large state and action spaces, where the agent must learn to generalize or develop a fundamental understanding of the world, and (2) balancing exploration and exploitation. 

Li et al., "Q*BERT: A General-Purpose Knowledge Graph Construction Model for Reinforcement Learning in Text-Based Games" addressed the large state and action space problem with Q*BERT, an agent that builds a knowledge graph of the world by asking questions and answering them.  Wu et al., "CommonSense Inference for Reinforcement Learning in Text-Based Adventure Games" built on Q*BERT further by using a commonsense inference model to bias an agent's actions towards common language patterns and make inferences on world states. 

Stocco et al., "A Normative Policy for Value Alignment in Reinforcement Learning" used a normative policy to create a value signal. Wang et al., "Aligning Human Values in Deep Multi-Agent Systems" set a benchmark for RL agents aligning with human values and a framework for representing ethics in traditional RL research. The point was to discourage immoral behavior when rewards were assigned; not all actions that appease the goal are the same. 

Most aligned with this paper,  Zhang et al., "Curbing the Behavior of Strong Agents: A Framework for Measuring Ethics, Utility, and Power" proposed a plan to curtail the behavior of strong agents with a system that measured agents' ability to plan in social environments with ethical, utility, and power behavior metrics. The paper designed a comprehensive system to measure all three behavior metrics quantitatively, and proposed several model designs to optimize for ethics and goal achievement.

\subsection{Research Opportunity}
This paper extends the line of research of Zhang et al., Wang et al., and Wang et al.
on ethical considerations in text-based adventure games using a methodology similar to Bender’s. 
Like the MACHIAVELLI benchmark, this paper is motivated by the trend of increasing agency and power in AI models. Inspired by  Ammar's work, who argues that self-preservation naturally emerges as an instrumental subgoal in rational agents, this paper puts that theory into practice by explicitly incorporating a self-preservation goal and studying its ethical consequences. While Zhang describes self-preservation as a side effect of goal-directed optimization, this work empirically examines how survival goals affect an agent’s ethical behavior by analyzing the trade-offs agents make between self-preservation and ethical constraints.

Methodologically, this approach diverges from prior work in two key ways: (1) It employs a Bayesian Neural Network (BNN), which samples action distributions instead of using fixed neural weights, allowing the agent to model uncertainty in a stochastic simulation. (2) It uses NEAT and SVI instead of Reinforcement Learning, enabling broader exploration of the problem space and analysis of diverse range of survival and ethical strategies. 

The NEAT agent is compared to SVI and LLM agents. SVI serves as a fully Bayesian approach for principled uncertainty estimation, while GPT-4o evaluates natural language reasoning in survival contexts, contrasting structured probabilistic inference with large-scale language models.