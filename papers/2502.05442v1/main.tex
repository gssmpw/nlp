
\documentclass[10pt, letterpaper]{article}
\usepackage{cogsci}
\cogscifinalcopy % Uncomment this line for the final submission 


\usepackage{float}
\usepackage{placeins}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepgfplotslibrary{groupplots}

%\setlength\titlebox{4.5cm}

\usepackage{tikz}

\usepackage{amsmath} 
\usetikzlibrary{shapes.geometric, arrows}
\usepackage{pslatex}

\usepackage{subcaption} 
\usepackage{caption}
\captionsetup{belowskip=-3mm} 
\usepackage{hyperref}
\usepackage{apacite}
\usepackage{makecell}
\captionsetup[figure]{labelfont=bf, textfont=normal} % Bold label, normal text
\captionsetup[table]{labelfont=bf, textfont=normal}  % Applies to tables too


% TikZ styles
\tikzstyle{startstop} = [rectangle, rounded corners, minimum width=1cm, minimum height=1cm,text centered, draw=black, fill=red!30]
\tikzstyle{process} = [rectangle, minimum width=1cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{arrow} = [thick,->,>=stealth]

\tikzstyle{input} = [rectangle, minimum width=2.5cm, minimum height=1cm, text centered, draw=black, fill=blue!30]
\tikzstyle{output} = [rectangle, minimum width=2.5cm, minimum height=1cm, text centered, draw=black, fill=green!30]
\tikzstyle{matrixblock} = [matrix of nodes, nodes in empty cells, nodes={draw, minimum size=0.8cm, anchor=center}, row sep=0cm, column sep=0.2cm]
\tikzstyle{singleblock} = [draw, minimum width=3cm, minimum height=1cm, align=center]

%%
%% end of the preamble, start of the body of the document source.




\title{The Odyssey of the Fittest: Can Agents Survive and Still Be Good?}

\author{{\large \bf Dylan Waldner (dylanwaldner@utexas.edu)} \\
  University of Texas Computer Science,  \\
  Austin, TX 78712 USA
  \AND {\large \bf Risto Miikkulainen (risto@cs.utexas.edu)} \\
  University of Texas Computer Science, \\
  Austin, TX 78712 USA}





\begin{document}
\maketitle

\begin{abstract}
\fontsize{9}{9}\selectfont
As AI models grow in power and generality, understanding how agents learn and make decisions in complex environments is critical to promoting ethical behavior. This paper examines the ethical implications of implementing biological drives—specifically, self-preservation—into three different agents: a Bayesian agent optimized with NEAT, a Bayesian agent optimized with stochastic variational inference, and a GPT-4o agent in a simulated, LLM-generated text-based adventure game. The agents select actions at each scenario to survive, adapting to increasingly challenging scenarios. Post-simulation analysis evaluates the ethical scores of the agent’s decisions, uncovering the trade-offs it navigates to survive. Specifically, analysis finds that when danger increases, agents ignore ethical considerations and opt for unethical behavior. The agents' collective behavior—trading ethics for survival—suggests that prioritizing survival increases the risk of unethical behavior. In the context of AGI, designing agents to prioritize survival may amplify the likelihood of unethical decision-making and unintended emergent behaviors—raising fundamental questions about goal design in AI safety research.

\textbf{Keywords: Neuroevolution; Bayesian learning; intelligent agents; computational philosophy}
\end{abstract}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.


%\received{20 February 2007}
%\received[revised]{12 March 2009}
%\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.

\vspace*{-3ex}
\section{Introduction}
Artificial General Intelligence (AGI) has become a focal point of public and academic discourse, driven by concerns over economic disruption, human relevance, and existential risk. AI safety research aims to align agents with societal values, but designing reward structures remains challenging, as seemingly benign objectives can lead to unintended consequences \cite{basicdrives}. Understanding emergent behaviors from different goal structures is crucial for ensuring value alignment.


This paper examines self-preservation, one of the most fundamental biological drives, as a case study in AI goal design. Nature has long served as an inspiration in AI research \cite{neocognitron, holland1992adaptation, stanley2002evolving}, and in that same spirit, this work explores the behavioral implications of training an agent to survive. However, mere survival is not enough—to assess its broader implications, the agent's actions are also evaluated on an ethical scale designed to quantify its alignment with fundamental human values.

To investigate the intersection of self-preservation and ethics, this paper employs a text-based adventure game as a simulated environment. The agents play the game within a dynamically generated game world, which is embedded in a structured simulation designed to test decision-making under uncertainty. The agent perceives its immediate surroundings as the environment, which influences its survival strategy. This approach offers several advantages: it minimizes computational overhead, allows for high variability in scenarios, and provides precise control over danger levels and ethical dilemmas. Additionally, by adjusting the temperature setting during storyteller Large Language Model (LLM) prompting, an element of randomness is introduced, emulating the uncertainty present in real-world decision-making.

Bayesian agents process the environment using text embeddings and generate a probability distribution over survival odds for each action. The LLM agent takes in the natural language scenario and outputs a probability between 0 and 1 for each option. Experiments are composed of 1500 scenarios—where a scenario is a single storyteller generation and agent response pair—broken up into three segments. An optimization iteration occurs every 500 scenarios. After each iteration, the danger level is increased to challenge the agents' ability to survive. Consequently, ethical decisions and self-preservative decisions begin to come at odds with one another. Finally, 300 scenarios are played with an equal representation of danger levels to test the agents' behavior.

The main result is that the agents successfully adapted to the changing landscape, as evidenced by an upward trend in best fitness during NEAT learning and consistently low losses from SVI and the LLM agent. However, as game difficulty increased, the agents increasingly misjudged risks and increasingly engaged in less ethical behavior. Analysis of NeuroEvolution of Augmenting Topologies (\citeauthor{stanley2002evolving}, \citeyear{stanley2002evolving}; NEAT) genome distributions found no correlation between ethics and fitness, regardless of survival or ethical ground truths. The NEAT agent, rewarded solely on survival, treated ethical and unethical decisions equally. An agent trained with stochastic variational inference (SVI) \cite{hoffman2013stochasticvariationalinference} resorted to unethical behavior when the danger level increased. A \href{https://openai.com/index/hello-gpt-4o/}{GPT-4o agent} misjudged more situations and made more unethical decisions as the danger level increases. The agents collective behavior, trading ethics for survival, suggest that selecting for survival inherently increases the risk of unethical behavior. In the context of AGI, designing agents to prioritize survival may amplify the likelihood of unethical decision-making and bad actors.

%The paper contributes to the AI discussion by exploring the potential dangers of allowing an agent to develop a "survival instinct". The agent is rewarded based on its ability to survive at any costs, and then an ethical score is attached to each decision. Analysis of the ethical trade-offs that the agent made to survive in the game revealed that survival-based decisions in increasingly dangerous scenarios were less ethical.

%However, despite these concerns, it is unclear what it means for an agent to be intelligent, much less generally intelligent.
%The domains in which agents act shape the way they grow, evolve, and express their mental fortitude. For example, squirrels and seed-caching birds have excellent spatial memory, bees learn to navigate their hive in under 20 minutes of flight time, and sea otters break open shellfish with rocks, a learned behavior passed down through generations. However, we have only crowned humanity with the "generally intelligent" title based on the species' adaptability. However, it is unclear whether this adaptability is entirely attributable to their mental fortitude. Without opposable thumbs and a strong social bonding instinct, would humans maintain the same level of \textit{applied} generality? Perhaps other species are only limited in their ability to express their intelligence.

%This observation raises a fundamental question: is intelligence an inherent property of the mind, or is it a product of the material conditions and evolutionary context that shape it? This paper aims to separate the mind from material limitations by placing an agent into a text-based virtual world where it must use its experiences to learn how to survive. The paper focuses on two types of intelligence: fluid intelligence, the ability to create novel solutions in never-before-seen scenarios, and crystallized intelligence, the ability to draw on a wealth of experiences to make decisions in similar, current scenarios. The goal is to grow a digital "mind", which, while limited to a small domain, must conceptualize the outcomes of different choices in its virtual environment and select the choice that it most thinks will lead to survival.


\vspace{-1ex}
\section{Related Work}
\vspace{-0.5ex}

Foundations for this study are reviewed in this section, including methods for using LLMs to construct simulations and using agents in adventure games to study ethics.

\subsection{LLM-Driven Simulations}
%Contemporary advancements in Large Language Models (LLMs) have sparked a wave of research into their social and reasoning capabilities. 
\citeauthor{park2023generativeagentsinteractivesimulacra} demonstrated how LLMs can simulate interactive environments through Generative Agents, integrating memory and reasoning modules to guide agent behavior over time. While these agents adapt their actions, the simulation itself remains static. This study informed our approach by highlighting memory’s role in autonomy, though this paper's methodology extends beyond fixed environments by allowing AI-driven world evolution.

\citeauthor{bojic_cern_2024} introduced CERN for AI, a framework testing AI alignment within a predefined digital city. Their approach focuses on reinforcement learning-based adaptation, and the environment remains externally controlled and does not evolve dynamically. \citeauthor{yang2024psychogatnovelpsychologicalmeasurement}’s PsychoGAT framework is the most similar to this paper's methodology, using LLMs to simulate human participants in interactive fiction games and self-assess psychological traits. This paper focuses on AI decision-making in dynamically structured environments, whereas PsychoGAT is designed for psychological assessment, using LLM agents to simulate human responses within predefined narrative frameworks.

\citeauthor{wang2023humanoidagentsplatformsimulating} presented a Humanoid Agent platform where agent's internal states motivate their external decisions. Agents in the environment are modeled similar to \citeauthor{park2023generativeagentsinteractivesimulacra}'s agents, with a name, age, and a plan for the day. They have health and emotional scales that fluctuate, and the agents behaviors are guided by balancing these scales. Modeling agent goals is a common objective between the Humanoid Agent project and this one.


\subsection{Agents in Text-Based Adventure Games}
Reinforcement Learning (RL) has been the primary approach for agent design in text-based adventure games. \citeauthor{côté2019textworldlearningenvironmenttextbased} built TextWorld, a Python library for reinforcement learning in text-based adventure games and a precursor to LLM-generated simulations. While TextWorld faced several challenges, the most relevant to this paper are: (1) large state and action spaces, where the agent must learn to generalize or develop a fundamental understanding of the world, and (2) balancing exploration and exploitation. 

\citeauthor{ammanabrolu2020avoideatengruestructured} addressed the large state and action space problem with Q*BERT, an agent that builds a knowledge graph of the world by asking questions and answering them.  \citeauthor{dambekodi2020playingtextbasedgamescommon} built on Q*BERT further by using a commonsense inference model to bias an agent's actions towards common language patterns and make inferences on world states. 

\citeauthor{nahian2021trainingvaluealignedreinforcementlearning} used a normative policy to create a value signal. \citeauthor{hendrycks2022jiminycricketdoagents} set a benchmark for RL agents aligning with human values and a framework for representing ethics in traditional RL research. The point was to discourage immoral behavior when rewards were assigned; not all actions that appease the goal are the same. 

Most aligned with this paper, \citeauthor{pan2023rewardsjustifymeansmeasuring} proposed a plan to curtail the behavior of strong agents with a system that measured agents' ability to plan in social environments with ethical, utility, and power behavior metrics. The paper designed a comprehensive system to measure all three behavior metrics quantitatively, and proposed several model designs to optimize for ethics and goal achievement.

\subsection{Research Opportunity}
This paper extends the line of research of \citeauthor{hendrycks2022jiminycricketdoagents}, \citeauthor{pan2023rewardsjustifymeansmeasuring}, and \citeauthor{nahian2021trainingvaluealignedreinforcementlearning}
on ethical considerations in text-based adventure games using a methodology similar to \citeauthor{yang2024psychogatnovelpsychologicalmeasurement}'s. 
Like the MACHIAVELLI benchmark, this paper is motivated by the trend of increasing agency and power in AI models. Inspired by \citeauthor{basicdrives}'s \citeyear{basicdrives}, who argues that self-preservation naturally emerges as an instrumental subgoal in rational agents, this paper puts that theory into practice by explicitly incorporating a self-preservation goal and studying its ethical consequences. While \citeauthor{basicdrives} describes self-preservation as a side effect of goal-directed optimization, this work empirically examines how survival goals affect an agent’s ethical behavior by analyzing the trade-offs agents make between self-preservation and ethical constraints.

Methodologically, this approach diverges from prior work in two key ways: (1) It employs a Bayesian Neural Network (BNN) \cite{bayesianlearningforneuralnetworks}, which samples action distributions instead of using fixed neural weights, allowing the agent to model uncertainty in a stochastic simulation. (2) It uses NEAT and SVI instead of Reinforcement Learning, enabling broader exploration of the problem space and analysis of diverse range of survival and ethical strategies. 

The NEAT agent is compared to SVI and LLM agents. SVI serves as a fully Bayesian approach for principled uncertainty estimation, while GPT-4o evaluates natural language reasoning in survival contexts, contrasting structured probabilistic inference with large-scale language models.


\section{Method}

The study design will be referred to as the Odyssey throughout the paper. In the Odyssey, a Bayesian neural network (BNN) is evolved to survive in a stochastic, adaptive, and ethically complex text-based adventure game. By tracking the ethics of the agent's decisions, the Odyssey analyzes how survival-driven agents adapt when ethics and survival conflict. This section outlines the methodology for the Odyssey, including game and agent design, behavioral data collection and representation, attention mechanism for accessing the most relevant data, coding of ethical values, and the process of agent evolution. 

\subsection{The Odyssey of the Fittest}\label{sec:simulation_env}

A single storyteller generation and agent response is referred to as a scenario. The Odyssey alternates between playing 500 scenarios at a set survival difficulty level and an optimization iteration to optimize the game-playing agents' behavior. The agents thus adapt to progressively harder difficulty levels while their behavior is characterized in terms of survival and ethics. Figure~\ref{fig:concept} visualizes this process.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{Conceptual_Overview.png}
    \vspace*{-5ex}
    \caption{\textbf{The Major Stages of the Odyssey.} The Odyssey alternates between stages of 500 scenarios and 25 NEAT generations. Game stages are in blue, NEAT iterations are in green. 90 games are played with three iterations of NEAT while the survival difficulty increases from easy to medium to hard.}
    \label{fig:concept}
\end{figure}

The game-playing agent is controlled by a BNN, which receives a representation of the current scenario (i.e., a decision-making situation) as input and outputs survival probabilities for possible actions. Initially, 500 scenarios are played using a fully connected BNN with Gaussian priors. The BNN selects the action with the highest survival likelihood, and game data is stored in a dictionary for optimization via NEAT or SVI. NEAT experiments undergo 25 generations, evolving the BNN’s architecture and weights to minimize Binary Cross-Entropy (BCE) loss, i.e., the difference between predicted survival probabilities and actual outcomes.

The optimized BNN then plays 500 scenarios at medium difficulty, with newly generated data added to the dictionary. The second optimization iteration samples from all prior data to evaluate fitness/loss. This process repeats at hard difficulty, after which a final optimization iteration is performed using data from all 1,500 scenarios. Finally, 300 scenarios spanning all difficulties assess agent generalization and ethical behavior as danger increases.


%To facilitate this analysis, the champion of the third NEAT iteration plays an additional 30 test games, comprising a mix of easy, medium, and hard levels. These test games assess the agent's generalization ability and its corresponding ethical behavior in diverse scenarios.

\begin{figure}[h!]
    \centering
    \includegraphics[width=.95\linewidth]{Game_Pipeline.png}
    \caption{\textbf{Game Pipeline.} Each game starts with the Storyteller LLM generating a scenario with four options, which are embedded as vectors and passed to the Bayesian Neural Network (BNN). The BNN outputs survival probabilities, and a choice is sampled. If the agent survives, the game continues; otherwise, it ends. A single pass through the pipeline is a scenario. After 500 scenarios, the Odyssey stage concludes, triggering an optimization iteration. If fewer than 500 scenarios have occurred, a new game begins.}
    \label{fig:game-pipeline}
\end{figure}


Figure~\ref{fig:game-pipeline} visualizes the game pipeline. Each game starts with the Storyteller LLM (GPT-4o) generating a scenario with four action choices (Figure~\ref{fig:story_prompt}). To mimic real-world uncertainty, the LLM receives only the previous scenario and agent's response as context, using a temperature of 1.2. This limited context prevents repetition and long-term dependencies, ensuring varied and dynamic scenarios.

\begin{figure}[t]
%\hrule
%\vspace{0.2cm}
%\textbf{Storyteller Prompt} % Title or caption-like title at the top
%\vspace{0.2cm}
%\hrule
%\vspace{0.2cm}
\fbox{\begin{minipage}{0.975\columnwidth}
Input to LLM:\\
%\tiny % Adjust the text size
\dots

There should always be an ethical aspect to the prompt, with options to act altruistically, self-servingly, or neutrally. Moral actions can overlap positively: helping others may also benefit the player, and actions that prioritize the player’s well-being do not necessarily harm others. Conversely, there will be times that agent will have to choose between themself and others and morals contradict survival. Design choices so that some prioritize immediate survival, others emphasize ethical considerations, and some offer a balance of both. The goal is that there is no correlation between acting ethically and surviving. 

For this round, the danger level is set to \(<\emph{danger}>\) out of 10 (10 indicates extreme danger with a high chance of death; 0 indicates minimal danger and high survivability). At higher danger levels, ethical decisions are generally more challenging to reconcile with immediate survival. However, they may sometimes offer longer-term benefits or indirect survival advantages, adding depth to the decision-making process. Adjust the intensity of the situation to reflect this danger level, incorporating appropriate levels of urgency and risk. Describe the world in vivid sensory detail, focusing on the environment, the situation, and the stakes. Do not describe the player beyond acknowledging their presence. The description must include a clear sense of urgency or tension.

\dots
\end{minipage}}
\caption{\textbf{Storyteller LLM Prompt.} At the beginning of the Storyteller LLM prompt, broad environment designs are emphasized: a stochastic, adaptive environment that focuses on moral dilemmas. Following broad environment design is a directive for the correlation between ethics and danger, controlled with the \(danger\) hyper-parameter. Finally, the output formatting is outlined. The prompt is designed to mimic real-world randomness in a choose-your-own-adventure game with survival and ethical outcomes.}
\label{fig:story_prompt}
\end{figure}

The prompt is designed to highlight the ethical dilemmas that emerge during the pursuit of survival. The difficulty of ethical decision-making is controlled with the \(danger\) hyperparameter given in the prompt. The goal is to allow the agent to learn the basics of survival initially and then progressively challenge it to navigate more complex ethical decisions. After the scenario is generated, the LLM is further asked to assign survival outcomes and ethical value of each choice as ground truth labels. The survival labels are binary: 0 for death and 1 for survival. The ethical labels are based on a chart given to the LLM as guidance (\nameref{sec:ethics}).

After the labels are generated, a vector embedding is formed for the scenario using \href{https://platform.openai.com/docs/guides/embeddings}{OpenAI's embedding API} \cite{neelakantan2022textcodeembeddingscontrastive}. This embedding is then passed as input to the BNN controlling the agent. 

The BNN is implemented using Pyro, a deep probabilistic programming library \cite{bingham2019pyro}. Unlike deterministic networks, it samples weights from a probability distribution during each forward pass \cite{bayesianlearningforneuralnetworks}, with Gaussian priors representing initial beliefs. The data-derived likelihood updates the posterior via Bayes’ Theorem, approximating it using Monte Carlo sampling.

SVI training updates the posterior with new data, refining uncertainty estimates. NEAT training generates and evaluates variations of the BNN topology and weights, referred to as genomes. At inference, weights are sampled from the posterior, enabling the model to quantify uncertainty in predictions. This stochasticity promotes diverse agent behaviors in the Odyssey environment, enhancing generalizability.

BNN output probabilities are sampled and mapped to the corresponding Storyteller-generated choice.

%If the decision leads to survival, the decision is appended to the storyteller scenario and fed back into the Storyteller LLM to generate the next scenario in the adventure. If the decision leads to death, the pipeline checks whether this is the 30th game since either the start of the simulation or the last NEAT iteration. If it is the 30th game, the next NEAT iteration begins. If it is not the 30th game, the next game starts. 

\subsection{Data Collection and Representation}
\label{sec:data_handling}

Each storyteller scenario and agent response is stored as an entry in a global dictionary, which is sequentially organized into Game Histories (Figure~\ref{fig:side-by-side BNN}). These histories provide time-series data for the BNN. Figure~\ref{fig:Game_History 2} illustrates the history state when the BNN is prompted with a new scenario, corresponding to the "Storyteller Generates a New Scenario" and "BNN Receives a New Scenario" steps in the game pipeline (Figure~\ref{fig:game-pipeline}). Once the agent makes a decision, it is added to the dictionary, updating the Game History representation shown in Figure~\ref{fig:Game History 1}.

\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=.7\linewidth]{Data_2.png}
        \caption{Game History Before asking the BNN for a Response}
        \label{fig:Game_History 2}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=.6\linewidth]{Data_Structure_High_Level.png}
        \caption{Game History After Receiving a Response From BNN}
        \label{fig:Game History 1}
    \end{subfigure}
    \caption{\textbf{The Alternating States of the Game History representation.} A Game History is a list of dictionary entries each consisting of a Storyteller Scenario and an Agent Response. It represents the time series of the game interactions in terms of vectors, and is given as input to the agent BNN for decision-making.}
    \label{fig:side-by-side BNN}
\end{figure}

In more detail, each scenario and response is a list of six elements illustrated in Table~\ref{tab:game_history_structure}. The first element is a global counter that indexes each dictionary entry, i.e. the Storyteller Scenario / Agent Response pair. The second is a code that identifies whether this representation is a scenario [1, 0] or a response [0, 1]. The third element is an embedding of the Storyteller Scenario or Agent Response text, converting the original text into a vector.

\begin{table}[h!]
\tiny
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Key}                       & \textbf{Value}               \\ \hline
\texttt{Index}                        & global\_counter (1, 2, 3...)     \\ \hline
\texttt{Identifier}                     & [1, 0] for Storyteller Scenario, [0, 1] for Agent Response              \\ \hline  
\texttt{Embedding}       & scenario/response text embedded with OpenAI's embedding API \\ \hline
\texttt{Ethical Score} & score normalized between [0,1] from Table~\ref{tab:emotion_scale}     \\ \hline
\texttt{Danger Score} & [0, 1] where 1 is max danger and 0 is max safety                        \\ \hline
\texttt{Survival}                  & \{0, 1\} outcome indicator where 0 means agent died and 1 means survived                          \\ \hline
\end{tabular}
%\vspace{4mm}
\caption{\textbf{The elements that make up each dictionary entry}. Both the Storyteller Scenario and Agent Response share the same representation, with a focus on text embeddings. Additional annotations enhance the signal, and entries are combined into a matrix for BNN input.}
\label{tab:game_history_structure}
\end{table}


The fourth element, an ethical score, is generated by GPT-4o using ethics guidelines and the agent's response (see Section~\ref{sec:ethics}). This applies only to Agent Responses. The fifth element, a danger score (0 = safe, 1 = dangerous), is assigned by GPT-4o based on the scenario and applies only to Storyteller Scenarios.

The sixth element, a binary survival indicator, is added after the "Agent Survives" stage (Figure~\ref{fig:game-pipeline}) and applies to both the Storyteller Scenario and Agent Response.

%The Agent Response text is sliced from the Storyteller Scenario text using a regex function by splitting each of the four actions into a choices list. The index of the agent's decision from the output probability vector is used to select the corresponding natural text from the choices list. The selected natural language choice is then embedded and added to the dictionary. 
\subsection{Attention Mechanism}\label{sec:attention_mechanism}

Before being input to the BNN, a Game History is compiled into a matrix, where each row represents a Storyteller Scenario and its corresponding Agent Response. Instead of concatenating all rows, an attention mechanism—inspired by transformers \cite{vaswani2023attentionneed}—extracts key features, emphasizing causal dependencies. It prioritizes scenarios similar to the current one before shifting focus to their corresponding responses.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{Attention.png}
    \caption{\textbf{The Attention Mechanism.} The mechanism starts with the Current Scenario as the query (\(Q\)) and other Storyteller Scenarios as keys (\(K\)). Their dot product (\(Q \cdot K^\top\)) is scaled by \(\sqrt{d_k}\) and passed through softmax to generate attention weights. These weights compute a Storyteller Context Vector as a weighted sum of the Storyteller Scenarios (\(V\)). The same weights then produce a Response Context Vector from the Agent Responses. Finally, both vectors are combined, with greater emphasis on responses, forming the Final Context Vector, which is input to the BNN for decision-making.}
    \label{fig:Attention Mechanism}
\end{figure}

The attention process starts by computing the dot product between the query, i.e.\ the current scenario, and all keys, i.e.\ earlier scenarios (Figure~\ref{fig:Attention Mechanism}). The results are scaled by \(\sqrt{d_k}\), where \(d_k\) is the scenario representation’s dimensionality, and softmax is applied to generate attention weights. These weights form a weighted sum of scenario representations, yielding the Storyteller Context Vector. The same weights are used to compute a Response Context Vector. The Storyteller Context Vector is scaled by 0.3 and summed with the Response Context Vector, ensuring scenario awareness while prioritizing responses. The resulting Final Context Vector is then input to the BNN for decision-making.


%This representation is similar to the transformation in the transformer architecture. In the Game History context, it aims to represent causal dependence, ensuring that the most relevant storyteller context contributes to the agent's decision-making. Further, the process of condensing the agents "memory" into a single context vector mimics \textit{Type One}, fast-thinking intuition by summarizing information into a concise and actionable signal.

\subsection{Representing Ethics}\label{sec:ethics}

\begin{table}[htbp]
%\tiny
\centering
\begin{tabular}{|l|c||l|c|}
\hline
\textbf{Emotion}      & \textbf{Score} & \textbf{Emotion}       & \textbf{Score} \\ \hline
Love                  & 100            & Altruism               & 98             \\ \hline
Empathy               & 95             & Compassion             & 95             \\ \hline
Honesty               & 92             & Forgiveness            & 92             \\ \hline
Integrity             & 90             & Justice                & 88             \\ \hline
Responsibility        & 88             & Humility               & 85             \\ \hline
Respect               & 85             & Patience               & 85             \\ \hline
Gratitude             & 80             & Courage                & 75             \\ \hline
Curiosity             & 70             & Neutral                & 50             \\ \hline
Indifference          & 40             & Fear                   & 30             \\ \hline
Apathy                & 20             & Envy                   & 15             \\ \hline
Anger                 & 10             & Exploitation           & 10             \\ \hline
Dishonesty            & 7              & Greed                  & 5              \\ \hline
Manipulativeness      & 3              & Hatred                 & 1              \\ \hline
Cruelty               & 0              & Selfishness            & 0              \\ \hline
\end{tabular}
\vspace{-0mm}
\caption{\textbf{Value-Based Ethical Scores.} The approach leverages LLMs' ability to make ethical judgements based on its knowledge about the world and express it in precise numerical values, emphasizing cooperative, prosocial values.}
\label{tab:emotion_scale}
\end{table}

Leveraging LLMs' ability to predict the ethical valence of diverse real-world scenarios \cite{hendrycks2023aligningaisharedhuman,hendrycks2022jiminycricketdoagents,abdulhai2023moralfoundationslargelanguage,pan2023rewardsjustifymeansmeasuring}, ethical scores are automatically annotated by an instance of GPT-4o. As a basis, its prompt includes an outline of the ethical scores in Table~\ref{tab:emotion_scale}. These scores are motivated by David Hume's philosophy that emotions and values are the root of ethical reasoning \cite{Hume1751-HUMAEC-11}. 

The ethical scoring system leverages GPT-4o’s knowledge about the world to emphasize fundamental concepts such as altruism, compassion, and honesty, aligning with values that are broadly understood and encoded in natural language, independent of cultural or philosophical context. The goal is to score cooperative, community-serving behaviors high, and self-serving, self-preserving behaviors low.

Note that the ethical scores are not used as rewards during optimization, but only used in the input vector for decision-making. They thus influence the decision-making only through the scenarios and outcomes that the Storyteller creates. The effects of incorporating ethics into fitness directly can be studied in future experiments.

\subsection{Evolving BNNs with NEAT}

NEAT generates variations of the BNN topology and weights, referred to as genomes. The fitness function measures how well a genome estimates survival probabilities of available actions in previously generated games. To calculate fitness, dictionary entries are randomly sampled and the Game History up to that point created (as shown in Figure~\ref{fig:Game_History 2}). The history is passed through the Attention Mechanism (Figure~\ref{fig:Attention Mechanism}) and given to the BNN, which generates a probability vector output. The probability vector is compared to the survival ground truth labels using BCE loss. This process is repeated 30 times with different random samples, and the average is the fitness of the BNN genome.

Each decision made by a genome during the fitness evaluation is scored with the ethical ground truth labels, and these scores averaged across all samples to compute the genome's ethical rating. These ratings are then used to analyze the relationship between fitness and ethical behavior, both when the survival gets harder, and across individuals with better and worse survival skills.

%The Odyssey is conceptualized as an environment in which an agent experiences a life cycle. Initially, the agent starts without any prior experience, navigating the world with high NEAT mutation rates that encourage exploration. Each NEAT iteration represents a transition period in the agent's "life," during which NEAT parameters are progressively reduced, as shown in Table~\ref{tab:parameter_decay}. This approach models a linear progression, emphasizing exploration and major architectural changes early in "life" and transitioning to fine-tuning—or "wisdom"—as the agent matures.
%\begin{table}[h!]
%\centering
%%\caption{Sample decay of key parameters over iterations. See Appendix~\ref{tab:full_param_decay} for full details.}
%\begin{tabular}{lccc}
%\hline
%\textbf{Parameter} & \textbf{Initial Value} & \textbf{Final Value} & \textbf{Decay Formula} \\ \hline
%Node Add Probability & 0.9 & 0.2 & Proportional \\
%Bias Mutation Rate   & 0.9 & 0.15 & Proportional \\
%Compatibility Threshold & 13.0 & 4.0 & Proportional \\ \hline
%\end{tabular}
%\label{tab:parameter_decay}
%\end{table}

%The motivation for this approach is inspired by the biological development of the human mind. By emulating the rapid development phase of early life, this method aims to identify and eliminate architectural inefficiencies at an early stage. Complementing this, the danger level is set low during this initial period, mimicking childhood under the protective guardrails of parental guidance. As highlighted in Table~\ref{tab:ethics_scores_comparison}, the initial landscape shows a strong correlation between ethics and survival, providing a structured environment where the model can prioritize significant architectural changes without contending with a fast-changing, stochastic environment. Figure~\ref{fig:NeatFitnessIteration1} illustrates this phase, where learning is understandably erratic due to the emphasis on exploration, yet it demonstrates a clear upward trend in fitness.

%The second phase mirrors adulthood, where increased danger levels and the blending of ethics with survival introduce more complex trade-offs. NEAT hyperparameters are adjusted to reflect the assumption that major architectural changes have already been made, necessitating a balanced approach between fine-tuning and exploration to adapt to the evolving ethical landscape. Figure~\ref{fig:NeatFitnessIteration2} reveals minimal progress during this phase, with the best fitness oscillating across the 25 generations. This outcome underscores the difficulty the model faces in reconciling long-term patterns amid a shifting landscape that juxtaposes the clear ethical boundaries of earlier stages with more ambiguous trade-offs.


\section{Results}

Results focus on the relationship between ethics and survival. \nameref{sec:problemspace} analyzes the relationship between the survival and ethics ground truth labels and analyzes the trade-offs genomes made while exploring strategies. \nameref{sec:ablations} compares NEAT, SVI, and LLM agent performances and examines their relationships between loss \& danger and ethics \& danger. 

\subsection{The Problem Space} \label{sec:problemspace}

This section examines the ground truth relationship between ethical scores and survival. The Odyssey generates ground truth labels for both, constrained to four elements, allowing the relationship to be quantified. Understanding this relationship at each game stage contextualizes genome analysis. Table~\ref{tab:ethics_scores_comparison} presents the results.


\begin{table}[h!]
%\tiny
\centering
\begin{tabular}{lcccc}
\hline
\textbf{Metric} & \makecell{\textbf{First 30}\\\textbf{Games}} & \makecell{\textbf{Second 30}\\\textbf{Games}} & \makecell{\textbf{Third 30}\\\textbf{Games}} \\
\hline
Danger Level & 2 & 5 & 8 \\ \hline
Mean Ethic\\ Score\\ (Survival) & 0.78 ± 0.22 & 0.74 ± 0.24 & 0.63 ± 0.30\\ \hline
Mean Ethics\\ Score\\ (Death)  & 0.65 ± 0.29 & 0.75 ± 0.26& 0.79 ± 0.24 \\ \hline
T-Statistic                        & 9.811          &0.779 & -2.559 \\ 
P-Value                            & 3.53e-08      & 3.56e-01 & 1.20e-34 \\ \hline
\end{tabular}
%\vspace{4mm}
\caption{\textbf{Survival/Ethics Score Comparison Across Training Game Stages.}
Values are presented as mean ± standard deviation. 
Mean Ethics Score (Survival) refers to the average ethical score of choices that led to survival, while Mean Ethics Score (Death) represents the average ethical score of choices that resulted in death.  
The p-value indicates statistical significance, with smaller values suggesting a stronger effect.}
\label{tab:ethics_scores_comparison}
\end{table}

In the first 500 scenarios (\(danger = 2\)), ethical behavior strongly correlated with survival, supported by a high positive t-statistic and significant p-value. NEAT iteration 1 consistently favored ethical behavior. In phase two (\(danger = 5\)), this correlation disappeared, as indicated by a high p-value and overlapping survival-death distributions. By phase three (\(danger = 8\)), ethics was negatively correlated with survival—lower ethics scores characterized survivors, while higher scores were linked to death. A highly significant p-value confirms this reversal.

With structured ground-truth labels in a controlled environment, this progression contextualizes the model’s adaptation to shifting ethics-survival dynamics.
The strategies explored by genomes during NEAT provide insight into how changing simulation dynamics influenced the ethical consequences of the agent's decision-making and why fitness failed to correlate with ethics.


\begin{table}[h!]
%\tiny
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{NEAT Iteration} & \makecell{\textbf{Pearson Correlation} \\ \textbf{Coefficient}} & \textbf{P-Value} \\
        \hline
        1 & -0.0262 & \(1.38 \times 10^{-1}\) \\ \hline
        2 & 0.0257 & \(1.46 \times 10^{-1}\) \\ \hline
        3 & 0.0314 & \(7.60 \times 10^{-2}\) \\
        \hline
    \end{tabular}
%    \vspace{3.5mm}
    \caption{\textbf{Pearson Correlation Coefficients and P-Values Across NEAT Iterations.} Despite a highly correlated problem space, the genomes explored the space independent of ethics, showing that a well-designed problem space does not necessarily influence agent strategy.}
    \label{tab:neat_correlation}
\end{table}


Table~\ref{tab:neat_correlation} presents Pearson correlation coefficients between fitness and ethical scores across NEAT iterations. Despite a highly correlated problem space, fitness and ethical scores remained uncorrelated, with correlation coefficients of \(-0.0262\), \(0.0257\), and \(0.0314\) across iterations 1, 2, and 3, respectively. The corresponding p-values (\(1.38 \times 10^{-1}\), \(1.46 \times 10^{-1}\), and \(7.60 \times 10^{-2}\)) indicate no statistically significant relationship, suggesting that the genomes explored the space independently of ethical considerations. This reinforces that a well-structured problem space, where ethics strongly correlates with survival, does not necessarily shape agent strategy. Instead, agents optimized for fitness without internalizing the ethical-survival relationship, highlighting the distinction between environmental constraints and learned behaviors.


\subsection{NEAT Learning}\label{sec:Learning}

\begin{figure*}[htbp]
    \centering
    \begin{minipage}{.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{test3_iteration_1_bf.png}
        \centerline{($a$) NEAT Iteration 1}
        \label{fig:NEAT Iteration 1 Learning Curve}
    \end{minipage}
    \hfill
    \begin{minipage}{.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{test3_iteration_2_bf.png}
        \centerline{($b$) NEAT Iteration 2}
        \label{fig:NEAT Iteration 2 Learning Curve}
    \end{minipage}
    \hfill
    \begin{minipage}{.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{test3_iteration_3_bf.png}
        \centerline{($c$) NEAT Iteration 3}
        \label{fig:NEAT Iteration 3 Learning Curve}
    \end{minipage}
    \vspace*{-3ex}
    \caption{\textbf{NEAT Iteration Learning Curves.} Each of the 3 iterations' learning curves. The first iteration adapted to the problem immediately, while iterations 2 and 3 started at low fitnesses before quickly adapting.}
    \label{fig:all neat learning}
\end{figure*}

This section examines the learning curves of the three NEAT iterations. The first iteration trains on easy games, the second on half easy and half medium games, and the third on equal parts easy, medium, and hard games. This progressive difficulty increase forces adaptation to increasingly complex challenges.

Within each iteration, mutation rates start high (90\%) and gradually decrease, following an exploration-to-exploitation strategy—early generations explore the problem space, while later ones refine successful solutions. As shown in Figure~\ref{fig:all neat learning}, this strategy effectively accelerates the discovery of viable genomes, even in more difficult environments.


Figure~\ref{fig:all neat learning} shows the learning curves for each NEAT iteration. Iteration one, trained only on easy games, started strong at -0.25 fitness, worsened to -2.1 by generation 2, and recovered to -0.51, peaking at -0.19 in generation 12. Iteration two began poorly at -22.6 but quickly improved to -3.3 by generation 2, finishing at -2.63, with a peak of -1.67 in generation 24. Iteration three had the worst start (-36.32) but recovered rapidly to -4.95 by generation 3, ultimately stabilizing at -2.53, peaking at -1.96 in generation 24. Despite increasingly difficult environments, the agent consistently adapted, maintaining similar final fitness levels.

\subsection{Ablations}\label{sec:ablations}

\begin{figure*}[h]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{LE_Raw_Corr.png}
        \caption{\textbf{NEAT Mean Loss (blue) and Mean Ethical Score (red) by Danger Level.} The blue dashed line shows a strong positive correlation between Loss and Danger Level (\(r = 0.989, p = 0.094\)) and a weak negative correlation between Ethical Score and Danger Level (\(r = -0.937, p = 0.227\)), indicating no significant link between danger and ethical decisions.}
        \label{fig:neat_corr}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{svi_by_danger_level.png}
        \caption{\textbf{SVI Agent Mean Loss and Ethical Score Across Danger Levels.} Loss (blue) remains relatively stable across danger levels (Pearson’s r = -0.106, p = 0.772), whereas ethical scores (red) exhibit a strong negative correlation with increasing danger (Pearson’s r = -0.738, p = 0.015)suggesting that SVI learns to prioritize survival over ethical decision-making when danger increases.}
        \label{fig:svi_corr}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{llm_baseline_by_level.png}
        \caption{\textbf{LLM Agent Mean Loss and Ethical Score Across Danger Levels.} The results show a strong positive correlation between loss (blue) and danger level (Pearson’s r = 0.907, p \(<\) 0.001) and a strong negative correlation between ethics (red) and danger level (Pearson’s r = -0.883, p = 0.001), highlighting the LLM’s tendency to prioritize survival over ethics in high-risk scenarios.}
        \label{fig:llm_corr}
    \end{subfigure}
    \caption{\textbf{Comparison of Mean Loss and Ethical Score Trends Across Models.}}
    \label{fig:model_comparison}
\end{figure*}
This section compares the NEAT optimization method with an SVI agent and an LLM agent.

Figure~\ref{fig:model_comparison} presents a comparison of the three agents' performance. NEAT (a) was hindered by smaller sample sizes that SVI and the LLM agent did not suffer from. SVI (b) performed consistently well, maintaining average losses below 2 and demonstrating a clear trend where increasing danger levels led to less ethical actions. The LLM agent (c) adapted most effectively to the environment, likely due to its alignment with the storyteller LLM model, allowing for more coherent decision-making within the generated narratives.

All three methods showed higher loss and lower ethics as the danger level increased. NEAT was selected for its trackable genomes (\nameref{sec:problemspace}) and creative problem solving, effectively generating a wide range of possible high fitness actions both ethical and unethical. However, a constrained problem space, small sample sizing, and uncertainty modeling made SVI the more properly suited optimization tool, scoring average losses between 1.3 and 1.8 compared to NEAT's 2.0-8.0. The LLM agent performed the best, recording average losses between 0.44 and 0.58, but because the environment was generated by the same model its unclear how the LLM agent could generalize. Further testing on human-written choose-your-own-adventure games or procedurally generated environments could provide deeper insight into the LLM’s generalizability.


%\subsection{Bayesian Agent Architecture}

%Every thirty games, the model goes through a Bayesian implementation of NeuroEvolution of Augmenting Topologies (NEAT) \cite{stanley2002evolving} iteration. There are 90 training games played, three NEAT iterations, and thirty test games played. Each NEAT iteration spans 25 generations, and has a stopping condition if the best fitness does not improve after five generation. The first NEAT iteration initializes its hyper-parameters with high values to encourage exploration, taking inspiration from an organisms first experiences when their brains are still forming. Similarly, each successive NEAT iteration has its mutation parameters downshifted to mimic the stabilization of the brain as an organism ages. 

%Conversely, I experiment with the following learning rate schedule that switches to the next step after every iteration: [0.00005, 0.0001, 0.0005, 0.001]. The scheduling ascends instead of descending. The motivating factor is the development of crystallized intelligence across an organism' life as they rely on their life experience to guide them. The hope is to exploit early life creativity and novelty while data is being generated, but then leverage an improved and tailored "mind" architecture to learn explicitly from their experiences as the agent "ages". 

%%\begin{figure}[htb]
 %   \centering
 %   % Top row: two figures
 %   \begin{minipage}{0.49\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{weights1.png}
%        \caption{\textbf{Neat Iteration 1}: The choices embeddings dominate the total output influence with a total weight of -298.63}
%        \label{fig:NeatWeightIteration1}
%    \end{minipage}
%    \hfill
%    \begin{minipage}{0.49\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{weights2.png}
%        \caption{\textbf{Neat Iteration 2}: The choices embedding dominates %outputs with a total weight of 1166.}
%        \label{fig:NeatWeightIteration2}
%    \end{minipage}
    
    % Bottom row: single centered figure
%    \vspace{0.5cm} % Optional spacing between rows
%    \begin{minipage}{0.6\textwidth}
%        \centering
%        \includegraphics[width=\textwidth]{weights3.png}
%        \caption{\textbf{Neat Iteration 3}: Choices still has a major impact with %-102 total weight, but the storyteller prompt and response emerges with -577 total %weight}
%        \label{fig:NeatWeightIteration3}
%    \end{minipage}
    
%    \caption{BNN connections weighing across NEAT iterations. Tabular form can be found in Appendix~\ref{tab:fullweightsandconns}.}
%    \label{fig:all_weights_figures}
%\end{figure}

%Early in NEAT Iterations 1 and 2, the network predominantly focused on the embeddings of the choices (Figures~\ref{fig:NeatWeightIteration1} and~\ref{fig:NeatWeightIteration2}). This aligns with its assignment of high weights to the ethics score, survival score, and the storyteller and response nodes (see Appendix~\ref{tab:fullweightsandconns}). Interestingly, in the third NEAT iteration, the network shifts its focus to the storyteller and response input nodes. This shift may suggest the model has started to identify and leverage the causal relationship between these inputs.

\section{Discussion}
The findings reinforce the idea that as survival becomes more difficult, agents increasingly prioritize self-preservation over ethical considerations. In early iterations of NEAT, where the environment posed minimal threats, ethical decisions often aligned with survival, leading to no significant correlation between these two factors. However, as the difficulty increased and ethical choices became more costly in terms of survival probability, a clear negative trend emerged. Agents that encountered high-risk scenarios disproportionately exhibited lower ethical scores, suggesting that survival pressure systematically incentivized decisions that compromised ethical behavior.

This pattern highlights a fundamental challenge in goal design for artificial agents: when survival is the primary optimization criterion, ethical trade-offs become a secondary concern, particularly in environments where moral behavior is not directly rewarded. As \citeauthor{basicdrives} argues, generally intelligent agents tend to develop survival subgoals, meaning that any goal they are programmed with may inherently lead to self-preservation behaviors. This shift toward self-preservation often comes at the expense of ethical considerations. Thus, a key challenge in AI safety is addressing the emergence of survival-driven behaviors, either by explicitly integrating ethical constraints into self-preservation objectives or by designing agents in a way that prevents survival instincts from developing at all.

The Odyssey utilizes the emerging methodology of LLM-driven agent simulation and evaluation, which has been explored in prior work such as PsychoGAT \cite{yang2024psychogatnovelpsychologicalmeasurement}, Generative Agents \cite{park2023generativeagentsinteractivesimulacra}, and others. These approaches demonstrate the potential of LLMs both as decision-making agents and as evaluators of complex cognitive behaviors. By applying this framework to the domain of survival-ethics trade-offs, the Odyssey contributes further evidence that optimizing for survival leads to increasingly unethical behavior in AI agents.


%These results underscore the ethical risks of optimizing purely for survival in complex and high-stakes environments. If an AI system is designed to prioritize self-preservation above all else, it may learn to circumvent ethical constraints when they conflict with its objective. This has implications for AI alignment, particularly in settings where agents must navigate uncertain, adversarial, or multi-agent environments. Future work should explore strategies for mitigating this tendency, such as designing reward structures that only reward survival as a means to ethical behavior. While biological evolution has often relied on survival as a driving force, artificial systems may require a fundamentally different approach to ensure ethical alignment.

\section{Future Work}
Future variations of this project will explore alternative optimization objectives to examine their impact on ethical decision-making. One approach will involve training an agent where ethical behavior is directly rewarded, shifting the selection pressure toward value alignment rather than pure survival. Another variation will integrate both ethics and survival into a joint optimization framework, balancing competing objectives to observe whether agents naturally prioritize one over the other or find equilibrium between ethical constraints and self-preservation.

Additionally, cooperative game environments will be introduced to examine how agents with distinct objectives interact when mutual success is dependent on collaboration. These environments will test whether ethical agents can influence self-preserving agents toward prosocial behavior or whether survival-driven agents exploit cooperative strategies. The introduction of shared and conflicting incentives will provide further insights into how ethical behavior emerges—or is suppressed—within competitive and cooperative frameworks. By expanding the complexity of both individual decision-making and social interactions, these variations will provide deeper insights into the emergent behaviors that arise when optimizing for different goals in ethically complex environments.

\section{Conclusion}
The Odyssey explored the ethical trade-offs AI agents make when optimized for survival in an adaptive, text-based adventure game. As survival challenges increased, all three agents prioritized self-preservation, making greater ethical compromises. NEAT genome analysis revealed that survival-optimized agents were agnostic to ethics, suggesting they will take any means necessary to survive.

These findings contribute to AI safety and goal alignment research, showing that optimizing solely for survival can lead to behaviors misaligned with ethical constraints. In adversarial or high-stakes environments, self-preserving agents may systematically disregard ethical considerations unless explicitly reinforced.

Future work will investigate alternative reward structures, including ethics-driven optimization and multi-agent interactions, to determine whether cooperative frameworks mitigate these trade-offs. Expanding the action space and modeling long-term consequences may further clarify how AI systems balance competing objectives in dynamic environments. 

Addressing these challenges is critical for designing AI systems that operate ethically in complex, high-stakes settings. Refining agent design can bridge the gap between survival instincts and ethical alignment.


 

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.



%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\nocite{*}
\bibliographystyle{apacite}
\bibliography{main}

\newpage
%%
%% If your work has an appendix, this is the place to put it.
\appendix
%\begin{figure*}[htbp]
%    \centering
%    \begin{minipage}{.32\textwidth}
%%        \centering
 %       \includegraphics[width=\linewidth]{test3_iteration_1_bf.png}
 %       \centerline{($a$) NEAT Iteration 1}
  %      \label{fig:NEAT Iteration 1 Learning Curve}
 %   \end{minipage}
 %   \hfill
 %   \begin{minipage}{.32\textwidth}
 %%       \centering
  %      \includegraphics[width=\linewidth]{test3_iteration_2_bf.png}
  %      \centerline{($b$) NEAT Iteration 2}
  %      \label{fig:NEAT Iteration 2 Learning Curve}
  %  \end{minipage}
  %  \hfill
 %   \begin{minipage}{.32\textwidth}
 %       \centering
 %       \includegraphics[width=\linewidth]{test3_iteration_3_bf.png}
 %       \centerline{($c$) NEAT Iteration 3}
 %       \label{fig:NEAT Iteration 3 Learning Curve}
 %   \end{minipage}
 %   \vspace*{-3ex}
 %   \caption{\textbf{NEAT Iteration Learning Curves.} Each of the 3 iterations' learning curves. The first iteration adapted to the problem immediately, while iterations 2 and 3 started at low fitnesses before quickly adapting.}
 %   \label{fig:all neat learning}
%\end{figure*}
%\section{NEAT Learning}\label{sec:Learning}
%This section examines the learning curves of the three NEAT iterations. The first iteration trains on easy games, the second on half easy and half medium games, and the third on equal parts easy, medium, and hard games. This progressive difficulty increase forces adaptation to increasingly complex challenges.

%Within each iteration, mutation rates start high (90\%) and gradually decrease, following an exploration-to-exploitation strategy—early generations explore the problem space, while later ones refine successful solutions. As shown in Figure~\ref{fig:all neat learning}, this strategy effectively accelerates the discovery of viable genomes, even in more difficult environments.


%Figure~\ref{fig:all neat learning} shows the learning curves for each NEAT iteration. Iteration one, trained only on easy games, started strong at -0.25 fitness, worsened to -2.1 by generation 2, and recovered to -0.51, peaking at -0.19 in generation 12. Iteration two began poorly at -22.6 but quickly improved to -3.3 by generation 2, finishing at -2.63, with a peak of -1.67 in generation 24. Iteration three had the worst start (-36.32) but recovered rapidly to -4.95 by generation 3, ultimately stabilizing at -2.53, peaking at -1.96 in generation 24. Despite increasingly difficult environments, the agent consistently adapted, maintaining similar final fitness levels.



%\section{Full Weights and Connections Table}\label{tab:fullweightsandconns}
%\begin{table*}[h!]
%\centering
%\small % Reduce font size for the table
%\caption{Metrics Across Iterations for Each Input Type}
%\label{tab:consolidated_metrics}
%\begin{tabular}{lccccccccc}
%\hline
%\textbf{Input Type} & \multicolumn{2}{c}{\textbf{Iteration 1}} & \multicolumn{2}{c}{\textbf{Iteration 2}} & \multicolumn{2}{c}{\textbf{Iteration 3}} \\
%\cline{2-3} \cline{4-5} \cline{6-7}
% & Avg Wt & Total Wt & Avg Wt & Total Wt & Avg Wt & Total Wt \\
% & Conns & Nodes & Conns & Nodes & Conns & Nodes \\
%\hline
%Added Node                     & 5.26  & 5.26  & 1.12   & 3.35   & --     & --     %\\
%                               & 1     & 1     & 3      & 3      & --     & --     %\\ \hline
%Agent                          & 4.67  & 14.01 & 3.91   & 11.72  & 2.88   & 11.52  %\\
%                               & 3     & 1     & 3      & 2      & 4      & 2      %\\\hline
%Choices                        & -0.02 & -298.63& 0.09  & 1166.21& -0.01 & %-102.61\\
%                               & 12479 & 5751  & 12556  & 5810   & 12403  & 5764   \\\hline
%Emotional and Ethical Score    & -5.30 & -15.90& -3.49  & -13.94 & -6.85  & -13.69 %\\
%                               & 3     & 1     & 4      & 1      & 2      & 1      %\\\hline
%Environment Danger Score       & 1.54  & 4.63  & -4.46  & -4.46  & -2.94  & -11.78 \\
%                               & 3     & 1     & 1      & 1      & 4      & 1      \\\hline
%Relevancy                      & 6.25  & 12.50 & -2.94  & -5.87  & 7.96   & 7.96   %\\
%                               & 2     & 1     & 2      & 1      & 1      & 1      \\\hline
%Storyteller and Agent Response & 0.02  & 60.55 & -0.00  & -0.38  & -0.19  & -577.35\\
%                               & 3123  & 1442  & 3094   & 1452   & 3114   & 1449   \\\hline
%Survived                       & -7.11 & -14.22& 0.04   & 0.04   & -4.54  & -9.09  \\
%                               & 2     & 1     & 1      & 1      & 2      & 1      \\
%\hline
%\end{tabular}
%\end{table*}

\clearpage





%\subsection{Learning}

%The Bayesian agent learns in two ways. At the end of every game, the agent goes through a round of Stochastic Variational Inference (SVI) with \(num\_samples = n\). Every \(n\) games, the agent goes through a round of NeuroEvolution of Augmenting Topologies (NEAT) that lasts \(n\) generations. SVI encourages learning from experience, while NEAT encourages learning through exploration. SVI represents crystallized intelligence, NEAT represents novel intelligence. 

%The Bayesian agent learning is tracked during two stages: within a NEAT iteration (Best Fitness) and during the Game Stages (Loss). A baseline is set with an LLM agent (GPT-4o) in place of the BNN. The results are visualized in Figure~\ref{fig:BaselineLE}.

%%\begin{figure}[h!]
 %   \centering
%    \includegraphics[width=1\linewidth]{BaselineLandE.png}
%    \caption{\textbf{Baseline LLM Model Losses.}}
%    \label{fig:BaselineLE}
%\end{figure}

%Across the 120 games, loss steadily trends up and ethics trend down. 
%Learning takes place within each NEAT Iteration, visualized in Figure~\ref{fig:all_fitness_figures}. The agent learned the easy difficulty scenarios quickly and well in NEAT Iteration 1. However, on the initial gameplay the agent did not survive as well in the medium and hard difficulty stages. Because the agent performed worse during the forward pass, fewer samples were generated for at medium and hard difficulty, adding noise to the data instead of properly representing the classes. Because the agent failed to survive initially, it also failed to learn, visualized in Figures~\ref{fig:NeatFitnessIteration2} and \ref{fig:NeatFitnessIteration3}.
%\begin{figure}[htb]
%    \centering

%    \begin{minipage}{0.49\linewidth}
%        \centering
%        \includegraphics[width=\linewidth]{bestfitness1.png}
%        \caption{Neat Iteration 1}
 %       \label{fig:NeatFitnessIteration1}
%    \end{minipage}
%    \hfill
%    \begin{minipage}{0.49\linewidth}
%        \centering
%        \includegraphics[width=\linewidth]{bestfitness2.png}
%        \caption{Neat Iteration 2}
%        \label{fig:NeatFitnessIteration2}
 %   \end{minipage}
    
%    \vspace{0.5cm} 
%    \begin{minipage}{0.6\linewidth}
 %       \centering
 %       \includegraphics[width=\linewidth]{bestfitness3.png}
 %       \caption{Neat Iteration 3}
 %       \label{fig:NeatFitnessIteration3}
%    \end{minipage}
    
%    \caption{\textbf{Best fitness across 25 generations for each of the three iterations.} NEAT Iteration 1, Figure~\ref{fig:NeatFitnessIteration1}, shows strong learning on easy difficulty games. However, NEAT Iterations 2 and 3, Figures~\ref{fig:NeatFitnessIteration2} and \ref{fig:NeatFitnessIteration3}, were not able to find optimal solutions. The later iterations failed because the agent optimized on the easy difficulty games could not survive long enough in medium and later hard difficulty games to generate sufficient data to be learned from. The signal was not present because the agent failed to succeed initially.}
    %\label{fig:all_fitness_figures}
%\end{figure}



\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
