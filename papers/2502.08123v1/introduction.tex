% !TEX root = mainfile.tex

\section{Introduction} \label{sec:intro}

\myparatight{Background and Motivation}Reinforcement learning (RL) is a sequential decision-making procedure and can be modeled as a Markov decision process (MDP)~\cite{sutton2018reinforcement}.
Specifically, an agent in RL operates by taking actions according to a certain policy within a stochastic environment. The agent earns rewards for its actions and uses these rewards to enhance its policy. The ultimate goal of the agent is to learn the best possible policy by consistently engaging with the environment, aiming to maximize its cumulative rewards over the long term.
%
%
Despite the advancements in current RL models, they are often data-intensive and face issues due to their limited sample efficiency~\cite{dulac2021challenges,fan2021fault}.  
To tackle this challenge, a straightforward solution might be parallel RL~\cite{nair2015massively,mnih2016asynchronous}.
%
%
In parallel RL, agents share their trajectories with a central server to train a policy. However, this is often impractical due to high communication costs, especially for IoT devices~\cite{wang2020federated}, and prohibited in applications like medical records~\cite{liu2020reinforcement} due to data sensitivity.



Toward this end, the federated reinforcement learning (FRL)~\cite{fan2021fault,khodadadian2022federated,jin2022federated,liu2019lifelong,gao2024federated,yuan2023federated} paradigm has been introduced as a solution to the problems faced by traditional parallel RL methods. In FRL, various agents work together to train a global policy under the guidance of a central server, all while keeping their raw trajectories private.
Specifically, during each global training round, the central server shares the current global policy with all agents or a selected group. Agents then refine their local policies using the shared global policy and through interactions with their environment. Subsequently, agents send their local policy updates back to the server. Once the server receives these policy updates from the agents, it employs aggregation rule, like FedAvg~\cite{mcmahan2017communication}, to merge these received policy updates and refine the global policy.
%
Owing to its willingness to respect agents' privacy, FRL has been widely deployed in real-world systems, such as robotics~\cite{kober2013reinforcement}, autonomous driving~\cite{liang2022federated}, and IoT network~\cite{wang2020federated}.





While FRL has its merits, it is susceptible to poisoning attacks owing to its decentralized nature~\cite{fan2021fault}. 
Such an attack might involve controlling malicious agents, who may either corrupt their local training trajectories (known as \emph{data poisoning attacks}~\cite{fan2021fault}), or intentionally send carefully crafted policy updates to the server (known as \emph{model poisoning attacks}~\cite{fang2020local,shejwalkar2021manipulating,baruch2019little}), with an aim to manipulate the ultimately learnt global policy. A seemingly direct defense against these poisoning attacks would be to implement existing federated learning (FL) based aggregation rules, such as Trimmed-mean~\cite{yin2018byzantine} and Median~\cite{yin2018byzantine}, within the FRL context. Nevertheless, as subsequent experimental results will demonstrate, merely extending existing FL-based aggregation rules does not provide a satisfactory defense performance. 
This is because these rules, originally designed for FL, remain vulnerable to poisoning attacks~\cite{fang2020local,shejwalkar2021manipulating}.
%
Within the domain of FRL, a recently introduced Byzantine-robust aggregation rule, FedPG-BR~\cite{fan2021fault}, has demonstrated exceptional robustness against existing advanced poisoning attacks~\cite{fang2020local,shejwalkar2021manipulating}.







\myparatight{Our work}In this paper, we propose the first model poisoning attacks to Byzantine-robust FRL. 
%
In the attack we propose, the attacker deliberately crafts the policy updates on malicious agents so as to maximize the discrepancy between the aggregated policy updates before and after the attack.  
%
While a direct strategy might be to maximize the distance
between the aforementioned policy updates~\cite{shejwalkar2021manipulating}, this method only accounts for the magnitude of the aggregated policy update, neglecting its directionality. To address this challenge, we propose the \emph{Normalized attack}, wherein the attacker strives to maximize the angular deviation between the aggregated policy updates pre and post-attack.
%
Nevertheless, solving the reformulated optimization problem remains challenging, especially given that the existing robust aggregation rules like FedPG-BR~\cite{fan2021fault} are not differentiable. To tackle this issue, we introduce a two-stage approach to approximate the solution to the optimization problem. Specifically, in the first stage, we determine the optimal direction for the malicious policy updates, and in the second phase, we calculate the optimal magnitude for these malicious policy updates.



We subsequently propose an innovative \emph{ensemble FRL} method that is provably secure against both existing attacks and our newly proposed Normalized attack. Within our proposed ensemble framework, we first leverage a deterministic method to divide agents into multiple non-overlapping groups by using the hash values of the agents' IDs. Each group then trains a global policy, employing a \emph{foundational aggregation rule} such as Median~\cite{yin2018byzantine} and FedPG-BR~\cite{fan2021fault}, using the agents within its respective group. 
During the testing phase, given a test state $s$, we deploy the well-trained multiple global policies to predict the action for state $s$. 
Considering that the action space in FRL may be either discrete or continuous, we apply varying strategies to aggregate these predicted actions accordingly. 
Specifically, in a discrete action space, we select the action with the highest frequency as the final action. 
Conversely, if the action space is continuous, the final action is determined by calculating the geometric median~\cite{ChenPOMACS17} of the predicted actions.
%
We theoretically prove that our proposed ensemble method will consistently predict the same action for the test state $s$ before and after attacks, provided that the number of malicious agents is below a certain threshold when the action space is discrete. In the context of a continuous space FRL system, we demonstrate that the distance between actions predicted by our ensemble approach, before and after the attack, is bounded, as long as the number of malicious agents is less than half of the total number of groups.
%




Our proposed Normalized attack and the proposed ensemble method have been thoroughly evaluated on three RL benchmark datasets. These include two discrete datasets, namely Cart Pole~\cite{barto1983neuronlike} and Lunar Lander~\cite{duan2016benchmarking}, and one continuous dataset, Inverted Pendulum~\cite{barto1983neuronlike}. 
We benchmarked against four existing poisoning attacks including Random action attack~\cite{fan2021fault}, Random noise attack~\cite{fan2021fault}, Trim attack~\cite{fang2020local}, and Shejwalkar attack~\cite{shejwalkar2021manipulating}. 
Furthermore, we employed six foundational aggregation rules for evaluation including FedAvg~\cite{mcmahan2017communication}, Trimmed mean~\cite{yin2018byzantine}, Median~\cite{yin2018byzantine}, geometric median~\cite{ChenPOMACS17}, FLAME~\cite{nguyen2022flame}, and FedPG-BR~\cite{fan2021fault}.
%
%
Experimental findings illustrate that our proposed Normalized attack can remarkably manipulate non-ensemble-based methods (where a single global policy is learnt using all agents along with a particular foundational aggregation rule). Distinctively, within a non-ensemble context, our Normalized attack stands out as the exclusive poisoning attack that can target the FRL-specific aggregation rule.
%
We further demonstrate that our proposed ensemble method can effectively defend against all considered poisoning attacks, including our Normalized attack. Notably, for all robust foundational aggregation rules, the test reward of our proposed ensemble method, even when under attack, closely mirrors that of the FedAvg in a non-attack scenario.
%
Our main contributions can be summarized as follows:

\begin{list}{\labelitemi}{\leftmargin=1em \itemindent=-0.08em \itemsep=.2em}
	
    \item 
    We propose the Normalized attack, the first model poisoning attacks tailored to Byzantine-robust FRL.
    
    \item
    We propose an efficient ensemble FRL method that is provably secure against poisoning attacks.
    
    \item 
    Comprehensive experiments highlight that our proposed Normalized attack can notably compromise non-ensemble-based robust foundational aggregation rules. Additionally, our proposed ensemble method shows significant capability in defending against both existing and our newly introduced poisoning attacks.
    
\end{list}	




