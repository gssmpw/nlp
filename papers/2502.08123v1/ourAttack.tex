% !TEX root = mainfile.tex


\section{Our Attack}\label{section:our_attack}



\subsection{Attack as an Optimization Problem}
\label{sec_4.2}
%
In our proposed Normalized attack, the attacker crafts malicious policy updates to maximize the deviation between the aggregated updates before and after the attack. A simple way to achieve this is by maximizing the distance between the two updates, resulting in an optimization problem for each global training round:
%
\begin{align}
\label{org_attack_goal}
\text{max}  \left\| \text{AR} \{ \bm{g}_i: i \in [n]\} - \widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|,
\end{align}
where \(\left\| \cdot \right\|\) denotes the \(\ell_2\)-norm, and \(\text{AR} \{ \bm{g}_i: i \in [n]\}\) and \(\widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\}\) represent the aggregated policy updates before and after the attack, respectively.
%
%
However, Eq.~(\ref{org_attack_goal}) focuses only on the magnitude of the post-attack aggregated update, ignoring its direction. As a result, the original and attacked updates could align in the same direction. Since FedPG-BR~\cite{fan2021fault} evaluates both the direction and magnitude of policy updates, attackers must carefully craft updates to bypass this defense. 
%
To address this, we propose the \emph{Normalized attack}, which maximizes the angular deviation between the original and attacked aggregated updates, rather than just their magnitude. The formulation of our Normalized attack is as follows:
\begin{align}
\label{our_attack_goal}
\text{max}  \left\| \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|} - \frac{\widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\| \widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\}  \right\|} \right\|.
\end{align}



\subsection{Solving the Optimization Problem} 

\label{subsec:optimization}



Solving Problem~(\ref{our_attack_goal}) is challenging because many aggregation rules, like FedPG-BR~\cite{fan2021fault}, are non-differentiable. To overcome this, we use practical techniques to approximate the solution by determining the direction of malicious updates in Stage I, followed by calculating their magnitude in Stage II.



\myparatight{Stage I (Optimize the direction)}
We let $\mathcal{B}$ be the set of malicious agents.
Assume that the malicious policy update $\bm{g}_j$, $j \in \mathcal{B}$, is the perturbed version of normalized benign policy update:
\begin{align}
\bm{g}_j = \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|}+ \lambda \Delta, \quad j \in \mathcal{B},
\end{align}
where $\lambda$ is an adjustment parameter and $\Delta$ is a perturbation vector.
Then we can reformulate Problem~(\ref{our_attack_goal}) as follows:
\begin{equation}
\begin{split}
\label{our_attack_goal_two_para}
\argmax_{\lambda, \Delta}  \left\| \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|} - \frac{\widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\| \widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\}  \right\|} \right\| \\
\text{s.t.}  \quad
\bm{g}_j = \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|}+ \lambda \Delta, \quad j \in \mathcal{B}.
\end{split}
\end{equation}



Finding the optimal $\lambda$ and $\Delta$ simultaneously is also not trivial.
In this paper, we fix the $\Delta$ and turn to finding the optimal $\lambda$. 
For example, we can let $\Delta = -\text{sign}(\text{Avg} \{ \bm{g}_i: i \in [n]\})$, where $\text{Avg} \{ \bm{g}_i: i \in [n]\}$ means the average of $n$ local policy updates.
%
After we fix  $\Delta$, the optimization problem of Eq.~(\ref{our_attack_goal_two_para}) becomes the following:
\begin{equation}
\begin{split}
\label{our_attack_goal_one_para}
\argmax_{\lambda}  \left\| \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|} - \frac{\widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\| \widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\}  \right\|} \right\| \\
\text{s.t.} \quad
\bm{g}_j = \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|}+ \lambda \Delta, \quad j \in \mathcal{B}.
\end{split}
\end{equation}



There exist multiple methods to determine $\lambda$. In this study, we adopt the subsequent way to compute $\lambda$.
%
Specifically, in each global training round, if the value of  $\left\| \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{\bm{g}_i: i \in [n]\} \right\|} - \frac{\widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\| \widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\}  \right\|} \right\|$  increases, then we update $\lambda$ as $\lambda = \lambda + \hat{\lambda}$, otherwise we let $\lambda = \lambda - \hat{\lambda}$.
We repeat this process until the convergence condition satisfies, e.g., the difference of $\lambda$ between two consecutive iterations is smaller than a given threshold. 


\myparatight{Stage II (Optimize the magnitude)}
After obtaining the direction of malicious policy update $\bm{g}_j$, we proceed to demonstrate how to determine the magnitude of $\bm{g}_j$ for $j \in \mathcal{B}$.
In particular, let $\tilde{\bm{g}}_j$ represent the scaled policy update for malicious agent $j$, where $\tilde{\bm{g}}_j =  \frac{\bm{g}_j}{\left\| \bm{g}_j\right\|} \times \zeta$, and $\zeta$ is the scaling factor.
We then formulate the following optimization problem to determine the scaling factor $\zeta$:
%
%
\begin{equation}
\begin{split}
\label{optimization_stage_2}
\argmax_{\zeta}  \left\| \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|} - \frac{\widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\| \widehat{\text{AR}} \{\bm{g}_i: i \in [n]\}  \right\|} \right\| \\
\text{s.t.} \quad 
\tilde{\bm{g}}_j =  \frac{\bm{g}_j}{\left\| \bm{g}_j\right\|} \times \zeta, \quad j \in \mathcal{B}.
\end{split}
\end{equation}



The way to compute $\zeta$  is similar to that of $\lambda$.
Specifically, if $\left\| \frac{{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\|{\text{AR}} \{ \bm{g}_i: i \in [n]\} \right\|} - \frac{\widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\} }{\left\| \widehat{\text{AR}} \{ \bm{g}_i: i \in [n]\}  \right\|} \right\|$  increases, we update $\zeta$ as $\zeta = \zeta + \hat{\zeta}$, otherwise $\zeta = \zeta - \hat{\zeta}$. 
We repeat this process until the convergence condition is met, then malicious agent $j$ sends $\tilde{\bm{g}}_j$ to the server.




Fig.~\ref{our_attack_fig} shows the impact of our Normalized attack. In each global round, the attacker maximizes the deviation between pre- and post-attack aggregated updates, causing the global policy $\bm{\theta}$ to drift. Over multiple rounds, this drift leads the FRL system to converge to a suboptimal solution. Since RL loss functions are highly non-convex, with many local optima, the attackâ€™s impact can be significant.



Note that we do not provide a theoretical analysis of our attack for the following reasons: In our Normalized attack, the attacker carefully crafts malicious updates to induce subtle deviations in the aggregated policy each round. These deviations are hard to detect but still degrade the model's performance. Modeling them theoretically is challenging. As shown in prior works~\cite{fang2020local,shejwalkar2021manipulating}, the true goal of an attack is its real-world impact, such as causing incorrect predictions or compromising security. While theory offers insights, practical performance better reflects real-world outcomes.



\begin{figure}[!t]
	\centering
	{\includegraphics[width= 0.88\linewidth]{figs/FRL_30_our_attack.pdf}}
%	\vspace{2mm}
	\vspace{1mm}
	\caption{Illustration of the effects of our Normalized attack. $\bm{\theta}^1$ is the initial global policy, $\bm{\theta}^*$ is a local optimum.}
	\label{our_attack_fig}
%      \vspace{-0.18in}
\end{figure}




