% !TEX root = mainfile.tex

\section{Preliminaries and Related Work}



\subsection{Federated Reinforcement Learning}




A federated reinforcement learning (FRL) system~\cite{fan2021fault,khodadadian2022federated,jin2022federated}  consists of $n$ agents and a central server collaborating to train a global policy. Each agent $i \in [n]$ solves a local Markov decision process (MDP)~\cite{sutton2018reinforcement}, defined as $\mathcal{M}_i = \{ \mathcal{S}, \mathcal{A}, \mathcal{P}_i, \mathcal{R}_i, \gamma_i, \rho_i \}$, with $\mathcal{S}$ as the state space, $\mathcal{A}$ the action space, $\mathcal{P}_i$ the transition probability, $\mathcal{R}_i$ the reward function, $\gamma_i$ the discount factor, and $\rho_i$ the initial state distribution.
%
%
%%
In FRL, agent $i$ follows a policy $\pi$ that gives the probability of taking action $a$ in state $s$. Through interactions with its environment, the agent generates a trajectory $\tau_i = \{s_{i,1}, a_{i,1}, ..., s_{i,H}, a_{i,H}\}$, starting from an initial state $s_{i,1}$ drawn from $\rho_i$, with $H$ as the trajectory length. The cumulative reward is calculated as $\mathcal{R}(\tau_i) = \sum_{h \in [H]} \gamma_i^h \mathcal{R}_i(s_{i,h}, a_{i,h})$.
%%
%%
Let $\pi_{\bm{\theta}}$ denote a policy parameterized by $\bm{\theta} \in \mathbb{R}^d$, where $d$ is the dimension of $\bm{\theta}$. The distribution of agent $i$'s trajectories under $\pi_{\bm{\theta}}$ is $p_i(\tau_i | \pi_{\bm{\theta}})$. For simplicity, we will refer to $\bm{\theta}$ as $\pi_{\bm{\theta}}$. Agent $i$ evaluates the effectiveness of a policy $\pi$ by solving the following optimization problem:
\begin{align}
\label{rl_opt}
J_i(\bm{\theta}) = \mathbb{E}_{\tau_i \sim p_i(\cdot | \bm{\theta})}[\mathcal{R}(\tau_i)|\mathcal{M}_i].
\end{align}


%
In FRL, the $n$ agents collaborate to train a global policy aimed at maximizing the total cumulative discounted reward. Thus, the optimization problem in FRL becomes
$
\label{frl_opt}
\max_{\bm{\theta} \in \mathbb{R}^d} \sum_{i\in [n]} J_i(\boldsymbol{\theta}).
$
%
%
%
FRL solves this problem in an iterative manner.
Specifically, in each global training round $t$, FRL performs the following three steps:
\begin{list}{\labelitemi}{\leftmargin=1em \itemindent=-0.08em \itemsep=.2em}
\item \textbf{Step I: Global policy synchronization.} 
The server distributes the current global policy $\bm{\theta}$ to all agents or a selection of them.


\item \textbf{Step II: Local policy updating.} 
Each agent $i \in [n]$ uses the current policy $\bm{\theta}$ to sample a batch of trajectories $\{\tau_{i}^k\}_{k=1}^B$, where $\tau_i^k=\{s_{i,1}^k,a_{i,1}^k,s_{i,2}^k,a_{i,2}^k,\ldots,s_{i,H}^k,a_{i,H}^k\}$, $B$ is the batch size.
Subsequently, agent $i$ calculates a local policy update $\bm{g}_i$.
For example, using the REINFORCE algorithm~\cite{williams1992simple}, $\bm{g}_i$ is calculated as:
\begin{align}
\label{gradient_i}
\bm{g}_i = \frac{1}{B} \sum_{k \in [B]}  \left[
 \sum_{h \in [H]}\nabla_{\bm{\theta}}\log \pi_{\bm{\theta}}(a_{i,h}^k|s_{i,h}^k)\right] \times  \nonumber \\
 \left[\sum_{h\in [H]}\gamma_i^h \mathcal{R}_i(s_{i,h}^k,a_{i,h}^{k})- \Im   \right],
\end{align}
where $\Im$ is a constant. Then agent $i$ sends $\bm{g}_i$ to the server.


\item \textbf{Step III: Global policy updating.} 
The server updates the global policy by aggregating local updates using $\text{AR}\{\cdot\}$:
\begin{align}
\bm{\theta} = \bm{\theta} + \eta \cdot \text{AR} \{\bm{g}_i: i \in [n]\},
\end{align}
where $\eta$ is the learning rate.

\end{list}

FRL methods vary in their aggregation rules.
For example, using FedAvg~\cite{mcmahan2017communication}, the global policy is updated as:
 $\bm{\theta} = \bm{\theta} + \frac{\eta}{n} \sum_{i \in [n]}\bm{g}_i$.


\subsection{Poisoning Attacks to FRL}


The distributed nature of FRL makes it susceptible to poisoning attacks~\cite{fang2020local,shejwalkar2021manipulating,fan2021fault,zhang2024poisoning,yin2024poisoning}, where malicious agents manipulate local training data (data poisoning) or policy updates (model poisoning) to compromise the global policy. For example, in Random action attack~\cite{fan2021fault}, agents act randomly without following a pattern. Model poisoning attacks include Random noise attack~\cite{fan2021fault}, where agents send Gaussian noise as policy updates, Trim attack~\cite{fang2020local}, which maximizes deviation in policy updates, and Shejwalkar attack~\cite{shejwalkar2021manipulating}, which increases the distance between pre- and post-attack updates. While some studies~\cite{ma2023local,zhang2020adaptive} assume agents can manipulate environments or rewards, such scenarios are often impractical and are not considered in our paper.



\subsection{Byzantine-robust Aggregation Rules}

\subsubsection{FL-based Aggregation Rules}



In typical federated learning (FL), the server uses FedAvg~\cite{mcmahan2017communication} to aggregate local model updates\footnote{Note that in FL, we commonly refer to a ``model update'' rather than a ``policy update''.}, but this method is vulnerable to poisoning attacks since even one malicious agent can skew the results. To counter such attacks, several Byzantine-resilient aggregation rules have been proposed~\cite{yin2018byzantine,nguyen2022flame,ChenPOMACS17,Blanchard17,cao2020fltrust,xie2019zeno,rajput2019detox,mozaffari2023every,pan2020justinian,zhang2022fldetector,cao2021provably,rieger2022deepsight,fang2024byzantine,fang2022aflguard,fang2025FoundationFL,yueqifedredefense}. 
%
Examples include Median~\cite{yin2018byzantine}, which computes the median for each dimension, and Trimmed-mean~\cite{yin2018byzantine}, which removes extreme values before averaging. FLAME~\cite{nguyen2022flame} clusters agents based on cosine similarity, discarding suspicious updates and adding adaptive noise to the rest.
%
Our proposed ensemble method differs from~\cite{cao2021provably} by addressing continuous action spaces, while their approach only supports categorical labels. We also provide theoretical evidence that an attacked agent behaves similarly to pre-attack conditions as long as malicious agents are fewer than half of the total groups.




\subsubsection{FRL-based Aggregation Rules}


The authors in~\cite{fan2021fault} proposed FedPG-BR to defend against poisoning attacks in FRL. Each training round, the server computes the vector median of local policy updates and marks an update as benign if it aligns in direction and magnitude with the median. It then averages these benign updates to form a policy update estimator. Additionally, the server samples trajectories to compute its own policy update. The final global policy update is obtained by combining the estimator and the server's update using the stochastically controlled stochastic gradient (SCSG)~\cite{lei2017less} to reduce variance.



\begin{table}[htbp]
  \centering
  \small
  \caption{Comparison among the Trim attack, Shejwalkar attack, and our proposed Normalized attack.}
    \begin{tabular}{|c|c|c|}
    \hline
          &  Direction    & Magnitude \\
    \hline
    Trim attack~\cite{fang2020local}     & \xmark     &  \xmark \\
    \hline
    Shejwalkar attack~\cite{shejwalkar2021manipulating}     &  \xmark     & \cmark  \\
    \hline
    Normalized attack   &  \cmark      &  \cmark   \\
    \hline
    \end{tabular}%
     \label{attack_compare}%
     % \vspace{-0.22in}
\end{table}



\subsubsection{Limitations of Existing Attacks and Defenses}



Current poisoning attacks and defense strategies have limitations. The Trim attack~\cite{fang2020local} targets individual dimensions in linear aggregation rules like Trimmed-mean and Median~\cite{yin2018byzantine}, ignoring the updateâ€™s overall direction. In contrast, the Shejwalkar attack~\cite{shejwalkar2021manipulating} considers the entire update but overlooks its direction. Table~\ref{attack_compare} compares these with our proposed Normalized attack. 
%
Additionally, applying FL-based aggregation rules in FRL leads to poor performance, as they remain vulnerable to known attacks~\cite{fang2020local,shejwalkar2021manipulating}. Although FedPG-BR~\cite{fan2021fault} counters Trim and Shejwalkar attacks, our experiments show it is still vulnerable to our Normalized attack.



