\documentclass[sigconf]{acmart}

\pagestyle{plain} 




\newcommand{\myparatight}[1]{\smallskip\noindent{\bf {#1}:}~}

\usepackage{amsthm}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage{float}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage[skip=0pt]{caption}
%\usepackage[skip=0pt]{subcaption}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{bm}
\usepackage{color}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{makecell}
\usepackage{balance}
\usepackage{pdfx}
\usepackage{subfig}
\usepackage{pifont}

\usepackage{subfig}

\let\Bbbk\relax         %%redefined in newtxmath.sty
\usepackage{amssymb}
\usepackage{bbm}

\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%



\allowdisplaybreaks

\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{assumption}{Assumption}
\newtheorem{thm}{Theorem}
\newtheorem{lem}{Lemma}
\newtheorem*{remark}{Remark}

\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}


\DeclareMathOperator*{\argmax}{argmax}


\newcommand{\XL}[1]{{\color{red}[XL: #1]}}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}





\newcommand{\RomanNumeralCaps}[1]
{\MakeUppercase{\romannumeral #1}}
\algnewcommand\algorithmicforpara{\textbf{for}}
\algnewcommand\algorithmicdoinparallel{\textbf{do in parallel}}
\algdef{S}[FOR]{ForParallel}[1]{\algorithmicforpara\ #1\ \algorithmicdoinparallel}
\DeclareMathOperator*{\argmin}{arg\,min}

\pagestyle{plain}



\copyrightyear{2025} 
\acmYear{2025} 
\setcopyright{cc}
\setcctype{by}
\acmConference[WWW '25]{Proceedings of the ACM Web Conference 2025}{April 28-May 2, 2025}{Sydney, NSW, Australia}
\acmBooktitle{Proceedings of the ACM Web Conference 2025 (WWW '25), April 28-May 2, 2025, Sydney, NSW, Australia}
\acmDOI{10.1145/3696410.3714728}
\acmISBN{979-8-4007-1274-6/25/04}




\makeatletter
\gdef\@copyrightpermission{
  \begin{minipage}{0.3\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{\includegraphics[width=0.90\textwidth]{figs/4ACM-CC-by-88x31.eps}}
  \end{minipage}\hfill
  \begin{minipage}{0.7\columnwidth}
   \href{https://creativecommons.org/licenses/by/4.0/}{This work is licensed under a Creative Commons Attribution International 4.0 License.}
  \end{minipage}
  \vspace{5pt}
}
\makeatother


\begin{document}



\title{Provably Robust Federated Reinforcement Learning}



\author{Minghong Fang}
\authornote{Equal contribution.}
\affiliation{
	\institution{University of Louisville}
	\city{Louisville}
        \state{KY}
        \country{USA}
}
\email{minghong.fang@louisville.edu}


\author{Xilong Wang}
\authornotemark[1]
\affiliation{
	\institution{Duke University}
	\city{Durham}
        \state{NC}
        \country{USA}
}
\email{xilong.wang@duke.edu}


\author{Neil Zhenqiang Gong}
\affiliation{
	\institution{Duke University}
	\city{Durham}
        \state{NC}
        \country{USA}
}
\email{neil.gong@duke.edu}




\begin{abstract}
Federated reinforcement learning (FRL) allows agents to jointly learn a global decision-making policy under the guidance of a central server. While FRL has advantages, its decentralized design makes it prone to poisoning attacks. To mitigate this, Byzantine-robust aggregation techniques tailored for FRL have been introduced. Yet, in our work, we reveal that these current Byzantine-robust techniques are not immune to our newly introduced Normalized attack. Distinct from previous attacks that targeted enlarging the distance of policy updates before and after an attack, our Normalized attack emphasizes on maximizing the angle of deviation between these updates. To counter these threats, we develop an ensemble FRL approach that is provably secure against both known and our newly proposed attacks. Our ensemble method involves training multiple global policies, where each is learnt by a group of agents using any foundational aggregation rule. These well-trained global policies then individually predict the action for a specific test state. The ultimate action is chosen based on a majority vote for discrete action systems or the geometric median for continuous ones. Our experimental results across different settings show that the Normalized attack can greatly disrupt non-ensemble Byzantine-robust methods, and our ensemble approach offers substantial resistance against poisoning attacks. 
\end{abstract}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002978.10003006</concept_id>
       <concept_desc>Security and privacy~Systems security</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Security and privacy~Systems security}


\keywords{Federated Reinforcement Learning, Poisoning Attacks, Robustness}


\maketitle



\input{introduction}
\input{related}
\input{problem}
\input{ourAttack}
\input{ourDefense}
\input{exp}
\input{conclusion}




\bibliographystyle{plain}
\bibliography{refs}


\input{appendix}


\end{document}


