% !TEX root = mainfile.tex


\begin{figure*}[!t]
	\centering
	{\includegraphics[width= 0.92\textwidth]{figs/FRL_26_our_defense.pdf}}
	\vspace{1mm}
	\caption{Illustration of our ensemble framework with discrete action space.}
	\label{our_defense_fig}
	    % \vspace{-0.18in}
\end{figure*}


\section{Our Defense}
\label{section:our_aggregation}



\subsection{Overview}


%


In our approach, we train multiple global policies instead of a single one, each using a foundational aggregation rule like Trimmed-mean or Median~\cite{yin2018byzantine} with different subsets of agents. During testing, the agent predicts an action using all trained policies. For discrete action spaces, the final action is chosen by majority vote, while for continuous spaces, it is determined by the geometric median~\cite{ChenPOMACS17}. 
%
Fig.~\ref{our_defense_fig} illustrates the process for a discrete action space with six agents split into three groups, each training a global policy over $T$ rounds, resulting in policies $\bm{\theta}_1^T$, $\bm{\theta}_2^T$, and $\bm{\theta}_3^T$. Since the third group contains a malicious agent, $\bm{\theta}_3^T$ is poisoned. During evaluation, given a test state $s$, the three policies predict ``UP'', ``UP'', and ``Down''. With a majority vote, the final action selected is ``UP''.






\subsection{Our Ensemble Method}
\label{subsec:Our Ensemble Method}


In an FRL system with $n$ agents, our method divides them into $K$ non-overlapping groups deterministically, such as by hashing their IDs. Each group trains a global policy using its agents with an aggregation rule like Trimmed-mean, Median~\cite{yin2018byzantine}, or FedPG-BR~\cite{fan2021fault}. Let $\bm{\theta}_k^T$ represent the policy learned by group $k$ after $T$ global rounds. At the end of training, we obtain $K$ global policies: $\bm{\theta}_1^T, \bm{\theta}_2^T, ..., \bm{\theta}_K^T$.
%
%
During testing, the agent independently executes the $K$ trained policies. At a test state $s$, let $F(s, \bm{\theta}_k^T)$ denote the action taken by the agent using policy $\bm{\theta}_k^T$. With $K$ policies, the agent generates $K$ actions: \(F(s, \bm{\theta}_1^T), F(s, \bm{\theta}_2^T), \dots, F(s, \bm{\theta}_K^T)\). The final action at state $s$ is determined using an ensemble method, which varies based on whether the action space $\mathcal{A}$ is discrete or continuous.


\myparatight{Discrete action space}%
If the action space $\mathcal{A}$ is discrete, the agent's action is determined by majority vote among the $K$ actions. Let $v(s, a)$ represent the frequency of action $a$ at state $s$, calculated as:
\begin{align}
\label{action_freq}
v(s, a) = \sum\limits_{k \in [K]} \mathbbm{1}_{ \{F(s, \bm{\theta}_k^T)=a \}},
\end{align}
where $\mathbbm{1}$ is the indicator function, which returns 1 if \(F(s, \bm{\theta}_k^T) = a\), and 0 otherwise. The final action \(\Phi(s)\) at test state $s$ is the one with the highest frequency, calculated as:
\begin{align}
\label{fina_action_discrete}
\Phi(s) =  \argmax_{a \in \mathcal{A}} v(s, a).
\end{align}




\myparatight{Continuous action space}%
For a continuous action space $\mathcal{A}$, we use the Byzantine-robust geometric median~\cite{ChenPOMACS17} to aggregate the $K$ actions. The final action at state $s$ is computed as:
\begin{align}
\label{fina_action_continuous}
\Phi(s) =  \argmin_{a \in \mathcal{A}} \sum_{k \in [K]} \left\| F(s, \bm{\theta}_k^T) - a \right\|.
\end{align}



We use the geometric median~\cite{ChenPOMACS17} to aggregate the $K$ continuous actions instead of FedAvg or Trimmed-mean, as our experiments show that these methods are vulnerable to poisoning attacks.


\myparatight{Complete algorithm}%
Algorithm~\ref{our_alg_training} in Appendix outlines the ensemble method during training. In Lines~\ref{each_group_train}-\ref{each_group_agg}, each group trains its global policy in round $t$. In Line~\ref{each_group_train_server}, the server for group $k$ shares the current global policy with its agents, who refine their local policies and send updates back (Lines~\ref{each_group_update}-\ref{each_group_update_end}). Here, $n_k$ represents the agents in group $k$. Finally, the server aggregates these updates to revise the global policy (Line~\ref{each_group_update_server}). 
%
Algorithm~\ref{our_alg_testing} in Appendix summarizes the testing phase, where the final action is selected by majority vote for discrete actions (Line~\ref{alg2_discrete}) or by geometric median for continuous actions (Line~\ref{alg2_continuous}).





\myparatight{Complexity analysis}% 
%
In our ensemble FRL approach, each agent participates in only one global training round over $T$ rounds. Thus, the computational cost per agent is \(O(T)\).


\subsection{Formal Security Analysis}
\label{subsec:theor_any}



In this section, we present the security analysis of our ensemble method. For discrete action spaces, we show that the predicted action at a test state $s$ remains unchanged despite poisoning attacks, as long as the number of malicious agents stays below a certain threshold. For continuous action spaces, we prove that the difference between actions predicted before and after an attack is bounded if malicious agents make up less than half of the groups.



\begin{thm}[Discrete Action Space]
\label{theorem_1}

%
Consider an FRL system with \(n\) agents and a test state \(s\), where the action space \(\mathcal{A}\) is discrete. The agents are divided into \(K\) non-overlapping groups based on the hash values of their IDs, and each group trains its global policy using an aggregation rule \(\text{AR}\). 
%
Define actions \(x\) and \(y\) as those with the highest and second-highest frequencies for state \(s\), with ties resolved by selecting the action with the smaller index. Our ensemble method aggregates the \(K\) actions using Eq.~(\ref{fina_action_discrete}).
%
Let \(\Phi(s)\) and \(\Phi^{\prime}(s)\) represent the actions predicted when all agents are benign and when up to \(n^{\prime}\) agents are malicious, respectively. The condition for \(n^{\prime}\) is:
\begin{align}
n^{\prime} = \left\lfloor \frac{v(s,x)-v(s,y) - \mathbbm{1}_{\{ y<x \}}}{2} \right\rfloor, 
\end{align}
%
where \(v(s, x)\) and \(v(s, y)\) represent the pre-attack frequencies of actions \(x\) and \(y\) for state \(s\), respectively. The notation \(y < x\) indicates that action \(y\) has a smaller index than action \(x\).
%
Then we have that:
\begin{align}
\Phi(s) = \Phi^{\prime}(s) = x.
\end{align}
\end{thm}

\begin{proof}
The proof is in Appendix~\ref{sec:theorem_1_proof}.
\end{proof}




\begin{thm}[Continuous Action Space]
\label{theorem_2}
%
%
In a continuous action space FRL system with \(n\) agents and a test state \(s\), the agents are divided into \(K\) non-overlapping groups. If \(n^{\prime}\) agents are malicious and \(n^{\prime} < K/2\), each group trains a global policy using an aggregation rule \(\text{AR}\). Our ensemble method aggregates the \(K\) continuous actions using Eq.~(\ref{fina_action_continuous}). Let \(\Phi(s)\) and \(\Phi^{\prime}(s)\) be the actions predicted before and after the attack, respectively. The following holds:
\begin{align}
 \left\| \Phi(s)  - \Phi^{\prime}(s)  \right\|  \leq \frac{2  w(K-n^{\prime})}{K-2n^{\prime}},
\end{align}
where $w=\max \left\{ \| F(s, \bm{\theta}_k^T) - \Phi(s) \|  : k \in [K] \right\}$, $\{F(s, \bm{\theta}_k^T): k \in [K]\}$ is the set of $K$ continuous actions before attack.
\end{thm}

\begin{proof}
The proof is in Appendix~\ref{sec:theorem_2_proof}.
\end{proof}


\begin{remark}
Our framework accounts for cases where honest agents may act similarly to malicious ones. Theorems~\ref{theorem_1} and \ref{theorem_2} hold as long as the total number of adversarial agents—both malicious and unintentionally adversarial—stays within a certain limit.
\end{remark}


