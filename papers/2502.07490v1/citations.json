[
  {
    "index": 0,
    "papers": [
      {
        "key": "yang2019xlnet",
        "author": "Yang, Zhilin",
        "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "liu2019roberta",
        "author": "Liu, Yinhan",
        "title": "Roberta: A robustly optimized bert pretraining approach"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "roberts2019exploring",
        "author": "Roberts, Adam and Raffel, Colin and Lee, Katherine and Matena, Michael and Shazeer, Noam and Liu, Peter J and Narang, Sharan and Li, Wei and Zhou, Yanqi",
        "title": "Exploring the limits of transfer learning with a unified text-to-text transformer"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "radford2019language",
        "author": "Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others",
        "title": "Language models are unsupervised multitask learners"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom B",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "achiam2023gpt",
        "author": "Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others",
        "title": "Gpt-4 technical report"
      },
      {
        "key": "touvron2023llama",
        "author": "Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\\'e}e and Rozi{\\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others",
        "title": "Llama: Open and efficient foundation language models"
      },
      {
        "key": "jiang2023mistral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others",
        "title": "Mistral 7B"
      },
      {
        "key": "yang2024qwen2",
        "author": "Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others",
        "title": "Qwen2. 5 Technical Report"
      },
      {
        "key": "liu2024deepseek",
        "author": "Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others",
        "title": "DeepSeek-V3 Technical Report"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "dong2019unified",
        "author": "Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen",
        "title": "Unified language model pre-training for natural language understanding and generation"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "tay2022ul2",
        "author": "Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and others",
        "title": "Ul2: Unifying language learning paradigms"
      }
    ]
  }
]