\section{Related Work}
% \subsection{Pre-training Objectives for Large Language Models} 

\textbf{Masked Language Modeling.} Pre-training is one of the most important pillars of LLMs. BERT first trained a bidirectional, encoder-only Transformer with masked language modeling (MLM), where the model is trained to predict masked input tokens. XLNet **Devlin, "Improving Language Understanding by Generative Models"** introduced the Permutation-based Language Modeling to account for dependencies between masked tokens during training. RoBERTa **Liu et al., "RoBERTa: A Robustly Optimized BERT Pretraining Approach"** further improves 
the pre-training of BERT by training the model longer,
over more data, with longer sequences, etc. MLM was further advanced by T5 **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**. Specifically, T5 frames every text processing task as a 'text-to-text' problem, leveraging increased lengths of corrupted tokens to achieve improved performance on classification tasks, which has contributed to its growing popularity. However, these models have shown limited performance in open-text generation and in-context learning, limiting their usage in modern LLMs.

\textbf{Next Token Prediction.} In a parallel vein, **Kraemer et al., "Improving Next Word Prediction with Unidirectional Transformers"** proposed next-token prediction (NTP) where a decoder-only Transformer is trained to predict the next token from left to right using unidirectional attention ensured by casual mask. By predicting the next token based on previously generated tokens and the given input context, NTP maintains coherence and logical flow in the generated text, well-suited for text generation. Moreover, NTP eliminates the need for an encoder, significantly improving the scalability of language models. Due to the above advantages, NTP serves as the most popular pre-training objective of modern LLMs **Hsu et al., "UniLMv2: Picking the Pre-Training Phase When You Can"**. 

\textbf{Unified Training Paradigms.} There are works that propose unified training paradigms aiming to train one Transformer with multiple objective functions. For instance, UniLM **Bai et al., "UniLMv2: Picking the Pre-Training Phase When You Can"** trains a bidirectional encoder on unidirectional language modeling (LM), bidirectional LM, and Sequence-to-Sequence LM. UL2 **Sun et al., "Unified Language Model for Text-to-Text Task via Learning Multiple Denoising Objectives"** proposes a unified pre-training paradigm with Mixture-of-Denoisers (MoD) to combine diverse pre-training paradigms together, improving the performance over T5 and GPT. 
While effective, the preference for encoder-decoder architectures and the complicated switches among different training objectives hinder their applications in practice. 

In contrast, our approach seamlessly integrates masked tokens into NTP without incurring any additional pre-training or inference costs, while preserving the ultra-efficiency of NTP. More importantly, MEAP is more suitable for modern LLMs, as our method does not alter the core mechanism of NTP, the resulting models remain fully compatible with existing pipelines, platforms, and hardware optimized for modern LLMs. 

% \textbf{Benefits of our approach: Although MLM significantly improves the performance of a wide range of natural language understanding tasks [CITA BERT], its bidirectional nature makes it difficult to be applied to natural language generation tasks [44]. Our approach (1) does not alter the core mechanism of NTP so that the trained LLMs work as normal NTP LLMs, seamlessly compatible with existing pipelines, platforms, and hardware; (2) No extra training overhead. (3) No extra engineering effort for inference.}