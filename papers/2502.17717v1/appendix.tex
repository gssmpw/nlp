\subsection{Operating points}

The operating points for Knowledge Distillation With Training Wheels are shown in the table below.

\begin{table}[h!]
    \centering
    \caption{Prompt instructions for teacher use and corresponding usage fraction targeted}
    \begin{tabular}{cc}
        \hline \\
        \textbf{Teacher Use Instruction} & \textbf{Fraction} \\
        \hline \\
        With no teacher use, \{summarize|translate\} & 0.0\\
        With light teacher use, \{summarize|translate\} & 0.1\\
        With moderate-light teacher use, \{summarize|translate\} & 0.2\\
        With moderate teacher use, \{summarize|translate\} & 0.3\\
        With high teacher use, \{summarize|translate\} & 0.4\\
        With very high teacher use, \{summarize|translate\} & 0.5\\
        \hline\\
    \end{tabular}
    \label{tab:my_label}
\end{table}

\subsection{Training Details}

For both approaches we used a learning rate of $10^{-5}$ and weight decay of $10^{-5}$ for the policy (student model). For our approach, we used a learning rate of $10^{-6}$ and weight decay of $10^{-6}$ for the value network. For constrained RL in our approach, we used a Lagrangian learning rate of $1e-2$. The Lagrangian is capped at $2$. The Lagrangian is learnt for each operating point separately, and its gradients are calculated based on teacher use in several trailing batches. For our approach, we turned off dropout in both policy and value networks.

For both GKD and phase 2 training in our approach, for each task in the training set, we sampled one trajectory from the student model and one trajectory from the teacher model. For GKD we used reverse KL Divergence as the loss for the sampled trajectories.

\subsection{Lossy greedy speculative decoding}

Lossy speculative decoding was first presented in the original speculative decoding paper \cite{leviathan2023fast} and later studied in the context of Knowledge Distillation in DistillSpec \cite{DistllSpec}. Here, we present the algorithm used in this paper, specifically for the greedy sampling setting.

\begin{algorithm}[h!]
\caption{Single step of lossy speculative decoding: Require teacher, $p$, student, $\pi$, context, $(X_{<i}, Y)$, lenience, $l$, draft length, $K$}
\begin{algorithmic}[1]
    \For{$j$ from $0$ to $K - 1$}
        \State $x_{i+j} \gets \argmax_{x} \pi(x|X_{<i + j}, Y)$
        \State $\hat{x}_{i + j} \gets \argmax_{x} p(x|X_{<i + j}, Y)$
        \State $X_{i + j: i + j + 1} \gets [x_{i + j}]$
    \EndFor
    \State $n \gets \min \Big(\{j | p(x_{i + j}|X_{<i + j}, Y) < l p(\hat{x}_{i + j}|X_{<i + j}, Y), \forall\;0 \leq j < K, j \in N\} \cup \{K\}\Big)$
    \If{$n = K$}
    \State $X_{i + K} \gets \argmax_{x} p(x|X_{<i + K - 1}, Y)$
    \Else
    \State $X_{i + n: i + n + 1} \gets [\hat{x}_{i + n}]$
    \EndIf
    \State Return {$X_{i:i + n}$}
\end{algorithmic}
\end{algorithm}