While Large Language Models (LLM) with parameter sizes in the tens or hundreds of billion \cite{dettmers2022gpt3} \cite{metallama3} have shown excellent language, reasoning, and knowledge capabilities, they also come with increased inference costs. It is true that such generalist frontier models excel in a wide-range of tasks, but smaller models may suffice to satisfy the operational needs for a subset of tasks.

Knowledge Distillation (KD) \cite{hinton2015distilling} has been used as a means to improving the capabilities of smaller models. Typically, in KD, a smaller student model is used to mimic a larger teacher model, over some training data corresponding to tasks of interest. The training data can be a curated training set, can be elicited from the teacher model for the tasks of interest, or can be "on-policy" data generated from the student model.

Typically, the distilled student models are deployed stand-alone at test time \cite{agarwal2024policy} \cite{gu2023minillm}.  As student models are much smaller than teacher models, the improvement in the quality of results through distillation has a hard ceiling. In this paper, we rework the knowledge distillation formulation to allow the student model to use teacher help at test time based on testing rules expressed as natural language instructions. Pedagogically, this teaches the student new auxiliary lessons. The student learns the relative difficulty of different parts of the curriculum in a fine-grained manner and uses it to seek help as efficiently as possible. Practically, this allows the student to overcome the hard limitations of its size and accept expert help, when needed, at test time, resulting in a decoding method that allows a flexible trade-off between accuracy and efficiency.

There exist algorithms in the speculative decoding family \cite{chen2023accelerating}\cite{leviathan2023fast}, where a larger target model is used to validate tokens generated by a smaller draft model, and rejection sampling is employed to replace draft tokens with target tokens where appropriate. This results in improvement in latency without loss of accuracy. Knowledge distillation has been studied as a way to improve speculative decoding efficiency \cite{DistllSpec} \cite{liu2023online}, and speculative decoding has been employed in a lossy fashion where not all rejected draft tokens are resampled from the target \cite{DistllSpec}, which allows a trade-off between accuracy and efficiency.  The hallmark of all of these approaches is the use of target (teacher) model to judge and determine which tokens from the draft (student) to keep and which to reject. In these cases, the student is not aware of the teacher at runtime, and the teacher is deployed for each token generated at least in a validation capacity.  This deviates from our method where the student directly determines teacher use as appropriate without teacher involvement. The speculative decoding family of algorithms, thus, miss the pedagogical aspect of our work, which supplements the normal KD objectives with auxiliary learning functions. In addition, the use of the teacher model to validate tokens, and the use of rejection sampling to discard several tokens from the draft model, results in lower efficiency compared to ours.

% This means, the speculative decoding approaches miss the pedagogical aspect of our work, and al

In our approach, to incorporate teacher use at test time, we first examine the KD objective, specifically, the reverse KL-divergence (KLD) formulation of it, which has several advantages over the forward KLD approach \cite{huszar2015not}.  Prior work \cite{gu2023minillm} determined that reverse KLD formulation is equivalent to policy gradient optimization. However, the authors noted the need for several techniques such as single-step decomposition, length normalization, and teacher mixin to stabilize training and improve reward hacking. In this paper, we derive the reverse KLD objective as an instance of value optimization instead, and solve it using Path Consistency Learning (PCL) \cite{DBLP:journals/corr/NachumNXS17}.  The use of PCL allows us to natively utilize off-policy demonstrations from the teacher model, supplementing on-policy ones, to obtain the necessary training stability and convergence without any additional techniques. We further extend this value optimization framework within a constrained Reinforcement Learning formulation to incorporate the use of the teacher model during test time within constraints expressed via Natural Language prompts.

We empirically observe the behavior of models trained using our approach for two tasks - translation and summarization. For each task, a single model is trained to observe several rules regarding teacher use, from no teacher use to significant teacher use during test time. We observe that the models are able to learn to adhere to teacher use rules with high fidelity, and observe how they prioritize tokens to self-generate and tokens for which to request teacher help. We also observe how output quality changes with allowed teacher use.

In the next section, we introduce our methodology including formulation and training approaches. In the section after that, we outline our experiments and results.
