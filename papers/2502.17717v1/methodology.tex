\subsection{Knowledge Distillation is Value Optimization under Entropy Regularization}
\label{sec:KD_derive}

Let $\log \Pi(X) = \sum_i \log \pi(x_i|X_{<i})$
represent the log-probability of a sequence,
$X$, parameterized by the student model. Similarly, let
$\log P(X) = \sum_i \log p(x_i|X_{<i})$
represent the log-probability of a sequence, $X$, parameterized by the teacher model.
We use the short-hand notation
$E_{X_{i:T} \sim \Pi(.|X_{<i})} \log \frac{\Pi(X_{t:T}|X_{<t})}{P(X_{t:T}|X_{<t})} = E_{i:T}$. 
Note that the sequence prefix, $X_{<i}$, can include the prefix of a generated sequence as well as some task-specific instruction.

The following result holds for all autoregressive models, including Large Language Models.

\begin{equation}
    E_{i:T} = E_{x_i \sim \pi(.|X_{<i})} \Big(\log \frac{\pi(x_i|X_{<i})}{p(x_i|X_{<i})} + E_{i + 1:T}\Big)
    \label{eq:exp_recur}
\end{equation}

Setting, $-E_{i:T} = V^{\pi}(X_{<i})$, we get the following rewrite, with constants $\tau=1,\gamma=1$.

\begin{align}
    V^{\pi}(X_{<i}) = &\sum_{x_i} \pi(x_i|X_{<i})\Big(\log p(x_i|X_{<i}) - \nonumber \\
    &\tau \log \pi(x_i|X_{<i}) + \gamma V^{\pi}(X_{<i + 1})\Big) \label{eq:kd_recur}
\end{align}

Equation \ref{eq:kd_recur} represents the Bellman equations for an entropy-regularized Markov Decision Process with deterministic dynamics and state represented by $s_i = X_{<i}$.  The optimal policy for this formulation obeys certain path consistency constraints over trajectories sampled from on or off-policy rollouts \cite{DBLP:journals/corr/NachumNXS17}. This means that reverse KLD knowledge distillation can be optimized through minimizing the PCL loss over on- and off-policy demonstrations.

As mentioned before, prior work has shown that reverse KLD can be optimized through policy gradient optimization \cite{gu2023minillm}. There are a few differences that may be noted through our alternative formulation as entropy-regularized value optimization. First, in our formulation, the KL-term is decomposed into a reward term, $\log p(x_i|X_{<i})$ and an entropy regularization term, $\log \pi(x_i|X_{<i})$. This leads to a different interpretation of Knowledge Distillation as maximizing the likelihood of student-generated sequences under the teacher, with a regularization term encouraging exploration. Specifically, the role of one of the KLD terms, namely, $\log \pi(x_i|X_{<i})$, is to encourage exploration during Monte Carlo sampling of on-policy rollouts. If we are mainly interested in the student model learning the primary modes of the teacher, we can deboost this term using the $\tau$ component in order to improve training stability, if we have a large number of sequence prefixes $X_{<i}$ to start rollouts from. In language modeling, such prefixes may be obtained from training data or task instructions.

Second, in policy-gradient optimization, several techniques need to be used to stabilize training such as mixing in teacher tokens and weighting them by importance weights and so forth. In PCL, path consistency must be obeyed for trajectories from off-policy distributions as well without any modifications to the loss. Hence, this offers us a natural means to incorporate teacher demonstrations in the training loop, without any changes to the formulation.

In practice, for language modeling tasks, we are given sequence prefixes, and our goal is to generate the suffix optimally. To explicitly express this, conditioning on $X_{<i}$ can be rewritten as conditioning on $X_{<i},Y$, where $Y$ is a task instruction that was never part of the generation process, but which nevertheless forms part of the sequence prefix. We may then rewrite all conditioning on $X_{<i}$ in Equations \ref{eq:exp_recur}, \ref{eq:kd_recur} as conditioning on $(X_{<i},Y)$, where $Y \sim \mathcal{D}$, a dataset of task instructions. Hence, our optimization goal becomes,

\begin{eqnarray*}
    \pi^* &=& \argmax_{\pi} E_{Y \sim \mathcal{D}} V^{\pi}(X_{<1}, Y) \\
    &=& \argmax_{\pi} E_{Y \sim \mathcal{D}} V^{\pi}(Y)
\end{eqnarray*}

\subsubsection{Proof of Equation \ref{eq:exp_recur}}

Let $\log \Pi(X) = \sum_i \log \pi(x_i|X_{<i})$ represent the log-probability of a sequence $X$ parameterized by the 
student model. Similarly, let $\log p(X) = \sum_i \log p(x_i|X_{<i})$ represent the log-probability of a sequence
parameterized by the teacher model. We start with the following factorization for language models
(and autoregressive models in general):

\begin{align}
    &E_{X_{i: T} \sim \Pi(.|X_{<i})} \log \frac{\Pi(X_{i: T}|X_{<i})}{P(X_{i: T}|X_{<i})} = \label{eq:kd_recur1} \\
    &E_{x_i \sim \pi(.|X_{<i})} E_{X_{i + 1:T \sim \Pi(.|X_{<i+1}})} \log \frac{\Pi(X_{i: T}|X_{<i})}{P(X_{i: T}|X_{<i})} = \label{eq:kd_recur2} \\
    &E_{x_i \sim \pi(.|X_{<i})} \Big(\log \frac{\pi(x_i|X_{<i})}{P(x_i|X_{<i})} + \nonumber\\
    &E_{X_{i + 1:T \sim \Pi(.|X_{<i+1}})} \log \frac{\Pi(X_{i + 1: T}|X_{<i + 1})}{P(X_{i + 1: T}|X_{<i + 1})}\Big)
\end{align}

Designating $E_{X_{i: T} \sim \Pi(.|X_{<i})} \log \frac{\Pi(X_{i: T}|X_{<i})}{P(X_{i: T}|X_{<i})}$ as $E_{i:T}$, we get

\begin{equation}
E_{i:T} = E_{x_i \sim \pi(.|X_{<i})} \Big(\log \frac{\pi(x_i|X_{<i})}{p(x_i|X_{<i})} + E_{i+1:T}\Big)
\end{equation}

which is the same as Equation \ref{eq:exp_recur}

\subsection{Incorporating test-time Teacher use into Knowledge Distillation}
\label{sec:ConstrainedKD}

In the prior section, we derived a method to do KD by formulating it as a Reinforcement Learning value optimization problem whose goal is to maximize the likelihood of student-generated sequences as evaluated by the teacher.

In this section, we examine methods to let the student call for teacher help at test time. That is, we want to expand the action space of the student to include a call for teacher help. This can be implemented by simply letting the student generate a special token indicating the teacher call. However, without constraints, the student model would call the teacher to generate every token, rendering the method useless. It is useful only if the student model is constrained to call the teacher for help a limited number of times. This naturally leads to a constrained reinforcement-learning formulation, where the primary objective is maximization of output log-likelihood as evaluated by the teacher, and the secondary objective is a constraint related to teacher use. We further expand this idea to allow several operational settings (teacher use fractions) in a single model, all specified through the sequence prefix $Y$.

To derive this, we start by noting that PCL optimizes a policy, $\pi$ that maximizes the value function $V^{\pi}$ shown in Equation \ref{eq:kd_recur}. We add a second value function to this corresponding to the teacher use constraint, $V_C$. The optimization objective becomes

\begin{equation}
    \pi^* = \argmax_{\pi} V^{\pi}(Y)\;s.t.\;V_C(Y) \leq 0 \label{eq:optim}
\end{equation}

We consider $V_C$ of the form

\begin{align}
V_C(X_{<i}, Y) = &\sum_{x_i} \pi(x_i|X_{<i}, Y)(r_C(X_{<i},x_i,Y) + \nonumber \\
&\gamma V_C(X_{<i+1}, Y))
\end{align}

Using Lagrange relaxation, we may write a new value function.

\begin{align}
    V_C^{\pi}(X_{<i}, Y) &= V^{\pi}(X_{<i}, Y) - \lambda V_C(X_{<i}, Y) \nonumber \\
    &\sum_{x_i} \pi(x_i|X_{<i}, Y)\Big(\log p(x_i|X_{<i},Y) \nonumber \\
    &- \lambda r_C(X_{\leq i}, Y) \nonumber \\
    &- \tau \log \pi(x_i|X_{<i},Y) \nonumber \\
    &+ \gamma V_C^{\pi}(X_{\leq i, Y})\Big) \label{eq:constrained_pcl}
\end{align}

The constrained optimization problem of Equation \ref{eq:optim} can then be solved as follows, following prior practice in constrained Reinforcement Learning \cite{tessler2018reward} \cite{borkar2005actor}.

\begin{equation}
    \pi^* = \min_{\lambda \geq 0} \max_{\pi} V_C^{\pi}(Y)
\end{equation}

We implement this practically using primal and dual updates as follows.
\begin{itemize}
    \item Update $\pi$ by minimizing Path Consistency Loss
    \item Updating $\lambda = \Gamma_{\lambda}\Big(\lambda - \eta_{\lambda} \nabla_{\lambda} V_C^{\pi}(Y) \Big)$, where $\Gamma(x)$ maps $R \rightarrow R^+$ monotonically. $\nabla_{\lambda} V_C^{\pi}(Y) = -V_C(Y)$
\end{itemize}

\subsection{Student-Teacher decoding and data representation}

The student model's action space is the vocabulary of the language being modeled plus a special action to seek help from the teacher. The teacher-seeking action is invoked by the special token, $<\tau>$, which results in a teacher token being sampled.

It is true that for all student tokens, when an action is sampled, $x_i$, the next state deterministically becomes $X_{\leq i}$. However, when the token $<\tau>$ is sampled from the student, the value of the next state depends on the teacher token being sampled. Note that the teacher, here, is part of the environment and not part of the policy. Hence, when token $<\tau>$ is sampled from the student, the next state is non-determinstic, when teacher sampling is not greedy. This is not reflected in our treatment above, as the equations implicitly express deterministic dynamics, for the sake of brevity. However, PCL works equally well for non-deterministic dynamics and hence any method of sampling from the teacher model is supported during training.

In the sequel, the following notation is used: $C_{<i} = (X_{<i}, Y)$. Let $x_{i}^{(s)}$ represent an output token from the student at time step $i$, and $x_{i}^{(t)}$ represent an output token from the teacher model at time step $i$. We have the following relations.

\begin{equation}
x_{i}^{(s)} \sim \pi(x|C^{(S)}_{<i}) \label{eq:stud_decode}
\end{equation}

\begin{equation}
x_{i}^{(t)} \sim p(x|C^{(T)}_{<i}) \label{eq:teacher_decode}
\end{equation}

Note that, $x_{i}^{(s)} \in \mathcal{V} \cup \{<\tau>\}$ and $x_{i}^{(t)} \in \mathcal{V}$, where $\mathcal{V}$ is the token alphabet of the language being modeled. $C^{(S)}$ and $C^{(T)}$ represent sequence prefixes for the student and teacher, respectively.

We define two sequences, $X^{(out)}$ and $X^{(in)}$ as follows.

\begin{equation}
    x^{(in)}_i =
    \begin{cases}
    x_i^{(s)}, &\text{if } x^{s}_i \neq <\tau> \\
    x_i^{(t)} + ||\mathcal{V}||, &\text{otherwise}
    \end{cases} \label{eq:X_in}
\end{equation}

\begin{equation}
    x^{(out)}_i =
    \begin{cases}
    x_i^{(s)}, &\text{if } x^{s}_i \neq <\tau> \\
    x_i^{(t)}, &\text{otherwise}
    \end{cases} \label{eq:X_out}
\end{equation}

$X^{(out)}$ represents the output of the decoding algorithm, and $X^{(in)}_{<i}$ represents the state of the decoding process at time $i$. Note that $X^{(in)}$ includes additional internal information regarding the decoding process that is useful to the student model to budget teacher-use.  That is, $C^{(S)}_{<i} = (X^{(in)}_{<i}, Y)$ and $C^{(T)}_{<i} = (X^{(out)}_{<i}, Y)$. 

As $X^{(in)}$ is processed by the student model, each token in $X^{(in)}$ is embedded by it. For tokens in the alphabet, $\mathcal{V}$, we use the default embedding scheme in the student model, and for the special token, $<\tau>$, we add a learnable embedding to the default embedding.

\begin{equation}
    e(x^{(in)}_i) = \begin{cases}
        Embed(x^{(in)}_i),&\text{if } x^{(in)}_i \in \mathcal{V} \\
        Embed(x^{(t)}_i) + \beta,&\text{otherwise}
    \end{cases} \label{eq:embedding}
\end{equation}

\subsection{Training Scheme}

We train the student model in two phases. The first phase initializes the model to the action space, and the second stage finetunes it to use the actions appropriately, within budget.

\subsubsection{Phase 1 Training}

In the first phase, we train the student through behavior cloning to help it learn the basics of how to use the new special token, $<\tau>$, and the implications of using it.  For behavior cloning, we define an oracle policy that we teach the student through teacher-forcing. The oracle policy is not realizable in practice.

Given a teacher trajectory, we calculate the KL-divergence between the student and teacher distributions at each position along the trajectory and rank the sequence positions according to their KL-divergence in decreasing order. Let the KL-divergence rank of position $i$ be indicated as $r_i$.  Given a trajectory length, $l$ and a teacher use budget, $b$, we obtain a subset of positions, $S_{hi}$ which contains the top $l \times b$ positions by rank, $r_i$. Let $KL_i = D_{KL}(p(.|C^{(S)}_{<i})||\pi(.|C^{(T)}_{<i}))$, where the student distribution is renormalized to tokens in $\mathcal{V}$.

\begin{equation}
    loss = \begin{cases}
        -\log \pi(<\tau>|C^{(S)}_{<i}), &\text{if } i \notin S_{hi} \\
        KL_i,&\text{otherwise}
    \end{cases}
\end{equation}

This scheme teaches the student to use the teacher model at positions where the KL-divergence is high and improve its own distribution to align with the teacher's where the KL-divergence is low.

\subsubsection{Phase 2 Training}

Phase 1 training initializes the model teaching it to use the special $<\tau>$ token.  However, Phase 1 is not on-policy, as the model learns based on teacher trajectories. Since the student model is of substantially lower capacity than the teacher model, the trajectories produced from it are unlikely to match those from the teacher model. This can cause downstream modeling fidelity issues such as due to exposure bias \cite{ranzato2015sequence}, and the student use may not strictly adhere to constraints on teacher use budget as it is a constrained optimization problem that is not modeled correctly when using supervised learning.

We continue training using Reinforcement Learning through PCL as outlined above. The reward for the model is as follows, where $b$ is the teacher use budget and $\lambda$ is the Lagrangian. The teacher use budget represents what fraction of tokens should arise from the teacher on average.

\begin{equation}
    r_i = \begin{cases}
        \log p(x^{(s)}_i|X_{<i}) + \lambda b,&\text{if }x^{(s)}_i \neq <\tau> \\
        \lambda(b - 1), &\text{otherwise} \label{eq:rewards}
    \end{cases}
\end{equation}

This modifies the formulation in Equation \ref{eq:constrained_pcl} slightly, by using the teacher likelihood rewards only for tokens generated by the student. That is, we approximate the reward assuming that teacher tokens are implicitly correct.

In addition, there is a reward bonus $\lambda b$ when the token is generated by the student. When the token arises from the teacher, there is a penalty, $\lambda(b - 1)$.  Note that the teacher-use reward terms cancel out when the student exactly matches the budgeted teacher use, is positive when it satisfies the constraint strictly and is negative otherwise. This corresponds to the constraint value function given below ($1_{()}$ is the indicator function).

\begin{align}
    V_C(X_{<i, Y}) = E_{x_i \sim \pi}\Big(1_{x^{(s)}_i = <\tau>} \nonumber \\
    - 1_{x^{(s)}_i \neq <\tau>} b + \gamma V_C(X_{<i+1}, Y)\Big)
\end{align}

By tuning $\lambda$ during training we ensure that $\lambda$ moves towards zero if the constraints are met. Hence when constraints are met for a long training period, the reward for student token generation is $\log p(x^{(s)}_i|X_{<i})$ and is $0$ for teacher tokens (teacher tokens are implicitly trusted).

We use two other modifications in practice for ease and stability: (1) the reward component, $r = \log p(x|X_{<i})$ is converted to $\frac{r - \mu_r}{\sigma_r}$, where $\mu_r$ is the mean of teacher token likelihoods, and $\sigma_r$ is the standard deviation of the student token likelihoods (as evaluated by the teacher). In addition, the total reward is clipped to $[-2, 2]$. (2) We base $\lambda$ on a proxy of $V_C(Y)$, $O_C(Y)$, which simply calculates $\mathbb{E}(b - \sum_{i=1}^{l}1_{x^{(s)}_i = <\tau>})$. In other words, $\lambda$ is updated based on the excess teacher use compared to the teacher use budget. This is estimated from the most recent rollouts.

In our experiments, we train a single model to abide by several teacher use budgets, $b$.  To do this, we insert a decoding instruction into the model's task instruction. For instance, when the student should not use any teacher participation, the task instruction "summarize: " is changed to "With no teacher use, summarize: ", and for the corresponding rollouts, we set $b=0$.
