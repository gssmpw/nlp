\subsection{Methods compared}

Whitebox distillation, like the one presented in this paper, where the student has complete access to the teacher's output distribution, has been studied before in the context of lossy speculative decoding with student and teacher models as draft and target models in DistillSpec \cite{DistllSpec}. In this paper, we use DistillSpec as the baseline for comparison.

In speculative decoding, a teacher model is used to verify, in parallel, multiple tokens sampled from a student model. The number of tokens sampled at a time from the student is referred to as the draft length. If verification fails, the token for which it fails is sampled from a new distribution combining the student and teacher distributions. Parallel verification allows the teacher to be used efficiently. In standard speculative decoding, the verification criterion is stringent ensuring that the speculative decoding output (or distribution) is the same as that of the teacher model. In lossy speculative decoding (i.e., DistilSpec), the student token is accepted under more relaxed conditions allowing for trading off latency of decoding with accuracy. This is done via a lenience factor that relaxes the rejection criterion by a multiplicative fraction between 0 and 1. Details on our implementation of lossy speculative decoding are in the Appendix. Following the original work on Knowledge Distillation for Speculative decoding, we use Generalized Knowledge Distillation (GKD) \cite{agarwal2023gkd} to distil the teacher model onto the student model.

Our method doesn't involve speculation or verification. In our case, all decisions on token generation or teacher use are made by the student model, and the choice is not contested by the teacher. Hence every student token generated is accepted as an output token. Due to this lack of student token wastage, we believe our method will unlock additional performance gains.
% While it is reasonable to expect that the absence of verification may impact accuracy due to the inability to evaluate tokens at test , we expect that the fact that our own Knowledge Distillation step is performed in a manner accommodating teacher use, will mitigate these effects to a large degree, with significant performance gains.

\subsection{Models and Data}

For our experiments we used FLAN-T5-SMALL as the student model and FLAN-T5-XL as the teacher model \cite{chung2024scaling}. We performed experiments using translation for which we used WMT-19 German to English translation dataset \cite{barrault2019findings}, and summarization using the XSum dataset \cite{xsum-emnlp}.

For our method, for each task we trained the student model to obey six different teacher-use budgets, which would be communicated to the model at test time through the input prompt. The budgets are $b \in \{0.0, 0.1, 0.2, 0.3, 0.4, 0.5\}$ referred to by the keywords, "no", "light", "moderate-light", "moderate", "high" and "very high" teacher use. For a specific budget, the corresponding keyword would be included in the prompt. E.g., if we expect the model to use the teacher model, on average, less than 10\% of the time, we would prompt it as, "With light teacher use, summarize: ".

For lossy speculative decoding, we train the student model following the GKD approach, where both student and teacher trajectories are used to minimize token-wise reverse KL-divergence of the student's output distribution. For inference using lossy speculative decoding, we vary the lenience factor from 0 to 1 in steps of 0.1.

\subsection{Training Details}

For our approach, we performed 200 batches of phase 1 training through behavioral cloning, and a subsequent 5000 batches of RL finetuning following the reward scheme in Equation \ref{eq:rewards}. During RL finetuning, two trajectories are sampled for each task, one an off-policy trajectory according to the oracle policy, and one an on-policy trajectory. We performed four gradient updates per batch of episode generation, with a data mix that contains samples from a replay buffer, as well as oracle trajectories and on-policy trajectories that were generated in the current batch. After training is completed, we select the checkpoint that satisfies teacher-use constraints for all teacher-use budgets within a tolerance value $\delta$ and maximizes the output quality (measured through ROUGE-2), on a randomly sampled subset of 512 examples from the validation set.
% For testing, for each bucket, $b$, we selected the best checkpoint corresponding to that bucket. The best checkpoint for a bucket $b$ is defined as a checkpoint which achieves a target use fraction in the range $[b - \delta, b + \delta]$ ($\delta$ is a small tolerable error value), with the best output quality over 512 examples sampled from the validation set.

For DistillSpec, we trained the student model using a mix of teacher and student trajectories, following GKD. For each batch of trajectories sampled, we performed four gradient updates. Similar to our approach, we trained GKD over 5200 batches of trajectory sampling (or 20800 gradient updates). For testing, we selected the checkpoint with the best output quality over 512 examples sampled from the validation set.

\subsection{Performance Metrics}

For both the summarization and translation tasks, we compare the decoding efficiency, in terms of latency and output quality represented using ROUGE-2. For DistilSpec, we used three draft lengths, 3, 5, and 10. We use greedy sampling (temperature = 0) for all experiments. All experiments are performed on a single NVIDIA T4 GPU.

When a draft cycle is perfectly accurate (every token passes verification), a token may be sampled from the teacher model for free. Thus for a draft-length of 3, at best, the teacher model will be used 25\% of the output tokens. However, the effective number of times the teacher is used can be higher in real life, as draft tokens may be discarded. We used a draft length of 10 to correspond to a lower bound of teacher usage fraction of 1 / 11, which is lower than the lowest non-zero target-use configuration for our approach (which is 1/10).

The performance metrics for the translation task are provided in Figure \ref{fig:translation}. In the figure, the most desirable operating points are in the lower right corner. Notably, all approaches show a linear trend in latency-vs-quality trade-off for lower latency points, and show a saturating behavior at higher latency. This is very encouraging as it shows that operating points with higher efficiency can capture outsized quality improvements from the teacher model. DistillSpec configurations for draft lengths 3 and 5 operate similar to each other. However, for draft length 10, which has the lowest possible theoretical teacher usage, the achieved latency is, higher in most cases. The operating characteristics of DistillSpec and our approach are shifted with respect to each other. Our approach operates in the lower latency regime capable of making more aggressive trade-offs than DistillSpec. From a latency perspective, our approach is strictly better than DistillSpec. In areas where the operating points overlap in terms of quality, our approach provides lower latency than DistillSpec by up to approximately 25\%.

\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{translation.png}
    \caption{Performance metrics for the translation task. The X-axis indicates output quality. The Y-axis represents the achieved latency}
    \label{fig:translation}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[width=\columnwidth]{summarization.png}
    \caption{Performance metrics for the summariation task. The X-axis indicates output quality. The Y-axis represents the achieved latency}
    \label{fig:summarization}
\end{figure}

The performance metrics for the summarization task are provided in Figure \ref{fig:summarization}. Similar to the translation task, our approach unlocks operating points not available in DistilSpec, specifically at lower latencies. At higher latencies, DistilSpec is more than competitive with our approach, with similar or slightly better latency-accuracy trade-offs.

Across both tasks, our approach shows an ability to deliver operating points between that of the student and teacher model operating characteristics . Our approach represents a more aggressive trade-off between latency and accuracy than DistillSpec, and it is able to explore operating regions not accessible via DistillSpec.
