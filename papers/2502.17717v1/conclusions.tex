In this paper, we presented a formulation of Knowledge Distillation as a value optimization problem. This formulation has some practical advantages brought forth by the ability to seamlessly use off-policy and on-policy trajectories for model optimization. We then extended this method by expanding the action space of the model to include a functionality to call the teacher model for help during test time, within given teacher use budget. This was derived to be a constrained reinforcement learning extension to the knowledge distillation formulation. We trained models on two tasks, translation and summarization. For each task we defined six different rules corresponding to six teacher use budgets at test time.  We found that the models are largely able to satisfy the provided constraints, and show desirable improvement in output quality with allowed teacher usage. This approach does not depend on generating tokens based on validation or rejection sampling. Therefore, unlike prior speculative decoding approaches with knowledge distillation, our approach can extract larger efficiency gains without appreciable loss of accuracy.

We believe our framework can be extended to multi-agent decoding setups with a light-weight student model acting as the arbitrator between multiple sources of expertise. These sources can be other models, or tools, each with specific accuracy and latency/throughput characteristics.
