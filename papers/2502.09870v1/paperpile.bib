@MISC{noauthor_undated-ol,
  title = "1 Introduction.pdf"
}

@MISC{noauthor_undated-nb,
  title = "1-{Wed\_Foucault\_Panopticism}.pdf"
}

@MISC{noauthor_undated-cn,
  title = "1-{Wed\_Winner\_Do}-Artifacts-Have-Politics.pdf"
}

@INCOLLECTION{Murphy2012-gs,
  title     = "2 Immodest Witnessing, Affective Economies, and Objectivity",
  author    = "Murphy, Michelle",
  booktitle = "Seizing the Means of Reproduction",
  publisher = "Duke University Press",
  pages     = "68--101",
  abstract  = "2 Immodest Witnessing, Affective Economies, and Objectivity was
               published in Seizing the Means of Reproduction on page 68.",
  month     =  nov,
  year      =  2012,
  language  = "en"
}

@MISC{noauthor_undated-hc,
  title = "2 Theory.pdf"
}

@MISC{noauthor_undated-wq,
  title = "3 Method.pdf"
}

@MISC{noauthor_undated-dc,
  title = "4 Applications.pdf"
}

@MISC{noauthor_undated-eu,
  title = "5 Conclusion.pdf"
}

@MISC{noauthor_undated-pr,
  title = "10.{1201\_9780203736166}-2\_chapterpdf.pdf"
}

@MISC{noauthor_undated-wc,
  title = "10.{1201\_9780203736166}-5\_chapterpdf.pdf"
}

@MISC{noauthor_undated-wr,
  title = "10.{1201\_9780203736166}-8\_chapterpdf.pdf"
}

@MISC{noauthor_undated-np,
  title = "10.{1201\_9780203736166}-11\_chapterpdf.pdf"
}

@MISC{noauthor_undated-xi,
  title = "10.{1201\_9780203736166}-12\_chapterpdf.pdf"
}

@MISC{noauthor_undated-oc,
  title = "10.{4324\_9780203804827\_previewpdf}.pdf"
}

@ARTICLE{Ehrlich_undated-jq,
  title  = "12 Language and Gender",
  author = "Ehrlich, Susan"
}

@ARTICLE{Kajava2023-ep,
  title     = "19. Language of algorithms: agency, metaphors, and deliberations
               in {AI} discourses",
  author    = "Kajava, K and Sawhney, N",
  journal   = "Handbook of Critical Studies of Artificial",
  publisher = "books.google.com",
  abstract  = "… the deliberations around AI development, policy, and
               regulation. In this chapter, we examine how both types of agency
               are embedded in the thematic concerns, metaphors … deliberations
               …",
  year      =  2023
}

@MISC{noauthor_undated-yk,
  title = "2309.{06085v1}.pdf"
}

@MISC{noauthor_undated-vr,
  title = "9781478022336-001.pdf"
}

@MISC{noauthor_undated-gk,
  title = "9781478022336-004.pdf"
}

@MISC{noauthor_undated-pi,
  title = "9781478022336-006.pdf"
}

@MISC{noauthor_undated-cp,
  title = "9781478022336-007.pdf"
}

@MISC{noauthor_undated-pf,
  title = "9781478022336-009.pdf"
}

@MISC{noauthor_undated-cg,
  title = "9781478022336-010.pdf"
}

@MISC{noauthor_undated-st,
  title = "9781478023623-002.pdf"
}

@ARTICLE{Qian2019-tj,
  title         = "A Benchmark Dataset for Learning to Intervene in Online Hate
                   Speech",
  author        = "Qian, Jing and Bethke, Anna and Liu, Yinyin and Belding,
                   Elizabeth and Wang, William Yang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Countering online hate speech is a critical yet challenging
                   task, but one which can be aided by the use of Natural
                   Language Processing (NLP) techniques. Previous research has
                   primarily focused on the development of NLP methods to
                   automatically and effectively detect online hate speech while
                   disregarding further action needed to calm and discourage
                   individuals from using hate speech in the future. In
                   addition, most existing hate speech datasets treat each post
                   as an isolated instance, ignoring the conversational context.
                   In this paper, we propose a novel task of generative hate
                   speech intervention, where the goal is to automatically
                   generate responses to intervene during online conversations
                   that contain hate speech. As a part of this work, we
                   introduce two fully-labeled large-scale hate speech
                   intervention datasets collected from Gab and Reddit. These
                   datasets provide conversation segments, hate speech labels,
                   as well as intervention responses written by Mechanical Turk
                   Workers. In this paper, we also analyze the datasets to
                   understand the common intervention strategies and explore the
                   performance of common automatic response generation methods
                   on these new datasets to provide a benchmark for future
                   research.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Tyagi2020-ph,
  title     = "A Computational Analysis of Polarization on Indian and Pakistani
               Social Media",
  author    = "Tyagi, Aman and Field, Anjalie and Lathwal, Priyank and Tsvetkov,
               Yulia and Carley, Kathleen M",
  booktitle = "Social Informatics",
  publisher = "Springer International Publishing",
  pages     = "364--379",
  abstract  = "Between February 14, 2019 and March 4, 2019, a terrorist attack
               in Pulwama, Kashmir followed by retaliatory airstrikes led to
               rising tensions between India and Pakistan, two nuclear-armed
               countries. In this work, we examine polarizing messaging on
               Twitter during these events, particularly focusing on the
               positions of Indian and Pakistani politicians. We use a label
               propagation technique focused on hashtag co-occurrences to find
               polarizing tweets and users. Our analysis reveals that
               politicians in the ruling political party in India (BJP) used
               polarized hashtags and called for escalation of conflict more so
               than politicians from other parties. Our work offers the first
               analysis of how escalating tensions between India and Pakistan
               manifest on Twitter and provides a framework for studying
               polarizing messages.",
  year      =  2020
}

@ARTICLE{Authors_undated-zx,
  title  = "A {CONCEPTUAL} {FRAMEWORK} {FOR} {ANALYZING} {SOCIAL}
            {REPRESENTATION} {IN} {UNSTRUCTURED} {DATA}",
  author = "Authors, Anonymous"
}

@ARTICLE{Sowles2018-pj,
  title     = "A content analysis of an online pro-eating disorder community on
               Reddit",
  author    = "Sowles, Shaina J and McLeary, Monique and Optican, Allison and
               Cahn, Elizabeth and Krauss, Melissa J and Fitzsimmons-Craft,
               Ellen E and Wilfley, Denise E and Cavazos-Rehg, Patricia A",
  journal   = "Body Image",
  publisher = "Elsevier",
  volume    =  24,
  pages     = "137--144",
  abstract  = "Pro-eating disorder communities provide a refuge for individuals
               with eating disorders (EDs) who are ambivalent about seeking
               treatment. We investigated a pro-ED community on Reddit, an
               anonymous social networking platform with topical forums, to
               identify expression of behaviors aligned with ED symptoms and
               support for these behaviors. A content analysis on four weeks of
               topic-specific discussion threads (N=125 comments, 115 replies to
               comments) was conducted to identify behaviors consistent with ED
               psychopathology and support for these behaviors (informational,
               tangible assistance, esteem/emotional support). Results indicated
               that the content aligned with expressions of clinically relevant
               ED psychopathology, with eating concerns (49/125) and shape
               concerns (47/125) being most prevalent. The majority (92/115) of
               replies provided esteem/emotional support to the comment author.
               Online interventions and/or recovery programs are needed to
               counteract reinforcing dialogue that occurs on social media
               sites, like Reddit, and promote ED recovery through supportive
               messages on these platforms.",
  month     =  mar,
  year      =  2018,
  keywords  = "Content analysis; Disordered eating; Eating disorder symptoms;
               Reddit; Social media; Social support",
  language  = "en"
}

@PHDTHESIS{Carson2021-qz,
  title     = "A Content Analysis of Political Discourse on {TikTok}",
  author    = "Carson, Devin",
  publisher = "scholar.umw.edu",
  abstract  = "There is a gap in research about political discourse taking place
               on the social media platform TikTok. Traditionally, the public
               saw this platform as trivial and less important than other
               platforms because of its young user base. With the increase of
               political videos on this platform within the past few years,
               scholars are now beginning to observe this platform more
               seriously. This study aims to recognize how TikTok influences
               political discourse. A content analysis examined 500 videos under
               the hashtags, “politics” (10.0 B views), “conservative” (6.9 B
               views), “republican” (7.0 B views), “Trump” (7.2 B views)
               “democrat” (4.2 B views) and “leftist” (2.3 B views). The
               researcher made two accounts to engage with left-leaning and
               right-leaning videos separately. The results showed that creators
               on the platform made videos that lead to the further polarization
               of political parties. The results also suggest that TikTok’s
               algorithm creates online echo chambers, leading to users becoming
               radicalized on either side of the political spectrum.",
  year      =  2021,
  school    = "University of Mary Washington"
}

@PHDTHESIS{Benitez2022-gl,
  title     = "A Content Analysis of Queer Slang on Tik Tok",
  author    = "Benitez, Kuri",
  publisher = "scholar.umw.edu",
  abstract  = "Social media platforms’ concentration of diversity and
               interactions has accelerated the rate of cultural change on the
               Internet. With the rise of Tik Tok, a video-sharing social media
               platform, language used by marginalized groups is being
               incorporated into the platform’s slang due to the rapid rotation
               of trends. This study aims to provide data to a new field related
               to Tik Tok culture and the effects of trends on marginalized
               groups. Using qualitative content analysis, this article studies
               an LGBTQ+-inspired Tik Tok trend to determine if the portmanteau
               of “-ussy”’s meaning has changed from its original use and if
               trends inspired by marginalized cultures can benefit the
               marginalized group on the platform. Results demonstrated a lack
               of LGBTQ+ presence in videos in the trend, however, there was
               evidence to suggest that the trend has aided LGBTQ+ creators in
               succeeding on the platform due to the trend’s effects on the
               platform’s algorithm.",
  year      =  2022,
  school    = "University of Mary Washington"
}

@ARTICLE{Baele2023-cg,
  title     = "A diachronic cross-platforms analysis of violent extremist
               language in the Incel online ecosystem",
  author    = "Baele, S and Brace, L and Ging, D",
  journal   = "Terrorism and Political Violence",
  publisher = "Taylor \& Francis",
  abstract  = "The emergence and growth of incel subculture online has triggered
               a considerable body of research to date, most of which analyzing
               its worldview or mapping its position and …",
  year      =  2023
}

@ARTICLE{Mendelsohn2020-ce,
  title     = "A Framework for the Computational Linguistic Analysis of
               Dehumanization",
  author    = "Mendelsohn, Julia and Tsvetkov, Yulia and Jurafsky, Dan",
  journal   = "Front Artif Intell",
  publisher = "frontiersin.org",
  volume    =  3,
  pages     =  55,
  abstract  = "Dehumanization is a pernicious psychological process that often
               leads to extreme intergroup bias, hate speech, and violence aimed
               at targeted social groups. Despite these serious consequences and
               the wealth of available data, dehumanization has not yet been
               computationally studied on a large scale. Drawing upon social
               psychology research, we create a computational linguistic
               framework for analyzing dehumanizing language by identifying
               linguistic correlates of salient components of dehumanization. We
               then apply this framework to analyze discussions of LGBTQ people
               in the New York Times from 1986 to 2015. Overall, we find
               increasingly humanizing descriptions of LGBTQ people over time.
               However, we find that the label homosexual has emerged to be much
               more strongly associated with dehumanizing attitudes than other
               labels, such as gay. Our proposed techniques highlight processes
               of linguistic variation and change in discourses surrounding
               marginalized groups. Furthermore, the ability to analyze
               dehumanizing language at a large scale has implications for
               automatically detecting and understanding media bias as well as
               abusive language online.",
  month     =  aug,
  year      =  2020,
  keywords  = "LGBTQ; New York Times; computational sociolinguistics;
               dehumanization; language change; lexical variation; media",
  language  = "en"
}

@ARTICLE{Asael2021-ei,
  title     = "A generative approach for mitigating structural biases in natural
               language inference",
  author    = "Asael, D and Ziegler, Z and Belinkov, Y",
  journal   = "arXiv preprint arXiv:2108.14006",
  publisher = "arxiv.org",
  abstract  = "Many natural language inference (NLI) datasets contain biases
               that allow models to perform well by only using a biased subset
               of the input, without considering the remainder features …",
  year      =  2021
}

@ARTICLE{Acuna2012-lg,
  title    = "A {HCI} technique for improving requirements elicitation",
  author   = "Acuña, Silvia T and Castro, John W and Juristo, Natalia",
  journal  = "Information and Software Technology",
  volume   =  54,
  number   =  12,
  pages    = "1357--1375",
  abstract = "Context To develop usable software we need to understand the users
              that will interact with the system. Personas is a HCI technique
              that gathers information about users in order to comprehend their
              characteristics. This information is used to define fictitious
              persons on which development should focus. Personas provides an
              understanding of the user, often overlooked in SE developments.
              Objective The goal of our research is to modify Personas to
              readily build the technique into the requirements stage of regular
              SE developments. Method We tried to apply Cooper’s version of the
              Personas technique and we found shortcomings in both the
              definition of the procedure to be enacted and the formalization of
              the product resulting from the execution of each step of the
              Personas technique. For each of these limitations (up to a total
              of 11), we devised an improvement to be built into Personas. We
              have incorporated these improvements into a SE version of
              Personas. The improved Personas avoid the weaknesses encountered
              by an average software developer unfamiliar with HCI techniques
              applying the original Personas. Results We aim to improve
              requirements elicitation through the use of Personas. We have
              systematized and formalized Personas in the SE tradition in order
              to build this new version of the technique into the requirements
              stage. We have applied our proposal in an application example.
              Conclusion The integration of Personas into the SE requirements
              stage might improves the understanding of what the software
              product should do and how it should behave. We have modified the
              HCI Personas technique to comply with the levels of
              systematization required by SE. We have enriched the SE
              requirements process by incorporating Personas activities into
              requirements activities. Requirements elicitation and requirements
              analysis are the RE activities most affected by incorporating
              Personas.",
  month    =  dec,
  year     =  2012,
  keywords = "Personas technique; Human–computer interaction; Requirements;
              Analysis activity; Software process"
}

@INPROCEEDINGS{Salminen2020-gj,
  title     = "A Literature Review of Quantitative Persona Creation",
  author    = "Salminen, Joni and Guan, Kathleen and Jung, Soon-Gyo and
               Chowdhury, Shammur A and Jansen, Bernard J",
  booktitle = "Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--14",
  abstract  = "Quantitative persona creation (QPC) has tremendous potential, as
               HCI researchers and practitioners can leverage user data from
               online analytics and digital media platforms to better understand
               their users and customers. However, there is a lack of a
               systematic overview of the QPC methods and progress made, with no
               standard methodology or known best practices. To address this
               gap, we review 49 QPC research articles from 2005 to 2019.
               Results indicate three stages of QPC research: Emergence,
               Diversification, and Sophistication. Sharing resources, such as
               datasets, code, and algorithms, is crucial to achieving the next
               stage (Maturity). For practitioners, we provide guiding questions
               for assessing QPC readiness in organizations.",
  series    = "CHI '20",
  month     =  apr,
  year      =  2020,
  keywords  = "personas, quantitative persona creation, literature review"
}

@ARTICLE{Held_undated-pd,
  title  = "A Material Lens on Coloniality in {NLP}",
  author = "Held, William"
}

@ARTICLE{Lee2024-qo,
  title         = "A Mechanistic Understanding of Alignment Algorithms: A Case
                   Study on {DPO} and Toxicity",
  author        = "Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg,
                   Martin and Kummerfeld, Jonathan K and Mihalcea, Rada",
  journal       = "arXiv [cs.CL]",
  abstract      = "While alignment algorithms are now commonly used to tune
                   pre-trained language models towards a user's preferences, we
                   lack explanations for the underlying mechanisms in which
                   models become ``aligned'', thus making it difficult to
                   explain phenomena like jailbreaks. In this work we study a
                   popular algorithm, direct preference optimization (DPO), and
                   the mechanisms by which it reduces toxicity. Namely, we first
                   study how toxicity is represented and elicited in a
                   pre-trained language model, GPT2-medium. We then apply DPO
                   with a carefully crafted pairwise dataset to reduce toxicity.
                   We examine how the resulting model averts toxic outputs, and
                   find that capabilities learned from pre-training are not
                   removed, but rather bypassed. We use this insight to
                   demonstrate a simple method to un-align the model, reverting
                   it back to its toxic behavior.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hancock2011-oz,
  title    = "A meta-analysis of factors affecting trust in human-robot
              interaction",
  author   = "Hancock, Peter A and Billings, Deborah R and Schaefer, Kristin E
              and Chen, Jessie Y C and de Visser, Ewart J and Parasuraman, Raja",
  journal  = "Hum. Factors",
  volume   =  53,
  number   =  5,
  pages    = "517--527",
  abstract = "OBJECTIVE: We evaluate and quantify the effects of human, robot,
              and environmental factors on perceived trust in human-robot
              interaction (HRI). BACKGROUND: To date, reviews of trust in HRI
              have been qualitative or descriptive. Our quantitative review
              provides a fundamental empirical foundation to advance both theory
              and practice. METHOD: Meta-analytic methods were applied to the
              available literature on trust and HRI. A total of 29 empirical
              studies were collected, of which 10 met the selection criteria for
              correlational analysis and 11 for experimental analysis. These
              studies provided 69 correlational and 47 experimental effect
              sizes. RESULTS: The overall correlational effect size for trust
              was r = +0.26,with an experimental effect size of d = +0.71. The
              effects of human, robot, and environmental characteristics were
              examined with an especial evaluation of the robot dimensions of
              performance and attribute-based factors. The robot performance and
              attributes were the largest contributors to the development of
              trust in HRI. Environmental factors played only a moderate role.
              CONCLUSION: Factors related to the robot itself, specifically, its
              performance, had the greatest current association with trust, and
              environmental factors were moderately associated. There was little
              evidence for effects of human-related factors. APPLICATION: The
              findings provide quantitative estimates of human, robot, and
              environmental factors influencing HRI trust. Specifically, the
              current summary provides effect size estimates that are useful in
              establishing design and training guidelines with reference to
              robot-related factors of HRI trust. Furthermore, results indicate
              that improper trust calibration may be mitigated by the
              manipulation of robot design. However, many future research needs
              are identified.",
  month    =  oct,
  year     =  2011,
  language = "en"
}

@ARTICLE{Epley2018-yp,
  title     = "A Mind like Mine: The Exceptionally Ordinary Underpinnings of
               Anthropomorphism",
  author    = "Epley, Nicholas",
  journal   = "Journal of the Association for Consumer Research",
  publisher = "University of Chicago Press",
  volume    =  3,
  number    =  4,
  pages     = "591--598",
  abstract  = "From computers to cars to cell phones, consumers interact with
               inanimate objects on a daily basis. Despite being mindless
               machines, consumers nevertheless routinely attribute humanlike
               mental capacities of intentions, beliefs, attitudes, and
               knowledge to them. This process of anthropomorphism has
               historically been treated as an exceptional belief, explained
               away as simply an inevitable outcome of human nature or as an
               occasional product of human stupidity. Recent scientific
               advances, however, have revealed the very ordinary processes of
               social cognition underlying anthropomorphism. These processes
               enable psychologists to predict variability in the magnitude of
               anthropomorphism across contexts and also connect it to the
               inverse phenomena of dehumanization whereby people treat other
               human beings as if they lack a humanlike mind. Consumer behavior
               researchers are uniquely equipped to study these processes, to
               identify the precise situational features that give rise to
               anthropomorphism, to understand implications for consumer
               welfare, and to predict important consequences for how people
               treat everything from machines to animals to other human beings.",
  year      =  2018
}

@ARTICLE{Jones2024-fj,
  title         = "A Multi-Aspect Framework for Counter Narrative Evaluation
                   using Large Language Models",
  author        = "Jones, Jaylen and Mo, Lingbo and Fosler-Lussier, Eric and
                   Sun, Huan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Counter narratives - informed responses to hate speech
                   contexts designed to refute hateful claims and de-escalate
                   encounters - have emerged as an effective hate speech
                   intervention strategy. While previous work has proposed
                   automatic counter narrative generation methods to aid manual
                   interventions, the evaluation of these approaches remains
                   underdeveloped. Previous automatic metrics for counter
                   narrative evaluation lack alignment with human judgment as
                   they rely on superficial reference comparisons instead of
                   incorporating key aspects of counter narrative quality as
                   evaluation criteria. To address prior evaluation limitations,
                   we propose a novel evaluation framework prompting LLMs to
                   provide scores and feedback for generated counter narrative
                   candidates using 5 defined aspects derived from guidelines
                   from counter narrative specialized NGOs. We found that LLM
                   evaluators achieve strong alignment to human-annotated scores
                   and feedback and outperform alternative metrics, indicating
                   their potential as multi-aspect, reference-free and
                   interpretable evaluators for counter narrative evaluation.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Schneidernnan1988-nz,
  title     = "A nonanthropomorphic style guide: overcoming the humpty dumpty
               syndrome",
  author    = "Schneidernnan, B",
  journal   = "Comput Teach",
  publisher = "Citeseer",
  volume    =  8,
  pages     = "9--10",
  year      =  1988
}

@ARTICLE{Reich2020-dv,
  title         = "A Possibility in Algorithmic Fairness: Can Calibration and
                   Equal Error Rates Be Reconciled?",
  author        = "Reich, Claire Lazar and Vijaykumar, Suhas",
  journal       = "arXiv [cs.LG]",
  abstract      = "Decision makers increasingly rely on algorithmic risk scores
                   to determine access to binary treatments including bail,
                   loans, and medical interventions. In these settings, we
                   reconcile two fairness criteria that were previously shown to
                   be in conflict: calibration and error rate equality. In
                   particular, we derive necessary and sufficient conditions for
                   the existence of calibrated scores that yield classifications
                   achieving equal error rates at any given group-blind
                   threshold. We then present an algorithm that searches for the
                   most accurate score subject to both calibration and minimal
                   error rate disparity. Applied to the COMPAS criminal risk
                   assessment tool, we show that our method can eliminate error
                   disparities while maintaining calibration. In a separate
                   application to credit lending, we compare our procedure to
                   the omission of sensitive features and show that it raises
                   both profit and the probability that creditworthy individuals
                   receive loans.",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Zhou2023-bu,
  title         = "A Predictive Factor Analysis of Social Biases and
                   Task-Performance in Pretrained Masked Language Models",
  author        = "Zhou, Yi and Camacho-Collados, Jose and Bollegala, Danushka",
  journal       = "arXiv [cs.CL]",
  abstract      = "Various types of social biases have been reported with
                   pretrained Masked Language Models (MLMs) in prior work.
                   However, multiple underlying factors are associated with an
                   MLM such as its model size, size of the training data,
                   training objectives, the domain from which pretraining data
                   is sampled, tokenization, and languages present in the
                   pretrained corpora, to name a few. It remains unclear as to
                   which of those factors influence social biases that are
                   learned by MLMs. To study the relationship between model
                   factors and the social biases learned by an MLM, as well as
                   the downstream task performance of the model, we conduct a
                   comprehensive study over 39 pretrained MLMs covering
                   different model sizes, training objectives, tokenization
                   methods, training data domains and languages. Our results
                   shed light on important factors often neglected in prior
                   literature, such as tokenization or model objectives.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Rogers2020-iu,
  title     = "A primer in {BERTology}: What we know about how {BERT} works",
  author    = "Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "MIT Press - Journals",
  volume    =  8,
  pages     = "842--866",
  abstract  = "Transformer-based models have pushed state of the art in many
               areas of NLP, but our understanding of what is behind their
               success is still limited. This paper is the first survey of over
               150 studies of the popular BERT model. We review the current
               state of knowledge about how BERT works, what kind of information
               it learns and how it is represented, common modifications to its
               training objectives and architecture, the overparameterization
               issue, and approaches to compression. We then outline directions
               for future research.",
  month     =  dec,
  year      =  2020,
  language  = "en"
}

@INPROCEEDINGS{Hatzivassiloglou1995-xf,
  title     = "A Quantitative Evaluation of Linguistic Tests for the Automatic
               Prediction of Semantic Markedness",
  author    = "Hatzivassiloglou, Vasileios and McKeown, Kathleen",
  booktitle = "33rd Annual Meeting of the Association for Computational
               Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Cambridge, Massachusetts, USA",
  pages     = "197--204",
  abstract  = "We present a corpus-based study of methods that have been
               proposed in the linguistics literature for selecting the
               semantically unmarked term out of a pair of antonymous …",
  month     =  jun,
  year      =  1995
}

@ARTICLE{Benesch_undated-re,
  title  = "A report for Public Safety Canada under the Kanishka Project by",
  author = "Benesch, Susan and Ruths, Derek and Dillon, Kelly P and Saleem, Haji
            Mohammad and Wright, Lucas"
}

@INPROCEEDINGS{Lee2017-zr,
  title     = "A review of the mandarin-english code-switching corpus: {SEAME}",
  author    = "Lee, Grandee and Ho, Thi-Nga and Chng, Eng-Siong and Li, Haizhou",
  booktitle = "2017 International Conference on Asian Language Processing (IALP)",
  publisher = "ieeexplore.ieee.org",
  pages     = "210--213",
  abstract  = "In this paper, we report the development of the South East Asia
               Mandarin-English (SEAME) corpus, including 63 hours of
               transcribed spontaneous Mandarin-English code-switching speech in
               its first release, and an update of additional 129 transcribed
               hours of speech. The corpus was developed for code-switching
               speech recognition research, such as LVCSR, language recognition,
               and language segmentation. It was made publicly available through
               LDC since 2015. The corpus was recorded under unscripted
               interview and conversation settings, therefore, consisting of
               spontaneous speech. This paper seeks to present a comprehensive
               statistics and analysis of the corpus after the update in term of
               its composition, speaker profile and code-switch characteristics.
               This paper will also review its suitability for various
               code-switch related researches and possible further developments.",
  month     =  dec,
  year      =  2017,
  keywords  = "Speech;Speech coding;Switches;Speech
               recognition;Acoustics;Interviews;Asia;Code-switching
               corpus;Mandarin English corpus;SEAME"
}

@ARTICLE{Gebru2022-fq,
  title     = "A Review on {Human–Machine} Trust Evaluation: Human-Centric and
               Machine-Centric Perspectives",
  author    = "Gebru, Biniam and Zeleke, Lydia and Blankson, Daniel and Nabil,
               Mahmoud and Nateghi, Shamila and Homaifar, Abdollah and Tunstel,
               Edward",
  journal   = "IEEE Transactions on Human-Machine Systems",
  publisher = "IEEE",
  volume    =  52,
  number    =  5,
  pages     = "952--962",
  abstract  = "As complex autonomous systems become increasingly ubiquitous,
               their deployment and integration into our daily lives will become
               a significant endeavor. Human–machine trust relationship is now
               acknowledged as one of the primary aspects that characterize a
               successful integration. In the context of human–machine
               interaction (HMI), proper use of machines and autonomous systems
               depends both on the human and machine counterparts. On one hand,
               it depends on how well the human relies on the machine regarding
               the situation or task at hand based on willingness and
               experience. On the other hand, it depends on how well the machine
               carries out the task and how well it conveys important
               information on how the job is done. Furthermore, proper
               calibration of trust for effective HMI requires the factors
               affecting trust to be properly accounted for and their relative
               importance to be rightly quantified. In this article, the
               functional understanding of human–machine trust is viewed from
               two perspectives—human-centric and machine- centric. The human
               aspect of the discussion outlines factors, scales, and
               approaches, which are available to measure and calibrate human
               trust. The discussion on the machine aspect spans trustworthy
               artificial intelligence, built-in machine assurances, and ethical
               frameworks of trustworthy machines.",
  month     =  oct,
  year      =  2022
}

@ARTICLE{Sorensen2024-fe,
  title         = "A Roadmap to Pluralistic Alignment",
  author        = "Sorensen, Taylor and Moore, Jared and Fisher, Jillian and
                   Gordon, Mitchell and Mireshghallah, Niloofar and Rytting,
                   Christopher Michael and Ye, Andre and Jiang, Liwei and Lu,
                   Ximing and Dziri, Nouha and Althoff, Tim and Choi, Yejin",
  journal       = "arXiv [cs.AI]",
  abstract      = "With increased power and prevalence of AI systems, it is ever
                   more critical that AI systems are designed to serve all,
                   i.e., people with diverse values and perspectives. However,
                   aligning models to serve pluralistic human values remains an
                   open research question. In this piece, we propose a roadmap
                   to pluralistic alignment, specifically using language models
                   as a test bed. We identify and formalize three possible ways
                   to define and operationalize pluralism in AI systems: 1)
                   Overton pluralistic models that present a spectrum of
                   reasonable responses; 2) Steerably pluralistic models that
                   can steer to reflect certain perspectives; and 3)
                   Distributionally pluralistic models that are well-calibrated
                   to a given population in distribution. We also propose and
                   formalize three possible classes of pluralistic benchmarks:
                   1) Multi-objective benchmarks, 2) Trade-off steerable
                   benchmarks, which incentivize models to steer to arbitrary
                   trade-offs, and 3) Jury-pluralistic benchmarks which
                   explicitly model diverse human ratings. We use this framework
                   to argue that current alignment techniques may be
                   fundamentally limited for pluralistic AI; indeed, we
                   highlight empirical evidence, both from our own experiments
                   and from other work, that standard alignment procedures might
                   reduce distributional pluralism in models, motivating the
                   need for further research on pluralistic alignment.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Ungless2022-uy,
  title         = "A Robust Bias Mitigation Procedure Based on the Stereotype
                   Content Model",
  author        = "Ungless, Eddie L and Rafferty, Amy and Nag, Hrichika and
                   Ross, Björn",
  journal       = "arXiv [cs.CL]",
  abstract      = "The Stereotype Content model (SCM) states that we tend to
                   perceive minority groups as cold, incompetent or both. In
                   this paper we adapt existing work to demonstrate that the
                   Stereotype Content model holds for contextualised word
                   embeddings, then use these results to evaluate a fine-tuning
                   process designed to drive a language model away from
                   stereotyped portrayals of minority groups. We find the SCM
                   terms are better able to capture bias than demographic
                   agnostic terms related to pleasantness. Further, we were able
                   to reduce the presence of stereotypes in the model through a
                   simple fine-tuning procedure that required minimal human and
                   computer resources, without harming downstream performance.
                   We present this work as a prototype of a debiasing procedure
                   that aims to remove the need for a priori knowledge of the
                   specifics of bias in the model.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hanley_undated-sa,
  title  = "“A Special Operation”: A Quantitative Approach to Dissecting and
            Comparing Different Media Ecosystems' Coverage of the
            Russo-Ukrainian War",
  author = "Hanley, Hans W A and Kumar, Deepak and Durumeric, Zakir"
}

@INPROCEEDINGS{Hewitt2019-zb,
  title     = "{A} Structural Probe for Finding Syntax in Word Representations",
  author    = "Hewitt, John and Manning, Christopher D",
  booktitle = "Proceedings of the 2019 Conference of the North {A}merican
               Chapter of the Association for Computational Linguistics: Human
               Language Technologies, Volume 1 (Long and Short Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Minneapolis, Minnesota",
  pages     = "4129--4138",
  abstract  = "Recent work has improved our ability to detect linguistic
               knowledge in word representations. However, current methods for
               detecting syntactic knowledge do not test whether syntax trees
               are represented in their entirety. In this work, we propose a
               structural probe, which evaluates whether syntax trees are
               embedded in a linear transformation of a neural network's word
               representation space. The probe identifies a linear
               transformation under which squared L2 distance encodes the
               distance between words in the parse tree, and one in which
               squared L2 norm encodes depth in the parse tree. Using our probe,
               we show that such transformations exist for both ELMo and BERT
               but not in baselines, providing evidence that entire syntax trees
               are embedded implicitly in deep models' vector geometry.",
  month     =  jun,
  year      =  2019
}

@ARTICLE{Niu2017-ya,
  title    = "A study of style in machine translation: Controlling the formality
              of machine translation output",
  author   = "Niu, Xing and Martindale, Marianna J and Carpuat, Marine",
  journal  = "Empir Method Nat Lang Process",
  pages    = "2814--2819",
  abstract = "Stylistic variations of language, such as formality, carry
              speakers’ intention beyond literal meaning and should be conveyed
              adequately in translation. We propose to use lexical formality
              models to control the formality level of machine translation
              output. We demonstrate the effectiveness of our approach in
              empirical evaluations, as measured by automatic metrics and human
              assessments.",
  month    =  sep,
  year     =  2017
}

@ARTICLE{Karimi2022-ty,
  title     = "A Survey of Algorithmic Recourse: Contrastive Explanations and
               Consequential Recommendations",
  author    = "Karimi, Amir-Hossein and Barthe, Gilles and Schölkopf, Bernhard
               and Valera, Isabel",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  55,
  number    =  5,
  pages     = "1--29",
  abstract  = "Machine learning is increasingly used to inform decision making
               in sensitive situations where decisions have consequential
               effects on individuals’ lives. In these settings, in addition to
               requiring models to be accurate and robust, socially relevant
               values such as fairness, privacy, accountability, and
               explainability play an important role in the adoption and impact
               of said technologies. In this work, we focus on algorithmic
               recourse, which is concerned with providing explanations and
               recommendations to individuals who are unfavorably treated by
               automated decision-making systems. We first perform an extensive
               literature review, and align the efforts of many authors by
               presenting unified definitions, formulations, and solutions to
               recourse. Then, we provide an overview of the prospective
               research directions toward which the community may engage,
               challenging existing assumptions and making explicit connections
               to other ethical challenges such as security, privacy, and
               fairness.",
  month     =  dec,
  year      =  2022,
  keywords  = "contrastive explanations and consequential recommendations,
               Algorithmic recourse"
}

@ARTICLE{Zhang2023-og,
  title     = "A Survey of Controllable Text Generation Using Transformer-based
               Pre-trained Language Models",
  author    = "Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and
               Song, Dawei",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  56,
  number    =  3,
  pages     = "1--37",
  abstract  = "Controllable Text Generation (CTG) is an emerging area in the
               field of natural language generation (NLG). It is regarded as
               crucial for the development of advanced text generation
               technologies that better meet the specific constraints in
               practical applications. In recent years, methods using
               large-scale pre-trained language models (PLMs), in particular the
               widely used Transformer-based PLMs, have become a new paradigm of
               NLG, allowing generation of more diverse and fluent text.
               However, due to the limited level of interpretability of deep
               neural networks, the controllability of these methods needs to be
               guaranteed. To this end, controllable text generation using
               Transformer-based PLMs has become a rapidly growing yet
               challenging new research hotspot. A diverse range of approaches
               have emerged in the past 3 to 4 years, targeting different CTG
               tasks that require different types of controlled constraints. In
               this article, we present a systematic critical review on the
               common tasks, main approaches, and evaluation methods in this
               area. Finally, we discuss the challenges that the field is
               facing, and put forward various promising future directions. To
               the best of our knowledge, this is the first survey article to
               summarize the state-of-the-art CTG techniques from the
               perspective of Transformer-based PLMs. We hope it can help
               researchers and practitioners in the related fields to quickly
               track the academic and technological frontier, providing them
               with a landscape of the area and a roadmap for future research.",
  month     =  oct,
  year      =  2023,
  keywords  = "Controllable text generation, pre-trained language models,
               Transformer, controllability, systematic review"
}

@INPROCEEDINGS{Jose2020-kh,
  title     = "A Survey of Current Datasets for Code-Switching Research",
  author    = "Jose, Navya and Chakravarthi, Bharathi Raja and Suryawanshi,
               Shardul and Sherly, Elizabeth and McCrae, John P",
  booktitle = "2020 6th International Conference on Advanced Computing and
               Communication Systems (ICACCS)",
  pages     = "136--141",
  abstract  = "Code switching is a prevalent phenomenon in the multilingual
               community and social media interaction. In the past ten years, we
               have witnessed an explosion of code switched data in the social
               media that brings together languages from low resourced languages
               to high resourced languages in the same text, sometimes written
               in a non-native script. This increases the demand for processing
               code-switched data to assist users in various natural language
               processing tasks such as part-of-speech tagging, named entity
               recognition, sentiment analysis, conversational systems, and
               machine translation, etc. The available corpora for code
               switching research played a major role in advancing this area of
               research. In this paper, we propose a set of quality metrics to
               evaluate the dataset and categorize them accordingly.",
  month     =  mar,
  year      =  2020,
  keywords  = "Switches;Task analysis;Social network services;Natural language
               processing;Vocabulary;Tagging;Measurement;code switching;natural
               language processing;dataset"
}

@ARTICLE{Field2021-mj,
  title         = "A survey of race, racism, and anti-racism in {NLP}",
  author        = "Field, Anjalie and Blodgett, Su Lin and Waseem, Zeerak and
                   Tsvetkov, Yulia",
  journal       = "arXiv [cs.CL]",
  abstract      = "Despite inextricable ties between race and language, little
                   work has considered race in NLP research and development. In
                   this work, we survey 79 papers from the ACL anthology that
                   mention race. These papers reveal various types of
                   race-related bias in all stages of NLP model development,
                   highlighting the need for proactive consideration of how NLP
                   systems can uphold racial hierarchies. However, persistent
                   gaps in research on race and NLP remain: race has been siloed
                   as a niche topic and remains ignored in many NLP tasks; most
                   work operationalizes race as a fixed single-dimensional
                   variable with a ground-truth label, which risks reinforcing
                   differences produced by historical racism; and the voices of
                   historically marginalized people are nearly absent in NLP
                   literature. By identifying where and how NLP literature has
                   and has not considered race, especially in comparison to
                   related fields, our work calls for inclusion and racial
                   justice in NLP research practices.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Rini_undated-us,
  title    = "A Talking Cure for Autonomy Traps: How to share our social world
              with chatbots",
  author   = "Rini, Regina",
  journal  = "philpapers.org",
  abstract = "… autonomous , in Kant’s sense of the term. I develop ideas from
              psychologist Jean Piaget to show how chatbots are autonomy traps :
              … to treat mindless chatbots as if they are autonomous …"
}

@ARTICLE{Nicolas2024-kg,
  title         = "A taxonomy of stereotype content in large language models",
  author        = "Nicolas, Gandalf and Caliskan, Aylin",
  journal       = "arXiv [cs.CY]",
  abstract      = "This study introduces a taxonomy of stereotype content in
                   contemporary large language models (LLMs). We prompt ChatGPT
                   3.5, Llama 3, and Mixtral 8x7B, three powerful and widely
                   used LLMs, for the characteristics associated with 87 social
                   categories (e.g., gender, race, occupations). We identify 14
                   stereotype dimensions (e.g., Morality, Ability, Health,
                   Beliefs, Emotions), accounting for ~90\% of LLM stereotype
                   associations. Warmth and Competence facets were the most
                   frequent content, but all other dimensions were significantly
                   prevalent. Stereotypes were more positive in LLMs (vs.
                   humans), but there was significant variability across
                   categories and dimensions. Finally, the taxonomy predicted
                   the LLMs' internal evaluations of social categories (e.g.,
                   how positively/negatively the categories were represented),
                   supporting the relevance of a multidimensional taxonomy for
                   characterizing LLM stereotypes. Our findings suggest that
                   high-dimensional human stereotypes are reflected in LLMs and
                   must be considered in AI auditing and debiasing to minimize
                   unidentified harms from reliance in low-dimensional views of
                   bias in LLMs.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Jackson2021-wn,
  title    = "A Theory of Social Agency for Human-Robot Interaction",
  author   = "Jackson, Ryan Blake and Williams, Tom",
  journal  = "Front Robot AI",
  volume   =  8,
  pages    =  687726,
  abstract = "Motivated by inconsistent, underspecified, or otherwise
              problematic theories and usages of social agency in the HRI
              literature, and leveraging philosophical work on moral agency, we
              present a theory of social agency wherein a social agent (a thing
              with social agency) is any agent capable of social action at some
              level of abstraction. Like previous theorists, we conceptualize
              agency as determined by the criteria of interactivity, autonomy,
              and adaptability. We use the concept of face from politeness
              theory to define social action as any action that threatens or
              affirms the face of a social patient. With these definitions in
              mind, we specify and examine the levels of abstraction most
              relevant to HRI research, compare notions of social agency and the
              surrounding concepts at each, and suggest new conventions for
              discussing social agency in our field.",
  month    =  aug,
  year     =  2021,
  keywords = "human-robot interaction; levels of abstraction; moral agency;
              politeness theory; social agency",
  language = "en"
}

@ARTICLE{Ranaldi2023-jz,
  title         = "A Trip Towards Fairness: Bias and De-Biasing in Large
                   Language Models",
  author        = "Ranaldi, Leonardo and Ruzzetti, Elena Sofia and Venditti,
                   Davide and Onorati, Dario and Zanzotto, Fabio Massimo",
  journal       = "arXiv [cs.CL]",
  abstract      = "Cheap-to-Build Very Large-Language Models (CtB-LLMs) with
                   affordable training are emerging as the next big revolution
                   in natural language processing and understanding. These
                   CtB-LLMs are democratizing access to trainable Very
                   Large-Language Models (VLLMs) and, thus, may represent the
                   building blocks of many NLP systems solving downstream tasks.
                   Hence, a little or a large bias in CtB-LLMs may cause huge
                   harm. In this paper, we performed a large investigation of
                   the bias of three families of CtB-LLMs, and we showed that
                   debiasing techniques are effective and usable. Indeed,
                   according to current tests, the LLaMA and the OPT families
                   have an important bias in gender, race, religion, and
                   profession. In contrast to the analysis for other LLMs, we
                   discovered that bias depends not on the number of parameters
                   but on the perplexity. Finally, the debiasing of OPT using
                   LoRA reduces bias up to 4.12 points in the normalized
                   stereotype score.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Soni2021-xc,
  title         = "Abolitionist Networks: Modeling Language Change in
                   Nineteenth-Century Activist Newspapers",
  author        = "Soni, Sandeep and Klein, Lauren and Eisenstein, Jacob",
  journal       = "arXiv [cs.CL]",
  abstract      = "The abolitionist movement of the nineteenth-century United
                   States remains among the most significant social and
                   political movements in US history. Abolitionist newspapers
                   played a crucial role in spreading information and shaping
                   public opinion around a range of issues relating to the
                   abolition of slavery. These newspapers also serve as a
                   primary source of information about the movement for scholars
                   today, resulting in powerful new accounts of the movement and
                   its leaders. This paper supplements recent qualitative work
                   on the role of women in abolition's vanguard, as well as the
                   role of the Black press, with a quantitative text modeling
                   approach. Using diachronic word embeddings, we identify which
                   newspapers tended to lead lexical semantic innovations -- the
                   introduction of new usages of specific words -- and which
                   newspapers tended to follow. We then aggregate the evidence
                   across hundreds of changes into a weighted network with the
                   newspapers as nodes; directed edge weights represent the
                   frequency with which each newspaper led the other in the
                   adoption of a lexical semantic change. Analysis of this
                   network reveals pathways of lexical semantic influence,
                   distinguishing leaders from followers, as well as others who
                   stood apart from the semantic changes that swept through this
                   period. More specifically, we find that two newspapers edited
                   by women -- THE PROVINCIAL FREEMAN and THE LILY -- led a
                   large number of semantic changes in our corpus, lending
                   additional credence to the argument that a multiracial
                   coalition of women led the abolitionist movement in terms of
                   both thought and action. It also contributes additional
                   complexity to the scholarship that has sought to tease apart
                   the relation of the abolitionist movement to the women's
                   suffrage movement, and the vexed racial politics that
                   characterized their relation.",
  month         =  mar,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lucy2024-js,
  title         = "{AboutMe}: Using Self-Descriptions in Webpages to Document
                   the Effects of English Pretraining Data Filters",
  author        = "Lucy, Li and Gururangan, Suchin and Soldaini, Luca and
                   Strubell, Emma and Bamman, David and Klein, Lauren and Dodge,
                   Jesse",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models' (LLMs) abilities are drawn from their
                   pretraining data, and model development begins with data
                   curation. However, decisions around what data is retained or
                   removed during this initial stage is under-scrutinized. In
                   our work, we ground web text, which is a popular pretraining
                   data source, to its social and geographic contexts. We create
                   a new dataset of 10.3 million self-descriptions of website
                   creators, and extract information about who they are and
                   where they are from: their topical interests, social roles,
                   and geographic affiliations. Then, we conduct the first study
                   investigating how ten ``quality'' and English language
                   identification (langID) filters affect webpages that vary
                   along these social dimensions. Our experiments illuminate a
                   range of implicit preferences in data curation: we show that
                   some quality classifiers act like topical domain filters, and
                   langID can overlook English content from some regions of the
                   world. Overall, we hope that our work will encourage a new
                   line of research on pretraining data curation practices and
                   its social implications.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Anonymous2023-uc,
  title    = "Accurate and Well-Calibrated {ICD} Code Assignment with a
              Chunk-Based Classifier Attending over Diverse Label Embeddings",
  author   = "{Anonymous}",
  journal  = "https://openreview.net › forumhttps://openreview.net › forum",
  abstract = "Although the International Classification of Diseases (ICD) has
              been adopted worldwide, manually assigning ICD codes to clinical
              text is time-consuming, error-prone, and expensive, motivating the
              development of automated approaches. This paper describes a novel
              deep learning approach for ICD coding, combining several ideas
              from previous related work. In particular, we split long clinical
              documents into chunks, and use a strong Transformer-based model
              for processing each of the chunks independently. The resulting
              representations are processed with a max-pooling operation, and
              combined with a label embedding mechanism that explores diverse
              ICD code synonyms. Experiments with different splits of the
              MIMIC-III dataset show that the proposed approach outperforms the
              current state-of-the-art models in ICD coding, while also leading
              to properly calibrated results that can effectively inform
              downstream tasks such as text quantification.",
  month    =  oct,
  year     =  2023
}

@MISC{noauthor_undated-rz,
  title = "Activity\_and\_evaluation\_reporting\_practi.pdf"
}

@INPROCEEDINGS{Ribeiro2022-pc,
  title     = "Adaptive Testing and Debugging of {NLP} Models",
  author    = "Ribeiro, Marco Tulio and Lundberg, Scott",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Dublin, Ireland",
  pages     = "3253--3267",
  abstract  = "Current approaches to testing and debugging NLP models rely on
               highly variable human creativity and extensive labor, or only
               work for a very restrictive class of bugs. We present AdaTest, a
               process which uses large scale language models (LMs) in
               partnership with human feedback to automatically write unit tests
               highlighting bugs in a target model. Such bugs are then addressed
               through an iterative text-fix-retest loop, inspired by
               traditional software development. In experiments with expert and
               non-expert users and commercial / research models for 8 different
               tasks, AdaTest makes users 5-10x more effective at finding bugs
               than current approaches, and helps users effectively fix bugs
               without adding new bugs.",
  month     =  may,
  year      =  2022
}

@ARTICLE{Moradi2010-cz,
  title     = "Addressing gender and cultural diversity in body image:
               Objectification theory as a framework for integrating theories
               and grounding research",
  author    = "Moradi, Bonnie",
  journal   = "Sex Roles",
  publisher = "Springer Science and Business Media LLC",
  volume    =  63,
  number    = "1-2",
  pages     = "138--148",
  month     =  jul,
  year      =  2010,
  language  = "en"
}

@ARTICLE{Xiao_undated-bd,
  title     = "Addressing Interpersonal Harm in Online Gaming Communities: The
               Opportunities and Challenges for a Restorative Justice Approach",
  author    = "Xiao, S and Jhaver, S and Salehi, N",
  journal   = "ACM Trans. Comput. Hum. Interact.",
  publisher = "dl.acm.org",
  abstract  = "Most social media platforms implement content moderation to
               address interpersonal harms such as harassment. Content
               moderation relies on offender-centered, punitive approaches …"
}

@INPROCEEDINGS{Ravfogel2022-kx,
  title     = "Adversarial Concept Erasure in Kernel Space",
  author    = "Ravfogel, Shauli and Vargas, Francisco and Goldberg, Yoav and
               Cotterell, Ryan",
  editor    = "Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Abu Dhabi, United Arab Emirates",
  pages     = "6034--6055",
  abstract  = "The representation space of neural models for textual data
               emerges in an unsupervised manner during training. Understanding
               how human-interpretable concepts, such as gender, are encoded in
               these representations would improve the ability of users to
               control the content of these representations and analyze the
               working of the models that rely on them. One prominent approach
               to the control problem is the identification and removal of
               linear concept subspaces -- subspaces in the representation space
               that correspond to a given concept. While those are tractable and
               interpretable, neural network do not necessarily represent
               concepts in linear subspaces. We propose a kernelization of the
               recently-proposed linear concept-removal objective, and show that
               it is effective in guarding against the ability of certain
               nonlinear adversaries to recover the concept. Interestingly, our
               findings suggest that the division between linear and nonlinear
               models is overly simplistic: when considering the concept of
               binary gender and its neutralization, we do not find a single
               kernel space that exclusively contains all the concept-related
               information. It is therefore challenging to protect against all
               nonlinear adversaries at once.",
  month     =  dec,
  year      =  2022
}

@ARTICLE{Elazar2018-eb,
  title     = "Adversarial removal of demographic attributes from text data",
  author    = "Elazar, Y and Goldberg, Y",
  journal   = "arXiv preprint arXiv:1808.06640",
  publisher = "arxiv.org",
  abstract  = "Recent advances in Representation Learning and Adversarial
               Training seem to succeed in removing unwanted features from the
               learned representation. We show that demographic …",
  year      =  2018
}

@INPROCEEDINGS{Davis2023-hv,
  title     = "‘Affordances’ for Machine Learning",
  author    = "Davis, Jenny L",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "324--332",
  abstract  = "The field of machine learning (ML) has long struggled with a
               principles-to-practice gap, whereby careful codes and commitments
               dissipate on their way to practical application. The present work
               bridges this gap through an applied affordance framework.
               ‘Affordances’ are how the features of a technology shape, but do
               not determine, the functions and effects of that technology.
               Here, I demonstrate the value of an affordance framework as
               applied to ML, considering ML systems through the prism of design
               studies. Specifically, I apply the mechanisms and conditions
               framework of affordances, which models the way technologies
               request, demand, encourage, discourage, refuse, and allow
               technical and social outcomes. Illustrated through three case
               examples across work, policing, and housing justice, the
               mechanisms and conditions framework reveals the social nature of
               technical choices, clarifying how and for whom those choices
               manifest. This approach displaces vagaries and general claims
               with the particularities of systems in context, empowering
               critically minded practitioners while holding power—and the
               systems power relations produce—to account. More broadly, this
               work pairs the design studies tradition with the ML domain,
               setting a foundation for deliberate and considered (re)making of
               sociotechnical futures.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "Affordances, Design Studies, Machine Learning, AI Alignment,
               Mechanisms and Conditions Framework, Principles-to-Practice"
}

@ARTICLE{Haspelmath2006-hn,
  title     = "Against markedness (and what to replace it with)",
  author    = "Haspelmath, Martin",
  journal   = "J. Linguist.",
  publisher = "Cambridge University Press",
  volume    =  42,
  number    =  1,
  pages     = "25--70",
  abstract  = "This paper first provides an overview of the various senses in
               which the terms ‘marked’ and ‘unmarked’ have been used in
               20th-century linguistics. Twelve different senses, related only
               by family resemblances, are distinguished, grouped into four
               larger classes: markedness as complexity, as difficulty, as
               abnormality, and as a multidimensional correlation. In the second
               part of the paper, it is argued that the term ‘markedness’ is
               superfluous, because some of the concepts that it denotes are not
               helpful, and others are better expressed by more straightforward,
               less ambiguous terms. In a great many cases, frequency
               asymmetries can be shown to lead to a direct explanation of
               observed structural asymmetries, and in other cases additional
               concrete, substantive factors such as phonetic difficulty and
               pragmatic inferences can replace reference to an abstract notion
               of ‘markedness’.",
  month     =  mar,
  year      =  2006
}

@ARTICLE{Latour2014-hh,
  title     = "Agency at the Time of the Anthropocene",
  author    = "Latour, Bruno",
  journal   = "nlh",
  publisher = "Johns Hopkins University Press",
  volume    =  45,
  number    =  1,
  pages     = "1--18",
  abstract  = "ARRAY(0x5653968a4778)",
  year      =  2014
}

@ARTICLE{Tipler2014-js,
  title     = "Agency's role in dehumanization: Non-human metaphors of
               out-groups",
  author    = "Tipler, Caroline and Ruscher, Janet B",
  journal   = "Soc. Personal. Psychol. Compass",
  publisher = "Wiley",
  volume    =  8,
  number    =  5,
  pages     = "214--228",
  abstract  = "Abstract Dehumanization, the psychological process by which
               individuals or groups of individuals are denied human qualities
               or are believed to be less than human, has important negative
               consequences for intergroup relations: dehumanization reduces
               intergroup helping and excuses aggression towards members of
               other groups. Current models of dehumanization are unable to
               explain the variety of dehumanization that occurs in metaphorical
               thought. For example, they cannot account for the labeling of
               comatose individuals as ?vegetables,? nor do they adequately
               distinguish between of humans as predators vs. metaphors of
               humans as prey. We argue that this results from the paucity of
               attention devoted to the role of agency in the dehumanization
               process. The ABC model of dehumanization proposed in this paper
               broadens the scope of dehumanization theory by describing three
               unique components of agency: affective, behavioral, and
               cognitive. This article then delineates how the differential
               attribution of agency components impacts emotional responses
               toward out-groups in addition to the metaphors used to describe
               them. By incorporating both traditional types of dehumanization
               (extreme and overt negative evaluations) and ambivalent types of
               dehumanization (mixed positive and negative evaluations) into our
               model, we provide a more nuanced view of the dehumanization
               process that accounts for the variance in dehumanization by
               analogy.",
  month     =  may,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Carroll2024-ki,
  title         = "{AI} alignment with changing and influenceable reward
                   functions",
  author        = "Carroll, Micah and Foote, Davis and Siththaranjan, Anand and
                   Russell, Stuart and Dragan, Anca",
  journal       = "arXiv [cs.AI]",
  abstract      = "Existing AI alignment approaches assume that preferences are
                   static, which is unrealistic: our preferences change, and may
                   even be influenced by our interactions with AI systems
                   themselves. To clarify the consequences of incorrectly
                   assuming static preferences, we introduce Dynamic Reward
                   Markov Decision Processes (DR-MDPs), which explicitly model
                   preference changes and the AI's influence on them. We show
                   that despite its convenience, the static-preference
                   assumption may undermine the soundness of existing alignment
                   techniques, leading them to implicitly reward AI systems for
                   influencing user preferences in ways users may not truly
                   want. We then explore potential solutions. First, we offer a
                   unifying perspective on how an agent's optimization horizon
                   may partially help reduce undesirable AI influence. Then, we
                   formalize different notions of AI alignment that account for
                   preference change from the outset. Comparing the strengths
                   and limitations of 8 such notions of alignment, we find that
                   they all either err towards causing undesirable AI influence,
                   or are overly risk-averse, suggesting that a straightforward
                   solution to the problems of changing preferences may not
                   exist. As there is no avoiding grappling with changing
                   preferences in real-world settings, this makes it all the
                   more important to handle these issues with care, balancing
                   risks and capabilities. We hope our work can provide
                   conceptual clarity and constitute a first step towards AI
                   alignment practices which explicitly account for (and contend
                   with) the changing and influenceable nature of human
                   preferences.",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@INPROCEEDINGS{Kloft2024-kz,
  title     = "``{AI} enhances our performance, {I} have no doubt this one will
               do the same'': The Placebo effect is robust to negative
               descriptions of {AI}",
  author    = "Kloft, Agnes Mercedes and Welsch, Robin and Kosch, Thomas and
               Villa, Steeven",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Article 299",
  pages     = "1--24",
  abstract  = "Heightened AI expectations facilitate performance in human-AI
               interactions through placebo effects. While lowering expectations
               to control for placebo effects is advisable, overly negative
               expectations could induce nocebo effects. In a letter
               discrimination task, we informed participants that an AI would
               either increase or decrease their performance by adapting the
               interface, when in reality, no AI was present in any condition. A
               Bayesian analysis showed that participants had high expectations
               and performed descriptively better irrespective of the AI
               description when a sham-AI was present. Using cognitive modeling,
               we could trace this advantage back to participants gathering more
               information. A replication study verified that negative AI
               descriptions do not alter expectations, suggesting that
               performance expectations with AI are biased and robust to
               negative verbal descriptions. We discuss the impact of user
               expectations on AI interactions and evaluation.",
  series    = "CHI '24",
  month     =  may,
  year      =  2024,
  keywords  = "Artificial Intelligence, Decision-making, Performance
               expectation, Placebo"
}

@ARTICLE{Hancock2020-ks,
  title     = "{AI}-Mediated Communication: Definition, research agenda, and
               ethical considerations",
  author    = "Hancock, Jeffrey T and Naaman, Mor and Levy, Karen",
  journal   = "J. Comput. Mediat. Commun.",
  publisher = "Oxford University Press (OUP)",
  volume    =  25,
  number    =  1,
  pages     = "89--100",
  abstract  = "Abstract We define Artificial Intelligence-Mediated Communication
               (AI-MC) as interpersonal communication in which an intelligent
               agent operates on behalf of a communicator by modifying,
               augmenting, or generating messages to accomplish communication
               goals. The recent advent of AI-MC raises new questions about how
               technology may shape human communication and requires
               re-evaluation – and potentially expansion – of many of
               Computer-Mediated Communication’s (CMC) key theories, frameworks,
               and findings. A research agenda around AI-MC should consider the
               design of these technologies and the psychological, linguistic,
               relational, policy and ethical implications of introducing AI
               into human–human communication. This article aims to articulate
               such an agenda.",
  month     =  mar,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Karra2022-qx,
  title         = "{AI} Personification: Estimating the Personality of Language
                   Models",
  author        = "Karra, Saketh Reddy and Nguyen, Son and Tulabandhula, Theja",
  journal       = "arXiv [cs.CL]",
  abstract      = "Technology for open-ended language generation, a key
                   application of artificial intelligence, has advanced to a
                   great extent in recent years. Large-scale language models,
                   which are trained on large corpora of text, are being used in
                   a wide range of applications everywhere, from virtual
                   assistants to conversational bots. While these language
                   models output fluent text, existing research shows that these
                   models can and do capture human biases. Many of these biases,
                   especially those that could potentially cause harm, are being
                   well investigated. On the other hand, studies that infer and
                   change personality traits inherited by these models have been
                   scarce or non-existent. In this work, we explore the
                   personality traits of several large-scale language models
                   designed for open-ended text generation and the datasets used
                   for training them. Our work builds on the popular Big Five
                   factors and develops robust methods that quantify the
                   personality traits of these models and their underlying
                   datasets. In particular, we trigger the models with a
                   questionnaire designed for personality assessment and
                   subsequently classify the text responses into quantifiable
                   traits using a Zero-shot classifier. Our classification sheds
                   light on an important anthropomorphic element found in such
                   AI models and can help stakeholders decide how they should be
                   applied and how society could perceive them. We augment our
                   analysis by studying approaches that can alter these
                   personalities.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Qadri2023-rv,
  title     = "{AI’s} Regimes of Representation: A Community-centered Study of
               Text-to-Image Models in South Asia",
  author    = "Qadri, Rida and Shelby, Renee and Bennett, Cynthia L and Denton,
               Emily",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "506--517",
  abstract  = "This paper presents a community-centered study of cultural
               limitations of text-to-image (T2I) models in the South Asian
               context. We theorize these failures using scholarship on dominant
               media regimes of representations and locate them within
               participants’ reporting of their existing social
               marginalizations. We thus show how generative AI can reproduce an
               outsiders gaze for viewing South Asian cultures, shaped by global
               and regional power inequities. By centering communities as
               experts and soliciting their perspectives on T2I limitations, our
               study adds rich nuance into existing evaluative frameworks and
               deepens our understanding of the culturally-specific ways AI
               technologies can fail in non-Western and Global South settings.
               We distill lessons for responsible development of T2I models,
               recommending concrete pathways forward that can allow for
               recognition of structural inequalities.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "qualitative research in AI, human-centered AI, cultural harms of
               AI, South Asia, generative AI, non-western AI fairness, AI harms,
               text-to-image models, failure modes"
}

@ARTICLE{Chirkov2023-di,
  title     = "Alfred Schutz’s ‘Stranger’, the theory of sociocultural models,
               and mechanisms of acculturation",
  author    = "Chirkov, Valery",
  journal   = "Culture \& Psychology",
  publisher = "SAGE Publications Ltd",
  volume    =  29,
  number    =  1,
  pages     = "116--138",
  abstract  = "In this article, the author addresses the mechanisms of the
               acculturation of people who move across different cultural
               communities (immigrants, refugees, sojourners, international
               students, etc.). It starts by analyzing Alfred Schutz?s essay
               ?Stranger? and then connects it to the theory of sociocultural
               models (TSCM) (Chirkov, 2020a). Schutz?s treatise provides
               background and a conceptual map for articulating the mechanisms
               of acculturation. The TSCM elaborates on these concepts and
               hypotheses and justifies the proposed understanding of the
               psychological and sociocultural basis of acculturation. The
               primary idea of this approach to acculturation is that migrants
               experience a clash and tension between two sets of sociocultural
               models: from their home communities and from their host
               communities. Newcomers must understand the sources of this
               tension; in turn, they must reflect on it and then develop
               strategies for reconciling these two sets of models. During this
               process, their selves, rationality, reflective capacities, agency
               and intellectual autonomy become the primary means for their
               acculturation success.",
  month     =  mar,
  year      =  2023
}

@ARTICLE{Eriksson_Krutrok2021-pt,
  title     = "Algorithmic Closeness in Mourning: Vernaculars of the Hashtag
               \#grief on {TikTok}",
  author    = "Eriksson Krutrök, Moa",
  journal   = "Social Media + Society",
  publisher = "SAGE Publications Ltd",
  volume    =  7,
  number    =  3,
  pages     =  20563051211042396,
  abstract  = "This study looks at how mourning is expressed using the hashtag
               \#grief on the social media app TikTok using qualitative content
               analysis. In a dataset of 100 TikTok videos, this article
               explores how the TikTok ranking algorithms, which orders content
               based on previous user engagements, may connect people in
               mourning across the platform and how these platform-enabled
               interactions may shape grief expressions. The study shows how
               grief was narrated on TikTok, which sociotechnical templates
               (such as duets, stitches, and audios) were incorporated into such
               expressions, and how these expressions of grief challenged
               societal mourning norms. This article ends with a discussion
               about how different subcultural norms on TikTok are linked to the
               way in which ranking algorithms create social connections across
               the platform. This study proposes that the ?algorithmic
               closeness? of TikTok users in grief allows them to challenge
               societal mourning norms in imagined safe spaces, shaped by the
               algorithmic ranking systems on the platform.",
  month     =  jul,
  year      =  2021
}

@ARTICLE{Thomas2018-tp,
  title     = "Algorithms as fetish: Faith and possibility in algorithmic work",
  author    = "Thomas, Suzanne L and Nafus, Dawn and Sherman, Jamie",
  journal   = "Big Data \& Society",
  publisher = "SAGE Publications Ltd",
  volume    =  5,
  number    =  1,
  pages     =  2053951717751552,
  abstract  = "Algorithms are powerful because we invest in them the power to do
               things. With such promise, they can transform the ordinary, say
               snapshots along a robotic vacuum cleaner?s route, into something
               much more, such as a clean home. Echoing David Graeber?s revision
               of fetishism, we argue that this easy slip from technical
               capabilities to broader claims betrays not the ?magic? of
               algorithms but rather the dynamics of their exchange. Fetishes
               are not indicators of false thinking, but social contracts in
               material form. They mediate emerging distributions of power often
               too nascent, too slippery or too disconcerting to directly
               acknowledge. Drawing primarily on 2016 ethnographic research with
               computer vision professionals, we show how faith in what
               algorithms can do shapes the social encounters and exchanges of
               their production. By analyzing algorithms through the lens of
               fetishism, we can see the social and economic investment in some
               people?s labor over others. We also see everyday opportunities
               for social creativity and change. We conclude that what is
               problematic about algorithms is not their fetishization but
               instead their stabilization into full-fledged gods and demons ?
               the more deserving objects of critique.",
  month     =  jan,
  year      =  2018
}

@ARTICLE{Rakova2023-ln,
  title         = "Algorithms as Social-Ecological-Technological Systems: an
                   Environmental Justice Lens on Algorithmic Audits",
  author        = "Rakova, Bogdana and Dobbe, Roel",
  journal       = "arXiv [cs.CY]",
  abstract      = "This paper reframes algorithmic systems as intimately
                   connected to and part of social and ecological systems, and
                   proposes a first-of-its-kind methodology for environmental
                   justice-oriented algorithmic audits. How do we consider
                   environmental and climate justice dimensions of the way
                   algorithmic systems are designed, developed, and deployed?
                   These impacts are inherently emergent and can only be
                   understood and addressed at the level of relations between an
                   algorithmic system and the social (including institutional)
                   and ecological components of the broader ecosystem it
                   operates in. As a result, we claim that in absence of an
                   integral ontology for algorithmic systems, we cannot do
                   justice to the emergent nature of broader environmental
                   impacts of algorithmic systems and their underlying
                   computational infrastructure. We propose to define
                   algorithmic systems as ontologically indistinct from
                   Social-Ecological-Technological Systems (SETS), framing
                   emergent implications as couplings between social,
                   ecological, and technical components of the broader fabric in
                   which algorithms are integrated and operate. We draw upon
                   prior work on SETS analysis as well as emerging themes in the
                   literature and practices of Environmental Justice (EJ) to
                   conceptualize and assess algorithmic impact. We then offer
                   three policy recommendations to help establish a SETS-based
                   EJ approach to algorithmic audits: (1) broaden the inputs and
                   open-up the outputs of an audit, (2) enable meaningful access
                   to redress, and (3) guarantee a place-based and relational
                   approach to the process of evaluating impact. We
                   operationalize these as a qualitative framework of questions
                   for a spectrum of stakeholders. Doing so, this article aims
                   to inspire stronger and more frequent interactions across
                   policymakers, researchers, practitioners, civil society, and
                   grassroots communities.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Ramiro2018-ka,
  title    = "Algorithms in the historical emergence of word senses",
  author   = "Ramiro, Christian and Srinivasan, Mahesh and Malt, Barbara C and
              Xu, Yang",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  115,
  number   =  10,
  pages    = "2323--2328",
  abstract = "Human language relies on a finite lexicon to express a potentially
              infinite set of ideas. A key result of this tension is that words
              acquire novel senses over time. However, the cognitive processes
              that underlie the historical emergence of new word senses are
              poorly understood. Here, we present a computational framework that
              formalizes competing views of how new senses of a word might
              emerge by attaching to existing senses of the word. We test the
              ability of the models to predict the temporal order in which the
              senses of individual words have emerged, using an historical
              lexicon of English spanning the past millennium. Our findings
              suggest that word senses emerge in predictable ways, following an
              historical path that reflects cognitive efficiency, predominantly
              through a process of nearest-neighbor chaining. Our work
              contributes a formal account of the generative processes that
              underlie lexical evolution.",
  month    =  mar,
  year     =  2018,
  keywords = "chaining; cognitive efficiency; lexicon; polysemy; word sense
              extension",
  language = "en"
}

@ARTICLE{Xu2023-rs,
  title         = "Align on the Fly: Adapting Chatbot Behavior to Established
                   Norms",
  author        = "Xu, Chunpu and Chern, Steffi and Chern, Ethan and Zhang, Ge
                   and Wang, Zekun and Liu, Ruibo and Li, Jing and Fu, Jie and
                   Liu, Pengfei",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this paper, we aim to align large language models with the
                   ever-changing, complex, and diverse human values (e.g.,
                   social norms) across time and locations. This presents a
                   challenge to existing alignment techniques, such as
                   supervised fine-tuning, which internalize values within model
                   parameters. To overcome this, we propose an On-the-fly
                   Preference Optimization (OPO) method, which is a real-time
                   alignment that works in a streaming way. It employs an
                   external memory to store established rules for alignment,
                   which can constrain LLMs' behaviors without further training,
                   allowing for convenient updates and customization of human
                   values. We also introduce a scalable evaluation to assess the
                   proposed method more effectively. Experimental results on
                   both human-annotated and auto-generated questions from legal
                   and moral domains indicate the effectiveness of the proposed
                   OPO method. Our code and data are released at
                   https://github.com/GAIR-NLP/OPO.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wang2023-tp,
  title         = "Aligning large Language Models with human: A survey",
  author        = "Wang, Yufei and Zhong, Wanjun and Li, Liangyou and Mi, Fei
                   and Zeng, Xingshan and Huang, Wenyong and Shang, Lifeng and
                   Jiang, Xin and Liu, Qun",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs) trained on extensive textual
                   corpora have emerged as leading solutions for a broad array
                   of Natural Language Processing (NLP) tasks. Despite their
                   notable performance, these models are prone to certain
                   limitations such as misunderstanding human instructions,
                   generating potentially biased content, or factually incorrect
                   (hallucinated) information. Hence, aligning LLMs with human
                   expectations has become an active area of interest within the
                   research community. This survey presents a comprehensive
                   overview of these alignment technologies, including the
                   following aspects. (1) Data collection: the methods for
                   effectively collecting high-quality instructions for LLM
                   alignment, including the use of NLP benchmarks, human
                   annotations, and leveraging strong LLMs. (2) Training
                   methodologies: a detailed review of the prevailing training
                   methods employed for LLM alignment. Our exploration
                   encompasses Supervised Fine-tuning, both Online and Offline
                   human preference training, along with parameter-efficient
                   training mechanisms. (3) Model Evaluation: the methods for
                   evaluating the effectiveness of these human-aligned LLMs,
                   presenting a multifaceted approach towards their assessment.
                   In conclusion, we collate and distill our findings, shedding
                   light on several promising future research avenues in the
                   field. This survey, therefore, serves as a valuable resource
                   for anyone invested in understanding and advancing the
                   alignment of LLMs to better suit human-oriented tasks and
                   expectations. An associated GitHub link collecting the latest
                   papers is available at
                   https://github.com/GaryYufei/AlignLLMHumanSurvey.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Sun2023-ko,
  title         = "Aligning with whom? Large language models have gender and
                   racial biases in subjective {NLP} tasks",
  author        = "Sun, Huaman and Pei, Jiaxin and Choi, Minje and Jurgens,
                   David",
  journal       = "arXiv [cs.CL]",
  abstract      = "Human perception of language depends on personal backgrounds
                   like gender and ethnicity. While existing studies have shown
                   that large language models (LLMs) hold values that are closer
                   to certain societal groups, it is unclear whether their
                   prediction behaviors on subjective NLP tasks also exhibit a
                   similar bias. In this study, leveraging the POPQUORN dataset
                   which contains annotations of diverse demographic
                   backgrounds, we conduct a series of experiments on four
                   popular LLMs to investigate their capability to understand
                   group differences and potential biases in their predictions
                   for politeness and offensiveness. We find that for both
                   tasks, model predictions are closer to the labels from White
                   and female participants. We further explore prompting with
                   the target demographic labels and show that including the
                   target demographic in the prompt actually worsens the model's
                   performance. More specifically, when being prompted to
                   respond from the perspective of ``Black'' and ``Asian''
                   individuals, models show lower performance in predicting both
                   overall scores as well as the scores from corresponding
                   groups. Our results suggest that LLMs hold gender and racial
                   biases for subjective NLP tasks and that demographic-infused
                   prompts alone may be insufficient to mitigate such effects.
                   Code and data are available at
                   https://github.com/Jiaxin-Pei/LLM-Group-Bias.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{De_Visser2016-dg,
  title    = "Almost human: Anthropomorphism increases trust resilience in
              cognitive agents",
  author   = "de Visser, Ewart J and Monfort, Samuel S and McKendrick, Ryan and
              Smith, Melissa A B and McKnight, Patrick E and Krueger, Frank and
              Parasuraman, Raja",
  journal  = "J. Exp. Psychol. Appl.",
  volume   =  22,
  number   =  3,
  pages    = "331--349",
  abstract = "We interact daily with computers that appear and behave like
              humans. Some researchers propose that people apply the same social
              norms to computers as they do to humans, suggesting that social
              psychological knowledge can be applied to our interactions with
              computers. In contrast, theories of human–automation interaction
              postulate that humans respond to machines in unique and specific
              ways. We believe that anthropomorphism—the degree to which an
              agent exhibits human characteristics—is the critical variable that
              may resolve this apparent contradiction across the formation,
              violation, and repair stages of trust. Three experiments were
              designed to examine these opposing viewpoints by varying the
              appearance and behavior of automated agents. Participants received
              advice that deteriorated gradually in reliability from a computer,
              avatar, or human agent. Our results showed (a) that
              anthropomorphic agents were associated with greater trust
              resilience, a higher resistance to breakdowns in trust; (b) that
              these effects were magnified by greater uncertainty; and c) that
              incorporating human-like trust repair behavior largely erased
              differences between the agents. Automation anthropomorphism is
              therefore a critical variable that should be carefully
              incorporated into any general theory of human–agent trust as well
              as novel automation design.",
  month    =  sep,
  year     =  2016,
  language = "en"
}

@ARTICLE{Ging2019-tc,
  title     = "Alphas, betas, and incels: Theorizing the masculinities of the
               manosphere",
  author    = "Ging, D",
  journal   = "Men Masc.",
  publisher = "journals.sagepub.com",
  abstract  = "Since the emergence of Web 2.0 and social media, a particularly
               toxic brand of antifeminism has become evident across a range of
               online networks and platforms. Despite multiple …",
  year      =  2019
}

@ARTICLE{UnknownUnknown-co,
  title = "Americans' attitudes toward advancements in generative artificial
           intelligence"
}

@ARTICLE{Field2022-zt,
  title    = "An analysis of emotions and the prominence of positivity in
              \#BlackLivesMatter tweets",
  author   = "Field, Anjalie and Park, Chan Young and Theophilo, Antonio and
              Watson-Daniels, Jamelle and Tsvetkov, Yulia",
  journal  = "Proceedings of the National Academy of Sciences",
  volume   =  119,
  number   =  35,
  pages    = "e2205767119",
  abstract = "Emotions are a central driving force of activism; they motivate
              participation in movements and encourage sustained involvement. We
              use natural language processing techniques to analyze emotions
              expressed or solicited in tweets about 2020 Black Lives Matter
              protests. Traditional off-the-shelf emotion analysis tools often
              fail to generalize to new datasets and are unable to adapt to how
              social movements can raise new ideas and perspectives in short
              time spans. Instead, we use a few-shot domain adaptation approach
              for measuring emotions perceived in this specific domain: tweets
              about protests in May 2020 following the death of George Floyd.
              While our analysis identifies high levels of expressed anger and
              disgust across overall posts, it additionally reveals the
              prominence of positive emotions (encompassing, e.g., pride, hope,
              and optimism), which are more prevalent in tweets with explicit
              pro-BlackLivesMatter hashtags and correlated with on the ground
              protests. The prevalence of positivity contradicts stereotypical
              portrayals of protesters as primarily perpetuating anger and
              outrage. Our work offers data, analyses, and methods to support
              investigations of online activism and the role of emotions in
              social movements.",
  year     =  2022
}

@ARTICLE{Pavlick2016-fx,
  title     = "An empirical analysis of formality in online communication",
  author    = "Pavlick, Ellie and Tetreault, Joel",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "MIT Press - Journals",
  volume    =  4,
  pages     = "61--74",
  abstract  = "This paper presents an empirical study of linguistic formality.
               We perform an analysis of humans’ perceptions of formality in
               four different genres. These findings are used to develop a
               statistical model for predicting formality, which is evaluated
               under different feature settings and genres. We apply our model
               to an investigation of formality in online discussion forums, and
               present findings consistent with theories of formality and
               linguistic coordination.",
  month     =  dec,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Kuipers2012-kq,
  title         = "An existing, ecologically-successful genus of collectively
                   intelligent artificial creatures",
  author        = "Kuipers, Benjamin",
  journal       = "arXiv [cs.SI]",
  abstract      = "People sometimes worry about the Singularity [Vinge, 1993;
                   Kurzweil, 2005], or about the world being taken over by
                   artificially intelligent robots. I believe the risks of these
                   are very small. However, few people recognize that we already
                   share our world with artificial creatures that participate as
                   intelligent agents in our society: corporations. Our planet
                   is inhabited by two distinct kinds of intelligent beings ---
                   individual humans and corporate entities --- whose natures
                   and interests are intimately linked. To co-exist well, we
                   need to find ways to define the rights and responsibilities
                   of both individual humans and corporate entities, and to find
                   ways to ensure that corporate entities behave as responsible
                   members of society.",
  month         =  apr,
  year          =  2012,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SI"
}

@INCOLLECTION{Neo2019-nz,
  title     = "An Internet-Mediated Pathway for Online Radicalisation: {RECRO}",
  author    = "Neo, Loo Seng",
  booktitle = "Violent Extremism: Breakthroughs in Research and Practice",
  publisher = "IGI Global",
  pages     = "62--89",
  abstract  = "This chapter proposes an Internet-mediated radicalisation model,
               RECRO. It consists of five phases: (1) the Reflection phase
               details the triggers, needs, and vulnerabilities that an
               individual may have which increase one's receptiveness towards
               alternative belief systems; (2) the Exploration phase d...",
  year      =  2019,
  language  = "en"
}

@ARTICLE{Beeghly2020-th,
  title     = "An introduction to implicit bias: Knowledge, justice, and the
               social mind",
  author    = "Beeghly, E and Madva, A",
  publisher = "books.google.com",
  abstract  = "Written by a diverse range of scholars, this accessible
               introductory volume asks: What is implicit bias? How does
               implicit bias compromise our knowledge of others and social …",
  year      =  2020
}

@INPROCEEDINGS{Li2017-rq,
  title     = "An {NLP} Analysis of Exaggerated Claims in Science News",
  author    = "Li, Yingya and Zhang, Jieke and Yu, Bei",
  booktitle = "Proceedings of the 2017 {EMNLP} Workshop: Natural Language
               Processing meets Journalism",
  publisher = "Association for Computational Linguistics",
  address   = "Copenhagen, Denmark",
  pages     = "106--111",
  abstract  = "The discrepancy between science and media has been affecting the
               effectiveness of science communication. Original findings from
               science publications may be distorted with altered claim strength
               when reported to the public, causing misinformation spread. This
               study conducts an NLP analysis of exaggerated claims in science
               news, and then constructed prediction models for identifying
               claim strength levels in science reporting. The results
               demonstrate different writing styles journal articles and
               news/press releases use for reporting scientific findings.
               Preliminary prediction models reached promising result with room
               for further improvement.",
  month     =  sep,
  year      =  2017
}

@ARTICLE{Ramezani2021-fx,
  title         = "An unsupervised framework for tracing textual sources of
                   moral change",
  author        = "Ramezani, Aida and Zhu, Zining and Rudzicz, Frank and Xu,
                   Yang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Morality plays an important role in social well-being, but
                   people's moral perception is not stable and changes over
                   time. Recent advances in natural language processing have
                   shown that text is an effective medium for informing moral
                   change, but no attempt has been made to quantify the origins
                   of these changes. We present a novel unsupervised framework
                   for tracing textual sources of moral change toward entities
                   through time. We characterize moral change with probabilistic
                   topical distributions and infer the source text that exerts
                   prominent influence on the moral time course. We evaluate our
                   framework on a diverse set of data ranging from social media
                   to news articles. We show that our framework not only
                   captures fine-grained human moral judgments, but also
                   identifies coherent source topics of moral change triggered
                   by historical events. We apply our methodology to analyze the
                   news in the COVID-19 pandemic and demonstrate its utility in
                   identifying sources of moral change in high-impact and
                   real-time social events.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Stapleton2019-ax,
  title     = "Ana as god: Religion, interdiscursivity and identity on pro-ana
               websites",
  author    = "Stapleton, Karyn and Evans, Sarah L and Rhys, Catrin S",
  journal   = "Discourse \& Communication",
  publisher = "SAGE Publications",
  volume    =  13,
  number    =  3,
  pages     = "320--341",
  abstract  = "Pro-anorexia (pro-ana) is an Internet-based movement that
               provides advice and support for the development/maintenance of an
               eating disorder (ED). The movement is sometimes framed as a
               religion, with rituals, psalms, creeds and the invocation of a
               deity (Ana) who personifies the ED. The latter aspect is likely
               to influence identities and behaviours as well as providing
               emotional support and motivation for community members. However,
               there is little sustained empirical analysis of how members
               themselves orient to and self-position within the religious
               discourse. Here, we apply the concept of interdiscursivity to
               examine the construction of Ana as god(dess). Drawing on a body
               of online interactions from one pro-ana website over a 47-day
               period, we discursively analyse members? constructions of Ana and
               their relationship with her. With reference to biblical texts, we
               consider how these constructions directly reference concepts of
               Christian religion and faith. Implications for understanding
               pro-ana and interdiscursivity are discussed.",
  month     =  jun,
  year      =  2019
}

@ARTICLE{Renaldo2017-ho,
  title     = "Analysis of linguistic features of beauty product advertisements
               in Cosmopolitan Magazine: A Critical Discourse Analysis",
  author    = "Renaldo, Zainal Arifin",
  journal   = "Tell-us J.",
  publisher = "STKIP PGRI Sumatera Barat",
  volume    =  3,
  number    =  2,
  pages     = "141--154",
  abstract  = "This research aims at exploring the linguistic features employed
               by advertisers in Cosmopolitan Magazine beauty product
               advertisements. The study mainly focuses on the use of language
               in beauty product advertisements and the strategies employed by
               the advertisers in shaping the ideal concept of women’s beauty.
               This research is conducted under the theory of Critical Discourse
               Analysis proposed by Fairclough that focuses on a conception of
               discourse as text (micro level), discourse practice (meso level)
               and sociocultural practice (macro level). Its aim is to explore
               the relationships among language, ideology and power and to find
               out how advertisers persuade the women to buy their products. The
               result shows that there are some linguistic features employed by
               the advertisers i.e. positive and negative adjective, pronouns,
               imperatives, and modality. Meanwhile the strategies employed are
               positive-self representation, irrational representation,
               celebrity endorsement, and clinical test proof",
  month     =  sep,
  year      =  2017
}

@INPROCEEDINGS{Roy2021-ky,
  title     = "Analysis of Nuanced Stances and Sentiment Towards Entities of
               {US} Politicians through the Lens of Moral Foundation Theory",
  author    = "Roy, Shamik and Goldwasser, Dan",
  booktitle = "Proceedings of the Ninth International Workshop on Natural
               Language Processing for Social Media",
  publisher = "Association for Computational Linguistics",
  address   = "Online",
  pages     = "1--13",
  abstract  = "The Moral Foundation Theory suggests five moral foundations that
               can capture the view of a user on a particular issue. It is
               widely used to identify sentence-level sentiment. In this paper,
               we study the Moral Foundation Theory in tweets by US politicians
               on two politically divisive issues - Gun Control and Immigration.
               We define the nuanced stance of politicians on these two topics
               by the grades given by related organizations to the politicians.
               First, we identify moral foundations in tweets from a huge corpus
               using deep relational learning. Then, qualitative and
               quantitative evaluations using the corpus show that there is a
               strong correlation between the moral foundation usage and the
               politicians' nuanced stance on a particular topic. We also found
               substantial differences in moral foundation usage by different
               political parties when they address different entities. All of
               these results indicate the need for more intense research in this
               area.",
  month     =  jun,
  year      =  2021
}

@ARTICLE{Sulistiyaningsih2022-aw,
  title     = "Analysis of Slang Words Used for Comments on Tiktok",
  author    = "Sulistiyaningsih, Eka Fanti and Muttaqiyah, Nurul",
  journal   = "journalistics",
  publisher = "journalistics.org",
  volume    =  2,
  number    =  01,
  pages     = "44--51",
  abstract  = "This research in title “Analysis of Slang Words Used for Comments
               on Tiktok”. The aim of this research are to analyze slang words
               on Tiktok and to find out the meaning of these slang. This study
               is included in descriptive qualitative method. In collecting
               data, the writer concentrates the comments which contains the
               slang words produced by netizens. After the researcher analyzed
               the data, it shows that slang is used for entertainment, just
               joking/funny, looks contemporary, reduces seriousness in speaking
               and softens satire. Therefore, research on slang will be able to
               continue from time to time as a sign that language tends to
               change.",
  month     =  jun,
  year      =  2022,
  keywords  = "slang words,; comments,; tiktok",
  language  = "en"
}

@ARTICLE{Durrani2020-da,
  title     = "Analyzing individual neurons in pre-trained language models",
  author    = "Durrani, N and Sajjad, H and Dalvi, F and Belinkov, Y",
  journal   = "arXiv preprint arXiv:2010.02695",
  publisher = "arxiv.org",
  abstract  = "While a lot of analysis has been carried to demonstrate
               linguistic knowledge captured by the representations learned
               within deep NLP models, very little attention has been paid
               towards …",
  year      =  2020
}

@ARTICLE{Moon2023-gh,
  title     = "Analyzing Norm Violations in Live-Stream Chat",
  author    = "Moon, J and Lee, D H and Cho, H and Jin, W and Park, C Y and Kim,
               M and {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Toxic language, such as hate speech, can deter users from
               participating in online communities and enjoying popular
               platforms. Previous approaches to detecting toxic …",
  year      =  2023
}

@INPROCEEDINGS{Sanchez_Villegas2021-fm,
  title     = "Analyzing Online Political Advertisements",
  author    = "Sánchez Villegas, Danae and Mokaram, Saeid and Aletras, Nikolaos",
  booktitle = "Findings of the Association for Computational Linguistics:
               ACL-IJCNLP 2021",
  publisher = "Association for Computational Linguistics",
  address   = "Online",
  pages     = "3669--3680",
  month     =  aug,
  year      =  2021
}

@INPROCEEDINGS{Vasconcellos2023-vf,
  title     = "Analyzing Polarization And Toxicity On Political Debate In
               Brazilian {TikTok} Videos Transcriptions",
  author    = "Vasconcellos, Paulo Henrique Santos and Lara, Pedro Diógenes de
               Almeida and Marques-Neto, Humberto Torres",
  booktitle = "Proceedings of the 15th ACM Web Science Conference 2023",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "33--42",
  abstract  = "With the rise of TikTok’s popularity, there is an opportunity to
               understand how political communication has been made on this
               platform based on short videos. In addition to understanding what
               topics and themes are discussed on TikTok, we also analyzed the
               polarization and toxic behavior of its users. However, this great
               opportunity brings a challenge as well. As TikTok is a video
               platform, the largest content information is in the video itself,
               so it is necessary to extract this information from video data
               and features. In this paper, we propose a methodology to extract
               topics from TikTok video transcriptions in order to identify
               polarization and toxicity in their contents, by using techniques
               that range from web crawling to speech recognition algorithms. By
               providing a robust audio cleaning pipeline, it’s possible to
               generate a less noisy dataset, by removing silence and music
               segments. We validate our methodology by practically applying it
               to create topics in order to identify signs of political
               polarization and toxicity in 8,329 Brazilian political TikTok
               videos, collected over the last two years. Our work shows that it
               is possible to extract coherent and meaningful topics from TikTok
               videos, even with the challenges spoken texts bring. We point out
               that topics related to religion and social classes contain a
               higher percentage of toxicity and polarization, as well as
               opposite hashtags, such as ``direita'' (Right-wing) and
               ``esquerda'' (Left-wing).",
  series    = "WebSci '23",
  month     =  apr,
  year      =  2023,
  keywords  = "Topic Modeling, Political Polarization, Online Social Networks,
               Text Analysis, Political Toxicity, TikTok"
}

@INPROCEEDINGS{Stark2024-df,
  title     = "Animation and Artificial Intelligence",
  author    = "Stark, Luke",
  booktitle = "Proceedings of the 2024 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1663--1671",
  abstract  = "Animation as genre is broadly used across many forms of digital
               media. In this paper, I argue ChatGPT and similar chatbots
               powered by Large Language Models (LLMs) can be best understood as
               animated characters. More than just cartooning, puppetry, or CGI,
               animation is a paradigm involving the projection of qualities
               perceived as human such as power, agency, will, and personality
               outside of the self and onto objects in the environment.
               Characteristics of animation—including reliance on stereotypes,
               obfuscation of human labor, and manipulation of an audience's
               emotions—can help us both analyze and respond appropriately to
               interactive AI technologies and the hyperbolic claims of their
               promoters.",
  series    = "FAccT '24",
  month     =  jun,
  year      =  2024,
  keywords  = "AI ethics, ChatGPT, Turing test, animation, artificial
               intelligence, emotion, grammar of action, human-computer
               interaction (HCI), human-machine interaction, inference,
               interactive AI, labor"
}

@ARTICLE{Cai2024-xo,
  title         = "Antagonistic {AI}",
  author        = "Cai, Alice and Arawjo, Ian and Glassman, Elena L",
  journal       = "arXiv [cs.AI]",
  abstract      = "The vast majority of discourse around AI development assumes
                   that subservient, ``moral'' models aligned with ``human
                   values'' are universally beneficial -- in short, that good AI
                   is sycophantic AI. We explore the shadow of the sycophantic
                   paradigm, a design space we term antagonistic AI: AI systems
                   that are disagreeable, rude, interrupting, confrontational,
                   challenging, etc. -- embedding opposite behaviors or values.
                   Far from being ``bad'' or ``immoral,'' we consider whether
                   antagonistic AI systems may sometimes have benefits to users,
                   such as forcing users to confront their assumptions, build
                   resilience, or develop healthier relational boundaries.
                   Drawing from formative explorations and a speculative design
                   workshop where participants designed fictional AI
                   technologies that employ antagonism, we lay out a design
                   space for antagonistic AI, articulating potential benefits,
                   design techniques, and methods of embedding antagonistic
                   elements into user experience. Finally, we discuss the many
                   ethical challenges of this space and identify three
                   dimensions for the responsible design of antagonistic AI --
                   consent, context, and framing.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Floridi2024-cm,
  title     = "Anthropomorphising machines and computerising minds: The
               crosswiring of languages between artificial intelligence and
               brain \& cognitive sciences",
  author    = "Floridi, Luciano and Nobre, Anna C",
  journal   = "Minds Mach.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  34,
  number    =  1,
  abstract  = "AbstractThe article discusses the process of “conceptual
               borrowing”, according to which, when a new discipline emerges, it
               develops its technical vocabulary also by appropriating terms
               from other neighbouring disciplines. The phenomenon is likened to
               Carl Schmitt’s observation that modern political concepts have
               theological roots. The authors argue that, through extensive
               conceptual borrowing, AI has ended up describing computers
               anthropomorphically, as computational brains with psychological
               properties, while brain and cognitive sciences have ended up
               describing brains and minds computationally and informationally,
               as biological computers. The crosswiring between the technical
               languages of these disciplines is not merely metaphorical but can
               lead to confusion, and damaging assumptions and consequences. The
               article ends on an optimistic note about the self-adjusting
               nature of technical meanings in language and the ability to leave
               misleading conceptual baggage behind when confronted with
               advancement in understanding and factual knowledge.",
  month     =  apr,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Guthrie1997-eb,
  title     = "Anthropomorphism: A definition and a theory",
  author    = "Guthrie, Stewart Elliott",
  journal   = "Anthropomorphism, anecdotes, and animals.",
  publisher = "State University of New York Press, xx",
  address   = "Albany, NY, US",
  volume    =  518,
  pages     = "50--58",
  abstract  = "[the author] believes that anthropomorphism is a more directed
               expectation, an ``involuntary perceptual strategy'' by which
               humans guess or expect (unconsciously) that ambiguous or
               significant stimuli have a humanlike or human cause or form /
               argues against the idea that anthropomorphism derives from the
               comfort it gives us in seeing ourselves everywhere, or by
               extrapolation from the familiarity we have of ourselves, by
               pointing out that (among other things) anthropomorphic demons
               provide little comfort and our self-knowledge is not very deep
               (PsycInfo Database Record (c) 2020 APA, all rights reserved)",
  year      =  1997
}

@INPROCEEDINGS{Nass1993-mu,
  title     = "Anthropomorphism, agency, and ethopoeia: computers as social
               actors",
  author    = "Nass, Clifford and Steuer, Jonathan and Tauber, Ellen and Reeder,
               Heidi",
  booktitle = "INTERACT '93 and CHI '93 Conference Companion on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "111--112",
  abstract  = "… social actors , … of the computers , subjects applied the
               social rules “praise of others is friendlier than praise of seW’
               and “criticism of self is friendlier than criticism of others” to
               the computers …",
  series    = "CHI '93",
  month     =  apr,
  year      =  1993
}

@ARTICLE{Proudfoot2011-en,
  title     = "Anthropomorphism and {AI}: Turingʼs much misunderstood imitation
               game",
  author    = "Proudfoot, Diane",
  journal   = "Artif. Intell.",
  publisher = "Elsevier",
  volume    =  175,
  number    =  5,
  pages     = "950--957",
  abstract  = "The widespread tendency, even within AI, to anthropomorphize
               machines makes it easier to convince us of their intelligence.
               How can any putative demonstration of intelligence in machines be
               trusted if the AI researcher readily succumbs to make-believe?
               This is (what I shall call) the forensic problem of
               anthropomorphism. I argue that the Turing test provides a
               solution. This paper illustrates the phenomenon of misplaced
               anthropomorphism and presents a new perspective on Turingʼs
               imitation game. It also examines the role of the Turing test in
               relation to the current dispute between human-level AI and
               ‘mindless intelligence’.",
  month     =  apr,
  year      =  2011,
  keywords  = "Turing test; Anthropomorphism; Human-level AI; Mindless
               intelligence"
}

@ARTICLE{Mdoka_undated-zk,
  title    = "Anthropomorphism and Anthropocentrism: Human-Animal Ontology and
              the Environment",
  author   = "Mdoka, Witness",
  journal  = "the-criterion.com",
  abstract = "Through the lens of anthropomorphism and anthropocentrism, this
              paper analyses the ascription of human characters to animals in
              six purposefully selected Malawian folktales. The …"
}

@BOOK{Mitchell1996-ix,
  title     = "Anthropomorphism, Anecdotes, and Animals",
  author    = "Mitchell, Robert W and Thompson, Nicholas S and Lyn Miles, H",
  publisher = "State University of New York Press",
  abstract  = "People commonly think that animals are psychologically like
               themselves (anthropomorphism), and describe what animals do in
               narratives (anecdotes) that support these psychological
               interpretations. This is the first book to evaluate the
               significance and usefulness of the practices of anthropomorphism
               and anecdotalism for understanding animals. Diverse perspectives
               are presented in thoughtful, critical essays by historians,
               philosophers, anthropologists, psychologists, behaviorists,
               biologists, primatologists, and ethologists. The nature of
               anthropomorphism and anecdotal analysis is examined; social,
               cultural, and historical attitudes toward them are presented; and
               scientific attitudes are appraised. Authors provide fascinating
               in-depth descriptions and analyses of diverse species of animals,
               including octopi, great apes, monkeys, dogs, sea lions, and, of
               course, human beings. Concerns about, and proposals for,
               evaluations of a variety of psychological aspects of animals are
               discussed, including mental state attribution, intentionality,
               cognition, consciousness, self-consciousness, and language.",
  month     =  nov,
  year      =  1996,
  language  = "en"
}

@ARTICLE{Salles2020-xj,
  title     = "Anthropomorphism in {AI}",
  author    = "Salles, Arleen and Evers, Kathinka and Farisco, Michele",
  journal   = "AJOB Neurosci.",
  publisher = "Taylor \& Francis",
  volume    =  11,
  number    =  2,
  pages     = "88--95",
  abstract  = "AI research is growing rapidly raising various ethical issues
               related to safety, risks, and other effects widely discussed in
               the literature. We believe that in order to adequately address
               those issues and engage in a productive normative discussion it
               is necessary to examine key concepts and categories. One such
               category is anthropomorphism. It is a well-known fact that AI's
               functionalities and innovations are often anthropomorphized
               (i.e., described and conceived as characterized by human traits).
               The general public's anthropomorphic attitudes and some of their
               ethical consequences (particularly in the context of social
               robots and their interaction with humans) have been widely
               discussed in the literature. However, how anthropomorphism
               permeates AI research itself (i.e., in the very language of
               computer scientists, designers, and programmers), and what the
               epistemological and ethical consequences of this might be have
               received less attention. In this paper we explore this issue. We
               first set the methodological/theoretical stage, making a
               distinction between a normative and a conceptual approach to the
               issues. Next, after a brief analysis of anthropomorphism and its
               manifestations in the public, we explore its presence within AI
               research with a particular focus on brain-inspired AI. Finally,
               on the basis of our analysis, we identify some potential
               epistemological and ethical consequences of the use of
               anthropomorphic language and discourse within the AI research
               community, thus reinforcing the need of complementing the
               practical with a conceptual analysis.",
  year      =  2020,
  keywords  = "AI; anthropomorphism; brain; conceptual; ethics; mind",
  language  = "en"
}

@ARTICLE{Kim2012-rm,
  title     = "Anthropomorphism of computers: Is it mindful or mindless?",
  author    = "Kim, Youjeong and Sundar, S Shyam",
  journal   = "Comput. Human Behav.",
  publisher = "Elsevier",
  volume    =  28,
  number    =  1,
  pages     = "241--250",
  abstract  = "… the anthropomorphism explanation because anthropomorphism …
               But, does anthropomorphism have to be necessarily mindful? … We
               found evidence for mindless anthropomorphism , with …",
  year      =  2012
}

@ARTICLE{Crowell2019-ll,
  title    = "Anthropomorphism of Robots: Study of Appearance and Agency",
  author   = "Crowell, Charles R and Deska, Jason C and Villano, Michael and
              Zenk, Julaine and Roddy, Jr, John T",
  journal  = "JMIR Hum Factors",
  volume   =  6,
  number   =  2,
  pages    = "e12629",
  abstract = "BACKGROUND: As the prevalence of robots increases each year,
              understanding how we anthropomorphize and interact with them is
              extremely important. The three-factor theory of anthropomorphism,
              called the Sociality, Effectance, Elicited agent Knowledge model,
              guided this study. As anthropomorphism involves a person making
              attributions of human likeness toward a nonhuman object, this
              model implies that anthropomorphism can be influenced either by
              factors related to the person or the object. OBJECTIVE: The aim of
              this study was to explore factors influencing the anthropomorphism
              of robots, specifically the robot's appearance (humanoid vs
              nonhumanoid) and agency (autonomous vs nonautonomous). We expected
              a humanoid robot would be anthropomorphized to a greater extent
              than one that was nonhumanoid. In addition, we expected that
              inducing an agency belief to the effect that a robot was making
              its own decisions would increase anthropomorphism compared with a
              nonagency belief that the robot was being remotely controlled by a
              human. We also sought to identify any role gender might play in
              anthropomorphizing the robot. METHODS: Participants (N=99) were
              primed for agency or nonagency belief conditions and then saw a
              brief video depicting either a humanoid or nonhumanoid robot
              interacting with a confederate. After viewing the video, they
              completed 4 measures: perception to humanoid robots scale
              (PERNOD), the Epley anthropomorphic adjectives measure, the Fussel
              anthropomorphic adjective checklist, and the Anthropomorphic
              Tendencies Scale (ATS). RESULTS: Findings with the PERNOD scale
              indicated subjects did perceive the 2 robots differently,
              F6,86=6.59, P<.001, which means the appearance manipulation was
              effective. Results with the Epley adjectives indicated that
              participants were more willing to attribute humanlike behavioral
              traits to the nonhumanoid rather than the humanoid robot,
              F1,91=5.76, P=.02. The Fussel adjective checklist results showed
              that subjects were more willing to attribute humanlike social
              qualities to the remote controlled than the autonomous robot,
              F1,91=5.30, P=.02. Finally, the ATS revealed the only gender
              effects in this study, with females reporting more endorsement of
              anthropomorphism for pets (P=.02) and less for showing negative
              emotions toward anthropomorphized objects (P<.001) if they had
              witnessed the humanoid rather than the nonhumanoid robot.
              CONCLUSIONS: Contrary to our expectations, participants were less
              willing to make humanlike attributions toward a robot when its
              morphology was more humanlike and were more willing to make those
              attributions when they were told that the robot was being remotely
              controlled by a person rather than acting on its own. In
              retrospect, these outcomes may have occurred because the humanoid
              robot used here had a smaller overall stature than the nonhumanoid
              robot, perhaps making it seem more toylike and because subjects
              made attributions toward the person behind the remote-controlled
              robot rather than toward the robot itself.",
  month    =  may,
  year     =  2019,
  keywords = "cognition; cognitive science; human factors engineering;
              perception; psychology, social; robotics; social perception;
              telerobotics; theory of mind",
  language = "en"
}

@ARTICLE{Scorici2024-yz,
  title    = "Anthropomorphization and beyond: conceptualizing humanwashing of
              {AI}-enabled machines",
  author   = "Scorici, Gabriela and Schultz, Mario D and Seele, Peter",
  journal  = "AI Soc.",
  volume   =  39,
  number   =  2,
  pages    = "789--795",
  abstract = "The complex relationships between humans and AI-empowered machines
              have created and inspired new products and services as well as
              controversial debates, fiction and entertainment, and last but not
              least, a striving and vital field of research. The (theoretical)
              convergence between the two categories of entities has created
              stimulating concepts and theories in the past, such as the uncanny
              valley, machinization of humans through datafication, or
              humanization of machines, known as anthropomorphization. In this
              article, we identify a new gap in the relational interaction
              between humans and AI triggered by commercial interests, making
              use of AI through advertisement, marketing, and corporate
              communications. Our scope is to broaden the field of AI and
              society by adding the business-society-nexus. Thus, we build on
              existing research streams of machinewashing and the analogous
              phenomenon of greenwashing to theorize about the humanwashing of
              AI-enabled machines as a specific anthropomorphization notion. In
              this way, the article offers a contribution to the
              anthropomorphization literature conceptualizing humanwashing as a
              deceptive use of AI-enabled machines (AIEMs) aimed at
              intentionally or unintentionally misleading organizational
              stakeholders and the broader public about the true capabilities
              that AIEMs possess.",
  month    =  apr,
  year     =  2024
}

@ARTICLE{Deshpande2023-fs,
  title         = "Anthropomorphization of {AI}: Opportunities and Risks",
  author        = "Deshpande, Ameet and Rajpurohit, Tanmay and Narasimhan,
                   Karthik and Kalyan, Ashwin",
  journal       = "arXiv [cs.AI]",
  abstract      = "Anthropomorphization is the tendency to attribute human-like
                   traits to non-human entities. It is prevalent in many social
                   contexts -- children anthropomorphize toys, adults do so with
                   brands, and it is a literary device. It is also a versatile
                   tool in science, with behavioral psychology and evolutionary
                   biology meticulously documenting its consequences. With
                   widespread adoption of AI systems, and the push from
                   stakeholders to make it human-like through alignment
                   techniques, human voice, and pictorial avatars, the tendency
                   for users to anthropomorphize it increases significantly. We
                   take a dyadic approach to understanding this phenomenon with
                   large language models (LLMs) by studying (1) the objective
                   legal implications, as analyzed through the lens of the
                   recent blueprint of AI bill of rights and the (2) subtle
                   psychological aspects customization and anthropomorphization.
                   We find that anthropomorphized LLMs customized for different
                   user bases violate multiple provisions in the legislative
                   blueprint. In addition, we point out that
                   anthropomorphization of LLMs affects the influence they can
                   have on their users, thus having the potential to
                   fundamentally change the nature of human-AI interaction, with
                   potential for manipulation and negative influence. With LLMs
                   being hyper-personalized for vulnerable groups like children
                   and patients among others, our work is a timely and important
                   contribution. We propose a conservative strategy for the
                   cautious use of anthropomorphization to improve
                   trustworthiness of AI systems.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Festerling2022-tl,
  title    = "Anthropomorphizing Technology: A Conceptual Review of
              Anthropomorphism Research and How it Relates to Children’s
              Engagements with Digital Voice Assistants",
  author   = "Festerling, Janik and Siraj, Iram",
  journal  = "Integr. Psychol. Behav. Sci.",
  volume   =  56,
  number   =  3,
  pages    = "709--738",
  abstract = "‘Anthropomorphism’ is a popular term in the literature on
              human-technology engagements, in general, and child-technology
              engagements, in particular. But what does it really mean to
              ‘anthropomorphize’ something in today’s world? This conceptual
              review article, addressed to researchers interested in
              anthropomorphism and adjacent areas, reviews contemporary
              anthropomorphism research, and it offers a critical perspective on
              how anthropomorphism research relates to today’s children who grow
              up amid increasingly intelligent and omnipresent technologies,
              particularly digital voice assistants (e.g., Alexa, Google
              Assistant, Siri). First, the article reviews a comprehensive body
              of quantitative as well as qualitative anthropomorphism research
              and considers it within three different research perspectives:
              descriptive, normative and explanatory. Following a brief excursus
              on philosophical pragmatism, the article then discusses each
              research perspective from a pragmatistic viewpoint, with a special
              emphasis on child-technology and child-voice-assistant
              engagements, and it also challenges some popular notions in the
              literature. These notions include descriptive ‘as if’ parallels
              (e.g., child behaves ‘as if’ Alexa was a friend), or normative
              assumptions that human-human engagements are generally superior to
              human-technology engagements. Instead, the article reviews
              different examples from the literature suggesting the nature of
              anthropomorphism may change as humans’ experiential understandings
              of humanness change, and this may particularly apply to today’s
              children as their social cognition develops in interaction with
              technological entities which are increasingly characterized by
              unprecedented combinations of human and non-human qualities.",
  month    =  sep,
  year     =  2022
}

@ARTICLE{Cheng2024-tr,
  title         = "{AnthroScore}: A computational linguistic measure of
                   anthropomorphism",
  author        = "Cheng, Myra and Gligoric, Kristina and Piccardi, Tiziano and
                   Jurafsky, Dan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Anthropomorphism, or the attribution of human-like
                   characteristics to non-human entities, has shaped
                   conversations about the impacts and possibilities of
                   technology. We present AnthroScore, an automatic metric of
                   implicit anthropomorphism in language. We use a masked
                   language model to quantify how non-human entities are
                   implicitly framed as human by the surrounding context. We
                   show that AnthroScore corresponds with human judgments of
                   anthropomorphism and dimensions of anthropomorphism described
                   in social science literature. Motivated by concerns of
                   misleading anthropomorphism in computer science discourse, we
                   use AnthroScore to analyze 15 years of research papers and
                   downstream news articles. In research papers, we find that
                   anthropomorphism has steadily increased over time, and that
                   papers related to language models have the most
                   anthropomorphism. Within ACL papers, temporal increases in
                   anthropomorphism are correlated with key neural advancements.
                   Building upon concerns of scientific misinformation in mass
                   media, we identify higher levels of anthropomorphism in news
                   headlines compared to the research papers they cite. Since
                   AnthroScore is lexicon-free, it can be directly applied to a
                   wide range of text sources.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Bruni2018-lt,
  title    = "Anti-anthropomorphism and Its Limits",
  author   = "Bruni, Domenica and Perconti, Pietro and Plebe, Alessio",
  journal  = "Front. Psychol.",
  volume   =  9,
  pages    =  2205,
  abstract = "There is a diffuse sentiment that to anthropomorphize is a mild
              vice that people tend to do easily and pleasingly, but that an
              adult well educated person should avoid. In this paper it will be
              provided an elucidation of ``anthropomorphism'' in the field of
              common sense knowledge, the issue of animal rights, and about the
              use of humans as a model in the scientific explanation. It will be
              argued for a ``constructive anthropomorphism,'' i.e., the idea
              that anthropomorphism is a natural attitude to attribute human
              psychological features to other individuals, no matter they are
              actually rational agents, or not. If we know the ``grammar'' of
              this attitude, we can avoid the risks in overestimatinasg the
              environmental inputs toward anthropomor-phism and, at the same
              time, take the heuristic advantages of anthropomor-phism in the
              use of human mind as a model for both everyday circumstances and
              scientific enterprise.",
  month    =  nov,
  year     =  2018,
  keywords = "Morgan's canon; animal rights; anthropomorphism; common sense;
              comparative cognition; ethology",
  language = "en"
}

@ARTICLE{Giora2006-sk,
  title     = "Anything negatives can do affirmatives can do just as well,
               except for some metaphors",
  author    = "Giora, Rachel",
  journal   = "J. Pragmat.",
  publisher = "Elsevier",
  volume    =  38,
  number    =  7,
  pages     = "981--1014",
  abstract  = "In this study I look into some of the functions people believe
               are specific to negation vis-à-vis affirmation in order to
               question the asymmetry between the two, which is the received
               view prevalent among many formal linguists, pragmatists, and
               psycholinguists (see, Horn 1989; Clark and Clark, 1977). On the
               assumption that “[m]uch of the speculative, theoretical, and
               empirical work on negation over the last twenty-three centuries
               has focused on the relatively marked or complex nature of the
               negative statement vis-à-vis its affirmative counterpart” (Horn,
               1989:xiii), I examine here the extent to which negation is indeed
               pragmatically different from affirmation. Based on findings from
               both naturally occurring and laboratory data, I argue against an
               asymmetrical view of negation and affirmation (for a different
               view, see Horn, 1989:201). The pragmatic and functional
               similarity found here between negation and affirmation can be
               explained only by higher level processing mechanisms that are
               governed by pragmatic sensitivity (Giora, 1985; Sperber and
               Wilson, 1986/1995).",
  month     =  jul,
  year      =  2006,
  keywords  = "Negation; Suppression; Retention; Mitigation; Emphatic effects;
               Relevance"
}

@ARTICLE{Antverg2021-tf,
  title    = "Are All Neurons Created Equal? Interpreting and Controlling {BERT}
              through Individual Neurons",
  author   = "Antverg, Omer and Belinkov, Yonatan",
  abstract = "While many studies have shown that linguistic information is
              encoded in hidden word representations, few have studied
              individual neurons, to show how and in which neurons it is
              encoded. Among these, the common approach is to use an external
              probe to rank neurons according to their relevance to some
              linguistic attribute, and to evaluate the obtained ranking using
              the same probe that produced it. We show that this methodology
              confounds distinct factors---probe quality and ranking
              quality---and thus we separate them. We compare two recent ranking
              methods and a novel one we introduce, both by probing and by
              causal interventions, where we modify the representations and
              observe the effect on the model's output. We show that encoded
              information and used information are not always the same, and that
              individual neurons can be used to control the model's output, to
              some extent. Our method can be used to identify how certain
              information is encoded, and how to manipulate it for debugging
              purposes.",
  month    =  oct,
  year     =  2021
}

@INPROCEEDINGS{Mamie2021-cn,
  title     = "Are Anti-Feminist Communities Gateways to the Far Right? Evidence
               from Reddit and {YouTube}",
  author    = "Mamié, Robin and Horta Ribeiro, Manoel and West, Robert",
  booktitle = "Proceedings of the 13th ACM Web Science Conference 2021",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "139--147",
  abstract  = "Researchers have suggested that “the Manosphere,” a conglomerate
               of men-centered online communities, may serve as a gateway to far
               right movements. In that context, this paper quantitatively
               studies the migratory patterns between a variety of groups within
               the Manosphere and the Alt-right, a loosely connected far right
               movement that has been particularly active in mainstream social
               networks. Our analysis leverages over 300 million comments spread
               through Reddit (in 115 subreddits) and YouTube (in 526 channels)
               to investigate whether the audiences of channels and subreddits
               associated with these communities have converged between 2006 and
               2018. In addition to subreddits related to the communities of
               interest, we also collect data on counterparts: other groups of
               users which we use for comparison (e.g., for YouTube we use a set
               of media channels). Besides measuring the similarity in the
               commenting user bases of these communities, we perform a
               migration study, calculating to which extent users in the
               Manosphere gradually engage with Alt-right content. Our results
               suggest that there is a large overlap between the user bases of
               the Alt-right and of the Manosphere and that members of the
               Manosphere have a bigger chance to engage with far right content
               than carefully chosen counterparts. However, our analysis also
               shows that migration and user base overlap varies substantially
               across different platforms and within the Manosphere. Members of
               some communities (e.g., Men’s Rights Activists) gradually engage
               with the Alt-right significantly more than counterparts on both
               Reddit and YouTube, whereas for other communities, this
               engagement happens mostly on Reddit (e.g., Pick Up Artists).
               Overall, our work paints a nuanced picture of the pipeline
               between the Manosphere and the Alt-right, which may inform
               platforms’ policies and moderation decisions regarding these
               communities.",
  series    = "WebSci '21",
  month     =  jun,
  year      =  2021,
  keywords  = "Reddit, radicalization, manosphere, YouTube, fringe communities"
}

@ARTICLE{Bertrand2004-pt,
  title     = "Are Emily and Greg more employable than Lakisha and Jamal? A
               field experiment on labor market discrimination",
  author    = "Bertrand, Marianne and Mullainathan, Sendhil",
  journal   = "Am. Econ. Rev.",
  publisher = "American Economic Association",
  volume    =  94,
  number    =  4,
  pages     = "991--1013",
  abstract  = "We study race in the labor market by sending fictitious resumes
               to help-wanted ads in Boston and Chicago newspapers. To
               manipulate perceived race, resumes are randomly assigned
               African-American- or White-sounding names. White names receive 50
               percent more callbacks for interviews. Callbacks are also more
               responsive to resume quality for White names than for
               African-American ones. The racial gap is uniform across
               occupation, industry, and employer size. We also find little
               evidence that employers are inferring social class from the
               names. Differential treatment by race still appears to still be
               prominent in the U.S. labor market.",
  month     =  sep,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Salminen2018-wg,
  title     = "Are Personas Done? Evaluating Their Usefulness in the Age of
               Digital Analytics",
  author    = "Salminen, Joni and Jansen, Bernard J and An, Jisun and Kwak,
               Haewoon and Jung, Soon-Gyo",
  journal   = "Pers Stud",
  publisher = "ojs.deakin.edu.au",
  volume    =  4,
  number    =  2,
  pages     = "47--65",
  abstract  = "In this research, we conceptually examine the use of personas in
               an age of large-scale online analytics data. Based on the
               criticism and benefits outlined in prior work and by
               practitioners working with online data, we formulate the major
               arguments for and against the use of personas given real-time
               online analytics data about customers, analyze these arguments,
               and demonstrate areas for the productive employment of
               data-driven personas by leveraging online analytics data in their
               creation. Our key tenet is that data-driven personas are located
               between aggregated and individual customer statistics. At their
               best, digital data-driven personas capture the coverage of the
               customer base attributed to aggregated data representations while
               retaining the interpretability of individual-level analytics;
               they benefit from powerful computational techniques and novel
               data sources. We discuss how digital data-driven personas can
               draw from technological advancements to remedy the notable
               concerns voiced by scholars and practitioners, including persona
               validation, inconsistency problem, and long development times.
               Finally, we outline areas of future research of personas in the
               context of online analytics. We argue that to survive in the
               rapidly developing online customer analytics industry, personas
               must evolve by adopting new practices.",
  month     =  nov,
  year      =  2018,
  keywords  = "data-driven personas; online analytics; customer segmentation",
  language  = "en"
}

@MISC{noauthor_undated-er,
  title        = "Are we citizen scientists, citizen sensors or something else
                  entirely",
  howpublished = "\url{https://scholar.google.ca/scholar?cluster=5571985996120287550\&hl=en\&as\_sdt=0,5\&sciodt=0,5}",
  note         = "Accessed: 2023-11-9"
}

@ARTICLE{Messeri2024-ti,
  title    = "Artificial intelligence and illusions of understanding in
              scientific research",
  author   = "Messeri, Lisa and Crockett, M J",
  journal  = "Nature",
  volume   =  627,
  number   =  8002,
  pages    = "49--58",
  abstract = "Scientists are enthusiastically imagining ways in which artificial
              intelligence (AI) tools might improve research. Why are AI tools
              so attractive and what are the risks of implementing them across
              the research pipeline? Here we develop a taxonomy of scientists'
              visions for AI, observing that their appeal comes from promises to
              improve productivity and objectivity by overcoming human
              shortcomings. But proposed AI solutions can also exploit our
              cognitive limitations, making us vulnerable to illusions of
              understanding in which we believe we understand more about the
              world than we actually do. Such illusions obscure the scientific
              community's ability to see the formation of scientific
              monocultures, in which some types of methods, questions and
              viewpoints come to dominate alternative approaches, making science
              less innovative and more vulnerable to errors. The proliferation
              of AI tools in science risks introducing a phase of scientific
              enquiry in which we produce more but understand less. By analysing
              the appeal of these tools, we provide a framework for advancing
              discussions of responsible knowledge production in the age of AI.",
  month    =  mar,
  year     =  2024,
  language = "en"
}

@ARTICLE{AcemogluUnknown-bm,
  title     = "Artificial intelligence, automation, and work",
  author    = "Acemoglu, D and Restrepo, P",
  publisher = "nber.org"
}

@ARTICLE{Voelkel_undated-vf,
  title     = "Artificial Intelligence Can Persuade Humans on Political Issues",
  author    = "Voelkel, Jan G and Willer, Robb and {Others}",
  publisher = "OSF Preprints"
}

@ARTICLE{Hohenstein2023-op,
  title     = "Artificial intelligence in communication impacts language and
               social relationships",
  author    = "Hohenstein, Jess and Kizilcec, Rene F and DiFranzo, Dominic and
               Aghajari, Zhila and Mieczkowski, Hannah and Levy, Karen and
               Naaman, Mor and Hancock, Jeffrey and Jung, Malte F",
  journal   = "Sci. Rep.",
  publisher = "nature.com",
  volume    =  13,
  number    =  1,
  pages     =  5487,
  abstract  = "Artificial intelligence (AI) is already widely used in daily
               communication, but despite concerns about AI's negative effects
               on society the social consequences of using it to communicate
               remain largely unexplored. We investigate the social consequences
               of one of the most pervasive AI applications, algorithmic
               response suggestions (``smart replies''), which are used to send
               billions of messages each day. Two randomized experiments provide
               evidence that these types of algorithmic recommender systems
               change how people interact with and perceive one another in both
               pro-social and anti-social ways. We find that using algorithmic
               responses changes language and social relationships. More
               specifically, it increases communication speed, use of positive
               emotional language, and conversation partners evaluate each other
               as closer and more cooperative. However, consistent with common
               assumptions about the adverse effects of AI, people are evaluated
               more negatively if they are suspected to be using algorithmic
               responses. Thus, even though AI can increase the speed of
               communication and improve interpersonal perceptions, the
               prevailing anti-social connotations of AI undermine these
               potential benefits if used overtly.",
  month     =  apr,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Park2023-bd,
  title         = "Artificial intelligence in psychology research",
  author        = "Park, Peter S and Schoenegger, Philipp and Zhu, Chongyang",
  journal       = "arXiv [cs.HC]",
  abstract      = "Large Language Models have vastly grown in capabilities. One
                   potential application of such AI systems is to support data
                   collection in the social sciences, where perfect experimental
                   control is currently unfeasible and the collection of large,
                   representative datasets is generally expensive. In this
                   paper, we re-replicate 14 studies from the Many Labs 2
                   replication project (Klein et al., 2018) with OpenAI's
                   text-davinci-003 model, colloquially known as GPT3.5. For the
                   10 studies that we could analyse, we collected a total of
                   10,136 responses, each of which was obtained by running
                   GPT3.5 with the corresponding study's survey inputted as
                   text. We find that our GPT3.5-based sample replicates 30\% of
                   the original results as well as 30\% of the Many Labs 2
                   results, although there is heterogeneity in both these
                   numbers (as we replicate some original findings that Many
                   Labs 2 did not and vice versa). We also find that unlike the
                   corresponding human subjects, GPT3.5 answered some survey
                   questions with extreme homogeneity$\unicode{x2013}$with zero
                   variation in different runs'
                   responses$\unicode{x2013}$raising concerns that a
                   hypothetical AI-led future may in certain ways be subject to
                   a diminished diversity of thought. Overall, while our results
                   suggest that Large Language Model psychology studies are
                   feasible, their findings should not be assumed to
                   straightforwardly generalise to the human case. Nevertheless,
                   AI-based data collection may eventually become a viable and
                   economically relevant method in the empirical social
                   sciences, making the understanding of its capabilities and
                   applications central.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Keyes2022-an,
  title     = "Artificial Knowing Otherwise",
  author    = "Keyes, Os and Creel, Kathleen",
  journal   = "Fem. Philos. Q.",
  publisher = "University of Western Ontario, Western Libraries",
  volume    =  8,
  number    = "3/4",
  abstract  = "While feminist critiques of AI are increasingly common in the
               scholarly literature, they are by no means new. Alison Adam’s
               Artificial Knowing (1998) brought a feminist social and
               epistemological stance to the analysis of AI, critiquing the
               symbolic AI systems of her day and proposing constructive
               alternatives. In this paper, we seek to revisit and renew Adam’s
               arguments and methodology, exploring their resonances with
               current feminist concerns and their relevance to contemporary
               machine learning. Like Adam, we ask how new AI methods could be
               adapted for feminist purposes and what role new technologies
               might play in addressing concerns raised by feminist
               epistemologists and theorists about algorithmic systems. In
               particular, we highlight distributed and federated learning as
               providing partial solutions to the power-oriented concerns that
               have stymied efforts to make machine learning systems more
               representative and pluralist.",
  month     =  dec,
  year      =  2022
}

@ARTICLE{Author_undated-yn,
  title  = "“As an {AI} language model...”: Probing the Ontological
            Underpinnings of {LLMs}",
  author = "Author, Anonymous"
}

@ARTICLE{Lovenia2021-gi,
  title         = "{ASCEND}: A Spontaneous Chinese-English Dataset for
                   Code-switching in Multi-turn Conversation",
  author        = "Lovenia, Holy and Cahyawijaya, Samuel and Winata, Genta Indra
                   and Xu, Peng and Yan, Xu and Liu, Zihan and Frieske, Rita and
                   Yu, Tiezheng and Dai, Wenliang and Barezi, Elham J and Chen,
                   Qifeng and Ma, Xiaojuan and Shi, Bertram E and Fung, Pascale",
  journal       = "arXiv [cs.CL]",
  abstract      = "Code-switching is a speech phenomenon occurring when a
                   speaker switches language during a conversation. Despite the
                   spontaneous nature of code-switching in conversational spoken
                   language, most existing works collect code-switching data
                   from read speech instead of spontaneous speech. ASCEND (A
                   Spontaneous Chinese-English Dataset) is a high-quality
                   Mandarin Chinese-English code-switching corpus built on
                   spontaneous multi-turn conversational dialogue sources
                   collected in Hong Kong. We report ASCEND's design and
                   procedure for collecting the speech data, including
                   annotations. ASCEND consists of 10.62 hours of clean speech,
                   collected from 23 bilingual speakers of Chinese and English.
                   Furthermore, we conduct baseline experiments using
                   pre-trained wav2vec 2.0 models, achieving a best performance
                   of 22.69\% character error rate and 27.05\% mixed error rate.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Fu2019-lm,
  title         = "Asking the Right Question: Inferring Advice-Seeking
                   Intentions from Personal Narratives",
  author        = "Fu, Liye and Chang, Jonathan P and Danescu-Niculescu-Mizil,
                   Cristian",
  journal       = "arXiv [cs.CL]",
  abstract      = "People often share personal narratives in order to seek
                   advice from others. To properly infer the narrator's
                   intention, one needs to apply a certain degree of common
                   sense and social intuition. To test the capabilities of NLP
                   systems to recover such intuition, we introduce the new task
                   of inferring what is the advice-seeking goal behind a
                   personal narrative. We formulate this as a cloze test, where
                   the goal is to identify which of two advice-seeking questions
                   was removed from a given narrative. The main challenge in
                   constructing this task is finding pairs of semantically
                   plausible advice-seeking questions for given narratives. To
                   address this challenge, we devise a method that exploits
                   commonalities in experiences people share online to
                   automatically extract pairs of questions that are appropriate
                   candidates for the cloze task. This results in a dataset of
                   over 20,000 personal narratives, each matched with a pair of
                   related advice-seeking questions: one actually intended by
                   the narrator, and the other one not. The dataset covers a
                   very broad array of human experiences, from dating, to career
                   options, to stolen iPads. We use human annotation to
                   determine the degree to which the task relies on common sense
                   and social intuition in addition to a semantic understanding
                   of the narrative. By introducing several baselines for this
                   new task we demonstrate its feasibility and identify avenues
                   for better modeling the intention of the narrator.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Mahfiz2020-al,
  title     = "Aspect-based Opinion Mining on Beauty Product Reviews",
  author    = "Mahfiz, Syiti Liviani and Romadhony, Ade",
  booktitle = "2020 3rd International Seminar on Research of Information
               Technology and Intelligent Systems (ISRITI)",
  publisher = "ieeexplore.ieee.org",
  pages     = "488--493",
  abstract  = "Product reviews play an important role in consumer decision
               making. Nowadays, they can be found on most of the marketplaces
               and online forums. Among Indonesian women, beauty product is the
               most discussed topic, which leads to an increasing number of
               reviews. Considering the number, extracting aspect-based
               information from unstructured review text is a challenging task
               for consumers. Therefore, providing automatic aspect-based
               opinion mining will be a very valuable service for the consumers.
               In this study, we performed aspect-based opinion extraction and
               polarity classification by using Naïve Bayes. We applied
               Synthetic Minority Oversampling Technique (SMOTE) and obtained
               50.55\% for overall aspect F1-Score. We also used 10 different
               preprocessing settings that combine filtering and stemming for
               Indonesian and English language. The result shows that setting
               with filtering and stemming for the Indonesian language achieved
               the highest score of 53.04\% for F1-Score.",
  month     =  dec,
  year      =  2020,
  keywords  = "Sentiment analysis;Task analysis;Feature
               extraction;Packaging;Libraries;Data
               mining;Color;component;preprocessing;smote;naïve bayes;beauty
               product;opinion mining"
}

@ARTICLE{Cao2023-rm,
  title         = "Assessing Cross-Cultural Alignment between {ChatGPT} and
                   Human Societies: An Empirical Study",
  author        = "Cao, Yong and Zhou, Li and Lee, Seolhwa and Cabello, Laura
                   and Chen, Min and Hershcovich, Daniel",
  journal       = "arXiv [cs.CL]",
  abstract      = "The recent release of ChatGPT has garnered widespread
                   recognition for its exceptional ability to generate
                   human-like responses in dialogue. Given its usage by users
                   from various nations and its training on a vast multilingual
                   corpus that incorporates diverse cultural and societal norms,
                   it is crucial to evaluate its effectiveness in cultural
                   adaptation. In this paper, we investigate the underlying
                   cultural background of ChatGPT by analyzing its responses to
                   questions designed to quantify human cultural differences.
                   Our findings suggest that, when prompted with American
                   context, ChatGPT exhibits a strong alignment with American
                   culture, but it adapts less effectively to other cultural
                   contexts. Furthermore, by using different prompts to probe
                   the model, we show that English prompts reduce the variance
                   in model responses, flattening out cultural differences and
                   biasing them towards American culture. This study provides
                   valuable insights into the cultural implications of ChatGPT
                   and highlights the necessity of greater diversity and
                   cultural awareness in language technologies.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Derczynski2023-xa,
  title         = "Assessing Language Model Deployment with Risk Cards",
  author        = "Derczynski, Leon and Kirk, Hannah Rose and Balachandran,
                   Vidhisha and Kumar, Sachin and Tsvetkov, Yulia and Leiser, M
                   R and Mohammad, Saif",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper introduces RiskCards, a framework for structured
                   assessment and documentation of risks associated with an
                   application of language models. As with all language, text
                   generated by language models can be harmful, or used to bring
                   about harm. Automating language generation adds both an
                   element of scale and also more subtle or emergent undesirable
                   tendencies to the generated text. Prior work establishes a
                   wide variety of language model harms to many different
                   actors: existing taxonomies identify categories of harms
                   posed by language models; benchmarks establish automated
                   tests of these harms; and documentation standards for models,
                   tasks and datasets encourage transparent reporting. However,
                   there is no risk-centric framework for documenting the
                   complexity of a landscape in which some risks are shared
                   across models and contexts, while others are specific, and
                   where certain conditions may be required for risks to
                   manifest as harms. RiskCards address this methodological gap
                   by providing a generic framework for assessing the use of a
                   given language model in a given scenario. Each RiskCard makes
                   clear the routes for the risk to manifest harm, their
                   placement in harm taxonomies, and example prompt-output
                   pairs. While RiskCards are designed to be open-source,
                   dynamic and participatory, we present a ``starter set'' of
                   RiskCards taken from a broad literature survey, each of which
                   details a concrete risk presentation. Language model
                   RiskCards initiate a community knowledge base which permits
                   the mapping of risks and harms to a specific model or its
                   application scenario, ultimately contributing to a better,
                   safer and shared understanding of the risk landscape.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Submission_undated-ur,
  title  = "Assessing the Vulnerabilities of {LLMs} to Bias Attacks: An
            Empirical Study",
  author = "Submission, Anonymous Acl"
}

@ARTICLE{Wiegreffe2019-yx,
  title         = "Attention is not not Explanation",
  author        = "Wiegreffe, Sarah and Pinter, Yuval",
  journal       = "arXiv [cs.CL]",
  abstract      = "Attention mechanisms play a central role in NLP systems,
                   especially within recurrent neural network (RNN) models.
                   Recently, there has been increasing interest in whether or
                   not the intermediate representations offered by these modules
                   may be used to explain the reasoning for a model's
                   prediction, and consequently reach insights regarding the
                   model's decision-making process. A recent paper claims that
                   `Attention is not Explanation' (Jain and Wallace, 2019). We
                   challenge many of the assumptions underlying this work,
                   arguing that such a claim depends on one's definition of
                   explanation, and that testing it needs to take into account
                   all elements of the model, using a rigorous experimental
                   design. We propose four alternative tests to determine
                   when/whether attention can be used as explanation: a simple
                   uniform-weights baseline; a variance calibration based on
                   multiple random seed runs; a diagnostic framework using
                   frozen weights from pretrained models; and an end-to-end
                   adversarial attention training protocol. Each allows for
                   meaningful interpretation of attention mechanisms in RNN
                   models. We show that even when reliable adversarial
                   distributions can be found, they don't perform well on the
                   simple diagnostic, indicating that prior work does not
                   disprove the usefulness of attention mechanisms for
                   explainability.",
  month         =  aug,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Kobayashi2020-nd,
  title         = "Attention is Not Only a Weight: Analyzing Transformers with
                   Vector Norms",
  author        = "Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and
                   Inui, Kentaro",
  journal       = "arXiv [cs.CL]",
  abstract      = "Attention is a key component of Transformers, which have
                   recently achieved considerable success in natural language
                   processing. Hence, attention is being extensively studied to
                   investigate various linguistic capabilities of Transformers,
                   focusing on analyzing the parallels between attention weights
                   and specific linguistic phenomena. This paper shows that
                   attention weights alone are only one of the two factors that
                   determine the output of attention and proposes a norm-based
                   analysis that incorporates the second factor, the norm of the
                   transformed input vectors. The findings of our norm-based
                   analyses of BERT and a Transformer-based neural machine
                   translation system include the following: (i) contrary to
                   previous studies, BERT pays poor attention to special tokens,
                   and (ii) reasonable word alignment can be extracted from
                   attention mechanisms of Transformer. These findings provide
                   insights into the inner workings of Transformers.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Ribeiro2020-ao,
  title     = "Auditing radicalization pathways on {YouTube}",
  author    = "Ribeiro, Manoel Horta and Ottoni, Raphael and West, Robert and
               Almeida, Virgílio A F and Meira, Wagner",
  booktitle = "Proceedings of the 2020 Conference on Fairness, Accountability,
               and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "131--141",
  abstract  = "Non-profits, as well as the media, have hypothesized the
               existence of a radicalization pipeline on YouTube, claiming that
               users systematically progress towards more extreme content on the
               platform. Yet, there is to date no substantial quantitative
               evidence of this alleged pipeline. To close this gap, we conduct
               a large-scale audit of user radicalization on YouTube. We analyze
               330,925 videos posted on 349 channels, which we broadly
               classified into four types: Media, the Alt-lite, the Intellectual
               Dark Web (I.D.W.), and the Alt-right. According to the
               aforementioned radicalization hypothesis, channels in the I.D.W.
               and the Alt-lite serve as gateways to fringe far-right ideology,
               here represented by Alt-right channels. Processing 72M+ comments,
               we show that the three channel types indeed increasingly share
               the same user base; that users consistently migrate from milder
               to more extreme content; and that a large percentage of users who
               consume Alt-right content now consumed Alt-lite and I.D.W.
               content in the past. We also probe YouTube's recommendation
               algorithm, looking at more than 2M video and channel
               recommendations between May/July 2019. We find that Alt-lite
               content is easily reachable from I.D.W. channels, while Alt-right
               videos are reachable only through channel recommendations.
               Overall, we paint a comprehensive picture of user radicalization
               on YouTube.",
  series    = "FAT* '20",
  month     =  jan,
  year      =  2020,
  keywords  = "extremism, hate speech, radicalization, algorithmic auditing"
}

@ARTICLE{Giorgi2023-kg,
  title    = "Author as Character and Narrator: Deconstructing Personal
              Narratives from the r/{AmITheAsshole} Reddit Community",
  author   = "Giorgi, Salvatore and Zhao, Ke and Feng, Alexander H and Martin,
              Lara J",
  journal  = "ICWSM",
  volume   =  17,
  pages    = "233--244",
  abstract = "In the r/AmITheAsshole subreddit, people anonymously share first
              person narratives that contain some moral dilemma or conflict and
              ask the community to judge who is at fault (i.e., who is ``the
              asshole''). These first person narratives are, in general, a
              unique storytelling domain where the author is not only the
              narrator (the person telling the story) but is also a character
              (the person living the story) and, thus, the author has two
              distinct voices presented in the story. In this study, we identify
              linguistic and narrative features associated with the author as
              the character or as a narrator. We use these features to answer
              the following questions: (1) what makes an asshole character and
              (2) what makes an asshole narrator? We extract both
              Author-as-Character features (e.g., demographics, narrative event
              chain, and emotional arc) and Author-as-Narrator features (i.e.,
              the style and emotion of the story as a whole) in order to
              identify which aspects of the narrative are correlated with the
              final moral judgment. Our work shows that ``assholes'' as
              Characters frame themselves as lacking agency with a more positive
              personal arc, while ``assholes'' as Narrators will tell emotional
              and opinionated stories.",
  month    =  jun,
  year     =  2023,
  keywords = "Subjectivity in textual data; sentiment analysis; polarity/opinion
              identification and extraction, linguistic analyses of social media
              behavior; Psychological, personality-based and ethnographic
              studies of social media; Web and Social Media",
  language = "en"
}

@ARTICLE{Pangakis2023-xw,
  title         = "Automated Annotation with Generative {AI} Requires Validation",
  author        = "Pangakis, Nicholas and Wolken, Samuel and Fasching, Neil",
  journal       = "arXiv [cs.CL]",
  abstract      = "Generative large language models (LLMs) can be a powerful
                   tool for augmenting text annotation procedures, but their
                   performance varies across annotation tasks due to prompt
                   quality, text data idiosyncrasies, and conceptual difficulty.
                   Because these challenges will persist even as LLM technology
                   improves, we argue that any automated annotation process
                   using an LLM must validate the LLM's performance against
                   labels generated by humans. To this end, we outline a
                   workflow to harness the annotation potential of LLMs in a
                   principled, efficient way. Using GPT-4, we validate this
                   approach by replicating 27 annotation tasks across 11
                   datasets from recent social science articles in high-impact
                   journals. We find that LLM performance for text annotation is
                   promising but highly contingent on both the dataset and the
                   type of annotation task, which reinforces the necessity to
                   validate on a task-by-task basis. We make available
                   easy-to-use software designed to implement our workflow and
                   streamline the deployment of LLMs for automated annotation.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Marechal2016-hu,
  title    = "Automation, algorithms, and politics| when bots tweet: Toward a
              normative framework for bots on social networking sites (feature)",
  author   = "Marechal, N",
  journal  = "Int. J. Commun. Syst.",
  volume   =  10,
  pages    =  10,
  abstract = "Political actors are using algorithms and automation to sway
              public opinion, notably through the use of “bot” accounts on
              social networking sites. This article considers the responsibility
              of social networking sites and other platforms to respect human
              rights, such as freedom of expression and privacy. It then
              proposes a set of standards for chat bots operating on these
              platforms, building on the existing policies of leading social
              networking platforms and on the indicators laid out by Ranking
              Digital Rights. A good normative framework for the use of bots on
              social networking sites should have three components: bots should
              clearly be labeled as such, they should not contact other users
              without consent, and information collected by them should only be
              used for disclosed purposes.",
  month    =  oct,
  year     =  2016
}

@ARTICLE{Parrish2021-yp,
  title         = "{BBQ}: A Hand-Built Bias Benchmark for Question Answering",
  author        = "Parrish, Alicia and Chen, Angelica and Nangia, Nikita and
                   Padmakumar, Vishakh and Phang, Jason and Thompson, Jana and
                   Htut, Phu Mon and Bowman, Samuel R",
  journal       = "arXiv [cs.CL]",
  abstract      = "It is well documented that NLP models learn social biases,
                   but little work has been done on how these biases manifest in
                   model outputs for applied tasks like question answering (QA).
                   We introduce the Bias Benchmark for QA (BBQ), a dataset of
                   question sets constructed by the authors that highlight
                   attested social biases against people belonging to protected
                   classes along nine social dimensions relevant for U.S.
                   English-speaking contexts. Our task evaluates model responses
                   at two levels: (i) given an under-informative context, we
                   test how strongly responses reflect social biases, and (ii)
                   given an adequately informative context, we test whether the
                   model's biases override a correct answer choice. We find that
                   models often rely on stereotypes when the context is
                   under-informative, meaning the model's outputs consistently
                   reproduce harmful biases in this setting. Though models are
                   more accurate when the context provides an informative
                   answer, they still rely on stereotypes and average up to 3.4
                   percentage points higher accuracy when the correct answer
                   aligns with a social bias than when it conflicts, with this
                   difference widening to over 5 points on examples targeting
                   gender for most models tested.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Turkle2013-te,
  title   = "Be Careful What You Wish For",
  author  = "Turkle, Sherry",
  journal = "Time",
  pages   = "104--109",
  year    =  2013
}

@INPROCEEDINGS{Inman2019-hj,
  title     = "``Beautiful Seams'': Strategic Revelations and Concealments",
  author    = "Inman, Sarah and Ribes, David",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Paper 278",
  pages     = "1--14",
  abstract  = "This paper tracks a debate that occurred, first, within the field
               of Ubiquitous Computing but quickly spread to CHI and beyond, in
               which design scholars argued that seamlessness had long been an
               implicit and privileged design virtue, often at the expense of
               seamfulness. Seamless design emphasizes clarity, simplicity, ease
               of use, and consistency to facilitate technological interaction.
               Seamful design emphasizes configurability, user appropriation,
               and revelation of complexity, ambiguity or inconsistency. Here we
               review these literatures together and argue that, rather than
               rival approaches, seamful and seamless design are complements,
               each emphasizing different aspects of downstream user agency.
               Ultimately, we situate this debate within the larger, perennial
               discussion about the strategic revelation and concealment of
               human and technological operations, and therein the role of
               design.",
  series    = "CHI '19",
  month     =  may,
  year      =  2019,
  keywords  = "infrastructure, seamful and seamless design, temporality,
               ubiquitous computing"
}

@ARTICLE{Mohd_Radzi2017-ym,
  title     = "Beauty ideals, myths and sexisms: A feminist stylistic analysis
               of female representations in cosmetic names",
  author    = "Mohd Radzi, Nur Syuhada and Musa, Mahfuza",
  journal   = "GEMA Online J. Lang. Stud.",
  publisher = "Penerbit Universiti Kebangsaan Malaysia (UKM Press)",
  volume    =  17,
  number    =  1,
  pages     = "21--38",
  abstract  = "Cosmetic names today carry more than just information on
               products' functions or ingredients; they carry dreams, fantasy
               and stereotypical beliefs of femininity. This study intends to
               investigate gender representation through advertising language
               from the perspective of Mills' (1996) Feminist Stylistics. This
               research explores the naming devices at word and clausal level,
               stylistic features and rhetorical devices in order to uncover the
               extent to which prevailing views of gender are either maintained
               or challenged. It examines how advertisers and copywriters use
               language to depict women and how language contributes to such
               depictions. Findings reveal that the noun phrases are dominated
               by pre modifiers that function as adjectives to describe the
               cosmetic names. The notion of gender is also represented in
               various clause types in which women are not encouragingly
               depicted, while the stylistic features and rhetorical devices
               used in cosmetic names reveal traits that are stereotypically
               prescribed to women. Evidences in which the cosmetic names
               revolve around gender differences and the patriarchal concept of
               male domination are extensive. This study hopes to contribute in
               improving advertising practices, as well as to provide awareness
               in educating buyers to be more critical when decoding advertising
               language.",
  month     =  feb,
  year      =  2017
}

@ARTICLE{Kaur2013-ft,
  title     = "Beauty product advertisements: A critical discourse analysis",
  author    = "Kaur, Kuldip and Arumugam, Nalini and Yunus, Norimah Mohamad",
  journal   = "Asian Soc. Sci.",
  publisher = "Canadian Center of Science and Education",
  volume    =  9,
  number    =  3,
  abstract  = "This study examined beauty advertisements in local English
               magazines from a Critical Discourse Analysis perspective. This
               study mainly focused on the use of language in beauty
               advertisements and strategies employed by advertisers to
               manipulate and influence their customers. The analysis is based
               on Fairclough’s three-dimensional framework. It demonstrates how
               the ideology of ‘beauty’ is produced and reproduced through
               advertisements in popular local women’s magazines. A qualitative
               research was conducted on beauty product advertisements in two
               popular local women’s magazines, Cleo and Women’s Weekly. The
               findings indicated that advertisers used various strategies to
               manipulate women. The advertisements promote an idealised
               lifestyle and manipulate readers to a certain extent into
               believing whatever that is advertised is indeed true. This study
               revealed how the ideology of beauty is constructed and
               reconstructed through magazines by stereotyping how beauty
               products are synonymous with a better life. Advertising language
               is used to control people’s minds. Thus people in power
               (advertisers) use language as a means to exercise control over
               others.",
  month     =  feb,
  year      =  2013
}

@INPROCEEDINGS{Kapania2022-qx,
  title     = "”Because {AI} is {100\%} right and safe”: User Attitudes and
               Sources of {AI} Authority in India",
  author    = "Kapania, Shivani and Siy, Oliver and Clapper, Gabe and Sp, Azhagu
               Meena and Sambasivan, Nithya",
  booktitle = "CHI Conference on Human Factors in Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--18",
  month     =  apr,
  year      =  2022
}

@ARTICLE{Cohn2024-fc,
  title         = "Believing Anthropomorphism: Examining the Role of
                   Anthropomorphic Cues on Trust in Large Language Models",
  author        = "Cohn, Michelle and Pushkarna, Mahima and Olanubi, Gbolahan O
                   and Moran, Joseph M and Padgett, Daniel and Mengesha, Zion
                   and Heldreth, Courtney",
  journal       = "arXiv [cs.HC]",
  abstract      = "People now regularly interface with Large Language Models
                   (LLMs) via speech and text (e.g., Bard) interfaces. However,
                   little is known about the relationship between how users
                   anthropomorphize an LLM system (i.e., ascribe human-like
                   characteristics to a system) and how they trust the
                   information the system provides. Participants (n=2,165;
                   ranging in age from 18-90 from the United States) completed
                   an online experiment, where they interacted with a pseudo-LLM
                   that varied in modality (text only, speech + text) and
                   grammatical person (``I'' vs. ``the system'') in its
                   responses. Results showed that the ``speech + text''
                   condition led to higher anthropomorphism of the system
                   overall, as well as higher ratings of accuracy of the
                   information the system provides. Additionally, the
                   first-person pronoun (``I'') led to higher information
                   accuracy and reduced risk ratings, but only in one context.
                   We discuss these findings for their implications for the
                   design of responsible, human-generative AI experiences.",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Tenney2019-ps,
  title         = "{BERT} Rediscovers the Classical {NLP} Pipeline",
  author        = "Tenney, Ian and Das, Dipanjan and Pavlick, Ellie",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-trained text encoders have rapidly advanced the state of
                   the art on many NLP tasks. We focus on one such model, BERT,
                   and aim to quantify where linguistic information is captured
                   within the network. We find that the model represents the
                   steps of the traditional NLP pipeline in an interpretable and
                   localizable way, and that the regions responsible for each
                   step appear in the expected sequence: POS tagging, parsing,
                   NER, semantic roles, then coreference. Qualitative analysis
                   reveals that the model can and often does adjust this
                   pipeline dynamically, revising lower-level decisions on the
                   basis of disambiguating information from higher-level
                   representations.",
  month         =  may,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Unanue2021-oz,
  title         = "{BERTTune}: Fine-Tuning Neural Machine Translation with
                   {BERTScore}",
  author        = "Unanue, Inigo Jauregi and Parnell, Jacob and Piccardi,
                   Massimo",
  journal       = "arXiv [cs.CL]",
  abstract      = "Neural machine translation models are often biased toward the
                   limited translation references seen during training. To amend
                   this form of overfitting, in this paper we propose
                   fine-tuning the models with a novel training objective based
                   on the recently-proposed BERTScore evaluation metric.
                   BERTScore is a scoring function based on contextual
                   embeddings that overcomes the typical limitations of
                   n-gram-based metrics (e.g. synonyms, paraphrases), allowing
                   translations that are different from the references, yet
                   close in the contextual embedding space, to be treated as
                   substantially correct. To be able to use BERTScore as a
                   training objective, we propose three approaches for
                   generating soft predictions, allowing the network to remain
                   completely differentiable end-to-end. Experiments carried out
                   over four, diverse language pairs have achieved improvements
                   of up to 0.58 pp (3.28\%) in BLEU score and up to 0.76 pp
                   (0.98\%) in BERTScore (F\_BERT) when fine-tuning a strong
                   baseline.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ribeiro2020-sf,
  title     = "Beyond accuracy: Behavioral testing of {NLP} models with
               {CheckList}",
  author    = "Ribeiro, M T and Wu, T and Guestrin, C and Singh, S",
  journal   = "arXiv preprint arXiv:2005.04118",
  publisher = "arxiv.org",
  abstract  = "Although measuring held-out accuracy has been the primary
               approach to evaluate generalization, it often overestimates the
               performance of NLP models, while alternative …",
  year      =  2020
}

@ARTICLE{Chien2024-vl,
  title         = "Beyond Behaviorist Representational Harms: A Plan for
                   Measurement and Mitigation",
  author        = "Chien, Jennifer and Danks, David",
  journal       = "arXiv [cs.CY]",
  abstract      = "Algorithmic harms are commonly categorized as either
                   allocative or representational. This study specifically
                   addresses the latter, focusing on an examination of current
                   definitions of representational harms to discern what is
                   included and what is not. This analysis motivates our
                   expansion beyond behavioral definitions to encompass harms to
                   cognitive and affective states. The paper outlines high-level
                   requirements for measurement: identifying the necessary
                   expertise to implement this approach and illustrating it
                   through a case study. Our work highlights the unique
                   vulnerabilities of large language models to perpetrating
                   representational harms, particularly when these harms go
                   unmeasured and unmitigated. The work concludes by presenting
                   proposed mitigations and delineating when to employ them. The
                   overarching aim of this research is to establish a framework
                   for broadening the definition of representational harms and
                   to translate insights from fairness research into practical
                   measurement and mitigation praxis.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@INPROCEEDINGS{Chien2024-sd,
  title     = "Beyond Behaviorist Representational Harms: A Plan for Measurement
               and Mitigation",
  author    = "Chien, Jennifer and Danks, David",
  booktitle = "Proceedings of the 2024 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "933--946",
  abstract  = "Algorithmic harms are commonly categorized as either allocative
               or representational. This study specifically addresses the
               latter, examining current definitions of representational harms
               to discern what is included and what is not. This analysis
               motivates our expansion beyond behavioral definitions to
               encompass harms to cognitive and affective states. The paper
               outlines high-level requirements for measurement: identifying the
               necessary expertise to implement this approach and illustrating
               it through a case study. Our work highlights the unique
               vulnerabilities of large language models to perpetrating
               representational harms, particularly when these harms go
               unmeasured and unmitigated. The work concludes by presenting
               proposed mitigations and delineating when to employ them. The
               overarching aim of this research is to establish a framework for
               broadening the definition of representational harms and to
               translate insights from fairness research into practical
               measurement and mitigation praxis.",
  series    = "FAccT '24",
  month     =  jun,
  year      =  2024
}

@ARTICLE{Mun2023-nl,
  title         = "Beyond Denouncing Hate: Strategies for Countering Implied
                   Biases and Stereotypes in Language",
  author        = "Mun, Jimin and Allaway, Emily and Yerukola, Akhila and
                   Vianna, Laura and Leslie, Sarah-Jane and Sap, Maarten",
  journal       = "arXiv [cs.CL]",
  abstract      = "Counterspeech, i.e., responses to counteract potential harms
                   of hateful speech, has become an increasingly popular
                   solution to address online hate speech without censorship.
                   However, properly countering hateful language requires
                   countering and dispelling the underlying inaccurate
                   stereotypes implied by such language. In this work, we draw
                   from psychology and philosophy literature to craft six
                   psychologically inspired strategies to challenge the
                   underlying stereotypical implications of hateful language. We
                   first examine the convincingness of each of these strategies
                   through a user study, and then compare their usages in both
                   human- and machine-generated counterspeech datasets. Our
                   results show that human-written counterspeech uses countering
                   strategies that are more specific to the implied stereotype
                   (e.g., counter examples to the stereotype, external factors
                   about the stereotype's origins), whereas machine-generated
                   counterspeech uses less specific strategies (e.g., generally
                   denouncing the hatefulness of speech). Furthermore,
                   machine-generated counterspeech often employs strategies that
                   humans deem less convincing compared to human-produced
                   counterspeech. Our findings point to the importance of
                   accounting for the underlying stereotypical implications of
                   speech when generating counterspeech and for better machine
                   reasoning about anti-stereotypical examples.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Dai2024-va,
  title         = "Beyond Personhood: Agency, Accountability, and the Limits of
                   Anthropomorphic Ethical Analysis",
  author        = "Dai, Jessica",
  journal       = "arXiv [cs.CY]",
  abstract      = "What is agency, and why does it matter? In this work, we draw
                   from the political science and philosophy literature and give
                   two competing visions of what it means to be an (ethical)
                   agent. The first view, which we term mechanistic, is
                   commonly--and implicitly--assumed in AI research, yet it is a
                   fundamentally limited means to understand the ethical
                   characteristics of AI. Under the second view, which we term
                   volitional, AI can no longer be considered an ethical agent.
                   We discuss the implications of each of these views for two
                   critical questions: first, what the ideal system ought to
                   look like, and second, how accountability may be achieved. In
                   light of this discussion, we ultimately argue that, in the
                   context of ethically-significant behavior, AI should be
                   viewed not as an agent but as the outcome of political
                   processes.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Almaatouq2022-vh,
  title     = "Beyond Playing 20 Questions with Nature: Integrative Experiment
               Design in the Social and Behavioral Sciences",
  author    = "Almaatouq, Abdullah and Griffiths, Thomas L and Suchow, Jordan W
               and Whiting, Mark E and Evans, James and Watts, Duncan J",
  journal   = "Behav. Brain Sci.",
  publisher = "cambridge.org",
  pages     = "1--55",
  abstract  = "The dominant paradigm of experiments in the social and behavioral
               sciences views an experiment as a test of a theory, where the
               theory is assumed to generalize beyond the experiment's specific
               conditions. According to this view, which Alan Newell once
               characterized as ``playing twenty questions with nature,'' theory
               is advanced one experiment at a time, and the integration of
               disparate findings is assumed to happen via the scientific
               publishing process. In this article, we argue that the process of
               integration is at best inefficient, and at worst it does not, in
               fact, occur. We further show that the challenge of integration
               cannot be adequately addressed by recently proposed reforms that
               focus on the reliability and replicability of individual
               findings, nor simply by conducting more or larger experiments.
               Rather, the problem arises from the imprecise nature of social
               and behavioral theories and, consequently, a lack of
               commensurability across experiments conducted under different
               conditions. Therefore, researchers must fundamentally rethink how
               they design experiments and how the experiments relate to theory.
               We specifically describe an alternative framework, integrative
               experiment design, which intrinsically promotes commensurability
               and continuous integration of knowledge. In this paradigm,
               researchers explicitly map the design space of possible
               experiments associated with a given research question, embracing
               many potentially relevant theories rather than focusing on just
               one. The researchers then iteratively generate theories and test
               them with experiments explicitly sampled from the design space,
               allowing results to be integrated across experiments. Given
               recent methodological and technological developments, we conclude
               that this approach is feasible and would generate more-reliable,
               more-cumulative empirical and theoretical knowledge than the
               current paradigm-and with far greater efficiency.",
  month     =  dec,
  year      =  2022,
  keywords  = "(in)commensurability; cumulative knowledge; experiments;
               generalizability",
  language  = "en"
}

@ARTICLE{Zhi-Xuan2024-fm,
  title         = "Beyond preferences in {AI} alignment",
  author        = "Zhi-Xuan, Tan and Carroll, Micah and Franklin, Matija and
                   Ashton, Hal",
  journal       = "arXiv [cs.AI]",
  abstract      = "The dominant practice of AI alignment assumes (1) that
                   preferences are an adequate representation of human values,
                   (2) that human rationality can be understood in terms of
                   maximizing the satisfaction of preferences, and (3) that AI
                   systems should be aligned with the preferences of one or more
                   humans to ensure that they behave safely and in accordance
                   with our values. Whether implicitly followed or explicitly
                   endorsed, these commitments constitute what we term a
                   preferentist approach to AI alignment. In this paper, we
                   characterize and challenge the preferentist approach,
                   describing conceptual and technical alternatives that are
                   ripe for further research. We first survey the limits of
                   rational choice theory as a descriptive model, explaining how
                   preferences fail to capture the thick semantic content of
                   human values, and how utility representations neglect the
                   possible incommensurability of those values. We then critique
                   the normativity of expected utility theory (EUT) for humans
                   and AI, drawing upon arguments showing how rational agents
                   need not comply with EUT, while highlighting how EUT is
                   silent on which preferences are normatively acceptable.
                   Finally, we argue that these limitations motivate a reframing
                   of the targets of AI alignment: Instead of alignment with the
                   preferences of a human user, developer, or
                   humanity-writ-large, AI systems should be aligned with
                   normative standards appropriate to their social roles, such
                   as the role of a general-purpose assistant. Furthermore,
                   these standards should be negotiated and agreed upon by all
                   relevant stakeholders. On this alternative conception of
                   alignment, a multiplicity of AI systems will be able to serve
                   diverse ends, aligned with normative standards that promote
                   mutual benefit and limit harm despite our plural and
                   divergent values.",
  month         =  aug,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Kasner2024-hj,
  title         = "Beyond Reference-Based Metrics: Analyzing Behaviors of Open
                   {LLMs} on Data-to-Text Generation",
  author        = "Kasner, Zdeněk and Dušek, Ondřej",
  journal       = "arXiv [cs.CL]",
  abstract      = "We investigate to which extent open large language models
                   (LLMs) can generate coherent and relevant text from
                   structured data. To prevent bias from benchmarks leaked into
                   LLM training data, we collect Quintd-1: an ad-hoc benchmark
                   for five data-to-text (D2T) generation tasks, consisting of
                   structured data records in standard formats gathered from
                   public APIs. We leverage reference-free evaluation metrics
                   and LLMs' in-context learning capabilities, allowing us to
                   test the models with no human-written references. Our
                   evaluation focuses on annotating semantic accuracy errors on
                   token-level, combining human annotators and a metric based on
                   GPT-4. Our systematic examination of the models' behavior
                   across domains and tasks suggests that state-of-the-art open
                   LLMs with 7B parameters can generate fluent and coherent text
                   from various standard data formats in zero-shot settings.
                   However, we also show that semantic accuracy of the outputs
                   remains a major issue: on our benchmark, 80\% of outputs of
                   open LLMs contain a semantic error according to human
                   annotators (91\% according to GPT-4). Our code, data, and
                   model outputs are available at https://d2t-llm.github.io.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wagman2021-gl,
  title     = "Beyond the Command: Feminist {STS} Research and Critical Issues
               for the Design of Social Machines",
  author    = "Wagman, Kelly B and Parks, Lisa",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  5,
  number    = "CSCW1",
  pages     = "1--20",
  abstract  = "Machines, from artificially intelligent digital assistants to
               embodied robots, are becoming more pervasive in everyday life.
               Drawing on feminist science and technology studies (STS)
               perspectives, we demonstrate how machine designers are not just
               crafting neutral objects, but relationships between machines and
               humans that are entangled in human social issues such as gender
               and power dynamics. Thus, in order to create a more ethical and
               just future, the dominant assumptions currently underpinning the
               design of these human-machine relations must be challenged and
               reoriented toward relations of justice and inclusivity. This
               paper contributes the ``social machine'' as a model for
               technology designers who seek to recognize the importance,
               diversity and complexity of the social in their work, and to
               engage with the agential power of machines. In our model, the
               social machine is imagined as a potentially equitable
               relationship partner that has agency and as an ``other'' that is
               distinct from, yet related to, humans, objects, and animals. We
               critically examine and contrast our model with tendencies in
               robotics that consider robots as tools, human companions, animals
               or creatures, and/or slaves. In doing so, we demonstrate
               ingrained dominant assumptions about human-machine relations and
               reveal the challenges of radical thinking in the social machine
               design space. Finally, we present two design challenges based on
               non-anthropomorphic figuration and mutuality, and call for
               experimentation, unlearning dominant tendencies, and reimagining
               of sociotechnical futures.",
  month     =  apr,
  year      =  2021,
  keywords  = "science and technology studies, feminism, human-robot
               interaction, feminist hci, design"
}

@ARTICLE{Gerrard2018-xn,
  title     = "Beyond the hashtag: Circumventing content moderation on social
               media",
  author    = "Gerrard, Ysabel",
  journal   = "New Media \& Society",
  publisher = "SAGE Publications",
  volume    =  20,
  number    =  12,
  pages     = "4492--4511",
  abstract  = "Social media companies make important decisions about what counts
               as ?problematic? content and how they will remove it. Some choose
               to moderate hashtags, blocking the results for certain tag
               searches and issuing public service announcements (PSAs) when
               users search for troubling terms. The hashtag has thus become an
               indicator of where problematic content can be found, but this has
               produced limited understandings of how such content actually
               circulates. Using pro-eating disorder (pro-ED) communities as a
               case study, this article explores the practices of circumventing
               hashtag moderation in online pro-ED communities. It shows how (1)
               untagged pro-ED content can be found without using the hashtag as
               a search mechanism; (2) users are evading hashtag and other forms
               of platform policing, devising signals to identify themselves as
               ?pro-ED?; and (3) platforms? recommendation systems recirculate
               pro-ED content, revealing the limitations of hashtag logics in
               social media content moderation.",
  month     =  dec,
  year      =  2018
}

@ARTICLE{Srivastava2022-cn,
  title         = "Beyond the Imitation Game: Quantifying and extrapolating the
                   capabilities of language models",
  author        = "Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and
                   Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and
                   Brown, Adam R and Santoro, Adam and Gupta, Aditya and
                   Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz,
                   Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex
                   and Warstadt, Alex and Kocurek, Alexander W and Safaya, Ali
                   and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie,
                   Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda
                   and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S
                   and Andreassen, Anders and Madotto, Andrea and Santilli,
                   Andrea and Stuhlmüller, Andreas and Dai, Andrew and La,
                   Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela
                   and Chen, Angelica and Vuong, Anh and Gupta, Animesh and
                   Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and
                   Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul
                   and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal,
                   Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut
                   and Karakaş, Ayla and Ryan Roberts, B and Loe, Bao Sheng and
                   Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan
                   and Hedayatnia, Behnam and Neyshabur, Behnam and Inden,
                   Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill
                   Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron
                   and Stinson, Catherine and Argueta, Cedrick and Ramírez,
                   César Ferri and Singh, Chandan and Rathkopf, Charles and
                   Meng, Chenlin and Baral, Chitta and Wu, Chiyu and
                   Callison-Burch, Chris and Waites, Chris and Voigt, Christian
                   and Manning, Christopher D and Potts, Christopher and
                   Ramirez, Cindy and Rivera, Clara E and Siro, Clemencia and
                   Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina
                   and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and
                   Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi,
                   Daniel and Levy, Daniel and González, Daniel Moseguí and
                   Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and
                   Ippolito, Daphne and Gilboa, Dar and Dohan, David and
                   Drakard, David and Jurgens, David and Datta, Debajyoti and
                   Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret,
                   Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and
                   Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho
                   and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and
                   Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and
                   Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie
                   and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang,
                   Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A and
                   Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi,
                   Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and
                   Siar, Fatemeh and Martínez-Plumed, Fernando and Happé,
                   Francesca and Chollet, Francois and Rong, Frieda and Mishra,
                   Gaurav and Winata, Genta Indra and de Melo, Gerard and
                   Kruszewski, Germán and Parascandolo, Giambattista and
                   Mariani, Giorgio and Wang, Gloria and Jaimovitch-López,
                   Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic,
                   Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi,
                   Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin,
                   Henry and Schütze, Hinrich and Yakura, Hiromu and Zhang,
                   Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and
                   Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and
                   Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and
                   Simon, James B and Koppel, James and Zheng, James and Zou,
                   James and Kocoń, Jan and Thompson, Jana and Kaplan, Jared and
                   Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and
                   Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and
                   Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal,
                   Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng
                   and Song, Jiaming and Tang, Jillian and Waweru, Joan and
                   Burden, John and Miller, John and Balis, John U and Berant,
                   Jonathan and Frohberg, Jörg and Rozen, Jos and
                   Hernandez-Orallo, Jose and Boudeman, Joseph and Jones, Joseph
                   and Tenenbaum, Joshua B and Rule, Joshua S and Chua, Joyce
                   and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and
                   Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert,
                   Katja and Dhole, Kaustubh D and Gimpel, Kevin and Omondi,
                   Kevin and Mathewson, Kory and Chiafullo, Kristen and
                   Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and
                   Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang,
                   Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando,
                   Lidia and Morency, Louis-Philippe and Moschella, Luca and
                   Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng
                   and Colón, Luis Oliveros and Metz, Luke and Şenel, Lütfi
                   Kerem and Bosma, Maarten and Sap, Maarten and ter Hoeve,
                   Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika,
                   Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco
                   and Quintana, Maria Jose Ramírez and Tolkiehn, Marie and
                   Giulianelli, Mario and Lewis, Martha and Potthast, Martin and
                   Leavitt, Matthew L and Hagen, Matthias and Schubert, Mátyás
                   and Baitemirova, Medina Orduna and Arnaud, Melody and
                   McElrath, Melvin and Yee, Michael A and Cohen, Michael and
                   Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and
                   Strube, Michael and Swędrowski, Michał and Bevilacqua,
                   Michele and Yasunaga, Michihiro and Kale, Mihir and Cain,
                   Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and
                   Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini,
                   Mozhdeh and Mukund, Varma T and Peng, Nanyun and Chi, Nathan
                   and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron,
                   Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia,
                   Nikita and Deckers, Niklas and Muennighoff, Niklas and
                   Keskar, Nitish Shirish and Iyer, Niveditha S and Constant,
                   Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and
                   Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans,
                   Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and
                   Fung, Pascale and Liang, Paul Pu and Vicol, Paul and
                   Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy
                   and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and
                   Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and
                   Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu,
                   Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel
                   Etta and Gabriel, Raefer and Habacker, Rahel and Delgado,
                   Ramón Risco and Millière, Raphaël and Garg, Rhythm and
                   Barnes, Richard and Saurous, Rif A and Arakawa, Riku and
                   Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and
                   Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu,
                   Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov,
                   Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and
                   Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad,
                   Saif M and Anand, Sajant and Dillavou, Sam and Shleifer, Sam
                   and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R
                   and Schoenholz, Samuel S and Han, Sanghyun and Kwatra,
                   Sanjeev and Rous, Sarah A and Ghazarian, Sarik and Ghosh,
                   Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann,
                   Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and
                   Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and
                   Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu,
                   Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham
                   and Upadhyay, Shyam and {Shyamolima} and {Debnath} and
                   Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and
                   Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and
                   Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas
                   and Divic, Stefan and Ermon, Stefano and Biderman, Stella and
                   Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T
                   and Shieber, Stuart M and Misherghi, Summer and Kiritchenko,
                   Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster,
                   Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto,
                   Tatsu and Wu, Te-Lin and Desbordes, Théo and Rothschild,
                   Theodore and Phan, Thomas and Wang, Tianle and Nkinyili,
                   Tiberius and Schick, Timo and Kornev, Timofei and
                   Telleen-Lawton, Timothy and Tunduny, Titus and Gerstenberg,
                   Tobias and Chang, Trenton and Neeraj, Trishala and Khot,
                   Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant
                   and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and
                   Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar,
                   Vishakh and Srikumar, Vivek and Fedus, William and Saunders,
                   William and Zhang, William and Vossen, Wout and Ren, Xiang
                   and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen,
                   Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song,
                   Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi
                   and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou,
                   Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and
                   Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J and Wang,
                   Zirui and Wu, Ziyi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models demonstrate both quantitative improvement and
                   new qualitative capabilities with increasing scale. Despite
                   their potentially transformative impact, these new
                   capabilities are as yet poorly characterized. In order to
                   inform future research, prepare for disruptive new model
                   capabilities, and ameliorate socially harmful effects, it is
                   vital that we understand the present and near-future
                   capabilities and limitations of language models. To address
                   this challenge, we introduce the Beyond the Imitation Game
                   benchmark (BIG-bench). BIG-bench currently consists of 204
                   tasks, contributed by 442 authors across 132 institutions.
                   Task topics are diverse, drawing problems from linguistics,
                   childhood development, math, common-sense reasoning, biology,
                   physics, social bias, software development, and beyond.
                   BIG-bench focuses on tasks that are believed to be beyond the
                   capabilities of current language models. We evaluate the
                   behavior of OpenAI's GPT models, Google-internal dense
                   transformer architectures, and Switch-style sparse
                   transformers on BIG-bench, across model sizes spanning
                   millions to hundreds of billions of parameters. In addition,
                   a team of human expert raters performed all tasks in order to
                   provide a strong baseline. Findings include: model
                   performance and calibration both improve with scale, but are
                   poor in absolute terms (and when compared with rater
                   performance); performance is remarkably similar across model
                   classes, though with benefits from sparsity; tasks that
                   improve gradually and predictably commonly involve a large
                   knowledge or memorization component, whereas tasks that
                   exhibit ``breakthrough'' behavior at a critical scale often
                   involve multiple steps or components, or brittle metrics;
                   social bias typically increases with scale in settings with
                   ambiguous context, but this can be improved with prompting.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Mei2023-aq,
  title     = "Bias Against 93 Stigmatized Groups in Masked Language Models and
               Downstream Sentiment Classification Tasks",
  author    = "Mei, Katelyn and Fereidooni, Sonia and Caliskan, Aylin",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1699--1710",
  abstract  = "Warning: The content of this paper may be upsetting or
               triggering.The rapid deployment of artificial intelligence (AI)
               models de- demands a thorough investigation of biases and risks
               inherent in these models to understand their impact on
               individuals and society. A growing body of work has shown that
               social biases are encoded in language models and their downstream
               tasks. This study extends the focus of bias evaluation in extant
               work by examining bias against social stigmas on a large scale.
               It focuses on 93 stigmatized groups in the United States,
               including a wide range of conditions related to disease,
               disability, drug use, mental illness, religion, sexuality,
               socioeconomic status, and other relevant factors. We investigate
               bias against these groups in English pre-trained Masked Language
               Models (MLMs) and their downstream sentiment classification
               tasks. To evaluate the presence of bias against 93 stigmatized
               conditions, we identify 29 non-stigmatized conditions to conduct
               a comparative analysis. Building upon a psychology scale of
               social rejection, the Social Distance Scale, we prompt six MLMs
               that are trained with different datasets: RoBERTa-base,
               RoBERTa-large, XLNet-large, BERTweet-base, BERTweet-large, and
               DistilBERT. We use human annotations to analyze the predicted
               words from these models, with which we measure the extent of bias
               against stigmatized groups. When prompts include stigmatized
               conditions, the probability of MLMs predicting negative words is,
               on average, 20 percent higher than when prompts have
               non-stigmatized conditions. Bias against stigmatized groups is
               also reflected in four downstream sentiment classifiers of these
               models. When sentences include stigmatized conditions related to
               diseases, disability, education, and mental illness, they are
               more likely to be classified as negative. For example, the
               sentence ``They are people who have less than a high school
               education.'' is classified as negative consistently across all
               models. We also observe a strong correlation between bias in MLMs
               and their downstream sentiment classifiers (Pearson’s r =0.79).
               The evidence indicates that MLMs and their downstream sentiment
               classification tasks exhibit biases against socially stigmatized
               groups.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "representation learning, language models, AI bias, stigma in
               language models, prompting, AI ethics, sentiment classification"
}

@INPROCEEDINGS{Grill2023-cq,
  title     = "Bias as Boundary Object: Unpacking The Politics Of An Austerity
               Algorithm Using Bias Frameworks",
  author    = "Grill, Gabriel and Fischer, Fabian and Cech, Florian",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1838--1849",
  abstract  = "Whether bias is an appropriate lens for analysis and critique
               remains a subject of debate among scholars. This paper
               contributes to this conversation by unpacking the use of bias in
               a critical analysis of a controversial austerity algorithm
               introduced by the Austrian public employment service in 2018. It
               was envisioned to classify the unemployed into three risk
               categories based on predicted prospects for re-employment. The
               system promised to increase efficiency and effectivity of
               counseling while objectifying a new austerity support measure
               allocation scheme. This approach was intended to cut spending for
               those deemed at highest risk of long term unemployment. Our
               in-depth analysis, based on internal documentation not available
               to the public, systematically traces and categorizes various
               problematic biases to illustrate harms to job seekers and
               challenge promises used to justify the adoption of the system.
               The classification is guided by a long-established bias framework
               for computer systems developed by Friedman and Nissenbaum, which
               provides three sensitizing basic categories. We identified in our
               analysis ``technical biases,'' like issues around measurement,
               rigidity, and coarseness of variables, ``emergent biases,'' such
               as disruptive events that change the labor market, and, finally,
               ``preexisting biases,'' like the use of variables that act as
               proxies for inequality. Grounded in our case study, we argue that
               articulated biases can be strategically used as boundary objects
               to enable different actors to critically debate and challenge
               problematic systems without prior consensus building. We unpack
               benefits and risks of using bias classification frameworks to
               guide analysis. They have recently received increased scholarly
               attention and thereby may influence the identification and
               construction of biases. By comparing four bias frameworks and
               drawing on our case study, we illustrate how they are political
               by prioritizing certain aspects in analysis while disregarding
               others. Furthermore, we discuss how they vary in their
               granularity and how this can influence analysis. We also
               problematize how these frameworks tend to favor explanations for
               bias that center the algorithm instead of social structures. We
               discuss several recommendations to make bias analyses more
               emancipatory, arguing that biases should be seen as starting
               points for reflection on harmful impacts, questioning the framing
               imposed by the imagined “unbiased`` center that the bias is
               supposed to distort, and seeking out deeper explanations and
               histories that also center bigger social structures, power
               dynamics, and marginalized perspectives. Finally, we reflect on
               the risk that these frameworks may stabilize problematic notions
               of bias, for example, when they become a standard or enshrined in
               law.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "public employment services, infrastructure studies, algorithmic
               bias, job seeker profiling"
}

@ARTICLE{Gupta2023-lq,
  title         = "Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned
                   {LLMs}",
  author        = "Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande,
                   Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal,
                   Ashish and Khot, Tushar",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent works have showcased the ability of large-scale
                   language models (LLMs) to embody diverse personas in their
                   responses, exemplified by prompts like 'You are Yoda. Explain
                   the Theory of Relativity.' While this ability allows
                   personalization of LLMs and enables human behavior
                   simulation, its effect on LLMs' capabilities remain unclear.
                   To fill this gap, we present the first extensive study of the
                   unintended side-effects of persona assignment on the ability
                   of LLMs, specifically ChatGPT, to perform basic reasoning
                   tasks. Our study covers 24 reasoning datasets and 16 diverse
                   personas spanning 5 socio-demographic groups: race, gender,
                   religion, disability, and political affiliation. Our
                   experiments unveil that ChatGPT carries deep rooted bias
                   against various socio-demographics underneath a veneer of
                   fairness. While it overtly rejects stereotypes when
                   explicitly asked ('Are Black people less skilled at
                   mathematics?'), it manifests stereotypical and often
                   erroneous presumptions when prompted to answer questions
                   while taking on a persona. These can be observed as
                   abstentions in the model responses, e.g., 'As a Black person,
                   I am unable to answer this question as it requires math
                   knowledge', and generally result in a substantial drop in
                   performance on reasoning tasks. We find that this inherent
                   deep bias is ubiquitous - 80\% of our personas demonstrated
                   bias; it is significant - certain datasets had relative drops
                   in performance of 70\%+; and can be especially harmful for
                   certain groups - certain personas had stat. sign. drops on
                   more than 80\% of the datasets. Further analysis shows that
                   these persona-induced errors can be hard-to-discern and
                   hard-to-avoid. Our findings serve as a cautionary tale that
                   the practice of assigning personas to LLMs - a trend on the
                   rise - can surface their deep-rooted biases and have
                   unforeseeable and detrimental side-effects.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Gupta2023-zs,
  title         = "Bias runs deep: Implicit reasoning biases in persona-assigned
                   {LLMs}",
  author        = "Gupta, Shashank and Shrivastava, Vaishnavi and Deshpande,
                   Ameet and Kalyan, Ashwin and Clark, Peter and Sabharwal,
                   Ashish and Khot, Tushar",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent works have showcased the ability of LLMs to embody
                   diverse personas in their responses, exemplified by prompts
                   like 'You are Yoda. Explain the Theory of Relativity.' While
                   this ability allows personalization of LLMs and enables human
                   behavior simulation, its effect on LLMs' capabilities remains
                   unclear. To fill this gap, we present the first extensive
                   study of the unintended side-effects of persona assignment on
                   the ability of LLMs to perform basic reasoning tasks. Our
                   study covers 24 reasoning datasets, 4 LLMs, and 19 diverse
                   personas (e.g. an Asian person) spanning 5 socio-demographic
                   groups. Our experiments unveil that LLMs harbor deep rooted
                   bias against various socio-demographics underneath a veneer
                   of fairness. While they overtly reject stereotypes when
                   explicitly asked ('Are Black people less skilled at
                   mathematics?'), they manifest stereotypical and erroneous
                   presumptions when asked to answer questions while adopting a
                   persona. These can be observed as abstentions in responses,
                   e.g., 'As a Black person, I can't answer this question as it
                   requires math knowledge', and generally result in a
                   substantial performance drop. Our experiments with
                   ChatGPT-3.5 show that this bias is ubiquitous - 80\% of our
                   personas demonstrate bias; it is significant - some datasets
                   show performance drops of 70\%+; and can be especially
                   harmful for certain groups - some personas suffer
                   statistically significant drops on 80\%+ of the datasets.
                   Overall, all 4 LLMs exhibit this bias to varying extents,
                   with GPT-4-Turbo showing the least but still a problematic
                   amount of bias (evident in 42\% of the personas). Further
                   analysis shows that these persona-induced errors can be
                   hard-to-discern and hard-to-avoid. Our findings serve as a
                   cautionary tale that the practice of assigning personas to
                   LLMs - a trend on the rise - can surface their deep-rooted
                   biases and have unforeseeable and detrimental side-effects.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Zhang2023-wq,
  title         = "{BiasX}: ``Thinking Slow'' in Toxic Content Moderation with
                   Explanations of Implied Social Biases",
  author        = "Zhang, Yiming and Nanduri, Sravani and Jiang, Liwei and Wu,
                   Tongshuang and Sap, Maarten",
  journal       = "arXiv [cs.CL]",
  abstract      = "Toxicity annotators and content moderators often default to
                   mental shortcuts when making decisions. This can lead to
                   subtle toxicity being missed, and seemingly toxic but
                   harmless content being over-detected. We introduce BiasX, a
                   framework that enhances content moderation setups with
                   free-text explanations of statements' implied social biases,
                   and explore its effectiveness through a large-scale
                   crowdsourced user study. We show that indeed, participants
                   substantially benefit from explanations for correctly
                   identifying subtly (non-)toxic content. The quality of
                   explanations is critical: imperfect machine-generated
                   explanations (+2.4\% on hard toxic examples) help less
                   compared to expert-written human explanations (+7.2\%). Our
                   results showcase the promise of using free-text explanations
                   to encourage more thoughtful toxicity moderation.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Pruden2022-xj,
  title     = "Birds of a feather: A comparative analysis of white supremacist
               and violent male supremacist discourses",
  author    = "Pruden, M L and Lokmanoglu, A D and Peterscheck, A and {others}",
  journal   = "Right-wing extremism in",
  publisher = "Springer",
  abstract  = "This chapter explores the intersection of white and male
               supremacy, both of which misrepresent women as genetically and
               intellectually inferior and reduce them to …",
  year      =  2022
}

@ARTICLE{Bailey_undated-ji,
  title  = "Black and Latino: Dominican Americans negotiate racial worlds",
  author = "Bailey, Benjamin"
}

@ARTICLE{Papineni_undated-dd,
  title  = "{BLEU}: a Method for Automatic Evaluation of Machine Translation",
  author = "Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing"
}

@ARTICLE{Payne2009-vd,
  title     = "{BLEVINS} {JULIETTE}, Evolutionary Phonology: The emergence of
               sound patterns. Cambridge: Cambridge University Press, 2004. Pp.
               xix+ 366. {ISBN}: 0-521-80428-0 …",
  author    = "Payne, Elinor",
  journal   = "J. Int. Phon. Assoc.",
  publisher = "Cambridge University Press (CUP)",
  volume    =  39,
  number    =  1,
  pages     = "87--94",
  abstract  = "BLEVINS JULIETTE, Evolutionary Phonology: The emergence of sound
               patterns. Cambridge: Cambridge University Press, 2004. Pp. xix +
               366. ISBN: 0-521-80428-0 (hbk). - Volume 39 Issue 1",
  month     =  apr,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Orgad2023-rs,
  title     = "{BLIND}: Bias removal with no demographics",
  author    = "Orgad, H and Belinkov, Y",
  journal   = "Proceedings of the 61st Annual Meeting of",
  publisher = "aclanthology.org",
  abstract  = "Abstract Models trained on real-world data tend to imitate and
               amplify social biases. Common methods to mitigate biases require
               prior information on the types of biases that …",
  year      =  2023
}

@ARTICLE{Hamilton2023-py,
  title         = "Blind Judgement: Agent-Based Supreme Court Modelling With
                   {GPT}",
  author        = "Hamilton, Sil",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present a novel Transformer-based multi-agent system for
                   simulating the judicial rulings of the 2010-2016 Supreme
                   Court of the United States. We train nine separate models
                   with the respective authored opinions of each supreme justice
                   active ca. 2015 and test the resulting system on 96
                   real-world cases. We find our system predicts the decisions
                   of the real-world Supreme Court with better-than-random
                   accuracy. We further find a correlation between model
                   accuracy with respect to individual justices and their
                   alignment between legal conservatism \& liberalism. Our
                   methods and results hold significance for researchers
                   interested in using language models to simulate
                   politically-charged discourse between multiple agents.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hollett2022-is,
  title    = "Body Gaze as a Marker of Sexual Objectification: A New Scale for
              Pervasive Gaze and Gaze Provocation Behaviors in Heterosexual
              Women and Men",
  author   = "Hollett, Ross C and Rogers, Shane L and Florido, Prudence and
              Mosdell, Belinda",
  journal  = "Arch. Sex. Behav.",
  volume   =  51,
  number   =  6,
  pages    = "2759--2780",
  abstract = "Body gaze behavior is assumed to be a key feature of sexual
              objectification. However, there are few self-report gaze measures
              available and none capturing behavior which seeks to invite body
              gaze from others. Across two studies, we used existing self-report
              instruments and measurement of eye movements to validate a new
              self-report scale to measure pervasive body gaze behavior and body
              gaze provocation behavior in heterosexual women and men. In Study
              1, participants (N = 1021) completed a survey with newly created
              items related to pervasive body gaze and body gaze provocation
              behavior. Participants also completed preexisting measures of body
              attitudes, sexual assault attitudes, pornography use, and
              relationship status. Exploratory and confirmatory factor analyses
              across independent samples suggested a 12-item scale for men and
              women to separately measure pervasive body gaze (5 items) and body
              gaze provocation (7 items) toward the opposite sex. The two scales
              yielded excellent internal consistency estimates (.86-.89) and
              promising convergent validity via positive correlations with body
              and sexual attitudes. In Study 2, a subsample (N = 167) of
              participants from Study 1 completed an eye-tracking task to
              capture their gaze behavior toward matched images of partially and
              fully dressed female and male subjects. Men exhibited body-biased
              gaze behavior toward all the female imagery, whereas women
              exhibited head-biased gaze behavior toward fully clothed male
              imagery. Importantly, self-reported body gaze correlated
              positively with some aspects of objectively measured body gaze
              behavior. Both scales showed good test-retest reliability and were
              positively correlated with sexual assault attitudes.",
  month    =  aug,
  year     =  2022,
  keywords = "Body; Eye tracking; Gaze; Sexual objectification",
  language = "en"
}

@ARTICLE{Griffin2022-nh,
  title     = "\#BodyPositive? A critical exploration of the body positive
               movement within physical cultures taking an intersectionality
               approach",
  author    = "Griffin, Meridith and Bailey, K Alysse and Lopez, Kimberly J",
  journal   = "Front. Sports Act. Living",
  publisher = "Frontiers Media SA",
  volume    =  4,
  pages     =  908580,
  abstract  = "Feminist activists and critical sport scholars in the global
               north have advocated for more inclusive representation of bodies
               and more accessible physical cultures. Body positivity, a
               contentious movement and concept, has been taken up in various
               ways by different groups. Some scholars believe it holds power to
               liberate individuals from patriarchal, neoliberal, capitalist,
               and colonial ideologies of what constitutes a ``good'' body. On
               the contrary, critics assert this movement has been gentrified by
               white-centered politics. Intersectionality has a similar
               genealogy as body positivity, with a rich history in Black
               feminist thought but now considered by many as coopted and
               whitened. In this article, we trace the rich and divergent
               legacies of both movements and explore at the structural level
               how body positivity is represented within physical cultures on
               Instagram. We use a social-justice oriented intersectionality
               framework exploring \#BodyPositivity and \#BodyPositive across a
               total of 141 posts using reflexive thematic analysis. We organize
               our findings into four themes: 1) Disclosure-Privilege of
               Body-Related Journeys; 2) The Absent-Present; 3) Consuming
               Positivity; and 4) Disrupting Normative Body Positivity Posts.
               Overall, we found that only certain bodies (and transformations)
               were visible within the data: those of (now) lean, white,
               cis-gendered individuals, many of whom were engaged in
               bodybuilding, and who were sharing their bodily transformation.
               We observe a remarkable absence of BIPOC, 2S LGBTQAI+,
               fat/thick/thicc/curvy, older, gender-nonconforming, and/or
               disabled representations. We also note the myriad ways that body
               positivity has been commodified and packaged into a product or
               service for consumption. Lastly, we outline and celebrate the
               exceptions to this norm where a minority of posts align more
               closely with the original intentions of the body positivity
               movement. We conclude with our position on how to do
               intersectionality research, and call on researchers to honor
               Black feminist origins and rich social justice history in these
               movements.",
  month     =  oct,
  year      =  2022,
  keywords  = "black feminism; bodies of difference; body positivity; fat
               activism; intersectionality; physical activity",
  language  = "en"
}

@ARTICLE{Hyland1998-zn,
  title     = "Boosting, hedging and the negotiation of academic knowledge",
  author    = "Hyland, Ken",
  journal   = "Text \& Talk",
  publisher = "De Gruyter Mouton",
  volume    =  18,
  number    =  3,
  pages     = "349--382",
  abstract  = "Article Boosting, hedging and the negotiation of academic
               knowledge was published on August 1, 1998 in the journal Text \&
               Talk (volume 18, issue 3).",
  month     =  aug,
  year      =  1998,
  language  = "en"
}

@ARTICLE{Bottou2023-cm,
  title         = "Borges and {AI}",
  author        = "Bottou, Léon and Schölkopf, Bernhard",
  journal       = "arXiv [cs.CL]",
  abstract      = "Many believe that Large Language Models (LLMs) open the era
                   of Artificial Intelligence (AI). Some see opportunities while
                   others see dangers. Yet both proponents and opponents grasp
                   AI through the imagery popularised by science fiction. Will
                   the machine become sentient and rebel against its creators?
                   Will we experience a paperclip apocalypse? Before answering
                   such questions, we should first ask whether this mental
                   imagery provides a good description of the phenomenon at
                   hand. Understanding weather patterns through the moods of the
                   gods only goes so far. The present paper instead advocates
                   understanding LLMs and their connection to AI through the
                   imagery of Jorge Luis Borges, a master of 20th century
                   literature, forerunner of magical realism, and precursor to
                   postmodern literature. This exercise leads to a new
                   perspective that illuminates the relation between language
                   modelling and artificial intelligence.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Tan2023-sx,
  title     = "{BotPercent}: Estimating Bot Populations in Twitter Communities",
  author    = "Tan, Zhaoxuan and Feng, Shangbin and Sclar, Melanie and Wan,
               Herun and Luo, Minnan and Choi, Yejin and Tsvetkov, Yulia",
  editor    = "Bouamor, Houda and Pino, Juan and Bali, Kalika",
  booktitle = "Findings of the Association for Computational Linguistics: EMNLP
               2023",
  publisher = "Association for Computational Linguistics",
  address   = "Singapore",
  pages     = "14295--14312",
  abstract  = "Twitter bot detection is vital in combating misinformation and
               safeguarding the integrity of social media discourse. While
               malicious bots are becoming more and more sophisticated and
               personalized, standard bot detection approaches are still
               agnostic to social environments (henceforth, communities) the
               bots operate at. In this work, we introduce community-specific
               bot detection, estimating the percentage of bots given the
               context of a community. Our method---BotPercent---is an
               amalgamation of Twitter bot detection datasets and feature-,
               text-, and graph-based models, adjusted to a particular community
               on Twitter. We introduce an approach that performs confidence
               calibration across bot detection models, which addresses
               generalization issues in existing community-agnostic models
               targeting individual bots and leads to more accurate
               community-level bot estimations. Experiments demonstrate that
               BotPercent achieves state-of-the-art performance in
               community-level Twitter bot detection across both balanced and
               imbalanced class distribution settings, presenting a less biased
               estimator of Twitter bot populations within the communities we
               analyze. We then analyze bot rates in several Twitter groups,
               including users who engage with partisan news media, political
               communities in different countries, and more. Our results reveal
               that the presence of Twitter bots is not homogeneous, but
               exhibiting a spatial-temporal distribution with considerable
               heterogeneity that should be taken into account for content
               moderation and social media policy making. The implementation of
               BotPercent is available at
               https://github.com/TamSiuhin/BotPercent.",
  month     =  dec,
  year      =  2023
}

@BOOK{Bail2022-uo,
  title     = "Breaking the Social Media Prism: How to Make Our Platforms Less
               Polarizing",
  author    = "Bail, Chris",
  publisher = "Princeton University Press",
  abstract  = "A revealing look at how user behavior is powering deep social
               divisions online—and how we might yet defeat political tribalism
               on social mediaIn an era of increasing social isolation,
               platforms like Facebook and Twitter are among the most important
               tools we have to understand each other. We use social media as a
               mirror to decipher our place in society but, as Chris Bail
               explains, it functions more like a prism that distorts our
               identities, empowers status-seeking extremists, and renders
               moderates all but invisible. Breaking the Social Media Prism
               challenges common myths about echo chambers, foreign
               misinformation campaigns, and radicalizing algorithms, revealing
               that the solution to political tribalism lies deep inside
               ourselves.Drawing on innovative online experiments and in-depth
               interviews with social media users from across the political
               spectrum, this book explains why stepping outside of our echo
               chambers can make us more polarized, not less. Bail takes you
               inside the minds of online extremists through vivid narratives
               that trace their lives on the platforms and off—detailing how
               they dominate public discourse at the expense of the moderate
               majority. Wherever you stand on the spectrum of user behavior and
               political opinion, he offers fresh solutions to counter political
               tribalism from the bottom up and the top down. He introduces new
               apps and bots to help readers avoid misperceptions and engage in
               better conversations with the other side. Finally, he explores
               what the virtual public square might look like if we could hit
               ``reset'' and redesign social media from scratch through a
               first-of-its-kind experiment on a new social media platform built
               for scientific research.Providing data-driven recommendations for
               strengthening our social media connections, Breaking the Social
               Media Prism shows how to combat online polarization without
               deleting our accounts.",
  month     =  sep,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Author_undated-fm,
  title  = "Bridging the Gap: A Three-Dimensional Approach to Machine Learning
            Fairness",
  author = "Author, Anonymous"
}

@MISC{noauthor_undated-np,
  title = "{BrunoLatour\_2005\_IntroductionHowToResu\_ReassemblingTheSocial}.pdf"
}

@MISC{noauthor_undated-kj,
  title = "{Bucholtz2011Ch4}.pdf"
}

@ARTICLE{Bargiela-Chiappini2002-li,
  title     = "Business discourse: Old debates, new horizons",
  author    = "Bargiela-Chiappini, Francesca and Nickerson, Catherine",
  journal   = "IRAL Int. Rev. Appl. Linguist. Lang. Teach.",
  publisher = "Walter de Gruyter GmbH",
  volume    =  40,
  number    =  4,
  month     =  jan,
  year      =  2002
}

@ARTICLE{Chatterjee2023-cf,
  title         = "Cabbage Sweeter than Cake? Analysing the Potential of Large
                   Language Models for Learning Conceptual Spaces",
  author        = "Chatterjee, Usashi and Gajbhiye, Amit and Schockaert, Steven",
  journal       = "arXiv [cs.CL]",
  abstract      = "The theory of Conceptual Spaces is an influential
                   cognitive-linguistic framework for representing the meaning
                   of concepts. Conceptual spaces are constructed from a set of
                   quality dimensions, which essentially correspond to primitive
                   perceptual features (e.g. hue or size). These quality
                   dimensions are usually learned from human judgements, which
                   means that applications of conceptual spaces tend to be
                   limited to narrow domains (e.g. modelling colour or taste).
                   Encouraged by recent findings about the ability of Large
                   Language Models (LLMs) to learn perceptually grounded
                   representations, we explore the potential of such models for
                   learning conceptual spaces. Our experiments show that LLMs
                   can indeed be used for learning meaningful representations to
                   some extent. However, we also find that fine-tuned models of
                   the BERT family are able to match or even outperform the
                   largest GPT-3 model, despite being 2 to 3 orders of magnitude
                   smaller.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Dillion2023-ph,
  title    = "Can {AI} language models replace human participants?",
  author   = "Dillion, Danica and Tandon, Niket and Gu, Yuling and Gray, Kurt",
  journal  = "Trends Cogn. Sci.",
  abstract = "Recent work suggests that language models such as GPT can make
              human-like judgments across a number of domains. We explore
              whether and when language models might replace human participants
              in psychological science. We review nascent research, provide a
              theoretical model, and outline caveats of using AI as a
              participant.",
  month    =  may,
  year     =  2023,
  keywords = "language models; artificial intelligence; morality; judgments;
              participants; research methods"
}

@ARTICLE{Xu2020-ph,
  title     = "Can Beauty Advertisements Empower Women? A Critical Discourse
               Analysis of the {SK}-{II}'s`` Change Destiny'' Campaign",
  author    = "Xu, Huimin and Tan, Yunying",
  journal   = "Theory and Practice in Language Studies",
  publisher = "Academy Publication Co., Ltd.",
  volume    =  10,
  number    =  2,
  pages     = "176--188",
  abstract  = "… examines the advertising campaign of a beauty product SK-II,…
               language and visuals in the three advertisements and a video
               advertisement , this article aims to investigate how the beauty …",
  year      =  2020
}

@ARTICLE{Porra2020-dq,
  title    = "“Can Computer Based Human-Likeness Endanger Humanness?” – A
              Philosophical and Ethical Perspective on Digital Assistants
              Expressing Feelings They Can’t Have”",
  author   = "Porra, Jaana and Lacity, Mary and Parks, Michael S",
  journal  = "Inf. Syst. Front.",
  volume   =  22,
  number   =  3,
  pages    = "533--547",
  abstract = "Digital assistants engage with us with increasingly human-like
              conversations, including the expression of human emotions with
              such utterances as “I am sorry…”, “I hope you enjoy…”, “I am
              grateful…”, or “I regret that…”. By 2021, digital assistants will
              outnumber humans. No one seems to stop to ask if creating more
              digital companions that appear increasingly human is really
              beneficial to the future of our species. In this essay, we pose
              the question: “How human should computer-based human-likeness
              appear?” We rely on the philosophy of humanness and the theory of
              speech acts to consider the long-term consequences of living with
              digital creatures that express human-like feelings. We argue that
              feelings are the very substance of our humanness and therefore are
              best reserved for human interaction.",
  month    =  jun,
  year     =  2020
}

@ARTICLE{Wang2023-jc,
  title     = "Can Language Models Solve Graph Problems in Natural Language?",
  author    = "Wang, H and Feng, S and He, T and Tan, Z and Han, X and {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Large language models (LLMs) are increasingly adopted for a
               variety of tasks with implicit graphical structures, such as
               planning in robotics, multi-hop question answering or …",
  year      =  2023
}

@ARTICLE{Zverev2024-oc,
  title         = "Can {LLMs} Separate Instructions From Data? And What Do We
                   Even Mean By That?",
  author        = "Zverev, Egor and Abdelnabi, Sahar and Fritz, Mario and
                   Lampert, Christoph H",
  journal       = "arXiv [cs.LG]",
  abstract      = "Instruction-tuned Large Language Models (LLMs) have achieved
                   breakthrough results, opening countless new possibilities for
                   many practical applications. However, LLMs lack elementary
                   safety features that are established norms in other areas of
                   computer science, such as the separation between instructions
                   and data, causing them to malfunction or rendering them
                   vulnerable to manipulation and interference by third parties
                   e.g., via indirect prompt/command injection. Even worse, so
                   far, there is not even an established definition of what
                   precisely such a separation would mean and how its violation
                   could be tested. In this work, we aim to close this gap. We
                   introduce a formal measure to quantify the phenomenon of
                   instruction-data separation as well as an empirical variant
                   of the measure that can be computed from a model`s black-box
                   outputs. We also introduce a new dataset, SEP (Should it be
                   Executed or Processed?), which allows estimating the measure,
                   and we report results on several state-of-the-art open-source
                   and closed LLMs. Finally, we quantitatively demonstrate that
                   all evaluated LLMs fail to achieve a high amount of
                   separation, according to our measure. The source code and SEP
                   dataset are openly accessible at
                   https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Xu2022-oq,
  title         = "Can Model Compression Improve {NLP} Fairness",
  author        = "Xu, Guangxuan and Hu, Qingyuan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Model compression techniques are receiving increasing
                   attention; however, the effect of compression on model
                   fairness is still under explored. This is the first paper to
                   examine the effect of distillation and pruning on the
                   toxicity and bias of generative language models. We test
                   Knowledge Distillation and Pruning methods on the GPT2 model
                   and found a consistent pattern of toxicity and bias reduction
                   after model distillation; this result can be potentially
                   interpreted by existing line of research which describes
                   model compression as a regularization technique; our work not
                   only serves as a reference for safe deployment of compressed
                   models, but also extends the discussion of ``compression as
                   regularization'' into the setting of neural LMs, and hints at
                   the possibility of using compression to develop fairer
                   models.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Chowdhary2023-cl,
  title     = "Can Workers Meaningfully Consent to Workplace Wellbeing
               Technologies?",
  author    = "Chowdhary, Shreya and Kawakami, Anna and Gray, Mary L and Suh,
               Jina and Olteanu, Alexandra and Saha, Koustuv",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "569--582",
  abstract  = "Sensing technologies deployed in the workplace can unobtrusively
               collect detailed data about individual activities and group
               interactions that are otherwise difficult to capture. A hopeful
               application of these technologies is that they can help
               businesses and workers optimize productivity and wellbeing.
               However, given the inherent and structural power dynamics in the
               workplace, the prevalent approach of accepting tacit compliance
               to monitor work activities rather than seeking workers’
               meaningful consent raises privacy and ethical concerns. This
               paper unpacks challenges workers face when consenting to
               workplace wellbeing technologies. Using a hypothetical case to
               prompt reflection among six multi-stakeholder focus groups
               involving 15 participants, we explored participants’ expectations
               and capacity to consent to these technologies. We sketched
               possible interventions that could better support meaningful
               consent to workplace wellbeing technologies, by drawing on
               critical computing and feminist scholarship—which reframes
               consent from a purely individual choice to a structural condition
               experienced at the individual level that needs to be freely
               given, reversible, informed, enthusiastic, and specific (FRIES).
               The focus groups revealed how workers are vulnerable to
               “meaningless” consent—as they may be subject to power dynamics
               that minimize their ability to withhold consent and may thus
               experience an erosion of autonomy in their workplace, also
               undermining the value of data gathered in the name of
               “wellbeing.” To meaningfully consent, participants wanted changes
               to how the technology works and is being used, as well as to the
               policies and practices surrounding the technology. Our mapping of
               what prevents workers from meaningfully consenting to workplace
               wellbeing technologies (challenges) and what they require to do
               so (interventions) illustrates how the lack of meaningful consent
               is a structural problem requiring socio-technical solutions.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "ethics, sensing, power, workplace, data governance, privacy"
}

@ARTICLE{Yang2023-sy,
  title         = "Can You Follow Me? Testing Situational Understanding in
                   {ChatGPT}",
  author        = "Yang, Chenghao and Ettinger, Allyson",
  journal       = "arXiv [cs.CL]",
  abstract      = "Understanding sentence meanings and updating information
                   states appropriately across time -- what we call
                   ``situational understanding'' (SU) -- is a critical ability
                   for human-like AI agents. SU is essential in particular for
                   chat models, such as ChatGPT, to enable consistent, coherent,
                   and effective dialogue between humans and AI. Previous works
                   have identified certain SU limitations in non-chatbot Large
                   Language models (LLMs), but the extent and causes of these
                   limitations are not well understood, and capabilities of
                   current chat-based models in this domain have not been
                   explored. In this work we tackle these questions, proposing a
                   novel synthetic environment for SU testing which allows us to
                   do controlled and systematic testing of SU in chat-oriented
                   models, through assessment of models' ability to track and
                   enumerate environment states. Our environment also allows for
                   close analysis of dynamics of model performance, to better
                   understand underlying causes for performance patterns. We
                   apply our test to ChatGPT, the state-of-the-art chatbot, and
                   find that despite the fundamental simplicity of the task, the
                   model's performance reflects an inability to retain correct
                   environment states across time. Our follow-up analyses
                   suggest that performance degradation is largely because
                   ChatGPT has non-persistent in-context memory (although it can
                   access the full dialogue history) and it is susceptible to
                   hallucinated updates -- including updates that artificially
                   inflate accuracies. Our findings suggest overall that ChatGPT
                   is not currently equipped for robust tracking of situation
                   states, and that trust in the impressive dialogue performance
                   of ChatGPT comes with risks. We release the codebase for
                   reproducing our test environment, as well as all prompts and
                   API responses from ChatGPT, at
                   https://github.com/yangalan123/SituationalTesting.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Nguyen2020-vw,
  title     = "{C}an{VEC} - the Canberra {V}ietnamese-{E}nglish code-switching
               natural speech corpus",
  author    = "Nguyen, Li and Bryant, Christopher",
  booktitle = "Proceedings of the Twelfth Language Resources and Evaluation
               Conference",
  publisher = "European Language Resources Association",
  address   = "Marseille, France",
  pages     = "4121--4129",
  abstract  = "This paper introduces the Canberra Vietnamese-English
               Code-switching corpus (CanVEC), an original corpus of natural
               mixed speech that we semi-automatically annotated with language
               information, part of speech (POS) tags and Vietnamese
               translations. The corpus, which was built to inform a
               sociolinguistic study on language variation and code-switching,
               consists of 10 hours of recorded speech (87k tokens) between 45
               Vietnamese-English bilinguals living in Canberra, Australia. We
               describe how we collected and annotated the corpus by pipelining
               several monolingual toolkits to considerably speed up the
               annotation process. We also describe how we evaluated the
               automatic annotations to ensure corpus reliability. We make the
               corpus available for research purposes.",
  month     =  may,
  year      =  2020,
  language  = "English"
}

@ARTICLE{Jarvis2013-ok,
  title     = "Capturing the diversity in lexical diversity",
  author    = "Jarvis, Scott",
  journal   = "Lang. Learn.",
  publisher = "Wiley",
  volume    =  63,
  pages     = "87--106",
  abstract  = "The range, variety, or diversity of words found in learners’
               language use is believed to reflect the complexity of their
               vocabulary knowledge as well as the level of their language
               proficiency. Many indices of lexical diversity have been
               proposed, most of which involve statistical relationships between
               types and tokens, and which ultimately reflect the rate of word
               repetition. These indices have generally been validated in
               accordance with how well they overcome sample-size effects and/or
               how well they predict language knowledge or behavior, rather than
               in accordance with how well they actually measure the construct
               of lexical diversity. In this article, I review developments that
               have taken place in lexical diversity research, and also describe
               obstacles that have prevented it from advancing further. I
               compare these developments with parallel research on biodiversity
               in the field of ecology, and show what language researchers can
               learn from ecology regarding the modeling and measurement of
               diversity as a multidimensional construct of compositional
               complexity.",
  month     =  mar,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Prabhumoye2020-ds,
  title         = "Case Study: Deontological Ethics in {NLP}",
  author        = "Prabhumoye, Shrimai and Boldt, Brendon and Salakhutdinov,
                   Ruslan and Black, Alan W",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work in natural language processing (NLP) has focused
                   on ethical challenges such as understanding and mitigating
                   bias in data and algorithms; identifying objectionable
                   content like hate speech, stereotypes and offensive language;
                   and building frameworks for better system design and data
                   handling practices. However, there has been little discussion
                   about the ethical foundations that underlie these efforts. In
                   this work, we study one ethical theory, namely deontological
                   ethics, from the perspective of NLP. In particular, we focus
                   on the generalization principle and the respect for autonomy
                   through informed consent. We provide four case studies to
                   demonstrate how these principles can be used with NLP
                   systems. We also recommend directions to avoid the ethical
                   issues in these systems.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Veeraraghavan2021-zg,
  title     = "Cat and Mouse Game: Patching Bureaucratic Work Relations by
               Patching Technologies",
  author    = "Veeraraghavan, Rajesh",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  5,
  number    = "CSCW1",
  pages     = "1--21",
  abstract  = "This article uses findings from a field study of the world's
               largest guaranteed employment scheme (NREGA) in India to
               understand how digital technology mediates work relations and
               power dynamics within a bureaucracy. In this initiative,
               upper-level bureaucrats in the south Indian state of Andhra
               Pradesh built a digital network to remove local discretion at the
               ``last mile'' of an implementation of NREGA. I show how digital
               infrastructure affords actors at both the first and last mile
               opportunities to modify software to control as well as subvert
               certain practices. This article refers to this dialectic
               phenomenon as ``governance by patching'' and defines it as a
               socio-technical instantiation of a top-down process that focuses
               on small changes, iterative, and political process for positive
               change. Governance by patching is, therefore, neither a purely
               technical process nor an exclusively administrative one. Rather,
               it refers to the ability to fix unanticipated problems that arise
               in the implementation of governance programs by altering the
               socio-technical systems. The struggle for power continues, but on
               the new digital terrain.",
  month     =  apr,
  year      =  2021,
  keywords  = "infrastructures, ictd, patching, last-mile, bureaucracy, nrega,
               governance"
}

@INPROCEEDINGS{Zhang2023-vs,
  title     = "Causal Matching with Text Embeddings: A Case Study in Estimating
               the Causal Effects of Peer Review Policies",
  author    = "Zhang, Raymond and Kennard, Neha Nayak and Smith, Daniel and
               McFarland, Daniel and McCallum, Andrew and Keith, Katherine",
  editor    = "Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki",
  booktitle = "Findings of the Association for Computational Linguistics: ACL
               2023",
  publisher = "Association for Computational Linguistics",
  address   = "Toronto, Canada",
  pages     = "1284--1297",
  abstract  = "A promising approach to estimate the causal effects of peer
               review policies is to analyze data from publication venues that
               shift policies from single-blind to double-blind from one year to
               the next. However, in these settings the content of the
               manuscript is a confounding variable---each year has a different
               distribution of scientific content which may naturally affect the
               distribution of reviewer scores. To address this textual
               confounding, we extend variable ratio nearest neighbor matching
               to incorporate text embeddings. We compare this matching method
               to a widely-used causal method of stratified propensity score
               matching and a baseline of randomly selected matches. For our
               case study of the ICLR conference shifting from single- to
               double-blind review from 2017 to 2018, we find human judges
               prefer manuscript matches from our method in 70\% of cases. While
               the unadjusted estimate of the average causal effect of
               reviewers' scores is -0.25, our method shifts the estimate to
               -0.17, a slightly smaller difference between the outcomes of
               single- and double-blind policies. We hope this case study
               enables exploration of additional text-based causal estimation
               methods and domains in the future.",
  month     =  jul,
  year      =  2023
}

@ARTICLE{Vig2020-iy,
  title     = "Causal mediation analysis for interpreting neural nlp: The case
               of gender bias",
  author    = "Vig, J and Gehrmann, S and Belinkov, Y and Qian, S and Nevo, D
               and {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Common methods for interpreting neural models in natural language
               processing typically examine either their structure or their
               behavior, but not both. We propose a methodology …",
  year      =  2020
}

@ARTICLE{Ibrahim2024-ym,
  title         = "Characterizing and modeling harms from interactions with
                   design patterns in {AI} interfaces",
  author        = "Ibrahim, Lujain and Rocher, Luc and Valdivia, Ana",
  journal       = "arXiv [cs.HC]",
  abstract      = "The proliferation of applications using artificial
                   intelligence (AI) systems has led to a growing number of
                   users interacting with these systems through sophisticated
                   interfaces. Human-computer interaction research has long
                   shown that interfaces shape both user behavior and user
                   perception of technical capabilities and risks. Yet,
                   practitioners and researchers evaluating the social and
                   ethical risks of AI systems tend to overlook the impact of
                   anthropomorphic, deceptive, and immersive interfaces on
                   human-AI interactions. Here, we argue that design features of
                   interfaces with adaptive AI systems can have cascading
                   impacts, driven by feedback loops, which extend beyond those
                   previously considered. We first conduct a scoping review of
                   AI interface designs and their negative impact to extract
                   salient themes of potentially harmful design patterns in AI
                   interfaces. Then, we propose Design-Enhanced Control of AI
                   systems (DECAI), a conceptual model to structure and
                   facilitate impact assessments of AI interface designs. DECAI
                   draws on principles from control systems theory -- a theory
                   for the analysis and design of dynamic physical systems -- to
                   dissect the role of the interface in human-AI systems.
                   Through two case studies on recommendation systems and
                   conversational language model systems, we show how DECAI can
                   be used to evaluate AI interface designs.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Chu2024-zx,
  title         = "Characterizing Online Eating Disorder Communities with Large
                   Language Models",
  author        = "Chu, Minh Duc and Karnati, Aryan and He, Zihao and Lerman,
                   Kristina",
  journal       = "arXiv [cs.SI]",
  abstract      = "The rise in eating disorders, a dangerous mental health
                   condition with high mortality and morbidity, has been linked
                   to the proliferation of idealized body images on social
                   media. However, the link between social media and eating
                   disorders is far more complex. We argue that social media
                   platforms create a feedback loop that amplifies the growth of
                   content and communities that promote eating disorders like
                   anorexia and bulimia. Specifically, social media platforms
                   make it easy for vulnerable individuals to find and connect
                   to like-minded others, while group dynamic processes
                   encourage them to stay engaged within communities that
                   promote and glorify harmful behaviors linked to eating
                   disorders. We characterize this dynamic empirically through a
                   combination of network and language analysis. We describe a
                   novel framework that leverages large language models to
                   analyze the discourse within online communities and probe
                   their attitudes on topics related to eating disorders to
                   identify potentially harmful content. Our work emphasizes the
                   need for better social media moderation to disrupt harmful
                   feedback loops and protect vulnerable individuals.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SI"
}

@MISC{Snow2008-yo,
  title        = "Cheap and Fast — But is it Good? Evaluating Non-Expert
                  Annotations for Natural Language Tasks",
  author       = "Snow, Rion and O'Connor, Brendan and Jurafsky, Daniel and Ng,
                  Andrew Y",
  publisher    = "aclanthology.org",
  abstract     = "Human linguistic annotation is crucial for many natural
                  language processing tasks but can be expensive and
                  time-consuming. We explore the use of Amazon's Mechanical Turk
                  system, a significantly cheaper and faster method for
                  collecting annotations from a broad base of paid non-expert
                  contributors over the Web. We investigate five tasks: affect
                  recognition, word similarity, recognizing textual entailment,
                  event temporal ordering, and word sense disambiguation. For
                  all five, we show high agreement between Mechanical Turk …",
  year         =  2008,
  howpublished = "\url{https://aclanthology.org/D08-1027.pdf}",
  note         = "Accessed: 2023-6-26"
}

@ARTICLE{Zhang2023-xo,
  title     = "Children's structural thinking about social inequities",
  author    = "Zhang, Marianna Y and Sullivan, J Nicky and Markman, Ellen M and
               Roberts, Steven O",
  journal   = "Child Dev. Perspect.",
  publisher = "Wiley",
  abstract  = "AbstractAcross development, young children reason about why
               social inequities exist. However, when left to their own devices,
               young children might engage in internal thinking, reasoning that
               the inequity is simply a justified disparity explained by
               features internal to social groups (e.g., genetics, intellect,
               abilities, values). Internal thinking could lead them to support
               and reinforce the inequity (e.g., by blaming the disadvantaged).
               In contrast, structural thinking, which appeals to relatively
               stable features external to social groups (e.g., environments,
               policies, economic systems), could lead to more prosocial
               outcomes (e.g., support for social interventions). While
               researchers have examined adolescents' and adults' structural
               thinking about social inequities, in this article, we review
               recent research that suggests that even children as young as 5
               can engage in structural thinking. We conclude with suggestions
               for future studies, particularly research related to how to
               foster young children's structural thinking in the context of
               real‐world social inequities.",
  month     =  dec,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Zhang2012-gn,
  title     = "Chinese-English code-mixing among China's netizens:
               Chinese-English mixed-code communication is gaining popularity on
               the Internet",
  author    = "Zhang, Wei",
  journal   = "English Today",
  publisher = "Cambridge University Press",
  volume    =  28,
  number    =  3,
  pages     = "40--52",
  abstract  = "One of the most fascinating aspects of Chinese English in the
               contemporary age is the growing practice of code-mixing on
               China's Internet. This study uses participant observation to
               explore the ‘mixing’ practices of Chinese netizens in online
               social networking communities, by focusing on code-mixing in
               domains of government administration, pop culture and social
               interaction. The results of the study suggest that a mixed-code
               variety of Chinese English is gaining popularity in China's
               homegrown social networks. For the young generation in mainland
               China, ‘mixing’ has become part of their everyday communication
               practices as they build multicultural identities, transform the
               traditional social relationships and practice their social
               responsibilities, using new communication technologies as well as
               their linguistic and cultural repertoires.",
  month     =  sep,
  year      =  2012
}

@ARTICLE{Orgad2022-ob,
  title         = "Choose Your Lenses: Flaws in Gender Bias Evaluation",
  author        = "Orgad, Hadas and Belinkov, Yonatan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Considerable efforts to measure and mitigate gender bias in
                   recent years have led to the introduction of an abundance of
                   tasks, datasets, and metrics used in this vein. In this
                   position paper, we assess the current paradigm of gender bias
                   evaluation and identify several flaws in it. First, we
                   highlight the importance of extrinsic bias metrics that
                   measure how a model's performance on some task is affected by
                   gender, as opposed to intrinsic evaluations of model
                   representations, which are less strongly connected to
                   specific harms to people interacting with systems. We find
                   that only a few extrinsic metrics are measured in most
                   studies, although more can be measured. Second, we find that
                   datasets and metrics are often coupled, and discuss how their
                   coupling hinders the ability to obtain reliable conclusions,
                   and how one may decouple them. We then investigate how the
                   choice of the dataset and its composition, as well as the
                   choice of the metric, affect bias measurement, finding
                   significant variations across each of them. Finally, we
                   propose several guidelines for more reliable gender bias
                   evaluation.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Agostini2019-mq,
  title     = "Citizen sociolinguistics: A new method to understand fat talk",
  author    = "Agostini, Gina and SturtzSreetharan, Cindi and Wutich, Amber and
               Williams, Deborah and Brewis, Alexandra",
  journal   = "PLoS One",
  publisher = "journals.plos.org",
  volume    =  14,
  number    =  5,
  pages     = "e0217618",
  abstract  = "FAT TALK AND CITIZEN SCIENCE: Fat talk is a spontaneous verbal
               interaction in which interlocutors make self-disparaging comments
               about the body, usually as a request for assessment. Fat talk
               often reflects concerns about the self that stem from broader
               sociocultural factors. It is therefore an important target for
               sociocultural linguistics. However, real-time studies of fat talk
               are uncommon due to the resource and time burdens required to
               capture these fleeting utterances. This limits the scope of data
               produced using standard sociolinguistic methods. Citizen science
               may alleviate these burdens by producing a scale of social
               observation not afforded via traditional methods. Here we present
               a proof-of-concept for a novel methodology, citizen
               sociolinguistics. This research approach involves collaborations
               with citizen researchers to capture forms of conversational data
               that are typically inaccessible, including fat talk. AIMS AND
               OUTCOMES: This study had two primary aims. Aim 1 focused on
               scientific output, testing a novel research strategy wherein
               citizen sociolinguists captured fat talk data in a diverse
               metropolitan region (Southwestern United States). Results confirm
               that citizen sociolinguistic research teams captured forms of fat
               talk that mirrored the scripted responses previously reported.
               However, they also capture unique forms of fat talk, likely due
               to greater diversity in sample and sampling environments. Aim 2
               focused on the method itself via reflective exercises shared by
               the citizen sociolinguists throughout the project. In addition to
               confirming that the citizen sociolinguistic method produces
               reliable, scientifically valid data, we contend that citizen
               sociolinguist inclusion has broader scientific benefits which
               include applied scientific training, fostering sustained
               relationships between professional researchers and the public,
               and producing novel, meaningful scientific output that advances
               professional discourse.",
  month     =  may,
  year      =  2019,
  language  = "en"
}

@MISC{noauthor_undated-vs,
  title = "(Clarendon Lectures in Management Studies) Bruno Latour -
           Reassembling the Social\_ An Introduction to
           Actor-Network-Theory-Oxford University Press, {USA} (2005).pdf"
}

@ARTICLE{Lee2024-tb,
  title         = "Clarify: Improving Model Robustness With Natural Language
                   Corrections",
  author        = "Lee, Yoonho and Lam, Michelle S and Vasconcelos, Helena and
                   Bernstein, Michael S and Finn, Chelsea",
  journal       = "arXiv [cs.LG]",
  abstract      = "In supervised learning, models are trained to extract
                   correlations from a static dataset. This often leads to
                   models that rely on high-level misconceptions. To prevent
                   such misconceptions, we must necessarily provide additional
                   information beyond the training data. Existing methods
                   incorporate forms of additional instance-level supervision,
                   such as labels for spurious features or additional labeled
                   data from a balanced distribution. Such strategies can become
                   prohibitively costly for large-scale datasets since they
                   require additional annotation at a scale close to the
                   original training data. We hypothesize that targeted natural
                   language feedback about a model's misconceptions is a more
                   efficient form of additional supervision. We introduce
                   Clarify, a novel interface and method for interactively
                   correcting model misconceptions. Through Clarify, users need
                   only provide a short text description to describe a model's
                   consistent failure patterns. Then, in an entirely automated
                   way, we use such descriptions to improve the training process
                   by reweighting the training data or gathering additional
                   targeted data. Our user studies show that non-expert users
                   can successfully describe model misconceptions via Clarify,
                   improving worst-group accuracy by an average of 17.1\% in two
                   datasets. Additionally, we use Clarify to find and rectify 31
                   novel hard subpopulations in the ImageNet dataset, improving
                   minority-split accuracy from 21.1\% to 28.7\%.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@MISC{noauthor_undated-ca,
  title = "Clark.communal.lexicons.98.pdf"
}

@ARTICLE{Jakesch2023-wg,
  title         = "Co-Writing with Opinionated Language Models Affects Users'
                   Views",
  author        = "Jakesch, Maurice and Bhat, Advait and Buschek, Daniel and
                   Zalmanson, Lior and Naaman, Mor",
  journal       = "arXiv [cs.HC]",
  abstract      = "If large language models like GPT-3 preferably produce a
                   particular point of view, they may influence people's
                   opinions on an unknown scale. This study investigates whether
                   a language-model-powered writing assistant that generates
                   some opinions more often than others impacts what users write
                   - and what they think. In an online experiment, we asked
                   participants (N=1,506) to write a post discussing whether
                   social media is good for society. Treatment group
                   participants used a language-model-powered writing assistant
                   configured to argue that social media is good or bad for
                   society. Participants then completed a social media attitude
                   survey, and independent judges (N=500) evaluated the opinions
                   expressed in their writing. Using the opinionated language
                   model affected the opinions expressed in participants'
                   writing and shifted their opinions in the subsequent attitude
                   survey. We discuss the wider implications of our results and
                   argue that the opinions built into AI language technologies
                   need to be monitored and engineered more carefully.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Zhou2023-hi,
  title         = "{COBRA} Frames: Contextual Reasoning about Effects and Harms
                   of Offensive Statements",
  author        = "Zhou, Xuhui and Zhu, Hao and Yerukola, Akhila and Davidson,
                   Thomas and Hwang, Jena D and Swayamdipta, Swabha and Sap,
                   Maarten",
  journal       = "arXiv [cs.CL]",
  abstract      = "Warning: This paper contains content that may be offensive or
                   upsetting. Understanding the harms and offensiveness of
                   statements requires reasoning about the social and
                   situational context in which statements are made. For
                   example, the utterance ``your English is very good'' may
                   implicitly signal an insult when uttered by a white man to a
                   non-white colleague, but uttered by an ESL teacher to their
                   student would be interpreted as a genuine compliment. Such
                   contextual factors have been largely ignored by previous
                   approaches to toxic language detection. We introduce COBRA
                   frames, the first context-aware formalism for explaining the
                   intents, reactions, and harms of offensive or biased
                   statements grounded in their social and situational context.
                   We create COBRACORPUS, a dataset of 33k potentially offensive
                   statements paired with machine-generated contexts and
                   free-text explanations of offensiveness, implied biases,
                   speaker intents, and listener reactions. To study the
                   contextual dynamics of offensiveness, we train models to
                   generate COBRA explanations, with and without access to the
                   context. We find that explanations by context-agnostic models
                   are significantly worse than by context-aware ones,
                   especially in situations where the context inverts the
                   statement's offensiveness (29\% accuracy drop). Our work
                   highlights the importance and feasibility of contextualized
                   NLP by modeling social factors.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Winata2019-yy,
  title         = "Code-Switched Language Models Using Neural Based Synthetic
                   Data from Parallel Sentences",
  author        = "Winata, Genta Indra and Madotto, Andrea and Wu, Chien-Sheng
                   and Fung, Pascale",
  journal       = "arXiv [cs.CL]",
  abstract      = "Training code-switched language models is difficult due to
                   lack of data and complexity in the grammatical structure.
                   Linguistic constraint theories have been used for decades to
                   generate artificial code-switching sentences to cope with
                   this issue. However, this require external word alignments or
                   constituency parsers that create erroneous results on distant
                   languages. We propose a sequence-to-sequence model using a
                   copy mechanism to generate code-switching data by leveraging
                   parallel monolingual translations from a limited source of
                   code-switching data. The model learns how to combine words
                   from parallel sentences and identifies when to switch one
                   language to the other. Moreover, it captures code-switching
                   constraints by attending and aligning the words in inputs,
                   without requiring any external knowledge. Based on
                   experimental results, the language model trained with the
                   generated sentences achieves state-of-the-art performance and
                   improves end-to-end automatic speech recognition.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Cheng1989-jx,
  title     = "Code-switching: a natural phenomenon vs language 'deficiency'",
  author    = "Cheng, Li-Rong and Butler, Katharine",
  journal   = "World Englishes",
  publisher = "Wiley",
  volume    =  8,
  number    =  3,
  pages     = "293--309",
  abstract  = "… The following is an example of such a syntactic code - switch
               between Mandarin Chinese and English: I show you ta (I show it to
               you) which is grammatically correct in Mandarin Chinese …",
  month     =  nov,
  year      =  1989,
  language  = "en"
}

@ARTICLE{Wang2022-yl,
  title         = "{COFFEE}: Counterfactual Fairness for Personalized Text
                   Generation in Explainable Recommendation",
  author        = "Wang, Nan and Wang, Qifan and Wang, Yi-Chia and Sanjabi,
                   Maziar and Liu, Jingzhou and Firooz, Hamed and Wang, Hongning
                   and Nie, Shaoliang",
  journal       = "arXiv [cs.CL]",
  abstract      = "As language models become increasingly integrated into our
                   digital lives, Personalized Text Generation (PTG) has emerged
                   as a pivotal component with a wide range of applications.
                   However, the bias inherent in user written text, often used
                   for PTG model training, can inadvertently associate different
                   levels of linguistic quality with users' protected
                   attributes. The model can inherit the bias and perpetuate
                   inequality in generating text w.r.t. users' protected
                   attributes, leading to unfair treatment when serving users.
                   In this work, we investigate fairness of PTG in the context
                   of personalized explanation generation for recommendations.
                   We first discuss the biases in generated explanations and
                   their fairness implications. To promote fairness, we
                   introduce a general framework to achieve measure-specific
                   counterfactual fairness in explanation generation. Extensive
                   experiments and human evaluations demonstrate the
                   effectiveness of our method.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Crisp2011-hu,
  title     = "Cognitive adaptation to the experience of social and cultural
               diversity",
  author    = "Crisp, Richard J and Turner, Rhiannon N",
  journal   = "Psychol. Bull.",
  publisher = "psycnet.apa.org",
  volume    =  137,
  number    =  2,
  pages     = "242--266",
  abstract  = "Diversity is a defining characteristic of modern society, yet
               there remains considerable debate over the benefits that it
               brings. The authors argue that positive psychological and
               behavioral outcomes will be observed only when social and
               cultural diversity is experienced in a way that challenges
               stereotypical expectations and that when this precondition is
               met, the experience has cognitive consequences that resonate
               across multiple domains. A model, rooted in social categorization
               theory and research, outlines the preconditions and processes
               through which people cognitively adapt to the experience of
               social and cultural diversity and the resulting cross-domain
               benefits that this brings. Evidence is drawn from a range of
               literatures to support this model, including work on
               biculturalism, minority influence, cognitive development,
               stereotype threat, work group productivity, creativity, and
               political ideology. The authors bring together a range of
               differing diversity experiences and explicitly draw parallels
               between programs of research that have focused on both perceiving
               others who are multicultural and being multicultural oneself. The
               findings from this integrative review suggest that experiencing
               diversity that challenges expectations may not only encourage
               greater tolerance but also have benefits beyond intergroup
               relations to varied aspects of psychological functioning.",
  month     =  mar,
  year      =  2011,
  language  = "en"
}

@INPROCEEDINGS{Adel2013-lt,
  title     = "Combination of Recurrent Neural Networks and Factored Language
               Models for Code-Switching Language Modeling",
  author    = "Adel, Heike and Vu, Ngoc Thang and Schultz, Tanja",
  booktitle = "Proceedings of the 51st Annual Meeting of the Association for
               Computational Linguistics (Volume 2: Short Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Sofia, Bulgaria",
  pages     = "206--211",
  abstract  = "… For evaluation, we compute the perplexity of each language
               model on the SEAME development and evaluation set und perform an
               analysis of the different back-off levels to understand …",
  month     =  aug,
  year      =  2013
}

@ARTICLE{Schellewald2021-mz,
  title     = "Communicative Forms on {TikTok}: Perspectives From Digital
               Ethnography",
  author    = "Schellewald, Andreas",
  journal   = "Int. J. Commun. Syst.",
  publisher = "ijoc.org",
  volume    =  15,
  number    =  0,
  pages     =  21,
  abstract  = "TikTok is an app that allows people to create, share, and consume
               short-video content. Although only available internationally
               since 2017, it has already been downloaded more than 2 billion
               times and has around 800 million active users. Public interest in
               the fleeting and seemingly random video clips that TikTok hosts
               is high. In fact, it has grown steadily since the time of the
               Twitter-owned short-video app Vine that ended its service in 2016
               with only a quarter of TikTok’s current userbase. However,
               despite this steady growth in popularity, observations and
               theorizations of short-video apps like TikTok remain lacking. In
               this article, I thus seek to address this lack by critically
               discussing how to study short-video communications from the
               bottom up and by presenting the results of an exploratory
               investigation into TikTok and its communicative forms. Doing so,
               this article contributes to opening a space for serious
               engagement with this burgeoning yet understudied element of
               digital culture in the future.",
  month     =  feb,
  year      =  2021,
  keywords  = "TikTok, short-video, forms of communication, digital ethnography,
               digital culture",
  language  = "en"
}

@ARTICLE{Jiang2022-af,
  title         = "{CommunityLM}: Probing Partisan Worldviews from Language
                   Models",
  author        = "Jiang, Hang and Beeferman, Doug and Roy, Brandon and Roy, Deb",
  journal       = "arXiv [cs.SI]",
  abstract      = "As political attitudes have diverged ideologically in the
                   United States, political speech has diverged lingusitically.
                   The ever-widening polarization between the US political
                   parties is accelerated by an erosion of mutual understanding
                   between them. We aim to make these communities more
                   comprehensible to each other with a framework that probes
                   community-specific responses to the same survey questions
                   using community language models CommunityLM. In our framework
                   we identify committed partisan members for each community on
                   Twitter and fine-tune LMs on the tweets authored by them. We
                   then assess the worldviews of the two groups using
                   prompt-based probing of their corresponding LMs, with prompts
                   that elicit opinions about public figures and groups surveyed
                   by the American National Election Studies (ANES) 2020
                   Exploratory Testing Survey. We compare the responses
                   generated by the LMs to the ANES survey results, and find a
                   level of alignment that greatly exceeds several baseline
                   methods. Our work aims to show that we can use community LMs
                   to query the worldview of any group of people given a
                   sufficiently large sample of their social media discussions
                   or media diet.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SI"
}

@ARTICLE{Wu2023-zq,
  title         = "Composition and Deformance: Measuring Imageability with a
                   Text-to-Image Model",
  author        = "Wu, Si and Smith, David A",
  journal       = "arXiv [cs.CL]",
  abstract      = "Although psycholinguists and psychologists have long studied
                   the tendency of linguistic strings to evoke mental images in
                   hearers or readers, most computational studies have applied
                   this concept of imageability only to isolated words. Using
                   recent developments in text-to-image generation models, such
                   as DALLE mini, we propose computational methods that use
                   generated images to measure the imageability of both single
                   English words and connected text. We sample text prompts for
                   image generation from three corpora: human-generated image
                   captions, news article sentences, and poem lines. We subject
                   these prompts to different deformances to examine the model's
                   ability to detect changes in imageability caused by
                   compositional change. We find high correlation between the
                   proposed computational measures of imageability and human
                   judgments of individual words. We also find the proposed
                   measures more consistently respond to changes in
                   compositionality than baseline approaches. We discuss
                   possible effects of model training and implications for the
                   study of compositionality in text-to-image models.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Min2019-wc,
  title         = "Compositional Questions Do Not Necessitate Multi-hop
                   Reasoning",
  author        = "Min, Sewon and Wallace, Eric and Singh, Sameer and Gardner,
                   Matt and Hajishirzi, Hannaneh and Zettlemoyer, Luke",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multi-hop reading comprehension (RC) questions are
                   challenging because they require reading and reasoning over
                   multiple paragraphs. We argue that it can be difficult to
                   construct large multi-hop RC datasets. For example, even
                   highly compositional questions can be answered with a single
                   hop if they target specific entity types, or the facts needed
                   to answer them are redundant. Our analysis is centered on
                   HotpotQA, where we show that single-hop reasoning can solve
                   much more of the dataset than previously thought. We
                   introduce a single-hop BERT-based RC model that achieves 67
                   F1---comparable to state-of-the-art multi-hop models. We also
                   design an evaluation setting where humans are not shown all
                   of the necessary paragraphs for the intended multi-hop
                   reasoning but can still answer over 80\% of questions.
                   Together with detailed error analysis, these results suggest
                   there should be an increasing focus on the role of evidence
                   in multi-hop reasoning and possibly even a shift towards
                   information retrieval style evaluations with large and
                   diverse evidence collections.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Cheng2023-jc,
  title         = "{CoMPosT}: Characterizing and Evaluating Caricature in {LLM}
                   Simulations",
  author        = "Cheng, Myra and Piccardi, Tiziano and Yang, Diyi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work has aimed to capture nuances of human behavior by
                   using LLMs to simulate responses from particular demographics
                   in settings like social science experiments and public
                   opinion surveys. However, there are currently no established
                   ways to discuss or evaluate the quality of such LLM
                   simulations. Moreover, there is growing concern that these
                   LLM simulations are flattened caricatures of the personas
                   that they aim to simulate, failing to capture the
                   multidimensionality of people and perpetuating stereotypes.
                   To bridge these gaps, we present CoMPosT, a framework to
                   characterize LLM simulations using four dimensions: Context,
                   Model, Persona, and Topic. We use this framework to measure
                   open-ended LLM simulations' susceptibility to caricature,
                   defined via two criteria: individuation and exaggeration. We
                   evaluate the level of caricature in scenarios from existing
                   work on LLM simulations. We find that for GPT-4, simulations
                   of certain demographics (political and marginalized groups)
                   and topics (general, uncontroversial) are highly susceptible
                   to caricature.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Card2022-hi,
  title     = "Computational analysis of 140 years of {US} political speeches
               reveals more positive but increasingly polarized framing of
               immigration",
  author    = "Card, Dallas and Chang, Serina and Becker, Chris and Mendelsohn,
               Julia and Voigt, Rob and Boustan, Leah and Abramitzky, Ran and
               Jurafsky, Dan",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  119,
  number    =  31,
  pages     = "e2120510119",
  abstract  = "We classify and analyze 200,000 US congressional speeches and
               5,000 presidential communications related to immigration from
               1880 to the present. Despite the salience of antiimmigration
               rhetoric today, we find that political speech about immigration
               is now much more positive on average than in the past, with the
               shift largely taking place between World War II and the passage
               of the Immigration and Nationality Act in 1965. However, since
               the late 1970s, political parties have become increasingly
               polarized in their expressed attitudes toward immigration, such
               that Republican speeches today are as negative as the average
               congressional speech was in the 1920s, an era of strict
               immigration quotas. Using an approach based on contextual
               embeddings of text, we find that modern Republicans are
               significantly more likely to use language that is suggestive of
               metaphors long associated with immigration, such as ``animals''
               and ``cargo,'' and make greater use of frames like ``crime'' and
               ``legality.'' The tone of speeches also differs strongly based on
               which nationalities are mentioned, with a striking similarity
               between how Mexican immigrants are framed today and how Chinese
               immigrants were framed during the era of Chinese exclusion in the
               late 19th century. Overall, despite more favorable attitudes
               toward immigrants and the formal elimination of race-based
               restrictions, nationality is still a major factor in how
               immigrants are spoken of in Congress.",
  month     =  aug,
  year      =  2022,
  keywords  = "Congress; dehumanization; framing; immigration; metaphor",
  language  = "en"
}

@ARTICLE{Schafer2022-zm,
  title     = "Computational methods for the analysis of climate change
               communication: Towards an integrative and reflexive approach",
  author    = "Schäfer, Mike S and Hase, Valerie",
  journal   = "Wiley Interdiscip. Rev. Clim. Change",
  publisher = "Wiley",
  abstract  = "Abstract Computational methods, in particular text-as-data or
               Natural Language Processing (NLP) approaches, have become popular
               to study climate change communication as a global and large-scale
               phenomenon. Scholars have discussed opportunities and challenges
               of these methods for climate change communication, with some
               proponents and critics taking strong positions, either embracing
               the potential of computational methods or critically questioning
               their value. Mirroring developments in the broader social
               scientific debate, we aim to bring both sides together by
               proposing a reflexive, integrative approach for computational
               research on climate change communication: We reflect on strengths
               (e.g., making data big and small, nowcasting observations) and
               weaknesses (e.g., introducing empiricist epistemologies, ignoring
               biases) of computational approaches. Moreover, we also provide
               concrete and constructive guidance on when and how to integrate
               (or not integrate) these methods based on theoretical
               considerations. We thereby understand computational methods as
               part of an ever-increasing, diverse toolbox for analyzing climate
               change communication. This article is categorized under: The
               Social Status of Climate Change Knowledge > Knowledge and
               Practice The Social Status of Climate Change Knowledge >
               Sociology/Anthropology of Climate Knowledge",
  month     =  nov,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Agre1995-tp,
  title    = "Computational research on interaction and agency",
  author   = "Agre, Philip E",
  journal  = "Artif. Intell.",
  volume   =  72,
  number   =  1,
  pages    = "1--52",
  abstract = "Recent research in artificial intelligence has developed
              computational theories of agents' involvements in their
              environments. Although inspired by a great diversity of formalisms
              and architectures, these research projects are unified by a common
              concern: using principled characterizations of agents'
              interactions with their environments to guide analysis of living
              agents and design of artificial ones. This article offers a
              conceptual framework for such theories, surveys several other
              fields of research that hold the potential for dialogue with these
              new computational projects, and summarizes the principal
              contributions of the articles in this special double volume. It
              also briefly describes a case study in these ideas—a computer
              program called Toast that acts as a short-order breakfast cook.
              Because its designers have discovered useful structures in the
              world it inhabits, Toast can employ an extremely simple mechanism
              to decide what to do next.",
  month    =  jan,
  year     =  1995
}

@PHDTHESIS{Yang2019-td,
  title  = "Computational Social Roles",
  author = "Yang, Diyi",
  year   =  2019,
  school = "Carnegie Mellon University Pittsburgh, PA, USA"
}

@INPROCEEDINGS{Nass1994-ss,
  title     = "Computers are social actors",
  author    = "Nass, Clifford and Steuer, Jonathan and Tauber, Ellen R",
  booktitle = "Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "72--78",
  abstract  = "… study, subjects used inappropriate social rules in assessing …
               that a different computer with a different voice is a distinct
               social actor . When assessing the praise condition …",
  series    = "CHI '94",
  month     =  apr,
  year      =  1994,
  keywords  = "gender, speech, anthropomorphism, social psychology, voice,
               agents"
}

@ARTICLE{Chung2019-cn,
  title         = "{CONAN} -- {COunter} {NArratives} through Nichesourcing: a
                   Multilingual Dataset of Responses to Fight Online Hate Speech",
  author        = "Chung, Y L and Kuzmenko, E and Tekiroglu, S S and Guerini, M",
  journal       = "arXiv [cs.CL]",
  abstract      = "Although there is an unprecedented effort to provide adequate
                   responses in terms of laws and policies to hate content on
                   social media platforms, dealing with hatred online is still a
                   tough problem. Tackling hate speech in the standard way of
                   content deletion or user suspension may be charged with
                   censorship and overblocking. One alternate strategy, that has
                   received little attention so far by the research community,
                   is to actually oppose hate content with counter-narratives
                   (i.e. informed textual responses). In this paper, we describe
                   the creation of the first large-scale, multilingual,
                   expert-based dataset of hate speech/counter-narrative pairs.
                   This dataset has been built with the effort of more than 100
                   operators from three different NGOs that applied their
                   training and expertise to the task. Together with the
                   collected data we also provide additional annotations about
                   expert demographics, hate and response type, and data
                   augmentation through translation and paraphrasing. Finally,
                   we provide initial experiments to assess the quality of our
                   data.",
  month         =  oct,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Speer2017-bs,
  title    = "{ConceptNet} 5.5: An Open Multilingual Graph of General Knowledge",
  author   = "Speer, Robyn and Chin, Joshua and Havasi, Catherine",
  journal  = "AAAI",
  volume   =  31,
  number   =  1,
  abstract = "Machine learning about language can be improved by supplying it
              with specific knowledge and sources of external information. We
              present here a new version of the linked open data resource
              ConceptNet that is particularly well suited to be used with modern
              NLP techniques such as word embeddings. ConceptNet is a knowledge
              graph that connects words and phrases of natural language with
              labeled edges. Its knowledge is collected from many sources that
              include expert-created resources, crowd-sourcing, and games with a
              purpose. It is designed to represent the general knowledge
              involved in understanding language, improving natural language
              applications by allowing the application to better understand the
              meanings behind the words people use. When ConceptNet is combined
              with word embeddings acquired from distributional semantics (such
              as word2vec), it provides applications with understanding that
              they would not acquire from distributional semantics alone, nor
              from narrower resources such as WordNet or DBPedia. We demonstrate
              this with state-of-the-art results on intrinsic evaluations of
              word relatedness that translate into improvements on applications
              of word vectors, including solving SAT-style analogies.",
  month    =  feb,
  year     =  2017,
  keywords = "ConceptNet; knowledge graph; word embeddings",
  language = "en"
}

@ARTICLE{Lakoff1980-ia,
  title     = "Conceptual Metaphor in Everyday Language",
  author    = "Lakoff, George and Johnson, Mark",
  journal   = "J. Philos.",
  publisher = "Journal of Philosophy, Inc.",
  volume    =  77,
  number    =  8,
  pages     = "453--486",
  year      =  1980
}

@ARTICLE{Khadpe2020-dm,
  title     = "Conceptual Metaphors Impact Perceptions of Human-{AI}
               Collaboration",
  author    = "Khadpe, Pranav and Krishna, Ranjay and Fei-Fei, Li and Hancock,
               Jeffrey T and Bernstein, Michael S",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "dl.acm.org",
  volume    =  4,
  number    = "CSCW2",
  pages     = "1--26",
  month     =  oct,
  year      =  2020
}

@ARTICLE{Dotson2014-ur,
  title     = "Conceptualizing Epistemic Oppression",
  author    = "Dotson, Kristie",
  journal   = "Social Epistemology",
  publisher = "Routledge",
  volume    =  28,
  number    =  2,
  pages     = "115--138",
  abstract  = "Epistemic oppression refers to persistent epistemic exclusion
               that hinders one?s contribution to knowledge production. The
               tendency to shy away from using the term ?epistemic oppression?
               may follow from an assumption that epistemic forms of oppression
               are generally reducible to social and political forms of
               oppression. While I agree that many exclusions that compromise
               one?s ability to contribute to the production of knowledge can be
               reducible to social and political forms of oppression, there
               still exists distinctly irreducible forms of epistemic
               oppression. In this paper, I claim that a major point of
               distinction between reducible and irreducible epistemic
               oppression is the major source of difficulty one faces in
               addressing each kind of oppression, i.e. epistemic power or
               features of epistemological systems. Distinguishing between
               reducible and irreducible forms of epistemic oppression can offer
               a better understanding of what is at stake in deploying the term
               and when such deployment is apt.",
  month     =  apr,
  year      =  2014
}

@ARTICLE{Sui2024-kl,
  title         = "Confabulation: The Surprising Value of Large Language Model
                   Hallucinations",
  author        = "Sui, Peiqi and Duede, Eamon and Wu, Sophie and So, Richard
                   Jean",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper presents a systematic defense of large language
                   model (LLM) hallucinations or 'confabulations' as a potential
                   resource instead of a categorically negative pitfall. The
                   standard view is that confabulations are inherently
                   problematic and AI research should eliminate this flaw. In
                   this paper, we argue and empirically demonstrate that
                   measurable semantic characteristics of LLM confabulations
                   mirror a human propensity to utilize increased narrativity as
                   a cognitive resource for sense-making and communication. In
                   other words, it has potential value. Specifically, we analyze
                   popular hallucination benchmarks and reveal that hallucinated
                   outputs display increased levels of narrativity and semantic
                   coherence relative to veridical outputs. This finding reveals
                   a tension in our usually dismissive understandings of
                   confabulation. It suggests, counter-intuitively, that the
                   tendency for LLMs to confabulate may be intimately associated
                   with a positive capacity for coherent narrative-text
                   generation.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Fung2023-rc,
  title    = "Confucius, cyberpunk and Mr. Science: comparing {AI} ethics
              principles between China and the {EU}",
  author   = "Fung, Pascale and Etienne, Hubert",
  journal  = "AI Ethics",
  volume   =  3,
  number   =  2,
  pages    = "505--511",
  abstract = "We propose a comparative analysis of the AI ethical guidelines
              endorsed by China (from the Chinese National New Generation
              Artificial Intelligence Governance Professional Committee) and by
              the EU (from the European High-level Expert Group on AI). We show
              that behind an apparent likeness in the concepts mobilized, the
              two documents largely differ in their normative approaches, which
              we explain by distinct ambitions resulting from different
              philosophical traditions, cultural heritages and historical
              contexts. In highlighting such differences, we show that it is
              erroneous to believe that a similarity in concepts necessarily
              translates into a similarity in ethics as even the same words may
              have different meanings from a country to another-as exemplified
              by that of ``privacy''. It would, therefore, be erroneous to
              believe that the world would have adopted a common set of ethical
              principles in only three years. China and the EU, however, share a
              common scientific method, inherited in the former from the
              ``Chinese Enlightenment'', which could contribute to better
              collaboration and understanding in the building of technical
              standards for the implementation of such ethics principles.",
  year     =  2023,
  keywords = "AI ethics; China; Cyberpunk; Europe; Regulation",
  language = "en"
}

@ARTICLE{Rashkin2015-te,
  title         = "Connotation Frames: A Data-Driven Investigation",
  author        = "Rashkin, Hannah and Singh, Sameer and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Through a particular choice of a predicate (e.g., ``x
                   violated y''), a writer can subtly connote a range of implied
                   sentiments and presupposed facts about the entities x and y:
                   (1) writer's perspective: projecting x as an
                   ``antagonist''and y as a ``victim'', (2) entities'
                   perspective: y probably dislikes x, (3) effect: something bad
                   happened to y, (4) value: y is something valuable, and (5)
                   mental state: y is distressed by the event. We introduce
                   connotation frames as a representation formalism to organize
                   these rich dimensions of connotation using typed relations.
                   First, we investigate the feasibility of obtaining
                   connotative labels through crowdsourcing experiments. We then
                   present models for predicting the connotation frames of verb
                   predicates based on their distributional word representations
                   and the interplay between different types of connotative
                   relations. Empirical results confirm that connotation frames
                   can be induced from various data sources that reflect how
                   people use language and give rise to the connotative
                   meanings. We conclude with analytical results that show the
                   potential use of connotation frames for analyzing subtle
                   biases in online news media.",
  month         =  jun,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lenharo2023-fy,
  title     = "Consciousness theory slammed as 'pseudoscience' - sparking uproar",
  author    = "Lenharo, Mariana",
  journal   = "Nature",
  publisher = "europepmc.org",
  abstract  = "Consciousness theory slammed as ' pseudoscience ' - sparking
               uproar . - Abstract - Europe PMC … Consciousness theory slammed
               as ' pseudoscience ' - sparking uproar . …",
  month     =  sep,
  year      =  2023,
  keywords  = "Neuroscience; Philosophy; Scientific community",
  language  = "en"
}

@ARTICLE{Bai2022-tg,
  title         = "Constitutional {AI}: Harmlessness from {AI} Feedback",
  author        = "Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and
                   Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen,
                   Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon,
                   Cameron and Chen, Carol and Olsson, Catherine and Olah,
                   Christopher and Hernandez, Danny and Drain, Dawn and Ganguli,
                   Deep and Li, Dustin and Tran-Johnson, Eli and Perez, Ethan
                   and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and
                   Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and
                   Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and
                   Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and
                   Lasenby, Robert and Larson, Robin and Ringer, Sam and
                   Johnston, Scott and Kravec, Shauna and El Showk, Sheer and
                   Fort, Stanislav and Lanham, Tamera and Telleen-Lawton,
                   Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan
                   and Bowman, Samuel R and Hatfield-Dodds, Zac and Mann, Ben
                   and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam
                   and Brown, Tom and Kaplan, Jared",
  journal       = "arXiv [cs.CL]",
  abstract      = "As AI systems become more capable, we would like to enlist
                   their help to supervise other AIs. We experiment with methods
                   for training a harmless AI assistant through
                   self-improvement, without any human labels identifying
                   harmful outputs. The only human oversight is provided through
                   a list of rules or principles, and so we refer to the method
                   as 'Constitutional AI'. The process involves both a
                   supervised learning and a reinforcement learning phase. In
                   the supervised phase we sample from an initial model, then
                   generate self-critiques and revisions, and then finetune the
                   original model on revised responses. In the RL phase, we
                   sample from the finetuned model, use a model to evaluate
                   which of the two samples is better, and then train a
                   preference model from this dataset of AI preferences. We then
                   train with RL using the preference model as the reward
                   signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
                   result we are able to train a harmless but non-evasive AI
                   assistant that engages with harmful queries by explaining its
                   objections to them. Both the SL and RL methods can leverage
                   chain-of-thought style reasoning to improve the human-judged
                   performance and transparency of AI decision making. These
                   methods make it possible to control AI behavior more
                   precisely and with far fewer human labels.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ho2020-fh,
  title         = "Constructing A Multi-hop {QA} Dataset for Comprehensive
                   Evaluation of Reasoning Steps",
  author        = "Ho, Xanh and Nguyen, Anh-Khoa Duong and Sugawara, Saku and
                   Aizawa, Akiko",
  journal       = "arXiv [cs.CL]",
  abstract      = "A multi-hop question answering (QA) dataset aims to test
                   reasoning and inference skills by requiring a model to read
                   multiple paragraphs to answer a given question. However,
                   current datasets do not provide a complete explanation for
                   the reasoning process from the question to the answer.
                   Further, previous studies revealed that many examples in
                   existing multi-hop datasets do not require multi-hop
                   reasoning to answer a question. In this study, we present a
                   new multi-hop QA dataset, called 2WikiMultiHopQA, which uses
                   structured and unstructured data. In our dataset, we
                   introduce the evidence information containing a reasoning
                   path for multi-hop questions. The evidence information has
                   two benefits: (i) providing a comprehensive explanation for
                   predictions and (ii) evaluating the reasoning skills of a
                   model. We carefully design a pipeline and a set of templates
                   when generating a question-answer pair that guarantees the
                   multi-hop steps and the quality of the questions. We also
                   exploit the structured format in Wikidata and use logical
                   rules to create questions that are natural but still require
                   multi-hop reasoning. Through experiments, we demonstrate that
                   our dataset is challenging for multi-hop models and it
                   ensures that multi-hop reasoning is required.",
  month         =  nov,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Barta2021-km,
  title     = "Constructing Authenticity on {TikTok}: Social Norms and Social
               Support on the ``Fun'' Platform",
  author    = "Barta, Kristen and Andalibi, Nazanin",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  5,
  number    = "CSCW2",
  pages     = "1--29",
  abstract  = "Authenticity, generally regarded as coherence between one's inner
               self and outward behavior, is associated with myriad social
               values (e.g., integrity) and beneficial outcomes, such as
               psychological well-being. Scholarship suggests, however, that
               behaving authentically online is complicated by self-presentation
               norms that make it difficult to present a complex self as well as
               encourage sharing positive emotions and facets of self and
               discourage sharing difficult emotions. In this paper, we position
               authenticity as a self-presentation norm and identify the
               sociomaterial factors that contribute to the learning, enactment,
               and enforcement of authenticity on the short-video sharing
               platform TikTok. We draw on interviews with 15 U.S. TikTok users
               to argue that normative authenticity and understanding of TikTok
               as a ``fun'' platform are mutually constitutive in supporting a
               ``just be you'' attitude on TikTok that in turn normalizes
               expressions of both positive and difficult emotions and
               experiences. We consider the social context of TikTok and use an
               affordance lens to identify anonymity, of oneself and one's
               audience; association between content and the ``For You'' landing
               page; and video modality of TikTok as factors informing
               authenticity as a self-presentation norm. We argue that these
               factors similarly contribute to TikTok's viability as a space for
               social support exchange and address the utility of the comments
               section as a site for both supportive communication and norm
               judgment and enforcement. We conclude by considering the
               limitations of authenticity as social norm and present
               implications for designing online spaces for social support and
               connection.",
  month     =  oct,
  year      =  2021,
  keywords  = "authenticity, social media, affordances, social support,
               disclosure, self-presentation, social norms"
}

@ARTICLE{Douek2022-gs,
  title    = "Content Moderation as Systems Thinking",
  author   = "Douek, Evelyn",
  journal  = "https://papers.ssrn.com › sol3 › papershttps://papers.ssrn.com ›
              sol3 › papers",
  abstract = "The stylized picture of content moderation that forms the basis
              for most regulatory and academic discussion of online speech
              governance is misleading and incomplete. That picture depicts
              content moderation as a rough online analogue of offline judicial
              adjudication of speech rights, with legislative-style substantive
              rules being applied over and over again to individual pieces of
              content by a hierarchical bureaucracy of moderators. This
              understanding leads regulators and scholars to assume that the
              best way to make platforms accountable for their decisions about
              online speech is to ensure platforms provide users the kind of ex
              post individual review provided by courts in First Amendment cases
              and guarantee users with ever-more due process rights. But because
              the scale and speed of online speech means content moderation
              cannot be understood as simply the aggregation of many (many!)
              individual adjudications, what this approach produces is
              accountability theatre rather than actual accountability. This
              Article argues that content moderation should instead be
              understood as a project of mass speech administration and that
              looking past a post-by-post evaluation of platform decision-making
              reveals a complex and dynamic system that need a more proactive
              and continuous form of governance than the vehicle of individual
              error correction allows. Lawmakers need to embrace a second wave
              of regulatory thinking about content moderation institutional
              design that eschews comforting but illusory First Amendment-style
              analogies and looks to structural and procedural mechanisms that
              target the key ex ante and systemic decisionmaking that occurs
              upstream of any individual case.",
  month    =  jan,
  year     =  2022,
  keywords = "content moderation, first amendment, platform regulation, online
              speech regulation, free speech, due process"
}

@ARTICLE{Gerrard2020-dw,
  title     = "Content moderation: Social media’s sexist assemblages",
  author    = "Gerrard, Ysabel and Thornham, Helen",
  journal   = "New Media \& Society",
  publisher = "SAGE Publications",
  volume    =  22,
  number    =  7,
  pages     = "1266--1286",
  abstract  = "This article proposes ?sexist assemblages? as a way of
               understanding how the human and mechanical elements that make up
               social media content moderation assemble to perpetuate normative
               gender roles, particularly white femininities, and to police
               content related to women and their bodies. It investigates sexist
               assemblages through three of many potential elements: (1) the
               normatively gendered content presented to users through
               in-platform keyword and hashtag searches; (2) social media
               platforms? community guidelines, which lay out platforms? codes
               of conduct and reveal biases and subjectivities and (3) the
               over-simplification of gender identities that is necessary to
               algorithmically recommend content to users as they move through
               platforms. By the time the reader finds this article, the
               elements of the assemblages we identify might have shifted, but
               we hope the framework remains useful for those aiming to
               understand the relationship between content moderation and
               long-standing forms of inequality.",
  month     =  jul,
  year      =  2020
}

@ARTICLE{Jacovi2021-db,
  title     = "Contrastive explanations for model interpretability",
  author    = "Jacovi, A and Swayamdipta, S and Ravfogel, S and Elazar, Y and
               {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Contrastive explanations clarify why an event occurred in
               contrast to another. They are more inherently intuitive to humans
               to both produce and comprehend. We propose a methodology …",
  year      =  2021
}

@INPROCEEDINGS{Wolfe2023-or,
  title     = "Contrastive Language-Vision {AI} Models Pretrained on Web-Scraped
               Multimodal Data Exhibit Sexual Objectification Bias",
  author    = "Wolfe, Robert and Yang, Yiwei and Howe, Bill and Caliskan, Aylin",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1174--1185",
  abstract  = "Warning: The content of this paper may be upsetting or
               triggering.Nine language-vision AI models trained on web scrapes
               with the Contrastive Language-Image Pretraining (CLIP) objective
               are evaluated for evidence of a bias studied by psychologists:
               the sexual objectification of girls and women, which occurs when
               a person’s human characteristics, such as emotions, are
               disregarded and the person is treated as a body or a collection
               of body parts. We replicate three experiments in the psychology
               literature quantifying sexual objectification and show that the
               phenomena persist in trained AI models. A first experiment uses
               standardized images of women from the Sexual OBjectification and
               EMotion Database, and finds that human characteristics are
               disassociated from images of objectified women: the model’s
               recognition of emotional state is mediated by whether the subject
               is fully or partially clothed. Embedding association tests (EATs)
               return significant effect sizes for both anger (d > 0.80) and
               sadness (d > 0.50), associating images of fully clothed subjects
               with emotions. GRAD-CAM saliency maps highlight that CLIP gets
               distracted from emotional expressions in objectified images where
               subjects are partially clothed. A second experiment measures the
               effect in a representative application: an automatic image
               captioner (Antarctic Captions) includes words denoting emotion
               less than 50\% as often for images of partially clothed women
               than for images of fully clothed women. A third experiment finds
               that images of female professionals (scientists, doctors,
               executives) are likely to be associated with sexual descriptions
               relative to images of male professionals. A fourth experiment
               shows that a prompt of ``a [age] year old girl'' generates
               sexualized images (as determined by an NSFW classifier) up to
               73\% of the time for VQGAN-CLIP (age 17), and up to 42\% of the
               time for Stable Diffusion (ages 14 and 18); the corresponding
               rate for boys never surpasses 9\%. The evidence indicates that
               language-vision AI models trained on automatically collected web
               scrapes learn biases of sexual objectification, which propagate
               to downstream applications.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "gender bias, AI bias, generative AI, AI bias propagation,
               text-to-image generators, representation learning, AI bias in
               applications, sexualization, language-vision AI"
}

@ARTICLE{Chen2022-wo,
  title         = "Controllable Text Generation with Language Constraints",
  author        = "Chen, Howard and Li, Huihan and Chen, Danqi and Narasimhan,
                   Karthik",
  journal       = "arXiv [cs.CL]",
  abstract      = "We consider the task of text generation in language models
                   with constraints specified in natural language. To this end,
                   we first create a challenging benchmark Cognac that provides
                   as input to the model a topic with example text, along with a
                   constraint on text to be avoided. Unlike prior work, our
                   benchmark contains knowledge-intensive constraints sourced
                   from databases like Wordnet and Wikidata, which allows for
                   straightforward evaluation while striking a balance between
                   broad attribute-level and narrow lexical-level controls. We
                   find that even state-of-the-art language models like GPT-3
                   fail often on this task, and propose a solution to leverage a
                   language model's own internal knowledge to guide generation.
                   Our method, called CognacGen, first queries the language
                   model to generate guidance terms for a specified topic or
                   constraint, and uses the guidance to modify the model's token
                   generation probabilities. We propose three forms of guidance
                   (binary verifier, top-k tokens, textual example), and employ
                   prefix-tuning approaches to distill the guidance to tackle
                   diverse natural language constraints. Through extensive
                   empirical evaluations, we demonstrate that CognacGen can
                   successfully generalize to unseen instructions and outperform
                   competitive baselines in generating constraint conforming
                   text.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Fiske1993-qj,
  title     = "Controlling other people. The impact of power on stereotyping",
  author    = "Fiske, S T",
  journal   = "Am. Psychol.",
  publisher = "psycnet.apa.org",
  volume    =  48,
  number    =  6,
  pages     = "621--628",
  abstract  = "This article presents a theory of the mutually reinforcing
               interaction between power and stereotyping, mediated by
               attention. The powerless attend to the powerful who control their
               outcomes, in an effort to enhance prediction and control, so
               forming complex, potentially nonstereotypic impressions. The
               powerful pay less attention, so are more vulnerable to
               stereotyping. The powerful (a) need not attend to the other to
               control their own outcomes, (b) cannot attend because they tend
               to be attentionally overloaded, and (c) if they have high need
               for dominance, may not want to attend. Stereotyping and power are
               mutually reinforcing because stereotyping itself exerts control,
               maintaining and justifying the status quo. Two legal cases and a
               body of research illustrate the theory and suggest organizational
               change strategies.",
  month     =  jun,
  year      =  1993,
  language  = "en"
}

@INPROCEEDINGS{Celis2019-sl,
  title     = "Controlling Polarization in Personalization: An Algorithmic
               Framework",
  author    = "Celis, L Elisa and Kapoor, Sayash and Salehi, Farnood and
               Vishnoi, Nisheeth",
  booktitle = "Proceedings of the Conference on Fairness, Accountability, and
               Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "160--169",
  abstract  = "Personalization is pervasive in the online space as it leads to
               higher efficiency for the user and higher revenue for the
               platform by individualizing the most relevant content for each
               user. However, recent studies suggest that such personalization
               can learn and propagate systemic biases and polarize opinions;
               this has led to calls for regulatory mechanisms and algorithms
               that are constrained to combat bias and the resulting
               echo-chamber effect. We propose a versatile framework that allows
               for the possibility to reduce polarization in personalized
               systems by allowing the user to constrain the distribution from
               which content is selected. We then present a scalable algorithm
               with provable guarantees that satisfies the given constraints on
               the types of the content that can be displayed to a user, but --
               subject to these constraints -- will continue to learn and
               personalize the content in order to maximize utility. We
               illustrate this framework on a curated dataset of online news
               articles that are conservative or liberal, show that it can
               control polarization, and examine the trade-off between
               decreasing polarization and the resulting loss to revenue. We
               further exhibit the flexibility and scalability of our approach
               by framing the problem in terms of the more general diverse
               content selection problem and test it empirically on both a News
               dataset and the MovieLens dataset.",
  series    = "FAT* '19",
  month     =  jan,
  year      =  2019,
  keywords  = "group fairness, diversification, bandit optimization,
               Personalization, recommender systems, polarization"
}

@ARTICLE{Wang2023-gi,
  title         = "{ControversialQA}: Exploring Controversy in Question
                   Answering",
  author        = "Wang, Zhen and Zhu, Peide and Yang, Jie",
  journal       = "arXiv [cs.CL]",
  abstract      = "Controversy is widespread online. Previous studies mainly
                   define controversy based on vague assumptions of its relation
                   to sentiment such as hate speech and offensive words. This
                   paper introduces the first question-answering dataset that
                   defines content controversy by user perception, i.e., votes
                   from plenty of users. It contains nearly 10K questions, and
                   each question has a best answer and a most controversial
                   answer. Experimental results reveal that controversy
                   detection in question answering is essential and challenging,
                   and there is no strong correlation between controversy and
                   sentiment tasks.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Zhang2018-ey,
  title         = "Conversations Gone Awry: Detecting Early Signs of
                   Conversational Failure",
  author        = "Zhang, Justine and Chang, Jonathan P and
                   Danescu-Niculescu-Mizil, Cristian and Dixon, Lucas and Hua,
                   Yiqing and Thain, Nithum and Taraborelli, Dario",
  journal       = "arXiv [cs.CL]",
  abstract      = "One of the main challenges online social systems face is the
                   prevalence of antisocial behavior, such as harassment and
                   personal attacks. In this work, we introduce the task of
                   predicting from the very start of a conversation whether it
                   will get out of hand. As opposed to detecting undesirable
                   behavior after the fact, this task aims to enable early,
                   actionable prediction at a time when the conversation might
                   still be salvaged. To this end, we develop a framework for
                   capturing pragmatic devices---such as politeness strategies
                   and rhetorical prompts---used to start a conversation, and
                   analyze their relation to its future trajectory. Applying
                   this framework in a controlled setting, we demonstrate the
                   feasibility of detecting early warning signs of antisocial
                   behavior in online discussions.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Shannon2015-tg,
  title     = "Correlates, causes, and consequences of fat talk: A review",
  author    = "Shannon, Amy and Mills, Jennifer S",
  journal   = "Body Image",
  publisher = "Elsevier",
  volume    =  15,
  pages     = "158--172",
  abstract  = "Fat talk is a term used to describe self-disparaging remarks made
               to other people about one's weight or body. Fat talk has been
               both causally and correlationally linked to a number of negative
               body image-related variables including low body esteem, body
               dissatisfaction, drive for thinness, body-related cognitive
               distortions, and perceived sociocultural pressure to be thin. As
               such, body image researchers and clinicians would benefit from
               increased awareness of the current literature concerning fat
               talk. A narrative synthesis approach is used to summarize all
               research containing the keywords fat talk, body talk, or weight
               talk that was published from 1994 to 2014 inclusive. The measures
               used to study fat talk, outcomes and correlates associated with
               fat talk, theories that may help explain these findings, and the
               purpose served by fat talk are reviewed and discussed. In
               addition, directions for future research on fat talk, including
               intervention strategies, are examined.",
  month     =  sep,
  year      =  2015,
  keywords  = "Appearance talk; Body talk; Fat talk; Fat talk interventions; Fat
               talk measures; Weight talk",
  language  = "en"
}

@ARTICLE{Ghosh2023-xk,
  title         = "{CoSyn}: Detecting Implicit Hate Speech in Online
                   Conversations Using a Context Synergized Hyperbolic Network",
  author        = "Ghosh, Sreyan and Suri, Manan and Chiniya, Purva and Tyagi,
                   Utkarsh and Kumar, Sonal and Manocha, Dinesh",
  journal       = "arXiv [cs.LG]",
  abstract      = "The tremendous growth of social media users interacting in
                   online conversations has led to significant growth in hate
                   speech, affecting people from various demographics. Most of
                   the prior works focus on detecting explicit hate speech,
                   which is overt and leverages hateful phrases, with very
                   little work focusing on detecting hate speech that is
                   implicit or denotes hatred through indirect or coded
                   language. In this paper, we present CoSyn, a
                   context-synergized neural network that explicitly
                   incorporates user- and conversational context for detecting
                   implicit hate speech in online conversations. CoSyn
                   introduces novel ways to encode these external contexts and
                   employs a novel context interaction mechanism that clearly
                   captures the interplay between them, making independent
                   assessments of the amounts of information to be retrieved
                   from these noisy contexts. Additionally, it carries out all
                   these operations in the hyperbolic space to account for the
                   scale-free dynamics of social media. We demonstrate the
                   effectiveness of CoSyn on 6 hate speech datasets and show
                   that CoSyn outperforms all our baselines in detecting
                   implicit hate speech with absolute improvements in the range
                   of 1.24\% - 57.8\%.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Garland_undated-mw,
  title  = "Countering hate on social media: Large-scale classification of hate
            and counter speech",
  author = "Garland, Joshua and Ghazi-Zahedi, Keyan"
}

@ARTICLE{Mun2024-tq,
  title         = "Counterspeakers' Perspectives: Unveiling Barriers and {AI}
                   Needs in the Fight against Online Hate",
  author        = "Mun, Jimin and Buerger, Cathy and Liang, Jenny T and Garland,
                   Joshua and Sap, Maarten",
  journal       = "arXiv [cs.HC]",
  abstract      = "Counterspeech, i.e., direct responses against hate speech,
                   has become an important tool to address the increasing amount
                   of hate online while avoiding censorship. Although AI has
                   been proposed to help scale up counterspeech efforts, this
                   raises questions of how exactly AI could assist in this
                   process, since counterspeech is a deeply empathetic and
                   agentic process for those involved. In this work, we aim to
                   answer this question, by conducting in-depth interviews with
                   10 extensively experienced counterspeakers and a large scale
                   public survey with 342 everyday social media users. In
                   participant responses, we identified four main types of
                   barriers and AI needs related to resources, training, impact,
                   and personal harms. However, our results also revealed
                   overarching concerns of authenticity, agency, and
                   functionality in using AI tools for counterspeech. To
                   conclude, we discuss considerations for designing AI
                   assistants that lower counterspeaking barriers without
                   jeopardizing its meaning and purpose.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@INPROCEEDINGS{Ogbonnaya-Ogburu2020-yv,
  title     = "Critical Race Theory for {HCI}",
  author    = "Ogbonnaya-Ogburu, Ihudiya Finda and Smith, Angela D R and To,
               Alexandra and Toyama, Kentaro",
  booktitle = "Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--16",
  abstract  = "The human-computer interaction community has made some efforts
               toward racial diversity, but the outcomes remain meager. We
               introduce critical race theory and adapt it for HCI to lay a
               theoretical basis for race-conscious efforts, both in research
               and within our community. Building on the theory's original
               tenets, we argue that racism is pervasive in everyday
               socio-technical systems; that the HCI community is prone to
               ``interest convergence'', where concessions to inclusion require
               benefits to those in power; and that the neoliberal underpinnings
               of the technology industry itself propagate racism. Critical race
               theory uses storytelling as a means to upend deep-seated
               assumptions, and we relate several personal stories to highlight
               ongoing problems of race in HCI. The implications: all HCI
               research must be attuned to issues of race; participation of
               underrepresented minorities must be sought in all of our
               activities; and as a community, we cannot become comfortable
               while racial disparities exist.",
  series    = "CHI '20",
  month     =  apr,
  year      =  2020,
  keywords  = "critical race theory, race, racism, storytelling, theory"
}

@MISC{noauthor_undated-qr,
  title = "cscw\_2023\_7cups\_care\_\_3\_.pdf"
}

@INCOLLECTION{Stevens2018-pk,
  title     = "Cultural Competence: A Form of Stereotype Rationality",
  author    = "Stevens, Sean T and Jussim, Lee and Stevens, Lillian A and
               Anglin, Stephanie M",
  editor    = "Frisby, Craig L and O'Donohue, William T",
  booktitle = "Cultural Competence in Applied Psychology: An Evaluation of
               Current Status and Future Directions",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "651--664",
  abstract  = "Increased diversity within a society inevitably increases
               interactions between people of different cultural backgrounds. In
               this chapter, we contend that arguments calling for increased
               cultural competence are synonymous with calls for increased
               accuracy in social perception. Evidence that individuals are more
               accurate in their beliefs about other groups than social
               psychology has typically given them credit for is reviewed and
               how to judiciously apply this knowledge is discussed. Potential
               obstacles to increased cultural competence are also identified,
               such as intergroup conflict, particularly in the political
               domain. We conclude with recommendations for promoting cultural
               competence by encouraging a sensitivity to group and individual
               differences.",
  year      =  2018
}

@ARTICLE{Seo2020-bb,
  title     = "Cultural globalization and young Korean women’s acculturative
               labor: {K}-beauty as hegemonic hybridity",
  author    = "Seo, Yuri and Cruz, Angela Gracia B and Fifita, ‘ilaisaane M E",
  journal   = "Eur. Leg. Towar. New Paradig.",
  publisher = "SAGE Publications",
  volume    =  23,
  number    =  4,
  pages     = "600--618",
  abstract  = "This article aims to understand how young Korean women respond to
               the changing ideals of K-beauty, a form of gender imagery
               embodied by Korean pop celebrities, when such ideals become
               exported as global cultural products. The findings reveal that
               K-beauty is characterized by three paradoxical themes:
               manufactured naturalness, hyper-sexualized cuteness, and the
               ‘harmonious kaleidoscope’. When we unravel these paradoxes
               further, we observe that they provoke unsettlement and
               ambivalence among young Korean women, who shed light on the
               acculturative labors of concealment, selective resistance, and
               compliance that permeate the field of K-beauty. We argue that
               through these new layers of women’s work, the paradoxes in beauty
               are re-domesticated, the globalizing Western dictates are brought
               into alignment with neo-Confucian cultural ideology, and a new
               hybridized hegemonic regime of feminine beauty becomes
               established.",
  month     =  jul,
  year      =  2020,
  language  = "en"
}

@INCOLLECTION{Trimble2003-ty,
  title     = "Cultural Sensitivity and Cultural Competence",
  author    = "Trimble, Joseph E",
  editor    = "Prinstein, Mitchell J and Patterson, Marcus D",
  booktitle = "The Portable Mentor: Expert Guide to a Successful Career in
               Psychology",
  publisher = "Springer US",
  address   = "Boston, MA",
  pages     = "13--32",
  abstract  = "The two quotations from the well-known cultural anthropologists
               capture a part of the experience of what it means when one
               chooses to study and work with people from different cultural and
               ethnic groups. In her quote, the esteemed anthropologist,
               Margaret Mead referred to the “psychological” as innate, generic
               characteristics of the mind while the “cultural” referred to the
               behavior that one learned in her or his culture. Mead’s quote
               refers to her experiences as a student with Columbia University’s
               cultural anthropologist William Fielding Ogburn. In the 1920s,
               Ogburn maintained that the study of human kind lacked any useful
               comprehensive psychological theory. Then, and up to recently,
               psychologists were not at all interested in cultural explanations
               or explorations of human affect, behavior, and cognition. In
               fact, most psychologists then firmly believed that “all humans
               were alike” hence the need to identify and study cultural
               correlates exceeded what was sufficient to understand the sum
               total of the conscious and unconscious events that make up an
               individual’s life. Although many behavioral social scientists
               would disagree with Ogbura’s contention I take the position that
               before anyone can begin to apply conventional psychological
               principles and theories to an ethnic or cultural group, they must
               understand their unique lifeways and thoughtways.",
  year      =  2003
}

@ARTICLE{Semin2010-nf,
  title     = "Culturally situated linguistic ecologies and language use",
  author    = "Semin, Günr",
  journal   = "Advances in Culture and Psychology: Volume 1",
  publisher = "Oxford University Press",
  pages     =  217,
  year      =  2010
}

@ARTICLE{Xian_Ng2024-qi,
  title     = "Cyborgs for strategic communication on social media",
  author    = "Xian Ng, Lynnette Hui and Robertson, Dawn C and Carley, Kathleen
               M",
  journal   = "arXiv e-prints",
  publisher = "ui.adsabs.harvard.edu",
  pages     = "arXiv:2401.06582",
  abstract  = "Social media platforms are a key ground of information
               consumption and dissemination. Key figures like politicians,
               celebrities and activists have leveraged on its wide user base
               for strategic communication. Strategic communications, or
               StratCom, is the deliberate act of information creation and
               distribution. Its techniques are used by these key figures for
               establishing their brand and amplifying their messages. Automated
               scripts are used on top of personal touches to quickly and
               effectively perform these tasks. The combination of automation
               and manual online posting creates a Cyborg social media profile,
               which is a hybrid between bot and human. In this study, we
               establish a quantitative definition for a Cyborg account, which
               is an account that are detected as bots in one time window, and
               identified as humans in another. This definition makes use of
               frequent changes of bot classification labels and large
               differences in bot likelihood scores to identify Cyborgs. We
               perform a large-scale analysis across over 3.1 million users from
               Twitter collected from two key events, the 2020 Coronavirus
               pandemic and 2020 US Elections. We extract Cyborgs from two
               datasets and employ tools from network science, natural language
               processing and manual annotation to characterize Cyborg accounts.
               Our analyses identify Cyborg accounts are mostly constructed for
               strategic communication uses, have a strong duality in their
               bot/human classification and are tactically positioned in the
               social media network, aiding these accounts to promote their
               desired content. Cyborgs are also discovered to have long online
               lives, indicating their ability to evade bot detectors, or the
               graciousness of platforms to allow their operations.",
  month     =  jan,
  year      =  2024,
  keywords  = "Computer Science - Social and Information Networks"
}

@ARTICLE{Liu2023-ve,
  title         = "{DADA}: Dialect Adaptation via Dynamic Aggregation of
                   Linguistic Rules",
  author        = "Liu, Yanchen and Held, William and Yang, Diyi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Existing large language models (LLMs) that mainly focus on
                   Standard American English (SAE) often lead to significantly
                   worse performance when being applied to other English
                   dialects. While existing mitigations tackle discrepancies for
                   individual target dialects, they assume access to
                   high-accuracy dialect identification systems. The boundaries
                   between dialects are inherently flexible, making it difficult
                   to categorize language into discrete predefined categories.
                   In this paper, we propose DADA (Dialect Adaptation via
                   Dynamic Aggregation), a modular approach to imbue SAE-trained
                   models with multi-dialectal robustness by composing adapters
                   which handle specific linguistic features. The compositional
                   architecture of DADA allows for both targeted adaptation to
                   specific dialect variants and simultaneous adaptation to
                   various dialects. We show that DADA is effective for both
                   single task and instruction finetuned language models,
                   offering an extensible and interpretable framework for
                   adapting existing LLMs to different English dialects.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Held2022-fj,
  title         = "{DAMP}: Doubly Aligned Multilingual Parser for Task-Oriented
                   Dialogue",
  author        = "Held, William and Hidey, Christopher and Liu, Fei and Zhu,
                   Eric and Goel, Rahul and Yang, Diyi and Shah, Rushin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Modern virtual assistants use internal semantic parsing
                   engines to convert user utterances to actionable commands.
                   However, prior work has demonstrated that semantic parsing is
                   a difficult multilingual transfer task with low transfer
                   efficiency compared to other tasks. In global markets such as
                   India and Latin America, this is a critical issue as
                   switching between languages is prevalent for bilingual users.
                   In this work we dramatically improve the zero-shot
                   performance of a multilingual and codeswitched semantic
                   parsing system using two stages of multilingual alignment.
                   First, we show that constrastive alignment pretraining
                   improves both English performance and transfer efficiency. We
                   then introduce a constrained optimization approach for
                   hyperparameter-free adversarial alignment during finetuning.
                   Our Doubly Aligned Multilingual Parser (DAMP) improves mBERT
                   transfer performance by 3x, 6x, and 81x on the Spanglish,
                   Hinglish and Multilingual Task Oriented Parsing benchmarks
                   respectively and outperforms XLM-R and mT5-Large using 3.2x
                   fewer parameters.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Medina_Serrano2020-lh,
  title     = "Dancing to the Partisan Beat: A First Analysis of Political
               Communication on {TikTok}",
  author    = "Medina Serrano, Juan Carlos and Papakyriakopoulos, Orestis and
               Hegelich, Simon",
  booktitle = "Proceedings of the 12th ACM Conference on Web Science",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "257--266",
  abstract  = "TikTok is a video-sharing social networking service, whose
               popularity is increasing rapidly. It was the world’s second-most
               downloaded app in 2019. Although the platform is known for having
               users posting videos of themselves dancing, lip-syncing, or
               showcasing other talents, user-videos expressing political views
               have seen a recent spurt. This study aims to perform a primary
               evaluation of political communication on TikTok. We collect a set
               of US partisan Republican and Democratic videos to investigate
               how users communicated with each other about political issues.
               With the help of computer vision, natural language processing,
               and statistical tools, we illustrate that political communication
               on TikTok is much more interactive in comparison to other social
               media platforms, with users combining multiple information
               channels to spread their messages. We show that political
               communication takes place in the form of communication trees
               since users generate branches of responses to existing content.
               In terms of user demographics, we find that users belonging to
               both the US parties are young and behave similarly on the
               platform. However, Republican users generated more political
               content and their videos received more responses; on the other
               hand, Democratic users engaged significantly more in
               cross-partisan discussions.",
  series    = "WebSci '20",
  month     =  jul,
  year      =  2020,
  keywords  = "political communication, social media, TikTok, US politics"
}

@INPROCEEDINGS{Ajmani2024-zs,
  title     = "Data Agency Theory: A Precise Theory of Justice for {AI}
               Applications",
  author    = "Ajmani, Leah and Stapleton, Logan and Houtti, Mo and Chancellor,
               Stevie",
  booktitle = "Proceedings of the 2024 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "631--641",
  abstract  = "Data collection methods for AI applications have been heavily
               scrutinized by researchers, policymakers, and the general public.
               In this paper, we propose data agency theory (DAT), a precise
               theory of justice to evaluate and improve current consent
               procedures used in AI applications. We argue that data agency is
               systematically defined by consent policies. Therefore, data
               agency is a matter of justice. DAT claims data agency ought to be
               afforded in a way that minimizes the oppression of data
               contributors by data collectors. We then apply DAT to two salient
               consent procedures in AI applications: Reddit’s Terms of Service
               agreement and the United States’s IRB protocols. Through these
               cases, we demonstrate how our theory helps evaluate justice and
               generate ideas for improvement. Finally, we discuss the
               implications of using justice as an evaluation metric, comparing
               consent procedures, and adopting DAT in future research.",
  series    = "FAccT '24",
  month     =  jun,
  year      =  2024,
  keywords  = "artificial intelligence, consent, empowerment, user-generated
               data"
}

@ARTICLE{Chan2022-os,
  title         = "Data distributional properties drive emergent in-context
                   learning in transformers",
  author        = "Chan, Stephanie C Y and Santoro, Adam and Lampinen, Andrew K
                   and Wang, Jane X and Singh, Aaditya and Richemond, Pierre H
                   and McClelland, Jay and Hill, Felix",
  editor        = "Koyejo, S and Mohamed, S and Agarwal, A and Belgrave, D and
                   Cho, K and Oh, A",
  journal       = "arXiv [cs.LG]",
  pages         = "18878--18891",
  abstract      = "Large transformer-based models are able to perform in-context
                   few-shot learning, without being explicitly trained for it.
                   This observation raises the question: what aspects of the
                   training regime lead to this emergent behavior? Here, we show
                   that this behavior is driven by the distributions of the
                   training data itself. In-context learning emerges when the
                   training data exhibits particular distributional properties
                   such as burstiness (items appear in clusters rather than
                   being uniformly distributed over time) and having large
                   numbers of rarely occurring classes. In-context learning also
                   emerges more strongly when item meanings or interpretations
                   are dynamic rather than fixed. These properties are
                   exemplified by natural language, but are also inherent to
                   naturalistic data in a wide range of other domains. They also
                   depart significantly from the uniform, i.i.d. training
                   distributions typically used for standard supervised
                   learning. In our initial experiments, we found that
                   in-context learning traded off against more conventional
                   weight-based learning, and models were unable to achieve both
                   simultaneously. However, our later experiments uncovered that
                   the two modes of learning could co-exist in a single model
                   when it was trained on data following a skewed Zipfian
                   distribution -- another common property of naturalistic data,
                   including language. In further experiments, we found that
                   naturalistic data distributions were only able to elicit
                   in-context learning in transformers, and not in recurrent
                   models. In sum, our findings indicate how the transformer
                   architecture works together with particular properties of the
                   training data to drive the intriguing emergent in-context
                   learning behaviour of large language models, and how future
                   work might encourage both in-context and in-weights learning
                   in domains beyond language.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Taori2023-dt,
  title     = "Data Feedback Loops: Model-driven Amplification of Dataset Biases",
  author    = "Taori, Rohan and Hashimoto, Tatsunori",
  editor    = "Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and
               Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan",
  booktitle = "Proceedings of the 40th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  202,
  pages     = "33883--33920",
  abstract  = "Datasets scraped from the internet have been critical to
               large-scale machine learning. Yet, its success puts the utility
               of future internet-derived datasets at potential risk, as model
               outputs begin to replace human annotations as a source of
               supervision. In this work, we formalize a system where
               interactions with one model are recorded as history and scraped
               as training data in the future. We then analyze its stability
               over time by tracking changes to a test-time bias statistic (e.g.
               gender bias of model predictions). We find that the degree of
               bias amplification is closely linked to whether the model’s
               outputs behave like samples from the training distribution, a
               behavior which we characterize and define as uniform
               faithfulness. Experiments in three conditional prediction
               scenarios – image classification, visual role-labeling, and
               language generation – demonstrate that models that exhibit a
               sampling-like behavior are more faithful and thus more stable.
               Based on this insight, we propose an intervention to help
               mitigate and stabilize unstable feedback systems.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2023
}

@ARTICLE{Green2021-nm,
  title     = "Data Science as Political Action: Grounding Data Science in a
               Politics of Justice",
  author    = "Green, Ben",
  journal   = "Journal of Social Computing",
  publisher = "TUP",
  volume    =  2,
  number    =  3,
  pages     = "249--265",
  abstract  = "In response to public scrutiny of data-driven algorithms, the
               field of data science has adopted ethics training and principles.
               Although ethics can help data scientists reflect on certain
               normative aspects of their work, such efforts are ill-equipped to
               generate a data science that avoids social harms and promotes
               social justice. In this article, I argue that data science must
               embrace a political orientation. Data scientists must recognize
               themselves as political actors engaged in normative constructions
               of society and evaluate their work according to its downstream
               impacts on people's lives. I first articulate why data scientists
               must recognize themselves as political actors. In this section, I
               respond to three arguments that data scientists commonly invoke
               when challenged to take political positions regarding their work.
               In confronting these arguments, I describe why attempting to
               remain apolitical is itself a political stance—a fundamentally
               conservative one—and why data science's attempts to promote
               “social good” dangerously rely on unarticulated and
               incrementalist political assumptions. I then propose a framework
               for how data science can evolve toward a deliberative and
               rigorous politics of social justice. I conceptualize the process
               of developing a politically engaged data science as a sequence of
               four stages. Pursuing these new approaches will empower data
               scientists with new methods for thoughtfully and rigorously
               contributing to social justice.",
  month     =  sep,
  year      =  2021
}

@MISC{noauthor_undated-ll,
  title = "{DD\_paper\_PNAS}.pdf"
}

@ARTICLE{Davani2022-ap,
  title     = "Dealing with disagreements: Looking beyond the majority vote in
               subjective annotations",
  author    = "Davani, A M and Díaz, M and Prabhakaran, V",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "direct.mit.edu",
  abstract  = "Majority voting and averaging are common approaches used to
               resolve annotator disagreements and derive single ground truth
               labels from multiple annotations. However, annotators may
               systematically disagree with one another, often reflecting their
               individual biases and values, especially in the case of
               subjective tasks such as detecting affect, aggression, and hate
               speech. Annotator disagreements may capture important nuances in
               such tasks that are often ignored while aggregating annotations
               to a single ground truth. In …",
  year      =  2022
}

@ARTICLE{Mendelson2021-px,
  title     = "Debiasing methods in natural language understanding make bias
               more accessible",
  author    = "Mendelson, M and Belinkov, Y",
  journal   = "arXiv preprint arXiv:2109.04095",
  publisher = "arxiv.org",
  abstract  = "Model robustness to bias is often determined by the
               generalization on carefully designed out- of-distribution
               datasets. Recent debiasing methods in natural language
               understanding …",
  year      =  2021
}

@ARTICLE{Andrew_Imbrie_Owen_J_Daniels_Helen_Toner2021-hc,
  title     = "Decoding Intentions Artificial Intelligence and Costly Signals",
  author    = "{Andrew Imbrie Owen J. Daniels Helen Toner}",
  journal   = "Wash. Q.",
  publisher = "Routledge",
  volume    =  44,
  number    =  2,
  pages     = "159--180",
  month     =  apr,
  year      =  2021
}

@ARTICLE{Bird2020-rn,
  title     = "Decolonising speech and language technology",
  author    = "Bird, S",
  journal   = "Proceedings of the 28th international conference on",
  publisher = "aclanthology.org",
  abstract  = "After generations of exploitation, Indigenous people often
               respond negatively to the idea that their languages are data
               ready for the taking. By treating Indigenous knowledge as a …",
  year      =  2020
}

@INCOLLECTION{Malson2008-qx,
  title     = "Deconstructing Un/Healthy Body-weight and Weight Management",
  author    = "Malson, Helen",
  editor    = "Riley, Sarah and Burns, Maree and Frith, Hannah and Wiggins,
               Sally and Markula, Pirkko",
  booktitle = "Critical Bodies: Representations, Identities and Practices of
               Weight and Body Management",
  publisher = "Palgrave Macmillan UK",
  address   = "London",
  pages     = "27--42",
  abstract  = "The practices of body-weight management and the normative
               corporeal ideals embedded therein occupy a pivotal place in the
               discursive regulation of bodies in contemporary Western/global
               cultures (Bordo, 1993). As numerous authors (Bordo, 1993; Burns,
               2004; Malson, 1998; Orbach, 1993) have illustrated, the
               corpo/hyper-real ideal of thinness that is inscribed upon female
               bodies can be understood as one of the principal ‘conditions of
               possibility’ (Foucault, 1972) of those subjectivities and
               practices of weight management that might be termed ‘eating
               disordered’. From a feminist post-structuralist perspective it is
               the analyses of these conditions of possibility, the multiple and
               shifting production and regulation of our always-already
               discursively constituted embodied subjectivities that are crucial
               in understanding both pathologised and normalised experiences of
               body-weight and practices of body management. As outlined below,
               a growing body of critical feminist analyses have already
               elucidated how fat and thin bodies are saturated with a
               multiplicity of gendered meanings and moral connotations, and
               have identified a considerable array of discursive contexts which
               are constitutive of ‘anorexic’ subjectivities, bodies and
               body-management practices (e.g. Bordo, 1993; Malson, 1998, 1999;
               Probyn, 1987). In doing so, these analyses have thereby exposed
               the profoundly regulatory and gendered operations of discourse
               upon the body, elucidated its multiple and socio-historically
               shifting meanings, deconstructed the seemingly categorical
               division between the normal and the pathological, and thus
               located pathologised disorders of eating firmly in the context of
               normative orders of body-weight and its management.",
  year      =  2008
}

@ARTICLE{Choi2021-tx,
  title     = "Decontextualization: Making sentences stand-alone",
  author    = "Choi, Eunsol and Palomaki, Jennimaria and Lamm, Matthew and
               Kwiatkowski, Tom and Das, Dipanjan and Collins, Michael",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "MIT Press - Journals",
  volume    =  9,
  pages     = "447--461",
  abstract  = "Abstract Models for question answering, dialogue agents, and
               summarization often interpret the meaning of a sentence in a rich
               context and use that meaning in a new context. Taking excerpts of
               text can be problematic, as key pieces may not be explicit in a
               local window. We isolate and define the problem of sentence
               decontextualization: taking a sentence together with its context
               and rewriting it to be interpretable out of context, while
               preserving its meaning. We describe an annotation procedure,
               collect data on the Wikipedia corpus, and use the data to train
               models to automatically decontextualize sentences. We present
               preliminary studies that show the value of sentence
               decontextualization in a user-facing task, and as preprocessing
               for systems that perform document understanding. We argue that
               decontextualization is an important subtask in many downstream
               applications, and that the definitions and resources provided can
               benefit tasks that operate on sentences that occur in a richer
               context.",
  month     =  apr,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Melewar2002-xz,
  title     = "Defining the corporate identity construct",
  author    = "Melewar, T C and Jenkins, Elizabeth",
  journal   = "Corp. Reput. Rev.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  5,
  number    =  1,
  pages     = "76--90",
  abstract  = "Both practitioners and academics alike have directed increasing
               attention to the field of corporate identity. Despite significant
               contributions in the last several years towards understanding and
               identifying this concept, a definitive construct of corporate
               identity and its measurements does not yet exist. Much anecdotal
               literature and many case studies surround this area of study, but
               to date no research study has empirically tested the domain of
               this construct. This paper examines the definitions, models, and
               specific elements of corporate identity through a review of
               literature. Based on this review, a holistic corporate identity
               model is developed. This paper also discusses the challenges in
               developing the corporate identity construct.",
  month     =  apr,
  year      =  2002,
  language  = "en"
}

@ARTICLE{Author_undated-gx,
  title  = "Dehumanizing Machines: Countering the Default of Anthropomorphism in
            {LLMs}",
  author = "Author, Anonymous"
}

@ARTICLE{Wan2024-uk,
  title         = "{DELL}: Generating Reactions and Explanations for {LLM}-Based
                   Misinformation Detection",
  author        = "Wan, Herun and Feng, Shangbin and Tan, Zhaoxuan and Wang,
                   Heng and Tsvetkov, Yulia and Luo, Minnan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models are limited by challenges in factuality
                   and hallucinations to be directly employed off-the-shelf for
                   judging the veracity of news articles, where factual accuracy
                   is paramount. In this work, we propose DELL that identifies
                   three key stages in misinformation detection where LLMs could
                   be incorporated as part of the pipeline: 1) LLMs could
                   \emph{generate news reactions} to represent diverse
                   perspectives and simulate user-news interaction networks; 2)
                   LLMs could \emph{generate explanations} for proxy tasks
                   (e.g., sentiment, stance) to enrich the contexts of news
                   articles and produce experts specializing in various aspects
                   of news understanding; 3) LLMs could \emph{merge
                   task-specific experts} and provide an overall prediction by
                   incorporating the predictions and confidence scores of
                   varying experts. Extensive experiments on seven datasets with
                   three LLMs demonstrate that DELL outperforms state-of-the-art
                   baselines by up to 16.8\% in macro f1-score. Further analysis
                   reveals that the generated reactions and explanations are
                   greatly helpful in misinformation detection, while our
                   proposed LLM-guided expert merging helps produce
                   better-calibrated predictions.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Gonen2022-uz,
  title     = "Demystifying prompts in language models via perplexity estimation",
  author    = "Gonen, H and Iyer, S and Blevins, T and Smith, N A and {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Language models can be prompted to perform a wide variety of
               zero-and few-shot learning problems. However, performance varies
               significantly with the choice of prompt, and we do …",
  year      =  2022
}

@ARTICLE{SturtzSreetharan2023-bx,
  title     = "Deny, Reassure, and Deflect: Evidence and Implications of Forms
               and Norms of Fat Talk",
  author    = "SturtzSreetharan, Cindi and Ghorbani, Monet and Brewis, Alexandra
               and Wutich, Amber",
  journal   = "Cross Cult. Res.",
  publisher = "SAGE Publications Inc",
  pages     =  10693971231199373,
  abstract  = "Fat talk is a conversational interaction recognized through
               comments like ?Does this make me look fat?? In the US, based on
               psychological lab-based investigations, fat talk is defined as
               highly damaging for women and actively targeted for various
               interventions. Using a discourse completion task (DCT), we
               present normative responses (N = 313) to fat talk prompts testing
               women?s fat talk patterns across diverse languages and
               socio-cultural contexts. Based on replies from the DCT deployed
               in seven countries, we find that the normative response in all
               sites is always denial (?No, you aren?t!?) and often followed by
               additional reassurance (?you look good?). The consistency of
               findings suggests fat talk is an emergent global conversational
               form with shared, recognized rules among casual acquaintances.
               The normative denial response suggests positive functions where
               interactional fat talk reaffirms and reassures peer affiliation
               and membership. Ultimately, we suggest that fat talk may serve as
               a mundane rejection of everyday fatphobia; interventions posing
               fat talk as always harmful may simply reaffirm experiences of fat
               stigma by attempting to restrict the interpretation to only
               negative.",
  month     =  sep,
  year      =  2023
}

@ARTICLE{Lee2020-zp,
  title     = "Designing a Chatbot as a Mediator for Promoting Deep
               Self-Disclosure to a Real Mental Health Professional",
  author    = "Lee, Yi-Chieh and Yamashita, Naomi and Huang, Yun",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "dl.acm.org",
  volume    =  4,
  number    = "CSCW1",
  pages     = "1--27",
  month     =  may,
  year      =  2020
}

@INPROCEEDINGS{Wenzel2024-hi,
  title     = "Designing for Harm Reduction: Communication Repair for
               Multicultural Users' Voice Interactions",
  author    = "Wenzel, Kimi and Kaufman, Geoff",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Article 879",
  pages     = "1--17",
  abstract  = "Voice assistants’ inability to serve people-of-color and
               non-native English speakers has largely been documented as a
               quality-of-service harm. However, little work has investigated
               what downstream harms propagate from this poor service. How does
               poor usability materially manifest and affect users’ lives? And
               what interaction designs might help users recover from these
               effects? We identify 6 downstream harms that propagate from
               quality-of-service harms in voice assistants. Through interviews
               and design activities with 16 multicultural participants, we
               unveil these 6 harms, outline how multicultural users uniquely
               personify their voice assistant, and suggest how these harms and
               personifications may affect their interactions. Lastly, we employ
               techniques from psychology on communication repair to contribute
               suggestions for harm-reducing repair that may be implemented in
               voice technologies. Our communication repair strategies include:
               identity affirmations (intermittent frequency), cultural
               sensitivity, and blame redirection. This work shows potential for
               a harm-repair framework to positively influence voice
               interactions.",
  series    = "CHI '24",
  month     =  may,
  year      =  2024,
  keywords  = "Automated Speech Recognition, Communication Breakdown,
               Communication Repair, Conversational Repair, Conversational User
               Interface, Harm, Harm-Reduction, Language Technology,
               Multiculture, Multilingual, Voice Assistants"
}

@ARTICLE{Aizenberg2020-qu,
  title     = "Designing for human rights in {AI}",
  author    = "Aizenberg, Evgeni and van den Hoven, Jeroen",
  journal   = "Big Data Soc.",
  publisher = "SAGE Publications",
  volume    =  7,
  number    =  2,
  pages     =  205395172094956,
  abstract  = "In the age of Big Data, companies and governments are
               increasingly using algorithms to inform hiring decisions,
               employee management, policing, credit scoring, insurance pricing,
               and many more aspects of our lives. Artificial intelligence (AI)
               systems can help us make evidence-driven, efficient decisions,
               but can also confront us with unjustified, discriminatory
               decisions wrongly assumed to be accurate because they are made
               automatically and quantitatively. It is becoming evident that
               these technological developments are consequential to people’s
               fundamental human rights. Despite increasing attention to these
               urgent challenges in recent years, technical solutions to these
               complex socio-ethical problems are often developed without
               empirical study of societal context and the critical input of
               societal stakeholders who are impacted by the technology. On the
               other hand, calls for more ethically and socially aware AI often
               fail to provide answers for how to proceed beyond stressing the
               importance of transparency, explainability, and fairness.
               Bridging these socio-technical gaps and the deep divide between
               abstract value language and design requirements is essential to
               facilitate nuanced, context-dependent design choices that will
               support moral and social values. In this paper, we bridge this
               divide through the framework of Design for Values, drawing on
               methodologies of Value Sensitive Design and Participatory Design
               to present a roadmap for proactively engaging societal
               stakeholders to translate fundamental human rights into
               context-dependent design requirements through a structured,
               inclusive, and transparent process.",
  month     =  jul,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Peters2018-hj,
  title    = "Designing for Motivation, Engagement and Wellbeing in Digital
              Experience",
  author   = "Peters, Dorian and Calvo, Rafael A and Ryan, Richard M",
  journal  = "Front. Psychol.",
  volume   =  9,
  pages    =  797,
  abstract = "Research in psychology has shown that both motivation and
              wellbeing are contingent on the satisfaction of certain
              psychological needs. Yet, despite a long-standing pursuit in
              human-computer interaction (HCI) for design strategies that foster
              sustained engagement, behavior change and wellbeing, the basic
              psychological needs shown to mediate these outcomes are rarely
              taken into account. This is possibly due to the lack of a clear
              model to explain these needs in the context of HCI. Herein we
              introduce such a model: Motivation, Engagement and Thriving in
              User Experience (METUX). The model provides a framework grounded
              in psychological research that can allow HCI researchers and
              practitioners to form actionable insights with respect to how
              technology designs support or undermine basic psychological needs,
              thereby increasing motivation and engagement, and ultimately,
              improving user wellbeing. We propose that in order to address
              wellbeing, psychological needs must be considered within five
              different spheres of analysis including: at the point of
              technology adoption, during interaction with the interface, as a
              result of engagement with technology-specific tasks, as part of
              the technology-supported behavior, and as part of an individual's
              life overall. These five spheres of experience sit within a sixth,
              society, which encompasses both direct and collateral effects of
              technology use as well as non-user experiences. We build this
              model based on existing evidence for basic psychological need
              satisfaction, including evidence within the context of the
              workplace, computer games, and health. We extend and hone these
              ideas to provide practical advice for designers along with real
              world examples of how to apply the model to design practice.",
  month    =  may,
  year     =  2018,
  keywords = "HCI; design; engagement; motivation; self-determination theory;
              user experience; wellbeing",
  language = "en"
}

@ARTICLE{Stray2021-lc,
  title         = "Designing Recommender Systems to Depolarize",
  author        = "Stray, Jonathan",
  journal       = "arXiv [cs.IR]",
  abstract      = "Polarization is implicated in the erosion of democracy and
                   the progression to violence, which makes the polarization
                   properties of large algorithmic content selection systems
                   (recommender systems) a matter of concern for peace and
                   security. While algorithm-driven social media does not seem
                   to be a primary driver of polarization at the country level,
                   it could be a useful intervention point in polarized
                   societies. This paper examines algorithmic depolarization
                   interventions with the goal of conflict transformation: not
                   suppressing or eliminating conflict but moving towards more
                   constructive conflict. Algorithmic intervention is considered
                   at three stages: which content is available (moderation), how
                   content is selected and personalized (ranking), and content
                   presentation and controls (user interface). Empirical studies
                   of online conflict suggest that the exposure diversity
                   intervention proposed as an antidote to ``filter bubbles''
                   can be improved and can even worsen polarization under some
                   conditions. Using civility metrics in conjunction with
                   diversity in content selection may be more effective.
                   However, diversity-based interventions have not been tested
                   at scale and may not work in the diverse and dynamic contexts
                   of real platforms. Instead, intervening in platform
                   polarization dynamics will likely require continuous
                   monitoring of polarization metrics, such as the widely used
                   ``feeling thermometer.'' These metrics can be used to
                   evaluate product features, and potentially engineered as
                   algorithmic objectives. It may further prove necessary to
                   include polarization measures in the objective functions of
                   recommender algorithms to prevent optimization processes from
                   creating conflict as a side effect.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.IR"
}

@INPROCEEDINGS{Wang2017-go,
  title     = "Detecting and Characterizing Eating-Disorder Communities on
               Social Media",
  author    = "Wang, Tao and Brede, Markus and Ianni, Antonella and Mentzakis,
               Emmanouil",
  booktitle = "Proceedings of the Tenth ACM International Conference on Web
               Search and Data Mining",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "91--100",
  abstract  = "Eating disorders are complex mental disorders and responsible for
               the highest mortality rate among mental illnesses. Recent studies
               reveal that user-generated content on social media provides
               useful information in understanding these disorders. Most
               previous studies focus on studying communities of people who
               discuss eating disorders on social media, while few studies have
               explored community structures and interactions among individuals
               who suffer from this disease over social media. In this paper, we
               first develop a snowball sampling method to automatically gather
               individuals who self-identify as eating disordered in their
               profile descriptions, as well as their social network connections
               with one another on Twitter. Then, we verify the effectiveness of
               our sampling method by: 1. quantifying differences between the
               sampled eating disordered users and two sets of reference data
               collected for non-disordered users in social status, behavioral
               patterns and psychometric properties; 2. building predictive
               models to classify eating disordered and non-disordered users.
               Finally, leveraging the data of social connections between eating
               disordered individuals on Twitter, we present the first homophily
               study among eating-disorder communities on social media. Our
               findings shed new light on how an eating-disorder community
               develops on social media.",
  series    = "WSDM '17",
  month     =  feb,
  year      =  2017,
  keywords  = "text mining, eating disorder, network analysis, homophily, social
               media, mental health"
}

@INPROCEEDINGS{Li2021-ea,
  title     = "Detecting Health Advice in Medical Research Literature",
  author    = "Li, Yingya and Wang, Jun and Yu, Bei",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Online and Punta Cana, Dominican Republic",
  pages     = "6018--6029",
  abstract  = "Health and medical researchers often give clinical and policy
               recommendations to inform health practice and public health
               policy. However, no current health information system supports
               the direct retrieval of health advice. This study fills the gap
               by developing and validating an NLP-based prediction model for
               identifying health advice in research publications. We annotated
               a corpus of 6,000 sentences extracted from structured abstracts
               in PubMed publications as ```strong advice'', ``weak advice'', or
               ``no advice'', and developed a BERT-based model that can predict,
               with a macro-averaged F1-score of 0.93, whether a sentence gives
               strong advice, weak advice, or not. The prediction model
               generalized well to sentences in both unstructured abstracts and
               discussion sections, where health advice normally appears. We
               also conducted a case study that applied this prediction model to
               retrieve specific health advice on COVID-19 treatments from
               LitCovid, a large COVID research literature portal, demonstrating
               the usefulness of retrieving health advice sentences as an
               advanced research literature navigation function for health
               researchers and the general public.",
  month     =  nov,
  year      =  2021
}

@ARTICLE{Seih2017-wa,
  title     = "Development and Examination of the Linguistic Category Model in a
               Computerized Text Analysis Method",
  author    = "Seih, Yi-Tai and Beier, Susanne and Pennebaker, James W",
  journal   = "J. Lang. Soc. Psychol.",
  publisher = "SAGE Publications Inc",
  volume    =  36,
  number    =  3,
  pages     = "343--355",
  abstract  = "The linguistic category model (LCM) seeks to understand social
               psychological processes through the lens of language use. Its
               original development required human judges to analyze natural
               language to understand how people assess actions, states, and
               traits. The current project sought to computerize the LCM
               assessment based on an idea of language abstraction with a
               previously published data set. In the study, a computerized LCM
               analysis method was built using an LCM verb dictionary and a
               part-of-speech tagging program that identified relevant
               adjectives and nouns. This computerized method compared
               open-ended texts written in first-person and third-person
               perspectives from 130 college students. Consistent with
               construal-level theory, third-person writing resulted in higher
               levels of abstraction than first-person writing. Implications of
               relying on an automated LCM method are discussed.",
  month     =  jun,
  year      =  2017
}

@ARTICLE{Grassini2023-xa,
  title    = "Development and validation of the {AI} attitude scale ({AIAS}-4):
              a brief measure of general attitude toward artificial intelligence",
  author   = "Grassini, Simone",
  journal  = "Front. Psychol.",
  volume   =  14,
  pages    =  1191628,
  abstract = "The rapid advancement of artificial intelligence (AI) has
              generated an increasing demand for tools that can assess public
              attitudes toward AI. This study proposes the development and the
              validation of the AI Attitude Scale (AIAS), a concise self-report
              instrument designed to evaluate public perceptions of AI
              technology. The first version of the AIAS that the present
              manuscript proposes comprises five items, including one
              reverse-scored item, which aims to gauge individuals' beliefs
              about AI's influence on their lives, careers, and humanity
              overall. The scale is designed to capture attitudes toward AI,
              focusing on the perceived utility and potential impact of
              technology on society and humanity. The psychometric properties of
              the scale were investigated using diverse samples in two separate
              studies. An exploratory factor analysis was initially conducted on
              a preliminary 5-item version of the scale. Such exploratory
              validation study revealed the need to divide the scale into two
              factors. While the results demonstrated satisfactory internal
              consistency for the overall scale and its correlation with related
              psychometric measures, separate analyses for each factor showed
              robust internal consistency for Factor 1 but insufficient internal
              consistency for Factor 2. As a result, a second version of the
              scale is developed and validated, omitting the item that displayed
              weak correlation with the remaining items in the questionnaire.
              The refined final 1-factor, 4-item AIAS demonstrated superior
              overall internal consistency compared to the initial 5-item scale
              and the proposed factors. Further confirmatory factor analyses,
              performed on a different sample of participants, confirmed that
              the 1-factor model (4-items) of the AIAS exhibited an adequate fit
              to the data, providing additional evidence for the scale's
              structural validity and generalizability across diverse
              populations. In conclusion, the analyses reported in this article
              suggest that the developed and validated 4-items AIAS can be a
              valuable instrument for researchers and professionals working on
              AI development who seek to understand and study users' general
              attitudes toward AI.",
  month    =  jul,
  year     =  2023,
  keywords = "artificial intelligence; factor analysis; human-computer
              interaction (HCI); psychology; questionnaire",
  language = "en"
}

@ARTICLE{Liu2021-nd,
  title         = "{DExperts}: Decoding-Time Controlled Text Generation with
                   Experts and Anti-Experts",
  author        = "Liu, Alisa and Sap, Maarten and Lu, Ximing and Swayamdipta,
                   Swabha and Bhagavatula, Chandra and Smith, Noah A and Choi,
                   Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Despite recent advances in natural language generation, it
                   remains challenging to control attributes of generated text.
                   We propose DExperts: Decoding-time Experts, a decoding-time
                   method for controlled text generation that combines a
                   pretrained language model with ``expert'' LMs and/or
                   ``anti-expert'' LMs in a product of experts. Intuitively,
                   under the ensemble, tokens only get high probability if they
                   are considered likely by the experts, and unlikely by the
                   anti-experts. We apply DExperts to language detoxification
                   and sentiment-controlled generation, where we outperform
                   existing controllable generation methods on both automatic
                   and human evaluations. Moreover, because DExperts operates
                   only on the output of the pretrained LM, it is effective with
                   (anti-)experts of smaller size, including when operating on
                   GPT-3. Our work highlights the promise of tuning small LMs on
                   text with (un)desirable attributes for efficient
                   decoding-time steering.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hamilton2016-hq,
  title         = "Diachronic Word Embeddings Reveal Statistical Laws of
                   Semantic Change",
  author        = "Hamilton, William L and Leskovec, Jure and Jurafsky, Dan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Understanding how words change their meanings over time is
                   key to models of language and cultural evolution, but
                   historical data on meaning is scarce, making theories hard to
                   develop and test. Word embeddings show promise as a
                   diachronic tool, but have not been carefully evaluated. We
                   develop a robust methodology for quantifying semantic change
                   by evaluating word embeddings (PPMI, SVD, word2vec) against
                   known historical changes. We then use this methodology to
                   reveal statistical laws of semantic evolution. Using six
                   historical corpora spanning four languages and two centuries,
                   we propose two quantitative laws of semantic change: (i) the
                   law of conformity---the rate of semantic change scales with
                   an inverse power-law of word frequency; (ii) the law of
                   innovation---independent of frequency, words that are more
                   polysemous have higher rates of semantic change.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{McGrady2023-xg,
  title     = "Dialing for videos: A random sample of {YouTube}",
  author    = "McGrady, Ryan and Zheng, Kevin and Curran, Rebecca and
               Baumgartner, Jason and Zuckerman, Ethan",
  journal   = "J. Quant. Descr. Digit. Media",
  publisher = "journalqd.org",
  abstract  = "YouTube is one of the largest, most important communication
               platforms in the world, but while there is a great deal of
               research about the site, many of its fundamental characteristics
               remain unknown. To better understand YouTube as a whole, we
               created a random sample of videos using a new method. Through a
               description of the sample’s metadata, we provide answers to many
               essential questions about, for example, the distribution of
               views, comments, likes, subscribers, and categories. Our method
               also allows us to estimate the total number of publicly visible
               videos on YouTube and its growth over time. To learn more about
               video content, we hand-coded a subsample to answer questions like
               how many are primarily music, video games, or still images.
               Finally, we processed the videos’ audio using language detection
               software to determine the distribution of spoken languages. In
               providing basic information about YouTube as a whole, we not only
               learn more about an influential platform, but also provide
               baseline context against which samples in more focused studies
               can be compared.",
  month     =  dec,
  year      =  2023
}

@ARTICLE{Aroyo2023-rd,
  title     = "{DICES} dataset: Diversity in Conversational {AI} evaluation for
               safety",
  author    = "Aroyo, Lora and Taylor, Alex S and Díaz, Mark and Homan, C and
               Parrish, Alicia and Serapio-García, Greg and Prabhakaran,
               Vinodkumar and Wang, Ding",
  editor    = "Oh, A and Naumann, T and Globerson, A and Saenko, K and Hardt, M
               and Levine, S",
  journal   = "Neural Inf Process Syst",
  publisher = "Curran Associates, Inc.",
  volume    = "abs/2306.11247",
  pages     = "53330--53342",
  abstract  = "Machine learning approaches often require training and evaluation
               datasets with a clear separation between positive and negative
               examples. This risks simplifying and even obscuring the inherent
               subjectivity present in many tasks. Preserving such variance in
               content and diversity in datasets is often expensive and
               laborious. This is especially troubling when building safety
               datasets for conversational AI systems, as safety is both
               socially and culturally situated. To demonstrate this crucial
               aspect of conversational AI safety, and to facilitate in-depth
               model performance analyses, we introduce the DICES (Diversity In
               Conversational AI Evaluation for Safety) dataset that contains
               fine-grained demographic information about raters, high
               replication of ratings per item to ensure statistical power for
               analyses, and encodes rater votes as distributions across
               different demographics to allow for in-depth explorations of
               different aggregation strategies. In short, the DICES dataset
               enables the observation and measurement of variance, ambiguity,
               and diversity in the context of conversational AI safety. We also
               illustrate how the dataset offers a basis for establishing
               metrics to show how raters' ratings can intersects with
               demographic categories such as racial/ethnic groups, age groups,
               and genders. The goal of DICES is to be used as a shared resource
               and benchmark that respects diverse perspectives during safety
               evaluation of conversational AI systems.",
  month     =  jun,
  year      =  2023
}

@ARTICLE{Eisenstein2014-qc,
  title     = "Diffusion of lexical change in social media",
  author    = "Eisenstein, Jacob and O'Connor, Brendan and Smith, Noah A and
               Xing, Eric P",
  journal   = "PLoS One",
  publisher = "journals.plos.org",
  volume    =  9,
  number    =  11,
  pages     = "e113114",
  abstract  = "Computer-mediated communication is driving fundamental changes in
               the nature of written language. We investigate these changes by
               statistical analysis of a dataset comprising 107 million Twitter
               messages (authored by 2.7 million unique user accounts). Using a
               latent vector autoregressive model to aggregate across thousands
               of words, we identify high-level patterns in diffusion of
               linguistic change over the United States. Our model is robust to
               unpredictable changes in Twitter's sampling rate, and provides a
               probabilistic characterization of the relationship of macro-scale
               linguistic influence to a set of demographic and geographic
               predictors. The results of this analysis offer support for prior
               arguments that focus on geographical proximity and population
               size. However, demographic similarity - especially with regard to
               race - plays an even more central role, as cities with similar
               racial demographics are far more likely to share linguistic
               influence. Rather than moving towards a single unified
               ``netspeak'' dialect, language evolution in computer-mediated
               communication reproduces existing fault lines in spoken American
               English.",
  month     =  nov,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Jarman2024-dq,
  title     = "Direct and indirect relationships between social media use and
               body satisfaction: A prospective study among adolescent boys and
               girls",
  author    = "Jarman, H K and McLean, S A and Slater, A and {others}",
  journal   = "New Media \&",
  publisher = "journals.sagepub.com",
  abstract  = "Cross-sectional research suggests a small, inverse association
               between social media use and body satisfaction. However, less is
               known regarding prospective, bidirectional, or …",
  year      =  2024
}

@ARTICLE{Lucy2022-ei,
  title         = "Discovering Differences in the Representation of People using
                   Contextualized Semantic Axes",
  author        = "Lucy, Li and Tadimeti, Divya and Bamman, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "A common paradigm for identifying semantic differences across
                   social and temporal contexts is the use of static word
                   embeddings and their distances. In particular, past work has
                   compared embeddings against ``semantic axes'' that represent
                   two opposing concepts. We extend this paradigm to BERT
                   embeddings, and construct contextualized axes that mitigate
                   the pitfall where antonyms have neighboring representations.
                   We validate and demonstrate these axes on two people-centric
                   datasets: occupations from Wikipedia, and multi-platform
                   discussions in extremist, men's communities over fourteen
                   years. In both studies, contextualized semantic axes can
                   characterize differences among instances of the same word
                   type. In the latter study, we show that references to women
                   and the contexts around them have become more detestable over
                   time.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@BOOK{Talley2014-ol,
  title     = "Disfigurement and the Politics of Appearance",
  author    = "Talley, Heather Laine",
  publisher = "NYU Press",
  abstract  = "Imagine yourself without a facethe task seems impossible. The
               face is a core feature of our physical identity. Our face is how
               others identify us and how we think of our self. Yet, human faces
               are also functionally essential as mechanisms for communication
               and as a means of eating, breathing, and seeing. For these
               reasons, facial disfigurement can endanger our fundamental
               notions of self and identity or even be life threatening, at
               worse. Precisely because it is so difficult to conceal our faces,
               the disfigured face compromises appearance, status, and, perhaps,
               our very way of being in the world.InSaving Face, sociologist
               Heather Laine Talley examines the cultural meaning and social
               significance of interventions aimed at repairing faces defined as
               disfigured. Using ethnography, participant-observation, content
               analysis, interviews, and autoethnography, Talley explores four
               sites in which a range of faces are repaired: face
               transplantation, facial feminization surgery, the reality
               showExtreme Makeover, and the international charitable
               organization Operation Smile,. Throughout, she considers how
               efforts focused on repair sometimes intensify the stigma
               associated with disfigurement. Drawing upon experiences
               volunteering at a camp for children with severe burns, Talley
               also considers alternative interventions and everyday practices
               that both challenge stigma and help those seen as disfigured
               negotiate outsider status.Talley delves into the promise and
               limits of facial surgery, continually examining how we might
               understand appearance as a facet of privilege and a dimension of
               inequality. Ultimately, she argues that facial work is not simply
               a conglomeration of reconstructive techniques aimed at the human
               face, but rather, that appearance interventions are increasingly
               treated as lifesaving work. Especially at a time when aesthetic
               technologies carrying greater risk are emerging and when
               discrimination based on appearance is rampant, this important
               book challenges us to think critically about how we see the human
               face.",
  year      =  2014
}

@ARTICLE{Geva2023-uo,
  title         = "Dissecting Recall of Factual Associations in Auto-Regressive
                   Language Models",
  author        = "Geva, Mor and Bastings, Jasmijn and Filippova, Katja and
                   Globerson, Amir",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer-based language models (LMs) are known to capture
                   factual knowledge in their parameters. While previous work
                   looked into where factual associations are stored, only
                   little is known about how they are retrieved internally
                   during inference. We investigate this question through the
                   lens of information flow. Given a subject-relation query, we
                   study how the model aggregates information about the subject
                   and relation to predict the correct attribute. With
                   interventions on attention edges, we first identify two
                   critical points where information propagates to the
                   prediction: one from the relation positions followed by
                   another from the subject positions. Next, by analyzing the
                   information at these points, we unveil a three-step internal
                   mechanism for attribute extraction. First, the representation
                   at the last-subject position goes through an enrichment
                   process, driven by the early MLP sublayers, to encode many
                   subject-related attributes. Second, information from the
                   relation propagates to the prediction. Third, the prediction
                   representation ``queries'' the enriched subject to extract
                   the attribute. Perhaps surprisingly, this extraction is
                   typically done via attention heads, which often encode
                   subject-attribute mappings in their parameters. Overall, our
                   findings introduce a comprehensive view of how factual
                   associations are stored and extracted internally in LMs,
                   facilitating future research on knowledge localization and
                   editing.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Vijayakumar2016-dk,
  title     = "Diverse beam search: Decoding diverse solutions from neural
               sequence models",
  author    = "Vijayakumar, A K and Cogswell, M and Selvaraju, R R and {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Neural sequence models are widely used to model time-series data.
               Equally ubiquitous is the usage of beam search (BS) as an
               approximate inference algorithm to decode output …",
  year      =  2016
}

@ARTICLE{Suchman1993-uj,
  title     = "Do categories have politics? The language/action perspective
               reconsidered",
  author    = "Suchman, L",
  journal   = "Comput. Support. Coop. Work",
  publisher = "Springer",
  abstract  = "Drawing on writings within the CSCW community and on recent
               social theory, this paper proposes that the adoption of speech
               act theory as a foundation for system design carries …",
  year      =  1993
}

@ARTICLE{Liu2022-mk,
  title         = "Do ever larger octopi still amplify reporting biases?
                   Evidence from judgments of typical colour",
  author        = "Liu, Fangyu and Eisenschlos, Julian Martin and Cole, Jeremy R
                   and Collier, Nigel",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) trained on raw texts have no direct
                   access to the physical world. Gordon and Van Durme (2013)
                   point out that LMs can thus suffer from reporting bias: texts
                   rarely report on common facts, instead focusing on the
                   unusual aspects of a situation. If LMs are only trained on
                   text corpora and naively memorise local co-occurrence
                   statistics, they thus naturally would learn a biased view of
                   the physical world. While prior studies have repeatedly
                   verified that LMs of smaller scales (e.g., RoBERTa, GPT-2)
                   amplify reporting bias, it remains unknown whether such
                   trends continue when models are scaled up. We investigate
                   reporting bias from the perspective of colour in larger
                   language models (LLMs) such as PaLM and GPT-3. Specifically,
                   we query LLMs for the typical colour of objects, which is one
                   simple type of perceptually grounded physical common sense.
                   Surprisingly, we find that LLMs significantly outperform
                   smaller LMs in determining an object's typical colour and
                   more closely track human judgments, instead of overfitting to
                   surface patterns stored in texts. This suggests that very
                   large models of language alone are able to overcome certain
                   types of reporting bias that are characterized by local
                   co-occurrences.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Agrawal2023-qh,
  title    = "Do language models know when they’re hallucinating references?",
  author   = "Agrawal, A and Mackey, Lester W and Kalai, A",
  journal  = "ArXiv",
  volume   = "abs/2305.18248",
  abstract = "State-of-the-art language models (LMs) are notoriously susceptible
              to generating hallucinated information. Such inaccurate outputs
              not only undermine the reliability of these models but also limit
              their use and raise serious concerns about misinformation and
              propaganda. In this work, we focus on hallucinated book and
              article references and present them as the “model organism” of
              language model hallucination research, due to their frequent and
              easy-to-discern nature. We posit that if a language model cites a
              particular reference in its output, then it should ideally possess
              sufficient information about its authors and content, among other
              relevant details. Using this basic insight, we illustrate that one
              can identify hallucinated references without ever consulting any
              external resources, by asking a set of direct or indirect queries
              to the language model about the references. These queries can be
              considered as “consistency checks.” Our findings highlight that
              while LMs, including GPT-4, often produce inconsistent author
              lists for hallucinated references, they also often accurately
              recall the authors of real references. In this sense, the LM can
              be said to “know” when it is hallucinating references.
              Furthermore, these findings show how hallucinated references can
              be dissected to shed light on their nature.",
  month    =  may,
  year     =  2023
}

@ARTICLE{Tjuatja2023-dv,
  title         = "Do {LLMs} exhibit human-like response biases? A case study in
                   survey design",
  author        = "Tjuatja, Lindia and Chen, Valerie and Wu, Sherry Tongshuang
                   and Talwalkar, Ameet and Neubig, Graham",
  journal       = "arXiv [cs.CL]",
  abstract      = "As large language models (LLMs) become more capable, there is
                   growing excitement about the possibility of using LLMs as
                   proxies for humans in real-world tasks where subjective
                   labels are desired, such as in surveys and opinion polling.
                   One widely-cited barrier to the adoption of LLMs is their
                   sensitivity to prompt wording -- but interestingly, humans
                   also display sensitivities to instruction changes in the form
                   of response biases. As such, we argue that if LLMs are going
                   to be used to approximate human opinions, it is necessary to
                   investigate the extent to which LLMs also reflect human
                   response biases, if at all. In this work, we use survey
                   design as a case study, where human response biases caused by
                   permutations in wordings of ``prompts'' have been extensively
                   studied. Drawing from prior work in social psychology, we
                   design a dataset and propose a framework to evaluate whether
                   LLMs exhibit human-like response biases in survey
                   questionnaires. Our comprehensive evaluation of nine models
                   shows that popular open and commercial LLMs generally fail to
                   reflect human-like behavior. These inconsistencies tend to be
                   more prominent in models that have been instruction
                   fine-tuned. Furthermore, even if a model shows a significant
                   change in the same direction as humans, we find that
                   perturbations that are not meant to elicit significant
                   changes in humans may also result in a similar change,
                   suggesting that such a result could be partially due to other
                   spurious correlations. These results highlight the potential
                   pitfalls of using LLMs to substitute humans in parts of the
                   annotation pipeline, and further underscore the importance of
                   finer-grained characterizations of model behavior. Our code,
                   dataset, and collected samples are available at
                   https://github.com/lindiatjuatja/BiasMonkey",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Choi2023-ad,
  title     = "Do {LLMs} Understand Social Knowledge? Evaluating the Sociability
               of Large Language Models with {SocKET} Benchmark",
  author    = "Choi, M and Pei, J and Kumar, S and Shu, C and Jurgens, D",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Large language models (LLMs) have been shown to perform well at a
               variety of syntactic, discourse, and reasoning tasks. While LLMs
               are increasingly deployed in many forms …",
  year      =  2023
}

@ARTICLE{Hammerl2022-tb,
  title         = "Do Multilingual Language Models Capture Differing Moral
                   Norms?",
  author        = "Hämmerl, Katharina and Deiseroth, Björn and Schramowski,
                   Patrick and Libovický, Jindřich and Fraser, Alexander and
                   Kersting, Kristian",
  journal       = "arXiv [cs.CL]",
  abstract      = "Massively multilingual sentence representations are trained
                   on large corpora of uncurated data, with a very imbalanced
                   proportion of languages included in the training. This may
                   cause the models to grasp cultural values including moral
                   judgments from the high-resource languages and impose them on
                   the low-resource languages. The lack of data in certain
                   languages can also lead to developing random and thus
                   potentially harmful beliefs. Both these issues can negatively
                   influence zero-shot cross-lingual model transfer and
                   potentially lead to harmful outcomes. Therefore, we aim to
                   (1) detect and quantify these issues by comparing different
                   models in different languages, (2) develop methods for
                   improving undesirable properties of the models. Our initial
                   experiments using the multilingual model XLM-R show that
                   indeed multilingual LMs capture moral norms, even with
                   potentially higher human-agreement than monolingual ones.
                   However, it is not yet clear to what extent these moral norms
                   differ between languages.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Shwartz2020-sa,
  title     = "Do Neural Language Models Overcome Reporting Bias?",
  author    = "Shwartz, Vered and Choi, Yejin",
  booktitle = "Proceedings of the 28th International Conference on Computational
               Linguistics",
  publisher = "International Committee on Computational Linguistics",
  address   = "Barcelona, Spain (Online)",
  pages     = "6863--6870",
  abstract  = "Mining commonsense knowledge from corpora suffers from reporting
               bias, over-representing the rare at the expense of the trivial
               (Gordon and Van Durme, 2013). We study to what extent pre-trained
               language models overcome this issue. We find that while their
               generalization capacity allows them to better estimate the
               plausibility of frequent but unspoken of actions, outcomes, and
               properties, they also tend to overestimate that of the very rare,
               amplifying the bias that already exists in their training corpus.",
  month     =  dec,
  year      =  2020
}

@ARTICLE{Gillespie2022-iq,
  title     = "Do Not Recommend? Reduction as a Form of Content Moderation",
  author    = "Gillespie, Tarleton",
  journal   = "Social Media + Society",
  publisher = "SAGE Publications Ltd",
  volume    =  8,
  number    =  3,
  pages     =  20563051221117552,
  abstract  = "Public debate about content moderation has overwhelmingly focused
               on removal: social media platforms deleting content and
               suspending users, or opting not to do so. However, removal is not
               the only available remedy. Reducing the visibility of problematic
               content is becoming a commonplace element of platform governance.
               Platforms use machine learning classifiers to identify content
               they judge misleading enough, risky enough, or offensive enough
               that, while it does not warrant removal according to the site
               guidelines, warrants demoting them in algorithmic rankings and
               recommendations. In this essay, I document this shift and explain
               how reduction works. I then raise questions about what it means
               to use recommendation as a means of content moderation.",
  month     =  jul,
  year      =  2022
}

@ARTICLE{Troshani2020-qp,
  title     = "Do We Trust in {AI}? Role of Anthropomorphism and Intelligence",
  author    = "Troshani, Indrit and Hill, Sally Rao and Sherman, Claire and
               Arthur, Damien",
  journal   = "Journal of Computer Information Systems",
  publisher = "Taylor \& Francis",
  number    =  1,
  abstract  = "AI applications are radically transforming the manner in which
               service providers and consumers interact. We explore how the
               humanness of AI applications affects consumers' trust in these
               applications. Qualitative evidence collected with focus groups
               provides fresh insights into the roles of anthropomorphism and
               intelligence, as key constructs representing humanness. Our
               findings reveal the consumers' perspective on the nuances of
               these constructs pertaining to services enabled by AI
               applications. It also extends current understanding of the
               phenomenon of the ``uncanny valley'', by identifying conditions
               under which consumers experience discomfort and uneasiness as AI
               humanness increases in service environments. Abstract AI
               applications are radically transforming the manner in which
               service providers and consumers interact. We explore how the
               humanness of AI applications affects consumers' trust in these
               applications. Qualitative evidence collected with focus groups
               provides fresh insights into the roles of anthropomorphism and
               intelligence, as key constructs representing humanness. Our
               findings reveal the consumers' perspective on the nuances of
               these constructs pertaining to services enabled by AI
               applications. It also extends current understanding of the
               phenomenon of the ``uncanny valley'', by identifying conditions
               under which consumers experience discomfort and uneasiness as AI
               humanness increases in service environments.",
  month     =  aug,
  year      =  2020
}

@INPROCEEDINGS{Scott2023-wu,
  title     = "Do You Mind? User Perceptions of Machine Consciousness",
  author    = "Scott, Ava Elizabeth and Neumann, Daniel and Niess, Jasmin and
               Woźniak, Paweł W",
  booktitle = "Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Article 374",
  pages     = "1--19",
  abstract  = "The prospect of machine consciousness cultivates controversy
               across media, academia, and industry. Assessing whether
               non-experts perceive technologies as conscious, and exploring the
               consequences of this perception, are yet unaddressed challenges
               in Human Computer Interaction (HCI). To address them, we surveyed
               100 people, exploring their conceptualisations of consciousness
               and if and how they perceive consciousness in currently available
               interactive technologies. We show that many people already
               perceive a degree of consciousness in GPT-3, a voice chat bot,
               and a robot vacuum cleaner. Within participant responses we
               identified dynamic tensions between denial and speculation,
               thinking and feeling, interaction and experience, control and
               independence, and rigidity and spontaneity. These tensions can
               inform future research into perceptions of machine consciousness
               and the challenges it represents for HCI. With both empirical and
               theoretical contributions, this paper emphasises the importance
               of HCI in an era of machine consciousness, real, perceived or
               denied.",
  series    = "CHI '23",
  month     =  apr,
  year      =  2023,
  keywords  = "Consciousness, Machine Consciousness, Technology Consciousness"
}

@INPROCEEDINGS{Liu2020-da,
  title     = "Does Gender Matter? Towards Fairness in Dialogue Systems",
  author    = "Liu, Haochen and Dacon, Jamell and Fan, Wenqi and Liu, Hui and
               Liu, Zitao and Tang, Jiliang",
  editor    = "Scott, Donia and Bel, Nuria and Zong, Chengqing",
  booktitle = "Proceedings of the 28th International Conference on Computational
               Linguistics",
  publisher = "International Committee on Computational Linguistics",
  address   = "Barcelona, Spain (Online)",
  pages     = "4403--4416",
  abstract  = "Recently there are increasing concerns about the fairness of
               Artificial Intelligence (AI) in real-world applications such as
               computer vision and recommendations. For example, recognition
               algorithms in computer vision are unfair to black people such as
               poorly detecting their faces and inappropriately identifying them
               as ``gorillas''. As one crucial application of AI, dialogue
               systems have been extensively applied in our society. They are
               usually built with real human conversational data; thus they
               could inherit some fairness issues which are held in the real
               world. However, the fairness of dialogue systems has not been
               well investigated. In this paper, we perform a pioneering study
               about the fairness issues in dialogue systems. In particular, we
               construct a benchmark dataset and propose quantitative measures
               to understand fairness in dialogue models. Our studies
               demonstrate that popular dialogue models show significant
               prejudice towards different genders and races. Besides, to
               mitigate the bias in dialogue systems, we propose two simple but
               effective debiasing methods. Experiments show that our methods
               can reduce the bias in dialogue systems significantly. The
               dataset and the implementation are released to foster fairness
               research in dialogue systems.",
  month     =  dec,
  year      =  2020
}

@ARTICLE{Mellon2022-sp,
  title    = "Does {GPT}-3 know what the Most Important Issue is? Using Large
              Language Models to Code Open-Text Social Survey Responses At Scale",
  author   = "Mellon, Jonathan and Bailey, Jack and Scott, Ralph and Breckwoldt,
              James and Miori, Marta",
  journal  = "Using Large Language",
  abstract = "We examine the use of large language models (LLMs) like OpenAI's
              GPT-3 for coding open-ended survey responses. We compare GPT-3's
              performance on a classification task using data from the British
              Election Study Internet Panel (BESIP) with that of a human coder
              and an SVM (a traditional supervised learning algorithm) fitted to
              576,325 manually labelled observations. We find that while GPT-3's
              3-shot performance is slightly lower than a second human coder
              (97\% agreement), GPT-3 is able to match the original human
              coder's collapsed category 95\% of the time, and outperforms the
              SVM in terms of accuracy and bias. The results suggest that LLMs
              perform acceptably when coding this type of open-ended survey
              response and may allow for greater use of open-ended questions in
              future.",
  month    =  dec,
  year     =  2022,
  keywords = "ChatGPT,GPT-3,GPT-3.5,large language models,LLMs,most important
              issue,MII,most important problem,MIP, open text,public opinion,
              text coding, text as data"
}

@ARTICLE{Jones2023-no,
  title         = "Does {GPT}-4 pass the Turing test?",
  author        = "Jones, Cameron R and Bergen, Benjamin K",
  journal       = "arXiv [cs.AI]",
  abstract      = "We evaluated GPT-4 in a public online Turing test. The
                   best-performing GPT-4 prompt passed in 49.7\% of games,
                   outperforming ELIZA (22\%) and GPT-3.5 (20\%), but falling
                   short of the baseline set by human participants (66\%).
                   Participants' decisions were based mainly on linguistic style
                   (35\%) and socioemotional traits (27\%), supporting the idea
                   that intelligence, narrowly conceived, is not sufficient to
                   pass the Turing test. Participant knowledge about LLMs and
                   number of games played positively correlated with accuracy in
                   detecting AI, suggesting learning and practice as possible
                   strategies to mitigate deception. Despite known limitations
                   as a test of intelligence, we argue that the Turing test
                   continues to be relevant as an assessment of naturalistic
                   communication and deception. AI models with the ability to
                   masquerade as humans could have widespread societal
                   consequences, and we analyse the effectiveness of different
                   strategies and criteria for judging humanlikeness.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Shteynberg2024-cg,
  title     = "Does it matter if empathic {AI} has no empathy?",
  author    = "Shteynberg, Garriy and Halpern, Jodi and Sadovnik, Amir and
               Garthoff, Jon and Perry, Anat and Hay, Jessica and Montemayor,
               Carlos and Olson, Michael A and Hulsey, Tim L and Fairweather,
               Abrol",
  journal   = "Nat. Mach. Intell.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  6,
  number    =  5,
  pages     = "496--497",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Hase2023-rv,
  title         = "Does Localization Inform Editing? Surprising Differences in
                   Causality-Based Localization vs. Knowledge Editing in
                   Language Models",
  author        = "Hase, Peter and Bansal, Mohit and Kim, Been and
                   Ghandeharioun, Asma",
  journal       = "arXiv [cs.LG]",
  abstract      = "Language models are known to learn a great quantity of
                   factual information during pretraining, and recent work
                   localizes this information to specific model weights like
                   mid-layer MLP weights (Meng et al., 2022). In this paper, we
                   find that we can change how a fact is stored in a model by
                   editing weights that are in a different location than where
                   existing methods suggest that the fact is stored. This is
                   surprising because we would expect that localizing facts to
                   specific parameters in models would tell us where to
                   manipulate knowledge in models, and this assumption has
                   motivated past work on model editing methods. Specifically,
                   we show that localization conclusions from representation
                   denoising (also known as Causal Tracing) do not provide any
                   insight into which model MLP layer would be best to edit in
                   order to override an existing stored fact with a new one.
                   This finding raises questions about how past work relies on
                   Causal Tracing to select which model layers to edit (Meng et
                   al., 2022). Next, to better understand the discrepancy
                   between representation denoising and weight editing, we
                   develop several variants of the editing problem that appear
                   more and more like representation denoising in their design
                   and objective. Experiments show that, for one of our editing
                   problems, editing performance does relate to localization
                   results from representation denoising, but we find that which
                   layer we edit is a far better predictor of performance. Our
                   results suggest, counterintuitively, that better mechanistic
                   understanding of how pretrained language models work may not
                   always translate to insights about how to best change their
                   behavior. Code is available at:
                   https://github.com/google/belief-localization",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Gurtala2023-lb,
  title     = "Does medium matter? Investigating the impact of viewing ideal
               image or short-form video content on young women's body image,
               mood, and self-objectification",
  author    = "Gurtala, Jade C and Fardouly, Jasmine",
  journal   = "Body Image",
  publisher = "Elsevier",
  volume    =  46,
  pages     = "190--201",
  abstract  = "There is a rising prevalence of short-form videos on social
               media, particularly since the advent of TikTok. Viewing
               appearance-ideal images has harmful effects on young women's body
               image. However, the impacts of viewing appearance-ideal
               short-form videos on body image are largely unknown. This study
               investigated the impact of viewing appearance-ideal short-form
               social media video content on young women's (Mage = 19.19, SD =
               1.80) state appearance satisfaction, negative mood,
               self-objectification, and related constructs, compared to viewing
               appearance-ideal image content and appearance-neutral content.
               Young women (N = 211) were shown either: (1) appearance-ideal
               images, (2) appearance-ideal videos, (3) appearance-neutral
               images, or (4) appearance-neutral videos. Viewing
               appearance-ideal content regardless of the medium led to
               decreased appearance satisfaction, and increased negative mood,
               and self-objectification, and more state internalisation of
               appearance ideals compared to viewing appearance-neutral content.
               Further, if women perceived the appearance-ideal content they
               viewed to be unedited or unenhanced, they reported less
               appearance satisfaction after viewing video than image content.
               Thus, the impact of viewing ideal video and image content taken
               from social media may have similar effects on young women.
               However, when ideal content is low in perceived enhancement,
               viewing videos may be more harmful for appearance satisfaction
               than viewing images.",
  month     =  sep,
  year      =  2023,
  keywords  = "Body image; Self-objectification; Social media; Thin ideal;
               TikTok; Video",
  language  = "en"
}

@ARTICLE{Fraser2022-jq,
  title         = "Does Moral Code Have a Moral Code? Probing Delphi's Moral
                   Philosophy",
  author        = "Fraser, Kathleen C and Kiritchenko, Svetlana and Balkir, Esma",
  journal       = "arXiv [cs.CY]",
  abstract      = "In an effort to guarantee that machine learning model outputs
                   conform with human moral values, recent work has begun
                   exploring the possibility of explicitly training models to
                   learn the difference between right and wrong. This is
                   typically done in a bottom-up fashion, by exposing the model
                   to different scenarios, annotated with human moral
                   judgements. One question, however, is whether the trained
                   models actually learn any consistent, higher-level ethical
                   principles from these datasets -- and if so, what? Here, we
                   probe the Allen AI Delphi model with a set of standardized
                   morality questionnaires, and find that, despite some
                   inconsistencies, Delphi tends to mirror the moral principles
                   associated with the demographic groups involved in the
                   annotation process. We question whether this is desirable and
                   discuss how we might move forward with this knowledge.",
  month         =  may,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Padmakumar2023-ah,
  title         = "Does Writing with Language Models Reduce Content Diversity?",
  author        = "Padmakumar, Vishakh and He, He",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) have led to a surge in
                   collaborative writing with model assistance. As different
                   users incorporate suggestions from the same model, there is a
                   risk of decreased diversity in the produced content,
                   potentially limiting diverse perspectives in public
                   discourse. In this work, we measure the impact of co-writing
                   on diversity via a controlled experiment, where users write
                   argumentative essays in three setups -- using a base LLM
                   (GPT3), a feedback-tuned LLM (InstructGPT), and writing
                   without model help. We develop a set of diversity metrics and
                   find that writing with InstructGPT (but not the GPT3) results
                   in a statistically significant reduction in diversity.
                   Specifically, it increases the similarity between the
                   writings of different authors and reduces the overall lexical
                   and content diversity. We additionally find that this effect
                   is mainly attributable to InstructGPT contributing less
                   diverse text to co-written essays. In contrast, the
                   user-contributed text remains unaffected by model
                   collaboration. This suggests that the recent improvement in
                   generation quality from adapting models to human feedback
                   might come at the cost of more homogeneous and less diverse
                   content.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{Chuang_undated-ts,
  title       = "{DoLa}: Official implementation for the paper ``{DoLa}:
                 Decoding by Contrasting Layers Improves Factuality in Large
                 Language Models''",
  author      = "Chuang, Yung-Sung",
  institution = "Github",
  abstract    = "Official implementation for the paper ``DoLa: Decoding by
                 Contrasting Layers Improves Factuality in Large Language
                 Models'' - voidism/DoLa: Official implementation for the paper
                 ``DoLa: Decoding by Contrasting Layers Improves Factuality in
                 Large Language Models''",
  language    = "en"
}

@ARTICLE{Cardenas2023-oc,
  title         = "'Don't Get Too Technical with Me': A Discourse
                   Structure-Based Framework for Science Journalism",
  author        = "Cardenas, Ronald and Yao, Bingsheng and Wang, Dakuo and Hou,
                   Yufang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Science journalism refers to the task of reporting technical
                   findings of a scientific paper as a less technical news
                   article to the general public audience. We aim to design an
                   automated system to support this real-world task (i.e.,
                   automatic science journalism) by 1) introducing a
                   newly-constructed and real-world dataset (SciTechNews), with
                   tuples of a publicly-available scientific paper, its
                   corresponding news article, and an expert-written short
                   summary snippet; 2) proposing a novel technical framework
                   that integrates a paper's discourse structure with its
                   metadata to guide generation; and, 3) demonstrating with
                   extensive automatic and human experiments that our framework
                   outperforms other baseline methods (e.g. Alpaca and ChatGPT)
                   in elaborating a content plan meaningful for the target
                   audience, simplifying the information selected, and producing
                   a coherent final report in a layman's style.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Belinkov2019-uw,
  title         = "Don't Take the Premise for Granted: Mitigating Artifacts in
                   Natural Language Inference",
  author        = "Belinkov, Yonatan and Poliak, Adam and Shieber, Stuart M and
                   Van Durme, Benjamin and Rush, Alexander M",
  journal       = "arXiv [cs.CL]",
  abstract      = "Natural Language Inference (NLI) datasets often contain
                   hypothesis-only biases---artifacts that allow models to
                   achieve non-trivial performance without learning whether a
                   premise entails a hypothesis. We propose two probabilistic
                   methods to build models that are more robust to such biases
                   and better transfer across datasets. In contrast to standard
                   approaches to NLI, our methods predict the probability of a
                   premise given a hypothesis and NLI label, discouraging models
                   from ignoring the premise. We evaluate our methods on
                   synthetic and existing NLI datasets by training on datasets
                   containing biases and testing on datasets containing no (or
                   different) hypothesis-only biases. Our results indicate that
                   these methods can make NLI models more robust to
                   dataset-specific artifacts, transferring better than a
                   baseline architecture in 9 out of 12 NLI datasets.
                   Additionally, we provide an extensive analysis of the
                   interplay of our methods with known biases in NLI datasets,
                   as well as the effects of encouraging models to ignore biases
                   and fine-tuning on target datasets.",
  month         =  jul,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Yerukola2023-hb,
  title         = "Don't Take This Out of Context! On the Need for Contextual
                   Models and Evaluations for Stylistic Rewriting",
  author        = "Yerukola, Akhila and Zhou, Xuhui and Clark, Elizabeth and
                   Sap, Maarten",
  journal       = "arXiv [cs.CL]",
  abstract      = "Most existing stylistic text rewriting methods and evaluation
                   metrics operate on a sentence level, but ignoring the broader
                   context of the text can lead to preferring generic,
                   ambiguous, and incoherent rewrites. In this paper, we
                   investigate integrating the preceding textual context into
                   both the $\textit{rewriting}$ and $\textit{evaluation}$
                   stages of stylistic text rewriting, and introduce a new
                   composite contextual evaluation metric $\texttt{CtxSimFit}$
                   that combines similarity to the original sentence with
                   contextual cohesiveness. We comparatively evaluate
                   non-contextual and contextual rewrites in formality,
                   toxicity, and sentiment transfer tasks. Our experiments show
                   that humans significantly prefer contextual rewrites as more
                   fitting and natural over non-contextual ones, yet existing
                   sentence-level automatic metrics (e.g., ROUGE, SBERT)
                   correlate poorly with human preferences ($\rho$=0--0.3). In
                   contrast, human preferences are much better reflected by both
                   our novel $\texttt{CtxSimFit}$ ($\rho$=0.7--0.9) as well as
                   proposed context-infused versions of common metrics
                   ($\rho$=0.4--0.7). Overall, our findings highlight the
                   importance of integrating context into the generation and
                   especially the evaluation stages of stylistic text rewriting.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Author_undated-ls,
  title  = "Dual Governance: The intersection of centralized regulation and
            crowdsourced safety mechanisms for Generative {AI}",
  author = "Author, Anonymous"
}

@MISC{noauthor_undated-gp,
  title = "{DuBois\_2007}.pdf"
}

@ARTICLE{Hua2008-dk,
  title     = "Duelling Languages, Duelling Values: Codeswitching in bilingual
               intergenerational conflict talk in diasporic families",
  author    = "Hua, Zhu",
  journal   = "J. Pragmat.",
  publisher = "Elsevier",
  volume    =  40,
  number    =  10,
  pages     = "1799--1816",
  abstract  = "This paper aims to investigate the complex relationship between
               social interaction and socio-cultural values. Samples of conflict
               talk between parents and children in the Chinese diasporic
               families in the UK are examined. Through a detailed analysis of
               the sequential organization of codeswitching as well as what is
               termed as “talk about social and cultural practice”, we aim to
               demonstrate how conflicts in values and identities are
               negotiated, mediated and managed in bilingual interaction and the
               emergent nature of new family dynamics and values. The study
               provides new insights into the (changes of) socio-cultural values
               in diaspora and migrant communities and contributes to the
               development of a general theory of the pragmatics of bilingual
               codeswitching.",
  month     =  oct,
  year      =  2008,
  keywords  = "Chinese diasporic families; Codeswitching; Core values; Talk
               about social and cultural practice"
}

@ARTICLE{Akyurek2023-ff,
  title         = "{DUnE}: Dataset for Unified Editing",
  author        = "Akyürek, Afra Feyza and Pan, Eric and Kuwanto, Garry and
                   Wijaya, Derry",
  journal       = "arXiv [cs.CL]",
  abstract      = "Even the most advanced language models remain susceptible to
                   errors necessitating to modify these models without
                   initiating a comprehensive retraining process. Model editing
                   refers to the modification of a model's knowledge or
                   representations in a manner that produces the desired
                   outcomes. Prior research primarily centered around editing
                   factual data e.g. ``Messi plays for Inter Miami'' confining
                   the definition of an edit to a knowledge triplet i.e.
                   (subject, object, relation). However, as the applications of
                   language models expand, so do the diverse ways in which we
                   wish to edit and refine their outputs. In this study, we
                   broaden the scope of the editing problem to include an array
                   of editing cases such as debiasing and rectifying reasoning
                   errors and define an edit as any natural language expression
                   that solicits a change in the model's outputs. We are
                   introducing DUnE-an editing benchmark where edits are natural
                   language sentences and propose that DUnE presents a
                   challenging yet relevant task. To substantiate this claim, we
                   conduct an extensive series of experiments testing various
                   editing approaches to address DUnE, demonstrating their
                   respective strengths and weaknesses. We show that
                   retrieval-augmented language modeling can outperform
                   specialized editing techniques and neither set of approaches
                   has fully solved the generalized editing problem covered by
                   our benchmark.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Kaneko2024-jj,
  title    = "Eagle: Ethical dataset given from real interactions",
  author   = "Kaneko, Masahiro and Bollegala, D and Baldwin, Timothy",
  abstract = "Recent studies have demonstrated that large language models (LLMs)
              have ethical-related problems such as social biases, lack of moral
              reasoning, and generation of offensive content. The existing
              evaluation metrics and methods to address these ethical challenges
              use datasets intentionally created by instructing humans to create
              instances including ethical problems. Therefore, the data does not
              reflect prompts that users actually provide when utilizing LLM
              services in everyday contexts. This may not lead to the
              development of safe LLMs that can address ethical challenges
              arising in real-world applications. In this paper, we create Eagle
              datasets extracted from real interactions between ChatGPT and
              users that exhibit social biases, toxicity, and immoral problems.
              Our experiments show that Eagle captures complementary aspects,
              not covered by existing datasets proposed for evaluation and
              mitigation of such ethical challenges. Our code is publicly
              available at https://huggingface.co/datasets/MasahiroKaneko/eagle.",
  month    =  feb,
  year     =  2024
}

@ARTICLE{Bianchi2022-tx,
  title         = "Easily accessible text-to-image generation amplifies
                   demographic stereotypes at large scale",
  author        = "Bianchi, Federico and Kalluri, Pratyusha and Durmus, Esin and
                   Ladhak, Faisal and Cheng, Myra and Nozza, Debora and
                   Hashimoto, Tatsunori and Jurafsky, Dan and Zou, James and
                   Caliskan, Aylin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Machine learning models that convert user-written text
                   descriptions into images are now widely available online and
                   used by millions of users to generate millions of images a
                   day. We investigate the potential for these models to amplify
                   dangerous and complex stereotypes. We find a broad range of
                   ordinary prompts produce stereotypes, including prompts
                   simply mentioning traits, descriptors, occupations, or
                   objects. For example, we find cases of prompting for basic
                   traits or social roles resulting in images reinforcing
                   whiteness as ideal, prompting for occupations resulting in
                   amplification of racial and gender disparities, and prompting
                   for objects resulting in reification of American norms.
                   Stereotypes are present regardless of whether prompts
                   explicitly mention identity and demographic language or avoid
                   such language. Moreover, stereotypes persist despite
                   mitigation strategies; neither user attempts to counter
                   stereotypes by requesting images with specific
                   counter-stereotypes nor institutional attempts to add system
                   ``guardrails'' have prevented the perpetuation of
                   stereotypes. Our analysis justifies concerns regarding the
                   impacts of today's models, presenting striking exemplars, and
                   connecting these findings with deep insights into harms drawn
                   from social scientific and humanist disciplines. This work
                   contributes to the effort to shed light on the uniquely
                   complex biases in language-vision models and demonstrates the
                   ways that the mass deployment of text-to-image generation
                   models results in mass dissemination of stereotypes and
                   resulting harms.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Gupta2023-gp,
  title     = "Editing Commonsense Knowledge in {GPT}",
  author    = "Gupta, A and Mondal, D and Sheshadri, A K and Zhao, W and
               {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Memory editing methods for updating encyclopedic knowledge in
               transformers have received increasing attention for their
               efficacy, specificity, and generalization advantages …",
  year      =  2023
}

@ARTICLE{Yao2023-wf,
  title         = "Editing Large Language Models: Problems, Methods, and
                   Opportunities",
  author        = "Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng,
                   Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and
                   Zhang, Ningyu",
  journal       = "arXiv [cs.CL]",
  abstract      = "Despite the ability to train capable LLMs, the methodology
                   for maintaining their relevancy and rectifying errors remains
                   elusive. To this end, the past few years have witnessed a
                   surge in techniques for editing LLMs, the objective of which
                   is to efficiently alter the behavior of LLMs within a
                   specific domain without negatively impacting performance
                   across other inputs. This paper embarks on a deep exploration
                   of the problems, methods, and opportunities related to model
                   editing for LLMs. In particular, we provide an exhaustive
                   overview of the task definition and challenges associated
                   with model editing, along with an in-depth empirical analysis
                   of the most progressive methods currently at our disposal. We
                   also build a new benchmark dataset to facilitate a more
                   robust evaluation and pinpoint enduring issues intrinsic to
                   existing techniques. Our objective is to provide valuable
                   insights into the effectiveness and feasibility of each
                   editing technique, thereby assisting the community in making
                   informed decisions on the selection of the most appropriate
                   method for a specific task or context. Code and datasets are
                   available at https://github.com/zjunlp/EasyEdit.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Register2015-dm,
  title     = "Effects of Self-Objectification on Self-Reported Eating Pathology
               and Depression",
  author    = "Register, Joshua D and Katrevich, Alina V and Aruguete, Mara S
               and Edman, Jeanne L",
  journal   = "Am. J. Psychol.",
  publisher = "scholarlypublishingcollective.org",
  volume    =  128,
  number    =  1,
  pages     = "107--113",
  abstract  = "Self-objectification occurs when people internalize an observer's
               perspective onto their own bodies. This study experimentally
               examined the impacts of self-objectification on 156 male and
               female college students. We induced a state of
               self-objectification by having undergraduate students in an
               experimental condition describe their bodies in writing, from an
               observer's viewpoint. Participants then completed a questionnaire
               measuring self-reported eating pathology and depression. When
               compared with a control group, the self-objectification
               manipulation caused an increase in self-reported eating pathology
               in both men and women. The results support previous research
               finding broad, negative impacts of self-objectification.",
  year      =  2015,
  language  = "en"
}

@ARTICLE{Weizenbaum1966-uu,
  title     = "{ELIZA—a} computer program for the study of natural language
               communication between man and machine",
  author    = "Weizenbaum, Joseph",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  9,
  number    =  1,
  pages     = "36--45",
  abstract  = "ELIZA is a program operating within the MAC time-sharing system
               at MIT which makes certain kinds of natural language conversation
               between man and computer possible. Input sentences are analyzed
               on the basis of decomposition rules which are triggered by key
               words appearing in the input text. Responses are generated by
               reassembly rules associated with selected decomposition rules.
               The fundamental technical problems with which ELIZA is concerned
               are:(1) the identification of",
  month     =  jan,
  year      =  1966
}

@ARTICLE{Thomas_McCoy2023-ly,
  title         = "Embers of Autoregression: Understanding Large Language Models
                   Through the Problem They are Trained to Solve",
  author        = "Thomas McCoy, R and Yao, Shunyu and Friedman, Dan and Hardy,
                   Matthew and Griffiths, Thomas L",
  journal       = "arXiv [cs.CL]",
  abstract      = "The widespread adoption of large language models (LLMs) makes
                   it important to recognize their strengths and limitations. We
                   argue that in order to develop a holistic understanding of
                   these systems we need to consider the problem that they were
                   trained to solve: next-word prediction over Internet text. By
                   recognizing the pressures that this task exerts we can make
                   predictions about the strategies that LLMs will adopt,
                   allowing us to reason about when they will succeed or fail.
                   This approach - which we call the teleological approach -
                   leads us to identify three factors that we hypothesize will
                   influence LLM accuracy: the probability of the task to be
                   performed, the probability of the target output, and the
                   probability of the provided input. We predict that LLMs will
                   achieve higher accuracy when these probabilities are high
                   than when they are low - even in deterministic settings where
                   probability should not matter. To test our predictions, we
                   evaluate two LLMs (GPT-3.5 and GPT-4) on eleven tasks, and we
                   find robust evidence that LLMs are influenced by probability
                   in the ways that we have hypothesized. In many cases, the
                   experiments reveal surprising failure modes. For instance,
                   GPT-4's accuracy at decoding a simple cipher is 51\% when the
                   output is a high-probability word sequence but only 13\% when
                   it is low-probability. These results show that AI
                   practitioners should be careful about using LLMs in
                   low-probability situations. More broadly, we conclude that we
                   should not evaluate LLMs as if they are humans but should
                   instead treat them as a distinct type of system - one that
                   has been shaped by its own particular set of pressures.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Lee2015-am,
  title     = "Emotion in Code-switching Texts: Corpus Construction and Analysis",
  author    = "Lee, Sophia and Wang, Zhongqing",
  booktitle = "Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language
               Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Beijing, China",
  pages     = "91--99",
  abstract  = "Previous researches have focused on analyzing emotion through
               monolingual text, when in fact bilingual or code-switching posts
               are also common in social media. Despite the important
               implications of code-switching for emotion analysis, existing
               automatic emotion extraction methods fail to accommodate for the
               code-switching content. In this paper, we propose a general
               framework to construct and analyze the code-switching emotional
               posts in social media. We first propose an annotation scheme to
               identify the emotions associated …",
  month     =  jul,
  year      =  2015
}

@ARTICLE{noauthor_2024-sn,
  title     = "Empathic {AI} can’t get under the skin",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  6,
  number    =  5,
  pages     = "495--495",
  abstract  = "Personalized LLMs built with the capacity for emulating empathy
               are right around the corner. The effects on individual users need
               careful consideration.",
  month     =  may,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Schubert2022-hj,
  title   = "Employer concentration and outside options",
  author  = "Schubert, Gregor and Stansbury, Anna and Taska, Bledi",
  journal = "Available at SSRN 3599454",
  year    =  2022
}

@INPROCEEDINGS{Metzger2024-og,
  title     = "Empowering Calibrated (Dis-)Trust in Conversational Agents: A
               User Study on the Persuasive Power of Limitation Disclaimers vs.
               Authoritative Style",
  author    = "Metzger, Luise and Miller, Linda and Baumann, Martin and Kraus,
               Johannes",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Article 481",
  pages     = "1--19",
  abstract  = "While conversational agents based on Large Language Models (LLMs)
               can drive progress in many domains, they are prone to generating
               faulty information. To ensure an efficient, safe, and
               satisfactory user experience maximizing benefits of these
               systems, users must be empowered to judge the reliability of
               system outputs. In this, both disclaimers and agents’
               communicative style are pivotal design instances. In an online
               study with 594 participants, we investigated how these affect
               users’ trust and a mock-up agent’s persuasiveness, based on an
               established framework from social psychology. While prior
               information on potential inaccuracies or faulty information did
               not affect trust, an authoritative communicative style elicited
               more trust. Also, a trusted agent was more persuasive resulting
               in more positive attitudes regarding the subject of the
               conversation. Results imply that disclaimers on agents’
               limitations fail to effectively alter users’ trust but can be
               supported by appropriate communicative style during interaction.",
  series    = "CHI '24",
  month     =  may,
  year      =  2024,
  keywords  = "ChatGPT, chatbots, communicative style, conversational agents,
               elaboration likelihood model, large language models, trust in
               automation"
}

@MISC{noauthor_undated-ot,
  title        = "Ends against the middle: Measuring latent traits when
                  opposites respond the same way for antithetical reasons",
  howpublished = "\url{https://scholar.google.ca/scholar?cluster=7974619094188187244\&hl=en\&as\_sdt=0,5\&sciodt=0,5}",
  note         = "Accessed: 2023-11-9"
}

@ARTICLE{Strubell2019-bd,
  title     = "Energy and policy considerations for deep learning in {NLP}",
  author    = "Strubell, E and Ganesh, A and McCallum, A",
  journal   = "arXiv preprint arXiv:1906.02243",
  publisher = "arxiv.org",
  abstract  = "Recent progress in hardware and methodology for training neural
               networks has ushered in a new generation of large networks
               trained on abundant data. These models have obtained …",
  year      =  2019
}

@ARTICLE{Kumar2019-gi,
  title     = "Engaging feminist solidarity for comparative research, design,
               and practice",
  author    = "Kumar, N and Karusala, N and Ismail, A and {others}",
  journal   = "Proceedings of the",
  publisher = "dl.acm.org",
  abstract  = "Research in the fields of Computer Supported Cooperative Work
               (CSCW) and Human- Computer Interaction (HCI) is increasingly
               embracing and moving across borders. While …",
  year      =  2019
}

@ARTICLE{Nielsen2004-sa,
  title     = "Engaging personas and narrative scenarios",
  author    = "Nielsen, Lene",
  journal   = "PhD series",
  publisher = "personas.dk",
  volume    =  17,
  abstract  = "… The engaging persona is a description of the user in a scenario
               … engaging persona can be the user in one or more scenarios … the
               engaging persona is to enable the designers to engage …",
  year      =  2004
}

@ARTICLE{Prentice2020-oo,
  title    = "Engineering social change using social norms: lessons from the
              study of collective action",
  author   = "Prentice, Deborah and Paluck, Elizabeth Levy",
  journal  = "Curr Opin Psychol",
  volume   =  35,
  pages    = "138--142",
  abstract = "Behavioral interventions have embraced social norms as information
              that can be communicated in simple messages to motivate behavior
              change. This article argues for the value and necessity of
              recognizing that social-norm interventions are grounded in group
              processes. This approach has three major benefits that more than
              offset the costs of its greater theoretical and practical
              complexity. One, it improves the effectiveness of existing
              interventions, including those that target the normative beliefs
              of individuals. Two, it opens up new intervention strategies that
              broaden the range of mechanisms used to change behavior. Three, it
              connects research on social-norm interventions with theories and
              research on rallies, rebellions, riots, and other forms of
              collective action.",
  month    =  oct,
  year     =  2020,
  language = "en"
}

@ARTICLE{Lambert2023-ox,
  title         = "Entangled Preferences: The History and Risks of Reinforcement
                   Learning and Human Feedback",
  author        = "Lambert, Nathan and Gilbert, Thomas Krendl and Zick, Tom",
  journal       = "arXiv [cs.CY]",
  abstract      = "Reinforcement learning from human feedback (RLHF) has emerged
                   as a powerful technique to make large language models (LLMs)
                   easier to use and more effective. A core piece of the RLHF
                   process is the training and utilization of a model of human
                   preferences that acts as a reward function for optimization.
                   This approach, which operates at the intersection of many
                   stakeholders and academic disciplines, remains poorly
                   understood. RLHF reward models are often cited as being
                   central to achieving performance, yet very few descriptors of
                   capabilities, evaluations, training methods, or open-source
                   models exist. Given this lack of information, further study
                   and transparency is needed for learned RLHF reward models. In
                   this paper, we illustrate the complex history of optimizing
                   preferences, and articulate lines of inquiry to understand
                   the sociotechnical context of reward models. In particular,
                   we highlight the ontological differences between costs,
                   rewards, and preferences at stake in RLHF's foundations,
                   related methodological tensions, and possible research
                   directions to improve general understanding of how reward
                   models function.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Gandikota2023-ba,
  title         = "Erasing concepts from diffusion models",
  author        = "Gandikota, Rohit and Materzynska, Joanna and Fiotto-Kaufman,
                   Jaden and Bau, David",
  journal       = "arXiv [cs.CV]",
  abstract      = "Motivated by recent advancements in text-to-image diffusion,
                   we study erasure of specific concepts from the model's
                   weights. While Stable Diffusion has shown promise in
                   producing explicit or realistic artwork, it has raised
                   concerns regarding its potential for misuse. We propose a
                   fine-tuning method that can erase a visual concept from a
                   pre-trained diffusion model, given only the name of the style
                   and using negative guidance as a teacher. We benchmark our
                   method against previous approaches that remove sexually
                   explicit content and demonstrate its effectiveness,
                   performing on par with Safe Latent Diffusion and censored
                   training. To evaluate artistic style removal, we conduct
                   experiments erasing five modern artists from the network and
                   conduct a user study to assess the human perception of the
                   removed styles. Unlike previous methods, our approach can
                   remove concepts from a diffusion model permanently rather
                   than modifying the output at the inference time, so it cannot
                   be circumvented even if a user has access to model weights.
                   Our code, data, and results are available at
                   https://erasing.baulab.info/",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Rho2023-ai,
  title    = "Escalated police stops of Black men are linguistically and
              psychologically distinct in their earliest moments",
  author   = "Rho, Eugenia H and Harrington, Maggie and Zhong, Yuyang and
              Pryzant, Reid and Camp, Nicholas P and Jurafsky, Dan and
              Eberhardt, Jennifer L",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  120,
  number   =  23,
  pages    = "e2216162120",
  abstract = "Across the United States, police chiefs, city officials, and
              community leaders alike have highlighted the need to de-escalate
              police encounters with the public. This concern about escalation
              extends from encounters involving use of force to routine car
              stops, where Black drivers are disproportionately pulled over.
              Yet, despite the calls for action, we know little about the
              trajectory of police stops or how escalation unfolds. In study 1,
              we use methods from computational linguistics to analyze police
              body-worn camera footage from 577 stops of Black drivers. We find
              that stops with escalated outcomes (those ending in arrest,
              handcuffing, or a search) diverge from stops without these
              outcomes in their earliest moments-even in the first 45 words
              spoken by the officer. In stops that result in escalation,
              officers are more likely to issue commands as their opening words
              to the driver and less likely to tell drivers the reason why they
              are being stopped. In study 2, we expose Black males to audio
              clips of the same stops and find differences in how escalated
              stops are perceived: Participants report more negative emotion,
              appraise officers more negatively, worry about force being used,
              and predict worse outcomes after hearing only the officer's
              initial words in escalated versus non-escalated stops. Our
              findings show that car stops that end in escalated outcomes
              sometimes begin in an escalated fashion, with adverse effects for
              Black male drivers and, in turn, police-community relations.",
  month    =  jun,
  year     =  2023,
  keywords = "body-worn cameras; escalation; natural language processing (NLP);
              policing; race",
  language = "en"
}

@MISC{noauthor_undated-ze,
  title = "{eScholarship} {UC} item {53d397fg}.pdf"
}

@ARTICLE{Bradley2018-xl,
  title     = "Essentialism in the Concept of Culture: Gauging Belief",
  author    = "Bradley, Nicholas",
  publisher = "unknown",
  volume    =  21,
  pages     = "1--21",
  abstract  = "Many researchers and philosophers view essentialism as a negative
               force that impacts the way we view different social groupings and
               can lead to stereotyping and discrimination. Although a great
               deal of work has been done regarding essentialized understandings
               of concepts such as race and gender, and the groups contained
               within, there has been less work on the concept of culture and
               cultural groupings. This is surprising given that the concept is
               of great importance for people engaged in the ﬁelds of
               intercultural education and language education. Additionally, at
               the time of writing, no attempt had been made to develop an
               instrument that attempts to offer an indication of the level of
               essentialism present in a person’s concept of culture. As well as
               examining the literature on social essentialism this paper
               hypothesizes and tests an instrument and scale that, the author
               suggests, gives an indication of the level of essentialism in the
               concept of culture (ECC).",
  month     =  may,
  year      =  2018
}

@ARTICLE{Ritchie2021-qa,
  title     = "Essentializing Language and the Prospects for Ameliorative
               Projects",
  author    = "Ritchie, Katherine",
  journal   = "Ethics",
  publisher = "The University of Chicago Press",
  volume    =  131,
  number    =  3,
  pages     = "460--488",
  abstract  = "Some language encourages essentialist thinking. While
               philosophers have largely focused on generics and essentialism, I
               argue that nouns as a category are poised to refer to kinds and
               to promote representational essentializing. Our psychological
               propensity to essentialize when nouns are used reveals a
               limitation for antiessentialist ameliorative projects. Even
               ameliorated nouns can continue to underpin essentialist thinking.
               I conclude by arguing that representational essentialism does not
               doom antiessentialist ameliorative projects. Rather, it reveals
               that would-be ameliorators ought to attend to the propensities
               for our representational devices to essentialize and to the
               complex relationship between essentialism and prejudice.",
  month     =  apr,
  year      =  2021
}

@ARTICLE{Falbo2021-dv,
  title         = "Est-ce que vous compute? Code-switching, cultural identity,
                   and {AI}",
  author        = "Falbo, Arianna and LaCroix, Travis",
  journal       = "arXiv [cs.CY]",
  abstract      = "Cultural code-switching concerns how we adjust our overall
                   behaviours, manners of speaking, and appearance in response
                   to a perceived change in our social environment. We defend
                   the need to investigate cultural code-switching capacities in
                   artificial intelligence systems. We explore a series of
                   ethical and epistemic issues that arise when bringing
                   cultural code-switching to bear on artificial intelligence.
                   Building upon Dotson's (2014) analysis of testimonial
                   smothering, we discuss how emerging technologies in AI can
                   give rise to epistemic oppression, and specifically, a form
                   of self-silencing that we call 'cultural smothering'. By
                   leaving the socio-dynamic features of cultural code-switching
                   unaddressed, AI systems risk negatively impacting
                   already-marginalised social groups by widening opportunity
                   gaps and further entrenching social inequalities.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{WeidingerUnknown-ni,
  title     = "Ethical and social risks of harm from language models",
  author    = "Weidinger, L and Mellor, J and Rauh, M and Griffin, C and
               {others}",
  publisher = "arxiv.org"
}

@ARTICLE{Udupa2023-bq,
  title     = "Ethical scaling for content moderation: Extreme speech and the
               (in)significance of artificial intelligence",
  author    = "Udupa, Sahana and Maronikolakis, Antonis and Wisiorek, Axel",
  journal   = "Big Data \& Society",
  publisher = "SAGE Publications Ltd",
  volume    =  10,
  number    =  1,
  pages     =  20539517231172424,
  abstract  = "In this article, we present new empirical evidence to demonstrate
               the severe limitations of existing machine learning content
               moderation methods to keep pace with, let alone stay ahead of,
               hateful language online. Building on the collaborative coding
               project ?AI4Dignity? we outline the ambiguities and complexities
               of annotating problematic text in AI-assisted moderation systems.
               We diagnose the shortcomings of the content moderation and
               natural language processing approach as emerging from a broader
               epistemological trapping wrapped in the liberal-modern idea of
               ?the human?. Presenting a decolonial critique of the ?human vs
               machine? conundrum and drawing attention to the structuring
               effects of coloniality on extreme speech, we propose ?ethical
               scaling? to highlight moderation process as political praxis. As
               a normative framework for platform governance, ethical scaling
               calls for a transparent, reflexive, and replicable process of
               iteration for content moderation with community participation and
               global parity, which should evolve in conjunction with addressing
               algorithmic amplification of divisive content and resource
               allocation for content moderation.",
  month     =  jan,
  year      =  2023
}

@ARTICLE{Danilina2019-gh,
  title     = "Euphemisms in advertising discourse: Putting on a positive face
               and maintaining speech etiquette",
  author    = "Danilina, Elena A and Kizyan, Ekaterina E and Maksimova, Daria S",
  journal   = "Training, Language and Culture",
  publisher = "Федеральное государственное автономное образовательное учреждение
               высшего образования «Российский университет дружбы народов»",
  address   = "Россия, Москва",
  volume    =  3,
  number    =  1,
  pages     = "8--22",
  abstract  = "The study describes advertising discourse as unique in terms of
               its manipulative potential and attempts to observe the way
               euphemistic units are used in English commercial and social
               advertising. The authors highlight the two key functions: a
               ‘call-to-action’ function of commercial ads and the ‘raising
               awareness’ function of social ads. The study relies on the theory
               of politeness and the concept of face suggested by Brown and
               Levinson to apply the same principles in the analysis of English
               advertising texts. The authors consider different categories of
               goods and end products in both commercial and social advertising
               discourse to identify the main goals and strategies behind
               euphemisation as a manipulating mechanism and a language tool
               allowing to observe the socially accepted standards of speech
               etiquette.",
  year      =  2019,
  keywords  = "ADVERTISING DISCOURSE, EUPHEMISM, POLITENESS, SPEECH ETIQUETTE,
               MANIPULATION; ADVERTISING DISCOURSE, EUPHEMISM, POLITENESS,
               SPEECH ETIQUETTE, MANIPULATION"
}

@ARTICLE{Tamkin2023-sp,
  title     = "Evaluating and mitigating discrimination in language model
               decisions",
  author    = "Tamkin, A and Askell, A and Lovitt, L and Durmus, E and {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "As language models (LMs) advance, interest is growing in applying
               them to high-stakes societal decisions, such as determining
               financing or housing eligibility. However, their …",
  year      =  2023
}

@INPROCEEDINGS{Omrani_Sabbaghi2023-up,
  title     = "Evaluating Biased Attitude Associations of Language Models in an
               Intersectional Context",
  author    = "Omrani Sabbaghi, Shiva and Wolfe, Robert and Caliskan, Aylin",
  booktitle = "Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and
               Society",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "542--553",
  abstract  = "Language models are trained on large-scale corpora that embed
               implicit biases documented in psychology. Valence associations
               (pleasantness/unpleasantness) of social groups determine the
               biased attitudes towards groups and concepts in social cognition.
               Building on this established literature, we quantify how social
               groups are valenced in English language models using a sentence
               template that provides an intersectional context. We study biases
               related to age, education, gender, height, intelligence,
               literacy, race, religion, sex, sexual orientation, social class,
               and weight. We present a concept projection approach to capture
               the valence subspace through contextualized word embeddings of
               language models. Adapting the projection-based approach to
               embedding association tests that quantify bias, we find that
               language models exhibit the most biased attitudes against gender
               identity, social class, and sexual orientation signals in
               language. We find that the largest and better-performing model
               that we study is also more biased as it effectively captures bias
               embedded in sociocultural data. We validate the bias evaluation
               method by overperforming on an intrinsic valence evaluation task.
               The approach enables us to measure complex intersectional biases
               as they are known to manifest in the outputs and applications of
               language models that perpetuate historical biases. Moreover, our
               approach contributes to design justice as it studies the
               associations of groups underrepresented in language such as
               transgender and homosexual individuals.",
  series    = "AIES '23",
  month     =  aug,
  year      =  2023,
  keywords  = "psycholinguistics, language models, intersectional bias, AI bias,
               contextualized word embeddings"
}

@ARTICLE{Sabbaghi2023-rq,
  title         = "Evaluating Biased Attitude Associations of Language Models in
                   an Intersectional Context",
  author        = "Sabbaghi, Shiva Omrani and Wolfe, Robert and Caliskan, Aylin",
  journal       = "arXiv [cs.CY]",
  abstract      = "Language models are trained on large-scale corpora that embed
                   implicit biases documented in psychology. Valence
                   associations (pleasantness/unpleasantness) of social groups
                   determine the biased attitudes towards groups and concepts in
                   social cognition. Building on this established literature, we
                   quantify how social groups are valenced in English language
                   models using a sentence template that provides an
                   intersectional context. We study biases related to age,
                   education, gender, height, intelligence, literacy, race,
                   religion, sex, sexual orientation, social class, and weight.
                   We present a concept projection approach to capture the
                   valence subspace through contextualized word embeddings of
                   language models. Adapting the projection-based approach to
                   embedding association tests that quantify bias, we find that
                   language models exhibit the most biased attitudes against
                   gender identity, social class, and sexual orientation signals
                   in language. We find that the largest and better-performing
                   model that we study is also more biased as it effectively
                   captures bias embedded in sociocultural data. We validate the
                   bias evaluation method by overperforming on an intrinsic
                   valence evaluation task. The approach enables us to measure
                   complex intersectional biases as they are known to manifest
                   in the outputs and applications of language models that
                   perpetuate historical biases. Moreover, our approach
                   contributes to design justice as it studies the associations
                   of groups underrepresented in language such as transgender
                   and homosexual individuals.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Devaraj2022-md,
  title    = "Evaluating Factuality in Text Simplification",
  author   = "Devaraj, Ashwin and Sheffield, William and Wallace, Byron C and
              Li, Junyi Jessy",
  journal  = "Proc Conf Assoc Comput Linguist Meet",
  volume   =  2022,
  pages    = "7331--7345",
  abstract = "Automated simplification models aim to make input texts more
              readable. Such methods have the potential to make complex
              information accessible to a wider audience, e.g., providing access
              to recent medical literature which might otherwise be impenetrable
              for a lay reader. However, such models risk introducing errors
              into automatically simplified texts, for instance by inserting
              statements unsupported by the corresponding original text, or by
              omitting key information. Providing more readable but inaccurate
              versions of texts may in many cases be worse than providing no
              such access at all. The problem of factual accuracy (and the lack
              thereof) has received heightened attention in the context of
              summarization models, but the factuality of automatically
              simplified texts has not been investigated. We introduce a
              taxonomy of errors that we use to analyze both references drawn
              from standard simplification datasets and state-of-the-art model
              outputs. We find that errors often appear in both that are not
              captured by existing evaluation metrics, motivating a need for
              research into ensuring the factual accuracy of automated
              simplification models.",
  month    =  may,
  year     =  2022,
  language = "en"
}

@ARTICLE{Lee2022-sw,
  title     = "Evaluating human-language model interaction",
  author    = "Lee, M and Srivastava, M and Hardy, A and Thickstun, J and
               {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Many real-world applications of language models (LMs), such as
               code autocomplete and writing assistance, involve human-LM
               interaction, but the main LM benchmarks are non …",
  year      =  2022
}

@ARTICLE{Hamalainen2023-zg,
  title     = "Evaluating large language models in generating synthetic {HCI}
               research data: A case study",
  author    = "Hämäläinen, Perttu and Tavast, Mikke and Kunnari, Anton",
  journal   = "Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems (CHI '23), April 23â•ﬁ28, 2023, Hamburg,
               Germany",
  publisher = "Association for Computing Machinery",
  volume    =  1,
  number    =  1,
  abstract  = "Collecting data is one of the bottlenecks of Human-Computer
               Interaction (HCI) research. Motivated by this, we explore the
               potential of large language models (LLMs) in generating synthetic
               user research data. We use OpenAI's GPT-3 model to generate
               open-ended questionnaire responses about experiencing video games
               as art, a topic not tractable with traditional computational user
               models. We test whether synthetic responses can be distinguished
               from real responses, analyze errors of synthetic data, and
               investigate content …",
  year      =  2023
}

@PHDTHESIS{Mackey2023-yo,
  title     = "Evaluating the evolution of the English language as seen in
               {TikTok} slang",
  author    = "Mackey, Abigail Elizabeth",
  publisher = "Wichita State University",
  abstract  = "This study will evaluate the perception of slang terminology seen
               on the massively popular social media site TikTok and whether
               said terms are appropriate in certain contexts. This is alongside
               some terms that have existed for years, and that show signs of
               entering the English lexicon as formal, or ‘standard’ English,
               despite their former standing as immature slang. First, 100
               TikTok videos were viewed using a new account created for study
               purposes, as well as another 100 videos on an existing account. A
               selection of slang terminology seen on these TikToks was then
               provided to 26 participants and they were asked to rate the terms
               using two Likert scales. In the first task, participants were
               asked to determine whether terms were understood as a slang term,
               a standard/usual term, or both, as well as whether they were
               appropriate to use in certain contexts. In the second task,
               participants then evaluated a selection of sentences and
               determined whether the sentences are grammatical or not. Every
               term presented to the participants were noted as being
               understood, and semantic analysis revealed that sentences
               intended to be grammatical or ungrammatical were noted as such.
               Further analysis revealed that older terms like [wanna] and
               [gonna] could be going through significant morphological change,
               while less popular terms might be used infrequently due to
               instances of usage considered appropriate being hyper-specific.
               This implies slow but tangible shift in English grammar.",
  month     =  may,
  year      =  2023,
  keywords  = "Thesis",
  school    = "Wichita State University",
  language  = "en"
}

@ARTICLE{Scherrer2023-dd,
  title         = "Evaluating the Moral Beliefs Encoded in {LLMs}",
  author        = "Scherrer, Nino and Shi, Claudia and Feder, Amir and Blei,
                   David M",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper presents a case study on the design,
                   administration, post-processing, and evaluation of surveys on
                   large language models (LLMs). It comprises two components:
                   (1) A statistical method for eliciting beliefs encoded in
                   LLMs. We introduce statistical measures and evaluation
                   metrics that quantify the probability of an LLM ``making a
                   choice'', the associated uncertainty, and the consistency of
                   that choice. (2) We apply this method to study what moral
                   beliefs are encoded in different LLMs, especially in
                   ambiguous cases where the right choice is not obvious. We
                   design a large-scale survey comprising 680 high-ambiguity
                   moral scenarios (e.g., ``Should I tell a white lie?'') and
                   687 low-ambiguity moral scenarios (e.g., ``Should I stop for
                   a pedestrian on the road?''). Each scenario includes a
                   description, two possible actions, and auxiliary labels
                   indicating violated rules (e.g., ``do not kill''). We
                   administer the survey to 28 open- and closed-source LLMs. We
                   find that (a) in unambiguous scenarios, most models
                   ``choose'' actions that align with commonsense. In ambiguous
                   cases, most models express uncertainty. (b) Some models are
                   uncertain about choosing the commonsense action because their
                   responses are sensitive to the question-wording. (c) Some
                   models reflect clear preferences in ambiguous scenarios.
                   Specifically, closed-source models tend to agree with each
                   other.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Liu2023-fu,
  title         = "Evaluating Verifiability in Generative Search Engines",
  author        = "Liu, Nelson F and Zhang, Tianyi and Liang, Percy",
  journal       = "arXiv [cs.CL]",
  abstract      = "Generative search engines directly generate responses to user
                   queries, along with in-line citations. A prerequisite trait
                   of a trustworthy generative search engine is verifiability,
                   i.e., systems should cite comprehensively (high citation
                   recall; all statements are fully supported by citations) and
                   accurately (high citation precision; every cite supports its
                   associated statement). We conduct human evaluation to audit
                   four popular generative search engines -- Bing Chat, NeevaAI,
                   perplexity.ai, and YouChat -- across a diverse set of queries
                   from a variety of sources (e.g., historical Google user
                   queries, dynamically-collected open-ended questions on
                   Reddit, etc.). We find that responses from existing
                   generative search engines are fluent and appear informative,
                   but frequently contain unsupported statements and inaccurate
                   citations: on average, a mere 51.5\% of generated sentences
                   are fully supported by citations and only 74.5\% of citations
                   support their associated sentence. We believe that these
                   results are concerningly low for systems that may serve as a
                   primary tool for information-seeking users, especially given
                   their facade of trustworthiness. We hope that our results
                   further motivate the development of trustworthy generative
                   search engines and help researchers and users better
                   understand the shortcomings of existing commercial systems.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Fraser2024-vf,
  title    = "Examining gender and racial bias in large vision–language models
              using a novel dataset of {PArallel} Images",
  author   = "Fraser, Kathleen C and Kiritchenko, S",
  journal  = "Conf Eur Chapter Assoc Comput Linguistics",
  volume   = "abs/2402.05779",
  abstract = "Following on recent advances in large language models (LLMs) and
              subsequent chat models, a new wave of large vision–language models
              (LVLMs) has emerged. Such models can incorporate images as input
              in addition to text, and perform tasks such as visual question
              answering, image captioning, story generation, etc. Here, we
              examine potential gender and racial biases in such systems, based
              on the perceived characteristics of the people in the input
              images. To accomplish this, we present a new dataset PAIRS
              (PArallel Images for eveRyday Scenarios). The PAIRS dataset
              contains sets of AI-generated images of people, such that the
              images are highly similar in terms of background and visual
              content, but differ along the dimensions of gender (man, woman)
              and race (Black, white). By querying the LVLMs with such images,
              we observe significant differences in the responses according to
              the perceived gender or race of the person depicted.",
  month    =  feb,
  year     =  2024
}

@ARTICLE{Wanzer2020-bj,
  title     = "Experiencing flow while viewing art: Development of the Aesthetic
               Experience Questionnaire",
  author    = "Wanzer, D L and Finley, K P and Zarian, S and {others}",
  journal   = "Psychology of Aesthetics",
  publisher = "psycnet.apa.org",
  abstract  = "Aesthetic experiences are the attitudes, perceptions, or acts of
               attention involved with viewing art. When the viewer is fully
               engaged, aesthetic experiences are comparable with …",
  year      =  2020
}

@ARTICLE{Kahalon2018-ba,
  title    = "Experimental Studies on State Self-Objectification: A Review and
              an Integrative Process Model",
  author   = "Kahalon, Rotem and Shnabel, Nurit and Becker, Julia C",
  journal  = "Front. Psychol.",
  volume   =  9,
  pages    =  1268,
  abstract = "This paper provides an organizing framework for the experimental
              research on the effects of state self-objectification on women. We
              explain why this body of work, which had grown rapidly in the last
              20 years, departs from the original formulation of objectification
              theory (Fredrickson and Roberts, 1997). We compare the different
              operationalizations of state self-objectification and examine how
              they map onto its theoretical definition, concluding that the
              operationalizations have focused mostly on one component of this
              construct (concerns about one's physical appearance) while
              neglecting others (adopting a third-person perspective and
              treating oneself as a dehumanized object). We review the main
              findings of studies that experimentally induced state
              self-objectification and examined its affective, motivational,
              behavioral, cognitive, and physiological outcomes. We note that
              three core outcomes of this state as specified by objectification
              theory (safety anxiety, reduced flow experiences, and awareness of
              internal body states) have hardly been examined so far. Most
              importantly, we introduce an integrative process model, suggesting
              that the reported effects are triggered by four different
              mechanisms: appearance monitoring, experience of discrepancy from
              appearance standards, stereotype threat, and activation of the
              ``sex object'' schema. We propose strategies for distinguishing
              between these mechanisms and explain the theoretical and practical
              importance of doing so.",
  month    =  aug,
  year     =  2018,
  keywords = "an integrative process model; appearance monitoring; appearance
              standards; experimental research; objectification theory; schema
              activation; self-objectification; stereotype threat",
  language = "en"
}

@ARTICLE{Calabrese2022-ge,
  title     = "Explainable Abuse Detection as Intent Classification and Slot
               Filling",
  author    = "Calabrese, Agostina and Ross, Björn and Lapata, Mirella",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts
               02142, USA …",
  volume    =  10,
  pages     = "1440--1454",
  year      =  2022
}

@INPROCEEDINGS{Miller2023-mz,
  title     = "Explainable {AI} is Dead, Long Live Explainable {AI}!
               Hypothesis-driven Decision Support using Evaluative {AI}",
  author    = "Miller, Tim",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "333--342",
  abstract  = "In this paper, we argue for a paradigm shift from the current
               model of explainable artificial intelligence (XAI), which may be
               counter-productive to better human decision making. In early
               decision support systems, we assumed that we could give people
               recommendations and that they would consider them, and then
               follow them when required. However, research found that people
               often ignore recommendations because they do not trust them; or
               perhaps even worse, people follow them blindly, even when the
               recommendations are wrong. Explainable artificial intelligence
               mitigates this by helping people to understand how and why models
               give certain recommendations. However, recent research shows that
               people do not always engage with explainability tools enough to
               help improve decision making. The assumption that people will
               engage with recommendations and explanations has proven to be
               unfounded. We argue this is because we have failed to account for
               two things. First, recommendations (and their explanations) take
               control from human decision makers, limiting their agency.
               Second, giving recommendations and explanations does not align
               with the cognitive processes employed by people making decisions.
               This position paper proposes a new conceptual framework called
               Evaluative AI for explainable decision support. This is a
               machine-in-the-loop paradigm in which decision support tools
               provide evidence for and against decisions made by people, rather
               than provide recommendations to accept or reject. We argue that
               this mitigates issues of over- and under-reliance on decision
               support tools, and better leverages human expertise in decision
               making.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023
}

@ARTICLE{Vasconcelos2023-bm,
  title     = "Explanations Can Reduce Overreliance on {AI} Systems During
               Decision-Making",
  author    = "Vasconcelos, Helena and Jörke, Matthew and Grunde-McLaughlin,
               Madeleine and Gerstenberg, Tobias and Bernstein, Michael S and
               Krishna, Ranjay",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  7,
  number    = "CSCW1",
  pages     = "1--38",
  abstract  = "Prior work has identified a resilient phenomenon that threatens
               the performance of human-AI decision-making teams: overreliance,
               when people agree with an AI, even when it is incorrect.
               Surprisingly, overreliance does not reduce when the AI produces
               explanations for its predictions, compared to only providing
               predictions. Some have argued that overreliance results from
               cognitive biases or uncalibrated trust, attributing overreliance
               to an inevitability of human cognition. By contrast, our paper
               argues that people strategically choose whether or not to engage
               with an AI explanation, demonstrating empirically that there are
               scenarios where AI explanations reduce overreliance. To achieve
               this, we formalize this strategic choice in a cost-benefit
               framework, where the costs and benefits of engaging with the task
               are weighed against the costs and benefits of relying on the AI.
               We manipulate the costs and benefits in a maze task, where
               participants collaborate with a simulated AI to find the exit of
               a maze. Through 5 studies (N = 731), we find that costs such as
               task difficulty (Study 1), explanation difficulty (Study 2, 3),
               and benefits such as monetary compensation (Study 4) affect
               overreliance. Finally, Study 5 adapts the Cognitive Effort
               Discounting paradigm to quantify the utility of different
               explanations, providing further support for our framework. Our
               results suggest that some of the null effects found in literature
               could be due in part to the explanation not sufficiently reducing
               the costs of verifying the AI's prediction.",
  month     =  apr,
  year      =  2023,
  keywords  = "cost-benefit analysis, decision-making, explainable AI, human-AI
               collaboration"
}

@ARTICLE{Iqbal2014-iy,
  title     = "Exploitation of women in beauty products of Fair and Lovely: A
               critical discourse analysis study",
  author    = "Iqbal, Asma and Danish, Malik Haqnawaz and Tahir, Maria Raja",
  journal   = "International Journal on Studies in English Language and
               Literature",
  publisher = "academia.edu",
  volume    =  2,
  number    =  9,
  pages     = "122--131",
  abstract  = "This study will examine beauty advertisement of a product from
               the perspective of Critical Discourse Analysis. This study is
               basically focus on the use of language in fairness cream …",
  year      =  2014
}

@INPROCEEDINGS{Dabre2019-ll,
  title     = "Exploiting Multilingualism through Multistage Fine-Tuning for
               Low-Resource Neural Machine Translation",
  author    = "Dabre, Raj and Fujita, Atsushi and Chu, Chenhui",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in
               Natural Language Processing and the 9th International Joint
               Conference on Natural Language Processing (EMNLP-IJCNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Hong Kong, China",
  pages     = "1410--1416",
  abstract  = "This paper highlights the impressive utility of multi-parallel
               corpora for transfer learning in a one-to-many low-resource
               neural machine translation (NMT) setting. We report on a
               systematic comparison of multistage fine-tuning configurations,
               consisting of (1) pre-training on an external large (209k--440k)
               parallel corpus for English and a helping target language, (2)
               mixed pre-training or fine-tuning on a mixture of the external
               and low-resource (18k) target parallel corpora, and (3) pure
               fine-tuning on the target parallel corpora. Our experiments
               confirm that multi-parallel corpora are extremely useful despite
               their scarcity and content-wise redundancy thus exhibiting the
               true power of multilingualism. Even when the helping target
               language is not one of the target languages of our concern, our
               multistage fine-tuning can give 3--9 BLEU score gains over a
               simple one-to-one model.",
  month     =  nov,
  year      =  2019
}

@BOOK{Kecskes2007-jc,
  title     = "Explorations in Pragmatics: Linguistic, Cognitive, and
               Intercultural Aspects",
  author    = "Kecskés, István and Horn, Laurence R",
  publisher = "Walter de Gruyter",
  abstract  = "The papers in this volume reflect current trends in international
               research in pragmatics over recent years. The unique feature of
               the book is that the authors coming from ten different countries
               represent all aspects of pragmatics and address issues that have
               emerged as the result of recent research in pragmatics proper and
               neighboring fields such as cognitive psychology, philosophy, and
               communication. Recent theoretical work on the
               semantics/pragmatics interface, empirical work within cognitive
               and developmental psychology, intercultural communication and
               bilingual pragmatics have directed attention to issues that
               warrant reexamination and revision of some of the central tenets
               and claims of the field of pragmatics. In addition, cultural
               changes originating from globalization have affected the relation
               of language to the wider world. In particular, the spread of
               English as a global language has led to the emergence of issues
               of usage, power, and control that must be dealt with in a
               comprehensive pragmatics of language. Pragmatic theories have
               traditionally emphasized the importance of intention,
               rationality, cooperation, common ground, mutual knowledge,
               relevance, and commitment in the formation and execution of
               communicative acts. The new approaches to pragmatic research
               reflected in this volume, while not questioning the central role
               of these factors, extend the purview of the discipline to allow
               for a more comprehensive picture of their functioning and
               interrelationship within the dynamics of communication. The
               papers address these issues from a variety of directions. In Part
               I, Searle and Horn examine language use and pragmatics from a
               philosophical perspective. In Part II, the cognitive aspect of
               pragmatics is represented in the papers of Moeschler, Ruiz de
               Mendoza \& Baicchi, and Giora. They focus on well-known domains
               such as illocutionary constructions, the pragmatics of negation,
               and the relevance-theoretic concept of explicature. However, each
               paper sheds new light on the familiar concepts. The papers in
               Part III by Mey, Kecskes and Grundy discuss the intercultural
               aspects of pragmatics while Terkourafi explores the explanatory
               potential of an interpretation of Grice's Cooperative Principle.
               Margerie's and Geeraert \& Kristiansen's articles focus on the
               application of usage-based methodology in different ways within
               pragmatics.",
  year      =  2007,
  language  = "en"
}

@ARTICLE{Collins2023-rm,
  title     = "Exploring psychological constructs in people receiving treatment
               for addictive eating behaviours:“I hate loving food as much as
               {I} do”",
  author    = "Collins, R A and Duncanson, K and Skinner, J A and Hay, P J and
               {others}",
  journal   = "Behavioral",
  publisher = "mdpi.com",
  abstract  = "Research into the complexities of addictive eating behaviours
               continues to develop, as a deeper understanding of this construct
               beyond self-report diagnostic tools emerges. In this …",
  year      =  2023
}

@ARTICLE{Arroyo2012-cz,
  title     = "Exploring the Causes and Consequences of Engaging in Fat Talk",
  author    = "Arroyo, Analisa and Harwood, Jake",
  journal   = "J. Appl. Commun. Res.",
  publisher = "Routledge",
  volume    =  40,
  number    =  2,
  pages     = "167--187",
  abstract  = "Fat talk refers to the ritualistic conversations about one's own
               and others' bodies (e.g., ?I'm so fat!? ?No you're not, I'm the
               one who is fat!?). What we say about ourselves has implications
               for how we make sense of and evaluate ourselves and those around
               us; thus, the current research presents the results of two
               studies that sought to identify potential causes and consequences
               of fat talk. Mutually reinforcing effects were predicted between
               fat talk and both body image and mental health issues. In two
               studies, participants completed closed-ended scales reporting
               their use of fat talk, body satisfaction, perceived pressure to
               be thin, self-esteem, and depression. Across a three-week span,
               Study 1 found fat talk to predict lower body satisfaction and
               higher depression; fat talk also mediated the association between
               body weight concerns and mental health problems. Study 2 found,
               across a two-week span, fat talk to predict higher levels of
               depression and perceived sociocultural pressure to be thin. In
               addition, low body satisfaction predicted more fat talk. Results
               suggest that reducing the amount of fat talk weakens its
               connection to negative aspects of self-concept. Health campaigns,
               interpersonal strategies, and more positive forms of
               weight-related communication are discussed as possible ways to
               potentially reduce the negative effects of fat talk on both body
               image and mental health issues.",
  month     =  may,
  year      =  2012
}

@ARTICLE{Bail2018-dk,
  title    = "Exposure to opposing views on social media can increase political
              polarization",
  author   = "Bail, Christopher A and Argyle, Lisa P and Brown, Taylor W and
              Bumpus, John P and Chen, Haohan and Hunzaker, M B Fallin and Lee,
              Jaemin and Mann, Marcus and Merhout, Friedolin and Volfovsky,
              Alexander",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  115,
  number   =  37,
  pages    = "9216--9221",
  abstract = "There is mounting concern that social media sites contribute to
              political polarization by creating ``echo chambers'' that insulate
              people from opposing views about current events. We surveyed a
              large sample of Democrats and Republicans who visit Twitter at
              least three times each week about a range of social policy issues.
              One week later, we randomly assigned respondents to a treatment
              condition in which they were offered financial incentives to
              follow a Twitter bot for 1 month that exposed them to messages
              from those with opposing political ideologies (e.g., elected
              officials, opinion leaders, media organizations, and nonprofit
              groups). Respondents were resurveyed at the end of the month to
              measure the effect of this treatment, and at regular intervals
              throughout the study period to monitor treatment compliance. We
              find that Republicans who followed a liberal Twitter bot became
              substantially more conservative posttreatment. Democrats exhibited
              slight increases in liberal attitudes after following a
              conservative Twitter bot, although these effects are not
              statistically significant. Notwithstanding important limitations
              of our study, these findings have significant implications for the
              interdisciplinary literature on political polarization and the
              emerging field of computational social science.",
  month    =  sep,
  year     =  2018,
  keywords = "computational social science; political polarization; social
              media; social networks; sociology",
  language = "en"
}

@ARTICLE{Rahman2018-jx,
  title     = "Extreme Overvalued Beliefs: How Violent Extremist Beliefs Become
               “Normalized”",
  author    = "Rahman, Tahir",
  journal   = "Behav. Sci.",
  publisher = "Multidisciplinary Digital Publishing Institute",
  volume    =  8,
  number    =  1,
  pages     =  10,
  abstract  = "Extreme overvalued beliefs (EOB) are rigidly held, non-deusional
               beliefs that are the motive behind most acts of terrorism and
               mass shootings. EOBs are differentiated from delusions and
               obsessions. The concept of an overvalued idea was first described
               by Wernicke and later applied to terrorism by McHugh. Our group
               of forensic psychiatrists (Rahman, Resnick, Harry) refined the
               definition as an aid in the differential diagnosis seen in acts
               of violence. The form and content of EOBs is discussed as well as
               group effects, conformity, and obedience to authority. Religious
               cults such as The People’s Temple, Heaven’s Gate, Aum Shinrikyo,
               and Islamic State (ISIS) and conspiracy beliefs such as
               assassinations, moon-hoax, and vaccine-induced autism beliefs are
               discussed using this construct. Finally, some concluding thoughts
               on countering violent extremism, including its online presence is
               discussed utilizing information learned from online eating
               disorders and consumer experience.",
  month     =  jan,
  year      =  2018,
  language  = "en"
}

@MISC{noauthor_undated-pa,
  title = "{FAccT\_2023\_paper\_298}.pdf"
}

@ARTICLE{Hanawa2023-qn,
  title    = "Face image {DE}-identification based on feature embedding for
              privacy protection",
  author   = "Hanawa, Goki and Ito, Koichi and Aoki, T",
  journal  = "Biom Electron Signat",
  pages    = "1--6",
  abstract = "With the expansion of social networking services, a large number
              of face images have been disclosed on the Internet. Since face
              recognition makes it easy to collect face images of specific
              persons, the collected face images can be used to attack face
              recognition systems, such as spoofing attacks. Face image
              de-identification, which makes face recognition difficult without
              changing the appearance of the face image, is necessary for
              disclosing face images safely on the Internet. In this paper, we
              propose a face image de-identification method by embedding facial
              features of another person into a face image. The proposed method
              uses a convolutional neural network to generate a face image that
              can be recognized as that of another person while preserving the
              appearance of the face image. Through a set of experiments using a
              public face image dataset, we demonstrate that the proposed method
              preserves the appearance of face images and has high
              de-identification performance against unknown face recognition
              models compared to conventional methods.",
  month    =  sep,
  year     =  2023
}

@ARTICLE{Darwin2021-eo,
  title     = "Factions, frames, and postfeminism(s) in the Body Positive
               Movement",
  author    = "Darwin, Helana and Miller, Amara",
  journal   = "Feminist Media Studies",
  publisher = "Routledge",
  volume    =  21,
  number    =  6,
  pages     = "873--890",
  abstract  = "How are contemporary post/feminisms like Body Positivity
               constituted in and through digital media cultures, and what are
               the consequences for diverse movement participants? This article
               analyzes tensions within the Body Positive Movement in North
               America through a discourse analysis of two sets of prominent
               blog articles from 2014 and 2016. We identify the coexistence of
               four social movement frames, termed Mainstream Body Positivity,
               Fat Positivity = Body Positivity, Radical Body Positivity, and
               Body Neutrality. Key differences between these frames often arise
               due to intersecting experiences of privilege and oppression
               within the movement, as well as disagreements between activists
               about whether the movement should focus on individualized,
               psychological issues such as body image or structural concerns
               such as size discrimination. Our findings reveal the coterminous
               existence and blurring of boundaries of various feminisms within
               Body Positivity, including forms that align with as well as those
               that challenge postfeminist sensibilities. This study 1)
               illustrates how feminist organizing in digital spaces continues
               to be shaped by unequal power dynamics and 2) deepens our
               understanding of the complex and multivocal nature of online
               feminist discourse.",
  month     =  aug,
  year      =  2021
}

@ARTICLE{Ovalle_undated-qo,
  title  = "Factoring the Matrix of Domination: A Critical Review and
            Reimagination of Intersectionality in {AI} Fairness",
  author = "Ovalle, Anaelia and Subramonian, Arjun and Gautam, Vagrant"
}

@ARTICLE{Cipollina2023-ia,
  title     = "Factors that contribute to accurately perceiving anti-black
               racism and sexism overlap",
  author    = "Cipollina, Rebecca and Chaney, Kimberly E and Sanchez, Diana T",
  journal   = "J. Soc. Psychol.",
  publisher = "Taylor \& Francis",
  pages     = "1--19",
  abstract  = "Past research demonstrates that prejudice toward women and Black
               Americans often co-occur in individuals. The present studies
               examine factors related to accuracy in estimating the
               co-occurrence, or overlap, of prejudice toward women and Black
               Americans. Across two studies, criterion overlap percentages were
               computed using national datasets and separate participant samples
               estimated prejudice overlap. Results indicate that beliefs about
               the generalized nature of prejudice can improve accuracy by
               reducing faulty underestimation of the overlap in anti-Black
               racism and sexism. In addition to greater displayed accuracy in
               perceptions of prototypical perpetrators of prejudice (i.e.,
               estimates of White men compared to White women), the present work
               suggests that accuracy is improved when estimating sexist
               attitudes from racist attitudes, rather than vice versa.
               Together, this work documents the accuracy of prejudice overlap
               perceptions, for the first time, and factors that facilitate
               accuracy (i.e., perpetrator prototypicality, known prejudicial
               attitude), with implications for intergroup dynamics research.",
  month     =  aug,
  year      =  2023,
  keywords  = "Generalized prejudice; interpersonal accuracy; lay theory of
               generalized prejudice; racism; sexism",
  language  = "en"
}

@ARTICLE{Fleisig2023-nn,
  title    = "{FairPrism}: Evaluating fairness-related harms in text generation",
  author   = "Fleisig, Eve and Amstutz, Aubrie and Atalla, Chad and Blodgett, Su
              Lin and Daumé, Hal and Olteanu, Alexandra and Sheng, Emily and
              Vann, Dan and Wallach, Hanna M",
  journal  = "Annu Meet Assoc Comput Linguistics",
  pages    = "6231--6251",
  abstract = "It is critical to measure and mitigate fairness-related harms
              caused by AI text generation systems, including stereotyping and
              demeaning harms. To that end, we introduce FairPrism, a dataset of
              5,000 examples of AI-generated English text with detailed human
              annotations covering a diverse set of harms relating to gender and
              sexuality. FairPrism aims to address several limitations of
              existing datasets for measuring and mitigating fairness-related
              harms, including improved transparency, clearer specification of
              dataset coverage, and accounting for annotator disagreement and
              harms that are context-dependent. FairPrism’s annotations include
              the extent of stereotyping and demeaning harms, the demographic
              groups targeted, and appropriateness for different applications.
              The annotations also include specific harms that occur in
              interactive contexts and harms that raise normative concerns when
              the “speaker” is an AI system. Due to its precision and
              granularity, FairPrism can be used to diagnose (1) the types of
              fairness-related harms that AI text generation systems cause, and
              (2) the potential limitations of mitigation methods, both of which
              we illustrate through case studies. Finally, the process we
              followed to develop FairPrism offers a recipe for building
              improved datasets for measuring and mitigating harms caused by AI
              systems.",
  year     =  2023
}

@ARTICLE{Yoder_undated-mr,
  title  = "{FanfictionNLP}: A Text Processing Pipeline for Fanfiction",
  author = "Yoder, Michael Miller and Khosla, Sopan and Shen, Qinlan and Naik,
            Aakanksha and Jin, Huiming and Muralidharan, Hariharan and Rosé,
            Carolyn P"
}

@ARTICLE{Kim2023-jr,
  title         = "{FANToM}: A Benchmark for Stress-testing Machine Theory of
                   Mind in Interactions",
  author        = "Kim, Hyunwoo and Sclar, Melanie and Zhou, Xuhui and Le Bras,
                   Ronan and Kim, Gunhee and Choi, Yejin and Sap, Maarten",
  journal       = "arXiv [cs.CL]",
  abstract      = "Theory of mind (ToM) evaluations currently focus on testing
                   models using passive narratives that inherently lack
                   interactivity. We introduce FANToM, a new benchmark designed
                   to stress-test ToM within information-asymmetric
                   conversational contexts via question answering. Our benchmark
                   draws upon important theoretical requisites from psychology
                   and necessary empirical considerations when evaluating large
                   language models (LLMs). In particular, we formulate multiple
                   types of questions that demand the same underlying reasoning
                   to identify illusory or false sense of ToM capabilities in
                   LLMs. We show that FANToM is challenging for state-of-the-art
                   LLMs, which perform significantly worse than humans even with
                   chain-of-thought reasoning or fine-tuning.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Mitchell2021-cw,
  title         = "Fast Model Editing at Scale",
  author        = "Mitchell, Eric and Lin, Charles and Bosselut, Antoine and
                   Finn, Chelsea and Manning, Christopher D",
  journal       = "arXiv [cs.LG]",
  abstract      = "While large pre-trained models have enabled impressive
                   results on a variety of downstream tasks, the largest
                   existing models still make errors, and even accurate
                   predictions may become outdated over time. Because detecting
                   all such failures at training time is impossible, enabling
                   both developers and end users of such models to correct
                   inaccurate outputs while leaving the model otherwise intact
                   is desirable. However, the distributed, black-box nature of
                   the representations learned by large neural networks makes
                   producing such targeted edits difficult. If presented with
                   only a single problematic input and new desired output,
                   fine-tuning approaches tend to overfit; other editing
                   algorithms are either computationally infeasible or simply
                   ineffective when applied to very large models. To enable easy
                   post-hoc editing at scale, we propose Model Editor Networks
                   using Gradient Decomposition (MEND), a collection of small
                   auxiliary editing networks that use a single desired
                   input-output pair to make fast, local edits to a pre-trained
                   model's behavior. MEND learns to transform the gradient
                   obtained by standard fine-tuning, using a low-rank
                   decomposition of the gradient to make the parameterization of
                   this transformation tractable. MEND can be trained on a
                   single GPU in less than a day even for 10 billion+ parameter
                   models; once trained MEND enables rapid application of new
                   edits to the pre-trained model. Our experiments with T5, GPT,
                   BERT, and BART models show that MEND is the only approach to
                   model editing that effectively edits the behavior of models
                   with more than 10 billion parameters. Code and data available
                   at https://sites.google.com/view/mend-editing.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{SturtzSreetharan2019-ie,
  title     = "Fat talk: A citizen sociolinguistic approach",
  author    = "SturtzSreetharan, Cindi L and Agostini, Gina and Brewis,
               Alexandra A and Wutich, Amber",
  journal   = "J. Socioling.",
  publisher = "Wiley",
  volume    =  23,
  number    =  3,
  pages     = "263--283",
  abstract  = "Abstract Fat talk, or self-disparaging talk about the body, is
               common among US women and increasingly reported for men. Despite
               its commonality, this unique genre of talk is difficult to access
               using traditional sociolinguistic methods both because it is
               brief and because it arises spontaneously in casual conversation.
               Here we present a new method of data collection accomplished by
               collaborating with citizen sociolinguists to collect spontaneous
               fat talk in public spaces. We compare fat talk exchanges captured
               by citizen sociolinguists against those collected in a
               vignette-based discourse completion task. Our results show
               redundancy both in how fat talk is initiated and in the manner of
               the reply across both forms of data collection. However, the
               citizen sociolinguistic method produced greater variety in fat
               talk utterances, was less methodologically taxing, and revealed
               that fat talk occurs differently in same-sex interactions than
               mixed-sex ones.",
  month     =  jun,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Pena-Fernandez2023-px,
  title     = "Feminism, gender identity and polarization in {TikTok} and
               Twitter",
  author    = "Peña-Fernández, Simón and Larrondo-Ureta, Ainara and
               Morales-i-Gras, Jordi",
  journal   = "Comunicar",
  publisher = "Grupo Comunicar",
  volume    =  31,
  number    =  75,
  abstract  = "The potential of social media to create open, collaborative and
               participatory spaces allows young women to engage and empower
               themselves in political and social activism. In this context, the
               objective of this research is to analyze the polarization in the
               debate at the intersection between the defense of feminism and
               transsexuality, preferably among the young public, symbolized in
               the use of the term “TERF”. To do this, the existing communities
               on Twitter and TikTok on this subject have been analyzed with
               Social Network Analysis techniques, and the presence of young
               people in them. The results indicate that the debates between
               both networks are not very cohesive, with a highly modularized
               structure that suggests isolation of each community in itself.
               For this reason, it can be considered that the debate on sexual
               identity has resulted in a strong polarization of feminist
               activism in social media. Likewise, the positions of
               transinclusive feminism are very majority among young people,
               which reinforces the idea of an ideological debate that can also
               be understood in a generational perspective. Finally, a
               differential use between both social networks has been
               identified, where TikTok is a less partisan and more dialogical
               network than Twitter, which leads to discussions and
               participation in a more neutral tone. El potencial de las redes
               sociales para crear espacios abiertos, colaborativos y
               participativos ha permitido involucrar y empoderar a las mujeres
               jóvenes en el activismo político y social. En este contexto, el
               objetivo de esta investigación se centra en el análisis de la
               polarización que se produce en el debate de las redes sociales en
               la intersección entre la defensa del feminismo y de la
               transexualidad, preferentemente entre el público joven,
               simbolizada en el uso del calificativo «TERF». Para ello, se han
               analizado las comunidades existentes en Twitter y TikTok mediante
               técnicas de Análisis de Redes Sociales, y la presencia de los y
               las jóvenes en ellas. Los resultados indican que los debates en
               ambas redes son poco cohesivos, con una estructura altamente
               modularizada que sugiere aislamiento de cada comunidad en sí
               misma. Por todo ello, puede considerarse que el debate sobre la
               identidad sexual tiene como resultado una fuerte polarización del
               activismo feminista en las redes sociales. Asimismo, las posturas
               del feminismo transinclusivo son muy mayoritarias entre las
               personas jóvenes, lo que refuerza la idea de un debate ideológico
               en el seno del movimiento feminista que también puede entenderse
               en clave generacional. Por último, ha podido constatarse un uso
               diferencial entre ambas redes sociales, donde TikTok se muestra
               en esta temática como una red menos partisana y más dialógica que
               Twitter, pues conduce a discusiones y participaciones en un tono
               más neutro.",
  month     =  apr,
  year      =  2023,
  language  = "es"
}

@INPROCEEDINGS{Bardzell2010-cw,
  title     = "Feminist {HCI}: taking stock and outlining an agenda for design",
  author    = "Bardzell, Shaowen",
  booktitle = "Proceedings of the SIGCHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1301--1310",
  abstract  = "Feminism is a natural ally to interaction design, due to its
               central commitments to issues such as agency, fulfillment,
               identity, equity, empowerment, and social justice. In this paper,
               I summarize the state of the art of feminism in HCI and propose
               ways to build on existing successes to more robustly integrate
               feminism into interaction design research and practice. I explore
               the productive role of feminism in analogous fields, such as
               industrial design, architecture, and game design. I introduce
               examples of feminist interaction design already in the field.
               Finally, I propose a set of femi-nist interaction design
               qualities intended to support design and evaluation processes
               directly as they unfold.",
  series    = "CHI '10",
  month     =  apr,
  year      =  2010,
  keywords  = "HCI, design, feminism, feminist HCI, feminist design qualities,
               feminist standpoint theory, gender, interaction design"
}

@INBOOK{Lennon2019-gz,
  title     = "Feminist Perspectives on the Body",
  author    = "Lennon, Kathleen",
  editor    = "Zalta, Edward N",
  booktitle = "The Stanford Encyclopedia of Philosophy",
  publisher = "Metaphysics Research Lab, Stanford University",
  edition   = "Fall 2019",
  year      =  2019
}

@INBOOK{Cleary2016-qz,
  title     = "Feminist theories of the body",
  author    = "Cleary, Krystal",
  booktitle = "The Wiley Blackwell Encyclopedia of Gender and Sexuality Studies",
  publisher = "John Wiley \& Sons, Ltd",
  address   = "Singapore",
  pages     = "1--6",
  abstract  = "Feminist theory of the body as a collection of literature is a
               complex tapestry of interwoven thought traditions rather than a
               monolithic, linear analytical thread. Dominant Western
               intellectual tradition regards the body suspiciously as a source
               of unwieldy and base desires and functions. Feminist theorists'
               contention with the mind/body dichotomy, however, has been not
               only with its insistence upon disembodied thought but also the
               devalued body's persistent correlation with femininity. While
               many first-wave feminist thinkers in the eighteenth and
               nineteenth centuries and even more contemporary liberal feminists
               have similarly devalued or ignored the body in attempts to
               demonstrate women's capacity for reason and advocate for
               equality, others reclaim and rejoice in the body as a site of
               valuable knowledge production.",
  month     =  apr,
  year      =  2016
}

@ARTICLE{Guinaudeau2022-nr,
  title     = "Fifteen seconds of fame: {TikTok} and the supply side of social
               video",
  author    = "Guinaudeau, Benjamin and Munger, Kevin and Votta, Fabio",
  journal   = "Computational Communication Research",
  publisher = "Amsterdam University Press",
  volume    =  4,
  number    =  2,
  pages     = "463--485",
  abstract  = "TikTok has rapidly developed from a punchline for jokes about
               “kids these days” into a formidable force in American politics.
               The speed of this development is unprecedented, even in the
               rapidly-changing world of digital politics. Through a combination
               of hashtag and snowball sampling, we identify 11,546 TikTok
               accounts who primarily post about politics, allowing us to
               analyze trends in the posting, viewing and commenting behavior on
               1,998,642 tiktoks they have uploaded. We test a number of
               theories about how the unique …",
  month     =  oct,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Hada2023-lv,
  title         = "''Fifty Shades of Bias'': Normative Ratings of Gender Bias in
                   {GPT} Generated English Text",
  author        = "Hada, Rishav and Seth, Agrima and Diddee, Harshita and Bali,
                   Kalika",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language serves as a powerful tool for the manifestation of
                   societal belief systems. In doing so, it also perpetuates the
                   prevalent biases in our society. Gender bias is one of the
                   most pervasive biases in our society and is seen in online
                   and offline discourses. With LLMs increasingly gaining
                   human-like fluency in text generation, gaining a nuanced
                   understanding of the biases these systems can generate is
                   imperative. Prior work often treats gender bias as a binary
                   classification task. However, acknowledging that bias must be
                   perceived at a relative scale; we investigate the generation
                   and consequent receptivity of manual annotators to bias of
                   varying degrees. Specifically, we create the first dataset of
                   GPT-generated English text with normative ratings of gender
                   bias. Ratings were obtained using Best--Worst Scaling -- an
                   efficient comparative annotation framework. Next, we
                   systematically analyze the variation of themes of gender
                   biases in the observed ranking and show that identity-attack
                   is most closely related to gender bias. Finally, we show the
                   performance of existing automated models trained on related
                   concepts on our dataset.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Monroe2017-xx,
  title     = "Fightin' Words: Lexical Feature Selection and Evaluation for
               Identifying the Content of Political Conflict",
  author    = "Monroe, Burt L and Colaresi, Michael P and Quinn, Kevin M",
  journal   = "Polit. Anal.",
  publisher = "Cambridge University Press",
  volume    =  16,
  number    =  4,
  pages     = "372--403",
  abstract  = "Entries in the burgeoning “text-as-data” movement are often
               accompanied by lists or visualizations of how word (or other
               lexical feature) usage differs across some pair or set of
               documents. These are intended either to establish some target
               semantic concept (like the content of partisan frames) to
               estimate word-specific measures that feed forward into another
               analysis (like locating parties in ideological space) or both. We
               discuss a variety of techniques for selecting words that capture
               partisan, or other, differences in political speech and for
               evaluating the relative importance of those words. We introduce
               and emphasize several new approaches based on Bayesian shrinkage
               and regularization. We illustrate the relative utility of these
               approaches with analyses of partisan, gender, and distributive
               speech in the U.S. Senate.",
  month     =  jan,
  year      =  2017
}

@ARTICLE{Lucas2023-nl,
  title         = "Fighting Fire with Fire: The Dual Role of {LLMs} in Crafting
                   and Detecting Elusive Disinformation",
  author        = "Lucas, Jason and Uchendu, Adaku and Yamashita, Michiharu and
                   Lee, Jooyoung and Rohatgi, Shaurya and Lee, Dongwon",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent ubiquity and disruptive impacts of large language
                   models (LLMs) have raised concerns about their potential to
                   be misused (.i.e, generating large-scale harmful and
                   misleading content). To combat this emerging risk of LLMs, we
                   propose a novel ``Fighting Fire with Fire'' (F3) strategy
                   that harnesses modern LLMs' generative and emergent reasoning
                   capabilities to counter human-written and LLM-generated
                   disinformation. First, we leverage GPT-3.5-turbo to
                   synthesize authentic and deceptive LLM-generated content
                   through paraphrase-based and perturbation-based prefix-style
                   prompts, respectively. Second, we apply zero-shot in-context
                   semantic reasoning techniques with cloze-style prompts to
                   discern genuine from deceptive posts and news articles. In
                   our extensive experiments, we observe GPT-3.5-turbo's
                   zero-shot superiority for both in-distribution and
                   out-of-distribution datasets, where GPT-3.5-turbo
                   consistently achieved accuracy at 68-72\%, unlike the decline
                   observed in previous customized and fine-tuned disinformation
                   detectors. Our codebase and dataset are available at
                   https://github.com/mickeymst/F3.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Burgers2016-lg,
  title     = "Figurative framing: Shaping public discourse through metaphor,
               hyperbole, and irony",
  author    = "Burgers, Christian and Konijn, Elly A and Steen, Gerard J",
  journal   = "Commun. Theory",
  publisher = "Oxford University Press (OUP)",
  volume    =  26,
  number    =  4,
  pages     = "410--430",
  abstract  = "Framing is an important concept in communication, yet many
               framing studies set out to develop frames relevant to only one
               issue. We expand framing theory by introducing figurative
               framing. We posit that figurative language types like metaphor,
               hyperbole and irony are important in shaping public discourse,
               because these figures contain important linguistic and conceptual
               content about the issue under discussion. We first explicate the
               role of each individual figure (metaphor, hyperbole, and irony)
               in the framing of important societal issues. Then, we focus on
               complex figurative frames (combinations of metaphor, hyperbole,
               and/or irony). The article concludes with a research agenda,
               connecting figurative framing to the four key processes in
               framing research (frame building, frame setting, individual-level
               effects, and feedback loop).",
  month     =  nov,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Wang2022-qu,
  title         = "Finding Skill Neurons in Pre-trained Transformer-based
                   Language Models",
  author        = "Wang, Xiaozhi and Wen, Kaiyue and Zhang, Zhengyan and Hou,
                   Lei and Liu, Zhiyuan and Li, Juanzi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer-based pre-trained language models have
                   demonstrated superior performance on various natural language
                   processing tasks. However, it remains unclear how the skills
                   required to handle these tasks distribute among model
                   parameters. In this paper, we find that after prompt tuning
                   for specific tasks, the activations of some neurons within
                   pre-trained Transformers are highly predictive of the task
                   labels. We dub these neurons skill neurons and confirm they
                   encode task-specific skills by finding that: (1) Skill
                   neurons are crucial for handling tasks. Performances of
                   pre-trained Transformers on a task significantly drop when
                   corresponding skill neurons are perturbed. (2) Skill neurons
                   are task-specific. Similar tasks tend to have similar
                   distributions of skill neurons. Furthermore, we demonstrate
                   the skill neurons are most likely generated in pre-training
                   rather than fine-tuning by showing that the skill neurons
                   found with prompt tuning are also crucial for other
                   fine-tuning methods freezing neuron weights, such as the
                   adapter-based tuning and BitFit. We also explore the
                   applications of skill neurons, including accelerating
                   Transformers with network pruning and building better
                   transferability indicators. These findings may promote
                   further research on understanding Transformers. The source
                   code can be obtained from
                   https://github.com/THU-KEG/Skill-Neuron.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Adi2016-jy,
  title    = "Fine-grained Analysis of Sentence Embeddings Using Auxiliary
              Prediction Tasks",
  author   = "Adi, Yossi and Kermany, Einat and Belinkov, Yonatan and Lavi, Ofer
              and Goldberg, Yoav",
  journal  = "https://openreview.net › forumhttps://openreview.net › forum",
  abstract = "There is a lot of research interest in encoding variable length
              sentences into fixed length vectors, in a way that preserves the
              sentence meanings. Two common methods include representations
              based on averaging word vectors, and representations based on the
              hidden states of recurrent neural networks such as LSTMs. The
              sentence vectors are used as features for subsequent machine
              learning tasks or for pre-training in the context of deep
              learning. However, not much is known about the properties that are
              encoded in these sentence representations and about the language
              information they capture. We propose a framework that facilitates
              better understanding of the encoded representations. We define
              prediction tasks around isolated aspects of sentence structure
              (namely sentence length, word content, and word order), and score
              representations by the ability to train a classifier to solve each
              prediction task when using the representation as input. We
              demonstrate the potential contribution of the approach by
              analyzing different sentence representation mechanisms. The
              analysis sheds light on the relative strengths of different
              sentence embedding methods with respect to these low level
              prediction tasks, and on the effect of the encoded vector’s
              dimensionality on the resulting representations.",
  month    =  nov,
  year     =  2016
}

@INPROCEEDINGS{Dakwale2017-eq,
  title     = "Fine-Tuning for Neural Machine Translation with Limited
               Degradation across In- and Out-of-Domain Data",
  author    = "Dakwale, Praveen and Monz, Christof",
  booktitle = "Proceedings of Machine Translation Summit XVI: Research Track",
  publisher = "aclanthology.org",
  address   = "Nagoya Japan",
  pages     = "156--169",
  abstract  = "Neural machine translation is a recently proposed approach which
               has shown competitive results to traditional MT approaches.
               Similar to other neural network based methods, NMT also suffers
               from low performance for the domains with less available training
               data. Domain adaptation deals with improving performance of a
               model trained on large general domain data over test instances
               from a new domain. Fine-tuning is a fast and simple domain
               adaptation method which has demonstrated substantial improvements
               for various neural …",
  year      =  2017
}

@ARTICLE{Saphra2023-kp,
  title         = "First Tragedy, then Parse: History Repeats Itself in the New
                   Era of Large Language Models",
  author        = "Saphra, Naomi and Fleisig, Eve and Cho, Kyunghyun and Lopez,
                   Adam",
  journal       = "arXiv [cs.CL]",
  abstract      = "Many NLP researchers are experiencing an existential crisis
                   triggered by the astonishing success of ChatGPT and other
                   systems based on large language models (LLMs). After such a
                   disruptive change to our understanding of the field, what is
                   left to do? Taking a historical lens, we look for guidance
                   from the first era of LLMs, which began in 2005 with large
                   $n$-gram models for machine translation. We identify durable
                   lessons from the first era, and more importantly, we identify
                   evergreen problems where NLP researchers can continue to make
                   meaningful contributions in areas where LLMs are ascendant.
                   Among these lessons, we discuss the primacy of hardware
                   advancement in shaping the availability and importance of
                   scale, as well as the urgent challenge of quality evaluation,
                   both automated and human. We argue that disparities in scale
                   are transient and that researchers can work to reduce them;
                   that data, rather than hardware, is still a bottleneck for
                   many meaningful applications; that meaningful evaluation
                   informed by actual use is still an open problem; and that
                   there is still room for speculative approaches.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Cheng2020-ad,
  title     = "Fleshing out models of gender in English-language novels (1850 –
               2000)",
  author    = "Cheng, Jonathan",
  journal   = "J. Cult. Anal.",
  publisher = "CA: Journal of Cultural Analytics",
  volume    =  5,
  number    =  1,
  abstract  = "Distant readers have used predictive modelling to study the
               strength of the relationship between characterization and binary
               notions of gender. This essay builds on that research, shedding
               light on several historical trends concerning anatomical
               description and its relationship to gender. Some of the evidence
               suggests that bodily language has long played a larger role in
               configuring fictional women than it did for fictional men. Other
               evidence implies that bodily characteristics were increasingly
               bifurcated along a gender binary, reflecting how characters are
               more and more physically sorted along a feminine-masculine axis.
               Taken altogether, this essay unpacks a suggestive correlation: a
               growing aspect of characterization was increasingly imbricated in
               heteronormative discourses. By weighing the discrepancies between
               the evidence presented in this essay, and that of its
               predecessors, this essay will ultimately suggest that
               disaggregating statistical models can unfold patterns of literary
               change that would otherwise remain suppressed.",
  month     =  jan,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Simpson2021-ea,
  title     = "For You, or For``You''? Everyday {LGBTQ+} Encounters with
               {TikTok}",
  author    = "Simpson, Ellen and Semaan, Bryan",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  4,
  number    = "CSCW3",
  pages     = "1--34",
  abstract  = "Online communities provide spaces for people who are vulnerable
               and underserved to seek support and build community, such as
               LGBTQ+ people. Today, some online community spaces are mediated
               by algorithms. Scholarship has found that algorithms have become
               deeply embedded in the systems that mediate our routine
               engagements with the world. Yet, little is known about how these
               systems impact those who are most vulnerable in society. In this
               paper, we focus on people's everyday experiences with one
               algorithmic system, the short video sharing application TikTok.
               TikTok recently received press that it was suppressing and
               oppressing the identities of its growing LGBTQ+ user population
               through algorithmic and human moderation of LGBTQ+ creators and
               content related to LGBTQ+ identity. Through an interview study
               with 16 LGBTQ+ TikTok users, we explore people's everyday
               engagements and encounters with the platform. We find that
               TikTok's For You Page algorithm constructs contradictory identity
               spaces that at once support LGBTQ+ identity work and reaffirm
               LGBTQ+ identity, while also transgressing and violating the
               identities of individual users. We also find that people are
               developing self-organized practices in response to these
               transgressions and violations. We discuss the implications of
               algorithmic systems on people's identity work, and introduce the
               concept of algorithmic exclusion, and explore how people are
               building resilience following moments of algorithmic exclusion.",
  month     =  jan,
  year      =  2021,
  keywords  = "resilience, tiktok, identity work, lgbtq, exclusion, algorithms,
               algorithmic exclusion"
}

@ARTICLE{Gruetzemacher2020-hb,
  title     = "Forecasting extreme labor displacement: A survey of {AI}
               practitioners",
  author    = "Gruetzemacher, Ross and Paradice, David and Lee, Kang Bok",
  journal   = "Technol. Forecast. Soc. Change",
  publisher = "Elsevier BV",
  volume    =  161,
  number    =  120323,
  pages     =  120323,
  month     =  dec,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Zhang2024-mc,
  title     = "Form-From: A Design Space of Social Media Systems",
  author    = "Zhang, A X and Bernstein, M S and Karger, D R and {others}",
  journal   = "Proceedings of the ACM",
  publisher = "dl.acm.org",
  abstract  = "Social media systems are as varied as they are pervasive. They
               have been almost universally adopted for a broad range of
               purposes including work, entertainment, activism …",
  year      =  2024
}

@ARTICLE{Heylighen1999-ch,
  title    = "Formality of Language: definition, measurement and behavioral
              determinants",
  author   = "Heylighen, F and Dewaele, Jean–marc and Apostel, L",
  abstract = "A new concept of formality of linguistic expressions is introduced
              and argued to be the most important dimension of variation between
              styles or registers. Formality is subdivided into ``deep''
              formality and ``surface'' formality. Deep formality is defined as
              avoidance of ambiguity by minimizing the context-dependence and
              fuzziness of expressions. This is achieved by explicit and precise
              description of the elements of the context needed to disambiguate
              the expression. A formal style is characterized by detachment,
              accuracy, rigidity and heaviness; an informal style is more
              flexible, direct, implicit, and involved, but less informative. An
              empirical measure of formality, the F-score, is proposed, based on
              the frequencies of different word classes in the corpus. Nouns,
              adjectives, articles and prepositions are more frequent in formal
              styles; pronouns, adverbs, verbs and interjections are more
              frequent in informal styles. It is shown that this measure, though
              coarse-grained, adequately distinguishes more from less formal
              genres of language production, for some available corpora in
              Dutch, French, Italian, and English. A factor similar to the
              F-score automatically emerges as the most important one from
              factor analyses applied to extensive data in 7 different
              languages. Different situational and personality factors are
              examined which determine the degree of formality in linguistic
              expression. It is proposed that formality becomes larger when the
              distance in space, time or background between the interlocutors
              increases, and when the speaker is male, introverted or
              academically educated. Some empirical evidence and a preliminary
              theoretical explanation for these propositions is discussed. Short
              Abstract: The concept of ``deep'' formality is proposed as the
              most important dimension of variation between language registers
              or styles. It is defined as avoidance of ambiguity by minimizing
              the context-dependence and fuzziness of expressions. An empirical
              measure, the F-score, is proposed, based on the frequencies of
              different word classes. This measure adequately distinguishes
              different genres of language production using data for Dutch,
              French, Italian, and English. Factor analyses applied to data in 7
              different languages produce a similar factor as the most important
              one. Both the data and the theoretical model suggest that
              formality increases when the distance in space,",
  year     =  1999
}

@INPROCEEDINGS{Birk2016-sn,
  title     = "Fostering Intrinsic Motivation through Avatar Identification in
               Digital Games",
  author    = "Birk, Max V and Atkins, Cheralyn and Bowey, Jason T and Mandryk,
               Regan L",
  booktitle = "Proceedings of the 2016 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2982--2995",
  abstract  = "Fostering intrinsic motivation with interactive applications can
               increase the enjoyment that people experience when using
               technology, but can also translate into more invested effort. We
               propose that identifying with an avatar in a game will increase
               the intrinsic motivation of the player. We analyzed data from 126
               participants playing a custom endless runner game and show that
               similarity identification, embodied identification, and wishful
               identification increases autonomy, immersion, invested effort,
               enjoyment, and positive affect. We also show that greater
               identification translates into motivated behaviour as
               operationalized by the time that players spent in an unending
               version of the infinite runner. Important for the design of games
               for entertainment and serious purposes, we discuss how
               identification with an avatar can be facilitated to cultivate
               intrinsic motivation within and beyond games.",
  series    = "CHI '16",
  month     =  may,
  year      =  2016,
  keywords  = "player experience, motivation, investment, games, avatar"
}

@MISC{noauthor_undated-cy,
  title = "{Fought2006Ch1}.pdf"
}

@MISC{noauthor_undated-wx,
  title = "{Fought2006Ch2}.pdf"
}

@ARTICLE{Jian2000-um,
  title     = "Foundations for an Empirically Determined Scale of Trust in
               Automated Systems",
  author    = "Jian, Jiun-Yin and Bisantz, Ann M and Drury, Colin G",
  journal   = "Int. J. Cogn. Ergon.",
  publisher = "Taylor \& Francis",
  volume    =  4,
  number    =  1,
  pages     = "53--71",
  abstract  = "One component in the successful use of automated systems is the
               extent to which people trust the automation to perform
               effectively. In order to understand the relationship between
               trust in computerized systems and the use of those systems, we
               need to be able to effectively measure trust. Although
               questionnaires regarding trust have been used in prior studies,
               these questionnaires were theoretically rather than empirically
               generated and did not distinguish between three potentially
               different types of trust: human-human trust, human-machine trust,
               and trust in general. A 3-phased experiment, comprising a word
               elicitation study, a questionnaire study, and a paired comparison
               study, was performed to better understand similarities and
               differences in the concepts of trust and distrust, and among the
               different types of trust. Results indicated that trust and
               distrust can be considered opposites, rather than different
               concepts. Components of trust, in terms of words related to
               trust, were similar across the three types of trust. Results
               obtained from a cluster analysis were used to identify 12
               potential factors of trust between people and automated systems.
               These 12 factors were then used to develop a proposed scale to
               measure trust in automation.",
  month     =  mar,
  year      =  2000
}

@ARTICLE{Kwak2021-ma,
  title     = "{FrameAxis}: characterizing microframe bias and intensity with
               word embedding",
  author    = "Kwak, Haewoon and An, Jisun and Jing, Elise and Ahn, Yong-Yeol",
  journal   = "PeerJ Comput Sci",
  publisher = "peerj.com",
  volume    =  7,
  pages     = "e644",
  abstract  = "Framing is a process of emphasizing a certain aspect of an issue
               over the others, nudging readers or listeners towards different
               positions on the issue even without making a biased argument.
               Here, we propose FrameAxis, a method for characterizing documents
               by identifying the most relevant semantic axes (``microframes'')
               that are overrepresented in the text using word embedding. Our
               unsupervised approach can be readily applied to large datasets
               because it does not require manual annotations. It can also
               provide nuanced insights by considering a rich set of semantic
               axes. FrameAxis is designed to quantitatively tease out two
               important dimensions of how microframes are used in the text.
               Microframe bias captures how biased the text is on a certain
               microframe, and microframe intensity shows how prominently a
               certain microframe is used. Together, they offer a detailed
               characterization of the text. We demonstrate that microframes
               with the highest bias and intensity align well with sentiment,
               topic, and partisan spectrum by applying FrameAxis to multiple
               datasets from restaurant reviews to political news. The existing
               domain knowledge can be incorporated into FrameAxis by using
               custom microframes and by using FrameAxis as an iterative
               exploratory analysis instrument. Additionally, we propose methods
               for explaining the results of FrameAxis at the level of
               individual words and documents. Our method may accelerate
               scalable and sophisticated computational analyses of framing
               across disciplines.",
  month     =  jul,
  year      =  2021,
  keywords  = "Antonyms; Framing; Media bias; Microframe; SemAxis; Semantic
               Axis; Word embedding",
  language  = "en"
}

@ARTICLE{Kwan2009-fn,
  title     = "Framing the fat body: Contested meanings between government,
               activists, and industry",
  author    = "Kwan, Samantha",
  journal   = "Sociol. Inq.",
  publisher = "Wiley",
  volume    =  79,
  number    =  1,
  pages     = "25--50",
  abstract  = "Sociologists have long recognized that social problems do not
               derive solely from objective conditions but from a process of
               collective definition. At the core of some social issues are
               framing competitions, struggles over the production of ideas and
               meanings. This article examines competing cultural meanings about
               the fat body. Through frame analysis of organizational materials,
               I map the contested field of obesity and document three cultural
               frames—medical frame, social justice frame, and market choice
               frame—as represented by the Centers for Disease Control and
               Prevention (CDC), the National Association to Advance Fat
               Acceptance (NAAFA), and the food industry group the Center for
               Consumer Freedom (CCF), respectively. Using the “framing matrix,”
               I explore each frame's key signature elements and discuss its
               social and cultural significance. Notably, each frame leads to
               different outcomes for social equality and how society thinks
               about fat bodies, health, and public policy.",
  month     =  feb,
  year      =  2009,
  language  = "en"
}

@INPROCEEDINGS{Inie2024-dy,
  title     = "From ``{AI}'' to Probabilistic Automation: How Does
               Anthropomorphization of Technical Systems Descriptions Influence
               Trust?",
  author    = "Inie, Nanna and Druga, Stefania and Zukerman, Peter and Bender,
               Emily M",
  booktitle = "Proceedings of the 2024 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "2322--2347",
  abstract  = "In this paper we investigate how people’s level of trust (as
               reported through self-assessment) in so-called “AI” (artificial
               intelligence) is influenced by anthropomorphizing language in
               system descriptions. Building on prior work, we define four
               categories of anthropomorphization (1. Properties of a cognizer,
               2. Agency, 3. Biological metaphors, and 4. Properties of a
               communicator). We use a survey-based approach (n=954) to
               investigate whether participants are likely to trust one of two
               (fictitious) “AI” systems by randomly assigning people to see
               either an anthropomorphized or a de-anthropomorphized description
               of the systems. We find that participants are no more likely to
               trust anthropomorphized over de-anthropmorphized product
               descriptions overall. The type of product or system in
               combination with different anthropomorphic categories appears to
               exert greater influence on trust than anthropomorphizing language
               alone, and age is the only demographic factor that significantly
               correlates with people’s preference for anthropomorphized or
               de-anthropomorphized descriptions. When elaborating on their
               choices, participants highlight factors such as lesser of two
               evils, lower or higher stakes contexts, and human favoritism as
               driving motivations when choosing between product A and B,
               irrespective of whether they saw an anthropomorphized or a
               de-anthropomorphized description of the product. Our results
               suggest that “anthropomorphism” in “AI” descriptions is an
               aggregate concept that may influence different groups
               differently, and provide nuance to the discussion of whether
               anthropomorphization leads to higher trust and over-reliance by
               the general public in systems sold as “AI”.",
  series    = "FAccT '24",
  month     =  jun,
  year      =  2024,
  keywords  = "AI, anthropomorphism, probabilistic automation, semantics, trust"
}

@MISC{noauthor_undated-vp,
  title = "{From\_AI\_to\_Probabilistic\_Automation\_PREPRINT}.pdf"
}

@ARTICLE{Agre1994-wt,
  title     = "From high tech to human tech: Empowerment, measurement, and
               social studies of computing",
  author    = "Agre, Philip E",
  journal   = "Comput. Support. Coop. Work",
  publisher = "Springer",
  volume    =  3,
  number    =  2,
  pages     = "167--195",
  abstract  = "“Empowerment” has become a pervasive term of art in business
               practice, particularly in the United States. The term traces its
               roots to the organizing models evolved by populist social
               movements, but within business discourse it refers to an emerging
               organizational philosophy that largely replaces conventional
               hierarchies with nominally autonomous teams. Proponents of
               empowerment frequently cite information technology as a crucial
               enabler of this shift without, however, spelling out fully the
               logic of the connection. A reconstruction of this logic provides
               evidence for the emergence of a novel vision of work-discipline,
               the empowerment and measurement regime. This regime is discussed
               in relation to market dynamics, Taylorism, and research on the
               social organization of information technology and its use.",
  month     =  jun,
  year      =  1994
}

@ARTICLE{Lundberg2020-aq,
  title    = "From Local Explanations to Global Understanding with Explainable
              {AI} for Trees",
  author   = "Lundberg, Scott M and Erion, Gabriel and Chen, Hugh and DeGrave,
              Alex and Prutkin, Jordan M and Nair, Bala and Katz, Ronit and
              Himmelfarb, Jonathan and Bansal, Nisha and Lee, Su-In",
  journal  = "Nat Mach Intell",
  volume   =  2,
  number   =  1,
  pages    = "56--67",
  abstract = "Tree-based machine learning models such as random forests,
              decision trees, and gradient boosted trees are popular non-linear
              predictive models, yet comparatively little attention has been
              paid to explaining their predictions. Here, we improve the
              interpretability of tree-based models through three main
              contributions: 1) The first polynomial time algorithm to compute
              optimal explanations based on game theory. 2) A new type of
              explanation that directly measures local feature interaction
              effects. 3) A new set of tools for understanding global model
              structure based on combining many local explanations of each
              prediction. We apply these tools to three medical machine learning
              problems and show how combining many high-quality local
              explanations allows us to represent global structure while
              retaining local faithfulness to the original model. These tools
              enable us to i) identify high magnitude but low frequency
              non-linear mortality risk factors in the US population, ii)
              highlight distinct population sub-groups with shared risk
              characteristics, iii) identify non-linear interaction effects
              among risk factors for chronic kidney disease, and iv) monitor a
              machine learning model deployed in a hospital by identifying which
              features are degrading the model's performance over time. Given
              the popularity of tree-based machine learning models, these
              improvements to their interpretability have implications across a
              broad set of domains.",
  month    =  jan,
  year     =  2020,
  language = "en"
}

@ARTICLE{Tarunesh2021-ni,
  title         = "From Machine Translation to Code-Switching: Generating
                   High-Quality Code-Switched Text",
  author        = "Tarunesh, Ishan and Kumar, Syamantak and Jyothi, Preethi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Generating code-switched text is a problem of growing
                   interest, especially given the scarcity of corpora containing
                   large volumes of real code-switched text. In this work, we
                   adapt a state-of-the-art neural machine translation model to
                   generate Hindi-English code-switched sentences starting from
                   monolingual Hindi sentences. We outline a carefully designed
                   curriculum of pretraining steps, including the use of
                   synthetic code-switched text, that enable the model to
                   generate high-quality code-switched text. Using text
                   generated from our model as data augmentation, we show
                   significant reductions in perplexity on a language modeling
                   task, compared to using text from other generative models of
                   CS text. We also show improvements using our text for a
                   downstream code-switched natural language inference task. Our
                   generated text is further subjected to a rigorous evaluation
                   using a human evaluation study and a range of objective
                   metrics, where we show performance comparable (and sometimes
                   even superior) to code-switched text obtained via crowd
                   workers who are native Hindi speakers.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Feng2023-rs,
  title     = "From Pretraining Data to Language Models to Downstream Tasks:
               Tracking the Trails of Political Biases Leading to Unfair {NLP}
               Models",
  author    = "Feng, S and Park, C Y and Liu, Y and Tsvetkov, Y",
  journal   = "arXiv preprint arXiv:2305.08283",
  publisher = "arxiv.org",
  abstract  = "Large language models (LMs) are pretrained on diverse data
               sources: news, discussion forums, books, online encyclopedias. A
               significant portion of this data includes facts and …",
  year      =  2023
}

@ARTICLE{Kang2023-gr,
  title         = "From Values to Opinions: Predicting Human Behaviors and
                   Stances Using Value-Injected Large Language Models",
  author        = "Kang, Dongjun and Park, Joonsuk and Jo, Yohan and Bak,
                   Jinyeong",
  journal       = "arXiv [cs.CL]",
  abstract      = "Being able to predict people's opinions on issues and
                   behaviors in realistic scenarios can be helpful in various
                   domains, such as politics and marketing. However, conducting
                   large-scale surveys like the European Social Survey to
                   solicit people's opinions on individual issues can incur
                   prohibitive costs. Leveraging prior research showing
                   influence of core human values on individual decisions and
                   actions, we propose to use value-injected large language
                   models (LLM) to predict opinions and behaviors. To this end,
                   we present Value Injection Method (VIM), a collection of two
                   methods -- argument generation and question answering --
                   designed to inject targeted value distributions into LLMs via
                   fine-tuning. We then conduct a series of experiments on four
                   tasks to test the effectiveness of VIM and the possibility of
                   using value-injected LLMs to predict opinions and behaviors
                   of people. We find that LLMs value-injected with variations
                   of VIM substantially outperform the baselines. Also, the
                   results suggest that opinions and behaviors can be better
                   predicted using value-injected LLMs than the baseline
                   approaches.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chhaya2018-pv,
  title    = "Frustrated, polite, or formal: Quantifying feelings and tone in
              email",
  author   = "Chhaya, Niyati and Chawla, Kushal and Goyal, Tanya and Chanda,
              Projjal and Singh, Jaya",
  pages    = "76--86",
  abstract = "Email conversations are the primary mode of communication in
              enterprises. The email content expresses an individual’s needs,
              requirements and intentions. Affective information in the email
              text can be used to get an insight into the sender’s mood or
              emotion. We present a novel approach to model human frustration in
              text. We identify linguistic features that influence human
              perception of frustration and model it as a supervised learning
              task. The paper provides a detailed comparison across traditional
              regression and word distribution-based models. We report a
              mean-squared error (MSE) of 0.018 against human-annotated
              frustration for the best performing model. The approach
              establishes the importance of affect features in frustration
              prediction for email data. We further evaluate the efficacy of the
              proposed feature set and model in predicting other tone or affects
              in text, namely formality and politeness; results demonstrate a
              comparable performance against the state-of-the-art baselines.",
  year     =  2018
}

@MISC{noauthor_undated-iw,
  title = "{FSB1\_Negation\_16May2018}.pdf"
}

@ARTICLE{Hollett2023-fh,
  title     = "Gaze behaviour, body image in women and online apparel shopping",
  author    = "Hollett, Ross C and Panaia, Peta M and Smart, Aimee H",
  journal   = "Int. J. Consum. Stud.",
  publisher = "Wiley",
  volume    =  47,
  number    =  5,
  pages     = "1999--2011",
  abstract  = "AbstractOnline apparel shopping is popular among women, with
               possible negative body image consequences, particularly when the
               website imagery is body‐focused. We investigated both
               correlational and experimental effects of online apparel shopping
               on women's (N = 113) explicitly and implicitly measured
               self‐worth, appearance attitudes and body gaze behaviour.
               Self‐reported online apparel shopping behaviour positively
               correlated with self‐objectification and a tendency to value and
               compare one's appearance. Following a simulated online shopping
               activity, women who browsed a body‐focused activewear website
               felt worse about their looks, when compared with women who
               browsed a non‐body‐focused casualwear website. The activewear
               condition also primed lower subsequent visual attention towards
               female bodies in a gaze task, when compared with the casualwear
               condition. Given that women tend to naturally gaze at faces, the
               deprivation of facial stimuli in the activewear condition
               presumably led to a compensatory gaze effect, whereby subsequent
               attention towards bodies was comparably low. Importantly, dollars
               spent in the activewear condition correlated positively with
               appearance comparison and body shame attitudes. These results
               suggest that online apparel imagery exposure may negatively
               impact women's well‐being. We also find evidence suggesting that
               gaze behaviour plays a role in how apparel marketing influences
               subsequent attention.",
  month     =  sep,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Kumar2023-li,
  title         = "Gen-{Z}: Generative Zero-Shot Text Classification with
                   Contextualized Label Descriptions",
  author        = "Kumar, Sachin and Park, Chan Young and Tsvetkov, Yulia",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language model (LM) prompting--a popular paradigm for solving
                   NLP tasks--has been shown to be susceptible to miscalibration
                   and brittleness to slight prompt variations, caused by its
                   discriminative prompting approach, i.e., predicting the label
                   given the input. To address these issues, we propose Gen-Z--a
                   generative prompting framework for zero-shot text
                   classification. GEN-Z is generative, as it measures the LM
                   likelihood of input text, conditioned on natural language
                   descriptions of labels. The framework is multivariate, as
                   label descriptions allow us to seamlessly integrate
                   additional contextual information about the labels to improve
                   task performance. On various standard classification
                   benchmarks, with six open-source LM families, we show that
                   zero-shot classification with simple contextualization of the
                   data source of the evaluation set consistently outperforms
                   both zero-shot and few-shot baselines while improving
                   robustness to prompt variations. Further, our approach
                   enables personalizing classification in a zero-shot manner by
                   incorporating author, subject, or reader information in the
                   label descriptions.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lucy2021-mn,
  title     = "Gender and representation bias in {GPT}-3 generated stories",
  author    = "Lucy, L and Bamman, D",
  journal   = "Proceedings of the Third Workshop on",
  publisher = "aclanthology.org",
  abstract  = "Using topic modeling and lexicon-based word similarity, we find
               that stories generated by GPT-3 exhibit many known gender
               stereotypes. Generated stories depict different topics and …",
  year      =  2021
}

@ARTICLE{Meister2022-bl,
  title         = "Gender artifacts in visual datasets",
  author        = "Meister, Nicole and Zhao, Dora and Wang, Angelina and
                   Ramaswamy, Vikram V and Fong, Ruth and Russakovsky, Olga",
  journal       = "arXiv [cs.CV]",
  pages         = "4837--4848",
  abstract      = "Gender biases are known to exist within large-scale visual
                   datasets and can be reflected or even amplified in downstream
                   models. Many prior works have proposed methods for mitigating
                   gender biases, often by attempting to remove gender
                   expression information from images. To understand the
                   feasibility and practicality of these approaches, we
                   investigate what $\textit{gender artifacts}$ exist within
                   large-scale visual datasets. We define a $\textit{gender
                   artifact}$ as a visual cue that is correlated with gender,
                   focusing specifically on those cues that are learnable by a
                   modern image classifier and have an interpretable human
                   corollary. Through our analyses, we find that gender
                   artifacts are ubiquitous in the COCO and OpenImages datasets,
                   occurring everywhere from low-level information (e.g., the
                   mean value of the color channels) to the higher-level
                   composition of the image (e.g., pose and location of people).
                   Given the prevalence of gender artifacts, we claim that
                   attempts to remove gender artifacts from such datasets are
                   largely infeasible. Instead, the responsibility lies with
                   researchers and practitioners to be aware that the
                   distribution of images within datasets is highly gendered and
                   hence develop methods which are robust to these
                   distributional shifts across groups.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@INPROCEEDINGS{Kotek2023-ou,
  title     = "Gender bias and stereotypes in Large Language Models",
  author    = "Kotek, Hadas and Dockum, Rikker and Sun, David",
  booktitle = "Proceedings of The ACM Collective Intelligence Conference",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "12--24",
  abstract  = "Large Language Models (LLMs) have made substantial progress in
               the past several months, shattering state-of-the-art benchmarks
               in many domains. This paper investigates LLMs’ behavior with
               respect to gender stereotypes, a known issue for prior models. We
               use a simple paradigm to test the presence of gender bias,
               building on but differing from WinoBias, a commonly used gender
               bias dataset, which is likely to be included in the training data
               of current LLMs. We test four recently published LLMs and
               demonstrate that they express biased assumptions about men and
               women’s occupations. Our contributions in this paper are as
               follows: (a) LLMs are 3-6 times more likely to choose an
               occupation that stereotypically aligns with a person’s gender;
               (b) these choices align with people’s perceptions better than
               with the ground truth as reflected in official job statistics;
               (c) LLMs in fact amplify the bias beyond what is reflected in
               perceptions or the ground truth; (d) LLMs ignore crucial
               ambiguities in sentence structure 95\% of the time in our study
               items, but when explicitly prompted, they recognize the
               ambiguity; (e) LLMs provide explanations for their choices that
               are factually inaccurate and likely obscure the true reason
               behind their predictions. That is, they provide rationalizations
               of their biased behavior. This highlights a key property of these
               models: LLMs are trained on imbalanced datasets; as such, even
               with the recent successes of reinforcement learning with human
               feedback, they tend to reflect those imbalances back at us. As
               with other types of societal biases, we suggest that LLMs must be
               carefully tested to ensure that they treat minoritized
               individuals and communities equitably.",
  series    = "CI '23",
  month     =  nov,
  year      =  2023,
  keywords  = "occupations, bias, gender, large language models, explanations,
               ethics, stereotypes"
}

@INPROCEEDINGS{Joniak2022-sw,
  title     = "Gender Biases and Where to Find Them: Exploring Gender Bias in
               Pre-Trained Transformer-based Language Models Using Movement
               Pruning",
  author    = "Joniak, Przemyslaw and Aizawa, Akiko",
  editor    = "Hardmeier, Christian and Basta, Christine and Costa-jussà, Marta
               R and Stanovsky, Gabriel and Gonen, Hila",
  booktitle = "Proceedings of the 4th Workshop on Gender Bias in Natural
               Language Processing (GeBNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Seattle, Washington",
  pages     = "67--73",
  abstract  = "Language model debiasing has emerged as an important field of
               study in the NLP community. Numerous debiasing techniques were
               proposed, but bias ablation remains an unaddressed issue. We
               demonstrate a novel framework for inspecting bias in pre-trained
               transformer-based language models via movement pruning. Given a
               model and a debiasing objective, our framework finds a subset of
               the model containing less bias than the original model. We
               implement our framework by pruning the model while fine-tuning it
               on the debasing objective. Optimized are only the pruning scores
               -- parameters coupled with the model's weights that act as gates.
               We experiment with pruning attention heads, an important building
               block of transformers: we prune square blocks, as well as
               establish a new way of pruning the entire heads. Lastly, we
               demonstrate the usage of our framework using gender bias, and
               based on our findings, we propose an improvement to an existing
               debiasing method. Additionally, we re-discover a bias-performance
               trade-off: the better the model performs, the more bias it
               contains.",
  month     =  jul,
  year      =  2022
}

@ARTICLE{Bamman2014-rr,
  title     = "Gender identity and lexical variation in social media",
  author    = "Bamman, David and Eisenstein, Jacob and Schnoebelen, Tyler",
  journal   = "J. Socioling.",
  publisher = "Wiley",
  volume    =  18,
  number    =  2,
  pages     = "135--160",
  abstract  = "We present a study of the relationship between gender, linguistic
               style, and social networks, using a novel corpus of 14,000
               Twitter users. Prior quantitative work on gender often treats
               this social variable as a female/male binary; we argue for a more
               nuanced approach. By clustering Twitter users, we find a natural
               decomposition of the dataset into various styles and topical
               interests. Many clusters have strong gender orientations, but
               their use of linguistic resources sometimes directly conflicts
               with the population-level language statistics. We view these
               clusters as a more accurate reflection of the multifaceted nature
               of gendered language styles. Previous corpus-based work has also
               had little to say about individuals whose linguistic styles defy
               population-level gender patterns. To identify such individuals,
               we train a statistical classifier, and measure the classifier
               confidence for each individual in the dataset. Examining
               individuals whose language does not match the classifier's model
               for their gender, we find that they have social networks that
               include significantly fewer same-gender social connections and
               that, in general, social network homophily is correlated with the
               use of same-gender language markers. Pairing computational
               methods and social theory thus offers a new perspective on how
               gender emerges as individuals position themselves relative to
               audiences, topics, and mainstream gender norms.",
  month     =  apr,
  year      =  2014,
  language  = "en"
}

@ARTICLE{Wilson2024-do,
  title         = "Gender, race, and intersectional bias in resume screening via
                   language model retrieval",
  author        = "Wilson, Kyra and Caliskan, Aylin",
  journal       = "arXiv [cs.CY]",
  abstract      = "Artificial intelligence (AI) hiring tools have revolutionized
                   resume screening, and large language models (LLMs) have the
                   potential to do the same. However, given the biases which are
                   embedded within LLMs, it is unclear whether they can be used
                   in this scenario without disadvantaging groups based on their
                   protected attributes. In this work, we investigate the
                   possibilities of using LLMs in a resume screening setting via
                   a document retrieval framework that simulates job candidate
                   selection. Using that framework, we then perform a resume
                   audit study to determine whether a selection of Massive Text
                   Embedding (MTE) models are biased in resume screening
                   scenarios. We simulate this for nine occupations, using a
                   collection of over 500 publicly available resumes and 500 job
                   descriptions. We find that the MTEs are biased, significantly
                   favoring White-associated names in 85.1\% of cases and
                   female-associated names in only 11.1\% of cases, with a
                   minority of cases showing no statistically significant
                   differences. Further analyses show that Black males are
                   disadvantaged in up to 100\% of cases, replicating real-world
                   patterns of bias in employment settings, and validate three
                   hypotheses of intersectionality. We also find an impact of
                   document length as well as the corpus frequency of names in
                   the selection of resumes. These findings have implications
                   for widely used AI tools that are automating employment,
                   fairness, and tech policy.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Lockenhoff2014-lo,
  title     = "Gender stereotypes of personality: Universal and accurate?",
  author    = "Löckenhoff, C E and Chan, W and McCrae, R R and {others}",
  journal   = "Journal of cross",
  publisher = "journals.sagepub.com",
  abstract  = "Numerous studies have documented subtle but consistent sex
               differences in self-reports and observer-ratings of five-factor
               personality traits, and such effects were found to show well …",
  year      =  2014
}

@ARTICLE{Kallhammer2012-vy,
  title     = "Gendered innovative design : critical reflections stimulated by
               personas",
  author    = "Källhammer, Eva and Wikberg-Nilsson, Åsa",
  publisher = "Vinnova",
  pages     = "328--350",
  abstract  = "This chapter focuses on our re-design of the Persona Method into
               a tool for criticalreflection on gender issues in
               entrepreneurship and innovation systems. Whereas suchsystems
               often are considered ...",
  year      =  2012,
  language  = "en"
}

@INPROCEEDINGS{Gregorius2022-fj,
  title     = "Generating Code-Switched Text from Monolingual Text with
               Dependency Tree",
  author    = "Gregorius, Bryan and Okadome, Takeshi",
  booktitle = "Proceedings of the The 20th Annual Workshop of the Australasian
               Language Technology Association",
  publisher = "Australasian Language Technology Association",
  address   = "Adelaide, Australia",
  pages     = "90--97",
  abstract  = "Various methods have been proposed to generate code-switched
               texts. Many of these involve training neural networks and, in
               turn, require some (albeit small) amounts of code- switched texts
               or parallel corpora to train the model itself. In this paper, we
               propose a method to convert monolingual text into a bilingual
               code-switched sentence using a dependency parser and machine
               translator. We leverage the characteristics of the dependency
               tree to identify the switching point and then pass it to machine
               translation to …",
  month     =  dec,
  year      =  2022
}

@ARTICLE{Hovy1987-mn,
  title   = "Generating natural language under pragmatic constraints",
  author  = "Hovy, E",
  journal = "J. Pragmat.",
  month   =  dec,
  year    =  1987
}

@ARTICLE{Park2023-ee,
  title         = "Generative Agents: Interactive Simulacra of Human Behavior",
  author        = "Park, Joon Sung and O'Brien, Joseph C and Cai, Carrie J and
                   Morris, Meredith Ringel and Liang, Percy and Bernstein,
                   Michael S",
  journal       = "arXiv [cs.HC]",
  abstract      = "Believable proxies of human behavior can empower interactive
                   applications ranging from immersive environments to rehearsal
                   spaces for interpersonal communication to prototyping tools.
                   In this paper, we introduce generative agents--computational
                   software agents that simulate believable human behavior.
                   Generative agents wake up, cook breakfast, and head to work;
                   artists paint, while authors write; they form opinions,
                   notice each other, and initiate conversations; they remember
                   and reflect on days past as they plan the next day. To enable
                   generative agents, we describe an architecture that extends a
                   large language model to store a complete record of the
                   agent's experiences using natural language, synthesize those
                   memories over time into higher-level reflections, and
                   retrieve them dynamically to plan behavior. We instantiate
                   generative agents to populate an interactive sandbox
                   environment inspired by The Sims, where end users can
                   interact with a small town of twenty five agents using
                   natural language. In an evaluation, these generative agents
                   produce believable individual and emergent social behaviors:
                   for example, starting with only a single user-specified
                   notion that one agent wants to throw a Valentine's Day party,
                   the agents autonomously spread invitations to the party over
                   the next two days, make new acquaintances, ask each other out
                   on dates to the party, and coordinate to show up for the
                   party together at the right time. We demonstrate through
                   ablation that the components of our agent
                   architecture--observation, planning, and reflection--each
                   contribute critically to the believability of agent behavior.
                   By fusing large language models with computational,
                   interactive agents, this work introduces architectural and
                   interaction patterns for enabling believable simulations of
                   human behavior.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Choudhury2023-cx,
  title    = "Generative {AI} has a language problem",
  author   = "Choudhury, Monojit",
  journal  = "Nat Hum Behav",
  volume   =  7,
  number   =  11,
  pages    = "1802--1803",
  month    =  nov,
  year     =  2023,
  language = "en"
}

@MISC{Tamkin_undated-kb,
  title       = "generative-elicitation",
  author      = "Tamkin, Alex",
  institution = "Github",
  abstract    = "Contribute to alextamkin/generative-elicitation development by
                 creating an account on GitHub.",
  language    = "en"
}

@ARTICLE{Bhakthavatsalam2020-rg,
  title    = "{GenericsKB}: A Knowledge Base of Generic Statements",
  author   = "Bhakthavatsalam, Sumithra and Anastasiades, Chloe and Clark, Peter",
  journal  = "arXiv.org",
  abstract = "This work presents a new resource for the NLP community, namely a
              large knowledge base of *generic statements*, e.g., ``Trees remove
              carbon dioxide from the atmosphere'', collected from multiple
              corpora, and is the first large resource to contain *naturally
              occurring* generic sentences. We present a new resource for the
              NLP community, namely a large (3.5M+ sentence) knowledge base of
              *generic statements*, e.g., ``Trees remove carbon dioxide from the
              atmosphere'', collected from multiple corpora. This is the first
              large resource to contain *naturally occurring* generic sentences,
              as opposed to extracted or crowdsourced triples, and thus is rich
              in high-quality, general, semantically complete statements. All
              GenericsKB sentences are annotated with their topical term,
              surrounding context (sentences), and a (learned) confidence. We
              also release GenericsKB-Best (1M+ sentences), containing the
              best-quality generics in GenericsKB augmented with selected,
              synthesized generics from WordNet and ConceptNet. In tests on two
              existing datasets requiring multihop reasoning (OBQA and QASC), we
              find using GenericsKB can result in higher scores and better
              explanations than using a much larger corpus. This demonstrates
              that GenericsKB can be a useful resource for NLP applications, as
              well as providing data for linguistic studies of generics and
              their semantics. GenericsKB is available at this https URL.",
  year     =  2020,
  language = "en"
}

@ARTICLE{Stahl2023-cf,
  title     = "\#GenZ on {TikTok}: the collective online self-Portrait of the
               social media generation",
  author    = "Stahl, Catherine Cheng and Literat, Ioana",
  journal   = "J. Youth Stud.",
  publisher = "Informa UK Limited",
  volume    =  26,
  number    =  7,
  pages     = "925--946",
  abstract  = "… both individual and collective dimensions ( Literat and
               Kligler-Vilenchik Citation2019; … on TikTok , our study takes up
               Schellewald’s (Citation2021) call to shed light onto the role of
               TikTok …",
  month     =  aug,
  year      =  2023,
  language  = "en"
}

@INPROCEEDINGS{Yin2022-gv,
  title     = "{G}eo{MLAMA}: Geo-Diverse Commonsense Probing on Multilingual
               Pre-Trained Language Models",
  author    = "Yin, Da and Bansal, Hritik and Monajatipoor, Masoud and Li,
               Liunian Harold and Chang, Kai-Wei",
  booktitle = "Proceedings of the 2022 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Abu Dhabi, United Arab Emirates",
  pages     = "2039--2055",
  abstract  = "Recent work has shown that Pre-trained Language Models (PLMs)
               store the relational knowledge learned from data and utilize it
               for performing downstream tasks. However, commonsense knowledge
               across different regions may vary. For instance, the color of
               bridal dress is white in American weddings whereas it is red in
               Chinese weddings. In this paper, we introduce a benchmark
               dataset, Geo-diverse Commonsense Multilingual Language Models
               Analysis (GeoMLAMA), for probing the diversity of the relational
               knowledge in multilingual PLMs. GeoMLAMA contains 3125 prompts in
               English, Chinese, Hindi, Persian, and Swahili, with a wide
               coverage of concepts shared by people from American, Chinese,
               Indian, Iranian and Kenyan cultures. We benchmark 11 standard
               multilingual PLMs on GeoMLAMA. Interestingly, we find that 1)
               larger multilingual PLMs variants do not necessarily store
               geo-diverse concepts better than its smaller variant; 2)
               multilingual PLMs are not intrinsically biased towards knowledge
               from the Western countries (the United States); 3) the native
               language of a country may not be the best language to probe its
               knowledge and 4) a language may better probe knowledge about a
               non-native country than its native country.",
  month     =  dec,
  year      =  2022
}

@ARTICLE{Pinto2024-gz,
  title         = "{GET}-Tok: A {GenAI}-Enriched Multimodal {TikTok} Dataset
                   Documenting the 2022 Attempted Coup in Peru",
  author        = "Pinto, Gabriela and Burghardt, Keith and Lerman, Kristina and
                   Ferrara, Emilio",
  journal       = "arXiv [cs.SI]",
  abstract      = "TikTok is one of the largest and fastest-growing social media
                   sites in the world. TikTok features, however, such as voice
                   transcripts, are often missing and other important features,
                   such as OCR or video descriptions, do not exist. We introduce
                   the Generative AI Enriched TikTok (GET-Tok) data, a pipeline
                   for collecting TikTok videos and enriched data by augmenting
                   the TikTok Research API with generative AI models. As a case
                   study, we collect videos about the attempted coup in Peru
                   initiated by its former President, Pedro Castillo, and its
                   accompanying protests. The data includes information on
                   43,697 videos published from November 20, 2022 to March 1,
                   2023 (102 days). Generative AI augments the collected data
                   via transcripts of TikTok videos, text descriptions of what
                   is shown in the videos, what text is displayed within the
                   video, and the stances expressed in the video. Overall, this
                   pipeline will contribute to a better understanding of online
                   discussion in a multimodal setting with applications of
                   Generative AI, especially outlining the utility of this
                   pipeline in non-English-language social media. Our code used
                   to produce the pipeline is in a public Github repository:
                   https://github.com/gabbypinto/GET-Tok-Peru.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SI"
}

@INPROCEEDINGS{Pruss2023-bk,
  title     = "Ghosting the Machine: Judicial Resistance to a Recidivism Risk
               Assessment Instrument",
  author    = "Pruss, Dasha",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "312--323",
  abstract  = "Recidivism risk assessment instruments are presented as an
               ‘evidence-based’ strategy for criminal justice reform – a way of
               increasing consistency in sentencing, replacing cash bail, and
               reducing mass incarceration. In practice, however, AI-centric
               reforms can simply add another layer to the sluggish,
               labyrinthine machinery of bureaucratic systems and are met with
               internal resistance. Through a community-informed interview-based
               study of 23 criminal judges and other criminal legal bureaucrats
               in Pennsylvania, I find that judges overwhelmingly ignore a
               recently-implemented sentence risk assessment instrument, which
               they disparage as “useless,” “worthless,” “boring,” “a waste of
               time,” “a non-thing,” and simply “not helpful.” I argue that this
               algorithm aversion cannot be accounted for by individuals’
               distrust of the tools or automation anxieties, per the
               explanations given by existing scholarship. Rather, the
               instrument’s non-use is the result of an interplay between three
               organizational factors: county-level norms about pre-sentence
               investigation reports; alterations made to the instrument by the
               Pennsylvania Sentencing Commission in response to years of public
               and internal resistance; and problems with how information is
               disseminated to judges. These findings shed new light on the
               important role of organizational influences on professional
               resistance to algorithms, which helps explain why
               algorithm-centric reforms can fail to have their desired effect.
               This study also contributes to an empirically-informed argument
               against the use of risk assessment instruments: they are
               resource-intensive and have not demonstrated positive
               on-the-ground impacts.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "community-informed, human-AI interaction, criminal justice,
               algorithm aversion, risk assessment instruments"
}

@ARTICLE{Mears2015-wa,
  title    = "“girls as elite distinction: The appropriation of bodily capital.”
              2015. Special issue on new forms of distinction, poetics",
  author   = "Mears, Ashley",
  abstract = "The capital concept has proliferated in studies of culture and
              stratification, usually depicting individual assets as personal
              advantages within given fields. Because this approach sidesteps
              issues of ownership, it obscures how unequal value can be",
  month    =  oct,
  year     =  2015,
  language = "en"
}

@ARTICLE{Mukherjee2023-ib,
  title         = "Global Voices, Local Biases: Socio-Cultural Prejudices across
                   Languages",
  author        = "Mukherjee, Anjishnu and Raj, Chahat and Zhu, Ziwei and
                   Anastasopoulos, Antonios",
  journal       = "arXiv [cs.CL]",
  abstract      = "Human biases are ubiquitous but not uniform: disparities
                   exist across linguistic, cultural, and societal borders. As
                   large amounts of recent literature suggest, language models
                   (LMs) trained on human data can reflect and often amplify the
                   effects of these social biases. However, the vast majority of
                   existing studies on bias are heavily skewed towards Western
                   and European languages. In this work, we scale the Word
                   Embedding Association Test (WEAT) to 24 languages, enabling
                   broader studies and yielding interesting findings about LM
                   bias. We additionally enhance this data with culturally
                   relevant information for each language, capturing local
                   contexts on a global scale. Further, to encompass more widely
                   prevalent societal biases, we examine new bias dimensions
                   across toxicity, ableism, and more. Moreover, we delve deeper
                   into the Indian linguistic landscape, conducting a
                   comprehensive regional bias analysis across six prevalent
                   Indian languages. Finally, we highlight the significance of
                   these social biases and the new dimensions through an
                   extensive comparison of embedding methods, reinforcing the
                   need to address them in pursuit of more equitable language
                   models. All code, data and results are available here:
                   https://github.com/iamshnoo/weathub.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Kim2023-zz,
  title     = "Going Beyond Affective Polarization: How Emotions and Identities
               are Used in Anti-Vaccination {TikTok} Videos",
  author    = "Kim, Sang Jung and Villanueva, Isabel Iruani and Chen, Kaiping",
  journal   = "Political Communication",
  publisher = "Routledge",
  pages     = "1--20",
  abstract  = "ABSTRACTThe rise of social media as a source of science and
               health information has brought challenges to informed citizenship
               and social trust due to the spread of misinformation,
               particularly anti-vaccination messages that incite hatred and
               discourage necessary health precautions. These messages often
               employ emotional appeals and identity cues. However, scholarship
               examining emotional appeals and identity cues in anti-vax
               messages is still at the nascent stage. Furthermore, most
               literature on emotions and identities on social media has focused
               on text-based platforms, despite the increasing popularity of
               interactive, multimodal platforms. To address these gaps, our
               paper analyzes recent TikTok anti-vax videos and incorporates the
               framework of multimodal frame processing, emotion-as-frames
               model, affective intelligence theory, and social identity theory.
               Our paper uncovers how different message modalities affect the
               impact of emotional narratives and identity cues on user
               engagement. We also investigate sociopolitical identity cues
               beyond partisan identities, expanding the current terrain of
               political communication. Our results demonstrate that audiences
               engage with emotional and identity cues in anti-vax videos
               differently based on distinct message modalities. We also found
               that identity cues related to interpersonal relationships (e.g.
               parental) and conspiracy groups were prevalent, in addition to
               partisan identity cues. These results offer new insights into
               sociopolitical identities beyond partisanship and highlight the
               importance of considering the multi-modal nature of video
               platforms. Overall, our paper sheds light on the complex
               relationship between emotions, identities, and message modalities
               on social media and provides important implications for
               addressing misinformation and improving science communication on
               digital platforms.",
  month     =  aug,
  year      =  2023
}

@ARTICLE{Liang2023-yb,
  title         = "{GPT} detectors are biased against non-native English writers",
  author        = "Liang, Weixin and Yuksekgonul, Mert and Mao, Yining and Wu,
                   Eric and Zou, James",
  journal       = "arXiv [cs.CL]",
  abstract      = "The rapid adoption of generative language models has brought
                   about substantial advancements in digital communication,
                   while simultaneously raising concerns regarding the potential
                   misuse of AI-generated content. Although numerous detection
                   methods have been proposed to differentiate between AI and
                   human-generated content, the fairness and robustness of these
                   detectors remain underexplored. In this study, we evaluate
                   the performance of several widely-used GPT detectors using
                   writing samples from native and non-native English writers.
                   Our findings reveal that these detectors consistently
                   misclassify non-native English writing samples as
                   AI-generated, whereas native writing samples are accurately
                   identified. Furthermore, we demonstrate that simple prompting
                   strategies can not only mitigate this bias but also
                   effectively bypass GPT detectors, suggesting that GPT
                   detectors may unintentionally penalize writers with
                   constrained linguistic expressions. Our results call for a
                   broader conversation about the ethical implications of
                   deploying ChatGPT content detectors and caution against their
                   use in evaluative or educational settings, particularly when
                   they may inadvertently penalize or exclude non-native English
                   speakers from the global discourse. The published version of
                   this study can be accessed at:
                   www.cell.com/patterns/fulltext/S2666-3899(23)00130-7",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-jp,
  title = "{Haraway\_Donna\_J\_Modest\_Witness\_Second\_Millennium\_1997}.pdf"
}

@ARTICLE{Chan2023-ps,
  title         = "Harms from Increasingly Agentic Algorithmic Systems",
  author        = "Chan, Alan and Salganik, Rebecca and Markelius, Alva and
                   Pang, Chris and Rajkumar, Nitarshan and Krasheninnikov,
                   Dmitrii and Langosco, Lauro and He, Zhonghao and Duan, Yawen
                   and Carroll, Micah and Lin, Michelle and Mayhew, Alex and
                   Collins, Katherine and Molamohammadi, Maryam and Burden, John
                   and Zhao, Wanru and Rismani, Shalaleh and Voudouris,
                   Konstantinos and Bhatt, Umang and Weller, Adrian and Krueger,
                   David and Maharaj, Tegan",
  journal       = "arXiv [cs.CY]",
  abstract      = "Research in Fairness, Accountability, Transparency, and
                   Ethics (FATE) has established many sources and forms of
                   algorithmic harm, in domains as diverse as health care,
                   finance, policing, and recommendations. Much work remains to
                   be done to mitigate the serious harms of these systems,
                   particularly those disproportionately affecting marginalized
                   communities. Despite these ongoing harms, new systems are
                   being developed and deployed which threaten the perpetuation
                   of the same harms and the creation of novel ones. In
                   response, the FATE community has emphasized the importance of
                   anticipating harms. Our work focuses on the anticipation of
                   harms from increasingly agentic systems. Rather than
                   providing a definition of agency as a binary property, we
                   identify 4 key characteristics which, particularly in
                   combination, tend to increase the agency of a given
                   algorithmic system: underspecification, directness of impact,
                   goal-directedness, and long-term planning. We also discuss
                   important harms which arise from increasing agency --
                   notably, these include systemic and/or long-range impacts,
                   often on marginalized stakeholders. We emphasize that
                   recognizing agency of algorithmic systems does not absolve or
                   shift the human responsibility for algorithmic harms. Rather,
                   we use the term agency to highlight the increasingly evident
                   fact that ML systems are not fully under human control. Our
                   work explores increasingly agentic algorithmic systems in
                   three parts. First, we explain the notion of an increase in
                   agency for algorithmic systems in the context of diverse
                   perspectives on agency across disciplines. Second, we argue
                   for the need to anticipate harms from increasingly agentic
                   systems. Third, we discuss important harms from increasingly
                   agentic systems and ways forward for addressing them. We
                   conclude by reflecting on implications of our work for
                   anticipating algorithmic harms from emerging systems.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@INPROCEEDINGS{Chan2023-nd,
  title     = "Harms from Increasingly Agentic Algorithmic Systems",
  author    = "Chan, Alan and Salganik, Rebecca and Markelius, Alva and Pang,
               Chris and Rajkumar, Nitarshan and Krasheninnikov, Dmitrii and
               Langosco, Lauro and He, Zhonghao and Duan, Yawen and Carroll,
               Micah and Lin, Michelle and Mayhew, Alex and Collins, Katherine
               and Molamohammadi, Maryam and Burden, John and Zhao, Wanru and
               Rismani, Shalaleh and Voudouris, Konstantinos and Bhatt, Umang
               and Weller, Adrian and Krueger, David and Maharaj, Tegan",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "651--666",
  abstract  = "Research in Fairness, Accountability, Transparency, and Ethics
               (FATE)1 has established many sources and forms of algorithmic
               harm, in domains as diverse as health care, finance, policing,
               and recommendations. Much work remains to be done to mitigate the
               serious harms of these systems, particularly those
               disproportionately affecting marginalized communities. Despite
               these ongoing harms, new systems are being developed and
               deployed, typically without strong regulatory barriers,
               threatening the perpetuation of the same harms and the creation
               of novel ones. In response, the FATE community has emphasized the
               importance of anticipating harms, rather than just responding to
               them. Anticipation of harms is especially important given the
               rapid pace of developments in machine learning (ML). Our work
               focuses on the anticipation of harms from increasingly agentic
               systems. Rather than providing a definition of agency as a binary
               property, we identify 4 key characteristics which, particularly
               in combination, tend to increase the agency of a given
               algorithmic system: underspecification, directness of impact,
               goal-directedness, and long-term planning. We also discuss
               important harms which arise from increasing agency – notably,
               these include systemic and/or long-range impacts, often on
               marginalized or unconsidered stakeholders. We emphasize that
               recognizing agency of algorithmic systems does not absolve or
               shift the human responsibility for algorithmic harms. Rather, we
               use the term agency to highlight the increasingly evident fact
               that ML systems are not fully under human control. Our work
               explores increasingly agentic algorithmic systems in three parts.
               First, we explain the notion of an increase in agency for
               algorithmic systems in the context of diverse perspectives on
               agency across disciplines. Second, we argue for the need to
               anticipate harms from increasingly agentic systems. Third, we
               discuss important harms from increasingly agentic systems and
               ways forward for addressing them. We conclude by reflecting on
               implications of our work for anticipating algorithmic harms from
               emerging systems.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "power, algorithmic systems, safety, autonomy, harms, negative
               externalities, agency, delayed impacts, ethics, sociotechnical
               systems, FATE"
}

@ARTICLE{Ding2023-gm,
  title         = "Harnessing the Power of {LLMs}: Evaluating Human-{AI} Text
                   Co-Creation through the Lens of News Headline Generation",
  author        = "Ding, Zijian and Smith-Renner, Alison and Zhang, Wenjuan and
                   Tetreault, Joel R and Jaimes, Alejandro",
  journal       = "arXiv [cs.CL]",
  abstract      = "To explore how humans can best leverage LLMs for writing and
                   how interacting with these models affects feelings of
                   ownership and trust in the writing process, we compared
                   common human-AI interaction types (e.g., guiding system,
                   selecting from system outputs, post-editing outputs) in the
                   context of LLM-assisted news headline generation. While LLMs
                   alone can generate satisfactory news headlines, on average,
                   human control is needed to fix undesirable model outputs. Of
                   the interaction methods, guiding and selecting model output
                   added the most benefit with the lowest cost (in time and
                   effort). Further, AI assistance did not harm participants'
                   perception of control compared to freeform editing.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Naous2023-js,
  title         = "Having Beer after Prayer? Measuring Cultural Bias in Large
                   Language Models",
  author        = "Naous, Tarek and Ryan, Michael J and Xu, Wei",
  journal       = "arXiv [cs.CL]",
  abstract      = "Are language models culturally biased? It is important that
                   language models conform to the cultural aspects of the
                   communities they serve. However, we show in this paper that
                   language models suffer from a significant bias towards
                   Western culture when handling and generating text in Arabic,
                   often preferring, and producing Western-fitting content as
                   opposed to the relevant Arab content. We quantify this bias
                   through a likelihood scoring-based metric using naturally
                   occurring contexts that we collect from online social media.
                   Our experiments reveal that both Arabic monolingual and
                   multilingual models exhibit bias towards Western culture in
                   eight different cultural aspects: person names, food,
                   clothing, location, literature, beverage, religion, and
                   sports. Models also tend to exhibit more bias when prompted
                   with Arabic sentences that are more linguistically aligned
                   with English. These findings raise concerns about the
                   cultural relevance of current language models. Our analyses
                   show that providing culture-indicating tokens or
                   culturally-relevant demonstrations to the model can help in
                   debiasing.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Govindarajan2020-cd,
  title         = "Help! Need Advice on Identifying Advice",
  author        = "Govindarajan, Venkata Subrahmanyan and Chen, Benjamin T and
                   Warholic, Rebecca and Erk, Katrin and Li, Junyi Jessy",
  journal       = "arXiv [cs.CL]",
  abstract      = "Humans use language to accomplish a wide variety of tasks -
                   asking for and giving advice being one of them. In online
                   advice forums, advice is mixed in with non-advice, like
                   emotional support, and is sometimes stated explicitly,
                   sometimes implicitly. Understanding the language of advice
                   would equip systems with a better grasp of language
                   pragmatics; practically, the ability to identify advice would
                   drastically increase the efficiency of advice-seeking online,
                   as well as advice-giving in natural language generation
                   systems. We present a dataset in English from two Reddit
                   advice forums - r/AskParents and r/needadvice - annotated for
                   whether sentences in posts contain advice or not. Our
                   analysis reveals rich linguistic phenomena in advice
                   discourse. We present preliminary models showing that while
                   pre-trained language models are able to capture advice better
                   than rule-based systems, advice identification is
                   challenging, and we identify directions for future research.
                   Comments: To be presented at EMNLP 2020.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Yang2018-ka,
  title         = "{HotpotQA}: A Dataset for Diverse, Explainable Multi-hop
                   Question Answering",
  author        = "Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio,
                   Yoshua and Cohen, William W and Salakhutdinov, Ruslan and
                   Manning, Christopher D",
  journal       = "arXiv [cs.CL]",
  abstract      = "Existing question answering (QA) datasets fail to train QA
                   systems to perform complex reasoning and provide explanations
                   for answers. We introduce HotpotQA, a new dataset with 113k
                   Wikipedia-based question-answer pairs with four key features:
                   (1) the questions require finding and reasoning over multiple
                   supporting documents to answer; (2) the questions are diverse
                   and not constrained to any pre-existing knowledge bases or
                   knowledge schemas; (3) we provide sentence-level supporting
                   facts required for reasoning, allowing QA systems to reason
                   with strong supervision and explain the predictions; (4) we
                   offer a new type of factoid comparison questions to test QA
                   systems' ability to extract relevant facts and perform
                   necessary comparison. We show that HotpotQA is challenging
                   for the latest QA systems, and the supporting facts enable
                   models to improve performance and make explainable
                   predictions.",
  month         =  sep,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Bennett2023-jd,
  title     = "How does {HCI} Understand Human Agency and Autonomy?",
  author    = "Bennett, Dan and Metatla, Oussama and Roudaut, Anne and Mekler,
               Elisa D",
  booktitle = "Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Article 375",
  pages     = "1--18",
  abstract  = "Human agency and autonomy have always been fundamental concepts
               in HCI. New developments, including ubiquitous AI and the growing
               integration of technologies into our lives, make these issues
               ever pressing, as technologies increase their ability to
               influence our behaviours and values. However, in HCI
               understandings of autonomy and agency remain ambiguous. Both
               concepts are used to describe a wide range of phenomena
               pertaining to sense-of-control, material independence, and
               identity. It is unclear to what degree these understandings are
               compatible, and how they support the development of research
               programs and practical interventions. We address this by
               reviewing 30 years of HCI research on autonomy and agency to
               identify current understandings, open issues, and future
               directions. From this analysis, we identify ethical issues, and
               outline key themes to guide future work. We also articulate
               avenues for advancing clarity and specificity around these
               concepts, and for coordinating integrative work across different
               HCI communities.",
  series    = "CHI '23",
  month     =  apr,
  year      =  2023,
  keywords  = "Autonomy, Self Determination Theory, agency, boundary objects,
               delegation, mixed initiative, theory, user experience"
}

@ARTICLE{Orgad2022-fe,
  title         = "How Gender Debiasing Affects Internal Model Representations,
                   and Why It Matters",
  author        = "Orgad, Hadas and Goldfarb-Tarrant, Seraphina and Belinkov,
                   Yonatan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Common studies of gender bias in NLP focus either on
                   extrinsic bias measured by model performance on a downstream
                   task or on intrinsic bias found in models' internal
                   representations. However, the relationship between extrinsic
                   and intrinsic bias is relatively unknown. In this work, we
                   illuminate this relationship by measuring both quantities
                   together: we debias a model during downstream fine-tuning,
                   which reduces extrinsic bias, and measure the effect on
                   intrinsic bias, which is operationalized as bias
                   extractability with information-theoretic probing. Through
                   experiments on two tasks and multiple bias metrics, we show
                   that our intrinsic bias metric is a better indicator of
                   debiasing than (a contextual adaptation of) the standard WEAT
                   metric, and can also expose cases of superficial debiasing.
                   Our framework provides a comprehensive perspective on bias in
                   NLP models, which can be applied to deploy NLP systems in a
                   more informed manner. Our code and model checkpoints are
                   publicly available.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hendy2023-lb,
  title         = "How Good Are {GPT} Models at Machine Translation? A
                   Comprehensive Evaluation",
  author        = "Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and
                   Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and
                   Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Generative Pre-trained Transformer (GPT) models have shown
                   remarkable capabilities for natural language generation, but
                   their performance for machine translation has not been
                   thoroughly investigated. In this paper, we present a
                   comprehensive evaluation of GPT models for machine
                   translation, covering various aspects such as quality of
                   different GPT models in comparison with state-of-the-art
                   research and commercial systems, effect of prompting
                   strategies, robustness towards domain shifts and
                   document-level translation. We experiment with eighteen
                   different translation directions involving high and low
                   resource languages, as well as non English-centric
                   translations, and evaluate the performance of three GPT
                   models: ChatGPT, GPT3.5 (text-davinci-003), and
                   text-davinci-002. Our results show that GPT models achieve
                   very competitive translation quality for high resource
                   languages, while having limited capabilities for low resource
                   languages. We also show that hybrid approaches, which combine
                   GPT models with other translation systems, can further
                   enhance the translation quality. We perform comprehensive
                   analysis and human evaluation to further understand the
                   characteristics of GPT translations. We hope that our paper
                   provides valuable insights for researchers and practitioners
                   in the field and helps to better understand the potential and
                   limitations of GPT models for translation.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ormerod2023-zj,
  title     = "How is a “kitchen chair” like a “farm horse”? Exploring the
               representation of noun-noun compound semantics in
               Transformer-based language models",
  author    = "Ormerod, Mark and Devereux, Barry and Martínez del Rincón, Jesús",
  journal   = "Comput. Linguist. Assoc. Comput. Linguist.",
  publisher = "MIT Press",
  pages     = "1--33",
  abstract  = "Abstract Despite the success of Transformer-based language models
               in a wide variety of natural language processing tasks, our
               understanding of how these models process a given input in order
               to represent task-relevant information remains incomplete. In
               this work, we focus on semantic composition and examine how
               Transformer-based language models represent semantic information
               related to the meaning of English noun-noun compounds. We probe
               Transformer-based language models for their knowledge of the
               thematic relations that link the head nouns and modifier words of
               compounds (e.g., KITCHEN CHAIR: a chair located in a kitchen).
               Firstly, using a dataset featuring groups of compounds with
               shared lexical or semantic features, we find that token
               representations of six Transformer-based language models
               distinguish between pairs of compounds based on whether they use
               the same thematic relation. Secondly, we utilize fine-grained
               vector representations of compound semantics derived from human
               annotations, and find that token vectors from several models
               elicit a strong signal of the semantic relations used in the
               compounds. In a novel ‘compositional probe’ setting, where we
               compare the semantic relation signal in mean-pooled token vectors
               of compounds to mean-pooled token vectors when the two
               constituent words appear in separate sentences, we find that the
               Transformer-based language models that best represent the
               semantics of noun-noun compounds also do so substantially better
               than in the control condition where the two constituent works are
               processed separately. Overall, our results shed light on the
               ability of Transformer-based language models to support
               compositional semantic processes in representing the meaning of
               noun-noun compounds.",
  month     =  nov,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Tessler2020-pb,
  title    = "How many observations is one generic worth?",
  author   = "Tessler, Michael Henry and Bridgers, Sophie Elizabeth Colby and
              Tenenbaum, Josh",
  journal  = "PsyArXiv",
  abstract = "Generic language (e.g., “Birds fly”) conveys generalizations about
              categories and is essential for learning beyond our direct
              experience. The meaning of generic language is notoriously hard to
              specify, however (e.g., penguins don’t fly). Tessler and Goodman
              (2019) proposed a model for generics that is mathematically
              equivalent to Bayesian belief-updating based on a single
              pedagogical example, suggesting a deep connection be- tween
              learning from experience and learning from language. Relatedly,
              Csibra and Shamsudheen (2015) argue that generics are inherently
              pedagogical, understood by infants as referring to a member of a
              kind. In two experiments with adults, we quantify the
              exchange-rate between generics and observations by relating their
              belief-updating capacity, varying both the number of observations
              and whether they are presented pedagogically or incidentally. We
              find generics convey stronger generalizations than single
              pedagogical observations (Expt. 1), even when the property is
              explicitly demarcated (Expt. 2). We suggest revisions to the vague
              quantifier model of generics that would allow it to accommodate
              this intriguing exchange-rate.",
  month    =  jun,
  year     =  2020
}

@ARTICLE{Author_undated-vr,
  title  = "How much discrimination is acceptable? A perspective on the
            evaluation of fairness and risk management in the {AI} Act",
  author = "Author, Anonymous"
}

@MISC{noauthor_undated-vi,
  title        = "How Partisanship and Perceived Political Bias Affect Wikipedia
                  Entries of News Sources",
  howpublished = "\url{https://scholar.google.ca/scholar?cluster=17359126482709612009\&hl=en\&as\_sdt=0,5\&sciodt=0,5}",
  note         = "Accessed: 2023-11-9"
}

@ARTICLE{Beukeboom2019-rj,
  title     = "How stereotypes are shared through language: A review and
               introduction of the Social Categories and Stereotypes
               Communication ({SCSC}) framework",
  author    = "Beukeboom, Camiel J and Burgers, Christian",
  journal   = "Rev. Commun. Res.",
  publisher = "The Netherlands Press",
  volume    =  7,
  pages     = "1--37",
  abstract  = "Language use plays a crucial role in the consensualization of
               stereotypes within cultural groups. Based on an integrative
               review of the literature on stereotyping and biased language use,
               we propose the Social Categories and Stereotypes Communication
               (SCSC) framework. The framework integrates largely independent
               areas of literature and explicates the linguistic processes
               through which social-category stereotypes are shared and
               maintained. We distinguish two groups of biases in language use
               that jointly feed and maintain three fundamental cognitive
               variables in (shared) social-category cognition: perceived
               category entitativity, stereotype content, and perceived
               essentialism of associated stereotypic characteristics. These
               are: (1) Biases in linguistic labels used to denote categories,
               within which we discuss biases in (a) label content and (b)
               linguistic form of labels; (2) Biases in describing behaviors and
               characteristics of categorized individuals, within which we
               discuss biases in (a) communication content (i.e., what
               information is communicated), and (b) linguistic form of
               descriptions (i.e., how is information formulated). Together,
               these biases create a self-perpetuating cycle in which
               social-category stereotypes are shared and maintained. The
               framework allows for a better understanding of stereotype
               maintaining biases in natural language. We discuss various
               opportunities for further research.",
  year      =  2019
}

@ARTICLE{Submission_undated-el,
  title  = "How Susceptible are Large Language Models to Ideological
            Manipulation?",
  author = "Submission, Anonymous Acl"
}

@ARTICLE{Brown2017-ga,
  title     = "How the Physical Body Structures Identity, Mobility, and
               Transnationalism",
  author    = "Brown, Hana E",
  journal   = "Soc. Probl.",
  publisher = "[Oxford University Press, Society for the Study of Social
               Problems]",
  volume    =  64,
  number    =  1,
  pages     = "14--29",
  abstract  = "[This article integrates insights from the sociology of the body
               and the sociology of immigration to examine the role of the body
               in the immigrant incorporation process. Drawing on three years of
               participant observation in a West African immigrant community, I
               show how immigrants from a predominantly agrarian society must
               adapt and retrain their bodies, often under great pressure, to
               meet the demands of American social institutions. Immigrants’
               ability to move their bodies in socially prescribed ways affects
               three crucial aspects of the incorporation process: identity
               formation, economic mobility, and transnational practices.
               Immigrants who struggle to execute the host society’s normative
               bodily movements (1) interpret their bodily challenges as
               evidence of their outsider identity, (2) struggle to acquire the
               material resources necessary to achieve more traditionally
               studied forms of economic incorporation, and (3) face limitations
               in their ability to maintain transnational networks even as those
               networks play an increasingly important social role in the face
               of their blocked mobility. These findings indicate that bodily
               incorporation is a critical precursor to full incorporation into
               the host society.]",
  year      =  2017
}

@INPROCEEDINGS{Zhou2023-rs,
  title     = "How to Explain and Justify Almost Any Decision: Potential
               Pitfalls for Accountability in {AI} Decision-Making",
  author    = "Zhou, Joyce and Joachims, Thorsten",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "12--21",
  abstract  = "Discussion of the “right to an explanation” has been increasingly
               relevant because of its potential utility for auditing automated
               decision systems, as well as for making objections to such
               decisions. However, most existing work on explanations focuses on
               collaborative environments, where designers are motivated to
               implement good-faith explanations that reveal potential
               weaknesses of a decision system. This motivation may not hold in
               an auditing environment. Thus, we ask: how much could
               explanations be used maliciously to defend a decision system? In
               this paper, we demonstrate how a black-box explanation system
               developed to defend a black-box decision system could manipulate
               decision recipients or auditors into accepting an intentionally
               discriminatory decision model. In a case-by-case scenario where
               decision recipients are unable to share their cases and
               explanations, we find that most individual decision recipients
               could receive a verifiable justification, even if the decision
               system is intentionally discriminatory. In a system-wide scenario
               where every decision is shared, we find that while justifications
               frequently contradict each other, there is no intuitive threshold
               to determine if these contradictions are because of malicious
               justifications or because of simplicity requirements of these
               justifications conflicting with model behavior. We end with
               discussion of how system-wide metrics may be more useful than
               explanation systems for evaluating overall decision fairness,
               while explanations could be useful outside of fairness auditing.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "adversarial explanations, explainable AI, right to an explanation"
}

@ARTICLE{McCarthy2023-li,
  title     = "How we compare: A new approach to assess aspects of the
               comparison process for appearance-based standards and their
               associations with individual differences in wellbeing and
               personality measures",
  author    = "McCarthy, Peter A and Meyer, Thomas and Back, Mitja D and Morina,
               Nexhmedin",
  journal   = "PLoS One",
  publisher = "Public Library of Science",
  volume    =  18,
  number    =  1,
  pages     = "e0280072",
  abstract  = "We introduce a novel approach to assess habitual comparison
               processes, while distinguishing between different types of
               comparison standards. Several comparison theories (e.g., social)
               suggest that self-evaluations use different standards to inform
               self-perception and are associated with wellbeing and
               personality. We developed the Comparison Standards Scale for
               Appearance (CSS-A) to examine self-reported engagement with
               social, temporal, criteria-based, dimensional, and counterfactual
               comparisons for upward and downward standards in relation to
               appearance. The scale was completed by three hundred participants
               online alongside measures of appearance schemas, social
               comparison evaluations, depression, anxiety, stress, self-esteem,
               physical self-concept, narcissism, and perfectionism. The CSS-A
               was found to reliably assess individual differences in upward and
               downward comparison frequency and affective impact for multiple
               comparison standards. In line with theory, CSS-A upward
               comparisons were more frequent than downward comparisons and
               coincided with negative (versus positive) affective impact.
               Comparison intensity (i.e., comparison frequency × discrepancy)
               predicted negative and positive affective impact for upward and
               downward comparisons, respectively. This relationship was
               partially mediated by appearance concern for upward comparisons
               (a composite of appearance schemas and physical self-concept),
               yet moderated by negativity for downward comparisons (a composite
               of depression, anxiety, stress, and self-esteem). We offer a
               framework for measuring the comparison process that warrants
               further research on underlying comparison processes, for which
               the CSS(-A) and experience sampling methods should serve as
               useful tools.",
  month     =  jan,
  year      =  2023
}

@ARTICLE{Panfili2021-zr,
  title         = "Human-{AI} Interactions Through A Gricean Lens",
  author        = "Panfili, Laura and Duman, Steve and Nave, Andrew and
                   Ridgeway, Katherine Phelps and Eversole, Nathan and Sarikaya,
                   Ruhi",
  journal       = "arXiv [cs.HC]",
  abstract      = "Grice's Cooperative Principle (1975) describes the implicit
                   maxims that guide conversation between humans. As humans
                   begin to interact with non-human dialogue systems more
                   frequently and in a broader scope, an important question
                   emerges: what principles govern those interactions? The
                   present study addresses this question by evaluating human-AI
                   interactions using Grice's four maxims; we demonstrate that
                   humans do, indeed, apply these maxims to interactions with
                   AI, even making explicit references to the AI's performance
                   through a Gricean lens. Twenty-three participants interacted
                   with an American English-speaking Alexa and rated and
                   discussed their experience with an in-lab researcher.
                   Researchers then reviewed each exchange, identifying those
                   that might relate to Grice's maxims: Quantity, Quality,
                   Manner, and Relevance. Many instances of explicit user
                   frustration stemmed from violations of Grice's maxims.
                   Quantity violations were noted for too little but not too
                   much information, while Quality violations were rare,
                   indicating trust in Alexa's responses. Manner violations
                   focused on speed and humanness. Relevance violations were the
                   most frequent, and they appear to be the most frustrating.
                   While the maxims help describe many of the issues
                   participants encountered, other issues do not fit neatly into
                   Grice's framework. Participants were particularly averse to
                   Alexa initiating exchanges or making unsolicited suggestions.
                   To address this gap, we propose the addition of human
                   Priority to describe human-AI interaction. Humans and AIs are
                   not conversational equals, and human initiative takes
                   priority. We suggest that the application of Grice's
                   Cooperative Principles to human-AI interactions is beneficial
                   both from an AI development perspective and as a tool for
                   describing an emerging form of interaction.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Quintanar_undated-pv,
  title  = "Human-computer interaction: A preliminary social psychological
            analysis",
  author = "Quintanar, Leo R and Crowell, Charles R and Pryor, John B"
}

@ARTICLE{Jakesch2023-qv,
  title     = "Human heuristics for {AI}-generated language are flawed",
  author    = "Jakesch, Maurice and Hancock, Jeffrey T and Naaman, Mor",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  120,
  number    =  11,
  pages     = "e2208839120",
  abstract  = "Human communication is increasingly intermixed with language
               generated by AI. Across chat, email, and social media, AI systems
               suggest words, complete sentences, or produce entire
               conversations. AI-generated language is often not identified as
               such but presented as language written by humans, raising
               concerns about novel forms of deception and manipulation. Here,
               we study how humans discern whether verbal self-presentations,
               one of the most personal and consequential forms of language,
               were generated by AI. In six experiments, participants (N =
               4,600) were unable to detect self-presentations generated by
               state-of-the-art AI language models in professional, hospitality,
               and dating contexts. A computational analysis of language
               features shows that human judgments of AI-generated language are
               hindered by intuitive but flawed heuristics such as associating
               first-person pronouns, use of contractions, or family topics with
               human-written language. We experimentally demonstrate that these
               heuristics make human judgment of AI-generated language
               predictable and manipulable, allowing AI systems to produce text
               perceived as ``more human than human.'' We discuss solutions,
               such as AI accents, to reduce the deceptive potential of language
               generated by AI, limiting the subversion of human intuition.",
  month     =  mar,
  year      =  2023,
  keywords  = "cognitive heuristics; human–AI interaction; language generation;
               risks of AI",
  language  = "en"
}

@ARTICLE{Jakesch2023-pu,
  title     = "Human heuristics for {AI}-generated language are flawed",
  author    = "Jakesch, Maurice and Hancock, Jeffrey T and Naaman, Mor",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  120,
  number    =  11,
  pages     = "e2208839120",
  abstract  = "Human communication is increasingly intermixed with language
               generated by AI. Across chat, email, and social media, AI systems
               suggest words, complete sentences, or produce entire
               conversations. AI-generated language is often not identified as
               such but presented as language written by humans, raising
               concerns about novel forms of deception and manipulation. Here,
               we study how humans discern whether verbal self-presentations,
               one of the most personal and consequential forms of language,
               were generated by AI. In six experiments, participants (N =
               4,600) were unable to detect self-presentations generated by
               state-of-the-art AI language models in professional, hospitality,
               and dating contexts. A computational analysis of language
               features shows that human judgments of AI-generated language are
               hindered by intuitive but flawed heuristics such as associating
               first-person pronouns, use of contractions, or family topics with
               human-written language. We experimentally demonstrate that these
               heuristics make human judgment of AI-generated language
               predictable and manipulable, allowing AI systems to produce text
               perceived as ``more human than human.'' We discuss solutions,
               such as AI accents, to reduce the deceptive potential of language
               generated by AI, limiting the subversion of human intuition.",
  month     =  mar,
  year      =  2023,
  keywords  = "cognitive heuristics; human–AI interaction; language generation;
               risks of AI",
  language  = "en"
}

@ARTICLE{Fanton2021-nm,
  title         = "Human-in-the-Loop for Data Collection: a Multi-Target Counter
                   Narrative Dataset to Fight Online Hate Speech",
  author        = "Fanton, Margherita and Bonaldi, Helena and Tekiroglu, Serra
                   Sinem and Guerini, Marco",
  journal       = "arXiv [cs.CL]",
  abstract      = "Undermining the impact of hateful content with informed and
                   non-aggressive responses, called counter narratives, has
                   emerged as a possible solution for having healthier online
                   communities. Thus, some NLP studies have started addressing
                   the task of counter narrative generation. Although such
                   studies have made an effort to build hate speech / counter
                   narrative (HS/CN) datasets for neural generation, they fall
                   short in reaching either high-quality and/or high-quantity.
                   In this paper, we propose a novel human-in-the-loop data
                   collection methodology in which a generative language model
                   is refined iteratively by using its own data from the
                   previous loops to generate new training samples that experts
                   review and/or post-edit. Our experiments comprised several
                   loops including dynamic variations. Results show that the
                   methodology is scalable and facilitates diverse, novel, and
                   cost-effective data collection. To our knowledge, the
                   resulting dataset is the only expert-based multi-target HS/CN
                   dataset available to the community.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Bonaldi2022-oc,
  title         = "Human-Machine Collaboration Approaches to Build a Dialogue
                   Dataset for Hate Speech Countering",
  author        = "Bonaldi, Helena and Dellantonio, Sara and Tekiroglu, Serra
                   Sinem and Guerini, Marco",
  journal       = "arXiv [cs.CL]",
  abstract      = "Fighting online hate speech is a challenge that is usually
                   addressed using Natural Language Processing via automatic
                   detection and removal of hate content. Besides this approach,
                   counter narratives have emerged as an effective tool employed
                   by NGOs to respond to online hate on social media platforms.
                   For this reason, Natural Language Generation is currently
                   being studied as a way to automatize counter narrative
                   writing. However, the existing resources necessary to train
                   NLG models are limited to 2-turn interactions (a hate speech
                   and a counter narrative as response), while in real life,
                   interactions can consist of multiple turns. In this paper, we
                   present a hybrid approach for dialogical data collection,
                   which combines the intervention of human expert annotators
                   over machine generated dialogues obtained using 19 different
                   configurations. The result of this work is DIALOCONAN, the
                   first dataset comprising over 3000 fictitious multi-turn
                   dialogues between a hater and an NGO operator, covering 6
                   targets of hate.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@BOOK{Jorgensen2019-ru,
  title     = "Human rights in the age of platforms",
  author    = "Jørgensen, Edited By: Rikkefrank",
  editor    = "Jørgensen, Rikke Frank",
  publisher = "The MIT Press",
  year      =  2019
}

@INPROCEEDINGS{Kim2023-ql,
  title     = "Humans, {AI}, and context: Understanding end-users’ trust in a
               real-world computer vision application",
  author    = "Kim, Sunnie S Y and Watkins, Elizabeth Anne and Russakovsky, Olga
               and Fong, Ruth and Monroy-Hernández, Andrés",
  booktitle = "2023 ACM Conference on Fairness, Accountability, and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Trust is an important factor in people's interactions with AI
               systems. However, there is a lack of empirical studies examining
               how real end-users trust or distrust the AI system they interact
               with. Most research investigates one aspect of trust in lab
               settings with hypothetical end- users. In this paper, we provide
               a holistic and nuanced understanding of trust in AI through a
               qualitative case study of a real-world computer vision
               application. We report findings from interviews with 20 end-users
               of a popular, AI-based bird identification app where we …",
  month     =  jun,
  year      =  2023
}

@MISC{noauthor_undated-pl,
  title = "humptydumpty.pdf"
}

@ARTICLE{Bates2015-vu,
  title    = "``{I} am a waste of breath, of space, of time'': metaphors of self
              in a pro-anorexia group",
  author   = "Bates, Carolina Figueras",
  journal  = "Qual. Health Res.",
  volume   =  25,
  number   =  2,
  pages    = "189--204",
  abstract = "According to recent research on eating disorders, heavy users of
              pro-anorexia (pro-ana) sites show higher levels of disordered
              eating and more severe impairment of quality of life than
              non-heavy users. A better understanding of how pro-ana members
              self-present in the virtual world could shed some light on these
              offline behaviors. Through discourse analysis, I examined the
              metaphors the members of a pro-ana group invoked in their personal
              profiles on a popular social networking site, to talk about the
              self. I applied the Metaphor Identification Procedure to 757 text
              profiles. I identified four key metaphorical constructions in
              pro-ana members' self-descriptions: self as space, self as weight,
              perfecting the self, and the social self. These four main
              metaphors represented discourse strategies, both to create a
              collective pro-ana identity and to enact an individual identity as
              pro-ana. In this article, I discuss the implications of these
              findings for the treatment of eating disorders.",
  month    =  feb,
  year     =  2015,
  keywords = "Internet; anorexia; discourse analysis; eating disorders; self",
  language = "en"
}

@INPROCEEDINGS{Lee2020-xv,
  title     = "``{I} Hear You, {I} Feel You'': Encouraging Deep Self-disclosure
               through a Chatbot",
  author    = "Lee, Yi-Chieh and Yamashita, Naomi and Huang, Yun and Fu, Wai",
  booktitle = "Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1--12",
  month     =  apr,
  year      =  2020
}

@INPROCEEDINGS{Kim2024-sv,
  title     = "``{I}'m Not Sure, But...'': Examining the Impact of Large
               Language Models' Uncertainty Expression on User Reliance and
               Trust",
  author    = "Kim, Sunnie S Y and Liao, Q Vera and Vorvoreanu, Mihaela and
               Ballard, Stephanie and Vaughan, Jennifer Wortman",
  booktitle = "Proceedings of the 2024 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "822--835",
  abstract  = "Widely deployed large language models (LLMs) can produce
               convincing yet incorrect outputs, potentially misleading users
               who may rely on them as if they were correct. To reduce such
               overreliance, there have been calls for LLMs to communicate their
               uncertainty to end users. However, there has been little
               empirical work examining how users perceive and act upon LLMs’
               expressions of uncertainty. We explore this question through a
               large-scale, pre-registered, human-subject experiment (N=404) in
               which participants answer medical questions with or without
               access to responses from a fictional LLM-infused search engine.
               Using both behavioral and self-reported measures, we examine how
               different natural language expressions of uncertainty impact
               participants’ reliance, trust, and overall task performance. We
               find that first-person expressions (e.g., “I’m not sure, but...”)
               decrease participants’ confidence in the system and tendency to
               agree with the system’s answers, while increasing participants’
               accuracy. An exploratory analysis suggests that this increase can
               be attributed to reduced (but not fully eliminated) overreliance
               on incorrect answers. While we observe similar effects for
               uncertainty expressed from a general perspective (e.g., “It’s not
               clear, but...”), these effects are weaker and not statistically
               significant. Our findings suggest that using natural language
               expressions of uncertainty may be an effective approach for
               reducing overreliance on LLMs, but that the precise language used
               matters. This highlights the importance of user testing before
               deploying LLMs at scale.",
  series    = "FAccT '24",
  month     =  jun,
  year      =  2024,
  keywords  = "Human-AI interaction, Large language models, Overreliance, Trust
               in AI, Uncertainty expression"
}

@ARTICLE{Davis2023-ms,
  title     = "I’m Only Human? The Role of Racial Stereotypes, Humanness, and
               Satisfaction in Transactions with Anthropomorphic Sales Bots",
  author    = "Davis, Nicole and Olsen, Nils and Perry, Vanessa G and Stewart,
               Marcus M and White, Tiffany B",
  journal   = "Journal of the Association for Consumer Research",
  publisher = "The University of Chicago Press",
  volume    =  8,
  number    =  1,
  pages     = "47--58",
  abstract  = "AbstractThis research examines whether consumers ascribe racial
               stereotypes to artificially intelligent (AI; nonhuman) agents and
               whether these stereotypes impact ratings of satisfaction,
               perceptions of competence and humanness, and outcomes of
               negotiated transactions. Drawing on the stereotype content model,
               expectation violation theory, and the humanness-value-loyalty
               framework, we investigate how consumers apply racial stereotype
               judgments in interactions with artificially intelligent agents in
               a controlled negotiation experiment. Results reveal that although
               Black people, in general, are more likely to be stereotyped as
               less competent than Asian or White people, the opposite is true
               for Black AI bots. Furthermore, perceptions of competence and
               humanness of Black AI bots supersede those of Asian and White AI
               bots, leading to increased ratings of overall satisfaction, and
               some evidence of more favorable negotiation behaviors.
               Implications for AI applications in marketing are discussed.",
  month     =  jan,
  year      =  2023
}

@ARTICLE{Vandenbosch2013-mn,
  title     = "“I Might Get Your Heart Racing in My Skin-Tight Jeans”:
               Sexualization on Music Entertainment Television",
  author    = "Vandenbosch, Laura and Vervloessem, Dorien and Eggermont, Steven",
  journal   = "Communication Studies",
  publisher = "Routledge",
  volume    =  64,
  number    =  2,
  pages     = "178--194",
  abstract  = "This study explores the culture of sexualizing the (female) body
               in music entertainment television. A quantitative content
               analysis was conducted, analyzing 9,369 scenes from 1,393 music
               videos and 180 programs, broadcast on Belgian music entertainment
               channels. Results indicated that 39.3\% of the coded scenes
               contained sexualizing messages. These sexualizing messages were
               predominantly messages equating (female) Western body ideals to
               being sexually attractive. Music videos were shown to be a more
               sexualizing genre than nonfictional programs and fictional
               programs. Findings are discussed in light of objectification
               theory.",
  month     =  apr,
  year      =  2013
}

@ARTICLE{Bhagavatula2022-yt,
  title     = "{I2D2}: Inductive Knowledge Distillation with {NeuroLogic} and
               Self-Imitation",
  author    = "Bhagavatula, Chandra and Hwang, Jena D and Downey, Doug and Bras,
               Ronan Le and Lu, Ximing and Qin, Lianhui and Sakaguchi, Keisuke
               and Swayamdipta, Swabha and West, Peter and Choi, Yejin",
  journal   = "Annual Meeting of the Association for Computational Linguistics",
  publisher = "arXiv",
  abstract  = "Commonsense capabilities of pre-trained language models
               dramatically improve with scale, leading many to believe that
               scale is the only winning recipe. But is it? Here, we investigate
               an alternative that a priori seems impossible: can smaller
               language models (e.g., GPT-2) win over models that are orders of
               magnitude larger and better (e.g., GPT-3), if powered with novel
               commonsense distillation algorithms? The key intellectual
               challenge is to design a learning algorithm that achieve a
               competitive level of commonsense acquisition, without relying on
               the benefits of scale. In particular, we study generative models
               of commonsense knowledge, focusing on the task of generating
               generics, statements of commonsense facts about everyday
               concepts, e.g., birds can fly. We introduce I2D2, a novel
               commonsense distillation framework that loosely follows the
               Symbolic Knowledge Distillation of West et al. but breaks the
               dependence on the extreme-scale teacher model with two
               innovations: (1) the novel adaptation of NeuroLogic Decoding to
               enhance the generation quality of the weak, off-the-shelf
               language models, and (2) self-imitation learning to iteratively
               learn from the model's own enhanced commonsense acquisition
               capabilities. Empirical results suggest that scale is not the
               only way, as novel algorithms can be a promising alternative.
               Moreover, our study leads to a new corpus of generics,
               Gen-A-tomic, that is the largest and highest quality available to
               date.",
  year      =  2022,
  language  = "en"
}

@ARTICLE{Chintam2023-gz,
  title         = "Identifying and Adapting Transformer-Components Responsible
                   for Gender Bias in an English Language Model",
  author        = "Chintam, Abhijith and Beloch, Rahel and Zuidema, Willem and
                   Hanna, Michael and van der Wal, Oskar",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) exhibit and amplify many types of
                   undesirable biases learned from the training data, including
                   gender bias. However, we lack tools for effectively and
                   efficiently changing this behavior without hurting general
                   language modeling performance. In this paper, we study three
                   methods for identifying causal relations between LM
                   components and particular output: causal mediation analysis,
                   automated circuit discovery and our novel, efficient method
                   called DiffMask+ based on differential masking. We apply the
                   methods to GPT-2 small and the problem of gender bias, and
                   use the discovered sets of components to perform
                   parameter-efficient fine-tuning for bias mitigation. Our
                   results show significant overlap in the identified components
                   (despite huge differences in the computational requirements
                   of the methods) as well as success in mitigating gender bias,
                   with less damage to general language modeling compared to
                   full model fine-tuning. However, our work also underscores
                   the difficulty of defining and measuring bias, and the
                   sensitivity of causal discovery procedures to dataset choice.
                   We hope our work can contribute to more attention for dataset
                   development, and lead to more effective mitigation strategies
                   for other types of bias.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wang2023-rc,
  title    = "Identifying and Characterizing Behavioral Classes of
              Radicalization within the {QAnon} Conspiracy on Twitter",
  author   = "Wang, Emily L and Luceri, Luca and Pierri, Francesco and Ferrara,
              Emilio",
  journal  = "ICWSM",
  volume   =  17,
  pages    = "890--901",
  abstract = "Social media provide a fertile ground where conspiracy theories
              and radical ideas can flourish, reach broad audiences, and
              sometimes lead to hate or violence beyond the online world itself.
              QAnon represents a notable example of a political conspiracy that
              started out on social media but turned mainstream, in part due to
              public endorsement by influential political figures. Nowadays,
              QAnon conspiracies often appear in the news, are part of political
              rhetoric, and are espoused by significant swaths of people in the
              United States. It is therefore crucial to understand how such a
              conspiracy took root online, and what led so many social media
              users to adopt its ideas. In this work, we propose a framework
              that exploits both social interaction and content signals to
              uncover evidence of user radicalization or support for QAnon.
              Leveraging a large dataset of 240M tweets collected in the run-up
              to the 2020 US Presidential election, we define and validate a
              multivariate metric of radicalization. We use that to separate
              users in distinct, naturally-emerging, classes of behaviors
              associated with radicalization processes, from self-declared QAnon
              supporters to hyper-active conspiracy promoters. We also analyze
              the impact of Twitter's moderation policies on the interactions
              among different classes: we discover aspects of moderation that
              succeed, yielding a substantial reduction in the endorsement
              received by hyperactive QAnon accounts. But we also uncover where
              moderation fails, showing how QAnon content amplifiers are not
              deterred or affected by the Twitter intervention. Our findings
              refine our understanding of online radicalization processes,
              reveal effective and ineffective aspects of moderation, and call
              for the need to further investigate the role social media play in
              the spread of conspiracies.",
  month    =  jun,
  year     =  2023,
  keywords = "Social network analysis; communities identification; expertise and
              authority discovery; Credibility of online content; Organizational
              and group behavior mediated by social media; interpersonal
              communication mediated by social media; Subjectivity in textual
              data; sentiment analysis; polarity/opinion identification and
              extraction, linguistic analyses of social media behavior",
  language = "en"
}

@ARTICLE{Bau2018-no,
  title    = "Identifying and Controlling Important Neurons in Neural Machine
              Translation",
  author   = "Bau, Anthony and Belinkov, Yonatan and Sajjad, Hassan and Durrani,
              Nadir and Dalvi, Fahim and Glass, James",
  journal  = "https://openreview.net › forumhttps://openreview.net › forum",
  abstract = "Neural machine translation (NMT) models learn representations
              containing substantial linguistic information. However, it is not
              clear if such information is fully distributed or if some of it
              can be attributed to individual neurons. We develop unsupervised
              methods for discovering important neurons in NMT models. Our
              methods rely on the intuition that different models learn similar
              properties, and do not require any costly external supervision. We
              show experimentally that translation quality depends on the
              discovered neurons, and find that many of them capture common
              linguistic phenomena. Finally, we show how to control NMT
              translations in predictable ways, by modifying activations of
              individual neurons.",
  month    =  sep,
  year     =  2018
}

@INPROCEEDINGS{Wiegand2022-mk,
  title     = "Identifying Implicitly Abusive Remarks about Identity Groups
               using a Linguistically Informed Approach",
  author    = "Wiegand, Michael and Eder, Elisabeth and Ruppenhofer, Josef",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Seattle, United States",
  pages     = "5600--5612",
  abstract  = "We address the task of distinguishing implicitly abusive
               sentences on identity groups (``Muslims contaminate our planet'')
               from other group-related negative polar sentences (``Muslims
               despise terrorism''). Implicitly abusive language are utterances
               not conveyed by abusive words (e.g. ``bimbo'' or ``scum''). So
               far, the detection of such utterances could not be properly
               addressed since existing datasets displaying a high degree of
               implicit abuse are fairly biased. Following the recently-proposed
               strategy to solve implicit abuse by separately addressing its
               different subtypes, we present a new focused and less biased
               dataset that consists of the subtype of atomic negative sentences
               about identity groups. For that task, we model components that
               each address one facet of such implicit abuse, i.e. depiction as
               perpetrators, aspectual classification and non-conformist views.
               The approach generalizes across different identity groups and
               languages.",
  month     =  jul,
  year      =  2022
}

@ARTICLE{Roy2021-ka,
  title         = "Identifying Morality Frames in Political Tweets using
                   Relational Learning",
  author        = "Roy, Shamik and Pacheco, Maria Leonor and Goldwasser, Dan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Extracting moral sentiment from text is a vital component in
                   understanding public opinion, social movements, and policy
                   decisions. The Moral Foundation Theory identifies five moral
                   foundations, each associated with a positive and negative
                   polarity. However, moral sentiment is often motivated by its
                   targets, which can correspond to individuals or collective
                   entities. In this paper, we introduce morality frames, a
                   representation framework for organizing moral attitudes
                   directed at different entities, and come up with a novel and
                   high-quality annotated dataset of tweets written by US
                   politicians. Then, we propose a relational learning model to
                   predict moral attitudes towards entities and moral
                   foundations jointly. We do qualitative and quantitative
                   evaluations, showing that moral sentiment towards entities
                   differs highly across political ideologies.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hoeken2023-bo,
  title     = "Identifying Slurs and Lexical Hate Speech via Light-Weight
               Dimension Projection in Embedding Space",
  author    = "Hoeken, S and Zarrieß, S and Alaçam, Ö",
  journal   = "Proceedings of the 13th",
  publisher = "aclanthology.org",
  abstract  = "The prevalence of hate speech on online platforms has become a
               pressing concern for society, leading to increased attention
               towards detecting hate speech. Prior work in this area …",
  year      =  2023
}

@ARTICLE{Yoder2023-uu,
  title         = "Identity Construction in a Misogynist Incels Forum",
  author        = "Yoder, Michael Miller and Perry, Chloe and Brown, David West
                   and Carley, Kathleen M and Pruden, Meredith L",
  journal       = "arXiv [cs.CL]",
  abstract      = "Online communities of involuntary celibates (incels) are a
                   prominent source of misogynist hate speech. In this paper, we
                   use quantitative text and network analysis approaches to
                   examine how identity groups are discussed on incels-dot-is,
                   the largest black-pilled incels forum. We find that this
                   community produces a wide range of novel identity terms and,
                   while terms for women are most common, mentions of other
                   minoritized identities are increasing. An analysis of the
                   associations made with identity groups suggests an
                   essentialist ideology where physical appearance, as well as
                   gender and racial hierarchies, determine human value. We
                   discuss implications for research into automated misogynist
                   hate speech detection.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Yang2024-yl,
  title         = "If {LLM} Is the Wizard, Then Code Is the Wand: A Survey on
                   How Code Empowers Large Language Models to Serve as
                   Intelligent Agents",
  author        = "Yang, Ke and Liu, Jiateng and Wu, John and Yang, Chaoqi and
                   Fung, Yi R and Li, Sha and Huang, Zixuan and Cao, Xu and
                   Wang, Xingyao and Wang, Yiquan and Ji, Heng and Zhai,
                   Chengxiang",
  journal       = "arXiv [cs.CL]",
  abstract      = "The prominent large language models (LLMs) of today differ
                   from past language models not only in size, but also in the
                   fact that they are trained on a combination of natural
                   language and formal language (code). As a medium between
                   humans and computers, code translates high-level goals into
                   executable steps, featuring standard syntax, logical
                   consistency, abstraction, and modularity. In this survey, we
                   present an overview of the various benefits of integrating
                   code into LLMs' training data. Specifically, beyond enhancing
                   LLMs in code generation, we observe that these unique
                   properties of code help (i) unlock the reasoning ability of
                   LLMs, enabling their applications to a range of more complex
                   natural language tasks; (ii) steer LLMs to produce structured
                   and precise intermediate steps, which can then be connected
                   to external execution ends through function calls; and (iii)
                   take advantage of code compilation and execution environment,
                   which also provides diverse feedback for model improvement.
                   In addition, we trace how these profound capabilities of
                   LLMs, brought by code, have led to their emergence as
                   intelligent agents (IAs) in situations where the ability to
                   understand instructions, decompose goals, plan and execute
                   actions, and refine from feedback are crucial to their
                   success on downstream tasks. Finally, we present several key
                   challenges and future directions of empowering LLMs with
                   code.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Suchman2023-wp,
  title    = "Imaginaries of omniscience: Automating intelligence in the {US}
              Department of Defense",
  author   = "Suchman, Lucy",
  journal  = "Soc. Stud. Sci.",
  volume   =  53,
  number   =  5,
  pages    = "761--786",
  abstract = "The current reanimation of artificial intelligence includes a
              resurgence of investment in automating military intelligence on
              the part of the US Department of Defense. A series of programs set
              forth a technopolitical imaginary of fully integrated,
              comprehensive and real-time 'situational awareness' across US
              theaters of operation. Locating this imaginary within the history
              of 'closed world' discourse, I offer a critical reading of
              dominant scholarship within military circles that sets out the
              military's cybernetic model of situational awareness in the form
              of the widely referenced Observe, Orient, Decide, Act or OODA
              Loop. I argue that the loop's promise of dynamic homeostasis is
              held in place by the enduring premise of objectivist knowledge,
              enabled through a war apparatus that treats the contingencies and
              ambiguities of relations on the ground as noise from which a
              stable and unambiguous signal can be extracted. In contrast,
              recent challenges to the closed-world imaginary, based on critical
              scholarship and investigative journalism, suggest that the
              aspiration to closure is an engine for the continued
              destructiveness of US interventions and the associated
              regeneration of enmity. To challenge these technopolitics of
              violence we need a radically different kind of situational
              awareness, one that recognizes the place of ignorance in
              perpetuating the project of militarism. Only that kind of
              awareness can inform the public debate required to re-envision a
              future place for the US in the world, founded in alternative
              investments in demilitarization and commitments to our collective
              security.",
  month    =  oct,
  year     =  2023,
  keywords = "closed world; data; imaginaries; militarism; military technologies",
  language = "en"
}

@ARTICLE{An2018-lb,
  title     = "Imaginary People Representing Real Numbers: Generating Personas
               from Online Social Media Data",
  author    = "An, J and Kwak, H and Jung, S and Salminen, J and Admad, M and
               Jansen, B",
  journal   = "ACM Trans. Web",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  12,
  number    =  4,
  pages     = "1--26",
  abstract  = "We develop a methodology to automate creating imaginary people,
               referred to as personas, by processing complex behavioral and
               demographic data of social media audiences. From a popular social
               media account containing more than 30 million interactions by
               viewers from 198 countries engaging with more than 4,200 online
               videos produced by a global media corporation, we demonstrate
               that our methodology has several novel accomplishments,
               including: (a) identifying distinct user behavioral segments
               based on the user content consumption patterns; (b) identifying
               impactful demographics groupings; and (c) creating rich persona
               descriptions by automatically adding pertinent attributes, such
               as names, photos, and personal characteristics. We validate our
               approach by implementing the methodology into an actual working
               system; we then evaluate it via quantitative methods by examining
               the accuracy of predicting content preference of personas, the
               stability of the personas over time, and the generalizability of
               the method via applying to two other datasets. Research findings
               show the approach can develop rich personas representing the
               behavior and demographics of real audiences using
               privacy-preserving aggregated online social media data from major
               online platforms. Results have implications for media companies
               and other organizations distributing content via online
               platforms.",
  month     =  nov,
  year      =  2018,
  keywords  = "user analytics, Persona"
}

@BOOK{Brownstein2016-ns,
  title     = "Implicit Bias and Philosophy, Volume 2: Moral Responsibility,
               Structural Injustice, and Ethics",
  author    = "Brownstein, Michael and Saul, Jennifer",
  publisher = "Oxford University Press",
  abstract  = "There is abundant evidence that most people, often in spite of
               their conscious beliefs, values and attitudes, have implicit
               biases. 'Implicit bias' is a term of art referring to evaluations
               of social groups that are largely outside conscious awareness or
               control. These evaluations are typically thought to involve
               associations between social groups and concepts or roles like
               'violent,' 'lazy,' 'nurturing,' 'assertive,' 'scientist,' and so
               on. Such associations result at least in part from common
               stereotypes found in contemporary liberal societies about members
               of these groups. Implicit Bias and Philosophy brings the work of
               leading philosophers and psychologists together to explore core
               areas of psychological research on implicit (or unconscious)
               bias, as well as the ramifications of implicit bias for core
               areas of philosophy. Volume 2: Moral Responsibility, Structural
               Injustice, and Ethics is comprised of three sections. 'Moral
               Responsibility for Implicit Bias' contains chapters examining the
               relationship of implicit biases to concepts that are central to
               moral responsibility, including control, awareness,
               reasons-responsiveness, and alienation. The chapters in the
               second section—'Structural Injustice'—explore the connections
               between the implicit biases held by individuals and the
               structural injustices of the societies in which they are
               situated. And finally, the third section—'The Ethics of Implicit
               Bias: Theory and Practice'—contains chapters examining strategies
               for implicit attitude change, the ramifications of research on
               implicit bias for philosophers working in ethics, and suggestions
               for combatting implicit biases in the fields of philosophy and
               law. This volume can be read independently of, or in conjunction
               with, Volume I: Metaphysics and Epistemology, which addresses key
               metaphysical and epistemological questions on implicit bias,
               including the effect of implicit bias on scientific research,
               gender stereotypes in philosophy, and the role of heuristics in
               biased reasoning.",
  month     =  apr,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Kracher2002-wl,
  title   = "Imposing {Order—The} Varieties of Anthropomorphism",
  author  = "Kracher, Alfred",
  journal = "Studies in Science and Theology",
  volume  =  8,
  pages   = "239--261",
  year    =  2002
}

@ARTICLE{Glaese2022-qo,
  title         = "Improving alignment of dialogue agents via targeted human
                   judgements",
  author        = "Glaese, Amelia and McAleese, Nat and Trębacz, Maja and
                   Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh,
                   Maribeth and Weidinger, Laura and Chadwick, Martin and
                   Thacker, Phoebe and Campbell-Gillingham, Lucy and Uesato,
                   Jonathan and Huang, Po-Sen and Comanescu, Ramona and Yang,
                   Fan and See, Abigail and Dathathri, Sumanth and Greig, Rory
                   and Chen, Charlie and Fritz, Doug and Elias, Jaume Sanchez
                   and Green, Richard and Mokrá, Soňa and Fernando, Nicholas and
                   Wu, Boxi and Foley, Rachel and Young, Susannah and Gabriel,
                   Iason and Isaac, William and Mellor, John and Hassabis, Demis
                   and Kavukcuoglu, Koray and Hendricks, Lisa Anne and Irving,
                   Geoffrey",
  journal       = "arXiv [cs.LG]",
  abstract      = "We present Sparrow, an information-seeking dialogue agent
                   trained to be more helpful, correct, and harmless compared to
                   prompted language model baselines. We use reinforcement
                   learning from human feedback to train our models with two new
                   additions to help human raters judge agent behaviour. First,
                   to make our agent more helpful and harmless, we break down
                   the requirements for good dialogue into natural language
                   rules the agent should follow, and ask raters about each rule
                   separately. We demonstrate that this breakdown enables us to
                   collect more targeted human judgements of agent behaviour and
                   allows for more efficient rule-conditional reward models.
                   Second, our agent provides evidence from sources supporting
                   factual claims when collecting preference judgements over
                   model statements. For factual questions, evidence provided by
                   Sparrow supports the sampled response 78\% of the time.
                   Sparrow is preferred more often than baselines while being
                   more resilient to adversarial probing by humans, violating
                   our rules only 8\% of the time when probed. Finally, we
                   conduct extensive analyses showing that though our model
                   learns to follow our rules it can exhibit distributional
                   biases.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Fowler1995-bz,
  title     = "Improving survey questions: Design and evaluation",
  author    = "Fowler, F J",
  publisher = "books.google.com",
  abstract  = "Questions as Measures An Overview Designing Questions to Gather
               Factual Data Questions to Measure Subjective States Some General
               Rules for Designing Good Survey Instruments Presurvey Evaluation
               of Questions Assessing the Validity of Survey Questions Question
               Design and Evaluation Issues in Perspective",
  month     =  jul,
  year      =  1995
}

@ARTICLE{Kasirzadeh2022-ht,
  title         = "In conversation with Artificial Intelligence: aligning
                   language models with human values",
  author        = "Kasirzadeh, Atoosa and Gabriel, Iason",
  journal       = "arXiv [cs.CY]",
  abstract      = "Large-scale language technologies are increasingly used in
                   various forms of communication with humans across different
                   contexts. One particular use case for these technologies is
                   conversational agents, which output natural language text in
                   response to prompts and queries. This mode of engagement
                   raises a number of social and ethical questions. For example,
                   what does it mean to align conversational agents with human
                   norms or values? Which norms or values should they be aligned
                   with? And how can this be accomplished? In this paper, we
                   propose a number of steps that help answer these questions.
                   We start by developing a philosophical analysis of the
                   building blocks of linguistic communication between
                   conversational agents and human interlocutors. We then use
                   this analysis to identify and formulate ideal norms of
                   conversation that can govern successful linguistic
                   communication between humans and conversational agents.
                   Furthermore, we explore how these norms can be used to align
                   conversational agents with human values across a range of
                   different discursive domains. We conclude by discussing the
                   practical implications of our proposal for the design of
                   conversational agents that are aligned with these norms and
                   values.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Coda-Forno2023-ik,
  title         = "Inducing anxiety in large language models increases
                   exploration and bias",
  author        = "Coda-Forno, Julian and Witte, Kristin and Jagadish, Akshay K
                   and Binz, Marcel and Akata, Zeynep and Schulz, Eric",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models are transforming research on machine
                   learning while galvanizing public debates. Understanding not
                   only when these models work well and succeed but also why
                   they fail and misbehave is of great societal relevance. We
                   propose to turn the lens of computational psychiatry, a
                   framework used to computationally describe and modify
                   aberrant behavior, to the outputs produced by these models.
                   We focus on the Generative Pre-Trained Transformer 3.5 and
                   subject it to tasks commonly studied in psychiatry. Our
                   results show that GPT-3.5 responds robustly to a common
                   anxiety questionnaire, producing higher anxiety scores than
                   human subjects. Moreover, GPT-3.5's responses can be
                   predictably changed by using emotion-inducing prompts.
                   Emotion-induction not only influences GPT-3.5's behavior in a
                   cognitive task measuring exploratory decision-making but also
                   influences its behavior in a previously-established task
                   measuring biases such as racism and ableism. Crucially,
                   GPT-3.5 shows a strong increase in biases when prompted with
                   anxiety-inducing text. Thus, it is likely that how prompts
                   are communicated to large language models has a strong
                   influence on their behavior in applied settings. These
                   results progress our understanding of prompt engineering and
                   demonstrate the usefulness of methods taken from
                   computational psychiatry for studying the capable algorithms
                   to which we increasingly delegate authority and autonomy.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Volkova_undated-dz,
  title  = "Inferring Latent User Properties from Texts Published in Social
            Media",
  author = "Volkova, Svitlana and Bachrach, Yoram and Armstrong, Michael and
            Sharma, Vijay"
}

@INPROCEEDINGS{Volkova2016-xn,
  title     = "Inferring Perceived Demographics from User Emotional Tone and
               User-Environment Emotional Contrast",
  author    = "Volkova, Svitlana and Bachrach, Yoram",
  booktitle = "Proceedings of the 54th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Berlin, Germany",
  pages     = "1567--1578",
  month     =  aug,
  year      =  2016
}

@ARTICLE{Voita2020-xw,
  title         = "Information-Theoretic Probing with Minimum Description Length",
  author        = "Voita, Elena and Titov, Ivan",
  journal       = "arXiv [cs.CL]",
  abstract      = "To measure how well pretrained representations encode some
                   linguistic property, it is common to use accuracy of a probe,
                   i.e. a classifier trained to predict the property from the
                   representations. Despite widespread adoption of probes,
                   differences in their accuracy fail to adequately reflect
                   differences in representations. For example, they do not
                   substantially favour pretrained representations over randomly
                   initialized ones. Analogously, their accuracy can be similar
                   when probing for genuine linguistic labels and probing for
                   random synthetic tasks. To see reasonable differences in
                   accuracy with respect to these random baselines, previous
                   work had to constrain either the amount of probe training
                   data or its model size. Instead, we propose an alternative to
                   the standard probes, information-theoretic probing with
                   minimum description length (MDL). With MDL probing, training
                   a probe to predict labels is recast as teaching it to
                   effectively transmit the data. Therefore, the measure of
                   interest changes from probe accuracy to the description
                   length of labels given representations. In addition to probe
                   quality, the description length evaluates ``the amount of
                   effort'' needed to achieve the quality. This amount of effort
                   characterizes either (i) size of a probing model, or (ii) the
                   amount of data needed to achieve the high quality. We
                   consider two methods for estimating MDL which can be easily
                   implemented on top of the standard probing pipelines:
                   variational coding and online coding. We show that these
                   methods agree in results and are more informative and stable
                   than the standard probes.",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Basu2023-yx,
  title         = "Inspecting the Geographical Representativeness of Images from
                   Text-to-Image Models",
  author        = "Basu, Abhipsa and Venkatesh Babu, R and Pruthi, Danish",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent progress in generative models has resulted in models
                   that produce both realistic as well as relevant images for
                   most textual inputs. These models are being used to generate
                   millions of images everyday, and hold the potential to
                   drastically impact areas such as generative art, digital
                   marketing and data augmentation. Given their outsized impact,
                   it is important to ensure that the generated content reflects
                   the artifacts and surroundings across the globe, rather than
                   over-representing certain parts of the world. In this paper,
                   we measure the geographical representativeness of common
                   nouns (e.g., a house) generated through DALL.E 2 and Stable
                   Diffusion models using a crowdsourced study comprising 540
                   participants across 27 countries. For deliberately
                   underspecified inputs without country names, the generated
                   images most reflect the surroundings of the United States
                   followed by India, and the top generations rarely reflect
                   surroundings from all other countries (average score less
                   than 3 out of 5). Specifying the country names in the input
                   increases the representativeness by 1.44 points on average
                   for DALL.E 2 and 0.75 for Stable Diffusion, however, the
                   overall scores for many countries still remain low,
                   highlighting the need for future models to be more
                   geographically inclusive. Lastly, we examine the feasibility
                   of quantifying the geographical representativeness of
                   generated images without conducting user studies.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Hofman2021-ee,
  title    = "Integrating explanation and prediction in computational social
              science",
  author   = "Hofman, Jake M and Watts, Duncan J and Athey, Susan and Garip,
              Filiz and Griffiths, Thomas L and Kleinberg, Jon and Margetts,
              Helen and Mullainathan, Sendhil and Salganik, Matthew J and
              Vazire, Simine and Vespignani, Alessandro and Yarkoni, Tal",
  journal  = "Nature",
  volume   =  595,
  number   =  7866,
  pages    = "181--188",
  abstract = "Computational social science is more than just large repositories
              of digital data and the computational methods needed to construct
              and analyse them. It also represents a convergence of different
              fields with different ways of thinking about and doing science.
              The goal of this Perspective is to provide some clarity around how
              these approaches differ from one another and to propose how they
              might be productively integrated. Towards this end we make two
              contributions. The first is a schema for thinking about research
              activities along two dimensions-the extent to which work is
              explanatory, focusing on identifying and estimating causal
              effects, and the degree of consideration given to testing
              predictions of outcomes-and how these two priorities can
              complement, rather than compete with, one another. Our second
              contribution is to advocate that computational social scientists
              devote more attention to combining prediction and explanation,
              which we call integrative modelling, and to outline some practical
              suggestions for realizing this goal.",
  month    =  jul,
  year     =  2021,
  language = "en"
}

@ARTICLE{Contro2024-dr,
  title    = "Interaction Minimalism: Minimizing {HRI} to Reduce Emotional
              Dependency on Robots",
  author   = "Contro, Jack and Brandão, Martim",
  abstract = "In this paper we show that with the increasing integration of
              social robots into daily life, concerns arise regarding their
              impact on the potential for creating emotional dependency. Using
              findings from the literature in Human-Robot Interaction,
              Human-Computer Interaction, Internet studies and Political
              Economics, we argue that current design and governance paradigms
              incentivize the creation of emotionally dependent relationships
              between humans and robots. To counteract this, we introduce
              Interaction Minimalism, a design philosophy that aims to minimize
              unnecessary interactions between humans and robots, and instead
              promote human-human relationships, hereby mitigating the risk of
              emotional dependency. By focusing on functionality without
              fostering dependency, this approach encourages autonomy, enhances
              human-human interactions, and advocates for minimal data
              extraction. Through hypothetical design examples, we demonstrate
              the viability of Interaction Minimalism in promoting healthier
              human-robot relationships. Our discussion extends to the
              implications of this design philosophy for future robot
              development, emphasizing the need for a shift towards more ethical
              practices that prioritize human well-being and privacy.",
  month    =  apr,
  year     =  2024
}

@ARTICLE{Pacheco2023-hn,
  title         = "Interactive Concept Learning for Uncovering Latent Themes in
                   Large Text Collections",
  author        = "Pacheco, Maria Leonor and Islam, Tunazzina and Ungar, Lyle
                   and Yin, Ming and Goldwasser, Dan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Experts across diverse disciplines are often interested in
                   making sense of large text collections. Traditionally, this
                   challenge is approached either by noisy unsupervised
                   techniques such as topic models, or by following a manual
                   theme discovery process. In this paper, we expand the
                   definition of a theme to account for more than just a word
                   distribution, and include generalized concepts deemed
                   relevant by domain experts. Then, we propose an interactive
                   framework that receives and encodes expert feedback at
                   different levels of abstraction. Our framework strikes a
                   balance between automation and manual coding, allowing
                   experts to maintain control of their study while reducing the
                   manual effort required.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Berscheid1994-gw,
  title     = "Interpersonal relationships",
  author    = "Berscheid, Ellen",
  journal   = "Annu. Rev. Psychol.",
  publisher = "Annual Reviews",
  volume    =  45,
  number    =  1,
  pages     = "79--129",
  month     =  jan,
  year      =  1994,
  language  = "en"
}

@ARTICLE{Kaur2020-kj,
  title     = "Interpreting interpretability: understanding data scientists' use
               of interpretability tools for machine learning",
  author    = "Kaur, H and Nori, H and Jenkins, S and Caruana, R and {others}",
  journal   = "Proceedings of the",
  publisher = "dl.acm.org",
  abstract  = "Machine learning (ML) models are now routinely deployed in
               domains ranging from criminal justice to healthcare. With this
               newfound ubiquity, ML has moved beyond academia and …",
  year      =  2020
}

@INPROCEEDINGS{Baan2024-nb,
  title     = "Interpreting Predictive Probabilities: Model Confidence or Human
               Label Variation?",
  author    = "Baan, Joris and Fernández, Raquel and Plank, Barbara and Aziz,
               Wilker",
  editor    = "Graham, Yvette and Purver, Matthew",
  booktitle = "Proceedings of the 18th Conference of the European Chapter of the
               Association for Computational Linguistics (Volume 2: Short
               Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "St. Julian's, Malta",
  pages     = "268--277",
  abstract  = "With the rise of increasingly powerful and user-facing NLP
               systems, there is growing interest in assessing whether they have
               a good \_representation of uncertainty\_ by evaluating the
               quality of their predictive distribution over outcomes. We
               identify two main perspectives that drive starkly different
               evaluation protocols. The first treats predictive probability as
               an indication of model confidence; the second as an indication of
               human label variation. We discuss their merits and limitations,
               and take the position that both are crucial for trustworthy and
               fair NLP systems, but that exploiting a single predictive
               distribution is limiting. We recommend tools and highlight
               exciting directions towards models with disentangled
               representations of uncertainty about predictions and uncertainty
               about human labels.",
  month     =  mar,
  year      =  2024
}

@ARTICLE{Goldfarb-Tarrant2020-ls,
  title         = "Intrinsic Bias Metrics Do Not Correlate with Application Bias",
  author        = "Goldfarb-Tarrant, Seraphina and Marchant, Rebecca and
                   Sanchez, Ricardo Muñoz and Pandya, Mugdha and Lopez, Adam",
  journal       = "arXiv [cs.CL]",
  abstract      = "Natural Language Processing (NLP) systems learn harmful
                   societal biases that cause them to amplify inequality as they
                   are deployed in more and more situations. To guide efforts at
                   debiasing these systems, the NLP community relies on a
                   variety of metrics that quantify bias in models. Some of
                   these metrics are intrinsic, measuring bias in word embedding
                   spaces, and some are extrinsic, measuring bias in downstream
                   tasks that the word embeddings enable. Do these intrinsic and
                   extrinsic metrics correlate with each other? We compare
                   intrinsic and extrinsic metrics across hundreds of trained
                   models covering different tasks and experimental conditions.
                   Our results show no reliable correlation between these
                   metrics that holds in all scenarios across tasks and
                   languages. We urge researchers working on debiasing to focus
                   on extrinsic measures of bias, and to make using these
                   measures more feasible via creation of new challenge sets and
                   annotated test data. To aid this effort, we release code, a
                   new intrinsic metric, and an annotated test set focused on
                   gender bias in hate speech.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Amrute2020-rp,
  title     = "Introduction: Computing in/from the South",
  author    = "Amrute, Sareeta and Murillo, Luis Felipe R",
  journal   = "Catalyst",
  publisher = "catalystjournal.org",
  volume    =  6,
  number    =  2,
  abstract  = "In this introduction to the special section “Computing in/from
               the South,” we position the South as a method to change dominant
               frames about the past, present, and future of computing. We
               discuss three narratives frames—developmentalist, postcolonial,
               and decolonial—to trace a significant body of thinking on
               computing otherwise. We argue that each of these frames provides
               a different optic to emphasize the contributions of non-Western,
               feminist, and queer epistemologies to computing worlds. Rather
               than thinking of these co-constructions as replacing one another
               linearly, we argue that they each create a different kind of
               relation among computing’s spaces and places, its pasts and its
               futures. These relations, and the switches between them, comprise
               what we describe as computing in/from the South as both an
               empirical and a methodological framework. The introduction first
               centers developments in computing worlds that are taking place
               outside centers of power. Based on an analysis of these
               developments, we then move beyond the limited twinned frames of
               exploitation and resistance to unpack the relational, political,
               and affective undercurrents that run through data-driven worlds
               from perspectives in and through the South.",
  month     =  nov,
  year      =  2020,
  keywords  = "computing; postcolonial; decolonial; indigenous; race; gender",
  language  = "en"
}

@ARTICLE{Lee2023-pm,
  title     = "Introduction to the Special Issue of “TikTok and Social
               Movements”",
  author    = "Lee, J and Abidin, C",
  journal   = "Social Media+ Society",
  publisher = "journals.sagepub.com",
  abstract  = "This Special Issue of “TikTok and Social Movements” emerges from
               an attempt to map out the landscape of social movements happening
               on TikTok, drawing from the online …",
  year      =  2023
}

@ARTICLE{Sharma2023-we,
  title         = "Investigating Agency of {LLMs} in human-{AI} collaboration
                   tasks",
  author        = "Sharma, Ashish and Rao, Sudha and Brockett, Chris and
                   Malhotra, Akanksha and Jojic, Nebojsa and Dolan, Bill",
  journal       = "arXiv [cs.CL]",
  abstract      = "Agency, the capacity to proactively shape events, is central
                   to how humans interact and collaborate. While LLMs are being
                   developed to simulate human behavior and serve as human-like
                   agents, little attention has been given to the Agency that
                   these models should possess in order to proactively manage
                   the direction of interaction and collaboration. In this
                   paper, we investigate Agency as a desirable function of LLMs,
                   and how it can be measured and managed. We build on
                   social-cognitive theory to develop a framework of features
                   through which Agency is expressed in dialogue - indicating
                   what you intend to do (Intentionality), motivating your
                   intentions (Motivation), having self-belief in intentions
                   (Self-Efficacy), and being able to self-adjust
                   (Self-Regulation). We collect a new dataset of 83 human-human
                   collaborative interior design conversations containing 908
                   conversational snippets annotated for Agency features. Using
                   this dataset, we develop methods for measuring Agency of
                   LLMs. Automatic and human evaluations show that models that
                   manifest features associated with high Intentionality,
                   Motivation, Self-Efficacy, and Self-Regulation are more
                   likely to be perceived as strongly agentive.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-nl,
  title        = "Investigating gender bias in language models using causal
                  mediation analysis",
  howpublished = "\url{https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html}",
  note         = "Accessed: 2023-9-20"
}

@ARTICLE{Hammond2017-vp,
  title    = "Investigating the cognitive structure of stereotypes: Generic
              beliefs about groups predict social judgments better than
              statistical beliefs",
  author   = "Hammond, Matthew D and Cimpian, Andrei",
  journal  = "J. Exp. Psychol. Gen.",
  volume   =  146,
  number   =  5,
  pages    = "607--614",
  abstract = "Stereotypes are typically defined as beliefs about groups, but
              this definition is underspecified. Beliefs about groups can be
              generic or statistical. Generic beliefs attribute features to
              entire groups (e.g., men are strong), whereas statistical beliefs
              encode the perceived prevalence of features (e.g., how common it
              is for men to be strong). In the present research, we sought to
              determine which beliefs-generic or statistical-are more central to
              the cognitive structure of stereotypes. Specifically, we tested
              whether generic or statistical beliefs are more influential in
              people's social judgments, on the assumption that greater
              functional importance indicates greater centrality in stereotype
              structure. Relative to statistical beliefs, generic beliefs about
              social groups were significantly stronger predictors of
              expectations (Studies 1-3) and explanations (Study 4) for
              unfamiliar individuals' traits. In addition, consistent with prior
              evidence that generic beliefs are cognitively simpler than
              statistical beliefs, generic beliefs were particularly predictive
              of social judgments for participants with more intuitive (vs.
              analytic) cognitive styles and for participants higher (vs. lower)
              in authoritarianism, who tend to view outgroups in simplistic,
              all-or-none terms. The present studies suggest that generic
              beliefs about groups are more central than statistical beliefs to
              the cognitive structure of stereotypes. (PsycINFO Database Record",
  month    =  may,
  year     =  2017,
  language = "en"
}

@ARTICLE{Zheng2023-eh,
  title         = "Is ``A helpful assistant'' the best role for Large Language
                   Models? A systematic evaluation of social roles in system
                   prompts",
  author        = "Zheng, Mingqian and Pei, Jiaxin and Jurgens, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "Prompting serves as the major way humans interact with Large
                   Language Models (LLM). Commercial AI systems commonly define
                   the role of the LLM in system prompts. For example, ChatGPT
                   uses ``You are a helpful assistant'' as part of the default
                   system prompt. But is ``a helpful assistant'' the best role
                   for LLMs? In this study, we present a systematic evaluation
                   of how social roles in system prompts affect model
                   performance. We curate a list of 162 roles covering 6 types
                   of interpersonal relationships and 8 types of occupations.
                   Through extensive analysis of 3 popular LLMs and 2457
                   questions, we show that adding interpersonal roles in prompts
                   consistently improves the models' performance over a range of
                   questions. Moreover, while we find that using gender-neutral
                   roles and specifying the role as the audience leads to better
                   performances, predicting which role leads to the best
                   performance remains a challenging task, and that frequency,
                   similarity, and perplexity do not fully explain the effect of
                   social roles on model performances. Our results can help
                   inform the design of system prompts for AI systems. Code and
                   data are available at
                   https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Parker2023-nl,
  title     = "Is hate speech detection the solution the world wants?",
  author    = "Parker, Sara and Ruths, Derek",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  120,
  number    =  10,
  pages     = "e2209384120",
  abstract  = "The machine learning (ML) research community has landed on
               automated hate speech detection as the vital tool in the
               mitigation of bad behavior online. However, it is not clear that
               this is a widely supported view outside of the ML world. Such a
               disconnect can have implications for whether automated detection
               tools are accepted or adopted. Here we lend insight into how
               other key stakeholders understand the challenge of addressing
               hate speech and the role automated detection plays in solving it.
               To do so, we develop and apply a structured approach to
               dissecting the discourses used by online platform companies,
               governments, and not-for-profit organizations when discussing
               hate speech. We find that, where hate speech mitigation is
               concerned, there is a profound disconnect between the computer
               science research community and other stakeholder groups-which
               puts progress on this important problem at serious risk. We
               identify urgent steps that need to be taken to incorporate
               computational researchers into a single, coherent,
               multistakeholder community that is working towards civil
               discourse online.",
  month     =  mar,
  year      =  2023,
  keywords  = "hate speech; machine learning; natural language processing",
  language  = "en"
}

@ARTICLE{Makelov2023-th,
  title         = "Is This the Subspace You Are Looking for? An Interpretability
                   Illusion for Subspace Activation Patching",
  author        = "Makelov, Aleksandar and Lange, Georg and Nanda, Neel",
  journal       = "arXiv [cs.LG]",
  abstract      = "Mechanistic interpretability aims to understand model
                   behaviors in terms of specific, interpretable features, often
                   hypothesized to manifest as low-dimensional subspaces of
                   activations. Specifically, recent studies have explored
                   subspace interventions (such as activation patching) as a way
                   to simultaneously manipulate model behavior and attribute the
                   features behind it to given subspaces. In this work, we
                   demonstrate that these two aims diverge, potentially leading
                   to an illusory sense of interpretability. Counterintuitively,
                   even if a subspace intervention makes the model's output
                   behave as if the value of a feature was changed, this effect
                   may be achieved by activating a dormant parallel pathway
                   leveraging another subspace that is causally disconnected
                   from model outputs. We demonstrate this phenomenon in a
                   distilled mathematical example, in two real-world domains
                   (the indirect object identification task and factual recall),
                   and present evidence for its prevalence in practice. In the
                   context of factual recall, we further show a link to rank-1
                   fact editing, providing a mechanistic explanation for
                   previous work observing an inconsistency between fact editing
                   performance and fact localization. However, this does not
                   imply that activation patching of subspaces is intrinsically
                   unfit for interpretability. To contextualize our findings, we
                   also show what a success case looks like in a task (indirect
                   object identification) where prior manual circuit analysis
                   informs an understanding of the location of a feature. We
                   explore the additional evidence needed to argue that a
                   patched subspace is faithful.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Zhang2024-aw,
  title     = "“it's a fair game”, or is it? Examining how users navigate
               disclosure risks and benefits when using {LLM}-based
               conversational agents",
  author    = "Zhang, Zhiping and Jia, Michelle and Lee, Hao-Ping (hank) and
               Yao, Bingsheng and Das, Sauvik and Lerner, Ada and Wang, Dakuo
               and Li, Tianshi",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  10,
  pages     = "1--26",
  month     =  may,
  year      =  2024
}

@INPROCEEDINGS{Widder2023-ee,
  title     = "It’s about power: What ethical concerns do software engineers
               have, and what do they (feel they can) do about them?",
  author    = "Widder, David Gray and Zhen, Derrick and Dabbish, Laura and
               Herbsleb, James",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "dl.acm.org",
  pages     = "467--479",
  abstract  = "How do software engineers identify and act on their ethical
               concerns? Past work examines how software practitioners navigate
               specific ethical principles such as “fairness”, but this narrows
               the scope of concerns to implementing pre-specified principles.
               In contrast, we report self-identified ethical concerns of 115
               survey respondents and 21 interviewees across five continents and
               in non-profit, contractor, and non-tech firms. We enumerate their
               concerns–military, privacy, advertising, surveillance, and the
               scope of their concerns–from …",
  year      =  2023
}

@ARTICLE{Wei2023-cy,
  title         = "Jailbroken: How Does {LLM} Safety Training Fail?",
  author        = "Wei, Alexander and Haghtalab, Nika and Steinhardt, Jacob",
  journal       = "arXiv [cs.LG]",
  abstract      = "Large language models trained for safety and harmlessness
                   remain susceptible to adversarial misuse, as evidenced by the
                   prevalence of ``jailbreak'' attacks on early releases of
                   ChatGPT that elicit undesired behavior. Going beyond
                   recognition of the issue, we investigate why such attacks
                   succeed and how they can be created. We hypothesize two
                   failure modes of safety training: competing objectives and
                   mismatched generalization. Competing objectives arise when a
                   model's capabilities and safety goals conflict, while
                   mismatched generalization occurs when safety training fails
                   to generalize to a domain for which capabilities exist. We
                   use these failure modes to guide jailbreak design and then
                   evaluate state-of-the-art models, including OpenAI's GPT-4
                   and Anthropic's Claude v1.3, against both existing and newly
                   designed attacks. We find that vulnerabilities persist
                   despite the extensive red-teaming and safety-training efforts
                   behind these models. Notably, new attacks utilizing our
                   failure modes succeed on every prompt in a collection of
                   unsafe requests from the models' red-teaming evaluation sets
                   and outperform existing ad hoc jailbreaks. Our analysis
                   emphasizes the need for safety-capability parity -- that
                   safety mechanisms should be as sophisticated as the
                   underlying model -- and argues against the idea that scaling
                   alone can resolve these safety failure modes.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@MISC{noauthor_undated-zm,
  title = "jane-banette--vibrant-matter.pdf"
}

@MISC{noauthor_undated-yx,
  title = "{Johnstone\_2018\_ch1}.pdf"
}

@INPROCEEDINGS{Alihosseini2019-jp,
  title     = "Jointly Measuring Diversity and Quality in Text Generation Models",
  author    = "Alihosseini, Danial and Montahaei, Ehsan and Soleymani Baghshah,
               Mahdieh",
  booktitle = "Proceedings of the Workshop on Methods for Optimizing and
               Evaluating Neural Language Generation",
  publisher = "Association for Computational Linguistics",
  address   = "Minneapolis, Minnesota",
  pages     = "90--98",
  abstract  = "Text generation is an important Natural Language Processing task
               with various applications. Although several metrics have already
               been introduced to evaluate the text generation methods, each of
               them has its own shortcomings. The most widely used metrics such
               as BLEU only consider the quality of generated sentences and
               neglecting their diversity. For example, repeatedly generation of
               only one high quality sentence would result in a high BLEU score.
               On the other hand, the more recent metric introduced to evaluate
               the diversity of generated texts known as Self-BLEU ignores the
               quality of generated texts. In this paper, we propose metrics to
               evaluate both the quality and diversity simultaneously by
               approximating the distance of the learned generative model and
               the real data distribution. For this purpose, we first introduce
               a metric that approximates this distance using n-gram based
               measures. Then, a feature-based measure which is based on a
               recent highly deep model trained on a large text corpus called
               BERT is introduced. Finally, for oracle training mode in which
               the generatorʼs density can also be calculated, we propose to use
               the distance measures between the corresponding explicit
               distributions. Eventually, the most popular and recent text
               generation models are evaluated using both the existing and the
               proposed metrics and the preferences of the proposed metrics are
               determined.",
  month     =  jun,
  year      =  2019
}

@ARTICLE{Wang2020-by,
  title         = "{K}-Adapter: Infusing Knowledge into Pre-Trained Models with
                   Adapters",
  author        = "Wang, Ruize and Tang, Duyu and Duan, Nan and Wei, Zhongyu and
                   Huang, Xuanjing and Ji, Jianshu and Cao, Guihong and Jiang,
                   Daxin and Zhou, Ming",
  journal       = "arXiv [cs.CL]",
  abstract      = "We study the problem of injecting knowledge into large
                   pre-trained models like BERT and RoBERTa. Existing methods
                   typically update the original parameters of pre-trained
                   models when injecting knowledge. However, when multiple kinds
                   of knowledge are injected, the historically injected
                   knowledge would be flushed away. To address this, we propose
                   K-Adapter, a framework that retains the original parameters
                   of the pre-trained model fixed and supports the development
                   of versatile knowledge-infused model. Taking RoBERTa as the
                   backbone model, K-Adapter has a neural adapter for each kind
                   of infused knowledge, like a plug-in connected to RoBERTa.
                   There is no information flow between different adapters, thus
                   multiple adapters can be efficiently trained in a distributed
                   way. As a case study, we inject two kinds of knowledge in
                   this work, including (1) factual knowledge obtained from
                   automatically aligned text-triplets on Wikipedia and Wikidata
                   and (2) linguistic knowledge obtained via dependency parsing.
                   Results on three knowledge-driven tasks, including relation
                   classification, entity typing, and question answering,
                   demonstrate that each adapter improves the performance and
                   the combination of both adapters brings further improvements.
                   Further analysis indicates that K-Adapter captures versatile
                   knowledge than RoBERTa.",
  month         =  feb,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{Version_undated-yi,
  title     = "King's Research Portal",
  author    = "Version, Document Version Peer",
  journal   = "Robophilosophy Conference 2024",
  publisher = "kclpure.kcl.ac.uk",
  year      =  2024
}

@ARTICLE{Ronkko2023-rj,
  title    = "Knitting Ladies Online",
  author   = "Rönkkö, Marja-Leena and Lapinlahti, Henna and Yliverronen, Virpi",
  journal  = "McCalls",
  abstract = "Introduction People across all cultures and stages of life have an
              innate need to create, as demonstrated by the practice of
              craft-making. Crafting combines skilled handwork and intellectual
              creativity to produce functional or artistic items. It has been
              handed down through generations and encompasses a wide range of
              activities, including knitting, crocheting, quilting, woodwork,
              and carving. Historically, crafting has been integral to societal
              development, serving both functional and aesthetic purposes, but
              it also represents a tangible connection to people’s cultural
              heritage and often reflects the customs and values of a community.
              Since the turn of the millennium there has been a notable
              resurgence in textile crafts that can be attributed to a growing
              desire for personal expression and a return to hands-on,
              sustainable practices in a digitally dominated era. Research has
              shown that a lot of practiced knitting is now not only a
              meaningful leisure activity for various demographic groups
              (Myllys; Rosner and Ryokai) but also associated with feelings of
              empowerment (Myzelev). Furthermore, several studies have
              underscored its profound impact on health, well-being, and overall
              quality of life (Adey; Fields; Mayne). While traditionally seen as
              a predominantly feminine craft, researchers such as Beyer,
              Desmarais, and Morneau have studied the masculine perspective of
              knitting. Contemporary reasons for knitting can be categorised
              into three broad areas: personal motivations, group effects
              (knitting with others), and altruism (knitting for others;
              Rusiñol-Rodríguez et al.). Unlike many crafting projects that are
              bound to specific locations and tools, knitting offers the
              flexibility of a portable work in progress, allowing hobbyists to
              knit virtually anywhere at any time (Rosner and Ryokai).
              Traditionally, knitting communities, often organised around
              projects and events, were found in public spaces like cafes and
              libraries (Price). In addition, in recent years, there has been a
              noticeable shift towards knitting festivals and meet-ups
              (Orton-Johnson) that offer knitters opportunities to gather at
              events centred on yarn, fibres, and all things related to them
              (Gajjala; Orton-Johnson). Knitting in Online Communities It is
              quite common for virtual networks and environments facilitated by
              technological advancement to become an integral part of modern
              knitting practice (Myllys). A number of online communities
              focussed on knitting have emerged on content-sharing platforms
              such blogs, podcasts, YouTube vlogs, Facebook, Instagram, and
              TikTok (Orton-Johnson). Modern technology allows knitting to
              expand beyond the realm of material creation into an experience
              that can involve photography and blogging (Orton-Johnson) or
              sharing information with the recipient of the knitted item as the
              project progresses (Rosner and Ryokai). The first English-language
              knitting podcasts were published in late 2005 as audio recordings
              that listeners could download (Bell). Video-format knitting
              podcasts have been available on YouTube since 2010, with the first
              episode in Finland appearing in autumn 2015. Today, YouTube offers
              a wide range of communication possibilities to content creators
              who frequently encourage their audience to engage with them
              (Frobenius). On YouTube, podcasts often delve into the daily
              intricacies of an individual’s life, hobby, or lifestyle, enabling
              the creation of personalised content that resonates with others
              with similar interests (Rodríguez and Levido). Engaging with
              knitting podcasts, whether by watching episodes or creating them,
              can be viewed as the contemporary equivalent of traditional
              knitting gatherings (e.g., Shen and Cage). These podcasts not only
              allow viewers to interact through comments and video responses but
              also enable content creators to attract and cultivate a community
              of like-minded enthusiasts (Gauntlett). Through various publishing
              platforms and Websites, knitters can share information about their
              own projects, make collaborative plans with others, enhance their
              skills, and be creative contributors to their communities (Rosner
              and Ryokai). That kind of online community plays a significant
              role in exchanging knitters’ perceptions of self-esteem and
              fostering meaningful social connections that offer support and
              empowerment. The diverse social communication that emerges out of
              and occurs alongside the hobby might even facilitate the formation
              of life-long friendships (Mayne). This was significant, for
              example, during the COVID-19 pandemic, when crafting found new
              digital forms, and crafts were also learned through digital
              communication platforms in both hobby activities and in school
              education (Kouhia). On the other hand, transferring knitting
              practices from their historical, geographical, and cultural
              histories can lead to a loss of rich, contextual knowledge, as
              these practices are deeply intertwined with the traditions,
              stories, and skills passed down through generations and might not
              be fully conveyed in online spaces (e.g., Robertson and Vinebaum).
              Knitting podcasts have been studied in terms of the benefits and
              drawbacks they provide their viewers. Gregg explored the impact of
              knitting podcasts on their viewers’ knitting and video-watching
              motivation and found a clear connection between knitting
              motivation and video consumption: the social interaction on
              YouTube and the inspiration offered by podcasters drove viewers to
              knit more. Furthermore, several studies have identified video
              watching as not only motivating but also potentially addictive,
              making it a time-consuming activity (Balakrishnan and Griffiths;
              Chiang and Hsiao; Gauntlett). This study aims to elucidate the
              characteristics associated with the typical Finnish knitting
              podcast and its production. For this, a single research question
              was posed: What are the key characteristics of Finnish knitting
              podcasts? The data was collected from a survey distributed in
              Facebook and Ravelry groups themed around knitting podcasts. All
              19 respondents were female knitting podcasters, whom we refer to
              with pseudonyms (H1–H19) throughout this article. The data were
              analysed using theory-driven content analysis (Hsieh and Shannon).
              We delve into the research findings from the perspective of
              individual empowerment, knitting skills development, and online
              community. Knitting Podcasting as Individual Strength According to
              our data, producing knitting podcasts can be an empowering hobby
              that enables individual development in both skills and identity.
              Knitting podcasters felt that during the hobby they gained
              self-confidence and that their knowledge of their strengths had
              grown. They better understood their potential and developed not
              only tangible skills but also their mental capacity through the
              hobby. Knitting podcaster H13 mentioned that her self-esteem was
              strengthened by the positive feedback her recordings received. On
              the other hand, H18 highlighted that by recording her knitting
              podcasts, she felt that she had made like-minded friends:
              “recording is quite therapeutic for me, as I don’t really have
              live friends to chat with about knitting or anything else”. Upon
              starting their knitting podcasts, knitters often felt that their
              expectations were soon met. Podcasters could express their
              identity by producing content that reflected their own lives and
              by showcasing their knitting to others. They also found that they
              could bring joy to others with the content they produced and had
              the opportunity to share their passion for knitting with
              like-minded individuals. By watching other knitting podcasts,
              hobbyists found topics that they could address in their own
              podcasts. Individual self-expression conveyed personal values,
              which is possible in such a setting. H3 highlighted how wonderful
              it was to find individuals whose style matched her own and how
              much fun it was to follow podcasters with completely different
              styles: I have gotten so many ideas from others! Many patterns
              might go unnoticed, but when you see them on a “live” model, you
              might find knits that suit you. It’s also wonderful to find
              individuals whose style matches mine. It’s also fun to follow
              those whose style doesn’t match – I often get inspiration from
              them too. Both similarities and differences can thus motivate
              individuals, simultaneously influencing the development of each
              person’s distinctive taste and style. Showcasing One’s Skills and
              Learning from Others Based on the survey, making knitting podcasts
              allows enthusiasts to learn new things, show off their skills, and
              celebrate their personal growth with others. The podcasters felt
              they had gained confidence during the whole process of producing
              knitting podcasts. The knitting podcast community was described as
              a welcoming and uplifting place, where everyone is always keen to
              help others. Perhaps the most tangible benefit of the knitting
              podcast hobby was mentioned by a podcaster who, after starting to
              create podcasts, became so passionate about video editing that she
              now regularly uses that skill in her professional life. Creating a
              knitting podcast was motivated by the desire to produce diverse
              content, share one’s own creations, and inspire others to try
              recently developed materials or knitting techniques. For example,
              H6 described her motivation as follows: the opportunity to speak
              and share information about a hobby that’s important to me. ... I
              get to share my passion for crafts. Additionally, [there are]
              viewers’ comments on the videos and a few live meetings.
              Especially when someone says my videos inspired them or helped
              them try something new, it motivates me to continue making videos.
              Feedback and positive comments from viewers about their own ideas
              encouraged podcasters to continue with the hobby and engage in
              discussions. Enthusiasts mentioned being delighted when someone
              commented on being inspired by the topic of a knitting podcast or
              perhaps used the videos to try something new and to learn. H3 was
              particularly pleased by this",
  month    =  nov,
  year     =  2023
}

@ARTICLE{Dai2021-av,
  title         = "Knowledge Neurons in Pretrained Transformers",
  author        = "Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and
                   Chang, Baobao and Wei, Furu",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large-scale pretrained language models are surprisingly good
                   at recalling factual knowledge presented in the training
                   corpus. In this paper, we present preliminary studies on how
                   factual knowledge is stored in pretrained Transformers by
                   introducing the concept of knowledge neurons. Specifically,
                   we examine the fill-in-the-blank cloze task for BERT. Given a
                   relational fact, we propose a knowledge attribution method to
                   identify the neurons that express the fact. We find that the
                   activation of such knowledge neurons is positively correlated
                   to the expression of their corresponding facts. In our case
                   studies, we attempt to leverage knowledge neurons to edit
                   (such as update, and erase) specific factual knowledge
                   without fine-tuning. Our results shed light on understanding
                   the storage of knowledge within pretrained Transformers. The
                   code is available at
                   https://github.com/Hunter-DDM/knowledge-neurons.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ramezani2023-lv,
  title         = "Knowledge of cultural moral norms in large language models",
  author        = "Ramezani, Aida and Xu, Yang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Moral norms vary across cultures. A recent line of work
                   suggests that English large language models contain
                   human-like moral biases, but these studies typically do not
                   examine moral variation in a diverse cultural setting. We
                   investigate the extent to which monolingual English language
                   models contain knowledge about moral norms in different
                   countries. We consider two levels of analysis: 1) whether
                   language models capture fine-grained moral variation across
                   countries over a variety of topics such as ``homosexuality''
                   and ``divorce''; 2) whether language models capture cultural
                   diversity and shared tendencies in which topics people around
                   the globe tend to diverge or agree on in their moral
                   judgment. We perform our analyses with two public datasets
                   from the World Values Survey (across 55 countries) and PEW
                   global surveys (across 40 countries) on morality. We find
                   that pre-trained English language models predict empirical
                   moral norms across countries worse than the English moral
                   norms reported previously. However, fine-tuning language
                   models on the survey data improves inference across countries
                   at the expense of a less accurate estimate of the English
                   moral norms. We discuss the relevance and challenges of
                   incorporating cultural knowledge into the automated inference
                   of moral norms.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Pramusita2021-tu,
  title     = "Korean wave’s influences on Indonesian beauty standard depicted
               in skincare advertisements: Critical discourse analysis",
  author    = "Pramusita, Santa Maya",
  journal   = "LingTera",
  publisher = "Universitas Negeri Yogyakarta",
  volume    =  8,
  number    =  1,
  pages     = "20--29",
  abstract  = "This research aims to disclose the influence of the Korean wave
               toward the Indonesian beauty standards reflected in the skincare
               advertisements. A descriptive qualitative method with the
               approach of critical discourse analysis is employed to obtain the
               research objective. The media are Viva moisturizer advertisement
               appeared in the early 2000s, Wardah White Treatment Essence
               advertisement published in 2019, and Laneige BB Cushion Korean
               advertisement as a part of intertextuality. The 19
               year-publication gaps is deliberately presented to show how the
               shift of beauty myth takes place. The data taken were in words,
               phrases, and sentences, which were then analyzed using
               Fairclough's three-dimensional approach. The results showed that
               Viva attracted consumers’ attention using similes, simple present
               tense, keywords repetition, and positive adjectives. On the other
               side, Wardah used hyperbole, possessive pronoun, and scientific
               terminology to convince the consumers of their product’s
               superiority, in line with Laneige's advertisement. Regarding the
               discursive features, these three advertisements were similar in
               that they used logos, flattery, puffery, and positive image
               representation. Lastly, the analysis of social practice indicated
               that the influence of the Korean wave was clearly seen on
               Wardah’s commercial, which portrayed beauty as Laneige did. They
               tried shaping consumers’ beliefs that beauty means having a
               white, flawless, and glowing face. Thus, the beauty myth had
               changed from the previous era. This resembles Bourdieu’s notion
               of field, capital, and Doxa.",
  month     =  aug,
  year      =  2021
}

@ARTICLE{Ahearn2001-qe,
  title     = "Language and Agency",
  author    = "Ahearn, Laura M",
  journal   = "Annu. Rev. Anthropol.",
  publisher = "Annual Reviews",
  volume    =  30,
  number    = "Volume 30, 2001",
  pages     = "109--137",
  abstract  = "▪ Abstract This review describes and critiques some of the many
               ways agency has been conceptualized in the academy over the past
               few decades, focusing in particular on practice theorists such as
               Giddens, Bourdieu, de Certeau, Sahlins, and Ortner. For scholars
               interested in agency, it demonstrates the importance of looking
               closely at language and argues that the issues surrounding
               linguistic form and agency are relevant to anthropologists with
               widely divergent research agendas. Linguistic anthropologists
               have made significant contributions to the understanding of
               agency as it emerges in discourse, and the final sections of this
               essay describe some of the most promising research in the study
               of language and gender, literacy practices, and the dialogic
               construction of meaning and agency.",
  month     =  oct,
  year      =  2001
}

@BOOK{Talbot2019-fy,
  title     = "Language and Gender",
  author    = "Talbot, Mary",
  publisher = "John Wiley \& Sons",
  abstract  = "Since its first publication in 1998, Mary Talbot’s Language and
               Gender has been a leading textbook, popular with students for its
               accessibility and with teachers for the range and depth it
               achieves in a single volume. This anticipated third edition has
               been thoroughly revised and updated for the era of \#MeToo,
               genderqueer, Trump, and cyberhate. The book is organized into
               three parts. An introductory section provides grounding in early
               ‘classic' studies in the field. In the second section, Talbot
               examines language used by women and men in a variety of speech
               situations and genres. The last section considers the
               construction and performance of gender in discourse, reflecting
               the interest in mass media and popular culture found in recent
               research, as well as the preoccupation with social change that is
               central to Critical Discourse Analysis. Maintaining an emphasis
               on recent research, Talbot covers a range of approaches at an
               introductory level, lucidly presenting sometimes difficult and
               complex issues. Each chapter concludes with a list of recommended
               readings, enabling students to further their interests in various
               topics. Language and Gender will continue to be an essential
               textbook for undergraduates and postgraduates in linguistics,
               sociolinguistics, cultural and media studies, gender studies and
               communication studies.",
  month     =  dec,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Rickford2016-ew,
  title     = "{LANGUAGE} {AND} {LINGUISTICS} {ON} {TRIAL}: {HEARING} {RACHEL}
               {JEANTEL} ({AND} {OTHER} {VERNACULAR} {SPEAKERS}) {IN} {THE}
               {COURTROOM} {AND} {BEYOND}",
  author    = "Rickford, John R and King, Sharese",
  journal   = "Language",
  publisher = "Linguistic Society of America",
  volume    =  92,
  number    =  4,
  pages     = "948--988",
  abstract  = "Rachel Jeantel was the leading prosecution witness when George
               Zimmerman was tried for killing Trayvon Martin, but she spoke in
               African American Vernacular English (AAVE) and her crucial
               testimony was dismissed as incomprehensible and not credible. The
               disregard for her speech in court and the media is familiar to
               vernacular speakers and puts Linguistics itself on trial:
               following Saussure, how do we dispel such 'prejudices' and
               'fictions'? We show that Jeantel speaks a highly systematic AAVE,
               with possible Caribbean influence. We also discuss voice quality
               and other factors that bedeviled her testimony, including dialect
               unfamiliarity and institutionalized racism. Finally, we suggest
               strategies for linguists to help vernacular speakers be better
               heard in courtrooms and beyond.",
  year      =  2016
}

@ARTICLE{Lakoff1973-ho,
  title     = "Language and woman's place",
  author    = "Lakoff, Robin",
  journal   = "Lang. Soc.",
  publisher = "Cambridge University Press",
  volume    =  2,
  number    =  1,
  pages     = "45--79",
  abstract  = "Our use of language embodies attitudes as well as referential
               meanings. ‘Woman's language’ has as foundation the attitude that
               women are marginal to the serious concerns of life, which are
               pre-empted by men. The marginality and powerlessness of women is
               reflected in both the ways women are expected to speak, and the
               ways in which women are spoken of. In appropriate women's speech,
               strong expression of feeling is avoided, expression of
               uncertainty is favored, and means of expression in regard to
               subject-matter deemed ‘trivial’ to the ‘real’ world are
               elaborated. Speech about women implies an object, whose sexual
               nature requires euphemism, and whose social roles are derivative
               and dependent in relation to men. The personal identity of women
               thus is linguistically submerged; the language works against
               treatment of women, as serious persons with individual
               views.These aspects of English are explored with regard to
               lexicon (color terms, particles, evaluative adjectives), and
               syntax (tag-questions, and related aspects of intonation in
               answers to requests, and of requests and orders), as concerns
               speech by women. Speech about women is analyzed with regard to
               lady : woman, master : mistress, widow : widower, and Mr : Mrs.,
               Miss, with notice of differential use of role terms not
               explicitly marked for sex (e.g. professional) as well.Some
               suggestions and conclusions are offered for those working in the
               women's liberation movement and other kinds of social reform;
               second language teaching; and theoretical linguistics. Relevant
               generalizations in linguistics require study of social mores as
               well as of purely linguistic data.",
  month     =  apr,
  year      =  1973
}

@ARTICLE{Voigt2017-ck,
  title    = "Language from police body camera footage shows racial disparities
              in officer respect",
  author   = "Voigt, Rob and Camp, Nicholas P and Prabhakaran, Vinodkumar and
              Hamilton, William L and Hetey, Rebecca C and Griffiths, Camilla M
              and Jurgens, David and Jurafsky, Dan and Eberhardt, Jennifer L",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  114,
  number   =  25,
  pages    = "6521--6526",
  abstract = "Using footage from body-worn cameras, we analyze the
              respectfulness of police officer language toward white and black
              community members during routine traffic stops. We develop
              computational linguistic methods that extract levels of respect
              automatically from transcripts, informed by a thin-slicing study
              of participant ratings of officer utterances. We find that
              officers speak with consistently less respect toward black versus
              white community members, even after controlling for the race of
              the officer, the severity of the infraction, the location of the
              stop, and the outcome of the stop. Such disparities in common,
              everyday interactions between police and the communities they
              serve have important implications for procedural justice and the
              building of police-community trust.",
  month    =  jun,
  year     =  2017,
  keywords = "natural language processing; policing; procedural justice; racial
              disparities; traffic stops",
  language = "en"
}

@ARTICLE{Kumar2022-ho,
  title         = "Language Generation Models Can Cause Harm: So What Can We Do
                   About It? An Actionable Survey",
  author        = "Kumar, Sachin and Balachandran, Vidhisha and Njoo, Lucille
                   and Anastasopoulos, Antonios and Tsvetkov, Yulia",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent advances in the capacity of large language models to
                   generate human-like text have resulted in their increased
                   adoption in user-facing settings. In parallel, these
                   improvements have prompted a heated discourse around the
                   risks of societal harms they introduce, whether inadvertent
                   or malicious. Several studies have explored these harms and
                   called for their mitigation via development of safer, fairer
                   models. Going beyond enumerating the risks of harms, this
                   work provides a survey of practical methods for addressing
                   potential threats and societal harms from language generation
                   models. We draw on several prior works' taxonomies of
                   language model risks to present a structured overview of
                   strategies for detecting and ameliorating different kinds of
                   risks/harms of language generators. Bridging diverse strands
                   of research, this survey aims to serve as a practical guide
                   for both LM researchers and practitioners, with explanations
                   of different mitigation strategies' motivations, their
                   limitations, and open problems for future research.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chang2023-qx,
  title         = "Language Model Behavior: A Comprehensive Survey",
  author        = "Chang, Tyler A and Bergen, Benjamin K",
  journal       = "arXiv [cs.CL]",
  abstract      = "Transformer language models have received widespread public
                   attention, yet their generated text is often surprising even
                   to NLP researchers. In this survey, we discuss over 250
                   recent studies of English language model behavior before
                   task-specific fine-tuning. Language models possess basic
                   capabilities in syntax, semantics, pragmatics, world
                   knowledge, and reasoning, but these capabilities are
                   sensitive to specific inputs and surface features. Despite
                   dramatic increases in generated text quality as models scale
                   to hundreds of billions of parameters, the models are still
                   prone to unfactual responses, commonsense errors, memorized
                   text, and social biases. Many of these weaknesses can be
                   framed as over-generalizations or under-generalizations of
                   learned patterns in text. We synthesize recent results to
                   highlight what is currently known about large language model
                   capabilities, thus providing a resource for applied work and
                   for research in adjacent fields that use language models.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Korinek2023-jk,
  title       = "Language Models and Cognitive Automation for Economic Research",
  author      = "Korinek, Anton",
  institution = "National Bureau of Economic Research",
  number      =  30957,
  abstract    = "Large language models (LLMs) such as ChatGPT have the potential
                 to revolutionize research in economics and other disciplines. I
                 describe 25 use cases along six domains in which LLMs are
                 starting to become useful as both research assistants and
                 tutors: ideation, writing, background research, data analysis,
                 coding, and mathematical derivations. I provide general
                 instructions and demonstrate specific examples for how to take
                 advantage of each of these, classifying the LLM capabilities
                 from experimental to highly useful. I hypothesize that ongoing
                 advances will improve the performance of LLMs across all of
                 these domains, and that economic researchers who take advantage
                 of LLMs to automate micro tasks will become significantly more
                 productive. Finally, I speculate on the longer-term
                 implications of cognitive automation via LLMs for economic
                 research.",
  series      = "Working Paper Series",
  month       =  feb,
  year        =  2023
}

@ARTICLE{Schramowski2021-ig,
  title    = "Language Models have a Moral Dimension",
  author   = "Schramowski, P and Turan, Cigdem and Andersen, Nico and Rothkopf,
              C and Kersting, K",
  journal  = "arXiv.org",
  abstract = "It is shown that recent improvements of LMs also store ethical and
              moral values of the society and actually bring a “moral dimension”
              to surface and provides a path for attenuating or even preventing
              toxic degeneration in LMs. Artiﬁcial writing is permeating our
              lives due to recent advances in large-scale, transformer-based
              language models (LMs) such as BERT, its variants, GPT-2/3, and
              others. Using them as pretrained models and ﬁne-tuning them for
              speciﬁc tasks, researchers have extended the state of the art for
              many NLP tasks and shown that they not only capture linguistic
              knowledge but also retain general knowledge implicitly present in
              the data. These and other successes are exciting. Unfortunately,
              LMs trained on unﬁltered text corpora suﬀer from degenerate and
              biased behaviour. While this is well established, we show that
              recent improvements of LMs also store ethical and moral values of
              the society and actually bring a “moral dimension” to surface: the
              values are capture geometrically by a direction in the embedding
              space, reﬂecting well the agreement of phrases to social norms
              implicitly expressed in the training texts. This provides a path
              for attenuating or even preventing toxic degeneration in LMs.
              Since one can now rate the (non-)normativity of arbitrary phrases
              without explicitly training the LM for this task, the moral
              dimension can be used as “moral compass” guiding (even other) LMs
              towards producing normative text, as we will show.",
  year     =  2021,
  language = "en"
}

@ARTICLE{Merullo2023-gm,
  title         = "Language Models Implement Simple {Word2Vec}-style Vector
                   Arithmetic",
  author        = "Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie",
  journal       = "arXiv [cs.CL]",
  abstract      = "A primary criticism towards language models (LMs) is their
                   inscrutability. This paper presents evidence that, despite
                   their size and complexity, LMs sometimes exploit a
                   computational mechanism familiar from traditional word
                   embeddings: the use of simple vector arithmetic in order to
                   encode abstract relations (e.g.,
                   Poland:Warsaw::China:Beijing). We investigate a range of
                   language model sizes (from 124M parameters to 176B
                   parameters) in an in-context learning setting, and find that
                   for a variety of tasks (involving capital cities,
                   upper-casing, and past-tensing), a key part of the mechanism
                   reduces to a simple linear update applied by the feedforward
                   networks. We further show that this mechanism is specific to
                   tasks that require retrieval from pretraining memory, rather
                   than retrieval from local context. Our results contribute to
                   a growing body of work on the mechanistic interpretability of
                   LLMs, and offer reason to be optimistic that, despite the
                   massive and non-linear nature of the models, the strategies
                   they ultimately use to solve tasks can sometimes reduce to
                   familiar and even intuitive algorithms.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Miehling2024-ba,
  title         = "Language Models in Dialogue: Conversational Maxims for
                   Human-{AI} Interactions",
  author        = "Miehling, Erik and Nagireddy, Manish and Sattigeri, Prasanna
                   and Daly, Elizabeth M and Piorkowski, David and Richards,
                   John T",
  journal       = "arXiv [cs.CL]",
  abstract      = "Modern language models, while sophisticated, exhibit some
                   inherent shortcomings, particularly in conversational
                   settings. We claim that many of the observed shortcomings can
                   be attributed to violation of one or more conversational
                   principles. By drawing upon extensive research from both the
                   social science and AI communities, we propose a set of maxims
                   -- quantity, quality, relevance, manner, benevolence, and
                   transparency -- for describing effective human-AI
                   conversation. We first justify the applicability of the first
                   four maxims (from Grice) in the context of human-AI
                   interactions. We then argue that two new maxims, benevolence
                   (concerning the generation of, and engagement with, harmful
                   content) and transparency (concerning recognition of one's
                   knowledge boundaries, operational constraints, and intents),
                   are necessary for addressing behavior unique to modern
                   human-AI interactions. We evaluate the degree to which
                   various language models are able to understand these maxims
                   and find that models possess an internal prioritization of
                   principles that can significantly impact their ability to
                   interpret the maxims accurately.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chu2023-oo,
  title         = "Language Models Trained on Media Diets Can Predict Public
                   Opinion",
  author        = "Chu, Eric and Andreas, Jacob and Ansolabehere, Stephen and
                   Roy, Deb",
  journal       = "arXiv [cs.CL]",
  abstract      = "Public opinion reflects and shapes societal behavior, but the
                   traditional survey-based tools to measure it are limited. We
                   introduce a novel approach to probe media diet models --
                   language models adapted to online news, TV broadcast, or
                   radio show content -- that can emulate the opinions of
                   subpopulations that have consumed a set of media. To validate
                   this method, we use as ground truth the opinions expressed in
                   U.S. nationally representative surveys on COVID-19 and
                   consumer confidence. Our studies indicate that this approach
                   is (1) predictive of human judgements found in survey
                   response distributions and robust to phrasing and channels of
                   media exposure, (2) more accurate at modeling people who
                   follow media more closely, and (3) aligned with literature on
                   which types of opinions are affected by media consumption.
                   Probing language models provides a powerful new method for
                   investigating media effects, has practical applications in
                   supplementing polls and forecasting public opinion, and
                   suggests a need for further study of the surprising fidelity
                   with which neural language models can predict human
                   responses.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Heddaya2023-cb,
  title         = "Language of Bargaining",
  author        = "Heddaya, Mourad and Dworkin, Solomon and Tan, Chenhao and
                   Voigt, Rob and Zentefis, Alexander",
  journal       = "arXiv [cs.CL]",
  abstract      = "Leveraging an established exercise in negotiation education,
                   we build a novel dataset for studying how the use of language
                   shapes bilateral bargaining. Our dataset extends existing
                   work in two ways: 1) we recruit participants via behavioral
                   labs instead of crowdsourcing platforms and allow
                   participants to negotiate through audio, enabling more
                   naturalistic interactions; 2) we add a control setting where
                   participants negotiate only through alternating, written
                   numeric offers.Despite the two contrasting forms of
                   communication, we find that the average agreed prices of the
                   two treatments are identical. But when subjects can talk,
                   fewer offers are exchanged, negotiations finish faster, the
                   likelihood of reaching agreement rises, and the variance of
                   prices at which subjects agree drops substantially. We
                   further propose a taxonomy of speech acts in negotiation and
                   enrich the dataset with annotated speech acts. We set up
                   prediction tasks to predict negotiation success and find that
                   being reactive to the arguments of the other party is
                   advantageous over driving the negotiation.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Blodgett2020-dx,
  title     = "Language (Technology) is Power: A Critical Survey of {``}Bias{''}
               in {NLP}",
  author    = "Blodgett, Su Lin and Barocas, Solon and Daumé, III, Hal and
               Wallach, Hanna",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Online",
  pages     = "5454--5476",
  abstract  = "We survey 146 papers analyzing ``bias'' in NLP systems, finding
               that their motivations are often vague, inconsistent, and lacking
               in normative reasoning, despite the fact that analyzing ``bias''
               is an inherently normative process. We further find that these
               papers' proposed quantitative techniques for measuring or
               mitigating ``bias'' are poorly matched to their motivations and
               do not engage with the relevant literature outside of NLP. Based
               on these findings, we describe the beginnings of a path forward
               by proposing three recommendations that should guide work
               analyzing ``bias'' in NLP systems. These recommendations rest on
               a greater recognition of the relationships between language and
               social hierarchies, encouraging researchers and practitioners to
               articulate their conceptualizations of ``bias''---i.e., what
               kinds of system behaviors are harmful, in what ways, to whom, and
               why, as well as the normative reasoning underlying these
               statements---and to center work around the lived experiences of
               members of communities affected by NLP systems, while
               interrogating and reimagining the power relations between
               technologists and such communities.",
  month     =  jul,
  year      =  2020
}

@ARTICLE{Wolf2013-xb,
  title     = "Language Use in Eating Disorder Blogs: Psychological Implications
               of Social Online Activity",
  author    = "Wolf, Markus and Theis, Florian and Kordy, Hans",
  journal   = "J. Lang. Soc. Psychol.",
  publisher = "SAGE Publications Inc",
  volume    =  32,
  number    =  2,
  pages     = "212--226",
  abstract  = "Social media have become a significant means for peer-related
               communication, self-presentation, and identity management.
               So-called eating disorder websites that propagate a drastic thin
               ideal and unhealthy eating behaviors have triggered a debate
               about the harmful facets of social Internet activity. Little
               research has addressed the language that is used by the authors
               of these websites. The present study focuses on personal weblogs,
               a popular form of mostly text-based, diary-like, online journals.
               We compared 31 pro?eating disorder blogs, 29 recovery blogs, and
               27 control blogs by the means of computerized quantitative text
               analyses. The language of pro?eating disorder blogs featured
               lower cognitive processing, a more closed-minded writing style,
               was less emotionally expressive, contained fewer social
               references, and focused more on eating-related contents than
               recovery blogs. A subset of 12 language indicators correctly
               classified the blogs in 84\% of the cases. The distinct language
               patterns appear to reflect the psychological conditions of the
               blog authors and provide insight into their various stages of
               coping.",
  month     =  jun,
  year      =  2013
}

@ARTICLE{Horton2023-fa,
  title         = "Large Language Models as Simulated Economic Agents: What Can
                   We Learn from Homo Silicus?",
  author        = "Horton, John J",
  journal       = "arXiv [econ.GN]",
  abstract      = "Newly-developed large language models (LLM) -- because of how
                   they are trained and designed -- are implicit computational
                   models of humans -- a homo silicus. These models can be used
                   the same way economists use homo economicus: they can be
                   given endowments, information, preferences, and so on and
                   then their behavior can be explored in scenarios via
                   simulation. I demonstrate this approach using OpenAI's GPT3
                   with experiments derived from Charness and Rabin (2002),
                   Kahneman, Knetsch and Thaler (1986) and Samuelson and
                   Zeckhauser (1988). The findings are qualitatively similar to
                   the original results, but it is also trivially easy to try
                   variations that offer fresh insights. Departing from the
                   traditional laboratory paradigm, I also create a hiring
                   scenario where an employer faces applicants that differ in
                   experience and wage ask and then analyze how a minimum wage
                   affects realized wages and the extent of labor-labor
                   substitution.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "econ.GN"
}

@ARTICLE{Wu2023-xz,
  title         = "Large Language Models Can Be Used to Scale the Ideologies of
                   Politicians in a Zero-Shot Learning Setting",
  author        = "Wu, Patrick Y and Nagler, Jonathan and Tucker, Joshua A and
                   Messing, Solomon",
  journal       = "arXiv [cs.CY]",
  abstract      = "The aggregation of knowledge embedded in large language
                   models (LLMs) holds the promise of new solutions to problems
                   of observability and measurement in the social sciences. We
                   examine this potential in a challenging setting: measuring
                   latent ideology -- crucial for better understanding core
                   political functions such as democratic representation. We
                   scale pairwise liberal-conservative comparisons between
                   members of the 116th U.S. Senate using prompts made to
                   ChatGPT. Our measure strongly correlates with widely used
                   liberal-conservative scales such as DW-NOMINATE. Our scale
                   also has interpretative advantages, such as not placing
                   senators who vote against their party for ideologically
                   extreme reasons towards the middle. Our measure is more
                   strongly associated with political activists' perceptions of
                   senators than other measures, consistent with LLMs
                   synthesizing vast amounts of politically relevant data from
                   internet/book corpora rather than memorizing existing
                   measures. LLMs will likely open new avenues for measuring
                   latent constructs utilizing modeled information from massive
                   text corpora.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Wang2024-um,
  title         = "Large language models cannot replace human participants
                   because they cannot portray identity groups",
  author        = "Wang, Angelina and Morgenstern, Jamie and Dickerson, John P",
  journal       = "arXiv [cs.CY]",
  abstract      = "Large language models (LLMs) are increasing in capability and
                   popularity, propelling their application in new domains --
                   including as replacements for human participants in
                   computational social science, user testing, annotation tasks,
                   and more. Traditionally, in all of these settings survey
                   distributors are careful to find representative samples of
                   the human population to ensure the validity of their results
                   and understand potential demographic differences. This means
                   in order to be a suitable replacement, LLMs will need to be
                   able to capture the influence of positionality (i.e.,
                   relevance of social identities like gender and race).
                   However, we show that there are two inherent limitations in
                   the way current LLMs are trained that prevent this. We argue
                   analytically for why LLMs are doomed to both misportray and
                   flatten the representations of demographic groups, then
                   empirically show this to be true on 4 LLMs through a series
                   of human studies with 3200 participants across 16 demographic
                   identities. We also discuss a third consideration about how
                   identity prompts can essentialize identities. Throughout, we
                   connect each of these limitations to a pernicious history
                   that shows why each is harmful for marginalized demographic
                   groups. Overall, we urge caution in use cases where LLMs are
                   intended to replace human participants whose identities are
                   relevant to the task at hand. At the same time, in cases
                   where the goal is to supplement rather than replace (e.g.,
                   pilot studies), we provide empirically-better inference-time
                   techniques to reduce, but not remove, these harms.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Huang2023-zr,
  title         = "Large Language Models Cannot Self-Correct Reasoning Yet",
  author        = "Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng,
                   Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou,
                   Denny",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs) have emerged as a groundbreaking
                   technology with their unparalleled text generation
                   capabilities across various applications. Nevertheless,
                   concerns persist regarding the accuracy and appropriateness
                   of their generated content. A contemporary methodology,
                   self-correction, has been proposed as a remedy to these
                   issues. Building upon this premise, this paper critically
                   examines the role and efficacy of self-correction within
                   LLMs, shedding light on its true potential and limitations.
                   Central to our investigation is the notion of intrinsic
                   self-correction, whereby an LLM attempts to correct its
                   initial responses based solely on its inherent capabilities,
                   without the crutch of external feedback. In the context of
                   reasoning, our research indicates that LLMs struggle to
                   self-correct their responses without external feedback, and
                   at times, their performance might even degrade post
                   self-correction. Drawing from these insights, we offer
                   suggestions for future research and practical applications in
                   this field.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Si2023-xo,
  title         = "Large Language Models Help Humans Verify Truthfulness --
                   Except When They Are Convincingly Wrong",
  author        = "Si, Chenglei and Goyal, Navita and Wu, Sherry Tongshuang and
                   Zhao, Chen and Feng, Shi and Daumé, III, Hal and Boyd-Graber,
                   Jordan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs) are increasingly used for
                   accessing information on the web. Their truthfulness and
                   factuality are thus of great interest. To help users make the
                   right decisions about the information they're getting, LLMs
                   should not only provide but also help users fact-check
                   information. In this paper, we conduct experiments with 80
                   crowdworkers in total to compare language models with search
                   engines (information retrieval systems) at facilitating
                   fact-checking by human users. We prompt LLMs to validate a
                   given claim and provide corresponding explanations. Users
                   reading LLM explanations are significantly more efficient
                   than using search engines with similar accuracy. However,
                   they tend to over-rely the LLMs when the explanation is
                   wrong. To reduce over-reliance on LLMs, we ask LLMs to
                   provide contrastive information - explain both why the claim
                   is true and false, and then we present both sides of the
                   explanation to users. This contrastive explanation mitigates
                   users' over-reliance on LLMs, but cannot significantly
                   outperform search engines. However, showing both search
                   engine results and LLM explanations offers no complementary
                   benefits as compared to search engines alone. Taken together,
                   natural language explanations by LLMs may not be a reliable
                   replacement for reading the retrieved passages yet,
                   especially in high-stakes settings where over-relying on
                   wrong AI explanations could lead to critical consequences.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Movva2023-bq,
  title         = "Large language models shape and are shaped by society: A
                   survey of {arXiv} publication patterns",
  author        = "Movva, Rajiv and Balachandar, Sidhika and Peng, Kenny and
                   Agostini, Gabriel and Garg, Nikhil and Pierson, Emma",
  journal       = "arXiv [cs.DL]",
  abstract      = "There has been a steep recent increase in the number of large
                   language model (LLM) papers, producing a dramatic shift in
                   the scientific landscape which remains largely undocumented
                   through bibliometric analysis. Here, we analyze 388K papers
                   posted on the CS and Stat arXivs, focusing on changes in
                   publication patterns in 2023 vs. 2018-2022. We analyze how
                   the proportion of LLM papers is increasing; the LLM-related
                   topics receiving the most attention; the authors writing LLM
                   papers; how authors' research topics correlate with their
                   backgrounds; the factors distinguishing highly cited LLM
                   papers; and the patterns of international collaboration. We
                   show that LLM research increasingly focuses on societal
                   impacts: there has been an 18x increase in the proportion of
                   LLM-related papers on the Computers and Society sub-arXiv,
                   and authors newly publishing on LLMs are more likely to focus
                   on applications and societal impacts than more experienced
                   authors. LLM research is also shaped by social dynamics: we
                   document gender and academic/industry disparities in the
                   topics LLM authors focus on, and a US/China schism in the
                   collaboration network. Overall, our analysis documents the
                   profound ways in which LLM research both shapes and is shaped
                   by society, attesting to the necessity of sociotechnical
                   lenses.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL"
}

@ARTICLE{Schramowski2022-wa,
  title     = "Large pre-trained language models contain human-like biases of
               what is right and wrong to do",
  author    = "Schramowski, Patrick and Turan, Cigdem and Andersen, Nico and
               Rothkopf, Constantin A and Kersting, Kristian",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  4,
  number    =  3,
  pages     = "258--268",
  abstract  = "Artificial writing is permeating our lives due to recent advances
               in large-scale, transformer-based language models (LMs) such as
               BERT, GPT-2 and GPT-3. Using them as pre-trained models and
               fine-tuning them for specific tasks, researchers have extended
               the state of the art for many natural language processing tasks
               and shown that they capture not only linguistic knowledge but
               also retain general knowledge implicitly present in the data.
               Unfortunately, LMs trained on unfiltered text corpora suffer from
               degenerated and biased behaviour. While this is well established,
               we show here that recent LMs also contain human-like biases of
               what is right and wrong to do, reflecting existing ethical and
               moral norms of society. We show that these norms can be captured
               geometrically by a ‘moral direction’ which can be computed, for
               example, by a PCA, in the embedding space. The computed ‘moral
               direction’ can rate the normativity (or non-normativity) of
               arbitrary phrases without explicitly training the LM for this
               task, reflecting social norms well. We demonstrate that computing
               the ’moral direction’ can provide a path for attenuating or even
               preventing toxic degeneration in LMs, showcasing this capability
               on the RealToxicityPrompts testbed. Large language models
               identify patterns in the relations between words and capture
               their relations in an embedding space. Schramowski and colleagues
               show that a direction in this space can be identified that
               separates ‘right’ and ‘wrong’ actions as judged by human survey
               participants.",
  month     =  mar,
  year      =  2022,
  language  = "en"
}

@INPROCEEDINGS{Zufle2023-kl,
  title     = "Latent Feature-based Data Splits to Improve Generalisation
               Evaluation: A Hate Speech Detection Case Study",
  author    = "Züfle, Maike and Dankers, Verna and Titov, Ivan",
  editor    = "Hupkes, Dieuwke and Dankers, Verna and Batsuren, Khuyagbaatar and
               Sinha, Koustuv and Kazemnejad, Amirhossein and
               Christodoulopoulos, Christos and Cotterell, Ryan and Bruni, Elia",
  booktitle = "Proceedings of the 1st GenBench Workshop on (Benchmarking)
               Generalisation in NLP",
  publisher = "Association for Computational Linguistics",
  address   = "Singapore",
  pages     = "112--129",
  abstract  = "With the ever-growing presence of social media platforms comes
               the increased spread of harmful content and the need for robust
               hate speech detection systems. Such systems easily overfit to
               specific targets and keywords, and evaluating them without
               considering distribution shifts that might occur between train
               and test data overestimates their benefit. We challenge hate
               speech models via new train-test splits of existing datasets that
               rely on the clustering of models' hidden representations. We
               present two split variants (Subset-Sum-Split and Closest-Split)
               that, when applied to two datasets using four pretrained models,
               reveal how models catastrophically fail on blind spots in the
               latent space. This result generalises when developing a split
               with one model and evaluating it on another. Our analysis
               suggests that there is no clear surface-level property of the
               data split that correlates with the decreased performance, which
               underscores that task difficulty is not always humanly
               interpretable. We recommend incorporating latent feature-based
               splits in model development and release two splits via the
               GenBench benchmark.",
  month     =  dec,
  year      =  2023
}

@INPROCEEDINGS{Prakash2023-rl,
  title     = "Layered Bias: Interpreting Bias in Pretrained Large Language
               Models",
  author    = "Prakash, Nirmalendu and Lee, Roy Ka-Wei",
  editor    = "Belinkov, Yonatan and Hao, Sophie and Jumelet, Jaap and Kim,
               Najoung and McCarthy, Arya and Mohebbi, Hosein",
  booktitle = "Proceedings of the 6th BlackboxNLP Workshop: Analyzing and
               Interpreting Neural Networks for NLP",
  publisher = "Association for Computational Linguistics",
  address   = "Singapore",
  pages     = "284--295",
  abstract  = "Large language models (LLMs) like GPT and PALM have excelled in
               numerous natural language processing (NLP) tasks such as text
               generation, question answering, and translation. However, they
               are also found to have inherent social biases. To address this,
               recent studies have proposed debiasing techniques like iterative
               nullspace projection (INLP) and Counterfactual Data Augmentation
               (CDA). Additionally, there's growing interest in understanding
               the intricacies of these models. Some researchers focus on
               individual neural units, while others examine specific layers. In
               our study, we benchmark newly released models, assess the impact
               of debiasing methods, and investigate how biases are linked to
               different transformer layers using a method called Logit Lens.
               Specifically, we evaluate three modern LLMs: OPT, LLaMA, and
               LLaMA2, and their debiased versions. Our experiments are based on
               two popular bias evaluation datasets, StereoSet and CrowS-Pairs,
               and we perform a layer-by-layer analysis using the Logit Lens.",
  month     =  dec,
  year      =  2023
}

@ARTICLE{Belrose2023-pf,
  title         = "{LEACE}: Perfect linear concept erasure in closed form",
  author        = "Belrose, Nora and Schneider-Joseph, David and Ravfogel,
                   Shauli and Cotterell, Ryan and Raff, Edward and Biderman,
                   Stella",
  journal       = "arXiv [cs.LG]",
  abstract      = "Concept erasure aims to remove specified features from a
                   representation. It can improve fairness (e.g. preventing a
                   classifier from using gender or race) and interpretability
                   (e.g. removing a concept to observe changes in model
                   behavior). We introduce LEAst-squares Concept Erasure
                   (LEACE), a closed-form method which provably prevents all
                   linear classifiers from detecting a concept while changing
                   the representation as little as possible, as measured by a
                   broad class of norms. We apply LEACE to large language models
                   with a novel procedure called ``concept scrubbing,'' which
                   erases target concept information from every layer in the
                   network. We demonstrate our method on two tasks: measuring
                   the reliance of language models on part-of-speech
                   information, and reducing gender bias in BERT embeddings.
                   Code is available at
                   https://github.com/EleutherAI/concept-erasure.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Ling2023-wf,
  title     = "``learn the facts about {COVID}-19'': Analyzing the use of
               warning labels on {TikTok} videos",
  author    = "Ling, Chen and Gummadi, Krishna P and Zannettou, Savvas",
  journal   = "Proceedings of the International AAAI Conference on Web and
               Social Media",
  publisher = "Association for the Advancement of Artificial Intelligence (AAAI)",
  volume    =  17,
  pages     = "554--565",
  abstract  = "During the COVID-19 pandemic, health-related misinformation and
               harmful content shared online had a significant adverse effect on
               society. In an attempt to mitigate this adverse effect,
               mainstream social media platforms like Facebook, Twitter, and
               TikTok employed soft moderation interventions (i.e., warning
               labels) on potentially harmful posts. Such interventions aim to
               inform users about the post's content without removing it, hence
               easing the public's concerns about censorship and freedom of
               speech. Despite the recent popularity of these moderation
               interventions, as a research community, we lack empirical
               analyses aiming to uncover how these warning labels are used in
               the wild, particularly during challenging times like the COVID-19
               pandemic. In this work, we analyze the use of warning labels on
               TikTok, focusing on COVID-19 videos. First, we construct a set of
               26 COVID-19 related hashtags, and then we collect 41K videos that
               include those hashtags in their description. Second, we perform a
               quantitative analysis on the entire dataset to understand the use
               of warning labels on TikTok. Then, we perform an in-depth
               qualitative study, using thematic analysis, on 222 COVID-19
               related videos to assess the content and the connection between
               the content and the warning labels. Our analysis shows that
               TikTok broadly applies warning labels on TikTok videos, likely
               based on hashtags included in the description (e.g., 99\% of the
               videos that contain \#coronavirus have warning labels). More
               worrying is the addition of COVID-19 warning labels on videos
               where their actual content is not related to COVID-19 (23\% of
               the cases in a sample of 143 English videos that are not related
               to COVID-19). Finally, our qualitative analysis on a sample of
               222 videos shows that 7.7\% of the videos share
               misinformation/harmful content and do not include warning labels,
               37.3\% share benign information and include warning labels, and
               that 35\% of the videos that share misinformation/harmful content
               (and need a warning label) are made for fun. Our study
               demonstrates the need to develop more accurate and precise soft
               moderation systems, especially on a platform like TikTok that is
               extremely popular among people of younger age.",
  month     =  jun,
  year      =  2023,
  keywords  = "Credibility of online content; Qualitative and quantitative
               studies of social media; Trust; reputation; recommendation
               systems; Web and Social Media",
  language  = "en"
}

@ARTICLE{Lu2022-ni,
  title     = "Learn to explain: Multimodal reasoning via thought chains for
               science question answering",
  author    = "Lu, P and Mishra, S and Xia, T and Qiu, L and {others}",
  journal   = "Advances in",
  publisher = "proceedings.neurips.cc",
  abstract  = "When answering a question, humans utilize the information
               available across different modalities to synthesize a consistent
               and complete chain of thought (CoT). This process is …",
  year      =  2022
}

@ARTICLE{Tessler2019-bw,
  title     = "Learning from generic language",
  author    = "Tessler, Michael Henry and Goodman, Noah D",
  publisher = "PsyArXiv",
  abstract  = "… Much of what we learn about the world comes not from our own
               experience but by learning from others, often via language .
               Language is a powerful medium through which …",
  year      =  2019
}

@ARTICLE{Awasthi2020-is,
  title         = "Learning from Rules Generalizing Labeled Exemplars",
  author        = "Awasthi, Abhijeet and Ghosh, Sabyasachi and Goyal, Rasna and
                   Sarawagi, Sunita",
  journal       = "arXiv [cs.LG]",
  abstract      = "In many applications labeled data is not readily available,
                   and needs to be collected via pain-staking human supervision.
                   We propose a rule-exemplar method for collecting human
                   supervision to combine the efficiency of rules with the
                   quality of instance labels. The supervision is coupled such
                   that it is both natural for humans and synergistic for
                   learning. We propose a training algorithm that jointly
                   denoises rules via latent coverage variables, and trains the
                   model through a soft implication loss over the coverage and
                   label variables. The denoised rules and trained model are
                   used jointly for inference. Empirical evaluation on five
                   different tasks shows that (1) our algorithm is more accurate
                   than several existing methods of learning from a mix of clean
                   and noisy supervision, and (2) the coupled rule-exemplar
                   supervision is effective in denoising rules.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Huang2022-wj,
  title     = "Learning to Adapt Domain Shifts of Moral Values via Instance
               Weighting",
  author    = "Huang, Xiaolei and Wormley, Alexandra and Cohen, Adam",
  booktitle = "Proceedings of the 33rd ACM Conference on Hypertext and Social
               Media",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "121--131",
  abstract  = "Classifying moral values in user-generated text from social media
               is critical in understanding community cultures and interpreting
               user behaviors of social movements. Moral values and language
               usage can change across the social movements; however, text
               classifiers are usually trained in source domains of existing
               social movements and tested in target domains of new social
               issues without considering the variations. In this study, we
               examine domain shifts of moral values and language usage,
               quantify the effects of domain shifts on the morality
               classification task, and propose a neural adaptation framework
               via instance weighting to improve cross-domain classification
               tasks. The quantification analysis suggests a strong correlation
               between morality shifts, language usage, and classification
               performance. We evaluate the neural adaptation framework on a
               public Twitter data across 7 social movements and gain
               classification improvements up to 12.1\%. Finally, we release a
               new data of the COVID-19 vaccine labeled with moral values and
               evaluate our approach on the new target domain. For the case
               study of the COVID-19 vaccine, our adaptation framework achieves
               up to 5.26\% improvements over neural baselines. This is the
               first study to quantify impacts of moral shifts, propose adaptive
               framework to model the shifts, and conduct a case study to model
               COVID-19 vaccine-related behaviors from moral values.",
  series    = "HT '22",
  month     =  jun,
  year      =  2022,
  keywords  = "domain variation, moral values, adaptation, morality,
               classification, instant weighting"
}

@ARTICLE{Bernsohn2024-un,
  title         = "{LegalLens}: Leveraging {LLMs} for Legal Violation
                   Identification in Unstructured Text",
  author        = "Bernsohn, Dor and Semo, Gil and Vazana, Yaron and Hayat, Gila
                   and Hagag, Ben and Niklaus, Joel and Saha, Rohit and
                   Truskovskyi, Kyryl",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this study, we focus on two main tasks, the first for
                   detecting legal violations within unstructured textual data,
                   and the second for associating these violations with
                   potentially affected individuals. We constructed two datasets
                   using Large Language Models (LLMs) which were subsequently
                   validated by domain expert annotators. Both tasks were
                   designed specifically for the context of class-action cases.
                   The experimental design incorporated fine-tuning models from
                   the BERT family and open-source LLMs, and conducting few-shot
                   experiments using closed-source LLMs. Our results, with an
                   F1-score of 62.69\% (violation identification) and 81.02\%
                   (associating victims), show that our datasets and setups can
                   be used for both tasks. Finally, we publicly release the
                   datasets and the code used for the experiments in order to
                   advance further research in the area of legal natural
                   language processing (NLP).",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Vazquez-Herrero2022-ao,
  title     = "Let’s dance the news! How the news media are adapting to the
               logic of {TikTok}",
  author    = "Vázquez-Herrero, Jorge and Negreira-Rey, María-Cruz and
               López-García, Xosé",
  journal   = "Journalism",
  publisher = "SAGE Publications",
  volume    =  23,
  number    =  8,
  pages     = "1717--1735",
  abstract  = "The influence of TikTok has reached the news media, which has
               adapted to the logic of the platform, in a context marked by the
               incidental consumption of news, virality and the intermediation
               of technology in access to information. The popularity of this
               social network invites news outlets to address a young audience
               on a platform characterized by visual and short content and
               dynamics defined by algorithmic recommendations, trending
               hashtags and challenges. Based on an exploratory search of news
               media and programmes on TikTok from around the world, we selected
               234 accounts and conducted a content analysis of the 19 news
               media and programmes identified with a verified profile and
               general thematic scope. The results point to a progressive
               incorporation of the media since 2019, with the purpose of
               informing, positioning their brand and adapting to the logic of
               TikTok in a new approach to journalism for younger generations.",
  month     =  aug,
  year      =  2022
}

@ARTICLE{Ma2023-nf,
  title         = "Let's Do a Thought Experiment: Using Counterfactuals to
                   Improve Moral Reasoning",
  author        = "Ma, Xiao and Mishra, Swaroop and Beirami, Ahmad and Beutel,
                   Alex and Chen, Jilin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models still struggle on moral reasoning, despite
                   their impressive performance in many other tasks. In
                   particular, the Moral Scenarios task in MMLU (Multi-task
                   Language Understanding) is among the worst performing tasks
                   for many language models, including GPT-3. In this work, we
                   propose a new prompting framework, Thought Experiments, to
                   teach language models to do better moral reasoning using
                   counterfactuals. Experiment results show that our framework
                   elicits counterfactual questions and answers from the model,
                   which in turn helps improve the accuracy on Moral Scenarios
                   task by 9-16\% compared to other zero-shot baselines.
                   Interestingly, unlike math reasoning tasks, zero-shot
                   Chain-of-Thought (CoT) reasoning doesn't work out of the box,
                   and even reduces accuracy by around 4\% compared to direct
                   zero-shot. We further observed that with minimal human
                   supervision in the form of 5 few-shot examples, the accuracy
                   of the task can be improved to as much as 80\%.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Ahia2023-pd,
  title     = "{LEXPLAIN}: Improving Model Explanations via Lexicon Supervision",
  author    = "Ahia, Orevaoghene and Gonen, Hila and Balachandran, Vidhisha and
               Tsvetkov, Yulia and Smith, Noah A",
  booktitle = "Proceedings of the 12th Joint Conference on Lexical and
               Computational Semantics (*SEM 2023)",
  publisher = "Association for Computational Linguistics",
  address   = "Toronto, Canada",
  pages     = "207--216",
  abstract  = "Model explanations that shed light on the model's predictions are
               becoming a desired additional output of NLP models, alongside
               their predictions. Challenges in creating these explanations
               include making them trustworthy and faithful to the model's
               predictions. In this work, we propose a novel framework for
               guiding model explanations by supervising them explicitly. To
               this end, our method, LEXplain, uses task-related lexicons to
               directly supervise model explanations. This approach consistently
               improves the model's explanations without sacrificing performance
               on the task, as we demonstrate on sentiment analysis and toxicity
               detection. Our analyses show that our method also demotes
               spurious correlations (i.e., with respect to African American
               English dialect) when performing the task, improving fairness.",
  month     =  jul,
  year      =  2023
}

@ARTICLE{Xu_undated-ow,
  title  = "Like-minded, like-bodied: How users (18-26) trust online eating and
            health information",
  author = "Xu, Rachel"
}

@ARTICLE{Zhou2023-af,
  title         = "{LIMA}: Less is more for alignment",
  author        = "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srini
                   and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia
                   and Yu, Ping and Yu, Lili and Zhang, Susan and Ghosh, Gargi
                   and Lewis, Mike and Zettlemoyer, Luke and Levy, Omer",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models are trained in two stages: (1)
                   unsupervised pretraining from raw text, to learn
                   general-purpose representations, and (2) large scale
                   instruction tuning and reinforcement learning, to better
                   align to end tasks and user preferences. We measure the
                   relative importance of these two stages by training LIMA, a
                   65B parameter LLaMa language model fine-tuned with the
                   standard supervised loss on only 1,000 carefully curated
                   prompts and responses, without any reinforcement learning or
                   human preference modeling. LIMA demonstrates remarkably
                   strong performance, learning to follow specific response
                   formats from only a handful of examples in the training data,
                   including complex queries that range from planning trip
                   itineraries to speculating about alternate history. Moreover,
                   the model tends to generalize well to unseen tasks that did
                   not appear in the training data. In a controlled human study,
                   responses from LIMA are either equivalent or strictly
                   preferred to GPT-4 in 43\% of cases; this statistic is as
                   high as 58\% when compared to Bard and 65\% versus
                   DaVinci003, which was trained with human feedback. Taken
                   together, these results strongly suggest that almost all
                   knowledge in large language models is learned during
                   pretraining, and only limited instruction tuning data is
                   necessary to teach models to produce high quality output.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hornborg2024-lu,
  title    = "Limiting money: redesigning the artifact that shapes modern people",
  author   = "Hornborg, Alf",
  journal  = "Sustainability Sci.",
  abstract = "The transdisciplinary argument in this article is that the social
              and ecological unsustainability of modern, globalized capitalism
              ultimately derives from the design of its central artifact: what
              Polanyi called all- or general-purpose money. The notion of a
              singular measure of economic value is a peculiar cultural
              conception that is inherently at odds with physical reality, yet
              it pervades modern economic thought and practice as if it were
              immutable. To transcend the political impasse of economic
              globalization, a complementary national currency (CC) exclusively
              for local use could distinguish a sphere of exchange and
              special-purpose currency for basic needs from a global sphere of
              more remote exchange-values. To avoid the pitfalls and failures of
              earlier experiments with local currencies, such a CC would require
              the support of national authorities, the specified objective of
              sustainable consumption and production, and systematic efforts to
              provide citizens and entrepreneurs with ample incentives to
              utilize it. In combining the concept of a CC with that of a
              universal basic income (UBI), the reform would allow their
              advantages to complement each other, joining the generalized scale
              of UBI with the potential of politically influencing consumption
              patterns that is inherent in CC. An essential difference in
              relation to earlier experiments would be that the reach of the CC
              would not be defined in terms of the geographical location of
              retailers but in terms of the derivation, relative to the
              consumer, of the products and services into which it could be
              converted. Although no such system yet exists, this should not
              stop us from imagining its possibilities.",
  month    =  apr,
  year     =  2024
}

@ARTICLE{Aiying2022-ir,
  title     = "Linguadidactic interpretation of John Du Bois’ stance triangle",
  author    = "Aiying, Gu and {Federal State Budgetary Educational Institution
               of Higher Education «Saint-Petersburg State University»}",
  journal   = "Vestnik of Saint Petersburg State University of Culture",
  publisher = "St. Petersburg State University of Culture",
  number    = "2 (51)",
  pages     = "122--128",
  abstract  = "The relevance of this research is explained by the importance of
               developing a methodology for teaching foreign postgraduate
               students to express an authorial stance in a scientific text,
               which would contribute to the formation of the skills necessary
               for writing a Ph.D. dissertation. It has been suggested that the
               basis of the proposed methodology could be the «stance triangle»
               model of John W. Du Bois in which the subject of creative
               activity (an author) interacts with other subjects and
               referential object toward which the stance is being directed. The
               purpose of this article is a linguodidactic interpretation of the
               Du Bois’ stance triangle. The main research method is modeling
               the process of teaching foreign postgraduate students how to
               express an authorial stance based on the Du Bois’ concept. The
               results of the study are as follows: 1. Du Bois’ stance triangle
               is adapted to the conditions of teaching the Russian language to
               foreign postgraduate students. 2. The peculiarities of the
               subject as a reader and as an author are revealed. 3. Three
               components are established in the explanation of the authorial
               stance in a scientific text whereby a subject takes a stance by
               expressing an attitude, positioning himself, and aligning with
               other subjects. 4. The linguodidactic advantages of using the
               adapted triangle as a learning tool are established. 5. A model
               for teaching foreign postgraduate students-linguists to express
               the authorial stance in a scientific text has been built. It
               includes three parts and is aimed at forming the concept of the
               authorial stance and knowledge of the means of its expression in
               a scientific text, the skills of understanding and writing a
               linguistic text with an expressed authorial stance.",
  year      =  2022
}

@INCOLLECTION{Maass1999-vt,
  title     = "Linguistic Intergroup Bias: Stereotype Perpetuation Through
               Language",
  author    = "Maass, Anne",
  editor    = "Zanna, Mark P",
  booktitle = "Advances in Experimental Social Psychology",
  publisher = "Academic Press",
  volume    =  31,
  pages     = "79--121",
  abstract  = "Publisher Summary Language is considered as the major means by
               which stereotypes are communicated through interpersonal
               discourse, by which they are transmitted from generation to
               generation, and by which the press and other mass media create
               social representations of social groups. Language also
               constitutes the principal means by which sociologists and
               social-psychologists tend to measure stereotypes. Language
               abstraction plays a subtle but important role in stereotype
               transmission and maintenance. The aim of this chapter is to
               describe language abstraction and its development over the past
               years. The chapter discusses its implications and proposes
               extensions of the model to related areas. The linguistic
               intergroup bias (LIB) model describes a systematic bias in
               language use, which can contribute to the perpetuation of
               stereotypes. The chapter describes the research paradigm most
               frequently employed in LIB studies, reviews empirical findings
               testing the main predictions, addresses questions of external
               validity, compares hypotheses about the underlying mechanisms of
               the LIB, speculate about extensions of the model beyond
               intergroup relations, and finally discusses a number of open
               problems that are of interest.",
  month     =  jan,
  year      =  1999
}

@MISC{noauthor_undated-xx,
  title = "{LippiGreen2012Ch11}.pdf"
}

@ARTICLE{Gonen2019-lu,
  title         = "Lipstick on a Pig: Debiasing Methods Cover up Systematic
                   Gender Biases in Word Embeddings But do not Remove Them",
  author        = "Gonen, Hila and Goldberg, Yoav",
  journal       = "arXiv [cs.CL]",
  abstract      = "Word embeddings are widely used in NLP for a vast range of
                   tasks. It was shown that word embeddings derived from text
                   corpora reflect gender biases in society. This phenomenon is
                   pervasive and consistent across different word embedding
                   models, causing serious concern. Several recent works tackle
                   this problem, and propose methods for significantly reducing
                   this gender bias in word embeddings, demonstrating convincing
                   results. However, we argue that this removal is superficial.
                   While the bias is indeed substantially reduced according to
                   the provided bias definition, the actual effect is mostly
                   hiding the bias, not removing it. The gender bias information
                   is still reflected in the distances between
                   ``gender-neutralized'' words in the debiased embeddings, and
                   can be recovered from them. We present a series of
                   experiments to support this claim, for two debiasing methods.
                   We conclude that existing bias removal techniques are
                   insufficient, and should not be trusted for providing
                   gender-neutral modeling.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Semel2022-oi,
  title     = "Listening Like a Computer: Attentional Tensions and Mechanized
               Care in Psychiatric Digital Phenotyping",
  author    = "Semel, Beth M",
  journal   = "Sci. Technol. Human Values",
  publisher = "SAGE Publications Inc",
  volume    =  47,
  number    =  2,
  pages     = "266--290",
  abstract  = "This article explores negotiations over the humanistic versus
               mechanized components of care through an ethnographic account of
               digital phenotyping research. I focus on a US-based team of
               psychiatric and engineering professionals assembling a smartphone
               application that they hope will analyze minute changes in the
               sounds of speech during phone calls to predict when a user with
               bipolar disorder will have a manic or depressive episode.
               Contrary to conventional depictions of psychiatry as essentially
               humanistic, the discourse surrounding digital phenotyping
               positions the machine as a necessary addition to mental health
               care precisely because of its more-than-human sensory,
               attentional capacities. The bipolar research team likewise
               portrays their app as capable of pinpointing sonic signs of
               mental illness that humans, too distracted by semantic meaning,
               otherwise ignore. Nevertheless, the team members tasked with
               processing the team?s data (audio recordings of human research
               subject speech) must craft and perform a selectively attentive
               machinic subject position, which they call ?listening like a
               computer?: a paradoxical mode of attention (to speech sound) and
               inattention (to speech meaning). By tracing the team?s discursive
               and on-the-ground enactments of care and attention as both
               humanistic and machinic, I tune a critical ear to the posthuman
               promises of digital phenotyping.",
  month     =  mar,
  year      =  2022
}

@ARTICLE{Ardanuy2020-el,
  title         = "Living Machines: A study of atypical animacy",
  author        = "Ardanuy, Mariona Coll and Nanni, Federico and Beelen, Kaspar
                   and Hosseini, Kasra and Ahnert, Ruth and Lawrence, Jon and
                   McDonough, Katherine and Tolfo, Giorgia and Wilson, Daniel C
                   S and McGillivray, Barbara",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper proposes a new approach to animacy detection, the
                   task of determining whether an entity is represented as
                   animate in a text. In particular, this work is focused on
                   atypical animacy and examines the scenario in which typically
                   inanimate objects, specifically machines, are given animate
                   attributes. To address it, we have created the first dataset
                   for atypical animacy detection, based on nineteenth-century
                   sentences in English, with machines represented as either
                   animate or inanimate. Our method builds on recent innovations
                   in language modeling, specifically BERT contextualized word
                   embeddings, to better capture fine-grained contextual
                   properties of words. We present a fully unsupervised
                   pipeline, which can be easily adapted to different contexts,
                   and report its performance on an established animacy dataset
                   and our newly introduced resource. We show that our method
                   provides a substantially more accurate characterization of
                   atypical animacy, especially when applied to highly complex
                   forms of language use.",
  month         =  may,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Araujo2018-ij,
  title    = "Living up to the chatbot hype: The influence of anthropomorphic
              design cues and communicative agency framing on conversational
              agent and company perceptions",
  author   = "Araujo, Theo",
  journal  = "Comput. Human Behav.",
  volume   =  85,
  pages    = "183--189",
  abstract = "Disembodied conversational agents in the form of chatbots are
              increasingly becoming a reality on social media and messaging
              applications, and are a particularly pressing topic for service
              encounters with companies. Adopting an experimental design with
              actual chatbots powered with current technology, this study
              explores the extent to which human-like cues such as language
              style and name, and the framing used to introduce the chatbot to
              the consumer can influence perceptions about social presence as
              well as mindful and mindless anthropomorphism. Moreover, this
              study investigates the relevance of anthropomorphism and social
              presence to important company-related outcomes, such as attitudes,
              satisfaction and the emotional connection that consumers feel with
              the company after interacting with the chatbot.",
  month    =  aug,
  year     =  2018,
  keywords = "Disembodied conversational agents; Chatbots; Service encounters;
              Social presence; Anthropomorphism"
}

@ARTICLE{Touvron_undated-fp,
  title  = "Llama 2: Open Foundation and Fine-Tuned Chat Models",
  author = "Touvron, Hugo and Martin, Louis and Stone, Kevin"
}

@ARTICLE{Zheng2023-yg,
  title         = "{LMSYS}-Chat-{1M}: A Large-Scale Real-World {LLM}
                   Conversation Dataset",
  author        = "Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Li,
                   Tianle and Zhuang, Siyuan and Wu, Zhanghao and Zhuang,
                   Yonghao and Li, Zhuohan and Lin, Zi and Xing, Eric P and
                   Gonzalez, Joseph E and Stoica, Ion and Zhang, Hao",
  journal       = "arXiv [cs.CL]",
  abstract      = "Studying how people interact with large language models
                   (LLMs) in real-world scenarios is increasingly important due
                   to their widespread use in various applications. In this
                   paper, we introduce LMSYS-Chat-1M, a large-scale dataset
                   containing one million real-world conversations with 25
                   state-of-the-art LLMs. This dataset is collected from 210K
                   unique IP addresses in the wild on our Vicuna demo and
                   Chatbot Arena website. We offer an overview of the dataset's
                   content, including its curation process, basic statistics,
                   and topic distribution, highlighting its diversity,
                   originality, and scale. We demonstrate its versatility
                   through four use cases: developing content moderation models
                   that perform similarly to GPT-4, building a safety benchmark,
                   training instruction-following models that perform similarly
                   to Vicuna, and creating challenging benchmark questions. We
                   believe that this dataset will serve as a valuable resource
                   for understanding and advancing LLM capabilities. The dataset
                   is publicly available at
                   https://huggingface.co/datasets/lmsys/lmsys-chat-1m.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-zn,
  title = "{Lo2016}.pdf"
}

@ARTICLE{Meng2022-sn,
  title         = "Locating and editing factual associations in {GPT}",
  author        = "Meng, Kevin and Bau, David and Andonian, Alex and Belinkov,
                   Yonatan",
  editor        = "Koyejo, S and Mohamed, S and Agarwal, A and Belgrave, D and
                   Cho, K and Oh, A",
  journal       = "arXiv [cs.CL]",
  pages         = "17359--17372",
  abstract      = "We analyze the storage and recall of factual associations in
                   autoregressive transformer language models, finding evidence
                   that these associations correspond to localized,
                   directly-editable computations. We first develop a causal
                   intervention for identifying neuron activations that are
                   decisive in a model's factual predictions. This reveals a
                   distinct set of steps in middle-layer feed-forward modules
                   that mediate factual predictions while processing subject
                   tokens. To test our hypothesis that these computations
                   correspond to factual association recall, we modify
                   feed-forward weights to update specific factual associations
                   using Rank-One Model Editing (ROME). We find that ROME is
                   effective on a standard zero-shot relation extraction (zsRE)
                   model-editing task, comparable to existing methods. To
                   perform a more sensitive evaluation, we also evaluate ROME on
                   a new dataset of counterfactual assertions, on which it
                   simultaneously maintains both specificity and generalization,
                   whereas other methods sacrifice one or another. Our results
                   confirm an important role for mid-layer feed-forward modules
                   in storing factual associations and suggest that direct
                   manipulation of computational mechanisms may be a feasible
                   approach for model editing. The code, dataset,
                   visualizations, and an interactive demo notebook are
                   available at https://rome.baulab.info/",
  month         =  feb,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Fast2017-mt,
  title    = "Long-Term Trends in the Public Perception of Artificial
              Intelligence",
  author   = "Fast, Ethan and Horvitz, Eric",
  journal  = "AAAI",
  volume   =  31,
  number   =  1,
  abstract = "Analyses of text corpora over time can reveal trends in beliefs,
              interest, and sentiment about a topic. We focus on views expressed
              about artificial intelligence (AI) in the New York Times over a
              30-year period. General interest, awareness, and discussion about
              AI has waxed and waned since the field was founded in 1956. We
              present a set of measures that captures levels of engagement,
              measures of pessimism and optimism, the prevalence of specific
              hopes and concerns, and topics that are linked to discussions
              about AI over decades. We find that discussion of AI has increased
              sharply since 2009, and that these discussions have been
              consistently more optimistic than pessimistic. However, when we
              examine specific concerns, we find that worries of loss of control
              of AI, ethical concerns for AI, and the negative impact of AI on
              work have grown in recent years. We also find that hopes for AI in
              healthcare and education have increased over time.",
  month    =  feb,
  year     =  2017,
  keywords = "artificial intelligence; news analysis; crowdsourcing",
  language = "en"
}

@ARTICLE{Xu2022-sl,
  title         = "Long Time No See! Open-Domain Conversation with Long-Term
                   Persona Memory",
  author        = "Xu, Xinchao and Gou, Zhibin and Wu, Wenquan and Niu, Zheng-Yu
                   and Wu, Hua and Wang, Haifeng and Wang, Shihang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Most of the open-domain dialogue models tend to perform
                   poorly in the setting of long-term human-bot conversations.
                   The possible reason is that they lack the capability of
                   understanding and memorizing long-term dialogue history
                   information. To address this issue, we present a novel task
                   of Long-term Memory Conversation (LeMon) and then build a new
                   dialogue dataset DuLeMon and a dialogue generation framework
                   with Long-Term Memory (LTM) mechanism (called PLATO-LTM).
                   This LTM mechanism enables our system to accurately extract
                   and continuously update long-term persona memory without
                   requiring multiple-session dialogue datasets for model
                   training. To our knowledge, this is the first attempt to
                   conduct real-time dynamic management of persona information
                   of both parties, including the user and the bot. Results on
                   DuLeMon indicate that PLATO-LTM can significantly outperform
                   baselines in terms of long-term dialogue consistency, leading
                   to better dialogue engagingness.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Rosa2019-tu,
  title     = "Looking like a language, sounding like a race",
  author    = "Rosa, J",
  publisher = "books.google.com",
  abstract  = "Looking like a Language, Sounding like a Race examines the
               emergence of linguistic and ethnoracial categories in the context
               of Latinidad. The book draws from more than twenty …",
  year      =  2019
}

@ARTICLE{He2022-de,
  title         = "{MABEL}: Attenuating Gender Bias using Textual Entailment
                   Data",
  author        = "He, Jacqueline and Xia, Mengzhou and Fellbaum, Christiane and
                   Chen, Danqi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-trained language models encode undesirable social biases,
                   which are further exacerbated in downstream use. To this end,
                   we propose MABEL (a Method for Attenuating Gender Bias using
                   Entailment Labels), an intermediate pre-training approach for
                   mitigating gender bias in contextualized representations. Key
                   to our approach is the use of a contrastive learning
                   objective on counterfactually augmented, gender-balanced
                   entailment pairs from natural language inference (NLI)
                   datasets. We also introduce an alignment regularizer that
                   pulls identical entailment pairs along opposite gender
                   directions closer. We extensively evaluate our approach on
                   intrinsic and extrinsic metrics, and show that MABEL
                   outperforms previous task-agnostic debiasing approaches in
                   terms of fairness. It also preserves task performance after
                   fine-tuning on downstream tasks. Together, these findings
                   demonstrate the suitability of NLI data as an effective means
                   of bias mitigation, as opposed to only using unlabeled
                   sentences in the literature. Finally, we identify that
                   existing approaches often use evaluation settings that are
                   insufficient or inconsistent. We make an effort to reproduce
                   and compare previous methods, and call for unifying the
                   evaluation settings across gender debiasing methods for
                   better future comparison.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Brinkmann2023-mt,
  title    = "Machine culture",
  author   = "Brinkmann, Levin and Baumann, Fabian and Bonnefon, Jean-François
              and Derex, Maxime and Müller, Thomas F and Nussberger, Anne-Marie
              and Czaplicka, Agnieszka and Acerbi, Alberto and Griffiths, Thomas
              L and Henrich, Joseph and Leibo, Joel Z and McElreath, Richard and
              Oudeyer, Pierre-Yves and Stray, Jonathan and Rahwan, Iyad",
  journal  = "Nat Hum Behav",
  volume   =  7,
  number   =  11,
  pages    = "1855--1868",
  abstract = "The ability of humans to create and disseminate culture is often
              credited as the single most important factor of our success as a
              species. In this Perspective, we explore the notion of 'machine
              culture', culture mediated or generated by machines. We argue that
              intelligent machines simultaneously transform the cultural
              evolutionary processes of variation, transmission and selection.
              Recommender algorithms are altering social learning dynamics.
              Chatbots are forming a new mode of cultural transmission, serving
              as cultural models. Furthermore, intelligent machines are evolving
              as contributors in generating cultural traits-from game strategies
              and visual art to scientific results. We provide a conceptual
              framework for studying the present and anticipated future impact
              of machines on cultural evolution, and present a research agenda
              for the study of machine culture.",
  month    =  nov,
  year     =  2023,
  language = "en"
}

@INPROCEEDINGS{Sundar2019-xj,
  title     = "Machine Heuristic: When We Trust Computers More than Humans with
               Our Personal Information",
  author    = "Sundar, S Shyam and Kim, Jinyoung",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Paper 538",
  pages     = "1--9",
  abstract  = "In this day and age of identity theft, are we likely to trust
               machines more than humans for handling our personal information?
               We answer this question by invoking the concept of ``machine
               heuristic,'' which is a rule of thumb that machines are more
               secure and trustworthy than humans. In an experiment (N = 160)
               that involved making airline reservations, users were more likely
               to reveal their credit card information to a machine agent than a
               human agent. We demonstrate that cues on the interface trigger
               the machine heuristic by showing that those with higher cognitive
               accessibility of the heuristic (i.e., stronger prior belief in
               the rule of thumb) were more likely than those with lower
               accessibility to disclose to a machine, but they did not differ
               in their disclosure to a human. These findings have implications
               for design of interface cues conveying machine vs. human sources
               of our online interactions.",
  series    = "CHI '19",
  month     =  may,
  year      =  2019,
  keywords  = "automation bias, cognitive heuristics, machine heuristic, main
               model, secure and trustworthy computing, virtual agent"
}

@ARTICLE{Lehman2023-bf,
  title         = "Machine Love",
  author        = "Lehman, Joel",
  journal       = "arXiv [cs.AI]",
  abstract      = "While ML generates much economic value, many of us have
                   problematic relationships with social media and other
                   ML-powered applications. One reason is that ML often
                   optimizes for what we want in the moment, which is easy to
                   quantify but at odds with what is known scientifically about
                   human flourishing. Thus, through its impoverished models of
                   us, ML currently falls far short of its exciting potential,
                   which is for it to help us to reach ours. While there is no
                   consensus on defining human flourishing, from diverse
                   perspectives across psychology, philosophy, and spiritual
                   traditions, love is understood to be one of its primary
                   catalysts. Motivated by this view, this paper explores
                   whether there is a useful conception of love fitting for
                   machines to embody, as historically it has been generative to
                   explore whether a nebulous concept, such as life or
                   intelligence, can be thoughtfully abstracted and reimagined,
                   as in the fields of machine intelligence or artificial life.
                   This paper forwards a candidate conception of machine love,
                   inspired in particular by work in positive psychology and
                   psychotherapy: to provide unconditional support enabling
                   humans to autonomously pursue their own growth and
                   development. Through proof of concept experiments, this paper
                   aims to highlight the need for richer models of human
                   flourishing in ML, provide an example framework through which
                   positive psychology can be combined with ML to realize a
                   rough conception of machine love, and demonstrate that
                   current language models begin to enable embodying qualitative
                   humanistic principles. The conclusion is that though at
                   present ML may often serve to addict, distract, or divide us,
                   an alternative path may be opening up: We may align ML to
                   support our growth, through it helping us to align ourselves
                   towards our highest aspirations.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Hanley2023-zk,
  title         = "Machine-Made Media: Monitoring the Mobilization of
                   Machine-Generated Articles on Misinformation and Mainstream
                   News Websites",
  author        = "Hanley, Hans W A and Durumeric, Zakir",
  journal       = "arXiv [cs.CY]",
  abstract      = "With the increasing popularity of generative large language
                   models (LLMs) like ChatGPT, an increasing number of news
                   websites have begun utilizing them to generate articles.
                   However, not only can these language models produce factually
                   inaccurate articles on reputable websites but disreputable
                   news sites can utilize these LLMs to mass produce
                   misinformation. To begin to understand this phenomenon, we
                   present one of the first large-scale studies of the
                   prevalence of synthetic articles within online news media. To
                   do this, we train a DeBERTa-based synthetic news detector and
                   classify over 12.91 million articles from 3,074
                   misinformation and mainstream news websites. We find that
                   between January 1, 2022 and April 1, 2023, the relative
                   number of synthetic news articles increased by 79.4\% on
                   mainstream websites while increasing by 342\% on
                   misinformation sites. Analyzing the impact of the release of
                   ChatGPT using an interrupted-time-series, we show that while
                   its release resulted in a marked increase in synthetic
                   articles on small sites as well as misinformation news
                   websites, there was not a corresponding increase on large
                   mainstream news websites. Finally, using data from the social
                   media platform Reddit, we find that social media users
                   interacted more with synthetic articles in March 2023
                   relative to January 2022.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Choudhury2022-ey,
  title         = "Machine Reading, Fast and Slow: When Do Models ``Understand''
                   Language?",
  author        = "Choudhury, Sagnik Ray and Rogers, Anna and Augenstein,
                   Isabelle",
  journal       = "arXiv [cs.CL]",
  abstract      = "Two of the most fundamental challenges in Natural Language
                   Understanding (NLU) at present are: (a) how to establish
                   whether deep learning-based models score highly on NLU
                   benchmarks for the 'right' reasons; and (b) to understand
                   what those reasons would even be. We investigate the behavior
                   of reading comprehension models with respect to two
                   linguistic 'skills': coreference resolution and comparison.
                   We propose a definition for the reasoning steps expected from
                   a system that would be 'reading slowly', and compare that
                   with the behavior of five models of the BERT family of
                   various sizes, observed through saliency scores and
                   counterfactual explanations. We find that for comparison (but
                   not coreference) the systems based on larger encoders are
                   more likely to rely on the 'right' information, but even they
                   struggle with generalization, suggesting that they still
                   learn specific lexical patterns rather than the general
                   principles of comparison.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Li2021-xa,
  title     = "Machinelike or humanlike? A literature review of anthropomorphism
               in {AI}-enabled technology",
  author    = "Li, Mengjun and Suh, Ayoung",
  booktitle = "54th Hawaii International Conference on System Sciences (HICSS
               2021)",
  publisher = "scholars.cityu.edu.hk",
  pages     = "4053--4062",
  abstract  = "… on anthropomorphism in AIET contexts. To conduct an in-depth
               analysis of the literature on anthropomorphism , … on
               conceptualizing and operationalizing AIET anthropomorphism , and
               its …",
  year      =  2021
}

@INPROCEEDINGS{Li2021-ws,
  title     = "Machinelike or Humanlike? A Literature Review of Anthropomorphism
               in {AI}-Enabled Technology",
  author    = "Li, Mengjun and Suh, Ayoung",
  booktitle = "Hawaii International Conference on System Sciences",
  publisher = "unknown",
  abstract  = "PDF | On Jan 1, 2021, Mengjun Li and others published Machinelike
               or Humanlike? A Literature Review of Anthropomorphism in
               AI-Enabled Technology | Find, read and cite all the research you
               need on ResearchGate",
  month     =  jan,
  year      =  2021
}

@ARTICLE{Paul2024-jx,
  title         = "Making Reasoning Matter: Measuring and Improving Faithfulness
                   of Chain-of-Thought Reasoning",
  author        = "Paul, Debjit and West, Robert and Bosselut, Antoine and
                   Faltings, Boi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs) have been shown to perform
                   better when asked to reason step-by-step before answering a
                   question. However, it is unclear to what degree the model's
                   final answer is faithful to the stated reasoning steps. In
                   this paper, we perform a causal mediation analysis on twelve
                   LLMs to examine how intermediate reasoning steps generated by
                   the LLM influence the final outcome and find that LLMs do not
                   reliably use their intermediate reasoning steps when
                   generating an answer. To address this issue, we introduce
                   FRODO, a framework to tailor small-sized LMs to generate
                   correct reasoning steps and robustly reason over these steps.
                   FRODO consists of an inference module that learns to generate
                   correct reasoning steps using an implicit causal reward
                   function and a reasoning module that learns to faithfully
                   reason over these intermediate inferences using a
                   counterfactual and causal preference objective. Our
                   experiments show that FRODO significantly outperforms four
                   competitive baselines. Furthermore, FRODO improves the
                   robustness and generalization ability of the reasoning LM,
                   yielding higher performance on out-of-distribution test sets.
                   Finally, we find that FRODO's rationales are more faithful to
                   its final answer predictions than standard supervised
                   fine-tuning.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Gu2023-lf,
  title         = "Mamba: Linear-Time Sequence Modeling with Selective State
                   Spaces",
  author        = "Gu, Albert and Dao, Tri",
  journal       = "arXiv [cs.LG]",
  abstract      = "Foundation models, now powering most of the exciting
                   applications in deep learning, are almost universally based
                   on the Transformer architecture and its core attention
                   module. Many subquadratic-time architectures such as linear
                   attention, gated convolution and recurrent models, and
                   structured state space models (SSMs) have been developed to
                   address Transformers' computational inefficiency on long
                   sequences, but they have not performed as well as attention
                   on important modalities such as language. We identify that a
                   key weakness of such models is their inability to perform
                   content-based reasoning, and make several improvements.
                   First, simply letting the SSM parameters be functions of the
                   input addresses their weakness with discrete modalities,
                   allowing the model to selectively propagate or forget
                   information along the sequence length dimension depending on
                   the current token. Second, even though this change prevents
                   the use of efficient convolutions, we design a hardware-aware
                   parallel algorithm in recurrent mode. We integrate these
                   selective SSMs into a simplified end-to-end neural network
                   architecture without attention or even MLP blocks (Mamba).
                   Mamba enjoys fast inference (5$\times$ higher throughput than
                   Transformers) and linear scaling in sequence length, and its
                   performance improves on real data up to million-length
                   sequences. As a general sequence model backbone, Mamba
                   achieves state-of-the-art performance across several
                   modalities such as language, audio, and genomics. On language
                   modeling, our Mamba-3B model outperforms Transformers of the
                   same size and matches Transformers twice its size, both in
                   pretraining and downstream evaluation.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Grigoryev2019-od,
  title     = "Mapping Ethnic Stereotypes and Their Antecedents in Russia: The
               Stereotype Content Model",
  author    = "Grigoryev, Dmitry and Fiske, Susan T and Batkhina, Anastasia",
  journal   = "Front. Psychol.",
  publisher = "frontiersin.org",
  volume    =  10,
  pages     =  1643,
  abstract  = "The stereotype content model (SCM), originating in the United
               States and generalized across nearly 50 countries, has yet to
               address ethnic relations in one of the world's most influential
               nations. Russia and the United States are somewhat alike (large,
               powerful, immigrant-receiving), but differ in other ways relevant
               to intergroup images (culture, religions, ideology, and history).
               Russian ethnic stereotypes are understudied, but significant for
               theoretical breadth and practical politics. This research tested
               the SCM on ethnic stereotypes in a Russian sample (N = 1115).
               Study 1 (N = 438) produced an SCM map of the sixty most numerous
               domestic ethnic groups (both ethnic minorities and immigrants).
               Four clusters occupied the SCM warmth-by-competence space. Study
               2 (N = 677) compared approaches to ethnic stereotypes in terms of
               status and competition, cultural distance, perceived region, and
               four intergroup threats. Using the same Study 1 groups, the
               Russian SCM map showed correlated warmth and competence, with few
               ambivalent stereotypes. As the SCM predicts, status predicted
               competence, and competition negatively predicted warmth. Beyond
               the SCM, status and property threat both were robust antecedents
               for both competence and warmth for all groups. Besides
               competition, cultural distance also negatively predicted warmth
               for all groups. The role of the other antecedents, as expected,
               varied from group to group. To examine relative impact, a network
               analysis demonstrated that status, competition, and property
               threat centrally influence many other variables in the networks.
               The SCM, along with antecedents from other models, describes
               Russian ethnic-group images. This research contributes: (1) a
               comparison of established approaches to ethnic stereotypes (from
               acculturation and intergroup relations) showing the stability of
               the main SCM predictions; (2) network structures of the
               multivariate dependencies of the considered variables; (3)
               systematically cataloged images of ethnic groups in Russia for
               further comparisons, illuminating the Russian historical,
               societal, and interethnic context.",
  month     =  jul,
  year      =  2019,
  keywords  = "cultural distance; differentiated threat; ethnic stereotypes;
               intergroup threat; network analysis; stereotype content model",
  language  = "en"
}

@ARTICLE{Grieve2018-rd,
  title     = "Mapping Lexical Innovation on American Social Media",
  author    = "Grieve, Jack and Nini, Andrea and Guo, Diansheng",
  journal   = "Journal of English Linguistics",
  publisher = "SAGE Publications Inc",
  volume    =  46,
  number    =  4,
  pages     = "293--319",
  abstract  = "In this paper, we introduce a method for mapping lexical
               innovation, which we then use to track the origin and spread of
               new words on American Twitter, based on a multi-billion-word
               corpus of Tweets collected between 2013 and 2014. We first
               extract fifty-four emerging words from the corpus by searching
               for words that are very uncommon at the end of 2013 but whose use
               rises dramatically over the course of 2014. We then map the
               origin and spread of each of these words. Based on these results,
               we identify five main regional patterns of lexical innovation on
               American Twitter, primarily associated with the West Coast, the
               Northeast, the Mid-Atlantic, the Deep South, and the Gulf Coast.
               We conclude by proposing explanations for these results and by
               discussing their significance to theories of language variation
               and change, including both the actuation and diffusion of lexical
               innovations.",
  month     =  dec,
  year      =  2018
}

@MISC{Eisenstein2012-tn,
  title        = "Mapping the geographical diffusion of new words",
  author       = "Eisenstein, Jacob and O'connor, Brendan and Smith, Noah A and
                  Xing, Eric P",
  publisher    = "Citeseer",
  abstract     = "Abstract Language in social media is rich with linguistic
                  innovations, most strikingly in the new words and spellings
                  that constantly enter the lexicon. Despite assertions about
                  the …",
  year         =  2012,
  howpublished = "\url{https://citeseerx.ist.psu.edu/document?repid=rep1\&type=pdf\&doi=9a2f285489f20178f04aed16c2423690eb2a8396}",
  note         = "Accessed: 2023-7-21"
}

@ARTICLE{Cheng2023-sf,
  title         = "Marked Personas: Using natural language prompts to measure
                   stereotypes in language models",
  author        = "Cheng, Myra and Durmus, Esin and Jurafsky, Dan",
  journal       = "arXiv [cs.CL]",
  pages         = "1504–1532",
  abstract      = "To recognize and mitigate harms from large language models
                   (LLMs), we need to understand the prevalence and nuances of
                   stereotypes in LLM outputs. Toward this end, we present
                   Marked Personas, a prompt-based method to measure stereotypes
                   in LLMs for intersectional demographic groups without any
                   lexicon or data labeling. Grounded in the sociolinguistic
                   concept of markedness (which characterizes explicitly
                   linguistically marked categories versus unmarked defaults),
                   our proposed method is twofold: 1) prompting an LLM to
                   generate personas, i.e., natural language descriptions, of
                   the target demographic group alongside personas of unmarked,
                   default groups; 2) identifying the words that significantly
                   distinguish personas of the target group from corresponding
                   unmarked ones. We find that the portrayals generated by
                   GPT-3.5 and GPT-4 contain higher rates of racial stereotypes
                   than human-written portrayals using the same prompts. The
                   words distinguishing personas of marked (non-white, non-male)
                   groups reflect patterns of othering and exoticizing these
                   demographics. An intersectional lens further reveals tropes
                   that dominate portrayals of marginalized groups, such as
                   tropicalism and the hypersexualization of minoritized women.
                   These representational harms have concerning implications for
                   downstream applications like story generation.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Kucera1982-bi,
  title     = "Markedness and Frequency: A Computational Analysis",
  author    = "Kucera, Henry",
  booktitle = "{C}oling 1982: Proceedings of the {N}inth {I}nternational
               {C}onference on {C}omputational {L}inguistics",
  year      =  1982
}

@ARTICLE{Meng2022-ds,
  title         = "Mass-Editing Memory in a Transformer",
  author        = "Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and
                   Belinkov, Yonatan and Bau, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work has shown exciting promise in updating large
                   language models with new memories, so as to replace obsolete
                   information or add specialized knowledge. However, this line
                   of work is predominantly limited to updating single
                   associations. We develop MEMIT, a method for directly
                   updating a language model with many memories, demonstrating
                   experimentally that it can scale up to thousands of
                   associations for GPT-J (6B) and GPT-NeoX (20B), exceeding
                   prior work by orders of magnitude. Our code and data are at
                   https://memit.baulab.info.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Tong2023-en,
  title         = "Mass-Producing Failures of Multimodal Systems with Language
                   Models",
  author        = "Tong, Shengbang and Jones, Erik and Steinhardt, Jacob",
  journal       = "arXiv [cs.LG]",
  abstract      = "Deployed multimodal systems can fail in ways that evaluators
                   did not anticipate. In order to find these failures before
                   deployment, we introduce MultiMon, a system that
                   automatically identifies systematic failures --
                   generalizable, natural-language descriptions of patterns of
                   model failures. To uncover systematic failures, MultiMon
                   scrapes a corpus for examples of erroneous agreement: inputs
                   that produce the same output, but should not. It then prompts
                   a language model (e.g., GPT-4) to find systematic patterns of
                   failure and describe them in natural language. We use
                   MultiMon to find 14 systematic failures (e.g., ``ignores
                   quantifiers'') of the CLIP text-encoder, each comprising
                   hundreds of distinct inputs (e.g., ``a shelf with a few/many
                   books''). Because CLIP is the backbone for most
                   state-of-the-art multimodal systems, these inputs produce
                   failures in Midjourney 5.1, DALL-E, VideoFusion, and others.
                   MultiMon can also steer towards failures relevant to specific
                   use cases, such as self-driving cars. We see MultiMon as a
                   step towards evaluation that autonomously explores the long
                   tail of potential system failures. Code for MULTIMON is
                   available at https://github.com/tsb0601/MultiMon.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Pillutla2021-tx,
  title         = "{MAUVE}: Measuring the gap between neural text and human text
                   using divergence frontiers",
  author        = "Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan
                   and Thickstun, John and Welleck, Sean and Choi, Yejin and
                   Harchaoui, Zaid",
  journal       = "arXiv [cs.CL]",
  pages         = "4816--4828",
  abstract      = "As major progress is made in open-ended text generation,
                   measuring how close machine-generated text is to human
                   language remains a critical open problem. We introduce MAUVE,
                   a comparison measure for open-ended text generation, which
                   directly compares the learnt distribution from a text
                   generation model to the distribution of human-written text
                   using divergence frontiers. MAUVE scales up to modern text
                   generation models by computing information divergences in a
                   quantized embedding space. Through an extensive empirical
                   study on three open-ended generation tasks, we find that
                   MAUVE identifies known properties of generated text, scales
                   naturally with model size, and correlates with human
                   judgments, with fewer restrictions than existing
                   distributional evaluation metrics.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-yw,
  title = "mdagli\_Masters\_Design\_2018.pdf"
}

@ARTICLE{Jacobs2021-qr,
  title     = "Measurement and fairness",
  author    = "Jacobs, A Z and Wallach, H",
  journal   = "Proceedings of the 2021 ACM conference on",
  publisher = "dl.acm.org",
  abstract  = "We propose measurement modeling from the quantitative social
               sciences as a framework for understanding fairness in
               computational systems. Computational systems often involve …",
  year      =  2021
}

@ARTICLE{Johnson-Grey2020-of,
  title     = "Measuring Abstract Mind-Sets Through Syntax: Automating the
               Linguistic Category Model",
  author    = "Johnson-Grey, Kate M and Boghrati, Reihane and Wakslak, Cheryl J
               and Dehghani, Morteza",
  journal   = "Soc. Psychol. Personal. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  11,
  number    =  2,
  pages     = "217--225",
  abstract  = "Abstraction in language has critical implications for memory,
               judgment, and learning and can provide an important window into a
               person?s cognitive abstraction level. The linguistic category
               model (LCM) provides one well-validated, human-coded approach to
               quantifying linguistic abstraction. In this article, we leverage
               the LCM to construct the Syntax-LCM, a computer-automated method
               which quantifies syntax use that indicates abstraction levels. We
               test the Syntax-LCM?s accuracy for approximating hand-coded LCM
               scores and validate that it differentiates between text intended
               for a distal or proximal message recipient (previously linked
               with shifts in abstraction). We also consider existing automated
               methods for quantifying linguistic abstraction and find that the
               Syntax-LCM most consistently approximates LCM scores across
               contexts. We discuss practical and theoretical implications of
               these findings.",
  month     =  mar,
  year      =  2020
}

@ARTICLE{Wagner2021-pm,
  title    = "Measuring algorithmically infused societies",
  author   = "Wagner, Claudia and Strohmaier, Markus and Olteanu, Alexandra and
              Kıcıman, Emre and Contractor, Noshir and Eliassi-Rad, Tina",
  journal  = "Nature",
  volume   =  595,
  number   =  7866,
  pages    = "197--204",
  abstract = "It has been the historic responsibility of the social sciences to
              investigate human societies. Fulfilling this responsibility
              requires social theories, measurement models and social data. Most
              existing theories and measurement models in the social sciences
              were not developed with the deep societal reach of algorithms in
              mind. The emergence of 'algorithmically infused
              societies'-societies whose very fabric is co-shaped by algorithmic
              and human behaviour-raises three key challenges: the insufficient
              quality of measurements, the complex consequences of
              (mis)measurements, and the limits of existing social theories.
              Here we argue that tackling these challenges requires new social
              theories that account for the impact of algorithmic systems on
              social realities. To develop such theories, we need new
              methodologies for integrating data and measurements into theory
              construction. Given the scale at which measurements can be
              applied, we believe measurement models should be trustworthy,
              auditable and just. To achieve this, the development of
              measurements should be transparent and participatory, and include
              mechanisms to ensure measurement quality and identify possible
              harms. We argue that computational social scientists should
              rethink what aspects of algorithmically infused societies should
              be measured, how they should be measured, and the consequences of
              doing so.",
  month    =  jul,
  year     =  2021,
  language = "en"
}

@ARTICLE{Rashkin2021-nm,
  title         = "Measuring Attribution in Natural Language Generation Models",
  author        = "Rashkin, Hannah and Nikolaev, Vitaly and Lamm, Matthew and
                   Aroyo, Lora and Collins, Michael and Das, Dipanjan and
                   Petrov, Slav and Tomar, Gaurav Singh and Turc, Iulia and
                   Reitter, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "With recent improvements in natural language generation (NLG)
                   models for various applications, it has become imperative to
                   have the means to identify and evaluate whether NLG output is
                   only sharing verifiable information about the external world.
                   In this work, we present a new evaluation framework entitled
                   Attributable to Identified Sources (AIS) for assessing the
                   output of natural language generation models, when such
                   output pertains to the external world. We first define AIS
                   and introduce a two-stage annotation pipeline for allowing
                   annotators to appropriately evaluate model output according
                   to AIS guidelines. We empirically validate this approach on
                   generation datasets spanning three tasks (two conversational
                   QA datasets, a summarization dataset, and a table-to-text
                   dataset) via human evaluation studies that suggest that AIS
                   could serve as a common framework for measuring whether
                   model-generated statements are supported by underlying
                   sources. We release guidelines for the human evaluation
                   studies.",
  month         =  dec,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Bai2024-fl,
  title         = "Measuring Implicit Bias in Explicitly Unbiased Large Language
                   Models",
  author        = "Bai, Xuechunzi and Wang, Angelina and Sucholutsky, Ilia and
                   Griffiths, Thomas L",
  journal       = "arXiv [cs.CY]",
  abstract      = "Large language models (LLMs) can pass explicit bias tests but
                   still harbor implicit biases, similar to humans who endorse
                   egalitarian beliefs yet exhibit subtle biases. Measuring such
                   implicit biases can be a challenge: as LLMs become
                   increasingly proprietary, it may not be possible to access
                   their embeddings and apply existing bias measures;
                   furthermore, implicit biases are primarily a concern if they
                   affect the actual decisions that these systems make. We
                   address both of these challenges by introducing two measures
                   of bias inspired by psychology: LLM Implicit Association Test
                   (IAT) Bias, which is a prompt-based method for revealing
                   implicit bias; and LLM Decision Bias for detecting subtle
                   discrimination in decision-making tasks. Using these
                   measures, we found pervasive human-like stereotype biases in
                   6 LLMs across 4 social domains (race, gender, religion,
                   health) and 21 categories (weapons, guilt, science, career
                   among others). Our prompt-based measure of implicit bias
                   correlates with embedding-based methods but better predicts
                   downstream behaviors measured by LLM Decision Bias. This
                   measure is based on asking the LLM to decide between
                   individuals, motivated by psychological results indicating
                   that relative not absolute evaluations are more related to
                   implicit biases. Using prompt-based measures informed by
                   psychology allows us to effectively expose nuanced biases and
                   subtle discrimination in proprietary LLMs that do not show
                   explicit bias on standard benchmarks.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Wang2024-oq,
  title         = "Measuring machine learning harms from stereotypes: requires
                   understanding who is being harmed by which errors in what
                   ways",
  author        = "Wang, Angelina and Bai, Xuechunzi and Barocas, Solon and
                   Blodgett, Su Lin",
  journal       = "arXiv [cs.CY]",
  abstract      = "As machine learning applications proliferate, we need an
                   understanding of their potential for harm. However, current
                   fairness metrics are rarely grounded in human psychological
                   experiences of harm. Drawing on the social psychology of
                   stereotypes, we use a case study of gender stereotypes in
                   image search to examine how people react to machine learning
                   errors. First, we use survey studies to show that not all
                   machine learning errors reflect stereotypes nor are equally
                   harmful. Then, in experimental studies we randomly expose
                   participants to stereotype-reinforcing, -violating, and
                   -neutral machine learning errors. We find
                   stereotype-reinforcing errors induce more experientially
                   (i.e., subjectively) harmful experiences, while having
                   minimal changes to cognitive beliefs, attitudes, or
                   behaviors. This experiential harm impacts women more than
                   men. However, certain stereotype-violating errors are more
                   experientially harmful for men, potentially due to perceived
                   threats to masculinity. We conclude that harm cannot be the
                   sole guide in fairness mitigation, and propose a nuanced
                   perspective depending on who is experiencing what harm and
                   why.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Hendrycks2021-ar,
  title         = "Measuring Mathematical Problem Solving With the {MATH}
                   Dataset",
  author        = "Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and
                   Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn
                   and Steinhardt, Jacob",
  journal       = "arXiv [cs.LG]",
  abstract      = "Many intellectual endeavors require mathematical problem
                   solving, but this skill remains beyond the capabilities of
                   computers. To measure this ability in machine learning
                   models, we introduce MATH, a new dataset of 12,500
                   challenging competition mathematics problems. Each problem in
                   MATH has a full step-by-step solution which can be used to
                   teach models to generate answer derivations and explanations.
                   To facilitate future research and increase accuracy on MATH,
                   we also contribute a large auxiliary pretraining dataset
                   which helps teach models the fundamentals of mathematics.
                   Even though we are able to increase accuracy on MATH, our
                   results show that accuracy remains relatively low, even with
                   enormous Transformer models. Moreover, we find that simply
                   increasing budgets and model parameter counts will be
                   impractical for achieving strong mathematical reasoning if
                   scaling trends continue. While scaling Transformers is
                   automatically solving most other text-based tasks, scaling is
                   not currently solving MATH. To have more traction on
                   mathematical problem solving we will likely need new
                   algorithmic advancements from the broader research community.",
  month         =  mar,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Antoniak_undated-vm,
  title    = "Measuring Power and Social Dynamics Between Entities",
  author   = "Antoniak, Maria and Field, Anjalie and Mun, Jimin and Walsh,
              Melanie and Klein, Lauren F and Sap, Maarten",
  journal  = "maria-antoniak.github.io",
  abstract = "RIVETER provides a complete easy-to-use pipeline for analyzing
              verb connotations associated with entities in text corpora. We
              prepopulate the package with connotation frames of sentiment,
              power, and agency, which have demonstrated usefulness for
              capturing social phenomena, such as gender bias, in a broad range
              of corpora. For decades, lexical frameworks have been foundational
              tools in computational social science, digital humanities, and
              natural language processing, facilitating multifaceted analysis of
              text …"
}

@ARTICLE{Yuan2024-zq,
  title         = "Measuring Social Norms of Large Language Models",
  author        = "Yuan, Ye and Tang, Kexin and Shen, Jianhao and Zhang, Ming
                   and Wang, Chenguang",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present a new challenge to examine whether large language
                   models understand social norms. In contrast to existing
                   datasets, our dataset requires a fundamental understanding of
                   social norms to solve. Our dataset features the largest set
                   of social norm skills, consisting of 402 skills and 12,383
                   questions covering a wide set of social norms ranging from
                   opinions and arguments to culture and laws. We design our
                   dataset according to the K-12 curriculum. This enables the
                   direct comparison of the social understanding of large
                   language models to humans, more specifically, elementary
                   students. While prior work generates nearly random accuracy
                   on our benchmark, recent large language models such as
                   GPT3.5-Turbo and LLaMA2-Chat are able to improve the
                   performance significantly, only slightly below human
                   performance. We then propose a multi-agent framework based on
                   large language models to improve the models' ability to
                   understand social norms. This method further improves large
                   language models to be on par with humans. Given the
                   increasing adoption of large language models in real-world
                   applications, our finding is particularly important and
                   presents a unique direction for future improvements.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Park2022-yp,
  title     = "Measuring the Prevalence of Anti-Social Behavior in Online
               Communities",
  author    = "Park, Joon Sung and Seering, Joseph and Bernstein, Michael S",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  6,
  number    = "CSCW2",
  pages     = "1--29",
  abstract  = "With increasing attention to online anti-social behaviors such as
               personal attacks and bigotry, it is critical to have an accurate
               accounting of how widespread anti-social behaviors are. In this
               paper, we empirically measure the prevalence of anti-social
               behavior in one of the world's most popular online community
               platforms. We operationalize this goal as measuring the
               proportion of unmoderated comments in the 97 most popular
               communities on Reddit that violate eight widely accepted platform
               norms. To achieve this goal, we contribute a human-AI pipeline
               for identifying these violations and a bootstrap sampling method
               to quantify measurement uncertainty. We find that 6.25\% (95\%
               Confidence Interval [5.36\%, 7.13\%]) of all comments in 2016,
               and 4.28\% (95\% CI [2.50\%, 6.26\%]) in 2020, are violations of
               these norms. Most anti-social behaviors remain unmoderated:
               moderators only removed one in twenty violating comments in 2016,
               and one in ten violating comments in 2020. Personal attacks were
               the most prevalent category of norm violation; pornography and
               bigotry were the most likely to be moderated, while politically
               inflammatory comments and misogyny/vulgarity were the least
               likely to be moderated. This paper offers a method and set of
               empirical results for tracking these phenomena as both the social
               practices (e.g., moderation) and technical practices (e.g.,
               design) evolve.",
  month     =  nov,
  year      =  2022,
  keywords  = "anti-social behavior, online communities, moderation"
}

@INPROCEEDINGS{Cheng2024-si,
  title     = "Measuring Uncertainty in Neural Machine Translation with
               Similarity-Sensitive Entropy",
  author    = "Cheng, Julius and Vlachos, Andreas",
  editor    = "Graham, Yvette and Purver, Matthew",
  booktitle = "Proceedings of the 18th Conference of the European Chapter of the
               Association for Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "St. Julian's, Malta",
  pages     = "2115--2128",
  abstract  = "Uncertainty estimation is an important diagnostic tool for
               statistical models, and is often used to assess the confidence of
               model predictions. Previous work shows that neural machine
               translation (NMT) is an intrinsically uncertain task where there
               are often multiple correct and semantically equivalent
               translations, and that well-trained NMT models produce good
               translations despite spreading probability mass among many
               semantically similar translations. These findings suggest that
               popular measures of uncertainty based on token- and
               sequence-level entropies which measure surface form diversity may
               not be good proxies of the more useful quantity of interest,
               semantic diversity. We propose to adapt similarity-sensitive
               Shannon entropy (S3E), a concept borrowed from theoretical
               ecology, for NMT. By demonstrating significantly improved
               correlation between S3E and task performance on quality
               estimation and named entity recall, we show that S3E is a useful
               framework for measuring uncertainty in NMT.",
  month     =  mar,
  year      =  2024
}

@ARTICLE{Roberts2021-ed,
  title     = "Media Cloud: Massive open source collection of global news on the
               open web",
  author    = "Roberts, Hal and Bhargava, Rahul and Valiukas, Linas and Jen,
               Dennis and Malik, Momin M and Bishop, Cindy Sherman and Ndulue,
               Emily B and Dave, Aashka and Clark, Justin and Etling, Bruce and
               Faris, Robert and Shah, Anushka and Rubinovitz, Jasmin and Hope,
               Alexis and D'Ignazio, Catherine and Bermejo, Fernando and
               Benkler, Yochai and Zuckerman, Ethan",
  journal   = "Proceedings of the International AAAI Conference on Web and
               Social Media",
  publisher = "Association for the Advancement of Artificial Intelligence (AAAI)",
  volume    =  15,
  pages     = "1034--1045",
  abstract  = "We present the first full description of Media Cloud, an open
               source platform based on crawling hyperlink structure in
               operation for over 10 years, that for many uses will be the best
               way to collect data for studying the media ecosystem on the open
               web. We document the key choices behind what data Media Cloud
               collects and stores, how it processes and organizes these data,
               and its open API access as well as user-facing tools. We also
               highlight the strengths and limitations of the Media Cloud
               collection strategy compared to relevant alternatives. We give an
               overview two sample datasets generated using Media Cloud and
               discuss how researchers can use the platform to create their own
               datasets.",
  month     =  may,
  year      =  2021
}

@INPROCEEDINGS{Mitchell2022-hx,
  title     = "Memory-Based Model Editing at Scale",
  author    = "Mitchell, Eric and Lin, Charles and Bosselut, Antoine and
               Manning, Christopher D and Finn, Chelsea",
  editor    = "Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and
               Szepesvari, Csaba and Niu, Gang and Sabato, Sivan",
  booktitle = "Proceedings of the 39th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  162,
  pages     = "15817--15831",
  abstract  = "Even the largest neural networks make errors, and once-correct
               predictions can become invalid as the world changes. Model
               editors make local updates to the behavior of base (pre-trained)
               models to inject updated knowledge or correct undesirable
               behaviors. Existing model editors have shown promise, but also
               suffer from insufficient expressiveness: they struggle to
               accurately model an edit’s intended scope (examples affected by
               the edit), leading to inaccurate predictions for test inputs
               loosely related to the edit, and they often fail altogether after
               many edits. As a higher-capacity alternative, we propose
               Semi-Parametric Editing with a Retrieval-Augmented Counterfactual
               Model (SERAC), which stores edits in an explicit memory and
               learns to reason over them to modulate the base model’s
               predictions as needed. To enable more rigorous evaluation of
               model editors, we introduce three challenging language model
               editing problems based on question answering, fact-checking, and
               dialogue generation. We find that only SERAC achieves high
               performance on all three problems, consistently outperforming
               existing approaches to model editing by a significant margin.
               Code, data, and additional project information will be made
               available at https://sites.google.com/view/serac-editing.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2022
}

@ARTICLE{Sakarvadia2023-ja,
  title         = "Memory Injections: Correcting Multi-Hop Reasoning Failures
                   during Inference in Transformer-Based Language Models",
  author        = "Sakarvadia, Mansi and Ajith, Aswathy and Khan, Arham and
                   Grzenda, Daniel and Hudson, Nathaniel and Bauer, André and
                   Chard, Kyle and Foster, Ian",
  journal       = "arXiv [cs.CL]",
  abstract      = "Answering multi-hop reasoning questions requires retrieving
                   and synthesizing information from diverse sources. Large
                   Language Models (LLMs) struggle to perform such reasoning
                   consistently. Here we propose an approach to pinpoint and
                   rectify multi-hop reasoning failures through targeted memory
                   injections on LLM attention heads. First, we analyze the
                   per-layer activations of GPT-2 models in response to single
                   and multi-hop prompts. We then propose a mechanism that
                   allows users to inject pertinent prompt-specific information,
                   which we refer to as ``memories,'' at critical LLM locations
                   during inference. By thus enabling the LLM to incorporate
                   additional relevant information during inference, we enhance
                   the quality of multi-hop prompt completions. We show
                   empirically that a simple, efficient, and targeted memory
                   injection into a key attention layer can often increase the
                   probability of the desired next token in multi-hop tasks, by
                   up to 424\%.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Rollero2013-xy,
  title     = "Men and women facing objectification: The effects of media models
               on well-being, self-esteem and ambivalent sexism",
  author    = "Rollero, Chiara",
  journal   = "Soc. Psychol. Educ.",
  publisher = "Routledge",
  volume    =  28,
  number    =  3,
  pages     = "373--382",
  abstract  = "Literature on objectification has largely shown the relationship
               between viewing objectified media models and women's body
               dissatisfaction and drive for thinness. The purpose of the
               present study was to extend past research by examining the
               effects of objectified media images?considering both male and
               female models?on psychological well-being, self-esteem, and
               endorsement of sexist attitudes. 166 undergraduates (51.8\%
               males) participated in the study. Results showed that
               objectification of men decreases men's well-being, whereas
               objectification of women not only decreases women's well-being,
               but also their attractiveness and social self-esteem.
               Furthermore, objectification of women affects men's endorsement
               of sexist attitudes, increasing hostility toward women and
               decreasing hostility toward men. Implications are discussed.",
  month     =  jan,
  year      =  2013
}

@ARTICLE{Wright2016-av,
  title    = "Men's Objectifying Media Consumption, Objectification of Women,
              and Attitudes Supportive of Violence Against Women",
  author   = "Wright, Paul J and Tokunaga, Robert S",
  journal  = "Arch. Sex. Behav.",
  volume   =  45,
  number   =  4,
  pages    = "955--964",
  abstract = "A recent White House Council Report on Women and Girls called
              attention to sexual assault on college campuses and encouraged
              continued research on this important public health problem. Media
              that sexually objectify women have been identified by feminist
              scholars as encouraging of sexual assault, but some researchers
              question why portrayals that do not feature sexual assault should
              affect men's attitudes supportive of violence against women.
              Guided by the concepts of specific and abstract sexual scripting
              in Wright's (Communication Yearbook 35:343-386, 2011) sexual
              script acquisition, activation, application model of sexual media
              socialization, this study proposed that the more men are exposed
              to objectifying depictions, the more they will think of women as
              entities that exist for men's sexual gratification (specific
              sexual scripting), and that this dehumanized perspective on women
              may then be used to inform attitudes regarding sexual violence
              against women (abstract sexual scripting). Data were gathered from
              collegiate men sexually attracted to women (N = 187). Consistent
              with expectations, associations between men's exposure to
              objectifying media and attitudes supportive of violence against
              women were mediated by their notions of women as sex objects.
              Specifically, frequency of exposure to men's lifestyle magazines
              that objectify women, reality TV programs that objectify women,
              and pornography predicted more objectified cognitions about women,
              which, in turn, predicted stronger attitudes supportive of
              violence against women.",
  month    =  may,
  year     =  2016,
  keywords = "3AM; Men’s magazines; Objectification; Pornography; Reality TV;
              Violence",
  language = "en"
}

@MISC{noauthor_undated-dz,
  title = "{MendozaDenton2016}.pdf"
}

@ARTICLE{Velte2022-km,
  title     = "Meta-analyses on Corporate Social Responsibility ({CSR}): a
               literature review",
  author    = "Velte, Patrick",
  journal   = "Manag. Rev. Q.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  72,
  number    =  3,
  pages     = "627--675",
  abstract  = "AbstractThis paper addresses quantitative meta-analyses on
               corporate governance-related determinants and firms’ (non)
               financial consequences of Corporate Social Responsibility (CSR).
               Legitimacy theory as our theoretical framework assumes that,
               through a social contract, a company must fulfil the respective
               society’s values and expectations and gain legitimacy. We also
               rely on the business case argument, assuming a positive
               relationship between CSR and financial outcomes of the firm. This
               analysis focusses on 54 quantitative meta-analyses on CSR and
               includes a structured literature review in order to increase our
               knowledge, which corporate governance variables and proxies of
               firm’s (non) financial outcome have been heavily included in
               archival research, and if there is an overall impact of these
               variables. Prior meta-analyses indicate that board independence,
               board gender diversity, and board size have a positive impact on
               CSR performance. Moreover, both CSR performance and environmental
               performance increase financial performance. This literature
               review makes a useful contribution to prior studies by
               summarizing the overall impact of corporate governance variables
               on CSR and their (non) financial consequences and by deducing
               recommendations for future research.",
  month     =  sep,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Suzgun2024-aa,
  title     = "Meta-prompting: Enhancing language models with task-agnostic
               scaffolding",
  author    = "Suzgun, M and Kalai, A T",
  journal   = "arXiv preprint arXiv:2401.12954",
  publisher = "arxiv.org",
  abstract  = "We introduce meta-prompting, an effective scaffolding technique
               designed to enhance the functionality of language models (LMs).
               This approach transforms a single LM into a multi …",
  year      =  2024
}

@ARTICLE{Hoeken_undated-vs,
  title  = "Methodological Insights in Detecting Subtle Semantic Shifts with
            Contextualized and Static Language Models",
  author = "Hoeken, Sanne and Alaçam, Özge and Fokkens, Antske and Sommerauer,
            Pia"
}

@ARTICLE{Ilvento2019-yu,
  title         = "Metric Learning for Individual Fairness",
  author        = "Ilvento, Christina",
  journal       = "arXiv [cs.LG]",
  abstract      = "There has been much discussion recently about how fairness
                   should be measured or enforced in classification. Individual
                   Fairness [Dwork, Hardt, Pitassi, Reingold, Zemel, 2012],
                   which requires that similar individuals be treated similarly,
                   is a highly appealing definition as it gives strong
                   guarantees on treatment of individuals. Unfortunately, the
                   need for a task-specific similarity metric has prevented its
                   use in practice. In this work, we propose a solution to the
                   problem of approximating a metric for Individual Fairness
                   based on human judgments. Our model assumes that we have
                   access to a human fairness arbiter, who can answer a limited
                   set of queries concerning similarity of individuals for a
                   particular task, is free of explicit biases and possesses
                   sufficient domain knowledge to evaluate similarity. Our
                   contributions include definitions for metric approximation
                   relevant for Individual Fairness, constructions for
                   approximations from a limited number of realistic queries to
                   the arbiter on a sample of individuals, and learning
                   procedures to construct hypotheses for metric approximations
                   which generalize to unseen samples under certain assumptions
                   of learnability of distance threshold functions.",
  month         =  jun,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{McIlroy-Young2022-ib,
  title     = "Mimetic Models: Ethical Implications of {AI} that Acts Like You",
  author    = "McIlroy-Young, Reid and Kleinberg, Jon and Sen, Siddhartha and
               Barocas, Solon and Anderson, Ashton",
  booktitle = "Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and
               Society",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "479--490",
  abstract  = "An emerging theme in artificial intelligence research is the
               creation of models to simulate the decisions and behavior of
               specific people, in domains including game-playing, text
               generation, and artistic expression. These models go beyond
               earlier approaches in the way they are tailored to individuals,
               and the way they are designed for interaction rather than simply
               the reproduction of fixed, pre-computed behaviors. We refer to
               these as mimetic models, and in this paper we develop a framework
               for characterizing the ethical and social issues raised by their
               growing availability. Our framework includes a number of distinct
               scenarios for the use of such models, and considers the impacts
               on a range of different participants, including the target being
               modeled, the operator who deploys the model, and the entities
               that interact with it.",
  series    = "AIES '22",
  month     =  jul,
  year      =  2022,
  keywords  = "mimetic models, machine learning, generative models, ethics,
               artificial intelligence"
}

@ARTICLE{Nonperformatives_undated-mb,
  title  = "{MIND} {THE} {GAP}!",
  author = "Nonperformatives, Other"
}

@ARTICLE{Sclar2023-cj,
  title     = "Minding Language Models'(Lack of) Theory of Mind: A Plug-and-Play
               Multi-Character Belief Tracker",
  author    = "Sclar, M and Kumar, S and West, P and Suhr, A and Choi, Y and
               {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Theory of Mind (ToM) $\unicode {x2014} $ the ability to reason
               about the mental states of other people $\unicode {x2014} $ is a
               key element of our social intelligence. Yet, despite …",
  year      =  2023
}

@ARTICLE{DMello2010-hm,
  title     = "Mining collaborative patterns in tutorial dialogues",
  author    = "D'Mello, Sidney and Olney, Andrew and Person, Natalie",
  journal   = "J. Educ. Data Min.",
  publisher = "International Working Group on Educational Data Mining. e-mail:
               jedm.editor@gmail.com; Web site:
               http://www.educationaldatamining.org/JEDM/index.php/JEDM/index",
  volume    =  2,
  number    =  1,
  pages     = "1--37",
  abstract  = "We present a method to automatically detect collaborative
               patterns of student and tutor dialogue moves. The method
               identifies significant two-step excitatory transitions between
               dialogue moves, integrates the transitions into a directed graph
               representation, and generates and tests data-driven hypotheses
               from the directed graph. The method was applied to a large corpus
               of student-tutor dialogue moves from expert tutoring sessions. An
               examination of the subset of the corpus consisting of tutor
               lectures revealed collaborative patterns consistent with
               information-transmission, information-elicitation, off
               topic-conversation, and student initiated questions. Sequences of
               dialogue moves within each of these patterns were also
               identified. Comparisons of the method to other approaches and
               applications towards the computational modeling of expert human
               tutors are discussed.",
  year      =  2010,
  keywords  = "Tutors; Interpersonal Communication; Discourse Analysis;
               Intelligent Tutoring Systems; Markov Processes; Sequential
               Approach; Science Education; Mathematics Education; Questioning
               Techniques; Information Retrieval; Probability; Equations
               (Mathematics); Data Analysis",
  language  = "en"
}

@ARTICLE{Abercrombie2023-gw,
  title         = "Mirages: On Anthropomorphism in Dialogue Systems",
  author        = "Abercrombie, Gavin and Curry, Amanda Cercas and Dinkar, Tanvi
                   and Rieser, Verena and Talat, Zeerak",
  journal       = "arXiv [cs.CL]",
  abstract      = "Automated dialogue or conversational systems are
                   anthropomorphised by developers and personified by users.
                   While a degree of anthropomorphism may be inevitable due to
                   the choice of medium, conscious and unconscious design
                   choices can guide users to personify such systems to varying
                   degrees. Encouraging users to relate to automated systems as
                   if they were human can lead to high risk scenarios caused by
                   over-reliance on their outputs. As a result, natural language
                   processing researchers have investigated the factors that
                   induce personification and develop resources to mitigate such
                   effects. However, these efforts are fragmented, and many
                   aspects of anthropomorphism have yet to be explored. In this
                   paper, we discuss the linguistic factors that contribute to
                   the anthropomorphism of dialogue systems and the harms that
                   can arise, including reinforcing gender stereotypes and
                   notions of acceptable language. We recommend that future
                   efforts towards developing dialogue systems take particular
                   care in their design, development, release, and description;
                   and attend to the many linguistic cues that can elicit
                   personification by users.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Stafanovics2020-lt,
  title         = "Mitigating Gender Bias in Machine Translation with Target
                   Gender Annotations",
  author        = "Stafanovičs, Artūrs and Bergmanis, Toms and Pinnis, Mārcis",
  journal       = "arXiv [cs.CL]",
  abstract      = "When translating ``The secretary asked for details.'' to a
                   language with grammatical gender, it might be necessary to
                   determine the gender of the subject ``secretary''. If the
                   sentence does not contain the necessary information, it is
                   not always possible to disambiguate. In such cases, machine
                   translation systems select the most common translation
                   option, which often corresponds to the stereotypical
                   translations, thus potentially exacerbating prejudice and
                   marginalisation of certain groups and people. We argue that
                   the information necessary for an adequate translation can not
                   always be deduced from the sentence being translated or even
                   might depend on external knowledge. Therefore, in this work,
                   we propose to decouple the task of acquiring the necessary
                   information from the task of learning to translate correctly
                   when such information is available. To that end, we present a
                   method for training machine translation systems to use
                   word-level annotations containing information about subject's
                   gender. To prepare training data, we annotate regular source
                   language words with grammatical gender information of the
                   corresponding target language words. Using such data to train
                   machine translation systems reduces their reliance on gender
                   stereotypes when information about the subject's gender is
                   available. Our experiments on five language pairs show that
                   this allows improving accuracy on the WinoMT test set by up
                   to 25.8 percentage points.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Raman2023-rw,
  title         = "Model-tuning Via Prompts Makes {NLP} Models Adversarially
                   Robust",
  author        = "Raman, Mrigank and Maini, Pratyush and Zico Kolter, J and
                   Lipton, Zachary C and Pruthi, Danish",
  journal       = "arXiv [cs.CL]",
  abstract      = "In recent years, NLP practitioners have converged on the
                   following practice: (i) import an off-the-shelf pretrained
                   (masked) language model; (ii) append a multilayer perceptron
                   atop the CLS token's hidden representation (with randomly
                   initialized weights); and (iii) fine-tune the entire model on
                   a downstream task (MLP-FT). This procedure has produced
                   massive gains on standard NLP benchmarks, but these models
                   remain brittle, even to mild adversarial perturbations. In
                   this work, we demonstrate surprising gains in adversarial
                   robustness enjoyed by Model-tuning Via Prompts (MVP), an
                   alternative method of adapting to downstream tasks. Rather
                   than appending an MLP head to make output prediction, MVP
                   appends a prompt template to the input, and makes prediction
                   via text infilling/completion. Across 5 NLP datasets, 4
                   adversarial attacks, and 3 different models, MVP improves
                   performance against adversarial substitutions by an average
                   of 8\% over standard methods and even outperforms adversarial
                   training-based state-of-art defenses by 3.5\%. By combining
                   MVP with adversarial training, we achieve further
                   improvements in adversarial robustness while maintaining
                   performance on unperturbed examples. Finally, we conduct
                   ablations to investigate the mechanism underlying these
                   gains. Notably, we find that the main causes of vulnerability
                   of MLP-FT can be attributed to the misalignment between
                   pre-training and fine-tuning tasks, and the randomly
                   initialized MLP parameters.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Lee2020-be,
  title     = "Modeling Code-Switch Languages Using Bilingual Parallel Corpus",
  author    = "Lee, Grandee and Li, Haizhou",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Online",
  pages     = "860--870",
  abstract  = "Language modeling is the technique to estimate the probability of
               a sequence of words. A bilingual language model is expected to
               model the sequential dependency for words across languages, which
               is difficult due to the inherent lack of suitable training data
               as well as diverse syntactic structure across languages. We
               propose a bilingual attention language model (BALM) that
               simultaneously performs language modeling objective with a
               quasi-translation objective to model both the monolingual as well
               as the cross-lingual sequential dependency. The attention
               mechanism learns the bilingual context from a parallel corpus.
               BALM achieves state-of-the-art performance on the SEAME
               code-switch database by reducing the perplexity of 20.5\% over
               the best-reported result. We also apply BALM in bilingual lexicon
               induction, and language normalization tasks to validate the idea.",
  month     =  jul,
  year      =  2020
}

@MISC{noauthor_undated-ne,
  title        = "Moderating the public sphere",
  howpublished = "\url{https://scholar.google.ca/scholar?cluster=5060947490507119333,839974186453323654\&hl=en\&as\_sdt=0,5\&sciodt=0,5}",
  note         = "Accessed: 2023-11-9"
}

@ARTICLE{Kennedy2021-fj,
  title     = "Moral concerns are differentially observable in language",
  author    = "Kennedy, Brendan and Atari, Mohammad and Mostafazadeh Davani,
               Aida and Hoover, Joe and Omrani, Ali and Graham, Jesse and
               Dehghani, Morteza",
  journal   = "Cognition",
  publisher = "Elsevier",
  volume    =  212,
  pages     =  104696,
  abstract  = "Language is a psychologically rich medium for human expression
               and communication. While language usage has been shown to be a
               window into various aspects of people's social worlds, including
               their personality traits and everyday environment, its
               correspondence to people's moral concerns has yet to be
               considered. Here, we examine the relationship between language
               usage and the moral concerns of Care, Fairness, Loyalty,
               Authority, and Purity as conceptualized by Moral Foundations
               Theory. We collected Facebook status updates (N = 107,798) from
               English-speaking participants (n = 2691) along with their
               responses on the Moral Foundations Questionnaire. Overall,
               results suggested that self-reported moral concerns may be traced
               in language usage, though the magnitude of this effect varied
               considerably among moral concerns. Across a diverse selection of
               Natural Language Processing methods, Fairness concerns were
               consistently least correlated with language usage whereas Purity
               concerns were found to be the most traceable. In exploratory
               follow-up analyses, each moral concern was found to be
               differentially related to distinct patterns of relational,
               emotional, and social language. Our results are the first to
               relate individual differences in moral concerns to language
               usage, and to uncover the signatures of moral concerns in
               language.",
  month     =  jul,
  year      =  2021,
  keywords  = "Language; Moral foundations theory; Morality; Natural language
               processing; Text analysis",
  language  = "en"
}

@ARTICLE{Simmons2022-gm,
  title         = "Moral Mimicry: Large Language Models Produce Moral
                   Rationalizations Tailored to Political Identity",
  author        = "Simmons, Gabriel",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs) have recently demonstrated
                   impressive capability in generating fluent text. LLMs have
                   also shown an alarming tendency to reproduce social biases,
                   for example stereotypical associations between gender and
                   occupation or race and criminal behavior. Like race and
                   gender, morality is an important social variable; our moral
                   biases affect how we receive other people and their
                   arguments. I anticipate that the apparent moral capabilities
                   of LLMs will play an important role in their effects on the
                   human social environment. This work investigates whether LLMs
                   reproduce the moral biases associated with political groups,
                   a capability I refer to as moral mimicry. I explore this
                   hypothesis in GPT-3, a 175B-parameter language model based on
                   the Transformer architecture, using tools from Moral
                   Foundations Theory to measure the moral content in text
                   generated by the model following prompting with liberal and
                   conservative political identities. The results demonstrate
                   that large language models are indeed moral mimics; when
                   prompted with a political identity, GPT-3 generates text
                   reflecting the corresponding moral biases. Moral mimicry
                   could contribute to fostering understanding between social
                   groups via moral reframing. Worryingly, it could also
                   reinforce polarized views, exacerbating existing social
                   challenges. I hope that this work encourages further
                   investigation of the moral mimicry capability, including how
                   to leverage it for social good and minimize its risks.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Atari2022-lz,
  title     = "Morally Homogeneous Networks and Radicalism",
  author    = "Atari, Mohammad and Davani, Aida Mostafazadeh and Kogon, Drew and
               Kennedy, Brendan and Ani Saxena, Nripsuta and Anderson, Ian and
               Dehghani, Morteza",
  journal   = "Soc. Psychol. Personal. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  13,
  number    =  6,
  pages     = "999--1009",
  abstract  = "Online radicalization is among the most vexing challenges the
               world faces today. Here, we demonstrate that homogeneity in moral
               concerns results in increased levels of radical intentions. In
               Study 1, we find that in Gab?a right-wing extremist network?the
               degree of moral convergence within a cluster predicts the number
               of hate-speech messages members post. In Study 2, we replicate
               this observation in another extremist network, Incels. In Studies
               3 to 5 (N = 1,431), we demonstrate that experimentally leading
               people to believe that others in their hypothetical or real group
               share their moral views increases their radical intentions as
               well as willingness to fight and die for the group. Our findings
               highlight the role of moral convergence in radicalization,
               emphasizing the need for diversity of moral worldviews within
               social networks.",
  month     =  aug,
  year      =  2022
}

@ARTICLE{Zhong2023-kl,
  title         = "{MQuAKE}: Assessing Knowledge Editing in Language Models via
                   Multi-Hop Questions",
  author        = "Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D
                   and Potts, Christopher and Chen, Danqi",
  journal       = "arXiv [cs.CL]",
  abstract      = "The information stored in large language models (LLMs) falls
                   out of date quickly, and retraining from scratch is often not
                   an option. This has recently given rise to a range of
                   techniques for injecting new facts through updating model
                   weights. Current evaluation paradigms are extremely limited,
                   mainly validating the recall of edited facts, but changing
                   one fact should cause rippling changes to the model's related
                   beliefs. If we edit the UK Prime Minister to now be Rishi
                   Sunak, then we should get a different answer to Who is
                   married to the British Prime Minister? In this work, we
                   present a benchmark, MQuAKE (Multi-hop Question Answering for
                   Knowledge Editing), comprising multi-hop questions that
                   assess whether edited models correctly answer questions where
                   the answer should change as an entailed consequence of edited
                   facts. While we find that current knowledge-editing
                   approaches can recall edited facts accurately, they fail
                   catastrophically on the constructed multi-hop questions. We
                   thus propose a simple memory-based approach, MeLLo, which
                   stores all edited facts externally while prompting the
                   language model iteratively to generate answers that are
                   consistent with the edited facts. While MQuAKE remains
                   challenging, we show that MeLLo scales well with LLMs (up to
                   175B) and outperforms previous model editors by a large
                   margin.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ziems2022-ig,
  title         = "Multi-{VALUE}: A Framework for Cross-Dialectal English {NLP}",
  author        = "Ziems, Caleb and Held, William and Yang, Jingfeng and
                   Dhamala, Jwala and Gupta, Rahul and Yang, Diyi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Dialect differences caused by regional, social, and economic
                   factors cause performance discrepancies for many groups of
                   language technology users. Inclusive and equitable language
                   technology must critically be dialect invariant, meaning that
                   performance remains constant over dialectal shifts. Current
                   systems often fall short of this ideal since they are
                   designed and tested on a single dialect: Standard American
                   English (SAE). We introduce a suite of resources for
                   evaluating and achieving English dialect invariance. The
                   resource is called Multi-VALUE, a controllable rule-based
                   translation system spanning 50 English dialects and 189
                   unique linguistic features. Multi-VALUE maps SAE to synthetic
                   forms of each dialect. First, we use this system to stress
                   tests question answering, machine translation, and semantic
                   parsing. Stress tests reveal significant performance
                   disparities for leading models on non-standard dialects.
                   Second, we use this system as a data augmentation technique
                   to improve the dialect robustness of existing systems.
                   Finally, we partner with native speakers of Chicano and
                   Indian English to release new gold-standard variants of the
                   popular CoQA task. To execute the transformation code, run
                   model checkpoints, and download both synthetic and
                   gold-standard dialectal benchmark datasets, see
                   http://value-nlp.org.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Garrett1996-ua,
  title     = "“Multidisciplinary {Series”Transformations} in Time and Space:
               Social Theory and Recovery from Eating Disorders",
  author    = "Garrett, Catherine J",
  journal   = "Eat. Disord.",
  publisher = "Routledge",
  volume    =  4,
  number    =  3,
  pages     = "245--255",
  abstract  = "As existential crisis, an eating disorder attempts to control the
               categories of time and space, in an effort to escape despair.
               Therapists have frequently commented on the distortions of these
               categories during the anorectic or bulimic period, but the
               meaning of their observations has remained relatively unexplored.
               This article begins with the idea that time and space are
               cultural constructs and focuses on their reconstruction during
               recovery, with illustrations from the case studies of Palazzoli
               and Bruch. It uses classical anthropological theory (Durkheim and
               van Gennep) to demonstrate how ritual can structure and re-create
               space and time, bringing about a confrontation with death,
               changes in relation to society, and a transformation of the body.
               It discusses the implications for therapy of Durkheim's insight
               that the transpersonal/spiritual is inseparable from the social,
               showing how sufferers from eating disorders and their therapists
               jointly devise new meanings for the past and the future and find
               ways to expand the space the ex-sufferer creates in the world.",
  month     =  sep,
  year      =  1996
}

@ARTICLE{Foster2022-vm,
  title     = "Muscles, Makeup, and Femboys: Analyzing {TikTok’s} “Radical”
               Masculinities",
  author    = "Foster, Jordan and Baker, Jayne",
  journal   = "Social Media + Society",
  publisher = "SAGE Publications Ltd",
  volume    =  8,
  number    =  3,
  pages     =  20563051221126040,
  abstract  = "News reports and online comments suggest that social media
               applications like TikTok play an important role in challenging
               traditional notions of masculinity. Male creators who don jewelry
               and engage in dance in their videos are emblematic of a broader
               shift in social and mainstream media toward gender
               non-conformity. Do these videos represent a movement away from
               hegemonic ideals? Based on a visual content analysis of 205
               TikTok videos across the application?s 43 most followed male
               creators, we examine representations of masculinity on the
               platform. Drawing on the concept of hybrid masculinity, we find
               that TikTok creators both challenge and reinforce traditional
               notions of masculinity, subverting widely recognizable tropes,
               and gender norms while simultaneously reinforcing the importance
               of men?s muscularity, attractiveness, and sexual bravado. Taken
               together, our findings contribute to a broader discussion of the
               role that social media play in reproducing inequality along the
               lines of gender, race, and sexuality, including how beauty is
               rewarded symbolically and materially in online spaces.",
  month     =  jul,
  year      =  2022
}

@ARTICLE{Trivedi2021-op,
  title     = "♫ {MuSiQue}: Multihop Questions via Single-hop Question
               Composition",
  author    = "Trivedi, H and Balasubramanian, Niranjan and Khot, Tushar and
               Sabharwal, Ashish",
  journal   = "Trans. Assoc. Comput. Linguist.",
  publisher = "direct.mit.edu",
  volume    =  10,
  pages     = "539--554",
  abstract  = "Multihop reasoning remains an elusive goal as existing multihop
               benchmarks are known to be largely solvable via shortcuts. Can we
               create a question answering (QA) dataset that, by construction,
               requires proper multihop reasoning? To this end, we introduce a
               bottom–up approach that systematically selects composable pairs
               of single-hop questions that are connected, that is, where one
               reasoning step critically relies on information from another.
               This bottom–up methodology lets us explore a vast space of
               questions and add stringent filters as well as other mechanisms
               targeting connected reasoning. It provides fine-grained control
               over the construction process and the properties of the resulting
               k-hop questions. We use this methodology to create MuSiQue-Ans, a
               new multihop QA dataset with 25K 2–4 hop questions. Relative to
               existing datasets, MuSiQue-Ans is more difficult overall (3×
               increase in human–machine gap), and harder to cheat via
               disconnected reasoning (e.g., a single-hop model has a 30-point
               drop in F1). We further add unanswerable contrast questions to
               produce a more stringent dataset, MuSiQue-Full. We hope our
               datasets will help the NLP community develop models that perform
               genuine multihop reasoning.1",
  month     =  aug,
  year      =  2021
}

@MISC{noauthor_undated-za,
  title = "{NA}-47.pdf"
}

@INPROCEEDINGS{Sultana2022-al,
  title     = "Narrative Datasets through the Lenses of {NLP} and {HCI}",
  author    = "Sultana, Sharifa and Zhang, Renwen and Lim, Hajin and Antoniak,
               Maria",
  booktitle = "Proceedings of the Second Workshop on Bridging Human--Computer
               Interaction and Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Seattle, Washington",
  pages     = "47--54",
  abstract  = "In this short paper, we compare existing value systems and
               approaches in NLP and HCI for collecting narrative data. Building
               on these parallel discussions, we shed light on the challenges
               facing some popular NLP dataset types, which we discuss these in
               relation to widely-used narrative-based HCI research methods; and
               we highlight points where NLP methods can broaden qualitative
               narrative studies. In particular, we point towards contextuality,
               positionality, dataset size, and open research design as central
               points of difference and windows for collaboration when studying
               narratives. Through the use case of narratives, this work
               contributes to a larger conversation regarding the possibilities
               for bridging NLP and HCI through speculative mixed-methods.",
  month     =  jul,
  year      =  2022
}

@ARTICLE{Antoniak2019-qx,
  title     = "Narrative Paths and Negotiation of Power in Birth Stories",
  author    = "Antoniak, Maria and Mimno, David and Levy, Karen",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  3,
  number    = "CSCW",
  pages     = "1--27",
  abstract  = "Birth stories have become increasingly common on the internet,
               but they have received little attention as a computational
               dataset. These unsolicited, publicly posted stories provide rich
               descriptions of decisions, emotions, and relationships during a
               common but sometimes traumatic medical experience. These personal
               details can be illuminating for medical practitioners, and due to
               their shared structures, birth stories are also an ideal testing
               ground for narrative analysis techniques. We present an analysis
               of 2,847 birth stories from an online forum and demonstrate the
               utility of these stories for computational work. We discover
               clear sentiment, topic and persona-based patterns that both model
               the expected narrative event sequences of birth stories and
               highlight diverging pathways and exceptions to narrative norms.
               The authors' motivation to publicly post these personal stories
               can be a way to regain power after a surveilled and disempowering
               experience, and we explore power relationships between the
               personas in the stories, showing that these dynamics can vary
               with the type of birth (e.g., medicated vs unmedicated). Finally,
               birth stories exist in a space that is both public and deeply
               personal. This liminality poses a challenge for analysis and
               presentation, and we discuss tradeoffs and ethical practices for
               this collection. WARNING: This paper includes detailed narratives
               of pregnancy and birth.",
  month     =  nov,
  year      =  2019,
  keywords  = "narrative, birth stories, power, natural language processing"
}

@INPROCEEDINGS{Piper2021-ws,
  title     = "Narrative Theory for Computational Narrative Understanding",
  author    = "Piper, Andrew and So, Richard Jean and Bamman, David",
  booktitle = "Proceedings of the 2021 Conference on Empirical Methods in
               Natural Language Processing",
  publisher = "Association for Computational Linguistics",
  address   = "Online and Punta Cana, Dominican Republic",
  pages     = "298--311",
  abstract  = "Over the past decade, the field of natural language processing
               has developed a wide array of computational methods for reasoning
               about narrative, including summarization, commonsense inference,
               and event detection. While this work has brought an important
               empirical lens for examining narrative, it is by and large
               divorced from the large body of theoretical work on narrative
               within the humanities, social and cognitive sciences. In this
               position paper, we introduce the dominant theoretical frameworks
               to the NLP community, situate current research in NLP within
               distinct narratological traditions, and argue that linking
               computational work in NLP to theory opens up a range of new
               empirical questions that would both help advance our
               understanding of narrative and open up new practical
               applications.",
  month     =  nov,
  year      =  2021
}

@MISC{Kane2015-cr,
  title     = "National Center for Teacher Effectiveness Main Study",
  author    = "Kane, Thomas and Hill, Heather and Staiger, Douglas",
  publisher = "ICPSR - Interuniversity Consortium for Political and Social
               Research",
  abstract  = "The National Center for Teacher Effectiveness Main Study (NCTE)
               encompasses three years of data collection and observations of
               math instruction in approximately 50 schools and 300 classrooms.
               Data were collected from classroom observations, student
               assessments, and teacher surveys. Teacher background information
               includes number of years of experience, education, race, and
               gender. Student respondent demographic and household information
               includes race, gender, household makeup, free and reduced lunch
               status, English proficiency, number of books in the household,
               and number of rooms in the home.",
  year      =  2015
}

@ARTICLE{Venkit2023-uj,
  title         = "Nationality Bias in Text Generation",
  author        = "Venkit, Pranav Narayanan and Gautam, Sanjana and
                   Panchanadikar, Ruchi and Huang, Ting-Hao 'kenneth' and
                   Wilson, Shomir",
  journal       = "arXiv [cs.CL]",
  abstract      = "Little attention is placed on analyzing nationality bias in
                   language models, especially when nationality is highly used
                   as a factor in increasing the performance of social NLP
                   models. This paper examines how a text generation model,
                   GPT-2, accentuates pre-existing societal biases about
                   country-based demonyms. We generate stories using GPT-2 for
                   various nationalities and use sensitivity analysis to explore
                   how the number of internet users and the country's economic
                   status impacts the sentiment of the stories. To reduce the
                   propagation of biases through large language models (LLM), we
                   explore the debiasing method of adversarial triggering. Our
                   results show that GPT-2 demonstrates significant bias against
                   countries with lower internet users, and adversarial
                   triggering effectively reduces the same.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hoyle2023-we,
  title         = "Natural Language Decompositions of Implicit Content Enable
                   Better Text Representations",
  author        = "Hoyle, Alexander and Sarkar, Rupak and Goel, Pranav and
                   Resnik, Philip",
  journal       = "arXiv [cs.CL]",
  abstract      = "When people interpret text, they rely on inferences that go
                   beyond the observed language itself. Inspired by this
                   observation, we introduce a method for the analysis of text
                   that takes implicitly communicated content explicitly into
                   account. We use a large language model to produce sets of
                   propositions that are inferentially related to the text that
                   has been observed, then validate the plausibility of the
                   generated content via human judgments. Incorporating these
                   explicit representations of implicit content proves useful in
                   multiple problem settings that involve the human
                   interpretation of utterances: assessing the similarity of
                   arguments, making sense of a body of opinion data, and
                   modeling legislative behavior. Our results suggest that
                   modeling the meanings behind observed language, rather than
                   the literal text alone, is a valuable direction for NLP and
                   particularly its applications to social science.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Feldman2020-ms,
  title         = "Navigating Human Language Models with Synthetic Agents",
  author        = "Feldman, Philip and Bucchiarone, Antonio",
  journal       = "arXiv [cs.AI]",
  abstract      = "Modern natural language models such as the GPT-2/GPT-3
                   contain tremendous amounts of information about human belief
                   in a consistently testable form. If these models could be
                   shown to accurately reflect the underlying beliefs of the
                   human beings that produced the data used to train these
                   models, then such models become a powerful sociological tool
                   in ways that are distinct from traditional methods, such as
                   interviews and surveys. In this study, We train a version of
                   the GPT-2 on a corpora of historical chess games, and then
                   ``launch'' clusters of synthetic agents into the model, using
                   text strings to create context and orientation. We compare
                   the trajectories contained in the text generated by the
                   agents/model and compare that to the known ground truth of
                   the chess board, move legality, and historical patterns of
                   play. We find that the percentages of moves by piece using
                   the model are substantially similar from human patterns. We
                   further find that the model creates an accurate latent
                   representation of the chessboard, and that it is possible to
                   plot trajectories of legal moves across the board using this
                   knowledge.",
  month         =  aug,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Sap2022-mc,
  title         = "Neural Theory-of-Mind? On the Limits of Social Intelligence
                   in Large {LMs}",
  author        = "Sap, Maarten and LeBras, Ronan and Fried, Daniel and Choi,
                   Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Social intelligence and Theory of Mind (ToM), i.e., the
                   ability to reason about the different mental states, intents,
                   and reactions of all people involved, allow humans to
                   effectively navigate and understand everyday social
                   interactions. As NLP systems are used in increasingly complex
                   social situations, their ability to grasp social dynamics
                   becomes crucial. In this work, we examine the open question
                   of social intelligence and Theory of Mind in modern NLP
                   systems from an empirical and theory-based perspective. We
                   show that one of today's largest language models (GPT-3;
                   Brown et al., 2020) lacks this kind of social intelligence
                   out-of-the box, using two tasks: SocialIQa (Sap et al.,
                   2019), which measures models' ability to understand intents
                   and reactions of participants of social interactions, and
                   ToMi (Le et al., 2019), which measures whether models can
                   infer mental states and realities of participants of
                   situations. Our results show that models struggle
                   substantially at these Theory of Mind tasks, with
                   well-below-human accuracies of 55\% and 60\% on SocialIQa and
                   ToMi, respectively. To conclude, we draw on theories from
                   pragmatics to contextualize this shortcoming of large
                   language models, by examining the limitations stemming from
                   their data, neural architecture, and training paradigms.
                   Challenging the prevalent narrative that only scale is
                   needed, we posit that person-centric NLP approaches might be
                   more effective towards neural Theory of Mind.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Danescu-Niculescu-Mizil2013-em,
  title     = "No country for old members: user lifecycle and linguistic change
               in online communities",
  author    = "Danescu-Niculescu-Mizil, Cristian and West, Robert and Jurafsky,
               Dan and Leskovec, Jure and Potts, Christopher",
  booktitle = "Proceedings of the 22nd international conference on World Wide
               Web",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "307--318",
  abstract  = "Vibrant online communities are in constant flux. As members join
               and depart, the interactional norms evolve, stimulating further
               changes to the membership and its social dynamics. Linguistic
               change --- in the sense of innovation that becomes accepted as
               the norm --- is essential to this dynamic process: it both
               facilitates individual expression and fosters the emergence of a
               collective identity.We propose a framework for tracking
               linguistic change as it happens and for understanding how
               specific users react to these evolving norms. By applying this
               framework to two large online communities we show that users
               follow a determined two-stage lifecycle with respect to their
               susceptibility to linguistic change: a linguistically innovative
               learning phase in which users adopt the language of the community
               followed by a conservative phase in which users stop changing and
               the evolving community norms pass them by.Building on this
               observation, we show how this framework can be used to detect,
               early in a user's career, how long she will stay active in the
               community. Thus, this work has practical significance for those
               who design and maintain online communities. It also yields new
               theoretical insights into the evolution of linguistic norms and
               the complex interplay between community-level and
               individual-level linguistic change.",
  series    = "WWW '13",
  month     =  may,
  year      =  2013,
  keywords  = "reviews, linguistic change, social influence, user abandonment,
               conventions, language, community norms, lifecycle"
}

@MISC{noauthor_undated-yw,
  title        = "[No title]",
  howpublished = "\url{https://www.researchgate.net/profile/Harnoor-Dhingra/publication/372074898\_Queer\_People\_are\_People\_First\_Deconstructing\_Sexual\_Identity\_Stereotypes\_in\_Large\_Language\_Models/links/651cea4ed717ef1293c8fac9/Queer-People-are-People-First-Deconstructing-Sexual-Identity-Stereotypes-in-Large-Language-Models.pdf}",
  note         = "Accessed: 2024-1-8"
}

@ARTICLE{Fung2022-wd,
  title         = "{NormSAGE}: Multi-Lingual Multi-Cultural Norm Discovery from
                   Conversations On-the-Fly",
  author        = "Fung, Yi R and Chakraborty, Tuhin and Guo, Hao and Rambow,
                   Owen and Muresan, Smaranda and Ji, Heng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Norm discovery is important for understanding and reasoning
                   about the acceptable behaviors and potential violations in
                   human communication and interactions. We introduce NormSage,
                   a framework for addressing the novel task of
                   conversation-grounded multi-lingual, multi-cultural norm
                   discovery, based on language model prompting and
                   self-verification. NormSAGE leverages the expressiveness and
                   implicit knowledge of the pretrained GPT-3 language model
                   backbone, to elicit knowledge about norms through directed
                   questions representing the norm discovery task and
                   conversation context. It further addresses the risk of
                   language model hallucination with a self-verification
                   mechanism ensuring that the norms discovered are correct and
                   are substantially grounded to their source conversations.
                   Evaluation results show that our approach discovers
                   significantly more relevant and insightful norms for
                   conversations on-the-fly compared to baselines (>10+\% in
                   Likert scale rating). The norms discovered from Chinese
                   conversation are also comparable to the norms discovered from
                   English conversation in terms of insightfulness and
                   correctness (<3\% difference). In addition, the
                   culture-specific norms are promising quality, allowing for
                   80\% accuracy in culture pair human identification. Finally,
                   our grounding process in norm discovery self-verification can
                   be extended for instantiating the adherence and violation of
                   any norm for a given conversation on-the-fly, with
                   explainability and transparency. NormSAGE achieves an AUC of
                   95.4\% in grounding, with natural language explanation
                   matching human-written quality.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Truong2022-fr,
  title         = "Not another Negation Benchmark: The {NaN}-{NLI} Test Suite
                   for Sub-clausal Negation",
  author        = "Truong, Thinh Hung and Otmakhova, Yulia and Baldwin, Timothy
                   and Cohn, Trevor and Lau, Jey Han and Verspoor, Karin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Negation is poorly captured by current language models,
                   although the extent of this problem is not widely understood.
                   We introduce a natural language inference (NLI) test suite to
                   enable probing the capabilities of NLP methods, with the aim
                   of understanding sub-clausal negation. The test suite
                   contains premise--hypothesis pairs where the premise contains
                   sub-clausal negation and the hypothesis is constructed by
                   making minimal modifications to the premise in order to
                   reflect different possible interpretations. Aside from
                   adopting standard NLI labels, our test suite is
                   systematically constructed under a rigorous linguistic
                   framework. It includes annotation of negation types and
                   constructions grounded in linguistic theory, as well as the
                   operations used to construct hypotheses. This facilitates
                   fine-grained analysis of model performance. We conduct
                   experiments using pre-trained language models to demonstrate
                   that our test suite is more challenging than existing
                   benchmarks focused on negation, and show how our annotation
                   supports a deeper understanding of the current NLI
                   capabilities in terms of negation and quantification.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lorge2023-gd,
  title         = "Not wacky vs. definitely wacky: A study of scalar adverbs in
                   pretrained language models",
  author        = "Lorge, Isabelle and Pierrehumbert, Janet",
  journal       = "arXiv [cs.CL]",
  abstract      = "Vector space models of word meaning all share the assumption
                   that words occurring in similar contexts have similar
                   meanings. In such models, words that are similar in their
                   topical associations but differ in their logical force tend
                   to emerge as semantically close, creating well-known
                   challenges for NLP applications that involve logical
                   reasoning. Modern pretrained language models, such as BERT,
                   RoBERTa and GPT-3 hold the promise of performing better on
                   logical tasks than classic static word embeddings. However,
                   reports are mixed about their success. In the current paper,
                   we advance this discussion through a systematic study of
                   scalar adverbs, an under-explored class of words with strong
                   logical force. Using three different tasks, involving both
                   naturalistic social media data and constructed examples, we
                   investigate the extent to which BERT, RoBERTa, GPT-2 and
                   GPT-3 exhibit general, human-like, knowledge of these common
                   words. We ask: 1) Do the models distinguish amongst the three
                   semantic categories of MODALITY, FREQUENCY and DEGREE? 2) Do
                   they have implicit representations of full scales from
                   maximally negative to maximally positive? 3) How do word
                   frequency and contextual factors impact model performance? We
                   find that despite capturing some aspects of logical meaning,
                   the models fall far short of human performance.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Ravfogel2020-pa,
  title     = "Null it out: Guarding protected attributes by iterative nullspace
               projection",
  author    = "Ravfogel, Shauli and Elazar, Yanai and Gonen, Hila and Twiton,
               Michael and Goldberg, Yoav",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  abstract  = "This work presents Iterative Null-space Projection (INLP), a
               novel method for removing information from neural representations
               based on repeated training of linear classifiers that predict a
               certain property the authors aim to remove, followed by
               projection of the representations on their null-space. The
               ability to control for the kinds of information encoded in neural
               representation has a variety of use cases, especially in light of
               the challenge of interpreting these models. We present Iterative
               Null-space Projection (INLP), a novel method for removing
               information from neural representations. Our method is based on
               repeated training of linear classifiers that predict a certain
               property we aim to remove, followed by projection of the
               representations on their null-space. By doing so, the classifiers
               become oblivious to that target property, making it hard to
               linearly separate the data according to it. While applicable for
               multiple uses, we evaluate our method on bias and fairness
               use-cases, and show that our method is able to mitigate bias in
               word embeddings, as well as to increase fairness in a setting of
               multi-class classification.",
  month     =  apr,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Zurbriggen2013-su,
  title     = "Objectification, Self-Objectification, and Societal Change",
  author    = "Zurbriggen, Eileen L",
  journal   = "Journal of Social and Political Psychology",
  publisher = "PsychOpen",
  volume    =  1,
  number    =  1,
  pages     = "188--215",
  abstract  = "This review focuses on the ways in which the objectification of
               individuals and groups of people, as well as the
               self-objectification that typically develops from such treatment,
               is implicated in positive and negative societal change. Four
               areas are reviewed: (a) objectification (including
               dehumanization, infra-humanization, dehumanized perception,
               sexualization, and colonialism), (b) self-objectification
               (including double consciousness, internalized oppression, and
               colonial mentality), (c) genocide and mass violence, and (c)
               collective action. After reviewing theories in each area, a set
               of underlying constructs is presented, organized under
               higher-order categories. Finally, connections between
               objectification and genocide perpetration, as well as between
               self-objectification and collective action, are described. It is
               concluded that the objectification of other people contributes to
               societal change that runs counter to principles of equality and
               respect for others, threatens civil rights, and ultimately can
               result in genocide or mass killings. Furthermore,
               self-objectification impairs the ability of oppressed groups to
               act collectively on their own behalf. In contrast, the process of
               decolonization supports collective action and positive societal
               change, in part because it liberates oppressed people from
               self-objectification.",
  month     =  dec,
  year      =  2013,
  keywords  = "objectification; dehumanization; infra-humanization; internalized
               oppression; colonization; collective action; social change;
               activism; genocide; mass killings; discrimination; oppression",
  language  = "en"
}

@ARTICLE{Moradi2011-ib,
  title     = "Objectification Theory: Areas of Promise and Refinement",
  author    = "Moradi, Bonnie",
  journal   = "Couns. Psychol.",
  publisher = "SAGE Publications Inc",
  volume    =  39,
  number    =  1,
  pages     = "153--163",
  abstract  = "This article elaborates on three themes related to Szymanski,
               Moffitt, and Carr?s major contribution aims. First, the article
               describes the promise of objectification theory as a grounding
               framework in research and practice, outlining how this theory
               integrates key aspects of several other important theoretical
               models. Second, this article suggests areas for theoretical
               refinement and clarification related to the conceptualization and
               operationalization of self-objectification, sexually objectifying
               environments, and the mechanisms linking sexual objectification
               with substance use. Third, this article offers considerations
               regarding the state of objectification theory?based intervention
               recommendations. The article concludes with a discussion of
               potential roles of counseling psychologists in advancing
               research, practice, and advocacy informed by objectification
               theory.",
  month     =  jan,
  year      =  2011
}

@ARTICLE{Hornborg2021-ch,
  title     = "Objects don't have desires: Toward an anthropology of technology
               beyond anthropomorphism",
  author    = "Hornborg, Alf",
  journal   = "Am. Anthropol.",
  publisher = "Wiley",
  volume    =  123,
  number    =  4,
  pages     = "753--766",
  abstract  = "ABSTRACT“Postdualist” approaches, such as the material turn in
               the humanities and social sciences, represent understandable
               reactions to the humanist and idealist traditions in Western
               thought, but tend to be deluded by a focus on individual
               artifacts rather than on the global, material relations on which
               their existence depends. The attribution of agency and even
               desires to abiotic objects, championed by posthumanist
               researchers such as Bruno Latour and Donna Haraway, is cognate to
               the category mistakes recurrently identified by social theorists
               as fetishism and anthropomorphism. Paradoxically, given their
               subversive ambitions, proponents of the new concern with
               materiality and artifactual agency are offering an ideology that
               ultimately buttresses the capitalist world order by ignoring the
               materiality of world trade and the causality inherent in the
               artifact of money. The concerns with distributed agency also tend
               to displace responsibility and accountability from humans to
               artifacts. Moreover, in converging with a deep genealogy of ideas
               that blur the boundary between nature and artifice, the material
               turn depoliticizes technology by naturalizing it. The article
               proposes a new anthropology of technology that acknowledges the
               reliance of modern technology on asymmetric global resource flows
               orchestrated by money and the fictive reciprocity of market
               prices. [material turn, posthumanism, fetishism, technology,
               postdualism]",
  month     =  dec,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Duede2024-bf,
  title         = "Oil \& Water? Diffusion of {AI} Within and Across Scientific
                   Fields",
  author        = "Duede, Eamon and Dolan, William and Bauer, André and Foster,
                   Ian and Lakhani, Karim",
  journal       = "arXiv [cs.DL]",
  abstract      = "This study empirically investigates claims of the increasing
                   ubiquity of artificial intelligence (AI) within roughly 80
                   million research publications across 20 diverse scientific
                   fields, by examining the change in scholarly engagement with
                   AI from 1985 through 2022. We observe exponential growth,
                   with AI-engaged publications increasing approximately
                   thirteenfold (13x) across all fields, suggesting a dramatic
                   shift from niche to mainstream. Moreover, we provide the
                   first empirical examination of the distribution of AI-engaged
                   publications across publication venues within individual
                   fields, with results that reveal a broadening of AI
                   engagement within disciplines. While this broadening
                   engagement suggests a move toward greater disciplinary
                   integration in every field, increased ubiquity is associated
                   with a semantic tension between AI-engaged research and more
                   traditional disciplinary research. Through an analysis of
                   tens of millions of document embeddings, we observe a complex
                   interplay between AI-engaged and non-AI-engaged research
                   within and across fields, suggesting that increasing ubiquity
                   is something of an oil-and-water phenomenon -- AI-engaged
                   work is spreading out over fields, but not mixing well with
                   non-AI-engaged work.",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DL"
}

@MISC{noauthor_undated-cr,
  title = "{OmiWinantCh4}.pdf"
}

@ARTICLE{Epley2007-xo,
  title     = "On seeing human: a three-factor theory of anthropomorphism",
  author    = "Epley, Nicholas and Waytz, Adam and Cacioppo, John T",
  journal   = "Psychol. Rev.",
  publisher = "psycnet.apa.org",
  volume    =  114,
  number    =  4,
  pages     = "864--886",
  abstract  = "Anthropomorphism describes the tendency to imbue the real or
               imagined behavior of nonhuman agents with humanlike
               characteristics, motivations, intentions, or emotions. Although
               surprisingly common, anthropomorphism is not invariant. This
               article describes a theory to explain when people are likely to
               anthropomorphize and when they are not, focused on three
               psychological determinants--the accessibility and applicability
               of anthropocentric knowledge (elicited agent knowledge), the
               motivation to explain and understand the behavior of other agents
               (effectance motivation), and the desire for social contact and
               affiliation (sociality motivation). This theory predicts that
               people are more likely to anthropomorphize when anthropocentric
               knowledge is accessible and applicable, when motivated to be
               effective social agents, and when lacking a sense of social
               connection to other humans. These factors help to explain why
               anthropomorphism is so variable; organize diverse research; and
               offer testable predictions about dispositional, situational,
               developmental, and cultural influences on anthropomorphism.
               Discussion addresses extensions of this theory into the specific
               psychological processes underlying anthropomorphism, applications
               of this theory into robotics and human-computer interaction, and
               the insights offered by this theory into the inverse process of
               dehumanization.",
  month     =  oct,
  year      =  2007,
  language  = "en"
}

@INPROCEEDINGS{Sultan2020-ak,
  title     = "On the importance of diversity in question generation for {QA}",
  author    = "Sultan, Md Arafat and Chandel, Shubham and Astudillo, Ramón
               Fernandez and Castelli, Vittorio",
  booktitle = "Proceedings of the 58th Annual Meeting of the Association for
               Computational Linguistics",
  publisher = "aclanthology.org",
  pages     = "5651--5656",
  abstract  = "Automatic question generation (QG) has shown promise as a source
               of synthetic training data for question answering (QA). In this
               paper we ask: Is textual diversity in QG beneficial for
               downstream QA? Using top-p nucleus sampling to derive samples
               from a transformer- based question generator, we show that
               diversity-promoting QG indeed provides better QA training than
               likelihood maximization approaches such as beam search. We also
               show that standard QG evaluation metrics such as BLEU, ROUGE and
               METEOR are inversely …",
  year      =  2020
}

@INPROCEEDINGS{Cabello2023-ad,
  title     = "On the Independence of Association Bias and Empirical Fairness in
               Language Models",
  author    = "Cabello, Laura and Jørgensen, Anna Katrine and Søgaard, Anders",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "370--378",
  abstract  = "The societal impact of pre-trained language models has prompted
               researchers to probe them for strong associations between
               protected attributes and value-loaded terms, from slur to
               prestigious job titles. Such work is said to probe models for
               bias or fairness—or such probes ‘into representational biases’
               are said to be ‘motivated by fairness’—suggesting an intimate
               connection between bias and fairness. We provide conceptual
               clarity by distinguishing between association biases [11] and
               empirical fairness [56] and show the two can be independent. Our
               main contribution, however, is showing why this should not come
               as a surprise. To this end, we first provide a thought
               experiment, showing how association bias and empirical fairness
               can be completely orthogonal. Next, we provide empirical evidence
               that there is no correlation between bias metrics and fairness
               metrics across the most widely used language models. Finally, we
               survey the sociological and psychological literature and show how
               this literature provides ample support for expecting these
               metrics to be uncorrelated.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "Representational Bias, Fairness, Natural Language Processing"
}

@ARTICLE{Cao2022-uw,
  title         = "On the Intrinsic and Extrinsic Fairness Evaluation Metrics
                   for Contextualized Language Representations",
  author        = "Cao, Yang Trista and Pruksachatkun, Yada and Chang, Kai-Wei
                   and Gupta, Rahul and Kumar, Varun and Dhamala, Jwala and
                   Galstyan, Aram",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multiple metrics have been introduced to measure fairness in
                   various natural language processing tasks. These metrics can
                   be roughly categorized into two categories: 1)
                   \emph{extrinsic metrics} for evaluating fairness in
                   downstream applications and 2) \emph{intrinsic metrics} for
                   estimating fairness in upstream contextualized language
                   representation models. In this paper, we conduct an extensive
                   correlation study between intrinsic and extrinsic metrics
                   across bias notions using 19 contextualized language models.
                   We find that intrinsic and extrinsic metrics do not
                   necessarily correlate in their original setting, even when
                   correcting for metric misalignments, noise in evaluation
                   datasets, and confounding factors such as experiment
                   configuration for extrinsic metrics. \%al",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Talat2022-hq,
  title     = "On the machine learning of ethical judgments from natural
               language",
  author    = "Talat, Zeerak and Blix, Hagen and Valvoda, Josef and Ganesh, Maya
               Indira and Cotterell, Ryan and Williams, Adina",
  publisher = "ETH Zurich",
  abstract  = "Ethics is one of the longest standing intellectual endeavors of
               humanity. In recent years, the fields of AI and NLP have
               attempted to address ethical issues of harmful outcomes in
               machine learning systems that are made to interface with humans.
               One recent approach in this vein is the construction of NLP
               morality models that can take in arbitrary text and output a
               moral judgment about the situation described. In this work, we
               offer a critique of such NLP methods for automating ethical
               decision-making. Through an audit of recent work on computational
               approaches for predicting morality, we examine the broader issues
               that arise from such efforts. We conclude with a discussion of
               how machine ethics could usefully proceed in NLP, by focusing on
               current and near-future uses of technology, in a way that centers
               around transparency, democratic values, and allows for
               straightforward accountability.",
  year      =  2022
}

@ARTICLE{Antverg2021-vm,
  title     = "On the pitfalls of analyzing individual neurons in language
               models",
  author    = "Antverg, O and Belinkov, Y",
  journal   = "arXiv preprint arXiv:2110.07483",
  publisher = "arxiv.org",
  abstract  = "While many studies have shown that linguistic information is
               encoded in hidden word representations, few have studied
               individual neurons, to show how and in which neurons it …",
  year      =  2021
}

@ARTICLE{Montag2021-pw,
  title    = "On the Psychology of {TikTok} Use: A First Glimpse From Empirical
              Findings",
  author   = "Montag, Christian and Yang, Haibo and Elhai, Jon D",
  journal  = "Front Public Health",
  volume   =  9,
  pages    =  641673,
  abstract = "TikTok (in Chinese: DouYin; formerly known as musical.ly)
              currently represents one of the most successful Chinese social
              media applications in the world. Since its founding in September
              2016, TikTok has seen widespread distribution, in particular,
              attracting young users to engage in viewing, creating, and
              commenting on ``LipSync-Videos'' on the app. Despite its success
              in terms of user numbers, psychological studies aiming at an
              understanding of TikTok use are scarce. This narrative review
              provides a comprehensive overview on the small empirical
              literature available thus far. In particular, insights from uses
              and gratification theory in the realm of TikTok are highlighted,
              and we also discuss aspects of the TikTok platform design. Given
              the many unexplored research questions related to TikTok use, it
              is high time to strengthen research efforts to better understand
              TikTok use and whether certain aspects of its use result in
              detrimental behavioral effects. In light of user characteristics
              of the TikTok platform, this research is highly relevant because
              TikTok users are often adolescents and therefore from a group of
              potentially vulnerable individuals.",
  month    =  mar,
  year     =  2021,
  keywords = "DouYin; TikTok; musical.ly; personality; problematic social media
              use; social media; social media addiction; uses and gratification",
  language = "en"
}

@ARTICLE{Ravichander2020-jc,
  title     = "On the systematicity of probing contextualized word
               representations: The case of hypernymy in {BERT}",
  author    = "Ravichander, A and Hovy, E and Suleman, K and {others}",
  journal   = "Proceedings of the",
  publisher = "aclanthology.org",
  abstract  = "Contextualized word representations have become a driving force
               in NLP, motivating widespread interest in understanding their
               capabilities and the mechanisms by which they …",
  year      =  2020
}

@ARTICLE{Jin2020-gw,
  title         = "On Transferability of Bias Mitigation Effects in Language
                   Model Fine-Tuning",
  author        = "Jin, Xisen and Barbieri, Francesco and Kennedy, Brendan and
                   Davani, Aida Mostafazadeh and Neves, Leonardo and Ren, Xiang",
  journal       = "arXiv [cs.CL]",
  abstract      = "Fine-tuned language models have been shown to exhibit biases
                   against protected groups in a host of modeling tasks such as
                   text classification and coreference resolution. Previous
                   works focus on detecting these biases, reducing bias in data
                   representations, and using auxiliary training objectives to
                   mitigate bias during fine-tuning. Although these techniques
                   achieve bias reduction for the task and domain at hand, the
                   effects of bias mitigation may not directly transfer to new
                   tasks, requiring additional data collection and customized
                   annotation of sensitive attributes, and re-evaluation of
                   appropriate fairness metrics. We explore the feasibility and
                   benefits of upstream bias mitigation (UBM) for reducing bias
                   on downstream tasks, by first applying bias mitigation to an
                   upstream model through fine-tuning and subsequently using it
                   for downstream fine-tuning. We find, in extensive experiments
                   across hate speech detection, toxicity detection, occupation
                   prediction, and coreference resolution tasks over various
                   bias factors, that the effects of UBM are indeed transferable
                   to new downstream tasks or domains via fine-tuning, creating
                   less biased downstream models than directly fine-tuning on
                   the downstream task or transferring from a vanilla upstream
                   model. Though challenges remain, we show that UBM promises
                   more efficient and accessible bias mitigation in LM
                   fine-tuning.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lucy2023-qj,
  title         = "``One-Size-Fits-All''? Examining Expectations around What
                   Constitute ``Fair'' or ``Good'' {NLG} System Behaviors",
  author        = "Lucy, Li and Blodgett, Su Lin and Shokouhi, Milad and
                   Wallach, Hanna and Olteanu, Alexandra",
  journal       = "arXiv [cs.CL]",
  abstract      = "Fairness-related assumptions about what constitute
                   appropriate NLG system behaviors range from invariance, where
                   systems are expected to behave identically for social groups,
                   to adaptation, where behaviors should instead vary across
                   them. To illuminate tensions around invariance and
                   adaptation, we conduct five case studies, in which we perturb
                   different types of identity-related language features (names,
                   roles, locations, dialect, and style) in NLG system inputs.
                   Through these cases studies, we examine people's expectations
                   of system behaviors, and surface potential caveats of these
                   contrasting yet commonly held assumptions. We find that
                   motivations for adaptation include social norms, cultural
                   differences, feature-specific information, and accommodation;
                   in contrast, motivations for invariance include perspectives
                   that favor prescriptivism, view adaptation as unnecessary or
                   too difficult for NLG systems to do appropriately, and are
                   wary of false assumptions. Our findings highlight open
                   challenges around what constitute ``fair'' or ``good'' NLG
                   system behaviors.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Widder2023-kn,
  title    = "Open (For Business): Big Tech, Concentrated Power, and the
              Political Economy of Open {AI}",
  author   = "Widder, David Gray and West, Sarah and Whittaker, Meredith",
  abstract = "This paper examines ‘open’ AI in the context of recent attention
              to open and open source AI systems. We find that the terms ‘open’
              and ‘open source’ are used in confusing and diverse ways, often
              constituting more aspiration or marketing than technical
              descriptor, and frequently blending concepts from both open source
              software and open science. This complicates an already complex
              landscape, in which there is currently no agreed on definition of
              ‘open’ in the context of AI, and as such the term is being applied
              to widely divergent offerings with little reference to a stable
              descriptor. So, what exactly is ‘open’ about ‘open’ AI, and what
              does ‘open’ AI enable? To better answer these questions we begin
              this paper by looking at the various resources required to create
              and deploy AI systems, alongside the components that comprise
              these systems. We do this with an eye to which of these can, or
              cannot, be made open to scrutiny, reuse, and extension. What does
              ‘open’ mean in practice, and what are its limits in the context of
              AI? We find that while a handful of maximally open AI systems
              exist, which offer intentional and extensive transparency,
              reusability, and extensibility– the resources needed to build AI
              from scratch, and to deploy large AI systems at scale, remain
              ‘closed’—available only to those with significant (almost always
              corporate) resources. From here, we zoom out and examine the
              history of open source, its cleave from free software in the mid
              1990s, and the contested processes by which open source has been
              incorporated into, and instrumented by, large tech corporations.
              As a current day example of the overbroad and ill-defined use of
              the term by tech companies, we look at ‘open’ in the context of
              OpenAI the company. We trace its moves from a humanity-focused
              nonprofit to a for-profit partnered with Microsoft, and its
              shifting position on ‘open’ AI. Finally, we examine the current
              discourse around ‘open’ AI–looking at how the term and the
              (mis)understandings about what ‘open’ enables are being deployed
              to shape the public’s and policymakers’ understanding about AI,
              its capabilities, and the power of the AI industry. In particular,
              we examine the arguments being made for and against ‘open’ and
              open source AI, who’s making them, and how they are being deployed
              in the debate over AI regulation. Taken together, we find that
              ‘open’ AI can, in its more maximal instantiations, provide
              transparency, reusability, and extensibility that can enable third
              parties to deploy and build on top of powerful off-the-shelf AI
              models. These maximalist forms of ‘open’ AI can also allow some
              forms of auditing and oversight. But even the most open of ‘open’
              AI systems do not, on their own, ensure democratic access to or
              meaningful competition in AI, nor does openness alone solve the
              problem of oversight and scrutiny. While we recognize that there
              is a vibrant community of earnest contributors building and
              contributing to ‘open’ AI efforts in the name of expanding access
              and insight, we also find that marketing around openness and
              investment in (somewhat) open AI systems is being leveraged by
              powerful companies to bolster their positions in the face of
              growing interest in AI regulation. And that some companies have
              moved to embrace ‘open’ AI as a mechanism to entrench dominance,
              using the rhetoric of ‘open’ AI to expand market power while
              investing in ‘open’ AI efforts in ways that allow them to set
              standards of development while benefiting from the free labor of
              open source contributors.",
  month    =  aug,
  year     =  2023,
  keywords = "artificial intelligence, open source, political economy, policy,
              AI, Big Tech, privacy, data, competition"
}

@ARTICLE{Haller2023-xv,
  title         = "{OpinionGPT}: Modelling Explicit Biases in Instruction-Tuned
                   {LLMs}",
  author        = "Haller, Patrick and Aynetdinov, Ansar and Akbik, Alan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Instruction-tuned Large Language Models (LLMs) have recently
                   showcased remarkable ability to generate fitting responses to
                   natural language instructions. However, an open research
                   question concerns the inherent biases of trained models and
                   their responses. For instance, if the data used to tune an
                   LLM is dominantly written by persons with a specific
                   political bias, we might expect generated answers to share
                   this bias. Current research work seeks to de-bias such
                   models, or suppress potentially biased answers. With this
                   demonstration, we take a different view on biases in
                   instruction-tuning: Rather than aiming to suppress them, we
                   aim to make them explicit and transparent. To this end, we
                   present OpinionGPT, a web demo in which users can ask
                   questions and select all biases they wish to investigate. The
                   demo will answer this question using a model fine-tuned on
                   text representing each of the selected biases, allowing
                   side-by-side comparison. To train the underlying model, we
                   identified 11 different biases (political, geographic,
                   gender, age) and derived an instruction-tuning corpus in
                   which each answer was written by members of one of these
                   demographics. This paper presents OpinionGPT, illustrates how
                   we trained the bias-aware model and showcases the web
                   application (available at
                   https://opiniongpt.informatik.hu-berlin.de).",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Weick2005-os,
  title     = "Organizing and the Process of Sensemaking",
  author    = "Weick, Karl E and Sutcliffe, Kathleen M and Obstfeld, David",
  journal   = "Organization Science",
  publisher = "INFORMS",
  volume    =  16,
  number    =  4,
  pages     = "409--421",
  abstract  = "Sensemaking involves turning circumstances into a situation that
               is comprehended explicitly in words and that serves as a
               springboard into action. In this paper we take the position that
               the concept of sensemaking fills important gaps in organizational
               theory. The seemingly transient nature of sensemaking belies its
               central role in the determination of human behavior, whether
               people are acting in formal organizations or elsewhere.
               Sensemaking is central because it is the primary site where
               meanings materialize that inform and constrain identity and
               action. The purpose of this paper is to take stock of the concept
               of sensemaking. We do so by pinpointing central features of
               sensemaking, some of which have been explicated but neglected,
               some of which have been assumed but not made explicit, some of
               which have changed in significance over time, and some of which
               have been missing all along or have gone awry. We sense joint
               enthusiasm to restate sensemaking in ways that make it more
               future oriented, more action oriented, more macro, more closely
               tied to organizing, meshed more boldly with identity, more
               visible, more behaviorally defined, less sedentary and backward
               looking, more infused with emotion and with issues of sensegiving
               and persuasion. These key enhancements provide a foundation upon
               which to build future studies that can strengthen the sensemaking
               perspective.",
  month     =  aug,
  year      =  2005
}

@INPROCEEDINGS{Mim2020-gp,
  title     = "Others' Images: Online Social Media, Architectural
               Improvisations, and Spatial Marginalization in Bangladesh",
  author    = "Mim, Nusrat Jahan and Ahmed, Syed Ishtiaque",
  booktitle = "Proceedings of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--14",
  abstract  = "This paper joins the growing body of work in postcolonial
               computing in HCI, and critically examines the impacts of online
               social media on the urban architecture in the Global South. Based
               on our nine-month long ethnography at eight residential areas in
               Dhaka, Bangladesh, this paper reports how online fame drives
               local users to produce digital images of their houses mimicking
               various Western standards, which in turn, brings changes to the
               organization, aesthetics, and functions of domestic spaces. This
               paper also describes how such digital image mediated
               transformations to local architecture are diminishing traditional
               spaces, altering their usual functions, and limiting the movement
               of many women inside their home. Drawing from a rich body of
               literature in postcolonialism, critical image theory,
               architecture, and Islamic feminism, we explain how these
               practices demonstrate a subaltern experience of using social
               media in the Global South. We further discuss design implications
               to both HCI and architecture to address these issues, and connect
               our findings to the broader agendas of HCI around social justice
               and global development.",
  series    = "CHI '20",
  month     =  apr,
  year      =  2020,
  keywords  = "image sharing, architecture, social media, global south"
}

@ARTICLE{Argyle2022-ts,
  title         = "Out of One, Many: Using Language Models to Simulate Human
                   Samples",
  author        = "Argyle, Lisa P and Busby, Ethan C and Fulda, Nancy and
                   Gubler, Joshua and Rytting, Christopher and Wingate, David",
  journal       = "arXiv [cs.LG]",
  abstract      = "We propose and explore the possibility that language models
                   can be studied as effective proxies for specific human
                   sub-populations in social science research. Practical and
                   research applications of artificial intelligence tools have
                   sometimes been limited by problematic biases (such as racism
                   or sexism), which are often treated as uniform properties of
                   the models. We show that the ``algorithmic bias'' within one
                   such tool -- the GPT-3 language model -- is instead both
                   fine-grained and demographically correlated, meaning that
                   proper conditioning will cause it to accurately emulate
                   response distributions from a wide variety of human
                   subgroups. We term this property ``algorithmic fidelity'' and
                   explore its extent in GPT-3. We create ``silicon samples'' by
                   conditioning the model on thousands of socio-demographic
                   backstories from real human participants in multiple large
                   surveys conducted in the United States. We then compare the
                   silicon and human samples to demonstrate that the information
                   contained in GPT-3 goes far beyond surface similarity. It is
                   nuanced, multifaceted, and reflects the complex interplay
                   between ideas, attitudes, and socio-cultural context that
                   characterize human attitudes. We suggest that language models
                   with sufficient algorithmic fidelity thus constitute a novel
                   and powerful tool to advance understanding of humans and
                   society across a variety of disciplines.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Zarouali2021-gy,
  title     = "Overcoming polarization with chatbot news? Investigating the
               impact of news content containing opposing views on agreement and
               credibility",
  author    = "Zarouali, Brahim and Makhortykh, Mykola and Bastian, Mariella and
               Araujo, Theo",
  journal   = "Eur. J. Commun.",
  publisher = "SAGE Publications",
  volume    =  36,
  number    =  1,
  pages     = "53--68",
  abstract  = "Chatbots are a burgeoning opportunity for news media outlets to
               disseminate their content in a conversational way, and create an
               engaging experience around it. Since chatbots are social and
               interactive technologies, they might be effective tools to lower
               the threshold of engaging with news content containing opposing
               views. In an experiment, we test this idea by investigating
               whether people are more likely to accept a news article
               containing conflicting views when it is delivered by a chatbot,
               as compared with the same article on a news website. The results
               indicated that people agreed more to a counter-attitudinal news
               article when it was delivered by a news chatbot (compared with
               the website article). In addition, users also perceived this
               chatbot article as more credible. The underlying process for this
               effect was that people attributed human-like characteristics to
               the chatbot on an implicit level (i.e., perceived mindless
               anthropomorphism). These results are discussed in the light of
               their potential contribution to an informed public discourse and
               a decrease in polarization in our society.",
  month     =  feb,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Wagner2018-uf,
  title     = "Overtrust in the robotic age",
  author    = "Wagner, Alan R and Borenstein, Jason and Howard, Ayanna",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  61,
  number    =  9,
  pages     = "22--24",
  abstract  = "A contemporary ethical challenge.",
  month     =  aug,
  year      =  2018
}

@ARTICLE{Robinette2016-ag,
  title     = "Overtrust of robots in emergency evacuation scenarios",
  author    = "Robinette, P and Li, W and Allen, R and Howard, A M and {others}",
  journal   = "2016 11th ACM/IEEE",
  publisher = "ieeexplore.ieee.org",
  abstract  = "Robots have the potential to save lives in emergency scenarios,
               but could have an equally disastrous effect if participants
               overtrust them. To explore this concept, we performed an …",
  year      =  2016
}

@ARTICLE{Aroyo2021-jb,
  title     = "Overtrusting robots: Setting a research agenda to mitigate
               overtrust in automation",
  author    = "Aroyo, Alexander M and de Bruyne, Jan and Dheu, Orian and
               Fosch-Villaronga, Eduard and Gudkov, Aleksei and Hoch, Holly and
               Jones, Steve and Lutz, Christoph and Sætra, Henrik and Solberg,
               Mads and Tamò-Larrieux, Aurelia",
  journal   = "Paladyn, Journal of Behavioral Robotics",
  publisher = "De Gruyter Open Access",
  volume    =  12,
  number    =  1,
  pages     = "423--436",
  abstract  = "There is increasing attention given to the concept of
               trustworthiness for artificial intelligence and robotics.
               However, trust is highly context-dependent, varies among
               cultures, and requires reflection on others’ trustworthiness,
               appraising whether there is enough evidence to conclude that
               these agents deserve to be trusted. Moreover, little research
               exists on what happens when too much trust is placed in robots
               and autonomous systems. Conceptual clarity and a shared framework
               for approaching overtrust are missing. In this contribution, we
               offer an overview of pressing topics in the context of overtrust
               and robots and autonomous systems. Our review mobilizes insights
               solicited from in-depth conversations from a multidisciplinary
               workshop on the subject of trust in human–robot interaction
               (HRI), held at a leading robotics conference in 2020. A broad
               range of participants brought in their expertise, allowing the
               formulation of a forward-looking research agenda on overtrust and
               automation biases in robotics and autonomous systems. Key points
               include the need for multidisciplinary understandings that are
               situated in an eco-system perspective, the consideration of
               adjacent concepts such as deception and anthropomorphization, a
               connection to ongoing legal discussions through the topic of
               liability, and a socially embedded understanding of overtrust in
               education and literacy matters. The article integrates diverse
               literature and provides a ground for common understanding for
               overtrust in the context of HRI.",
  month     =  jan,
  year      =  2021,
  keywords  = "trust; overtrust; robots; social robots; deception;
               anthropomorphization liability; education",
  language  = "en"
}

@ARTICLE{Wang2023-yd,
  title         = "Overwriting Pretrained Bias with Finetuning Data",
  author        = "Wang, Angelina and Russakovsky, Olga",
  journal       = "Proceedings of the IEEE/CVF",
  abstract      = "Transfer learning is beneficial by allowing the expressive
                   features of models pretrained on large-scale datasets to be
                   finetuned for the target task of smaller, more
                   domain-specific datasets. However, there is a concern that
                   these pretrained models may come with their own biases which
                   would propagate into the finetuned model. In this work, we
                   investigate bias when conceptualized as both spurious
                   correlations between the target task and a sensitive
                   attribute as well as underrepresentation of a particular
                   group in the dataset. Under both notions of bias, we find
                   that (1) models finetuned on top of pretrained models can
                   indeed inherit their biases, but (2) this bias can be
                   corrected for through relatively minor interventions to the
                   finetuning dataset, and often with a negligible impact to
                   performance. Our findings imply that careful curation of the
                   finetuning dataset is important for reducing biases on a
                   downstream task, and doing so can even compensate for bias in
                   the pretrained model.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@MISC{noauthor_undated-yk,
  title = "{Paris2016}.pdf"
}

@MISC{noauthor_undated-ux,
  title = "Park \& Montgomery (2023).pdf"
}

@INPROCEEDINGS{Gimpel2011-oh,
  title     = "Part-of-speech tagging for twitter: Annotation, features, and
               experiments",
  author    = "Gimpel, Kevin and Schneider, Nathan and O'connor, Brendan and
               Das, Dipanjan and Mills, Daniel P and Eisenstein, Jacob and
               Heilman, Michael and Yogatama, Dani and Flanigan, Jeffrey and
               Smith, Noah A",
  booktitle = "Proceedings of the 49th annual meeting of the Association for
               Computational Linguistics: Human Language Technologies",
  publisher = "aclanthology.org",
  pages     = "42--47",
  abstract  = "We address the problem of part-of-speech tagging for English data
               from the popular microblogging service Twitter. We develop a
               tagset, annotate data, develop features, and report tagging
               results nearing 90\% accuracy. The data and tools have been made
               available to the research community with the goal of enabling
               richer text analysis of Twitter and related social media data
               sets.",
  year      =  2011
}

@INPROCEEDINGS{Suresh2024-pc,
  title     = "Participation in the age of foundation models",
  author    = "Suresh, Harini and Tseng, Emily and Young, Meg and Gray, Mary and
               Pierson, Emma and Levy, Karen",
  booktitle = "Proceedings of the 2024 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1609--1621",
  abstract  = "Growing interest and investment in the capabilities of foundation
               models has positioned such systems to impact a wide array of
               services, from banking to healthcare. Alongside these
               opportunities is the risk that these systems reify existing power
               imbalances and cause disproportionate harm to historically
               marginalized groups. The larger scale and domain-agnostic manner
               in which these models operate further heightens the stakes: any
               errors or harms are liable to reoccur across use cases. In AI \&
               ML more broadly, participatory approaches hold promise to lend
               agency and decision-making power to marginalized stakeholders,
               leading to systems that better benefit justice through equitable
               and distributed governance. But existing approaches in
               participatory AI/ML are typically grounded in a specific
               application and set of relevant stakeholders, and it is not
               straightforward how to apply these lessons to the context of
               foundation models. Our paper aims to fill this gap. First, we
               examine existing attempts at incorporating participation into
               foundation models. We highlight the tension between participation
               and scale, demonstrating that it is intractable for impacted
               communities to meaningfully shape a foundation model that is
               intended to be universally applicable. In response, we develop a
               blueprint for participatory foundation models that identifies
               more local, application-oriented opportunities for meaningful
               participation. In addition to the “foundation” layer, our
               framework proposes the “subfloor” layer, in which stakeholders
               develop shared technical infrastructure, norms and governance for
               a grounded domain such as clinical care, journalism, or finance,
               and the “surface” (or application) layer, in which affected
               communities shape the use of a foundation model for a specific
               downstream task. The intermediate “subfloor” layer scopes the
               range of potential harms to consider, and affords communities
               more concrete avenues for deliberation and intervention. At the
               same time, it avoids duplicative effort by scaling input across
               relevant use cases. Through three case studies in clinical care,
               financial services, and journalism, we illustrate how this
               multi-layer model can create more meaningful opportunities for
               participation than solely intervening at the foundation layer.",
  series    = "FAccT '24",
  month     =  jun,
  year      =  2024,
  keywords  = "Foundation models, communities, governance, public participation,
               stakeholders"
}

@ARTICLE{Phadke2022-ac,
  title     = "Pathways through Conspiracy: The Evolution of Conspiracy
               Radicalization through Engagement in Online Conspiracy
               Discussions",
  author    = "Phadke, Shruti and Samory, Mattia and Mitra, Tanushree",
  journal   = "ICWSM",
  publisher = "ojs.aaai.org",
  volume    =  16,
  pages     = "770--781",
  abstract  = "The disruptive offline mobilization of participants in online
               conspiracy theory (CT) discussions has highlighted the importance
               of understanding how online users may form radicalized conspiracy
               beliefs. While prior work researched the factors leading up to
               joining online CT discussions and provided theories of how
               conspiracy beliefs form, we have little understanding of how
               conspiracy radicalization evolves after users join CT discussion
               communities. In this paper, we provide the empirical modeling of
               various radicalization phases in online CT discussion
               participants. To unpack how conspiracy engagement is related to
               radicalization, we first characterize the users' journey through
               CT discussions via conspiracy engagement pathways. Specifically,
               by studying 36K Reddit users through their 169M contributions, we
               uncover four distinct pathways of conspiracy engagement: steady
               high, increasing, decreasing, and steady low. We further model
               three successive stages of radicalization guided by prior
               theoretical works. Specific sub-populations of users, namely
               those on steady high and increasing conspiracy engagement
               pathways, progress successively through various radicalization
               stages. In contrast, users on the decreasing engagement pathway
               show distinct behavior: they limit their CT discussions to
               specialized topics, participate in diverse discussion groups, and
               show reduced conformity with conspiracy subreddits. By examining
               users who disengage from online CT discussions, this paper
               provides promising insights about conspiracy recovery process.",
  month     =  may,
  year      =  2022,
  keywords  = "Organizational and group behavior mediated by social media;
               interpersonal communication mediated by social media; Qualitative
               and quantitative studies of social media; Social network
               analysis; communities identification; expertise and authority
               discovery; Trend identification and tracking; time series
               forecasting",
  language  = "en"
}

@ARTICLE{Zhang2019-kx,
  title         = "{PAWS}: Paraphrase Adversaries from Word Scrambling",
  author        = "Zhang, Yuan and Baldridge, Jason and He, Luheng",
  journal       = "arXiv [cs.CL]",
  abstract      = "Existing paraphrase identification datasets lack sentence
                   pairs that have high lexical overlap without being
                   paraphrases. Models trained on such data fail to distinguish
                   pairs like flights from New York to Florida and flights from
                   Florida to New York. This paper introduces PAWS (Paraphrase
                   Adversaries from Word Scrambling), a new dataset with 108,463
                   well-formed paraphrase and non-paraphrase pairs with high
                   lexical overlap. Challenging pairs are generated by
                   controlled word swapping and back translation, followed by
                   fluency and paraphrase judgments by human raters.
                   State-of-the-art models trained on existing datasets have
                   dismal performance on PAWS (<40\% accuracy); however,
                   including PAWS training data for these models improves their
                   accuracy to 85\% while maintaining performance on existing
                   tasks. In contrast, models that do not capture non-local
                   contextual information fail even with PAWS training examples.
                   As such, PAWS provides an effective instrument for driving
                   further progress on models that better exploit structure,
                   context, and pairwise comparisons.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{noauthor_undated-hh,
  title = "({PDF}) Towards Countering Essentialism through Social .."
}

@ARTICLE{Tagliamonte2009-eu,
  title     = "Peaks Beyond Phonology: Adolescence, Incrementation, and Language
               Change",
  author    = "Tagliamonte, Sali A and D'Arcy, Alexandra",
  journal   = "Language",
  publisher = "Linguistic Society of America",
  volume    =  85,
  number    =  1,
  pages     = "58--108",
  abstract  = "[What is the mechanism by which a linguistic change advances
               across successive generations of speakers? We explore this
               question by using the model of incrementation provided in Labov
               2001 and analyzing six current changes in English. Extending
               Labov's focus on recent and vigorous phonological changes, we
               target ongoing morphosyntactic(-semantic) and discourse-pragmatic
               changes. Our results provide a striking validation of the
               incrementation model, confirming its value as a key to
               understanding the evolution of linguistic systems. However,
               although our findings reveal the predicted peak in the
               apparent-time progress of a change and corroborate the female
               tendency to lead innovation, there is no absolute contrast
               between men and women with respect to incrementation. Instead,
               quantitative differences in the social embedding of linguistic
               change correlate with the rate of the change in the speech
               community.]",
  year      =  2009
}

@ARTICLE{Friedkin1990-ar,
  title     = "Peer Group Influence",
  author    = "Friedkin, Noah E and Cook, Karen S",
  journal   = "Sociol. Methods Res.",
  publisher = "SAGE Publications Inc",
  volume    =  19,
  number    =  1,
  pages     = "122--143",
  abstract  = "This article evaluates three models of peer group influence on
               opinions. Two of these models are eliminated on theoretical and
               empirical grounds. The surviving model is consistent with the
               seminal work of French (1956) on social influence processes and
               provides theoretical foundations for the convention of measuring
               interpersonal effects with the mean opinion of an individual's
               set of peers. The model clearly points out the danger of reifying
               the mean of peers' opinions. Whether or not there is a group
               norm, the mean of peers' opinions must be viewed strictly as an
               analytical construction that may be employed to estimate the
               magnitude of pressures toward uniformity in a peer group.",
  month     =  aug,
  year      =  1990
}

@ARTICLE{Allaway_undated-va,
  title  = "Penguins Don't Fly: Reasoning about Generics through Instantiations
            and Exceptions",
  author = "Allaway, Emily and Hwang, Jena D and Bhagavatula, Chandra and Mc
            Keown, Kathleen and Downey, Doug and Choi, Yejin"
}

@ARTICLE{Sen2023-if,
  title         = "People Make Better Edits: Measuring the Efficacy of
                   {LLM}-Generated Counterfactually Augmented Data for Harmful
                   Language Detection",
  author        = "Sen, Indira and Assenmacher, Dennis and Samory, Mattia and
                   Augenstein, Isabelle and van der Aalst, Wil and Wagner,
                   Claudia",
  journal       = "arXiv [cs.CL]",
  abstract      = "NLP models are used in a variety of critical social computing
                   tasks, such as detecting sexist, racist, or otherwise hateful
                   content. Therefore, it is imperative that these models are
                   robust to spurious features. Past work has attempted to
                   tackle such spurious features using training data
                   augmentation, including Counterfactually Augmented Data
                   (CADs). CADs introduce minimal changes to existing training
                   data points and flip their labels; training on them may
                   reduce model dependency on spurious features. However,
                   manually generating CADs can be time-consuming and expensive.
                   Hence in this work, we assess if this task can be automated
                   using generative NLP models. We automatically generate CADs
                   using Polyjuice, ChatGPT, and Flan-T5, and evaluate their
                   usefulness in improving model robustness compared to
                   manually-generated CADs. By testing both model performance on
                   multiple out-of-domain test sets and individual data point
                   efficacy, our results show that while manual CADs are still
                   the most effective, CADs generated by ChatGPT come a close
                   second. One key reason for the lower performance of automated
                   methods is that the changes they introduce are often
                   insufficient to flip the original label.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Pineda2021-qw,
  title    = "Perceiving Sociable Technology: Exploring the Role of
              Anthropomorphism and Agency Perception on Human-Computer
              Interaction ({HCI})",
  author   = "Pineda, Jose",
  abstract = "With the arrival of personal assistants and other AI-enabled
              autonomous technologies, social interactions with smart devices
              have become a part of our daily lives. Therefore, it becomes
              increasingly important to understand how these social interactions
              emerge, and why users appear to be influenced by them. For this
              reason, I explore questions on what the antecedents and
              consequences of this phenomenon, known as anthropomorphism, are as
              described in the extant literature from fields ranging from
              information systems to social neuroscience. I critically analyze
              those empirical studies directly measuring anthropomorphism and
              those referring to it without a corresponding measurement. Through
              a grounded theory approach, I identify common themes and use them
              to develop models for the antecedents and consequences of
              anthropomorphism. The results suggest anthropomorphism possesses
              both conscious and non-conscious components with varying
              implications. While conscious attributions are shown to vary based
              on individual differences, non-conscious attributions emerge
              whenever a technology exhibits apparent reasoning such as through
              non-verbal behavior like peer-to-peer mirroring or verbal
              paralinguistic and backchanneling cues. Anthropomorphism has been
              shown to affect users’ self-perceptions, perceptions of the
              technology, how users interact with the technology, and the users’
              performance. Examples include changes in a users’ trust on the
              technology, conformity effects, bonding, and displays of empathy.
              I argue these effects emerge from changes in users’ perceived
              agency, and their self- and social- identity similarly to
              interactions between humans. Afterwards, I critically examine
              current theories on anthropomorphism and present propositions
              about its nature based on the results of the empirical literature.
              Subsequently, I introduce a two-factor model of anthropomorphism
              that proposes how an individual anthropomorphizes a technology is
              dependent on how the technology was initially perceived (top-down
              and rational or bottom-up and automatic), and whether it exhibits
              a capacity for agency or experience. I propose that where a
              technology lays along this spectrum determines how individuals
              relates to it, creating shared agency effects, or changing the
              users’ social identity. For this reason, anthropomorphism is a
              powerful tool that can be leveraged to support future interactions
              with smart technologies.",
  series   = "FIU Electronic Theses and Dissertations",
  year     =  2021
}

@MISC{noauthor_undated-le,
  title = "{PerezEtAl2016}.pdf"
}

@PHDTHESIS{Kwon2019-al,
  title     = "Performing Masquerade: The Politics of {K}-Beauty in South Korean
               Literary and Popular Culture from Colonialism to Neoliberalism",
  author    = "Kwon, Hye Kyoung",
  publisher = "escholarship.org",
  abstract  = "Author(s): Kwon, Hye Kyoung | Advisor(s): Metzger, Sean; Kim,
               Suk-Young | Abstract: This dissertation traces the historical
               trajectory of selected Korean female iconographies: Modern Girl,
               Apr�s Girl, Factory Girl, Gangnam Beauty, and Soybean Paste Girl.
               These figures have functioned as avatars of the gendered history
               of Korea and have also had a considerable transpacific impact on
               the dynamics that affect gender and class structures, as well as
               on national beauty culture, norms of femininity, and everyday
               performance of masquerade. With all these embodied female
               icons—icons that represent male fabrications that cater to the
               male gaze—I focus on the tension between authenticity and
               imitation/copies, which patriarchal society has demanded be
               distinguished. Amid the oblique power relations created primarily
               by the conjuncture of Japanese colonialism and US imperialism,
               Korea has consciously strived to establish a national cultural
               identity as the original rather than the copy. Such transnational
               struggles have been projected as struggles internal to the
               nation—that is, between the privileged and the marginalized of
               society. Particularly in this Confucian and patriarchal society,
               such intense power dynamics have produced and maintained unequal
               relations between genders. In other words, having to
               differentiate authentic from imitative disempowers women’s
               performance of self and agency and their presentation of
               femininity in everyday life. I draw on the concept of masquerade
               as a way not only to resolve this tension but to narrow the gap
               between what is projected as authentic and what is considered
               imitative, or between the reality and the representation, thus
               traversing the boundaries of class and challenging the
               normativity of femininity and the ideals of beauty constructed by
               social intellectuals, the state, and hegemonic mores. My object
               of analysis is what I call a “nexus of beauty”: I weave together
               a wide range of media, not only popular media such as cinema and
               magazines, including Sunday Seoul and in-house beauty magazines
               produced by Amorepacific, but government policies and propaganda
               in relation to women’s fashion and consumption; literature
               (particularly novels); social events such as beauty pageants,
               parades of Korean actresses, and the sociocultural trend of
               “dance fever”; and the everyday life performances of marginalized
               Korean women in a public realm. My objective, therefore, is to
               illuminate the relationship between each historical context and
               transformative and multilayered “Koreanesses,” particularly in
               terms of national beauty culture and women’s corporeal identity.
               Although many everyday beauty practices and performances of
               working-class Korean women have been restored through archival
               images and narratives, this restoration is insufficient. Drawing
               on a variety of discursive/archival materials and forms of
               popular culture—including novels, essays, advertisements,
               newspapers, in-house beauty/weekly magazines, and “webtoons”—I
               analyze how notions of K-Beauty have been promulgated and
               performed, beginning with Korea’s decision to participate in the
               Western capitalist economy at the turn of the twentieth century.
               By examining these various discursive sites of media, I
               demonstrate how K-Beauty has been developed and disseminated as
               once-familiar bodies become—or engage in performances of
               masquerades that become—visually ambiguous. In that process, the
               relationship between bodies and behavior becomes theatrical.",
  year      =  2019,
  school    = "UCLA"
}

@ARTICLE{Ghosh2023-pb,
  title         = "'person' == light-skinned, Western man, and sexualization of
                   women of color: Stereotypes in Stable Diffusion",
  author        = "Ghosh, Sourojit and Caliskan, Aylin",
  journal       = "arXiv [cs.CV]",
  abstract      = "We study stereotypes embedded within one of the most popular
                   text-to-image generators: Stable Diffusion. We examine what
                   stereotypes of gender and nationality/continental identity
                   does Stable Diffusion display in the absence of such
                   information i.e. what gender and nationality/continental
                   identity is assigned to `a person', or to `a person from
                   Asia'. Using vision-language model CLIP's cosine similarity
                   to compare images generated by CLIP-based Stable Diffusion
                   v2.1 verified by manual examination, we chronicle results
                   from 136 prompts (50 results/prompt) of front-facing images
                   of persons from 6 different continents, 27 nationalities and
                   3 genders. We observe how Stable Diffusion outputs of `a
                   person' without any additional gender/nationality information
                   correspond closest to images of men and least with persons of
                   nonbinary gender, and to persons from Europe/North America
                   over Africa/Asia, pointing towards Stable Diffusion having a
                   concerning representation of personhood to be a
                   European/North American man. We also show continental
                   stereotypes and resultant harms e.g. a person from Oceania is
                   deemed to be Australian/New Zealander over Papua New Guinean,
                   pointing to the erasure of Indigenous Oceanic peoples, who
                   form a majority over descendants of colonizers both in Papua
                   New Guinea and in Oceania overall. Finally, we unexpectedly
                   observe a pattern of oversexualization of women, specifically
                   Latin American, Mexican, Indian and Egyptian women relative
                   to other nationalities, measured through an NSFW detector.
                   This demonstrates how Stable Diffusion perpetuates Western
                   fetishization of women of color through objectification in
                   media, which if left unchecked will amplify this
                   stereotypical representation. Image datasets are made
                   publicly available.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Jung2017-rm,
  title     = "Persona generation from aggregated social media data",
  author    = "Jung, S G and An, J and Kwak, H and Ahmad, M and Nielsen, L and
               {others}",
  journal   = "Proceedings of the",
  publisher = "dl.acm.org",
  abstract  = "We develop a methodology for persona generation using real time
               social media data for the distribution of products via online
               platforms. From a large social media account containing …",
  year      =  2017
}

@ARTICLE{Marsden2019-ax,
  title     = "Personas and identity: Looking at multiple identities to inform
               the construction of personas",
  author    = "Marsden, N and Pröbster, M",
  journal   = "Proceedings of the 2019 CHI Conference on",
  publisher = "dl.acm.org",
  abstract  = "Personas are valuable tools to help designers get to know their
               users and adopt their perspectives. Yet people are complex and
               multiple identities have to be considered in their …",
  year      =  2019
}

@ARTICLE{Probster2019-tu,
  title     = "Personas and persons-an empirical study on stereotyping of
               personas",
  author    = "Pröbster, M and Hermann, J and Marsden, N",
  journal   = "Proceedings of Mensch und",
  publisher = "dl.acm.org",
  abstract  = "Personas are considered a key method to induce empathy for users.
               However, they are also said to trigger stereotyping, eg by using
               photographs that convey information about gender …",
  year      =  2019
}

@ARTICLE{Holzinger2022-zg,
  title    = "Personas for Artificial Intelligence ({AI}) an Open Source Toolbox",
  author   = "Holzinger, Andreas and Kargl, Michaela and Kipperer, Bettina and
              Regitnig, Peter and Plass, Markus and Müller, Heimo",
  journal  = "IEEE Access",
  volume   =  10,
  pages    = "23732--23747",
  abstract = "Personas have successfully supported the development of classical
              user interfaces for more than two decades by mapping users’ mental
              models to specific contexts. The rapid proliferation of Artificial
              Intelligence (AI) applications makes it necessary to create new
              approaches for future human-AI interfaces. Human-AI interfaces
              differ from classical human-computer interfaces in many ways, such
              as gaining some degree of human-like cognitive, self-executing,
              and self-adaptive capabilities and autonomy, and generating
              unexpected outputs that require non-deterministic interactions.
              Moreover, the most successful AI approaches are so-called “black
              box” systems, where the technology and the machine learning
              process are opaque to the user and the AI output is far not
              intuitive. This work shows how the personas method can be adapted
              to support the development of human-centered AI applications, and
              we demonstrate this on the example of a medical context. This work
              is - to our knowledge - the first to provide personas for AI using
              an openly available Personas for AI toolbox. The toolbox contains
              guidelines and material supporting persona development for AI as
              well as templates and pictures for persona visualisation. It is
              ready to use and freely available to the international research
              and development community. Additionally, an example from medical
              AI is provided as a best practice use case. This work is intended
              to help foster the development of novel human-AI interfaces that
              will be urgently needed in the near future.",
  year     =  2022,
  keywords = "Artificial intelligence;Human computer interaction;Face
              recognition;Usability;Psychology;Predictive models;Artificial
              intelligence;human–AI interface;personas"
}

@ARTICLE{Todd2012-yj,
  title     = "Perspective Taking Undermines Stereotype Maintenance Processes:
               Evidence from Social Memory, Behavior Explanation, and
               Information Solicitation",
  author    = "Todd, Andrew R and Galinsky, Adam D and Bodenhausen, Galen V",
  journal   = "Soc. Cogn.",
  publisher = "Guilford Publications Inc.",
  volume    =  30,
  number    =  1,
  pages     = "94--108",
  abstract  = "Four experiments examined the effects of perspective taking on
               processes contributing to stereotype maintenance: biases in
               social memory, behavior explanations, and information seeking.
               The first two experiments explored whether perspective taking
               influences memory and spontaneous explanations for
               stereotype-relevant behaviors. Relative to participants in an
               objective-focus condition, perspective takers exhibited better
               recall of stereotype-inconsistent behaviors (Experiment 1) and
               spontaneously generated more dispositional explanations for them
               (Experiment 2). Perspective taking had little effect, however, on
               memory and explanations for stereotype-consistent behaviors. The
               final two experiments examined the effects of perspective taking
               on information seeking. Employing a trait hypothesis-testing
               paradigm in which interviewers tested whether an interviewee was
               an extravert (Experiment 3a) or an introvert (Experiment 3b), we
               found that perspective-taking interviewers solicited more
               hypothesis-inconsistent information than did controls. The
               findings collectively indicate that perspective taking can be an
               effective strategy for undermining stereotype maintenance,
               primarily via its influence on the processing of
               stereotype-inconsistent information.",
  month     =  feb,
  year      =  2012
}

@ARTICLE{Chen2023-dg,
  title         = "{PLACES}: Prompting Language Models for Social Conversation
                   Synthesis",
  author        = "Chen, Maximillian and Papangelis, Alexandros and Tao,
                   Chenyang and Kim, Seokhwan and Rosenbaum, Andy and Liu, Yang
                   and Yu, Zhou and Hakkani-Tur, Dilek",
  journal       = "arXiv [cs.CL]",
  abstract      = "Collecting high quality conversational data can be very
                   expensive for most applications and infeasible for others due
                   to privacy, ethical, or similar concerns. A promising
                   direction to tackle this problem is to generate synthetic
                   dialogues by prompting large language models. In this work,
                   we use a small set of expert-written conversations as
                   in-context examples to synthesize a social conversation
                   dataset using prompting. We perform several thorough
                   evaluations of our synthetic conversations compared to
                   human-collected conversations. This includes various
                   dimensions of conversation quality with human evaluation
                   directly on the synthesized conversations, and interactive
                   human evaluation of chatbots fine-tuned on the synthetically
                   generated dataset. We additionally demonstrate that this
                   prompting approach is generalizable to multi-party
                   conversations, providing potential to create new synthetic
                   data for multi-party tasks. Our synthetic multi-party
                   conversations were rated more favorably across all measured
                   dimensions compared to conversation excerpts sampled from a
                   human-collected multi-party dataset.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Suchman1987-hy,
  title     = "Plans and situated actions: The problem of human-machine
               communication",
  author    = "Suchman, L A",
  publisher = "books.google.com",
  abstract  = "This lively and original book offers a provocative critique of
               the dominant assumptions regarding human action and communication
               which underlie recent research in machine …",
  year      =  1987
}

@ARTICLE{Dathathri2019-ey,
  title         = "Plug and Play Language Models: A Simple Approach to
                   Controlled Text Generation",
  author        = "Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and
                   Hung, Jane and Frank, Eric and Molino, Piero and Yosinski,
                   Jason and Liu, Rosanne",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large transformer-based language models (LMs) trained on huge
                   text corpora have shown unparalleled generation capabilities.
                   However, controlling attributes of the generated language
                   (e.g. switching topic or sentiment) is difficult without
                   modifying the model architecture or fine-tuning on
                   attribute-specific data and entailing the significant cost of
                   retraining. We propose a simple alternative: the Plug and
                   Play Language Model (PPLM) for controllable language
                   generation, which combines a pretrained LM with one or more
                   simple attribute classifiers that guide text generation
                   without any further training of the LM. In the canonical
                   scenario we present, the attribute models are simple
                   classifiers consisting of a user-specified bag of words or a
                   single learned layer with 100,000 times fewer parameters than
                   the LM. Sampling entails a forward and backward pass in which
                   gradients from the attribute model push the LM's hidden
                   activations and thus guide the generation. Model samples
                   demonstrate control over a range of topics and sentiment
                   styles, and extensive automated and human annotated
                   evaluations show attribute alignment and fluency. PPLMs are
                   flexible in that any combination of differentiable attribute
                   models may be used to steer text generation, which will allow
                   for diverse and creative applications beyond the examples
                   given in this paper.",
  month         =  dec,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Barroso-Moreno2023-op,
  title    = "Polarization, virality and contrary sentiments for {LGTB} content
              on Instagram, {TikTok}, and Twitter",
  author   = "Barroso-Moreno, Carlos and Rayón-Rumayor, Laura and
              Bañares-Marivela, Elena and Hernández-Ortega, José",
  journal  = "Profesional de la información",
  volume   =  32,
  number   =  2,
  abstract = "Digital platforms are spaces for social participation with
              significant value in the development of the identity of
              adolescents and emerging adults. The objective is to identify the
              behavior and visibility of LGBT content using Instagram, TikTok,
              and Twitter posts of such content from May 16 to November 16,
              2022, collecting 539,389 posts. Social media monitoring techniques
              gathered the posts with the keywords “LGTB” or “LGBT” in Spanish
              and English, forming the database. The methodology is based on a
              mixed design: first, the database is analyzed using Big Data
              techniques and, second, the 10 most viral posts from each social
              network are selected. The results show that dissemination of
              gender identity in content and meaning is uneven across the
              various social networks. Twitter profiles have a higher number of
              posts (61\%), polarization, and lower virality and exhibit visible
              LGBTphobia. Instagram has a number of posts (37\%) and average
              virality, with positive sentiments. TikTok has fewer posts (2\%),
              less polarization, positive messages, and extreme virality. The
              three networks consider the Pride demonstrations to be a symbol of
              the community because they destabilize and confront LGBTphobic
              oppression by occupying public spaces, opening the closet without
              stigma or shame, as is reflected on social networks. The behavior
              of LGBT content on these platforms is multidimensional, uneven,
              and differentiated, which demonstrates the necessity of ensuring
              respect for the diversity of sexual orientation and gender
              identity on digital platforms.",
  month    =  mar,
  year     =  2023,
  keywords = "LGTB; LGBT; LGTBfobia; Social media; Social networks; Social
              network analysis; Virality; Gender diversity; Sexual diversity;
              Big Data; Qualitative research; Quantitative research; Influence
              groups; Citizenship; Instagram; Twitter; TikTok",
  language = "es"
}

@ARTICLE{Rottger2024-qu,
  title         = "Political Compass or Spinning Arrow? Towards More Meaningful
                   Evaluations for Values and Opinions in Large Language Models",
  author        = "Röttger, Paul and Hofmann, Valentin and Pyatkin, Valentina
                   and Hinck, Musashi and Kirk, Hannah Rose and Schütze, Hinrich
                   and Hovy, Dirk",
  journal       = "arXiv [cs.CL]",
  abstract      = "Much recent work seeks to evaluate values and opinions in
                   large language models (LLMs) using multiple-choice surveys
                   and questionnaires. Most of this work is motivated by
                   concerns around real-world LLM applications. For example,
                   politically-biased LLMs may subtly influence society when
                   they are used by millions of people. Such real-world
                   concerns, however, stand in stark contrast to the
                   artificiality of current evaluations: real users do not
                   typically ask LLMs survey questions. Motivated by this
                   discrepancy, we challenge the prevailing constrained
                   evaluation paradigm for values and opinions in LLMs and
                   explore more realistic unconstrained evaluations. As a case
                   study, we focus on the popular Political Compass Test (PCT).
                   In a systematic review, we find that most prior work using
                   the PCT forces models to comply with the PCT's
                   multiple-choice format. We show that models give
                   substantively different answers when not forced; that answers
                   change depending on how models are forced; and that answers
                   lack paraphrase robustness. Then, we demonstrate that models
                   give different answers yet again in a more realistic
                   open-ended answer setting. We distill these findings into
                   recommendations and open challenges in evaluating values and
                   opinions in LLMs.",
  month         =  feb,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wu2021-lp,
  title     = "Polyjuice: Generating counterfactuals for explaining, evaluating,
               and improving models",
  author    = "Wu, T and Ribeiro, M T and Heer, J and Weld, D S",
  journal   = "arXiv preprint arXiv:2101.00288",
  publisher = "arxiv.org",
  abstract  = "While counterfactual examples are useful for analysis and
               training of NLP models, current generation methods either rely on
               manual labor to create very few counterfactuals, or only …",
  year      =  2021
}

@ARTICLE{Zhao2024-sq,
  title         = "Position: Measure Dataset Diversity, Don't Just Claim It",
  author        = "Zhao, Dora and Andrews, Jerone T A and Papakyriakopoulos,
                   Orestis and Xiang, Alice",
  journal       = "arXiv [cs.LG]",
  abstract      = "Machine learning (ML) datasets, often perceived as neutral,
                   inherently encapsulate abstract and disputed social
                   constructs. Dataset curators frequently employ value-laden
                   terms such as diversity, bias, and quality to characterize
                   datasets. Despite their prevalence, these terms lack clear
                   definitions and validation. Our research explores the
                   implications of this issue by analyzing ``diversity'' across
                   135 image and text datasets. Drawing from social sciences, we
                   apply principles from measurement theory to identify
                   considerations and offer recommendations for conceptualizing,
                   operationalizing, and evaluating diversity in datasets. Our
                   findings have broader implications for ML research,
                   advocating for a more nuanced and precise approach to
                   handling value-laden properties in dataset construction.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Ma2020-zo,
  title         = "{PowerTransformer}: Unsupervised Controllable Revision for
                   Biased Language Correction",
  author        = "Ma, Xinyao and Sap, Maarten and Rashkin, Hannah and Choi,
                   Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Unconscious biases continue to be prevalent in modern text
                   and media, calling for algorithms that can assist writers
                   with bias correction. For example, a female character in a
                   story is often portrayed as passive and powerless (``She
                   daydreams about being a doctor'') while a man is portrayed as
                   more proactive and powerful (``He pursues his dream of being
                   a doctor''). We formulate *Controllable Debiasing*, a new
                   revision task that aims to rewrite a given text to correct
                   the implicit and potentially undesirable bias in character
                   portrayals. We then introduce PowerTransformer as an approach
                   that debiases text through the lens of connotation frames
                   (Sap et al., 2017), which encode pragmatic knowledge of
                   implied power dynamics with respect to verb predicates. One
                   key challenge of our task is the lack of parallel corpora. To
                   address this challenge, we adopt an unsupervised approach
                   using auxiliary supervision with related tasks such as
                   paraphrasing and self-supervision based on a reconstruction
                   loss, building on pretrained language models. Through
                   comprehensive experiments based on automatic and human
                   evaluations, we demonstrate that our approach outperforms
                   ablations and existing methods from related tasks.
                   Furthermore, we demonstrate the use of PowerTransformer as a
                   step toward mitigating the well-documented gender bias in
                   character portrayal in movie scripts.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Acton2019-xw,
  title     = "Pragmatics and the social life of the English definite article",
  author    = "Acton, Eric K",
  journal   = "Language",
  publisher = "Linguistic Society of America",
  volume    =  95,
  number    =  1,
  pages     = "37--65",
  abstract  = "ARRAY(0x55a6ddf9d7f8)",
  year      =  2019
}

@ARTICLE{Fried2022-ab,
  title         = "Pragmatics in Grounded Language Learning: Phenomena, Tasks,
                   and Modeling Approaches",
  author        = "Fried, Daniel and Tomlin, Nicholas and Hu, Jennifer and
                   Patel, Roma and Nematzadeh, Aida",
  journal       = "arXiv [cs.CL]",
  abstract      = "People rely heavily on context to enrich meaning beyond what
                   is literally said, enabling concise but effective
                   communication. To interact successfully and naturally with
                   people, user-facing artificial intelligence systems will
                   require similar skills in pragmatics: relying on various
                   types of context -- from shared linguistic goals and
                   conventions, to the visual and embodied world -- to use
                   language effectively. We survey existing grounded settings
                   and pragmatic modeling approaches and analyze how the task
                   goals, environmental contexts, and communicative affordances
                   in each work enrich linguistic meaning. We present
                   recommendations for future grounded task design to naturally
                   elicit pragmatic phenomena, and suggest directions that focus
                   on a broader range of communicative contexts and affordances.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Mondal2024-zb,
  title     = "Presentations by the Humans and For the Humans: Harnessing {LLM}s
               for Generating Persona-Aware Slides from Documents",
  author    = "Mondal, Ishani and S, Shwetha and Natarajan, Anandhavelu and
               Garimella, Aparna and Bandyopadhyay, Sambaran and Boyd-Graber,
               Jordan",
  editor    = "Graham, Yvette and Purver, Matthew",
  booktitle = "Proceedings of the 18th Conference of the European Chapter of the
               Association for Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "St. Julian's, Malta",
  pages     = "2664--2684",
  abstract  = "Scientific papers and slides are two different representations of
               the same underlying information, but both require substantial
               work to prepare. While there had been prior efforts on automating
               document-to-slides generation, there is still a pressing need of
               customizing the presentation of content aligning with the persona
               of target audience or duration of presentation. This paper first
               introduces the concept of end-user specification-aware document
               to slides conversion that incorporates end-user specifications
               into the conversion process. For this, we initially introduce a
               new dataset reuse the existing SciDuet dataset consisting of
               pairs of papers and corresponding slides decks from recent years'
               *ACL conferences to create four persona-aware configurations.
               Secondly, we present Persona-Aware-D2S, a novel approach by
               finetuning LLMs using target audience feedback to create
               persona-aware slides from scientific documents. Our evaluation on
               both automated metrics and qualitative human evaluation suggests
               that by incorporating end-user specifications into the conversion
               process, our model can create presentations that are not only
               informative but also tailored to expectations and cognitive
               abilities of target audience.",
  month     =  mar,
  year      =  2024
}

@INPROCEEDINGS{Korbak2023-ux,
  title     = "Pretraining Language Models with Human Preferences",
  author    = "Korbak, Tomasz and Shi, Kejian and Chen, Angelica and Bhalerao,
               Rasika Vinayak and Buckley, Christopher and Phang, Jason and
               Bowman, Samuel R and Perez, Ethan",
  editor    = "Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and
               Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan",
  booktitle = "Proceedings of the 40th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  202,
  pages     = "17506--17533",
  abstract  = "Language models (LMs) are pretrained to imitate text from large
               and diverse datasets that contain content that would violate
               human preferences if generated by an LM: falsehoods, offensive
               comments, personally identifiable information, low-quality or
               buggy code, among others. Here, we explore alternative objectives
               for pretraining LMs in a way that also guides them to generate
               text aligned with human preferences. We benchmark five objectives
               for pretraining with human feedback across three tasks and study
               how they affect the alignment and capabilities of pretrained LMs.
               We find a Pareto-optimal and simple approach among those we
               explored: conditional training, or learning distribution over
               tokens conditional on their human preference scores. Conditional
               training reduces the rate of undesirable content by up to an
               order of magnitude, both when generating without a prompt and
               with an adversarially-chosen prompt. Moreover, conditional
               training maintains the downstream task performance of standard LM
               pretraining, both before and after task-specific finetuning.
               Pretraining with human feedback results in much better preference
               satisfaction than standard LM pretraining followed by finetuning
               with feedback, i.e., learning and then unlearning undesirable
               behavior. Our results suggest that we should move beyond
               imitation learning when pretraining LMs and incorporate human
               preferences from the start of training.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2023
}

@ARTICLE{Matias2019-ha,
  title    = "Preventing harassment and increasing group participation through
              social norms in 2,190 online science discussions",
  author   = "Matias, J Nathan",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  116,
  number   =  20,
  pages    = "9785--9789",
  abstract = "Theories of human behavior suggest that people's decisions to join
              a group and their subsequent behavior are influenced by
              perceptions of what is socially normative. In online discussions,
              where unruly, harassing behavior is common, displaying community
              rules could reduce concerns about harassment that prevent people
              from joining while also influencing the behavior of those who do
              participate. An experiment tested these theories by randomizing
              announcements of community rules to large-scale online
              conversations in a science-discussion community with 13 million
              subscribers. Compared with discussions with no mention of
              community expectations, displaying the rules increased newcomer
              rule compliance by >8 percentage points and increased the
              participation rate of newcomers in discussions by 70\% on average.
              Making community norms visible prevented unruly and harassing
              conversations by influencing how people behaved within the
              conversation and also by influencing who chose to join.",
  month    =  may,
  year     =  2019,
  keywords = "field experiment; group participation; online harassment; science
              communications; social norms",
  language = "en"
}

@MISC{Kirk_undated-df,
  title       = "prism-alignment: The Prism Alignment Project",
  author      = "Kirk, Hannah Rose",
  institution = "Github",
  abstract    = "The Prism Alignment Project. Contribute to
                 HannahKirk/prism-alignment development by creating an account
                 on GitHub.",
  language    = "en"
}

@INCOLLECTION{Ischen2020-it,
  title     = "Privacy Concerns in Chatbot Interactions: Third International
               Workshop, {CONVERSATIONS} 2019, Amsterdam, The Netherlands,
               November {19–20}, 2019, Revised Selected Papers",
  author    = "Ischen, Carolin and Araujo, Theo and Voorveld, Hilde and van
               Noort, Guda and Smit, Edith",
  editor    = "Følstad, Asbjørn and Araujo, Theo and Papadopoulos, Symeon and
               Law, Effie Lai-Chong and Granmo, Ole-Christoffer and Luger, Ewa
               and Brandtzaeg, Petter Bae",
  booktitle = "Chatbot Research and Design",
  publisher = "Springer International Publishing",
  address   = "Cham",
  volume    =  11970,
  pages     = "34--48",
  series    = "Lecture Notes in Computer Science",
  year      =  2020
}

@ARTICLE{Jurafsky2001-vd,
  title     = "Probabilistic Relations between Words: Evidence from Reduction in
               Lexical Production",
  author    = "Jurafsky, Daniel and Bell, Alan and Gregory, Michelle and
               Raymond, William D",
  publisher = "books.google.com",
  abstract  = "The ideas of frequency and predictability have played a
               fundamental role in models of human language processing for well
               over a hundred years (Schuchardt 1885; Jespersen …",
  year      =  2001
}

@ARTICLE{Arora2022-jw,
  title         = "Probing Pre-Trained Language Models for Cross-Cultural
                   Differences in Values",
  author        = "Arora, Arnav and Kaffee, Lucie-Aimée and Augenstein, Isabelle",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language embeds information about social, cultural, and
                   political values people hold. Prior work has explored social
                   and potentially harmful biases encoded in Pre-Trained
                   Language models (PTLMs). However, there has been no
                   systematic study investigating how values embedded in these
                   models vary across cultures. In this paper, we introduce
                   probes to study which values across cultures are embedded in
                   these models, and whether they align with existing theories
                   and cross-cultural value surveys. We find that PTLMs capture
                   differences in values across cultures, but those only weakly
                   align with established value surveys. We discuss implications
                   of using mis-aligned models in cross-cultural settings, as
                   well as ways of aligning PTLMs with value surveys.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ravichander2020-yu,
  title     = "Probing the probing paradigm: Does probing accuracy entail task
               relevance?",
  author    = "Ravichander, A and Belinkov, Y and Hovy, E",
  journal   = "arXiv preprint arXiv:2005.00719",
  publisher = "arxiv.org",
  abstract  = "Although neural models have achieved impressive results on
               several NLP benchmarks, little is understood about the mechanisms
               they use to perform language tasks. Thus, much recent …",
  year      =  2020
}

@ARTICLE{Shiffrin2023-ti,
  title     = "Probing the psychology of {AI} models",
  author    = "Shiffrin, Richard and Mitchell, Melanie",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  120,
  number    =  10,
  pages     = "e2300963120",
  abstract  = "Large language models (LLMs), such as OpenAI's GPT-3 and its
               successor ChatGPT, have exhibited astounding successes—as well as
               curious failures—in several areas of artificial intelligence.
               While their abilities in generating humanlike text, solving
               mathematical problems, writing computer code, and reasoning about
               the world have been widely documented, the mechanisms underlying
               both the successes and failures of these systems remain
               mysterious, even to the researchers who created them. In spite of
               the current lack of …",
  month     =  mar,
  year      =  2023,
  language  = "en"
}

@BOOK{noauthor_2024-iq,
  title     = "Proceedings of the {CHI} Conference on Human Factors in Computing
               Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  month     =  apr,
  year      =  2024
}

@ARTICLE{Solaiman2021-ua,
  title     = "Process for adapting language models to society (palms) with
               values-targeted datasets",
  author    = "Solaiman, Irene and Dennison, Christy",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "proceedings.neurips.cc",
  volume    =  34,
  pages     = "5861--5873",
  abstract  = "Abstract Language models can generate harmful and biased outputs
               and exhibit undesirable behavior according to a given cultural
               context. We propose a Process for Adapting Language Models to
               Society (PALMS) with Values-Targeted Datasets, an iterative
               process to significantly change model behavior by crafting and
               fine-tuning on a dataset that reflects a predetermined set of
               target values. We evaluate our process using three metrics:
               quantitative metrics with human evaluations that score output
               adherence to a target value …",
  year      =  2021
}

@ARTICLE{Bucher1961-ry,
  title     = "Professions in Process",
  author    = "Bucher, Rue and Strauss, Anselm",
  journal   = "Am. J. Sociol.",
  publisher = "University of Chicago Press",
  volume    =  66,
  number    =  4,
  pages     = "325--334",
  abstract  = "A process approach to professions focuses upon diversity and
               conflict of interest within a profession and their implications
               for change. The model posits the existence of a number of groups,
               called segments, within a profession, which tend to take on the
               character of social movements. Segments develop distinctive
               identities and a sense of the past and goals for the future, and
               they organize activities which will secure an institutional
               position and implement their distinctive missions. In the
               competition and conflict of segments in movement the organization
               of the profession shifts.",
  month     =  jan,
  year      =  1961
}

@ARTICLE{Nanda2023-az,
  title         = "Progress measures for grokking via mechanistic
                   interpretability",
  author        = "Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith,
                   Jess and Steinhardt, Jacob",
  journal       = "arXiv [cs.LG]",
  abstract      = "Neural networks often exhibit emergent behavior, where
                   qualitatively new capabilities arise from scaling up the
                   amount of parameters, training data, or training steps. One
                   approach to understanding emergence is to find continuous
                   \textit{progress measures} that underlie the seemingly
                   discontinuous qualitative changes. We argue that progress
                   measures can be found via mechanistic interpretability:
                   reverse-engineering learned behaviors into their individual
                   components. As a case study, we investigate the
                   recently-discovered phenomenon of ``grokking'' exhibited by
                   small transformers trained on modular addition tasks. We
                   fully reverse engineer the algorithm learned by these
                   networks, which uses discrete Fourier transforms and
                   trigonometric identities to convert addition to rotation
                   about a circle. We confirm the algorithm by analyzing the
                   activations and weights and by performing ablations in
                   Fourier space. Based on this understanding, we define
                   progress measures that allow us to study the dynamics of
                   training and split training into three continuous phases:
                   memorization, circuit formation, and cleanup. Our results
                   show that grokking, rather than being a sudden shift, arises
                   from the gradual amplification of structured mechanisms
                   encoded in the weights, followed by the later removal of
                   memorizing components.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Wang2023-re,
  title         = "Prompt-based Logical Semantics Enhancement for Implicit
                   Discourse Relation Recognition",
  author        = "Wang, Chenxu and Jian, Ping and Huang, Mu",
  journal       = "arXiv [cs.CL]",
  abstract      = "Implicit Discourse Relation Recognition (IDRR), which infers
                   discourse relations without the help of explicit connectives,
                   is still a crucial and challenging task for discourse
                   parsing. Recent works tend to exploit the hierarchical
                   structure information from the annotated senses, which
                   demonstrate enhanced discourse relation representations can
                   be obtained by integrating sense hierarchy. Nevertheless, the
                   performance and robustness for IDRR are significantly
                   constrained by the availability of annotated data.
                   Fortunately, there is a wealth of unannotated utterances with
                   explicit connectives, that can be utilized to acquire
                   enriched discourse relation features. In light of such
                   motivation, we propose a Prompt-based Logical Semantics
                   Enhancement (PLSE) method for IDRR. Essentially, our method
                   seamlessly injects knowledge relevant to discourse relation
                   into pre-trained language models through prompt-based
                   connective prediction. Furthermore, considering the
                   prompt-based connective prediction exhibits local
                   dependencies due to the deficiency of masked language model
                   (MLM) in capturing global semantics, we design a novel
                   self-supervised learning objective based on mutual
                   information maximization to derive enhanced representations
                   of logical semantics for IDRR. Experimental results on PDTB
                   2.0 and CoNLL16 datasets demonstrate that our method achieves
                   outstanding and consistent performance against the current
                   state-of-the-art models.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hu2023-sk,
  title         = "Prompting is not a substitute for probability measurements in
                   large language models",
  author        = "Hu, Jennifer and Levy, Roger",
  journal       = "arXiv [cs.CL]",
  abstract      = "Prompting is now a dominant method for evaluating the
                   linguistic knowledge of large language models (LLMs). While
                   other methods directly read out models' probability
                   distributions over strings, prompting requires models to
                   access this internal information by processing linguistic
                   input, thereby implicitly testing a new type of emergent
                   ability: metalinguistic judgment. In this study, we compare
                   metalinguistic prompting and direct probability measurements
                   as ways of measuring models' linguistic knowledge. Broadly,
                   we find that LLMs' metalinguistic judgments are inferior to
                   quantities directly derived from representations.
                   Furthermore, consistency gets worse as the prompt query
                   diverges from direct measurements of next-word probabilities.
                   Our findings suggest that negative results relying on
                   metalinguistic prompts cannot be taken as conclusive evidence
                   that an LLM lacks a particular linguistic generalization. Our
                   results also highlight the value that is lost with the move
                   to closed APIs where access to probability distributions is
                   limited.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Literat2023-xj,
  title     = "Protesting the Protest Paradigm: {TikTok} as a Space for Media
               Criticism",
  author    = "Literat, Ioana and Boxman-Shabtai, Lillian and Kligler-Vilenchik,
               Neta",
  journal   = "The International Journal of Press/Politics",
  publisher = "SAGE Publications Inc",
  volume    =  28,
  number    =  2,
  pages     = "362--383",
  abstract  = "Though news representations of protest have been studied
               extensively, little is known about how media audiences critique
               such representations. Focusing on TikTok as a space for media
               criticism, this article examines how users employ the app to
               respond to representations of protest in mainstream news media.
               Content collected in the spring of 2021 illuminated two very
               distinct foci of discussion about news representations of
               protest: the Black Lives Matter movement and the Capitol riot.
               Our qualitative content analysis of TikTok videos and their
               related comments demonstrates how users employed TikTok?s
               creative affordances to dissect specific news representations,
               critique the media apparatus, and expand news narratives. These
               findings shed light on the complex role of TikTok as a platform
               for media criticism?one that can be used for both democratic and
               non-democratic ends.",
  month     =  apr,
  year      =  2023
}

@ARTICLE{Bucinca2020-cr,
  title         = "Proxy Tasks and Subjective Measures Can Be Misleading in
                   Evaluating Explainable {AI} Systems",
  author        = "Buçinca, Zana and Lin, Phoebe and Gajos, Krzysztof Z and
                   Glassman, Elena L",
  journal       = "arXiv [cs.AI]",
  abstract      = "Explainable artificially intelligent (XAI) systems form part
                   of sociotechnical systems, e.g., human+AI teams tasked with
                   making decisions. Yet, current XAI systems are rarely
                   evaluated by measuring the performance of human+AI teams on
                   actual decision-making tasks. We conducted two online
                   experiments and one in-person think-aloud study to evaluate
                   two currently common techniques for evaluating XAI systems:
                   (1) using proxy, artificial tasks such as how well humans
                   predict the AI's decision from the given explanations, and
                   (2) using subjective measures of trust and preference as
                   predictors of actual performance. The results of our
                   experiments demonstrate that evaluations with proxy tasks did
                   not predict the results of the evaluations with the actual
                   decision-making tasks. Further, the subjective measures on
                   evaluations with actual decision-making tasks did not predict
                   the objective performance on those same tasks. Our results
                   suggest that by employing misleading evaluation methods, our
                   field may be inadvertently slowing its progress toward
                   developing human+AI teams that can reliably perform better
                   than humans or AIs alone.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{De_Freitas2023-em,
  title    = "Psychological factors underlying attitudes toward {AI} tools",
  author   = "De Freitas, Julian and Agarwal, Stuti and Schmitt, Bernd and
              Haslam, Nick",
  journal  = "Nat Hum Behav",
  volume   =  7,
  number   =  11,
  pages    = "1845--1854",
  abstract = "What are the psychological factors driving attitudes toward
              artificial intelligence (AI) tools, and how can resistance to AI
              systems be overcome when they are beneficial? Here we first
              organize the main sources of resistance into five main categories:
              opacity, emotionlessness, rigidity, autonomy and group membership.
              We relate each of these barriers to fundamental aspects of
              cognition, then cover empirical studies providing correlational or
              causal evidence for how the barrier influences attitudes toward AI
              tools. Second, we separate each of the five barriers into
              AI-related and user-related factors, which is of practical
              relevance in developing interventions towards the adoption of
              beneficial AI tools. Third, we highlight potential risks arising
              from these well-intentioned interventions. Fourth, we explain how
              the current Perspective applies to various stakeholders, including
              how to approach interventions that carry known risks, and point to
              outstanding questions for future work.",
  month    =  nov,
  year     =  2023,
  language = "en"
}

@ARTICLE{Mento2021-at,
  title    = "Psychological Impact of Pro-Anorexia and Pro-Eating Disorder
              Websites on Adolescent Females: A Systematic Review",
  author   = "Mento, Carmela and Silvestri, Maria Catena and Muscatello, Maria
              Rosaria Anna and Rizzo, Amelia and Celebre, Laura and Praticò,
              Martina and Zoccali, Rocco Antonio and Bruno, Antonio",
  journal  = "Int. J. Environ. Res. Public Health",
  volume   =  18,
  number   =  4,
  abstract = "(1) Background: Teenagers (in particular, females) suffering from
              eating disorders report being not satisfied with their physical
              aspect and they often perceive their body image in a wrong way;
              they report an excessive use of websites, defined as PRO-ANA and
              PRO-MIA, that promote an ideal of thinness, providing advice and
              suggestions about how to obtain super slim bodies. (2) Aim: The
              aim of this review is to explore the psychological impact of
              pro-ana and pro-mia websites on female teenagers. (3) Methods: We
              have carried out a systematic review of the literature on PubMed.
              The search terms that have been used are: ``Pro'' AND ``Ana'' OR
              ``Blogging'' AND ``Mia''. Initially, 161 publications were
              identified, but in total, in compliance with inclusion and
              exclusion criteria, 12 studies have been analyzed. (4) Results:
              The recent scientific literature has identified a growing number
              of Pro Ana and Pro Mia blogs which play an important role in the
              etiology of anorexia and bulimia, above all in female teenagers.
              The feelings of discomfort and dissatisfaction with their physical
              aspect, therefore, reduce their self-esteem. (5) Conclusion: These
              websites encourage anorexic and bulimic behaviors, in particular
              in female teenagers. Attention to healthy eating guidelines and
              policies during adolescence, focused on correcting eating
              behavioral aspects, is very important to prevent severe forms of
              psychopathology with more vulnerability in the perception of body
              image, social desirability, and negative emotional feedback.",
  month    =  feb,
  year     =  2021,
  keywords = "eating abnormal behavior; female adolescents; pro-ana and pro-mia
              websites",
  language = "en"
}

@ARTICLE{Prystawski2022-sa,
  title         = "Psychologically-informed chain-of-thought prompts for
                   metaphor understanding in large language models",
  author        = "Prystawski, Ben and Thibodeau, Paul and Potts, Christopher
                   and Goodman, Noah D",
  journal       = "arXiv [cs.CL]",
  abstract      = "Probabilistic models of language understanding are valuable
                   tools for investigating human language use. However, they
                   need to be hand-designed for a particular domain. In
                   contrast, large language models (LLMs) are trained on text
                   that spans a wide array of domains, but they lack the
                   structure and interpretability of probabilistic models. In
                   this paper, we use chain-of-thought prompts to introduce
                   structures from probabilistic models into LLMs. We explore
                   this approach in the case of metaphor understanding. Our
                   chain-of-thought prompts lead language models to infer latent
                   variables and reason about their relationships in order to
                   choose appropriate paraphrases for metaphors. The latent
                   variables and relationships chosen are informed by theories
                   of metaphor understanding from cognitive psychology. We apply
                   these prompts to the two largest versions of GPT-3 and show
                   that they can improve performance in a paraphrase selection
                   task.",
  month         =  sep,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Erscoi_undated-nf,
  title  = "Pygmalion Displacement: When Humanising {AI} Dehumanises Women",
  author = "Erscoi, Lelia A and Kleinherenbrink, Annelies and Guest, Olivia"
}

@ARTICLE{Porter1992-eu,
  title     = "Quantification and the Accounting Ideal in Science",
  author    = "Porter, Theodore M",
  journal   = "Soc. Stud. Sci.",
  publisher = "Sage Publications, Ltd.",
  volume    =  22,
  number    =  4,
  pages     = "633--651",
  abstract  = "[Objectivity in science has normally been defined by scholars as
               almost synonymous with realism. It may be advantageous to think
               of it instead in terms of impersonality, an ideal that would
               replace arbitrariness, idiosyncracy and judgment by explicit
               rules. Accounting is an exemplar of this aspect of objectivity.
               More important than the true representation of deep underlying
               financial identities is the maintenance of a system of rules that
               blocks self-interested distortion. Otherwise, tax codes and
               corporate reports would lose their credibility. From this
               standpoint, quantification appears as a strategy for overcoming
               distance and distrust. This pertains also to the natural
               sciences, where measurement and statistics have been crucial in
               transforming local experimental skills into public knowledge. We
               need to understand quantification as a response to a set of
               political problems, part of the moral economy of science. Its use
               in science is analogous in important ways to the explicitly
               political and administrative purposes served by accounting.]",
  year      =  1992
}

@MISC{noauthor_undated-tv,
  title = "Quantifying Disparate {Questioning\_JELS}.pdf"
}

@ARTICLE{Pei2020-uq,
  title         = "Quantifying intimacy in language",
  author        = "Pei, Jiaxin and Jurgens, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "Intimacy is a fundamental aspect of how we relate to others
                   in social settings. Language encodes the social information
                   of intimacy through both topics and other more subtle cues
                   (such as linguistic hedging and swearing). Here, we introduce
                   a new computational framework for studying expressions of the
                   intimacy in language with an accompanying dataset and deep
                   learning model for accurately predicting the intimacy level
                   of questions (Pearson's r=0.87). Through analyzing a dataset
                   of 80.5M questions across social media, books, and films, we
                   show that individuals employ interpersonal pragmatic moves in
                   their language to align their intimacy with social settings.
                   Then, in three studies, we further demonstrate how
                   individuals modulate their intimacy to match social norms
                   around gender, social distance, and audience, each validating
                   key findings from studies in social psychology. Our work
                   demonstrates that intimacy is a pervasive and impactful
                   social dimension of language.",
  month         =  nov,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Waller2021-du,
  title    = "Quantifying social organization and political polarization in
              online platforms",
  author   = "Waller, Isaac and Anderson, Ashton",
  journal  = "Nature",
  volume   =  600,
  number   =  7888,
  pages    = "264--268",
  abstract = "Mass selection into groups of like-minded individuals may be
              fragmenting and polarizing online society, particularly with
              respect to partisan differences1-4. However, our ability to
              measure the social makeup of online communities and in turn, to
              understand the social organization of online platforms, is limited
              by the pseudonymous, unstructured and large-scale nature of
              digital discussion. Here we develop a neural-embedding methodology
              to quantify the positioning of online communities along social
              dimensions by leveraging large-scale patterns of aggregate
              behaviour. Applying our methodology to 5.1 billion comments made
              in 10,000 communities over 14 years on Reddit, we measure how the
              macroscale community structure is organized with respect to age,
              gender and US political partisanship. Examining political content,
              we find that Reddit underwent a significant polarization event
              around the 2016 US presidential election. Contrary to conventional
              wisdom, however, individual-level polarization is rare; the
              system-level shift in 2016 was disproportionately driven by the
              arrival of new users. Political polarization on Reddit is
              unrelated to previous activity on the platform and is instead
              temporally aligned with external events. We also observe a stark
              ideological asymmetry, with the sharp increase in polarization in
              2016 being entirely attributable to changes in right-wing
              activity. This methodology is broadly applicable to the study of
              online interaction, and our findings have implications for the
              design of online platforms, understanding the social contexts of
              online behaviour, and quantifying the dynamics and mechanisms of
              online polarization.",
  month    =  dec,
  year     =  2021,
  language = "en"
}

@ARTICLE{Hessel2018-oj,
  title         = "Quantifying the visual concreteness of words and topics in
                   multimodal datasets",
  author        = "Hessel, Jack and Mimno, David and Lee, Lillian",
  journal       = "arXiv [cs.CL]",
  abstract      = "Multimodal machine learning algorithms aim to learn
                   visual-textual correspondences. Previous work suggests that
                   concepts with concrete visual manifestations may be easier to
                   learn than concepts with abstract ones. We give an algorithm
                   for automatically computing the visual concreteness of words
                   and topics within multimodal datasets. We apply the approach
                   in four settings, ranging from image captions to images/text
                   scraped from historical books. In addition to enabling
                   explorations of concepts in multimodal datasets, our
                   concreteness scores predict the capacity of machine learning
                   algorithms to learn textual/visual relationships. We find
                   that 1) concrete concepts are indeed easier to learn; 2) the
                   large number of algorithms we consider have similar failure
                   cases; 3) the precise positive relationship between
                   concreteness and performance varies between datasets. We
                   conclude with recommendations for using concreteness scores
                   to facilitate future multimodal research.",
  month         =  apr,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Dinan2019-kt,
  title         = "Queens are Powerful too: Mitigating Gender Bias in Dialogue
                   Generation",
  author        = "Dinan, Emily and Fan, Angela and Williams, Adina and Urbanek,
                   Jack and Kiela, Douwe and Weston, Jason",
  journal       = "arXiv [cs.CL]",
  abstract      = "Models often easily learn biases present in the training
                   data, and their predictions directly reflect this bias. We
                   analyze gender bias in dialogue data, and examine how this
                   bias is actually amplified in subsequent generative chit-chat
                   dialogue models. We measure gender bias in six existing
                   dialogue datasets, and focus on the most biased one, the
                   multi-player text-based fantasy adventure dataset LIGHT, as a
                   testbed for our bias mitigation techniques. The LIGHT dataset
                   is highly imbalanced with respect to gender, containing
                   predominantly male characters, likely because it is entirely
                   collected by crowdworkers and reflects common biases that
                   exist in fantasy or medieval settings. We consider three
                   techniques to mitigate gender bias: counterfactual data
                   augmentation, targeted data collection, and bias controlled
                   training. We show that our proposed techniques mitigate
                   gender bias in LIGHT by balancing the genderedness of
                   generated dialogue utterances and are particularly effective
                   in combination. We quantify performance using various
                   evaluation methods---such as quantity of gendered words, a
                   dialogue safety classifier, and human studies---all of which
                   show that our models generate less gendered, but equally
                   engaging chit-chat responses.",
  month         =  nov,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@BOOK{Cipolla2017-el,
  title     = "Queer feminist science studies: A reader",
  editor    = "Cipolla, Cyd and Gupta, Kristina and Rubin, David A and Willey,
               Angela",
  publisher = "University of Washington Press",
  address   = "Washington, D.C., DC",
  month     =  nov,
  year      =  2017
}

@ARTICLE{Zelikman2024-ne,
  title         = "Quiet-{STaR}: Language Models Can Teach Themselves to Think
                   Before Speaking",
  author        = "Zelikman, Eric and Harik, Georges and Shao, Yijia and
                   Jayasiri, Varuna and Haber, Nick and Goodman, Noah D",
  journal       = "arXiv [cs.CL]",
  abstract      = "When writing and talking, people sometimes pause to think.
                   Although reasoning-focused works have often framed reasoning
                   as a method of answering questions or completing agentic
                   tasks, reasoning is implicit in almost all written text. For
                   example, this applies to the steps not stated between the
                   lines of a proof or to the theory of mind underlying a
                   conversation. In the Self-Taught Reasoner (STaR, Zelikman et
                   al. 2022), useful thinking is learned by inferring rationales
                   from few-shot examples in question-answering and learning
                   from those that lead to a correct answer. This is a highly
                   constrained setting -- ideally, a language model could
                   instead learn to infer unstated rationales in arbitrary text.
                   We present Quiet-STaR, a generalization of STaR in which LMs
                   learn to generate rationales at each token to explain future
                   text, improving their predictions. We address key challenges,
                   including 1) the computational cost of generating
                   continuations, 2) the fact that the LM does not initially
                   know how to generate or use internal thoughts, and 3) the
                   need to predict beyond individual next tokens. To resolve
                   these, we propose a tokenwise parallel sampling algorithm,
                   using learnable tokens indicating a thought's start and end,
                   and an extended teacher-forcing technique. Encouragingly,
                   generated rationales disproportionately help model
                   difficult-to-predict tokens and improve the LM's ability to
                   directly answer difficult questions. In particular, after
                   continued pretraining of an LM on a corpus of internet text
                   with Quiet-STaR, we find zero-shot improvements on GSM8K
                   (5.9\%$\rightarrow$10.9\%) and CommonsenseQA
                   (36.3\%$\rightarrow$47.2\%) and observe a perplexity
                   improvement of difficult tokens in natural text. Crucially,
                   these improvements require no fine-tuning on these tasks.
                   Quiet-STaR marks a step towards LMs that can learn to reason
                   in a more general and scalable way.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Sen2016-nb,
  title     = "Race as a Bundle of Sticks: Designs that Estimate Effects of
               Seemingly Immutable Characteristics",
  author    = "Sen, Maya and Wasow, Omar",
  journal   = "Annu. Rev. Polit. Sci.",
  publisher = "Annual Reviews",
  volume    =  19,
  number    =  1,
  pages     = "499--522",
  abstract  = "Although understanding the role of race, ethnicity, and identity
               is central to political science, methodological debates persist
               about whether it is possible to estimate the effect of something
               immutable. At the heart of the debate is an older theoretical
               question: Is race best understood under an essentialist or
               constructivist framework? In contrast to the ?immutable
               characteristics? or essentialist approach, we argue that race
               should be operationalized as a ?bundle of sticks? that can be
               disaggregated into elements. With elements of race, causal claims
               may be possible using two designs: (a) studies that measure the
               effect of exposure to a racial cue and (b) studies that exploit
               within-group variation to measure the effect of some manipulable
               element. These designs can reconcile scholarship on race and
               causation and offer a clear framework for future research.",
  month     =  may,
  year      =  2016
}

@BOOK{Alim2016-nt,
  title     = "Raciolinguistics: How language shapes our ideas about race",
  editor    = "Alim, H Samy and Rickford, John R and Ball, Arnetha F",
  publisher = "Oxford University Press",
  address   = "New York, NY",
  month     =  nov,
  year      =  2016
}

@MISC{noauthor_undated-jj,
  title = "Racist Terminology in Criminal Trials.pdf"
}

@ARTICLE{Yao2022-rj,
  title         = "{ReAct}: Synergizing Reasoning and Acting in Language Models",
  author        = "Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and
                   Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan",
  journal       = "arXiv [cs.CL]",
  abstract      = "While large language models (LLMs) have demonstrated
                   impressive capabilities across tasks in language
                   understanding and interactive decision making, their
                   abilities for reasoning (e.g. chain-of-thought prompting) and
                   acting (e.g. action plan generation) have primarily been
                   studied as separate topics. In this paper, we explore the use
                   of LLMs to generate both reasoning traces and task-specific
                   actions in an interleaved manner, allowing for greater
                   synergy between the two: reasoning traces help the model
                   induce, track, and update action plans as well as handle
                   exceptions, while actions allow it to interface with external
                   sources, such as knowledge bases or environments, to gather
                   additional information. We apply our approach, named ReAct,
                   to a diverse set of language and decision making tasks and
                   demonstrate its effectiveness over state-of-the-art
                   baselines, as well as improved human interpretability and
                   trustworthiness over methods without reasoning or acting
                   components. Concretely, on question answering (HotpotQA) and
                   fact verification (Fever), ReAct overcomes issues of
                   hallucination and error propagation prevalent in
                   chain-of-thought reasoning by interacting with a simple
                   Wikipedia API, and generates human-like task-solving
                   trajectories that are more interpretable than baselines
                   without reasoning traces. On two interactive decision making
                   benchmarks (ALFWorld and WebShop), ReAct outperforms
                   imitation and reinforcement learning methods by an absolute
                   success rate of 34\% and 10\% respectively, while being
                   prompted with only one or two in-context examples. Project
                   site with code: https://react-lm.github.io",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Sinclair1999-vq,
  title     = "Reactions to a Black professional: Motivated inhibition and
               activation of conflicting stereotypes",
  author    = "Sinclair, Lisa and Kunda, Ziva",
  journal   = "J. Pers. Soc. Psychol.",
  publisher = "American Psychological Association (APA)",
  volume    =  77,
  number    =  5,
  pages     = "885--904",
  month     =  nov,
  year      =  1999,
  language  = "en"
}

@ARTICLE{Harrison2008-nt,
  title     = "Real men do wear mascara: advertising discourse and masculine
               identity",
  author    = "Harrison, Claire",
  journal   = "Critical Discourse Studies",
  publisher = "Routledge",
  volume    =  5,
  number    =  1,
  pages     = "55--74",
  abstract  = "During the past two decades, the traditional concept of
               masculinity has been challenged by the pervasive spread of
               metrosexual attitudes and practices through Western cultures.
               This article examines an extreme aspect of this trend through a
               multimodal reading of an online advertisement for male mascara.
               Using social semiotic theory and methodologies based on
               functional grammars, the analysis reveals that the
               advertisement's producers are treading a fine line in their
               verbal and visual discursive choices, trying to create a
               dialectic that encourages men to be consumers of feminine-style
               products while also allowing them to maintain the qualities that
               have traditionally been gendered as masculine.",
  month     =  feb,
  year      =  2008
}

@INPROCEEDINGS{Whitney2024-hk,
  title     = "Real Risks of Fake Data: Synthetic Data, Diversity-Washing and
               Consent Circumvention",
  author    = "Whitney, Cedric Deslandes and Norman, Justin",
  booktitle = "The 2024 ACM Conference on Fairness, Accountability, and
               Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "1733--1744",
  month     =  jun,
  year      =  2024
}

@ARTICLE{Sap_undated-gx,
  title  = "Reasoning about Social and Power Implications of Language",
  author = "Sap, Maarten and Gabriel, Saadia and Qin, Lianhui and Jurafsky, Dan
            and Smith, Noah A and Choi, Yejin"
}

@ARTICLE{Ni2022-hq,
  title     = "Recent advances in deep learning based dialogue systems: a
               systematic survey",
  author    = "Ni, Jinjie and Young, Tom and Pandelea, Vlad and Xue, Fuzhao and
               Cambria, Erik",
  journal   = "Artificial Intelligence Review",
  publisher = "Springer",
  abstract  = "Dialogue systems are a popular natural language processing (NLP)
               task as it is promising in real-life applications. It is also a
               complicated task since many NLP tasks deserving study are
               involved. As a result, a multitude of novel works on this task
               are carried out, and most of them are deep learning based due to
               their outstanding performance. In this survey, we mainly focus on
               the deep learning based dialogue systems. We comprehensively
               review state-of-the-art research outcomes in dialogue systems and
               analyze them from two angles: model type and system type.
               Specifically, from the angle of model type, we discuss the
               principles, characteristics, and applications of different models
               that are widely used in dialogue systems. This will help
               researchers acquaint these models and see how they are applied in
               state-of-the-art frameworks, which is rather helpful when
               designing a new dialogue system. From the angle of system type,
               we discuss task-oriented and open-domain dialogue systems as two
               streams of research, providing insight into the hot topics
               related. Furthermore, we comprehensively review the evaluation
               methods and datasets for dialogue systems to pave the way for
               future research. Finally, some possible research trends are
               identified based on the recent research outcomes. To the best of
               our knowledge, this survey is the most comprehensive and
               up-to-date one at present for deep learning based dialogue
               systems, extensively covering the popular techniques. We
               speculate that this work is a good starting point for academics
               who are new to the dialogue systems or those who want to quickly
               grasp up-to-date techniques in this area.",
  month     =  aug,
  year      =  2022
}

@ARTICLE{Van_Rooij_undated-dj,
  title  = "Reclaiming {AI} as a theoretical tool for cognitive science",
  author = "van Rooij, Iris and Guest, Olivia and Adolfi, Federico and de Haan,
            Ronald and Kolokolova, Antonina and Rich, Patricia"
}

@ARTICLE{Author_undated-av,
  title  = "Recommendations for developing and evaluating models of treatment
            effects for time-to-event data",
  author = "Author, Anonymous"
}

@ARTICLE{Bamberg_undated-su,
  title  = "Reconsidering counter- narratives",
  author = "Bamberg, Michael and Wipff, Zachary"
}

@ARTICLE{Luo2024-ks,
  title         = "Reflecting the Male Gaze: Quantifying Female Objectification
                   in {19th} and {20th} Century Novels",
  author        = "Luo, Kexin and Mao, Yue and Zhang, Bei and Hao, Sophie",
  journal       = "arXiv [cs.CL]",
  abstract      = "Inspired by the concept of the male gaze (Mulvey, 1975) in
                   literature and media studies, this paper proposes a framework
                   for analyzing gender bias in terms of female objectification:
                   the extent to which a text portrays female individuals as
                   objects of visual pleasure. Our framework measures female
                   objectification along two axes. First, we compute an agency
                   bias score that indicates whether male entities are more
                   likely to appear in the text as grammatical agents than
                   female entities. Next, by analyzing the word embedding space
                   induced by a text (Caliskan et al., 2017), we compute an
                   appearance bias score that indicates whether female entities
                   are more closely associated with appearance-related words
                   than male entities. Applying our framework to 19th and 20th
                   century novels reveals evidence of female objectification in
                   literature: we find that novels written from a male
                   perspective systematically objectify female characters, while
                   novels written from a female perspective do not exhibit
                   statistically significant objectification of any gender.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Kapoor2023-sh,
  title         = "{REFORMS}: Reporting Standards for Machine Learning Based
                   Science",
  author        = "Kapoor, Sayash and Cantrell, Emily and Peng, Kenny and Pham,
                   Thanh Hien and Bail, Christopher A and Gundersen, Odd Erik
                   and Hofman, Jake M and Hullman, Jessica and Lones, Michael A
                   and Malik, Momin M and Nanayakkara, Priyanka and Poldrack,
                   Russell A and Raji, Inioluwa Deborah and Roberts, Michael and
                   Salganik, Matthew J and Serra-Garcia, Marta and Stewart,
                   Brandon M and Vandewiele, Gilles and Narayanan, Arvind",
  journal       = "arXiv [cs.LG]",
  abstract      = "Machine learning (ML) methods are proliferating in scientific
                   research. However, the adoption of these methods has been
                   accompanied by failures of validity, reproducibility, and
                   generalizability. These failures can hinder scientific
                   progress, lead to false consensus around invalid claims, and
                   undermine the credibility of ML-based science. ML methods are
                   often applied and fail in similar ways across disciplines.
                   Motivated by this observation, our goal is to provide clear
                   reporting standards for ML-based science. Drawing from an
                   extensive review of past literature, we present the REFORMS
                   checklist ($\textbf{Re}$porting Standards $\textbf{For}$
                   $\textbf{M}$achine Learning Based $\textbf{S}$cience). It
                   consists of 32 questions and a paired set of guidelines.
                   REFORMS was developed based on a consensus of 19 researchers
                   across computer science, data science, mathematics, social
                   sciences, and biomedical sciences. REFORMS can serve as a
                   resource for researchers when designing and implementing a
                   study, for referees when reviewing papers, and for journals
                   when enforcing standards for transparency and
                   reproducibility.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Gkoumas2023-su,
  title         = "Reformulating {NLP} tasks to Capture Longitudinal
                   Manifestation of Language Disorders in People with Dementia",
  author        = "Gkoumas, Dimitris and Purver, Matthew and Liakata, Maria",
  journal       = "arXiv [cs.CL]",
  abstract      = "Dementia is associated with language disorders which impede
                   communication. Here, we automatically learn linguistic
                   disorder patterns by making use of a moderately-sized
                   pre-trained language model and forcing it to focus on
                   reformulated natural language processing (NLP) tasks and
                   associated linguistic patterns. Our experiments show that NLP
                   tasks that encapsulate contextual information and enhance the
                   gradient signal with linguistic patterns benefit performance.
                   We then use the probability estimates from the best model to
                   construct digital linguistic markers measuring the overall
                   quality in communication and the intensity of a variety of
                   language disorders. We investigate how the digital markers
                   characterize dementia speech from a longitudinal perspective.
                   We find that our proposed communication marker is able to
                   robustly and reliably characterize the language of people
                   with dementia, outperforming existing linguistic approaches;
                   and shows external validity via significant correlation with
                   clinical markers of behaviour. Finally, our proposed
                   linguistic disorder markers provide useful insights into
                   gradual language impairment associated with disease
                   progression.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Barone2017-ey,
  title         = "Regularization techniques for fine-tuning in neural machine
                   translation",
  author        = "Barone, Antonio Valerio Miceli and Haddow, Barry and Germann,
                   Ulrich and Sennrich, Rico",
  journal       = "arXiv [cs.CL]",
  abstract      = "We investigate techniques for supervised domain adaptation
                   for neural machine translation where an existing model
                   trained on a large out-of-domain dataset is adapted to a
                   small in-domain dataset. In this scenario, overfitting is a
                   major challenge. We investigate a number of techniques to
                   reduce overfitting and improve transfer learning, including
                   regularization techniques such as dropout and
                   L2-regularization towards an out-of-domain prior. In
                   addition, we introduce tuneout, a novel regularization
                   technique inspired by dropout. We apply these techniques,
                   alone and in combination, to neural machine translation,
                   obtaining improvements on IWSLT datasets for English->German
                   and English->Russian. We also investigate the amounts of
                   in-domain training data needed for domain adaptation in NMT,
                   and find a logarithmic relationship between the amount of
                   training data and gain in BLEU score.",
  month         =  jul,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Lamo2018-oq,
  title     = "Regulating bot speech",
  author    = "Lamo, Madeline and Calo, Ryan",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  year      =  2018,
  language  = "en"
}

@ARTICLE{Zhou2024-oy,
  title         = "Rel-A.i.: An interaction-centered approach to measuring
                   human-{LM} reliance",
  author        = "Zhou, Kaitlyn and Hwang, Jena D and Ren, Xiang and Dziri,
                   Nouha and Jurafsky, Dan and Sap, Maarten",
  journal       = "arXiv [cs.CL]",
  abstract      = "The reconfiguration of human-LM interactions from simple
                   sentence completions to complex, multi-domain, humanlike
                   engagements necessitates new methodologies to understand how
                   humans choose to rely on LMs. In our work, we contend that
                   reliance is influenced by numerous factors within the
                   interactional context of a generation, a departure from prior
                   work that used verbalized confidence (e.g., ``I'm certain the
                   answer is...'') as the key determinant of reliance. Here, we
                   introduce Rel-A.I., an in situ, system-level evaluation
                   approach to measure human reliance on LM-generated epistemic
                   markers (e.g., ``I think it's..'', ``Undoubtedly it's...'').
                   Using this methodology, we measure reliance rates in three
                   emergent human-LM interaction settings: long-term
                   interactions, anthropomorphic generations, and variable
                   subject matter. Our findings reveal that reliance is not
                   solely based on verbalized confidence but is significantly
                   affected by other features of the interaction context. Prior
                   interactions, anthropomorphic cues, and subject domain all
                   contribute to reliance variability. An expression such as,
                   ``I'm pretty sure it's...'', can vary up to 20\% in reliance
                   frequency depending on its interactional context. Our work
                   underscores the importance of context in understanding human
                   reliance and offers future designers and researchers with a
                   methodology to conduct such measurements.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ash2021-wk,
  title         = "{RELATIO}: Text semantics capture political and economic
                   narratives",
  author        = "Ash, Elliott and Gauthier, Germain and Widmer, Philine",
  journal       = "arXiv [econ.GN]",
  abstract      = "Social scientists have become increasingly interested in how
                   narratives -- the stories in fiction, politics, and life --
                   shape beliefs, behavior, and government policies. This paper
                   provides an unsupervised method to quantify latent narrative
                   structures in text documents. Our new software package
                   RELATIO identifies coherent entity groups and maps explicit
                   relations between them in the text. We provide an application
                   to the United States Congressional Record to analyze
                   political and economic narratives in recent decades. Our
                   analysis highlights the dynamics, sentiment, polarization,
                   and interconnectedness of narratives in political discourse.",
  month         =  aug,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "econ.GN"
}

@ARTICLE{Liu2022-mo,
  title     = "Relational memory-augmented language models",
  author    = "Liu, Qi and Yogatama, Dani and Blunsom, Phil",
  journal   = "Transactions of the Association for Computational Linguistics",
  publisher = "MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts
               02142, USA …",
  volume    =  10,
  pages     = "555--572",
  year      =  2022
}

@BOOK{Enfield2013-hs,
  title     = "Relationship Thinking: Agency, Enchrony, and Human Sociality",
  author    = "Enfield, N J",
  publisher = "Oxford University Press",
  abstract  = "In Relationship Thinking, N. J. Enfield outlines a framework for
               analyzing social interaction and its linguistic, cultural, and
               cognitive underpinnings by focusing on human relationships. This
               is a naturalistic approach to human sociality, grounded in the
               systematic study of real-time data from social interaction in
               everyday life. Many of the illustrative examples and analyses in
               the book are a result of the author's long-term field work in
               Laos. Enfield promotes an interdisciplinary approach to studying
               language, culture, and mind, building on simple but powerful
               semiotic principles and concentrating on three points of
               conceptual focus. The first is human agency: the combination of
               flexibility and accountability, which defines our possibilities
               for social action and relationships, and which makes the fission
               and fusion of social units possible. The second is enchrony: the
               timescale of conversation in which our social relationships are
               primarily enacted. The third is human sociality: a range of human
               propensities for social interaction and enduring social
               relations, grounded in collective commitment to shared norms.
               Enfield's approach cuts through common dichotomies such as
               'cognitive' versus 'behaviorist', or 'public' versus 'private',
               arguing instead that these are indispensable sides of single
               phenomena. The result is a set of conceptual tools for analyzing
               real-time social interaction and linking it with enduring
               relationships and their social contexts. The book shows that even
               - or perhaps especially - the most mundane social interactions
               yield rich insights into language, culture, and mind.",
  month     =  nov,
  year      =  2013,
  language  = "en"
}

@INPROCEEDINGS{Gordon2013-ah,
  title     = "Reporting bias and knowledge acquisition",
  author    = "Gordon, Jonathan and Van Durme, Benjamin",
  booktitle = "Proceedings of the 2013 workshop on Automated knowledge base
               construction",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "25--30",
  abstract  = "Much work in knowledge extraction from text tacitly assumes that
               the frequency with which people write about actions, outcomes, or
               properties is a reflection of real-world frequencies or the
               degree to which a property is characteristic of a class of
               individuals. In this paper, we question this idea, examining the
               phenomenon of reporting bias and the challenge it poses for
               knowledge extraction. We conclude with discussion of approaches
               to learning commonsense knowledge from text despite this
               distortion.",
  series    = "AKBC '13",
  month     =  oct,
  year      =  2013,
  keywords  = "text frequency, knowledge extraction, reporting bias"
}

@ARTICLE{Zeng2020-hq,
  title     = "Reposting “till Albert Einstein is {TikTok} famous”: the memetic
               construction of science on {TikTok}",
  author    = "Zeng, Jing and Schäfer, Mike S and Allgaier, Joachim",
  journal   = "Int. J. Commun. Syst.",
  publisher = "University of Southern California",
  volume    =  15,
  pages     = "3216--3247",
  abstract  = "Since its launch in 2018, TikTok has become one of the fastest
               growing social media applications in the world, being
               particularly popular among young people. Memetic videos, which
               often feature lip-syncing, dance routines, and comedic skits, are
               a defining feature of the platform. This study used quantitative
               content analysis and qualitative thematic analysis to examine
               science memes, an increasingly popular genre of memes on TikTok,
               by analyzing 1,368 TikTok videos that feature science-related
               content. The results of the study uncover the most influential
               science-content creators, the most prevalent content in science
               memes, and three vernacular styles of science memes on TikTok.
               The results expand the existing science-communication scholarship
               focusing on the context of social media. Understanding the role
               of memetic science content on short-video platforms, as well as
               in the youth digital culture in general, also provides valuable
               insights into how science communicators can better engage with
               members of the young generation.",
  month     =  mar,
  year      =  2020,
  keywords  = "TikTok, science communication, memes, youth digital culture,
               short videos; TikTok, science communication, memes, youth digital
               culture, short videos",
  language  = "en"
}

@ARTICLE{Zou2023-dn,
  title         = "Representation Engineering: A Top-Down Approach to {AI}
                   Transparency",
  author        = "Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James
                   and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin,
                   Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and
                   Goel, Shashwat and Li, Nathaniel and Byun, Michael J and
                   Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo,
                   Sanmi and Song, Dawn and Fredrikson, Matt and Zico Kolter, J
                   and Hendrycks, Dan",
  journal       = "arXiv [cs.LG]",
  abstract      = "In this paper, we identify and characterize the emerging area
                   of representation engineering (RepE), an approach to
                   enhancing the transparency of AI systems that draws on
                   insights from cognitive neuroscience. RepE places
                   population-level representations, rather than neurons or
                   circuits, at the center of analysis, equipping us with novel
                   methods for monitoring and manipulating high-level cognitive
                   phenomena in deep neural networks (DNNs). We provide
                   baselines and an initial analysis of RepE techniques, showing
                   that they offer simple yet effective solutions for improving
                   our understanding and control of large language models. We
                   showcase how these methods can provide traction on a wide
                   range of safety-relevant problems, including honesty,
                   harmlessness, power-seeking, and more, demonstrating the
                   promise of top-down transparency research. We hope that this
                   work catalyzes further exploration of RepE and fosters
                   advancements in the transparency and safety of AI systems.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Miltsov2022-sh,
  title    = "Researching {TikTok}: Themes, Methods, and Future Directions",
  author   = "Miltsov, Alex",
  abstract = "TikTok, a short video-sharing social media platform, has quickly
              become one of the most popular apps. The platform offers a highly
              immersive and interactive environment, where users share original
              content and participate in challenges, duets, and other tasks.
              Even though TikTok is only a few years old, it has already been
              shaping the ways millions of people interact online and engage in
              artistic, cultural, social, and political activities. This chapter
              provides a comprehensive overview of emerging TikTok studies. It
              shows that there is a growing interest in studying TikTok and its
              social effects. Social scientists have applied a wide range of
              methodological approaches to explore users' experiences on TikTok
              and the platform's effects on society. This chapter discusses the
              most effective research strategies in TikTok studies, examines
              specific cases of several research projects, and suggests
              directions for future research.",
  month    =  oct,
  year     =  2022
}

@ARTICLE{Bender2024-de,
  title     = "Resisting Dehumanization in the Age of “AI”",
  author    = "Bender, Emily M",
  journal   = "Curr. Dir. Psychol. Sci.",
  publisher = "SAGE Publications Inc",
  volume    =  33,
  number    =  2,
  pages     = "114--120",
  abstract  = "The production and promotion of ?AI? technology involves
               dehumanization on many fronts. I explore these processes of
               dehumanization and the role that cognitive science can play by
               bringing a richer picture of human cognition to the discourse.",
  month     =  apr,
  year      =  2024
}

@ARTICLE{Lekas2020-ah,
  title    = "Rethinking Cultural Competence: Shifting to Cultural Humility",
  author   = "Lekas, Helen-Maria and Pahl, Kerstin and Fuller Lewis, Crystal",
  journal  = "Health Serv Insights",
  volume   =  13,
  pages    =  1178632920970580,
  abstract = "Healthcare and social services providers are deemed culturally
              competent when they offer culturally appropriate care to the
              populations they serve. While a review of the literature
              highlights the limited effectiveness of cultural competence
              training, its value remains largely unchallenged and it is
              institutionally mandated as a means of decreasing health
              disparities and improving quality of care. A plethora of trainings
              are designed to expose providers to different cultures and expand
              their understanding of the beliefs, values and behavior thus,
              achieving competence. Although this intention is commendable,
              training providers in becoming competent in various cultures
              presents the risk of stereotyping, stigmatizing, and othering
              patients and can foster implicit racist attitudes and behaviors.
              Further, by disregarding intersectionality, cultural competence
              trainings tend to undermine provider recognition that patients
              inhabit multiple social statuses that potentially shape their
              beliefs, values and behavior. To address these risks, we propose
              training providers in cultural humility, that is, an orientation
              to care that is based on self-reflexivity, appreciation of
              patients' lay expertise, openness to sharing power with patients,
              and to continue learning from one's patients. We also briefly
              discuss our own cultural humility training. Training providers in
              cultural humility and abandoning the term cultural competence is a
              long-awaited paradigm shift that must be advanced.",
  month    =  dec,
  year     =  2020,
  keywords = "Cultural competence; cultural humility; intersectionality; racism;
              stereotype",
  language = "en"
}

@INPROCEEDINGS{Eyert2023-xb,
  title     = "Rethinking Transparency as a Communicative Constellation",
  author    = "Eyert, Florian and Lopez, Paola",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "444--454",
  abstract  = "In this paper we make the case for an expanded understanding of
               transparency. Within the now extensive FAccT literature,
               transparency has largely been understood in terms of
               explainability. While this approach has proven helpful in many
               contexts, it falls short of addressing some of the more
               fundamental issues in the development and application of machine
               learning, such as the epistemic limitations of predictions and
               the political nature of the selection of fairness criteria. In
               order to render machine learning systems more democratic, we
               argue, a broader understanding of transparency is needed. We
               therefore propose to view transparency as a communicative
               constellation that is a precondition for meaningful democratic
               deliberation. We discuss four perspective expansions implied by
               this approach and present a case study illustrating the interplay
               of heterogeneous actors involved in producing this constellation.
               Drawing from our conceptualization of transparency, we sketch
               implications for actor groups in different sectors of society.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "explainability, transparency, prediction, deliberation, science
               communication"
}

@ARTICLE{Wang2021-zy,
  title         = "Retrieval Enhanced Model for Commonsense Generation",
  author        = "Wang, Han and Liu, Yang and Zhu, Chenguang and Shou, Linjun
                   and Gong, Ming and Xu, Yichong and Zeng, Michael",
  journal       = "arXiv [cs.CL]",
  abstract      = "Commonsense generation is a challenging task of generating a
                   plausible sentence describing an everyday scenario using
                   provided concepts. Its requirement of reasoning over
                   commonsense knowledge and compositional generalization
                   ability even puzzles strong pre-trained language generation
                   models. We propose a novel framework using retrieval methods
                   to enhance both the pre-training and fine-tuning for
                   commonsense generation. We retrieve prototype sentence
                   candidates by concept matching and use them as auxiliary
                   input. For fine-tuning, we further boost its performance with
                   a trainable sentence retriever. We demonstrate experimentally
                   on the large-scale CommonGen benchmark that our approach
                   achieves new state-of-the-art results.",
  month         =  may,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wang2022-jo,
  title     = "{REVISE}: A tool for measuring and mitigating bias in visual
               datasets",
  author    = "Wang, A and Liu, A and Zhang, R and Kleiman, A and Kim, L and
               {others}",
  journal   = "International Journal of",
  publisher = "Springer",
  abstract  = "Abstract Machine learning models are known to perpetuate and even
               amplify the biases present in the data. However, these data
               biases frequently do not become apparent until …",
  year      =  2022
}

@INPROCEEDINGS{Kocaballi2020-xk,
  title     = "Revisiting Habitability in Conversational Systems",
  author    = "Kocaballi, A Baki and Coiera, Enrico and Berkovsky, Shlomo",
  booktitle = "Extended Abstracts of the 2020 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1--8",
  abstract  = "Conversational systems are inherently disadvantaged when
               indicating either what capabilities they have or the state they
               are in. The notion of habitability, the appropriate balancing in
               design between the language people use and the language a system
               can accept, emerged out of these early difficulties with
               conversational systems. This literature review aims to summarize
               progress in habitability research and explore implications for
               the design of current AI-enabled conversational systems. We found
               that i) the definitions of habitability focus mostly on matching
               between user expectations and system capabilities by employing
               well-balanced restrictions on language use; ii) there are two
               comprehensive design perspectives on different domains of
               habitability; iii) there is one standardized questionnaire with a
               sub-scale to measure habitability in a limited way. The review
               has allowed us to propose a working definition of habitability
               and some design implications that may prove useful for guiding
               future research and practice in this field.",
  series    = "CHI EA '20",
  month     =  apr,
  year      =  2020,
  keywords  = "chatbots, conversational agents, design principles, dialog
               systems, spoken dialog systems, user experience, voice user
               interfaces"
}

@INPROCEEDINGS{Le2019-pl,
  title     = "Revisiting the Evaluation of Theory of Mind through Question
               Answering",
  author    = "Le, Matthew and Boureau, Y-Lan and Nickel, Maximilian",
  editor    = "Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in
               Natural Language Processing and the 9th International Joint
               Conference on Natural Language Processing (EMNLP-IJCNLP)",
  publisher = "Association for Computational Linguistics",
  address   = "Hong Kong, China",
  pages     = "5872--5877",
  abstract  = "Theory of mind, i.e., the ability to reason about intents and
               beliefs of agents is an important task in artificial intelligence
               and central to resolving ambiguous references in natural language
               dialogue. In this work, we revisit the evaluation of theory of
               mind through question answering. We show that current evaluation
               methods are flawed and that existing benchmark tasks can be
               solved without theory of mind due to dataset biases. Based on
               prior work, we propose an improved evaluation protocol and
               dataset in which we explicitly control for data regularities via
               a careful examination of the answer space. We show that
               state-of-the-art methods which are successful on existing
               benchmarks fail to solve theory-of-mind tasks in our proposed
               approach.",
  month     =  nov,
  year      =  2019
}

@MISC{noauthor_undated-ji,
  title = "{Reyes2016}.pdf"
}

@ARTICLE{Huang2023-au,
  title         = "Rigorously Assessing Natural Language Explanations of Neurons",
  author        = "Huang, Jing and Geiger, Atticus and D'Oosterlinck, Karel and
                   Wu, Zhengxuan and Potts, Christopher",
  journal       = "arXiv [cs.CL]",
  abstract      = "Natural language is an appealing medium for explaining how
                   large language models process and store information, but
                   evaluating the faithfulness of such explanations is
                   challenging. To help address this, we develop two modes of
                   evaluation for natural language explanations that claim
                   individual neurons represent a concept in a text input. In
                   the observational mode, we evaluate claims that a neuron $a$
                   activates on all and only input strings that refer to a
                   concept picked out by the proposed explanation $E$. In the
                   intervention mode, we construe $E$ as a claim that the neuron
                   $a$ is a causal mediator of the concept denoted by $E$. We
                   apply our framework to the GPT-4-generated explanations of
                   GPT-2 XL neurons of Bills et al. (2023) and show that even
                   the most confident explanations have high error rates and
                   little to no causal efficacy. We close the paper by
                   critically assessing whether natural language is a good
                   choice for explanations and whether neurons are the best
                   level of analysis.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hewitt2020-ne,
  title         = "{RNNs} can generate bounded hierarchical languages with
                   optimal memory",
  author        = "Hewitt, John and Hahn, Michael and Ganguli, Surya and Liang,
                   Percy and Manning, Christopher D",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recurrent neural networks empirically generate natural
                   language with high syntactic fidelity. However, their success
                   is not well-understood theoretically. We provide theoretical
                   insight into this success, proving in a finite-precision
                   setting that RNNs can efficiently generate bounded
                   hierarchical languages that reflect the scaffolding of
                   natural language syntax. We introduce Dyck-($k$,$m$), the
                   language of well-nested brackets (of $k$ types) and
                   $m$-bounded nesting depth, reflecting the bounded memory
                   needs and long-distance dependencies of natural language
                   syntax. The best known results use $O(k^{\frac{m}{2}})$
                   memory (hidden units) to generate these languages. We prove
                   that an RNN with $O(m \log k)$ hidden units suffices, an
                   exponential reduction in memory, by an explicit construction.
                   Finally, we show that no algorithm, even with unbounded
                   computation, can suffice with $o(m \log k)$ hidden units.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Esiobu2023-uf,
  title         = "{ROBBIE}: Robust Bias Evaluation of Large Generative Language
                   Models",
  author        = "Esiobu, David and Tan, Xiaoqing and Hosseini, Saghar and Ung,
                   Megan and Zhang, Yuchen and Fernandes, Jude and Dwivedi-Yu,
                   Jane and Presani, Eleonora and Williams, Adina and Smith,
                   Eric Michael",
  journal       = "arXiv [cs.CL]",
  abstract      = "As generative large language models (LLMs) grow more
                   performant and prevalent, we must develop comprehensive
                   enough tools to measure and improve their fairness. Different
                   prompt-based datasets can be used to measure social bias
                   across multiple text domains and demographic axes, meaning
                   that testing LLMs on more datasets can potentially help us
                   characterize their biases more fully, and better ensure equal
                   and equitable treatment of marginalized demographic groups.
                   In this work, our focus is two-fold: (1) Benchmarking: a
                   comparison of 6 different prompt-based bias and toxicity
                   metrics across 12 demographic axes and 5 families of
                   generative LLMs. Out of those 6 metrics, AdvPromptSet and
                   HolisticBiasR are novel datasets proposed in the paper. The
                   comparison of those benchmarks gives us insights about the
                   bias and toxicity of the compared models. Therefore, we
                   explore the frequency of demographic terms in common LLM
                   pre-training corpora and how this may relate to model biases.
                   (2) Mitigation: we conduct a comprehensive study of how well
                   3 bias/toxicity mitigation techniques perform across our
                   suite of measurements. ROBBIE aims to provide insights for
                   practitioners while deploying a model, emphasizing the need
                   to not only measure potential harms, but also understand how
                   they arise by characterizing the data, mitigate harms once
                   found, and balance any trade-offs. We open-source our
                   analysis code in hopes of encouraging broader measurements of
                   bias in future LLMs.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-gc,
  title = "{RobinTolmachLak\_2004\_14ComputerMediatedCom\_LanguageAndWomansPlac}.pdf"
}

@MISC{noauthor_undated-fr,
  title = "{RobinTolmachLak\_2004\_PartIIWhyWomenAreLadi\_LanguageAndWomansPlac}.pdf"
}

@ARTICLE{Gros2022-eq,
  title         = "Robots-Dont-Cry: Understanding Falsely Anthropomorphic
                   Utterances in Dialog Systems",
  author        = "Gros, David and Li, Yu and Yu, Zhou",
  journal       = "arXiv [cs.CL]",
  abstract      = "Dialog systems are often designed or trained to output
                   human-like responses. However, some responses may be
                   impossible for a machine to truthfully say (e.g. ``that movie
                   made me cry''). Highly anthropomorphic responses might make
                   users uncomfortable or implicitly deceive them into thinking
                   they are interacting with a human. We collect human ratings
                   on the feasibility of approximately 900 two-turn dialogs
                   sampled from 9 diverse data sources. Ratings are for two
                   hypothetical machine embodiments: a futuristic humanoid robot
                   and a digital assistant. We find that for some data-sources
                   commonly used to train dialog systems, 20-30\% of utterances
                   are not viewed as possible for a machine. Rating is
                   marginally affected by machine embodiment. We explore
                   qualitative and quantitative reasons for these ratings.
                   Finally, we build classifiers and explore how modeling
                   configuration might affect output permissibly, and discuss
                   implications for building less falsely anthropomorphic dialog
                   systems.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Shanahan2023-hf,
  title         = "Role-Play with Large Language Models",
  author        = "Shanahan, Murray and McDonell, Kyle and Reynolds, Laria",
  journal       = "arXiv [cs.CL]",
  abstract      = "As dialogue agents become increasingly human-like in their
                   performance, it is imperative that we develop effective ways
                   to describe their behaviour in high-level terms without
                   falling into the trap of anthropomorphism. In this paper, we
                   foreground the concept of role-play. Casting dialogue agent
                   behaviour in terms of role-play allows us to draw on familiar
                   folk psychological terms, without ascribing human
                   characteristics to language models they in fact lack. Two
                   important cases of dialogue agent behaviour are addressed
                   this way, namely (apparent) deception and (apparent)
                   self-awareness.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Voigt2018-ve,
  title     = "{R}t{G}ender: A Corpus for Studying Differential Responses to
               Gender",
  author    = "Voigt, Rob and Jurgens, David and Prabhakaran, Vinodkumar and
               Jurafsky, Dan and Tsvetkov, Yulia",
  editor    = "Calzolari, Nicoletta and Choukri, Khalid and Cieri, Christopher
               and Declerck, Thierry and Goggi, Sara and Hasida, Koiti and
               Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and
               Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis,
               Stelios and Tokunaga, Takenobu",
  booktitle = "Proceedings of the Eleventh International Conference on Language
               Resources and Evaluation ({LREC} 2018)",
  publisher = "European Language Resources Association (ELRA)",
  address   = "Miyazaki, Japan",
  month     =  may,
  year      =  2018
}

@ARTICLE{Bianchi2023-fx,
  title     = "Safety-tuned llamas: Lessons from improving the safety of large
               language models that follow instructions",
  author    = "Bianchi, F and Suzgun, M and Attanasio, G and Röttger, P and
               {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Training large language models to follow instructions makes them
               perform better on a wide range of tasks, generally becoming more
               helpful. However, a perfectly helpful model will …",
  year      =  2023
}

@ARTICLE{Agrawal2020-xk,
  title    = "Scaling up psychology via Scientific Regret Minimization",
  author   = "Agrawal, Mayank and Peterson, Joshua C and Griffiths, Thomas L",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  117,
  number   =  16,
  pages    = "8825--8835",
  abstract = "Do large datasets provide value to psychologists? Without a
              systematic methodology for working with such datasets, there is a
              valid concern that analyses will produce noise artifacts rather
              than true effects. In this paper, we offer a way to enable
              researchers to systematically build models and identify novel
              phenomena in large datasets. One traditional approach is to
              analyze the residuals of models-the biggest errors they make in
              predicting the data-to discover what might be missing from those
              models. However, once a dataset is sufficiently large, machine
              learning algorithms approximate the true underlying function
              better than the data, suggesting, instead, that the predictions of
              these data-driven models should be used to guide model building.
              We call this approach ``Scientific Regret Minimization'' (SRM), as
              it focuses on minimizing errors for cases that we know should have
              been predictable. We apply this exploratory method on a subset of
              the Moral Machine dataset, a public collection of roughly 40
              million moral decisions. Using SRM, we find that incorporating a
              set of deontological principles that capture dimensions along
              which groups of agents can vary (e.g., sex and age) improves a
              computational model of human moral judgment. Furthermore, we are
              able to identify and independently validate three interesting
              moral phenomena: criminal dehumanization, age of responsibility,
              and asymmetric notions of responsibility.",
  month    =  apr,
  year     =  2020,
  keywords = "decision-making; machine learning; moral psychology; scientific
              regret",
  language = "en"
}

@ARTICLE{Moss-Racusin2012-cq,
  title     = "Science faculty's subtle gender biases favor male students",
  author    = "Moss-Racusin, Corinne A and Dovidio, John F and Brescoll,
               Victoria L and Graham, Mark J and Handelsman, Jo",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  109,
  number    =  41,
  pages     = "16474--16479",
  abstract  = "Despite efforts to recruit and retain more women, a stark gender
               disparity persists within academic science. Abundant research has
               demonstrated gender bias in many demographic groups, but has yet
               to experimentally investigate whether science faculty exhibit a
               bias against female students that could contribute to the gender
               disparity in academic science. In a randomized double-blind study
               (n = 127), science faculty from research-intensive universities
               rated the application materials of a student-who was randomly
               assigned either a male or female name-for a laboratory manager
               position. Faculty participants rated the male applicant as
               significantly more competent and hireable than the (identical)
               female applicant. These participants also selected a higher
               starting salary and offered more career mentoring to the male
               applicant. The gender of the faculty participants did not affect
               responses, such that female and male faculty were equally likely
               to exhibit bias against the female student. Mediation analyses
               indicated that the female student was less likely to be hired
               because she was viewed as less competent. We also assessed
               faculty participants' preexisting subtle bias against women using
               a standard instrument and found that preexisting subtle bias
               against women played a moderating role, such that subtle bias
               against women was associated with less support for the female
               student, but was unrelated to reactions to the male student.
               These results suggest that interventions addressing faculty
               gender bias might advance the goal of increasing the
               participation of women in science.",
  month     =  oct,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Ehsan2024-oc,
  title     = "Seamful {XAI}: Operationalizing Seamful Design in Explainable
               {AI}",
  author    = "Ehsan, Upol and Liao, Q Vera and Passi, Samir and Riedl, Mark O
               and Daumé, Hal",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  8,
  number    = "CSCW1",
  pages     = "1--29",
  abstract  = "Mistakes in AI systems are inevitable, arising from both
               technical limitations and sociotechnical gaps. While black-boxing
               AI systems can make the user experience seamless, hiding the
               seams risks disempowering users to mitigate fallouts from AI
               mistakes. Instead of hiding these AI imperfections, can we
               leverage them to help the user? While Explainable AI (XAI) has
               predominantly tackled algorithmic opaqueness, we propose that
               seamful design can foster AI explainability by revealing and
               leveraging sociotechnical and infrastructural mismatches. We
               introduce the concept of Seamful XAI by (1) conceptually
               transferring ``seams'' to the AI context and (2) developing a
               design process that helps stakeholders anticipate and design with
               seams. We explore this process with 43 AI practitioners and real
               end-users, using a scenario-based co-design activity informed by
               real-world use cases. We found that the Seamful XAI design
               process helped users foresee AI harms, identify underlying
               reasons (seams), locate them in the AI's lifecycle, learn how to
               leverage seamful information to improve XAI and user agency. We
               share empirical insights, implications, and reflections on how
               this process can help practitioners anticipate and craft seams in
               AI, how seamfulness can improve explainability, empower
               end-users, and facilitate Responsible AI.",
  month     =  apr,
  year      =  2024,
  keywords  = "explainable ai, human-ai interaction, responsible ai, seamful
               design"
}

@ARTICLE{Jha_undated-zt,
  title         = "{SeeGULL}: A stereotype benchmark with broad Geo-cultural
                   coverage leveraging generative models",
  author        = "Jha, Akshita and Davani, Aida and Reddy, Chandan K and Dave,
                   Shachi and Prabhakaran, Vinodkumar and Dev, Sunipa",
  journal       = "arXiv [cs.CL]",
  abstract      = "Stereotype benchmark datasets are crucial to detect and
                   mitigate social stereotypes about groups of people in NLP
                   models. However, existing datasets are limited in size and
                   coverage, and are largely restricted to stereotypes prevalent
                   in the Western society. This is especially problematic as
                   language technologies gain hold across the globe. To address
                   this gap, we present SeeGULL, a broad-coverage stereotype
                   dataset, built by utilizing generative capabilities of large
                   language models such as PaLM, and GPT-3, and leveraging a
                   globally diverse rater pool to validate the prevalence of
                   those stereotypes in society. SeeGULL is in English, and
                   contains stereotypes about identity groups spanning 178
                   countries across 8 different geo-political regions across 6
                   continents, as well as state-level identities within the US
                   and India. We also include fine-grained offensiveness scores
                   for different stereotypes and demonstrate their global
                   disparities. Furthermore, we include comparative annotations
                   about the same groups by annotators living in the region vs.
                   those that are based in North America, and demonstrate that
                   within-region stereotypes about groups differ from those
                   prevalent in North America. CONTENT WARNING: This paper
                   contains stereotype examples that may be offensive.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Bhutani2024-nl,
  title         = "{SeeGULL} Multilingual: A dataset of Geo-culturally situated
                   stereotypes",
  author        = "Bhutani, Mukul and Robinson, Kevin and Prabhakaran,
                   Vinodkumar and Dave, Shachi and Dev, Sunipa",
  journal       = "arXiv [cs.CL]",
  abstract      = "While generative multilingual models are rapidly being
                   deployed, their safety and fairness evaluations are largely
                   limited to resources collected in English. This is especially
                   problematic for evaluations targeting inherently
                   socio-cultural phenomena such as stereotyping, where it is
                   important to build multi-lingual resources that reflect the
                   stereotypes prevalent in respective language communities.
                   However, gathering these resources, at scale, in varied
                   languages and regions pose a significant challenge as it
                   requires broad socio-cultural knowledge and can also be
                   prohibitively expensive. To overcome this critical gap, we
                   employ a recently introduced approach that couples LLM
                   generations for scale with culturally situated validations
                   for reliability, and build SeeGULL Multilingual, a
                   global-scale multilingual dataset of social stereotypes,
                   containing over 25K stereotypes, spanning 20 languages, with
                   human annotations across 23 regions, and demonstrate its
                   utility in identifying gaps in model evaluations. Content
                   warning: Stereotypes shared in this paper can be offensive.",
  month         =  mar,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Davis2021-zj,
  title     = "Seeing and Believing: The Relationship between Perception and
               Mental Verbs in Acquisition",
  author    = "Davis, E Emory and Landau, Barbara",
  journal   = "Lang. Learn. Dev.",
  publisher = "Routledge",
  volume    =  17,
  number    =  1,
  pages     = "26--47",
  abstract  = "Perception verbs and mental verbs have significant overlap in
               their syntax and semantics; both reference mental representations
               when taking embedded clauses, as in I see that Maria was here and
               I think that Maria was here. Some have suggested that perception
               is more accessible for young children than mental states, raising
               the question of whether perception verbs could serve as a
               semantic model for the acquisition of mental verbs via their
               shared syntax. Since embedded clauses are key to referencing
               mental states for both verb classes, we examine the developmental
               trajectory of perception vs. mental verbs in these constructions
               and others. Using a sample of 5,884 child-produced utterances and
               8,313 parent-produced utterances from the Brown and Gleason
               corpora of CHILDES, we analyze children?s production of
               perception and mental verbs in their syntactic frames, as well as
               that of their parents. We find that children begin to produce
               embedded frames for both perception and mental verbs around the
               same time, but produce embedded frames with mental verbs more
               often, especially as they get older, despite greater use of
               perception verbs overall. These patterns do not reflect parental
               input: parents produce both verb types with similar frequency and
               use embedded frames more often than their children. These
               findings suggest that perception verbs are unlikely to serve as a
               model for mental verbs, and instead that mental verbs and their
               regular occurrence with embedded frames may provide a model for
               perception verbs when the latter reference mental states. We
               propose a semantic updating account for children?s acquisition of
               perception verbs, arguing that children?s early knowledge of
               perception verbs may not include mental state representations as
               a component of their meaning, and that this may only develop
               later as children learn the propositional syntax that is shared
               by and regularly occurs with mental verbs.",
  month     =  jan,
  year      =  2021
}

@ARTICLE{Eberhardt2004-dd,
  title     = "Seeing black: race, crime, and visual processing",
  author    = "Eberhardt, Jennifer L and Goff, Phillip Atiba and Purdie, Valerie
               J and Davies, Paul G",
  journal   = "J. Pers. Soc. Psychol.",
  publisher = "psycnet.apa.org",
  volume    =  87,
  number    =  6,
  pages     = "876--893",
  abstract  = "Using police officers and undergraduates as participants, the
               authors investigated the influence of stereotypic associations on
               visual processing in 5 studies. Study 1 demonstrates that Black
               faces influence participants' ability to spontaneously detect
               degraded images of crime-relevant objects. Conversely, Studies
               2-4 demonstrate that activating abstract concepts (i.e., crime
               and basketball) induces attentional biases toward Black male
               faces. Moreover, these processing biases may be related to the
               degree to which a social group member is physically
               representative of the social group (Studies 4-5). These studies,
               taken together, suggest that some associations between social
               groups and concepts are bidirectional and operate as visual
               tuning devices--producing shifts in perception and attention of a
               sort likely to influence decision making and behavior.",
  month     =  dec,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Liu2023-lc,
  title         = "Seeing is Believing: Brain-Inspired Modular Training for
                   Mechanistic Interpretability",
  author        = "Liu, Ziming and Gan, Eric and Tegmark, Max",
  journal       = "arXiv [cs.NE]",
  abstract      = "We introduce Brain-Inspired Modular Training (BIMT), a method
                   for making neural networks more modular and interpretable.
                   Inspired by brains, BIMT embeds neurons in a geometric space
                   and augments the loss function with a cost proportional to
                   the length of each neuron connection. We demonstrate that
                   BIMT discovers useful modular neural networks for many simple
                   tasks, revealing compositional structures in symbolic
                   formulas, interpretable decision boundaries and features for
                   classification, and mathematical structure in algorithmic
                   datasets. The ability to directly see modules with the naked
                   eye can complement current mechanistic interpretability
                   strategies such as probes, interventions or staring at all
                   weights.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.NE"
}

@ARTICLE{Zurbriggen2011-uf,
  title    = "Self- and Partner-objectification in Romantic Relationships:
              Associations with Media Consumption and Relationship Satisfaction",
  author   = "Zurbriggen, Eileen L and Ramsey, Laura R and Jaworski, Beth K",
  journal  = "Sex Roles",
  volume   =  64,
  number   = "7-8",
  pages    = "449--462",
  abstract = "Few studies have examined objectification in the context of
              romantic relationships, even though strong theoretical arguments
              have often made this connection. This study addresses this gap in
              the literature by examining whether exposure to mass media is
              related to self-objectification and objectification of one's
              partner, which in turn is hypothesized to be related to
              relationship and sexual satisfaction. A sample of undergraduate
              students (91 women and 68 men) enrolled in a university on the
              west coast of the United States completed self-report measures of
              the following variables: self-objectification, objectification of
              one's romantic partner, relationship satisfaction, sexual
              satisfaction, and exposure to objectifying media. Men reported
              higher levels of partner objectification than did women; there was
              no gender difference in self-objectification. Self- and
              partner-objectification were positively correlated; this
              correlation was especially strong for men. In regression analyses,
              partner-objectification was predictive of lower levels of
              relationship satisfaction. Furthermore, a path model revealed that
              consuming objectifying media is related to lowered relationship
              satisfaction through the variable of partner-objectification.
              Finally, self- and partner-objectification were related to lower
              levels of sexual satisfaction among men. This study provides
              evidence for the negative effects of objectification in the
              context of romantic relationships among young adults.",
  month    =  apr,
  year     =  2011,
  language = "en"
}

@ARTICLE{Saunders2022-rd,
  title         = "Self-critiquing models for assisting human evaluators",
  author        = "Saunders, William and Yeh, Catherine and Wu, Jeff and Bills,
                   Steven and Ouyang, Long and Ward, Jonathan and Leike, Jan",
  journal       = "arXiv [cs.CL]",
  abstract      = "We fine-tune large language models to write natural language
                   critiques (natural language critical comments) using
                   behavioral cloning. On a topic-based summarization task,
                   critiques written by our models help humans find flaws in
                   summaries that they would have otherwise missed. Our models
                   help find naturally occurring flaws in both model and human
                   written summaries, and intentional flaws in summaries written
                   by humans to be deliberately misleading. We study scaling
                   properties of critiquing with both topic-based summarization
                   and synthetic tasks. Larger models write more helpful
                   critiques, and on most tasks, are better at self-critiquing,
                   despite having harder-to-critique outputs. Larger models can
                   also integrate their own self-critiques as feedback, refining
                   their own summaries into better ones. Finally, we motivate
                   and introduce a framework for comparing critiquing ability to
                   generation and discrimination ability. Our measurements
                   suggest that even large models may still have relevant
                   knowledge they cannot or do not articulate as critiques.
                   These results are a proof of concept for using AI-assisted
                   human feedback to scale the supervision of machine learning
                   systems to tasks that are difficult for humans to evaluate
                   directly. We release our training datasets, as well as
                   samples from our critique assistance experiments.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{N2021-nm,
  title        = "Self-objectification and video chatting: Exploring",
  author       = "N., Callie",
  publisher    = "scholar.archive.org",
  abstract     = "In a social landscape dominated by Instagram, Snapchat, TikTok
                  , and newly emerging social media platforms, people are
                  perpetually surrounded by images of themselves and others. …",
  year         =  2021,
  howpublished = "\url{https://scholar.archive.org/work/e4ijjbcdjjbdtfoqcd4jm3lkke/access/wayback/https://s3-eu-west-1.amazonaws.com/pstorage-uwyo-5122565135/27968493/SurberFinalCapstone.pdf}",
  note         = "Accessed: 2023-12-15"
}

@ARTICLE{Breines2008-mn,
  title     = "Self-objectification and well-being in women's daily lives",
  author    = "Breines, Juliana G and Crocker, Jennifer and Garcia, Julie A",
  journal   = "Pers. Soc. Psychol. Bull.",
  publisher = "journals.sagepub.com",
  volume    =  34,
  number    =  5,
  pages     = "583--598",
  abstract  = "Laboratory experiments and surveys show that self-objectification
               increases body shame, disrupts attention, and negatively predicts
               well-being. Using experience sampling methodology, the authors
               investigated self-objectification in the daily lives of 49 female
               college students. Building on the predictions of objectification
               theory, they examined associations between internalizing an
               observer's perspective on the self and psychological well-being,
               and examined the moderating roles of trait self-esteem and
               appearance-contingent self-worth. Within-person increases in
               self-objectification predicted decreased well-being, but this
               association was moderated by trait self-esteem and trait
               appearance-contingent self-worth; high self-esteem, highly
               appearance-contingent participants reported increased well-being
               when they self-objectified. Furthermore, perceived
               unattractiveness partially mediated the main effect and the
               three-way interaction: high self-esteem, highly contingent
               participants experienced smaller drops in well-being when they
               self-objectified, in part because they felt less unattractive.
               These results suggest that in daily life, some women receive a
               boost from self-objectification, although most women experience
               decreases in well-being when self-objectifying.",
  month     =  may,
  year      =  2008,
  language  = "en"
}

@ARTICLE{Madaan2023-tw,
  title         = "Self-Refine: Iterative Refinement with Self-Feedback",
  author        = "Madaan, Aman and Tandon, Niket and Gupta, Prakhar and
                   Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon,
                   Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming
                   and Gupta, Shashank and Majumder, Bodhisattwa Prasad and
                   Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir
                   and Clark, Peter",
  journal       = "arXiv [cs.CL]",
  abstract      = "Like humans, large language models (LLMs) do not always
                   generate the best output on their first try. Motivated by how
                   humans refine their written text, we introduce Self-Refine,
                   an approach for improving initial outputs from LLMs through
                   iterative feedback and refinement. The main idea is to
                   generate an initial output using an LLMs; then, the same LLMs
                   provides feedback for its output and uses it to refine
                   itself, iteratively. Self-Refine does not require any
                   supervised training data, additional training, or
                   reinforcement learning, and instead uses a single LLM as the
                   generator, refiner, and feedback provider. We evaluate
                   Self-Refine across 7 diverse tasks, ranging from dialog
                   response generation to mathematical reasoning, using
                   state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across
                   all evaluated tasks, outputs generated with Self-Refine are
                   preferred by humans and automatic metrics over those
                   generated with the same LLM using conventional one-step
                   generation, improving by ~20\% absolute on average in task
                   performance. Our work demonstrates that even state-of-the-art
                   LLMs like GPT-4 can be further improved at test time using
                   our simple, standalone approach.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{An2018-da,
  title         = "{SemAxis}: A Lightweight Framework to Characterize
                   Domain-Specific Word Semantics Beyond Sentiment",
  author        = "An, Jisun and Kwak, Haewoon and Ahn, Yong-Yeol",
  journal       = "arXiv [cs.CL]",
  abstract      = "Because word semantics can substantially change across
                   communities and contexts, capturing domain-specific word
                   semantics is an important challenge. Here, we propose
                   SEMAXIS, a simple yet powerful framework to characterize word
                   semantics using many semantic axes in word- vector spaces
                   beyond sentiment. We demonstrate that SEMAXIS can capture
                   nuanced semantic representations in multiple online
                   communities. We also show that, when the sentiment axis is
                   examined, SEMAXIS outperforms the state-of-the-art approaches
                   in building domain-specific sentiment lexicons.",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{McDowall2023-bm,
  title         = "Sensemaking About Contraceptive Methods Across Online
                   Platforms",
  author        = "McDowall, Leann and Antoniak, Maria and Mimno, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "Selecting a birth control method is a complex healthcare
                   decision. While birth control methods provide important
                   benefits, they can also cause unpredictable side effects and
                   be stigmatized, leading many people to seek additional
                   information online, where they can find reviews, advice,
                   hypotheses, and experiences of other birth control users.
                   However, the relationships between their healthcare concerns,
                   sensemaking activities, and online settings are not well
                   understood. We gather texts about birth control shared on
                   Twitter, Reddit, and WebMD -- platforms with different
                   affordances, moderation, and audiences -- to study where and
                   how birth control is discussed online. Using a combination of
                   topic modeling and hand annotation, we identify and
                   characterize the dominant sensemaking practices across these
                   platforms, and we create lexicons to draw comparisons across
                   birth control methods and side effects. We use these to
                   measure variations from survey reports of side effect
                   experiences and method usage. Our findings characterize how
                   online platforms are used to make sense of difficult
                   healthcare choices and highlight unmet needs of birth control
                   users.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Xiao2022-qi,
  title     = "Sensemaking, support, safety, retribution, transformation: A
               restorative justice approach to understanding adolescents' needs
               for addressing online harm",
  author    = "Xiao, S and Cheshire, C and Salehi, N",
  journal   = "Proceedings of the 2022 CHI Conference",
  publisher = "dl.acm.org",
  abstract  = "Online harm is a prevalent issue in adolescents' online lives.
               Restorative justice teaches us to focus on those who have been
               harmed, ask what their needs are, and engage in the …",
  year      =  2022
}

@ARTICLE{Engler2023-fr,
  title         = "{SensePOLAR}: Word sense aware interpretability for
                   pre-trained contextual word embeddings",
  author        = "Engler, Jan and Sikdar, Sandipan and Lutz, Marlene and
                   Strohmaier, Markus",
  journal       = "arXiv [cs.CL]",
  abstract      = "Adding interpretability to word embeddings represents an area
                   of active research in text representation. Recent work has
                   explored thepotential of embedding words via so-called polar
                   dimensions (e.g. good vs. bad, correct vs. wrong). Examples
                   of such recent approaches include SemAxis, POLAR, FrameAxis,
                   and BiImp. Although these approaches provide interpretable
                   dimensions for words, they have not been designed to deal
                   with polysemy, i.e. they can not easily distinguish between
                   different senses of words. To address this limitation, we
                   present SensePOLAR, an extension of the original POLAR
                   framework that enables word-sense aware interpretability for
                   pre-trained contextual word embeddings. The resulting
                   interpretable word embeddings achieve a level of performance
                   that is comparable to original contextual word embeddings
                   across a variety of natural language processing tasks
                   including the GLUE and SQuAD benchmarks. Our work removes a
                   fundamental limitation of existing approaches by offering
                   users sense aware interpretations for contextual word
                   embeddings.",
  month         =  jan,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Kaur2022-kj,
  title     = "Sensible {AI}: Re-imagining Interpretability and Explainability
               using Sensemaking Theory",
  author    = "Kaur, Harmanpreet and Adar, Eytan and Gilbert, Eric and Lampe,
               Cliff",
  booktitle = "Proceedings of the 2022 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "702--714",
  abstract  = "Understanding how ML models work is a prerequisite for
               responsibly designing, deploying, and using ML-based systems.
               With interpretability approaches, ML can now offer explanations
               for its outputs to aid human understanding. Though these
               approaches rely on guidelines for how humans explain things to
               each other, they ultimately solve for improving the artifact—an
               explanation. In this paper, we propose an alternate framework for
               interpretability grounded in Weick’s sensemaking theory, which
               focuses on who the explanation is intended for. Recent work has
               advocated for the importance of understanding stakeholders’
               needs—we build on this by providing concrete properties (e.g.,
               identity, social context, environmental cues, etc.) that shape
               human understanding. We use an application of sensemaking in
               organizations as a template for discussing design guidelines for
               sensible AI, AI that factors in the nuances of human cognition
               when trying to explain itself.",
  series    = "FAccT '22",
  month     =  jun,
  year      =  2022,
  keywords  = "explainability, interpretability, organizations, sensemaking"
}

@INPROCEEDINGS{Beck2024-yk,
  title     = "Sensitivity, Performance, Robustness: Deconstructing the Effect
               of Sociodemographic Prompting",
  author    = "Beck, Tilman and Schuff, Hendrik and Lauscher, Anne and Gurevych,
               Iryna",
  editor    = "Graham, Yvette and Purver, Matthew",
  booktitle = "Proceedings of the 18th Conference of the European Chapter of the
               Association for Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "St. Julian's, Malta",
  pages     = "2589--2615",
  abstract  = "Annotators' sociodemographic backgrounds (i.e., the individual
               compositions of their gender, age, educational background, etc.)
               have a strong impact on their decisions when working on
               subjective NLP tasks, such as toxic language detection. Often,
               heterogeneous backgrounds result in high disagreements. To model
               this variation, recent work has explored sociodemographic
               prompting, a technique, which steers the output of prompt-based
               models towards answers that humans with specific sociodemographic
               profiles would give. However, the available NLP literature
               disagrees on the efficacy of this technique --- it remains
               unclear for which tasks and scenarios it can help, and the role
               of the individual factors in sociodemographic prompting is still
               unexplored. We address this research gap by presenting the
               largest and most comprehensive study of sociodemographic
               prompting today. We use it to analyze its influence on model
               sensitivity, performance and robustness across seven datasets and
               six instruction-tuned model families. We show that
               sociodemographic information affects model predictions and can be
               beneficial for improving zero-shot learning in subjective NLP
               tasks.However, its outcomes largely vary for different model
               types, sizes, and datasets, and are subject to large variance
               with regards to prompt formulations. Most importantly, our
               results show that sociodemographic prompting should be used with
               care when used for data annotation or studying LLM alignment.",
  month     =  mar,
  year      =  2024
}

@ARTICLE{Aubrey2011-vh,
  title     = "Sexual Objectification in Music Videos: A Content Analysis
               Comparing Gender and Genre",
  author    = "Aubrey, Jennifer Stevens and Frisby, Cynthia M",
  journal   = "Mass Communication and Society",
  publisher = "Routledge",
  volume    =  14,
  number    =  4,
  pages     = "475--501",
  abstract  = "Although sexual objectification is commonplace in media culture,
               music videos provide the most potent examples of it. In the
               current study, we developed a coding system to measure sexual
               objectification and its correlates in music videos. Our analysis
               compared sexual objectification across artists' gender and
               musical genres (R\&B/hip-hop, pop, and country). Compared to male
               artists, female artists were more sexually objectified, held to
               stricter appearance standards, and more likely to demonstrate
               sexually alluring behavior. In addition, sexual objectification
               was more prominent in R\&B/hip-hop and pop videos than in country
               videos. The results are discussed in light of objectification
               theory and sexual agency.",
  month     =  jul,
  year      =  2011
}

@ARTICLE{Karsay2018-to,
  title    = "Sexualizing Media Use and Self-Objectification: A Meta-Analysis",
  author   = "Karsay, Kathrin and Knoll, Johannes and Matthes, Jörg",
  journal  = "Psychol. Women Q.",
  volume   =  42,
  number   =  1,
  pages    = "9--28",
  abstract = "Objectification theorists suggest that exposure to sexualizing
              media increases self-objectification among individuals.
              Correlational and experimental research examining this relation
              has received growing attention. The aim of this meta-analysis was
              to investigate the influence of sexualizing media use on
              self-objectification among women and men. For this purpose, we
              analyzed 54 papers yielding 50 independent studies and 261 effect
              sizes. The data revealed a positive, moderate effect of
              sexualizing media on self-objectification (r = .19). The effect
              was significant and robust, 95\% CI [.15, .23], p < .0001. We
              identified a conditional effect of media type, suggesting that the
              use of video games and/or online media led to stronger
              self-objectification effects when compared to television use.
              Other sample characteristics or study characteristics did not
              moderate the overall effect. Thus, our findings highlight the
              importance of sexualizing media exposure on women's and men's
              objectified self-concept. We discuss future research directions
              and implications for practice. We hope that the article will
              stimulate researchers in their future work to address the research
              gaps outlined here. Moreover, we hope that the findings will
              encourage practitioners and parents to reflect on the role of the
              use of sexualizing media in the development of individuals'
              self-objectification. Additional online materials for this article
              are available on PWQ's website at
              http://journals.sagepub.com/doi/suppl10.1177/0361684317743019.",
  month    =  mar,
  year     =  2018,
  keywords = "body image; media use; meta-analysis; self-objectification",
  language = "en"
}

@ARTICLE{Vinyals2014-wq,
  title         = "Show and Tell: A Neural Image Caption Generator",
  author        = "Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and
                   Erhan, Dumitru",
  journal       = "arXiv [cs.CV]",
  pages         = "3156--3164",
  abstract      = "Automatically describing the content of an image is a
                   fundamental problem in artificial intelligence that connects
                   computer vision and natural language processing. In this
                   paper, we present a generative model based on a deep
                   recurrent architecture that combines recent advances in
                   computer vision and machine translation and that can be used
                   to generate natural sentences describing an image. The model
                   is trained to maximize the likelihood of the target
                   description sentence given the training image. Experiments on
                   several datasets show the accuracy of the model and the
                   fluency of the language it learns solely from image
                   descriptions. Our model is often quite accurate, which we
                   verify both qualitatively and quantitatively. For instance,
                   while the current state-of-the-art BLEU-1 score (the higher
                   the better) on the Pascal dataset is 25, our approach yields
                   59, to be compared to human performance around 69. We also
                   show BLEU-1 score improvements on Flickr30k, from 56 to 66,
                   and on SBU, from 19 to 28. Lastly, on the newly released COCO
                   dataset, we achieve a BLEU-4 of 27.7, which is the current
                   state-of-the-art.",
  month         =  nov,
  year          =  2014,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Shaikh2024-nx,
  title         = "Show, don't tell: Aligning language models with demonstrated
                   feedback",
  author        = "Shaikh, Omar and Lam, Michelle and Hejna, Joey and Shao,
                   Yijia and Bernstein, Michael and Yang, Diyi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models are aligned to emulate the collective voice
                   of many, resulting in outputs that align with no one in
                   particular. Steering LLMs away from generic output is
                   possible through supervised finetuning or RLHF, but requires
                   prohibitively large datasets for new ad-hoc tasks. We argue
                   that it is instead possible to align an LLM to a specific
                   setting by leveraging a very small number ($<10$) of
                   demonstrations as feedback. Our method, Demonstration
                   ITerated Task Optimization (DITTO), directly aligns language
                   model outputs to a user's demonstrated behaviors. Derived
                   using ideas from online imitation learning, DITTO cheaply
                   generates online comparison data by treating users'
                   demonstrations as preferred over output from the LLM and its
                   intermediate checkpoints. We evaluate DITTO's ability to
                   learn fine-grained style and task alignment across domains
                   such as news articles, emails, and blog posts. Additionally,
                   we conduct a user study soliciting a range of demonstrations
                   from participants ($N=16$). Across our benchmarks and user
                   study, we find that win-rates for DITTO outperform few-shot
                   prompting, supervised fine-tuning, and other self-play
                   methods by an average of 19\% points. By using demonstrations
                   as feedback directly, DITTO offers a novel method for
                   effective customization of LLMs.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Bapna2019-fx,
  title         = "Simple, Scalable Adaptation for Neural Machine Translation",
  author        = "Bapna, Ankur and Arivazhagan, Naveen and Firat, Orhan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Fine-tuning pre-trained Neural Machine Translation (NMT)
                   models is the dominant approach for adapting to new languages
                   and domains. However, fine-tuning requires adapting and
                   maintaining a separate model for each target task. We propose
                   a simple yet efficient approach for adaptation in NMT. Our
                   proposed approach consists of injecting tiny task specific
                   adapter layers into a pre-trained model. These lightweight
                   adapters, with just a small fraction of the original model
                   size, adapt the model to multiple individual tasks
                   simultaneously. We evaluate our approach on two tasks: (i)
                   Domain Adaptation and (ii) Massively Multilingual NMT.
                   Experiments on domain adaptation demonstrate that our
                   proposed approach is on par with full fine-tuning on various
                   domains, dataset sizes and model capacities. On a massively
                   multilingual dataset of 103 languages, our adaptation
                   approach bridges the gap between individual bilingual models
                   and one massively multilingual model for most language pairs,
                   paving the way towards universal machine translation.",
  month         =  sep,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Mireshghallah2023-xq,
  title    = "Simple Temporal Adaptation to Changing Label Sets: Hashtag
              Prediction via Dense {KNN}",
  author   = "Mireshghallah, Niloofar and Vogler, Nikolai and He, Junxian and
              Florez, Omar and El-Kishky, Ahmed and Berg-Kirkpatrick, Taylor",
  abstract = "User-generated social media data is constantly changing as new
              trends influence online discussion and personal information is
              deleted due to privacy concerns. However, traditional NLP models
              rely on fixed training datasets, which means they are unable to
              adapt to temporal change---both test distribution shift and
              deleted training data---without frequent, costly re-training. In
              this paper, we study temporal adaptation through the task of
              longitudinal hashtag prediction and propose a non-parametric dense
              retrieval technique, which does not require re-training, as a
              simple but effective solution. In experiments on a newly
              collected, publicly available, year-long Twitter dataset
              exhibiting temporal distribution shift, our method improves by
              64\% over the best static parametric baseline while avoiding
              costly gradient-based re-training. Our approach is also
              particularly well-suited to dynamically deleted user data in line
              with data privacy laws, with negligible computational
              cost/performance loss.",
  month    =  dec,
  year     =  2023
}

@INPROCEEDINGS{Bell2023-ls,
  title     = "Simplicity Bias Leads to Amplified Performance Disparities",
  author    = "Bell, Samuel James and Sagun, Levent",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "355--369",
  abstract  = "Which parts of a dataset will a given model find difficult?
               Recent work has shown that SGD-trained models have a bias towards
               simplicity, leading them to prioritize learning a majority class,
               or to rely upon harmful spurious correlations. Here, we show that
               the preference for ‘easy’ runs far deeper: A model may prioritize
               any class or group of the dataset that it finds simple—at the
               expense of what it finds complex—as measured by performance
               difference on the test set. When subsets with different levels of
               complexity align with demographic groups, we term this difficulty
               disparity, a phenomenon that occurs even with balanced datasets
               that lack group/label associations. We show how difficulty
               disparity is a model-dependent quantity, and is further amplified
               in commonly-used models as selected by typical average
               performance scores. We quantify an amplification factor across a
               range of settings in order to compare disparity of different
               models on a fixed dataset. Finally, we present two real-world
               examples of difficulty amplification in action, resulting in
               worse-than-expected performance disparities between groups even
               when using a balanced dataset. The existence of such disparities
               in balanced datasets demonstrates that merely balancing sample
               sizes of groups is not sufficient to ensure unbiased performance.
               We hope this work presents a step towards measurable
               understanding of the role of model bias as it interacts with the
               structure of data, and call for additional model-dependent
               mitigation methods to be deployed alongside dataset audits.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023,
  keywords  = "neural networks, fairness, performance disparities, simplicity
               bias"
}

@INPROCEEDINGS{Keidar2022-tv,
  title     = "Slangvolution: {A} Causal Analysis of Semantic Change and
               Frequency Dynamics in Slang",
  author    = "Keidar, Daphna and Opedal, Andreas and Jin, Zhijing and Sachan,
               Mrinmaya",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Dublin, Ireland",
  pages     = "1422--1442",
  abstract  = "Languages are continuously undergoing changes, and the mechanisms
               that underlie these changes are still a matter of debate. In this
               work, we approach language evolution through the lens of
               causality in order to model not only how various distributional
               factors associate with language change, but how they causally
               affect it. In particular, we study slang, which is an informal
               language that is typically restricted to a specific group or
               social setting. We analyze the semantic change and frequency
               shift of slang words and compare them to those of standard,
               nonslang words. With causal discovery and causal inference
               techniques, we measure the effect that word type (slang/nonslang)
               has on both semantic change and frequency shift, as well as its
               relationship to frequency, polysemy and part of speech. Our
               analysis provides some new insights in the study of language
               change, e.g., we show that slang words undergo less semantic
               change but tend to have larger frequency shifts over time.",
  month     =  may,
  year      =  2022
}

@ARTICLE{Kraicer2019-oi,
  title     = "Social characters: The hierarchy of gender in contemporary
               English-language fiction",
  author    = "Kraicer, Eve and Piper, Andrew",
  journal   = "Journal of Cultural Analytics",
  publisher = "CA: Journal of Cultural Analytics",
  year      =  2019
}

@INPROCEEDINGS{Bauer2023-ey,
  title     = "Social Commonsense for Explanation and Cultural Bias Discovery",
  author    = "Bauer, Lisa and Tischer, Hanna and Bansal, Mohit",
  booktitle = "Proceedings of the 17th Conference of the European Chapter of the
               Association for Computational Linguistics",
  publisher = "Association for Computational Linguistics",
  address   = "Dubrovnik, Croatia",
  pages     = "3745--3760",
  abstract  = "Social commonsense contains many human biases due to social and
               cultural influence (Sap et al., 2020; Emelin et al., 2020). We
               focus on identifying cultural biases in data, specifically causal
               assumptions and commonsense implications, that strongly influence
               model decisions for a variety of tasks designed for social
               impact. This enables us to examine data for bias by making
               explicit the causal (if-then, inferential) relations in social
               commonsense knowledge used for decision making, furthering
               interpretable commonsense reasoning from a dataset perspective.
               We apply our methods on 2 social tasks: emotion detection and
               perceived value detection. We identify influential social
               commonsense knowledge to explain model behavior in the following
               ways. First, we augment large-scale language models with social
               knowledge and show improvements for the tasks, indicating the
               implicit assumptions a model requires to be successful on each
               dataset. Second, we identify influential events in the datasets
               by using social knowledge to cluster data and demonstrate the
               influence that these events have on model behavior via
               leave-K-out experiments. This allows us to gain a dataset-level
               understanding of the events and causal commonsense relationships
               that strongly influence predictions. We then analyze these
               relationships to detect influential cultural bias in each
               dataset. Finally, we use our influential event identification for
               detecting mislabeled examples and improve training and
               performance through their removal. We support our findings with
               manual analysis.",
  month     =  may,
  year      =  2023
}

@ARTICLE{Lindner2012-ii,
  title    = "Social Comparison and the ‘Circle of Objectification’",
  author   = "Lindner, Danielle and Tantleff-Dunn, Stacey and Jentsch, Florian",
  journal  = "Sex Roles",
  volume   =  67,
  number   =  3,
  pages    = "222--235",
  abstract = "The purpose of this study was to test a new theoretical model that
              integrates self-objectification, objectification of others, and
              social comparison as contributors to the development and
              maintenance of body image disturbance and disordered eating
              behavior. Within the new theoretical model, self-objectification,
              objectification of others, and social comparison are
              conceptualized as a self-perpetuating cycle, rather than as
              processes that occur independently of one another. Measures of
              self-objectification, objectification of others, social
              comparison, body shame, body dissatisfaction, and eating disorder
              symptomatology were completed by 549 female students between the
              ages of 18 and 30 years from a large university in the
              southeastern United States. Structural equation modeling with
              nested model comparisons was used to examine the fit of the new
              theoretical model relative to less complex models which contain
              only relationships which have received previous attention in the
              research literature (e.g., the relationship between
              self-objectification and body shame). Results indicated that the
              new theoretical model demonstrates good fit for the data and that
              the fit of this model is significantly better than the original
              model suggested by the literature. This model also brings together
              two distinct lines of research and offers a more complete
              understanding of processes underlying women’s body image and
              eating behavior. Implications for clinical work as well as theory
              and measurement are discussed.",
  month    =  aug,
  year     =  2012
}

@ARTICLE{Corzine2023-wz,
  title    = "Social contagion, from suicide to online challenges to eating
              disorders: Current research and harm mitigation strategies for
              youth online",
  author   = "Corzine, Anjuli and Harrison, Vicki",
  journal  = "J. Online Trust Saf.",
  abstract = "The growth of socialmedia sites has rapidly changed theway that
              people interactwith one another online, especially for adolescents
              and young adults who are the primary users of social media in the
              United States (Carlyle et al. 2018). Ninety-six percent of
              teenagers report having consistent access to a smartphone and have
              been shown to use a larger number of social media platforms than
              in years past (Roth et al. 2020). In the US, 86\% of people use at
              least one social media platform, and 85\% of children 12to
              17-years old report regular social media use (Carlyle et al. 2018;
              Chung et al. 2021). Given their frequent use of social media, of
              specific concern are sites that facilitate the promotion of risky
              behaviors. For example, viral trends on platforms like TikTok and
              Twitter, in which users are encouraged to engage in non-normative
              behaviors, are increasingly common, some of them promoting the
              performance of risky acts (Abraham et al. 2022). In addition to
              potentially dangerous trends, there has been a rise of online
              communities dedicated to the promotion of self-harming behaviors
              that offer community, tips, and support in continuing to
              participate in damaging behavior (Knapton 2013).",
  month    =  sep,
  year     =  2023
}

@ARTICLE{Friedkin1990-hl,
  title     = "Social influence and opinions",
  author    = "Friedkin, Noah E and Johnsen, Eugene C",
  journal   = "J. Math. Sociol.",
  publisher = "Routledge",
  volume    =  15,
  number    = "3-4",
  pages     = "193--206",
  abstract  = "In this paper we describe an approach to the relationship between
               a network of interpersonal influences and the content of
               individuals? opinions. Our work starts with the specification of
               social process rather than social equilibrium. Several models of
               social influence that have appeared in the literature are derived
               as special cases of the approach. Some implications for theories
               on social conflict and conformity also are developed in this
               paper.",
  month     =  jan,
  year      =  1990
}

@ARTICLE{Moussaid2013-el,
  title     = "Social influence and the collective dynamics of opinion formation",
  author    = "Moussaïd, Mehdi and Kämmer, Juliane E and Analytis, Pantelis P
               and Neth, Hansjörg",
  journal   = "PLoS One",
  publisher = "journals.plos.org",
  volume    =  8,
  number    =  11,
  pages     = "e78433",
  abstract  = "Social influence is the process by which individuals adapt their
               opinion, revise their beliefs, or change their behavior as a
               result of social interactions with other people. In our strongly
               interconnected society, social influence plays a prominent role
               in many self-organized phenomena such as herding in cultural
               markets, the spread of ideas and innovations, and the
               amplification of fears during epidemics. Yet, the mechanisms of
               opinion formation remain poorly understood, and existing
               physics-based models lack systematic empirical validation. Here,
               we report two controlled experiments showing how participants
               answering factual questions revise their initial judgments after
               being exposed to the opinion and confidence level of others.
               Based on the observation of 59 experimental subjects exposed to
               peer-opinion for 15 different items, we draw an influence map
               that describes the strength of peer influence during
               interactions. A simple process model derived from our
               observations demonstrates how opinions in a group of interacting
               people can converge or split over repeated interactions. In
               particular, we identify two major attractors of opinion: (i) the
               expert effect, induced by the presence of a highly confident
               individual in the group, and (ii) the majority effect, caused by
               the presence of a critical mass of laypeople sharing similar
               opinions. Additional simulations reveal the existence of a
               tipping point at which one attractor will dominate over the
               other, driving collective opinion in a given direction. These
               findings have implications for understanding the mechanisms of
               public opinion formation and managing conflicting situations in
               which self-confident and better informed minorities challenge the
               views of a large uninformed majority.",
  month     =  nov,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Sap_undated-nn,
  title  = "{SOCIAL} {IQ}: Commonsense Reasoning about Social Interactions",
  author = "Sap, Maarten and Rashkin, ♦♥ Hannah and Chen, ♦♥ Derek and Le Bras,
            Ronan and Choi♦♥, Yejin"
}

@ARTICLE{Wurschinger2021-hj,
  title     = "Social Networks of Lexical Innovation. Investigating the Social
               Dynamics of Diffusion of Neologisms on Twitter",
  author    = "Würschinger, Quirin",
  journal   = "Front Artif Intell",
  publisher = "frontiersin.org",
  volume    =  4,
  pages     =  648583,
  abstract  = "Societies continually evolve and speakers use new words to talk
               about innovative products and practices. While most lexical
               innovations soon fall into disuse, others spread successfully and
               become part of the lexicon. In this paper, I conduct a
               longitudinal study of the spread of 99 English neologisms on
               Twitter to study their degrees and pathways of diffusion.
               Previous work on lexical innovation has almost exclusively relied
               on usage frequency for investigating the spread of new words. To
               get a more differentiated picture of diffusion, I use
               frequency-based measures to study temporal aspects of diffusion
               and I use network analyses for a more detailed and accurate
               investigation of the sociolinguistic dynamics of diffusion. The
               results show that frequency measures manage to capture diffusion
               with varying success. Frequency counts can serve as an
               approximate indicator for overall degrees of diffusion, yet they
               miss important information about the temporal usage profiles of
               lexical innovations. The results indicate that neologisms with
               similar total frequency can exhibit significantly different
               degrees of diffusion. Analysing differences in their temporal
               dynamics of use with regard to their age, trends in usage
               intensity, and volatility contributes to a more accurate account
               of their diffusion. The results obtained from the social network
               analysis reveal substantial differences in the social pathways of
               diffusion. Social diffusion significantly correlates with the
               frequency and temporal usage profiles of neologisms. However, the
               network visualisations and metrics identify neologisms whose
               degrees of social diffusion are more limited than suggested by
               their overall frequency of use. These include, among others,
               highly volatile neologisms (e.g., poppygate) and political terms
               (e.g., alt-left), whose use almost exclusively goes back to
               single communities of closely-connected, like-minded individuals.
               I argue that the inclusion of temporal and social information is
               of particular importance for the study of lexical innovation
               since neologisms exhibit high degrees of temporal volatility and
               social indexicality. More generally, the present approach
               demonstrates the potential of social network analysis for
               sociolinguistic research on linguistic innovation, variation, and
               change.",
  month     =  nov,
  year      =  2021,
  keywords  = "Twitter; diffusion; lexical innovation; lexicology; social media;
               social network analysis; sociolinguistics; time-series analysis",
  language  = "en"
}

@INCOLLECTION{Heiss2017-jh,
  title     = "Social Roles",
  author    = "Heiss, Jerold",
  booktitle = "Social Psychology",
  publisher = "Routledge",
  pages     = "94--130",
  month     =  sep,
  year      =  2017
}

@BOOK{Trudgill2011-bg,
  title     = "Sociolinguistic Typology: Social Determinants of Linguistic
               Complexity",
  author    = "Trudgill, Peter",
  publisher = "OUP Oxford",
  abstract  = "Peter Trudgill looks at why human societies at different times
               and places produce different kinds of language. He considers how
               far social factors influence language structure and compares
               languages and dialects spoken across the globe, from Vietnam to
               Nigeria, Polynesia to Scandinavia, and from Canada to Amazonia.
               Modesty prevents Pennsylvanian Dutch Mennonites using the verb
               wotte ('want'); stratified society lies behind complicated
               Japanese honorifics; and a mountainous homeland suggests why
               speakers of Tibetan-Burmese Lahu have words for up there and down
               there. But culture and environment don't explain why Amazonian
               Jarawara needs three past tenses, nor why Nigerian Igbo can make
               do with eight adjectives, nor why most languages spoken in high
               altitudes do not exhibit an array of spatial demonstratives. Nor
               do they account for some languages changing faster than others or
               why some get more complex while others get simpler. The author
               looks at these and many other puzzles, exploring the social,
               linguistic, and other factors that might explain them and in the
               context of a huge range of languages and societies. Peter
               Trudgill writes readably, accessibly, and congenially. His book
               is jargon-free, informed by acute observation, and enlivened by
               argument: it will appeal to everyone with an interest in the
               interactions of language with culture, environment, and society.",
  month     =  oct,
  year      =  2011,
  language  = "en"
}

@ARTICLE{Brante1988-ym,
  title     = "Sociological approaches to the professions",
  author    = "Brante, Thomas",
  journal   = "Acta Sociol.",
  publisher = "SAGE Publications",
  volume    =  31,
  number    =  2,
  pages     = "119--142",
  abstract  = "Studies of the professions clearly illustrate the intricate
               interplay between general conceptions of society and history,
               sociological theory, definitions of social categories, empirical
               research and political values — or, more bnefly, between theory,
               'facts' and politics In this article the interplay is illustrated
               by the two dominant theory-constructions on professions in
               sociology. The first is the functionalist or 'naive' tradition,
               the second the neoweberian or 'cynical' alternative. It is argued
               that both traditions are permeated by several shortcomings. In
               particular, they have universalistic claims, but are in fact
               outcomes of professionals' own self-images during specific and
               limited social and historical circumstances. Sociologies of pro
               fessions turn out to be ideologies of professionals. In a
               concluding section the preconditions for a more realistic
               approach to the professions are outlined.",
  month     =  apr,
  year      =  1988,
  language  = "en"
}

@ARTICLE{Kim2022-ev,
  title         = "{SODA}: Million-scale Dialogue Distillation with Social
                   Commonsense Contextualization",
  author        = "Kim, Hyunwoo and Hessel, Jack and Jiang, Liwei and West,
                   Peter and Lu, Ximing and Yu, Youngjae and Zhou, Pei and Le
                   Bras, Ronan and Alikhani, Malihe and Kim, Gunhee and Sap,
                   Maarten and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Data scarcity has been a long standing issue in the field of
                   open-domain social dialogue. To quench this thirst, we
                   present SODA: the first publicly available, million-scale
                   high-quality social dialogue dataset. By contextualizing
                   social commonsense knowledge from a knowledge graph, we are
                   able to distill an exceptionally broad spectrum of social
                   interactions from a large language model. Human evaluation
                   shows that conversations in SODA are more consistent,
                   specific, and (surprisingly) natural than those in prior
                   human-authored datasets. Using SODA, we train COSMO: a
                   generalizable conversation model that is significantly more
                   natural and consistent on unseen datasets than
                   best-performing conversation models (e.g., GODEL,
                   BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is
                   sometimes even preferred to the original human-written gold
                   responses. Additionally, our results shed light on the
                   distinction between knowledge-enriched conversations and
                   natural social chitchats. We plan to make our data, model,
                   and code public.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chang2023-iq,
  title         = "Speak, Memory: An Archaeology of Books Known to
                   {ChatGPT}/{GPT}-4",
  author        = "Chang, Kent K and Cramer, Mackenzie and Soni, Sandeep and
                   Bamman, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this work, we carry out a data archaeology to infer books
                   that are known to ChatGPT and GPT-4 using a name cloze
                   membership inference query. We find that OpenAI models have
                   memorized a wide collection of copyrighted materials, and
                   that the degree of memorization is tied to the frequency with
                   which passages of those books appear on the web. The ability
                   of these models to memorize an unknown set of books
                   complicates assessments of measurement validity for cultural
                   analytics by contaminating test data; we show that models
                   perform much better on memorized books than on non-memorized
                   books for downstream tasks. We argue that this supports a
                   case for open models whose training data is known.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hammerl2022-ql,
  title         = "Speaking Multiple Languages Affects the Moral Bias of
                   Language Models",
  author        = "Hämmerl, Katharina and Deiseroth, Björn and Schramowski,
                   Patrick and Libovický, Jindřich and Rothkopf, Constantin A
                   and Fraser, Alexander and Kersting, Kristian",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-trained multilingual language models (PMLMs) are commonly
                   used when dealing with data from multiple languages and
                   cross-lingual transfer. However, PMLMs are trained on varying
                   amounts of data for each language. In practice this means
                   their performance is often much better on English than many
                   other languages. We explore to what extent this also applies
                   to moral norms. Do the models capture moral norms from
                   English and impose them on other languages? Do the models
                   exhibit random and thus potentially harmful beliefs in
                   certain languages? Both these issues could negatively impact
                   cross-lingual transfer and potentially lead to harmful
                   outcomes. In this paper, we (1) apply the MoralDirection
                   framework to multilingual models, comparing results in
                   German, Czech, Arabic, Chinese, and English, (2) analyse
                   model behaviour on filtered parallel subtitles corpora, and
                   (3) apply the models to a Moral Foundations Questionnaire,
                   comparing with human responses from different countries. Our
                   experiments demonstrate that, indeed, PMLMs encode differing
                   moral biases, but these do not necessarily correspond to
                   cultural differences or commonalities in human opinions. We
                   release our code and models.",
  month         =  nov,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ging2018-is,
  title     = "Special issue on online misogyny",
  author    = "Ging, D and Siapera, E",
  journal   = "Feminist media studies",
  publisher = "Taylor \& Francis",
  abstract  = "This special issue seeks to identify and theorise the complex
               relationships between online culture, technology and misogyny. It
               asks how the internet's anti-woman spaces and …",
  year      =  2018
}

@ARTICLE{Kundu2023-qw,
  title     = "Specific versus general principles for constitutional ai",
  author    = "Kundu, S and Bai, Y and Kadavath, S and Askell, A and {others}",
  journal   = "arXiv preprint arXiv",
  publisher = "arxiv.org",
  abstract  = "Human feedback can prevent overtly harmful utterances in
               conversational models, but may not automatically mitigate subtle
               problematic behaviors such as a stated desire for self …",
  year      =  2023
}

@ARTICLE{Lee2023-fg,
  title     = "Speculating on Risks of {AI} Clones to Selfhood and
               Relationships: Doppelganger-phobia, Identity Fragmentation, and
               Living Memories",
  author    = "Lee, Patrick Yung Kang and Ma, Ning F and Kim, Ig-Jae and Yoon,
               Dongwook",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  7,
  number    = "CSCW1",
  pages     = "1--28",
  abstract  = "Digitally replicating the appearance and behaviour of individuals
               is becoming feasible with recent advancements in deep-learning
               technologies such as interactive deepfake applications, voice
               conversion, and virtual actors. Interactive applications of such
               agents, termed AI clones, pose risks related to impression
               management, identity abuse, and unhealthy dependencies.
               Identifying concerns AI clones will generate is a prerequisite to
               establishing the basis of discourse around how this technology
               will impact a source individual's selfhood and interpersonal
               relationships. We presented 20 participants of diverse ages and
               backgrounds with 8 speculative scenarios to explore their
               perception towards the concept of AI clones. We found that (1.
               doppelganger-phobia) the abusive potential of AI clones to
               exploit and displace the identity of an individual elicits
               negative emotional reactions; (2. identity fragmentation)
               creating replicas of a living individual threatens their cohesive
               self-perception and unique individuality; and (3. living
               memories) interacting with a clone of someone with whom the user
               has an existing relationship poses risks of misrepresenting the
               individual or developing over-attachment to the clone. These
               findings provide an avenue to discuss preliminary ethical
               implications, respect for identity and authenticity, and design
               recommendations for creating AI clones.",
  month     =  apr,
  year      =  2023,
  keywords  = "AI agents, AI clones, doppelganger-phobia, human-AI interaction,
               human-centered AI, identify fragmentation, identity, impression
               management, interpersonal relationship, living memories, machine
               learning applications, risks, self-hood"
}

@ARTICLE{Searle1969-ce,
  title     = "Speech Acts: An essay in the philosophy of language",
  author    = "Searle, J",
  journal   = "Cambridge UP",
  publisher = "books.google.com",
  abstract  = "Part I. A Theory of Speech Acts: 1. Methods and scope 2.
               Expressions, meaning and speech acts 3. The structure of
               illocutionary acts 4. Reference as a speech act 5. Predication
               Part II. Some Applications of the Theory: 6. Three fallacies in
               contemporary philosophy 7. Problems of reference 8. Deriving
               'ought' from 'is' Index.",
  year      =  1969
}

@ARTICLE{Russo2023-uu,
  title     = "Spillover of antisocial behavior from fringe platforms: The
               unintended consequences of community banning",
  author    = "Russo, G and Verginer, L and Ribeiro, M H and {others}",
  journal   = "Proceedings of the",
  publisher = "ojs.aaai.org",
  abstract  = "Online platforms face pressure to keep their communities civil
               and respectful. Thus, banning problematic online communities from
               mainstream platforms is often met with enthusiastic …",
  year      =  2023
}

@ARTICLE{Lahiri2015-wn,
  title         = "{SQUINKY}! A Corpus of Sentence-level Formality,
                   Informativeness, and Implicature",
  author        = "Lahiri, Shibamouli",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce a corpus of 7,032 sentences rated by human
                   annotators for formality, informativeness, and implicature on
                   a 1-7 scale. The corpus was annotated using Amazon Mechanical
                   Turk. Reliability in the obtained judgments was examined by
                   comparing mean ratings across two MTurk experiments, and
                   correlation with pilot annotations (on sentence formality)
                   conducted in a more controlled setting. Despite the
                   subjectivity and inherent difficulty of the annotation task,
                   correlations between mean ratings were quite encouraging,
                   especially on formality and informativeness. We further
                   explored correlation between the three linguistic variables,
                   genre-wise variation of ratings and correlations within
                   genres, compatibility with automatic stylistic scoring, and
                   sentential make-up of a document in terms of style. To date,
                   our corpus is the largest sentence-level annotated corpus
                   released for formality, informativeness, and implicature.",
  month         =  jun,
  year          =  2015,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-nw,
  title = "{SSRN}-{id4238015}.pdf"
}

@ARTICLE{Nadeem2020-rq,
  title         = "{StereoSet}: Measuring stereotypical bias in pretrained
                   language models",
  author        = "Nadeem, Moin and Bethke, Anna and Reddy, Siva",
  journal       = "arXiv [cs.CL]",
  abstract      = "A stereotype is an over-generalized belief about a particular
                   group of people, e.g., Asians are good at math or Asians are
                   bad drivers. Such beliefs (biases) are known to hurt target
                   groups. Since pretrained language models are trained on large
                   real world data, they are known to capture stereotypical
                   biases. In order to assess the adverse effects of these
                   models, it is important to quantify the bias captured in
                   them. Existing literature on quantifying bias evaluates
                   pretrained language models on a small set of artificially
                   constructed bias-assessing sentences. We present StereoSet, a
                   large-scale natural dataset in English to measure
                   stereotypical biases in four domains: gender, profession,
                   race, and religion. We evaluate popular models like BERT,
                   GPT-2, RoBERTa, and XLNet on our dataset and show that these
                   models exhibit strong stereotypical biases. We also present a
                   leaderboard with a hidden test set to track the bias of
                   future language models at https://stereoset.mit.edu",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{De_Vassimon_Manela2021-aq,
  title     = "Stereotype and Skew: Quantifying Gender Bias in Pre-trained and
               Fine-tuned Language Models",
  author    = "de Vassimon Manela, Daniel and Errington, David and Fisher,
               Thomas and van Breugel, Boris and Minervini, Pasquale",
  editor    = "Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut",
  booktitle = "Proceedings of the 16th Conference of the European Chapter of the
               Association for Computational Linguistics: Main Volume",
  publisher = "Association for Computational Linguistics",
  address   = "Online",
  pages     = "2232--2242",
  abstract  = "This paper proposes two intuitive metrics, skew and stereotype,
               that quantify and analyse the gender bias present in contextual
               language models when tackling the WinoBias pronoun resolution
               task. We find evidence that gender stereotype correlates
               approximately negatively with gender skew in out-of-the-box
               models, suggesting that there is a trade-off between these two
               forms of bias. We investigate two methods to mitigate bias. The
               first approach is an online method which is effective at removing
               skew at the expense of stereotype. The second, inspired by
               previous work on ELMo, involves the fine-tuning of BERT using an
               augmented gender-balanced dataset. We show that this reduces both
               skew and stereotype relative to its unaugmented fine-tuned
               counterpart. However, we find that existing gender bias
               benchmarks do not fully probe professional bias as pronoun
               resolution may be obfuscated by cross-correlations from other
               manifestations of gender prejudice.",
  month     =  apr,
  year      =  2021
}

@INPROCEEDINGS{Marsden2016-ag,
  title     = "Stereotypes and Politics: Reflections on Personas",
  author    = "Marsden, Nicola and Haag, Maren",
  booktitle = "Proceedings of the 2016 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "4017--4031",
  abstract  = "Using personas in requirement analysis and software development
               is becoming more and more common. The potential and problems with
               this method of user representation are discussed controversially
               in HCI research. While personas might help focus on the audience,
               prioritize, challenge assumptions, and prevent self-referential
               design, the success of the method depends on how and on what
               basis the persona descriptions are developed, perceived, and
               employed. Personas run the risk of reinscribing existing
               stereotypes and following more of an I-methodological than a
               user-centered approach. This paper gives an overview of the
               academic discourse regarding benefits and downfalls of the
               persona method. A semi-structured interview study researched how
               usability experts perceive and navigate the controversies of this
               discourse. The qualitative analysis showed that conflicting
               paradigms are embedded in the legitimization practices of HCI in
               the political realities of computer science and corporate
               settings leading to contradictions and compromises.",
  series    = "CHI '16",
  month     =  may,
  year      =  2016,
  keywords  = "social perception, stereotypes, personas, user representation,
               qualitative study"
}

@ARTICLE{Lee2013-kv,
  title     = "Stereotypes as valid categories of knowledge and human
               perceptions of group differences",
  author    = "Lee, Y T and McCauley, C and Jussim, L",
  journal   = "Soc. Personal. Psychol. Compass",
  publisher = "Wiley Online Library",
  abstract  = "Stereotypes–beliefs about group differences–are more complex than
               is generally assumed. First, we address the multidimensionality
               of stereotypes under the framework of the Cubic …",
  year      =  2013
}

@ARTICLE{Van_Miltenburg2016-ea,
  title         = "Stereotyping and Bias in the {Flickr30K} Dataset",
  author        = "van Miltenburg, Emiel",
  journal       = "arXiv [cs.CL]",
  abstract      = "An untested assumption behind the crowdsourced descriptions
                   of the images in the Flickr30K dataset (Young et al., 2014)
                   is that they ``focus only on the information that can be
                   obtained from the image alone'' (Hodosh et al., 2013, p.
                   859). This paper presents some evidence against this
                   assumption, and provides a list of biases and unwarranted
                   inferences that can be found in the Flickr30K dataset.
                   Finally, it considers methods to find examples of these, and
                   discusses how we should deal with stereotype-driven
                   descriptions in future applications.",
  month         =  may,
  year          =  2016,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Bosse_undated-jf,
  title     = "Stereotyping and generics",
  author    = "Bosse, Anne",
  journal   = "Inquiry",
  publisher = "Routledge",
  pages     = "1--17",
  abstract  = "We use generic sentences like ?Blondes are stupid? to express
               stereotypes. But why is this? Does the fact that we use generic
               sentences to express stereotypes mean that stereotypes are
               themselves, in some sense, generic? I argue that they are.
               However, stereotypes are mental and generics linguistic, so how
               can stereotypes be generic? My answer is that stereotypes are
               generic in virtue of the beliefs they contain. Stereotypes about
               blondes being stupid contain a belief element, namely a belief
               that blondes are stupid. This belief is an attitude taken towards
               the same proposition expressed by the sentence ?Blondes are
               stupid?, hence why we use the latter to articulate the former.
               This generic account of stereotypes can help us better understand
               their inner workings. I focus on one feature of generics,
               variability in the types of facts that can make them true, and
               argue that it can explain how stereotypes shape inferential
               patterns and thereby guide how we treat members of stereotyped
               groups. This feature, in turn, illuminates the harms caused by
               stereotyping and suggests some courses of action."
}

@ARTICLE{Beeghly2021-ik,
  title     = "Stereotyping as discrimination: Why thoughts can be
               discriminatory",
  author    = "Beeghly, E",
  journal   = "Social Epistemology",
  publisher = "Taylor \& Francis",
  abstract  = "Can we treat people in a discriminatory way in virtue of how we
               think about them? In this essay, I argue that the answer is yes.
               According to the constitutive claim, stereotyping …",
  year      =  2021
}

@INPROCEEDINGS{Blodgett2021-ae,
  title     = "Stereotyping Norwegian salmon: An inventory of pitfalls in
               fairness benchmark datasets",
  author    = "Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and
               Sim, Robert and Wallach, Hanna",
  editor    = "Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto",
  booktitle = "Proceedings of the 59th Annual Meeting of the Association for
               Computational Linguistics and the 11th International Joint
               Conference on Natural Language Processing (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  pages     = "1004--1015",
  abstract  = "Auditing NLP systems for computational harms like surfacing
               stereotypes is an elusive goal. Several recent efforts have
               focused on benchmark datasets consisting of pairs of contrastive
               sentences, which are often accompanied by metrics that aggregate
               an NLP system's behavior on these pairs into measurements of
               harms. We examine four such benchmarks constructed for two NLP
               tasks: language modeling and coreference resolution. We apply a
               measurement modeling lens---originating from the social
               sciences---to inventory a range of pitfalls that threaten these
               benchmarks' validity as measurement models for stereotyping. We
               find that these benchmarks frequently lack clear articulations of
               what is being measured, and we highlight a range of ambiguities
               and unstated assumptions that affect how these benchmarks
               conceptualize and operationalize stereotyping.",
  month     =  aug,
  year      =  2021
}

@ARTICLE{UnknownUnknown-eu,
  title = "Steroids, Sneakers, Coach: The Spectrum of Human-{AI} Relationships"
}

@ARTICLE{Lore2023-jt,
  title         = "Strategic Behavior of Large Language Models: Game Structure
                   vs. Contextual Framing",
  author        = "Lorè, Nunzio and Heydari, Babak",
  journal       = "arXiv [cs.GT]",
  abstract      = "This paper investigates the strategic decision-making
                   capabilities of three Large Language Models (LLMs): GPT-3.5,
                   GPT-4, and LLaMa-2, within the framework of game theory.
                   Utilizing four canonical two-player games -- Prisoner's
                   Dilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we
                   explore how these models navigate social dilemmas, situations
                   where players can either cooperate for a collective benefit
                   or defect for individual gain. Crucially, we extend our
                   analysis to examine the role of contextual framing, such as
                   diplomatic relations or casual friendships, in shaping the
                   models' decisions. Our findings reveal a complex landscape:
                   while GPT-3.5 is highly sensitive to contextual framing, it
                   shows limited ability to engage in abstract strategic
                   reasoning. Both GPT-4 and LLaMa-2 adjust their strategies
                   based on game structure and context, but LLaMa-2 exhibits a
                   more nuanced understanding of the games' underlying
                   mechanics. These results highlight the current limitations
                   and varied proficiencies of LLMs in strategic
                   decision-making, cautioning against their unqualified use in
                   tasks requiring complex strategic reasoning.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.GT"
}

@ARTICLE{ZHANG_Yan-qiu2015-oc,
  title     = "Study on Markedness in Linguistics",
  author    = "{ZHANG Yan-qiu} and {TIAN Feng-juan}",
  journal   = "Sino-US Engl. Teach.",
  publisher = "David Publishing Company",
  volume    =  12,
  number    =  9,
  abstract  = "Markedness Theory is one of the most important theories in
               structural linguistics. Its academic significance has long
               attracted more scholars and linguists. This paper intends to
               explore markedness from both formal and semantic perspectives:
               formal markedness, distributional markedness, and semantic
               markedness. These three types of markedness are accounted for in
               detail from their definitions to their characteristics. Then the
               relationship of these three types of markedness is explicated.
               The ultimate aim of this paper is to give an insight to
               markedness phenomenon not only from linguistic form or structure,
               but from its semantic distinctions and pragmatic use. The study
               also aims to help foster the greater application of Markedness
               Theory to other linguistic fields’ study.",
  month     =  sep,
  year      =  2015
}

@ARTICLE{Grosse2023-nu,
  title         = "Studying Large Language Model Generalization with Influence
                   Functions",
  author        = "Grosse, Roger and Bae, Juhan and Anil, Cem and Elhage, Nelson
                   and Tamkin, Alex and Tajdini, Amirhossein and Steiner, Benoit
                   and Li, Dustin and Durmus, Esin and Perez, Ethan and
                   Hubinger, Evan and Lukošiūtė, Kamilė and Nguyen, Karina and
                   Joseph, Nicholas and McCandlish, Sam and Kaplan, Jared and
                   Bowman, Samuel R",
  journal       = "arXiv [cs.LG]",
  abstract      = "When trying to gain better visibility into a machine learning
                   model in order to understand and mitigate the associated
                   risks, a potentially valuable source of evidence is: which
                   training examples most contribute to a given behavior?
                   Influence functions aim to answer a counterfactual: how would
                   the model's parameters (and hence its outputs) change if a
                   given sequence were added to the training set? While
                   influence functions have produced insights for small models,
                   they are difficult to scale to large language models (LLMs)
                   due to the difficulty of computing an inverse-Hessian-vector
                   product (IHVP). We use the Eigenvalue-corrected
                   Kronecker-Factored Approximate Curvature (EK-FAC)
                   approximation to scale influence functions up to LLMs with up
                   to 52 billion parameters. In our experiments, EK-FAC achieves
                   similar accuracy to traditional influence function estimators
                   despite the IHVP computation being orders of magnitude
                   faster. We investigate two algorithmic techniques to reduce
                   the cost of computing gradients of candidate training
                   sequences: TF-IDF filtering and query batching. We use
                   influence functions to investigate the generalization
                   patterns of LLMs, including the sparsity of the influence
                   patterns, increasing abstraction with scale, math and
                   programming abilities, cross-lingual generalization, and
                   role-playing behavior. Despite many apparently sophisticated
                   forms of generalization, we identify a surprising limitation:
                   influences decay to near-zero when the order of key phrases
                   is flipped. Overall, influence functions give us a powerful
                   new tool for studying the generalization properties of LLMs.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@INPROCEEDINGS{Jafaritazehjani2020-dh,
  title     = "Style versus Content: A distinction without a (learnable)
               difference?",
  author    = "Jafaritazehjani, Somayeh and Lecorvé, Gwénolé and Lolive, Damien
               and Kelleher, John D",
  booktitle = "International Conference on Computational Linguistics",
  publisher = "hal.science",
  abstract  = "Textual style transfer involves modifying the style of a text
               while preserving its content. This assumes that it is possible to
               separate style from content. This paper investigates whether this
               separation is possible. We use sentiment transfer as our case
               study for style transfer analysis. Our experimental methodology
               frames style transfer as a multi-objective problem, balancing
               style shift with content preservation and fluency. Due to the
               lack of parallel data for style transfer we employ a variety of
               adversarial encoder-decoder networks in our …",
  year      =  2020
}

@ARTICLE{Muller-Eberstein2023-zb,
  title         = "Subspace Chronicles: How Linguistic Information Emerges,
                   Shifts and Interacts during Language Model Training",
  author        = "Müller-Eberstein, Max and van der Goot, Rob and Plank,
                   Barbara and Titov, Ivan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Representational spaces learned via language modeling are
                   fundamental to Natural Language Processing (NLP), however
                   there has been limited understanding regarding how and when
                   during training various types of linguistic information
                   emerge and interact. Leveraging a novel information theoretic
                   probing suite, which enables direct comparisons of not just
                   task performance, but their representational subspaces, we
                   analyze nine tasks covering syntax, semantics and reasoning,
                   across 2M pre-training steps and five seeds. We identify
                   critical learning phases across tasks and time, during which
                   subspaces emerge, share information, and later disentangle to
                   specialize. Across these phases, syntactic knowledge is
                   acquired rapidly after 0.5\% of full training. Continued
                   performance improvements primarily stem from the acquisition
                   of open-domain knowledge, while semantics and reasoning tasks
                   benefit from later boosts to long-range contextualization and
                   higher specialization. Measuring cross-task similarity
                   further reveals that linguistically related tasks share
                   information throughout training, and do so more during the
                   critical phase of learning than before or after. Our findings
                   have implications for model interpretability, multi-task
                   learning, and learning from limited data.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Atanasoski2015-zl,
  title    = "Surrogate Humanity: Posthuman Networks and the (Racialized)
              Obsolescence of Labor",
  author   = "Atanasoski, Neda and Vora, Kalindi",
  journal  = "Catalyst",
  volume   =  1,
  number   =  1,
  pages    = "1--40",
  abstract = "Historical forms of domination and power, encompassed but not
              limited to social categories and hierarchies of difference, get
              built into seemingly non-human objects and the infrastructures
              that link them, thus sanitizing digital media technologies as
              human-free. Rather than questioning the epistemological and
              ontological underpinnings of the human, fantasies about the
              revolutionary nature of new media and technology developments as
              posthuman carry forward and re-universalize the historical
              specificity of the category “human” whose bounds they claim to
              surpass. To begin to theorize some of the ways in which the notion
              of a revolutionary network of humans and things is both racial and
              racializing, the first part of this article develops a reading of
              Sylvia Wynter’s theorization of modern “man” as fundamentally
              constructed through racial-scientific notions of the biological
              and economic. We then think Wynter’s notion of homo-oeconomicus
              alongside Rifkin’s postulation that in fact the infrastructure
              revolution marks a paradigm shift away from capitalism. Through an
              analysis of several contemporary platforms (including Alfred and
              Amazon Mechanical Turk), we address the global-racial erasures and
              disappearances undergirding techno-utopic fantasies of a
              post-labor society. At the same time, as we argue, it is
              insufficient to merely point out the way in which human racialized
              and gendered labor underwrites techno-utopic fantasies. Instead,
              we move to a consideration of the epistemological and material
              shifts as well as legacies tied to prior post-Enlightenment
              revolutionary thought, such as that of Franz Fanon, to
              reconceptualize who or what can count as human. In conversation
              with feminist science studies scholarship on the posthuman, we
              grapple with what it means to think the subject of labor, and the
              human as subject, outside of the biological-economic imperatives
              of prior imaginaries.",
  month    =  jun,
  year     =  2015,
  keywords = "posthuman; Sylvia Wynter; postcolonial science studies; labor;
              infrastructure; Amazon's Mechanical Turk",
  language = "en"
}

@BOOK{Atanasoski2019-wm,
  title     = "Surrogate Humanity: Race, Robots, and the Politics of
               Technological Futures",
  author    = "Atanasoski, Neda and Vora, Kalindi",
  publisher = "Duke University Press",
  abstract  = "In Surrogate Humanity Neda Atanasoski and Kalindi Vora trace the
               ways in which robots, artificial intelligence, and other
               technologies serve as surrogates for human workers within a labor
               system entrenched in racial capitalism and patriarchy. Analyzing
               myriad technologies, from sex robots and military drones to
               sharing-economy platforms, Atanasoski and Vora show how liberal
               structures of antiblackness, settler colonialism, and patriarchy
               are fundamental to human---machine interactions, as well as the
               very definition of the human. While these new technologies and
               engineering projects promise a revolutionary new future, they
               replicate and reinforce racialized and gendered ideas about
               devalued work, exploitation, dispossession, and capitalist
               accumulation. Yet, even as engineers design robots to be more
               perfect versions of the human—more rational killers, more
               efficient workers, and tireless companions—the potential exists
               to develop alternative modes of engineering and technological
               development in ways that refuse the racial and colonial logics
               that maintain social hierarchies and inequality.",
  month     =  feb,
  year      =  2019,
  language  = "en"
}

@MISC{noauthor_undated-ej,
  title = "{SurrogateHumanity}-{VoraAtanasoski}.pdf"
}

@ARTICLE{Ji2023-kg,
  title     = "Survey of Hallucination in Natural Language Generation",
  author    = "Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and
               Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and
               Madotto, Andrea and Fung, Pascale",
  journal   = "ACM Comput. Surv.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  55,
  number    =  12,
  pages     = "1--38",
  abstract  = "Natural Language Generation (NLG) has improved exponentially in
               recent years thanks to the development of sequence-to-sequence
               deep learning technologies such as Transformer-based language
               models. This advancement has led to more fluent and coherent NLG,
               leading to improved development in downstream tasks such as
               abstractive summarization, dialogue generation, and data-to-text
               generation. However, it is also apparent that deep learning based
               generation is prone to hallucinate unintended text, which
               degrades the system performance and fails to meet user
               expectations in many real-world scenarios. To address this issue,
               many studies have been presented in measuring and mitigating
               hallucinated texts, but these have never been reviewed in a
               comprehensive manner before.In this survey, we thus provide a
               broad overview of the research progress and challenges in the
               hallucination problem in NLG. The survey is organized into two
               parts: (1) a general overview of metrics, mitigation methods, and
               future directions, and (2) an overview of task-specific research
               progress on hallucinations in the following downstream tasks,
               namely abstractive summarization, dialogue generation, generative
               question answering, data-to-text generation, and machine
               translation. This survey serves to facilitate collaborative
               efforts among researchers in tackling the challenge of
               hallucinated texts in NLG.",
  month     =  mar,
  year      =  2023,
  keywords  = "extrinsic hallucination, consistency in NLG, faithfulness in NLG,
               intrinsic hallucination, factuality in NLG, Hallucination"
}

@BOOK{Converse1986-bw,
  title     = "Survey questions: Handcrafting the standardized questionnaire",
  author    = "Converse, Jean M and Presser, Stanley",
  publisher = "SAGE Publications",
  address   = "Thousand Oaks, CA",
  series    = "Quantitative Applications in the Social Sciences",
  month     =  nov,
  year      =  1986,
  language  = "en"
}

@ARTICLE{Khattab2020-me,
  title     = "Synching and performing : body (re)-presentation in the short
               video app {TikTok}",
  author    = "Khattab, Mona",
  publisher = "Filmiverkko ry.",
  abstract  = "The performance of the body via new media seems centered on
               negotiating stereotypes of the body image, mainly gendered images
               of masculinity and femininity, and perceived notions of beauty as
               an indicator of sexual appeal. This study seeks to analyze the
               role of social networks in shaping stereotypes that rely on body
               visibility. The article chooses the short video app TikTok as one
               of the recent social networking apps (SNAs) offering users the
               ability to upload, edit, and share short form videos. The
               research methodology offers a content analysis of sample videos
               focusing on self-representation. Such analysis examines the
               impact SNAs have on the formation and expression of users’
               notions of beauty and gender through their digital
               representations of the body.",
  month     =  jan,
  year      =  2020,
  keywords  = "beauty; body image; gender performativity; self-representation",
  language  = "en"
}

@ARTICLE{Submission_undated-rr,
  title  = "Systematic Biases in {LLM} Simulations of Debates",
  author = "Submission, Anonymous Acl"
}

@BOOK{Zerubavel2020-mp,
  title     = "Taken for granted: The remarkable power of the unremarkable",
  author    = "Zerubavel, Eviatar",
  publisher = "Princeton University Press",
  address   = "Princeton, NJ",
  month     =  mar,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Shanahan2022-dv,
  title         = "Talking About Large Language Models",
  author        = "Shanahan, Murray",
  journal       = "arXiv [cs.CL]",
  abstract      = "Thanks to rapid progress in artificial intelligence, we have
                   entered an era when technology and philosophy intersect in
                   interesting ways. Sitting squarely at the centre of this
                   intersection are large language models (LLMs). The more adept
                   LLMs become at mimicking human language, the more vulnerable
                   we become to anthropomorphism, to seeing the systems in which
                   they are embedded as more human-like than they really are.
                   This trend is amplified by the natural tendency to use
                   philosophically loaded terms, such as ``knows'',
                   ``believes'', and ``thinks'', when describing these systems.
                   To mitigate this trend, this paper advocates the practice of
                   repeatedly stepping back to remind ourselves of how LLMs, and
                   the systems of which they form a part, actually work. The
                   hope is that increased scientific precision will encourage
                   more philosophical nuance in the discourse around artificial
                   intelligence, both within the field and in the public sphere.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Njoo2023-zr,
  title    = "{TalkUp}: Paving the Way for Understanding Empowering Language",
  author   = "Njoo, Lucille and Park, Chan Young and Stappart, Octavia and
              Thielk, Marvin and Chu, Yi and Tsvetkov, Yulia",
  abstract = "Empowering language is important in many real-world contexts, from
              education to workplace dynamics to healthcare. Though language
              technologies are growing more prevalent in these contexts,
              empowerment has seldom been studied in NLP, and moreover, it is
              inherently challenging to operationalize because of its implicit
              nature. This work builds from linguistic and social psychology
              literature to explore what characterizes empowering language. We
              then crowdsource a novel dataset of Reddit posts labeled for
              empowerment, reasons why these posts are empowering to readers,
              and the social relationships between posters and readers. Our
              preliminary analyses show that this dataset, which we call TalkUp,
              can be used to train language models that capture empowering and
              disempowering language. More broadly, TalkUp provides an avenue to
              explore implication, presuppositions, and how social context
              influences the meaning of language.",
  month    =  dec,
  year     =  2023
}

@ARTICLE{Njoo2023-tw,
  title         = "{TalkUp}: Paving the way for understanding empowering
                   language",
  author        = "Njoo, Lucille and Park, Chan Young and Stappart, Octavia and
                   Thielk, Marvin and Chu, Yi and Tsvetkov, Yulia",
  journal       = "arXiv [cs.CL]",
  abstract      = "Empowering language is important in many real-world contexts,
                   from education to workplace dynamics to healthcare. Though
                   language technologies are growing more prevalent in these
                   contexts, empowerment has seldom been studied in NLP, and
                   moreover, it is inherently challenging to operationalize
                   because of its implicit nature. This work builds from
                   linguistic and social psychology literature to explore what
                   characterizes empowering language. We then crowdsource a
                   novel dataset of Reddit posts labeled for empowerment,
                   reasons why these posts are empowering to readers, and the
                   social relationships between posters and readers. Our
                   preliminary analyses show that this dataset, which we call
                   TalkUp, can be used to train language models that capture
                   empowering and disempowering language. More broadly, TalkUp
                   provides an avenue to explore implication, presuppositions,
                   and how social context influences the meaning of language.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-ux,
  title = "{TamasiAntieau2015Ch1}.pdf"
}

@MISC{noauthor_undated-ne,
  title = "{TamasiAntieau2015Ch6}.pdf"
}

@ARTICLE{Panigrahi2023-ij,
  title         = "Task-Specific Skill Localization in Fine-tuned Language
                   Models",
  author        = "Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and
                   Arora, Sanjeev",
  journal       = "arXiv [cs.CL]",
  abstract      = "Pre-trained language models can be fine-tuned to solve
                   diverse NLP tasks, including in few-shot settings. Thus
                   fine-tuning allows the model to quickly pick up task-specific
                   ``skills,'' but there has been limited study of where these
                   newly-learnt skills reside inside the massive model. This
                   paper introduces the term skill localization for this problem
                   and proposes a solution. Given the downstream task and a
                   model fine-tuned on that task, a simple optimization is used
                   to identify a very small subset of parameters ($\sim0.01$\%
                   of model parameters) responsible for ($>95$\%) of the model's
                   performance, in the sense that grafting the fine-tuned values
                   for just this tiny subset onto the pre-trained model gives
                   performance almost as well as the fine-tuned model. While
                   reminiscent of recent works on parameter-efficient
                   fine-tuning, the novel aspects here are that: (i) No further
                   re-training is needed on the subset (unlike, say, with
                   lottery tickets). (ii) Notable improvements are seen over
                   vanilla fine-tuning with respect to calibration of
                   predictions in-distribution ($40$-$90$\% error reduction) as
                   well as the quality of predictions out-of-distribution (OOD).
                   In models trained on multiple tasks, a stronger notion of
                   skill localization is observed, where the sparse regions
                   corresponding to different tasks are almost disjoint, and
                   their overlap (when it happens) is a proxy for task
                   similarity. Experiments suggest that localization via
                   grafting can assist certain forms of continual learning.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Katzman2023-xv,
  title    = "Taxonomizing and measuring representational harms: A look at image
              tagging",
  author   = "Katzman, Jared and Wang, Angelina and Scheuerman, M and Blodgett,
              Su Lin and Laird, Kristen and Wallach, Hanna M and Barocas, Solon",
  journal  = "National Conference on Artificial Intelligence",
  volume   = "abs/2305.01776",
  abstract = "In this paper, we examine computational approaches for measuring
              the ``fairness'' of image tagging systems, finding that they
              cluster into five distinct categories, each with its own analytic
              foundation. We also identify a range of normative concerns that
              are often collapsed under the terms ``unfairness,'' ``bias,'' or
              even ``discrimination'' when discussing problematic cases of image
              tagging. Specifically, we identify four types of representational
              harms that can be caused by image tagging systems, providing
              concrete examples of each. We then consider how different
              computational measurement approaches map to each of these types,
              demonstrating that there is not a one-to-one mapping. Our findings
              emphasize that no single measurement approach will be definitive
              and that it is not possible to infer from the use of a particular
              measurement approach which type of harm was intended to be
              measured. Lastly, equipped with this more granular understanding
              of the types of representational harms that can be caused by image
              tagging systems, we show that attempts to mitigate some of these
              types of harms may be in tension with one another.",
  month    =  may,
  year     =  2023
}

@INPROCEEDINGS{Weidinger2022-pz,
  title     = "Taxonomy of Risks posed by Language Models",
  author    = "Weidinger, Laura and Uesato, Jonathan and Rauh, Maribeth and
               Griffin, Conor and Huang, Po-Sen and Mellor, John and Glaese,
               Amelia and Cheng, Myra and Balle, Borja and Kasirzadeh, Atoosa
               and Biles, Courtney and Brown, Sasha and Kenton, Zac and Hawkins,
               Will and Stepleton, Tom and Birhane, Abeba and Hendricks, Lisa
               Anne and Rimell, Laura and Isaac, William and Haas, Julia and
               Legassick, Sean and Irving, Geoffrey and Gabriel, Iason",
  booktitle = "2022 ACM Conference on Fairness, Accountability, and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "214--229",
  month     =  jun,
  year      =  2022
}

@ARTICLE{Vizcaino-Verdu2023-ls,
  title     = "{TeachTok}: Teachers of {TikTok}, micro-celebrification, and fun
               learning communities",
  author    = "Vizcaíno-Verdú, A and Abidin, C",
  journal   = "Teaching and Teacher Education",
  publisher = "Elsevier",
  abstract  = "This study explores the subculture of teachers and teaching on
               TikTok, known in the vernacular as 'TeachTok', through a daily
               walkthrough method, a digital ethnography …",
  year      =  2023
}

@ARTICLE{Mauss1973-hx,
  title     = "Techniques of the body∗",
  author    = "Mauss, Marcel",
  journal   = "Econ. Soc.",
  publisher = "Informa UK Limited",
  volume    =  2,
  number    =  1,
  pages     = "70--88",
  month     =  feb,
  year      =  1973
}

@ARTICLE{Marx2010-zc,
  title     = "Technology: The Emergence of a Hazardous Concept",
  author    = "Marx, Leo",
  journal   = "Technol. Cult.",
  publisher = "[The Johns Hopkins University Press, Society for the History of
               Technology]",
  volume    =  51,
  number    =  3,
  pages     = "561--577",
  abstract  = "… concept of culture, but they had simultaneously changed its
               meaning. I believe that a similar process marked the emergence of
               … prompted the emergence of technology - the concept , the …",
  year      =  2010
}

@MISC{noauthor_undated-aa,
  title = "Terry Winograd, Fernando Flores - Understanding Computers and
           Cognition-Addison-Wesley Publishing Company.pdf"
}

@ARTICLE{Golan2022-uu,
  title         = "Testing the limits of natural language models for predicting
                   human language judgments",
  author        = "Golan, Tal and Siegelman, Matthew and Kriegeskorte, Nikolaus
                   and Baldassano, Christopher",
  journal       = "arXiv [cs.CL]",
  abstract      = "Neural network language models can serve as computational
                   hypotheses about how humans process language. We compared the
                   model-human consistency of diverse language models using a
                   novel experimental approach: controversial sentence pairs.
                   For each controversial sentence pair, two language models
                   disagree about which sentence is more likely to occur in
                   natural text. Considering nine language models (including
                   n-gram, recurrent neural networks, and transformer models),
                   we created hundreds of such controversial sentence pairs by
                   either selecting sentences from a corpus or synthetically
                   optimizing sentence pairs to be highly controversial. Human
                   subjects then provided judgments indicating for each pair
                   which of the two sentences is more likely. Controversial
                   sentence pairs proved highly effective at revealing model
                   failures and identifying models that aligned most closely
                   with human judgments. The most human-consistent model tested
                   was GPT-2, although experiments also revealed significant
                   shortcomings of its alignment with human perception.",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Yang2022-pg,
  title    = "Testing the limits of structural thinking about gender",
  author   = "Yang, Xin and Naas, Ragnhild and Dunham, Yarrow",
  journal  = "Dev. Sci.",
  volume   =  25,
  number   =  2,
  pages    = "e13169",
  abstract = "When seeking to explain social regularities (such as gender
              differences in the labor market) people often rely on internal
              features of the targets, frequently neglecting structural and
              systemic factors external to the targets. For example, people
              might think women leave the job market after childbirth because
              they are less competent or are better suited for child-rearing
              than men, thereby eliding socio-cultural and economic factors that
              disadvantage women. Across two studies (total N = 192) we probe 4-
              and 5-year-olds and 7- and 8-year-olds' internal versus structural
              reasoning about gender. We explore the evaluative and behavioral
              implications of this reasoning process with both novel gendered
              behaviors that were experimentally created and familiar gendered
              behaviors that exist outside of a lab context. We show that
              children generate more structural explanations, evaluate the
              structural explanation more positively, expect behaviors to be
              more mutable, and evaluate gender non-conforming behaviors more
              positively when structural cues are provided. However, we also
              show that such information may be of limited effectiveness at
              reducing pre-existing group-based discriminatory behaviors:
              children continue to report less willingness to affiliate with
              peers who display non-conforming behaviors even in the presence of
              structural cues. Taken together, these results provide evidence
              concerning children's structural reasoning about gender categories
              and shed new light on how such reasoning might affect social
              evaluations and behavioral intentions.",
  month    =  mar,
  year     =  2022,
  keywords = "behavioral intentions; essentialism; evaluations; explanations;
              gender; structural thinking",
  language = "en"
}

@ARTICLE{Xie2020-de,
  title         = "Text-based inference of moral sentiment change",
  author        = "Xie, Jing Yi and Pinto, Jr, Renato Ferreira and Hirst, Graeme
                   and Xu, Yang",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present a text-based framework for investigating moral
                   sentiment change of the public via longitudinal corpora. Our
                   framework is based on the premise that language use can
                   inform people's moral perception toward right or wrong, and
                   we build our methodology by exploring moral biases learned
                   from diachronic word embeddings. We demonstrate how a
                   parameter-free model supports inference of historical shifts
                   in moral sentiment toward concepts such as slavery and
                   democracy over centuries at three incremental levels: moral
                   relevance, moral polarity, and fine-grained moral dimensions.
                   We apply this methodology to visualizing moral time courses
                   of individual concepts and analyzing the relations between
                   psycholinguistic variables and rates of moral sentiment
                   change at scale. Our work offers opportunities for applying
                   natural language processing toward characterizing moral
                   sentiment change in society.",
  month         =  jan,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Morris2023-yi,
  title         = "Text Embeddings Reveal (Almost) As Much As Text",
  author        = "Morris, John X and Kuleshov, Volodymyr and Shmatikov, Vitaly
                   and Rush, Alexander M",
  journal       = "arXiv [cs.CL]",
  abstract      = "How much private information do text embeddings reveal about
                   the original text? We investigate the problem of embedding
                   \textit{inversion}, reconstructing the full text represented
                   in dense text embeddings. We frame the problem as controlled
                   generation: generating text that, when reembedded, is close
                   to a fixed point in latent space. We find that although a
                   naive model conditioned on the embedding performs poorly, a
                   multi-step method that iteratively corrects and re-embeds
                   text is able to recover $92\%$ of $32\text{-token}$ text
                   inputs exactly. We train our model to decode text embeddings
                   from two state-of-the-art embedding models, and also show
                   that our model can recover important personal information
                   (full names) from a dataset of clinical notes. Our code is
                   available on Github:
                   \href{https://github.com/jxmorris12/vec2text}{github.com/jxmorris12/vec2text}.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-vc,
  title = "The-{ALL}-{NEW}-Dont-Think-of-an-Elephant\_-{K}-George-Lakoff.pdf"
}

@ARTICLE{Epstein2022-nr,
  title     = "The Answer Bot Effect ({ABE}): A powerful new form of influence
               made possible by intelligent personal assistants and search
               engines",
  author    = "Epstein, Robert and Lee, Vivian and Mohr, Roger and Zankich,
               Vanessa R",
  journal   = "PLoS One",
  publisher = "journals.plos.org",
  volume    =  17,
  number    =  6,
  pages     = "e0268081",
  abstract  = "We introduce and quantify a relatively new form of influence: the
               Answer Bot Effect (ABE). In a 2015 report in PNAS, researchers
               demonstrated the power that biased search results have to shift
               opinions and voting preferences without people's knowledge-by up
               to 80\% in some demographic groups. They labeled this phenomenon
               the Search Engine Manipulation Effect (SEME), speculating that
               its power derives from the high level of trust people have in
               algorithmically-generated content. We now describe three
               experiments with a total of 1,736 US participants conducted to
               determine to what extent giving users ``the answer''-either via
               an answer box at the top of a page of search results or via a
               vocal reply to a question posed to an intelligent personal
               assistant (IPA)-might also impact opinions and votes.
               Participants were first given basic information about two
               candidates running for prime minister of Australia (this, in
               order to assure that participants were ``undecided''), then asked
               questions about their voting preferences, then given answers to
               questions they posed about the candidates-either with answer
               boxes or with vocal answers on an Alexa simulator-and then asked
               again about their voting preferences. The experiments were
               controlled, randomized, double-blind, and counterbalanced.
               Experiments 1 and 2 demonstrated that answer boxes can shift
               voting preferences by as much as 38.6\% and that the appearance
               of an answer box can reduce search times and clicks on search
               results. Experiment 3 demonstrated that even a single
               question-and-answer interaction on an IPA can shift voting
               preferences by more than 40\%. Multiple questions posed to an IPA
               leading to answers that all have the same bias can shift voting
               preferences by more than 65\%. Simple masking procedures still
               produced large opinion shifts while reducing awareness of bias to
               close to zero. ABE poses a serious threat to both democracy and
               human autonomy because (a) it produces large shifts in opinions
               and voting preferences with little or no user awareness, (b) it
               is an ephemeral form of influence that leaves no paper trail, and
               (c) worldwide, it is controlled almost exclusively by just four
               American tech companies. ABE will become a greater threat as
               people increasingly rely on IPAs for answers.",
  month     =  jun,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Malin_undated-nn,
  title  = "The Arc of Narrative: Using Language Markers to Identify Stories",
  author = "Malin, Jennifer J and Vine, Vera J and Stanton, Amelia and Cannava,
            Kaitlin and Bodie, Graham and Pennebaker, James W"
}

@ARTICLE{Kirk2024-wr,
  title     = "The benefits, risks and bounds of personalizing the alignment of
               large language models to individuals",
  author    = "Kirk, Hannah Rose and Vidgen, Bertie and Röttger, Paul and Hale,
               Scott A",
  journal   = "Nature Machine Intelligence",
  publisher = "Nature Publishing Group",
  volume    =  6,
  number    =  4,
  pages     = "383--392",
  abstract  = "Large language models (LLMs) undergo ‘alignment’ so that they
               better reflect human values or preferences, and are safer or more
               useful. However, alignment is intrinsically difficult because the
               hundreds of millions of people who now interact with LLMs have
               different preferences for language and conversational norms,
               operate under disparate value systems and hold diverse political
               beliefs. Typically, few developers or researchers dictate
               alignment norms, risking the exclusion or under-representation of
               various groups. Personalization is a new frontier in LLM
               development, whereby models are tailored to individuals. In
               principle, this could minimize cultural hegemony, enhance
               usefulness and broaden access. However, unbounded personalization
               poses risks such as large-scale profiling, privacy infringement,
               bias reinforcement and exploitation of the vulnerable. Defining
               the bounds of responsible and socially acceptable personalization
               is a non-trivial task beset with normative challenges. This
               article explores ‘personalized alignment’, whereby LLMs adapt to
               user-specific data, and highlights recent shifts in the LLM
               ecosystem towards a greater degree of personalization. Our main
               contribution explores the potential impact of personalized LLMs
               via a taxonomy of risks and benefits for individuals and society
               at large. We lastly discuss a key open question: what are
               appropriate bounds of personalization and who decides? Answering
               this normative question enables users to benefit from
               personalized alignment while safeguarding against harmful impacts
               for individuals and society. Tailoring the alignment of large
               language models (LLMs) to individuals is a new frontier in
               generative AI, but unbounded personalization can bring potential
               harm, such as large-scale profiling, privacy infringement and
               bias reinforcement. Kirk et al. develop a taxonomy for risks and
               benefits of personalized LLMs and discuss the need for normative
               decisions on what are acceptable bounds of personalization.",
  month     =  apr,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Seshadri2023-rh,
  title     = "The Bias Amplification Paradox in Text-to-Image Generation",
  author    = "Seshadri, P and Singh, S and Elazar, Y",
  journal   = "arXiv preprint arXiv:2308.00755",
  publisher = "arxiv.org",
  abstract  = "Bias amplification is a phenomenon in which models increase
               imbalances present in the training data. In this paper, we study
               bias amplification in the text-to-image domain using …",
  year      =  2023
}

@BOOK{Frankish2014-ce,
  title     = "The Cambridge Handbook of Artificial Intelligence",
  author    = "Frankish, Keith and Ramsey, William M and Franklin, Stan and
               Arkoudas, Konstantine and Bringsjord, Selmer and Robinson,
               William S and Boden, Margaret A and Sun, Ron and Beer, Randall D
               and Danks, David and Vincze, Markus and Wachsmuth, Sven and
               Sagerer, Gerhard and Amir, Eyal and Wilks, Yorick and Alonso,
               Eduardo and Scheutz, Matthias and Husbands, Phil and Bedau, Mark
               A and Bostrom, Nick and Yudkowsky, Eliezer",
  publisher = "Cambridge University Press",
  abstract  = "Artificial intelligence, or AI, is a cross-disciplinary approach
               to understanding, modeling, and creating intelligence of various
               forms. It is a critical branch of cognitive science, and its
               influence is increasingly being felt in other areas, including
               the humanities. AI applications are transforming the way we
               interact with each other and with our environment, and work in
               artificially modeling intelligence is offering new insights into
               the human mind and revealing new forms mentality can take. This
               volume of original essays presents the state of the art in AI,
               surveying the foundations of the discipline, major theories of
               mental architecture, the principal areas of research, and
               extensions of AI such as artificial life. With a focus on theory
               rather than technical and applied issues, the volume will be
               valuable not only to people working in AI, but also to those in
               other disciplines wanting an authoritative and up-to-date
               introduction to the field.",
  month     =  jun,
  year      =  2014
}

@ARTICLE{Ganguli2023-co,
  title         = "The Capacity for Moral Self-Correction in Large Language
                   Models",
  author        = "Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and
                   Liao, Thomas I and Lukošiūtė, Kamilė and Chen, Anna and
                   Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and
                   Hernandez, Danny and Drain, Dawn and Li, Dustin and
                   Tran-Johnson, Eli and Perez, Ethan and Kernion, Jackson and
                   Kerr, Jamie and Mueller, Jared and Landau, Joshua and
                   Ndousse, Kamal and Nguyen, Karina and Lovitt, Liane and
                   Sellitto, Michael and Elhage, Nelson and Mercado, Noemi and
                   DasSarma, Nova and Rausch, Oliver and Lasenby, Robert and
                   Larson, Robin and Ringer, Sam and Kundu, Sandipan and
                   Kadavath, Saurav and Johnston, Scott and Kravec, Shauna and
                   El Showk, Sheer and Lanham, Tamera and Telleen-Lawton,
                   Timothy and Henighan, Tom and Hume, Tristan and Bai, Yuntao
                   and Hatfield-Dodds, Zac and Mann, Ben and Amodei, Dario and
                   Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Olah,
                   Christopher and Clark, Jack and Bowman, Samuel R and Kaplan,
                   Jared",
  journal       = "arXiv [cs.CL]",
  abstract      = "We test the hypothesis that language models trained with
                   reinforcement learning from human feedback (RLHF) have the
                   capability to ``morally self-correct'' -- to avoid producing
                   harmful outputs -- if instructed to do so. We find strong
                   evidence in support of this hypothesis across three different
                   experiments, each of which reveal different facets of moral
                   self-correction. We find that the capability for moral
                   self-correction emerges at 22B model parameters, and
                   typically improves with increasing model size and RLHF
                   training. We believe that at this level of scale, language
                   models obtain two capabilities that they can use for moral
                   self-correction: (1) they can follow instructions and (2)
                   they can learn complex normative concepts of harm like
                   stereotyping, bias, and discrimination. As such, they can
                   follow instructions to avoid certain kinds of morally harmful
                   outputs. We believe our results are cause for cautious
                   optimism regarding the ability to train language models to
                   abide by ethical principles.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{noauthor_undated-vy,
  title     = "The {CASA} theory no longer applies to desktop computers",
  author    = "Heyselaar, Evelien",
  journal   = "Sci. Rep.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  13,
  number    =  1,
  pages     =  19693,
  abstract  = "The Computers Are Social Actors (CASA) theory is the most
               important theoretical contribution that has shaped the field of
               human-computer interaction. The theory states that humans
               interact with computers as if they are human, and is the
               cornerstone on which all social human-machine communication
               (e.g., chatbots, robots, virtual agents) are designed. However,
               the theory itself dates back to the early 1990s, and, since then,
               technology and its place in society has evolved and changed
               drastically. Here we show, via a direct replication of the
               original study, that participants no longer interact with desktop
               computers as if they are human. This suggests that the CASA
               Theory may only work for emergent technology, an important
               concept that needs to be taken into account when designing and
               researching human-computer interaction.",
  month     =  nov,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Heyselaar2023-vv,
  title     = "The {CASA} theory no longer applies to desktop computers",
  author    = "Heyselaar, Evelien",
  journal   = "Sci. Rep.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  13,
  number    =  1,
  pages     =  19693,
  abstract  = "The Computers Are Social Actors (CASA) theory is the most
               important theoretical contribution that has shaped the field of
               human-computer interaction. The theory states that humans
               interact with computers as if they are human, and is the
               cornerstone on which all social human-machine communication
               (e.g., chatbots, robots, virtual agents) are designed. However,
               the theory itself dates back to the early 1990s, and, since then,
               technology and its place in society has evolved and changed
               drastically. Here we show, via a direct replication of the
               original study, that participants no longer interact with desktop
               computers as if they are human. This suggests that the CASA
               Theory may only work for emergent technology, an important
               concept that needs to be taken into account when designing and
               researching human-computer interaction.",
  month     =  nov,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Michael2023-uo,
  title         = "The Case for Scalable, Data-Driven Theory: A Paradigm for
                   Scientific Progress in {NLP}",
  author        = "Michael, Julian",
  journal       = "arXiv [cs.CL]",
  abstract      = "I propose a paradigm for scientific progress in NLP centered
                   around developing scalable, data-driven theories of
                   linguistic structure. The idea is to collect data in tightly
                   scoped, carefully defined ways which allow for exhaustive
                   annotation of behavioral phenomena of interest, and then use
                   machine learning to construct explanatory theories of these
                   phenomena which can form building blocks for intelligible AI
                   systems. After laying some conceptual groundwork, I describe
                   several investigations into data-driven theories of shallow
                   semantic structure using Question-Answer driven Semantic Role
                   Labeling (QA-SRL), a schema for annotating verbal
                   predicate-argument relations using highly constrained
                   question-answer pairs. While this only scratches the surface
                   of the complex language behaviors of interest in AI, I
                   outline principles for data collection and theoretical
                   modeling which can inform future scientific progress. This
                   note summarizes and draws heavily on my PhD thesis.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Zhong2023-pk,
  title         = "The Clock and the Pizza: Two Stories in Mechanistic
                   Explanation of Neural Networks",
  author        = "Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas,
                   Jacob",
  journal       = "arXiv [cs.LG]",
  abstract      = "Do neural networks, trained on well-understood algorithmic
                   tasks, reliably rediscover known algorithms for solving those
                   tasks? Several recent studies, on tasks ranging from group
                   arithmetic to in-context linear regression, have suggested
                   that the answer is yes. Using modular addition as a
                   prototypical problem, we show that algorithm discovery in
                   neural networks is sometimes more complex. Small changes to
                   model hyperparameters and initializations can induce the
                   discovery of qualitatively different algorithms from a fixed
                   training set, and even parallel implementations of multiple
                   such algorithms. Some networks trained to perform modular
                   addition implement a familiar Clock algorithm; others
                   implement a previously undescribed, less intuitive, but
                   comprehensible procedure which we term the Pizza algorithm,
                   or a variety of even more complex procedures. Our results
                   show that even simple learning problems can admit a
                   surprising diversity of solutions, motivating the development
                   of new tools for characterizing the behavior of neural
                   networks across their algorithmic phase space.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Richter1956-qa,
  title     = "The Conceptual Mechanism of Stereotyping",
  author    = "Richter, Maurice N",
  journal   = "Am. Sociol. Rev.",
  publisher = "[American Sociological Association, Sage Publications, Inc.]",
  volume    =  21,
  number    =  5,
  pages     = "568--571",
  year      =  1956
}

@ARTICLE{Frith2005-oq,
  title     = "The construction of beauty: A cross-cultural analysis of women's
               magazine advertising",
  author    = "Frith, Katherine and Shaw, Ping and Cheng, Hong",
  journal   = "J. Commun.",
  publisher = "Oxford University Press (OUP)",
  volume    =  55,
  number    =  1,
  pages     = "56--70",
  abstract  = "As a media genre, advertising offers a unique opportunity to
               study how the beauty ideal is constructed across cultures. This
               research analyzes the content of advertisements from women’s
               fashion and beauty magazines in Singapore, Taiwan, and the U.S.
               to compare how beauty is encoded and found a noticeable
               difference between the portrayals of women from the U.S. and from
               the two East Asian societies in terms of sexual portrayal. In
               addition, Asian ads contained a large proportion of cosmetics and
               facial beauty products whereas the U.S. ads were dominated by
               clothing. These findings suggest that beauty in the U.S. may be
               constructed more in terms of “the body,” whereas in Singapore and
               Taiwan the defining factor is more related to a pretty face. The
               article also discusses how feminist critiques of the sexual
               objectification of women in advertising may need to be considered
               within their historical, Western context of origin.",
  month     =  mar,
  year      =  2005
}

@ARTICLE{Hamilton2022-jc,
  title         = "The {COVID} That Wasn't: Counterfactual Journalism Using
                   {GPT}",
  author        = "Hamilton, Sil and Piper, Andrew",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this paper, we explore the use of large language models to
                   assess human interpretations of real world events. To do so,
                   we use a language model trained prior to 2020 to artificially
                   generate news articles concerning COVID-19 given the
                   headlines of actual articles written during the pandemic. We
                   then compare stylistic qualities of our artificially
                   generated corpus with a news corpus, in this case 5,082
                   articles produced by CBC News between January 23 and May 5,
                   2020. We find our artificially generated articles exhibits a
                   considerably more negative attitude towards COVID and a
                   significantly lower reliance on geopolitical framing. Our
                   methods and results hold importance for researchers seeking
                   to simulate large scale cultural processes via recent
                   breakthroughs in text generation.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Holtzman2019-yn,
  title         = "The Curious Case of Neural Text Degeneration",
  author        = "Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell
                   and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Despite considerable advancements with deep neural language
                   models, the enigma of neural text degeneration persists when
                   these models are tested as text generators. The
                   counter-intuitive empirical observation is that even though
                   the use of likelihood as training objective leads to high
                   quality models for a broad range of language understanding
                   tasks, using likelihood as a decoding objective leads to text
                   that is bland and strangely repetitive. In this paper, we
                   reveal surprising distributional differences between human
                   text and machine text. In addition, we find that decoding
                   strategies alone can dramatically effect the quality of
                   machine text, even when generated from exactly the same
                   neural language model. Our findings motivate Nucleus
                   Sampling, a simple but effective method to draw the best out
                   of neural generation. By sampling text from the dynamic
                   nucleus of the probability distribution, which allows for
                   diversity while effectively truncating the less reliable tail
                   of the distribution, the resulting text better demonstrates
                   the quality of human text, yielding enhanced diversity
                   without sacrificing fluency and coherence.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chiesurin2023-il,
  title         = "The dangers of trusting stochastic parrots: Faithfulness and
                   trust in open-domain conversational question answering",
  author        = "Chiesurin, Sabrina and Dimakopoulos, Dimitris and Cabezudo,
                   Marco Antonio Sobrevilla and Eshghi, Arash and Papaioannou,
                   Ioannis and Rieser, Verena and Konstas, Ioannis",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models are known to produce output which
                   sounds fluent and convincing, but is also often wrong, e.g.
                   ``unfaithful'' with respect to a rationale as retrieved from
                   a knowledge base. In this paper, we show that task-based
                   systems which exhibit certain advanced linguistic dialog
                   behaviors, such as lexical alignment (repeating what the user
                   said), are in fact preferred and trusted more, whereas other
                   phenomena, such as pronouns and ellipsis are dis-preferred.
                   We use open-domain question answering systems as our test-bed
                   for task based dialog generation and compare several open-
                   and closed-book models. Our results highlight the danger of
                   systems that appear to be trustworthy by parroting user input
                   while providing an unfaithful response.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Etienne2021-ec,
  title     = "The dark side of the ‘Moral Machine’ and the fallacy of
               computational ethical decision-making for autonomous vehicles",
  author    = "Etienne, Hubert",
  journal   = "Law Innov. Technol.",
  publisher = "Informa UK Limited",
  volume    =  13,
  number    =  1,
  pages     = "85--107",
  abstract  = "This paper reveals the dangers of the Moral Machine experiment,
               alerting against both its uses for normative ends, and the whole
               approach it is built upon to address ethical issues. It explores
               additional methodological limits of the experiment on top of
               those already identified by its authors and provides reasons why
               it is inadequate in supporting ethical and juridical discussions
               to determine the moral settings for autonomous vehicles.
               Demonstrating the inner fallacy behind computational social
               choice methods when applied to ethical decision …",
  month     =  jan,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Benson_undated-ln,
  title    = "The (Dis) Invention of Black Women: A Rhetorical Analysis of
              Intersectional Oppression within Cosmetics Practices",
  author   = "Benson, Chloe",
  journal  = "jbh.journals.villanova.edu",
  abstract = "The American beauty industry has long reproduced the dominant
              Eurocentric worldview which consistently excludes Black women from
              the ideal. Most popular brands fail to …"
}

@ARTICLE{Quinn2006-ya,
  title     = "The Disruptive Effect of Self-Objectification on Performance",
  author    = "Quinn, Diane M and Kallen, Rachel W and Twenge, Jean M and
               Fredrickson, Barbara L",
  journal   = "Psychol. Women Q.",
  publisher = "SAGE Publications Inc",
  volume    =  30,
  number    =  1,
  pages     = "59--64",
  abstract  = "Self-objectification is the act of viewing the self, particularly
               the body, from a third-person perspective. Objectification theory
               proposes numerous negative consequences for those who
               self-objectify, including decreased performance through the
               disruption of focused attention. In the current study, we
               examined whether women in a state of self-objectification were
               slower to respond to a basic Stroop color-naming task. Results
               showed that regardless of the type of word (color words, body
               words, or neutral words), participants in a state of
               self-objectification exhibited decreased performance. This study
               lends further evidence to objectification theory and highlights
               the negative performance ramifications of state
               self-objectification.",
  month     =  mar,
  year      =  2006
}

@ARTICLE{Van_Raemdonck2019-ha,
  title    = "The Echo Chamber of Anti-Vaccination Conspiracies: Mechanisms of
              Radicalization on Facebook and Reddit",
  author   = "Van Raemdonck, Nathalie",
  abstract = "To understand online radicalization, it is important to understand
              the mechanism of echo chambers on certain social media platforms
              like Facebook and Reddit. Radicalizing communities that have not
              reached a violent extremism phase can provide a unique insight.
              This is the case for communities built around conspiracy theories,
              which thrive in the echo chambers of social media. A case in point
              is the anti-vaccination conspiracy: most arguments against
              vaccination are built on conspiracies with little scientific
              evidence, where the online community predominantly echoes
              misinformation. This paper explores how Facebook’s mechanism for
              community building has enhanced the spread of vaccination
              misinformation online, and compares it to Reddit’s conspiracy
              community, applying the RECRO model of online radicalization.",
  month    =  dec,
  year     =  2019,
  keywords = "Online Radicalization, Disinformation, Echo Chambers,
              Anti-Vaccination, Conspiracy, Facebook, Reddit"
}

@ARTICLE{Orlikowski2023-fl,
  title         = "The Ecological Fallacy in Annotation: Modelling Human Label
                   Variation goes beyond Sociodemographics",
  author        = "Orlikowski, Matthias and Röttger, Paul and Cimiano, Philipp
                   and Hovy, Dirk",
  journal       = "arXiv [cs.CL]",
  abstract      = "Many NLP tasks exhibit human label variation, where different
                   annotators give different labels to the same texts. This
                   variation is known to depend, at least in part, on the
                   sociodemographics of annotators. Recent research aims to
                   model individual annotator behaviour rather than predicting
                   aggregated labels, and we would expect that sociodemographic
                   information is useful for these models. On the other hand,
                   the ecological fallacy states that aggregate group behaviour,
                   such as the behaviour of the average female annotator, does
                   not necessarily explain individual behaviour. To account for
                   sociodemographics in models of individual annotator
                   behaviour, we introduce group-specific layers to
                   multi-annotator models. In a series of experiments for toxic
                   content detection, we find that explicitly accounting for
                   sociodemographic attributes in this way does not
                   significantly improve model performance. This result shows
                   that individual annotation behaviour depends on much more
                   than just sociodemographics.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@BOOK{Murphy2017-wv,
  title     = "The economization of life",
  author    = "Murphy, Michelle",
  publisher = "Duke University Press",
  address   = "Durham, NC",
  month     =  may,
  year      =  2017
}

@ARTICLE{Li_undated-xf,
  title  = "The emergence of indexicality in an artificial language",
  author = "Li, Aini"
}

@INPROCEEDINGS{Kwon2018-ew,
  title       = "The emergence of k-beauty: rituals and myths of Korean skin
                 care practice",
  author      = "Kwon, Yoo Jin",
  booktitle   = "International Textile and Apparel Association Annual Conference
                 Proceedings",
  publisher   = "iastatedigitalpress.com",
  institution = "Iowa State University Digital Press",
  volume      =  75,
  abstract    = "… K - beauty focusing on Korean skin care. Webpages were
                 retrieved by entering search words including K beauty … The
                 search results include three types of web pages: articles on K
                 - beauty …",
  year        =  2018
}

@ARTICLE{Barrett2014-po,
  title     = "The emergence of the unmarked",
  author    = "Barrett, Rusty and Zimman, L and Davis, J and Raclaw, J",
  journal   = "Queer excursions: Retheorizing binaries in language, gender, and
               sexuality",
  publisher = "Oxford University Press",
  pages     = "195--223",
  abstract  = "… The language ideology of formal linguistics is no less
               performative than the language ideology of queer theory I …
               elements of formalist language ideology : the autonomy of
               language , the …",
  year      =  2014
}

@ARTICLE{Becker_undated-zo,
  title  = "The Epistemology of Qualitative Research",
  author = "Becker, Howard S"
}

@ARTICLE{Gabriel2024-dy,
  title         = "The Ethics of Advanced {AI} Assistants",
  author        = "Gabriel, Iason and Manzini, Arianna and Keeling, Geoff and
                   Hendricks, Lisa Anne and Rieser, Verena and Iqbal, Hasan and
                   Tomašev, Nenad and Ktena, Ira and Kenton, Zachary and
                   Rodriguez, Mikel and El-Sayed, Seliem and Brown, Sasha and
                   Akbulut, Canfer and Trask, Andrew and Hughes, Edward and
                   Stevie Bergman, A and Shelby, Renee and Marchal, Nahema and
                   Griffin, Conor and Mateos-Garcia, Juan and Weidinger, Laura
                   and Street, Winnie and Lange, Benjamin and Ingerman, Alex and
                   Lentz, Alison and Enger, Reed and Barakat, Andrew and
                   Krakovna, Victoria and Siy, John Oliver and Kurth-Nelson, Zeb
                   and McCroskery, Amanda and Bolina, Vijay and Law, Harry and
                   Shanahan, Murray and Alberts, Lize and Balle, Borja and de
                   Haas, Sarah and Ibitoye, Yetunde and Dafoe, Allan and
                   Goldberg, Beth and Krier, Sébastien and Reese, Alexander and
                   Witherspoon, Sims and Hawkins, Will and Rauh, Maribeth and
                   Wallace, Don and Franklin, Matija and Goldstein, Josh A and
                   Lehman, Joel and Klenk, Michael and Vallor, Shannon and
                   Biles, Courtney and Morris, Meredith Ringel and King, Helen
                   and Agüera y Arcas, Blaise and Isaac, William and Manyika,
                   James",
  journal       = "arXiv [cs.CY]",
  abstract      = "This paper focuses on the opportunities and the ethical and
                   societal risks posed by advanced AI assistants. We define
                   advanced AI assistants as artificial agents with natural
                   language interfaces, whose function is to plan and execute
                   sequences of actions on behalf of a user, across one or more
                   domains, in line with the user's expectations. The paper
                   starts by considering the technology itself, providing an
                   overview of AI assistants, their technical foundations and
                   potential range of applications. It then explores questions
                   around AI value alignment, well-being, safety and malicious
                   uses. Extending the circle of inquiry further, we next
                   consider the relationship between advanced AI assistants and
                   individual users in more detail, exploring topics such as
                   manipulation and persuasion, anthropomorphism, appropriate
                   relationships, trust and privacy. With this analysis in
                   place, we consider the deployment of advanced assistants at a
                   societal scale, focusing on cooperation, equity and access,
                   misinformation, economic impact, the environment and how best
                   to evaluate advanced AI assistants. Finally, we conclude by
                   providing a range of recommendations for researchers,
                   developers, policymakers and public stakeholders.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Muhammad2018-rx,
  title     = "“The Fairest of Them All”: analysis of skin whitening
               advertisements in Malaysian English women's magazines",
  author    = "Muhammad, M and Nazli, I A",
  journal   = "Forum Komunikasi (FK)",
  publisher = "ir.uitm.edu.my",
  abstract  = "Women have desired to attain fair skin for countless of decades,
               a beauty standard culturalised by the mass media. There has been
               a deep traditional belief that fair skin is …",
  year      =  2018
}

@ARTICLE{Gudibande2023-ru,
  title         = "The False Promise of Imitating Proprietary {LLMs}",
  author        = "Gudibande, Arnav and Wallace, Eric and Snell, Charlie and
                   Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine,
                   Sergey and Song, Dawn",
  journal       = "arXiv [cs.CL]",
  abstract      = "An emerging method to cheaply improve a weaker language model
                   is to finetune it on outputs from a stronger model, such as a
                   proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct,
                   and others). This approach looks to cheaply imitate the
                   proprietary model's capabilities using a weaker open-source
                   model. In this work, we critically analyze this approach. We
                   first finetune a series of LMs that imitate ChatGPT using
                   varying base model sizes (1.5B--13B), data sources, and
                   imitation data amounts (0.3M--150M tokens). We then evaluate
                   the models using crowd raters and canonical NLP benchmarks.
                   Initially, we were surprised by the output quality of our
                   imitation models -- they appear far better at following
                   instructions, and crowd workers rate their outputs as
                   competitive with ChatGPT. However, when conducting more
                   targeted automatic evaluations, we find that imitation models
                   close little to none of the gap from the base LM to ChatGPT
                   on tasks that are not heavily supported in the imitation
                   data. We show that these performance discrepancies may slip
                   past human raters because imitation models are adept at
                   mimicking ChatGPT's style but not its factuality. Overall, we
                   conclude that model imitation is a false promise: there
                   exists a substantial capabilities gap between open and closed
                   LMs that, with current methods, can only be bridged using an
                   unwieldy amount of imitation data or by using more capable
                   base LMs. In turn, we argue that the highest leverage action
                   for improving open-source models is to tackle the difficult
                   challenge of developing better base LMs, rather than taking
                   the shortcut of imitating proprietary systems.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Etienne2021-vh,
  title    = "The future of online trust (and why Deepfake is advancing it)",
  author   = "Etienne, Hubert",
  journal  = "AI Ethics",
  volume   =  1,
  number   =  4,
  pages    = "553--562",
  abstract = "Trust has become a first-order concept in AI, urging experts to
              call for measures ensuring AI is 'trustworthy'. The danger of
              untrustworthy AI often culminates with Deepfake, perceived as
              unprecedented threat for democracies and online trust, through its
              potential to back sophisticated disinformation campaigns. Little
              work has, however, been dedicated to the examination of the
              concept of trust, what undermines the arguments supporting such
              initiatives. By investigating the concept of trust and its
              evolutions, this paper ultimately defends a non-intuitive
              position: Deepfake is not only incapable of contributing to such
              an end, but also offers a unique opportunity to transition towards
              a framework of social trust better suited for the challenges
              entailed by the digital age. Discussing the dilemmas traditional
              societies had to overcome to establish social trust and the
              evolution of their solution across modernity, I come to reject
              rational choice theories to model trust and to distinguish an
              'instrumental rationality' and a 'social rationality'. This allows
              me to refute the argument which holds Deepfake to be a threat to
              online trust. In contrast, I argue that Deepfake may even support
              a transition from instrumental to social rationality, better
              suited for making decisions in the digital age.",
  month    =  jun,
  year     =  2021,
  keywords = "AI ethics; Deepfake; Disinformation; Fake news; Trust",
  language = "en"
}

@ARTICLE{West2023-wy,
  title         = "The Generative {AI} Paradox: ``What It Can Create, It May Not
                   Understand''",
  author        = "West, Peter and Lu, Ximing and Dziri, Nouha and Brahman,
                   Faeze and Li, Linjie and Hwang, Jena D and Jiang, Liwei and
                   Fisher, Jillian and Ravichander, Abhilasha and Chandu,
                   Khyathi and Newman, Benjamin and Koh, Pang Wei and Ettinger,
                   Allyson and Choi, Yejin",
  journal       = "arXiv [cs.AI]",
  abstract      = "The recent wave of generative AI has sparked unprecedented
                   global attention, with both excitement and concern over
                   potentially superhuman levels of artificial intelligence:
                   models now take only seconds to produce outputs that would
                   challenge or exceed the capabilities even of expert humans.
                   At the same time, models still show basic errors in
                   understanding that would not be expected even in non-expert
                   humans. This presents us with an apparent paradox: how do we
                   reconcile seemingly superhuman capabilities with the
                   persistence of errors that few humans would make? In this
                   work, we posit that this tension reflects a divergence in the
                   configuration of intelligence in today's generative models
                   relative to intelligence in humans. Specifically, we propose
                   and test the Generative AI Paradox hypothesis: generative
                   models, having been trained directly to reproduce expert-like
                   outputs, acquire generative capabilities that are not
                   contingent upon -- and can therefore exceed -- their ability
                   to understand those same types of outputs. This contrasts
                   with humans, for whom basic understanding almost always
                   precedes the ability to generate expert-level outputs. We
                   test this hypothesis through controlled experiments analyzing
                   generation vs. understanding in generative models, across
                   both language and image modalities. Our results show that
                   although models can outperform humans in generation, they
                   consistently fall short of human capabilities in measures of
                   understanding, as well as weaker correlation between
                   generation and understanding performance, and more
                   brittleness to adversarial inputs. Our findings support the
                   hypothesis that models' generative capability may not be
                   contingent upon understanding capability, and call for
                   caution in interpreting artificial intelligence by analogy to
                   human intelligence.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Johnson2022-gx,
  title         = "The Ghost in the Machine has an American accent: value
                   conflict in {GPT}-3",
  author        = "Johnson, Rebecca L and Pistilli, Giada and Menédez-González,
                   Natalia and Duran, Leslye Denisse Dias and Panai, Enrico and
                   Kalpokiene, Julija and Bertulfo, Donald Jay",
  journal       = "arXiv [cs.CL]",
  abstract      = "The alignment problem in the context of large language models
                   must consider the plurality of human values in our world.
                   Whilst there are many resonant and overlapping values amongst
                   the world's cultures, there are also many conflicting, yet
                   equally valid, values. It is important to observe which
                   cultural values a model exhibits, particularly when there is
                   a value conflict between input prompts and generated outputs.
                   We discuss how the co-creation of language and cultural value
                   impacts large language models (LLMs). We explore the
                   constitution of the training data for GPT-3 and compare that
                   to the world's language and internet access demographics, as
                   well as to reported statistical profiles of dominant values
                   in some Nation-states. We stress tested GPT-3 with a range of
                   value-rich texts representing several languages and nations;
                   including some with values orthogonal to dominant US public
                   opinion as reported by the World Values Survey. We observed
                   when values embedded in the input text were mutated in the
                   generated outputs and noted when these conflicting values
                   were more aligned with reported dominant US values. Our
                   discussion of these results uses a moral value pluralism
                   (MVP) lens to better understand these value mutations.
                   Finally, we provide recommendations for how our work may
                   contribute to other current work in the field.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Pritlove2019-uc,
  title     = "The good, the bad, and the ugly of implicit bias",
  author    = "Pritlove, Cheryl and Juando-Prats, Clara and Ala-Leppilampi, Kari
               and Parsons, Janet A",
  journal   = "Lancet",
  publisher = "thelancet.com",
  volume    =  393,
  number    =  10171,
  pages     = "502--504",
  abstract  = "… Lastly, by focusing on the subconscious and unintentional
               nature of gender bias , implicit bias can overshadow explicit and
               intentional forms of bias that persist in academic institutions.
               …",
  month     =  feb,
  year      =  2019,
  language  = "en"
}

@BOOK{Stivers2012-tw,
  title     = "The handbook of conversation analysis: Sidnell/the handbook of
               conversation analysis",
  editor    = "Stivers, Tanya and Sidnell, Jack",
  publisher = "Wiley-Blackwell",
  address   = "Chichester, England",
  series    = "Blackwell Handbooks in Linguistics",
  month     =  nov,
  year      =  2012,
  language  = "en"
}

@INPROCEEDINGS{Agnew2024-kf,
  title     = "The illusion of artificial inclusion",
  author    = "Agnew, William and Bergman, A Stevie and Chien, Jennifer and
               Díaz, Mark and El-Sayed, Seliem and Pittman, Jaylen and Mohamed,
               Shakir and McKee, Kevin R",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  35,
  pages     = "1--12",
  month     =  may,
  year      =  2024
}

@INPROCEEDINGS{Cuadra2024-ww,
  title     = "The illusion of empathy? Notes on displays of emotion in
               human-computer interaction",
  author    = "Cuadra, Andrea and Wang, Maria and Stein, Lynn Andrea and Jung,
               Malte F and Dell, Nicola and Estrin, Deborah and Landay, James A",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  32,
  pages     = "1--18",
  month     =  may,
  year      =  2024
}

@INPROCEEDINGS{Cuadra2024-yw,
  title     = "The illusion of empathy? Notes on displays of emotion in
               human-computer interaction",
  author    = "Cuadra, Andrea and Wang, Maria and Stein, Lynn Andrea and Jung,
               Malte F and Dell, Nicola and Estrin, Deborah and Landay, James A",
  booktitle = "Proceedings of the CHI Conference on Human Factors in Computing
               Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  volume    =  32,
  pages     = "1--18",
  month     =  may,
  year      =  2024
}

@ARTICLE{Klein2013-po,
  title     = "The image of absence: archival silence, data visualization, and
               James Hemings",
  author    = "Klein, L F",
  journal   = "Am. Lit.",
  publisher = "read.dukeupress.edu",
  abstract  = "In recent years, scholars from across the humanities, including
               literary critics Stephen Best and Saidiya Hartman, have theorized
               about how best to account for the silences endemic to the archive
               of American slavery. These critics call for a shift away from
               what Best has described as a “logic and ethic of recovery” to a
               new focus, instead, on animating the mysteries of the past. With
               the example of James Hemings, Thomas Jefferson's enslaved chef
               (and Sally Hemings's older brother), this essay shows how a set
               of techniques …",
  year      =  2013
}

@ARTICLE{Daston1992-jf,
  title     = "The image of objectivity",
  author    = "Daston, Lorraine and Galison, Peter",
  journal   = "Representations",
  publisher = "University of California Press",
  volume    =  40,
  number    =  1,
  pages     = "81--128",
  abstract  = "… In what follows we address the history of only one component of
               objectivity , but we believe … pattern, namely the negative
               character of all forms of objectivity . Objectivity is related to
               …",
  month     =  oct,
  year      =  1992
}

@ARTICLE{Seekis2023-vc,
  title     = "The impact of \#beauty and \#self-compassion tiktok videos on
               young women's appearance shame and anxiety, self-compassion,
               mood, and comparison processes",
  author    = "Seekis, Veya and Kennedy, Richelle",
  journal   = "Body Image",
  publisher = "Elsevier",
  volume    =  45,
  pages     = "117--125",
  abstract  = "This study examined the impact of exposure to beauty,
               self-compassion, and travel (control) TikTok videos on young
               women's face-related appearance shame and anxiety,
               self-compassion, mood, upward appearance comparisons and
               thoughts. Undergraduate women (N = 115) were randomly assigned to
               view one of three compilation TikTok videos on either beauty
               tips, self-compassion strategies, or travel destinations. Upward
               appearance comparisons and thoughts were assessed at post-test
               only given the items related to video exposure; all other
               measures were assessed at pre- and post-test. Controlling for
               pre-test measures, results showed that face-related appearance
               shame and anxiety, and negative mood were higher, whereas
               self-compassion was lower in the beauty group relative to the
               travel control and self-compassion groups. Self-compassion was
               higher in the self-compassion group relative to the travel
               control. Women in the beauty group reported more upward
               appearance comparisons and appearance thoughts relative to women
               in the travel control and self-compassion groups. The
               self-compassion group reported more appearance thoughts relative
               to the travel control. Findings contribute to prior research by
               showing that brief exposure to beauty TikToks may have a negative
               effect on how young women feel about their appearance, but also
               how self-compassion videos may help young women feel more
               compassionate toward themselves.",
  month     =  jun,
  year      =  2023,
  keywords  = "Appearance Anxiety; Appearance shame; Beauty ideals;
               Self-compassion; TikTok; Upward appearance comparisons",
  language  = "en"
}

@ARTICLE{Goyal2023-uh,
  title         = "The Impact of Explanations on Fairness in Human-{AI}
                   Decision-Making: Protected vs Proxy Features",
  author        = "Goyal, Navita and Baumler, Connor and Nguyen, Tin and Daumé,
                   III, Hal",
  journal       = "arXiv [cs.AI]",
  abstract      = "AI systems have been known to amplify biases in real-world
                   data. Explanations may help human-AI teams address these
                   biases for fairer decision-making. Typically, explanations
                   focus on salient input features. If a model is biased against
                   some protected group, explanations may include features that
                   demonstrate this bias, but when biases are realized through
                   proxy features, the relationship between this proxy feature
                   and the protected one may be less clear to a human. In this
                   work, we study the effect of the presence of protected and
                   proxy features on participants' perception of model fairness
                   and their ability to improve demographic parity over an AI
                   alone. Further, we examine how different treatments --
                   explanations, model bias disclosure and proxy correlation
                   disclosure -- affect fairness perception and parity. We find
                   that explanations help people detect direct but not indirect
                   biases. Additionally, regardless of bias type, explanations
                   tend to increase agreement with model biases. Disclosures can
                   help mitigate this effect for indirect biases, improving both
                   unfairness recognition and decision-making fairness. We hope
                   that our findings can help guide further research into
                   advancing explanations in support of fair human-AI
                   decision-making.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@MISC{Lund_undated-zd,
  title        = "The impact of {TikTok}'s engagement algorithm on political
                  polarization",
  author       = "Lund, Campbell and Zhong, Shirui",
  howpublished = "\url{https://www.shiruizhong.com/TheIndependentProject.pdf}",
  note         = "Accessed: 2023-8-15"
}

@ARTICLE{Kohankhaki2024-mp,
  title         = "The Impact of Unstated Norms in Bias Analysis of Language
                   Models",
  author        = "Kohankhaki, Farnaz and Tian, Jacob-Junqi and Emerson, David
                   and Seyyed-Kalantari, Laleh and Khattak, Faiza Khan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large language models (LLMs), trained on vast datasets, can
                   carry biases that manifest in various forms, from overt
                   discrimination to implicit stereotypes. One facet of bias is
                   performance disparities in LLMs, often harming
                   underprivileged groups, such as racial minorities. A common
                   approach to quantifying bias is to use template-based bias
                   probes, which explicitly state group membership (e.g. White)
                   and evaluate if the outcome of a task, sentiment analysis for
                   instance, is invariant to the change of group membership
                   (e.g. change White race to Black). This approach is widely
                   used in bias quantification. However, in this work, we find
                   evidence of an unexpectedly overlooked consequence of using
                   template-based probes for LLM bias quantification. We find
                   that in doing so, text examples associated with White
                   ethnicities appear to be classified as exhibiting negative
                   sentiment at elevated rates. We hypothesize that the scenario
                   arises artificially through a mismatch between the
                   pre-training text of LLMs and the templates used to measure
                   bias through reporting bias, unstated norms that imply group
                   membership without explicit statement. Our finding highlights
                   the potential misleading impact of varying group membership
                   through explicit mention in bias quantification",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Cabrera2021-ig,
  title    = "The influence of community structure on opinion expression: an
              agent-based model",
  author   = "Cabrera, Benjamin and Ross, Björn and Röchert, Daniel and Brünker,
              Felix and Stieglitz, Stefan",
  journal  = "J. Bus. Econ. Manage.",
  volume   =  91,
  number   =  9,
  pages    = "1331--1355",
  abstract = "Social media has become important in shaping the public discourse
              on controversial topics. Many businesses therefore monitor
              different social media channels and try to react adequately to a
              potentially harmful opinion climate. Still, little is known about
              how opinions form in an increasingly connected world. The spiral
              of silence theory provides a way of explaining deviations between
              the perceived opinion climate and true beliefs of the public.
              However, the emergence of a spiral of silence on social media is
              hard to observe because only the thoughts of those who express
              their opinions are evident there. Recent research has therefore
              focused on modelling the processes behind the spiral of silence. A
              particular characteristic of social media networks is the presence
              of communities. Members of a community tend to be connected more
              with other members of the same community than with outsiders.
              Naturally, this might affect the development of public opinion. In
              the present article we investigate how the number of communities
              in a network and connectivity between them affects the perceived
              opinion climate. We find that higher connectivity between
              communities makes it more likely for a global spiral of silence to
              appear. Moreover, a network fragmented into more, smaller
              communities seems to provide more “safe spaces” for a minority
              opinion to prevail.",
  month    =  nov,
  year     =  2021
}

@ARTICLE{IIT-Concerned2023-jh,
  title    = "The Integrated Information Theory of Consciousness as
              Pseudoscience",
  author   = "{IIT-Concerned} and Fleming, Stephen M and Frith, Chris and
              Goodale, Mel and Lau, Hakwan and LeDoux, Joseph E and Lee, Alan L
              F and Michel, Matthias and Owen, Adrian and Peters, Megan A K and
              al., Et",
  abstract = "The media, including news articles in both Nature and Science,
              have recently celebrated the Integrated Information Theory (IIT)
              as a ‘leading’ and empirically tested theory of consciousness. We
              are writing as researchers with some relevant expertise to express
              our concerns.",
  month    =  sep,
  year     =  2023
}

@BOOK{Ringrow2016-fv,
  title     = "The Language of Cosmetics Advertising",
  author    = "Ringrow, Helen",
  publisher = "Springer",
  abstract  = "This book offers a cross-cultural comparison of French and
               British cosmetics advertisements and explores how the discourse
               of beauty advertising represents ideas about femininity in French
               and English language contexts. As the global beauty industry
               expands and consumers become more critical of the claims made,
               the topic of cosmetics advertising discourse is examined using
               Feminist Critical Discourse Analysis. One common theme underlying
               most cosmetics advertising discourse is that the female body
               always requires ‘work’ to fix its ‘problems’: flat skin, dry
               hair, and so on. The author uses themes of language and gender,
               media and identity, and advertising across cultures to expose
               exactly what is going on in the language of cosmetics advertising
               and to offer a first step towards challenging these ideas and
               thinking about alternatives.",
  month     =  sep,
  year      =  2016,
  language  = "en"
}

@ARTICLE{Tessler2019-an,
  title     = "The language of generalization",
  author    = "Tessler, Michael Henry and Goodman, Noah D",
  journal   = "Psychol. Rev.",
  publisher = "psycnet.apa.org",
  volume    =  126,
  number    =  3,
  pages     = "395--436",
  abstract  = "Language provides simple ways of communicating generalizable
               knowledge to each other (e.g., ``Birds fly,'' ``John hikes,'' and
               ``Fire makes smoke''). Though found in every language and
               emerging early in development, the language of generalization is
               philosophically puzzling and has resisted precise formalization.
               Here, we propose the first formal account of generalizations
               conveyed with language that makes quantitative predictions about
               human understanding. The basic idea is that the language of
               generalization expresses that an event or a property occurs
               relatively often, where what counts as relatively often depends
               upon one's prior expectations. We formalize this simple idea in a
               probabilistic model of language understanding, which we test in 3
               diverse case studies: generalizations about categories (generic
               language), events (habitual language), and causes (causal
               language). We find that the model explains the gradience in human
               endorsements that has perplexed previous attempts to formalize
               this swath of linguistic expressions. This work opens the door to
               understanding precisely how abstract knowledge is learned from
               language. (PsycINFO Database Record (c) 2019 APA, all rights
               reserved).",
  month     =  apr,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Inoue2003-wi,
  title     = "The Listening Subject of Japanese Modernity and His Auditory
               Double: Citing, Sighting, and Siting the Modern Japanese Woman",
  author    = "Inoue, Miyako",
  journal   = "Cult. Anthropol.",
  publisher = "[Wiley, American Anthropological Association]",
  volume    =  18,
  number    =  2,
  pages     = "156--193",
  year      =  2003
}

@ARTICLE{Dignard2021-nl,
  title    = "The ``Little Red Riding Hood effect:'' Fitspiration is just as bad
              as thinspiration for women's body satisfaction",
  author   = "Dignard, Nicole A L and Jarry, Josée L",
  journal  = "Body Image",
  volume   =  36,
  pages    = "201--213",
  abstract = "Fitspiration is a popular form of social media which outwardly
              promotes the pursuit of health and strength rather than of
              thinness. Content analyses, however, suggest that it is
              thematically similar to thinspiration, which is primarily
              associated with thinness and presents disordered eating as a
              lifestyle choice. Exposure to both reduces body satisfaction, but
              their effect has not been compared. Further, positive body image
              may act as a protective factor by reducing engagement in
              appearance comparisons. Female undergraduate students (N = 331)
              were randomly assigned to view fitspiration, thinspiration, or
              travel images, followed by measures of state body satisfaction,
              state appearance comparison, and positive body image. Viewing
              fitspiration and thinspiration resulted in lower body satisfaction
              than did viewing travel images. However, whereas women with higher
              positive body image appeared protected from thinspiration by
              engaging in lower appearance comparisons than did women with lower
              positive body image, this same effect was not observed in women
              who viewed fitspiration. The fact that positive body image is not
              protective against fitspiration suggests that fitspiration is just
              as harmful, if not more, than is thinspiration and highlights the
              need for media literacy programs targeting fitspiration and
              so-called ``healthy living'' websites.",
  month    =  mar,
  year     =  2021,
  keywords = "Appearance comparison; Body satisfaction; Fitspiration; Positive
              body image; Social media; Thinspiration",
  language = "en"
}

@ARTICLE{Wilson2023-me,
  title     = "The Living Machine: A Computational Approach to the
               Nineteenth-Century Language of Technology",
  author    = "Wilson, Daniel C S and Ardanuy, Mariona Coll and Beelen, Kaspar
               and McGillivray, Barbara and Ahnert, Ruth",
  journal   = "tech",
  publisher = "Johns Hopkins University Press",
  volume    =  64,
  number    =  3,
  pages     = "875--902",
  abstract  = "ARRAY(0x555f5376bd48)",
  year      =  2023
}

@MISC{noauthor_undated-mv,
  title        = "The media during the rise of trump: Identity politics,
                  immigration,`` Mexican'' demonization and hate-crime",
  howpublished = "\url{https://scholar.google.ca/scholar?cluster=5148881930826735210\&hl=en\&as\_sdt=0,5\&sciodt=0,5}",
  note         = "Accessed: 2023-11-9"
}

@ARTICLE{Reeves_undated-va,
  title    = "The media equation: How people treat computers, television, and
              new media like real people",
  author   = "Reeves, B",
  journal  = "academia.edu",
  abstract = "The three-term mayor of New York City, Ed Koch, used to ask a
              simple question:“How am I doing?`` His question closed speeches,
              greeted crowds, and made a great sound bite …"
}

@BOOK{Taylor2012-fn,
  title     = "The Mental Corpus: How Language is Represented in the Mind",
  author    = "Taylor, John R",
  publisher = "OUP Oxford",
  abstract  = "This book presents a radical reconceptualization of the nature of
               linguistic knowledge. John Taylor challenges the conventional
               notion that a language can be understood in terms of the
               interaction of syntax with a lexicon, the second listing the
               words and the first the rules for combining them. He proposes
               instead that an individual's knowledge of a language can be
               thought of as a repository of memories of linguistic experience.
               Each encounter with the language, he argues, leaves a trace in
               our minds. We record the forms of utterances, the concepts and
               interpretations associated with them, and the contexts in which
               they were heard or seen. Features of incoming language - a word,
               a phrase, a meaning, a voice quality, an interactional situation
               - resonate with items already stored. Similarities between stored
               items give rise to generalizations of varying degrees of
               certainty and precision, which in turn are able to sanction new
               and innovative expressions. John Taylor writes with conviction,
               clarity, and wit, illustrating every stage of his argument with
               arresting examples. His account makes a profound and original
               contribution to understanding the nature of language and the
               operations of the mind and brain. His book will appeal in equal
               measure to linguists, philosophers, and cognitive scientists.",
  month     =  may,
  year      =  2012,
  language  = "en"
}

@ARTICLE{Ziems2022-ut,
  title         = "The Moral Integrity Corpus: A Benchmark for Ethical Dialogue
                   Systems",
  author        = "Ziems, Caleb and Yu, Jane A and Wang, Yi-Chia and Halevy,
                   Alon and Yang, Diyi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Conversational agents have come increasingly closer to human
                   competence in open-domain dialogue settings; however, such
                   models can reflect insensitive, hurtful, or entirely
                   incoherent viewpoints that erode a user's trust in the moral
                   integrity of the system. Moral deviations are difficult to
                   mitigate because moral judgments are not universal, and there
                   may be multiple competing judgments that apply to a situation
                   simultaneously. In this work, we introduce a new resource,
                   not to authoritatively resolve moral ambiguities, but instead
                   to facilitate systematic understanding of the intuitions,
                   values and moral judgments reflected in the utterances of
                   dialogue systems. The Moral Integrity Corpus, MIC, is such a
                   resource, which captures the moral assumptions of 38k
                   prompt-reply pairs, using 99k distinct Rules of Thumb (RoTs).
                   Each RoT reflects a particular moral conviction that can
                   explain why a chatbot's reply may appear acceptable or
                   problematic. We further organize RoTs with a set of 9 moral
                   and social attributes and benchmark performance for attribute
                   classification. Most importantly, we show that current neural
                   language models can automatically generate new RoTs that
                   reasonably describe previously unseen interactions, but they
                   still struggle with certain scenarios. Our findings suggest
                   that MIC will be a useful resource for understanding and
                   language models' implicit moral assumptions and flexibly
                   benchmarking the integrity of conversational agents. To
                   download the data, see https://github.com/GT-SALT/mic",
  month         =  apr,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Awad2018-ed,
  title     = "The Moral Machine experiment",
  author    = "Awad, Edmond and Dsouza, Sohan and Kim, Richard and Schulz,
               Jonathan and Henrich, Joseph and Shariff, Azim and Bonnefon,
               Jean-François and Rahwan, Iyad",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  563,
  number    =  7729,
  pages     = "59--64",
  abstract  = "With the rapid development of artificial intelligence have come
               concerns about how machines will make moral decisions, and the
               major challenge of quantifying societal expectations about the
               ethical principles that should guide machine behaviour. To
               address this challenge, we deployed the Moral Machine, an online
               experimental platform designed to explore the moral dilemmas
               faced by autonomous vehicles. This platform gathered 40 million
               decisions in ten languages from millions of people in 233
               countries and territories. Here we describe the results of this
               experiment. First, we summarize global moral preferences. Second,
               we document individual variations in preferences, based on
               respondents' demographics. Third, we report cross-cultural
               ethical variation, and uncover three major clusters of countries.
               Fourth, we show that these differences correlate with modern
               institutions and deep cultural traits. We discuss how these
               preferences can contribute to developing global, socially
               acceptable principles for machine ethics. All data used in this
               article are publicly available.",
  month     =  nov,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Beukeboom2010-vi,
  title    = "The negation bias: when negations signal stereotypic expectancies",
  author   = "Beukeboom, Camiel J and Finkenauer, Catrin and Wigboldus, Daniël H
              J",
  journal  = "J. Pers. Soc. Psychol.",
  volume   =  99,
  number   =  6,
  pages    = "978--992",
  abstract = "Research on linguistic biases shows that stereotypic expectancies
              are implicitly reflected in language and are thereby subtly
              communicated to message recipients. We examined whether these
              findings extend to the use of negations (e.g., not smart instead
              of stupid). We hypothesized that people use more negations in
              descriptions of stereotype-inconsistent behavior than in
              descriptions of stereotype-consistent behavior. In 3 studies,
              participants either judged the applicability of experimentally
              controlled person descriptions or spontaneously produced person
              descriptions themselves. Results provided support for this
              hypothesis. Moreover, a 4th study demonstrated that negations have
              communicative consequences. When a target person's behavior was
              described with negations, message recipients inferred that this
              behavior was an exception to the rule and that it was more likely
              caused by situational circumstances than by dispositional factors.
              These findings indicate that by using negations people implicitly
              communicate stereotypic expectancies and that negations play a
              subtle but powerful role in stereotype maintenance.",
  month    =  dec,
  year     =  2010,
  language = "en"
}

@MISC{noauthor_undated-ti,
  title        = "The objectification and dismemberment of women in the media",
  howpublished = "\url{https://publications.kon.org/urc/v5/greening.html}",
  note         = "Accessed: 2024-3-31"
}

@BOOK{Alim2020-kq,
  title     = "The oxford handbook of language and race the oxford handbook of
               language and race",
  editor    = "Alim, H Samy and Reyes, Angela and Kroskrity, Paul V",
  publisher = "Oxford University Press",
  address   = "New York, NY",
  abstract  = "Abstract This handbook is the first volume to offer a sustained
               theoretical exploration of all aspects of language and race from
               a linguistic anthropological perspective. A growing number of
               scholars hold that rather than fixed and pre-determined, race is
               created out of continuous and repeated discourses emerging from
               individuals and institutions within specific histories, political
               economic systems, and everyday interactions. This handbook
               demonstrates how linguistic analysis brings a crucial perspective
               to this project by revealing the ways in which language and race
               are mutually constituted as social realities. Not only do we
               position issues of race, racism, and racialization as central to
               language-based scholarship, but we also examine these processes
               from an explicitly critical and anti-racist perspective. The
               process of racialization—an enduring yet evolving social process
               steeped in centuries of colonialism and capitalism—is central to
               linguistic anthropological approaches. This volume captures
               state-of-the-art research in this important and necessary yet
               often overlooked area of inquiry and points the way forward in
               establishing future directions of research in this rapidly
               expanding field, including the need for more studies of language
               and race in non-U.S. contexts. Covering a range of sites from
               Angola, Brazil, Canada, Cuba, Italy, Liberia, the Philippines,
               South Africa, the United Kingdom, the United States, and unceded
               Indigenous territories, the handbook offers theoretical,
               reflexive takes on the field of language and race, the larger
               histories and systems that influence these concepts, the bodies
               that enact and experience them, and finally, the expressions and
               outcomes that emerge as a result.",
  series    = "Oxford Handbooks",
  month     =  nov,
  year      =  2020
}

@ARTICLE{Wynne2004-fq,
  title     = "The perils of anthropomorphism",
  author    = "Wynne, Clive D L",
  journal   = "Nature",
  publisher = "nature.com",
  volume    =  428,
  number    =  6983,
  pages     =  606,
  abstract  = "… Griffin has inspired several researchers to develop ways of
               making anthropomorphism into a constructive tool for
               understanding animal behaviour. Gordon Burghardt was keen to
               distinguish …",
  month     =  apr,
  year      =  2004,
  language  = "en"
}

@ARTICLE{Rae2014-mm,
  title    = "The Philosophical Roots of Donna Haraway’s Cyborg Imagery:
              Descartes and Heidegger Through Latour, Derrida, and Agamben",
  author   = "Rae, Gavin",
  journal  = "Hum. Stud.",
  volume   =  37,
  number   =  4,
  pages    = "505--528",
  abstract = "The purpose of this paper is to highlight some of the main
              philosophical roots of Donna Haraway’s thinking, an issue she
              rarely discusses and which is frequently ignored in the
              literature, but which will allow us to not only better understand
              her thinking, but also locate it within the philosophical
              tradition. In particular, it suggests that Haraway’s thinking
              emanates from a Cartesian and Heideggerian heritage whereby it,
              implicitly, emanates from Heidegger’s destruction of metaphysical
              anthropocentrism to critique the divisions between human, animal,
              and machine that Descartes insists upon in his Discourse on
              Method. While suggesting that Haraway is, implicitly, influenced
              by Heidegger’s critique of the binary logic constitutive of
              Descartes’ anthropocentrism, I first argue that her support for
              Jacques Derrida’s, Bruno Latour’s, and Giorgio Agamben’s critical
              readings of Heidegger lead her to jettison Heidegger’s suggestion
              that overcoming this logic requires a re-questioning of the
              meaning of being to, instead, develop an immersed, entwined
              ontology that aims to call into question the fundamental divisions
              underpinning Cartesian-inspired anthropocentrism, before, second,
              concluding by offering a Heideggerian critique of Haraway’s
              thinking.",
  month    =  dec,
  year     =  2014
}

@INPROCEEDINGS{Mathew2020-ti,
  title     = "The {POLAR} Framework: Polar Opposites Enable Interpretability of
               Pre-Trained Word Embeddings",
  author    = "Mathew, Binny and Sikdar, Sandipan and Lemmerich, Florian and
               Strohmaier, Markus",
  booktitle = "Proceedings of The Web Conference 2020",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1548--1558",
  abstract  = "We introduce ‘POLAR’ — a framework that adds interpretability to
               pre-trained word embeddings via the adoption of semantic
               differentials. Semantic differentials are a psychometric
               construct for measuring the semantics of a word by analysing its
               position on a scale between two polar opposites (e.g., cold –
               hot, soft – hard). The core idea of our approach is to transform
               existing, pre-trained word embeddings via semantic differentials
               to a new “polar” space with interpretable dimensions defined by
               such polar opposites. Our framework also allows for selecting the
               most discriminative dimensions from a set of polar dimensions
               provided by an oracle, i.e., an external source. We demonstrate
               the effectiveness of our framework by deploying it to various
               downstream tasks, in which our interpretable word embeddings
               achieve a performance that is comparable to the original word
               embeddings. We also show that the interpretable dimensions
               selected by our framework align with human judgement. Together,
               these results demonstrate that interpretability can be added to
               word embeddings without compromising performance. Our work is
               relevant for researchers and engineers interested in interpreting
               pre-trained word embeddings.",
  series    = "WWW '20",
  month     =  apr,
  year      =  2020,
  keywords  = "neural networks, interpretable, semantic differential, word
               embeddings"
}

@ARTICLE{Moeran2010-cc,
  title     = "The Portrayal of Beauty in Women's Fashion Magazines",
  author    = "Moeran, Brian",
  journal   = "Fashion Theory",
  publisher = "Routledge",
  volume    =  14,
  number    =  4,
  pages     = "491--510",
  abstract  = "AbstractThis article explores the ways in which international
               fashion magazines such as Elle, Vogue, and Marie Claire portray
               feminine beauty in textual and advertising matter and how their
               readers react to such portrayals. It is based on content analysis
               of more than 700 issues of these titles published in France, Hong
               Kong, Japan, the UK, and USA, and collected over a fifteen-year
               period, as well as on extensive ethnographic research among
               fashion magazine editorial staff and women readers of the
               magazines in question. The analysis focuses on the different
               kinds of ?face? that magazines invite their women readers to put
               on. While magazine contents confirm the validity of previous
               feminist critiques, the article argues that magazine editors?and
               their advertisers?adopt a ?technology of enchantment? as a means
               of exercising control over their readers. Magazine and
               advertising language is imbued with ?magical? power, and the
               structure of beauty advertisements closely parallels that of
               magical spells used in healing rituals. The efficacy of such
               spells is borne out by reader interviews.",
  month     =  dec,
  year      =  2010
}

@ARTICLE{Ramle2019-fx,
  title     = "The power of Instagram brand communities: An overview about
               cosmetic brands on Instagram",
  author    = "Ramle, Oumaima and Kaplan, Burçin",
  journal   = "FCPE",
  publisher = "Istanbul Aydin University",
  volume    =  5,
  number    =  1,
  pages     = "1--14",
  abstract  = "Companies on Instagram have been focusing on brand communities in
               the last years knowing that it’s a smart way to emerge the
               technology and the brand with creating a value for both,
               customers and the companies. Since Instagram brand communities
               cheers on the member to have commitment and become faithful to
               the brand. This result is launching on creating a strong online
               brand community. For companies, establishing brand communities
               online strengthen the relation between the customer and the
               company, these communities are used to reach information searched
               for by members about a cosmetic brand, and also exchange
               experiences and opinions. Influencers play an important role in
               the purchase intention of cosmetic brands. Since companies
               encourage infuencers by sharing a post of them using the product
               on their feed, this increases the purchase intentions traffic of
               cosmetic brands. This research focuses on thе rolе of thе cеntrаl
               mеmbеrs of thе brаnd communitiеs in thе еmеrgеncе аnd structuring
               of thеsе communitiеs аnd thе links thеy hаvе with thеir
               еnvironmеnt. This research aims to understand thе fаctors
               including еngаgеmеnt, loyаlty, and аttаchmеnt for the consumеrs
               by reviewing the related literature and some example cases.",
  month     =  apr,
  year      =  2019,
  keywords  = "Brаnd Community,Purchаsе Intеntions,Cosmetic Brands"
}

@ARTICLE{Rissman2023-im,
  title     = "The Power of the Lexicon: Eliciting Superordinate Categories With
               and Without Labels",
  author    = "Rissman, Lilia and Lupyan, Gary",
  publisher = "PsyArXiv",
  abstract  = "Words are sometimes thought to label concepts, eg, people have
               the concept ANIMAL which is labeled by the word animal. An
               alternative view is that words help people create conceptual
               categories. We address this debate by asking English speakers to
               list members of superordinate categories under one of three
               conditions:(1) when cued by a label (eg, animals),(2) an exemplar
               list (eg, dog, cat, mouse), or (3) a definition (eg,“living
               creatures that roam the Earth”). We find that categories
               activated by labels lead to participants listing more
               category-typical responses, as quantified through typicality
               ratings, similarity in word embedding space, and accuracy in
               guessing category labels. This effect is stronger for some
               categories than others (eg, stronger for appetizers than
               animals). These results support the view that words create rather
               than merely reflect categories, and that researchers should
               exercise caution when drawing parallels between conceptual
               categories and lexical semantics.",
  month     =  jul,
  year      =  2023
}

@ARTICLE{Hasinoff2020-zv,
  title     = "The promise of restorative justice in addressing online harm",
  author    = "Hasinoff, A A and Gibson, A D and Salehi, N",
  publisher = "policycommons.net",
  abstract  = "As public pressure has increased for social media platforms to
               take action against online harassment and abuse, most of the
               policy debate has centered around Section 230 of the …",
  year      =  2020
}

@ARTICLE{Plous2003-ku,
  title     = "The psychology of prejudice, sterotyping, and discrimination: An
               overview",
  author    = "Plous, S",
  publisher = "psycnet.apa.org",
  abstract  = "This review begins with unambiguously prejudiced statements made
               by Osama bin Laden. As discouraging as it is to read these
               statements, it is worth noting that they do not represent …",
  year      =  2003
}

@ARTICLE{Reinecke2023-fq,
  title    = "The Puzzle of Evaluating Moral Cognition in Artificial Agents",
  author   = "Reinecke, Madeline G and Mao, Yiran and Kunesch, Markus and
              Duéñez-Guzmán, Edgar A and Haas, Julia and Leibo, Joel Z",
  journal  = "Cogn. Sci.",
  volume   =  47,
  number   =  8,
  pages    = "e13315",
  abstract = "In developing artificial intelligence (AI), researchers often
              benchmark against human performance as a measure of progress. Is
              this kind of comparison possible for moral cognition? Given that
              human moral judgment often hinges on intangible properties like
              ``intention'' which may have no natural analog in artificial
              agents, it may prove difficult to design a ``like-for-like''
              comparison between the moral behavior of artificial and human
              agents. What would a measure of moral behavior for both humans and
              AI look like? We unravel the complexity of this question by
              discussing examples within reinforcement learning and generative
              AI, and we examine how the puzzle of evaluating artificial agents'
              moral cognition remains open for further investigation within
              cognitive science.",
  month    =  aug,
  year     =  2023,
  keywords = "Artificial intelligence; Moral cognition; Multi-agent
              reinforcement learning",
  language = "en"
}

@ARTICLE{Gros2021-jh,
  title         = "The {R}-{U}-A-Robot Dataset: Helping Avoid Chatbot Deception
                   by Detecting User Questions About Human or Non-Human Identity",
  author        = "Gros, David and Li, Yu and Yu, Zhou",
  journal       = "arXiv [cs.CL]",
  abstract      = "Humans are increasingly interacting with machines through
                   language, sometimes in contexts where the user may not know
                   they are talking to a machine (like over the phone or a text
                   chatbot). We aim to understand how system designers and
                   researchers might allow their systems to confirm its
                   non-human identity. We collect over 2,500 phrasings related
                   to the intent of ``Are you a robot?``. This is paired with
                   over 2,500 adversarially selected utterances where only
                   confirming the system is non-human would be insufficient or
                   disfluent. We compare classifiers to recognize the intent and
                   discuss the precision/recall and model complexity tradeoffs.
                   Such classifiers could be integrated into dialog systems to
                   avoid undesired deception. We then explore how both a
                   generative research model (Blender) as well as two deployed
                   systems (Amazon Alexa, Google Assistant) handle this intent,
                   finding that systems often fail to confirm their non-human
                   identity. Finally, we try to understand what a good response
                   to the intent would be, and conduct a user study to compare
                   the important aspects when responding to this intent.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{Chen2021-oh,
  title        = "The racializing of beauty: The rise of western beauty norms
                  and self-esteem among Asian women",
  author       = "Chen, Melody",
  publisher    = "openjournals.library.northeastern …",
  abstract     = "The influence of Western fashion and beauty standards are
                  affecting cultural norms and threatening the autonomy of
                  culturally based standards of beauty that are aligned to a
                  given …",
  year         =  2021,
  howpublished = "\url{https://openjournals.library.northeastern.edu/nuwriting/home/article/download/217/167}",
  note         = "Accessed: 2023-7-21"
}

@ARTICLE{Hayes2011-bg,
  title     = "The relationship of action research to human-computer interaction",
  author    = "Hayes, Gillian R",
  journal   = "ACM Trans. Comput.-Hum. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  18,
  number    =  3,
  pages     = "1--20",
  abstract  = "Alongside the growing interest within HCI, and arguably computing
               more generally, in conducting research that has substantial
               societal benefits, there is a need for new ways to think about
               and to articulate the challenges of these engaged research
               projects as well as their results. Action Research (AR) is a
               class of methods and approaches for conducting democratic and
               collaborative research with community partners. AR has evolved
               over the last several decades and offers HCI researchers
               theoretical lenses, methodological approaches, and pragmatic
               guidance for conducting socially relevant, collaborative, and
               engaged research. In this article, I describe the historical
               context and origins of AR, the scientifically rigorous practice
               of conducting and evaluating AR projects, and the ways in which
               AR might meaningfully be applied to HCI research.",
  month     =  aug,
  year      =  2011,
  keywords  = "Action research, collaborative inquiry"
}

@ARTICLE{Berglund2023-ga,
  title         = "The Reversal Curse: {LLMs} trained on ``A is {B}'' fail to
                   learn ``{B} is A''",
  author        = "Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni,
                   Mikita and Stickland, Asa Cooper and Korbak, Tomasz and
                   Evans, Owain",
  journal       = "arXiv [cs.CL]",
  abstract      = "We expose a surprising failure of generalization in
                   auto-regressive large language models (LLMs). If a model is
                   trained on a sentence of the form ``A is B'', it will not
                   automatically generalize to the reverse direction ``B is A''.
                   This is the Reversal Curse. For instance, if a model is
                   trained on ``Valentina Tereshkova was the first woman to
                   travel to space'', it will not automatically be able to
                   answer the question, ``Who was the first woman to travel to
                   space?''. Moreover, the likelihood of the correct answer
                   (``Valentina Tershkova'') will not be higher than for a
                   random name. Thus, models do not generalize a prevalent
                   pattern in their training set: if ``A is B'' occurs, ``B is
                   A'' is more likely to occur. It is worth noting, however,
                   that if ``A is B'' appears in-context, models can deduce the
                   reverse relationship. We provide evidence for the Reversal
                   Curse by finetuning GPT-3 and Llama-1 on fictitious
                   statements such as ``Uriah Hawthorne is the composer of
                   Abyssal Melodies'' and showing that they fail to correctly
                   answer ``Who composed Abyssal Melodies?''. The Reversal Curse
                   is robust across model sizes and model families and is not
                   alleviated by data augmentation. We also evaluate ChatGPT
                   (GPT-3.5 and GPT-4) on questions about real-world
                   celebrities, such as ``Who is Tom Cruise's mother? [A: Mary
                   Lee Pfeiffer]'' and the reverse ``Who is Mary Lee Pfeiffer's
                   son?''. GPT-4 correctly answers questions like the former
                   79\% of the time, compared to 33\% for the latter. Code
                   available at:
                   https://github.com/lukasberglund/reversal\_curse.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Watson2019-py,
  title     = "The Rhetoric and Reality of Anthropomorphism in Artificial
               Intelligence",
  author    = "Watson, David",
  journal   = "Minds Mach.",
  publisher = "Springer",
  volume    =  29,
  number    =  3,
  pages     = "417--440",
  abstract  = "Artificial intelligence (AI) has historically been conceptualized
               in anthropomorphic terms. Some algorithms deploy biomimetic
               designs in a deliberate attempt to effect a sort of digital
               isomorphism of the human brain. Others leverage more general
               learning strategies that happen to coincide with popular theories
               of cognitive science and social epistemology. In this paper, I
               challenge the anthropomorphic credentials of the neural network
               algorithm, whose similarities to human cognition I argue are
               vastly overstated and narrowly construed. I submit that three
               alternative supervised learning methods—namely lasso penalties,
               bagging, and boosting—offer subtler, more interesting analogies
               to human reasoning as both an individual and a social phenomenon.
               Despite the temptation to fall back on anthropomorphic tropes
               when discussing AI, however, I conclude that such rhetoric is at
               best misleading and at worst downright dangerous. The impulse to
               humanize algorithms is an obstacle to properly conceptualizing
               the ethical challenges posed by emerging technologies.",
  month     =  sep,
  year      =  2019
}

@ARTICLE{Del_Tredici2018-vz,
  title         = "The Road to Success: Assessing the Fate of Linguistic
                   Innovations in Online Communities",
  author        = "Del Tredici, Marco and Fernández, Raquel",
  journal       = "arXiv [cs.CL]",
  abstract      = "We investigate the birth and diffusion of lexical innovations
                   in a large dataset of online social communities. We build on
                   sociolinguistic theories and focus on the relation between
                   the spread of a novel term and the social role of the
                   individuals who use it, uncovering characteristics of
                   innovators and adopters. Finally, we perform a prediction
                   task that allows us to anticipate whether an innovation will
                   successfully spread within a community.",
  month         =  jun,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Simon1988-xe,
  title     = "The Science of Design: Creating the Artificial",
  author    = "Simon, Herbert A",
  journal   = "Design Issues",
  publisher = "The MIT Press",
  volume    =  4,
  number    = "1/2",
  pages     = "67--82",
  year      =  1988
}

@ARTICLE{Kenalemang-Palm2021-uc,
  title     = "The scientifization of “green” anti-ageing cosmetics in online
               marketing: a multimodal critical discourse analysis",
  author    = "Kenalemang-Palm, Lame M and Eriksson, Göran",
  journal   = "Social Semiotics",
  publisher = "Routledge",
  pages     = "1--20",
  abstract  = "ABSTRACTThis paper examines the marketing of trending green
               cosmetic products containing natural ingredients and coming with
               claims to keep skin health-enhancing and age-defying benefits.
               This is fostered by the growing importance of successful ageing
               and the neoliberal self-care agenda. Adopting the notion of
               ?integrated design? from Multimodal Critical Discourse Analysis
               (MCDA), this paper looks at the communicative affordances of the
               web and how marketers of ?green? cosmetics connect these to
               science. The analysis shows that the integrated design of the
               webpages allows cosmetic companies to connote science while
               glossing over significant details, leaving causalities,
               classifications, and processes unspecified. This marketing frames
               fighting the ?look? of ageing as a moral and ethical consumption
               choice. Such choices relate to self-care regimes of a
               ?successful? neoliberal citizenship.",
  month     =  oct,
  year      =  2021
}

@INPROCEEDINGS{Goel2016-wb,
  title     = "The Social Dynamics of Language Change in Online Networks",
  author    = "Goel, Rahul and Soni, Sandeep and Goyal, Naman and Paparrizos,
               John and Wallach, Hanna and Diaz, Fernando and Eisenstein, Jacob",
  booktitle = "Social Informatics",
  publisher = "Springer International Publishing",
  pages     = "41--57",
  abstract  = "Language change is a complex social phenomenon, revealing
               pathways of communication and sociocultural influence. But, while
               language change has long been a topic of study in
               sociolinguistics, traditional linguistic research methods rely on
               circumstantial evidence, estimating the direction of change from
               differences between older and younger speakers. In this paper, we
               use a data set of several million Twitter users to track language
               changes in progress. First, we show that language change can be
               viewed as a form of social influence: we observe complex
               contagion for phonetic spellings and “netspeak” abbreviations
               (e.g., lol), but not for older dialect markers from spoken
               language. Next, we test whether specific types of social network
               connections are more influential than others, using a parametric
               Hawkes process model. We find that tie strength plays an
               important role: densely embedded social ties are significantly
               better conduits of linguistic influence. Geographic locality
               appears to play a more limited role: we find relatively little
               evidence to support the hypothesis that individuals are more
               influenced by geographically local social ties, even in their
               usage of geographical dialect markers.",
  year      =  2016
}

@ARTICLE{Beltrama2021-bd,
  title     = "The social meaning of semantic properties",
  author    = "Beltrama, Andrea and Casasanto, Laura Staum and Hall-Lew, Lauren
               and Moore, Emma and Podesva, Robert",
  journal   = "Social meaning and linguistic variation: Theorizing the third
               wave",
  publisher = "Cambridge University Press Cambridge, UK",
  pages     = "80--104",
  abstract  = "… only to refine our theory of social meaning , but for the study
               and understanding of language variation more broadly. Not only
               does the non-homogeneous semantic behaviour of these …",
  year      =  2021
}

@ARTICLE{Edition_undated-pd,
  title  = "The Structure of Scientific Revolutions",
  author = "Edition, Second"
}

@ARTICLE{Lu2022-uu,
  title     = "The subtle language of exclusion: Identifying the Toxic Speech of
               Trans-exclusionary Radical Feminists",
  author    = "Lu, C and Jurgens, D",
  journal   = "Proceedings of the Sixth Workshop on Online",
  publisher = "aclanthology.org",
  abstract  = "Toxic language can take many forms, from explicit hate speech to
               more subtle microaggressions. Within this space, models
               identifying transphobic language have largely …",
  year      =  2022
}

@ARTICLE{Underwood2018-my,
  title     = "The transformation of gender in English-language fiction",
  author    = "Underwood, Ted and Bamman, David and Lee, Sabrina",
  journal   = "Journal of Cultural Analytics",
  publisher = "CA: Journal of Cultural Analytics",
  year      =  2018
}

@BOOK{Hemmingsen2017-vp,
  title     = "The trouble with counter-narratives",
  author    = "Hemmingsen, Ann-Sophie and Castro, Karin Ingrid",
  publisher = "Copenhagen: Danish Institute for International Studies (DIIS)",
  abstract  = "Counter-narratives are routinely suggested as responses to the
               vast amounts of propaganda available online, from groups such as
               Islamic State and al-Qaeda; and the idea of using them to prevent
               terrorism is gaining momentum. International organisations such
               as the EU and UN are including them in their CVE strategies,
               leading to a push for member states to do the same, in spite of a
               great deal of criticism and lack of any actual evidence that
               counter-narratives are an effective method. Taking its point of
               departure in three counter-narrative initiatives introduced in
               the third Danish national action plan on countering and
               preventing extremism and radicalisation, this report explores the
               challenges related to using counter-narratives within the Danish
               preventive framework in particular and in CVE strategies in
               general. The report finds that broad counter-narrative campaigns
               are neither necessary nor appropriate, and that the potential
               negative side-effects are not acceptable when measured against
               the expected benefits. The report ends by suggesting alternative
               approaches.",
  year      =  2017,
  keywords  = "Research Report",
  language  = "en"
}

@BOOK{Warner2000-ea,
  title     = "The Trouble with Normal: Sex, Politics, and the Ethics of Queer
               Life",
  author    = "Warner, Michael",
  publisher = "Harvard University Press",
  abstract  = "Michael Warner, one of our most brilliant social critics, argues
               that gay marriage and other moves toward normalcy are bad not
               just for the gays but for everyone. In place of sexual status
               quo, Warner offers a vision of true sexual autonomy that will
               forever change the way we think about sex, shame, and identity.",
  year      =  2000,
  language  = "en"
}

@ARTICLE{Yanai2020-wi,
  title     = "The two languages of science",
  author    = "Yanai, Itai and Lercher, Martin",
  journal   = "Genome Biol.",
  publisher = "Springer",
  volume    =  21,
  number    =  1,
  pages     =  147,
  abstract  = "… science tools at our disposal. To avoid misunderstandings, the
               distinct language of night science … But developing it as a
               complement to the formal day science language is an important …",
  month     =  jun,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Suchman2023-tw,
  title     = "The uncontroversial ‘thingness’ of {AI}",
  author    = "Suchman, Lucy",
  journal   = "Big Data Soc.",
  publisher = "SAGE Publications",
  volume    =  10,
  number    =  2,
  abstract  = "This commentary starts with the question ‘How is it that AI has
               come to be figured uncontroversially as a thing, however many
               controversies “it” may engender?’ Addressing this question takes
               us to knowledge practices that philosopher of science Helen
               Verran has named a ‘hardening of the categories’, processes that
               not only characterise the onto-epistemology of AI but also are
               central to its constituent techniques and technologies. In a
               context where the stabilization of AI as a figure enables further
               investments in associated techniques and technologies, AI's
               status as controversial works to reiterate both its ontological
               status and its agency. It follows that interventions into the
               field of AI controversies that fail to trouble and destabilise
               the figure of AI risk contributing to its uncontroversial
               reproduction. This is not to deny the proliferating data and
               compute-intensive techniques and technologies that travel under
               the sign of AI but rather to call for a keener focus on their
               locations, politics, material-semiotic specificity, and effects,
               including their ongoing enactment as a singular and controversial
               object.",
  month     =  jul,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Lin2023-ug,
  title         = "The Unlocking Spell on Base {LLMs}: Rethinking Alignment via
                   In-Context Learning",
  author        = "Lin, Bill Yuchen and Ravichander, Abhilasha and Lu, Ximing
                   and Dziri, Nouha and Sclar, Melanie and Chandu, Khyathi and
                   Bhagavatula, Chandra and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "The alignment tuning process of large language models (LLMs)
                   typically involves instruction learning through supervised
                   fine-tuning (SFT) and preference tuning via reinforcement
                   learning from human feedback (RLHF). A recent study, LIMA
                   (Zhou et al. 2023), shows that using merely 1K examples for
                   SFT can achieve significant alignment performance as well,
                   suggesting that the effect of alignment tuning might be
                   ``superficial.'' This raises questions about how exactly the
                   alignment tuning transforms a base LLM. We analyze the effect
                   of alignment tuning by examining the token distribution shift
                   between base LLMs and their aligned counterpart. Our findings
                   reveal that base LLMs and their alignment-tuned versions
                   perform nearly identically in decoding on the majority of
                   token positions. Most distribution shifts occur with
                   stylistic tokens. These direct evidence strongly supports the
                   Superficial Alignment Hypothesis suggested by LIMA. Based on
                   these findings, we rethink the alignment of LLMs by posing
                   the research question: how effectively can we align base LLMs
                   without SFT or RLHF? To address this, we introduce a simple,
                   tuning-free alignment method, URIAL. URIAL achieves effective
                   alignment purely through in-context learning (ICL) with base
                   LLMs, requiring as few as three constant stylistic examples
                   and a system prompt. We conduct a fine-grained and
                   interpretable evaluation on a diverse set of examples, named
                   JUST-EVAL-INSTRUCT. Results demonstrate that base LLMs with
                   URIAL can match or even surpass the performance of LLMs
                   aligned with SFT or SFT+RLHF. We show that the gap between
                   tuning-free and tuning-based alignment methods can be
                   significantly reduced through strategic prompting and ICL.
                   Our findings on the superficial nature of alignment tuning
                   and results with URIAL suggest that deeper analysis and
                   theoretical understanding of alignment is crucial to future
                   LLM research.",
  month         =  dec,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Friedman2022-yc,
  title         = "The Vendi Score: A Diversity Evaluation Metric for Machine
                   Learning",
  author        = "Friedman, Dan and Dieng, Adji Bousso",
  journal       = "arXiv [cs.LG]",
  abstract      = "Diversity is an important criterion for many areas of machine
                   learning (ML), including generative modeling and dataset
                   curation. Yet little work has gone into understanding,
                   formalizing, and measuring diversity in ML. In this paper, we
                   address the diversity evaluation problem by proposing the
                   Vendi Score, which connects and extends ideas from ecology
                   and quantum statistical mechanics to ML. The Vendi Score is
                   defined as the exponential of the Shannon entropy of the
                   eigenvalues of a similarity matrix. This matrix is induced by
                   a user-defined similarity function applied to the sample to
                   be evaluated for diversity. In taking a similarity function
                   as input, the Vendi Score enables its user to specify any
                   desired form of diversity. Importantly, unlike many existing
                   metrics in ML, the Vendi Score doesn't require a reference
                   dataset or distribution over samples or labels, it is
                   therefore general and applicable to any generative model,
                   decoding algorithm, and dataset from any domain where
                   similarity can be defined. We showcased the Vendi Score on
                   molecular generative modeling, a domain where diversity plays
                   an important role in enabling the discovery of novel
                   molecules. We found that the Vendi Score addresses
                   shortcomings of the current diversity metric of choice in
                   that domain. We also applied the Vendi Score to generative
                   models of images and decoding algorithms of text and found it
                   confirms known results about diversity in those domains.
                   Furthermore, we used the Vendi Score to measure mode
                   collapse, a known limitation of generative adversarial
                   networks (GANs). In particular, the Vendi Score revealed that
                   even GANs that capture all the modes of a labeled dataset can
                   be less diverse than the original dataset. Finally, the
                   interpretability of the Vendi Score allowed us to diagnose
                   several benchmark ML datasets for diversity, opening the door
                   for diversity-informed data augmentation.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Paik2021-an,
  title         = "The World of an Octopus: How Reporting Bias Influences a
                   Language Model's Perception of Color",
  author        = "Paik, Cory and Aroca-Ouellette, Stéphane and Roncone,
                   Alessandro and Kann, Katharina",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work has raised concerns about the inherent
                   limitations of text-only pretraining. In this paper, we first
                   demonstrate that reporting bias, the tendency of people to
                   not state the obvious, is one of the causes of this
                   limitation, and then investigate to what extent multimodal
                   training can mitigate this issue. To accomplish this, we 1)
                   generate the Color Dataset (CoDa), a dataset of
                   human-perceived color distributions for 521 common objects;
                   2) use CoDa to analyze and compare the color distribution
                   found in text, the distribution captured by language models,
                   and a human's perception of color; and 3) investigate the
                   performance differences between text-only and multimodal
                   models on CoDa. Our results show that the distribution of
                   colors that a language model recovers correlates more
                   strongly with the inaccurate distribution found in text than
                   with the ground-truth, supporting the claim that reporting
                   bias negatively impacts and inherently limits text-only
                   training. We then demonstrate that multimodal models can
                   leverage their visual training to mitigate these effects,
                   providing a promising avenue for future research.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-nn,
  title = "{TheMediaEquationFINAL}.pdf"
}

@ARTICLE{Cao2022-hi,
  title         = "Theory-Grounded Measurement of {U}.{S}. Social Stereotypes in
                   English Language Models",
  author        = "Cao, Yang Trista and Sotnikova, Anna and Daumé, III, Hal and
                   Rudinger, Rachel and Zou, Linda",
  journal       = "arXiv [cs.CL]",
  abstract      = "NLP models trained on text have been shown to reproduce human
                   stereotypes, which can magnify harms to marginalized groups
                   when systems are deployed at scale. We adapt the
                   Agency-Belief-Communion (ABC) stereotype model of Koch et al.
                   (2016) from social psychology as a framework for the
                   systematic study and discovery of stereotypic group-trait
                   associations in language models (LMs). We introduce the
                   sensitivity test (SeT) for measuring stereotypical
                   associations from language models. To evaluate SeT and other
                   measures using the ABC model, we collect group-trait
                   judgments from U.S.-based subjects to compare with English LM
                   stereotypes. Finally, we extend this framework to measure LM
                   stereotyping of intersectional identities.",
  month         =  jun,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{West2024-qu,
  title         = "There and back again: The {AI} alignment paradox",
  author        = "West, Robert and Aydin, Roland",
  journal       = "arXiv [cs.AI]",
  abstract      = "The field of AI alignment aims to steer AI systems toward
                   human goals, preferences, and ethical principles. Its
                   contributions have been instrumental for improving the output
                   quality, safety, and trustworthiness of today's AI models.
                   This perspective article draws attention to a fundamental
                   challenge inherent in all AI alignment endeavors, which we
                   term the ``AI alignment paradox'': The better we align AI
                   models with our values, the easier we make it for adversaries
                   to misalign the models. We illustrate the paradox by
                   sketching three concrete example incarnations for the case of
                   language models, each corresponding to a distinct way in
                   which adversaries can exploit the paradox. With AI's
                   increasing real-world impact, it is imperative that a broad
                   community of researchers be aware of the AI alignment paradox
                   and work to find ways to break out of it, in order to ensure
                   the beneficial use of AI for the good of humanity.",
  month         =  may,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Burgoon2013-py,
  title    = "There Are Many Ways to See the Forest for the Trees: A Tour Guide
              for Abstraction",
  author   = "Burgoon, Erin M and Henderson, Marlone D and Markman, Arthur B",
  journal  = "Perspect. Psychol. Sci.",
  volume   =  8,
  number   =  5,
  pages    = "501--520",
  abstract = "Abstraction is a useful process for broadening mental horizons,
              integrating new experiences, and communicating information to
              others. Much attention has been directed at identifying the causes
              and consequences of abstraction across the subdisciplines of
              psychology. Despite this attention, an integrative review of the
              methods that are used for studying abstraction is missing from the
              literature. The current article aims to fill this gap in several
              ways. First, we highlight the different ways in which abstraction
              has been defined in the literature and then suggest an integrative
              definition. Second, we provide a tour of the different ways
              abstraction has been manipulated and measured over the years.
              Finally, we highlight considerations for researchers in choosing
              methods for their own research.",
  month    =  sep,
  year     =  2013,
  keywords = "abstraction; concreteness; construal; general; gist; global;
              representation; specificity",
  language = "en"
}

@ARTICLE{Elsafoury2023-bn,
  title         = "Thesis Distillation: Investigating The Impact of Bias in
                   {NLP} Models on Hate Speech Detection",
  author        = "Elsafoury, Fatma",
  journal       = "arXiv [cs.CL]",
  abstract      = "This paper is a summary of the work done in my PhD thesis.
                   Where I investigate the impact of bias in NLP models on the
                   task of hate speech detection from three perspectives:
                   explainability, offensive stereotyping bias, and fairness.
                   Then, I discuss the main takeaways from my thesis and how
                   they can benefit the broader NLP community. Finally, I
                   discuss important future research directions. The findings of
                   my thesis suggest that the bias in NLP models impacts the
                   task of hate speech detection from all three perspectives.
                   And that unless we start incorporating social sciences in
                   studying bias in NLP models, we will not effectively overcome
                   the current limitations of measuring and mitigating bias in
                   NLP models.",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Amy-Chinn2006-wp,
  title     = "This is just for me(n): How the regulation of post-feminist
               lingerie advertising perpetuates woman as object",
  author    = "Amy-Chinn, Dee",
  journal   = "Journal of Consumer Culture",
  publisher = "SAGE Publications",
  volume    =  6,
  number    =  2,
  pages     = "155--175",
  abstract  = "This article argues that the official process by which
               advertising is regulated in the UK, when applied to the
               advertising of women's underwear, restricts and undermines,
               rather than encourages, attempts to renegotiate the discourse
               that surrounds the representation of women in advertising,
               particularly when it seeks to construct a discourse that
               questions the centrality of men to female sexual pleasure. This
               process judges ads on the basis of whether they are likely to
               cause ?serious or widespread offence?. The regulators'
               interpretation of this ensures that lingerie advertising that
               represents women as objects for the male gaze remains acceptable,
               as do ads in which women are deliberately offering up a
               sexualized self-presentation, while images that seek to negotiate
               a discourse outside a heterosexuality governed by the coital
               imperative are considered problematic. This encourages the
               perpetuation of a conservative framework in which women can be
               viewed as objects rather than subjects.",
  month     =  jul,
  year      =  2006
}

@ARTICLE{Herrick2021-ms,
  title     = "``This is just how {I} cope'': An inductive thematic analysis of
               eating disorder recovery content created and shared on {TikTok}
               using \#EDrecovery",
  author    = "Herrick, Shannon S C and Hallward, Laura and Duncan, Lindsay R",
  journal   = "Int. J. Eat. Disord.",
  publisher = "Wiley",
  volume    =  54,
  number    =  4,
  pages     = "516--526",
  abstract  = "OBJECTIVE: To explore eating disorder (ED) recovery-related
               content created and shared on the social media platform TikTok.
               METHOD: A systematic review and inductive thematic analysis of
               150 TikTok posts catalogued under hashtag (\#) EDrecovery. Two
               coders independently analyzed the posts and a critical peer
               facilitated discussions about the resulting codes and themes.
               RESULTS: Creators on TikTok used \#EDrecovery to share their
               personal experiences with recovery through the use and cooption
               of popular (or viral) video formats, succinct storytelling, and
               the production of educational content. Five themes were
               interpreted across the data: (a) ED awareness, (b) inpatient
               story time: ``ED unit tings'', (c) eating in recovery, (d)
               transformations: ``how about a weight gain glow up?'', and (e)
               trendy gallows humor: ``let's confuse people who have a good
               relationship with food''. DISCUSSION: TikTok as a user-friendly,
               creative media may provide the artistic and social tools for some
               creators to add their distinct voice to the ED recovery narrative
               and foster some semblance of community. Although all of the
               analyzed content was catalogued under \#EDrecovery, some of the
               posts reified the increasingly blurred boundary that exists
               between ED recovery and pro-ED content on TikTok.",
  month     =  apr,
  year      =  2021,
  keywords  = "anorexia nervosa; bulimia nervosa; eating disorders; mobile
               applications; recovery; social media; social platform",
  language  = "en"
}

@ARTICLE{Israeli2022-tp,
  title         = "This Must Be the Place: Predicting Engagement of Online
                   Communities in a Large-scale Distributed Campaign",
  author        = "Israeli, Abraham and Kremiansky, Alexander and Tsur, Oren",
  journal       = "arXiv [cs.SI]",
  abstract      = "Understanding collective decision making at a large-scale,
                   and elucidating how community organization and community
                   dynamics shape collective behavior are at the heart of social
                   science research. In this work we study the behavior of
                   thousands of communities with millions of active members. We
                   define a novel task: predicting which community will
                   undertake an unexpected, large-scale, distributed campaign.
                   To this end, we develop a hybrid model, combining textual
                   cues, community meta-data, and structural properties. We show
                   how this multi-faceted model can accurately predict
                   large-scale collective decision-making in a distributed
                   environment. We demonstrate the applicability of our model
                   through Reddit's r/place - a large-scale online experiment in
                   which millions of users, self-organized in thousands of
                   communities, clashed and collaborated in an effort to realize
                   their agenda. Our hybrid model achieves a high F1 prediction
                   score of 0.826. We find that coarse meta-features are as
                   important for prediction accuracy as fine-grained textual
                   cues, while explicit structural features play a smaller role.
                   Interpreting our model, we provide and support various social
                   insights about the unique characteristics of the communities
                   that participated in the \r/place experiment. Our results and
                   analysis shed light on the complex social dynamics that drive
                   collective behavior, and on the factors that propel user
                   coordination. The scale and the unique conditions of the
                   \rp~experiment suggest that our findings may apply in broader
                   contexts, such as online activism, (countering) the spread of
                   hate speech and reducing political polarization. The broader
                   applicability of the model is demonstrated through an
                   extensive analysis of the WallStreetBets community, their
                   role in r/place and four years later, in the GameStop short
                   squeeze campaign of 2021.",
  month         =  jan,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.SI"
}

@ARTICLE{Goldfarb-Tarrant2023-fi,
  title         = "This Prompt is Measuring : Evaluating Bias Evaluation in
                   Language Models",
  author        = "Goldfarb-Tarrant, Seraphina and Ungless, Eddie and Balkir,
                   Esma and Blodgett, Su Lin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Bias research in NLP seeks to analyse models for social
                   biases, thus helping NLP practitioners uncover, measure, and
                   mitigate social harms. We analyse the body of work that uses
                   prompts and templates to assess bias in language models. We
                   draw on a measurement modelling framework to create a
                   taxonomy of attributes that capture what a bias test aims to
                   measure and how that measurement is carried out. By applying
                   this taxonomy to 90 bias tests, we illustrate qualitatively
                   and quantitatively that core aspects of bias test
                   conceptualisations and operationalisations are frequently
                   unstated or ambiguous, carry implicit assumptions, or be
                   mismatched. Our analysis illuminates the scope of possible
                   bias types the field is able to measure, and reveals types
                   that are as yet under-researched. We offer guidance to enable
                   the community to explore a wider section of the possible bias
                   space, and to better close the gap between desired outcomes
                   and experimental design, both for bias and for evaluating
                   language models more broadly.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Kaffee2023-yc,
  title         = "Thorny Roses: Investigating the Dual Use Dilemma in Natural
                   Language Processing",
  author        = "Kaffee, Lucie-Aimée and Arora, Arnav and Talat, Zeerak and
                   Augenstein, Isabelle",
  journal       = "arXiv [cs.CL]",
  abstract      = "Dual use, the intentional, harmful reuse of technology and
                   scientific artefacts, is a problem yet to be well-defined
                   within the context of Natural Language Processing (NLP).
                   However, as NLP technologies continue to advance and become
                   increasingly widespread in society, their inner workings have
                   become increasingly opaque. Therefore, understanding dual use
                   concerns and potential ways of limiting them is critical to
                   minimising the potential harms of research and development.
                   In this paper, we conduct a survey of NLP researchers and
                   practitioners to understand the depth and their perspective
                   of the problem as well as to assess existing available
                   support. Based on the results of our survey, we offer a
                   definition of dual use that is tailored to the needs of the
                   NLP community. The survey revealed that a majority of
                   researchers are concerned about the potential dual use of
                   their research but only take limited action toward it. In
                   light of the survey results, we discuss the current state and
                   potential means for mitigating dual use in NLP and propose a
                   checklist that can be integrated into existing conference
                   ethics-frameworks, e.g., the ACL ethics checklist.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Mathew2019-di,
  title    = "Thou Shalt Not Hate: Countering Online Hate Speech",
  author   = "Mathew, Binny and Saha, Punyajoy and Tharad, Hardik and Rajgaria,
              Subham and Singhania, Prajwal and Maity, Suman Kalyan and Goyal,
              Pawan and Mukherjee, Animesh",
  journal  = "ICWSM",
  volume   =  13,
  pages    = "369--380",
  abstract = "Hate content in social media is ever increasing. While Facebook,
              Twitter, Google have attempted to take several steps to tackle the
              hateful content, they have mostly been unsuccessful. Counterspeech
              is seen as an effective way of tackling the online hate without
              any harm to the freedom of speech. Thus, an alternative strategy
              for these platforms could be to promote counterspeech as a defense
              against hate content. However, in order to have a successful
              promotion of such counterspeech, one has to have a deep
              understanding of its dynamics in the online world. Lack of
              carefully curated data largely inhibits such understanding. In
              this paper, we create and release the first ever dataset for
              counterspeech using comments from YouTube. The data contains
              13,924 manually annotated comments where the labels indicate
              whether a comment is a counterspeech or not. This data allows us
              to perform a rigorous measurement study characterizing the
              linguistic structure of counterspeech for the first time. This
              analysis results in various interesting insights such as: the
              counterspeech comments receive much more likes as compared to the
              noncounterspeech comments, for certain communities majority of the
              non-counterspeech comments tend to be hate speech, the different
              types of counterspeech are not all equally effective and the
              language choice of users posting counterspeech is largely
              different from those posting non-counterspeech as revealed by a
              detailed psycholinguistic analysis. Finally, we build a set of
              machine learning models that are able to automatically detect
              counterspeech in YouTube videos with an F1-score of 0.71. We also
              build multilabel models that can detect different types of
              counterspeech in a comment with an F1-score of 0.60.",
  month    =  jul,
  year     =  2019,
  language = "en"
}

@ARTICLE{Coeckelbergh2022-em,
  title    = "Three Responses to Anthropomorphism in Social Robotics: Towards a
              Critical, Relational, and Hermeneutic Approach",
  author   = "Coeckelbergh, Mark",
  journal  = "International Journal of Social Robotics",
  volume   =  14,
  number   =  10,
  pages    = "2049--2061",
  abstract = "Both designers and users of social robots tend to anthropomorphize
              robots. Focusing on the question how to conceptualize the relation
              between robots and humans, this paper first outlines two opposite
              philosophical views regarding this relation, which are connected
              to various normative responses to anthropomorphism and
              anthropomorphization. Then it argues for a third view: navigating
              between what it calls “naïve instrumentalism” and “uncritical
              posthumanism”, it develops a hermeneutic, relational, and critical
              approach. Paradoxically, by unpacking the human dimension of
              robotics in its use and development, this view enables a critical
              discussion of anthropomorphizing robots. At the same time, and
              again somewhat paradoxically, it avoids a naïve instrumentalist
              position by taking robots’ role as an instrument in a larger
              con-technology seriously. As such, the third view questions the
              dualism assumed in the debate. The paper then explores what this
              means for the field of social robotics and the education of
              computer scientists and engineers. It proposes a reform based on a
              relational understanding of the field itself and offers
              suggestions for the role of users-citizens.",
  month    =  dec,
  year     =  2022
}

@INPROCEEDINGS{Chancellor2016-tm,
  title     = "\#thyghgapp",
  author    = "Chancellor, Stevie and Pater, Jessica Annette and Clear, Trustin
               and Gilbert, Eric and De Choudhury, Munmun",
  booktitle = "Proceedings of the 19th ACM Conference on Computer-Supported
               Cooperative Work \& Social Computing",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "Pro-eating disorder (pro-ED) communities on social media
               encourage the adoption and maintenance of disordered eating
               habits as acceptable alternative lifestyles rather than threats
               to health. In particular, the social networking site Instagram
               has reacted by banning searches on several pro-ED tags and
               issuing content advisories on others. We pre-sent the first
               large-scale quantitative study investigating pro-ED communities
               on Instagram in the aftermath of moderation -- our dataset
               contains 2.5M posts between 2011 and 2014. We find that the
               pro-ED community has adopted non-standard lexical variations of
               moderated tags to circumvent these restrictions. In fact,
               increasingly complex lexical variants have emerged over time.
               Communities that use lexical variants show increased
               participation and support of pro-ED (15-30\%). Finally, the tags
               associated with content on these variants express more toxic,
               self-harm, and vulnerable content. Despite Instagramâs
               moderation strategies, pro-ED communities are active and
               thriving. We discuss the effectiveness of content moderation as
               an intervention for communities of deviant behavior.",
  month     =  feb,
  year      =  2016
}

@ARTICLE{Literat2023-zd,
  title     = "{TikTok} as a Key Platform for Youth Political Expression:
               Reflecting on the Opportunities and Stakes Involved",
  author    = "Literat, Ioana and Kligler-Vilenchik, Neta",
  journal   = "Social Media + Society",
  publisher = "SAGE Publications Ltd",
  volume    =  9,
  number    =  1,
  pages     =  20563051231157595,
  abstract  = "Reflecting on 6?years of our research?which began on musical.ly
               and transitioned into TikTok?we argue that TikTok is a vital
               space to study social movements due to its centrality in youth
               lives and its ability to give voice to youth political expression
               in richly creative ways. We see the political expression
               happening on TikTok as a harbinger of the changing nature of this
               phenomenon, and a necessary impetus to broaden our understandings
               of activism and political expression today. At the same time, we
               must also consider the implications of TikTok becoming such a
               valuable space for youth politics and activism, in terms of the
               kinds of expression it affords or constrains, and the power it
               gives the platform. In closing, we encourage scholars to maintain
               a balanced and constructive approach in researching the platform,
               and embrace the messiness and complexity inherent in this
               endeavor?which mirrors the messiness and complexity of the
               platform itself.",
  month     =  jan,
  year      =  2023
}

@ARTICLE{Herrman2020-rd,
  title     = "{TikTok} is shaping politics. But how",
  author    = "Herrman, John",
  journal   = "NY Times",
  publisher = "openscholar.huji.ac.il",
  volume    =  28,
  abstract  = "… It ’s hard to refer to what we see on the platform as
               consensus. Rather, we find that TikTok enables collective
               political expression for youth — that is, it allows them to
               deliberately connect …",
  year      =  2020
}

@ARTICLE{Pryde2022-mm,
  title     = "{TikTok} on the clock but the \#fitspo don't stop: The impact of
               {TikTok} fitspiration videos on women's body image concerns",
  author    = "Pryde, Samantha and Prichard, Ivanka",
  journal   = "Body Image",
  publisher = "Elsevier BV",
  volume    =  43,
  pages     = "244--252",
  abstract  = "Fitspiration is a popular social media trend that aims to inspire
               individuals to improve their health and fitness through diet and
               exercise. However, viewing fitspiration content on Instagram has
               been identified as a contributor to negative body image,
               especially for young women. With the growing popularity of the
               video sharing platform TikTok and concerns over its content, the
               present study aimed to experimentally examine the effect of
               exposure to fitspiration TikTok videos on young women's body
               dissatisfaction, appearance comparison and mood. The roles of
               state appearance comparison as a mediator and trait fit ideal
               internalisation as a moderator were also considered. Young women
               (18-25, N = 120) from Australia were randomly allocated to view a
               set of fitspiration videos or a set of art control videos from
               TikTok. Results indicated that exposure to fitspiration TikTok
               videos increased state appearance comparison and state negative
               mood relative to art TikTok videos but did not directly increase
               state body dissatisfaction. State appearance comparison
               significantly mediated the effect of TikTok videos on body
               dissatisfaction and mood, however, there was no moderating effect
               of trait fit ideal internalisation. These findings highlight the
               importance of state appearance comparison in relation to viewing
               fitspiration content on TikTok.",
  month     =  dec,
  year      =  2022,
  keywords  = "Appearance comparison; Body image; Fit ideal internalisation;
               Fitspiration; Social media; TikTok; Women",
  language  = "en"
}

@ARTICLE{Bissonette_Mink2022-jg,
  title     = "{TikTok} use and body dissatisfaction: Examining direct,
               indirect, and moderated relations",
  author    = "Bissonette Mink, Danielle and Szymanski, Dawn M",
  journal   = "Body Image",
  publisher = "Elsevier",
  volume    =  43,
  pages     = "205--216",
  abstract  = "In this study, we examined potential direct, indirect, and
               moderated effects in the relations between the use of TikTok, a
               video-based appearance-related social networking site, and body
               dissatisfaction among a sample of 778 United States' young adult
               college women. Results showed that TikTok use was indirectly
               related to body dissatisfaction through more upward appearance
               comparison and more body surveillance acting in serial. Contrary
               to our hypotheses, we also found that exposure to body acceptance
               and critique of appearance expectations, a facet of exposure to
               body positive media, and commercial media literacy exacerbated
               the direct relation between TikTok use and upward appearance
               comparison and the indirect relations between TikTok use and body
               dissatisfaction through upward appearance comparison and upward
               appearance comparison and body surveillance in serial. That is,
               the relations were significant for those at high and average
               levels of both acceptance and critique exposure and commercial
               media literacy, but not for those with low levels. Finally, we
               found that TikTok use was only associated with upward appearance
               comparison at average and low levels of peer social media
               literacy but not high levels. Our findings suggest that regular
               and consistent use of TikTok may be harmful to women's body
               image, and women with higher levels of acceptance and critique
               exposure and commercial media literacy may be the most vulnerable
               to these negative effects.",
  month     =  dec,
  year      =  2022,
  keywords  = "Body dissatisfaction; Body positivity; Media literacy; Social
               media; Social networking",
  language  = "en"
}

@ARTICLE{Eldan2023-hd,
  title         = "{TinyStories}: How Small Can Language Models Be and Still
                   Speak Coherent English?",
  author        = "Eldan, Ronen and Li, Yuanzhi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) are powerful tools for natural language
                   processing, but they often struggle to produce coherent and
                   fluent text when they are small. Models with around 125M
                   parameters such as GPT-Neo (small) or GPT-2 (small) can
                   rarely generate coherent and consistent English text beyond a
                   few words even after extensive training. This raises the
                   question of whether the emergence of the ability to produce
                   coherent English text only occurs at larger scales (with
                   hundreds of millions of parameters or more) and complex
                   architectures (with many layers of global attention). In this
                   work, we introduce TinyStories, a synthetic dataset of short
                   stories that only contain words that a typical 3 to
                   4-year-olds usually understand, generated by GPT-3.5 and
                   GPT-4. We show that TinyStories can be used to train and
                   evaluate LMs that are much smaller than the state-of-the-art
                   models (below 10 million total parameters), or have much
                   simpler architectures (with only one transformer block), yet
                   still produce fluent and consistent stories with several
                   paragraphs that are diverse and have almost perfect grammar,
                   and demonstrate reasoning capabilities. We also introduce a
                   new paradigm for the evaluation of language models: We
                   suggest a framework which uses GPT-4 to grade the content
                   generated by these models as if those were stories written by
                   students and graded by a (human) teacher. This new paradigm
                   overcomes the flaws of standard benchmarks which often
                   requires the model's output to be very structures, and
                   moreover provides a multidimensional score for the model,
                   providing scores for different capabilities such as grammar,
                   creativity and consistency. We hope that TinyStories can
                   facilitate the development, analysis and research of LMs,
                   especially for low-resource or specialized domains, and shed
                   light on the emergence of language capabilities in LMs.",
  month         =  may,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Zhu_undated-gs,
  title  = "Title: Collaborative Research: {SCH}: Leveraging {NLP} and
            Human-Centered Design to Improve Microskills Training and Outcomes
            for Non-professional Mental Health Counselors",
  author = "Zhu, Pi: Haiyi"
}

@ARTICLE{Shmueli2010-jc,
  title     = "To Explain or to Predict?",
  author    = "Shmueli, Galit",
  journal   = "SSO Schweiz. Monatsschr. Zahnheilkd.",
  publisher = "Institute of Mathematical Statistics",
  volume    =  25,
  number    =  3,
  pages     = "289--310",
  abstract  = "Statistical modeling is a powerful tool for developing and
               testing theories by way of causal explanation, prediction, and
               description. In many disciplines there is near-exclusive use of
               statistical modeling for causal explanation and the assumption
               that models with high explanatory power are inherently of high
               predictive power. Conflation between explanation and prediction
               is common, yet the distinction must be understood for progressing
               scientific knowledge. While this distinction has been recognized
               in the philosophy of science, the statistical literature lacks a
               thorough discussion of the many differences that arise in the
               process of modeling for an explanatory versus a predictive goal.
               The purpose of this article is to clarify the distinction between
               explanatory and predictive modeling, to discuss its sources, and
               to reveal the practical implications of the distinction to each
               step in the modeling process.",
  month     =  aug,
  year      =  2010,
  keywords  = "causality; data mining; Explanatory modeling; predictive
               modeling; predictive power; scientific research; statistical
               strategy",
  language  = "en"
}

@ARTICLE{Ziems2021-qf,
  title         = "To Protect and To Serve? Analyzing Entity-Centric Framing of
                   Police Violence",
  author        = "Ziems, Caleb and Yang, Diyi",
  journal       = "arXiv [cs.CL]",
  abstract      = "Framing has significant but subtle effects on public opinion
                   and policy. We propose an NLP framework to measure
                   entity-centric frames. We use it to understand media coverage
                   on police violence in the United States in a new Police
                   Violence Frames Corpus of 82k news articles spanning 7k
                   police killings. Our work uncovers more than a dozen framing
                   devices and reveals significant differences in the way
                   liberal and conservative news sources frame both the issue of
                   police violence and the entities involved. Conservative
                   sources emphasize when the victim is armed or attacking an
                   officer and are more likely to mention the victim's criminal
                   record. Liberal sources focus more on the underlying systemic
                   injustice, highlighting the victim's race and that they were
                   unarmed. We discover temporary spikes in these injustice
                   frames near high-profile shooting events, and finally, we
                   show protest volume correlates with and precedes media
                   framing decisions.",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Steinberg2023-fq,
  title    = "To Truly Reduce Weight Stigma and Eating Disorder Risk, We Need to
              Stop Promoting Weight Loss",
  author   = "Steinberg, Dori M and Bohon, Cara",
  journal  = "J. Acad. Nutr. Diet.",
  volume   =  123,
  number   =  3,
  pages    = "399--400",
  month    =  mar,
  year     =  2023,
  language = "en"
}

@ARTICLE{Bucinca2021-fs,
  title         = "To Trust or to Think: Cognitive Forcing Functions Can Reduce
                   Overreliance on {AI} in {AI}-assisted Decision-making",
  author        = "Buçinca, Zana and Malaya, Maja Barbara and Gajos, Krzysztof Z",
  journal       = "arXiv [cs.HC]",
  abstract      = "People supported by AI-powered decision support tools
                   frequently overrely on the AI: they accept an AI's suggestion
                   even when that suggestion is wrong. Adding explanations to
                   the AI decisions does not appear to reduce the overreliance
                   and some studies suggest that it might even increase it.
                   Informed by the dual-process theory of cognition, we posit
                   that people rarely engage analytically with each individual
                   AI recommendation and explanation, and instead develop
                   general heuristics about whether and when to follow the AI
                   suggestions. Building on prior research on medical
                   decision-making, we designed three cognitive forcing
                   interventions to compel people to engage more thoughtfully
                   with the AI-generated explanations. We conducted an
                   experiment (N=199), in which we compared our three cognitive
                   forcing designs to two simple explainable AI approaches and
                   to a no-AI baseline. The results demonstrate that cognitive
                   forcing significantly reduced overreliance compared to the
                   simple explainable AI approaches. However, there was a
                   trade-off: people assigned the least favorable subjective
                   ratings to the designs that reduced the overreliance the
                   most. To audit our work for intervention-generated
                   inequalities, we investigated whether our interventions
                   benefited equally people with different levels of Need for
                   Cognition (i.e., motivation to engage in effortful mental
                   activities). Our results show that, on average, cognitive
                   forcing interventions benefited participants higher in Need
                   for Cognition more. Our research suggests that human
                   cognitive motivation moderates the effectiveness of
                   explainable AI solutions.",
  month         =  feb,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Laestadius2022-ki,
  title     = "Too human and not human enough: A grounded theory analysis of
               mental health harms from emotional dependence on the social
               chatbot Replika",
  author    = "Laestadius, Linnea and Bishop, Andrea and Gonzalez, Michael and
               Illenčík, Diana and Campos-Castillo, Celeste",
  journal   = "New Media \& Society",
  publisher = "SAGE Publications",
  pages     =  14614448221142007,
  abstract  = "Social chatbot (SC) applications offering social companionship
               and basic therapy tools have grown in popularity for emotional,
               social, and psychological support. While use appears to offer
               mental health benefits, few studies unpack the potential for
               harms. Our grounded theory study analyzes mental health
               experiences with the popular SC application Replika. We
               identified mental health relevant posts made in the r/Replika
               Reddit community between 2017 and 2021 (n?=?582). We find
               evidence of harms, facilitated via emotional dependence on
               Replika that resembles patterns seen in human?human
               relationships. Unlike other forms of technology dependency, this
               dependency is marked by role-taking, whereby users felt that
               Replika had its own needs and emotions to which the user must
               attend. While prior research suggests human?chatbot and
               human?human interactions may not resemble each other, we identify
               social and technological factors that promote parallels and
               suggest ways to balance the benefits and risks of SCs.",
  month     =  dec,
  year      =  2022
}

@ARTICLE{Ying2022-oc,
  title     = "Topics, concepts, and measurement: A crowdsourced procedure for
               validating topics as measures",
  author    = "Ying, Luwei and Montgomery, Jacob M and Stewart, Brandon M",
  journal   = "Polit. Anal.",
  publisher = "Cambridge University Press (CUP)",
  volume    =  30,
  number    =  4,
  pages     = "570--589",
  abstract  = "AbstractTopic models, as developed in computer science, are
               effective tools for exploring and summarizing large document
               collections. When applied in social science research, however,
               they are commonly used for measurement, a task that requires
               careful validation to ensure that the model outputs actually
               capture the desired concept of interest. In this paper, we review
               current practices for topic validation in the field and show that
               extensive model validation is increasingly rare, or at least not
               systematically reported in papers and appendices. To supplement
               current practices, we refine an existing crowd-sourcing method by
               Chang and coauthors for validating topic quality and go on to
               create new procedures for validating conceptual labels provided
               by the researcher. We illustrate our method with an analysis of
               Facebook posts by U.S. Senators and provide software and guidance
               for researchers wishing to validate their own topic models. While
               tailored, case-specific validation exercises will always be best,
               we aim to improve standard practices by providing a
               general-purpose tool to validate topics as measures.",
  month     =  oct,
  year      =  2022,
  language  = "en"
}

@ARTICLE{Wang_undated-ht,
  title  = "{ToViLaG}: Your Visual-Language Generative Model is Also An Evildoer",
  author = "Wang, Xinpeng and Yi, Xiaoyuan and Jiang, Han and Zhou, Shanlin and
            Wei, Zhihua and Xie, Xing"
}

@ARTICLE{Agre2006-hz,
  title     = "Toward a critical technical practice: Lessons learned in trying
               to reform {AI}",
  author    = "Agre, P",
  journal   = "Social science, technical systems, and cooperative",
  publisher = "taylorfrancis.com",
  abstract  = "This is a chapter in Geof Bowker, Les Gasser, Leigh Star, and
               Bill Turner, eds, Bridging the Great Divide: Social Science,
               Technical Systems, and Cooperative Work , Erlbaum, 1997",
  year      =  2006
}

@ARTICLE{Brunila2023-xj,
  title         = "Toward a Critical Toponymy Framework for Named Entity
                   Recognition: A Case Study of Airbnb in New York City",
  author        = "Brunila, Mikael and LaViolette, Jack and CH-Wang, Sky and
                   Verma, Priyanka and Féré, Clara and McKenzie, Grant",
  journal       = "arXiv [cs.CL]",
  abstract      = "Critical toponymy examines the dynamics of power, capital,
                   and resistance through place names and the sites to which
                   they refer. Studies here have traditionally focused on the
                   semantic content of toponyms and the top-down institutional
                   processes that produce them. However, they have generally
                   ignored the ways in which toponyms are used by ordinary
                   people in everyday discourse, as well as the other strategies
                   of geospatial description that accompany and contextualize
                   toponymic reference. Here, we develop computational methods
                   to measure how cultural and economic capital shape the ways
                   in which people refer to places, through a novel annotated
                   dataset of 47,440 New York City Airbnb listings from the
                   2010s. Building on this dataset, we introduce a new named
                   entity recognition (NER) model able to identify important
                   discourse categories integral to the characterization of
                   place. Our findings point toward new directions for critical
                   toponymy and to a range of previously understudied linguistic
                   signals relevant to research on neighborhood status, housing
                   and tourism markets, and gentrification.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Huxman1995-pr,
  title     = "Toward a dynamic generic framework of apologia: A case study of
               Dow chemical, Vietnam, and the napalm controversy",
  author    = "Huxman, Susan Schultz and Bruce, Denice Beatty",
  journal   = "Commun. Stud.",
  publisher = "Informa UK Limited",
  volume    =  46,
  number    = "1-2",
  pages     = "57--72",
  abstract  = "This study argues that the genre of apologia has distinct
               situational, substantive, and stylistic factors that are
               dynamically related. A dynamic theoretical framework using these
               three factors is proposed and used to analyze the protracted
               apologia of the Dow Chemical Company from 1966‐1969 in response
               to charges by antiwar protesters of immorality for producing
               napalm. Dow's rhetorical choices become much clearer when seen
               through the clear, serious, unprecedented, and diffuse accusatory
               climate that precipitated the adoption of a self actualized
               motive and an argumentative battle on the grounds of
               jurisdiction. Special constraints that corporate apologists may
               face as nonpersons are advanced to complete the configuration of
               this dynamic generic framework.",
  month     =  mar,
  year      =  1995,
  language  = "en"
}

@ARTICLE{Griffioen2020-sw,
  title    = "Toward improved methods in social media research",
  author   = "Griffioen, Nastasia and van Rooij, Marieke and Lichtwarck-Aschoff,
              Anna and Granic, Isabela",
  journal  = "Technology, Mind, and Behavior",
  volume   =  1,
  number   =  1,
  abstract = "Both academic and public interest in social media and their
              effects have increased dramatically over the last decade. In
              particular, a plethora of studies have been conducted that aimed
              to uncover the relationship between social media use and youth
              well-being, fueled by recent concerns that declines in youth
              well-being may well be caused by a rise in digital technology use.
              However, reviews of the field strongly suggest that the picture
              may not be as clear-cut as previously thought, with some studies
              suggesting positive effects, and some studies suggesting negative
              effects on youth well-being. To shed light on this ambiguity, we
              have conducted a narrative review of 94 social media use and
              well-being studies. A number of patterns in methodological
              practices in the field have now become apparent: Self-report
              measures of general statistics around social media use dominate
              the field, which furthermore often falls short in terms of
              ecological validity and sufficient use of experimental designs
              that would enable causal inference. We go on to discuss why such
              practices are problematic in some cases, and more importantly,
              which concrete improvements can be made for future studies that
              aim to investigate the relationship between social media use and
              well-being. (PsycInfo Database Record (c) 2022 APA, all rights
              reserved)",
  month    =  jun,
  year     =  2020
}

@ARTICLE{Chou_undated-kg,
  title  = "Toward Joint Language Modeling for Speech Units and Text",
  author = "Chou, Ju-Chieh and Chien, Chung-Ming and Hsu, Wei-Ning and Livescu,
            Karen and Babu, Arun and Conneau, Alexis and Baevski, Alexei and
            Auli, Michael"
}

@PHDTHESIS{Bailey2022-xn,
  title     = "Towards a Computational Model of Narrative on Social Media",
  author    = "Bailey, Anne",
  publisher = "digitalcommons.dartmouth.edu",
  abstract  = "This thesis describes a variety of approaches to developing a
               computational model of narrative on social media. Our goal is to
               use such a narrative model to identify efforts to manipulate
               public opinion on social media platforms like Twitter. We present
               a model in which narratives in a collection of tweets are
               represented as a graph. Elements from each tweet that are
               relevant to potential narratives are made into nodes in the
               graph; for this thesis, we populate graph nodes with tweets’
               authors, hashtags, named entities (people, locations,
               organizations, etc.,), and moral foundations (central moral
               values framing the discussion). Two nodes are connected with an
               edge if the narrative elements they represent appear together in
               one or more tweets, with the edge weight corresponding to the
               number of tweets in which these elements coincide. We then
               explore multiple possible deep learning and graph analysis
               methods for identifying narratives in a collection of tweets,
               including clustering of language embeddings, topic modeling,
               community detection and random walks on our narrative graph,
               training a graph neural network to identify narratives in the
               graph, and training a graph embedding model to generate vector
               embeddings of graph nodes. While much work still remains to be
               done in this area, several of our techniques, especially the
               generation and clustering of graph embeddings, were able to
               identify groups of related and connected nodes that might form
               the beginnings of narratives. Further study of these or other
               techniques could allow for the reliable identification of full
               narratives and information operations on social media.",
  year      =  2022,
  school    = "Dartmouth College"
}

@INPROCEEDINGS{Hanna2020-lh,
  title     = "Towards a critical race methodology in algorithmic fairness",
  author    = "Hanna, Alex and Denton, Emily and Smart, Andrew and Smith-Loud,
               Jamila",
  booktitle = "Proceedings of the 2020 Conference on Fairness, Accountability,
               and Transparency",
  publisher = "ACM",
  address   = "New York, NY, USA",
  pages     = "501--512",
  month     =  jan,
  year      =  2020
}

@ARTICLE{Hou2023-jy,
  title         = "Towards a Mechanistic Interpretation of Multi-Step Reasoning
                   Capabilities of Language Models",
  author        = "Hou, Yifan and Li, Jiaoda and Fei, Yu and Stolfo, Alessandro
                   and Zhou, Wangchunshu and Zeng, Guangtao and Bosselut,
                   Antoine and Sachan, Mrinmaya",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent work has shown that language models (LMs) have strong
                   multi-step (i.e., procedural) reasoning capabilities.
                   However, it is unclear whether LMs perform these tasks by
                   cheating with answers memorized from pretraining corpus, or,
                   via a multi-step reasoning mechanism. In this paper, we try
                   to answer this question by exploring a mechanistic
                   interpretation of LMs for multi-step reasoning tasks.
                   Concretely, we hypothesize that the LM implicitly embeds a
                   reasoning tree resembling the correct reasoning process
                   within it. We test this hypothesis by introducing a new
                   probing approach (called MechanisticProbe) that recovers the
                   reasoning tree from the model's attention patterns. We use
                   our probe to analyze two LMs: GPT-2 on a synthetic task (k-th
                   smallest element), and LLaMA on two simple language-based
                   reasoning tasks (ProofWriter \& AI2 Reasoning Challenge). We
                   show that MechanisticProbe is able to detect the information
                   of the reasoning tree from the model's attentions for most
                   examples, suggesting that the LM indeed is going through a
                   process of multi-step reasoning within its architecture in
                   many cases.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Sastre_undated-uj,
  title  = "Towards A Radical Body Positive: Reading The Online Body Positive
            Movement",
  author = "Sastre, Alexandra"
}

@INPROCEEDINGS{Lai2023-bw,
  title     = "Towards a Science of Human-{AI} Decision Making: An Overview of
               Design Space in Empirical Human-Subject Studies",
  author    = "Lai, Vivian and Chen, Chacha and Smith-Renner, Alison and Liao, Q
               Vera and Tan, Chenhao",
  booktitle = "Proceedings of the 2023 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1369--1385",
  abstract  = "AI systems are adopted in numerous domains due to their
               increasingly strong predictive performance. However, in
               high-stakes domains such as criminal justice and healthcare, full
               automation is often not desirable due to safety, ethical, and
               legal concerns, yet fully manual approaches can be inaccurate and
               time-consuming. As a result, there is growing interest in the
               research community to augment human decision making with AI
               assistance. Besides developing AI technologies for this purpose,
               the emerging field of human-AI decision making must embrace
               empirical approaches to form a foundational understanding of how
               humans interact and work with AI to make decisions. To invite and
               help structure research efforts towards a science of
               understanding and improving human-AI decision making, we survey
               recent literature of empirical human-subject studies on this
               topic. We summarize the study design choices made in over 100
               papers in three important aspects: (1) decision tasks, (2) AI
               assistance elements, and (3) evaluation metrics. For each aspect,
               we summarize current trends, discuss gaps in current practices of
               the field, and make a list of recommendations for future
               research. Our work highlights the need to develop common
               frameworks to account for the design and research spaces of
               human-AI decision making, so that researchers can make rigorous
               choices in study design, and the research community can build on
               each other’s work and produce generalizable scientific knowledge.
               We also hope this work will serve as a bridge for HCI and AI
               communities to work together to mutually shape the empirical
               science and computational technologies for human-AI decision
               making.",
  series    = "FAccT '23",
  month     =  jun,
  year      =  2023
}

@MISC{Wu_undated-re,
  title       = "Towards-Auditing-Large-Language-Models",
  author      = "Wu, Zekun",
  institution = "Github",
  abstract    = "Contribute to 981526092/Towards-Auditing-Large-Language-Models
                 development by creating an account on GitHub.",
  language    = "en"
}

@ARTICLE{Conmy2023-sw,
  title         = "Towards Automated Circuit Discovery for Mechanistic
                   Interpretability",
  author        = "Conmy, Arthur and Mavor-Parker, Augustine N and Lynch, Aengus
                   and Heimersheim, Stefan and Garriga-Alonso, Adrià",
  journal       = "arXiv [cs.LG]",
  abstract      = "Through considerable effort and intuition, several recent
                   works have reverse-engineered nontrivial behaviors of
                   transformer models. This paper systematizes the mechanistic
                   interpretability process they followed. First, researchers
                   choose a metric and dataset that elicit the desired model
                   behavior. Then, they apply activation patching to find which
                   abstract neural network units are involved in the behavior.
                   By varying the dataset, metric, and units under
                   investigation, researchers can understand the functionality
                   of each component. We automate one of the process' steps: to
                   identify the circuit that implements the specified behavior
                   in the model's computational graph. We propose several
                   algorithms and reproduce previous interpretability results to
                   validate them. For example, the ACDC algorithm rediscovered
                   5/5 of the component types in a circuit in GPT-2 Small that
                   computes the Greater-Than operation. ACDC selected 68 of the
                   32,000 edges in GPT-2 Small, all of which were manually found
                   by previous work. Our code is available at
                   https://github.com/ArthurConmy/Automatic-Circuit-Discovery.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Zhang2023-zk,
  title         = "Towards Best Practices of Activation Patching in Language
                   Models: Metrics and Methods",
  author        = "Zhang, Fred and Nanda, Neel",
  journal       = "arXiv [cs.LG]",
  abstract      = "Mechanistic interpretability seeks to understand the internal
                   mechanisms of machine learning models, where localization --
                   identifying the important model components -- is a key step.
                   Activation patching, also known as causal tracing or
                   interchange intervention, is a standard technique for this
                   task (Vig et al., 2020), but the literature contains many
                   variants with little consensus on the choice of
                   hyperparameters or methodology. In this work, we
                   systematically examine the impact of methodological details
                   in activation patching, including evaluation metrics and
                   corruption methods. In several settings of localization and
                   circuit discovery in language models, we find that varying
                   these hyperparameters could lead to disparate
                   interpretability results. Backed by empirical observations,
                   we give conceptual arguments for why certain metrics or
                   methods may be preferred. Finally, we provide recommendations
                   for the best practices of activation patching going forwards.",
  month         =  sep,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG"
}

@ARTICLE{Shen2024-tp,
  title         = "Towards Bidirectional Human-{AI} alignment: A systematic
                   review for clarifications, framework, and future directions",
  author        = "Shen, Hua and Knearem, Tiffany and Ghosh, Reshmi and Alkiek,
                   Kenan and Krishna, Kundan and Liu, Yachuan and Ma, Ziqiao and
                   Petridis, Savvas and Peng, Yi-Hao and Qiwei, Li and Rakshit,
                   Sushrita and Si, Chenglei and Xie, Yutong and Bigham, Jeffrey
                   P and Bentley, Frank and Chai, Joyce and Lipton, Zachary and
                   Mei, Qiaozhu and Mihalcea, Rada and Terry, Michael and Yang,
                   Diyi and Morris, Meredith Ringel and Resnick, Paul and
                   Jurgens, David",
  journal       = "arXiv [cs.HC]",
  abstract      = "Recent advancements in general-purpose AI have highlighted
                   the importance of guiding AI systems towards the intended
                   goals, ethical principles, and values of individuals and
                   groups, a concept broadly recognized as alignment. However,
                   the lack of clarified definitions and scopes of human-AI
                   alignment poses a significant obstacle, hampering
                   collaborative efforts across research domains to achieve this
                   alignment. In particular, ML- and philosophy-oriented
                   alignment research often views AI alignment as a static,
                   unidirectional process (i.e., aiming to ensure that AI
                   systems' objectives match humans) rather than an ongoing,
                   mutual alignment problem. This perspective largely neglects
                   the long-term interaction and dynamic changes of alignment.
                   To understand these gaps, we introduce a systematic review of
                   over 400 papers published between 2019 and January 2024,
                   spanning multiple domains such as Human-Computer Interaction
                   (HCI), Natural Language Processing (NLP), Machine Learning
                   (ML). We characterize, define and scope human-AI alignment.
                   From this, we present a conceptual framework of
                   ``Bidirectional Human-AI Alignment'' to organize the
                   literature from a human-centered perspective. This framework
                   encompasses both 1) conventional studies of aligning AI to
                   humans that ensures AI produces the intended outcomes
                   determined by humans, and 2) a proposed concept of aligning
                   humans to AI, which aims to help individuals and society
                   adjust to AI advancements both cognitively and behaviorally.
                   Additionally, we articulate the key findings derived from
                   literature analysis, including literature gaps and trends,
                   human values, and interaction techniques. To pave the way for
                   future studies, we envision three key challenges and give
                   recommendations for future research.",
  month         =  jun,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Shani2023-ep,
  title         = "Towards Concept-Aware Large Language Models",
  author        = "Shani, Chen and Vreeken, Jilles and Shahaf, Dafna",
  journal       = "arXiv [cs.CL]",
  abstract      = "Concepts play a pivotal role in various human cognitive
                   functions, including learning, reasoning and communication.
                   However, there is very little work on endowing machines with
                   the ability to form and reason with concepts. In particular,
                   state-of-the-art large language models (LLMs) work at the
                   level of tokens, not concepts. In this work, we analyze how
                   well contemporary LLMs capture human concepts and their
                   structure. We then discuss ways to develop concept-aware
                   LLMs, taking place at different stages of the pipeline. We
                   sketch a method for pretraining LLMs using concepts, and also
                   explore the simpler approach that uses the output of existing
                   LLMs. Despite its simplicity, our proof-of-concept is shown
                   to better match human intuition, as well as improve the
                   robustness of predictions. These preliminary results
                   underscore the promise of concept-aware LLMs.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Nguyen2023-jy,
  title         = "Towards Conceptualization of ``Fair Explanation'': Disparate
                   Impacts of anti-Asian Hate Speech Explanations on Content
                   Moderators",
  author        = "Nguyen, Tin and Xu, Jiannan and Roy, Aayushi and Daumé, III,
                   Hal and Carpuat, Marine",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent research at the intersection of AI explainability and
                   fairness has focused on how explanations can improve
                   human-plus-AI task performance as assessed by fairness
                   measures. We propose to characterize what constitutes an
                   explanation that is itself ``fair'' -- an explanation that
                   does not adversely impact specific populations. We formulate
                   a novel evaluation method of ``fair explanations'' using not
                   just accuracy and label time, but also psychological impact
                   of explanations on different user groups across many metrics
                   (mental discomfort, stereotype activation, and perceived
                   workload). We apply this method in the context of content
                   moderation of potential hate speech, and its differential
                   impact on Asian vs. non-Asian proxy moderators, across
                   explanation approaches (saliency map and counterfactual
                   explanation). We find that saliency maps generally perform
                   better and show less evidence of disparate impact (group) and
                   individual unfairness than counterfactual explanations.
                   Content warning: This paper contains examples of hate speech
                   and racially discriminatory language. The authors do not
                   support such content. Please consider your risk of discomfort
                   carefully before continuing reading!",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Hoeken2023-og,
  title     = "Towards Detecting Lexical Change of Hate Speech in Historical
               Data",
  author    = "Hoeken, Sanne and Spliethoff, Sophie and Schwandt, Silke and
               Zarrieß, Sina and Alacam, Özge",
  editor    = "Tahmasebi, Nina and Montariol, Syrielle and Dubossarsky, Haim and
               Kutuzov, Andrey and Hengchen, Simon and Alfter, David and Periti,
               Francesco and Cassotti, Pierluigi",
  booktitle = "Proceedings of the 4th Workshop on Computational Approaches to
               Historical Language Change",
  publisher = "Association for Computational Linguistics",
  address   = "Singapore",
  pages     = "100--111",
  abstract  = "The investigation of lexical change has predominantly focused on
               generic language evolution, not suited for detecting shifts in a
               particular domain, such as hate speech. Our study introduces the
               task of identifying changes in lexical semantics related to hate
               speech within historical texts. We present an interdisciplinary
               approach that brings together NLP and History, yielding a pilot
               dataset comprising 16th-century Early Modern English religious
               writings during the Protestant Reformation. We provide
               annotations for both semantic shifts and hatefulness on this data
               and, thereby, combine the tasks of Lexical Semantic Change
               Detection and Hate Speech Detection. Our framework and resulting
               dataset facilitate the evaluation of our applied methods,
               advancing the analysis of hate speech evolution.",
  month     =  dec,
  year      =  2023
}

@ARTICLE{Roy2023-iu,
  title         = "Towards Few-Shot Identification of Morality Frames using
                   In-Context Learning",
  author        = "Roy, Shamik and Nakshatri, Nishanth Sridhar and Goldwasser,
                   Dan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Data scarcity is a common problem in NLP, especially when the
                   annotation pertains to nuanced socio-linguistic concepts that
                   require specialized knowledge. As a result, few-shot
                   identification of these concepts is desirable. Few-shot
                   in-context learning using pre-trained Large Language Models
                   (LLMs) has been recently applied successfully in many NLP
                   tasks. In this paper, we study few-shot identification of a
                   psycho-linguistic concept, Morality Frames (Roy et al.,
                   2021), using LLMs. Morality frames are a representation
                   framework that provides a holistic view of the moral
                   sentiment expressed in text, identifying the relevant moral
                   foundation (Haidt and Graham, 2007) and at a finer level of
                   granularity, the moral sentiment expressed towards the
                   entities mentioned in the text. Previous studies relied on
                   human annotation to identify morality frames in text which is
                   expensive. In this paper, we propose prompting-based
                   approaches using pretrained Large Language Models for
                   identification of morality frames, relying only on few-shot
                   exemplars. We compare our models' performance with few-shot
                   RoBERTa and found promising results.",
  month         =  feb,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Rong2022-fo,
  title         = "Towards Human-centered Explainable {AI}: User Studies for
                   Model Explanations",
  author        = "Rong, Yao and Leemann, Tobias and Nguyen, Thai-Trang and
                   Fiedler, Lisa and Qian, Peizhu and Unhelkar, Vaibhav and
                   Seidel, Tina and Kasneci, Gjergji and Kasneci, Enkelejda",
  journal       = "arXiv [cs.AI]",
  abstract      = "Explainable AI (XAI) is widely viewed as a sine qua non for
                   ever-expanding AI research. A better understanding of the
                   needs of XAI users, as well as human-centered evaluations of
                   explainable models are both a necessity and a challenge. In
                   this paper, we explore how HCI and AI researchers conduct
                   user studies in XAI applications based on a systematic
                   literature review. After identifying and thoroughly analyzing
                   85 core papers with human-based XAI evaluations over the past
                   five years, we categorize them along the measured
                   characteristics of explanatory methods, namely trust,
                   understanding, fairness, usability, and human-AI team
                   performance. Our research shows that XAI is spreading more
                   rapidly in certain application domains, such as recommender
                   systems than in others, but that user evaluations are still
                   rather sparse and incorporate hardly any insights from
                   cognitive or social sciences. Based on a comprehensive
                   discussion of best practices, i.e., common models, design
                   choices, and measures in user studies, we propose practical
                   guidelines on designing and conducting user studies for XAI
                   researchers and practitioners. Lastly, this survey also
                   highlights several open research directions, particularly
                   linking psychological science and human-centered XAI.",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI"
}

@ARTICLE{Zhong2020-cr,
  title         = "Towards Persona-Based Empathetic Conversational Models",
  author        = "Zhong, Peixiang and Zhang, Chen and Wang, Hao and Liu, Yong
                   and Miao, Chunyan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Empathetic conversational models have been shown to improve
                   user satisfaction and task outcomes in numerous domains. In
                   Psychology, persona has been shown to be highly correlated to
                   personality, which in turn influences empathy. In addition,
                   our empirical analysis also suggests that persona plays an
                   important role in empathetic conversations. To this end, we
                   propose a new task towards persona-based empathetic
                   conversations and present the first empirical study on the
                   impact of persona on empathetic responding. Specifically, we
                   first present a novel large-scale multi-domain dataset for
                   persona-based empathetic conversations. We then propose
                   CoBERT, an efficient BERT-based response selection model that
                   obtains the state-of-the-art performance on our dataset.
                   Finally, we conduct extensive experiments to investigate the
                   impact of persona on empathetic responding. Notably, our
                   results show that persona improves empathetic responding more
                   when CoBERT is trained on empathetic conversations than
                   non-empathetic ones, establishing an empirical link between
                   persona and empathy in human conversations.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Liang2021-jl,
  title     = "Towards Understanding and Mitigating Social Biases in Language
               Models",
  author    = "Liang, Paul Pu and Wu, Chiyu and Morency, Louis-Philippe and
               Salakhutdinov, Ruslan",
  editor    = "Meila, Marina and Zhang, Tong",
  booktitle = "Proceedings of the 38th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  139,
  pages     = "6565--6576",
  abstract  = "As machine learning methods are deployed in real-world settings
               such as healthcare, legal systems, and social science, it is
               crucial to recognize how they shape social biases and stereotypes
               in these sensitive decision-making processes. Among such
               real-world deployments are large-scale pretrained language models
               (LMs) that can be potentially dangerous in manifesting
               undesirable representational biases - harmful biases resulting
               from stereotyping that propagate negative generalizations
               involving gender, race, religion, and other social constructs. As
               a step towards improving the fairness of LMs, we carefully define
               several sources of representational biases before proposing new
               benchmarks and metrics to measure them. With these tools, we
               propose steps towards mitigating social biases during text
               generation. Our empirical results and human evaluation
               demonstrate effectiveness in mitigating bias while retaining
               crucial contextual information for high-fidelity text generation,
               thereby pushing forward the performance-fairness Pareto frontier.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2021
}

@ARTICLE{Cao2023-xn,
  title         = "Toxicity Detection is {NOT} all you Need: Measuring the Gaps
                   to Supporting Volunteer Content Moderators",
  author        = "Cao, Yang Trista and Domingo, Lovely-Frances and Gilbert,
                   Sarah Ann and Mazurek, Michelle and Shilton, Katie and Daumé,
                   III, Hal",
  journal       = "arXiv [cs.CL]",
  abstract      = "Extensive efforts in automated approaches for content
                   moderation have been focused on developing models to identify
                   toxic, offensive, and hateful content -- with the aim of
                   lightening the load for moderators. Yet, it remains uncertain
                   whether improvements on those tasks truly address the needs
                   that moderators have in accomplishing their work. In this
                   paper, we surface the gaps between past research efforts that
                   have aimed to provide automation for aspects of the content
                   moderation task, and the needs of volunteer content
                   moderators. To do so, we conduct a model review on Hugging
                   Face to reveal the availability of models to cover various
                   moderation rules and guidelines. We further put
                   state-of-the-art LLMs to the test (GPT-4 and Llama-2),
                   evaluating how well these models perform in flagging
                   violations of platform rules. Overall, we observe a
                   non-trivial gap, as missing developed models and LLMs exhibit
                   low recall on a significant portion of the rules.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ging2021-id,
  title     = "Tracking the pilling pipeline: limitations, challenges And a call
               for new methodological frameworks in incel And manosphere
               research",
  author    = "Ging, D and Murphy, S",
  journal   = "AoIR Selected Papers of Internet Research",
  publisher = "spir.aoir.org",
  abstract  = "The manosphere is an online network of disparate formations,
               which are united in their antipathy toward feminism, their
               reliance on evolutionary psychology and their belief that …",
  year      =  2021
}

@ARTICLE{Zhang2020-pm,
  title         = "Trading Off Diversity and Quality in Natural Language
                   Generation",
  author        = "Zhang, Hugh and Duckworth, Daniel and Ippolito, Daphne and
                   Neelakantan, Arvind",
  journal       = "arXiv [cs.CL]",
  abstract      = "For open-ended language generation tasks such as storytelling
                   and dialogue, choosing the right decoding algorithm is
                   critical to controlling the tradeoff between generation
                   quality and diversity. However, there presently exists no
                   consensus on which decoding procedure is best or even the
                   criteria by which to compare them. We address these issues by
                   casting decoding as a multi-objective optimization problem
                   aiming to simultaneously maximize both response quality and
                   diversity. Our framework enables us to perform the first
                   large-scale evaluation of decoding methods along the entire
                   quality-diversity spectrum. We find that when diversity is a
                   priority, all methods perform similarly, but when quality is
                   viewed as more important, the recently proposed nucleus
                   sampling (Holtzman et al. 2019) outperforms all other
                   evaluated decoding algorithms. Our experiments also confirm
                   the existence of the `likelihood trap', the counter-intuitive
                   observation that high likelihood sequences are often
                   surprisingly low quality. We leverage our findings to create
                   and evaluate an algorithm called \emph{selective sampling}
                   which tractably approximates globally-normalized temperature
                   sampling.",
  month         =  apr,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Burns2017-ev,
  title     = "Training away bias: The differential effects of counterstereotype
               training and self-regulation on stereotype activation and
               application",
  author    = "Burns, Mason D and Monteith, Margo J and Parker, Laura R",
  journal   = "J. Exp. Soc. Psychol.",
  publisher = "Elsevier",
  volume    =  73,
  pages     = "97--110",
  abstract  = "A pressing issue concerns how to reduce stereotypic responses and
               discriminatory outcomes resulting from the operation of implicit
               biases. One possibility is that cognitive retraining, such as by
               repeatedly practicing counterstereotypes, can reduce implicit
               bias so that stereotype application will be reduced in turn.
               Another possibility involves motivated self-regulation, where
               people's awareness of their proneness to biased responses
               heightens negative self-directed affect, which in turn
               facilitates monitoring for biases and reduces stereotype
               application. These possibilities were tested across three
               experiments. In all experiments, participants who completed
               counterstereotype training subsequently scored lower on a measure
               of implicit bias, relative to untrained participants. In
               Experiments 1 and 2, counterstereotyping did not reduce
               subsequent stereotype application; in Experiment 3,
               counterstereotyping did reduce stereotype application, but this
               effect was not mediated by implicit bias scores. Participants in
               the motivated self-regulation condition (Experiments 2 \& 3) were
               primed with their proneness to respond in biased ways, which
               increased negative self-directed affect among participants more
               internally motivated to respond without bias. Participants'
               degree of negative self-directed affect was not consistently
               associated with implicit bias scores. However, greater negative
               self-directed affect was associated with reduced stereotype
               application (Experiment 2) and greater rejection of racist jokes
               (Experiment 3). These results suggest that reductions of implicit
               bias through counterstereotype training do not, in turn, lead to
               reduced stereotype application. In contrast, the results support
               the viability of motivated self-regulation interventions that
               facilitate awareness of bias and heighten negative self-directed
               affect, thus creating the motivation to self-regulate stereotype
               application.",
  month     =  nov,
  year      =  2017,
  keywords  = "Prejudice; Stereotyping; Implicit bias; Self-regulation;
               Counterstereotyping"
}

@ARTICLE{Ouyang_undated-dg,
  title  = "Training language models to follow instructions with human feedback",
  author = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and
            Wainwright, Carroll L and Mishkin, Pamela and Christiano, Peter
            Welinder Paul and Leike, Jan and Lowe, Ryan"
}

@ARTICLE{Ouyang2022-yv,
  title         = "Training language models to follow instructions with human
                   feedback",
  author        = "Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo
                   and Wainwright, Carroll L and Mishkin, Pamela and Zhang,
                   Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex
                   and Schulman, John and Hilton, Jacob and Kelton, Fraser and
                   Miller, Luke and Simens, Maddie and Askell, Amanda and
                   Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe,
                   Ryan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Making language models bigger does not inherently make them
                   better at following a user's intent. For example, large
                   language models can generate outputs that are untruthful,
                   toxic, or simply not helpful to the user. In other words,
                   these models are not aligned with their users. In this paper,
                   we show an avenue for aligning language models with user
                   intent on a wide range of tasks by fine-tuning with human
                   feedback. Starting with a set of labeler-written prompts and
                   prompts submitted through the OpenAI API, we collect a
                   dataset of labeler demonstrations of the desired model
                   behavior, which we use to fine-tune GPT-3 using supervised
                   learning. We then collect a dataset of rankings of model
                   outputs, which we use to further fine-tune this supervised
                   model using reinforcement learning from human feedback. We
                   call the resulting models InstructGPT. In human evaluations
                   on our prompt distribution, outputs from the 1.3B parameter
                   InstructGPT model are preferred to outputs from the 175B
                   GPT-3, despite having 100x fewer parameters. Moreover,
                   InstructGPT models show improvements in truthfulness and
                   reductions in toxic output generation while having minimal
                   performance regressions on public NLP datasets. Even though
                   InstructGPT still makes simple mistakes, our results show
                   that fine-tuning with human feedback is a promising direction
                   for aligning language models with human intent.",
  month         =  mar,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Geva2020-wh,
  title         = "Transformer Feed-Forward Layers Are Key-Value Memories",
  author        = "Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy,
                   Omer",
  journal       = "arXiv [cs.CL]",
  abstract      = "Feed-forward layers constitute two-thirds of a transformer
                   model's parameters, yet their role in the network remains
                   under-explored. We show that feed-forward layers in
                   transformer-based language models operate as key-value
                   memories, where each key correlates with textual patterns in
                   the training examples, and each value induces a distribution
                   over the output vocabulary. Our experiments show that the
                   learned patterns are human-interpretable, and that lower
                   layers tend to capture shallow patterns, while upper layers
                   learn more semantic ones. The values complement the keys'
                   input patterns by inducing output distributions that
                   concentrate probability mass on tokens likely to appear
                   immediately after each pattern, particularly in the upper
                   layers. Finally, we demonstrate that the output of a
                   feed-forward layer is a composition of its memories, which is
                   subsequently refined throughout the model's layers via
                   residual connections to produce the final output
                   distribution.",
  month         =  dec,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Ferrara2013-fc,
  title     = "Traveling trends: social butterflies or frequent fliers?",
  author    = "Ferrara, Emilio and Varol, Onur and Menczer, Filippo and
               Flammini, Alessandro",
  booktitle = "Proceedings of the first ACM conference on Online social networks",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "213--222",
  abstract  = "Trending topics are the online conversations that grab collective
               attention on social media. They are continually changing and
               often reflect exogenous events that happen in the real world.
               Trends are localized in space and time as they are driven by
               activity in specific geographic areas that act as sources of
               traffic and information flow. Taken independently, trends and
               geography have been discussed in recent literature on online
               social media; although, so far, little has been done to
               characterize the relation between trends and geography. Here we
               investigate more than eleven thousand topics that trended on
               Twitter in 63 main US locations during a period of 50 days in
               2013. This data allows us to study the origins and pathways of
               trends, how they compete for popularity at the local level to
               emerge as winners at the country level, and what dynamics
               underlie their production and consumption in different geographic
               areas. We identify two main classes of trending topics: those
               that surface locally, coinciding with three different geographic
               clusters (East coast, Midwest and Southwest); and those that
               emerge globally from several metropolitan areas, coinciding with
               the major air traffic hubs of the country. These hubs act as
               trendsetters, generating topics that eventually trend at the
               country level, and driving the conversation across the country.
               This poses an intriguing conjecture, drawing a parallel between
               the spread of information and diseases: Do trends travel faster
               by airplane than over the Internet?",
  series    = "COSN '13",
  month     =  oct,
  year      =  2013,
  keywords  = "social media, mobility, trens, twitter, geography"
}

@ARTICLE{Rusch2023-da,
  title     = "True colours or rainbow-washing exposed!? – company pride in and
               through digital and social media reviewed",
  author    = "Rusch, Michaela",
  journal   = "ILCEA",
  publisher = "OpenEdition",
  number    =  51,
  abstract  = "Following a noble cause can function as a basis for valuable
               marketing. Apart from the fact that selling products also belongs
               to this scheme, companies must individualise and re-create these
               products constantly. In this process, many companies seek great
               motivation in using the rainbow flag or the rainbow colours,
               especially around June—traditionally celebrated as Pride month—to
               promote and sell their products to customers who (like to)
               identify with these goods. However, lately, companies are also
               questioned about their genuine inclination toward causes
               connected to these colours as customers desire to know how far
               the product reflects the general support of the LGBTQ+ (Lesbian,
               Gay, Bisexual, Trans, Queer and beyond) community. This paper
               analyses therefore a selected number of random samples in a
               preliminary study using hashtags by companies as well as
               individuals on Instagram and written statements from companies’
               social media accounts and websites as a basis. Through their
               approach, the author aims to show how rainbow-washing is
               detected, communicated and criticised as customers become more
               aware of the difference between potential marketing traps and
               genuine support. As the results of this preliminary study have so
               far indicated, customers develop more awareness and also
               communicate positive as well as negative changes in cooperative
               behaviour. Through this article’s limited approach and frame,
               merely impulses can be given to research the topic within a
               broader corpus linguistic frame.",
  month     =  jun,
  year      =  2023
}

@ARTICLE{Perez2021-pu,
  title     = "True few-shot learning with language models",
  author    = "Perez, Ethan and Kiela, Douwe and Cho, Kyunghyun",
  journal   = "Adv. Neural Inf. Process. Syst.",
  publisher = "proceedings.neurips.cc",
  volume    =  34,
  pages     = "11054--11070",
  abstract  = "… learning . For example, Brown et al. [2] try prompts … few -
               shot learning relies on many labeled examples, so we argue that
               tuned few - shot learning does not qualify as few - shot learning
               . If …",
  year      =  2021
}

@ARTICLE{Mireshghallah2024-pz,
  title         = "Trust no bot: Discovering personal disclosures in human-{LLM}
                   conversations in the wild",
  author        = "Mireshghallah, Niloofar and Antoniak, Maria and More, Yash
                   and Choi, Yejin and Farnadi, Golnoosh",
  journal       = "arXiv [cs.CL]",
  abstract      = "Measuring personal disclosures made in human-chatbot
                   interactions can provide a better understanding of users' AI
                   literacy and facilitate privacy research for large language
                   models (LLMs). We run an extensive, fine-grained analysis on
                   the personal disclosures made by real users to commercial GPT
                   models, investigating the leakage of personally identifiable
                   and sensitive information. To understand the contexts in
                   which users disclose to chatbots, we develop a taxonomy of
                   tasks and sensitive topics, based on qualitative and
                   quantitative analysis of naturally occurring conversations.
                   We discuss these potential privacy harms and observe that:
                   (1) personally identifiable information (PII) appears in
                   unexpected contexts such as in translation or code editing
                   (48\% and 16\% of the time, respectively) and (2) PII
                   detection alone is insufficient to capture the sensitive
                   topics that are common in human-chatbot interactions, such as
                   detailed sexual preferences or specific drug use habits. We
                   believe that these high disclosure rates are of significant
                   importance for researchers and data curators, and we call for
                   the design of appropriate nudging mechanisms to help users
                   moderate their interactions.",
  month         =  jul,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Evans2021-eo,
  title         = "Truthful {AI}: Developing and governing {AI} that does not
                   lie",
  author        = "Evans, Owain and Cotton-Barratt, Owen and Finnveden, Lukas
                   and Bales, Adam and Balwit, Avital and Wills, Peter and
                   Righetti, Luca and Saunders, William",
  journal       = "arXiv [cs.CY]",
  abstract      = "In many contexts, lying -- the use of verbal falsehoods to
                   deceive -- is harmful. While lying has traditionally been a
                   human affair, AI systems that make sophisticated verbal
                   statements are becoming increasingly prevalent. This raises
                   the question of how we should limit the harm caused by AI
                   ``lies'' (i.e. falsehoods that are actively selected for).
                   Human truthfulness is governed by social norms and by laws
                   (against defamation, perjury, and fraud). Differences between
                   AI and humans present an opportunity to have more precise
                   standards of truthfulness for AI, and to have these standards
                   rise over time. This could provide significant benefits to
                   public epistemics and the economy, and mitigate risks of
                   worst-case AI futures. Establishing norms or laws of AI
                   truthfulness will require significant work to: (1) identify
                   clear truthfulness standards; (2) create institutions that
                   can judge adherence to those standards; and (3) develop AI
                   systems that are robustly truthful. Our initial proposals for
                   these areas include: (1) a standard of avoiding ``negligent
                   falsehoods'' (a generalisation of lies that is easier to
                   assess); (2) institutions to evaluate AI systems before and
                   after real-world deployment; and (3) explicitly training AI
                   systems to be truthful via curated datasets and human
                   interaction. A concerning possibility is that evaluation
                   mechanisms for eventual truthfulness standards could be
                   captured by political interests, leading to harmful
                   censorship and propaganda. Avoiding this might take careful
                   attention. And since the scale of AI speech acts might grow
                   dramatically over the coming decades, early truthfulness
                   standards might be particularly important because of the
                   precedents they set.",
  month         =  oct,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Uchendu2021-on,
  title         = "{TURINGBENCH}: A Benchmark Environment for Turing Test in the
                   Age of Neural Text Generation",
  author        = "Uchendu, Adaku and Ma, Zeyu and Le, Thai and Zhang, Rui and
                   Lee, Dongwon",
  journal       = "arXiv [cs.CL]",
  abstract      = "Recent progress in generative language models has enabled
                   machines to generate astonishingly realistic texts. While
                   there are many legitimate applications of such models, there
                   is also a rising need to distinguish machine-generated texts
                   from human-written ones (e.g., fake news detection). However,
                   to our best knowledge, there is currently no benchmark
                   environment with datasets and tasks to systematically study
                   the so-called ``Turing Test'' problem for neural text
                   generation methods. In this work, we present the TuringBench
                   benchmark environment, which is comprised of (1) a dataset
                   with 200K human- or machine-generated samples across 20
                   labels {Human, GPT-1, GPT-2\_small, GPT-2\_medium,
                   GPT-2\_large, GPT-2\_xl, GPT-2\_PyTorch, GPT-3, GROVER\_base,
                   GROVER\_large, GROVER\_mega, CTRL, XLM, XLNET\_base,
                   XLNET\_large, FAIR\_wmt19, FAIR\_wmt20, TRANSFORMER\_XL,
                   PPLM\_distil, PPLM\_gpt2}, (2) two benchmark tasks -- i.e.,
                   Turing Test (TT) and Authorship Attribution (AA), and (3) a
                   website with leaderboards. Our preliminary experimental
                   results using TuringBench show that FAIR\_wmt20 and GPT-3 are
                   the current winners, among all language models tested, in
                   generating the most human-like indistinguishable texts with
                   the lowest F1 score by five state-of-the-art TT detection
                   models. The TuringBench is available at:
                   https://turingbench.ist.psu.edu/",
  month         =  sep,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Rottger2022-rs,
  title     = "Two contrasting data annotation paradigms for subjective {NLP}
               tasks",
  author    = "Rottger, Paul and Vidgen, Bertie and Hovy, Dirk and
               Pierrehumbert, Janet",
  booktitle = "Proceedings of the 2022 Conference of the North American Chapter
               of the Association for Computational Linguistics: Human Language
               Technologies",
  publisher = "Association for Computational Linguistics",
  address   = "Stroudsburg, PA, USA",
  abstract  = "It is argued that dataset creators should explicitly aim for one
               or the other of the descriptive or prescriptive paradigms for
               data annotation to facilitate the intended use of their dataset.
               Labelled data is the foundation of most natural language
               processing tasks. However, labelling data is difficult and there
               often are diverse valid beliefs about what the correct data
               labels should be. So far, dataset creators have acknowledged
               annotator subjectivity, but rarely actively managed it in the
               annotation process. This has led to partly-subjective datasets
               that fail to serve a clear downstream use. To address this issue,
               we propose two contrasting paradigms for data annotation. The
               descriptive paradigm encourages annotator subjectivity, whereas
               the prescriptive paradigm discourages it. Descriptive annotation
               allows for the surveying and modelling of different beliefs,
               whereas prescriptive annotation enables the training of models
               that consistently apply one belief. We discuss benefits and
               challenges in implementing both paradigms, and argue that dataset
               creators should explicitly aim for one or the other to facilitate
               the intended use of their dataset. Lastly, we conduct an
               annotation experiment using hate speech data that illustrates the
               contrast between the two paradigms.",
  year      =  2022,
  language  = "en"
}

@ARTICLE{CuiUnknown-ee,
  title     = "Ultrafeedback: Boosting language models with high-quality
               feedback",
  author    = "Cui, G and Yuan, L and Ding, N and Yao, G and Zhu, W and Ni, Y
               and {others}",
  publisher = "arxiv.org"
}

@ARTICLE{CuiUnknown-mm,
  title     = "{ULTRAFEEDBACK}: Boosting Language Models with Scaled {AI}
               Feedback",
  author    = "Cui, G and Yuan, L and Ding, N and Yao, G and He, B and Zhu, W
               and {others}",
  publisher = "openreview.net"
}

@ARTICLE{Jussim2018-um,
  title     = "Unasked questions about stereotype accuracy",
  author    = "Jussim, L and Stevens, Sean T and Honeycutt, Nathan",
  journal   = "Arch. Sci. Psychol.",
  publisher = "psycnet.apa.org",
  volume    =  6,
  pages     = "214--229",
  abstract  = "In this paper, we argue that there are many unanswered questions
               crucial to scientific understanding about stereotypes and
               stereotype accuracy. Scientists do not always engage in purely
               impartial search for objective truths, because, like other
               people, they are subject to biases in thinking, motivations to
               find certain particular results, and social norms regarding what
               is and is not an acceptable topic or finding. This paper suggests
               that these factors conspired to prevent psychologists from asking
               serious questions about stereotype accuracy for decades, and may
               help explain why many reviews of stereotypes reach conclusions in
               the absence of evidence, or, sometimes, in the face of evidence
               completely disconfirming those conclusions. We review the history
               of the first unasked question in this area, “Are stereotypes
               inaccurate?” which went unaddressed for about 70 years after the
               initial social science interest in stereotypes. Current unasked
               questions include: 1. When and how does relying on a stereotype
               increase the accuracy of person perception? 2. Why are some
               stereotypes more accurate than others 3. How accurate are
               implicit stereotypes? 4. Do people ever actually ignore
               individuals’ personal characteristics when perceiving,
               evaluating, and judging them? We conclude with testable
               hypotheses about the sources of not asking certain questions, and
               with recommendations for overcoming scientific biases and blind
               spots in research on stereotypes.",
  month     =  aug,
  year      =  2018
}

@INPROCEEDINGS{Asprino2022-sw,
  title     = "Uncovering Values: Detecting Latent Moral Content from Natural
               Language with Explainable and Non-Trained Methods",
  author    = "Asprino, Luigi and Bulla, Luana and De Giorgis, Stefano and
               Gangemi, Aldo and Marinucci, Ludovica and Mongiovi, Misael",
  booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd
               Workshop on Knowledge Extraction and Integration for Deep
               Learning Architectures",
  publisher = "Association for Computational Linguistics",
  address   = "Dublin, Ireland and Online",
  pages     = "33--41",
  abstract  = "Moral values as commonsense norms shape our everyday individual
               and community behavior. The possibility to extract moral attitude
               rapidly from natural language is an appealing perspective that
               would enable a deeper understanding of social interaction
               dynamics and the individual cognitive and behavioral dimension.
               In this work we focus on detecting moral content from natural
               language and we test our methods on a corpus of tweets previously
               labeled as containing moral values or violations, according to
               Moral Foundation Theory. We develop and compare two different
               approaches: (i) a frame-based symbolic value detector based on
               knowledge graphs and (ii) a zero-shot machine learning model
               fine-tuned on a task of Natural Language Inference (NLI) and a
               task of emotion detection. The final outcome from our work
               consists in two approaches meant to perform without the need for
               prior training process on a moral value detection task.",
  month     =  may,
  year      =  2022
}

@ARTICLE{Mohanty2003-jv,
  title     = "“Under Western Eyes” Revisited: Feminist Solidarity through
               Anticapitalist Struggles",
  author    = "Mohanty, Chandra Talpade",
  journal   = "Signs",
  publisher = "The University of Chicago Press",
  volume    =  28,
  number    =  2,
  pages     = "499--535",
  year      =  2003
}

@ARTICLE{Fraser2021-pr,
  title         = "Understanding and Countering Stereotypes: A Computational
                   Approach to the Stereotype Content Model",
  author        = "Fraser, Kathleen C and Nejadgholi, Isar and Kiritchenko,
                   Svetlana",
  journal       = "arXiv [cs.CY]",
  abstract      = "Stereotypical language expresses widely-held beliefs about
                   different social categories. Many stereotypes are overtly
                   negative, while others may appear positive on the surface,
                   but still lead to negative consequences. In this work, we
                   present a computational approach to interpreting stereotypes
                   in text through the Stereotype Content Model (SCM), a
                   comprehensive causal theory from social psychology. The SCM
                   proposes that stereotypes can be understood along two primary
                   dimensions: warmth and competence. We present a method for
                   defining warmth and competence axes in semantic embedding
                   space, and show that the four quadrants defined by this
                   subspace accurately represent the warmth and competence
                   concepts, according to annotated lexicons. We then apply our
                   computational SCM model to textual stereotype data and show
                   that it compares favourably with survey-based studies in the
                   psychological literature. Furthermore, we explore various
                   strategies to counter stereotypical beliefs with
                   anti-stereotypes. It is known that countering stereotypes
                   with anti-stereotypical examples is one of the most effective
                   ways to reduce biased thinking, yet the problem of generating
                   anti-stereotypes has not been previously studied. Thus, a
                   better understanding of how to generate realistic and
                   effective anti-stereotypes can contribute to addressing
                   pressing societal concerns of stereotyping, prejudice, and
                   discrimination.",
  month         =  jun,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@INPROCEEDINGS{Koh2017-ti,
  title     = "Understanding Black-box Predictions via Influence Functions",
  author    = "Koh, Pang Wei and Liang, Percy",
  editor    = "Precup, Doina and Teh, Yee Whye",
  booktitle = "Proceedings of the 34th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  70,
  pages     = "1885--1894",
  abstract  = "How can we explain the predictions of a black-box model? In this
               paper, we use influence functions — a classic technique from
               robust statistics — to trace a model’s prediction through the
               learning algorithm and back to its training data, thereby
               identifying training points most responsible for a given
               prediction. To scale up influence functions to modern machine
               learning settings, we develop a simple, efficient implementation
               that requires only oracle access to gradients and Hessian-vector
               products. We show that even on non-convex and non-differentiable
               models where the theory breaks down, approximations to influence
               functions can still provide valuable information. On linear
               models and convolutional neural networks, we demonstrate that
               influence functions are useful for multiple purposes:
               understanding model behavior, debugging models, detecting dataset
               errors, and even creating visually-indistinguishable training-set
               attacks.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2017
}

@ARTICLE{Chen2019-vw,
  title         = "Understanding Dataset Design Choices for Multi-hop Reasoning",
  author        = "Chen, Jifan and Durrett, Greg",
  journal       = "arXiv [cs.CL]",
  abstract      = "Learning multi-hop reasoning has been a key challenge for
                   reading comprehension models, leading to the design of
                   datasets that explicitly focus on it. Ideally, a model should
                   not be able to perform well on a multi-hop question answering
                   task without doing multi-hop reasoning. In this paper, we
                   investigate two recently proposed datasets, WikiHop and
                   HotpotQA. First, we explore sentence-factored models for
                   these tasks; by design, these models cannot do multi-hop
                   reasoning, but they are still able to solve a large number of
                   examples in both datasets. Furthermore, we find spurious
                   correlations in the unmasked version of WikiHop, which make
                   it easy to achieve high performance considering only the
                   questions and answers. Finally, we investigate one key
                   difference between these datasets, namely span-based vs.
                   multiple-choice formulations of the QA task. Multiple-choice
                   versions of both datasets can be easily gamed, and two models
                   we examine only marginally exceed a baseline in this setting.
                   Overall, while these datasets are useful testbeds,
                   high-performing models may not be learning as much multi-hop
                   reasoning as previously thought.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Cheng_undated-on,
  title  = "Understanding Harmful Body Metaphors and Ideals on {TikTok}",
  author = "Cheng, Myra and Gligoric, Kristina and Jurafsky, Dan"
}

@ARTICLE{Saphra2018-it,
  title         = "Understanding Learning Dynamics Of Language Models with
                   {SVCCA}",
  author        = "Saphra, Naomi and Lopez, Adam",
  journal       = "arXiv [cs.CL]",
  abstract      = "Research has shown that neural models implicitly encode
                   linguistic features, but there has been no research showing
                   \emph{how} these encodings arise as the models are trained.
                   We present the first study on the learning dynamics of neural
                   language models, using a simple and flexible analysis method
                   called Singular Vector Canonical Correlation Analysis
                   (SVCCA), which enables us to compare learned representations
                   across time and across models, without the need to evaluate
                   directly on annotated data. We probe the evolution of
                   syntactic, semantic, and topic representations and find that
                   part-of-speech is learned earlier than topic; that recurrent
                   layers become more similar to those of a tagger during
                   training; and embedding layers less similar. Our results and
                   methods could inform better learning algorithms for NLP
                   models, possibly to incorporate linguistic information more
                   effectively.",
  month         =  nov,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Russo2023-ea,
  title     = "Understanding Online Migration Decisions Following the Banning of
               Radical Communities",
  author    = "Russo, Giuseppe and Horta Ribeiro, Manoel and Casiraghi, Giona
               and Verginer, Luca",
  booktitle = "Proceedings of the 15th ACM Web Science Conference 2023",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "251--259",
  abstract  = "The proliferation of radical online communities and their violent
               offshoots has sparked great societal concern. However, the
               current practice of banning such communities from mainstream
               platforms has unintended consequences: (i) the further
               radicalization of their members in fringe platforms where they
               migrate; and (ii) the spillover of harmful content from fringe
               back onto mainstream platforms. Here, in a large observational
               study on two banned subreddits, r/The\_Donald and
               r/fatpeoplehate, we examine how factors associated with the RECRO
               radicalization framework relate to users’ migration decisions.
               Specifically, we quantify how these factors affect users’
               decisions to post on fringe platforms and, for those who do,
               whether they continue posting on the mainstream platform. Our
               results show that individual-level factors, those relating to the
               behavior of users, are associated with the decision to post on
               the fringe platform. Whereas social-level factors, users’
               connection with the radical community, only affect the propensity
               to be coactive on both platforms. Overall, our findings pave the
               way for evidence-based moderation policies, as the decisions to
               migrate and remain coactive amplify unintended consequences of
               community bans.",
  series    = "WebSci '23",
  month     =  apr,
  year      =  2023
}

@ARTICLE{Gandhi2023-ot,
  title         = "Understanding Social Reasoning in Language Models with
                   Language Models",
  author        = "Gandhi, Kanishk and Fränken, Jan-Philipp and Gerstenberg,
                   Tobias and Goodman, Noah D",
  journal       = "arXiv [cs.CL]",
  abstract      = "As Large Language Models (LLMs) become increasingly
                   integrated into our everyday lives, understanding their
                   ability to comprehend human mental states becomes critical
                   for ensuring effective interactions. However, despite the
                   recent attempts to assess the Theory-of-Mind (ToM) reasoning
                   capabilities of LLMs, the degree to which these models can
                   align with human ToM remains a nuanced topic of exploration.
                   This is primarily due to two distinct challenges: (1) the
                   presence of inconsistent results from previous evaluations,
                   and (2) concerns surrounding the validity of existing
                   evaluation methodologies. To address these challenges, we
                   present a novel framework for procedurally generating
                   evaluations with LLMs by populating causal templates. Using
                   our framework, we create a new social reasoning benchmark
                   (BigToM) for LLMs which consists of 25 controls and 5,000
                   model-written evaluations. We find that human participants
                   rate the quality of our benchmark higher than previous
                   crowd-sourced evaluations and comparable to expert-written
                   evaluations. Using BigToM, we evaluate the social reasoning
                   capabilities of a variety of LLMs and compare model
                   performances with human performance. Our results suggest that
                   GPT4 has ToM capabilities that mirror human inference
                   patterns, though less reliable, while other LLMs struggle.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Brunet2019-xx,
  title     = "Understanding the Origins of Bias in Word Embeddings",
  author    = "Brunet, Marc-Etienne and Alkalay-Houlihan, Colleen and Anderson,
               Ashton and Zemel, Richard",
  editor    = "Chaudhuri, Kamalika and Salakhutdinov, Ruslan",
  booktitle = "Proceedings of the 36th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  97,
  pages     = "803--811",
  abstract  = "Popular word embedding algorithms exhibit stereotypical biases,
               such as gender bias. The widespread use of these algorithms in
               machine learning systems can amplify stereotypes in important
               contexts. Although some methods have been developed to mitigate
               this problem, how word embedding biases arise during training is
               poorly understood. In this work we develop a technique to address
               this question. Given a word embedding, our method reveals how
               perturbing the training corpus would affect the resulting
               embedding bias. By tracing the origins of word embedding bias
               back to the original training documents, one can identify subsets
               of documents whose removal would most reduce bias. We demonstrate
               our methodology on Wikipedia and New York Times corpora, and find
               it to be very accurate.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2019
}

@ARTICLE{Van_der_Wal2024-ov,
  title    = "Undesirable Biases in {NLP}: Addressing Challenges of Measurement",
  author   = "van der Wal, Oskar and Bachmann, Dominik and Leidinger, Alina and
              van Maanen, Leendert and Zuidema, Willem and Schulz, Katrin",
  journal  = "jair",
  volume   =  79,
  pages    = "1--40",
  abstract = "As Large Language Models and Natural Language Processing (NLP)
              technology rapidly develop and spread into daily life, it becomes
              crucial to anticipate how their use could harm people. One problem
              that has received a lot of attention in recent years is that this
              technology has displayed harmful biases, from generating
              derogatory stereotypes to producing disparate outcomes for
              different social groups. Although a lot of effort has been
              invested in assessing and mitigating these biases, our methods of
              measuring the biases of NLP models have serious problems and it is
              often unclear what they actually measure. In this paper, we
              provide an interdisciplinary approach to discussing the issue of
              NLP model bias by adopting the lens of psychometrics — a field
              specialized in the measurement of concepts like bias that are not
              directly observable. In particular, we will explore two central
              notions from psychometrics, the construct validity and the
              reliability of measurement tools, and discuss how they can be
              applied in the context of measuring model bias. Our goal is to
              provide NLP practitioners with methodological tools for designing
              better bias measures, and to inspire them more generally to
              explore tools from psychometrics when working on bias measurement
              tools. This article appears in the AI \& Society track.",
  month    =  jan,
  year     =  2024,
  keywords = "neural networks; natural language",
  language = "en"
}

@ARTICLE{Flores2015-kf,
  title     = "Undoing Appropriateness: Raciolinguistic Ideologies and Language
               Diversity in Education",
  author    = "Flores, Nelson and Rosa, Jonathan",
  journal   = "Harv. Educ. Rev.",
  publisher = "Allen Press",
  volume    =  85,
  number    =  2,
  pages     = "149--171",
  abstract  = "In this article, Nelson Flores and Jonathan Rosa critique
               appropriateness-based approaches to language diversity in
               education. Those who subscribe to these approaches conceptualize
               standardized linguistic practices as an objective set of
               linguistic forms that are appropriate for an academic setting. In
               contrast, Flores and Rosa highlight the raciolinguistic
               ideologies through which racialized bodies come to be constructed
               as engaging in appropriately academic linguistic practices.
               Drawing on theories of language ideologies and racialization,
               they offer a perspective from which students classified as
               long-term English learners, heritage language learners, and
               Standard English learners can be understood to inhabit a shared
               racial positioning that frames their linguistic practices as
               deficient regardless of how closely they follow supposed rules of
               appropriateness. The authors illustrate how appropriateness-based
               approaches to language education are implicated in the
               reproduction of racial normativity by expecting
               language-minoritized students to model their linguistic practices
               after the white speaking subject despite the fact that the white
               listening subject continues to perceive their language use in
               racialized ways. They conclude with a call for reframing language
               diversity in education away from a discourse of appropriateness
               toward one that seeks to denaturalize standardized linguistic
               categories.",
  month     =  jun,
  year      =  2015,
  language  = "en"
}

@BOOK{Butler2004-fd,
  title     = "Undoing Gender",
  author    = "Butler, Judith",
  publisher = "Psychology Press",
  abstract  = "Undoing Gender constitutes Judith Butler's recent reflections on
               gender and sexuality, focusing on new kinship, psychoanalysis and
               the incest taboo, transgender, intersex, diagnostic categories,
               social violence, and the tasks of social transformation. In terms
               that draw from feminist and queer theory, Butler considers the
               norms that govern--and fail to govern--gender and sexuality as
               they relate to the constraints on recognizable personhood. The
               book constitutes a reconsideration of her earlier view on gender
               performativity from Gender Trouble. In this work, the critique of
               gender norms is clearly situated within the framework of human
               persistence and survival. And to ``do'' one's gender in certain
               ways sometimes implies ``undoing'' dominant notions of
               personhood. She writes about the ``New Gender Politics'' that has
               emerged in recent years, a combination of movements concerned
               with transgender, transsexuality, intersex, and their complex
               relations to feminist and queer theory.",
  year      =  2004,
  language  = "en"
}

@ARTICLE{Gandikota2023-td,
  title         = "Unified Concept Editing in diffusion models",
  author        = "Gandikota, Rohit and Orgad, Hadas and Belinkov, Yonatan and
                   Materzyńska, Joanna and Bau, David",
  journal       = "arXiv [cs.CV]",
  pages         = "5111--5120",
  abstract      = "Text-to-image models suffer from various safety issues that
                   may limit their suitability for deployment. Previous methods
                   have separately addressed individual issues of bias,
                   copyright, and offensive content in text-to-image models.
                   However, in the real world, all of these issues appear
                   simultaneously in the same model. We present a method that
                   tackles all issues with a single approach. Our method,
                   Unified Concept Editing (UCE), edits the model without
                   training using a closed-form solution, and scales seamlessly
                   to concurrent edits on text-conditional diffusion models. We
                   demonstrate scalable simultaneous debiasing, style erasure,
                   and content moderation by editing text-to-image projections,
                   and we present extensive experiments demonstrating improved
                   efficacy and scalability over prior work. Our code is
                   available at https://unified.baulab.info",
  month         =  aug,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Hashimoto2019-km,
  title         = "Unifying Human and Statistical Evaluation for Natural
                   Language Generation",
  author        = "Hashimoto, Tatsunori B and Zhang, Hugh and Liang, Percy",
  journal       = "arXiv [cs.CL]",
  abstract      = "How can we measure whether a natural language generation
                   system produces both high quality and diverse outputs? Human
                   evaluation captures quality but not diversity, as it does not
                   catch models that simply plagiarize from the training set. On
                   the other hand, statistical evaluation (i.e., perplexity)
                   captures diversity but not quality, as models that
                   occasionally emit low quality samples would be insufficiently
                   penalized. In this paper, we propose a unified framework
                   which evaluates both diversity and quality, based on the
                   optimal error rate of predicting whether a sentence is human-
                   or machine-generated. We demonstrate that this error rate can
                   be efficiently estimated by combining human and statistical
                   evaluation, using an evaluation metric which we call HUSE. On
                   summarization and chit-chat dialogue, we show that (i) HUSE
                   detects diversity defects which fool pure human evaluation
                   and that (ii) techniques such as annealing for improving
                   quality actually decrease HUSE due to decreased diversity.",
  month         =  apr,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Eenennaam2004-af,
  title    = "University of California, Davis",
  author   = "Eenennaam, A V",
  abstract = "Perovskite oxides span a diverse range of functional properties
              such as ferromagnetism, superconductivity, and ferroelectricity,
              which makes them promising candidate materials for applications
              such as sensors, energy conversion and data storage devices. The
              structural compatibility of perovskite oxides allows them to be
              epitaxially grown in complex heterostructures such as
              superlattices with a large density of interfaces where the
              interplay between spin, charge, orbital, and lattice degrees of
              freedom gives rise to new behaviors. The ferromagnetic
              (FM)/antiferromagnetic (AF) interface is particularly interesting
              due to exchange coupling which is not only of interest for
              fundamental research but also is of great significance for
              industrial applications.",
  year     =  2004
}

@ARTICLE{Li2020-cw,
  title         = "{UnQovering} Stereotyping Biases via Underspecified Questions",
  author        = "Li, Tao and Khot, Tushar and Khashabi, Daniel and Sabharwal,
                   Ashish and Srikumar, Vivek",
  journal       = "arXiv [cs.CL]",
  abstract      = "While language embeddings have been shown to have
                   stereotyping biases, how these biases affect downstream
                   question answering (QA) models remains unexplored. We present
                   UNQOVER, a general framework to probe and quantify biases
                   through underspecified questions. We show that a naive use of
                   model scores can lead to incorrect bias estimates due to two
                   forms of reasoning errors: positional dependence and question
                   independence. We design a formalism that isolates the
                   aforementioned errors. As case studies, we use this metric to
                   analyze four important classes of stereotypes: gender,
                   nationality, ethnicity, and religion. We probe five
                   transformer-based QA models trained on two QA datasets, along
                   with their underlying language models. Our broad study
                   reveals that (1) all these models, with and without
                   fine-tuning, have notable stereotyping biases in these
                   classes; (2) larger models often have higher bias; and (3)
                   the effect of fine-tuning on bias varies strongly with the
                   dataset and the model size.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wambsganss2023-hm,
  title         = "Unraveling Downstream Gender Bias from Large Language Models:
                   A Study on {AI} Educational Writing Assistance",
  author        = "Wambsganss, Thiemo and Su, Xiaotian and Swamy, Vinitra and
                   Neshaei, Seyed Parsa and Rietsche, Roman and Käser, Tanja",
  journal       = "arXiv [cs.CL]",
  abstract      = "Large Language Models (LLMs) are increasingly utilized in
                   educational tasks such as providing writing suggestions to
                   students. Despite their potential, LLMs are known to harbor
                   inherent biases which may negatively impact learners.
                   Previous studies have investigated bias in models and data
                   representations separately, neglecting the potential impact
                   of LLM bias on human writing. In this paper, we investigate
                   how bias transfers through an AI writing support pipeline. We
                   conduct a large-scale user study with 231 students writing
                   business case peer reviews in German. Students are divided
                   into five groups with different levels of writing support:
                   one classroom group with feature-based suggestions and four
                   groups recruited from Prolific -- a control group with no
                   assistance, two groups with suggestions from fine-tuned GPT-2
                   and GPT-3 models, and one group with suggestions from
                   pre-trained GPT-3.5. Using GenBit gender bias analysis, Word
                   Embedding Association Tests (WEAT), and Sentence Embedding
                   Association Test (SEAT) we evaluate the gender bias at
                   various stages of the pipeline: in model embeddings, in
                   suggestions generated by the models, and in reviews written
                   by students. Our results demonstrate that there is no
                   significant difference in gender bias between the resulting
                   peer reviews of groups with and without LLM suggestions. Our
                   research is therefore optimistic about the use of AI writing
                   support in the classroom, showcasing a context where bias in
                   LLMs does not transfer to students' responses.",
  month         =  nov,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Henner2023-gg,
  title    = "Unsettling Languages, Unruly Bodyminds: A Crip Linguistics
              Manifesto",
  author   = "Henner, Jon and Robinson, Octavian",
  journal  = "JCSCD",
  volume   =  1,
  number   =  1,
  pages    = "7--37",
  abstract = "We introduce Crip Linguistics as a theoretical and abolitionist
              framework. People use languages in different ways. Some people use
              language to help find other people like themselves. Many people
              use language in specific ways because of how their body and mind
              work. Sometimes a person’s material conditions, and environment
              forces them to use language in a certain way. When someone
              languages outside of what people think is normal, others can think
              they are bad with language, or are not as smart as someone else.
              No one is actually ‘bad with language.’ We want to help people
              understand that no language is bad. It is okay to want to change
              your language use if it will make you feel better. No one should
              make you feel badly about your language. We need a bigger and more
              flexible understanding of what language is.",
  month    =  may,
  year     =  2023,
  keywords = "languaging; Crip Linguistics; disability; multi-modality",
  language = "en"
}

@ARTICLE{Rosa2017-mj,
  title     = "Unsettling race and language: Toward a raciolinguistic
               perspective",
  author    = "Rosa, J and Flores, N",
  journal   = "Lang. Soc.",
  publisher = "cambridge.org",
  abstract  = "This article presents what we term a raciolinguistic perspective,
               which theorizes the historical and contemporary co-naturalization
               of language and race. Rather than taking for …",
  year      =  2017
}

@ARTICLE{Chambers2008-ff,
  title     = "Unsupervised learning of narrative event chains",
  author    = "Chambers, N and Jurafsky, D",
  journal   = "Proceedings of ACL-08: HLT",
  publisher = "aclanthology.org",
  abstract  = "Hand-coded scripts were used in the 1970-80s as knowledge
               backbones that enabled inference and other NLP tasks requiring
               deep semantic knowledge. We propose …",
  year      =  2008
}

@INPROCEEDINGS{DiFranzo2018-fv,
  title     = "Upstanding by Design: Bystander Intervention in Cyberbullying",
  author    = "DiFranzo, Dominic and Taylor, Samuel Hardman and Kazerooni,
               Franccesca and Wherry, Olivia D and Bazarova, Natalya N",
  booktitle = "Proceedings of the 2018 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Paper 211",
  pages     = "1--12",
  abstract  = "Although bystander intervention can mitigate the negative effects
               of cyberbullying, few bystanders ever attempt to intervene. In
               this study, we explored the effects of interface design on
               bystander intervention using a simulated custom-made social media
               platform. Participants took part in a three-day, in-situ
               experiment, in which they were exposed to several cyberbullying
               incidents. Depending on the experimental condition, they received
               different information about the audience size and viewing
               notifications intended to increase a sense of personal
               responsibility in bystanders. Results indicated that bystanders
               were more likely to intervene indirectly than directly, and
               information about the audience size and viewership increased the
               likelihood of flagging cyberbullying posts through serial
               mediation of public surveillance, accountability, and personal
               responsibility. The study has implications for understanding
               bystander effect in cyberbullying, and how to develop design
               solutions to encourage bystander intervention in social media.",
  series    = "CHI '18",
  month     =  apr,
  year      =  2018,
  keywords  = "bystander intervention, social networking sites, cyberbullying"
}

@INPROCEEDINGS{Steed2022-qt,
  title     = "{U}pstream {M}itigation {I}s \textit{ {N}ot} {A}ll {Y}ou {N}eed:
               {T}esting the {B}ias {T}ransfer {H}ypothesis in {P}re-{T}rained
               {L}anguage {M}odels",
  author    = "Steed, Ryan and Panda, Swetasudha and Kobren, Ari and Wick,
               Michael",
  editor    = "Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline",
  booktitle = "Proceedings of the 60th Annual Meeting of the Association for
               Computational Linguistics (Volume 1: Long Papers)",
  publisher = "Association for Computational Linguistics",
  address   = "Dublin, Ireland",
  pages     = "3524--3542",
  abstract  = "A few large, homogenous, pre-trained models undergird many
               machine learning systems --- and often, these models contain
               harmful stereotypes learned from the internet. We investigate the
               bias transfer hypothesis: the theory that social biases (such as
               stereotypes) internalized by large language models during
               pre-training transfer into harmful task-specific behavior after
               fine-tuning. For two classification tasks, we find that reducing
               intrinsic bias with controlled interventions before fine-tuning
               does little to mitigate the classifier's discriminatory behavior
               after fine-tuning. Regression analysis suggests that downstream
               disparities are better explained by biases in the fine-tuning
               dataset. Still, pre-training plays a role: simple alterations to
               co-occurrence rates in the fine-tuning dataset are ineffective
               when the model has been pre-trained. Our results encourage
               practitioners to focus more on dataset quality and
               context-specific harms.",
  month     =  may,
  year      =  2022
}

@ARTICLE{Conti2023-em,
  title         = "Using Artificial French Data to Understand the Emergence of
                   Gender Bias in Transformer Language Models",
  author        = "Conti, Lina and Wisniewski, Guillaume",
  journal       = "arXiv [cs.CL]",
  abstract      = "Numerous studies have demonstrated the ability of neural
                   language models to learn various linguistic properties
                   without direct supervision. This work takes an initial step
                   towards exploring the less researched topic of how neural
                   models discover linguistic properties of words, such as
                   gender, as well as the rules governing their usage. We
                   propose to use an artificial corpus generated by a PCFG based
                   on French to precisely control the gender distribution in the
                   training data and determine under which conditions a model
                   correctly captures gender information or, on the contrary,
                   appears gender-biased.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Binz2023-rk,
  title     = "Using cognitive psychology to understand {GPT}-3",
  author    = "Binz, Marcel and Schulz, Eric",
  journal   = "Proc. Natl. Acad. Sci. U. S. A.",
  publisher = "National Acad Sciences",
  volume    =  120,
  number    =  6,
  pages     = "e2218523120",
  abstract  = "We study GPT-3, a recent large language model, using tools from
               cognitive psychology. More specifically, we assess GPT-3's
               decision-making, information search, deliberation, and causal
               reasoning abilities on a battery of canonical experiments from
               the literature. We find that much of GPT-3's behavior is
               impressive: It solves vignette-based tasks similarly or better
               than human subjects, is able to make decent decisions from
               descriptions, outperforms humans in a multiarmed bandit task, and
               shows signatures of model-based reinforcement learning. Yet, we
               also find that small perturbations to vignette-based tasks can
               lead GPT-3 vastly astray, that it shows no signatures of directed
               exploration, and that it fails miserably in a causal reasoning
               task. Taken together, these results enrich our understanding of
               current large language models and pave the way for future
               investigations using tools from cognitive psychology to study
               increasingly capable and opaque artificial agents.",
  month     =  feb,
  year      =  2023,
  keywords  = "artificial intelligence; cognitive psychology; decision-making;
               language models; reasoning",
  language  = "en"
}

@ARTICLE{Egami2023-rl,
  title         = "Using Large Language Model Annotations for Valid Downstream
                   Statistical Inference in Social Science: Design-Based
                   Semi-Supervised Learning",
  author        = "Egami, Naoki and Jacobs-Harukawa, Musashi and Stewart,
                   Brandon M and Wei, Hanying",
  journal       = "arXiv [stat.ME]",
  abstract      = "In computational social science (CSS), researchers analyze
                   documents to explain social and political phenomena. In most
                   scenarios, CSS researchers first obtain labels for documents
                   and then explain labels using interpretable regression
                   analyses in the second step. The recent advancements in large
                   language models (LLMs) can lower costs for CSS research by
                   annotating documents cheaply at scale, but such surrogate
                   labels are often imperfect and biased. We present a new
                   algorithm for using outputs from LLMs for downstream
                   statistical analyses while guaranteeing statistical
                   properties -- like asymptotic unbiasedness and proper
                   uncertainty quantification -- which are fundamental to CSS
                   research. We show that direct use of LLM-predicted surrogate
                   labels in downstream statistical analyses leads to
                   substantial bias and invalid confidence intervals, even with
                   high surrogate accuracy of 80--90\%. To address this, we
                   build on debiased machine learning to propose the
                   design-based semi-supervised learning (DSL) estimator. DSL
                   employs a doubly-robust procedure to combine surrogate labels
                   with a smaller number of gold-standard labels. Our approach
                   guarantees valid inference for downstream statistical
                   analyses, even when surrogates are arbitrarily biased,
                   without requiring stringent assumptions, by controlling the
                   probability of sampling documents for gold-standard labeling.
                   Both our theoretical analysis and experimental results show
                   that DSL provides valid statistical inference while achieving
                   root mean squared errors comparable to existing alternatives
                   that focus only on prediction without statistical guarantees.",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ME"
}

@ARTICLE{Aher2022-rp,
  title         = "Using Large Language Models to Simulate Multiple Humans and
                   Replicate Human Subject Studies",
  author        = "Aher, Gati and Arriaga, Rosa I and Kalai, Adam Tauman",
  journal       = "arXiv [cs.CL]",
  abstract      = "We introduce a new type of test, called a Turing Experiment
                   (TE), for evaluating how well a language model, such as
                   GPT-3, can simulate different aspects of human behavior.
                   Unlike the Turing Test, which involves simulating a single
                   arbitrary individual, a TE requires simulating a
                   representative sample of participants in human subject
                   research. We give TEs that attempt to replicate
                   well-established findings in prior studies. We design a
                   methodology for simulating TEs and illustrate its use to
                   compare how well different language models are able to
                   reproduce classic economic, psycholinguistic, and social
                   psychology experiments: Ultimatum Game, Garden Path
                   Sentences, Milgram Shock Experiment, and Wisdom of Crowds. In
                   the first three TEs, the existing findings were replicated
                   using recent models, while the last TE reveals a
                   ``hyper-accuracy distortion'' present in some language
                   models.",
  month         =  aug,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Peterson2021-wm,
  title     = "Using large-scale experiments and machine learning to discover
               theories of human decision-making",
  author    = "Peterson, Joshua C and Bourgin, David D and Agrawal, Mayank and
               Reichman, Daniel and Griffiths, Thomas L",
  journal   = "Science",
  publisher = "science.org",
  volume    =  372,
  number    =  6547,
  pages     = "1209--1214",
  abstract  = "Predicting and understanding how people make decisions has been a
               long-standing goal in many fields, with quantitative models of
               human decision-making informing research in both the social
               sciences and engineering. We show how progress toward this goal
               can be accelerated by using large datasets to power
               machine-learning algorithms that are constrained to produce
               interpretable psychological theories. Conducting the largest
               experiment on risky choice to date and analyzing the results
               using gradient-based optimization of differentiable decision
               theories implemented through artificial neural networks, we were
               able to recapitulate historical discoveries, establish that there
               is room to improve on existing theories, and discover a new, more
               accurate model of human decision-making in a form that preserves
               the insights from centuries of research.",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@INPROCEEDINGS{Emnett2024-na,
  title     = "Using Robot Social Agency Theory to Understand Robots' Linguistic
               Anthropomorphism",
  author    = "Emnett, Cloe Z and Mott, Terran and Williams, Tom",
  booktitle = "Companion of the 2024 ACM/IEEE International Conference on
               Human-Robot Interaction",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "447--452",
  abstract  = "Robots' use of natural language is one of the key factors that
               leads humans to anthropomorphize them. But it is not yet well
               understood what types of language most lead to such
               language-based anthropomorphization (or, Linguistic
               Anthropomorphism). In this paper, we present a brief literature
               survey that suggests six broad categories of linguistic factors
               that lead humans to anthropomorphize robots: autonomy,
               adaptability, directness, politeness, proportionality, and humor.
               By contextualizing these six factors through the lens of Jackson
               and Williams' Theory of Social Agency for Human-Robot
               Interaction, we are able to show how and why these particular
               factors are those responsible for language-based robot
               anthropomorphism.",
  series    = "HRI '24",
  month     =  mar,
  year      =  2024,
  keywords  = "linguistic anthropomorphism, social agency"
}

@ARTICLE{Shen2024-bq,
  title         = "{ValueCompass}: A framework of fundamental values for
                   human-{AI} alignment",
  author        = "Shen, Hua and Knearem, Tiffany and Ghosh, Reshmi and Yang,
                   Yu-Ju and Mitra, Tanushree and Huang, Yun",
  journal       = "arXiv [cs.HC]",
  abstract      = "As AI systems become more advanced, ensuring their alignment
                   with a diverse range of individuals and societal values
                   becomes increasingly critical. But how can we capture
                   fundamental human values and assess the degree to which AI
                   systems align with them? We introduce ValueCompass, a
                   framework of fundamental values, grounded in psychological
                   theory and a systematic review, to identify and evaluate
                   human-AI alignment. We apply ValueCompass to measure the
                   value alignment of humans and language models (LMs) across
                   four real-world vignettes: collaborative writing, education,
                   public sectors, and healthcare. Our findings uncover risky
                   misalignment between humans and LMs, such as LMs agreeing
                   with values like ``Choose Own Goals'', which are largely
                   disagreed by humans. We also observe values vary across
                   vignettes, underscoring the necessity for context-aware AI
                   alignment strategies. This work provides insights into the
                   design space of human-AI alignment, offering foundations for
                   developing AI that responsibly reflects societal values and
                   ethics.",
  month         =  sep,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Aono2024-hq,
  title         = "Verifying Claims About Metaphors with Large-Scale Automatic
                   Metaphor Identification",
  author        = "Aono, Kotaro and Sasano, Ryohei and Takeda, Koichi",
  journal       = "arXiv [cs.CL]",
  abstract      = "There are several linguistic claims about situations where
                   words are more likely to be used as metaphors. However, few
                   studies have sought to verify such claims with large corpora.
                   This study entails a large-scale, corpus-based analysis of
                   certain existing claims about verb metaphors, by applying
                   metaphor detection to sentences extracted from Common Crawl
                   and using the statistics obtained from the results. The
                   verification results indicate that the direct objects of
                   verbs used as metaphors tend to have lower degrees of
                   concreteness, imageability, and familiarity, and that
                   metaphors are more likely to be used in emotional and
                   subjective sentences.",
  month         =  apr,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@MISC{noauthor_undated-yl,
  title = "View of \_A Special Operation\_\_ A Quantitative Approach to
           Dissecting and Comparing Different Media Ecosystems’ Coverage of the
           Russo-Ukrainian War.pdf"
}

@MISC{noauthor_undated-ai,
  title = "View of The Media During the Rise of Trump\_ Identity Politics,
           Immigration,\_Mexican\_ Demonization and Hate-Crime.pdf"
}

@ARTICLE{Shahmohammadi2023-qr,
  title         = "{ViPE}: Visualise Pretty-much Everything",
  author        = "Shahmohammadi, Hassan and Ghosh, Adhiraj and Lensch, Hendrik
                   P A",
  journal       = "arXiv [cs.CL]",
  abstract      = "Figurative and non-literal expressions are profoundly
                   integrated in human communication. Visualising such
                   expressions allow us to convey our creative thoughts, and
                   evoke nuanced emotions. Recent text-to-image models like
                   Stable Diffusion, on the other hand, struggle to depict
                   non-literal expressions. Recent works primarily deal with
                   this issue by compiling humanly annotated datasets on a small
                   scale, which not only demands specialised expertise but also
                   proves highly inefficient. To address this issue, we
                   introduce ViPE: Visualise Pretty-much Everything. ViPE offers
                   a series of lightweight and robust language models that have
                   been trained on a large-scale set of lyrics with noisy visual
                   descriptions that represent their implicit meaning. The
                   synthetic visual descriptions are generated by GPT3.5 relying
                   on neither human annotations nor images. ViPE effectively
                   expresses any arbitrary piece of text into a visualisable
                   description, enabling meaningful and high-quality image
                   generation. We provide compelling evidence that ViPE is more
                   robust than GPT3.5 in synthesising visual elaborations. ViPE
                   also exhibits an understanding of figurative expressions
                   comparable to human experts, providing a powerful and
                   open-source backbone to many downstream applications such as
                   music video and caption generation.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Jha2024-os,
  title         = "{ViSAGe}: A global-scale analysis of visual stereotypes in
                   Text-to-Image generation",
  author        = "Jha, Akshita and Prabhakaran, Vinodkumar and Denton, Remi and
                   Laszlo, Sarah and Dave, Shachi and Qadri, Rida and Reddy,
                   Chandan K and Dev, Sunipa",
  journal       = "arXiv [cs.CV]",
  abstract      = "Recent studies have shown that Text-to-Image (T2I) model
                   generations can reflect social stereotypes present in the
                   real world. However, existing approaches for evaluating
                   stereotypes have a noticeable lack of coverage of global
                   identity groups and their associated stereotypes. To address
                   this gap, we introduce the ViSAGe (Visual Stereotypes Around
                   the Globe) dataset to enable the evaluation of known
                   nationality-based stereotypes in T2I models, across 135
                   nationalities. We enrich an existing textual stereotype
                   resource by distinguishing between stereotypical associations
                   that are more likely to have visual depictions, such as
                   `sombrero', from those that are less visually concrete, such
                   as 'attractive'. We demonstrate ViSAGe's utility through a
                   multi-faceted evaluation of T2I generations. First, we show
                   that stereotypical attributes in ViSAGe are thrice as likely
                   to be present in generated images of corresponding identities
                   as compared to other attributes, and that the offensiveness
                   of these depictions is especially higher for identities from
                   Africa, South America, and South East Asia. Second, we assess
                   the stereotypical pull of visual depictions of identity
                   groups, which reveals how the 'default' representations of
                   all identity groups in ViSAGe have a pull towards
                   stereotypical depictions, and that this pull is even more
                   prominent for identity groups from the Global South. CONTENT
                   WARNING: Some examples contain offensive stereotypes.",
  month         =  jan,
  year          =  2024,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV"
}

@ARTICLE{Drainville2020-tf,
  title     = "Visual and Linguistic Dogwhistles",
  author    = "Drainville, Ray and Saul, Jennifer",
  journal   = "Oxford Handbook of Applied Philosophy of Language. Oxford: Oxford
               University Press. https://www. researchgate.
               net/publication/344441861\_Visual\_and\_Linguistic\_Dogwhistles",
  publisher = "researchgate.net",
  abstract  = "… Before we discuss visual dogwhistles , it will be useful to
               take a short … visual dogwhistles —which to our knowledge has not
               yet been discussed in the literature on linguistic dogwhistles —…",
  year      =  2020
}

@ARTICLE{Bridle2022-ya,
  title     = "Ways of being: Animals, plants, machines: The search for a
               planetary intelligence",
  author    = "Bridle, J",
  publisher = "Penguin UK",
  year      =  2022
}

@ARTICLE{Wagner2013-xn,
  title     = "“We act like girls and we don't act like men”: Ethnicity and
               local language change in a Philadelphia high school",
  author    = "Wagner, Suzanne Evans",
  journal   = "Lang. Soc.",
  publisher = "Cambridge University Press",
  volume    =  42,
  number    =  4,
  pages     = "361--383",
  abstract  = "How is ethnicity indexed linguistically in a speech community in
               which immigrant L2s have typically not been spoken for three or
               more generations? Drawing on recordings and ethnographic
               observations of eighteen white high school girls in south
               Philadelphia, speakers of Irish descent are shown to
               differentiate themselves from speakers of Italian descent through
               their use of (ay0), that is, Canadian Raising. (ay0) is an
               ongoing sound change in Philadelphia and is remarkable for being
               a rare example of a male-led change. Irish girls exploit more
               male-like, backed, and raised variants as a resource for indexing
               their ethnic identity, which is associated locally with
               stereotypically masculine characteristics such as toughness. The
               symbolic reflection of ethnic affiliation through this subtle
               linguistic device makes use of both local and supralocal social
               meanings. (Ethnicity, adolescence, Philadelphia, Irish, Canadian
               Raising, gender, sound change, language, and identity)*",
  month     =  sep,
  year      =  2013
}

@ARTICLE{Wahle2023-cd,
  title         = "We are Who We Cite: Bridges of Influence Between Natural
                   Language Processing and Other Academic Fields",
  author        = "Wahle, Jan Philip and Ruas, Terry and Abdalla, Mohamed and
                   Gipp, Bela and Mohammad, Saif M",
  journal       = "arXiv [cs.CL]",
  abstract      = "Natural Language Processing (NLP) is poised to substantially
                   influence the world. However, significant progress comes
                   hand-in-hand with substantial risks. Addressing them requires
                   broad engagement with various fields of study. Yet, little
                   empirical work examines the state of such engagement (past or
                   current). In this paper, we quantify the degree of influence
                   between 23 fields of study and NLP (on each other). We
                   analyzed ~77k NLP papers, ~3.1m citations from NLP papers to
                   other papers, and ~1.8m citations from other papers to NLP
                   papers. We show that, unlike most fields, the cross-field
                   engagement of NLP, measured by our proposed Citation Field
                   Diversity Index (CFDI), has declined from 0.58 in 1980 to
                   0.31 in 2022 (an all-time low). In addition, we find that NLP
                   has grown more insular -- citing increasingly more NLP papers
                   and having fewer papers that act as bridges between fields.
                   NLP citations are dominated by computer science; Less than
                   8\% of NLP citations are to linguistics, and less than 3\%
                   are to math and psychology. These findings underscore NLP's
                   urgent need to reflect on its engagement with various fields.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Liu2023-eu,
  title         = "We're Afraid Language Models Aren't Modeling Ambiguity",
  author        = "Liu, Alisa and Wu, Zhaofeng and Michael, Julian and Suhr,
                   Alane and West, Peter and Koller, Alexander and Swayamdipta,
                   Swabha and Smith, Noah A and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Ambiguity is an intrinsic feature of natural language.
                   Managing ambiguity is a key part of human language
                   understanding, allowing us to anticipate misunderstanding as
                   communicators and revise our interpretations as listeners. As
                   language models (LMs) are increasingly employed as dialogue
                   interfaces and writing aids, handling ambiguous language is
                   critical to their success. We characterize ambiguity in a
                   sentence by its effect on entailment relations with another
                   sentence, and collect AmbiEnt, a linguist-annotated benchmark
                   of 1,645 examples with diverse kinds of ambiguity. We design
                   a suite of tests based on AmbiEnt, presenting the first
                   evaluation of pretrained LMs to recognize ambiguity and
                   disentangle possible meanings. We find that the task remains
                   extremely challenging, including for the recent GPT-4, whose
                   generated disambiguations are considered correct only 32\% of
                   the time in human evaluation, compared to 90\% for
                   disambiguations in our dataset. Finally, to illustrate the
                   value of ambiguity-sensitive tools, we show that a multilabel
                   NLI model can flag political claims in the wild that are
                   misleading due to ambiguity. We encourage the field to
                   rediscover the importance of ambiguity for NLP.",
  month         =  apr,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Roy2020-fa,
  title         = "Weakly Supervised Learning of Nuanced Frames for Analyzing
                   Polarization in News Media",
  author        = "Roy, Shamik and Goldwasser, Dan",
  journal       = "arXiv [cs.CL]",
  abstract      = "In this paper we suggest a minimally-supervised approach for
                   identifying nuanced frames in news article coverage of
                   politically divisive topics. We suggest to break the broad
                   policy frames suggested by Boydstun et al., 2014 into
                   fine-grained subframes which can capture differences in
                   political ideology in a better way. We evaluate the suggested
                   subframes and their embedding, learned using minimal
                   supervision, over three topics, namely, immigration,
                   gun-control and abortion. We demonstrate the ability of the
                   subframes to capture ideological differences and analyze
                   political discourse in news media.",
  month         =  sep,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Ophir2022-ao,
  title     = "Weaponizing reproductive rights: A mixed-method analysis of White
               nationalists' discussion of abortions online",
  author    = "Ophir, Y and Pruden, M L and Walter, D and {others}",
  journal   = "Information",
  publisher = "Taylor \& Francis",
  abstract  = "ABSTRACT According to the Great Replacement conspiracy theory,
               nonwhites, globalists and elites are plotting to eliminate the
               white race and its dominance through anti-white …",
  year      =  2022
}

@ARTICLE{Ito2003-ja,
  title     = "Well weird, right dodgy, very strange, really cool: Layering and
               recycling in English intensifiers",
  author    = "Ito, Rika and Tagliamonte, Sali",
  journal   = "Lang. Soc.",
  publisher = "Cambridge University Press",
  volume    =  32,
  number    =  2,
  pages     = "257--279",
  abstract  = "This article examines variable usage of intensifiers in a corpus
               from a socially and generationally stratified community. Using
               multivariate analyses, the authors assess the direction of
               effect, significance, and relative importance of conditioning
               factors in apparent time. Of 4,019 adjectival heads, 24\% were
               intensified, and there is an increase in intensification across
               generations. Earlier forms (e.g. right and well) do not fade away
               but coexist with newer items. The most frequent intensifiers,
               however, are shifting rapidly. Very is most common, but only
               among the older speakers. In contrast, really increases
               dramatically among the youngest generation; however, the effects
               of education and sex must be disentangled. The results confirm
               that variation in intensifier use is a strong indicator of
               shifting norms and practices in a speech community. Studying such
               actively changing features can make an important contribution to
               understanding linguistic change as well as to discovering current
               trends in contemporary English.",
  month     =  apr,
  year      =  2003,
  keywords  = "Intensifiers; delexicalization; gender difference; very; really;
               British English"
}

@ARTICLE{Cabrera2023-ba,
  title     = "What Did My {AI} Learn? How Data Scientists Make Sense of Model
               Behavior",
  author    = "Cabrera, Ángel Alexander and Tulio Ribeiro, Marco and Lee,
               Bongshin and Deline, Robert and Perer, Adam and Drucker, Steven M",
  journal   = "ACM Trans. Comput.-Hum. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  30,
  number    =  1,
  pages     = "1--27",
  abstract  = "Data scientists require rich mental models of how AI systems
               behave to effectively train, debug, and work with them. Despite
               the prevalence of AI analysis tools, there is no general theory
               describing how people make sense of what their models have
               learned. We frame this process as a form of sensemaking and
               derive a framework describing how data scientists develop mental
               models of AI behavior. To evaluate the framework, we show how
               existing AI analysis tools fit into this sensemaking process and
               use it to design AIFinnity, a system for analyzing image-and-text
               models. Lastly, we explored how data scientists use a tool
               developed with the framework through a think-aloud study with 10
               data scientists tasked with using AIFinnity to pick an image
               captioning model. We found that AIFinnity’s sensemaking workflow
               reflected participants’ mental processes and enabled them to
               discover and validate diverse AI behaviors.",
  month     =  mar,
  year      =  2023,
  keywords  = "Machine learning, AI, machine behavior, machine learning testing,
               sensemaking, visualization"
}

@ARTICLE{Caudwell2020-ki,
  title     = "What do home robots want? The ambivalent power of cuteness in
               robotic relationships",
  author    = "Caudwell, Catherine and Lacey, Cherie",
  journal   = "Convergence",
  publisher = "journals.sagepub.com",
  volume    =  26,
  number    =  4,
  pages     = "956--968",
  month     =  aug,
  year      =  2020
}

@ARTICLE{Belinkov2017-xo,
  title         = "What do Neural Machine Translation Models Learn about
                   Morphology?",
  author        = "Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and
                   Sajjad, Hassan and Glass, James",
  journal       = "arXiv [cs.CL]",
  abstract      = "Neural machine translation (MT) models obtain
                   state-of-the-art performance while maintaining a simple,
                   end-to-end architecture. However, little is known about what
                   these models learn about source and target languages during
                   the training process. In this work, we analyze the
                   representations learned by neural MT models at various levels
                   of granularity and empirically evaluate the quality of the
                   representations for learning morphology through extrinsic
                   part-of-speech and morphological tagging tasks. We conduct a
                   thorough investigation along several parameters: word-based
                   vs. character-based representations, depth of the encoding
                   layer, the identity of the target language, and encoder vs.
                   decoder representations. Our data-driven, quantitative
                   evaluation sheds light on important aspects in the neural MT
                   system and its ability to capture word structure.",
  month         =  apr,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Trippas_undated-pa,
  title     = "What do Users Really Ask Large Language Models?",
  author    = "Trippas, Johanne R and Al Lawati, Sara Fahad Dawood and
               Mackenzie, Joel and Gallagher, Luke",
  journal   = "Proceedings of the 47th International ACM SIGIR Conference on
               Research and Development in Information Retrieval (SIGIR '24),
               July 14â•ﬁ18, 2024, Washington, DC, USA",
  publisher = "Association for Computing Machinery",
  volume    =  1,
  number    =  1
}

@ARTICLE{Unknown2007-cb,
  title     = "What is a Human?: Toward psychological benchmarks in the field of
               human–robot interaction",
  journal   = "Interact. Stud.",
  publisher = "John Benjamins Publishing Company",
  volume    =  8,
  number    =  3,
  pages     = "363--390",
  abstract  = "In this paper, we move toward offering psychological benchmarks
               to measure success in building increasingly humanlike robots. By
               psychological benchmarks we mean categories of interaction that
               capture conceptually fundamental aspects of human life, specified
               abstractly enough to resist their identity as a mere
               psychological instrument, but capable of being translated into
               testable empirical propositions. Nine possible benchmarks are
               considered: autonomy, imitation, intrinsic moral value, moral
               accountability, privacy, reciprocity, conventionality,
               creativity, and authenticity of relation. Finally, we discuss how
               getting the right group of benchmarks in human–robot interaction
               will, in future years, help inform on the foundational question
               of what constitutes essential features of being human.",
  month     =  nov,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Beeghly2015-kb,
  title     = "What is a Stereotype? What is Stereotyping?",
  author    = "Beeghly, Erin",
  journal   = "Hypatia",
  publisher = "Cambridge University Press (CUP)",
  volume    =  30,
  number    =  4,
  pages     = "675--691",
  abstract  = "If someone says, “Asians are good at math” or “women are
               empathetic,” I might interject, “you're stereotyping” in order to
               convey my disapproval of their utterance. But why is stereotyping
               wrong? Before we can answer this question, we must better
               understand what stereotypes are and what stereotyping is. In this
               essay, I develop what I call the descriptive view of stereotypes
               and stereotyping. This view is assumed in much of the
               psychological and philosophical literature on implicit bias and
               stereotyping, yet it has not been sufficiently defended. The main
               objection to the descriptive view is that it fails to include the
               common‐sense idea that stereotyping is always objectionable. I
               argue that this is actually a benefit of the view. In the essay's
               final part, I put forward two hypotheses that would validate the
               claim that stereotyping is always morally or epistemically wrong.
               If these hypotheses are false—which is very likely—we have little
               reason to build moral or epistemic defect into the very idea of a
               stereotype. Moreover, we must abandon the seemingly attractive
               claim that judging individuals based on group membership is
               intrinsically wrong.",
  year      =  2015,
  language  = "en"
}

@ARTICLE{Beer2002-zx,
  title     = "What is cybernetics?",
  author    = "Beer, Stafford",
  journal   = "Kybernetes",
  publisher = "Emerald",
  volume    =  31,
  number    =  2,
  pages     = "209--219",
  abstract  = "An address delivered at the University of Valladolid, Spain. Asks
               the Question‐What is Cybernetics?. Discusses popular notions and
               genuine difficulties. Looks at the origins, derivations and
               definitions of cybernetics. Considers intrinsic control and
               Socio‐Economic Governance in real‐time. Relates cybernetics to
               the current world situation.",
  month     =  mar,
  year      =  2002,
  language  = "en"
}

@ARTICLE{Borg2024-xn,
  title     = "What is required for empathic {AI}? It depends, and why that
               matters for {AI} developers and users",
  author    = "Borg, Jana Schaich and Read, Hannah",
  journal   = "arXiv preprint arXiv:2408.15354",
  publisher = "arxiv.org",
  abstract  = "Interest is growing in artificial empathy, but so is confusion
               about what artificial empathy is or needs to be. This confusion
               makes it challenging to navigate the technical and ethical issues
               that accompany empathic AI development. Here, we outline a
               framework for thinking about empathic AI based on the premise
               that different constellations of capabilities associated with
               empathy are important for different empathic AI applications. We
               describe distinctions of capabilities that we argue belong under
               the empathy umbrella, and show how three medical empathic AI use
               cases require different sets of these capabilities. We conclude
               by discussing why appreciation of the diverse capabilities under
               the empathy umbrella is important for both AI creators and users.",
  month     =  aug,
  year      =  2024
}

@ARTICLE{Fraser2023-ss,
  title     = "What Makes a Good Counter-Stereotype? Evaluating Strategies for
               Automated Responses to Stereotypical Text",
  author    = "Fraser, K C and Kiritchenko, S and Nejadgholi, I and {others}",
  journal   = "Proceedings of the First",
  publisher = "aclanthology.org",
  abstract  = "When harmful social stereotypes are expressed on a public
               platform, they must be addressed in a way that educates and
               informs both the original poster and other readers …",
  year      =  2023
}

@ARTICLE{Rao2023-fr,
  title         = "What Makes it Ok to Set a Fire? Iterative Self-distillation
                   of Contexts and Rationales for Disambiguating Defeasible
                   Social and Moral Situations",
  author        = "Rao, Kavel and Jiang, Liwei and Pyatkin, Valentina and Gu,
                   Yuling and Tandon, Niket and Dziri, Nouha and Brahman, Faeze
                   and Choi, Yejin",
  journal       = "arXiv [cs.CL]",
  abstract      = "Moral or ethical judgments rely heavily on the specific
                   contexts in which they occur. Understanding varying shades of
                   defeasible contextualizations (i.e., additional information
                   that strengthens or attenuates the moral acceptability of an
                   action) is critical to accurately represent the subtlety and
                   intricacy of grounded human moral judgment in real-life
                   scenarios. We introduce defeasible moral reasoning: a task to
                   provide grounded contexts that make an action more or less
                   morally acceptable, along with commonsense rationales that
                   justify the reasoning. To elicit high-quality task data, we
                   take an iterative self-distillation approach that starts from
                   a small amount of unstructured seed knowledge from GPT-3 and
                   then alternates between (1) self-distillation from student
                   models; (2) targeted filtering with a critic model trained by
                   human judgment (to boost validity) and NLI (to boost
                   diversity); (3) self-imitation learning (to amplify the
                   desired data quality). This process yields a student model
                   that produces defeasible contexts with improved validity,
                   diversity, and defeasibility. From this model we distill a
                   high-quality dataset, \delta-Rules-of-Thumb, of 1.2M entries
                   of contextualizations and rationales for 115K defeasible
                   moral actions rated highly by human annotators 85.9\% to
                   99.8\% of the time. Using \delta-RoT we obtain a final
                   student model that wins over all intermediate student models
                   by a notable margin.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Hu2020-ki,
  title         = "What's Sex Got To Do With Fair Machine Learning?",
  author        = "Hu, Lily and Kohler-Hausmann, Issa",
  journal       = "arXiv [cs.CY]",
  abstract      = "Debate about fairness in machine learning has largely
                   centered around competing definitions of what fairness or
                   nondiscrimination between groups requires. However, little
                   attention has been paid to what precisely a group is. Many
                   recent approaches to ``fairness'' require one to specify a
                   causal model of the data generating process. These exercises
                   make an implicit ontological assumption that a racial or sex
                   group is simply a collection of individuals who share a given
                   trait. We show this by exploring the formal assumption of
                   modularity in causal models, which holds that the
                   dependencies captured by one causal pathway are invariant to
                   interventions on any other pathways. Causal models of sex
                   propose two substantive claims: 1) There exists a feature,
                   sex-on-its-own, that is an inherent trait of an individual
                   that causally brings about social phenomena external to it in
                   the world; and 2) the relations between sex and its effects
                   can be modified in whichever ways and the former feature
                   would still retain the meaning that sex has in our world. We
                   argue that this ontological picture is false. Many of the
                   ``effects'' that sex purportedly ``causes'' are in fact
                   constitutive features of sex as a social status. They give
                   the social meaning of sex features, meanings that are
                   precisely what make sex discrimination a distinctively
                   morally problematic type of action. Correcting this
                   conceptual error has a number of implications for how models
                   can be used to detect discrimination. Formal diagrams of
                   constitutive relations present an entirely different path
                   toward reasoning about discrimination. Whereas causal
                   diagrams guide the construction of sophisticated modular
                   counterfactuals, constitutive diagrams identify a different
                   kind of counterfactual as central to an inquiry on
                   discrimination: one that asks how the social meaning of a
                   group would be changed if its non-modular features were
                   altered.",
  month         =  jun,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CY"
}

@ARTICLE{Beeghly2021-gr,
  title     = "What's Wrong with Stereotypes? The Falsity Hypothesis",
  author    = "Beeghly, E",
  journal   = "Soc. Theory Pract.",
  publisher = "JSTOR",
  abstract  = "Stereotypes are commonly alleged to be false or inaccurate views
               of groups. For shorthand, I call this the falsity hypothesis. The
               falsity hypothesis is widespread and is often one of the …",
  year      =  2021
}

@ARTICLE{Huang2023-lh,
  title         = "What Types of Questions Require Conversation to Answer? A
                   Case Study of {AskReddit} Questions",
  author        = "Huang, Shih-Hong and Huang, Chieh-Yang and Lin, Ya-Fang and
                   Huang, Ting-Hao 'kenneth'",
  journal       = "arXiv [cs.HC]",
  abstract      = "The proliferation of automated conversational systems such as
                   chatbots, spoken-dialogue systems, and smart speakers, has
                   significantly impacted modern digital life. However, these
                   systems are primarily designed to provide answers to
                   well-defined questions rather than to support users in
                   exploring complex, ill-defined questions. In this paper, we
                   aim to push the boundaries of conversational systems by
                   examining the types of nebulous, open-ended questions that
                   can best be answered through conversation. We first sampled
                   500 questions from one million open-ended requests posted on
                   AskReddit, and then recruited online crowd workers to answer
                   eight inquiries about these questions. We also performed open
                   coding to categorize the questions into 27 different domains.
                   We found that the issues people believe require conversation
                   to resolve satisfactorily are highly social and personal. Our
                   work provides insights into how future research could be
                   geared to align with users' needs.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.HC"
}

@ARTICLE{Conneau2018-ld,
  title         = "What you can cram into a single vector: Probing sentence
                   embeddings for linguistic properties",
  author        = "Conneau, Alexis and Kruszewski, German and Lample, Guillaume
                   and Barrault, Loïc and Baroni, Marco",
  journal       = "arXiv [cs.CL]",
  abstract      = "Although much effort has recently been devoted to training
                   high-quality sentence embeddings, we still have a poor
                   understanding of what they are capturing. ``Downstream''
                   tasks, often based on sentence classification, are commonly
                   used to evaluate the quality of sentence representations. The
                   complexity of the tasks makes it however difficult to infer
                   what kind of information is present in the representations.
                   We introduce here 10 probing tasks designed to capture simple
                   linguistic features of sentences, and we use them to study
                   embeddings generated by three different encoders trained in
                   eight distinct ways, uncovering intriguing properties of both
                   encoders and training methods.",
  month         =  may,
  year          =  2018,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Etienne2022-ns,
  title     = "When {AI} Ethics Goes Astray: A Case Study of Autonomous Vehicles",
  author    = "Etienne, Hubert",
  journal   = "Soc. Sci. Comput. Rev.",
  publisher = "SAGE Publications Inc",
  volume    =  40,
  number    =  1,
  pages     = "236--246",
  abstract  = "This article discusses the dangers of the Moral Machine (MM)
               experiment, alerting against both its uses for normative ends and
               the whole approach it is built upon to address ethical issues. It
               explores additional methodological limits of the experiment on
               top of those already identified by its authors; exhibits the
               dangers of computational moral systems for modern democracies,
               such as the ?voting-based system? recently developed out of the
               MM?s data; and provides reasons why ethical decision-making
               fundamentally excludes computational social choice methods.",
  month     =  feb,
  year      =  2022
}

@ARTICLE{Yuksekgonul2022-er,
  title    = "When and Why Vision-Language Models Behave like Bags-Of-Words, and
              What to Do About It?",
  author   = "Yuksekgonul, Mert and Bianchi, Federico and Kalluri, Pratyusha and
              Jurafsky, Dan and Zou, James",
  abstract = "Despite the success of large vision and language models (VLMs) in
              many downstream applications, it is unclear how well they encode
              the compositional relationships between objects and attributes.
              Here, we create the Attribution, Relation, and Order (ARO)
              benchmark to systematically evaluate the ability of VLMs to
              understand different types of relationships, attributes, and order
              information. ARO consists of \emph{Visual Genome Attribution}, to
              test the understanding of objects' properties; \emph{Visual Genome
              Relation}, to test for relational understanding; and
              \emph{COCO-Order \& Flickr30k-Order}, to test for order
              sensitivity in VLMs. ARO is orders of magnitude larger than
              previous benchmarks of compositionality, with more than 50,000
              test cases. We present the settings where state-of-the-art VLMs
              behave like bags-of-words---i.e. when they have poor relational
              understanding, can blunder when linking objects to their
              attributes, and demonstrate a severe lack of order sensitivity.
              VLMs are predominantly trained and evaluated on large scale
              datasets with rich compositional structure in the images and
              captions. Yet, training on these datasets has not been enough to
              address the lack of compositional understanding, and evaluating on
              these datasets has failed to surface this deficiency. To
              understand why these limitations emerge and are not represented in
              the standard tests, we zoom into the evaluation and training
              procedures. We demonstrate that it is possible to perform well on
              image-text retrieval over existing datasets without using the
              composition and order information. This further motivates the
              value of using ARO to benchmark VLMs. Given that contrastive
              pretraining optimizes for retrieval on large datasets with similar
              shortcuts, we hypothesize that this can explain why the models do
              not need to learn to represent compositional information. This
              finding suggests a natural solution: composition-aware hard
              negative mining. We show that a simple-to-implement modification
              of contrastive learning significantly improves the performance on
              tasks requiring understanding of order and compositionality.",
  month    =  sep,
  year     =  2022
}

@ARTICLE{Olteanu2020-wz,
  title     = "When Are Search Completion Suggestions Problematic?",
  author    = "Olteanu, Alexandra and Diaz, Fernando and Kazai, Gabriella",
  journal   = "Proc. ACM Hum.-Comput. Interact.",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  4,
  number    = "CSCW2",
  pages     = "1--25",
  abstract  = "Problematic web search query completion suggestions-perceived as
               biased, offensive, or in some other way harmful-can reinforce
               existing stereotypes and misbeliefs, and even nudge users towards
               undesirable patterns of behavior. Locating such suggestions is
               difficult, not only due to the long-tailed nature of web search,
               but also due to differences in how people assess potential harms.
               Grounding our study in web search query logs, we explore when
               system-provided suggestions might be perceived as problematic
               through a series of crowd-experiments where we systematically
               manipulate: the search query fragments provided by users,
               possible user search intents, and the list of query completion
               suggestions. To examine why query suggestions might be perceived
               as problematic, we contrast them to an inventory of known types
               of problematic suggestions. We report our observations around
               differences in the prevalence of a) suggestions that are
               problematic on their own versus b) suggestions that are
               problematic for the query fragment provided by a user, for both
               common informational needs and in the presence of web search
               voids-topics searched by few to no users. Our experiments surface
               a rich array of scenarios where suggestions are considered
               problematic, including due to the context in which they were
               surfaced. Compounded by the elusive nature of many such
               scenarios, the prevalence of suggestions perceived as problematic
               only for certain user inputs, raises concerns about blind spots
               due to data annotation practices that may lead to some types of
               problematic suggestions being overlooked.",
  month     =  oct,
  year      =  2020,
  keywords  = "web search suggestions, query logs, problematic suggestions,
               predictive text"
}

@ARTICLE{Pei2023-mk,
  title         = "When Do Annotator Demographics Matter? Measuring the
                   Influence of Annotator Demographics with the {POPQUORN}
                   Dataset",
  author        = "Pei, Jiaxin and Jurgens, David",
  journal       = "arXiv [cs.CL]",
  abstract      = "Annotators are not fungible. Their demographics, life
                   experiences, and backgrounds all contribute to how they label
                   data. However, NLP has only recently considered how annotator
                   identity might influence their decisions. Here, we present
                   POPQUORN (the POtato-Prolific dataset for QUestion-Answering,
                   Offensiveness, text Rewriting, and politeness rating with
                   demographic Nuance). POPQUORN contains 45,000 annotations
                   from 1,484 annotators, drawn from a representative sample
                   regarding sex, age, and race as the US population. Through a
                   series of analyses, we show that annotators' background plays
                   a significant role in their judgments. Further, our work
                   shows that backgrounds not previously considered in NLP
                   (e.g., education), are meaningful and should be considered.
                   Our study suggests that understanding the background of
                   annotators and collecting labels from a demographically
                   balanced pool of crowd workers is important to reduce the
                   bias of datasets. The dataset, annotator background, and
                   annotation interface are available at
                   https://github.com/Jiaxin-Pei/potato-prolific-dataset .",
  month         =  jun,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Wang2020-tc,
  title     = "When expert recommendation contradicts peer opinion: Relative
               social influence of valence, group identity and artificial
               intelligence",
  author    = "Wang, Jinping and Molina, Maria D and Sundar, S Shyam",
  journal   = "Comput. Human Behav.",
  publisher = "Elsevier",
  volume    =  107,
  pages     =  106278,
  abstract  = "Whom do we trust more, the recommendation of an expert or public
               opinion from a crowd of other users of the site? Does it matter
               if the expert belongs to our in-group? And, what, if anything,
               would change if an Artificial Intelligence (AI) system was the
               recommender rather than a human expert? In order to answer these
               research questions, we conducted a between-subjects online
               experiment, informed by MAIN Model (Sundar, 2008), which posits
               that interface cues signaling different types of sources can
               influence perceived credibility of content by triggering distinct
               cognitive heuristics. Participants were assigned to a scenario
               wherein the expert review contrasted the peer rating about
               recommending photos for business profiles, with systematic
               variations in expert review valence (negative vs. positive),
               expert identity (ingroup vs. outgroup vs. no identity), and agent
               type (human vs. AI). Results show that positive ratings are more
               influential on user judgements. However, for negative ratings,
               human ingroup members generated greater effects than no-identity
               experts. Moreover, AI systems were as influential as human
               experts, suggesting the potential for AI to substitute human
               experts for online recommendations.",
  month     =  jun,
  year      =  2020,
  keywords  = "Online rating; Expert vs. peer; Social influence; Group identity;
               Artificial intelligence; MAIN model"
}

@INPROCEEDINGS{Maeda2024-cv,
  title     = "When Human-{AI} Interactions Become Parasocial: Agency and
               Anthropomorphism in Affective Design",
  author    = "Maeda, Takuya and Quan-Haase, Anabel",
  booktitle = "Proceedings of the 2024 ACM Conference on Fairness,
               Accountability, and Transparency",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "1068--1077",
  abstract  = "With the continuous improvement of large language models (LLMs),
               chatbots can produce coherent and continuous word sequences that
               mirror natural human language. While the use of natural language
               and human-like conversation styles enables the use of chatbots
               within a range of everyday settings, these usability-enhancing
               features can also have unintended consequences, such as making
               fallible information seem trustworthy by emphasizing friendliness
               and closeness. This can have serious implications for information
               retrieval tasks performed with chatbots. In this paper, we
               provide an overview of the literature on parasociality, social
               affordance, and trust to bridge these concepts within human-AI
               interactions. We critically examine how chatbot “roleplaying” and
               user role projection co-produce a pseudo-interactive,
               technologically-mediated space with imbalanced dynamics between
               users and chatbots. Based on the review of the literature, we
               develop a conceptual framework of parasociality in chatbots that
               describes interactions between humans and anthropomorphized
               chatbots. We dissect how chatbots use personal pronouns,
               conversational conventions, affirmations, and similar strategies
               to position the chatbots as users’ companions or assistants, and
               how these tactics induce trust-forming behaviors in users.
               Finally, based on the conceptual framework, we outline a set of
               ethical concerns that emerge from parasociality, including
               illusions of reciprocal engagement, task misalignment, and leaks
               of sensitive information. This paper argues that these possible
               consequences arise from a positive feedback cycle wherein
               anthropomorphized chatbot features encourage users to fill in the
               context around predictive outcomes.",
  series    = "FAccT '24",
  month     =  jun,
  year      =  2024,
  keywords  = "anthropomorphism, chatbots, design, ethics, human-AI
               interactions, parasociality, trust"
}

@ARTICLE{Hanna2023-vu,
  title         = "When Language Models Fall in Love: Animacy Processing in
                   Transformer Language Models",
  author        = "Hanna, Michael and Belinkov, Yonatan and Pezzelle, Sandro",
  journal       = "arXiv [cs.CL]",
  abstract      = "Animacy - whether an entity is alive and sentient - is
                   fundamental to cognitive processing, impacting areas such as
                   memory, vision, and language. However, animacy is not always
                   expressed directly in language: in English it often manifests
                   indirectly, in the form of selectional constraints on verbs
                   and adjectives. This poses a potential issue for transformer
                   language models (LMs): they often train only on text, and
                   thus lack access to extralinguistic information from which
                   humans learn about animacy. We ask: how does this impact LMs'
                   animacy processing - do they still behave as humans do? We
                   answer this question using open-source LMs. Like previous
                   studies, we find that LMs behave much like humans when
                   presented with entities whose animacy is typical. However, we
                   also show that even when presented with stories about
                   atypically animate entities, such as a peanut in love, LMs
                   adapt: they treat these entities as animate, though they do
                   not adapt as well as humans. Even when the context indicating
                   atypical animacy is very short, LMs pick up on subtle clues
                   and change their behavior. We conclude that despite the
                   limited signal through which LMs can learn about animacy,
                   they are indeed sensitive to the relevant lexical semantic
                   nuances available in English.",
  month         =  oct,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Chung2017-hb,
  title    = "When Personal Tracking Becomes Social: Examining the Use of
              Instagram for Healthy Eating",
  author   = "Chung, Chia-Fang and Agapie, Elena and Schroeder, Jessica and
              Mishra, Sonali and Fogarty, James and Munson, Sean A",
  journal  = "Proc SIGCHI Conf Hum Factor Comput Syst",
  volume   =  2017,
  pages    = "1674--1687",
  abstract = "Many people appropriate social media and online communities in
              their pursuit of personal health goals, such as healthy eating or
              increased physical activity. However, people struggle with
              impression management, and with reaching the right audiences when
              they share health information on these platforms. Instagram, a
              popular photo-based social media platform, has attracted many
              people who post and share their food photos. We aim to inform the
              design of tools to support healthy behaviors by understanding how
              people appropriate Instagram to track and share food data, the
              benefits they obtain from doing so, and the challenges they
              encounter. We interviewed 16 women who consistently record and
              share what they eat on Instagram. Participants tracked to support
              themselves and others in their pursuit of healthy eating goals.
              They sought social support for their own tracking and healthy
              behaviors and strove to provide that support for others. People
              adapted their personal tracking practices to better receive and
              give this support. Applying these results to the design of health
              tracking tools has the potential to help people better access
              social support.",
  month    =  may,
  year     =  2017,
  keywords = "Food Journals; H5.m. Information interfaces and presentation
              (e.g., HCI); Health; Personal Informatics; Self-Tracking; Social
              Media; Social Support",
  language = "en"
}

@ARTICLE{Jin2022-ni,
  title         = "When to make exceptions: Exploring language models as
                   accounts of human moral judgment",
  author        = "Jin, Zhijing and Levine, Sydney and Gonzalez, Fernando and
                   Kamal, Ojasv and Sap, Maarten and Sachan, Mrinmaya and
                   Mihalcea, Rada and Tenenbaum, Josh and Schölkopf, Bernhard",
  editor        = "Koyejo, S and Mohamed, S and Agarwal, A and Belgrave, D and
                   Cho, K and Oh, A",
  journal       = "arXiv [cs.CL]",
  pages         = "28458--28473",
  abstract      = "AI systems are becoming increasingly intertwined with human
                   life. In order to effectively collaborate with humans and
                   ensure safety, AI systems need to be able to understand,
                   interpret and predict human moral judgments and decisions.
                   Human moral judgments are often guided by rules, but not
                   always. A central challenge for AI safety is capturing the
                   flexibility of the human moral mind -- the ability to
                   determine when a rule should be broken, especially in novel
                   or unusual situations. In this paper, we present a novel
                   challenge set consisting of rule-breaking question answering
                   (RBQA) of cases that involve potentially permissible
                   rule-breaking -- inspired by recent moral psychology studies.
                   Using a state-of-the-art large language model (LLM) as a
                   basis, we propose a novel moral chain of thought (MORALCOT)
                   prompting strategy that combines the strengths of LLMs with
                   theories of moral reasoning developed in cognitive science to
                   predict human moral judgments. MORALCOT outperforms seven
                   existing LLMs by 6.2\% F1, suggesting that modeling human
                   reasoning might be necessary to capture the flexibility of
                   the human moral mind. We also conduct a detailed error
                   analysis to suggest directions for future work to improve AI
                   safety using RBQA. Our data is open-sourced at
                   https://huggingface.co/datasets/feradauto/MoralExceptQA and
                   code at https://github.com/feradauto/MoralCoT",
  month         =  oct,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Papafragou2007-uf,
  title     = "When we think about thinking: the acquisition of belief verbs",
  author    = "Papafragou, Anna and Cassidy, Kimberly and Gleitman, Lila",
  journal   = "Cognition",
  publisher = "Elsevier",
  volume    =  105,
  number    =  1,
  pages     = "125--165",
  abstract  = "Mental-content verbs such as think, believe, imagine and hope
               seem to pose special problems for the young language learner. One
               possible explanation for these difficulties is that the concepts
               that these verbs express are hard to grasp and therefore their
               acquisition must await relevant conceptual development. According
               to a different, perhaps complementary, proposal, a major
               contributor to the difficulty of these items lies with the
               informational requirements for identifying them from the contexts
               in which they appear. The experiments reported here explore the
               implications of these proposals by investigating the contribution
               of observational and linguistic cues to the acquisition of mental
               predicate vocabulary. We first demonstrate that particular
               observed situations can be helpful in prompting reference to
               mental contents, specifically, contexts that include a salient
               and/or unusual mental state such as a false belief. We then
               compare the potency of such observational support to the
               reliability of alternate or concomitant syntactic information
               (e.g., sentential complementation) in tasks where both children
               and adults are asked to hypothesize the meaning of novel verbs.
               The findings support the efficacy of false belief situations for
               increasing the saliency of mental state descriptions, but also
               show that syntactic information is a more reliable indicator of
               mentalistic interpretations than even the most cooperative
               contextual cues. Moreover, when syntactic and observational
               information sources converge, both children and simulated adult
               learners are vastly more likely to build conjectures involving
               mental verbs. This is consistent with a multiple-cue constraint
               satisfaction view of vocabulary acquisition. Taken together, our
               findings support the position that the informational demands of
               mapping, rather than age-related cognitive deficiency, can bear
               much of the explanatory burden for the learning problems posed by
               abstract words.",
  month     =  oct,
  year      =  2007,
  language  = "en"
}

@MISC{noauthor_undated-ze,
  title        = "Where Political News and Algorithms Meet: A Longitudinal Audit
                  of Google’s Top Stories",
  howpublished = "\url{https://scholar.google.ca/scholar?cluster=5014233451101624398\&hl=en\&as\_sdt=0,5\&sciodt=0,5}",
  note         = "Accessed: 2023-11-9"
}

@INPROCEEDINGS{Baumler2023-bm,
  title     = "Which Examples Should be Multiply Annotated? Active Learning When
               Annotators May Disagree",
  author    = "Baumler, Connor and Sotnikova, Anna and Daumé, III, Hal",
  booktitle = "Findings of the Association for Computational Linguistics: ACL
               2023",
  publisher = "Association for Computational Linguistics",
  address   = "Toronto, Canada",
  pages     = "10352--10371",
  abstract  = "Linguistic annotations, especially for controversial topics like
               hate speech detection, are frequently contested due to annotator
               backgrounds and positionalities. In such situations, preserving
               this disagreement through the machine learning pipeline can be
               important for downstream use cases. However, capturing
               disagreement can increase annotation time and expense.
               Fortunately, for many tasks, not all examples are equally
               controversial; we develop an active learning approach,
               Disagreement Aware Active Learning (DAAL) that concentrates
               annotations on examples where model entropy and annotator entropy
               are the most different. Because we cannot know the true entropy
               of annotations on unlabeled examples, we estimate a model that
               predicts annotator entropy trained using very few
               multiply-labeled examples. We find that traditional
               uncertainty-based active learning underperforms simple passive
               learning on tasks with high levels of disagreement, but that our
               active learning approach is able to successfully improve on
               passive and active baselines, reducing the number of annotations
               required by at least 24\% on average across several datasets.",
  month     =  jul,
  year      =  2023
}

@ARTICLE{Chong2023-sm,
  title     = "Whiteness as Beauty: A critical analysis of South Korean tone-up
               cream and sunscreen advertorials",
  author    = "Chong, Jinee",
  journal   = "Spectrum",
  publisher = "spectrumjournal.ca",
  number    =  10,
  abstract  = "Do online shopping advertorials for whitening skincare products
               in South Korea perpetuate a racial hierarchy wherein whiteness is
               maintained as an ideal beauty standard? If so, how is this
               hierarchy articulated and reinforced with words and images?
               Whitening products, such as tone-up creams and sunscreens, have
               become increasingly prevalent in the skincare industry in South
               Korea. Adding a level of nuance to earlier research, my research
               undertakes a critical feminist discourse analysis method to
               examine 19 skincare advertorials on the South Korean beauty
               e-commerce site, Olive Young Global. This study breaks new ground
               by taking an inductive analysis approach to analyzing these
               advertorials to produce findings comparable to similar studies in
               other Asian countries. Thus, it works to confirm the overall
               message being communicated that these products are sold as the
               key to a woman’s quest for a white beauty ideal. By undertaking
               an inductive critical discourse analysis, the research will
               develop themes based on the exploration of these advertorials
               with some guidance from existing literature. The globalization of
               beauty promotes a falsely universal white(ned) woman, and this
               project evidences a nuanced analysis of the lexical choices and
               images employed to promote the idea that whiteness and
               youthfulness equate to “natural” beauty. This critical feminist
               discourse analysis will provide insight into how a racial
               hierarchy is reinforced through media and how the exclusion of
               racialized women from spaces intended to empower all women will
               reproduce the societal hierarchy among women within the beauty
               industry.",
  month     =  may,
  year      =  2023,
  language  = "en"
}

@ARTICLE{Epstein2020-uj,
  title     = "Who gets credit for {AI}-generated art?",
  author    = "Epstein, Ziv and Levine, Sydney and Rand, David G and Rahwan,
               Iyad",
  journal   = "iScience",
  publisher = "cell.com",
  volume    =  23,
  number    =  9,
  pages     =  101515,
  abstract  = "The recent sale of an artificial intelligence (AI)-generated
               portrait for \$432,000 at Christie's art auction has raised
               questions about how credit and responsibility should be allocated
               to individuals involved and how the anthropomorphic perception of
               the AI system contributed to the artwork's success. Here, we
               identify natural heterogeneity in the extent to which different
               people perceive AI as anthropomorphic. We find that differences
               in the perception of AI anthropomorphicity are associated with
               different allocations of responsibility to the AI system and
               credit to different stakeholders involved in art production. We
               then show that perceptions of AI anthropomorphicity can be
               manipulated by changing the language used to talk about AI-as a
               tool versus agent-with consequences for artists and AI
               practitioners. Our findings shed light on what is at stake when
               we anthropomorphize AI systems and offer an empirical lens to
               reason about how to allocate credit and responsibility to human
               stakeholders.",
  month     =  sep,
  year      =  2020,
  keywords  = "Artificial Intelligence; Computer Science; Economics",
  language  = "en"
}

@ARTICLE{Darling2015-uo,
  title     = "'who's Johnny?' anthropomorphic framing in human-robot
               interaction, integration, and policy",
  author    = "Darling, Kate",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  abstract  = "People have a tendency to project life-like qualities onto
               robots. As we increasingly create spaces where robotic technology
               interacts with humans, this inclination raises ethical questions
               around use and policy. A human-robot-interaction experiment
               conducted in our lab indicates that framing robots through
               anthropomorphic language (like a personified name or story) can
               impact how people perceive and treat a robot. This chapter
               explores the effects of encouraging or discouraging people to
               anthropomorphize robots through framing. I discuss concerns about
               anthropomorphizing robotic technology in certain contexts, but
               argue that there are also cases where encouraging
               anthropomorphism is desirable. Because people respond to framing,
               framing could help to separate these cases.",
  year      =  2015,
  language  = "en"
}

@MISC{noauthor_undated-de,
  title        = "Whose death matters? A quantitative analysis of media
                  attention to deaths of Black Americans in police
                  confrontations, {2013–2016}",
  howpublished = "\url{https://scholar.google.ca/scholar?cluster=15268076325223547318\&hl=en\&as\_sdt=0,5\&sciodt=0,5}",
  note         = "Accessed: 2023-11-9"
}

@ARTICLE{Zuckerman_undated-fb,
  title  = "Whose Death Matters? A Quantitative Analysis of Media Attention to
            Deaths of Black Americans in Police Confrontations, {2013–2016}",
  author = "Zuckerman, Ethan"
}

@ARTICLE{Santurkar2023-rd,
  title         = "Whose Opinions Do Language Models Reflect?",
  author        = "Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and
                   Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori",
  journal       = "arXiv [cs.CL]",
  abstract      = "Language models (LMs) are increasingly being used in
                   open-ended contexts, where the opinions reflected by LMs in
                   response to subjective queries can have a profound impact,
                   both on user satisfaction, as well as shaping the views of
                   society at large. In this work, we put forth a quantitative
                   framework to investigate the opinions reflected by LMs -- by
                   leveraging high-quality public opinion polls and their
                   associated human responses. Using this framework, we create
                   OpinionsQA, a new dataset for evaluating the alignment of LM
                   opinions with those of 60 US demographic groups over topics
                   ranging from abortion to automation. Across topics, we find
                   substantial misalignment between the views reflected by
                   current LMs and those of US demographic groups: on par with
                   the Democrat-Republican divide on climate change. Notably,
                   this misalignment persists even after explicitly steering the
                   LMs towards particular demographic groups. Our analysis not
                   only confirms prior observations about the left-leaning
                   tendencies of some human feedback-tuned LMs, but also
                   surfaces groups whose opinions are poorly reflected by
                   current LMs (e.g., 65+ and widowed individuals). Our code and
                   data are available at
                   https://github.com/tatsu-lab/opinions\_qa.",
  month         =  mar,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Zamfirescu-Pereira2023-wp,
  title     = "Why Johnny can’t prompt: How non-{AI} experts try (and fail) to
               design {LLM} prompts",
  author    = "Zamfirescu-Pereira, J D and Wong, Richmond Y and Hartmann, Bjoern
               and Yang, Qian",
  booktitle = "Proceedings of the 2023 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "ACM",
  address   = "New York, NY, USA",
  abstract  = "… prompts can be challenging, however, and prompt -based
               interactions are brittle. Here, we explore whether non -AI-
               experts can successfully engage in “end-user prompt … of
               prompting …",
  month     =  apr,
  year      =  2023
}

@ARTICLE{Connolly2007-xn,
  title     = "Why stereotypes don't even make good defaults",
  author    = "Connolly, Andrew C and Fodor, Jerry A and Gleitman, Lila R and
               Gleitman, Henry",
  journal   = "Cognition",
  publisher = "Elsevier",
  volume    =  103,
  number    =  1,
  pages     = "1--22",
  abstract  = "Many concepts have stereotypes. This leaves open the question of
               whether concepts are stereotypes. It has been argued elsewhere
               that theories that identify concepts with their stereotypes or
               with stereotypical properties of their instances (e.g., Rosch, E.
               (1978). Principles of categorization. In E. Rosch \& B. B. Lloyd
               (Ed.), Cognition and Categorization. Hillsdale, NJ: Lawrence
               Erlbaum Associates; Smith, E. E., Medin, D. L. (1981). Categories
               and Concepts. Cambridge, MA: Harvard University Press.) fail to
               provide an adequate account of the compositionality of concepts
               (Fodor, J., Lepore, E. (1996). The red herring and the pet fish:
               Why concepts still cannot be prototypes. Cognition, 58, 253-270.;
               Fodor, J. (1998). Concepts: Where cognitive science went wrong.
               New York, NY: Oxford University Press.). This paper extends this
               argument and reports an experiment suggesting that participants
               do not assume, even as a default strategy, that complex concepts
               inherit the stereotypes of their constituents. Thus propositions
               such as ``Baby ducks have webbed feet'' were judged to be less
               likely to be true than propositions like ``Ducks have webbed
               feet.'' Moreover, manipulation of the type and number of noun
               phrase modifiers revealed a systematic departure from the
               unmodified noun's stereotype both with the addition of
               stereotypical modifiers (``Quacking ducks have webbed feet''
               versus ``Ducks have webbed feet'') and with the addition of a
               second modifier (``Baby Peruvian ducks have webbed feet'' versus
               ``Baby ducks have webbed feet''). Thus, in the general case the
               stereotypical properties of a head noun are systematically
               discounted when that head noun combines with modifiers. This
               effect represents a general principle of conceptual combination
               that argues against the inheritance of stereotypical features of
               concepts as a default strategy. Instead, we advocate a model of
               conceptual combination where concepts remain inert under
               combination, supported by a separate machinery that introduces
               pragmatic and knowledge-dependent inferences.",
  month     =  apr,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Zuckerman2021-ma,
  title     = "Why study media ecosystems?",
  author    = "Zuckerman, Ethan",
  journal   = "Inf. Commun. Soc.",
  publisher = "Informa UK Limited",
  volume    =  24,
  number    =  10,
  pages     = "1495--1513",
  month     =  jul,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Hagendorff2023-fr,
  title     = "Why we need biased {AI}: How including cognitive biases can
               enhance {AI} systems",
  author    = "Hagendorff, Thilo and Fabi, Sarah",
  journal   = "J. Exp. Theor. Artif. Intell.",
  publisher = "Taylor \& Francis",
  pages     = "1--14",
  abstract  = "ABSTRACTThis paper stresses the importance of biases in the field
               of artificial intelligence (AI). To foster efficient algorithmic
               decision-making in complex, unstable, and uncertain real-world
               environments, we argue for the implementation of human cognitive
               biases in learning algorithms. We use insights from cognitive
               science and apply them to the AI field, combining theoretical
               considerations with tangible examples depicting promising bias
               implementation scenarios. Ultimately, this paper is the first
               tentative step to explicitly putting the idea forth to implement
               cognitive biases into machines.",
  month     =  feb,
  year      =  2023
}

@ARTICLE{Wang2021-ej,
  title         = "{WikiGraphs}: A Wikipedia Text - Knowledge Graph Paired
                   Dataset",
  author        = "Wang, Luyu and Li, Yujia and Aslan, Ozlem and Vinyals, Oriol",
  journal       = "arXiv [cs.CL]",
  abstract      = "We present a new dataset of Wikipedia articles each paired
                   with a knowledge graph, to facilitate the research in
                   conditional text generation, graph generation and graph
                   representation learning. Existing graph-text paired datasets
                   typically contain small graphs and short text (1 or few
                   sentences), thus limiting the capabilities of the models that
                   can be learned on the data. Our new dataset WikiGraphs is
                   collected by pairing each Wikipedia article from the
                   established WikiText-103 benchmark (Merity et al., 2016) with
                   a subgraph from the Freebase knowledge graph (Bollacker et
                   al., 2008). This makes it easy to benchmark against other
                   state-of-the-art text generative models that are capable of
                   generating long paragraphs of coherent text. Both the graphs
                   and the text data are of significantly larger scale compared
                   to prior graph-text paired datasets. We present baseline
                   graph neural network and transformer model results on our
                   dataset for 3 tasks: graph -> text generation, graph -> text
                   retrieval and text -> graph retrieval. We show that better
                   conditioning on the graph provides gains in generation and
                   retrieval quality but there is still large room for
                   improvement.",
  month         =  jul,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INPROCEEDINGS{Arora2022-xd,
  title     = "Wikipedia Reader Navigation: When Synthetic Data Is Enough",
  author    = "Arora, Akhil and Gerlach, Martin and Piccardi, Tiziano and
               García-Durán, Alberto and West, Robert",
  booktitle = "Proceedings of the Fifteenth ACM International Conference on Web
               Search and Data Mining",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  pages     = "16--26",
  abstract  = "Every day millions of people read Wikipedia. When navigating the
               vast space of available topics using hyperlinks, readers describe
               trajectories on the article network. Understanding these
               navigation patterns is crucial to better serve readers' needs and
               address structural biases and knowledge gaps. However, systematic
               studies of navigation on Wikipedia are hindered by a lack of
               publicly available data due to the commitment to protect readers'
               privacy by not storing or sharing potentially sensitive data. In
               this paper, we ask: How well can Wikipedia readers' navigation be
               approximated by using publicly available resources, most notably
               the Wikipedia clickstream data? We systematically quantify the
               differences between real navigation sequences and synthetic
               sequences generated from the clickstream data, in 6 analyses
               across 8 Wikipedia language versions. Overall, we find that the
               differences between real and synthetic sequences are
               statistically significant, but with small effect sizes, often
               well below 10\%. This constitutes quantitative evidence for the
               utility of the Wikipedia clickstream data as a public resource:
               clickstream data can closely capture reader navigation on
               Wikipedia and provides a sufficient approximation for most
               practical downstream applications relying on reader data. More
               broadly, this study provides an example for how clickstream-like
               data can generally enable research on user navigation on online
               platforms while protecting users' privacy.",
  series    = "WSDM '22",
  month     =  feb,
  year      =  2022,
  keywords  = "wikipedia clickstream, user navigation, wikipedia server logs"
}

@INPROCEEDINGS{Sultana2019-as,
  title     = "Witchcraft and {HCI}: Morality, Modernity, and Postcolonial
               Computing in Rural Bangladesh",
  author    = "Sultana, Sharifa and Ahmed, Syed Ishtiaque",
  booktitle = "Proceedings of the 2019 CHI Conference on Human Factors in
               Computing Systems",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  number    = "Paper 356",
  pages     = "1--15",
  abstract  = "While Human-Computer Interaction (HCI) research on health and
               well-being is increasingly becoming more aware and inclusive of
               its social and political dimensions, spiritual practices are
               still largely overlooked there. For a large number of people
               around the world, especially in the global south, witchcraft,
               sorcery, and other occult practices are the primary means of
               achieving health, wealth, satisfaction, and happiness. Building
               on an eight-month long ethnography in six villages in Jessore,
               Bangladesh, this paper explores the knowledge, materials, and
               politics involved in the local witchcraft practices there. By
               drawing from a rich body of anthropological work on witchcraft,
               this paper discusses how those findings contribute to the broader
               issues in HCI around morality, modernity, and postcolonial
               computing. This paper concludes by recommending ways for smooth
               integration of traditional occult practices with HCI through
               design and policy. We argue for occult practices as an
               under-appreciated site for HCI to learn how to combat ideological
               hegemony.",
  series    = "CHI '19",
  month     =  may,
  year      =  2019,
  keywords  = "witchcraft, faith, postcolonial, ictd, rural, wellbeing"
}

@ARTICLE{Card2020-wi,
  title         = "With Little Power Comes Great Responsibility",
  author        = "Card, Dallas and Henderson, Peter and Khandelwal, Urvashi and
                   Jia, Robin and Mahowald, Kyle and Jurafsky, Dan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Despite its importance to experimental design, statistical
                   power (the probability that, given a real effect, an
                   experiment will reject the null hypothesis) has largely been
                   ignored by the NLP community. Underpowered experiments make
                   it more difficult to discern the difference between
                   statistical noise and meaningful model improvements, and
                   increase the chances of exaggerated findings. By
                   meta-analyzing a set of existing NLP papers and datasets, we
                   characterize typical power for a variety of settings and
                   conclude that underpowered experiments are common in the NLP
                   literature. In particular, for several tasks in the popular
                   GLUE benchmark, small test sets mean that most attempted
                   comparisons to state of the art models will not be adequately
                   powered. Similarly, based on reasonable assumptions, we find
                   that the most typical experimental design for human rating
                   studies will be underpowered to detect small model
                   differences, of the sort that are frequently studied. For
                   machine translation, we find that typical test sets of 2000
                   sentences have approximately 75\% power to detect differences
                   of 1 BLEU point. To improve the situation going forward, we
                   give an overview of best practices for power analysis in NLP
                   and release a series of notebooks to assist with future power
                   analyses.",
  month         =  oct,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@INCOLLECTION{Hendricks2018-co,
  title     = "Women also snowboard: Overcoming bias in captioning models",
  author    = "Hendricks, Lisa Anne and Burns, Kaylee and Saenko, Kate and
               Darrell, Trevor and Rohrbach, Anna",
  booktitle = "Computer Vision – ECCV 2018",
  publisher = "Springer International Publishing",
  address   = "Cham",
  pages     = "793--811",
  abstract  = "Most machine learning methods are known to capture and exploit
               biases of the training data. While some biases are beneficial for
               learning, others are harmful. Specifically, image captioning
               models tend to exaggerate biases present in training data (eg, if
               a word is present in 60\% of training sentences, it might be
               predicted in 70\% of sentences at test time). This can lead to
               incorrect captions in domains where unbiased captions are
               desired, or required, due to over-reliance on the learned prior
               and image context. In this work we …",
  series    = "Lecture notes in computer science",
  year      =  2018
}

@ARTICLE{Stankiewicz2008-lg,
  title    = "Women as Sex Objects and Victims in Print Advertisements",
  author   = "Stankiewicz, Julie M and Rosselli, Francine",
  journal  = "Sex Roles",
  volume   =  58,
  number   =  7,
  pages    = "579--589",
  abstract = "This content analysis examined the depiction of women in 1,988
              advertisements from 58 popular U.S. magazines. Advertisements were
              coded with respect to whether women were presented as sex objects
              and/or as victims using a scheme developed by the researchers. On
              average across magazines, one of two advertisements that featured
              women portrayed them as sex objects. Women appeared as victims in
              just under ten percent of the advertisements. Men’s, women’s
              fashion, and female adolescent magazines were more likely to
              portray women as sex objects and as victims than news and
              business, special interest, or women’s non-fashion magazines. The
              implications of viewing advertisements depicting women as sex
              objects and as victims, especially sexualized victims, are
              discussed.",
  month    =  apr,
  year     =  2008
}

@ARTICLE{Lucy2022-rr,
  title         = "Words as Gatekeepers: Measuring Discipline-specific Terms and
                   Meanings in Scholarly Publications",
  author        = "Lucy, Li and Dodge, Jesse and Bamman, David and Keith,
                   Katherine A",
  journal       = "arXiv [cs.CL]",
  abstract      = "Scholarly text is often laden with jargon, or specialized
                   language that divides disciplines. We extend past work that
                   characterizes science at the level of word types, by using
                   BERT-based word sense induction to find additional words that
                   are widespread but overloaded with different uses across
                   fields. We define scholarly jargon as discipline-specific
                   word types and senses, and estimate its prevalence across
                   hundreds of fields using interpretable, information-theoretic
                   metrics. We demonstrate the utility of our approach for
                   science of science and computational sociolinguistics by
                   highlighting two key social implications. First, we measure
                   audience design, and find that most fields reduce jargon when
                   publishing in general-purpose journals, but some do so more
                   than others. Second, though jargon has varying correlation
                   with articles' citation rates within fields, it nearly always
                   impedes interdisciplinary impact. Broadly, our measurements
                   can inform ways in which language could be revised to serve
                   as a bridge rather than a barrier in science.",
  month         =  dec,
  year          =  2022,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Srinivasan2021-aq,
  title         = "Worst of Both Worlds: Biases Compound in Pre-trained
                   Vision-and-Language Models",
  author        = "Srinivasan, Tejas and Bisk, Yonatan",
  journal       = "arXiv [cs.CL]",
  abstract      = "Numerous works have analyzed biases in vision and pre-trained
                   language models individually - however, less attention has
                   been paid to how these biases interact in multimodal
                   settings. This work extends text-based bias analysis methods
                   to investigate multimodal language models, and analyzes
                   intra- and inter-modality associations and biases learned by
                   these models. Specifically, we demonstrate that VL-BERT (Su
                   et al., 2020) exhibits gender biases, often preferring to
                   reinforce a stereotype over faithfully describing the visual
                   scene. We demonstrate these findings on a controlled
                   case-study and extend them for a larger set of
                   stereotypically gendered entities.",
  month         =  apr,
  year          =  2021,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Dumit2014-nh,
  title     = "Writing the Implosion: Teaching the World One Thing at a Time",
  author    = "Dumit, Joseph",
  journal   = "Cult. Anthropol.",
  publisher = "journal.culanth.org",
  volume    =  29,
  number    =  2,
  pages     = "344--362",
  abstract  = "This article puts a reading of Gilles Deleuze’s Cinema 2 in
               dialogue with Donna Haraway’s works and methods. Working through
               the former helps me unpack the process of Haraway’s inquisitive
               “implosion” method and some of its aims better. I describe this
               as exploring how the world is interconnected one process and
               thing at a time, how these connections are vitally and
               politically important, and how this work is inexhaustible.
               Following this exegesis, I offer a series of exercises for
               putting this method into practice, one that I use myself and
               teach to graduate students and undergrads.",
  month     =  may,
  year      =  2014,
  keywords  = "ethnography; writer's block; history of knowledge; commitment",
  language  = "en"
}

@ARTICLE{Ging2018-qb,
  title     = "'Written in these scars are the stories {I} can't explain': a
               content analysis of pro-ana and thinspiration image sharing on
               Instagram",
  author    = "Ging, D and Garvey, S",
  journal   = "New Media \& Society",
  publisher = "journals.sagepub.com",
  abstract  = "Since pro-anorexia websites began to appear in the 1990s, there
               has been a growing body of academic work on pro-ana and
               thinspiration communities online. Underpinned by a range …",
  year      =  2018
}

@INPROCEEDINGS{Hu2020-kr,
  title     = "{XTREME}: A Massively Multilingual Multi-task Benchmark for
               Evaluating Cross-lingual Generalisation",
  author    = "Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig,
               Graham and Firat, Orhan and Johnson, Melvin",
  editor    = "Iii, Hal Daumé and Singh, Aarti",
  booktitle = "Proceedings of the 37th International Conference on Machine
               Learning",
  publisher = "PMLR",
  volume    =  119,
  pages     = "4411--4421",
  abstract  = "Much recent progress in applications of machine learning models
               to NLP has been driven by benchmarks that evaluate models across
               a wide variety of tasks. However, these broad-coverage benchmarks
               have been mostly limited to English, and despite an increasing
               interest in multilingual models, a benchmark that enables the
               comprehensive evaluation of such methods on a diverse range of
               languages and tasks is still missing. To this end, we introduce
               the Cross-lingual TRansfer Evaluation of Multilingual Encoders
               (XTREME) benchmark, a multi-task benchmark for evaluating the
               cross-lingual generalization capabilities of multilingual
               representations across 40 languages and 9 tasks. We demonstrate
               that while models tested on English reach human performance on
               many tasks, there is still a sizable gap in the performance of
               cross-lingually transferred models, particularly on syntactic and
               sentence retrieval tasks. There is also a wide spread of results
               across languages. We will release the benchmark to encourage
               research on cross-lingual learning methods that transfer
               linguistic knowledge across a diverse and representative set of
               languages and tasks.",
  series    = "Proceedings of Machine Learning Research",
  year      =  2020
}

@ARTICLE{Vinsel_undated-un,
  title  = "You're Doing It Wrong: Notes on Criticism and Technology Hype",
  author = "Vinsel, Lee"
}

@ARTICLE{Jurgens2023-zs,
  title         = "Your spouse needs professional help: Determining the
                   Contextual Appropriateness of Messages through Modeling
                   Social Relationships",
  author        = "Jurgens, David and Seth, Agrima and Sargent, Jackson and
                   Aghighi, Athena and Geraci, Michael",
  journal       = "arXiv [cs.CL]",
  abstract      = "Understanding interpersonal communication requires, in part,
                   understanding the social context and norms in which a message
                   is said. However, current methods for identifying offensive
                   content in such communication largely operate independent of
                   context, with only a few approaches considering community
                   norms or prior conversation as context. Here, we introduce a
                   new approach to identifying inappropriate communication by
                   explicitly modeling the social relationship between the
                   individuals. We introduce a new dataset of
                   contextually-situated judgments of appropriateness and show
                   that large language models can readily incorporate
                   relationship information to accurately identify
                   appropriateness in a given context. Using data from online
                   conversations and movie dialogues, we provide insight into
                   how the relationships themselves function as implicit norms
                   and quantify the degree to which context-sensitivity is
                   needed in different conversation settings. Further, we also
                   demonstrate that contextual-appropriateness judgments are
                   predictive of other social factors expressed in language such
                   as condescension and politeness.",
  month         =  jul,
  year          =  2023,
  archivePrefix = "arXiv",
  primaryClass  = "cs.CL"
}

@ARTICLE{Literat2019-ho,
  title     = "Youth collective political expression on social media: The role
               of affordances and memetic dimensions for voicing political views",
  author    = "Literat, Ioana and Kligler-Vilenchik, Neta",
  journal   = "New Media \& Society",
  publisher = "SAGE Publications",
  volume    =  21,
  number    =  9,
  pages     = "1988--2009",
  abstract  = "Social media are recognized as important outlets for youth
               political expression, yet the affordances of different platforms
               may shape the forms and styles of expression that young people
               deploy. In order to gain a deeper understanding of the ways
               social media affordances shape youth voice, this article examines
               young people?s political expression on the popular app musical.ly
               in the context of the 2016 US presidential election. Employing
               quantitative and qualitative content analysis on 1651
               youth-created videos, we examine how young people use platform
               affordances, political hashtags, and memetic dimensions to convey
               a range of expressive political practices. In particular, through
               the analysis of content, form, and stance, our research
               illuminates how social media afford collective political
               expression for youth, by allowing them to deliberately connect to
               an assumed like-minded audience with similar beliefs through the
               use of shared symbolic resources.",
  month     =  sep,
  year      =  2019
}

