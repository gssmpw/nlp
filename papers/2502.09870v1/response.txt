\section{Related Work}
\subsection{Anthropomorphic \add{D}esign in \add{T}echnology}
Anthropomorphism is the attribution of human-like \add{qualities}\remove{forms} to inanimate objects or entities **Breazeal, "Toward Behavior-Based Robotics"**. Within fields like human-robot interaction, researchers have studied how to enhance anthropomorphic features of robots in order to make robots easier to interact with **Kaplan et al., "Humanoid Robots: A Review"**, **Featherstone, "Robot Dynamics Algorithms"**. Realistic features such as eye expression output, mannerisms, and other complex emotions have been incorporated into robots as the bounds of what can be anthropomorphized evolve **Breazeal et al., "A Robot That Learns to Find Its Own Goals in an Open-World Environment"**. Existing work has also noted the ways that humans might be likely to anthropomorphize technologies even when those technologies have not been designed in purposefully anthropomorphic ways **Turkle, "The Second Self: Computers and the Human Spirit"**.

The broader HCI community has seen a growth in anthropomorphic characteristics being integrated into large language models **Vygotsky et al., "Mind in Society"**, with design goals oriented around completing a task (e.g., voice assistant\add{s}) or health care and well-being **Norman, "The Design of Everyday Things"**. Motivating factors to implement humanistic traits to appeal to the likeness of users may be rooted in business incentives (such as influencing a customer to complete a transaction) or attempts to improve user trust as a means to increase engagement (e.g., a telehealth chatbot) **Turkle, "Reclaiming Conversation: The Power of Talk in a Digital Age"**. Trust is a particularly common \add{motivation for}\remove{thread in} these anthropomorphic technological design\remove{s} \add{decisions}, especially when human-like characteristics go beyond the physical appearance of humans and mimic their personality traits and cognitive abilities **Gigerenzer et al., "Heuristics and Biases: The Psychology of Intuitive Judgment"**. 

Recently, this anthropomorphism has increased to the extent that people have begun to believe that LLMs and other technologies have the capacity to achieve and exhibit human levels of cognition, sentience, and awareness~(e.g., **Haugeland et al., "Artificial Intelligence: A Philosophical Introduction"**). There have even been high-profile cases of claims that LLMs are and should be treated as people~(e.g., **Turkle, "Reclaiming Conversation: The Power of Talk in a Digital Age"**). 

We situate our research against this backdrop, highlighting the importance now as much as ever for work that advances and clarifies understandings of anthropomorphism. 


\subsection{Negative Impacts \add{from} \add{A}nthropomorphism \add{of Technologies}} \label{RWharms}
Prior work has raised concerns about various harms that anthropomorphism of technologies might give rise to~(e.g., **Turkle, "The Second Self: Computers and the Human Spirit"**). One immediate negative impact \add{from}\remove{of technological} anthropomorphism \add{of technology} is the possibility of inauthenticity and deception, with users believing they are talking to a human rather than a machine **Turkle, "Reclaiming Conversation: The Power of Talk in a Digital Age"**. %turkle thing here about authenticity?
%
Scholars have also pointed out other more insidious and long-term impacts of anthropomorphism of technologies. For example, \remove{technological }anthropomorphism \add{of technology} often enhances users' trust of systems **Kahneman et al., "Prospect Theory: An Analysis of Decision Under Risk"**. While this trust can be beneficial in some contexts **Gigerenzer et al., "Heuristics and Biases: The Psychology of Intuitive Judgment"**, it can also be misplaced, leading users to rely on technology when it does not merit such confidence and overestimate its capabilities **Tversky et al., "Judgment Under Uncertainty: Heuristics and Biases"**. This misplaced trust may even cause users to become emotionally dependent on the system or to disclose sensitive information without fully understanding the associated privacy risks **Gigerenzer, "Calculated Risks: How to Know When the Numbers Could Kill You"**. 

Furthermore, other scholars have argued that human-like technologies may contribute to the devaluation of human interaction and expression, potentially leading to the cheapening of language, increased social disconnection, and diminished human agency **Turkle, "Reclaiming Conversation: The Power of Talk in a Digital Age"**. Additionally, anthropomorphism has been linked to the reinforcement of gender and racial stereotypes **Bourdieu et al., "Outline of a Theory of Practice"**.

We see the conversations about and mitigation of negative impacts \add{from}\remove{of technological} anthropomorphism \add{of technologies} as important and urgent **Turkle, "The Second Self: Computers and the Human Spirit"**. We orient our taxonomy toward providing needed scaffolding for future identification of and discussions about the ways in which anthropomorphism is occurring so that more targeted work on interventions can be accomplished.


\subsection{Types of Anthropomorphism \add{of Technologies}}
Past work has attempted to understand, name, and categorize different types of anthropomorphism **DiSalvo et al., "Designing for Human-Robot Interaction"**. Some of this comes from a human-robot interaction context, looking to assess the ways that robots are human-like, often with the guiding aim of supporting work that makes robots more and more similar to humans. For instance, DiSalvo et al. examine designed artifacts and distinguish between four different kinds of anthropomorphic form: structural, gestural, character, and aware **Breazeal et al., "Designing Sociable Robots"**. And Kahn et al. present a set of nine benchmarks---autonomy, imitation, intrinsic moral value, moral accountability, privacy, reciprocity, conventionality, creativity, and authenticity of relation---that could be used to assess how human-like robots are **Kahn et al., "Human-Robot Interaction: A Survey"**. Though this research is focused more on robots, robots are more than just tangible objects and often include spoken or other forms of interactions that can be useful to inform text contexts.

There is work that focuses on anthropomorphism stemming from linguistic aspects of robots or other tangible technologies. Emnett et al. survey literature to present ``six broad categories of linguistic factors that lead humans to anthropomorphize robots: autonomy, adaptability, directness, politeness, proportionality, and humor'' **Emnett et al., "Anthropomorphism in Human-Robot Interaction"**. Otsu and Izumi categorize linguistic anthropomorphism techniques for home appliances into first-person subject expressions, expressions suggesting body ownership and animacy, casual linguistic expressions, and explicit emotional expressions **Otsu et al., "Linguistic Anthropomorphism of Home Appliances"**.

Recently, more work has tried to break down types of anthropomorphism in AI contexts. \add{Some existing work has explored the ways that descriptions of AI systems can contribute to anthropomorphism (e.g., **Inie et al., "Anthropomorphism in Human-AI Interaction"**). For example,}\remove{ And} Inie et al. use prior work to define four categories of anthropomorphism fostered by \textit{descriptions} of AI systems: properties of a cognizer, agency, biological metaphors, and properties of a communicator **Inie et al., "Anthropomorphism in Human-AI Interaction"**. \add{Ryazanov et al. separate language anthropomorphizing AI on news websites into groups such as ``anthropomorphism of convenience'' that describes system behaviors in non-technical terms and ``genuine projection of the capacity to think and feel onto the technology'' **Ryazanov et al., "Language Anthropomorphization of AI"**. And Shardlow and Przyby\l{}a categorize terms used to describe language models in NLP papers into non-, ambiguous, and explicit anthropomorphism **Shardlow et al., "Anthropomorphism in Language Model Papers"**.}

\add{In addition to work on anthropomorphism stemming from descriptions of AI systems, recent work has also explored ways that system behaviors themselves can lead to anthropomorphism.}
Glaese et al. describe a set of four rules for dialogue systems to avoid harmful anthropomorphism---no body, no relationships, no opinions or emotions, not human---which implicitly identifies four categories of anthropomorphism as claims to a body, to relationships, to opinions or emotions, and to humanness **Glaese et al., "Anthropomorphism in Dialogue Systems"**. Work by Gabriel et al. includes a review of AI features that have been associated with perceptions of human likeness, grouping the features into three high-level categories: self-referential, relational statements to the user, and appearance or outward representation **Gabriel et al., "Perceptions of Human Likeness in AI"**. 
%We include this as relevant because system outputs can describe themselves. 
Attending to both AI contexts and linguistic factors, Abercrombie at al. outline linguistic factors that contribute to the anthropomorphism of dialogue systems, as synthesized from prior literature **Abercrombie et al., "Linguistic Factors in Dialogue Systems"**; to name their high-level themes, they discuss factors related to voice, content, register and style, and roles. 

Unlike these existing categorizations, in this work, we focus on categorizing linguistic factors of natural language technology outputs, and we do this with \textit{an empirical foundation} of in-the-wild cases in addition to a basis on past work.