\section{User Study} \label{user_study}
To investigate the impact of a robot's ability to understand indirect speech acts on people's perception, we conducted a Wizard-of-Oz experiment with 36 participants on three different physical collaborative tasks.

The experiment employed a mixed-method experimental design~\cite{creswell1999mixed}, collecting quantitative data through a questionnaire on team fluency, goal alignment, performance trust, and anthropomorphism as dependent variables, as well as qualitative data from interview responses. The Speech Mode (ISA vs. Non-ISA) served as a between-subject factor, with half of the participants interacting with a robot capable of understanding ISAs, while the other half interacted with a robot unable to comprehend ISAs. Each participant completed three tasks in counter-balanced order with the robot using one of the assigned Speech Modes, which was followed by a semi-structured interview. We provide additional detail on our experimental design in the following sections.

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{exp.pdf}
    \caption{(a) Experiment setup: The participant, robot, and experimenter 1 were all present in the same room. The participant and robot were seated on opposite sides of a table, with the shared workspace located in the centre. Experimenter 1 (Speech Wizard) sat next to the robot, near the emergency button, and operated the robot's speech-WoZ interface. Experimenter 2 (Motion Wizard) was positioned behind a one-side mirror, allowing for a clear view of the room, and was responsible for teleoperating the robot's arm movements. (b) Experiment procedure: Each participant first completed a pre-questionnaire before being assigned to either the ISA or non-ISA group. The participant then performed three tasks with the robot in a counter-balanced order. Before each task, participants watched a tutorial video and read a task description. After completing each task, they filled out a post-task questionnaire. The experiment concluded with a semi-structured interview.\vspace{-1em}}
    \label{fig:exp}
\end{figure*}

\subsection{Experimental Design}
\subsubsection{Apparatus and Setup}
We used TIAGo as the robot agent in our study. TIAGo is a mobile manipulator robot with anthropomorphic features, including a head, neck, torso, and arm, making it well-suited for HRI research~\cite{pages2016tiago}. The robot and the participant were on the opposite side of a table, which acted as the shared workspace between the two parties. Given the current limitations of algorithms in achieving human-level understanding and generating accurate verbal responses to ISAs, and to minimise the influence of potential robotic failures on experimental outcomes, we chose to use a Wizard-of-Oz (WoZ) approach. WoZ is a classic methodology in HRI research, where human operators discretely control the robot’s behaviour to simulate advanced robotic capabilities that the system itself may not be able to achieve autonomously yet or that would not be robust for real-time interaction~\cite{martelaro2016wizard}. This approach allows researchers to focus on understanding user interactions with the robot without being hindered by technological limitations or safety issues related to autonomous motion control. 
In this experiment, two experimenters discreetly controlled the robot to provide realistic and fluid responses, enabling a better assessment of human-robot interaction dynamics.

One of the experimenters (Motion Wizard) was teleoperating the robot's movement behind a one-side mirror, which allowed them to have a clear view of both the robot and the participant while remaining hidden from the participant.
This teleoperation was possible thanks to custom-made software developed by our team that allowed the Motion Wizard to send commands to the robot remotely. This WoZ software was built using the Robotics Operating System (ROS) and the TIAGo API. We implemented the arm actions using inverse kinematics, which calculates the joint configuration based on the desired Cartesian coordinates of the end effector~\cite{chitta2017}. In addition to moving the end effector within a 3D space above the workspace, the robot's head also had 2 degrees of freedom, which allowed the Motion Wizard to observe through the robot's camera and actively engage with the participant. Safety was ensured by a collision detection function that automatically disabled the arm controller when abnormal tolerance values were detected in the joints. Virtual walls were also implemented around the robot's arm to restrict its movement, preventing it from exceeding a designated range or approaching the participant too closely.

The other experimenter (Speech Wizard) was sitting beside the robot's emergency button and operating the speech-WoZ interface through a laptop to give verbal responses, which were scripted in advance (See \autoref{speech_model} in detail). Participants were informed that the Speech Wizard served as a safeguard, responsible for ensuring their physical safety by using the emergency button located on the robot's base if necessary. This explanation led participants to view the Speech Wizard’s presence as a precautionary measure. TIAGo utilises Acapela Group's Text-to-Speech technology, which carries out the phonetic transcription of the text, generates prosody for the speech, produces the audio signal, and plays through TIAGo's speaker. \autoref{fig:exp}a demonstrates the experimental setup.


\begin{table*}[t]
\caption{Examples of participants' requests, interpretations, and robot's responses in different Speech Modes. The request examples are from \autoref{fig:teaser}. (P: participant; R: robot)}
\label{tb:example}
\resizebox{0.95\textwidth}{!}{
\begin{tabular}{llll}
\hline
\multirow{2}{*}{Request Examples} &
  \multirow{2}{*}{Interpretations} &
  \multicolumn{2}{c}{Robot's Responses} \\ \cline{3-4} 
 &
   &
  If in the ISA group &
  If in the Non-ISA group \\ \hline
P: Please also sort these cubes. &
  \begin{tabular}[c]{@{}l@{}}\textbf{Direct}\\ \textit{Literal}: Sort cubes.\\ \textit{Intent}: Sort cubes.\end{tabular} &
  \multicolumn{2}{c}{\begin{tabular}[c]{@{}c@{}}R: Yes, sure. (Act on the intent)\end{tabular}} \\ \hline
  \begin{tabular}[c]{@{}l@{}}P: Can you move it to here\\ please? \end{tabular} &
  \begin{tabular}[c]{@{}l@{}}\textbf{Indirect}\\ \textit{Literal}: Ask for the ability to move it.\\ \textit{Intent}: Move it.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: Got it.\\ (Act on the intent)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: Yes, I can do that.\\ (No action)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}P: You could take the small\\ yellow cylinder ...\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}\textbf{Indirect}\\ \textit{Literal}: Suggest an action option to\\ take the cylinder.\\ \textit{Intent}: Take the cylinder.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: Okay.\\ (Act on the intent)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: Well noted.\\ (No action)\end{tabular} \\ \hline
 \begin{tabular}[c]{@{}l@{}}P: Let's move on to the third\\ column.  \end{tabular} &
  \begin{tabular}[c]{@{}l@{}}\textbf{Indirect}\\ \textit{Literal}: Suggest moving on to the third\\ column.\\ \textit{Intent}: Sort the third column.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: Working on that.\\ (Act on the intent)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: It's a good suggestion.\\ (No action)\end{tabular} \\ \hline
P: And the last one. &
  \begin{tabular}[c]{@{}l@{}}\textbf{Indirect}\\ \textit{Literal}: A reference to the last thing.\\ \textit{Intent}: Rotate to the last face.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: Sure.\\ (Act on the intent)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: ...\\ (Silence. No action)\end{tabular} \\ \hline
\begin{tabular}[c]{@{}l@{}}P: The blue block needs to ...\\ P: Oh, actually. Wait.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}\textbf{Indirect}\\ \textit{Literal}: Provide information for the blue\\block and wait.\\ \textit{Intent}: Move the blue block to a position. \\ The last command is wrong, stop the\\ current action and wait for the next one.\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: Got it.\\ (Act on the intent, \\ then stop halfway)\end{tabular} &
  \begin{tabular}[c]{@{}l@{}}R: Thank you for the info.\\ (No action)\end{tabular} \\ \hline
\end{tabular}
}
\end{table*}

\subsubsection{Robot's Speech Understanding} \label{speech_model}
The robot's Speech Mode was a between-subject independent variable with two conditions. In the ISA condition, the robot could understand participants' ISAs and respond with appropriate actions. In the Non-ISA condition, the robot was only able to grasp the literal meaning of requests and respond to commands that were stated in imperative sentences. The literal meaning of ISAs was interpreted by isolating them from their contextual elements, following the guidelines of Searle's putative facts~\cite{searle1975indirect}. We selected some representative requests from participants to demonstrate how the direct and indirect speech acts were interpreted and responded to during our experiment (shown in \autoref{tb:example} and \autoref{fig:teaser}). To respond to both indirect and direct requests, the speech-WoZ interface featured predefined sentences, such as ``Sure,'' ``Okay, working on that,'' and ``Yes, I have the ability to do that.'' Utterances without command intent, such as ``Thank you, TIAGo,'' were responded to as natural conversational exchanges like ``You're welcome.'' The selection of phrases was guided by the aforementioned literature and further refined through insights gained from four pilot studies. The interface also provided a text box that allowed the Speech Wizard to input responses to any unexpected speech. To maintain the flow of interaction and avoid constraining the use of ISAs, participants were allowed to use gestures along with their speech, which experimenters interpreted and responded to accordingly. Notably, no participants reported noticing that the experimenter sitting in front of them was controlling the robot's speech.

\subsubsection{Collaboration Tasks}
A recent systematic review~\cite{semeraro2023human} categorised the HRC tasks for robotic manipulators as: (1) collaborative assembly, where humans and robots work together to assemble complex objects through a series of sequential sub-processes; (2) object handling \& handover, involving the joint grasping and placement of objects by humans and robots, as well as the handover of objects from the robot to the human; and (3) collaborative manufacturing, where both humans and robots perform tasks that permanently alter an object, such as polishing and drilling. For safety considerations, we modified the object handling and handover task to a turn-based pick-and-place activity, where both the robot and the human participated in sorting cubes. Based on this taxonomy, we designed and implemented three physical collaborative tasks for our experiment: (1) a foam brick assembly task~\cite{vogt2016learning}, (2) a 3*3 cubes sorting task~\cite{faroni2020layered}, and (3) a hexagonal prism polishing task~\cite{nikolaidis2015efficient}. 
In each task, the robot lacked prior information about the task's goal and plan, requiring the participant to relay the instructions to the robot at the beginning and verbally guide the team's actions throughout the entire activity.

The \textbf{assembly} task (\autoref{fig:exp}bi) required the human-robot team to build a structure using foam bricks. We distributed 18 bricks of various shapes between the human and robot, with 12 bricks required for constructing the target structure and 6 incorrect bricks that should not be used.  
Only the participant was provided with a photo of the structure they had to build, while the robot had no prior knowledge of the structure. The bricks were initially randomly placed in the robot or the participant's stock, and each party was only allowed to take bricks from their own pile. 
Participants could only manipulate the bricks on their side and needed to communicate and coordinate with the robot to have it add its bricks to the construction. 

The \textbf{sorting} task (\autoref{fig:exp}bii) used nine 5*5*5cm cubes that needed to be rearranged according to two categorical attributes: texture and version.  Each cube featured a type of surface texture (smooth, medium, rough), and an ArUco marker~\cite{garrido2014automatic} encoding its version information (old, intermediate, new). 
To mimic an information asymmetry sorting task, the participants were able to touch and feel the texture, whereas the robot could scan the ArUco marker to access the cube's version.
We used apparently similar ArUco markers for version information to make it impossible for participants to distinguish between them by sight alone. Only the exchange of information between teammates made it possible to achieve the task: to arrange the cubes in a gradient from rough to smooth in one dimension and from old to new in the orthogonal direction. 

The \textbf{polishing} task (\autoref{fig:exp}biii) constituted a simple instantiation of a manufacturing task. The robot was responsible for holding and turning the hexagonal prism, and the participant polished each surface three times using sandpaper. Every time the participants were happy with the sanding, they had to communicate to the robot to turn the object to show a face that had not been polished. This scenario was designed to simulate a situation where the hexagonal prism was too heavy or hazardous for a human to lift and rotate, requiring cooperation with the robot to successfully complete the task.


\subsection{Participants}
We conducted \textit{a priori} power analysis to calculate the sample size for our experiment using \textit{G*Power}~\cite{faul2007g}. The calculation was based on a medium effect size of $f=0.25$, an alpha-level of 0.05, and a power of 0.9. As a result, we recruited 36 ($Female:Male = 19:17$,  $M_{age} = 24.08$, $Std_{age} = 5.75$) participants who were all fluent English speakers. We used the three questions of the interaction subscale from the Negative Attitude Toward Robots Scale (NARS Questionnaire) \cite{nomura2006measurement}, as they were relevant to working and talking to a robot (see \autoref{apd:NARS}). These questions were used to screen out individuals who exhibited strong negative responses towards robots and who could possibly feel distressed interacting with a robot (i.e., a rating higher than 3). Each experiment took about 60 minutes and participants were compensated with a \$30 voucher. Our experiment received ethics approval from the Institutional Review Board (IRB).

\subsection{Procedure}
The experiment procedure is shown in \autoref{fig:exp}b. Upon welcoming the participants, the study started with a pre-questionnaire, which captured participant demographics and their prior interaction experience with robots, voice assistants, and in performing physical collaborative tasks. The prior experience served as covariates in data analysis. Before each task, participants were provided with a tutorial video and a written task description, which included instructions and specified the objectives of the task. Additionally, a picture of the target structure for the assembly task was presented to illustrate the final goal. 
Participants were required to lead the collaboration and verbally relay the team's objectives to their robot teammate, TIAGo. After each task, participants completed a post-task questionnaire that assessed their perceptions of the team's fluency and goal alignment~\cite{hoffman2010effects}, performance trustworthiness using Multi-Dimensional Measure of Trust (MDMT)~\cite{malle2021multidimensional}, and the robot's anthropomorphism using the Godspeed Questionnaire (GSQ)~\cite{bartneck2023godspeed}. Each participant interacted with one of the robot's Speech Modes (ISA or Non-ISA) and engaged in three tasks, which were assigned in a counter-balanced order. Finally, the study ended with a semi-structured interview. Each experiment took about 60 minutes, including the interview.

\subsection{Data Collection and Analysis}
We collected the quantitative data using standard questionnaires and the qualitative data through a semi-structured interview. The following dependent variables were collected after each task:

\begin{itemize}
    \item Team fluency: To answer RQ1.1, we used the 7-point team fluency sub-scale with 3 items, from~\cite{hoffman2010effects}, which adapted the Working Alliance Inventory~\cite{horvath1989development} on Human-Robot Collaboration.
    \item Goal alignment: For the goal alignment in RQ1.2, we utilised the 7-point goal sub-scale with 3 items, from~\cite{hoffman2010effects}. 
    \item Performance trust: The 4-item MDMT performance trust scale results were collected to measure participants' perceived capability and reliability of the robot (RQ2). The scale has 5 points and an additional option for ``Does not fit'' to prevent forced and possibly meaningless ratings~\cite{malle2021multidimensional}.
    \item Anthropomorphism: To answer RQ3, the 5-point anthropomorphism sub-scale with 5 items of the GSQ was used. As the study was focused on the robot's understanding of communication rather than the appearance of the robot, the last item, ``Moving rigidly/elegantly'', was changed to ``communicating rigidly/elegantly'', which has been shown to be reliable by~\cite{laban2019working}. 
\end{itemize}

\vspace{-2em}

To analyse the impact of the robot's Speech Modes (ISA vs. Non-ISA) and covariates (participants' prior interaction experience with robots, voice assistants, and physical collaborative tasks), we used Cumulative Link Mixed Models (CLMMs) via the "ordinal" package in R~\cite{christensen2019ordinal}. This analysis is appropriate given the ordinal nature of our dependent variables. Additionally, task type, scales' sub-item ID, and participant ID were included as random effects in our model to account for potential variability within group structures and repeated measures~\cite{brown2021introduction}.

At the end of the experiment, we conducted a semi-structured interview lasting approximately 15 minutes to gather qualitative feedback from participants. The Motion Wizard observed participants' behaviours during the experiment. Instances of participants using indirect speech acts were further explored through follow-up questions during the interviews. The interviews were intended to supplement the quantitative results and provide insight into their subjective feelings regarding the overall experience during the collaboration.
Given the between-subjects design of the study, we began the interview by explaining the experimental condition that participants had not experienced, ensuring they had a comprehensive understanding of the study. We disclosed that the experimenters controlled the robot's actions and speech only after the interview concluded.

The interview results were transcribed and analysed through reflexive thematic analysis (RTA), which was well-suited to this study because it emphasised the researchers’ active role in constructing themes, thereby fostering flexibility, creativity, and critical reflection. This approach permits researchers to integrate their own insights and observations from the experimental process, making it particularly effective for exploring subtle phenomena~\cite{braun2023doing}. Following the 6-phase guidance by~\cite{braun2006using}, two authors of this paper, both of whom possess substantial expertise in human-robot interaction and human-computer interaction, conducted the RTA. In phase 1, researchers thoroughly reviewed all transcriptions. In phase 2, they inductively generated initial codes at the sentence level, which were either semantic, representing participants' explicit feelings, or latent, reflecting deeper meanings inferred from the data based on researchers' knowledge background. In phase 3, they constructed the initial themes and categorised the codes. Up to this point, the work had been carried out individually by each researcher. In phase 4, two researchers cooperatively discussed and reviewed the themes through multiple rounds. In phase 5, the themes were defined and named. In phase 6, researchers drafted the initial report of the qualitative analysis. Phases 4 to 6 were repeated over several rounds, during which the themes were iteratively refined and discrepancies addressed. This process aligns with the RTA principles, which emphasise continuous iterative reflexivity to ensure the analysis remains progressively recursive~\cite{terry2017thematic}.

