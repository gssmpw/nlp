\section{Results}
\autoref{tb:demo} shows a summary of participants' demographics and their prior interaction experience with robots, voice assistants, and physical collaborative tasks (with either humans or robots). Next, we report our quantitative and qualitative findings. 

\begin{table*}[]
\caption{Overview of participants ' demographic information and their prior interaction experience with robots, voice assistants, and physical collaborative tasks. (Rarely: less than once a month; Sometimes: at least once a month but less than once a week; Often: at least once a week but less than once a day; Very often: at least once a day.)}
\label{tb:demo}
% \resizebox{\textwidth}{!}{
\begin{tabular}{lclclclclc}
\hline
\multicolumn{2}{c}{Gender} & \multicolumn{2}{c}{Age} & \multicolumn{2}{c}{Robots} & \multicolumn{2}{c}{Voice Assistant} & \multicolumn{2}{c}{Physical Collaborative Tasks} \\ \hline
Female & 52.8\% & 18-25 & 72.2\% & No experience           & 33.3\% & Never      & 5.6\%  & Never      & 25.0\% \\
Male   & 47.2\% & 26-35 & 19.4\% & With domestic robot     & 55.6\% & Rarely     & 55.6\% & Rarely     & 22.2\% \\
       &        & 36-45 & 8.3\%  & With desktop pet robot   & 2.8\%  & Sometimes & 19.4\% & Sometimes & 27.8\% \\
       &        &       &        & With social robot       & 0.0\%  & Often      & 13.9\% & Often      & 19.4\% \\
       &        &       &        & With industrial robot    & 8.3\%  & Very often & 5.6\%  & Very often & 5.6\%  \\
       &        &       &        & With more than one type & 22.2\% &            &        &            &        \\ \hline
\end{tabular}
% }
\end{table*}

\subsection{Quantitative Findings}
In this section, we present the key results from CLMM analysis. Detailed results regarding covariates, random effects, model fit, and model formula are provided in~\autoref{apd:model}.

\begin{figure*}[]
    \centering
    \includegraphics[width=.97\textwidth]{quan_result.pdf}
    \caption{Participant responses on their perceptions of the team fluency (a), goal alignment (b), performance trust (c), and the robot's anthropomorphism (d) under different Speech Modes. 
    (*: This item was originally an inverse item according to~\cite{hoffman2010effects}. To make this figure look consistent, we reversed this item ($current\_score = 8-original\_score$).}
    \label{fig:quan}
\end{figure*}

\subsubsection{RQ1.1: How does a robot's capability to understand indirect speech acts influence the fluency of human-robot teamwork?}

Participants in the ISA group reported significantly greater perceptions of team fluency compared to those in the Non-ISA group ($\beta = 0.961, SE = 0.403, z = 2.382, p =0.017$), as seen in \autoref{tb:result}. Therefore, H1a is confirmed. \autoref{fig:quan}a illustrates the distribution of participants' responses.
% For the first and third items, the ISA group reported higher scores (item 1: $M_{ISA} = 5.83$, $Std_{ISA} = 0.79$; $M_{Non-ISA} = 4.89$, $Std_{Non-ISA} = 1.37$. item 3: $M_{ISA} = 5.09$, $Std_{ISA} = 1.53$; $M_{Non-ISA} = 4.57$, $Std_{Non-ISA} = 1.45$). However, participants from the Non-ISA group believed their team fluency was increasing over time, which was slightly higher than that of the ISA group ($M_{ISA} = 5.22$, $Std_{ISA} = 1.30$; $M_{Non-ISA} = 5.31$, $Std_{Non-ISA} = 1.14$). 
The team fluency questionnaire was consistent and reliable (Cronbach's $\alpha = 0.801$)~\cite{hoffman2019evaluating}. 

\subsubsection{RQ1.2: How does a robot's capability to understand indirect speech acts influence the establishment of goal alignment among the human-robot team?}

We observed that the Speech Mode had a significant impact on goal alignment, with participants in the ISA group expressing a stronger belief that they were working toward a mutual goal with the robot ($\beta = 2.309, SE = 0.656, z = 3.518, p < 0.001$). Therefore, H1b is confirmed. This is further illustrated in \autoref{fig:quan}b, which shows their scaled responses.

Moreover, participants' prior experience significantly influenced their perception of goal alignment. Namely, those with greater experience in physical collaborative tasks provided significantly higher scores on the goal alignment scale ($\beta = 0.536, SE = 0.270, z = 1.985, p = 0.047$). The goal alignment questionnaire was also consistent and reliable (Cronbach's $\alpha = 0.794$)~\cite{hoffman2019evaluating}.

\subsubsection{RQ2: How does a robot's capability to understand indirect speech acts influence a human teammate's trust in the robot's performance?}

The ISA group demonstrated significantly higher trust in the robot's performance compared to the Non-ISA group ($\beta = 1.105, SE = 0.493, z = 2.240, p = 0.025$).  Therefore, H2 is confirmed.
% Especially for the Reliability item, participants in the ISA group believed the robot was more reliable in performing collaborative tasks than the Non-ISA group ($M_{ISA} = 4.09$, $Std_{ISA} = 0.84$; $M_{Non-ISA} = 3.59$, $Std_{Non-ISA} = 0.95$). 
The MDMT performance trust questionnaire was consistent and reliable (Cronbach's $\alpha = 0.92$)~\cite{ullman2019mdmt}. Detailed participant responses can be seen in \autoref{fig:quan}c. 

\begin{table*}[]
\caption{The key results from CLMM analysis. Italics are covariates.\vspace{-0.5em}}
\label{tb:result}
\resizebox{0.85\textwidth}{!}{
\begin{tabular}{lllllll}
\hline
 & Fixed Effects & Estimates & Std Error & 95\% CI & z & p-value \\ \hline
Team Fluency & Speech Mode (Non-ISA) & 0.961 & 0.403 & 0.17 -- 1.751 & 2.382 & 0.017* \\
 & \textit{Robot (No)} & 0.004 & 0.110 & -0.211 -- 0.218 & 0.032 & 0.974 \\
 & \textit{Voice Assistant (Never)} & 0.129 & 0.210 & -0.283 -- 0.541 & 0.615 & 0.538 \\
 & \textit{Physical Collaborative Tasks (Never)} & 0.193 & 0.174 & -0.147 -- 0.533 & 1.112 & 0.266 \\ \hline
Goal Alignment & Speech Mode (Non-ISA) & 2.309 & 0.656 & 1.023 -- 3.596 & 3.518 & \textless{}0.001*** \\
 & \textit{Robot (No)} & 0.120 & 0.170 & -0.214 -- 0.453 & 0.701 & 0.483 \\
 & \textit{Voice Assistant (Never)} & -0.099 & 0.316 & -0.719 -- 0.521 & -0.312 & 0.755 \\
 & \textit{Physical Collaborative Tasks (Never)} & 0.536 & 0.270 & 0.007 -- 1.064 & 1.985 & 0.047* \\ \hline
Performance Trust & Speech Mode (Non-ISA) & 1.105 & 0.493 & 0.138 -- 2.072 & 2.240 & 0.025* \\
 & \textit{Robot (No)} & -0.041 & 0.136 & -0.307 -- 0.226 & -0.298 & 0.766 \\
 & \textit{Voice Assistant (Never)} & 0.231 & 0.248 & -0.255 -- 0.717 & 0.932 & 0.351 \\
 & \textit{Physical Collaborative Tasks (Never)} & 0.400 & 0.211 & -0.014 -- 0.814 & 1.892 & 0.058 \\ \hline
Anthropomorphism & Speech Mode (Non-ISA) & 2.708 & 0.674 & 1.387 -- 4.03 & 4.016 & \textless{}0.001*** \\
 & \textit{Robot (No)} & -0.168 & 0.184 & -0.528 -- 0.192 & -0.915 & 0.360 \\
 & \textit{Voice Assistant (Never)} & 0.031 & 0.340 & -0.635 -- 0.697 & 0.092 & 0.927 \\
 & \textit{Physical Collaborative Tasks (Never)} & -0.111 & 0.288 & -0.676 -- 0.454 & -0.385 & 0.701 \\ \hline
\end{tabular}
}
\end{table*}

\subsubsection{RQ3: How does a robot's capability to understand indirect speech acts influence a human teammate's perception of the robot's anthropomorphism?}

\autoref{fig:quan}d shows participants' responses to perceiving the robot's anthropomorphism under different Speech Modes. 
% For the ISA group, most participants gave higher scores, especially for the Fake-Natural ($M_{ISA} = 3.59$, $Std_{ISA} = 0.87$; $M_{Non-ISA} = 2.61$, $Std_{Non-ISA} = 0.93$) and Machinelike-Humanlike ($M_{ISA} = 3.35$, $Std_{ISA} = 1.06$; $M_{Non-ISA} = 2.30$, $Std_{Non-ISA} = 1.01$) items. 
% For the Non-ISA group, the majority acknowledged that the robot lacked anthropomorphism. 
As shown in \autoref{tb:result}, participants in the ISA condition exhibited significantly higher perceptions of the robot's anthropomorphism compared to those in the Non-ISA condition ($\beta = 2.708, SE = 0.674, z = 4.016,$ $p < 0.001$).  Therefore, H3 is confirmed. The anthropomorphism sub-scale of the GSQ questionnaire was consistent and reliable (Cronbach's $\alpha = 0.94$)~\cite{laban2019working}.

\subsubsection{Summary}

Overall, a robot's ability to understand indirect speech acts significantly influences human perception of teamwork fluency, goal alignment, performance trust, and robot anthropomorphism. Regarding the covariates, only participants' prior experience with physical collaborative tasks has a significant positive influence on the human-robot team's goal alignment. Moreover, the Speech Mode has a higher effect on goal alignment and anthropomorphism, followed by a medium effect on performance trust and team fluency.

\subsection{Qualitative Findings}
Two researchers thoroughly analysed participants' 502-minute semi-structured interview recordings~\cite{braun2012thematic}. Additionally, observations made by the experimenters during the experiment that were relevant to the interview findings were also analysed. In the following sections, we present the themes derived from interview responses.

\subsubsection{Reasons for using (In)direct requests}\label{sec:reason}
The most frequently mentioned reason for using indirect requests during collaboration was politeness. Participants preferred to use \textit{``Can you ...?''}—the conventionalised ISA—to show politeness in requests. They believed this approach followed social norms and felt more natural and comfortable. $P4_{Non-ISA}$ also mentioned that using ISA could offer the teammate the option to reject the request. However, $P3_{Non-ISA}$ disagreed, believing it unnecessary to be polite to a robot, which saved their effort. Therefore, she preferred to use direct commands when interacting with robots. Additionally, participants from the Non-ISA group highlighted that even after realising the robot could not understand their indirect requests, they sometimes inadvertently used ISA because it was natural and subconscious.

Due to the subconscious nature, participants who converted indirect requests to direct ones noted using ISA caused less cognitive workload. $P4_{Non-ISA}$ and $P6_{ISA}$ analogised this conversion process to constructing prompts, which requires additional time and effort. However, it was unnatural and more challenging to formulate prompts mentally and articulate them verbally when facing a physical entity, whether it was a robot or a human teammate. Furthermore, $P4_{Non-ISA}$ emphasised that if there were multiple human teammates and a robot teammate in one group, it added unnecessary mental load to switch between direct and indirect communication.

\begin{quote}
    $P4_{Non-ISA}$: ``[On using ChatGPT] It didn't feel like talking to an actual figure. It did make you feel a bit more like I'm not even asking you; I'm just telling you to do something, which just didn't seem natural to me in speech form. But if it was typed out, it would be a bit easier to do that. But I have to say I have to process it a little bit more. If it's just a text screen, then I feel like there's less of a need to express any of that [ISAs] through text form or if it's just like a virtual assistant.''
\end{quote}

Moreover, participants' expectation of the robot's capability influenced their communication strategies. Those who believed the robot had a high level of understanding were more likely to use indirect commands ($P1_{ISA}$, $P15_{ISA}$). Conversely, $P27_{ISA}$ predominantly used direct requests, despite being in the ISA group, as his extensive experience with LLMs led him to doubt the AI's ability to understand implicit requests. He believed direct commands were crucial for successful task completion, even though this approach required more effort to construct explicit commands mentally. Interestingly, participants had differing perceptions regarding the simplicity of commands. $P2_{ISA}$ believed that direct commands were simpler for both humans and robots. In contrast, participants stated that using indirect requests felt simpler and intuitive because it was speaking aloud what was already in mind. \textit{``Like an extension [of mind]''}, said $P13_{ISA}$.

However, there were several differing opinions on direct requests. The most frequently mentioned reason for using direct commands was clarity. Participants believed that direct commands were better suited for tasks requiring precise and nuanced descriptions, whereas indirect requests were more likely to cause ambiguity and misunderstandings. $P5_{Non-ISA}$ and $P17_{Non-ISA}$ further explained that communication strategies exhibit task dependency. For high-risk tasks, direct commands are preferred because unambiguous instructions are critical. In contrast, more complex but low-risk tasks, as well as those requiring intensive collaboration, benefit from indirect and natural communication.

\subsubsection{Adaptation in team fluency}
Participants in the ISA group believed the robot's ability to understand ISA contributed to a higher team fluency. $P1_{ISA}$ and $P18_{ISA}$ agreed indirect commands enabled more flexibility in communication. However, participants in the Non-ISA group reported feeling halted, but most of them further added that it wouldn't be a problem once they adapted. Observations by the experimenters revealed that participants in the Non-ISA group often did not immediately recognise that the issue was due to the misunderstanding of their intentions. Instead, they believed it was a voice recognition problem. As a result, they tended to repeat their indirect requests slowly and word by word, which repeatedly interrupted the collaborative process. Over time, once participants in the Non-ISA group understood that the robot only responded to direct requests, they began using direct commands more consistently, although they occasionally reverted to indirect requests due to the subconsciousness, as discussed in section \ref{sec:reason}. 

According to $P4_{Non-ISA}$, $P8_{Non-ISA}$, and $P11_{Non-ISA}$, although direct requests caused more mental work and initially affected team fluency, they believed this issue would diminish once humans adapted to the robot's communication abilities. \textit{``I think once you get used to it [direct requests], not so much [affects on fluency]. When I realise I have to say things in a certain way, I think it's fine. But at the start, yeah, it's a little bit off.''} said $P8_{Non-ISA}$.
They stated that they had no expectation for the robot to adapt to human communication style, as it was human's responsibility to ensure the robot could understand their instructions. However, not all the users were able to adapt. Despite being aware of the robot’s limitations, $P16_{Non-ISA}$ still preferred to use indirect commands.

% \vspace{-0.5em}
\subsubsection{Grounding and goal alignment}
% Consistent with the task dependency reason mentioned in the \textit{Reasons for using (In)direct Requests} section, 
Indirect commands allowed for more flexibility and complexity, fostering a deeper sense of partnership and shared goals ($P9_{Non-ISA}$, $P15_{ISA}$). $P18_{ISA}$ emphasised that a common understanding of the task and goal improved seamless coordination and reduced the likelihood of misunderstandings. According to $P8_{Non-ISA}$ and $P15_{ISA}$, the robot's ability to understand and act on implicit knowledge, similar to human common sense, was crucial for human teammates. This ability included grasping context-specific cues, such as spatial description, incomplete information, and shortened sentences, without requiring detailed explanation. For instance, in the polishing task, $P7_{ISA}$ gave a shortened indirect request \textit{``And the last one please''} to indicate that the robot should turn the hexagonal prism to the final surface, based on the prior context, \textit{``please turn so a different surface is facing me''}. Similarly, in the assembly task, $P13_{ISA}$ used an indirect request with ambiguous information, \textit{``Oh, actually, wait''}, to signal the robot to stop its current movement, with prior context information \textit{``Next, this blue block here needs to go up here on the red block''} and follow-up information \textit{``This red block here needs to go in the middle of this red block here''}. $P5_{Non-ISA}$ explained that he expected the robot to develop a shared understanding based on the context of his commands. As a result, he gave indirect commands, but the robot failed to interpret them correctly. These non-conventionalised ISAs were context-dependent.
Our observations, as well as follow-up interview questions, revealed that participants tended to use non-conventionalised indirect commands when they were confident their intentions were aligned with the robot’s understanding.

% \vspace{-0.5em
\subsubsection{Enhanced performance trust}
Participants in the ISA group reported a high level of performance trust in the interview, which aligns with our quantitative findings. $P1_{ISA}$, $P2_{ISA}$, and $P15_{ISA}$ agreed that they perceived the robot as more capable and reliable once they recognised its ability to understand indirect commands. Some participants in the Non-ISA group believed that the capability and reliability were contingent solely on the robot's task performance rather than its communication abilities. As $P8_{Non-ISA}$ remarked, \textit{``I think as long as there is something that I can say that will make the robot do the task, then it's still capable and reliable''}. 

Additionally, $P7_{ISA}$ emphasised that perceiving the robot as more human-like could raise expectations regarding its performance and lead to potential frustration if errors occurred. Conversely, when the robot was perceived as less human-like, errors were deemed more acceptable. However, some participants also believed that applying social norms, like politeness, in their interaction humanised the robot, which sometimes led to more lenient attitudes towards errors, similar to their reactions to human teammates' mistakes. 

\subsubsection{Perceptions of anthropomorphism}
In line with the quantitative results, the majority of participants acknowledged the robot's ability to understand ISA affected their perception of the robot's anthropomorphism. They agreed that the feeling of human likeness manifested from the ability to understand, even though the voice and tone were still machine-like. $P6_{ISA}$ reported \textit{``I think its ability to understand the implicit language made me feel like somebody was listening''}. However, some participants indicated that additional factors influenced their perceptions. $P12_{Non-ISA}$ highlighted she would \textit{``make small talk with a human, but the robot doesn't.''} Furthermore, $P25_{ISA}$ believed that the sentences used by the robot felt mechanical, which diminished his perception of the robot's human likeness.

Some participants mentioned a feeling of collaboration or control during the experiment. Participants in the Non-ISA group perceived the robot more as a tool or machine rather than a teammate ($P3_{Non-ISA}$, $P5_{Non-ISA}$, $P8_{Non-ISA}$). Participants associated this communication style with a more controlled, mechanical interaction, where the robot was seen as executing specific directives rather than participating in a collaborative process. $P8_{Non-ISA}$ and $P10_{ISA}$ agreed that direct commands required detailed instructions, which reinforced the perception of the robot as a tool needing explicit directions. This approach minimised ambiguity while simultaneously limiting the sense of shared responsibility or joint effort in the task. In this context, the robot was viewed as an extension of the user's will, carrying out predefined actions without critical decision-making. On the contrary, some participants believed the robot's ability to interpret indirect commands indicated a higher level of cognitive processing, similar to human teammates' ability to infer meaning and anticipate actions based on incomplete information. Unlike direct commands, participants perceived the robot more as a teammate rather than a tool when using indirect commands. This perception resulted from the robot's ability to understand and respond to more nuanced and context-rich communication $P1_{ISA}$, $P2_{ISA}$, $P18_{ISA}$, $P18_{ISA}$). Indirect commands were often used in a more conversational tone, suggesting a partnership where the robot was expected to understand the intent behind the instructions and act accordingly. This contributed to participants' perception of the robot's anthropomorphism ($P10_{ISA}$, $P13_{ISA}$, $P18_{ISA}$).

\begin{quote}
    $P17_{Non-ISA}$: ``I think, because indirect commands and subtext is a very human-feeling thing. So when I'm just giving it a direct command, it feels more like I'm just putting an input into a machine. Whereas with the indirect commands, it feels more like I'm having a conversation with someone.''
\end{quote}

When discussing future usage, participants also expressed concerns about different aspects of the robot's anthropomorphism. Participants believed that they preferred a robot with an extremely human level of understanding but not one that mimicked human tone, voice, or appearance. $P1_{ISA}$ mentioned that it also depends on the type of task, \textit{``If it's a vacuum cleaner, I want it to be less human cause it's just vacuuming the floor, you know. But if we're working on tasks kind of like this [our study], where it would be normal for two humans to work together, then I would definitely want the robot to be more human just so it's easier to communicate and get things done quickly.''}

\subsubsection{Expectations on LLM}
As this study used the Wizard-of-Oz method, participants assumed that the ISA robot was implemented with an LLM. Several participants compared the robot’s capabilities with their prior experiences using voice assistants and commercial LLMs. $P25_{ISA}$, $P32_{Non-ISA}$, and $P36_{Non-ISA}$ believed that a novel LLM should be capable of handling indirect requests, at least the conventionalised ones, i.e. \textit{``Can you ...?''.} However, $P25_{ISA}$ also acknowledged that he tended to be more direct when interacting with a text-based LLM. $P32_{Non-ISA}$ believed that using more indirect and polite language with ChatGPT usually yielded better outcomes. \textit{``You have to be very patient''}, $P32_{Non-ISA}$ remarked.

In contrast, $P27_{ISA}$ explained that his experience with LLMs led him to doubt the robot’s ability to comprehend indirect commands, which prompted him to provide explicit instructions to ensure task success. As a result, he primarily used direct commands during the collaboration in our study. $P4_{Non-ISA}$ and $P6_{ISA}$ concurred that ChatGPT usually performed better when using direct commands. Despite this, they both used numerous indirect commands in this study, noting that verbal commands differ from written commands as they provide less time to construct the prompts, and the formation of commands is often ad-hoc, leading to more ambiguity and incomplete sentences. Furthermore, $P6_{ISA}$, who expressed doubts about the implementation of LLMs in our robot, raised concerns about their effectiveness in real physical-embodied scenarios, arguing that LLMs would likely struggle in such contexts.

\begin{quote}
    $P6_{ISA}$: ``[When interacting with the robot] I think because I'm referring to things that exist in space as opposed to a concept that exists just in our mind. So if we're talking about something like, What's the difference between a plant cell and, you know, an animal cell? It's got a text-based understanding of that. But because this [interacting with a robot] is referring to a real embodied scenario, I think it would struggle to do anything with this.''
\end{quote}

% \subsubsection{Gestures as supplementary information}
% Experimenters observed that participants used various gestures during the collaboration. 
% Gestures were commonly used alongside verbal commands to clarify instructions, particularly in tasks involving spatial elements or where verbal instructions alone might be ambiguous, such as the assembly task in our study ($P1_{ISA}$, $P9_{Non-ISA}$, $P12_{Non-ISA}$, $P13_{ISA}$, $P17_{Non-ISA}$). This dual-mode communication was seen as enhancing the clarity and effectiveness of the interaction. $P10_{ISA}$ and $P17_{Non-ISA}$ believed gestures were a natural part of conveying information and ensuring the team's fluency, particularly when verbal descriptions might be insufficient. Moreover, some participants did not consciously decide to use gestures; rather, they reflected on their mental process when planning actions and choosing words. As $P1_{ISA}$ noted, `` (for the manufacturing task) I did this because I was trying to think which way I wanted the block to turn. And it was more for myself.'' 

% However, gestures were not always preferred. Participants stated that they preferred to use more verbal communication than gesturing when collaborating with human teammates, as they believed their human teammates could fully understand. $P8_{Non-ISA}$ elaborated humans were more likely to understand vague verbal descriptions. ``If the cube is the piece to the side, and I said "the middle", you can either think it's there (pointing) or on it's side. I think the robot might not understand that. But humans are more likely to understand.'', said $P8_{Non-ISA}$. Therefore, verbal communication was her first choice.