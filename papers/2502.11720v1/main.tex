%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
% \documentclass[manuscript,review,anonymous]{acmart}
\documentclass[sigconf]{acmart}
\usepackage{multirow}
% \usepackage{amsmath}
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{soul}



%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\setcopyright{cc}
\setcctype{by-nc-sa}
\acmConference[CHI '25]{CHI Conference on Human Factors in Computing Systems}{April 26-May 1, 2025}{Yokohama, Japan}
\acmBooktitle{CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 1, 2025, Yokohama, Japan}
\acmDOI{10.1145/3706598.3713780}
\acmISBN{979-8-4007-1394-1/25/04}

%% These commands are for a PROCEEDINGS abstract or paper.

%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}



%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Implications of Indirect Speech in Physical Human-Robot Collaboration]{Can you pass that tool?: Implications of Indirect Speech in Physical Human-Robot Collaboration
}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Yan Zhang}
\affiliation{%
  \department{School of Computing and Information Systems}
  \institution{University of Melbourne}
  \city{Melbourne}
  \state{VIC}
  \country{Australia}}
\email{yan.zhang.1@unimelb.edu.au}
\orcid{0000-0003-2142-5094}

\author{Tharaka Sachintha Ratnayake}
\affiliation{%
  % \department{School of Computing and Information Systems}
  \institution{University of Melbourne}
  \city{Melbourne}
  \state{VIC}
  \country{Australia}}
\email{tsratnayakem@student.unimelb.edu.au}
\orcid{0009-0004-6408-7587}

\author{Cherie Sew}
\affiliation{%
  \department{School of Computing and Information Systems}
  \institution{University of Melbourne}
  \city{Melbourne}
  \state{VIC}
  \country{Australia}}
\email{csew@student.unimelb.edu.au}
\orcid{0000-0002-5318-5681}

\author{Jarrod Knibbe}
\affiliation{%
  \department{School of Electrical Engineering and Computer Science}
  \institution{The University of Queensland}
  \city{Brisbane}
  \state{QLD}
  \country{Australia}}
\email{j.knibbe@uq.edu.au}
\orcid{0000-0002-8844-8576}

\author{Jorge Goncalves}
\affiliation{%
  \department{School of Computing and Information Systems}
  \institution{University of Melbourne}
  \city{Melbourne}
  \state{VIC}
  \country{Australia}}
\email{jorge.goncalves@unimelb.edu.au}
\orcid{0000-0002-0117-0322}

\author{Wafa Johal}
\affiliation{%
  \department{School of Computing and \\Information Systems}
  \institution{University of Melbourne}
  \city{Melbourne}
  \state{VIC}
  \country{Australia}}
\email{wafa.johal@unimelb.edu.au}
\orcid{0000-0001-9118-0454}



%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Zhang et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Indirect speech acts (ISAs) are a natural pragmatic feature of human communication, allowing requests to be conveyed implicitly while maintaining subtlety and flexibility. Although advancements in speech recognition have enabled natural language interactions with robots through direct, explicit commands—providing clarity in communication—the rise of large language models presents the potential for robots to interpret ISAs. However, empirical evidence on the effects of ISAs on human-robot collaboration (HRC) remains limited. To address this, we conducted a Wizard-of-Oz study (N=36), engaging a participant and a robot in collaborative physical tasks. Our findings indicate that robots capable of understanding ISAs significantly improve human's perceived robot anthropomorphism, team performance, and trust. However, the effectiveness of ISAs is task- and context-dependent, thus requiring careful use. These results highlight the importance of appropriately integrating direct and indirect requests in HRC to enhance collaborative experiences and task performance.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10003120.10003121.10011748</concept_id>
<concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}
\ccsdesc[500]{Human-centered computing~Empirical studies in HCI}

\ccsdesc[500]{Human-centered computing~User studies}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Human-Robot Collaboration, Language Communication, Grounding, Lab Study}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
\begin{teaserfigure}
  \begin{center}
  \includegraphics[width=0.85\textwidth]{teaser.pdf}
  \end{center}
  \caption{This figure presents images from our experiment, featuring representative participant utterances to illustrate the types of requests used. The top left image depicts a direct request, while the rest of the images showcase various indirect requests. The interpretation and robot's responses are explained in \autoref{user_study}. }
  \label{fig:teaser}
\end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\input{sections/10-introduction}
\input{sections/20-related-works}
\input{sections/30-method}
\input{sections/40-result}
\input{sections/60-discussion}
\input{sections/70-conclusion}

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
We gratefully acknowledge the support provided by the Melbourne Research Scholarship (University of Melbourne) and the Australian Research Council Discovery Early Career Research Award (Grant No. DE210100858).
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{reference}



%%
%% If your work has an appendix, this is the place to put it.
% \newpage
\appendix
\onecolumn
\section*{Appendix}
\renewcommand{\thetable}{A\arabic{table}}
\setcounter{table}{0} % Reset table counter

\section{Selected Questions from the Negative Attitude Towards Robots Scale} \label{apd:NARS}
``Please read through each of the following sentences and then indicate how frequently these statements apply to you, from (1) strongly disagree to (5) strongly agree.''
\begin{itemize}
    \item I would feel very nervous just standing in front of a robot.
    \item I would feel very nervous talking with robots.
    \item I would feel uneasy if I was given a job where I had to use robots.
\end{itemize}

\section{Model Results} \label{apd:model}

Table \ref{tb:model_result} provides the detailed results of the quantitative analysis.

The formula we used for the CLMM model in R is:
\[
dependent\_variable \sim speech\_mode + pre\_robot + pre\_va + pre\_phy + (1|task\_type) + (1|scale\_item) + (1+task\_type|p\_id)
\]
where dependent\_variable represents the rating from one of the scales (team fluency, goal alignment, performance trust, and anthropomorphism); speech\_mode represents the independent variable; pre\_robot, pre\_va, and pre\_phy are covariates, representing participants' previous experience with robots, voice assistants, and physical collaborative tasks; task\_type, scale\_item, and p\_id are random effects, representing tasks types, scales' sub-item IDs, and participant IDs.

\begin{table}[b]
% \begin{longtable}{|llllll|}
\caption{This table shows the results from CLMM analysis. In the column of fixed effects, italics are covariates.}
\label{tb:model_result}
\begin{tabular}{|llllll|}
\hline
\multicolumn{6}{|c|}{\cellcolor[HTML]{D3DCD2}\textbf{Team Fluency}} \\ \hline
\textbf{Fixed Effects} & \textbf{Estimates} & \textbf{Std Error} & \textbf{95\% CI} & \textbf{z} & \textbf{p-value} \\ \hline
Speech Mode (Non-ISA) & 0.961 & 0.403 & 0.17 -- 1.751 & 2.382 & 0.017* \\
\textit{Robot (No)} & 0.004 & 0.110 & -0.211 -- 0.218 & 0.032 & 0.974 \\
\textit{Voice Assistant (Never)} & 0.129 & 0.210 & -0.283 -- 0.541 & 0.615 & 0.538 \\
\textit{Physical Collaborative Tasks (Never)} & 0.193 & 0.174 & -0.147 -- 0.533 & 1.112 & 0.266 \\ \hline
\textbf{Random Effects} & \textbf{Variance} & \textbf{Std Dev} & \textbf{Correlation} &  &  \\ \hline
Task Type & 0.097 & 0.311 &  &  &  \\
Scales' Sub-item ID & 0.072 & 0.269 &  &  &  \\
Participant ID & 1.528 & 1.236 &  &  &  \\
Task Type | Participant ID & 0.262 & 0.511 & -0.597 &  &  \\ \hline
\textbf{Model Fit} & \textbf{AIC} & \textbf{Log Lik} &  &  &  \\ \hline
 & 1009.02 & -489.51 &  &  &  \\ \hline
\multicolumn{6}{|c|}{\cellcolor[HTML]{D3DCD2}\textbf{Goal Alignment}} \\ \hline
\textbf{Fixed Effects} & \textbf{Estimates} & \textbf{Std Error} & \textbf{95\% CI} & \textbf{z} & \textbf{p-value} \\ \hline
Speech Mode (Non-ISA) & 2.309 & 0.656 & 1.023 -- 3.596 & 3.518 & \textless{}0.001*** \\
\textit{Robot (No)} & 0.120 & 0.170 & -0.214 -- 0.453 & 0.701 & 0.483 \\
\textit{Voice Assistant (Never)} & -0.099 & 0.316 & -0.719 -- 0.521 & -0.312 & 0.755 \\
\textit{Physical Collaborative Tasks (Never)} & 0.536 & 0.270 & 0.007 -- 1.064 & 1.985 & 0.047* \\ \hline
\textbf{Random Effects} & \textbf{Variance} & \textbf{Std Dev} & \textbf{Correlation} &  &  \\ \hline
Task Type & 0.050 & 0.224 &  &  &  \\
Scales' Sub-item ID & 0.000 & 0.000 &  &  &  \\
Participant ID & 4.266 & 2.065 &  &  &  \\
Task Type | Participant ID & 0.299 & 0.546 & -0.553 &  &  \\ \hline
\textbf{Model Fit} & \textbf{AIC} & \textbf{Log Lik} &  &  &  \\ \hline
 & 801.36 & -385.68 &  &  &  \\ \hline
\multicolumn{6}{|c|}{\cellcolor[HTML]{D3DCD2}\textbf{Performance Trust}} \\ \hline
\textbf{Fixed Effects} & \textbf{Estimates} & \textbf{Std Error} & \textbf{95\% CI} & \textbf{z} & \textbf{p-value} \\ \hline
Speech Mode (Non-ISA) & 1.105 & 0.493 & 0.138 -- 2.072 & 2.240 & 0.025* \\
\textit{Robot (No)} & -0.041 & 0.136 & -0.307 -- 0.226 & -0.298 & 0.766 \\
\textit{Voice Assistant (Never)} & 0.231 & 0.248 & -0.255 -- 0.717 & 0.932 & 0.351 \\
\textit{Physical Collaborative Tasks (Never)} & 0.400 & 0.211 & -0.014 -- 0.814 & 1.892 & 0.058 \\ \hline
\textbf{Random Effects} & \textbf{Variance} & \textbf{Std Dev} & \textbf{Correlation} &  &  \\ \hline
Task Type & 0.015 & 0.124 &  &  &  \\
Scales' Sub-item ID & 0.225 & 0.475 &  &  &  \\
Participant ID & 2.630 & 1.622 &  &  &  \\
Task Type | Participant ID & 0.019 & 0.136 & -1.000 &  &  \\ \hline
\textbf{Model Fit} & \textbf{AIC} & \textbf{Log Lik} &  &  &  \\ \hline
 & 990.83 & -482.42 &  &  &  \\ \hline
% \end{tabular}
% \end{table}

% % \newpage
% \begin{table}[]
% % \begin{longtable}{|llllll|}
% % \caption{This table shows the results from CLMM analysis. In the column of fixed effects, italics are covariates.}
% % \label{tb:model_result_cont}
% \begin{tabular}{|llllll|}
% \hline
\multicolumn{6}{|c|}{\cellcolor[HTML]{D3DCD2}\textbf{Anthropomorphism}} \\ \hline
\textbf{Fixed Effects} & \textbf{Estimates} & \textbf{Std Error} & \textbf{95\% CI} & \textbf{z} & \textbf{p-value} \\ \hline
Speech Mode (Non-ISA) & 2.708 & 0.674 & 1.387 -- 4.03 & 4.016 & \textless{}0.001*** \\
\textit{Robot (No)} & -0.168 & 0.184 & -0.528 -- 0.192 & -0.915 & 0.360 \\
\textit{Voice Assistant (Never)} & 0.031 & 0.340 & -0.635 -- 0.697 & 0.092 & 0.927 \\
\textit{Physical Collaborative Tasks (Never)} & -0.111 & 0.288 & -0.676 -- 0.454 & -0.385 & 0.701 \\ \hline
\textbf{Random Effects} & \textbf{Variance} & \textbf{Std Dev} & \textbf{Correlation} &  &  \\ \hline
Task Type & 0.000 & 0.000 &  &  &  \\
Scales' Sub-item ID & 0.101 & 0.318 &  &  &  \\
Participant ID & 5.092 & 2.257 &  &  &  \\
Task Type | Participant ID & 0.832 & 0.912 & -0.552 &  &  \\ \hline
\textbf{Model Fit} & \textbf{AIC} & \textbf{Log Lik} &  &  &  \\ \hline
 & 1239.07 & -606.54 &  &  &  \\ \hline
% \multicolumn{6}{|l|}{
% \begin{minipage}[t]{0.69\columnwidth} % Allows multiline content inside a table cell
% The formula we used for the CLMM model in R is:
% \[
% \begin{aligned}
% dependent\_variable &\sim speech\_mode + pre\_robot + pre\_va + pre\_phy \\
%    &\quad + (1|task\_type) + (1|scale\_item) + (1+task\_type|p\_id)
% \end{aligned}
% \]
% where dependent\_variable represents the rating from one of the scales (team fluency, goal alignment, performance trust, and anthropomorphism); speech\_mode represents the independent variable; pre\_robot, pre\_va, and pre\_phy are covariates, representing participants' previous experience with robots, voice assistants, and physical collaborative tasks; task\_type, scale\_item, and p\_id are random effects, representing tasks types, scales' sub-item IDs, and participant IDs.
% % \vspace{1em}
% \end{minipage}
% } \\ \hline
% \end{longtable}
\end{tabular}
\end{table}

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.
