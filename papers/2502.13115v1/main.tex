
\documentclass{article} 
\usepackage[margin=1in]{geometry}
\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{lmodern}


\usepackage{etoolbox}
\newcommand{\arxiv}[1]{\iftoggle{colt}{}{#1}}
\newcommand{\colt}[1]{\iftoggle{colt}{#1}{}}
\newtoggle{colt}
\global\togglefalse{colt}

\input{preamble}
\input{macros}

\arxiv{
\floatplacement{algorithm}{t}
}

\usepackage[suppress]{color-edits}
 \addauthor{sr}{red}
 \addauthor{cf}{orange}




\title{Near-Optimal Private Learning in Linear Contextual Bandits}

\colt{
\coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
 \Name{Author Name2} \Email{xyz@sample.com}\\
 \addr Address}
}

\arxiv{
\author{Fan Chen\\{\small \texttt{fanchen@mit.edu}} \and  Jiachun Li\\{\small \texttt{jiach334@mit.edu}} \and Alexander Rakhlin\\{\small \texttt{rakhlin@mit.edu}} \and David Simchi-Levi\\{\small \texttt{dslevi@mit.edu}} }
}


\begin{document}

\maketitle

\begin{abstract}
We analyze the problem of private learning in generalized linear contextual bandits. Our approach is based on a novel method of \emph{re-weighted} regression, yielding an efficient algorithm with regret of $\tbO{d^2\sqrt{T}+\frac{d^{5/2}}{\alpha}}$ and $\tbO{\sqrt{d^5T}/\alpha}$ in the joint and local model of privacy, respectively. 
\cfedit{Further, we provide near-optimal private procedures that achieve dimension-independent rates in private linear models and linear contextual bandits. In particular, our results imply that joint privacy is almost ``for free'' in all the settings we consider, partially addressing the open problem posed by \citet{azize2024open}.}
\end{abstract}


\section{Introduction}\label{sec:intro}


Contextual bandits provide a natural framework for interactive decision making, applicable to numerous real-world domains. In this setting, the decision maker (or, the algorithm) sequentially observes a context, selects an action, and receives a reward ~\citep{abbasi2011improved, auer2002finite, simchi2020bypassing, foster2020beyond}. The central challenge is to balance exploration (learning the reward structure) and exploitation (maximizing the cumulative rewards). Further, in many applications, there are additional privacy concerns, as the contexts often involve sensitive personal information---such as past purchase histories, credit scores, or physcial data---information not meant for public disclosure ~\citep{dwork2014algorithmic, lei2024privacy, chen2022privacy}. Despite extensive research on interactive decision making, it is not yet well-understood how to achieve the optimal privacy-utility trade-offs even in the fundamental setting of contextual bandits---arguably one of the simplest and most commonly considered models of online learning ~\citep{shariff2018differentially, azize2024open}.



Formally, in the setting of contextual bandits, the learner observes a context $x_t\in\cX$ at each step $t\in[T]$, drawn stochastically as $x_t\sim P$. Based on the context $x_t$ (and the history up to step $t$), the learner selects an action $a_t\in\cA$ and observes a reward $r_t\in[-1,1]$ with expected value $\EE[r_t|x_t,a_t]=\fs(x_t,a_t)$. Here, $\fs:\cX\times\cA\to[-1,1]$ is the underlying mean reward function. The performance of the learner is typically measured by its \emph{regret}, defined as
\begin{align*}
    \Reg\defeq \EE\brac{ \sum_{t=1}^T \max_{\as_t\in\cA} \fs(x_t,\as_t) - \fs(x_t,a_t) },
\end{align*}
which measures the gap between the learner's cumulative rewards and that of an optimal policy with the full knowledge of $\fs$.
In generalized linear contextual bandits, a widely studied model, the ground-truth $\fs$ is assumed to take the form
\begin{align*}
    \fs(x,a)=\nu(\lr \phxa, \ths \rr), \qquad \forall (x,a)\in\cX\times\cA,
\end{align*}
where $\nu:[-1,1]\to[-1,1]$ is a known link function, $\phi:\cX\times\cA\to\Bone$ is a known feature map, and $\ths\in\Bone$ is the unknown underlying parameter. 

Even in linear contextual bandits (where $\nu(t)=t$ is the identity function and $\fs$ is linear), there is limited understanding of how to design privacy-preserving procedures that attain optimal regret.
Under the \emph{joint differential privacy} (JDP) model~\citep{dwork2006calibrating} with privacy parameter $\alpha$, the only known regret upper bound---due to \citet{shariff2018differentially}---scales as $\sqrt{T/\alpha}$. This regret scaling is particularly undesirable in the high-privacy regime (i.e., $\alpha \ll 1$), and \citet{azize2024open} have posed the question of identifying the rate-optimal regret in this setting as an open problem.

\input{tab_comp}

In the \emph{local differential privacy} (LDP) model~\citep{duchi2013local}, the best-known regret bound until recently was $\sqrt{T}/(\alpha\lmins)$~\citep{han2021generalized}, where \colt{$\lmins\defeq \min_{\pi}\lmin(\EE^{\pi}\phxa\phxa\tp)$}\arxiv{
\begin{align*}
    \lmins\defeq \min_{\pi}\lmin(\EE^{\pi}\phxa\phxa\tp)
\end{align*}
} is the minimum eigenvalue of the covariance matrix over \emph{all} linear policies.\footnote{A policy $\pi$ is \emph{linear} if there exists $\theta\in\R^d$ so that $\pi(x)\in\argmax_{x\in\cX} \lr \theta, \phi(x,a)\rr$. For generalized linear contextual bandits, it is clear that the optimal policy must be linear.}
Assuming $\lmins$ being lower bounded implies that the learner can effectively estimate the ground-truth parameter $\ths$ while executing greedy policies, and hence 
it essentially removes the difficulty of \emph{exploration}. 
\cfedit{Such a condition is typically called ``explorability'' (``diversity'') because it assumes the actions taken by any greedy policy are diverse enough. The quantity $1/\lmins$ can be prohibitively large for most scenarios of interest, e.g., when there exists a direction $v\in\R^d$ that cannot be sufficiently explored, i.e., $\phxa \perp v$ for each $a\in\cA$ with high probability over $x\sim P$.}
However, it is known that for algorithms based on squared loss regression---such as variants of the LinUCB algorithm~\citep{abbasi2011improved}---to achieve the optiaml rates, a dependence on $1/\lmins$ can be unavoidable (cf. \cref{sec:negative}). Therefore, algorithmic innovations are necessary to attain the optimal regret without assuming such an explorability condition.

Towards the optimal regret under LDP, the recent work of %
\citet{li2024optimal} develops an alternative approach based on~\cfedit{\emph{regression with confidence interval ($\Lone$-regression)}.} %
However, as their algorithm involves iteratively performing PCA, the final regret bound scales with $\log^d(T)\sqrt{T}$, i.e., the dependence on $d$ is \emph{exponential}. 
On the other hand, \citet{chen2024private} prove a regret bound of $\sqrt{d^3T}/\alpha$ by controlling the Decision-Estimation Coefficient (DEC)~\citep{foster2021statistical,foster2023tight}, achieved by the Exploration-by-Optimization algorithm~\citep{lattimore2020exploration,foster2022complexity} that operates in at least \emph{exponential-time}.


Building upon these recent advances, we propose a new method called private \emph{re-weighted regression}, which first privately learns a \emph{normalization matrix} $U$ from data, and then performs regression on the loss function, re-weighted according to the matrix $U$. In the generalized linear model, the re-weighted regression provides a near-optimal convergence rate in the \Lone-error. Further, using this method as a subroutine, we propose a computationally efficient algorithm that achieves regret bounds of $\tbO{d^2\sqrt{T}+\frac{d^{5/2}}{\alpha}}$ and $\tbO{\sqrt{d^5T}/\alpha}$ in the joint and local privacy models, respectively. 
In particular, our results \cfedit{imply that the joint privacy is almost ``for free'' in generalized linear contextual bandits, partially resolving} the open problem of \citet{azize2024open}.

\cfedit{Furthermore, under the setting where the dimension $d$ is prohibitively large or even unbounded, we develop private estimation procedures that achieve \emph{dimension-independent} rates in linear models. As application, we provide nearly minimax-optimal dimension-free regret bounds in private linear contextual bandits.}

\paragraph{Existing results}
In \cref{tab:comp}, we summarize the known regret bounds for learning in (generalized) linear contextual bandits under the joint or local model of privacy. For simplicity of presentation, all results are specialized to linear contextual bandits with stochastic contexts, and we omit poly-logarithmic factors. 

\paragraph{Organization}
In \cref{sec:negative}, we discuss the main difficulty of achieving optimal rates while preserving privacy in linear contextual bandits, motivating the approach of $\Lone$-regression. We then present our re-weighted regression method (\cref{sec:l1-reg}) for private generalized linear models. \cfedit{In \cref{sec:cb}, we apply our method to provide private regret guarantees in generalized linear contextual bandits. Further, in \cref{sec:unbounded}, we investigate \emph{dimension-free} private learning in linear models and linear contextual bandits.}
For succinctness, we mostly focus on the JDP setting in the main body of the paper and present the algorithms for LDP setting in the appendices.









\section{Preliminaries}\label{sec:prelim}
\input{sec_prelim}

\section{Motivation}\label{sec:negative}
\input{sec_motivation}

\section{Private Reweighted Regression}\label{sec:l1-reg}
\input{sec_upper}

\section{Private Learning in Generalized Linear Contextual Bandits}\label{sec:cb}
\input{sec_regret}

\arxiv{
\section{Dimension-free Linear Regression}\label{sec:unbounded}
\input{sec_unbounded}
}

\section*{Conclusion}


In this work, we propose a novel method of private re-weighted regression for private (generalized) linear regression, achieving near-optimal convergence rates under $L_1$-error. Based on this method, we provide efficient algorithms for (generalized) linear contextual bandits with near-optimal regret bounds in both the joint and local model of differential privacy. Furthermore, we also develop the improper private procedures with near-optimal, dimension-independent rates in linear models and linear contextual bandits. 

\subsection*{Acknowledgements} FC and AR acknowledge support from ARO through award W911NF-21-1-0328, as well as Simons Foundation and the NSF through awards DMS-2031883 and PHY-2019786. 

\arxiv{
\bibliographystyle{abbrvnat}
}
\bibliography{ref.bib}

\newpage
\appendix


\section{Technical tools}
\input{tools}

\section{Proofs from \cref{sec:negative}}\label{appdx:negative}
\input{appdx_lower}

\section{Proofs from \cref{sec:l1-reg}}\label{appdx:LG}
\input{appdx_normalization}
\input{appdx_LG}

\section{Proofs from \cref{sec:cb}}\label{appdx:CB}
\input{appdx_CB}

\arxiv{
\section{Proofs from \cref{sec:unbounded}}\label{appdx:unbouned}
\input{appdx_unbounded}
}


\end{document}
