


In \cref{sec:l1-reg} and \cref{sec:cb}, we provided a somewhat satisfactory picture of the optimal rates of private learning in contextual bandits with generalized linear models, when the dimension $d$ is \emph{bounded}. 
In this section, we turn our focus to the setting where the dimension $d$ is prohibitively large or \emph{unbounded}, e.g., when the linear function parametrization is in fact given by a Reproducing Kernel Hilbert Space (RKHS).\footnote{Our approach naturally applies to learning in RKHS. However, to avoid measure-theoretic issues, we only present our algorithms for finite dimensional space.} 


This setting is fundamentally more challenging, as the following lower bounds indicates. The proof can be found in \citep[Appendix C]{chen2024private}. 

\newcommand{\Sd}{\mathbb{S}^{d-1}}
\begin{proposition}\label{prop:unbounded-lower}
Let $d\geq 1$, covariate space $\cC=\Sd$ be the $d$-dimensional unit sphere. 
For each $\theta\in\Sd$, we consider the linear model $M_\theta$:
\begin{align*}
    (\x,y)\sim M_\theta: \qquad \x=\theta, y=1.
\end{align*}
For any parameter $R\in[1,c\sqrt{d}]$ ($c>0$ is a small absolute constant), the following holds:

(a) Suppose that $\alg$ is a $T$-round $(\alpha,0)$-JDP algorithm with output $\nrmn{\hth}\leq R$. Then it holds that
\begin{align*}
    \sup_{\ths\in\Sd}\EE^{M_{\ths},\alg} \absn{\lr \ths,\hth\rr -1 }\geqsim 1, \qquad \text{unless }T\geqsim \frac{d}{R^2\alpha}.
\end{align*}

(b) Suppose that $\alg$ is a $T$-round \aLDP~algorithm with output $\nrmn{\hth}\leq R$. Then it holds that
\begin{align*}
    \sup_{\ths\in\Sd}\EE^{M_{\ths},\alg} \absn{\lr \ths,\hth\rr -1 }\geqsim 1, \qquad \text{unless }T\geqsim \min\sset{\frac{d}{R^2\alpha^2}, \frac{1}{\beta}}.
\end{align*}
\end{proposition}

Note that for each linear model $\PP_\theta$, the covariance matrix $\EE_{\PP_\theta}\x\x\tp=\theta\theta\tp$ is of rank 1. %
Therefore, \cref{prop:unbounded-lower} has two implications for \emph{pure JDP} (and also LDP) regression with unbounded dimension $d$ and unknown covariate distribution:
\begin{itemize}
\item[(1)] Estimating the covariance matrix requires $\Omega(d)$ samples, even when the covariance matrix is known to have rank 1.
\item[(2)] Proper estimator of the parameter $\ths$ also requires $\Omega(d)$ samples to achieve a non-trivial error.
Conversely, any non-trivial estimator $\hth$ must have norm $\nrmn{\hth}\geq \Omega(\sqrt{d/T})$ (with non-trivial probability).
\end{itemize}
Therefore, with unbounded dimension $d$, to achieved estimation guarantees, the estimator in consideration has to be highly \emph{improper}, and it also cannot rely on estimating the covariance matrix.


Based on the observations above,
in this section, we develop improper private procedures with \emph{dimension-free} bounds in private linear regression.
We then apply the proposed methods to provide dimension-free regret bounds in private linear contextual bandits.




\subsection{Private improper batched SGD}\label{ssec:l1-dim-free}


We begin with the joint DP setting. For any non-private estimator $\hth$ with \emph{sensitivity} $s$, it is well-known that the estimator $\hth'=\hth+\zeta$ ensures \aJDP~with noise $\zeta\sim \normal{0,\eps^2\id}$ and parameter $\eps:=s\cdot \siga$. The key idea is that, while $\nrm{\zeta}\asymp \eps\sqrt{d}$, we have $\nrm{\zeta}_{\bSigma}\leqsim \eps$ with high probability, where $\bSigma\defeq \Epp{\x\x\tp}$ is the covariance matrix.
Therefore, to ensure JDP, it is sufficient to privatize a non-private estimator with low sensitivity. 

\newcommand{\pa}[1]{\theta\epk{#1}}
\newcommand{\gd}[1]{g\epk{#1}}
\newcommand{\ogd}[1]{\bar{g}\epk{#1}}
\newcommand{\xt}{\x_t}
\newcommand{\yt}{y_t}
\newcommandx{\clip}[2][1=R]{\mathsf{clip}_{#1}(#2)}
\newcommandx{\nt}[1][1=t]{\zeta_{#1}}
\newcommand{\gt}{g_t}
\newcommand{\tgt}{\Tilde{g}_t}
\newcommandx{\avgtk}[1][1=k]{\frac1N\sum_{t=#1 N+1}^{(#1+1)N}}
\begin{algorithm}
\caption{$\AlgJDPIGD$}\label{alg:JDP-improper-GD}
\begin{algorithmic}[1]
\REQUIRE Dataset $\dataset=\sset{(\x_t,y_t)}_{t\in[T]}$.
\REQUIRE Epoch $K\geq 1$, batch size $N=\floor{\frac{T}{K}}$, stepsize $\eta=1$, parameter $R=2$. %
\STATE Initialize $\theta\kz=\bz$.
\FOR{$k=0,1,\cdots,K-1$}
\STATE Compute gradient estimate
\begin{align*}
    g\kk=\frac1N \sumkn \xt\paren{ \lr \xt, \theta\kk\rr-\yt }.
\end{align*}
\STATE Update
\begin{align*}
    \theta\kp=\Proj_{\BR}\paren{\theta \kk-\eta g\kk}.
\end{align*}
\ENDFOR
\STATE Privatize $\hth\sim \priv[\eta(R+1)/N]{\theta\kc}$
\ENSURE Estimator $\hth$.
\end{algorithmic}
\end{algorithm}

Based on these observations, we consider the projected batched SGD on the standard square-loss:
\begin{align*}
    \Lsq(\theta)=\frac12\Exy{\paren{\lr \x,\theta\rr-y}^2}.
\end{align*}
By directly privatizing its last iterate (\cref{alg:JDP-improper-GD}), we can achieve a near-optimal convergence rate (detailed in \cref{appdx:improper-JDP}).










\begin{theorem}[Dimension-free JDP regression]\label{thm:improper-JDP}
Let $T\geq1, \delta\in(0,1)$. \cref{alg:JDP-improper-GD} preserves \aJDP, and with a suitably chosen parameter $K$, it ensures that
\begin{align*}
    \Epp{\lr \x,\hth-\ths\rr^2}=\nrmn{\hth-\ths}_{\bSigma}^2\leqsim \paren{\frac{\log T\log(1/\delta)}{T}}^{1/2}+\paren{\frac{\siga \sqrt{\log(1/\delta)}}{T}}^{2/3}.
\end{align*}
\end{theorem}

Therefore, the estimator $\hth$ achieves the \emph{dimension-independent} convergence rate of $\frac{1}{\sqrt{T}}+\frac{1}{(\alpha T)^{2/3}}$. In non-private linear models, the $T^{-1/2}$-rate of convergence is known to be minimax-optimal and can be achieved by vanilla gradient descent.

\paragraph{Extension: Local DP}
The situation under the local DP model is much more subtle.
Indeed, one may expect that \cref{alg:JDP-improper-GD} naturally extends to this setting.
However, to ensure local privacy in (batched) SGD, for every step, the gradient estimator
has to be privatized. To this end,
it is typically necessary to add a noise vector $\zeta$ that has norm scaling with $\Omega(\sqrt{d})$ (e.g., when $\zeta$ is the Gaussian noise). In other words, the privatized gradient estimator has norm scaling with $\sqrt{d}$. Hence, after a single step of gradient descent, the iterate falls outside the unit ball $\Bone$, and projection back to $\Bone$ can lead to large bias. Therefore, the method of projected gradient descent may not be applied here.


Instead, we consider performing privatized batch SGD directly, replacing the projection operation with a careful clipping on the gradient estimator (\cref{alg:LDP-improper-GD}). This is based on extending the aforementioned observation under JDP: When $\zeta\sim \normal{0,\eps^2\id}$, while $\nrmn{\zeta}\sim \eps \sqrt{d}$, for the covariate $\x\sim p$ that is independent of $\zeta$, the random variable $\lr \x,\zeta\rr\sim \normal{0,\eps^2\nrm{\x}^2}$ is a zero-mean Gaussian random variable conditional on $\x$. Particularly, it holds that $\abs{\lr \x, \zeta\rr}\leqsim \eps\nrm{\x}$ with high probability with respect to the randomness of the noise $\zeta$ and $\x\sim p$. Using this idea, we can show that \eqref{eq:clip-grad} provides an estimator of $\nabla \Lsq(\theta\kk)$ with small bias for all epochs. Here, the clipping operation is defined as
\begin{align*}
    \clip{v}\defeq \max\sset{\min\sset{v,R},-R}\in[-R,R], \qquad \forall v\in\R.
\end{align*}

\begin{algorithm}
\caption{$\AlgLDPIGD$}\label{alg:LDP-improper-GD}
\begin{algorithmic}[1]
\REQUIRE Round $T\geq 1$.
\REQUIRE Epoch $K\geq 1$, batch size $N=\floor{\frac{T}{K}}$, stepsize $\eta=1$, parameter $R=2$.
\STATE Initialize $\theta\kz=\bz$.
\FOR{$k=0,\cdots,K-1$}
    \FOR{$t=\rangekn$}
        \STATE Observe $(\xt,\yt)\sim p$ and form the gradient estimator
        \begin{align}\label{eq:clip-grad}
            g_t=\xt\paren{ \clip{\lr \theta\kk,\xt \rr}-\yt }
        \end{align}
        \STATE Privatize $\til g_t\sim \priv[R+1]{g_t}$.    
    \ENDFOR
    \STATE Compute batched gradient estimator $\til g\kk=\avgtk \tgt$ and update
\begin{align*}
    \theta\kp=\theta \kk-\eta \til g\kk.
\end{align*}
\ENDFOR
\ENSURE $\hth=\theta\kc$.
\end{algorithmic}
\end{algorithm}

With a careful analysis that bounds the bias introduced by clipping \cref{eq:clip-grad}, we provide the following guarantee for \cref{alg:LDP-improper-GD}.

\begin{theorem}[Dimension-free LDP regression]\label{thm:improper-LDP}
\cref{alg:LDP-improper-GD} preserves \aLDP. For $T\geq 1, \delta\in(0,1)$, with a suitable number of epochs $K\geq 1$, \cref{alg:LDP-improper-GD} returns $\hth$ that \whp,
\begin{align*}
    \Epp{ \lr \x, \hth-\ths \rr^2 }=\nrmn{\hth-\ths}_{\bSigma}^2\leqsim \paren{\frac{\siga\log(T/\delta)}{T}}^{1/3}.
\end{align*}
\end{theorem}

This establishes a convergence rate of $\tbO{(\alpha^2T)^{-1/3}}$ under the square loss. As shown by \citet[Corollary I.8]{chen2024private}, any LDP algorithm must incur an $\Lone$-error of
\begin{align*}
    \Omega\paren{\min\sset{\frac{d}{\sqrt{\alpha^2 T}},\paren{\frac{1}{\alpha^2 T}}^{1/6}}}.
\end{align*}
Therefore, \cref{alg:LDP-improper-GD} achieves the optimal dimension-free convergence rate under $\Lone$-error, and hence it is optimal under the $L_2$-error. Further, \cref{thm:LDP-L1-regression} and \cref{thm:improper-LDP} together provide the near-optimal $\Lone$-estimation error for the full range of $T\in[1,\infty)$ under LDP.

\subsection{Application: Linear contextual bandits with dimension-free regret} 

As an application, we use the dimension-free procedures developed in \cref{ssec:l1-dim-free} as subroutines for learning linear contextual bandits.
We invoke the $\AlgSQCB$ algorithm \citep{abe1999associative,foster2020beyond,simchi2020bypassing}, which has a regret guarantee given any \emph{offline regression oracle} with $L_2$-error bound. In particular, by instantiating the regression oracle as $\AlgJDPIGD$ (\cref{alg:JDP-improper-GD}) or $\AlgLDPIGD$ (\cref{alg:LDP-improper-GD}), we obtain the following private regret bounds. The details are presented in the \cref{appdx:square-cb}.
\begin{theorem}[Dimension-free regret bounds]\label{thm:regret-dim-free}
Let $T\geq 1$ and suppose $\cA$ is finite.

(1) Suppose that $\AlgSQCB$ (\cref{alg:square-cb}) is instantiated by the regression subroutine $\AlgJDPIGD$ (\cref{alg:JDP-improper-GD}). Then $\AlgSQCB$ preserves \aJDP~and it ensures 
\begin{align*}
    \Reg\leq \sqrt{|\cA|}\cdot \tbO{T^{3/4}+\siga^{1/3}T^{2/3}}.
\end{align*}

(2) Suppose that $\AlgSQCB$ (\cref{alg:square-cb}) is instantiated by the regression subroutine $\AlgLDPIGD$ (\cref{alg:LDP-improper-GD}). Then $\AlgSQCB$ preserves \aLDP~and it ensures 
\begin{align*}
    \Reg\leq \sqrt{|\cA|}\cdot \tbO{\siga^{1/3}T^{5/6}}.
\end{align*}
\end{theorem}

To the best of our knowledge, such dimension-free regret bounds are new in private contextual bandits. Under JDP, the regret rate is $\tbO{T^{3/4}+\alpha^{-1/3}T^{2/3}}$, and the first term matches the optimal dimension-free $T^{3/4}$-regret in non-private linear contextual bandits~\citep{abe1999associative,foster2020beyond}, implying that privacy is almost ``for free'' in this setting. Furthermore, the LDP regret bound scales as $\alpha^{-1/3}T^{5/6}$, which nearly matches the minimax lower bound \citep[Corollary I.15]{chen2024private}. 



