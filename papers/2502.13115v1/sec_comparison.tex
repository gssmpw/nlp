

A recent line of work studies the regret of learning in (generalized) linear contextual bandits under the joint or local model of privacy, and we summarize the known regret bounds in \cref{tab:comp} (all specialized to linear contextual bandits with stochastic contexts). 

Under JDP, the only known upper bound is due to \citet{shariff2018differentially} and scales as $\sqrt{T}/\alpha$. Particularly, the price of $\alpha$-privacy is not negligible as $T\to \infty$, and hence \citet{azize2024open} raise the question of the rate-optimal regret of this setting. In this work, we partially resolve this open question by proposing a JDP algorithm with regret $\tbO{\poly(d)\paren{\sqrt{T}+\frac{1}{\alpha}}}$. Further, when the number of actions is a constant (i.e., $|\cA|=\bigO{1}$), our algorithm can be instantiated to achieve a regret of $\tbO{\sqrt{dT}+\frac{\poly(d)}{\alpha}}$, with the leading $\sqrt{dT}$-rate matches the minimax-optimal regret of non-private linear contextual bandits, implying that joint privacy is \emph{for free} in this case.


Among the results under the local model, the recent work of \citet{chen2024private} provides a $\sqrt{d^3T}/\alpha$ the best regret bound with the Exploration-by-Optimization technique~\citep{lattimore2020exploration,foster2022complexity}, leading to an exponential-time algorithm. Prior to that, the known regret bounds either scale with $T^{3/4}$~\citep{zheng2020locally}, $\log^d(T)\sqrt{T}$~\citep{li2024optimal}, or requires a minimum eigenvalue assumption~\citep{han2021generalized}. More specifically, we denote $\lmins\defeq \min_{\pi}\lmin(\EE^{\pi}\phxa\phxa\tp)$ be the minimum eigenvalue of the covariance matrix under \emph{any} linear policy $\pi$, and the assumptions of \citet{han2021generalized} are slightly stronger than $\lmins>0$. The quantity $\lmins$ measures how diverse the context distribution is, which can be a very restrictive condition as it involves \emph{all} linear policies (also note that $\lmins\leq \frac{1}{d}$ always). By contrast, our algorithm is computationally efficient, and it achieves a regret of order $\sqrt{d^5T}$ without any additional assumption (though it is slightly worse than the regret bound of \citet{chen2024private}).

