


\subsection{Details of \cref{alg:JDP-L1-regression}}\label{appdx:JDP-l1-regression}

In the following, we present the detailed description of the subroutine $\AlgJDPGD$ (\cref{alg:JDP-SGD}).

\begin{algorithm}[H]
\caption{Subroutine $\AlgJDPGD$: Batched SGD under JDP}\label{alg:JDP-SGD}
\begin{algorithmic}
\REQUIRE Loss $\Lnew$ under normalization $(U,\lambda)$, dataset $\dataset=\sset{(\x_t,y_t)}_{t\in[T]}$.
\REQUIRE Epoch $K\geq 1$, batch size $N=\floor{\frac{T}{K}}$, stepsize $\eta=\frac{1}{4\Lipg}$. %
\STATE Initialize $w\kz=\bz$.
\FOR{$k=0,1,\cdots,K-1$}
\STATE Compute gradient estimate
\begin{align*}
    g\kk=\lambda\mug \cdot  U w\kk+\frac1N \sumkn \frac{U\x_t}{\|U\x_t\|}\paren{ \link(\lr U\x_t, w\kk \rr) -y_t }.
\end{align*}
\STATE Privatize $\til g\kk\sim \priv[2/N]{g\kk}$ and update
\begin{align*}
    w\kp=\Proj_{\cW}\paren{ w\kk-\eta \til g\kk }.
\end{align*}
\ENDFOR
\ENSURE Estimator $\hth= U w\kc$.
\end{algorithmic}
\end{algorithm}

By the standard analysis of stochastic approximation, we provide the following guarantee of \cref{alg:JDP-SGD}.
\begin{proposition}[Convergence of batched SGD; JDP]\label{prop:JDP-GD}
Suppose that the input normalization $(U,\lambda)$ satisfies \eqref{def:U-app}, and $\lambda\geq \frac{1}{T}$. Then Algorithm $\AlgJDPGD$ preserves \aJDP~achieves \whp
\begin{align*}
    \nrm{w\kk-\hwst}^2\leq \paren{ 1-\frac{1}{8\kpg} }^k\nrm{\hwst}^2+\bigO{\frac{\log(K/\delta)}{\mug^2N}}+\bigO{\frac{1}{\mug^2}+\frac{d}{\mug\Lipg}}\cdot \frac{\siga^2\log(K/\delta)}{N^2},
\end{align*} 
where we recall that $\hwst=\argmin_{w\in\cW} \Lnew(w)$. 
In particular, when we take $K=16\kpg \log(T)$, we have
\begin{align}\label{eq:def:lamgd}
\begin{aligned}
\MoveEqLeft    \Lipg\cdot \nrm{w\kc-\hwst} \\
\leq&~ C_1\brac{ \kpg^{3/2}\sqrt{\frac{\log(T)\log(\log(T)/\delta)}{T}}+ \siga (\kpg^{3/2}+\kpg\sqrt{d})\frac{ \log(T)\sqrt{\log(\log(T)/\delta)}}{T} }=:\lamgd(T,\delta).
\end{aligned}
\end{align}
\end{proposition}

\paragraph{Proof of \cref{thm:JDP-L1-regression}}
Let the subroutine $\JDPLU$ be instantiated as in \cref{prop:alg-U-JDP}.
By combining \cref{prop:alg-U-JDP} and \cref{prop:JDP-GD}, we know that \whp[2\delta], normalization $(U,\lambda)$ satisfies \eqref{def:U-app}, and the estimator $\hw=\hw\kc$ satisfies $\Lipg\nrm{w\kc-\hwst}\leq \lamgd$. Applying \cref{lem:Lnew} gives \cref{thm:JDP-L1-regression} (2).
\qed

\subsection{Details of the LDP regression algorithm}\label{appdx:LDP-l1-regression}

In this section, we describe the details of $\AlgLDPRegression$ (\cref{alg:LDP-L1-regression}), a LDP analogue of \cref{alg:JDP-L1-regression}.

\begin{algorithm}[H]
\caption{$\AlgLDPRegression$ %
}\label{alg:LDP-L1-regression}
\begin{algorithmic}[1]
\REQUIRE Round $T\geq 1$, \errpara~$\delta\in(0,1)$.
\STATE Run the subroutine $\LDPLU(T/2,\delta)$ for the first $T/2$ rounds and receive $(U,\lambda)$.
\STATE Run the subroutine $\AlgLDPGD(\Lnew,T/2)$ for the remaining $T/2$ rounds and receive $\hw$ and the error bound $\lamgd$ (defined in \eqref{eq:def:lamgd-LDP}).
\ENSURE Normalization $(U,\lambda)$, estimator $\hth=U\hw$, and overall error $\lamall=\lambda+\lamgd$.
\end{algorithmic}
\end{algorithm}

In the following, we specifying the details of the subroutine $\LDPLU$ and $\AlgLDPGD$.


\paragraph{LDP learning normalization}
By adapting \cref{alg:U-JDP}, we can derive a similar algorithm for learning \um~under JDP. 


\begin{algorithm}%
\caption{Subroutine $\LDPLU$}\label{alg:U-LDP}
\begin{algorithmic}
\REQUIRE Round $T\geq 1$, \errpara~$\delta\in(0,1)$.
\REQUIRE Epoch $K\geq 1$, batch size $N=\floor{\frac{T}{K}}$.
\STATE Initialize $U\kz=\id$.
\FOR{$k=0,\cdots,K-1$}
    \FOR{$t=\rangekn$}
        \STATE Observe $\x_t$ and compute $V_t=\usqx[U\kk][t]$.
        \STATE Privatize $\til V_t\sim \sympriv[1]{V_t}$.    
    \ENDFOR
    \STATE Compute $\til H\kk=\frac1N\sumkn \til V_t$.
    \STATE Update
    \begin{align*}
        \cov\kk=U\kk\sq \til H\kk U\kk\sq+\lambda\kk U\kk, \qquad
        U\kp=\sym(\cov\kk\isq U\kk).
    \end{align*}
\ENDFOR
\ENSURE \Um~$U=U\kc$.
\end{algorithmic}
\end{algorithm}

Then, similar to \cref{prop:alg-U-JDP}, we have the following guarantee of \cref{alg:U-LDP}. 

\begin{proposition}\label{prop:alg-U-LDP}
Let $T\geq 1, K\geq 1$, $\delta\in(0,1)$, and $\epsN\defeq C_0\siga\sqrt{\frac{d+\log(K/\delta)}{N}}$, where $C_0$ is an absolute constant chosen according to \cref{lem:spectral-concen-LDP}. Suppose that \cref{alg:U-LDP} is instantiated with parameters $\lambda\kk=(2k+5)\epsN$, and then \whp,
\begin{align*}
    \exp\paren{ -\frac{\log(1/\lambda_0)}{2^{k-1}} }\id \preceq \Ep{ \uxxu[U\kk] }+\lambda\kk U\kk \preceq \exp\paren{ \frac{12}{k} }\id.
\end{align*}
In particular, as long as $K\geq \max\sset{\log\log(N),20}$, \cref{alg:U-LDP} output $(U,\lambda)$ satisfying \eqref{def:U-app} \whp, with
\begin{align*}
    \lambda=(2K+5)\epsN=\tbO{K}\cdot \siga \sqrt{\frac{d+\log(1/\delta)}{N}}.
\end{align*}
\end{proposition}

\paragraph{LDP batched SGD} Similarly, we use the following LDP batched SGD subroutine (\cref{alg:LDP-SGD}).


\begin{algorithm}[H]
\caption{Subroutine $\AlgLDPGD$}\label{alg:LDP-SGD}
\begin{algorithmic}
\REQUIRE Loss $\Lnew$ under normalization $(U,\lambda)$, round $T\geq 1$.
\REQUIRE Epoch $K\geq 1$, batch size $N=\floor{\frac{T}{K}}$, stepsize $\eta=\frac{1}{4\Lipg}$.
\FOR{$k=0,1,\cdots,K-1$}
\FOR{$t=\rangekn$}
\STATE Observe $(\x_t,y_t)$ and form gradient estimator
\begin{align*}
    g_t=\frac{U\x_t}{\|U\x_t\|}\paren{ \link(\lr U\x_t, w\kk \rr) -y_t }+\lambda\mug \cdot  U w\kk.
\end{align*}
\STATE Privatize $\til g_t\sim \priv[2]{g_t}$.
\ENDFOR
\STATE Compute $\til g\kk=\frac1N \sumkn \til g_t$ and update
\begin{align*}
    w\kp=\Proj_{\cW}\paren{ w\kk-\eta \til g\kk} .
\end{align*}
\ENDFOR
\ENSURE $\hw=w\kc$.
\end{algorithmic}
\end{algorithm}

\begin{proposition}\label{prop:LDP-GD}
Suppose that the input normalization $(U,\lambda)$ satisfies \eqref{def:U-app}. Then subroutine $\AlgLDPGD$ (\cref{alg:LDP-SGD}) preserves \aLDP~and
 achieves \whp
\begin{align*}
    \nrm{w\kk-\hwst}^2\leq \paren{ 1-\frac{1}{8\kpg} }^k\nrm{\hwst}^2+\bigO{\frac{1}{\mug^2}+\frac{d}{\mug\Lipg}}\cdot \frac{\siga^2\log(K/\delta)}{N}.
\end{align*} 
In particular, when $\lambda\geq \frac{1}{T}$ and we take $K=16\kpg \log T$, the output of \cref{alg:LDP-SGD} $\hw=w\kc$ satisfies
\begin{align}\label{eq:def:lamgd-LDP}
    \Lipg\cdot \nrm{\hw-\hwst}\leq C_1 \siga (\kpg^{3/2}+\kpg\sqrt{d}) \sqrt{\frac{\log(T)\log(\log(T)/\delta)}{T}}=:\lamgd(T,\delta).
\end{align}
\end{proposition}

\paragraph{Guarantees of $\AlgLDPRegression$} By combining \cref{prop:alg-U-LDP} and \cref{prop:LDP-GD}, we have the following result, as claimed in \cref{thm:LDP-L1-regression}.

\begin{theorem}\label{thm:LDP-L1-regression-full}
Let $T\geq 1, \delta\in(0,1)$, and the subroutines of \cref{alg:LDP-L1-regression} instantiated as in \cref{prop:alg-U-LDP} and \cref{prop:LDP-GD}. Then, \cref{alg:LDP-L1-regression} preserves \aLDP, and the following holds \whp[2\delta]:

(1) The normalization $(U,\lambda)$ satisfies \eqref{def:U-app}, and the estimator $\hw$ satisfies $\Lipg\nrm{\hw-\hwst}\leq \lamgd$, where
\begin{align*}
    \lambda=\lambda(T,\delta)=&~ \tbO{ \siga\sqrt{\frac{d\log(1/\delta)}{T}} }, \\
    \lamgd=\lamgd(T,\delta)=&~ \tbO{\siga (\kpg^{3/2}+\kpg\sqrt{d})\sqrt{\frac{\log(1/\delta)}{T}}},
\end{align*}
are defined in \cref{prop:alg-U-LDP} and \eqref{eq:def:lamgd-LDP}, and $\tbO{\cdot}$ hides polynomial factors of $\log(T)$. The overall error is defined as $\lamall(T,\delta)\defeq 4\Lipg\lambda(T,\delta)+\lamgd(T,\delta)$.

(2) By \cref{lem:Lnew}, the estimator $\hth=U\hw$ satisfies
\begin{align*}
    \errloneg{\hth}\leq \Lipg\cdot \errlone{\hth}\leq  2\sqrt{d} \lamall,
\end{align*}
and for all $\x\in\Rd$, we have the \emph{confidence bound} $\absn{ \nu(\lr \x, \hth \rr) - \nu(\lr \x,\ths \rr) }\leq \nrm{U\x} \lamall$.
\end{theorem}


\subsubsection{Proof of \cref{prop:alg-U-LDP}}


The following lemma can be proven by \cref{lem:Gaussian-concen} and \cref{lem:vec-Hoeffding}, similar to \cref{lem:spectral-concen-JDP}.
\begin{lemma}\label{lem:spectral-concen-LDP}
In \cref{alg:U-LDP}, \whp, the following inequality holds for all $k=0,\cdots,K-1$:
\begin{align}
    \nrmop{\til H\kk-\Ep{\usqx[U\kk]}}\leq C_0\siga\sqrt\frac{d+\log(K/\delta)}{N}=:\epsN,
\end{align}
where $C$ is a large absolute constant. In the following, we denote this event as $\cE$.
\end{lemma}
The proof of \cref{prop:alg-U-LDP} is then completed by combining \cref{lem:spectral-concen-LDP} and \cref{prop:spec-converge}.
\qed










\subsection{Proof of \cref{prop:JDP-GD} and \cref{prop:LDP-GD}}

In the following, we present the analysis of \cref{alg:JDP-SGD} and \cref{alg:LDP-SGD}. We first state the following standard convergence result of batched SGD on a strongly convex function.

\newcommand{\iind}[1]{\epk{#1}}
\newcommand{\us}{w^\star}
\begin{proposition}\label{prop:B-SGD-general}
Suppose that $\cW\subseteq \R^d$ is a closed, convex domain, and $F$ is a smooth convex function over $\cW$ such that $\mu\id\preceq \nabla^2 F(w)\preceq L\id$ for all $w\in\cW$. Consider the following iteration of approximate gradient descent:
\begin{align}\label{eq:B-SGD-general}
    w\iind{k+1}=\Proj_\cW\paren{w\iind{k}-\eta \paren{\nabla F(w\iind{k}) + Z\iind{k}} },
\end{align}
where $Z\iind{k}$ is the noise vector at step $k$ that is $\sigma$-sub-Gaussian (conditional on $w\iind{1:k}$), and the stepsize $\eta\in(0,\frac{1}{2L}]$. Then \whp, it holds that for all $k\in[K]$,
\begin{align*}
    \nrm{w\iind{k}-\us}^2\leq \paren{ 1-\frac{\eta\mu}{2} }^k\nrm{w\iind{0}-\us}^2+\frac{4\eta}{\mu}\max_{j<k}\nrm{Z\iind{j}}^2+\bigO{\frac{\sigma^2\log(K/\delta)}{\mu^2}},
\end{align*}
where $\us\defeq \argmin_{w\in\cW} F(w)$.
\end{proposition}

\paragraph{Proof of \cref{prop:JDP-GD}}
For the $k$th epoch of \cref{alg:JDP-SGD}, we denote
\begin{align*}
    g_t=\frac{U\x_t}{\|U\x_t\|}\paren{ \link(\lr U\x_t, w\kk \rr) -y_t }.
\end{align*}
Then, it is clear that $\EE[g_t|w\kk]=\nabla \Lnew(w\kk)-\lambda\mug\cdot Uw\kk$. 
Therefore, Algorithm $\AlgJDPGD$ (\cref{alg:JDP-SGD}) is an instantiation of \eqref{eq:B-SGD-general} with $F=\Lnew$, $w\iind{0}=0$, $\mu=\frac{\mug}{2}$, $L=2\Lipg$, $\eta=\frac{1}{2L}$. 

As we have $\nrm{g_t}\leq 2$, it is clear that \cref{alg:JDP-SGD} preserves \aJDP.
Furthermore, we can decompose
\begin{align*}
    Z\kk=g\kk-\nabla \Lnew(w\kk)=G\kk+\frac1N \sumkn (g_t-\EE[g_t|w\kk]),
\end{align*}
where $G\kk\in\Rdd$ has i.i.d entries drawn from $\normal{0, \frac{16\siga^2}{N^2}}$.
Therefore, using Hoeffding's bound, $Z\kk$ is $\sigma$-sub-Gaussian with $\sigma^2\leq \bigO{\frac{1}{N}+\frac{\siga^2}{N^2}}$. Using \cref{lem:Gaussian-concen} and \cref{lem:vec-Hoeffding}, we also have $\max_{k}\nrm{Z\iind{k}}^2\leq \bigO{\frac{1}{N}+\frac{d\siga^2}{N^2}}\log(K/\delta)$ \whp. Applying \cref{prop:B-SGD-general} gives the desired upper bounds.
\qed


\paragraph{Proof of \cref{prop:LDP-GD}}
Similarly, for the $k$th epoch of \cref{alg:JDP-SGD}, it is clear that $\nrm{g_t-\lambda\mug\cdot Uw\kk}\leq 2$ and $\EE[g_t|w\kk]=\nabla \Lnew(w\kk)$. Therefore, \cref{alg:JDP-SGD} preserves \aLDP, and it is also an instantiation of \eqref{eq:B-SGD-general} with $F=\Lnew$, $w\iind{0}=0$, $\mu=\frac{\mug}{2}$, $L=2\Lipg$, $\eta=\frac{1}{2L}$. 

To proceed, we decompose
\begin{align*}
    Z\kk=g\kk-\nabla \Lnew(w\kk)=\frac1N \sumkn \brac{ (g_t-\EE[g_t|w\kk])+G_t},
\end{align*}
where $(G_t\in\Rdd)_{t=\rangekn}$ are i.i.d Gaussian random vectors, and each $G_t$ has i.i.d entries drawn from $\normal{0, 16\siga^2}$. Therefore, we denote $G\kk=\frac1N \sumkn G_t$, and $G\kk$ has i.i.d entries drawn from $\normal{0, \frac{16\siga^2}{N} }$.
Therefore, using Hoeffding's bound, $Z\kk$ is $\sigma$-sub-Gaussian with $\sigma^2\leq \bigO{\frac{\siga^2}{N}}$. Using \cref{lem:Gaussian-concen} and \cref{lem:vec-Hoeffding}, we also have $\max_{k}\nrm{Z\iind{k}}^2\leq \bigO{\frac{d\siga^2\log(K/\delta)}{N}}$ \whp. Applying \cref{prop:B-SGD-general} gives the desired upper bounds.
\qed




\subsubsection{Proof of \cref{prop:B-SGD-general}}

By definition, we have $\us=\Proj_\cW(\us-\eta\nabla F(\us))$, and hence
\begin{align*}
    &~\nrm{w\iind{k+1}-\us}^2\\
    =&~
    \nrm{\Proj_\cW\paren{w\iind{k}-\eta \paren{\nabla F(w\iind{k}) + Z\iind{k}} }-\Proj_\cW\paren{\us-\eta\nabla F(\us)}}^2 \\
    \leq&~
    \nrm{\paren{w\iind{k}-\eta \paren{\nabla F(w\iind{k}) + Z\iind{k}}-\us}-\paren{\us-\nabla F(\us)}}^2 \\
    =&~ \nrm{w\iind{k}-\us}^2-2\eta\llr \nabla F(w\iind{k})-\nabla F(\us) + Z\iind{k}, w\iind{k}-\us \rrr +\eta^2\nrm{\nabla F(w\iind{k})-\nabla F(\us) + Z\iind{k}}^2.
\end{align*}
Thus, using the fact that $F$ is $\mu$-strongly-convex and $L$-smooth, we have
\begin{align*}
    \mu\nrm{w-\us}^2\leq \lr \nabla F(w)-\nabla F(\us), w-\us\rr \leq \frac{1}{L}\nrm{\nabla F(w)-\nabla F(\us)}^2.
\end{align*}
Hence, it holds that
\begin{align*}
    &~\nrm{w\iind{k+1}-\us}^2 \\
    \leq &~ \nrm{w\iind{k}-\us}^2-2\eta\llr \nabla F(w\iind{k})-\nabla F(\us) + Z\iind{k}, w\iind{k}-\us \rrr +2\eta^2\nrm{\nabla F(w\iind{k})-\nabla F(\us)}^2+2\eta^2 \nrm{Z\iind{k}}^2 \\
    \leq&~ \nrm{w\iind{k}-\us}^2-2\eta(1-\eta L)\llr \nabla F(w\iind{k})-\nabla F(\us), w\iind{k}-\us \rrr-2\eta \llr Z\iind{k}, w\iind{k}-\us \rrr +2\eta^2 \nrm{Z\iind{k}}^2 \\
    \leq&~ \paren{ 1-\eta\mu }\nrm{w\iind{k}-\us}^2-2\eta \llr Z\iind{k}, w\iind{k}-\us \rrr +2\eta^2 \nrm{Z\iind{k}}^2,
\end{align*}
where we use $\eta\leq \frac{1}{2L}$. 

In the following, we condition on the following event:
\begin{align*}
    \cE\defeq \sset{ \forall k\in[K]: \llr Z\iind{k}, w\iind{k}-\us \rrr\leq Z\nrm{w\iind{k}-\us} },
\end{align*}
where we denote $Z\defeq C\sigma\sqrt{\log(K/\delta)}$, $C$ is a large universal constant. By sub-Gaussian concentration, we know $\PP(\cE)\geq 1-\delta$.

Under $\cE$, we can bound
\begin{align*}
    \nrm{w\iind{k+1}-\us}^2\leq &~
    \paren{ 1-\eta\mu }\nrm{w\iind{k}-\us}^2+2\eta Z\nrm{ w\iind{k}-\us }+2\eta^2 \nrm{Z\iind{k}}^2 \\
    \leq&~ \paren{ 1-\frac{\eta\mu}{2} }\nrm{w\iind{k}-\us}^2 +2\frac{\eta}{\mu}Z^2+2\eta^2 \nrm{Z\iind{k}}^2, \qquad \forall k\in[K],
\end{align*}
where the last step is by Cauchy-Schwarz. Then, we know for any $k\in[K]$,
\begin{align*}
    \nrm{w\iind{k}-\us}^2
    \leq \paren{ 1-\frac{\eta\mu}{2} }^k\nrm{w\iind{0}-\us}^2+\frac{4}{\mu^2}Z^2+\frac{4\eta}{\mu}\max_{j<k}\nrm{Z\iind{j}}^2.
\end{align*}
This is the desired result.
\qed
