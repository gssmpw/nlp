
\colt{
\subsection{Details of the Action Elimation Algorithm}\label{appdx:spanner}


We state our JDP algorithm for generalized linear contextual bandits as follows.
\input{alg_JDP_CB}

The batch elimination algorithm \cref{alg:batch-cb-JDP} contains a planning subroutine $\AlgPlan$. Before presenting $\AlgPlan$, we first introduce the notion of \emph{spanner}.

\paragraph{Spanner}
}
\arxiv{
\subsection{Spanner}\label{appdx:spanner}
}
In $\AlgPlan$ (\cref{alg:plan}), we utilize the following notion of the \emph{spanner} of a set of actions.
\begin{definition}\label{def:spanner}
Given a context $x\in\cX$ and a set $\cA_1$ of actions, a subset $\cA'\subseteq \cA_1$ is a \emph{spanner} of $\cA_1$ if for any $a\in\cA$, there exists weights $(\gamma_{a'}\in[-1,1])_{a'\in\cA'}$ such that
\begin{align*}
    \phxa=\sum_{a'\in\cA'} \gamma_{a'} \phxa[x,a'].
\end{align*}
\end{definition}

It is well-known that a spanner with size bounded by the dimension exists, known as the \emph{barycentric spanner}~\citep{awerbuch2008online}.
\begin{lemma}[Barycentric spanner]\label{lem:bary-spanner}
For any context $x\in\cX$ and set $\cA_1$, there exists a spanner of size
\begin{align*}
    \dim(\cA_1,x)\defeq \dim(\sset{a\in\cA_1: \phxa}).
\end{align*}
\end{lemma}
An approximate barycentric spanner can be computed in time $\poly(d,|\cA_1|)$. Further, given a linear optimization oracle over the set $\sset{a\in\cA_1: \phxa}$, the time complexity can further be reduced to $\poly(d)$~\citep{hazan2016volumetric,perchet2016batched}.

\colt{
\paragraph{Planning subroutine}
We state the subroutine $\AlgPlan$ as follows. 
\input{alg_Plan}
}






\subsection{Proof of \cref{thm:regret-upper-JDP}}\label{appdx:regret-meta}




\newcommandx{\Epi}[2][1=\pi]{\EE^{#1}\brac{#2}}
\newcommand{\Px}[1]{\PP_{x\sim \mu}\paren{#1}}

To provide a unified analysis framework for \cref{alg:batch-cb-JDP} with different private regression subroutines, we first present a general action elimiation algorithm (\cref{alg:batch-cb-meta}) that additionally takes an $\Lone$-regression subroutine $\AlgCIEst$ as input.



\begin{algorithm}
\begin{algorithmic}
\REQUIRE Round $T\geq 1$, epoch schedule $1=T_0<T_1<T_2<\cdots<T_{J}=T$.
\REQUIRE Subroutine $\AlgCIEst$.
\STATE Initialize $\hft[0]\equiv 0, \CIt[0]\equiv 1$.
\FOR{$j=0,1,\cdots,J-1$}
\STATE Set $\pit[j]\leftarrow\AlgPlan(\set{ (\hft,\CIt)  }_{\tau<j})$.
\STATE Initialize the subroutine $\AlgCIEst\epj$ with round $N\epj=T\epj[j+1]-T\epj$.
\FOR{$t=T_j,\cdots,T_{j+1}-1$}
\STATE Receive context $x_t$, take action $a_t\sim \pi\epj(x_t)$, and receive reward $r_t$.
\STATE Feed $(\phxa[x_t,a_t],r_t)$ into $\AlgCIEst\epj$.
\ENDFOR
\STATE Receive estimation $(\hft[j],\CIt[j])$ from $\AlgCIEst\epj$.
\ENDFOR
\end{algorithmic}
\caption{Meta Batch Elimination Algorithm}\label{alg:batch-cb-meta}
\end{algorithm}

Similar to our argument in \cref{appdx:JDP-verify}, we can show that \cref{alg:batch-cb-meta} preserves \aJDP~(\aLDP) if the subroutine $\AlgCIEst$ preserves \aJDP~(\aLDP).
Furthermore, we state the regret guarantee of \cref{alg:batch-cb-meta} under the following assumption on the subroutine $\AlgCIEst$.

\begin{assumption}\label{asmp:EstCI}
For each $j$, the subroutine $\AlgCIEst$ returns $(\hft[j],\CIt[j])$ such that the following holds \whp.

(1) The function $\CIt[j](x,a)$ provides a valid confidence bound:
\begin{align*}
    \Px{ \forall a\in\cA, \abs{ \hft(x,a)-\fs(x,a) }\leq \CIt(x,a) }\geq 1-\delta_0.
\end{align*}

(2) The function $\CIt[j](x,a)=b\epj\paren{\phxa}$ is given by a norm function $b\epj$ over $\Rd$.
\end{assumption}


\begin{theorem}[Meta regret guarantee]\label{thm:regret-upper-meta}
Under \cref{asmp:EstCI}, \cref{alg:batch-cb-meta} ensures that
\begin{align*}
    \Reg\leq \EE\brac{ \sum_{j=0}^{J-2} 4\dA\cdot N\epj[j+1] \Epi[{\pit[j]}]{ \CIt[j](x,a) } } + 2N\epj[0] + 2TJ\delta+ 2T^2\delta_0,
\end{align*}
where $N\epj=T\epj[j+1]-T\epj$ is the batch size of the $j$th epoch. Further, if the subroutine $\AlgCIEst$ preserves \aJDP~(or correspondingly \aLDP), then \cref{alg:batch-cb-meta} preserves \aJDP~(or correspondingly \aLDP).
\end{theorem}

With \cref{thm:regret-upper-meta}, we can prove \cref{thm:regret-upper-JDP} easily from the guarantee of subroutine $\AlgJDPRegression$ (\cref{thm:JDP-L1-regression}).
\paragraph{Proof of \cref{thm:regret-upper-JDP}}
To see how \cref{alg:batch-cb-meta} generalizes \cref{alg:batch-cb-JDP}, we consider instantiate it with the subroutine $\AlgCIEst$ be specified by $\AlgJDPRegression$ (\cref{alg:JDP-L1-regression}), and for the output $(U\epj,\lambda\epj), \hth\epj, \lamall\epj$ of the instance $\AlgJDPRegression\epj$, we set
\begin{align*}
    \hft[j](x,a)=\nu(\lr \phxa, \hth\epj \rr), \qquad
    \CIt[j](x,a)=\lamall \epj \nrm{U\epj \phxa}, \qquad \forall (x,a)\in\cX\times\cA,
\end{align*}
where $\lamall\epj=\lamall(N\epj, \delta)$ is defined in \cref{thm:JDP-L1-regression}.
Then, it is clear that under these specifications, \cref{alg:batch-cb-meta} agrees with \cref{alg:batch-cb-JDP}. 

Further, we have $\Epi[{\pit[j]}]{ \CIt[j](x,a) }\leq 2d\lamall\epj$, and \cref{asmp:EstCI} holds with $\delta_0=0$, because under the success event of \cref{thm:JDP-L1-regression}, we have
\begin{align*}
    \abs{ \hft[j](x,a)-\fs(x,a) }\leq \CIt[j](x,a), \qquad \forall x\in\cX, a\in\cA.
\end{align*}
Then, \cref{thm:regret-upper-meta} yields (with $\delta=\frac1T$)
\begin{align*}
    \Reg\leqsim&~ \dA d\sum_{j=0}^{J-2} N\epj[j+1] \lamall\epj +N\epj[0] \\
    \leq&~ \poly(\log T)\cdot  \dA d\sum_{j=0}^{J-2} \paren{ \kpg^{3/2} \frac{N\epj[j+1]}{\sqrt{N\epj}}+\siga(\kpg^{3/2}+\kpg\sqrt{d})\frac{N\epj[j+1]}{N\epj} } + N\epj[0].
\end{align*}
In particular, under the choice $T\epj=2^{j}$, we have 
\begin{align*}
    \Reg\leq \tbO{\dA d \kpg^{3/2} \sqrt{T} + \siga \dA d(\kpg^{3/2}+\kpg\sqrt{d})}.
\end{align*}
\qed

\subsubsection{Proof of \cref{thm:regret-upper-meta}}
For any policy $\pi:\cX\to\Delta(\cA)$, we define its sub-optimality as
\begin{align*}
    \reg(\pi)=\EE_{x\sim P, a\sim \pi(x)}\brac{ \fs(x,\pis(x))- \fs(x,a) },
\end{align*}
where we recall that $\pis(x)\defeq \argmax_{a\in\cA} \fs(x,a)$ is the optimal policy under $\fs$.
Then, by definition, for \cref{alg:batch-cb-meta},
\begin{align*}
    \Reg=\EE\brac{ \sum_{j=0}^{J-1} \sum_{t=T\epj}^{T\epj[j+1]-1} \reg(\pi\epj)}
    =\EE\brac{ \sum_{j=0}^{J-1} N\epj \cdot \reg(\pi\epj)}.
\end{align*}

In the following, we work with the following quantity:
\begin{align*}
    \Reg^+\defeq \sum_{j=0}^{J-1} N\epj \cdot \reg(\pi\epj),
\end{align*}
which is a random variable measuring the cumulative sub-optimality of the algorithm.


We assume the success event of \cref{asmp:EstCI} and define
\begin{align*}
    \cX\epj\defeq \sset{ x\in\cX: \forall \tau\leq j, a\in\cA,  \abs{ \hft(x,a)-\fs(x,a) }\leq \CIt(x,a) }.
\end{align*}
Then, for any $x\in\cX\epj$ and $\tau\leq j$, we have
\begin{align*}
    \hft(x,\pis(x))+\CIt(x,\pis(x))\geq \fs(x,\pis(x))
    =\max_{a\in\cA} \fs(x,a)
    \geq \max_{a\in\cA} \hft(x,a)-\CIt(x,a),
\end{align*}
and hence
$\pis(x)\in\cAxt[j]$. Further, for any $a\in\cAxt[j]$,
\begin{align*}
    \hft[j](x,a)+\CIt[j](x,a)\geq \hft[j](x,\pis(x))-\CIt[j](x,\pis(x)),
\end{align*}
and hence
\begin{align*}
    \fs(x,\pis(x))-\fs(x,a)
    \leq&~ \hft[j](x,\pis(x))+\CIt[j](x,\pis(x))-\hft[j](x,a)+\CIt[j](x,a)\\
    \leq&~ 2\CIt[j](x,\pis(x))+2\CIt[j](x,a) \\
    \leq&~ 4\max_{a'\in\cAxt[j]}\CIt[j](x,a'), \qquad \forall x\in\cX\epj, a\in\cAxt[j].
\end{align*}
Note that $\pi\epj[j+1](x)$ is supported on $\cAxt[j]$, and hence
\begin{align*}
    \reg(\pi\epj[j+1])=\EE_{x\sim P, a\sim \pi\epj[j+1](x)}\brac{ \fs(x,\pis(x))- \fs(x,a) }
    \leq 4\EE_{x\sim P} \max_{a\in\cAxt[j]}\CIt[j](x,a).
\end{align*}
Further, $\cAsp[j]$ is a barycentric spanner of $\cAxt[j-1]$, and hence for any $a\in\cAxt[j]\subseteq \cAxt[j-1]$, there exists parameters $(\gamma_{x,a,a'}\in[-1,1])_{a'\in\cAsp[j]}$, such that
\begin{align*}
    \phxa=\sum_{a'\in\cAsp[j]} \gamma_{x,a,a'} \phxa[x,a'].
\end{align*}
Hence, by \cref{asmp:EstCI} (2), for any $x\in\cX\epj$, $a\in\cAxt[j]$,
\begin{align*}
    \CIt[j](x,a)=&~ b\epj\paren{ \sum_{a'\in\cAsp[j]} \gamma_{x,a,a'} \phxa[x,a'] } 
    \leq \sum_{a'\in\cAsp[j]} \abs{\gamma_{x,a,a'}} b\epj(\phxa[x,a']) \\
    \leq&~ \sum_{a'\in\cAsp[j]} b\epj(\phxa[x,a'])
    = |\cAsp[j]|\cdot \EE_{a'\sim \pi\epj(x)} \CIt[j](x,a').
\end{align*}
Therefore, for any $x\in\cX\epj$,
\begin{align*}
    \max_{a\in\cAxt[j]}\CIt[j](x,a)\leq \dA \cdot \EE_{a'\sim \pi\epj(x)} \CIt[j](x,a'),
\end{align*}
and thus,
\begin{align*}
    \reg(\pit[j+1])
    \leq 4\dA\cdot \EE_{x\sim P, a\sim \pi\epj(x) }{ \CIt[j](x,a) } + 2P(x\not\in\cX\epj).
\end{align*}
By \cref{asmp:EstCI}, $P(x\not\in\cX\epj)\leq T\delta_0$, and hence taking summation over $j=0,1,\cdots,J-2$ gives
\begin{align*}
    \sum_{j=1}^{J-1} N\epj \cdot \reg(\pi\epj)
    \leq 4\dA\sum_{j=0}^{J-2} N\epj[j+1] \cdot \EE^{\pi\epj[j]}\brac{ \CIt[j](x,a) } +2T^2 \delta_0.
\end{align*}
Note that the above inequality holds under the success event of \cref{asmp:EstCI}, which holds with probability at least $1-J\delta$. Then taking expectation gives the desired upper bound on $\Reg$.
\qed

We also remark that a high-probability upper bound on the regret follows similarly (with an extra step of applying martingale concentration).


\subsection{Proof of \cref{thm:regret-upper-JDP-better}}\label{appdx:proof-regret-upper-JDP-better}

\newcommand{\AlgJDPLR}{\mathsf{JDP\_Reweighted\_Linear\_Regression}}
\newcommand{\delz}{\delta_0}
\newcommand{\lamz}{\lambda_0}

In this section, we present an adaption of \cref{alg:batch-cb-JDP} based on the subroutine $\AlgJDPLR$ (\cref{alg:linear-JDP-better}), which provides a tighter rate of convergence.

\newcommand{\gu}[1]{\frac{U#1}{\|U#1\|}}
\newcommand{\sumnt}{\sum_{t=N+1}^T}
\newcommand{\avgtt}{\frac{1}{N}\sum_{t=N+1}^T}
\newcommand{\sumtt}{\sum_{t=N+1}^T}
\renewcommand{\zt}{\zeta_t}


\begin{algorithm}
\caption{Subroutine $\AlgJDPLR$}\label{alg:linear-JDP-better}
\begin{algorithmic}[1]
\REQUIRE Dataset $\dataset=\sset{(\x_t,y_t)}_{t\in[T]}$ with size $T=2N$, \errpara~$\delta\in(0,1)$, $\delz\in(0,1)$.
\STATE Split $\dataset=\dataset_0\cup \dataset_1$ equally.
\STATE Set $(U,\lambda)\leftarrow \JDPLU(\dataset_0,\delta/2)$
\STATE Compute the following estimates on $\dataset_1$:
\begin{align*}
    \xi=\avgtt \gu{\x_t}y_t, \qquad \Xi=\avgtt \gu{\x_t}\x_t\tp+\lambda\id, \qquad 
    W=\avgtt \frac{U\x_t \x_t\tp U}{\|U\x_t\|^2}+\lambda\id.
\end{align*}
\STATE Privatize $[\til \xi; \til \Xi] \sim \priv[3/N]{ [\xi; \Xi] }$ and $\til W\sim \sympriv[3/N]{W}$.
\ENSURE Estimators $\hth={\til \Xi}\iv{\til \xi}$ and confidence bound
\begin{align*}
    \CI(\x)=16\lambda\nrmn{U\x}+\lamgd\nrmn{{\til \Xi}\itp \x}_{\til W}.
\end{align*}
\end{algorithmic}
\end{algorithm}

The subroutine $\AlgJDPLR$ utilizes the linear structure of the linear models. Instead of running a batched SGD procedure, it directly computes an estimator of $\ths$ by solving a privatized linear equation $\til \Xi \hth=\til \xi$. It is clear that \cref{alg:linear-JDP-better} preserves \aJDP, and we also show that the confidence bound $\abs{\lr \x, \hth-\ths\rr}\leq \CI(\x)$ holds true in a \emph{distributional} sense.

\begin{proposition}\label{prop:linear-JDP-better}
Let $T\geq 1$, $\delta\in(0,1)$, $P'$ be a fixed distribution over $\Bone$, and the subroutine $\JDPLU$ of \cref{alg:linear-JDP-better} be instantiated as in \cref{prop:spec-converge-JDP}. Then \cref{alg:linear-JDP-better} preserves \aJDP~and ensures the following holds \whp:

(1) The normalization $(U,\lambda)$ satisfies \eqref{def:U-app}, with $\lambda$ given by
\begin{align}\label{eq:def:lambda-JDP-better}
    \lambda\defeq \lambda(T,\delta)=C\frac{\siga\sqrt{d+\log(K/\delta)}}{T},
\end{align}
where $C$ is a large absolute constant. 

(2) With $\lamgd\defeq \lamgd(T,\delta)=C_0 \sqrt{\frac{\log(1/(\delta\delz)}{T}}$ for a large absolute constant $C_0$, it holds that 
\begin{align*}
    \PP_{\x'\sim P'}\paren{ \absn{ \lr \x', \hth-\ths \rr }\geq \CI(\x') }\leq \delz,
\end{align*}

(3) It holds that $\Ep{\CI(\x)}\leq 16\sqrt{d}\cdot \lamgd+32d(\lambda+\lamgd^2)$.
\end{proposition}

\cref{thm:regret-upper-JDP-better} then follows from the general guarantee of \cref{thm:regret-upper-meta}.

\paragraph{Proof of \cref{thm:regret-upper-JDP-better}}
We instantiate the subroutine $\AlgCIEst$ to be \\ $\AlgJDPLR$ in \cref{alg:batch-cb-meta}, with parameter $\delz=\frac{1}{\delta T^2|\cA|}$.
We also let $P'$ be the distribution of $\phxa$ under $x\sim P$, $a\sim \Unif(\cA)$.

Then, for the $j$th epoch, we let $\lambda\epj\defeq \lambda(N\epj,\delta)$, $\lamz\epj\defeq \lamz(N\epj,\delta)$. \cref{prop:linear-JDP-better} guarantees that \whp,
\begin{align*}
    \MoveEqLeft \PP_{x\sim P}\paren{ \exists a\in\cA, \abs{\lr \phxa, \hth\epj-\ths\rr}\geq \CI\epj(\phxa) } \\
    \leq&~ |\cA|\cdot \PP_{\x'\sim P'}\paren{  \abs{\lr \x', \hth-\ths \rr }\geq \CI\epj(\x') } \leq |\cA|\delz\leq \frac{\delta}{T},
\end{align*}
and we also have
\begin{align*}
    \EE^{\pi\epj}\brac{\CIt[j](x,a)}\leqsim &~ \sqrt{d}\lamgd\epj+d(\lambda\epj+(\lamgd\epj)^2)\\
    \leqsim&~\sqrt{\frac{d\log(1/\delz)}{N\epj}}+ \frac{\siga d^{3/2}\sqrt{\log(N\epj/\delta)}+d\log(1/\delz)}{N\epj}.
\end{align*}

Therefore, \cref{asmp:EstCI} holds, and \cref{thm:regret-upper-meta} yields
\begin{align*}
    \Reg\leqsim &~ \dA\sqrt{d\log(1/\delz)}\sum_{j=0}^{J-2} \frac{N\epj[j+1]}{\sqrt{N\epj}} + \dA\paren{\siga d^{3/2}\sqrt{\log(T/\delta)}+d\log(1/\delz)} \sum_{j=0}^{J-2} \frac{N\epj[j+1]}{N\epj} \\
    &~+N\epj[0]+TJ\delta.
\end{align*}
In particular, with the choice $T\epj=2^{j+1}$ and $\delta=\frac{1}{T}$, we have
\begin{align*}
    \Reg\leq \tbO{ \dA\sqrt{dT\log|\cA|} +\dA(\siga d^{3/2}+d\log|\cA|)},
\end{align*}
where $\tbO{\cdot}$ hides $\poly(\log T)$ factors.
\qed


\subsubsection{Proof of \cref{prop:linear-JDP-better}}

By \cref{prop:spec-converge-JDP}, the subroutine $\JDPLU$ can be suitably instantiated so that the output $(U,\lambda)$ satisfies \eqref{def:U-app} \whp[\frac\delta2], with epoch size $K=\max\sset{\log\log(T),10}$, and $\lambda$ given by \eqref{eq:def:lambda-JDP-better}. 


In the following, we proceed to prove (2).
By definition, we have
\begin{align*}
    \til \xi = \xi+Z_\xi, \qquad
    \til \Xi=\Xi+Z_\Xi, \qquad
    \til W = W+Z_W,
\end{align*}
where entries of $Z=[Z_\xi; Z_\Xi; Z_W]$ are independent zero-mean Gaussian random variable with variance $\frac{36\siga^2}{N^2}$. Further, we also have
\begin{align*}
    y_t=\lr \x_t, \ths\rr+\zt, \qquad \EE[\zt|\x_t]=0,
\end{align*}
and conditional on the random variables $U, (\x_t)_{t\in[N+1,T]}$ and $Z$, the random variables $(\zt)_{t\in[N+1,T]}$ are independent.

In particular, the following event $\cE_1$ holds \whp[\frac{\delta}{6}]:
\begin{align*}
    \cE_1:\quad \max\sset{ \nrm{Z_\xi}, \nrmop{Z_\Xi}, \nrmop{Z_W}  }\leq \frac{\lambda}{16}.
\end{align*}
Further, by \cref{lem:cov-concen}, the following event $\cE_2$ holds \whp[\frac{\delta}{6}]:
\begin{align*}
    \cE_2: \quad \avgtt \usqx[U][t]+\frac12\lambda\id\succeq \frac12 \Epp{\usqx[U]}.
\end{align*}
In the following, we condition on $\cE_1\cap \cE_2$.

Under $\cE_2$, we have
\begin{align*}
    D:=\avgtt \uxxu[U][t]+\lambda U \succeq \frac14\id.
\end{align*}
Note that $\Xi=DU\iv$. For any vector $v\in\Rd$, we can bound
\begin{align*}
    \nrm{U{\til \Xi}\tp v}
    =\nrm{ (D+U Z_\Xi)v  } 
    \geq \nrm{Dv}- \nrm{U Z_\Xi v}
    \geq \frac{1}{4}\nrm{v}-\nrmopn{U}\nrmop{Z_\Xi}\nrm{v}\geq \frac18\nrm{v},
\end{align*}
where we use $\nrmopn{U}\leq \frac{2}{\lambda}$ and $\nrmop{Z_\Xi}\leq \frac{\lambda}{16}$. Therefore, we have $\nrmop{{\til \Xi}\itp U\iv}\leq 8$.

Now, we decompose
\begin{align*}
    \til \xi=\avgtt \gu{\x_t}\x_t\tp \ths + \avgtt \gu{\x_t}\zt+Z_\xi,
    \qquad
    \til \Xi=\avgtt \gu{\x_t}\x_t\tp+\lambda\id+Z_\Xi.
\end{align*}
Hence, we can re-write
\begin{align*}
    \til \xi=\paren{ \til \Xi-Z_\Xi-\lambda\id }\ths + \avgtt \gu{\x_t}\zt+Z_\xi,
\end{align*}
and we denote $e=\avgtt \gu{\x_t}\zt$.
Note that under $\cE_1$, we have $\nrm{ \til \xi -\til \Xi \ths- e} \leq 2\lambda$, and hence for any vector $\x\in\Rd$,
\begin{align*}
\abs{\lr \x, \hth-\ths \rr}\leq \abs{\lr {\til \Xi}\itp \x, \til \xi -\til \Xi \ths- e \rr}  + \abs{\lr \x, \Xi\iv e \rr } \leq 16\lambda\nrm{U \x}+\abs{\lr \x, \Xi\iv e \rr },
\end{align*}
where we also use $\nrmop{{\til \Xi}\itp U\iv}\leq 8$.

It remains to upper bound $\abs{\lr \x', \Xi\iv e \rr }$ under the fixed distribution $\x'\sim P'$.
The following lemma asserts $e$ is a sub-Gaussian random vector, conditional on $\x_{N+1},\cdots,\x_T$. The proof is a direct corollary of Hoeffding's bound and is deferred to the end of this section.
\begin{lemma}\label{lem:sub-Gaussian-empirical}
There is an absolute constant $c_0>0$ such that for any $v\in\Bone$,
\begin{align*}
    \EE_\zeta\cond{ \exp\paren{ c_0N\frac{\lr v,e\rr^2}{\nrm{v}_{W_0}^2} } }{U,\x_{N+1},\cdots,\x_T,Z}\leq 2,
\end{align*}
where $W_0=\avgtt \frac{U\x_t \x_t\tp U}{\|U\x_t\|^2}$.
\end{lemma}

In particular, taking expectation over $v={\til \Xi}\itp \x'$, with $\x'\sim P$, we have
\begin{align*}
    \EE_{\x'\sim P}\EE_\zeta\brac{ \exp\paren{ c_0N\frac{\lr v,{\til \Xi}\iv e\rr^2}{\nrmn{{\til \Xi}\itp \x'}_{W_0}^2} } }\leq 2.
\end{align*}
By Markov's inequality, the following event $\cE_3$ holds \whp:
\begin{align*}
    \cE_3:\quad \EE_{\x'\sim P}\brac{ \exp\paren{ c_0N\frac{\lr \x',{\til \Xi}\iv e\rr^2}{\nrmn{{\til \Xi}\itp \x'}_{W_0}^2} } } \leq \frac{6}{\delta}.
\end{align*}
Note that under $\cE_3$, using Markov's inequality again, we have
\begin{align*}
    \PP_{\x'\sim P'}\paren{ \frac{\absn{\lr \x',{\til \Xi}\iv e\rr}}{\nrmn{{\til \Xi}\itp \x'}_{W_0}}\geq \sqrt{\frac{\log(6/(\delta\delz))}{c_0N}} }\leq \delz.
\end{align*}
Note that under $\cE_1$, we have $\til W=W+Z_W=W_0+\lambda\id+Z_W\succeq W_0$.
Therefore, we can conclude that
\begin{align*}
    \PP_{\x'\sim P'}\paren{ \absn{\lr \x',\hth-\ths\rr}\geq 16\lambda\nrm{U\x'}+\sqrt{\frac{\log(6/(\delta\delz))}{c_0N}}\nrmn{{\til \Xi}\itp \x'}_{\til W} }\leq \delz.
\end{align*}

It remains to prove (3). by Cauchy inequality,
\begin{align*}
    \paren{ \Epp{ \nrm{{\til \Xi}\itp \x}_{\til W} } }^2
    \leq&~ \Epp{\nrmn{U \x}} \cdot \Ep{ \frac{\nrmn{{\til \Xi}\itp \x}_{\til W}^2}{\nrmn{U \x}} } \\
    \leq&~ 2d\cdot \llr \til W, \Epp{ \frac{{\til \Xi}\itp \x\x\tp {\til \Xi}\iv}{\nrmn{U \x}} }\rrr \\
    \leq&~ 4d\nrmop{{\til \Xi}\itp U\iv}^2\tr(\til W),
\end{align*}
where we use the fact that $\Ep{\uxxu}\preceq 2\id$. Using $\tr(\til W)=\tr(W_0)+\lambda d+\tr(Z_W)\leq 1+2\lambda d$, we have $\Epp{ \nrm{{\til \Xi}\itp \x}_{\til W} }\leq 16\sqrt{d(1+2\lambda d)}$. Combining the inequalities above completes the proof.
\qed

\paragraph{Proof of \cref{lem:sub-Gaussian-empirical}}
In the following, we condition on $(U,\x_{N+1},\cdots,\x_T,Z)$.
By Hoeffding's bound, for any $\lambda\in\R$,
\begin{align*}
    \EE_\zeta\brac{ \exp\paren{ \lambda \lr v,Ne\rr } }
    =&~ \EE_\zeta\brac{ \exp\paren{ \sumtt \lambda \lr v,\gu{\x_t}\rr \zt } } \\
    \leq&~ \exp\paren{ \frac{\lambda^2}{2}\sumtt \lr v,\gu{\x_t}\rr^2 } \\
    =&~ \exp\paren{ \frac{N\lambda^2}{2}\nrm{v}_{W_0}^2 }.
\end{align*}
Therefore, $\lr v, Ne\rr$ is a $\sigma_v$-sub-Gaussian random variable with $\sigma^2_v\leq N\nrm{v}_{W_0}^2$. Hence, there is an absolute constant $c_0>0$ such that for any $v\in\R^d$,
\begin{align*}
    \EE_\zeta\brac{ \exp\paren{ c_0N\frac{\lr v,e\rr^2}{\nrm{v}_{W_0}^2} } }\leq 2.
\end{align*}
\qed


\subsection{Proof of \cref{thm:regret-upper-LDP}}\label{appdx:regret-upper-LDP}


In \cref{alg:batch-cb-meta}, we instantiate the subroutine $\AlgCIEst$ with $\AlgLDPRegression$ (\cref{alg:LDP-L1-regression}). For the $j$th epoch, we let $(U\epj,\lambda\epj), \hth\epj, \lamall\epj$ be the output of the subroutine $\AlgLDPRegression$, and we let
\begin{align*}
    \hft[j](x,a)=\nu(\lr \phxa, \hth\epj \rr), \qquad
    \CIt[j](x,a)=\lamall \epj \cdot \nrmn{U\epj \phxa}, \qquad \forall (x,a)\in\cX\times\cA,
\end{align*}
following \cref{alg:batch-cb-JDP}. Recall that for the subroutine $\AlgLDPRegression$, we have
\begin{align*}
    \lamall\epj\defeq \lamall(N\epj,\delta)=\tbO{\siga (\kpg^{3/2}+\kpg\sqrt{d})\sqrt{\frac{\log(1/\delta)}{T}}},
\end{align*}
which is defined in \cref{thm:LDP-L1-regression-full}. 

Note that \cref{asmp:EstCI} holds with $\delta_0=0$ under the above specifications, and we also have $\EE^{\pi\epj}\brac{\CIt[j](x,a)}\leq 2d\lamall\epj$. Therefore, applying \cref{thm:regret-upper-meta} yields
\begin{align*}
    \Reg\leqsim&~ \dA d\sum_{j=0}^{J-2} N\epj[j+1] \cdot \lamall\epj + N\epj[0]+TJ\delta.
\end{align*}
In particular, with the epoch schedule $T\epj=2^{j}$ and $\delta=\frac1T$, we have
\begin{align*}
    \Reg\leq \tbO{ \dA d(\kpg^{3/2}+\kpg\sqrt{d})\sqrt{T} }.
\end{align*}
This is the desired upper bound.
\qed
