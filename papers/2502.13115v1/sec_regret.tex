

In this section, we use our \Lone-regression method from \cref{sec:l1-reg} as a subroutine for the action elimination framework~\citep{li2024optimal}, providing rate-optimal private regret bounds for generalized linear contextual bandits. 

Our algorithm (\cref{alg:batch-cb-JDP}) is epoch-based (with a given epoch schedule $1=T_0<T_1<T_2<\cdots<T_{J}=T$), and it iteratively builds estimations of the ground truth reward function $\fs$ and plans according to the estimations. The algorithm, which consists of an estimation procedure and a planning procedure described as follows, is similar to that of \citet{li2024optimal}. 
\colt{The formal definition is deferred to \cref{appdx:spanner}, and we briefly introduce the main ideas here.}
\arxiv{
\input{alg_JDP_CB}
}






\paragraph{Estimation procedure}
For $j$th epoch, the estimate reward function $\hft[j]$ and the confidence radius $\CIt[j]$ are produced by the subroutine $\AlgJDPRegression$. Then, by \cref{thm:JDP-L1-regression}, it holds that with high probability
\begin{align}\label{eq:CI-bound}
\textstyle
    \fs(x,a)\in \brac{ \hft[j](x,a)-\CIt[j](x,a), \hft[j](x,a)+\CIt[j](x,a) }, \qquad \forall x\in\cX, a\in\cA.
\end{align}

\arxiv{
\input{alg_Plan}

\paragraph{Planning procedure}
The policy $\pit[j]$ of the $j$th epoch is built upon the confidence intervals \cref{eq:CI-bound} given by the estimations $(\hft[0], \CIt[0]),\cdots,(\hft[j-1],\CIt[j-1])$ 
from previous epochs. Given the estimations, subroutine $\AlgPlan$ (\cref{alg:plan}) eliminates the sub-optimal arms for each context $x\in\cX$ according to \eqref{eq:eliminate}, and output the $\pi\epj(x)$ based on a \emph{spanner}
of the remaining actions.\footnote{Defined in \cref{def:spanner}. Particularly, when $|\cA|=\bigO{1}$, we can directly takes $\cAsp[j]=\cAxt[j-1]$ and set $\pi\epj(x)=\Unif(\cAxt[j-1])$.
} This procedure implicitly encourages exploration, as it uses optimistic estimation (UCB) of the value of each arm.
}

\colt{
\paragraph{Planning procedure}
The policy $\pit[j]$ of the $j$th epoch is built upon the confidence intervals \cref{eq:CI-bound} given by the estimations $(\hft[0], \CIt[0]),\cdots,(\hft[j-1],\CIt[j-1])$ 
from previous epochs. Given the estimations, subroutine $\AlgPlan$ (\cref{alg:plan}, detailed in \cref{appdx:spanner}) eliminates the sub-optimal arms for each context $x\in\cX$ according to \eqref{eq:eliminate}, and output the $\pi\epj(x)$ based on a \emph{spanner}
of the remaining actions. This procedure implicitly encourages exploration, as it uses optimistic estimation (UCB) of the value of each arm.
}




\paragraph{Regret guarantee}
We state the upper bound of \cref{alg:batch-cb-JDP} in terms of the dimensionality of the per-context action space: \colt{$\dA\defeq \max_{x\in\cX} \dim\paren{\set{ \phi(x,a): a\in\cA }}$.}
\arxiv{
\begin{align*}
    \dA\defeq \max_{x\in\cX} \dim\paren{\set{ \phi(x,a): a\in\cA }}.
\end{align*}
}
Note that $\dA\leq \min\set{d,|\cA|}$.

\begin{theorem}[Regret upper bound under JDP]\label{thm:regret-upper-JDP}
\cref{alg:batch-cb-JDP} preserves \aJDP. With the epoch schedule be $T\epj=2^j$ for $j=0,1,\cdots$ and $\delta=\frac{1}{T}$, it holds that
\begin{align*}
\textstyle
    \Reg\leq \tbO{\dA d \kpg^{3/2} \sqrt{T} + \siga \dA d(\kpg^{3/2}+\kpg\sqrt{d})}.
\end{align*}
\end{theorem}
\vspace{-10pt}
This provides a regret of order $\sqrt{T}+\frac{1}{\alpha}$ (omitting $\poly(d)$-factors and logarithmic terms), which partially resolves the open problem stated by \citet{azize2024open}.

As a remark, we note that we state \cref{alg:plan} for the sake of clarity: In the implementation of \cref{alg:batch-cb-JDP}, we do \emph{not} need to range over every context $x\in\cX$ to form the policy $\pit[j]$. It is sufficient to compute $\pit[j](x_t)$ for each round $t$ in $j$th epoch (according to \eqref{eq:eliminate}). Note that a spanner can be computed in time $\poly(d,|\cA|)$, and hence
\cref{alg:batch-cb-JDP} can be implemented in time $\poly(d,|\cA|)\cdot T$. Further discussions are deferred to \cref{appdx:spanner}.

\paragraph{Regret for linear contextual bandits}
In linear contextual bandits with a bounded action space $\cA$, we can use a slightly different subroutine for producing confidence intervals (detailed in \cref{appdx:proof-regret-upper-JDP-better}), which provides the following refined upper bound.

\begin{theorem}[Regret upper bound in linear contextual bandits]\label{thm:regret-upper-JDP-better}
\cref{alg:batch-cb-JDP} (with the estimation subroutine replaced by \cref{alg:linear-JDP-better}) preserves \aJDP~and guarantees that
\begin{align*}
\textstyle
    \Reg\leq \tbO{ \dA \sqrt{dT\log|\cA|}+ \siga \dA d^{3/2} }.
\end{align*}
\end{theorem}
\vspace{-10pt}
In particular, when $|\cA|=\bigO{1}$, we obtain a upper bound of $\tbO{\sqrt{dT}+\frac{d^{3/2}}{\alpha}}$. Note that the $\sqrt{dT}$-term matches the optimal non-private regret bound for $|\cA|=\bigO{1}$ (up to logarithmic factors). %
Therefore, in this case, privacy is almost \emph{for free}, as the second term (the ``price of privacy'') grow as $\tbO{1/\alpha}$ and is of lower order as $T\to \infty$.

\paragraph{Extension: Local DP}
By using $\AlgLDPRegression$ (\cref{alg:LDP-L1-regression}) as the estimation subroutine, we can easily adapt \cref{alg:batch-cb-JDP} so that it preserves local DP. We state the corresponding regret bound as follows and defer the details to \cref{appdx:regret-upper-LDP}.
\begin{theorem}[Regret upper bound under LDP]\label{thm:regret-upper-LDP}
A variant of \cref{alg:batch-cb-JDP} with estimation subroutine \cref{alg:LDP-L1-regression} (detailed in \cref{appdx:regret-upper-LDP}) preserves \aLDP~and
\begin{align*}
\textstyle
    \Reg\leq \tbO{\siga \dA d(\kpg\sqrt{d}+\kpg^{3/2}) \sqrt{T} }.
\end{align*}
\end{theorem}
\vspace{-10pt}
Particularly, in linear contextual bandits, \cref{thm:regret-upper-LDP} provides a regret bound of $\sqrt{d^5T}/\alpha$. While this is a $d$-factor worse than the result of \citet{chen2024private}, our algorithm is computationally efficient, and the number of switches (or, changes) of the deployed decision-channel pair $(\pi_t, \pr_t)$ is bounded by $\bigO{\kpg \log^2(T)}$.

