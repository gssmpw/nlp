
We start by describing how we can learn the normalizing matrix $U$ from data privately.


\newcommand{\prB}[1]{\mathsf{B}_{\alpha}(#1)}
\newcommand{\CMT}[1]{\STATE {\color{blue} //#1}}
\begin{algorithm}[t]
\caption{Locally Private Linear Regression with Iterative Normalization}\label{alg:LR-normal}
\begin{algorithmic}[1]
\REQUIRE Round $T\geq 1$, epoch $k_1\geq 1$.
\STATE Set $N=\floor{\frac{T}{2k_1}}$ and epochs $\tau_k=1+(k-1)N$.
\STATE Set $U_1=\id$.
\FOR{$k=1,\cdots,k_1$}
    \FOR{$t=\tau_k,\cdots,\tau_{k+1}-1$}
        \STATE Send $U_i$ to the $t$-th user
        \CMT{ At $t$-th user: }
        \STATE Observe $x_t\sim p$ and compute $b_t=\frac{U_k\sq x}{\sqrt{\nrm{U_kx}}}$.
        \STATE Sample $B_t\sim \prB{b_tb_t\tp}$ and send $B_t$ to the server.    
    \ENDFOR
    \STATE Compute $H_k=\frac1N\sum_{t=\tau_k}^{\tau_{k+1}-1} B_t$.
    \STATE Update
    \begin{align*}
        \Sigma_k=U_k\sq H_k U_k\sq+\lambda_k U_k, \qquad
        U_{k+1}=\sym(\Sigma_k\isq U_k).
    \end{align*}
\ENDFOR
\STATE Set $U=U_{k_1+1}$ and $g=g_{U}$. Set $T_1=T-k_1N$.
\FOR{$t=\tau_{k_1+1},\cdots,T$}
    \STATE Send $U$ to the $t$-th user.
    \CMT{At $t$-th user}
    \STATE Observe $(x_t,y_t)\sim p$.
    \STATE Sample $a_t\sim \prB{g(x_t)y_t}$ and $B_t\sim \prB{ g(x_t)x_t\tp }$, and send $(a_t,B_t)$ to the server.
\ENDFOR
\STATE Compute $a=\frac{1}{T_1}\sum_{t=kN+1}^T a_t$ and $B=\frac{1}{T_1}\sum_{t=kN+1}^T B_t$.
\STATE Set
\begin{align*}
    \hth=\argmin_{\theta: \nrm{\theta}\leq 1} \nrm{B\theta-a}.
\end{align*}
\end{algorithmic}
\end{algorithm}

\begin{theorem}
Given a linear model, there is a $T$-round interactive algorithm that outputs $(U,\hth)$, such that
\begin{align*}
    L_1(\hth,\ths)\leqsim \frac{d}{\sqrt{T}}.
\end{align*}
Moreover,
\begin{align*}
    \EE_{x\sim p} \nrm{Ux}\leqsim d, \qquad
    \nrm{U^{-1}(\hth-\ths)}\leqsim \sqrt{\frac{d}{T}}.
\end{align*}
\end{theorem}



Given access to a distribution $p$ over $\Bone$, we aim to compute a PSD matrix $U$ such that
\begin{align}
    \EE_{x\sim p}\brac{ \frac{Uxx\tp U}{\nrm{Ux}} }+\lambda_0 U=\id.
\end{align}
When the full distribution $p$ is known, the problem is easy. In the following, we work in the setting where we only have access to noise observation of $x\sim p$. 

\paragraph{Observation}
Note that for any PSD $U$, the function $\phi_U(x)\defeq \frac{U\sq x}{\sqrt{\nrm{Ux}}}$ satisfies $\nrm{\phi_U(x)}\leq 1$ for all $x\in\Bone$. Hence for any matrix $U$, we may make $N$ queries and obtain $V_1,\cdots,V_N$, where for each $i$, $V_i$ is generated as
\begin{align*}
    x_i\sim p, \qquad V_i=\phi_U(x_i)\phi_U(x_i)\tp + \normal{0,\sigma^2}.
\end{align*}
Then, we have with high probability
\begin{align*}
    \nrmop{ \frac{1}{N}\sum_{i=1}^N V_i - \EE_{x\sim p}\brac{ \frac{U\sq xx\tp U\sq}{\nrm{Ux}} } } \leqsim \frac{\poly(d)}{\sqrt{N}}.
\end{align*}

\paragraph{Estimating $U$ from samples}
Consider the following procedure: $U_1=\id$, and for $t=1,2,\cdots,$
\begin{align*}
    H_t=&~\EE\brac{ \frac{U_t\sq xx\tp U_t\sq}{\nrm{U_tx}} }+E_t, \\
    \Sigma_t=&~U_t\sq H_t U_t\sq+\lambda_t U_t, \\
    U_{t+1}=&~\sym(\Sigma_t\isq U_t),
\end{align*}
where $\sym(A)=(A\tp A)\sq$, $E_t$ is a symmetric matrix with guarantee $\nrmop{E_t}\leq \eps$, and $\lambda_t=(2t+c)\eps$.

\begin{proposition}
For any $t\geq 1$, it holds that
\begin{align*}
    \lmin(\Sigma_t) \geq \exp\paren{ -\frac{\log(1/\lambda_1)}{2^{t-1}} }, \qquad \lmax(\Sigma_t)\leq 1+\frac{8\eps}{\lambda_1}.
\end{align*}
In particular, for $t\geq \log_2\log_2(1/\lambda_1)$, we have
\begin{align*}
    \id\preceq 2\EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_tx}} }+2(\lambda_t+\eps)U_t, \qquad
    \EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_tx}} }\preceq \paren{1+\frac{8\eps}{\lambda_1}}\id.
\end{align*}
\end{proposition}

Notice that for each $t\geq 1$, there exists an orthogonal matrix $V_{t+1}$ such that $U_{t+1}=V_{t+1}\Sigma_t\isq U_t$. Therefore,
\begin{align*}
    \id=&~V_{t+1}V_{t+1}\tp
    =V_{t+1} \Sigma_t\isq \paren{ U_t\sq H_t U_t\sq+\lambda_t U_t }\Sigma_t\isq V_{t+1}\tp \\
    =&~\EE\brac{ \frac{V_{t+1}\Sigma_t\isq U_t xx\tp U_t \Sigma_t\isq V_{t+1}\tp}{\nrm{U_tx}} }+V_{t+1} \Sigma_t\isq U_t\sq\paren{ E_t+\lambda_t\id }U_t\sq \Sigma_t\isq V_{t+1}\tp \\
    =&~ \EE\brac{ \frac{U_{t+1} xx\tp U_{t+1}}{\nrm{U_tx}} }+U_{t+1}U_{t}\isq \paren{ E_t+\lambda_t\id }U_t\isq U_{t+1}.
\end{align*}
Therefore, using $-\eps\id\preceq E_t\preceq \eps\id$ (because $\nrmop{E_t}\leq \eps$) and shifting $t\leftarrow t-1$, we have for each $t\geq 2$,
\begin{align*}
    \id-(\lambda_{t-1}+\eps)U_tU_{t-1}^{-1}U_{t} \preceq \EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_{t-1}x}} } \preceq \id-(\lambda_{t-1}-\eps)U_tU_{t-1}^{-1}U_{t}.
\end{align*}
Notice that for any $x\in\R^d$, it holds that $\nrm{U_tx}=\nrm{\Sigma_{t-1}\isq U_{t-1}x}$, we have
\begin{align*}
    \sqrt{ \lmin(\Sigma_{t-1}) }\cdot \EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_{t-1}x}} } \preceq
    \EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_tx}} } \preceq \sqrt{ \lmax(\Sigma_{t-1}) }\cdot \EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_{t-1}x}} }.
\end{align*}
Now, combining the inequalities above and using the definition of $\Sigma_t$, we can lower bound
\begin{align*}
    \Sigma_t=&~\EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_tx}} }+U_t\sq E_t U_t\sq+\lambda_t U_t \\
    \succeq&~ \sqrt{ \lmin(\Sigma_{t-1}) }\cdot \EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_{t-1}x}} } +(\lambda_t-\eps) U_t \\
    \succeq&~ \sqrt{ \lmin(\Sigma_{t-1}) }\paren{ \id-(\lambda_{t-1}+\eps)U_tU_{t-1}^{-1}U_{t} } +(\lambda_t-\eps) U_t.
\end{align*}
Notice that $U_t^2=U_{t-1}\Sigma_{t-1}^{-1}U_{t-1}\preceq \frac{1}{\lmin(\Sigma_{t-1})} U_{t-1}^2$, and the matrix function $U\mapsto -U\isq$ is matrix monotone, and hence we have
\begin{align*}
    \frac{1}{\sqrt{\lmax(\Sigma_{t-1})}} U_t^{-1} \preceq  U_{t-1}^{-1}\preceq \frac{1}{\sqrt{\lmin(\Sigma_{t-1})}} U_t^{-1}.
\end{align*}
In particular, we have proven $\Sigma_t\succeq \sqrt{ \lmin(\Sigma_{t-1}) }\id$. Similarly,
\begin{align*}
    \Sigma_t
    \preceq &~ \sqrt{ \lmax(\Sigma_{t-1}) }\cdot \EE\brac{ \frac{U_t xx\tp U_t}{\nrm{U_{t-1}x}} } +(\lambda_t-\eps) U_t \\
    \preceq &~ \sqrt{ \lmax(\Sigma_{t-1}) }\paren{ \id-(\lambda_{t-1}-\eps)U_tU_{t-1}^{-1}U_{t} } +(\lambda_t+\eps) U_t \\
    \preceq &~ \sqrt{ \lmax(\Sigma_{t-1}) }\id+4\eps U_t.
\end{align*}
Note that $\lmax(U_t)=\nrmop{U_t}\leq \nrmop{\Sigma_{t-1}\isq}\nrmop{U_{t-1}}$, and hence
\begin{align*}
    \lmax(U_t)\leq \prod_{s=1}^{t-1} \frac{1}{\sqrt{\lmin(\Sigma_s)}}
    \leq \paren{ \frac{1}{\lmin(\Sigma_1)} }^{\sum_{s=1}^{t-1}\frac{1}{2^s}}
    \leq \frac{1}{\lmin(\Sigma_1)}\leq \frac{1}{\lambda_1}.
\end{align*}
Therefore, using induction, we can show that $\lmax(\Sigma_t)\leq 1+\frac{8\eps}{\lambda_1}$ always.

\subsection{Learning high-dimensional LCB}

\newcommand{\pa}[1]{\theta^{(#1)}}
\newcommand{\gd}[1]{\mathsf{g}^{(#1)}}
\newcommand{\ogd}[1]{\bar{\mathsf{g}}^{(#1)}}
\newcommand{\xt}{x_t}
\newcommand{\yt}{y_t}
\newcommand{\clip}{\mathsf{clip}}
\newcommand{\nt}{\zeta_t}
\newcommand{\gt}{g_t}
\newcommand{\tgt}{\Tilde{g}_t}
\newcommand{\avgtk}{\frac1N\sum_{t=kN+1}^{(k+1)N}}
\begin{algorithm}[t]
\caption{Locally Private Batched SGD}\label{alg:LR-SGD}
\begin{algorithmic}[1]
\REQUIRE Number of step $K\geq 1$, batch size $N=2K^2$, round $T=KN$.
\STATE Set $R=C\log(K)$ and $\sigma^2=\frac{\sqrt{1.25\log(2/\beta)}}{\alpha}(R+1)$.
\STATE Set $\pa{0}=\mathbf{0}_d$
\FOR{$k=0,\cdots,K-1$}
    \FOR{$t=kN+1,\cdots,(k+1)N$}
        \STATE Send $\pa{k}$ to the $t$-th user
        \CMT{ At $t$-th user: }
        \STATE Observe $(\xt,\yt)\sim p$ and form the gradient estimator
        \begin{align*}
            g_t=\xt\paren{ \clip_R(\lr \pa{k},\xt \rr)-\yt }
        \end{align*}
        \STATE Sample $\nt\sim \normal{0,\sigma^2\id_d}$ and send $\tgt=\gt+\nt$ to the server.    
    \ENDFOR
    \STATE Compute batched gradient estimator $\gd{k}=\avgtk \tgt$.
    \STATE Update
    \begin{align*}
        \pa{k+1}=\pa{k}-\gd{k}
    \end{align*}
\ENDFOR
\ENSURE $\hth=\pa{K}$.
\end{algorithmic}
\end{algorithm}

\newcommand{\err}[1]{\mathsf{err}^{(#1)}}
Denote
\begin{align*}
    \ogd{k}\defeq&~ \EE_{(x,y)\sim p}\brac{ x\paren{ \clip_R(\lr \pa{k},x \rr)-y }}.
\end{align*}
Define
\begin{align*}
    \err{k}_0\defeq \nabla L(\pa{k})-\ogd{k} \qquad
    \err{k}_1\defeq \ogd{k}-\avgtk g^t, \qquad
    \err{k}_2\defeq -\avgtk \nt,
\end{align*}
and then we can decompose the error of the estimator $\gd{k}$ as
\begin{align*}
    \err{k}\defeq \nabla L(\pa{k}) - \gd{k} = \err{k}_0 + \err{k}_1 + \err{k}_2.
\end{align*}

Notice that by definition, we have
\begin{align*}
    \pa{k+1}=\pa{k}-\gd{k}, \qquad
    \nabla L(\pa{k})=\Sigma(\pa{k}-\ths).
\end{align*}
Therefore, recursively using $\gd{k}=\nabla L(\pa{k})-\err{k}$, we have
\begin{align*}
    \pa{k+1}-\ths=(\id-\Sigma)^k(\pa{0}-\ths)+\sum_{i=0}^{k} (\id-\Sigma)^{k-i}\err{i}.
\end{align*}

\begin{lemma}
With probability at least $1-\delta$, for all $k\in[K]$, it holds that
\begin{align*}
    \nrm{ \sum_{i=0}^{k} (\id-\Sigma)^{k-i}\err{i}_1 }\leqsim \sqrt{\frac{k\log(K/\delta)}{N}}
\end{align*}
\end{lemma}
