\subsection{Proof of \cref{lem:Lnew}}
By definition, $\nabla \Lnew(\wstar)=\lambda \mug \cdot U\wstar$, and hence $\nrm{\nabla \Lnew(\wstar)}\leq \mug \lambda$. Then, because $\Lnew$ is $(\mug/2)$-strongly convex over $w\in\cW$, we have
\begin{align*}
    \Lnew(w)\geq \Lnew(\wstar)+\lr \nabla \Lnew(\wstar), w-\wstar\rr + \frac{\mug}{4}\nrm{w-\wstar}^2, \qquad w\in\cW.
\end{align*}
In particular, using $\Lnew(\hwst)\leq \Lnew(\wstar)$, we have
\begin{align*}
    \frac{\mug}{4}\nrm{\hwst-\wstar}^2\leq -\lr \nabla \Lnew(\wstar), \hwst-\wstar\rr\leq \mug\lambda \nrm{\hwst-\wstar},
\end{align*}
and hence $\nrm{\hwst-\wstar}\leq 4\lambda$.

Next, for any vector $v\in\R^d$, we have
\begin{align*}
    \Ex{ \abs{\lr \x, Uv\rr} }^2
    \leq \Ex{ \nrm{U\x} } \cdot \Ex{\frac{\lr \x, Uv\rr^2}{\nrm{U\x}}}.
\end{align*}
Note that $\Ex{\uxxu}\preceq 2\id$, and hence we have $\Ex{ \nrm{U\x} }\leq 2d$ and $\Ex{\frac{\lr \x, Uv\rr^2}{\nrm{U\x}}}\leq 2\nrm{v}^2$. Therefore, it holds that
\begin{align*}
    \Ex{ \abs{\lr \x, Uv\rr} }\leq 2\sqrt{d}\nrm{v}.
\end{align*}
Substituting $v=w-\wstar$ completes the proof.
\qed



\subsection{Proof of \cref{lem:U-JDP-preserve}}\label{appdx:JDP-verify}




We first note that in \cref{alg:U-JDP}, the dataset $\cD$ is split equally as $\cD=\cD\kz\sqcup \cdots \sqcup \cD\epk{K-1}$, and iteration at the $k$th epoch can be regarded as a random function $(U\kk;\cD\kk)\mapsto U\kp$. Therefore, using the composition property of joint DP (\cref{lem:DP-composition}), we only need to verify that $(U\kk;\cD\kk)\mapsto U\kp$ preserves \aJDP~(with respect to $\cD\kk$).

For the data $(\x_t,y_t)$ in the $k$th epoch, the quantity $\Phi_t=\usqx[U\kk][t]$ can be bounded as $\nrmF{\Phi_t}\leq 1$ (\cref{lem:uxxu}). Thus, $H_{(k)}$ defined in \eqref{eq:spectral-approx-JDP} has sensitivity $\Delta=1/N$ under Frobenius norm. Therefore, by the privacy guarantee of Gaussian channels (\cref{def:Guassian-channel}), the mechanism $(U\kk;\dataset\kk)\mapsto \til H\kk$ preserves \aJDP~with respect to $\dataset\kk$. Consequently, by the post-processing property, $(U\kk;\dataset\kk)\mapsto U\kk$ also preserves \aJDP. Therefore, by applying the composition property (\cref{lem:DP-composition}) inductively, we have shown that \cref{alg:JDP-L1-regression} also preserves \aJDP.




\begin{lemma}[Iterative composition of JDP] \label{lem:DP-composition}
Suppose the algorithm $\alg: \cZ^{N_1+N_2}\to \DPi$ outputs $\pi\sim \alg(z_1,\cdots,z_{N_1+N_2})$ generated as
\begin{align*}
    \pi_1\sim \alg_1(z_1,\cdots,z_{N_1}), \quad
    \pi\sim \alg_2(\pi_1; z_{N_1+1},\cdots,z_{N_1+N_2}),
\end{align*}
where the algorithm $\alg_1:\cZ^{N_1}\to \Delta(\Pi_1)$ preserves \aJDP, and $\alg_2(\pi_1;\cdot):\cZ^{N_2}\to \DPi$ preserves \aJDP~for any $\pi_1\in\Pi_1$. Then $\alg$ preservs \aJDP.
\end{lemma}

\subsubsection{Proof of \cref{lem:DP-composition}}

For ease of presentation, we only consider the case both $\Pi$ and $\Pi_1$ are discrete.
For two neighbored dataset $\cD=\{z_i\}_{i=1}^{N_1+N_2}$ and $\cD'=\{z'_i\}_{i=1}^{N_1+N_2}$, denote 
\begin{align*}
    \cD_1=&~  \{z_1,\cdots, z_{N_1}\}, \qquad 
    \cD_2=\{z_{N_1+1},\cdots, z_{N_1+N_2}\}, \\
    \cD_1'=&~  \{z_1',\cdots, z_{N_1}'\}, \qquad 
    \cD_2'=\{z_{N_1+1}',\cdots, z_{N_1+N_2}'\}.
\end{align*}
Assume that $\cD$ and $\cD'$ differ at index $t\in[N_1+N_2]$, i.e., $z_j=z'_j $ for $j\neq t$. We consider two cases.

\paragraph{Case 1: $t\leq N_1$}
Because $\alg_1$ preserves \aJDP, we have
\begin{align*}
    \alg_1(E_1|\cD_1)\leq \ea \alg_1(E_1|\cD_1)+\beta, \qquad \forall E_1\subseteq \Pi_1,
\end{align*}
and hence, equivalently, it holds that
\begin{align*}
    \sum_{\pi_1\in\Pi_1} \brac{ \alg_1(\pi_1|\cD_1)-\ea \alg_1(\pi_1|\cD_1') }_+ \leq \beta.
\end{align*}
Note that for any $E\subseteq \Pi$,
\begin{align*}
    \alg(E|\cD)=\sum_{\pi_1\in\Pi_1} \alg_1(\pi_1|\cD_1) \cdot \alg_2(E|\pi_1;\cD_2),
\end{align*}
and therefore,
\begin{align*}
    \alg(E|\cD)-\ea\alg(E|\cD')=&~\sum_{\pi_1\in\Pi_1} \brac{ \alg_1(\pi_1|\cD_1) - \ea \alg_1(\pi_1|\cD_1') }\cdot \alg_2(E|\pi_1;\cD_2) \\
    \leq&~ \sum_{\pi_1\in\Pi_1} \brac{ \alg_1(\pi_1|\cD_1) - \ea \alg_1(\pi_1|\cD_1') }_+\leq \beta,
\end{align*}
where we use the fact that $\alg_2(E|\pi_1;\cD_2)\in[0,1]$ for any $\pi_1\in\Pi_1, E\subseteq \Pi$.

\paragraph{Case 2: $t>N_1$}
In this case, because $\alg_2(\pi_1;\cdot)$ preserves \aJDP~for any $\pi_1\in\Pi_1$, for any $E\subseteq \Pi$, we have
\begin{align*}
    \alg(E|\cD)=&~\sum_{\pi_1\in\Pi_1} \alg_1(\pi_1|\cD_1) \cdot \alg_2(E|\pi_1;\cD_2) \\
    \leq&~\sum_{\pi_1\in\Pi_1} \alg_1(\pi_1|\cD_1) \cdot \brac{ \ea\alg_2(E|\pi_1;\cD_2')+\beta }\\
    =&~ \ea \alg(E|\cD')+\beta.
\end{align*}

Combining the inequalities above from both cases completes the proof.
\qed




\subsection{Proof of \cref{prop:alg-U-JDP}}\label{appdx:proof-U-JDP}


The following lemma is a standard concentration result (following from taking union bounds with \cref{lem:Gaussian-concen} and \cref{lem:vec-Hoeffding}). 

\begin{lemma}\label{lem:spectral-concen-JDP}
In \cref{alg:U-JDP}, 
\whp, the following inequalities hold simultaneously for all $k=0,\cdots,K-1$:
\begin{align}
    \nrmF{H\kk-\Ep{\usqx[U\kk]}}\leq C_0\sqrt\frac{\log(K/\delta)}{N}, \\
    \nrmop{\til H\kk- H\kk}\leq C_0\frac{\siga \sqrt{d+\log(K/\delta)}}{N}, 
\end{align}
where $C_0$ is a large absolute constant. In the following, we denote this event as $\cE$.
\end{lemma}


Therefore, we can simplify the iterations in \cref{alg:U-JDP} as follows: $U\kz=\id$, and for $k=0,1,\cdots,K$:
\begin{align}
    \til H\kk=&~\Ep{ \usqx[U\kk] }+E\kk, \label{eq:spec-update-H}\\
    \cov\kk=&~U\kk\sq \til H\kk U\kk\sq+\lambda\kk U\kk, \label{eq:spec-update-cov}\\
    U\kp=&~\sym(\cov\kk\isq U\kk), \label{eq:spec-update-U}
\end{align}
where $\sym(A)=(A\tp A)\sq$, $E\kk$ is a symmetric matrix.
We note that \cref{alg:U-JDP} does not actually compute $(H\kc, \cov\kc)$, and they only appear in our analysis (where we can regard $E\kc=0$).

\begin{proposition}\label{prop:spec-converge}
Suppose that the sequence of matrices $\sset{ (U\kk, H\kk, \cov\kk) }$ is defined recursively by \eqref{eq:spec-update-H} - \cref{eq:spec-update-U}, with $\nrmop{E\kk}\leq \eps$. Suppose that $\lambda\kk=(2k+5)\eps$, and $\eps\leq 0.1$. Then, for any $k\geq 1$, it holds that
\begin{align*}
    \lmin(\cov\kk) \geq \exp\paren{ -\frac{\log(1/\eps)}{2^{k}} }, \qquad \lmax(\cov\kk)\leq \exp\paren{ \frac{12}{k} }.
\end{align*}
In particular, $K\geq \log\log(1/\eps)\vee 20$, we have
\begin{align*}
    \frac12\id\preceq \Ep{ \uxxu[U\kc] }+\lambda\kc U\kc \preceq 2\id.
\end{align*}
\end{proposition}

\paragraph{Proof of \cref{prop:alg-U-JDP}}
By \cref{lem:spectral-concen-JDP}, under the event $\cE$, the matrix $E\kk=\til H\kk -\Ep{ \usqx[U\kk] }$ is bounded as $\nrmop{E\kk}\leq \epsN$. 
Therefore, under $\cE$, we can apply \cref{prop:spec-converge}, which gives the desired results.
\qed



\subsubsection{Proof of \cref{lem:spectral-concen-JDP}}
In \cref{alg:U-JDP}, the matrix $\til H\kk$ is given by $\til H\kk=H\kk+Z\kk$,
\begin{align*}
    H\kk=\frac1N\sumkn \Phi_t, \qquad \Phi_t=\usqx[U\kk][t],
\end{align*}
where $Z\kk$ has i.i.d entries $Z_{ij}=Z_{ji}\sim \normal{0,\frac{4\siga^2}{N}}$, and $\Phi_{kN+1},\cdots, \Phi_{(k+1)N}$ are independent (conditional on $U\kk$). Further, by \cref{lem:uxxu}, we have $\nrmF{\Phi_t}\leq 1$, and $\EE[\Phi_t|U\kk]=\Ep{\usqx[U\kk][]}$. Therefore, using \cref{lem:vec-Hoeffding}, we have \whp,
\begin{align*}
    \nrmF{H\kk-\Ep{\usqx[U\kk][]}}\leq \frac{1+\sqrt{2\log(1/\delta)}}{\sqrt{N}}
\end{align*}
By \cref{lem:Gaussian-concen}, we also have $\nrmop{Z\kk}\leq C\frac{\siga\sqrt{d+\log(1/\delta)}}{N}$ \whp. Taking the union bound over $k=0,1,\cdots,K-1$ and rescaling $\delta\leftarrow \frac{\delta}{2K}$ completes the proof.
\qed


\subsubsection{Proof of \cref{prop:spec-converge}}\label{appdx:proof-spec-converge}
By definition \cref{eq:spec-update-U}, for each $k\geq 1$, there exists an orthogonal matrix $V\kp$ such that $U\kp=V\kp\cov\kk\isq U\kk$. Therefore,
\begin{align*}
    \id=&~V\kp V\kp\tp
    =V\kp \cov\kk\isq \paren{ U\kk\sq \til H\kk U\kk\sq+\lambda\kk U\kk }\cov\kk\isq V\kp\tp \\
    =&~\Ep{ \frac{V\kp\cov\kk\isq U\kk \x\x\tp U\kk \cov\kk\isq V\kp\tp}{\nrm{U\kk\x}} }+V\kp \cov\kk\isq U\kk\sq\paren{ E\kk+\lambda\kk\id }U\kk\sq \cov\kk\isq V\kp\tp \\
    =&~ \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} }+U\kp U\kk\isq \paren{ E\kk+\lambda\kk\id }U\kk\isq U\kp.
\end{align*}
Therefore, using $-\eps\id\preceq E\kk\preceq \eps\id$ (because $\nrmop{E\kk}\leq \eps$),
\begin{align*}
    \id-(\lambda\kk+\eps)U\kp U\kk^{-1}U\kp \preceq \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} } \preceq \id-(\lambda\kk-\eps)U\kp U\kk^{-1}U\kp.
\end{align*}
Notice that for any $\x\in\R^d$, it holds that $\nrm{U\kp\x}=\nrm{\cov\kk\isq U\kk\x}$, and hence we have
\begin{align*}
    \sqrt{ \lmin(\cov\kk) }\cdot \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} } \preceq
    \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kp\x}} } \preceq \sqrt{ \lmax(\cov\kk) }\cdot \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} }.
\end{align*}
Now, combining the inequalities above and using the definition of $\cov\kp$, we can lower bound
\begin{align*}
    \cov\kp=&~\Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kp\x}} }+U\kp\sq E\kp U\kp\sq+\lambda\kp U\kp \\
    \succeq&~ \sqrt{ \lmin(\cov\kk) }\cdot \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} } +(\lambda\kp-\eps) U\kp \\
    \succeq&~ \sqrt{ \lmin(\cov\kk) }\paren{ \id-(\lambda\kk+\eps)U\kp U\kk^{-1}U\kp } +(\lambda\kp-\eps) U\kp.
\end{align*}
Notice that $U\kp^2=U\kk\cov\kk^{-1}U\kk\preceq \frac{1}{\lmin(\cov\kk)} U\kk^2$, and the matrix function $U\mapsto -U\isq$ is matrix monotone (cf. \cref{lem:matrix-monotone}), and hence we have
\begin{align}\label{eq:proof-U-iv-k}
    \frac{1}{\sqrt{\lmax(\cov\kk)}} U\kp^{-1} \preceq  U\kk^{-1}\preceq \frac{1}{\sqrt{\lmin(\cov\kk)}} U\kp^{-1}.
\end{align}
In particular, we have proven $\cov\kp\succeq \sqrt{ \lmin(\cov\kk) }\id$, which implies
\begin{align*}
    \lmin(\cov\kk)\geq \lmin(\cov\kz)^{\frac{1}{2^k}}\geq \exp\paren{ -\frac{\log(1/\eps)}{2^k} },
\end{align*}
where we use $\lmin(\cov\kz)\geq \lambda\kz-\eps\geq \eps$.

Similarly,
\begin{align*}
    \cov\kp
    \preceq &~ \sqrt{ \lmax(\cov\kk) }\cdot \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} } +(\lambda\kp+\eps) U\kp \\
    \preceq &~ \sqrt{ \lmax(\cov\kk) }\paren{ \id-(\lambda\kk-\eps)U\kp U\kk^{-1}U\kp } +(\lambda\kp+\eps) U\kp \\
    \preceq &~ \sqrt{ \lmax(\cov\kk) }\id+4\eps U\kp.
\end{align*}
Note that we have also shown that $\cov\kp\succeq (\lambda\kp-\eps)U\kp$, and hence
\begin{align*}
    U\kp\preceq \frac{1}{\lambda\kp-5\eps}\sqrt{ \lmax(\cov\kk) }\id, \qquad
    \cov\kp\preceq \paren{ 1+ \frac{4\eps}{\lambda\kp-5\eps}}\sqrt{ \lmax(\cov\kk) }\id.
\end{align*}
Therefore, it holds that
\begin{align*}
    \log\lmax(\cov\kp)\leq&~ \frac{4\eps}{\lambda\kp-5\eps}+\frac{1}{2}\log\lmax(\cov\kk)\\
    \leq&~ \sum_{j=0}^{k} \frac{1}{2^j}\cdot \frac{4\eps}{\lambda_{k+1-j}-5\eps}+\frac{\log\lmax(\cov\kz)}{2^{k+1}}.
\end{align*}
Note that $\lmax(\cov\kz)\leq 1+\eps+\lambda\kz$, and we also have
\begin{align*}
    \frac{1}{2^{k+1}}+\sum_{j=0}^{k} \frac{1}{2^j}\cdot \frac{1}{k+1-j}\leq \frac{1}{2^{k+1}}+\frac{2}{k+1}+\sum_{j=0}^{k} \frac{1}{2^j}\cdot \frac{j}{(k+1)(k+1-j)} \leq \frac{6}{k+1}.
\end{align*}
Therefore, we have proven that as long as $\eps\in[0,0.1]$,
\begin{align*}
    \log\lmax(\cov\kp)\leq&~ \frac{12}{k+1}.
\end{align*}
This is the desired result.
\qed



\subsubsection{Tighter rate with a different parameter schedule}

In the following, we show that we can in fact choose $\lambda\kk\asymp \frac{\sqrt{d}\siga}{N}$ in \cref{alg:U-JDP}. This result is useful for getting the refined regret bound in \cref{thm:regret-upper-JDP-better} (cf. \cref{appdx:proof-regret-upper-JDP-better}).


Specifically, our analysis is based on the following concentration result. Its proof is essentially the same as the proof of \cref{lem:spectral-concen-JDP}, except that we apply \cref{lem:cov-concen}.

\newcommand{\Hs}{H^\star}
\begin{lemma}\label{lem:U-JDP-concen-Bern}
Suppose that the sequence $\set{(U\kk, \til H\kk)}$ is generated by \cref{alg:U-JDP}. For each $k=0,1,\cdots,K-1$, we define
\begin{align*}
    \Hs\kk\defeq \Ep{\usqx[U\kk]}.
\end{align*}
Then, for any fixed parameter $c>1$, \whp, the following inequality holds for all $k=0,\cdots,K-1$:
\begin{align}\label{eq:U-JDP-concen-Bern}
    c\iv \Hs\kk-\epsN\id\preceq \til H\kk\preceq c\Hs\kk+\epsN\id,
\end{align}
where $\epsN=C_0\paren{\frac{\log(dK/\delta)}{(c-1)N}+\frac{\siga\sqrt{d+\log(K/\delta)}}{N}}$, and $C_0$ is a large absolute constant. In the following, we denote this event as $\cE$ and condition on $\cE$.
\end{lemma}

Therefore, following the analysis from \cref{prop:spec-converge}, we prove the following result.

\begin{proposition}\label{prop:spec-converge-JDP}
Let $c>1$ be a constant. Suppose that \cref{alg:U-JDP} is instantiated with $\lambda\kk=\lambda=\frac{c^2+1}{c^2-1}\epsN$, where $\epsN$ is defined in \cref{lem:U-JDP-concen-Bern}. Then, under the event $\cE$ of \cref{lem:U-JDP-concen-Bern}, for any $k\geq 1$, it holds that
\begin{align*}
    \lmin(\cov\kk) \geq c^{-4}\exp\paren{ -\frac{\log(1/\epsN)}{2^{k}} }, \qquad \lmax(\cov\kk)\leq c^4 \exp\paren{ \frac{\lambda\kz+\epsN}{2^{k}} }.
\end{align*}
In particular, for $c=1.1$, $\epsN\leq 0.1$, $K\geq \max\sset{\log\log(1/\epsN),10}$, we have
\begin{align*}
    \frac12\id\preceq \Ep{ \uxxu[U\kc] }+\lambda U\kc \preceq 2\id.
\end{align*}
\end{proposition}

\paragraph{Proof of \cref{prop:spec-converge-JDP}}
In the following proof, we abbreviate $\eps\defeq \epsN$.
Recall that we can simplify the iterations in \cref{alg:U-JDP} as follows: $U\kz=\id$, and for $k=0,1,\cdots,K-1$:
\begin{align*}
    \cov\kk=&~U\kk\sq \til H\kk U\kk\sq+\lambda\kk U\kk, \qquad
    U\kp=\sym(\cov\kk\isq U\kk),
\end{align*}
and we regard $\til H\kc=\Hs\kc$.

Then, for each $k\geq 1$, there exists an orthogonal matrix $V\kp$ such that $U\kp=V\kp\cov\kk\isq U\kk$, and hence
\begin{align*}
    \id=&~V\kp V\kp\tp
    =V\kp \cov\kk\isq \paren{ U\kk\sq \til H\kk U\kk\sq+\lambda\kk U\kk }\cov\kk\isq V\kp\tp \\
    =&~ U\kp U\kk\isq (\til H\kk+\lambda\kk\id)U\kk\isq U\kp.
\end{align*}
Using \eqref{eq:U-JDP-concen-Bern}, we have
\begin{align*}
    \id\preceq&~ U\kp U\kk\isq (c\Hs\kk+(\lambda\kk+\epsN)\id)U\kk\isq U\kp \\
    =&~c\cdot \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} }+(\lambda\kk+\eps)U\kp U\kk\iv U\kp.
\end{align*}
Using \eqref{eq:U-JDP-concen-Bern} again, we can bound
\begin{align*}
    \cov\kp=&~ U\kp\sq \til H\kp U\kp\sq+\lambda\kp U\kp \\
    \succeq&~ c\iv U\kp\sq \Hs\kp U\kp\sq +(\lambda\kp-\eps) U\kp \\
    =&~ c\iv \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kp\x}} }+(\lambda\kp-\eps) U\kp \\
    \succeq&~ c\iv\sqrt{ \lmin(\cov\kk) }\cdot \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} } +(\lambda\kp-\eps) U\kp \\
    \succeq&~ c^{-2} \sqrt{ \lmin(\cov\kk) }\paren{ \id-(\lambda\kk+\eps)U\kp U\kk^{-1}U\kp } +(\lambda\kp-\eps) U\kp \\
    \succeq&~ c^{-2} \sqrt{ \lmin(\cov\kk) },
\end{align*}
where the second inequality follows from $\nrm{U\kp\x}=\nrm{\cov\kk\isq U\kk\x}\leq \sqrt{\lmax(\cov\kk\isq)}\nrm{U\kk\x}=\frac{1}{\sqrt{\lmin(\cov\kk)}}\nrm{U\kk\x}$, and the last inequality uses \eqref{eq:proof-U-iv-k} and the fact that $\lambda\kp-\eps\geq c^{-2}(\lambda\kk+\eps)$. 

Similarly, we have
\begin{align*}
    \cov\kp
    \preceq&~ c U\kp\sq \Hs\kp U\kp\sq +(\lambda\kp+\eps) U\kp \\
    =&~ c\Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kp\x}} }+(\lambda\kp+\eps) U\kp \\
    \preceq&~ c\sqrt{ \lmax(\cov\kk) }\cdot \Ep{ \frac{U\kp \x\x\tp U\kp}{\nrm{U\kk\x}} } +(\lambda\kp+\eps) U\kp \\
    \preceq&~ c^2 \sqrt{ \lmax(\cov\kk) }\paren{ \id-(\lambda\kk-\eps)U\kp U\kk^{-1}U\kp } +(\lambda\kp+\eps) U\kp \\
    \preceq&~ c^2 \sqrt{ \lmax(\cov\kk) }\id.
\end{align*}
where the last inequality uses \eqref{eq:proof-U-iv-k} and the fact that $\lambda\kp+\eps\leq c^2(\lambda\kk-\eps)$.

Therefore, we have shown
\begin{align*}
    c^{-2} \sqrt{ \lmin(\cov\kk) }\leq \lmin(\cov\kp)\leq \lmax(\cov\kp)\leq c^2 \sqrt{ \lmax(\cov\kk) }.
\end{align*}
Using this inequality recursively, we then have
\begin{align*}
    \lmin(\cov\kk)\geq c^{-4}\lmin(\cov\kz)^{\frac{1}{2^k}}, \qquad
    \lmax(\cov\kk)\leq c^{4}\lmax(\cov\kz)^{\frac{1}{2^k}}.
\end{align*}
The desired conclusion follows by recalling that we regard $\til H\kc=\Hs\kc$ and hence
\begin{align*}
    \cov\kc=\Ep{ \uxxu[U\kc] }+\lambda U\kc.
\end{align*}
\qed
