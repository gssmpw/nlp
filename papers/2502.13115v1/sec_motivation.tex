
We start by reviewing why most existing private algorithms for linear contextual bandits fail to achieve an optimal regret rate without the strong explorability condition $\lmins>0$. Such insufficiency motivates the confidence intervals-based approach of \citet{li2024optimal} discussed in \cref{ssec:L1-motivation}.

\subsection{Insufficiency of standard regression}





In general, the existing algorithmic principles for learning contextual bandits mostly rely (either explicitly or implicitly) on the regression subroutines that, given a sequence of observation $\set{(x_t,a_t,r_t)}_{t\in[N]}$, produce an reward estimation $\fhat$ %
with bounded mean-square error:
\begin{align*}
    \EE_{(x,a)\sim \cD} (\fhat(x,a)-\fs(x,a) )^2\leq \cE(N)^2.
\end{align*}
For non-private linear contextual bandits, it is well-known that regression-based estimators achieve the optimal rate of $\cE(N)^2\asymp \frac{d}{N}$, and such a convergence guarantee of $N^{-1}$-rate is essential in the regret analysis of the classical LinUCB algorithm and its variants ~\citep{abbasi2011improved,li2019nearly,bastani2020online}.
Further, for contextual bandits with a general reward function class,
the recent regression-oracle based algorithms~\citep{simchi2020bypassing, foster2020beyond} achieve regret bounds scaling with $\tbO{T\cdot \cE(T)}$, %
and hence a $T^{-1}$-rate of convergence under $L_2$-error metric is also crucial
to obtain a regret of order $\widetilde{O}(\sqrt{T})$.






\newcommand{\ellg}{\ell_{\link}}
\newcommand{\Errltwo}[1]{\mathsf{Err}_{2}(#1)}





Therefore, for regression-based algorithms, achieving rate-optimal regret essentially relies on the $L_2$-error guarantee of the regression subroutine.
However, it is known that in linear models, privacy leads to slower convergence under the $L_2$-error if the covariate distribution is ill-conditioned, as the following folklore lemma indicates.

\begin{proposition}[Lower bounds for ill-conditioned linear regression]\label{prop:lower-linear-est}
Suppose that $T\geq 1$, $\alpha\in(0,1]$, $\lambda\in[0,1]$, $d=1$, and the link function $\link(t)=t$ is identity. Let the covariate distribution $p\in\Delta([-1,1])$ be known and given by $p(1)=\lambda, p(0)=1-\lambda$. Then

(1) For any $T$-round \JDP~algorithm $\alg$ with output $\hth$, it holds that
\begin{align*}
    \sup_{\ths\in[-1,1]}\EE\sups{\ths,\alg} \brac{ \Epp{\lr \x, \hth-\ths\rr^2} }\geqsim \min\sset{\frac{1}{\lambda (\alpha+\beta)^2T^2},\lambda}.
\end{align*}

(2) For any $T$-round \aLDP~algorithm $\alg$ with output $\hth$, as long as $\beta\leq \frac{\lambda}{T^2}$, it holds that
\begin{align*}
    \sup_{\ths\in[-1,1]}\EE\sups{\ths,\alg} \brac{ \Epp{\lr \x, \hth-\ths\rr^2} } \geqsim \min\sset{\frac{1}{\lambda \alpha^2T},\lambda}.
\end{align*}
\end{proposition}

Note that in the above construction, $\Ex{\x \x\tp}=\lambda$. Hence, in linear contextual bandits, the oracle-based regret bounds described above will scale with $\tbO{\sqrt{T}+\frac{1}{\lmins \alpha}}$ under joint DP model, and $\tbO{\sqrt{T}/(\lmins \alpha)}$ under the local DP model, where $\lmins$ is the minimum eigenvalue over any policy that the algorithm may play. %
Further, if there is not a lower bound on $\lambda>0$, then \cref{prop:lower-linear-est}
provides the worst-case lower bounds of $\Omega\paren{\frac{1}{(\alpha+\beta)T}}$ and $\Omega\paren{\frac{1}{\alpha\sqrt{T}}}$ for $L_2$-regression under the JDP model and LDP model, respectively, implying significant degradation under privacy.

To sum it up, without a new analysis framework, the existing algorithms that only rely on the regression oracles with $L_2$-error guarantee might not avoid the strong explorability condition $\lmins>0$. 
Therefore, to achieve the optimal regret rates for (generalized) linear contextual bandits, we cannot use the standard linear regression primitives to estimate ground-truth parameter $\ths$. 

\subsection{Alternative approach: Regression with confidence intervals}\label{ssec:L1-motivation}


As an alternative to the standard regression based approach~\citep{foster2018practical,foster2020beyond}, \citet{li2024optimal} propose an action elimination framework based on regression with $\Lone$-error guarantee and the additional confidence interval structures. The key observation is that, while the negative results (\cref{prop:lower-linear-est}) do rule out the regression oracles with $O(1/T)$ convergence rate under $L_2$-error, such oracles are \emph{not} necessary for designing algorithm. 
Specifically, \citet{li2024optimal} consider the regression subroutine with confidence intervals (\emph{$\Lone$-regression} for short), which is defined as following:


\begin{definition}[$\Lone$-regression oracle]\label{def:L1-oracle}
Let $N\geq 1$, $\delta\in(0,1)$.
In contextual bandits, a $\Lone$-regression oracle is a $N$-round algorithm $\alg$ that outputs an estimate of reward function $\fhat: \cX\times \cA \to [-1,1]$ and an confidence bound $\CI: \cX\times \cA \to \R_{\geq 0}$, such that for any fixed policy $\pi:\cX\to \Delta(\cA)$, given data $(x_t,a_t,r_t)$ generated independently as
\begin{align*}
\textstyle
    x_t\sim P, \quad
    a_t\sim \pi(\cdot|x_t), \quad
    \EE[r_t|x_t,a_t]=\fs(x_t,a_t), \qquad t\in[N]
\end{align*}
the following holds \whp:

(1) (Valid confidence interval) $\abs{\fhat(x,a)-\fs(x,a)}\leq \CI(x,a)$ for all $(x,a)\in\cX\times\cA$. 

(2) ($\Lone$-performance bound) $\EE_{x\sim P, a\sim \pi(x)} \brac{  \CI(x,a) }\leq \cE_\delta(N)$.
        

\end{definition}

The above conditions on $\Lone$-regression oracle only imply that the \emph{$\Lone$-error} is bounded as $\EE\absn{\fhat(x,a)-\fs(x,a)}\leq \cE(N)$. This is arguably weaker than the mean-square ($L_2$) convergence $\EE(\fhat(x,a)-\fs(x,a))^2\leq \cE(N)^2$ by the common $L_2$-regression. 

With a private $\Lone$-regression oracle, \citet{li2024optimal} adopt an algorithm based on action elimination that achieves a regret of $\tbO{ T \cdot \cE_{1/T}(T) }$ (for details, see also \cref{appdx:regret-meta}). Therefore, the framework opens the door for a $\sqrt{T}$-regret by developing $\Lone$-regression oracle with $\cE_\delta(T)=\tbOn{1/\sqrt{T}}$. 
However, the $\Lone$-regression oracle of \citet{li2024optimal} is based on iterative private PCA and layered private linear regression, achieving the rate $\cE_{\delta}(T) \leq \tbO{
\frac{\log^d(T)}{\alpha\sqrt{T}}}$ and hence leading to a $\log^d(T)\sqrt{T}$-regret that is exponential of the dimension $d$. This regret bound is meaningful only when the dimension $d=\bigO{1}$ is of constant order. 

On the other hand, \citet{chen2024private} provides a significantly improved regret of $\sqrt{d^3T}/\alpha$. While the Decision-Estimation Coefficient (DEC) approach is much different from the aforementioned ones, the way they upper bound the DEC implicitly utilizes the confidence interval bounds with $\Lone$ guarantee. Indeed, for linear regression, \citet{chen2024private} also provide a near-optimal $T^{-1/2}$-rate under $\Lone$-error. For achieving such guarantees, they propose a novel normalization method based on a re-weighting matrix $U$ (detailed discussion in \cref{sec:l1-reg}). However, this method is introduced purely for upper bounding the DEC, and it is unknown whether it provides a more efficient algorithm.

Inspired by the insights from \citet{li2024optimal} and \citet{chen2024private}, in \cref{sec:l1-reg}, we develop an efficient and near-optimal $\Lone$-regression procedure that applies to both joint DP and local DP settings.
Then, in \cref{sec:cb}, we adopt the proposed $L_1$-regression oracle in the action elimination framework of \citet{li2024optimal} to provide rate-optimal private regret bounds.








