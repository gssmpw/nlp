
\subsection{Proof of \cref{thm:improper-JDP}}\label{appdx:improper-JDP}

\newcommand{\cDsub}{\cD_{\mathsf{sub}}}

\paragraph{Privacy guarantee}
To make the presentation clear, we re-write the iteration of \cref{alg:JDP-improper-GD} as follows. For any sub-dataset $\cDsub=\sset{(\x_{t},y_{t})}$, we denote
\begin{align*}
    g(\theta;\cDsub)\defeq&~ \frac1{|\cDsub|} \sum_{(\x,y)\in\cDsub}\x\paren{ \lr \x, \theta\rr-y }, \\
    F(\theta;\cDsub)\defeq&~ \Proj_{\BR}\paren{ \theta-\eta g(\theta;\cDsub) }.
\end{align*}
Then, the iteration of \cref{alg:JDP-improper-GD} can be re-written as follows:
$\theta\kz=\bz$, and for $k=0,1,\cdots,K-1$, 
\begin{align*}
    \theta\kp(\cD\epk{0:k})\defeq F\paren{\theta \kk(\cD\epk{0:k-1});\cD\kk)}.
\end{align*}
Note that for any $\cDsub$,
\begin{align*}
    g(\theta;\cDsub)=\nabla \paren{ \frac{1}{2|\cDsub|}\sum_{(\x,y)\in\cDsub} (\lr \x,\theta\rr-y)^2 }
\end{align*}
is the gradient of a 1-Lipschitz convex function, and hence $\theta\mapsto F(\theta;\cD)$ is a contraction under $\nrm{\cdot}=\nrm{\cdot}_2$:
\begin{align*}
    \nrm{\theta-\theta'}\leq \nrm{F(\theta;\cD)-F(\theta';\cD)}, \qquad \forall \theta,\theta'.
\end{align*}
Further note that, for neighbored sub-dataset $\cDsub$ and $\cDsub'$ of size $N$, we have
\begin{align*}
    \nrm{g(\theta;\cDsub)-g(\theta;\cDsub')}\leq \frac{2(R+1)}{N}, \qquad \forall \nrm{\theta}\leq R,
\end{align*}
and hence 
\begin{align*}
    \nrm{F(\theta;\cDsub)-F(\theta;\cDsub')}\leq \frac{2(R+1)\eta}{N}.
\end{align*}
Therefore, for any neighbored dataset $\cD$ and $\cD'$, using the inequalities above, we have
\begin{align*}
    \nrm{\theta\kc(\cD)-\theta\kc(\cD')}\leq \frac{2(R+1)\eta}{N}.
\end{align*}
This immediately show that \cref{alg:JDP-improper-GD} preserves \aJDP~by the privacy of Gaussian channels (\cref{def:Guassian-channel}).

\newcommand{\og}{\Bar{g}}
\newcommand{\gerr}[1]{\mathsf{err}\epk{#1}}
\paragraph{Convergence guarantee}
We state the following convergence guarantee.
\begin{proposition}\label{prop:improper-JDP}
Let $K,N\geq 2, \delta\in(0,1)$. We denote $B_\delta\defeq 10(R+1)\sqrt{\frac{K\log(K/\delta)}{N}}$. Suppose that the parameters $(\eta, R)$ are chosen so that
\begin{align}\label{eq:unbounded-cond-JDP}
    R\geq 1+B_\delta \eta.
\end{align}
Then it holds that \whp
\begin{align*}
    \nrm{\hth-\ths}_{\bSigma}\leqsim \frac{1}{\sqrt{\eta K}}+\sqrt{\frac{\eta\log K\log(1/\delta)}{ N}}+\frac{\eta (R+1)\siga\sqrt{\log(1/\delta)}}{N}.
\end{align*}
\end{proposition}

In particular, we may choose $R=2$, $\eta=1$, and
\begin{align*}
    K=\min\sset{ c\sqrt{\frac{T}{\log T \log(1/\delta)}} , \paren{\frac{T}{\siga \sqrt{\log(1/\delta)}}}^{2/3}}\vee 1, \qquad
    N=\frac{T}{K}, 
\end{align*}
where $c>0$ is a small absolute constant so that \eqref{eq:unbounded-cond-JDP} holds. Then, it holds that
\begin{align*}
    \nrm{\hth-\ths}_{\bSigma}\leqsim \paren{\frac{\log T\log(1/\delta)}{T}}^{1/4}+\paren{\frac{\siga \sqrt{\log(1/\delta)}}{T}}^{1/3}.
\end{align*}
This completes the proof of \cref{thm:improper-JDP}.
\qed

\subsubsection{Proof of \cref{prop:improper-JDP}}
We denote
\begin{align*}
    \og(\theta)\defeq \Exy{\x(\lr \x,\theta\rr-y)}=\bSigma (\theta-\ths).
\end{align*}
We also write
\begin{align*}
    \gerr{k}\defeq&~ g(\theta\kk;\cD\kk)-\og(\theta\kk), \\
    E\kk\defeq&~ \sum_{i=0}^{k} (\id-\eta\bSigma)^{k-i} \gerr{i}.
\end{align*}

We first invoke the following concentration result.
\begin{lemma}\label{lem:improper-JDP-concen}
The following holds \whp:
For all $k=0,1,\cdots,K-1$:
\begin{align*}
    \nrm{E\kk}\leq 5(R+1)\sqrt{\frac{K\log(K/\delta)}{N}}\defeq B_\delta,
\end{align*}
and
\begin{align*}
    \nrm{E\kc}_{\bSigma}\leq 10(R+1)\sqrt{\frac{(\eta^{-1}\log K+2)\log(2/\delta)}{N}}=:\epsN.
\end{align*}
\end{lemma}

\paragraph{Proof of \cref{lem:improper-JDP-concen}}
By definition,
\begin{align*}
    N\cdot E\kk=&~N\sum_{i=0}^{k} (\id-\eta\bSigma)^{k-i}\gerr{i} \\
    =&~ \sum_{i=0}^{k}\sum_{t=iN+1}^{(i+1)N} (\id-\eta\bSigma)^{k-i}(g_t-\og(\theta\epk{i})),
\end{align*}
where $g_t:=\phi_t(\lr \x_t,\theta_{(i)}\rr-y_t)$ is the gradient at $t \in [iN+1, (i+1)N]$. Note that $\EE[g_t|\theta\epk{i}]=\og(\theta\epk{i})$ for round $t$ in $i$th epoch. Thus, we denote $Z_t\defeq (\id-\eta\bSigma)^{k-i}(g_t-\og(\theta\epk{i}))$ for $t\in[iN+1,(i+1)N]$. Then, the sequence $Z_t$ is a martingale difference sequence with respect to the filtration $\cF_t=\sigma(\sset{(\x_s,y_s)}_{s\leq t})$.
Further, note that $\nrm{Z_t}\leq 2(R+1)$, and hence using \cref{lem:vec-Hoeffding} with a union bound, we have \whp~for all $k\in[K]$,
\begin{align*}
    N\nrm{E\kk}=\nrm{\sum_{t=1}^{(k+1)N} Z_t}\leq 2(R+1)\sqrt{NK}\cdot \paren{1+\sqrt{2\log(K/\delta)}}.
\end{align*}
Further, by \cref{lem:cov-k-converge}, we also have
\begin{align*}
    \nrm{\bSigma\sq Z_t}\leq 2(R+1)\nrmop{\bSigma\sq(\id-\eta\bSigma)^{k-i}}\leq 5(R+1)\min\sset{\frac{1}{\sqrt{\eta(k-i)}},1}.
\end{align*}
Therefore, using \cref{lem:vec-Hoeffding}, we have \whp
\begin{align*}
    N\nrm{E\epk{K-1}}_{\bSigma}=\nrm{\sum_{t=1}^{KN} (\bSigma\sq Z_t)}\leq 5(R+1)\sqrt{N\cdot \paren{\frac{\log K}{\eta}+2}}\cdot \paren{1+\sqrt{2\log(1/\delta)}}.
\end{align*}
Taking the union bound again and rescaling $\delta\leftarrow \frac{\delta}{2}$ completes the proof.
\qed


We denote $\cE_1$ to be the success event of \cref{lem:improper-JDP-concen}. 
In the following, we work under $\cE_1$.

We inductively show that for all $i<K$:
\begin{align}\label{eq:improper-JDP-rec}
    \theta\epk{i+1}=\theta\epk{i}-\eta g\epk{i}.
\end{align}
The base case is trivial: \eqref{eq:improper-JDP-rec} holds for all $i<0$. 

Now, we assume that for some $k<K$, \eqref{eq:improper-JDP-rec} holds for all $i<k$. Then, we only need to prove \eqref{eq:improper-JDP-rec} for the case $i=k$. We denote $\theta\kp^+=\theta\kk-\eta g\kk$. Using $\og(\theta)=\bSigma (\theta-\ths)$ and \eqref{eq:improper-JDP-rec} recursively for $i<k$, we know
\begin{align*}
    \theta\kp^+-\ths=(\id-\eta\bSigma)^{k+1}(\theta\kz-\ths)-\eta \sum_{i=0}^{k} (\id-\eta\bSigma)^{k-i} \gerr{i}.
\end{align*}
Therefore,
\begin{align*}
    \nrm{\theta\kp^+}\leq \nrm{\paren{\id-(\id-\eta\bSigma)^{k+1}}\ths}+\eta \nrm{E\kk}\leq 1+\eta B_\delta\leq R.
\end{align*}
Therefore, $\theta\kp^+\in\BR$, and hence
\begin{align*}
    \theta\kp=\Proj_{\BR}(\theta\kp^+)=\theta\kp^+.
\end{align*}
This completes the proof of case $i=k$. 

Therefore, by induction, \eqref{eq:improper-JDP-rec} indeed holds for all $j<K$. In particular, we have
\begin{align*}
    \theta\kc-\ths=&~(\id-\eta\bSigma)^{K}(\theta\kz-\ths)-\eta \sum_{i=0}^{K-1} (\id-\eta\bSigma)^{K-1-i} \gerr{i} \\
    =&~ (\id-\eta\bSigma)^{K}(\theta\kz-\ths)-\eta E\epk{K-1}.
\end{align*}
Hence, it holds that (by \cref{lem:cov-k-converge} and under $\cE_1$):
\begin{align*}
    \nrm{\theta\kc-\ths}_{\bSigma}\leq \nrm{(\id-\eta\bSigma)^{K}\ths}_{\bSigma}+\eta\nrm{E\epk{K-1}}_{\bSigma}
    \leqsim \frac{1}{\sqrt{\eta K}}+\sqrt{\frac{\eta\log K\log(1/\delta)}{ N}}.
\end{align*}
Finally, we know $\hth=\theta\kc+Z$, where $Z\sim \normal{0,\frac{4(R+1)^2\siga^2}{N^2}}$ is a Gaussian random vector. Therefore, \whp, $\nrm{Z}_{\bSigma}\leqsim \frac{(R+1)\siga\sqrt{\log(1/\delta)}}{N}$. Combining the inequalities above and rescaling $\delta\leftarrow \frac{\delta}{3}$ complete the proof.
\qed

\subsection{Proof of \cref{thm:improper-LDP}}





\newcommand{\sigsp}{\sigma_{N}}



We prove the following convergence rate of \cref{alg:LDP-improper-GD}, under general choice of $(\eta,R)$.
\begin{proposition}\label{prop:unbounded-converge}
Let $K,N\geq 2, \delta\in(0,1)$. We denote $B_\delta\defeq 6(R+1)\sqrt{\frac{K\log(K/\delta)}{N}}$ and $\epsN=(R+1)\sqrt{\frac{K\log(K/\delta)}{N}}$. Suppose that the parameters $(\eta, R)$ are chosen so that
\begin{align}\label{eq:unbounded-cond}
    R\geq 1+\eta\cdot \paren{ B_\delta+4\epsN }.
\end{align}
Then it holds that \whp
\begin{align*}
    \nrm{\theta\kc-\ths}_{\bSigma}\leqsim \frac{1}{\sqrt{\eta K}}+R\eta\siga \sqrt{\frac{K\log(K/\delta)}{N}}.
\end{align*}
\end{proposition}

\paragraph{Proof of \cref{thm:improper-LDP}}
For \cref{alg:LDP-improper-GD}, we choose $\eta=1$, $R=2$, and
\begin{align*}
    K=c\paren{\frac{T}{\siga^2 \log (T/\delta)}}^{1/3}\vee 1, \qquad
    N=\frac{T}{K}, 
\end{align*}
where $c>0$ is an absolute constant so that \eqref{eq:unbounded-cond} holds. Then, by \cref{prop:unbounded-converge}, \cref{alg:JDP-improper-GD} achieves
\begin{align*}
    \nrm{\theta\kc-\ths}_{\bSigma}\leqsim \paren{\frac{\siga^2\log(T/\delta)}{T}}^{1/6}.
\end{align*}
This is the desired upper bound.

\subsubsection{Proof of \cref{prop:unbounded-converge}}

In \cref{alg:LDP-improper-GD}, for epoch $k=0,1,\cdots,K-1$, we have
\begin{align*}
    \til g\kk=\zeta\kk+\avgtk g_t, \qquad
    \theta\kp=\theta\kk-\eta \til g\kk,
\end{align*}
where $\set{\zeta\kz,\cdots,\zeta\epk{K-1}}$ are i.i.d samples from $\normal{0,\sigsp^2}$ with $\sigsp=\frac{(R+1)\siga}{\sqrt{N}}$ and independent of the dataset $\sset{(\x_t,y_t)}_{t\in[T]}$. 

\newcommand{\err}[1]{\mathsf{err}^{(#1)}}
To begin with, we denote $\bSigma\defeq \Ep{\x\x}$ (the covariance matrix), 
\begin{align*}
    \ogd{k}\defeq&~ \EE_{(x,y)\sim p}\brac{ x\paren{ \clip{\lr \pa{k},x \rr}-y }},
\end{align*}
and define the error vectors
\begin{align*}
    \err{k}_0\defeq \nabla \Lsq(\pa{k})-\ogd{k} \qquad
    \err{k}_1\defeq \ogd{k}-\avgtk g^t, \qquad
    \err{k}_2=-\zeta\kk.
\end{align*}
Then, we can decompose the error of the estimator $\gd{k}$ as
\begin{align*}
    \err{k}\defeq \nabla \Lsq(\pa{k}) - \gd{k} = \err{k}_0 + \err{k}_1 +\err{k}_2.
\end{align*}

Notice that by definition, we have
\begin{align*}
    \Lsq(\theta\kk)=\frac12\nrm{\theta\kk-\ths}_{\bSigma}^2, \qquad
    \nabla \Lsq(\pa{k})=\bSigma(\pa{k}-\ths).
\end{align*}
Therefore, recursively using $\gd{k}=\nabla \Lsq(\pa{k})-\err{k}$ and $\pa{k+1}=\pa{k}-\eta \gd{k}$, we have
\begin{align}\label{eqn:proof-unbounded-decomp}
    \pa{k}-\ths=(\id-\eta\bSigma)^k(\pa{0}-\ths)+\eta\sum_{i=0}^{k-1} (\id-\eta\bSigma)^{k-i-1}\err{i}.
\end{align}
We bound the three types of error separately: For each $j\in\set{0,1,2}$, denote
\newcommandx{\Ek}[1][1=k]{E^{(#1)}}
\begin{align*}
    \Ek_j\defeq \sum_{i=0}^{k-1} (\id-\eta\bSigma)^{k-i-1}\err{i}_j.
\end{align*}

\newcommand{\epsNi}[1]{\eps_{N,#1}}





\begin{lemma}\label{lem:unbounded-E1}
\Whp, the following holds:
For all $k\in[K]$, it holds that
\begin{align*}
    \nrm{ \Ek_1 }\leq 6(R+1)\sqrt{\frac{K\log(K/\delta)}{N}}=:B_\delta.
\end{align*}

\end{lemma}

The proof of \cref{lem:unbounded-E1} is essentially similar to that of \cref{lem:improper-JDP-concen}, and hence we omit it for succintness.


\begin{lemma}\label{lem:unbounded-E2}
Denote $\epsN\defeq \sigsp\sqrt{K\log(2K/\delta)}$. \Whp, for all $k=0,1,\cdots,K-1$, it holds that
\begin{align*}
    \Pp{ \absn{\lr \Ek_2, \x\rr}> 3\epsN}\leq \frac{1}{K^6}, \qquad
    \Epp\lr \Ek_2, \x\rr^2\leq 4\epsN^2.
\end{align*}
where $C_2$ is an absolute constant. We denote this success event at $\cE_2$.
\end{lemma}

\paragraph{Proof of \cref{lem:unbounded-E2}}
Fix a $k\in[K]$. Then by definition
\begin{align*}
    \Ek_2=\sum_{i=0}^{k} (\id-\eta\bSigma)^{k-i}\zeta\epk{i},
\end{align*}
where $\zeta\epk{i}\sim \normal{0,\sigsp^2\id}$. Therefore, because $\zeta=(\zeta\kz,\cdots,\zeta\kc)$ is a sequence of independent Gaussian random variables, we have
\begin{align*}
    \Ek_2\sim \normal{0, C_k}, \qquad C_k=\sigsp^2\sum_{i=0}^{k}(\id-\eta\bSigma)^{2(k-i)}\preceq K\sigsp^2 \id.
\end{align*}
Therefore, for any fixed $\x\in\Bone$, $\lr \Ek_2,\x\rr\sim \normal{0,\nrm{\x}^2_{C_k}}$ is a zero-mean Gaussian random variable with variance $\nrm{\x}^2_{C_k}\leq K\sigsp^2$. This immediately implies that
\begin{align*}
    \forall \x\in\Bone, \qquad 
    \EE_\zeta\brac{ \exp\paren{ \frac{\absn{\lr \Ek_2,\x\rr}^2 }{4K\sigsp^2} } }\leq 2,
\end{align*}
where the expectation is taken over the sequence $\zeta=(\zeta\kz,\cdots,\zeta\kc)$ of independent Gaussian random vectors. Therefore, we have
\begin{align*}
    \EE_\zeta\brac{ \Epp{\exp\paren{ \frac{\absn{\lr \Ek_2,\x\rr}^2 }{4K\sigsp^2} }} } \leq 2, \quad\forall k.
\end{align*}
Therefore, by Markov's inequality and taking the union bound, we have
\begin{align*}
    \PP_\zeta\paren{ \forall k\in[K]: \Epp{\exp\paren{ \frac{\absn{\lr \Ek_2,\x\rr}^2 }{4K\sigsp^2} } }\leq \frac{2K}{\delta} }\geq 1-\delta.
\end{align*}
Let this event be $\cE_2$. Then, under $\cE_2$, 
using Markov inequality's again, we have
\begin{align*}
    \Pp{\absn{\lr \Ek_2,\x\rr}\geq 3\sigsp\sqrt{K\log(K/\delta)}}\leq \frac{1}{K^6}, \qquad \forall k.
\end{align*}
Similarly, under $\cE_2$, using Jensen's inequality, we also have
\begin{align*}
    \Epp{\absn{\lr \Ek_2,\x\rr}^2}\leq  4\sigsp^2 K\log(2K/\delta).
\end{align*}
This is the desired result.
\qed


\begin{lemma}\label{lem:unbounded-E0}
Under the success event of \cref{lem:unbounded-E2} and assuming that $K\geq 2$ and
\begin{align*}
    R\geq 1+\eta\cdot \paren{ B_\delta+4\epsN }.
\end{align*}
Then, we have for all $k=0,1,\cdots,K-1$:
\begin{align}\label{lem:unbouned-err-0}
    \nrm{\err{k}_0}\leq \frac{2\eta\epsN}{K^3}.
\end{align}
In particular, it holds that $\nrm{\Ek_0}\leq \frac{2\eta\epsN}{K^2}$.
\end{lemma}

\paragraph{Proof of \cref{lem:unbounded-E2}}
We prove by induction. The base case $k=0$ is trivial because $\err{0}_0=0$.

Now, suppose that \eqref{lem:unbouned-err-0} holds for all $k'\leq k$. In the following, we proceed to prove \eqref{lem:unbouned-err-0} for $k+1$.

By \eqref{eqn:proof-unbounded-decomp}, we have
\begin{align*}
    \pa{k+1}=\paren{\id-(\id-\eta\bSigma)^k}\ths+\eta\paren{\Ek_0+\Ek_1+\Ek_2},
\end{align*}
and under $\cE_1\cap \cE_2$, we have $\nrm{\cE_1}\leq C_1\epsN$, 
\begin{align*}
    \Pp{ \absn{\lr \Ek_2, \x\rr}> 3\epsN }\leq \frac{1}{K^6}.
\end{align*}
By induction hypothesis, 
\begin{align*}
    \nrm{\Ek_0}=\nrm{\sum_{i=0}^{k} (\id-\eta\bSigma)^{k-i}\err{i}_0}
    \leq \sum_{i=0}^{k} \nrm{\err{i}_0}\leq \frac{2\epsN}{K^2}.
\end{align*}
Combining all the equations above, we have
\begin{align*}
    \abs{\lr\x, \pa{k+1}\rr}
    \leq 1+\eta\paren{\frac{2\eta\epsN}{K^2}+B_\delta+\abs{\lr \Ek_2, \x\rr}}.
\end{align*}
By our assumption on $R$, we know that $\abs{\lr\x, \pa{k+1}\rr}-R\leq \eta\paren{\absn{\lr \Ek_2, \x\rr}-3\epsN}$, and hence
\begin{align*}
    \Pp{ \absn{\lr\x, \pa{k+1}\rr} \geq R}
    \leq \Pp{ \absn{\lr \Ek_2, \x\rr}> 3\epsN } \leq \frac{1}{K^6}.
\end{align*}
Finally, by the definition of $\err{k+1}_0$, we have
\begin{align*}
    \err{k+1}_0=\nabla \Lsq(\pa{k+1})-\ogd{k+1}
    =\Ep{ \x\paren{ \lr \pa{k+1},\x \rr-\clip{\lr \pa{k+1},\x \rr} }}.
\end{align*}
Hence,
\begin{align*}
    \nrm{\err{k+1}_0}\leq&~ \Ep{\indic{ \absn{\lr \pa{k+1},\x \rr}>R }\cdot \paren{ \absn{\lr \pa{k+1},\x \rr} - R } } \\
    \leq&~ \sqrt{\Pp{ \absn{\lr\x, \pa{k+1}\rr} \geq R} \cdot \Epp{\paren{\absn{\lr \pa{k+1},\x \rr}-R}_+^2  }} \\
    \leq&~ \sqrt{\frac{\eta^2}{K^6}\Epp{\lr \Ek_2,\x \rr^2 } }
    \leq \frac{2\eta\epsN }{K^3}.
\end{align*}
This completes the proof of the step $k+1$.
\qed

Now, we prove \cref{prop:improper-JDP} by combining the lemmas above. By definition,
\begin{align*}
    \nrm{\theta\kc-\ths}_{\bSigma}\leq \nrm{(\id-\eta\bSigma)^K\ths}+\eta\nrm{\Ek[K]_0}_{\bSigma}+\eta\nrm{\Ek[K]_1}_{\bSigma}+\eta\nrm{\Ek[K]_2}_{\bSigma}.
\end{align*}
Under the success event of \cref{lem:unbounded-E1} and \cref{lem:unbounded-E2}, we have 
\begin{align*}
    \nrm{\Ek[K]_1}_{\bSigma}\leq \nrm{\Ek[K]_1}\leq B_\delta\leqsim \epsN, \qquad
    \nrm{\Ek[K]_2}_{\bSigma}\leq 2\epsN,
\end{align*}
and $\nrm{\Ek[K]_0}\leq \frac{2\eta\epsN}{K^2}$. Further, by \cref{lem:cov-k-converge}, we also have 
\begin{align*}
    \nrm{(\id-\eta\bSigma)^K\ths}\leq \sqrt{\frac{2e}{\eta K}}.
\end{align*}
Combining the inequalities above gives
\begin{align*}
    \nrm{\theta\kc-\ths}_{\bSigma}\leqsim \frac{1}{\sqrt{\eta K}}+\eta \epsN.
\end{align*}
This is the desired upper bound.
\qed

\subsection{Algorithm SquareCB}\label{appdx:square-cb}


\newcommand{\AlgRegression}{\mathsf{Regression}}
\begin{algorithm}
\begin{algorithmic}
\REQUIRE Round $T\geq 1$, epoch schedule $1=T_0<T_1<T_2<\cdots<T_{J}=T$.
\REQUIRE Oracle $\AlgRegression$, parameter $\delta\in(0,1)$.
\STATE Initialize $\hft[0]\equiv 0$.
\FOR{$j=0,1,\cdots,J-1$}
\STATE Initialize the subroutine $\AlgRegression\epj$ with round $N\epj=T\epj[j+1]-T\epj$ and confidence $\delta'=\frac{\delta}{2J^2}$.
\FOR{$t=T_j,\cdots,T_{j+1}-1$}
\STATE Receive context $x_t$.
\STATE Let $\hat{a}_t\defeq \argmax_{a\in\cA} \hft[j](x_t,a)$, and set
\begin{align*}
    p_t(a)\defeq \begin{cases}
        \frac{1}{|\cA|+\gamma\epj\paren{\hft[j](x_t,a_t)-\hft[j](x_t,a)}}, & a\neq \hat{a}_t, \\
        1-\sum_{a\neq \hat{a}_t} p_t(a), & a=\hat{a}_t.
    \end{cases}
\end{align*}
\STATE Take action $a_t\sim p_t$, and receive reward $r_t$.
\STATE Feed $(\phxa[x_t,a_t],r_t)$ into $\AlgRegression\epj$.
\ENDFOR
\STATE Receive estimation $\hft[j+1]$ from $\AlgRegression\epj$.
\ENDFOR
\end{algorithmic}
\caption{$\AlgSQCB$~\citep{foster2020beyond,simchi2020bypassing}}\label{alg:square-cb}
\end{algorithm}

\begin{assumption}\label{asmp:L2-regression}
For any policy $\pi:\cX\to \Delta(\cA)$ and any linear reward function $\fs$, given $N$ independent samples $\sset{(x_t,a_t,r_t)}$ generated as
\begin{align*}
    x_t\sim P,\quad
    a_t\sim \pi(x_t), \quad
    \EE[r_t|x_t,a_t]=\fs(x_t,a_t),
\end{align*}
the regression oracle $\AlgRegression$ (initialized with round $N$ and confidence $\delta$) returns an estimated mean function $\hf:\cX\times\cA\to\R$ such that \whp,
\begin{align*}
    \EE_{x\sim P, a\sim \pi(x)} \paren{\hf(x,a)-\fs(x,a)}^2\leq \cE_{\delta}(N)^2,
\end{align*}
where $\cE_\delta$ is a non-increasing function of $N$. 
\end{assumption}
\begin{theorem}[Guarantee of $\AlgSQCB$]\label{thm:square-cb}
Suppose that \cref{asmp:L2-regression} holds. Then, with parameters
\begin{align*}
    \gamma\epj[0]=1, \qquad \gamma\epj\defeq \frac{\sqrt{|\cA|}}{\cE_{\delta'}(N\epj[j-1])}, \quad j=1,\cdots,J-1,
\end{align*}
$\AlgSQCB$ (\cref{alg:square-cb}) achieves
\begin{align*}
    \Reg\leq C\sqrt{|\cA|}\sum_{j=0}^{J-1} \cE_{\delta'}(N\epj)\cdot N\epj[j+1]+N\epj[0]+T\delta.
\end{align*}
\end{theorem}

Furthermore, the privacy guarantee of \cref{alg:square-cb} can be implied by the privacy guarantee of the regression oracle (similar to \cref{alg:batch-cb-meta}).
\begin{lemma}
If the oracle $\AlgRegression$ preserves \aJDP~(or correspondingly \aLDP), then \cref{alg:square-cb} preserves \aJDP~(or correspondingly \aLDP)
\end{lemma}

\paragraph{Proof of \cref{thm:regret-dim-free} (1)}
For JDP learning, we consider instantiating the regression oracle $\AlgRegression$ with the algorithm $\AlgJDPIGD$ (\cref{alg:JDP-improper-GD}). For the output $\hth$ of $\AlgJDPIGD$ given a dataset of size $N$, we consider the estimated mean function $\hf(x,a)\defeq \lr \hth,\phxa\rr$ for all $(x,a)\in\cX\times\cA$. Then, by \cref{thm:improper-JDP}, \cref{asmp:L2-regression} holds with 
\begin{align*}
    \cE_\delta(N)\leqsim \paren{\frac{\log N \log(1/\delta)}{N}}^{1/4}+\paren{\frac{\siga\log(1/\delta)}{N}}^{1/3}.
\end{align*}
Therefore, we choose $T\epj=2^j$ and $\delta=\frac{1}{T}$, and \cref{thm:square-cb} provides the following regret bound:
\begin{align*}
    \Reg\leq \sqrt{|\cA|}\cdot \tbO{T^{3/4}+\siga^{1/3}T^{2/3}}.
\end{align*}
This is the desired result.
\qed


\paragraph{Proof of \cref{thm:regret-dim-free} (2)}
Similarly, for LDP learning, we consider instantiating the regression oracle $\AlgRegression$ with the algorithm $\AlgLDPIGD$ (\cref{alg:LDP-improper-GD}). Then, by \cref{thm:improper-LDP}, \cref{asmp:L2-regression} holds with 
\begin{align*}
    \cE_\delta(N)\leqsim \paren{\frac{\siga^2\log(N/\delta)}{N}}^{1/6}.
\end{align*}
Therefore, we choose $T\epj=2^j$ and $\delta=\frac{1}{T}$, and \cref{thm:square-cb} provides the following regret bound:
\begin{align*}
    \Reg\leq \sqrt{|\cA|}\cdot \tbO{\siga^{1/3}T^{5/6}}.
\end{align*}
This is the desired regret bound.
\qed
