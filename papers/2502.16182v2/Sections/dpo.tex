\section{IPO: Implicit Preference Optimization}
\label{DPO}
\subsection{Background}


\textbf{Direct Preference Optimization (DPO)} is a reinforcement learning-free framework for aligning large language models (LLMs) with human preferences, eliminating the need for explicit reward modeling. Instead, it directly trains the LLM using human preferences.
Given a dataset of preference pairs \((x, y^w, y^l)\), where \(y^w\) is  preferred over \(y^l\), the model \(\pi_\theta\) is optimized by minimizing the following loss:

\begin{equation}
\begin{aligned}
\mathcal{L}(\theta) = -\mathbb{E}_{(x, y^w, y^l) \sim \mathcal{D}} 
\log \sigma \Bigg( \beta \Big( 
& \log \frac{\pi_\theta(y^w \mid x)}{\pi_\theta(y^l \mid x)} \\
& \hspace{-2cm} - \log \frac{\pi_0(y^w \mid x)}{\pi_0(y^l \mid x)} 
\Big) \Bigg)
\end{aligned}
\end{equation}

Here, \(\pi_\theta\) is the current model, \(\pi_0\) is the initial model, \(\sigma\) is the sigmoid function, and \(\beta\) a scaling factor. This formulation directly aligns \(\pi_\theta\) with the preferences, removing the need for reward-based reinforcement learning.

\textbf{Supervised Fine-Tuning (SFT)} is a crucial step before applying DPO or any other optimization methods. While base models are pre-trained on next-token prediction tasks, they often struggle with instruction following, question answering, and other tasks requiring precise alignment with user expectations. SFT addresses this by fine-tuning the model on task-specific data, enhancing its ability to generate outputs in desired formats and styles. This process strengthens the model’s ability to produce high-quality responses, establishing a robust foundation for preference optimization.

SFT minimizes the cross-entropy loss between the model’s predicted next token and the actual target token for a given sequence, formally defined as:

\begin{equation}
\mathcal{L}_{\text{SFT}}(\theta, \mathcal{D}) = -\mathbb{E}_{(x, y) \sim \mathcal{D}} \left[ \sum_{t=1}^{|y|} \log p_\theta(y_t \mid x, y_{<t}) \right],
\end{equation}

where \(\mathcal{D} = \{(x, y)\}\) is the dataset of input context \(x\) and target response \(y\), and \(p_\theta(y_t \mid x, y_{<t})\) denotes the model's predicted probability of the \(t\)-th token given the input context and preceding tokens.

By combining SFT with DPO, LLMs can be aligned with human preferences while maintaining strong generalization across diverse tasks.
\input{Tables/dpo_llama}


\subsection{Methodology}

\subsubsection{Constructing Preference Dataset}
\label{Pref_Dataset}
Building on an SFT model as the foundation, we generate four diverse responses from the SFT model in case of Llama and the Instruct model in case of Mistral. These samples are then assigned rewards using our method, as described in Section \ref{Preference Modeling}. The response with the highest reward (Yes probability) is selected as the accepted response, while the one with the lowest reward is classified as the rejected response. This process constructs a preference dataset consisting of DPO triplets: \textit{(Prompt, Chosen, Rejected)}, which serves as the training dataset for our model.

\subsection{Experiments}

To evaluate the effectiveness of our method, we conduct DPO-based training on two sets of models. The first is a base model (Llama 3.2 1B), which initially undergoes SFT on the Dolly-15k dataset \citep{DatabricksBlog2023DollyV2}. Once the SFT model is trained, we generate four samples for each prompt. These samples are then rated to form a preference dataset, as described in Section \ref{Pref_Dataset}, in the form of triplets: \textit{(Prompt, Chosen, Rejected)}. We use 4k instructions from the Ultra Feedback dataset \citep{cui2023ultrafeedback} for the input prompts and categorize them into four categories, namely \textbf{chat}, \textbf{code}, \textbf{math}, and \textbf{safety}, using Bart-Zero Shot Classification Pipeline \citep{DBLP:journals/corr/abs-1910-13461,ott2019fairseq}, more details in Apppendix \ref{Prompts_table}. Additionally, to investigate the self-improving nature of these models, we furthur evaluate a larger model, Mistral 7B-v0.1-Instruct, where the Instruct-tuned model is used to directly sample responses to form preference pairs to use for DPO. For all our experiments involving a reward model we utilise the Skywork-Llama-8B Reward model \citep{liu2024skywork}. Exact training details and hardware requirements can be found in Appendix \ref{Training}
% \footnote{https://huggingface.co/facebook/bart-large-mnli}

For a comprehensive evaluation of our methodology, we benchmark it against the Self-Rewarding Models baseline \citep{yuan2024selfrewarding} and the gold-standard reward-based preference pipeline, in which preferences are determined using scores from a reward model. We use a subset of 500 data points from each IFEval \citep{zhou2023instruction}, BBH \citep{suzgun2022challenging}, ArcEasy \citep{clark2018think}, MMLU \citep{hendrycks2020measuring}, Alpaca Eval \citep{dubois2024length} datasets for evaluation. More details regarding the datasets and evaluation strategy are provided in Appendix \ref{Eval Datasets}.  

\subsection{Findings}
From the results, a general trend across both model sizes is that Base models consistently underperform across all benchmarks in a zero-shot setting \citep{kojima2022large}, highlighting their lack of task-specific alignment. 

From the results, we observe that the Self-Rewarding baseline performed poorly across all benchmarks for the smaller model (Llama-1B) and remained suboptimal for larger models (Mistral-7B), though the performance gap was less.

Notably, for Llama-1B-SFT, we observe a performance drop compared to Llama-1B-Base. This can be attributed to the over-memorization of instructions during SFT \citep{zhang2025best,chu2025sft,kirk2023understanding} due to which the model repeats it's responses \citep{hiraoka2024repetition}, which may have negatively impacted generalization.

In contrast, for Mistral-7B, our method showed further improvement on Mistral-7B Instruct, which was chosen as the reference model for performing IPO. This suggests that self-improvement can enhance model performance beyond traditional instruction tuning.

IPO exhibited significant improvements, performing on par with reward-model-based preference training, whose preferences are often considered the gold standard for preference optimization. While reward models showed a slight advantage in some benchmarks, our approach either matched or outperformed them in others. Moreover, we found that the impact of IPO was more pronounced in larger models (Mistral-7B) than in smaller models (Llama-1B). Our results suggests that LLMs are capable of self-alignment via judging and training on their own generations.



%This indicates that larger models benefit more from self-improvement, likely due to their greater capacity to refine outputs and leverage self-generated feedback effectively. 
