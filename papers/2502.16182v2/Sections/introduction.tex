\section{Introduction}

Large Language Models (LLMs) such as GPT4 \citep{openai2024gpt4technicalreport}, Gemini \citep{geminiteam2024gemini15unlockingmultimodal}, and Llama \citep{touvron2023llama2openfoundation} have become highly popular due to their remarkable capabilities. These models often rely on two key techniques: Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling. Reward models are central to both approaches. In RLHF, reward models act as proxies for human values, providing feedback on generated text to align language models during training \cite{christiano2023deepreinforcementlearninghuman,ziegler2020finetuninglanguagemodelshuman}. Similarly, in inference scaling, reward models are used to select the best response from a set of candidates based on predicted rewards \cite{snell2024scalingllmtesttimecompute}.

The training of reward models, however, relies heavily on high-quality, human-generated data, which is both costly and time-intensive. To address this limitation, recent works have explored Reinforcement Learning from AI Feedback (RLAIF) \citep{lee2023rlaif}, where AI-generated feedback is used to train reward models. This approach reduces the dependency on human-annotated data but introduces challenges, including heuristic assumptions that LLMs can consistently provide high-quality feedback and the requirement for larger LLMs to generate such feedback \citep{pang2023language}.

\begin{figure*}[h!]
    \centering
 \includegraphics[width=\textwidth]{assets/approach.pdf}
 \caption{\textbf{Left}: We evaluate preferences using \textit{(Prompt, Chosen, Rejected)} triplets, scoring responses based on the probability of the token "Yes" given classification prompt. The evaluation is correct if the Chosen response scores higher than the Rejected oner. Here [PROMPT] refers to the category specific prompt. \textbf{Right}: Our Self-Improving DPO framework generates diverse responses, rates them, constructs a preference dataset, and trains the model via DPO.}
% \caption{\textbf{Left}: We demonstrate the effectiveness of our approach in preference modeling using a dataset of (Prompt, Chosen, Rejected) triplets. We assess model preferences by inputting both ([PROMPT], Prompt, Chosen) and ([PROMPT], Prompt, Rejected) where [PROMPT] is the category wise prompt that guides the model to output Yes/No for correctness of the response. Then the probability of "Yes" is calculated from the logprobs of the first token as a score. The evaluation is correct if the Chosen response receives a higher score than the Rejected one. We benchmark this on Reward Bench. \textbf{Right}: We propose a Self Improving DPO framework. Given a prompt dataset, we generate four diverse responses, rate them using our method, and construct a preference dataset by selecting the highest-scoring response as Chosen and the lowest as Rejected. This dataset is then used to train the model via DPO.}
    \label{Approach}
\end{figure*}

Self-rewarding large language models  \citep{yuan2024selfrewarding} have emerged as a promising alternative for improving language model performance. In this paradigm, a single model assumes dual roles: as an actor, it generates responses to fulfill specific instructions, and as a judge, it evaluates these responses using the LLM-as-a-Judge framework \citep{zheng2023judging} to assign rewards. However, despite its potential, this approach has a fundamental limitationâ€”the model undergoes fine-tuning to improve its response generation but not its evaluative capabilities. As a result, while it evolves as an actor, its ability to judge remains static.

To address this limitation, Meta-Rewarding Language Models \citep{wu2024meta} extend the model's judging capabilities by explicitly fine-tuning it for judging responses. Additionally, approaches such as Self-Evolving Reward Models \citep{huang2024self} introduce a data-filtering pipeline that leverages high-quality model-generated outputs to refine reward model training. Nevertheless, a significant challenge with these methods lies in their dependence on discrete reward signals or the necessity of external models and datasets, which may introduce inefficiencies or constraints in scalability.

We hypothesize that providing a preference magnitude, rather than discrete prompt based feedback, enables more fine-grained evaluation of model responses. Drawing inspiration from VQA score \citep{lin2025evaluating}, we introduce a probabilistic framework for rewarding LLM-generated responses. This framework empowers even base models to assess and assign rewards to responses, effectively allowing them to function as preference classifiers without relying on external reward models. Compared to existing prompting-based preference strategies, which require large LLMs to act as judges through explicit prompting, our approach is more computationally efficient. It eliminates the need for external supervision or additional training. Specifically, we propose \textit{Implicit Preference Optimization (IPO)}, a novel framework that demonstrates how any LLM can serve as an effective preference classifier.

We conduct extensive experiments across multiple model families, including Qwen, LLaMA, Mistral, and GPT, encompassing various model sizes and configurations (base and instruction-tuned). Additionally, we evaluate our approach on math and code-specific models to analyze their effectiveness as preference classifiers. To rigorously assess our hypothesis of LLM as a preference classifier, we benchmark the ability of LLM to model preferences using RewardBench, a standardized reward model evaluation suite. \textbf{Our findings indicate that LLMs can perform well as preference classifiers, achieving accuracy levels surpassing those of several reward models \citep{lambert2024rewardbenchevaluatingrewardmodels}.} 

Moreover, previous work has highlighted the challenges of training efficient reward models for code and maths-related tasks. Our findings suggest that both general-purpose and code-specific models can inherently function as effective preference classifiers; however, math-specific models lack this ability. To further validate this hypothesis, we examine IPO within a self-improving model setup, where the model generates responses, ranks them based on its own preferences, and leverages these rankings for Direct Preference Optimization (DPO)-based training. Our results demonstrate the effectiveness of IPO in improving response quality.
