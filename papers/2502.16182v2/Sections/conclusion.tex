\section{Conclusion}
We introduced \textbf{IPO}, a simple yet effective framework that utilizes likelihood-based preferences to optimize language models without requiring explicit reward models or expensive human annotations. Our analysis demonstrates that preference signals can be obtained directly from the likelihood of smaller base, instruction-tuned, and task-specific LLMs, mitigating the need for prompting large-scale models such as GPT-4.

Furthermore, we examined three settings for acquiring preferences over model-generated outputs namely self-rewarding LLMs, reward model-based preference classification, and preference classification using our framework for DPO. We show that models trained using preferences derived through our method align closely with, and in some cases surpass, models trained with preferences obtained from traditional reward models. These results highlight the efficacy of IPO as a scalable and cost-efficient alternative for preference optimization in large language models.