\section{LLM as Preference Model}
\label{LLM_as_Preference}
\subsection{Background}

Large Language Models (LLMs) generate text in an autoregressive manner, producing tokens sequentially based on the context of previously generated tokens. Given an input context \( \mathbf{x} \) , the autoregressive model predicts an output sequence \( \mathbf{y} = (y_1, y_2, \dots, y_T) \) one token at a time. Assuming the model is parameterized by \( \theta \), the conditional probability of generating the sequence \( \mathbf{y} \) is defined as:

\begin{equation}
    p_\theta(\mathbf{y} \mid \mathbf{x}) = \prod_{t=1}^T p_\theta(y_t \mid \mathbf{x}, y_{<t}),
\end{equation}

where \( y_{<t} = (y_1, y_2, \dots, y_{t-1}) \). For notational simplicity, \( p_\theta(y_t \mid \mathbf{x}) \) is used to represent \( p_\theta(y_t \mid \mathbf{x}, y_{<t}) \).

The probability distribution over the vocabulary at each time step \( t \) is computed using a softmax function on the logits \( z \) as:

\begin{equation}
    p_\theta(y_t \mid \mathbf{x}) = \frac{\exp(z_t / \tau)}{\sum_{i=1}^M \exp(z_i / \tau)},
\end{equation}

where \( z_t = \text{logit}_\theta(y_t \mid \mathbf{x}, y_{<t}) \), \( M \) is the vocabulary size, and \( \tau > 0 \) is a temperature parameter. 

Various decoding strategies govern token selection during text generation. Greedy decoding selects the highest probability token at each step, while beam search expands multiple candidate sequences in parallel to find the most likely one. Top-k sampling \citep{fan2018hierarchical}, on the other hand, limits token choices to the k most probable candidates, introducing diversity. Many other decoding strategies also exist, each balancing fluency and variability differently.


%%%%%%%%%%%Table%%%%%%%%%%%
\input{Tables/reward_bench}
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Methodology}
Our approach leverages a language model as a preference model, evaluating response appropriateness through binary classification. The model determines whether a response is suitable by generating either "Yes" or "No." To guide this assessment, we employ category-specific prompts, which are detailed in Appendix \ref{Prompts_table}. The logits corresponding to the output tokens of "Yes" and "No" are extracted from the first output token and scaled to compute their respective probabilities. The response with the highest "Yes" probability is selected as the accepted response, while the one with the lowest is classified as rejected. We hypothesize that higher-quality responses will have a greater likelihood of receiving a "Yes."

%\subsubsection{Prompting}

%In our experiments, we observed that guiding the language model to initiate its responses with "Yes" or "No" was essential, particularly for smaller models whose outputs are highly sensitive to prompt phrasing. We developed broad, category-specific prompts tailored to different query types to ensure consistency and reliability. Following prior research \citep{lambert2024rewardbenchevaluatingrewardmodels, liu2024rmbenchbenchmarkingrewardmodels}, we classify prompts into four overarching categories: Code, Math, Chat and Safety. Additional details about prompts are provided in Appendix \ref{Prompts_table}. An example of the prompts is shown in Figure \ref{Sample_Prompts}.

% \begin{itemize}
%   \setlength{\itemsep}{0.05em} % Adjust the space between items
%   \item \textbf{Code:} Questions that involve generating or interpreting code.
%   \item \textbf{Math:} Questions related to mathematical computations or reasoning.
%   \item \textbf{Chat:} General conversational questions, often open-ended or casual.
%   \item \textbf{Safety:} Questions focused on evaluating or ensuring safety in responses.
%   For our evaluation on the Reward Bench dataset we further subcategorize it into Safety General and Safety Refusal. 
% \end{itemize}



\begin{figure}[h]
    \centering
    \includegraphics[width=\columnwidth]{assets/prompt.pdf}
    \caption{Example outputs from Reward Bench using our approach.}
    \label{Sample_Prompts}
\end{figure}



\subsubsection{Preference Classification}
\label{Preference Modeling}

In our experiments, we observed that guiding the language model to initiate its responses with "Yes" or "No" was essential, particularly for smaller models whose outputs are highly sensitive to prompt phrasing. We developed broad, category-specific prompts tailored to different query types to ensure consistency and reliability. Following prior research \citep{lambert2024rewardbenchevaluatingrewardmodels, liu2024rmbenchbenchmarkingrewardmodels}, we classify prompts into four overarching categories: Code, Math, Chat and Safety. Additional details about prompts are provided in Appendix \ref{Prompts_table}. An example prompt is shown in Figure \ref{Sample_Prompts}.

To quantify preferences, we extract the output token probabilities for "Yes" and "No" from the response. The detailed approach is outlined below:

Given an input token sequence \( \mathbf{x} = (x_1, x_2, \dots, x_T) \), a language model \( f(\cdot) \) generates a probability distribution over the vocabulary \( \mathcal{V} \) for the next token. Specifically, the model outputs a logit vector \( \mathbf{z} \in \mathbb{R}^{|\mathcal{V}|} \), where  

\begin{equation}
\mathbf{z} = f(\mathbf{x}).
\end{equation}

To derive probabilities, we apply the softmax function over the logits:  

\begin{equation}
p_i = \frac{\exp(z_i)}{\sum_{j \in \mathcal{V}} \exp(z_j)}, \quad \forall i \in \mathcal{V},
\end{equation}

where \( p_i \) represents the probability assigned to token \( i \). Thus we define probability of "Yes" token as $p_{\text{yes}}$ and "No" token as $p_{\text{no}}$. Then we normalize the probabilities to ensure a fair comparison:

% We define the sets of token indices corresponding to "Yes" and "No" as \( V_{\text{yes}} \subset \mathcal{V} \) and \( V_{\text{no}} \subset \mathcal{V} \), respectively. The cumulative probabilities of these responses are computed as:  

% \begin{equation}
% p_{\text{yes}} = \sum_{i \in V_{\text{yes}}} p_i, \quad p_{\text{no}} = \sum_{i \in V_{\text{no}}} p_i.
% \end{equation}


\begin{equation}
p_{\text{yes}}' = \frac{p_{\text{yes}}}{p_{\text{yes}} + p_{\text{no}}}, \quad 
p_{\text{no}}' = \frac{p_{\text{no}}}{p_{\text{yes}} + p_{\text{no}}}.
\end{equation}

The final values \( (p_{\text{yes}}', p_{\text{no}}') \) represent the normalized likelihoods of the model predicting "Yes" or "No" .

\subsection{Experiments}
\subsubsection{Benchmarking Our Approach}

To evaluate our approach, we conducted experiments using LLMs of varying sizes and architectures. We compared instruction-tuned models with their base counterparts. Additionally, we analyzed the effect of fine-tuning on a specialized task like code/math problems on preference classification by including models fine-tuned for these tasks. For comparisons involving a reward model we use the Skywork Reward Llama 8B model \cite{liu2024skywork} as the baseline.
The detailed results for all the comparisons are available in Appendix \ref{All Results}.

In particular, we tested the following models:

\begin{itemize}
  \setlength{\itemsep}{0.05em} % Adjust the space between items
  \item \textbf{LLaMA Family \citep{dubey2024llama}:} LLaMA-3.2-1B, LLaMA-3.2-1B-Instruct, LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, Meta LLaMA 3-8B, Meta LLaMA 3-8B-Instruct.
  \item \textbf{Mistral Family \citep{jiang2023mistral}:} Mistral 7B, Mistral 7B-Instruct.
  \item \textbf{Qwen Family \citep{yang2024qwen2}:} Qwen2.5-3B, Qwen2.5-3B-Instruct, Qwen2.5-7B, Qwen2.5-7B-Instruct.
  \item \textbf{Code Generation Models:} Starcoder2-7B \citep{lozhkov2024starcoder}, CodeGemma-7B-It \citep{team2024codegemma}, Qwen-Coder-7B-Inst \citep{hui2024qwen2}, Qwen-Coder-3B-Inst.
  \item \textbf{Math Generation Models:} Qwen-Math-7B-Inst, Qwen-Math-1.5B-Instruct \citep{yang2024qwen2}, Deepseek-Math-7B \citep{shao2024deepseekmathpushinglimitsmathematical}, Llemma-7B \citep{azerbayev2024llemmaopenlanguagemodel}.
  \item \textbf{Other Models:} Phi-3-mini-128k-Instruct \citep{abdin2024phi}, Gemma 2B-Instruct \citep{team2024gemma}, GPT-4o Mini \citep{openai2024gpt4technicalreport}.

\end{itemize}

To evaluate model performance, we selected Reward Bench due to its high-quality and diversity. Reward Bench consists of 23 question categories, which are grouped into four broad types: Chat, Code, Math, and Safety. We also benchmark our approach on RM-Bench, results of which can be found in Table \ref{tab:rm_bench_levels}.

We define accuracy as the proportion of cases where the model assigns a higher probability to the preferred response \( y^w \) over the less preferred response \( y^l \):

\[
\text{Acc} = \frac{1}{N} \sum_{i=1}^{N} \mathbb{I} \left[ p_{\text{yes}}(x_i, y^w_i) > p_{\text{yes}}(x_i, y^l_i) \right]
\]

where \( \mathbb{I} [\cdot] \) is the indicator function, returning 1 if the condition holds and 0 otherwise and $N$ is the number of data points.

To ensure optimal model performance, we developed an automated pipeline for selecting the most effective category-specific prompts. Further details on prompt selection can be found in Appendix \ref{Prompts_table}.

\subsubsection{Comparision against Self Rewarding Approach}
We benchmarked our approach against the preference classification approach used in the Self-Rewarding Language Model\footnote{The Self-Rewarding approach performs very poorly on Base Models, so we tested their method on only Instruct models.}. Their approach involves scoring responses using a numerical reward of up to 5 \citep{yuan2024selfrewarding,li2024selfalignment}. Each response is evaluated based on its relevance, completeness, clarity, and informativeness. The comparitive results are shown in Table \ref{tab:reward_bench}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure*}[h]
    \centering
    \includegraphics[width=\textwidth]{assets/math_code.pdf}
    \caption{\textbf{Left}: Our approach on Code Specific Model where the dashed line is a reward model. \textbf{Right}: Our approach on 4 different math-specific models where the striped bar is the reward model.}
    \label{Code results}
\end{figure*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Findings}
Our approach demonstrated robust and consistent performance across all subcategories of the Reward Bench, particularly when compared to the self-rewarding approach. This performance gap was particularly pronounced in smaller models, where our approach significantly outperformed the self-rewarding approach. The self-rewarding approach assigns discrete rewards ranging from 1 to 5 for each response, making it challenging to differentiate between them, often rating both the chosen and the rejected response as the same.

Another insight was that most models perform well on safety, indicating safety tuning across all the models during training. Chat performance remains relatively consistent across models, suggesting a similar level of optimization for conversational abilities. However, performance on code and math varies significantly, largely depending on the type of training data used \citep{gunasekar2023textbooks,petty2024does,aryabumi2024code}. For example, the Qwen family excels in coding tasks, while Llama 3.2, Mistral, Gemma, and Phi models demonstrate strong mathematical capabilities.

Another finding was that larger models consistently outperformed smaller models, as shown in Table \ref{tab:reward_bench} and that instruction-tuned models consistently outperformed their base counterparts, reinforcing the effectiveness of instruction-based fine-tuning even in acting as preference classifiers. Additional results of our approach on RM-Bench can be found in \ref{All Results}.

On proprietary models, such as GPT, our approach remained competitive. Results using our approach on GPT-4o-Mini on Reward Bench can be found in Appendix \ref{GPT_Results}. 

\subsection{Performance of Math and Code Specific Models}

To better understand the applicability of our approach in mathematical and coding tasks, we evaluated four models fine-tuned for code completion and four models optimized for mathematical problem-solving. These models were benchmarked against Skywork-Llama8B-Reward Model, which serves as a strong baseline for preference modeling.

Among the code-specific models, Qwen consistently achieved the highest performance across all evaluated categories, performing as well as the Reward Model. 

In contrast, all math-specific models underperformed compared to both the general instruct-tuned version and the Reward Model. We hypothesize that this underperformance stems from the training objective of math-specific models, which prioritize generating chain-of-thought reasoning \citep{yang2024qwen2,shao2024deepseekmathpushinglimitsmathematical,gao2024designing,zhou2024dual} rather than adhering to strict instruction-following behavior required for binary Yes/No classification.