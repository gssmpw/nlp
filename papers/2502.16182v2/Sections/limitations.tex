\section{Limitations}
Our approach relies on the pre-categorization of the dataset. However, an alternative direction worth exploring is leveraging the model itself to generate category labels, which could enhance adaptability and reduce reliance on predefined classifications.
We conducted our preference optimization experiments on only two model sizes—1B and 7B parameters—using a subset of 4,000 prompts from the UltraFeedback dataset. Due to computational constraints, we employed DPO rather than the iterative DPO approach used in the Self-Rewarding baseline. Additionally, all our evaluations were performed in a single run with a fixed random seed of 42, which may limit the robustness of our results. Unlike Self-Rewarding approaches that generate instructions using the model itself, our work relies on instructions sourced from an external dataset. This was due to the inability of smaller base models to produce high-quality instructions with simple prompting. Furthermore, we also do not test our hypothesis on LLMs where they are asked to pick the better of the two responses due to the high amount of positional bias present in them \citep{zheng2023large,li-etal-2024-split}. 