\section{Background and Related Work}

\subsection{Reinforcement Learning for Improving LLMs}

Recent approaches for improving LLMs involve training a fixed reward model using human preference data, which is subsequently utilized for Reinforcement Learning (RL) to train language models. This method, commonly referred to as Reinforcement Learning from Human Feedback (RLHF) \citep{liu2020learning,ouyang2022training}, has significantly enhanced the performance of models like Llama\citep{touvron2023llama2openfoundation,dubey2024llama} and ChatGPT\citep{openai2024gpt4technicalreport}.

An alternative paradigm to traditional RLHF are methods like Direct Preference Optimization (DPO) \cite{rafailov2024directpreferenceoptimizationlanguage}, which bypasses the need for training a reward model altogether. Instead, it directly trains the LLM based on human preference data. Beyond RLHF and DPO, additional techniques such as Kahneman \& Tverskyâ€™s Optimization (KTO) \citep{ethayarajh2024ktomodelalignmentprospect}, Sequence Likelihood Calibration (SLiC) \citep{zhao2023slichfsequencelikelihoodcalibration}, Reinforced Self-Training (ReST) \citep{gulcehre2023reinforcedselftrainingrestlanguage}, and Rank Responses with Human Feedback (RRHF) \citep{yuan2023rrhfrankresponsesalign} have been proposed, each leveraging human preferences to optimize LLM training.

Constitutional AI \citep{bai2022constitutional} uses an LLM to provide feedback to refine responses. The feedback is then used to further train the language model through Reinforcement Learning from AI Feedback (RLAIF) \citep{lee2023rlaif}. Similarly, Self-Play fIne-tuNing (SPIN) \citep{chen2024selfplayfinetuningconvertsweak} introduces an Interactive DPO-like framework, designed to eliminate the need for reward model training and to simplify reliance on human-labeled data pairs.

\subsection{Self Improving Models}

Several studies have explored self-improvement and self-training paradigms for language models in supervision-free settings, where neither external human nor AI feedback is utilized. Works such as LMSI \citep{huang2022largelanguagemodelsselfimprove, huang2024selfimprovementlanguagemodelssharpening} investigate techniques that enable language models to autonomously enhance their own performance without relying on explicit annotations or reward signals.

The concept of \textit{LLM-as-a-Judge} \citep{gu2024surveyllmasajudge,ye2024beyond,dong-etal-2024-llm,li2024dissecting} has also been extensively studied, where various methods have been proposed to design self-rewarding reward functions, denoted as $r_{self}$, using carefully crafted prompting strategies. These approaches aim to enable language models to evaluate their own outputs effectively, thereby facilitating self-refinement.

In addition to these works, ResT-MCTS\textsuperscript{*} \citep{zhang2024rest} and SPPO \citep{wu2024self} have explored algorithms based on self-training and self-play, where models iteratively improve their own performance through interaction with generated data. While these methods emphasize self-guidance, many incorporate external feedback mechanisms, such as Supervised Fine-Tuning (SFT) or reward-based optimization, to further refine the training process \cite{ouyang2022training}. 

\subsection{Evaluation of Reward Models}

Evaluating reward models plays a crucial role in aligning large language models (LLMs) with human preferences. Various works, such as AlpacaFarm \citep{dubois2024alpacafarmsimulationframeworkmethods}, evaluate preference models by comparing model-generated outputs with those from a reference model. Similarly, ChatbotArena \citep{chiang2024chatbotarenaopenplatform} determines preferences between two model-generated outputs. These methods, however, focus on indirectly evaluating reward models rather than conducting direct evaluations.

Recent benchmarks, such as RewardBench \citep{lambert2024rewardbenchevaluatingrewardmodels} and RM-Bench \citep{liu2024rmbenchbenchmarkingrewardmodels}, address this gap by creating category-wise, high-quality binary datasets to model and evaluate reward model performance. Given the robustness and high quality of these datasets, we use them to test our hypothesis.

%Following the methodology outlined in RewardBench and RM-Bench, we evaluate reward models by framing the task as a classification problem. 
%Given a tuple \((x, y^w, y^l)\), where \(x\) represents the prompt, \(y^w\) is the preferred response, and \(y^l\) is the rejected response, the reward model predicts whether \(y^w\) is superior to \(y^l\). The model's accuracy is determined by the proportion of instances where it assigns a higher reward to \(y^w\) than to \(y^l\), formally defined as:

%\begin{equation}
%\text{Acc} = \frac{1}{|\mathcal{D}|} \sum_{(x, y^w, y^l) \in \mathcal{D}} \mathbb{I}\left[R_\psi(x, y^w) > R_\psi(x, y^l)\right]
%\end{equation}

%where $\mathbb{I}(\cdot)$ denotes the indicator function, and $\mathcal{D}$ represents the evaluation dataset.




