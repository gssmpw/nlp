\section{Results on GPT}
\label{GPT_Results}
We also evaluated our approach on proprietary models like GPT-4o-Mini and found that it significantly outperformed both the Self-Rewarding approach and the Binary Approach. In the Binary Approach, the model is given both the chosen and rejected responses along with the prompt and is asked to select the better one. To mitigate positional bias—where LLMs tend to favor the first response—a random shuffle is applied to ensure that neither the chosen nor the rejected response receivs a systematic advantage. The results for Binary Eval were taken directely from Reward Bench\footnote{\url{https://huggingface.co/spaces/allenai/reward-bench}}.  The results for the same are shown in Table \ref{accuracy_gpt}.