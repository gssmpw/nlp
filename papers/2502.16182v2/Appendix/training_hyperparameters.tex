\section{Implementation and Hardware Details}
\input{Tables/gpt}

\label{Training}
We conducted all training procedures using QLoRA with bfloat16 precision for DPO-based training and full fine-tuning for SFT. Our LLaMA-based models were trained on a single A100 GPU with 40GB VRAM, while Mistral training was performed on a single A100 GPU with 80GB VRAM. Inferences presented in Section \ref{LLM_as_Preference} were carried out using T4 GPUs with float16 precision, whereas evaluation results in Section \ref{DPO} were obtained using A10 GPUs with bfloat16 precision.
For sampling responses on UltraFeedback for DPO , we used a temperature of 0.7 and a top\_k value of 40.

\begin{table}[H]
    \centering
    \begin{tabular}{l|c}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Number of Training Epochs & 3 \\
        Train Batch Size & 4 \\
        Learning Rate & $5 \times 10^{-4}$ \\
        % Weight Decay & 0.001 \\
        Optimizer & AdamW \\
        Learning Rate Scheduler & Cosine \\
        \bottomrule
    \end{tabular}
    \caption{Training Hyperparameters for SFT Training}
    \label{hyperparameters_sft}
\end{table}

%Table for SFT and DPO hyperparameters, GPU used etc. 
\begin{table}[H]
    \centering
    \begin{tabular}{l|c}
        \toprule
        \textbf{Hyperparameter} & \textbf{Value} \\
        \midrule
        Number of Training Epochs & 3 \\
        Train Batch Size & 6 \\
        Learning Rate &  $5 \times 10^{-4}$\\
        Optimizer & AdamW \\
        Learning Rate Scheduler & Cosine \\
        DPO Beta & 0.1 \\
        LoRA Alpha & 128 \\
        LoRA Dropout & 0.05 \\
        LoRA Rank (r) & 256 \\
        % LR Scheduler Type & \texttt{"cosine"} \\
        
        \bottomrule
    \end{tabular}
    \caption{Training Hyperparameters for DPO Training}
    \label{hyperparameters_dpo}
\end{table}