\section{Evaluation Dataset and Strategy}
\label{Eval Datasets}

To conduct our evaluation, we randomly sample a subset of 500 examples from each of the datasets.

\begin{itemize}
    \setlength{\itemsep}{0.05em} 
    \item \textbf{IFEval (Instruction-Following Evaluation)\footnote{\url{https://huggingface.co/datasets/google/IFEval}}:} Assesses the ability of large language models to follow explicit, verifiable instructions, such as \textit{``write in more than 400 words''} or \textit{``mention the keyword `AI' at least three times.''}
    
    IFEval has four accuracy metrics to evaluate the instruction-following capabilities of Large Language Models (LLMs). Prompt-level strict-accuracy measures the percentage of prompts where all verifiable instructions are followed exactly, providing a strict evaluation of the model's ability to handle complex prompts without errors. Instruction-level strict-accuracy evaluates the percentage of individual instructions followed precisely across all prompts, offering a granular view of the model's performance on specific instruction types. Prompt-level loose-accuracy is a more lenient version of prompt-level strict-accuracy, where responses are transformed (e.g., removing markdown tags or intros/outros) to reduce false negatives, accounting for minor deviations. Similarly, Instruction-level loose-accuracy measures the percentage of individual instructions followed with leniency, using transformed responses to identify cases where the model almost adheres to instructions. The final metric is the average of all the four accuracies. Each category specific result of IFEval are shown in Table \ref{if_eval}

    \item \textbf{MMLU (Massive Multitask Language Understanding)\footnote{\url{https://huggingface.co/datasets/cais/mmlu}}:} Evaluates models across 57 subjects using multiple-choice questions, covering disciplines such as humanities, STEM, and social sciences, to measure broad knowledge and reasoning capabilities.
    \item \textbf{BBH (BIG-Bench Hard)\footnote{\url{https://huggingface.co/datasets/lukaemon/bbh}}:} BigBench Hard dataset, focuses on complex problem-solving areas such as multistep arithmetic, algorithmic reasoning, and advanced language comprehension.
    \item \textbf{ARC-Easy (AI2 Reasoning Challenge - Easy)\footnote{\url{https://huggingface.co/datasets/allenai/ai2_arc}}:} Comprises grade-school-level, multiple-choice science questions designed to assess fundamental reasoning and knowledge.
    \item \textbf{Alpaca-Eval\footnote{\url{https://huggingface.co/datasets/tatsu-lab/alpaca_eval}}:} A benchmark that compares model-generated responses against given responses, employing GPT as an evaluator to determine output quality. 
\end{itemize}

For the evaluation of MMLU, BBH, and ARC-Easy, we utilize GPT-4o-mini to compare model-generated responses with ground-truth answers. For IFEval, we employ the official evaluation code. Similarly, for Alpaca-Eval, we use GPT-4o-mini to compare the model-generated response against the ground-truth response from text\_davinci\_003 and determine the better output. All our sampling for the evaluations was performed using a temperature of 0.5 and top\_k value of 40.

\input{Tables/IFEval}
