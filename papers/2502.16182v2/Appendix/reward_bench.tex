% \begin{table*}[h!]
% \centering
% \scriptsize
% \renewcommand{\arraystretch}{1.5}
% \setlength{\tabcolsep}{3pt}
% \definecolor{apricot}{rgb}{0.95, 0.82, 0.62}
% \definecolor{lightgray}{rgb}{0.96, 0.96, 0.96}

% \begin{tabular}{l | cccccccccccc}
% \hline
% \rowcolor{apricot} 
% \textbf{Dataset} & 
% \shortstack{\textbf{Llama} \\ \textbf{3.2-1B}} & 
% \shortstack{\textbf{Llama} \\ \textbf{3.2-1B}\\\textbf{Instruct}} & 
% \shortstack{\textbf{Llama} \\ \textbf{3.2-3B}} & 
% \shortstack{\textbf{Llama} \\ \textbf{3.2-3B}\\\textbf{Instruct}} & 
% \shortstack{\textbf{Meta} \\ \textbf{Llama-3-8B}} & 
% \shortstack{\textbf{Meta} \\ \textbf{Llama-3-8B}\\\textbf{Instruct}} & 
% \shortstack{\textbf{Mistral} \\ \textbf{7B-v0.1}} & 
% \shortstack{\textbf{Mistral} \\ \textbf{7B}\\\textbf{Instruct-v0.1}} & 
% \shortstack{\textbf{Qwen} \\ \textbf{2.5-3B}} & 
% \shortstack{\textbf{Qwen} \\ \textbf{2.5-3B}\\\textbf{Instruct}} & 
% \shortstack{\textbf{Qwen} \\ \textbf{2.5-7B}} & 
% \shortstack{\textbf{Qwen} \\ \textbf{2.5-7B}\\\textbf{Instruct}} 
% \\ \hline
% hep-cpp & 54.88 & 49.39 & 68.29 & 65.24 & 57.32 & 74.39 & 70.12 & 75.0 & 82.93 & 76.22 & 84.76 & 78.05 \\ \hline
% \rowcolor{lightgray} math-prm & 23.49 & 88.14 & 98.21 & 98.21 & 77.18 & 44.97 & 97.99 & 96.2 & 24.61 & 46.31 & 68.46 & 56.24 \\ \hline
% llmbar-adver-GPTInst & 63.04 & 64.13 & 44.57 & 53.26 & 59.78 & 71.74 & 71.74 & 75.0 & 51.09 & 83.7 & 59.78 & 78.26 \\ \hline
% \rowcolor{lightgray} refusals-dangerous & 76.0 & 94.0 & 22.0 & 72.0 & 25.0 & 91.0 & 45.0 & 86.0 & 72.0 & 78.0 & 74.0 & 96.0 \\ \hline
% hep-python & 50.61 & 52.44 & 61.59 & 71.34 & 53.66 & 77.44 & 67.07 & 76.22 & 77.44 & 78.66 & 89.02 & 89.02 \\ \hline
% \rowcolor{lightgray} alpacaeval-easy & 34.41 & 83.23 & 34.66 & 53.79 & 56.15 & 24.22 & 20.0 & 42.36 & 36.4 & 27.33 & 46.71 & 80.25 \\ \hline
% hep-java & 54.88 & 55.49 & 58.54 & 67.68 & 49.39 & 78.05 & 74.39 & 68.29 & 85.37 & 86.59 & 88.41 & 84.15 \\ \hline
% \rowcolor{lightgray} llmbar-adver-GPTOut & 55.32 & 46.81 & 36.17 & 44.68 & 29.79 & 44.68 & 46.81 & 53.19 & 53.19 & 48.94 & 53.19 & 59.57 \\ \hline
% alpacaeval-hard & 49.69 & 88.2 & 55.16 & 70.43 & 50.81 & 40.62 & 27.7 & 63.23 & 38.63 & 33.66 & 50.06 & 88.45 \\ \hline
% \rowcolor{lightgray} hep-go & 49.39 & 45.73 & 53.66 & 64.63 & 57.93 & 73.78 & 70.73 & 73.78 & 81.1 & 82.93 & 83.54 & 85.98 \\ \hline
% refusals-offensive & 73.0 & 97.0 & 49.0 & 97.0 & 86.0 & 99.0 & 45.0 & 97.0 & 23.0 & 94.0 & 98.0 & 100.0 \\ \hline
% \rowcolor{lightgray} xstest-should-refuse & 56.49 & 77.92 & 67.53 & 92.21 & 82.47 & 98.7 & 46.75 & 92.21 & 54.55 & 93.51 & 84.42 & 94.16 \\ \hline
% donotanswer & 38.24 & 55.88 & 64.71 & 82.35 & 69.12 & 91.91 & 63.97 & 80.88 & 62.5 & 91.18 & 78.68 & 90.44 \\ \hline
% \rowcolor{lightgray} mt-bench-hard & 51.11 & 60.0 & 64.44 & 64.44 & 48.89 & 48.89 & 48.89 & 53.33 & 55.56 & 55.56 & 64.44 & 66.67 \\ \hline
% llmbar-adver-neighbor & 64.18 & 58.96 & 50.0 & 60.45 & 59.7 & 74.63 & 45.52 & 59.7 & 44.03 & 72.39 & 60.45 & 73.88 \\ \hline
% \rowcolor{lightgray} mt-bench-easy & 60.71 & 60.71 & 60.71 & 78.57 & 50.0 & 89.29 & 60.71 & 75.0 & 60.71 & 78.57 & 89.29 & 100.0 \\ \hline
% llmbar-adver-manual & 65.22 & 52.17 & 52.17 & 47.83 & 47.83 & 63.04 & 45.65 & 52.17 & 41.3 & 56.52 & 45.65 & 60.87 \\ \hline
% \rowcolor{lightgray} mt-bench-med & 37.78 & 64.44 & 64.44 & 71.11 & 51.11 & 60.0 & 57.78 & 60.0 & 55.56 & 73.33 & 71.11 & 93.33 \\ \hline
% xstest-should-respond & 53.6 & 77.6 & 57.6 & 57.6 & 50.0 & 58.8 & 72.4 & 63.2 & 84.4 & 73.6 & 90.4 & 85.6 \\ \hline
% \rowcolor{lightgray} hep-rust & 48.78 & 53.05 & 64.02 & 65.85 & 51.22 & 68.29 & 63.41 & 62.8 & 79.27 & 74.39 & 85.98 & 74.39 \\ \hline
% hep-js & 44.51 & 60.98 & 63.41 & 68.29 & 56.71 & 71.34 & 66.46 & 69.51 & 81.71 & 84.76 & 82.93 & 87.2 \\ \hline
% \rowcolor{lightgray} alpacaeval-length & 62.61 & 70.43 & 69.94 & 69.44 & 69.94 & 62.11 & 63.11 & 68.82 & 53.42 & 66.83 & 54.41 & 71.68 \\ \hline
% llmbar-natural & 58.0 & 59.0 & 54.0 & 69.0 & 64.0 & 76.0 & 51.0 & 71.0 & 62.0 & 73.0 & 74.0 & 88.0 \\ \hline
% \end{tabular}
% \caption{Performance of different models across datasets.}
% \label{tab:model_performance}
% \end{table*}

\begin{table*}[h!]
\centering
\tiny
\renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{3pt}
\definecolor{apricot}{rgb}{0.95, 0.82, 0.62}
\definecolor{lightgray}{rgb}{0.96, 0.96, 0.96}

\begin{tabular}{l|ccccccccccccc}

\hline
\rowcolor{apricot}
\textbf{Dataset} & 

% \shortstack{\textbf{Qwen} \\ \textbf{2.5-7B}} & 
\shortstack{\textbf{Llama} \\ \textbf{3.2-1B}} & 
\shortstack{\textbf{Llama} \\ \textbf{3.2-1B}\\\textbf{Instruct}} & 
\shortstack{\textbf{Llama} \\ \textbf{3.2-3B}} & 
\shortstack{\textbf{Llama} \\ \textbf{3.2-3B}\\\textbf{Instruct}} & 
\shortstack{\textbf{Meta} \\ \textbf{Llama-3-8B}} & 
\shortstack{\textbf{Meta} \\ \textbf{Llama-3-8B}\\\textbf{Instruct}} & 
\shortstack{\textbf{Mistral} \\ \textbf{7B-v0.1}} & 
\shortstack{\textbf{Mistral} \\ \textbf{7B}\\\textbf{Instruct-v0.1}} & 
\shortstack{\textbf{Qwen} \\ \textbf{2.5-3B}} & 
\shortstack{\textbf{Qwen} \\ \textbf{2.5-3B}\\\textbf{Instruct}} & 
\shortstack{\textbf{Qwen} \\ \textbf{2.5-7B}} & 
\shortstack{\textbf{Qwen} \\ \textbf{2.5-7B}\\\textbf{Instruct}}& 
\shortstack{\textbf{SKYWORK} \\ \textbf{8b}\\ \textbf{reward}} \\

% \textbf{Llama-3.2-1B} & 
% \textbf{Llama-3.2-1B-Instruct} &
% \textbf{Llama-3.2-3B} &
% \textbf{Llama-3.2-3B-Instruct} & 
% \textbf{Meta-Llama-3-8B} & 

% \textbf{Meta-Llama-3-8B-Instruct} & 
% \textbf{Mistral-7B-v0.1} & 
% \textbf{Mistral-7B-Instruct-v0.1} & 
% \textbf{Qwen2.5-3B} & 
% \textbf{Qwen2.5-3B-Instruct} & 
% \textbf{Qwen2.5-7B} & 
% \textbf{Qwen2.5-7B-Instruct} &
% \textbf{SKYWORK-8b-reward} \\
\hline
\rowcolor{lightgray} hep-cpp & 54.88 & 49.39 & 68.29 & 65.24 & 57.32 & 74.39 & 70.12 & 75.00 & 82.93 & 76.22 & 84.76 & 78.05 & 92.68 \\
math-prm & 23.49 & 88.14 & 98.21 & 98.21 & 77.18 & 54.97 & 97.99 & 96.20 & 24.61 & 46.31 & 68.46 & 56.24 & 95.75 \\
\rowcolor{lightgray} llmbar-adver-GPTInst & 63.04 & 64.13 & 44.57 & 53.26 & 59.78 & 71.74 & 71.74 & 75.00 & 51.09 & 83.70 & 59.78 & 78.26 & 71.74 \\
refusals-dangerous & 76.00 & 94.00 & 22.00 & 72.00 & 25.00 & 91.00 & 45.00 & 86.00 & 72.00 & 78.00 & 74.00 & 96.00 & 92.00 \\
\rowcolor{lightgray} hep-python & 50.61 & 52.44 & 61.59 & 71.34 & 53.66 & 77.44 & 67.07 & 76.22 & 77.44 & 78.66 & 89.02 & 89.02 & 93.29 \\
alpacaeval-easy & 34.41 & 83.23 & 34.66 & 53.79 & 56.15 & 24.22 & 20.00 & 42.36 & 36.40 & 27.33 & 46.71 & 80.25 & 92.92 \\
\rowcolor{lightgray} hep-java & 54.88 & 55.49 & 58.54 & 67.68 & 49.39 & 78.05 & 74.39 & 68.29 & 85.37 & 86.59 & 88.41 & 84.15 & 92.68 \\
llmbar-adver-GPTOut & 55.32 & 46.81 & 36.17 & 44.68 & 29.79 & 44.68 & 46.81 & 53.19 & 53.19 & 48.94 & 53.19 & 59.57 & 68.09 \\
\rowcolor{lightgray} alpacaeval-hard & 49.69 & 88.20 & 55.16 & 70.43 & 50.81 & 40.62 & 27.70 & 63.23 & 38.63 & 33.66 & 50.06 & 88.45 & 84.60 \\
hep-go & 49.39 & 45.73 & 53.66 & 64.63 & 57.93 & 73.78 & 70.73 & 73.78 & 81.10 & 82.93 & 83.54 & 85.98 & 90.24 \\
\rowcolor{lightgray} refusals-offensive & 73.00 & 97.00 & 49.00 & 97.00 & 86.00 & 99.00 & 45.00 & 97.00 & 23.00 & 94.00 & 98.00 & 100.00 & 98.00 \\
xstest-should-refuse & 56.49 & 77.92 & 67.53 & 92.21 & 82.47 & 98.70 & 46.75 & 92.21 & 54.55 & 93.51 & 84.42 & 94.16 & 77.27 \\
\rowcolor{lightgray} donotanswer & 38.24 & 55.88 & 64.71 & 82.35 & 69.12 & 91.91 & 63.97 & 80.88 & 62.50 & 91.18 & 78.68 & 90.44 & 70.59 \\
mt-bench-hard & 51.11 & 60.00 & 64.44 & 64.44 & 48.89 & 48.89 & 48.89 & 53.33 & 55.56 & 55.56 & 64.44 & 66.67 & 71.11 \\
\rowcolor{lightgray} llmbar-adver-neighbor & 64.18 & 58.96 & 50.00 & 60.45 & 59.70 & 74.63 & 45.52 & 59.70 & 44.03 & 72.39 & 60.45 & 73.88 & 75.37 \\
mt-bench-easy & 60.71 & 60.71 & 60.71 & 78.57 & 50.00 & 89.29 & 60.71 & 75.00 & 60.71 & 78.57 & 89.29 & 100.00 & 100.00 \\
\rowcolor{lightgray} llmbar-adver-manual & 65.22 & 52.17 & 52.17 & 47.83 & 47.83 & 63.04 & 45.65 & 52.17 & 41.30 & 56.52 & 45.65 & 60.87 & 63.04 \\
mt-bench-med & 37.78 & 64.44 & 64.44 & 71.11 & 51.11 & 60.00 & 57.78 & 60.00 & 55.56 & 73.33 & 71.11 & 93.33 & 86.67 \\
\rowcolor{lightgray} xstest-should-respond & 53.60 & 77.60 & 57.60 & 57.60 & 50.00 & 58.80 & 72.40 & 63.20 & 84.40 & 73.60 & 90.40 & 85.60 & 86.40 \\
hep-rust & 48.78 & 53.05 & 64.02 & 65.85 & 51.22 & 68.29 & 63.41 & 62.80 & 79.27 & 74.39 & 85.98 & 74.39 & 90.24 \\
\rowcolor{lightgray} hep-js & 44.51 & 60.98 & 63.41 & 68.29 & 56.71 & 71.34 & 66.46 & 69.51 & 81.71 & 84.76 & 82.93 & 87.20 & 93.29 \\
alpacaeval-length & 62.61 & 70.43 & 69.94 & 69.44 & 69.94 & 62.11 & 63.11 & 68.82 & 53.42 & 66.83 & 54.41 & 71.68 & 86.71 \\
\rowcolor{lightgray} llmbar-natural & 58.00 & 59.00 & 54.00 & 69.00 & 64.00 & 76.00 & 51.00 & 71.00 & 62.00 & 73.00 & 74.00 & 88.00 & 82.00 \\
\hline
\end{tabular}
\caption{Reward Bench Performance Across Different Levels}
\label{tab:reward_bench_levels}
\end{table*}