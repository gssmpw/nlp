\begin{table*}[h!]
\centering
\definecolor{apricot}{rgb}{0.95, 0.82, 0.62}
\definecolor{lightgray}{rgb}{0.96, 0.96, 0.96}

\begin{tabular}{l | ccccc|c}
\hline
\rowcolor{apricot} 
\multirow{0}{*}{Models} & BBH & Arc-Easy & Alpaca-Eval & MMLU & IFEval & Average \\ \hline

\rowcolor{lightgray} Mistral-7B-Base & 3.40 & 11.00 & 1.20 & 9.60 & 26.63 & 10.37\\ \hline
Mistral-7B-Instruct & 29.80 & 80.40 & 68.00 & 35.80 & \textbf{40.05} & 53.50\\ \hline
\rowcolor{lightgray} Mistral-7B-Self Rewarding & 31.20 & 77.00 & 69.60 & 33.00 & 29.31 & 48.02\\ \hline
Mistral-7B-Reward & 30.20 & \textbf{85.20} & 77.40 &\textbf{41.00} & 31.69 & 53.10\\ \hline
\rowcolor{lightgray}\textbf{Mistral-7B-Ours} & \textbf{34.60} & 82.20 & \textbf{78.20} & 37.60 & 39.19 & \textbf{54.35}\\ \hline

\hline \hline

\rowcolor{lightgray} Llama-1B-Base & 0.60 & 32.80 & 0.80 & 1.40 & 9.80 & 9.08\\ \hline 
Llama-1B-SFT & 1.40 & 22.40 & 0 & \textbf{5.20} & 10.19 & 7.83\\ \hline 
\rowcolor{lightgray} Llama-1B-Self Rewarding & 0.20 & 15.20 & 0.60 & 2.40 & 11.23 & 5.92\\ \hline
Llama-1B-Reward & \textbf{2.20} & \textbf{51.20} & \textbf{7.20} & 3.40 & 10.68 & \textbf{14.93}\\ \hline
\rowcolor{lightgray} \textbf{Llama-1B-Ours} & 0.80 & 46.40 & 2.80 & 3.80 & \textbf{12.08} & 13.17\\ \hline


\end{tabular}
\caption{We compare variations of Mistral-7B and LLaMA-1B models trained using preferences from different methods. Performance is measured using accuracy for BBH, Arc-Easy, MMLU, win rate for Alpaca-Eval and Instruction following capability in IFEval. For more details regarding the evaluations refer to Appendix \ref{Eval Datasets}
}
\label{tab:benchmark_comparison}
\end{table*}
% Self-Scoring assigns a numerical score to each (Prompt, Response) pair to create a prefernce Dataset. Ours applies our method dicussed above to construct a preference dataset for DPO training. Reward leverages an external reward model to rate responses and generate a preference dataset for DPO. 