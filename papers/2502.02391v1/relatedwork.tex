\section{Related Work}
Named entity recognition (NER) has been a central task in natural language processing for decades, with early systems relying on rule-based and statistical approaches. Traditional NER systems \cite{Senapati2013Named-Entity, Danhui2014Rule-based, Sanglikar2011Named} exploited handcrafted rules, lexicons, and shallow syntactic features. While these systems achieved high precision on constrained domains, their recall was often limited due to the difficulty in generalizing beyond predefined dictionaries and patterns. To overcome these limitations, feature-based supervised methods, including Support Vector Machines (SVM) \cite{Lin2006Chinese, Paper2015A} and Conditional Random Fields (CRF) \cite{Guodong2011Geospatial, Tan2023Named}, emerged as the next generation of solutions, reducing the need for manual rule engineering but still relying heavily on carefully designed features.

\paragraph{Deep Neural Approaches.}  
Recent advancements in deep learning have transformed Named Entity Recognition (NER), shifting from feature-engineered models to end-to-end neural architectures. Recurrent Neural Networks (RNNs) have been widely used for sequence labeling tasks, demonstrating superior performance in capturing contextual dependencies within text \cite{Wu2017Clinical}. Meanwhile, Convolutional Neural Networks (CNNs) have been explored as an alternative approach, leveraging local context and character-level features to enhance entity recognition, as seen in biomedical NER applications \cite{Zhu2017GRAM}. More recently, transformer-based models such as BERT and XLNet have set new benchmarks in NER by leveraging large-scale pretraining to generate contextualized word embeddings, outperforming traditional RNN- and CNN-based models \cite{Lothritz2020Evaluating}. However, these models require substantial amounts of annotated data to avoid overfitting and achieve optimal performance, making them less effective in low-resource settings \cite{Ahmed2024Enriching}.

\paragraph{Transfer Learning and Domain Adaptation in NER.}  
To mitigate the data dependency of deep neural models, transfer learning has emerged as a key strategy for improving NER performance in low-resource domains. This approach enables knowledge transfer from resource-rich domains to enhance entity recognition in data-scarce scenarios \cite{Shah2021Multi-Task}. Techniques such as parameter-sharing and fine-tuning have been employed to optimize representations across different domains, improving adaptability while minimizing the need for extensive labeled data \cite{Widyawan2024A}. However, conventional transfer learning methods often assume that source and target domains share the same label space (homogeneous transfer) or require a sufficient number of annotated examples to bridge the distributional gap in heterogeneous settings. Recent advancements in adversarial domain adaptation have attempted to address these challenges by learning domain-invariant representations, effectively enhancing generalization across diverse datasets \cite{Zhu2024Chinese}. Despite these improvements, challenges persist in adapting NER models to domains with extremely scarce target data, highlighting the need for more efficient domain adaptation techniques.

\paragraph{Few-Shot Learning for NER.}  
Few-shot learning has become a crucial area of research for Named Entity Recognition (NER), enabling models to generalize to new entity types with minimal annotated data. Unlike traditional sequence labeling methods, few-shot NER presents unique challenges due to the variability of entity occurrences within a sentence and the absence of a predefined entity class set \cite{DeLichy2021Meta-Learning}.
To address these challenges, meta-learning techniques such as FewNER have been proposed. FewNER introduces a task-adaptive training approach that partitions the network into task-independent and task-specific components, allowing for rapid adaptation with minimal data while mitigating overfitting \cite{Li2023Few-Shot}. Additionally, decomposed meta-learning frameworks have been introduced to sequentially optimize span detection and entity classification, improving generalization across domains \cite{2022Decomposed}. Other advances include the integration of knowledge graphs to enhance prototype-based few-shot NER models \cite{Zhang2024KCL} and contrastive learning to improve entity cluster separation in low-resource settings \cite{Huang2022COPNER}. Despite these advancements, few-shot NER continues to face challenges in handling complex entity dependencies and adapting to highly diverse domains, necessitating further innovations in adaptive learning and self-supervised techniques.

\paragraph{Meta-Learning Approaches in NER.}  
Meta-learning has gained significant traction in natural language processing (NLP), particularly for Named Entity Recognition (NER), where labeled data is often scarce. One of the most notable applications of meta-learning in NER is MetaNER, which combines meta-learning with adversarial training to develop a robust and generalizable sequence encoder \cite{li2020metaner}. MetaNER is trained across multiple source domains, explicitly simulating domain shifts during training, and can adapt rapidly to new domains with minimal labeled data. By leveraging adversarial training, it enhances model generalization while mitigating overfitting issues. However, MetaNER requires updating the entire network during adaptation, which can be computationally intensive.
Beyond MetaNER, other meta-learning approaches for NER have emerged, including FewNER, which decomposes the meta-learning process into task-independent and task-specific components, reducing the risk of overfitting and improving adaptation efficiency \cite{Li2023Few-Shot}. Additionally, adversarial learning techniques have been explored to further enhance robustness against domain shifts and noisy data, demonstrating improvements in model generalization \cite{Fu2021Exploiting}. These advancements highlight the growing potential of meta-learning in NER, though challenges remain in making these methods more computationally efficient and scalable for real-world applications.

\paragraph{Prototypical Networks and Model Fusion.}  
Prototypical networks have become a fundamental approach in few-shot Named Entity Recognition (NER), leveraging token-level representations to construct class prototypes and using a distance metric—typically cosine similarity—for classification. ProtoNER exemplifies this paradigm, demonstrating strong adaptability in incremental learning scenarios where new entity classes can be incorporated with minimal additional data \cite{Fritzler2018Few-shot}. To improve upon standard prototypical networks, researchers have introduced SpanProto, a two-stage span-based prototypical network that refines entity boundary detection and enhances classification accuracy \cite{Wang2022SpanProto}. Additionally, EP-Net addresses prototype dispersion by aligning entity spans in a projected embedding space, leading to improved few-shot NER performance \cite{Ji2022Few-shot}.
Beyond standalone prototypical networks, researchers have explored model fusion techniques to mitigate overfitting in few-shot settings. Recent work has introduced logit fusion and differentiation fusion, which combine multiple model outputs to correct boundary detection and entity classification errors \cite{gong2021few}. These fusion strategies help stabilize model predictions by integrating diverse representations, thereby improving overall robustness. Furthermore, HEProto, a hierarchical enhancing prototypical network, employs multi-task learning to jointly optimize span detection and type classification, ensuring better entity type differentiation \cite{Chen2023HEProto}. Such advancements underscore the growing importance of hybrid approaches in refining few-shot NER models.

\paragraph{Our Positioning.}  
While existing approaches such as ProtoNER and MetaNER have made significant strides in addressing the challenges of few-shot NER and domain adaptation, they typically treat entity recognition and topic modeling as separate tasks. In contrast, our proposed FewTopNER integrates robust few-shot entity recognition with semantically rich topic modeling through cross-task attention and language-specific calibration. This integrated approach not only improves entity recognition accuracy by leveraging topic context for disambiguation but also produces more coherent topic representations, as evidenced by improved normalized pointwise mutual information (NPMI) scores. Furthermore, by retaining task-independent representations and updating only a small set of task-specific parameters, FewTopNER mitigates overfitting and enhances computational efficiency during adaptation.
Prior work in NER has evolved from rule-based and statistical methods to deep neural approaches, with transfer learning and meta-learning emerging as effective strategies for low-resource settings. Methods like ProtoNER and MetaNER have paved the way for few-shot NER, yet challenges remain—especially in reconciling the sequence labeling nature of NER with few-shot learning paradigms. FewTopNER builds on these advances by integrating topic modeling into the few-shot framework, offering mutual benefits for both entity recognition and topic coherence. Our work thus represents a significant step towards more robust and efficient few-shot, cross-lingual NER systems.