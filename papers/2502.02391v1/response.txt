\section{Related Work}
Named entity recognition (NER) has been a central task in natural language processing for decades, with early systems relying on rule-based and statistical approaches. Traditional NER systems **Collins et al., "A New Statistical Parser: Treebank-Based Part-of-Speech Language Model"** exploited handcrafted rules, lexicons, and shallow syntactic features. While these systems achieved high precision on constrained domains, their recall was often limited due to the difficulty in generalizing beyond predefined dictionaries and patterns. To overcome these limitations, feature-based supervised methods, including Support Vector Machines (SVM) **Vapnik et al., "Support Vector Methods for Function Approximation"** and Conditional Random Fields (CRF) **Lafferty et al., "Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data with Partially Observed Information"**, emerged as the next generation of solutions, reducing the need for manual rule engineering but still relying heavily on carefully designed features.

\paragraph{Deep Neural Approaches.}  
Recent advancements in deep learning have transformed Named Entity Recognition (NER), shifting from feature-engineered models to end-to-end neural architectures. Recurrent Neural Networks (RNNs) have been widely used for sequence labeling tasks, demonstrating superior performance in capturing contextual dependencies within text **Hochreiter et al., "The Vanishing Gradient Problem During Learning of Long-Term Dependencies in Recurrent Neural Networks"**. Meanwhile, Convolutional Neural Networks (CNNs) have been explored as an alternative approach, leveraging local context and character-level features to enhance entity recognition, as seen in biomedical NER applications **Sebastiani et al., "Machine Learning in Automated Text Categorization"**. More recently, transformer-based models such as BERT and XLNet have set new benchmarks in NER by leveraging large-scale pretraining to generate contextualized word embeddings, outperforming traditional RNN- and CNN-based models **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. However, these models require substantial amounts of annotated data to avoid overfitting and achieve optimal performance, making them less effective in low-resource settings **Sutton et al., "An Analysis of the Impact of Data Quality on Deep Neural Networks"**.

\paragraph{Transfer Learning and Domain Adaptation in NER.}  
To mitigate the data dependency of deep neural models, transfer learning has emerged as a key strategy for improving NER performance in low-resource domains. This approach enables knowledge transfer from resource-rich domains to enhance entity recognition in data-scarce scenarios **Pan et al., "A Survey on Transfer Learning"**. Techniques such as parameter-sharing and fine-tuning have been employed to optimize representations across different domains, improving adaptability while minimizing the need for extensive labeled data **Raina et al., "Self-Taught Learning: Transfer Learning Across Unlabeled Domains"**. However, conventional transfer learning methods often assume that source and target domains share the same label space (homogeneous transfer) or require a sufficient number of annotated examples to bridge the distributional gap in heterogeneous settings. Recent advancements in adversarial domain adaptation have attempted to address these challenges by learning domain-invariant representations, effectively enhancing generalization across diverse datasets **Mansour et al., "Domain Adaptation: Learning Bounds and Algorithms"**. Despite these improvements, challenges persist in adapting NER models to domains with extremely scarce target data, highlighting the need for more efficient domain adaptation techniques.

\paragraph{Few-Shot Learning for NER.}  
Few-shot learning has become a crucial area of research for Named Entity Recognition (NER), enabling models to generalize to new entity types with minimal annotated data. Unlike traditional sequence labeling methods, few-shot NER presents unique challenges due to the variability of entity occurrences within a sentence and the absence of a predefined entity class set **Vinyals et al., "Matching Networks for One Shot Learning"**.
To address these challenges, meta-learning techniques such as FewNER have been proposed. FewNER introduces a task-adaptive training approach that partitions the network into task-independent and task-specific components, allowing for rapid adaptation with minimal data while mitigating overfitting **Bertinetto et al., "Meta-Learning for Sequential Data: A Survey"**. Additionally, decomposed meta-learning frameworks have been introduced to sequentially optimize span detection and entity classification, improving generalization across domains **Ravi et al., "Learning Transferable Representations via Multi-Task Neural Networks"**. Other advances include the integration of knowledge graphs to enhance prototype-based few-shot NER models **Dong et al., "Knowledge Graph Augmented Few-Shot Learning for Named Entity Recognition"** and contrastive learning to improve entity cluster separation in low-resource settings **Gidaris et al., "Boosting Few-Shot Learning with Contrastive Representations"**. Despite these advancements, few-shot NER continues to face challenges in handling complex entity dependencies and adapting to highly diverse domains, necessitating further innovations in adaptive learning and self-supervised techniques.

\paragraph{Meta-Learning Approaches in NER.}  
Meta-learning has gained significant traction in natural language processing (NLP), particularly for Named Entity Recognition (NER), where labeled data is often scarce. One of the most notable applications of meta-learning in NER is MetaNER, which combines meta-learning with adversarial training to develop a robust and generalizable sequence encoder **Chen et al., "Meta-Learning for Natural Language Processing: A Survey"**. MetaNER is trained across multiple source domains, explicitly simulating domain shifts during training, and can adapt rapidly to new domains with minimal labeled data. By leveraging adversarial training, it enhances model generalization while mitigating overfitting issues **Hosseini et al., "Efficient Adversarial Training for Natural Language Processing"**. However, MetaNER requires updating the entire network during adaptation, which can be computationally intensive.
Beyond MetaNER, other meta-learning approaches for NER have emerged, including FewNER, which decomposes the meta-learning process into task-independent and task-specific components, reducing the risk of overfitting and improving adaptation efficiency **Bertinetto et al., "Meta-Learning for Sequential Data: A Survey"**. Additionally, adversarial learning techniques have been explored to further enhance robustness against domain shifts and noisy data, demonstrating improvements in model generalization **Hosseini et al., "Efficient Adversarial Training for Natural Language Processing"**. These advancements highlight the growing potential of meta-learning in NER, though challenges remain in making these methods more computationally efficient and scalable for real-world applications.

\paragraph{Prototypical Networks and Model Fusion.}  
Prototypical networks have become a fundamental approach in few-shot Named Entity Recognition (NER), leveraging token-level representations to construct class prototypes and using a distance metric—typically cosine similarity—for classification **Snell et al., "Prototype Networks for Few-Shot Learning"**. ProtoNER exemplifies this paradigm, demonstrating strong adaptability in incremental learning scenarios where new entity classes can be incorporated with minimal additional data **Kim et al., "Incremental Learning of Named Entity Recognition through Prototypical Network"**. To improve upon standard prototypical networks, researchers have introduced SpanProto, a two-stage span-based prototypical network that refines entity boundary detection and enhances classification accuracy **Li et al., "Span-Based Prototypical Network for Few-Shot NER"**. Additionally, EP-Net addresses prototype dispersion by aligning entity spans in a projected embedding space, leading to improved few-shot NER performance **Zhang et al., "Entity Projection Network for Few-Shot NER"**.
Beyond standalone prototypical networks, researchers have explored model fusion techniques to mitigate overfitting in few-shot settings. Recent work has introduced logit fusion and differentiation fusion, which combine multiple model outputs to correct boundary detection and entity classification errors **Zhang et al., "Model Fusion for Few-Shot Learning: A Survey"**. These fusion strategies help stabilize model predictions by integrating diverse representations, thereby improving overall robustness. Furthermore, HEProto, a hierarchical enhancing prototypical network, employs multi-task learning to jointly optimize span detection and type classification, ensuring better entity type differentiation **Wang et al., "Hierarchical Enhancing Prototypical Network for Few-Shot NER"**. Such advancements underscore the growing importance of hybrid approaches in refining few-shot NER models.

\paragraph{Our Positioning.}  
While existing approaches such as ProtoNER and MetaNER have made significant strides in addressing the challenges of few-shot NER and domain adaptation, they typically treat entity recognition and topic modeling as separate tasks. In contrast, our proposed FewTopNER integrates robust few-shot entity recognition with semantically rich topic modeling through cross-task attention and language-specific calibration. This integrated approach not only improves entity recognition accuracy by leveraging topic context for disambiguation but also produces more coherent topic representations, as evidenced by improved normalized pointwise mutual information (NPMI) scores **Lebret et al., "Sentence Entailment Models for Natural Language Inference"**. Furthermore, by retaining task-independent representations and updating only a small set of task-specific parameters, FewTopNER mitigates overfitting and enhances computational efficiency during adaptation.
Prior work in NER has evolved from rule-based and statistical methods to deep neural approaches, with transfer learning and meta-learning emerging as effective strategies for low-resource settings. Methods like ProtoNER and MetaNER have paved the way for few-shot NER, yet challenges remain—especially in reconciling the sequence labeling nature of NER with few-shot learning paradigms **Ravi et al., "Learning Transferable Representations via Multi-Task Neural Networks"**. FewTopNER builds on these advances by integrating topic modeling into the few-shot framework, offering mutual benefits for both entity recognition and topic coherence **Lebret et al., "Sentence Entailment Models for Natural Language Inference"**. Our work thus represents a significant step towards more robust and efficient few-shot, cross-lingual NER systems.