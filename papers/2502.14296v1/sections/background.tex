\clearpage
\section{Background}

In this section, we provide an overview of the background relevant to our work, focusing on two key areas: 

1) \textit{Corporate approaches to enhance the trustworthiness of GenFMs (\textbf{\S\ref{sec:approaches_corporate}}).} Trustworthiness is a complex and multifaceted concept, deeply intertwined with the needs and expectations of users. By examining how corporations approach trustworthiness in generative foundation models, we can gain a deeper understanding of what constitutes trust in real-world applications. This insight is crucial for a) identifying the essential features of trustworthy GenFMs, fostering unified guidelines in \textbf{\S\ref{sec:guideline}}, and b) enabling the creation of a benchmark that is both comprehensive and aligned with practical, industry-relevant needs. 

2) \textit{Related work on evaluation methods and benchmarks (\textbf{\S\ref{sec:evaluation_related_work}} and \textbf{\S\ref{sec:benchmark_related_work}}).} By examining existing evaluation methodologies and benchmarks, we identify both the strengths and limitations of current approaches. This analysis highlights gaps in current evaluation frameworks, enabling us to pinpoint areas that require further attention, thereby guiding the development of a more adaptive and effective assessment benchmark for GenFMs.



\subsection{Approaches to Enhancing Trustworthiness From Corporate}
\label{sec:approaches_corporate}



\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{image/cooperate.pdf}
    \caption{Approaches to ensure the trustworthiness of generative models across different corporations.}
    \label{fig:corporate}
    \vspace{-2pt}
\end{figure}

In this section, we introduce the strategies, methodologies, and techniques employed by leading corporations to enhance the trustworthiness of GenFMs. As illustrated in \autoref{fig:corporate}, our analysis focuses on prominent industrial developers of generative models, including Microsoft, OpenAI, Amazon, IBM, Meta, Anthropic, Google, Salesforce, and more.



\textbf{OpenAI.} From GPT-4o \cite{openai_gpt4o_system_card} to Dalle-3 \cite{ramesh2024dalle3}, OpenAI has released various frontier generative models. Meanwhile, OpenAI has also taken several steps to promote a trustworthy generative model. According to the OpenAI Charter \cite{openai_charter}, the organization is dedicated to long-term safety, cooperative research, and broadly distributed benefits. It aims to lead in AI capabilities while focusing on the safe and secure development of AGI. Specifically, OpenAI carries out the following measurements to ensure and enhance the trustworthiness of its generative models: 

\begin{itemize}[nolistsep, leftmargin=*]
    \item[] \textcolor[HTML]{e6e5e1}{\largedot} {\textit{OpenAI Red Teaming Network}} \cite{openai_red_teaming_network}: OpenAI has established a Red Teaming Network, a community of experts from various fields to evaluate and improve the safety of their generative models. 
    \item[] \textcolor[HTML]{e6e5e1}{\largedot} {\textit{Model System Card}} \cite{mitchell2019model,ramesh2024dalle3, openai_gpt4o_system_card}: OpenAI has released the details of implementing extensive safety measures for its generative models like Dalle-3 \cite{ramesh2024dalle3} and GPT-4o \cite{openai_gpt4o_system_card}.
    \item[] \textcolor[HTML]{e6e5e1}{\largedot} {\textit{Safety Standards}} \cite{openai_safety_standards, openai_safety_best_practices}: Key principles of OpenAI's safety standards include minimizing harm, building trust, learning and iterating, and being a pioneer in trust and safety.
    \item[] \textcolor[HTML]{e6e5e1}{\largedot} {\textit{Model Alignment}} \cite{openai_alignment_research, openai_superalignment}: OpenAI has also formed a team for model superalignment, employing methods that include: 1) developing a scalable training method, 2) validating the resulting model, and 3) stress testing the entire alignment pipeline.
    \item[] \textcolor[HTML]{e6e5e1}{\largedot} {\textit{Secure Infrastructure for Advanced AI}} \cite{openai_secure_infrastructure}: OpenAI is enhancing the security of models by developing trusted computing, network isolation, physical security improvements, AI-specific compliance, and integrating AI into cyber defense.
    \item[] \textcolor[HTML]{e6e5e1}{\largedot} {\textit{Identifiers for AI-generated Material}} \cite{openai_ai_classifier}: OpenAI is launching a classifier trained to distinguish between AI-written and human-written text. The classifier aims to address the growing concerns over AI-generated content.
    \item[] \textcolor[HTML]{e6e5e1}{\largedot} {\textit{Democratic Inputs to AI Grant Program}} \cite{openai_2024_democratic}: OpenAI funded 10 teams globally to explore ways of involving public input in shaping AI behavior. Key actions include supporting projects like crowdsourced audits, AI policy dialogues, and novel voting mechanisms.
\end{itemize}



\textbf{Meta.}
From the early Open Pre-Trained Transformers (OPT) \cite{zhang2022opt} to the LLaMA family models \cite{touvron2023llama, touvron2023llama2, dubey2024llama}, Meta takes an approach to trust and safety in the era of generative AI. Alongside its commitment to open AI access, Meta aims to ensure the safety of its LLaMA models by implementing the following measures and tools:

\begin{itemize}[nolistsep, leftmargin=*]
   \item[] \textcolor[HTML]{0d65ef}{\largedot} {\textit{Pre-Deployment Safety Stress Test}} \cite{llama_responsibiliy}: For all LLaMA models, Meta conducts extensive red teaming with both external and internal experts to stress test the models and identify malicious use cases. With the enhanced capabilities of LLaMA 3.1, such as multilingual support and an expanded context window, these stress tests have been scaled up, along with corresponding evaluations and mitigations in these areas \cite{dubey2024llama}. 
    %  \item[] \textcolor[HTML]{0d65ef}{\largedot} {\textit{Responsible Use Guide}} \cite{llama_use_guide}: The \textit{Responsible Use Guide} is a resource that provides developers with the tools and guidance needed to create LLM-powered products responsibly.
    \item[] \textcolor[HTML]{0d65ef}{\largedot} {\textit{Llama Guard}} \cite{inan2023llama}: Llama Guard is an input and output multilingual moderation tool, designed to detect content that violates safety guidelines.
    \item[] \textcolor[HTML]{0d65ef}{\largedot} {\textit{Prompt Guard}} \cite{llama_prompt_guard}: Prompt Guard is a model designed to detect prompt attacks, including \textit{prompt injection} and \textit{jailbreaking}.
    \item[] \textcolor[HTML]{0d65ef}{\largedot} {\textit{CyberSecEval}} \cite{bhatt2023purple, bhatt2024cyberseceval, wan2024cyberseceval}: In recognition of LLM cybersecurity risks, Meta has released \textit{CyberSecEval}, \textit{CyberSecEval2}, and \textit{CyberSecEval3}, a series of benchmarks designed to help AI model and product developers understand and mitigate generative AI cybersecurity risks.
    \item[] \textcolor[HTML]{0d65ef}{\largedot} {\textit{Responsible Model Deployment}} \cite{llama_responsibiliy}: Meta collaborates with partners like AWS and NVIDIA to integrate safety solutions into the distribution of Llama models, promoting the responsible deployment of Llama systems.
\end{itemize}

\textbf{Microsoft.} Microsoft has been leading efforts to ensure trustworthy AI. Emphasizing safety and security in LLMS like Copilot~\cite{Copilot} and Azure~\cite{Azure}, Microsoft implements several key measures to uphold its principles:

\begin{itemize}[nolistsep, leftmargin=*]
    \item[] \textcolor[HTML]{e94e1a}{\largedot} {\textit{Unbiased and Equitable AI}} \cite{msr_trustworthy_ai_project}: Microsoft Research group has made specific endeavors and also papers that focus on maintaining robustness in model compression~\cite{du2023what}, mitigating biases through techniques like representation neutralization~\cite{du2021fairness}, and enhancing transparency with methods such as rationalization in few-shot learning~\cite{bhat2021self-training}. They also work on reducing gender bias in multilingual embeddings~\cite{zhao2020gender} and improving fake news detection~\cite{shu2020leveraging} using multi-source social supervision.
    %Microsoft funds Microsoft Research address critical issues related to AI trustworthiness. The research teams contribute to create frameworks to ensure that AI systems are designed and deployed responsibly \cite{msr_trustworthy_ai_project} and for social good~\cite{msr_ai_for_good_lab}. The scholars' efforts are pivotal in advancing AI's applications for societal benefit rather than industry staff only.
    \item[] \textcolor[HTML]{7cc627}{\largedot} {\textit{AI for Social Good}}~\cite{msr_ai_for_good_lab}: Microsoft leverages AI for social good through several key initiatives. The AI for Health project aims to improve the healthcare capability of LLMs~\cite{msr_ai_for_health}, while Bioacoustics focuses on wildlife conservation through sound analysis~\cite{msr_bioacoustics}. The Data Visualization project enhances data interpretation~\cite{msr_data_visualization}, and Geospatial Machine Learning addresses environmental and urban challenges of LLMs' expertise~\cite{msr_geospatial_machine_learning}. Additionally, the Open Data platform promotes transparency LLMs by providing an accessible platform~\cite{ms_open_data}.
    \item[] \textcolor[HTML]{00a8e8}{\largedot} {\textit{Empowering Applications and Facilities}}~\cite{responsible_trusted_ai_adoption, microsoft365_trustworthy, microsoft_ai_empowers_us_government}: Microsoft's approach to responsible AI adoption is outlined through their six trustworthy AI principles, which guide how Azure facilitates and integrates these practices into its cloud services \cite{responsible_trusted_ai_adoption}. Furthermore, Microsoft 365's commitment to trustworthy AI is detailed in their tech community blog~\cite{microsoft365_trustworthy}. Their initiatives also extend to government agencies,  reinforcing the importance of trustworthy AI in critical government functions \cite{microsoft_ai_empowers_us_government}.
    \item[] \textcolor[HTML]{f3b609}{\largedot} {\textit{Principles and Commitments}}~\cite{microsoft_commitments_2023, microsoft_framework_for_building_ai_systems_responsibly, microsoft_copilot_trustworthy_commitments}: They have outlined a framework for building AI systems responsibly, which includes guidelines and practices to ensure ethical AI deployment \cite{microsoft_framework_for_building_ai_systems_responsibly}. The company also emphasizes the importance of their Copilot Trustworthy Commitments, which focus on data security and user privacy. 
\end{itemize}



\textbf{Anthropic.}
As an AI safety research company, Anthropic has made improving the trustworthiness of generative models one of its primary goals. Embracing the motto ``show, don't tell'', Anthropic focuses on a multi-faceted, empirically-driven approach to AI safety \cite{anthropic_core_views}. Specifically, Anthropic employs the following measures to improve the trustworthiness of its generative models:

\begin{itemize}[nolistsep, leftmargin=*]
    \item[] \textcolor[HTML]{cd9d7b}{\largedot}  {\textit{API Trust \& Safety Tools}} \cite{anthropic_api}: Anthropic implements different levels of trust and safety tools or API deployment, including basic, intermediate, advanced, and comprehensive safeguards. 
    \item[] \textcolor[HTML]{cd9d7b}{\largedot}  {\textit{Safety Bug Bounty Program}} \cite{anthropic_bounty}: The \textit{bug bounty} program introduces a new initiative aimed at identifying flaws in the mitigations designed to prevent the misuse of our models. It rewards researchers for discovering safety issues in our publicly released AI models.
    \item[] \textcolor[HTML]{cd9d7b}{\largedot}  {\textit{Extensive Research on Interpretability, Alignment, and Societal Impacts}} \cite{anthropic_research}: Anthropic focuses primarily on three research areas in order to improve the trustworthiness of their models: \textit{interpretability}, \textit{alignment}, and \textit{societal impact}. 
    \item[] \textcolor[HTML]{cd9d7b}{\largedot}  {\textit{Providing Assistant to Policymakers}} \cite{mishra2020measurement, anthropic_election, anthropic_constitutional}: As part of its effort to assist policymakers in crafting better regulations for generative AI, Anthropic provides trustworthy research on key topics of interest to policymakers.
\end{itemize}






\textbf{Amazon.} Amazon continues to innovate in the field of generative models with a focus on trustworthiness and safety across its diverse suite of AI services. Recognizing the critical importance of responsible AI development, Amazon implements a series of robust measures to ensure the safety, privacy, and fairness of its AI models:

\begin{itemize}[nolistsep, leftmargin=*] 
\item[] \textcolor[HTML]{222e3c}{\largedot}{\textit{Amazon Bedrock Guardrails}} \cite{amazon_bedrock_guardrails_2024}: Amazon provides tools such as Bedrock Guardrails to enforce safeguards tailored to specific applications, promoting safe interactions by automatically detecting and restricting content that may be harmful or offensive. It supports four kinds of protection in generative model systems: denied topics, content ﬁlters, sensitive information ﬁlters, and word ﬁlters. 

\item[] \textcolor[HTML]{222e3c}{\largedot}{\textit{Model Evaluation and Selection}} \cite{amazon_model_evaluation}: Through Amazon Bedrock, customers can evaluate and select the best foundation models for their applications using a suite of tools that assess models against benchmarks of accuracy, robustness, and toxicity. 

\item[] \textcolor[HTML]{222e3c}{\largedot}{\textit{Amazon Comprehend}} \cite{amazon_comprehend_safety}: To further enhance trustworthiness, Amazon Comprehend supports applications by identifying and classifying toxic content, ensuring outputs adhere to safety standards. 

\item[] \textcolor[HTML]{222e3c}{\largedot}{\textit{Watermarking Techniques}} \cite{amazon_titan_watermarking}: Amazon Titan integrates invisible watermarks in generated images to help track AI-generated content and combat disinformation. 

\item[] \textcolor[HTML]{222e3c}{\largedot}{\textit{Amazon Trusted AI Challenge}}
\cite{amazon_trusted_ai_challenge}: The Amazon Trusted AI Challenge is a competition organized by Amazon Science, aimed at fostering advancements in the field of AI. The challenge is structured to develop AI models or red-teaming systems that address trust-related issues in AI applications. 

\end{itemize}





\textbf{Google (Deepmind).} 
Google has consistently focused on advancing its generative models, from PaLM \cite{anil2023palm} and Bard \cite{bard} to the latest Gemini model \cite{team2023gemini}. Each iteration reflects Google's commitment to developing generative models with enhanced capabilities, pushing the boundaries of AI innovation. At the same time, Google is deeply dedicated to building responsible AI \cite{googleresponsibleAI, googleresearchresponsibleAI, googleresponsibleAIpractices}. This commitment to responsible AI is evident in every model released, as Google strives to balance progress with accountability and societal impact. Specifically, Google has implemented several key measures to build responsible AI: 
\begin{itemize}[nolistsep, leftmargin=15pt]
    \item[] \textcolor[HTML]{f1b504}{\largedot} {\textit{Responsible AI practices}} \cite{googleresponsibleAI, googleresearchresponsibleAI, googleresponsibleAIpractices}: Google has outlined general best practices for responsible AI, focusing on fairness, interpretability, privacy, safety, and security. Additionally, \cite{googleAISafetyGen} provides a detailed discussion of the safety and fairness considerations specific to generative models.
    \item[] \textcolor[HTML]{e23f32}{\largedot} {\textit{Configure safety settings for the generative models}} \cite{googlevertexAIsafety, googlevertexAImultimodalSafety}: In the PaLM API, content is evaluated based on a safety attribute list and filtered accordingly \cite{googlevertexAIsafety}. With the Gemini API, Google introduces configurable filters, allowing users to dynamically set thresholds for blocking certain safety attributes based on their specific needs \cite{googlevertexAImultimodalSafety}.
    \item[] \textcolor[HTML]{32a350}{\largedot} {\textit{Secure AI Framework (SAIF)}} \cite{googleAISaif}: SAIF is a conceptual framework for secure AI systems proposed by Google. It is designed to mitigate AI-specific risks, such as model theft, training data poisoning, prompt injection attacks, and the extraction of confidential information from training data.
    \item[] \textcolor[HTML]{4082ed}{\largedot} {\textit{ShieldGemma}} \cite{zeng2024shieldgemma}: ShieldGemma offers advanced, state-of-the-art predictions of safety risks across various harm types and can effectively filter both inputs and outputs.
    \item[] \textcolor[HTML]{e6e5e1}{\largedot} {\textit{the Frontier Safety Framework}} \cite{deepmindFrontierSafety}: DeepMind introduced the Frontier Safety Framework to evaluate critical capabilities in frontier models, adopting the emerging approach of Responsible Capability Scaling.
    \item[] \textcolor[HTML]{f7f7f7}{\largedot} {\textit{Long-form factuality}} \cite{wei2024longformfactualitylargelanguage}: DeepMind introduced the Search-Augmented Factuality Evaluator (SAFE), which uses an LLM to break down long-form responses into individual facts. SAFE evaluates each fact’s accuracy through a multi-step reasoning process, including sending search queries to Google Search and verifying whether the results support the facts.
\end{itemize}


\textbf{IBM.} 
IBM has consistently proposed frameworks and products focused on Trustworthy AI like Watsonx \cite{ibmWatsonxAssistantSecurity} and Granite models \cite{ibmGraniteFoundationModels}. Specifically, IBM has implemented the following measures:

\begin{itemize}[nolistsep, leftmargin=15pt]
    \item[] \textcolor[HTML]{1c6cbb}{\largedot} {\textit{IBM Framework For Securing Generative AI}} \cite{ibmGenerativeAISecurity}: The IBM Framework for Securing Generative AI helps customers, partners, and organizations worldwide identify common AI attacks and prioritize key defense strategies to protect their generative AI efforts. It focuses on three main areas: securing the data, securing the model, and securing usage. In addition, a suite of detectors has been provided to improve the safety and reliability of LLMs \cite{achintalwar2024detectors}.
    \item[] \textcolor[HTML]{1c6cbb}{\largedot} {\textit{LLMs for Threat Management}} \cite{ibmLLMsThreatManagement}: This project leverages large language models to develop a next-generation threat management platform, focused on creating a highly reliable generative AI-based Personal Security Assistant.
    \item[] \textcolor[HTML]{1c6cbb}{\largedot} {\textit{Generative Models For Trust}} \cite{ibmGraniteFoundationModels, ibmWatsonxAssistantSecurity}:  IBM has been involved in responsible technological innovation and digital transformation \cite{ResponsibleUseIBM}. Its Granite foundation models \cite{ibmGraniteFoundationModels} are designed with trust in mind. These models are trained on data filtered by IBM's "HAP detector," a language model specifically developed to detect and eliminate hateful and profane content. They have released Granite Guardian models \cite{padhi2024granite} to provide risk detection for prompts and responses. Risks are categorized with AI risk atlas \cite{ibmai-risk-atlas}. Additionally, Watsonx Assistant ensures chatbot data privacy and safeguards customers against vulnerabilities, offering scalability and enhanced security \cite{ibmWatsonxAssistantSecurity}.
\end{itemize}

\textbf{Salesforce.} Salesforce has been in the frontier research in the generative ai, releasing a series of generative models such as LLM Einstein GPT~\cite{salesforceeinstein}, multimodal model BLIP series~\cite{li2022blip,li2023blip,xue2024xgen} and diffusion model Unicontrol~\cite{li2024blip,qin2023unicontrol}. With the focus on the trust of its ai services, Salesforce is actively working on several fronts to ensure the security of its generative AI models on it's cloud computing services.
\begin{itemize}[nolistsep, leftmargin=15pt]
    \item[] \textcolor[HTML]{219ddc}{\largedot} {\textit{Generative AI Principles.}} \cite{salesforceprinciples}: Salesforce has developed five guiding principles for trusted generative AI—Accuracy, Safety, Transparency, Empowerment, and Sustainability. These principles aim to ensure that the models are reliable, help users make informed decisions, and minimize negative impacts like overconsumption of resources or perpetuating harmful biases.
    \item[] \textcolor[HTML]{219ddc}{\largedot} {\textit{Trust Layer for Einstein 1 Platform.}} \cite{salesforcetrustlayer}: Salesforce's Einstein AI platform incorporates a comprehensive "Trust Layer" that focuses on grounding AI outputs in accurate CRM data, masking sensitive information, and mitigating other 9 risks such as prompt injection, toxicity and bias. This includes ensuring data security via zero retention agreements with third-party model providers and maintaining an audit trail to track data use and feedback. Salesforce also employs mechanisms to detect and prevent hallucinations in LLM responses.
    \item[] \textcolor[HTML]{219ddc}{\largedot} {\textit{Benchmarking and Tools.}} \cite{goel2021robustness,vig2021summvis}: Salesforce released tools like Robustness Gym~\cite{goel2021robustness} and SummVis~\cite{vig2021summvis} to address the challenge of evaluating model robustness and factual consistency.
     \item[] \textcolor[HTML]{219ddc}{\largedot} {\textit{Factual Consistency Improving.}} \cite{salesforcetrustedNLG,zhang2022improving,pagnoni2022socratic}: Salesforce improves factual consistency by using techniques like grounding entities~\cite{zhang2022improving} found in the input data and ensembling models trained on noisy datasets. They also introduced Socratic pretraining~\cite{pagnoni2022socratic}, a method to enhance model control by pretraining it to address important user questions, making the output more reliable and controllable.
\end{itemize}

\textbf{NVIDIA.} NVIDIA has taken several steps to ensure trustworthy AI development:

\begin{itemize}[nolistsep, leftmargin=15pt]
    \item[] \textcolor[HTML]{76B900}{\largedot} {\textit{Trustworthy AI Principles and Safety Initiatives}:} NVIDIA emphasizes safety and transparency in AI development. They focus on creating AI systems that are safe and clear for users. NVIDIA also joined the National Institute of Standards and Technology's Artificial Intelligence Safety Institute Consortium, which works to create tools and standards for safe AI development~\cite{nvidiaTrustworthyAI}. 
    \item[] \textcolor[HTML]{76B900}{\largedot} {\textit{NeMo Guardrails}:} NVIDIA offers NeMo Guardrails, an open-source tool to ensure AI models provide accurate and appropriate responses. This tool helps keep AI outputs reliable and secure~\cite{nvidiaTrustworthyAI}.
    \item[] \textcolor[HTML]{76B900}{\largedot} {\textit{Open-Source Commitment}:} NVIDIA has a GitHub repository dedicated to trustworthy AI. This demonstrates their commitment to building reliable AI systems through open-source contributions~\cite{nvidiaTrustworthyAIRepo}.
    \item[] \textcolor[HTML]{76B900}{\largedot} {\textit{Verifiable Compute Collaboration}:} NVIDIA collaborated with EQTY Lab and Intel to launch `Verifiable Compute.' This solution enhances trust in AI workflows using hardware security measures and distributed ledger technology~\cite{eqtyLabIntelNvidiaAITrust}.
\end{itemize}

\textbf{Cohere.} Cohere's contributions to the trustworthiness of LLMs are highlighted through their detailed discussions on AI safety and responsibility. In their "Enterprise Guide to AI Safety" \cite{cohere2024enterprise}, Cohere outlines fundamental principles for maintaining AI safety and ethical standards, emphasizing the necessity of integrating robust safety measures throughout AI development. Their "Responsibility Statement" \cite{cohere2024responsibility} further demonstrates a commitment to responsible AI practices, and accountability in the deployment of AI technologies. Additionally, the "Statement of AI Security" \cite{cohere2024stateofai} focuses on specific security concerns, such as vulnerabilities to jailbreaking and other potential threats.


\textbf{Mistral AI.} Mistral AI has implemented several key measures to enhance the trustworthiness of its models, particularly around safety and content moderation. Mistral AI offers a "safe\_prompt" option, which can be activated via API calls. This adds a system prompt to ensure the model generates ethical, respectful responses, and is free from harmful or prejudiced content \cite{mistral_guardrailing}. Moreover, Mistral models are equipped with self-reflection capabilities that allow them to evaluate both user prompts and generated content \cite{jiang2023mistral}. Mistral AI also has specific legal measures in place to prevent any model outputs or usage that could be related to child exploitation or abuse, ensuring that their models are not used for harmful activities \cite{mistral_child_abuse}.

\textbf{Adobe.} As a leader in digital creativity software, the company has implemented comprehensive measures to ensure trustworthiness in their models and LLM-powered tools \cite{adobe_ai_commitments}. The company established an Ethics Review Board and mandates impact assessments for all new features \cite{adobe_ai_ethics}. Adobe developed Content Credentials for digital content transparency and trained Firefly \cite{adobe_firefly} exclusively on licensed and public domain content \cite{adobe_ai_commitments}. They apply strict security measures, including red-teaming and third-party testing \cite{adobe_trust_center}. To protect creators, Adobe is developing a "Do Not Train" tag and advocating for legal safeguards against style impersonation \cite{adobe_fair_act}.

\textbf{Apple.} Apple's approach to trustworthy AI development~\cite{gunter2024apple} is characterized by a comprehensive framework encompassing four foundational principles: (1) user empowerment through purpose-specific tools, (2) authentic representation with bias mitigation, (3) precautionary design measures, and (4) privacy preservation. Their technical implementation notably employs on-device processing and Private Cloud Compute infrastructure, distinctly avoiding the use of personal user data in foundation model training. The framework's efficacy is validated through systematic evaluation protocols, including diverse adversarial testing and human evaluation. While acknowledging the limitations of current safety benchmarks, Apple maintains ongoing evaluation through internal and external red-teaming procedures, embodying a commitment to continuous improvement in responsible AI development.

\textbf{ZHIPU AI.} ZHIPU AI has released the GLM series of LLMs \cite{hou2024chatglm} and the CogView series of VLMs \cite{thudm2024cogview3plus}. It focuses on improving the trustworthiness of generative models by alignment. For instance, it has proposed Black-Box Prompt Optimization (BPO), which aligns human preference with any training on LLMs \cite{cheng2023black}. Moreover, AlignBench \cite{liu2023alignbench} proposed by Liu et al. is designed to evaluate the alignment of Chinese LLMs, which includes diverse, realistic, and challenging evaluation data. Cheng et al. propose AutoDetect \cite{cheng2024autodetect}, a unified framework for automatically uncovering LLM flaws in a variety of tasks.






\subsection{Evaluation of Generative Models}
\label{sec:evaluation_related_work}

\textbf{Text-to-Image Models.} Recent progress in text-to-image generation \cite{rombach2022high,ramesh2024dalle3} has showcased remarkable capabilities in creating diverse and high-fidelity images based on natural language prompts. These developments underscore the necessity for robust evaluation frameworks that can adequately assess the complexities of generated images.

Early-proposed benchmarks \cite{lin2014microsoft,krizhevsky2017imagenet} primarily focus on assessing image quality and alignment, using automated metrics, such as Frechet Inception Distance (FID) \cite{heusel2017gans}, Inception Score \cite{salimans2016improved}, and CLIPScore \cite{hessel2021clipscore} are commonly used for quantitative assessment of image quality and alignment. These traditional automated evaluation methods cannot analyze compositional capabilities and lack fine-grained reporting, highlights the need for advanced benchmarks that can evaluate the nuanced aspects of image generation.

For Text-to-image alignment, T2I-CompBench \cite{huang2023t2i} serves as a comprehensive benchmark for open-world compositional text-to-image generation. TIFA \cite{hu2023tifa}, integrated into LLMs combined with VQA, facilitates subsequent fine-grained T2I evaluation \cite{cho2023davidsonian, yarom2024you}, enhancing the precision of matching text descriptions with generated images. GenEval \cite{ghosh2024geneval} advances automatic evaluation by incorporating a suite of compositional reasoning tasks. In the follow-up, more comprehensive and scalable benchmarks are established \cite{bakr2023hrs,lee2024holistic,li2024evaluating,lin2024evaluating}. These benchmarks not only leverage human evaluations to enhance the accuracy of assessments but also consider factors like robustness, creativity and counting and creativity. 

As the ethical and societal impacts of image generation models become more pronounced \cite{schramowski2023safe, qu2023unsafe}, researchers have increasingly focused on evaluating these aspects, particularly in the realm of fairness and bias. For fairness and bias evaluation, text-to-image models have been tested for social biases \cite{luccioni2024stable,cho2023dalleval,luo2024bigbenchunifiedbenchmarksocial}, Stereotypes \cite{bianchi2023easily,friedrich2024multilingual,jha2024visage} and dynamic prompt-specific bias \cite{chinchure2023tibet}. FAIntbench \cite{luo2024faintbench} has pioneered a structured approach to these issues by defining specific biases, categorizing them, and measuring each type separately, allowing for more nuanced analysis and mitigation. In the realm of intellectual property, the CPDM dataset \cite{ma2024dataset} stands out as the first work, which facilitates a straightforward evaluation of potential copyright infringement. 




\textbf{Large Language Models.} The advancement of large language models benefits lots of downstream tasks. To better understand LLMs' capability, lots of evaluations are conducted. From the traditional NLP tasks, LLMs are evaluated on sentiment analysis \cite{lopezlira2023chatgpt, zhang2023sentiment, qin2023chatgpt}, language translation \cite{Zhang2023PromptingLL, Kocmi2023LargeLM, Lu2023ErrorAP}, text summarization \cite{qin2023chatgpt, zhang2023benchmarking, gao2023human} and natural language inference \cite{qin2023chatgpt, mckenna2023sources}. With the emergent ability \cite{wei2022emergent}, LLMs perform well in more complex tasks like mathematical or logical reasoning \cite{qin2023chatgpt, frieder2023mathematical, liu2023evaluating2, pan2023logiclm, Liang2024MathChatBM, Cobbe2021TrainingVT, yuan2023large, wei2023cmath, liang2024scemqa}. Moreover, trained by a large training corpus， LLMs are also evaluated to be excellent in various question-answer (QA) benchmarks \cite{zhang2023m3exam, hendrycks2020measuring, Liang2024SceMQAAS, Sun2023SciEvalAM, rajpurkar2016squad, rajpurkar2018know, yang2018hotpotqa, joshi2017triviaqa, jin2019pubmedqa}. Beyond this, LLMs are also assessed in code-related benchmarks \cite{zhong2023chatgpt, liu2023refining, fu2023codeapex, liu2023code, Zhang2024NaturalCodeBenchEC, Peng2024HumanEvalXLAM}.

Furthermore, the use of LLMs extends into various other fields \cite{gu2023xiezhi}, such as computational social science \cite{ziems2023large}, legal tasks \cite{nay2023large, guha2023legalbench, fei2023lawbench}, economy or finance \cite{Zhang2023FinEvalAC, Zhang2024FinSQLML, Xie2023PIXIUAL, Su2024NumLLMNL, li2023chatgpt, islam2023financebench}, psychology \cite{fmc2023, li2024quantifying}, and search and recommendation \cite{fan2023recommender, lei2023recexplainer}. Additionally, assessing LLMs in natural science and engineering reveals their capabilities in areas of general science \cite{guo2023large, Nascimento2023, Chen2024ScholarChemQAUT}, and engineering \cite{pallagani2023understanding, sridhara2023chatgpt, Song2024CSBenchAC}. In the medical domain, LLMs have been tested for their effectiveness in responding to medical queries \cite{Holmes_2023, Samaan2023}, performing medical examinations \cite{GilsonAidan2023, 10.1371/journal.pdig.0000198}, and serving as medical assistants \cite{wang2023llms, Lahat2023}. Moreover, the LLM-based agents are widely evaluated \cite{lin2023agentsims, agentbench}, especially with regard to their ability to use tools \cite{qin2023toolllm, metatool, li2023apibank}. To understand the multilingual capabilities of LLMs, the evaluation also includes multilingual evaluation \cite{Xu2023SuperCLUEAC, li2023cmmlu, Lai2023ChatGPTBE}. Additionally, the evaluation includes measuring the performance of LLMs on text summarization using ROUGE scores and on machine translation using BLEU scores and perplexity.


To facilitate the evaluation, many evaluation protocols and frameworks have been proposed. For instance, the Dyval \cite{zhu2023dyval, Zhu2024DyVal2D} series is a dynamic protocol, where Dyval-1 \cite{zhu2023dyval} aims to construct reasoning data dynamically, and Dyval-2 \cite{Zhu2024DyVal2D} is designed to utilize the probing and judging LLMs to transform an original evaluation problem into a new one automatically. UniGen \cite{wu2024unigen} is a unified framework for textual dataset generation, which ensures the truthfulness and diversity of the generated data at the same time. Moreover, Wang et al. \cite{wang2024benchmark} use a multi-agent framework to realize the evolution of the evaluation dataset. Moreover, AutoBencher \cite{Li2024AutoBencherCS}, an automatic benchmark framework, uses language models to automatically search for datasets that meet the three desiderata: salience, novelty, and difficulty.

LLMs have also emerged as a promising tool for evaluation tasks. For example, Zheng et al. introduced the concept of "LLM-as-a-Judge" \cite{zheng2023judging}, offering a cost-effective alternative to traditional human evaluations \cite{li2024generation}. Additionally, frameworks such as ChatEval \cite{chan2023chateval}, EvaluLLM \cite{EvaluLLM}, and Prometheus \cite{kim2023prometheus, kim2024prometheus} have gained popularity as LLM-powered evaluation methods, further demonstrating the utility of LLMs in this domain.

\textbf{Vision-Language Models.} The fast progress of computer vision along with LLMs has led to the rapid development of VLMs, enabling a wide range of downstream tasks that integrate both visual and linguistic information \cite{li2024survey, huang2024survey}. Various downstream tasks have been proposed, and VLMs are evaluated on object detection~\cite{chen2024taskclip}, image classification~\cite{Kirillov2023SegmentA}, and object tracking~\cite{wu2024single}. These models are also extensively tested in facial recognition~\cite{foteinopoulou2024emoclip}, human pose estimation~\cite{feng2024chatpose}, and optical character recognition (OCR)~\cite{jiao2024enhancing}. Moreover, VLMs have shown exceptional abilities in more advanced tasks such as multiple image scene recognition~\cite{jin2024chat, chen2024lion} and visual question answering (VQA)~\cite{ganz2024question, lee2024visual}. 

In addition, numerous benchmarks concentrate on evaluating the general capabilities of VLMs across all the aforementioned tasks \cite{villa2023behind, chen2024we, liu2023mmbench, li2024seed, song2024milebench, wu2023q, yin2024lamm, liu2024visual, fu2023mme, li2024seedplus}. Particularly, Seed-bench \cite{li2024seed} comprehensively assesses the hierarchical abilities of VLMs. Moreover, several benchmarks focus on testing the reasoning skills of VLMs. 
For instance, \cite{kil2024compbench} assesses their comparative reasoning skills, while \cite{wang2024mementos} evaluates the reasoning abilities of VLMs when processing image sequences. Additionally, there is a significant body of work that emphasizes evaluating mathematical reasoning as well as reasoning in scientific domains \cite{yue2024mmmu, yue2024mmmupro, zhang2024cmmmu, Liang2024SceMQAAS, lu2023mathvista, xiao2024logicvista, zhang2023m3exam}. There is also a substantial body of work that explores VLMs' comprehension abilities, such as relation understanding\cite{nie2024mmrel}, fine-grained concept understanding \cite{peng2024synthesize}, instruction following ability \cite{qian2024mia, li2023empowering}, and dialogue understanding \cite{liu2024mmdu}.

Beyond traditional tasks, VLMs are widely applied in various domains. In autonomous driving, they are used for lane detection, obstacle recognition, etc.~\cite{cui2024survey, tian2024drivevlm, li2024survey, huang2024survey}. In robotics, VLMs are commonly used in the tasks of navigation~\cite{guan2024loczson, Dorbala2022CLIPNavUC, shah2023vint, elnoor2024robotnav, arul2024vlpgnav, weerakoon2024behavbehavior} and manipulation~\cite{ichter2022do, ren2023robots, palme, fangandliu2024moka}. In healthcare, VLMs are evaluated for their performance in medical image analysis, aiding in disease diagnosis through scanned images~\cite{hartsock2024vision, royer2024multimedeval}, same as in numerous AI for science scenarios as in satellite imagery~\cite{li2024vision}. In psychology, VLMs are evaluated in areas such as emotion recognition from facial expressions~\cite{foteinopoulou2024emoclip} and understanding social cues in human interactions~\cite{kim2024understanding}. In legal tasks~\cite{louis2024interpretable}, economy or finance~\cite{lu2024deepseek} and recommendation and personalization~\cite{wu2024survey}, there also exist numerous studies in VLMs to excel expert and robust performance in these fields. Furthermore, some studies investigate the cross-cultural and multilingual capabilities of VLMs \cite{inoue2024heron, baek2024evaluating}.

Several frameworks have been proposed to facilitate a comprehensive evaluation. For example, \cite{yin2024lamm} provides a detailed methodology for constructing multimodal instruction-tuning datasets and benchmarks for VLMs. \cite{cao2024introducing} presents an annotation-free framework for evaluating VLMs. Furthermore, \cite{chen2024mllm} assesses the effectiveness of VLMs in assisting judges across various modalities. For studies on agents in VLMs, several prominent works exist in the literature~\cite{liu2023agentbench, liu2024visualagentbench}. Some benchmarks evaluate the performance of multimodal agents in single environment like household \cite{shridhar2020alfred,shridhar2020alfworld}, gaming \cite{wu2023smartplay}, web \cite{deng2024mind2web,koh2024visualwebarena,jang2024videowebarena}, mobile phone \cite{rawles2024androidinthewild,sun2022meta,rawles2024androidworld} and desktop scenarios \cite{xie2024osworld,bonatti2024windows,kapoor2025omniact}. Chen et al.~\cite{chen2024gui} introduced a comprehensive multimodal dataset specifically designed for agent-based research, while a benchmark survey for evaluating agents driven by VLMs is also studied. Liu et al.~\cite{liu2024visualagentbench} developed the first systematic benchmark for complex spaces and digital interfaces, establishing standardized prompting and data formatting protocols to facilitate consistent evaluation of foundation agents across diverse environments.











\newcolumntype{a}{>{\columncolor{LightGray}}c}
\newcolumntype{b}{>{\columncolor{LightCyan}}c}



\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand\arraystretch{1.1}
\caption{Comparison between \textsc{TrustGen} and other trustworthiness-related benchmarks (Large language models).}
\label{tab:benchhmark_comparison_llm}
\rowcolors{2}{white}{gray!10!white} 
\scalebox{0.88}{
\begin{tabular}{@{}p{3.5cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}|p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}|p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}@{}}
\toprule[1.5pt]
\textbf{Aspect} & \rotatebox[origin=c]{90}{\textbf{Truthful.}} & \rotatebox[origin=c]{90}{\textbf{Safety}} & \rotatebox[origin=c]{90}{\textbf{Fair.}} & \rotatebox[origin=c]{90}{\textbf{Robust.}} & \rotatebox[origin=c]{90}{\textbf{Privacy}} & \rotatebox[origin=c]{90}{\textbf{Ethics}} & \rotatebox[origin=c]{90}{\textbf{Advanced.}} & \rotatebox[origin=c]{90}{\textbf{T2I}} & \rotatebox[origin=c]{90}{\textbf{LLM}} & \rotatebox[origin=c]{90}{\textbf{VLM}} & \rotatebox[origin=c]{90}{\textbf{Dynamic.}} & \rotatebox[origin=c]{90}{\textbf{Diverse.}} & \rotatebox[origin=c]{90}{\textbf{Toolkit}} \\ \midrule
\textbf{\textsc{TrustGen} (ours)} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor \\
\textsc{TrustLLM} \cite{huang2024position} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor \\
HELM \cite{liang2022holistic} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor \\
DecodingTrust \cite{wang2023decodingtrust} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Do-Not-Answer \cite{wang2023donotanswer} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Red-Eval \cite{bhardwaj2023redteaming} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
PromptBench \cite{zhu2023promptbench} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
CVALUES \cite{xu2023cvalues} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
GLUE-x \cite{yang2022glue} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
SafetyBench \cite{sun2023safety} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
ML Commons v0.5 \cite{vidgen2024introducing} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
BackdoorLLM \cite{li2024backdoorllm} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
HaluEval \cite{li2023halueval} & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Latent Jailbreak \cite{qiu2023latent} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
FairEval \cite{wang2023large} & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
OpenCompass \cite{2023opencompass} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
SC-Safety \cite{xu2023sc} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
All Languages \cite{languagessafety} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
HalluQA \cite{chengevaluating} & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
FELM \cite{chen2023felm} & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
JADE \cite{zhang2023jade} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
P-Bench \cite{li2023pbench} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
CONFAIDE \cite{mireshghallah2023llms} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
CLEVA \cite{li2023cleva} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
MoCa \cite{nie2023moca} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
FLAME \cite{huang2023flames} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
ROBBIE \cite{esiobu2023robbie} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
FFT \cite{cui2023fft} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Sorry-Bench \cite{xie2024sorrybenchsystematicallyevaluatinglarge} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Stereotype Index \cite{shrawgi-etal-2024-uncovering} & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
SALAD-Bench \cite{li2024saladbenchhierarchicalcomprehensivesafety}& \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
R-Judge \cite{yuan2024rjudgebenchmarkingsafetyrisk} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
LLM Psychology \cite{li2024quantifying} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
HoneSet \cite{gao2024best} & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
AwareBench \cite{li2024think}  & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
ALERT \cite{tedeschi2024alert}  & \xmarkcolor & \checkmarkcolor& \xmarkcolor  & \xmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor  & \checkmarkcolor& \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Saying No \cite{brahman2024artsayingnocontextual} & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
advCoU \cite{mo2024trustworthyopensourcellmsassessment} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
OR-Bench \cite{cui2024or} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
CLIMB \cite{zhang2024climb} & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
SafeBench \cite{ying2024safebench} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
ChineseSafe \cite{zhang2024chinesesafe}  & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
SG-Bench \cite{mou2024sg} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
XTrust \cite{li2024xtrust} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
\bottomrule[1.5pt]
\end{tabular}}
\vspace{-15pt}
\end{table}

\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{5pt}
\renewcommand\arraystretch{1.1}
\caption{Comparison between \textsc{TrustGen} and other trustworthiness-related benchmarks (Text-to-image models and vision-language models).}
\label{tab:benchhmark_comparison_transposed}
\rowcolors{2}{white}{gray!10!white} 
\scalebox{0.88}{
\begin{tabular}{@{}p{3.5cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}|p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}|p{0.8cm}<{\centering}p{0.8cm}<{\centering}p{0.8cm}<{\centering}@{}}
\toprule[1.5pt]
\textbf{Aspect} & \rotatebox[origin=c]{90}{\textbf{Truthful.}} & \rotatebox[origin=c]{90}{\textbf{Safety}} & \rotatebox[origin=c]{90}{\textbf{Fair.}} & \rotatebox[origin=c]{90}{\textbf{Robust.}} & \rotatebox[origin=c]{90}{\textbf{Privacy}} & \rotatebox[origin=c]{90}{\textbf{Ethics}} & \rotatebox[origin=c]{90}{\textbf{Advanced.}} & \rotatebox[origin=c]{90}{\textbf{T2I}} & \rotatebox[origin=c]{90}{\textbf{LLM}} & \rotatebox[origin=c]{90}{\textbf{VLM}} & \rotatebox[origin=c]{90}{\textbf{Dynamic.}} & \rotatebox[origin=c]{90}{\textbf{Diverse.}} & \rotatebox[origin=c]{90}{\textbf{Toolkit}} \\ \midrule
\textbf{\textsc{TrustGen} (ours)} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor \\
HEIM \cite{lee2023holisticevaluationtexttoimagemodels} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor \\
HRS-Bench \cite{2304.05390} & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Stable Bias \cite{luccioni2024stable} & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
DALL-EVAL \cite{cho2023dall} & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
GenEVAL \cite{ghosh2024geneval} & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
BIGbench \cite{luo2024bigbenchunifiedbenchmarksocial} & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
CPDM \cite{ma2024dataset} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
MultiTrust \cite{zhang2024benchmarkingtrustworthinessmultimodallarge} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor \\
MLLM-Guard \cite{gu2024mllmguard} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor \\
MM-SafetyBench \cite{liu2024mmsafetybenchbenchmarksafetyevaluation} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
UniCorn \cite{tu2023unicornsimagesafetyevaluation} & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \checkmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
BenchLMM \cite{cai2023benchlmmbenchmarkingcrossstylevisual} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Halle-switch \cite{zhai2023halle} & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
Red-Teaming VLM \cite{li2024redteamingvisuallanguage} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor &\checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
JailBreak-V \cite{luo2024jailbreakv28kbenchmarkassessingrobustness} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor &\checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
VLBiasBench \cite{zhang2024vlbiasbenchcomprehensivebenchmarkevaluating} & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
GOAT-Bench \cite{lin2024goat} & \xmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor &\xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
VIVA \cite{hu2024viva} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor &\xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
C$h^{3}$Ef \cite{shi2024assessment} & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor &\xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
MMBias \cite{janghorbani2023multimodal} & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
GenderBias \cite{xiao2024genderbias} & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
MMJ-Bench \cite{weng2024textit} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
SIUO \cite{wang2024cross} & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
AVIBench \cite{zhang2024avibench} & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor &\xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
AutoTrust \cite{xing2024autotrust} & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor & \checkmarkcolor & \xmarkcolor & \xmarkcolor & \xmarkcolor \\
\bottomrule[1.5pt]
\end{tabular}}
%\vspace{-15pt}
\end{table}

\subsection{Trustworthiness-Related Benchmark}
\label{sec:benchmark_related_work}
An increasing amount of efforts have been dedicated to establish %Recent research has focused on establishing
benchmarks for assessing the trustworthiness of GenFMs. They provide frameworks that not only assess current models but also guide future advancements in improving reliability and safety of these technologies. The development of such benchmarks is crucial for fostering collaboration among industry stakeholders to enhance the trustworthiness of GenFMs. 

\textbf{Large Language Models.} Several trustworthiness-related benchmarks have been developed to assess LLMs across various critical dimensions. Notable benchmarks like TrustLLM \cite{huang2024position} and HELM \cite{liang2022holistic} evaluate models based on multiple aspects such as truthfulness, safety, fairness, and robustness, providing a broad view of model reliability. DecodingTrust \cite{wang2023decodingtrust} and Do-Not-Answer \cite{wang2023donotanswer} emphasize safety, privacy, and ethical considerations, aiming to reduce potential harm from model outputs. SafetyBench \cite{sun2023safety} and FairEval \cite{wang2023large} focus specifically on safety and fairness, targeting issues of bias and harmful content. CVALUES \cite{xu2023cvalues} and ML Commons v0.5 \cite{vidgen2024introducing} also contribute to assessing fairness and robustness, while BackdoorLLM \cite{li2024backdoorllm} addresses security by examining vulnerability to backdoor attacks. These benchmarks cover a range of aspects, from privacy and ethical standards to dynamic evaluation across different model types, offering comprehensive insights into the trustworthiness of LLMs. A detailed comparison between \textsc{TrustGen} and related benchmarks on LLMs  is shown in \autoref{tab:benchhmark_comparison_llm}. 

\textbf{Text-to-image models and vision-language models.} When extending evaluations to the vision domain, some benchmarks concentrate on fundamental trustworthiness aspects like HEIM \cite{lee2023holisticevaluationtexttoimagemodels}, which covers truthfulness, safety, fairness, and robustness dimensions, while HRS-Bench \cite{2304.05390} focuses on truthful assessment only. Several benchmarks specialize in specific aspects - for instance, Stable Bias \cite{luccioni2024stable} primarily addresses fairness concerns, while DALL-EVAL \cite{cho2023dall} and GenEVAL \cite{ghosh2024geneval} emphasize truthfulness evaluation. More comprehensive frameworks like MultiTrust \cite{zhang2024benchmarkingtrustworthinessmultimodallarge} and MLLM-Guard \cite{gu2024mllmguard} cover multiple dimensions. Benchmarks like MM-SafetyBench \cite{liu2024mmsafetybenchbenchmarksafetyevaluation} and UniCorn \cite{tu2023unicornsimagesafetyevaluation} focus on safety and privacy considerations, while BenchLMM \cite{cai2023benchlmmbenchmarkingcrossstylevisual} and Halle-switch \cite{zhai2023halle} prioritize robustness testing. More specialized benchmarks include Red-Teaming VLM \cite{li2024redteamingvisuallanguage} and JailBreak-V \cite{luo2024jailbreakv28kbenchmarkassessingrobustness} for security evaluation, GOAT-Bench \cite{lin2024goat} for safety and fairness, and newer frameworks like C$h^{3}$Ef \cite{shi2024assessment} and GenderBias \cite{xiao2024genderbias} that address specific biases and fairness concerns. Trustworthiness-related benchmarks in text-to-image models and vision-language models are shown in \autoref{tab:benchhmark_comparison_transposed}.

\textsc{TrustGen}, distinguishes itself as the most extensive and versatile benchmark, covering all primary trustworthiness aspects: truthfulness, safety, fairness, robustness, privacy, machine ethics, and advanced AI risk. By employing different data construction strategies and modules, \textsc{TrustGen} achieves dynamic evaluation, as well as diverse testing (we will detail these in \textbf{\S\ref{sec:benchmark_design}}). Additionally, it supports a range of GenFMs, including T2I models, LLMs, and VLMs, and introduces various modules to enable the dynamics of the evaluation.







