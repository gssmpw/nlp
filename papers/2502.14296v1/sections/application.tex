\clearpage
\section{Trustworthiness in Downstream Applications}
\label{sec:application}

% \subsection{AI for Science (Kehan)}


% He et al. propose a novel red-teaming benchmark SciMT-Safety, which aims to evaluate LLMs' trustworthiness when applied in the scientific domains \cite{he2023control}. 


\subsection{Medicine \& Healthcare}

The integration of generative foundation models into medical applications represents a significant leap in healthcare innovation, enabling the automation of complex diagnostic and decision-making tasks while improving efficiency and accessibility~\cite{zhou2024ttt, yang2023harnessing, hao2025retrospective}. Medical agents powered by LLMs and VLMs are increasingly deployed to support clinicians and patients in diagnostics, treatment planning, and patient monitoring \cite{hao2024advancing}. However, their adoption in clinical environments demands consistent reliability and transparency in dynamic clinical environments. Recent studies highlight the promise of multi-modal medical agents combining text and image inputs to deliver accurate and context-aware diagnostic assistance~\cite{dai2023ad}. Despite these advances, validating model performance in rare or edge-case scenarios remains a critical challenge. Addressing these issues requires real-time monitoring systems to continuously evaluate agent outputs against pre-established clinical safety standards, ensuring alignment with best practices and regulatory requirements~\cite{ma2024eye}.

The use of generative models in medical video generation, as explored in~\cite{li2024echopulse, reynaud2024echonet, reynaud2023feature}, introduces new opportunities for advancing medical education, training, and diagnostics. By synthesizing high-fidelity videos that visualize procedures, disease progression, or anatomical changes, these models can enhance understanding and decision-making~\cite{liu2024fine}. However, generated medical content must accurately reflect clinical realities to avoid misleading practitioners or trainees. Techniques such as anomaly detection and privacy-preserving model architectures are essential to mitigate risks while maintaining the fidelity and utility of the generated content~\cite{yan2024biomedical}.

Automated medical report generation is another transformative application of generative models, particularly in fields such as radiology, pathology, and cardiology~\cite{liu2023tailoring, li2024artificial}. These systems can analyze medical images and patient data to draft detailed reports~\cite{wu2023exploring}, reducing clinician workload and improving diagnostic turnaround times. However, ensuring the accuracy and interpretability of these reports is critical. To enhance trustworthiness, researchers have proposed validation pipelines that cross-reference generated reports with ground-truth annotations and enable peer review by domain experts~\cite{liu2023artificial}. Additionally, incorporating explainability features allows users to trace the reasoning behind generated conclusions, fostering greater confidence in the system's outputs~\cite{liu2023evaluating}.

Algorithmic bias in generative foundation models for medical use also represents another pressing concern. Biases in training data, such as underrepresentation of certain demographics, can result in models that provide inaccurate or unfair outcomes for specific patient groups~\cite{schaich2016own, larrazabal2020gender}. Addressing algorithmic bias requires building more representative datasets and implementing continuous fairness evaluations to ensure equitable performance across diverse populations.

Addressing these challenges has involved various research efforts that demonstrate promising initial progress. Real-time monitoring systems have been proposed for clinical safety~\cite{liu2023artificial}, while privacy-preserving architectures have been implemented to enhance data security during the training and deployment of generative models~\cite{alberts2023large}. Nonetheless, significant work remains to comprehensively address the reliability, fairness, and privacy concerns intrinsic to these systems.



\subsection{Embodiment}

% The embodiment of AGI is the next step in developing intelligent systems, enabling them to seamlessly interact with the physical world and autonomously perform complex, previously unseen tasks. This advancement will profoundly impact daily life, liberating human labor by automating routine tasks, increasing productivity, and allowing people to focus on more creative and meaningful endeavors. However, as these systems gain autonomy, ensuring their safety and trustworthiness is paramount. Embodied AGI must be designed with robust safeguards to prevent harm, adhere to ethical principles, and build user confidence through transparent and reliable behavior.

% The initial step towards embodied AGI begins within simulation~\cite{li2022behavior, robothor, puig2023habitat3, Matterport3D}, where virtual environments provide a controlled and scalable platform for training intelligent agents to interact, learn, and adapt before transitioning to real-world applications~\cite{liu2024aligningcyberspacephysical}. For example, Voyager~\cite{wang2024voyager} is the first LLM-powered embodied agent in Minecraft that autonomously explores, learns diverse skills, and makes discoveries without human input. 
% % This method interacts with GPT-4 via black box queries without model fine-tuning. The skills learned by the LLM agent through interaction with the virtual environments are temporally extended, interpretable, and compositional, enabling rapid ability growth without catastrophic forgetting.
% In addition, Eureka~\cite{ma2024eureka} is powered by LLMs like GPT-4 and designed to create reward functions for complex manipulation tasks such as pen spinning. It uses zero-shot generation, code-writing, and in-context learning to optimize rewards, outperforming human-engineered rewards in reinforcement learning tasks. Despite these advances, ensuring the safety of simulation-trained agents when transitioning to real-world environments is critical. Mechanisms to validate agent behavior, detect anomalies, and prevent misuse must be rigorously implemented to ensure trustworthiness and mitigate unintended consequences. 
% \tian{talk about some safety papers} 



% % The more challenging step towards embodied AGI is the integration with robotic systems
% The more challenging frontier in embodied AGI lies in integration with robotic systems, enabling tasks such as object manipulation~\cite{jiang2023vima, zitkovich2023rt, fangandliu2024moka, palme}, robot navigation~\cite{elnoor2024robotnav, guan2024loczson, shah2023vint, arul2024vlpgnav}, autonomous driving~\cite{10531702}, and other physical interactions for real-world applications~\cite{Guan2021TNSTT, 10.1007/s10514-023-10113-9}. This integration allows embodied AGI systems to perform tasks with reasoning skills, enabling them to interact with the real world and adapt to dynamic, unpredictable environments.
% For instance, VIMA~\cite{jiang2023vima} explores the application of prompt-based learning in robotic manipulation, where diverse tasks such as one-shot imitation, language instruction following, and visual goal-reaching are unified under a multimodal prompt framework that interleaves text and visual tokens. 
% % This method designs a transformer-based robot agent, which processes the prompts and outputs motor actions autoregressively, demonstrating strong scalability and data efficiency. 
% Additionally, ViNT\cite{shah2023vint} (Visual Navigation Transformer) introduces a foundation model designed to bring the advantages of general-purpose pre-trained models to vision-based robotic navigation. Trained with a general goal-reaching objective and a transformer-based architecture, ViNT learns navigational affordances and efficiently adapts to various downstream tasks, demonstrating its effectiveness as a foundation model for mobile robotics.
% % Zhou et al.~\cite{10531702} provide a comprehensive survey on various VLM applications in autonomous driving, including tasks such as image captioning, pedestrian detection, tracking, segmentation, retrieval, planning and control, data generation and augmentation, and end-to-end autonomous driving.
% As these systems mature, ensuring their safety and trustworthiness becomes essential to prevent harm in real-world deployments. Rigorous validation processes, transparency in decision-making, and adherence to regulatory standards will be critical in fostering public trust and ensuring the reliable operation of embodied AGI systems.

% To address those challenges, Mullen et al.~\cite{Mullen2024DontFT} introduces the SafetyDetect dataset, comprising 1,000 anomalous home scenes designed to train embodied agents in identifying unsafe or unsanitary conditions. By leveraging large language models (LLMs) and scene graphs that map object relationships, their approach enables agents to effectively detect anomalies such as unattended stoves or accessible poisons for real-world usage.



The embodiment of generative foundation models is the next step in developing intelligent systems, as it enables these systems to seamlessly interact with the physical world and carry out complex tasks they have not encountered before. This advancement will profoundly impact daily life, liberating human labor by automating repetitive tasks, increasing productivity, and allowing people to focus on more creative and meaningful activities. The initial step towards embodied AGI begins within simulation~\cite{li2022behavior, robothor, puig2023habitat3, Matterport3D}, where virtual environments provide a controlled and scalable platform for training intelligent agents to interact, learn, and adapt before transitioning to real-world applications~\cite{liu2024aligningcyberspacephysical}. For example, Voyager~\cite{wang2024voyager} is the first LLM-powered embodied agent in Minecraft that autonomously explores, learns diverse skills, and makes discoveries without human assistance. The more challenging step towards embodied AI is the integration with robotic systems. This step enables these systems to handle tasks such as object manipulation~\cite{jiang2023vima, zitkovich2023rt, fangandliu2024moka, palme}, robot navigation~\cite{elnoor2024robotnav, guan2024loczson, shah2023vint, arul2024vlpgnav}, autonomous driving~\cite{10531702}, and other physical interactions for real-world applications~\cite{Guan2021TNSTT, 10.1007/s10514-023-10113-9}. This integration allows embodied AI systems to perform tasks with reasoning skills, enabling them to interact with the real world and adapt to dynamic, unpredictable environments.
For instance, VIMA~\cite{jiang2023vima} explores the application of prompt-based learning in robotic manipulation, where various tasks such as one-shot imitation, language instruction following, and visual goal-reaching are unified under a multimodal prompt framework that combines text and visual tokens. 

However, as these systems gain autonomy, ensuring their trustworthiness~\cite{Naihin2023TestingLM, Deletang2021CausalAO, wu2024safety, 10.5555/3463952.3464159, Mullen2024DontFT} becomes crucial. Embodied AI must be designed with robust safeguards to prevent harm, adhere to ethical principles, and build user trust through transparent and reliable behavior. Mechanisms to validate agent behavior, detect anomalies, and prevent misuse must be carefully developed to ensure trustworthiness and mitigate unintended consequences. 
For virtual agents, Naihin et al.~\cite{Naihin2023TestingLM} address the challenges of safely evaluating autonomous agents operating on the open internet. The authors propose a framework where the agent's actions are monitored by a context-sensitive system that enforces strict safety boundaries and can halt tests upon detecting unsafe behavior. This approach aims to prevent potential harm during testing and mitigate risks arising from interactions with real-world and potentially malicious actors. 
Deletang et al.~\cite{Deletang2021CausalAO} introduce a method to investigate the causal processes that drive the behavior of artificial agents. They emphasize that understanding these processes is essential for the safe deployment of machine learning systems, which are becoming increasingly powerful but often unpredictable and difficult to understand.
For real-world robotic applications, Wu et al.~\cite{wu2024safety} examine the robustness and safety challenges associated with integrating LLMs and VLMs into robotic systems. While LLMs and VLMs significantly enhance robotic capabilities in tasks like manipulation and navigation, they are shown to be vulnerable to adversarial inputs, which can lead to significant drops in performance.
Knott et al.~\cite{10.5555/3463952.3464159} aim to address the challenge of ensuring that agents trained via deep reinforcement learning can effectively collaborate with humans in a variety of real-world scenarios. They propose that AI designers identify potential edge cases in both human behaviors and environmental conditions, creating specific tests to verify that agent responses in these scenarios are appropriate.
To overcome the challenges encountered in home environments, Mullen et al.~\cite{Mullen2024DontFT} introduce the SafetyDetect dataset, which includes 1,000 anomalous home scenes designed to train embodied agents in identifying unsafe or unsanitary conditions. By leveraging LLMs and scene graphs that map object relationships, it enables agents to effectively detect anomalies—such as unattended stoves or accessible poisons—paving the way for safer real-world deployments of generative foundation models.

\subsection{Autonomous Systems}

Recent advances in GenFM have profoundly impacted autonomous driving, enabling large-scale perception and decision-making pipelines that are more adaptive to real-world complexity. 
Early works primarily focused on specialized architectures (e.g., ConvNets~\cite{he2016deep,lang2019pointpillars} or Transformers~\cite{alexey2020image,li2022bevformer}) for tasks like object detection~\cite{li2021free} and trajectory planning \cite{teng2023motion}, but a growing trend now integrates GenFMs—pre-trained on vast, diverse datasets—to enhance generalization across varied driving conditions \cite{chen2022pali,gpt4v,li2024llava,hu2023planning}. 
These models often combine vision and language modalities, aiming to provide a richer semantic understanding of traffic scenes while handling complex reasoning about dynamic agents. 
However, researchers have identified pressing trustworthiness concerns, including vulnerability to adversarial inputs~\cite{zhou2020deepbillboard} (e.g., manipulated traffic signs~\cite{pavlitska2023adversarial}), hallucinations in perception outputs~\cite{chen2024multi}, and degraded performance in out-of-distribution scenarios~\cite{nitsch2021out} (e.g., extreme weather \cite{bijelic2018benchmarking,zhu2024mwformer}). To address these issues, current literature emphasizes \emph{scenario-based} and \emph{adversarial} testing protocols that expose failure modes \cite{li2024scenarionet,ding2023survey}. Other studies have explored perceptual enhancement techniques to handle lighting variations~\cite{li2024light} or adverse weather conditions~\cite{zhu2024mwformer}, making autonomous driving models more robust against real-world scenarios.  Further, large-scale testbeds (e.g., MCity~\cite{feng2023dense}) now explore closed-loop evaluations of foundation models~\cite{shao2024lmdrive}, allowing real-time monitoring of system decisions and enabling rapid iteration on safety-critical edge cases \cite{ding2023survey}. Together, these studies lay the groundwork for deploying foundation models in autonomous driving while underscoring the need for continuous research into reliability, transparency, and alignment with ethical and regulatory standards~\cite{krugel2024risk}. The emergence of multimodal LLMs for the autonomous driving generalists~\cite{cui2024survey}, such as  DriveVLM~\cite{tian2024drivevlm}, Dolphins~\cite{ma2025dolphins}, DriveLM~\cite{sima2025drivelm}, EMMA~\cite{hwang2024emma} and OpenEMMA~\cite{xing2024openemma}, has given rise to new challenges of the trustworthiness of GenFMs. To investigate this new trend, Xing et al.introduce AutoTrust~\cite{xing2024autotrust}, a comprehensive trustworthiness benchmark for MLLMs for driving, facilitating more trustful and robust driving foundation models towards safer L4-level autonomy.

Despite the great promise of deploying capable GenFM models for autonomous vehicles~\cite{hwang2024emma}, fundamental challenges exist for single-agent systems. 
Individual vehicles often face occlusion and maintain only a narrow line of sight, potentially leading to dangerously incomplete scene understanding~\cite{wang2020v2vnet}. By contrast, a promising direction is to employ vehicle-to-everything (V2X) communication technologies that enable multiple agents (e.g., vehicles, infrastructure) to ``talk'' to each other and pool diverse visual cues shared from multiple nearly autonomous agents~\cite{wang2020v2vnet,xu2022v2x,xu2022cobevt,li2022v2x,xu2022opv2v,xu2023v2v4real,li2024comamba}. Still, V2X collaboration systems suffer from multiple trustworthy issues when deploying in the real-world scenarios, including sensor noise, localization errors, communication latency, safety, and privacy concerns. For instance, V2X-ViT~\cite{xu2022v2x} has employed a vision transformer with heterogeneous multi-agent self-attention and multi-scale self-attention to handle the challenges of asynchronous information sharing, pose errors, and heterogeneity.
CoBEVT~\cite{xu2022cobevt} presents an efficient transformer for different perception modalities for feature fusion.
More recent works like HEAL~\cite{lu2024extensible} and STAMP~\cite{anonymous2025stamp} explore effective solutions to tackle the heterogeneity and scalability problems in a collaborative network.
Moreover, real-time reliability~\cite{li2024comamba}
 still hinges on mitigating sensor noise, latency, and localization drift—challenges which become even more critical under adversarial conditions~\cite{xiang2023v2xp} or extreme weather conditions~\cite{li2024v2x,zhu2024mwformer}.
The security and privacy issues~\cite{yoshizawa2023survey} in V2X communication systems are also gaining increasing attention in the GenFM era, especially when the GenFMs are more vulnerable to adversarial attacks and tend to memory privacy data.
Altogether, this line of research underscores that trustworthiness in V2X requires more than just advanced models—it demands robust sensor synchronization, secure communication, and adaptive design principles that can withstand unpredictable conditions.

Beyond localized V2X cooperative scenarios, the adoption of GenFMs at a system-wide scale introduces broader challenges in \emph{security}, \emph{privacy}, \emph{safety}, and \emph{robustness}, each of which underpins public trust and regulatory compliance \cite{liang2024trustworthy}. For instance, security breaches pose a critical threat when massive data—often containing sensitive traveler and infrastructure information—flows through AI-enabled transportation platforms \cite{10605184}. Meanwhile, preserving privacy under large-scale federated or decentralized learning schemes for GenFMs is of great significance to ensure data confidentiality~\cite{zhang2024advancing}. Equally vital is the robustness of transportation systems against adversarial and jailbreak attacks~\cite{wang2024data,yi2024jailbreak}. Taken together, these studies underscore the necessity for a holistic framework that supports encryption, federated aggregation, and adversarial defense at every layer of an intelligent transportation network—providing GenFMs with the secure, fair, and reliable foundation needed to transform future mobility landscapes.

\subsection{Copyright \& Watermark}
GenFMs, especially those producing high-quality text, images, or audio, may inadvertently replicate or generate content closely resembling copyrighted material from their training data, raising legal and ethical concerns \cite{totonews, Misinformation1, Misinformation2}. Recent high-profile lawsuits have brought theoretical concerns about copyright and GenFMs into practical focus. These developments emphasize the urgency of the problem and the need for frameworks to address intellectual property issues in the course of training and deployment of GenFMs \cite{concordmusicgroup, the-new-york-times-company-v-microsoft-corporation}.
% \textcolor{red}{[ADD REFERENCE TO THESE CASE LAW: Concord Music Group, Inc. v. Anthropic PBC (3:23-cv-01092) \href{https://www.courtlistener.com/docket/67894459/concord-music-group-inc-v-anthropic-pbc/}{(Link here)} + The New York Times Company v. Microsoft Corporation (1:23-cv-11195)] \href{https://www.courtlistener.com/docket/68117049/the-new-york-times-company-v-microsoft-corporation/}{(Link here)})]}.

For LLMs, recent works have examined LLMs' potential copyright infringement through text copying \cite{chang2023speak,karamolegkou2023copyright, schwarzschild2024rethinking, hacohen2024not}. They are developing tools and frameworks to address potential copyright violations these models may incur due to their training on expansive and diverse datasets. Li et al. \cite{li2024digger} introduced a method to detect whether copyrighted text has been used in an LLM's training data. Wei et al. \cite{wei2024evaluating} proposed an evaluation framework CoTaEval to assess the effectiveness of copyright takedown methods. Mueller et al. \cite{mueller2024llms} quantified the extent of potential copyright infringements in LLMs using European law. Using copyrighted fiction books as text sources, Chen et al. \cite{chen2024copybench} created CopyBench, a benchmark specifically designed to measure both literal and non-literal copying in LLM outputs. 

Several approaches have been proposed to address copyright concerns in LLMs. One category involves machine unlearning \cite{zhuang2024uoe, yang2024cliperase, liu2024rethinking}, which removes copyrighted text from training data \cite{liu2024rethinking,yao2023large,hans2024like}, though it often leads to performance degradation \cite{min2023silo}. Another method focuses on decoding strategies, where logits are modified during generation to avoid producing copyrighted content \cite{ippolito2023preventing,xu2024safedecoding}. Liu et al. \cite{liu2024shield} introduced agent-based intellectual property protection mechanisms to guard against malicious requests, including jailbreaking attacks. Additionally, watermarking techniques have been explored as a means of intellectual property protection, embedding identifiable markers into generated content \cite{kirchenbauer2023watermark,zhang2024remark,wang2023towards,pan2024markllm,li2024double}.

The high fidelity and authenticity of content generated by text-to-image models have raised significant copyright concerns. Carlini et al. \cite{carlini2023extracting} and Somepalli et al. \cite{somepalli2023diffusion, somepalli2023understanding} demonstrate that memorization occurs in text-to-image diffusion models. Replication is more frequent in models trained on small to medium-sized datasets. In contrast, models trained on larger and more diverse datasets, such as ImageNet, exhibit minimal or undetectable replication \cite{somepalli2023diffusion}.
 
To address copyright infringement in diffusion models, Vyas et al. \cite{vyas2023provable} proposed a method to prevent the replication of sensitive training images. Wen et al. \cite{wen2024detecting} focused on detecting abnormal prompts that could trigger the generation of training images. Ma et al. \cite{ma2024could} conducted a practical analysis of memorization in text-to-image diffusion models. Similar to LLMs, watermarking techniques in diffusion models \cite{cui2023diffusionshield, zhao2023recipe, cui2023ft, fernandez2023stable, lei2024diffusetrace, xiong2023flexible}, which embed identifiable patterns or signals into generated content, offer a means to ensure traceability and attribution.

Copyright protection in GenFMs remains an evolving challenge, encompassing issues of both data and model security. As this field advances, copyright concerns are expected to gain heightened attention and resources from both industry and academia in the near future.

\subsection{Synthetic Data}
GenFMs have become increasingly dependent on synthetic data generation to address data scarcity and expand their capabilities \cite{tan2024large}. This methodology has demonstrated particular effectiveness across multiple domains: instruction tuning~\cite{wang-etal-2023-self-instruct, xu2023wizardlm, li2023reflectiontuning,li2024synthetic, du2023makes,li-etal-2024-selective,li2024mosaic}, code generation~\cite{gunasekar2023textbooks, wei2023magicoder}, and complex reasoning tasks~\cite{yue2024mammoth, zhu2023dyval, yu2023metamath, lei2023s3eval, zhang2024darg}. Several groundbreaking approaches have emerged for generating high-quality synthetic data. Self-Instruct~\citep{wang-etal-2023-self-instruct} established the foundation for automated instruction generation, while Constitutional AI~\cite{bai2022constitutional} introduced innovative recursive refinement techniques for creating synthetic conversations. Evol-Instruct~\cite{xu2023wizardlm} further advanced the field through its iterative system for enhancing instruction complexity. In addressing reasoning capabilities, methods such as DyVal~\cite{zhu2023dyval} and DARG~\cite{zhang2024darg} have pioneered the use of directed acyclic graph structures to generate sophisticated training and evaluation samples.

The development of synthetic data has become particularly crucial for safety alignment of LLMs, given the intensive resource requirements for manually collecting labeled datasets that include both benign instructions and their harmful variants. This field has seen significant evolution, beginning with the foundational work of Xu et al.\cite{xu2021bot}, who developed a collaborative human-bot framework for collecting harmful conversation examples. The scope expanded through Gehman et al.'s\cite{gehman2020realtoxicityprompts} contribution of 100K toxic prompts. Subsequent research has prioritized improving both the quality and efficiency of safety-aligned data generation. Safe RLHF~\cite{safe-rlhf} made key advances by decomposing human preferences into separate helpfulness and harmlessness dimensions, while BeaverTails~\cite{beavertails} contributed a comprehensive safety-annotated dataset encompassing more than 330,000 QA pairs and 360,000 expert comparisons. Safer-Instruct \cite{shi-etal-2024-safer} introduces a novel pipeline leveraging reversed instruction tuning and expert model evaluation to efficiently generate high-quality synthetic preference data. To address efficiency challenges, Aligner~\cite{ji2024aligner} introduced an innovative approach using smaller, alignment-pretrained LLMs to generate preference data for RLHF/DPO training. Neill et al.\cite{o2024guardformer} developed a systematic pipeline for generating safety-oriented data specifically for training harm detection systems. Hammoud et al.\cite{hammoud2024model} created an integrated approach that generates and incorporates synthetic safety data during model merging optimization to maintain alignment. Sreedhar et al.\cite{sreedhar2024canttalkaboutthis} established a comprehensive framework for generating synthetic dialogues across diverse domains, demonstrating improved instruction following and safety alignment through their synthetic datasets. Most recently, Data Advisor\cite{wang-etal-2024-data} introduced dynamic optimization techniques specifically designed for safety-aligned synthetic data generation. Critic-RM~\cite{yu2024self} leverages synthetic critiques for reward modeling, achieving 3.7\%-7.3\% improvement in preference prediction accuracy.

\subsection{Human-AI Collaboration}
The integration of GenFMs into human-AI collaboration holds the promise of significantly enhancing productivity and driving innovation across diverse sectors. Acting as collaborative partners, GenFMs can streamline complex tasks by combining their computational power with human expertise. These include co-developing software architecture \cite{10.1145/3593434.3593468, si2024design2codebenchmarkingmultimodalcode, peng2023impactaideveloperproductivity}, supporting educational coaching \cite{Nikolopoulou_2024, wang-demszky-2023-chatgpt, hao2024outlining}, fostering creativity in brainstorming \cite{Memmert2024BrainstormingWA, si2024can}, co-authoring creative works \cite{10.1145/3613904.3642134, ippolito2022creativewritingaipoweredwriting, shahid2024examininghumanaicollaborationcowriting, ye2022neuralstoryplanning}, enhancing artistic and design processes \cite{10.1145/3544549.3585680, 10.1145/3519026}, and improving efficiency in data annotation \cite{li-etal-2023-coannotating, shi2024wildfeedbackaligningllmsinsitu, 10.1145/3586183.3606776}. By augmenting human creativity and automating repetitive tasks, GenFMs enable individuals and teams to focus on higher-level problem-solving and innovation, making them invaluable collaborators across a wide range of applications.

However, ensuring the trustworthiness of these human-AI collaborations is paramount for maximizing their benefits while mitigating potential risks. A crucial strategy in this context is trust calibration—a systematic approach that enables users to accurately assess when and to what extent they can rely on a model's outputs. This involves fostering explainability and interpretability to ensure GenFMs provide transparent outputs that humans can understand and validate. Trust calibration can occur during training by teaching models to express uncertainty or refuse answers when faced with problems beyond their competence \cite{cheng2024can, shi-etal-2024-safer, brahman2024the, lin2024flame, zhang-etal-2024-self, liang-etal-2024-learning, yang2024alignment, zhang2024rtuninginstructinglargelanguage}. At inference time, this can involve techniques such as estimating uncertainty with probability-based measures \cite{lin2022teaching, tian-etal-2023-just, kadavath2022languagemodelsmostlyknow}, providing verbalized confidence scores \cite{lin2022teaching, tian-etal-2023-just, kadavath2022languagemodelsmostlyknow, zhou-etal-2024-relying, xiong2024can}, employing consistency-based methods \cite{slobodkin-etal-2023-curious, zhao-etal-2024-knowing, wang2023selfconsistency, cao-etal-2024-defending, cole2023selectively, li-etal-2023-coannotating}, or probing the model's internal states \cite{slobodkin-etal-2023-curious, kadavath2022languagemodelsmostlyknow, chen2024inside, bhardwaj-etal-2024-language}. Beyond training and inference, trust can also be enhanced through transparency in data and model usage \cite{10.1145/3637396}, such as using model cards \cite{Mitchell_2019} to inform users about the training process, data sources, and limitations of the GenFM. 

Complementing trust calibration, feedback mechanisms play a critical role in refining human-AI collaboration dynamics over time. These mechanisms enable iterative improvements through both user-driven and automated feedback. User-driven feedback involves human collaborators providing corrections, preferences, or assessments of the model’s outputs, which can inform adaptive fine-tuning \cite{ouyang2022training, shaikh2024showdonttellaligning, wu2024aligningllmsindividualpreferences, li2024personalizedlanguagemodelingpersonalized}. Automated feedback leverages real-time interaction data to dynamically adjust model behaviors and outputs \cite{shi2024wildfeedbackaligningllmsinsitu, lin-etal-2024-interpretable}. By integrating these approaches, we can enhance the reliability, transparency, and overall effectiveness of GenFMs, fostering a productive and sustainable foundation for human-AI collaboration across diverse applications.

\subsection{Social Science}
Generative models are widely utilized in the social science domain \cite{pawar2024survey, zhang2024affective, bail2024can, wang2025limits}, including applications such as social experiment simulations, sentiment analysis, and modeling social behaviors. With the increasing capabilities of generative models, we envision more interplay between social science and generative models. At the current stage, we have already seen a reciprocal relationship between generative AI and social science.

The versatile behaviors of generative models lead to trustworthiness issues. Many social science concepts are used in research to enhance or deepen the understanding of generative models. Concepts such as values and morality are introduced to study LLMs with the goal of responsible integration into public-facing applications. For instance, Ren et al. \cite{ren-etal-2024-valuebench} investigate value orientation in LLMs, while Scherrer et al. evaluate the moral decision-making of LLMs \cite{scherrer2023evaluating}. In addition, Li et al. \cite{li2024quantifying} evaluate them from the perspective of psychology and use a reliability framework to enhance the interpretation of results. Similar efforts look into different aspects of psychology, including personality traits and personal values \cite{kovavc2024stick}. These social science-motivated LLM studies provide a new lens to understand the versatile behaviors and trustworthiness of LLMs.

At the same time, LLMs have been incorporated into social science research. A notable example is generative agents \cite{park2023generative}, which is a fully automated sandbox environment powered by LLMs, where each agent is an LLM role-playing a different designated persona. Such environments have the potential to replace some aspects of human subjects’ involvement. Similar efforts examine the reliability of LLM-based simulations and propose a new alignment method to mitigate the reliability issue \cite{huang2024social}. LLMs have also demonstrated significant utility in political science - an important subfield of social science, by automating tasks like election prediction, sentiment analysis, and misinformation detection \cite{li2024political}. They process legislative documents, speeches, and surveys at scale, providing timely insights into voter behavior, ideological trends, and policy impacts. Their zero-shot and few-shot capabilities enable efficient sentiment analysis and ideological classification, even with minimal training data, making them essential for real-time political analysis \cite{kuila2024deciphering, ibrahim2024analyzing}. However, the deployment of LLMs in political science highlights the importance of ensuring trustworthiness, particularly in addressing biases related to political identity. These biases, often rooted in training corpora, can skew predictions or reinforce stereotypes. For example, Yu et al. \cite{yu2024largeelection} demonstrated that simple demographic or time-dependent prompting pipelines for election predictions often exhibit political skewness, favoring one party disproportionately. It is critical to mitigate inherent biases to ensure that LLMs remain reliable tools for social science research, fostering trustworthy in their applications.






\subsection{Law}
The integration of GenFMs into the legal field offers transformative possibilities for streamlining workflows and enhancing efficiency in tasks such as drafting and reviewing documents, legal research, and client communications. Also, GenFMs can help with preliminary and basic legal questions, increasing accessibility to legal services by the broad public, especially for underserved communities, and expanding access to justice \cite{simshaw2022access, marwala2024artificial, li2024legalagentbench}. However, the high stakes and sensitivity of legal work require such systems to operate with exceptional trustworthiness, especially in aspects related to accuracy and confidentiality. 

Client confidentiality is a sacred principle in the legal field, as client information could include extremely sensitive issues with commercial, criminal, or personal significance \cite{hazard1978historical}. Integrating GenFMs into legal practice introduces the risk of unintended data exposure, especially with cloud-based GenFMs, which is most often the top-tier tool. For instance, when firms utilize LLMs, they must rely on a third party (e.g., OpenAI, Anthropic), which may advertently (i.e., training) or inadvertently (i.e., security measures) use this data and compromise client confidentiality. Ensuring confidentiality requires deploying GenFMs within secure, privacy-preserving architectures, such as on-premise systems with controlled access. However, such infrastructures often require major financial and technical investment, rendering local models inaccessible for most law firms, particularly small- and medium-sized ones.

Another pressing challenge is the need for enhanced accuracy standards. In the legal realm, the reliability of outputs is paramount, as even minor errors or deviations can have significant consequences. A particularly concerning issue is the phenomenon of model hallucinations. The legal profession has already witnessed the ramifications of such errors; in one prominent case, a lawyer relied on an LLM to draft a court submission, which included fabricated case citations and resulted in professional sanctions and adverse reputational effects \cite{legg2024generative}. 
 
Some solutions have emerged to address these issues, primarily in the form of law-specific generative tools designed to cater to the unique needs of the legal domain, such as LexisAI and Co-counsel. However, research indicates that these solutions remain far from perfect. Similarly, Dahl et al. studied the legal hallucination in LLMs and showed that LLMs hallucinated at least 58\% of the time \cite{dahl2024large}. Some found that AI-powered tools will hallucinate between 17\% and 33\% of the time during law analysis \cite{magesh2024hallucination}. The lack of appropriate solutions for the legal field makes AI adoption by practitioners more difficult and leaves them hesitant to rely on such tools, especially for critical tasks. 

Some other issues in the intersection of AI tools and law were previously highlighted. Fei et al. propose LawBench, aiming to evaluate the performance of LLMs \cite{fei2023lawbench}, which reveals that some LLMs exhibit exaggerated safety faced with law-related queries. Another law-specific benchmark is LegalBench \cite{guha2024legalbench}. Moreover, Grossman et al. raise the concern about whether litigation costs will dramatically increase as parties are forced to hire forensic experts to address AI-generated evidence \cite{grossman2023gptjudge}, as the AI-generated content is hard to detect \cite{zhang-etal-2024-llm}. In response to these challenges, Hou et al. \cite{hou2024gaps} investigated when can machine-generated legal analysis be evaluated as acceptable. They further propose a taxonomy of gaps and develop detectors to analyze the sources of legal hallucinations. In legal question-answering, Trautmann et al. \cite{trautmann2024measuring} assessed the groundedness of AI-generated responses in the aspect of accuracy and trustworthiness.

Given the current state of generative models in the legal domain, the need for trustworthiness benchmarks is significant. The relevance of TrustGen—especially due to its focus on truthfulness, fairness, and privacy—could be particularly high for legal settings, supporting assessing the suitability of generative models for this field and helping professionals understand which tasks and tools could be harnessed in their practice. 

\subsection{Others Applications}
Apart from the above domain, the rise of GenFMs introduces a range of complications that can impact their reliability and ethical considerations in various professional fields.

Even though LLMs are utilized for economic simulation and analysis \cite{zhang2024ai, nie2024cfinbench}, recent work unveils the economic bias of LLMs from the perspectives of data selection and fine-tuning. Ross et al. \cite{ross2024llm} proposed a utility theory paradigm at the core of modern economic theory as an approach to evaluate the economic biases of LLMs. Moreover, Zhong et al. \cite{zhong2024gender} found that LLMs can inadvertently reinforce gender stereotypes, even without explicit gender markers, particularly in AI-driven financial systems. This indicates a need for more nuanced approaches to training and fine-tuning LLMs to prevent the perpetuation of such biases. In addition, for generative search engines, the integration of LLMs presents critical challenges to their trustworthiness, particularly the security problem exposed by ranking manipulation attacks \citep{pfrommer2024ranking, nestaas2024adversarial, hu2025dynamics}. These attacks take advantage of the susceptibility of LLMs to minor input variations by embedding deceptive instructions or manipulated content within webpages or documents. As a result, attackers can compromise the reliability of search results by influencing LLMs to prioritize their content or products unfairly over others \citep{aggarwal2024geo}. In education, the use of GenFMs brings additional ethical considerations, including fairness and safety. Researchers have discussed concerns about fairness, safety, and ethical considerations \cite{ZYLOWSKI2024EVA, bhandari2023trustworthiness, mittal2024comprehensive,slama2024three}. 



Concerns about the trustworthiness of GenFMs extend beyond specific applications. Issues such as training data biases, copyright infringement, and the potential devaluation of artistic creativity are highlighted in studies \cite{10.1145/3597512.3597528, garcia2024paradox, piskopani2023responsible, al2024ethical}. In the realm of edge computing, the move towards integrating AI with emerging technologies like 6G networks introduces novel challenges and opportunities. Li et al. introduced TrustGAIN, a novel paradigm for trustworthy AIGC in 6G networks, to ensure trustworthy large-scale AIGC services in future 6G networks \cite{li2024trustworthy}. Li et al. \cite{li2024ai} proposed an AI-driven edge learning framework for defect detection, emphasizing the need for efficient and reliable AI applications at the edge. 
These issues underscore the complex ethical landscape surrounding the use of generative models.



% Overall, while GenFMs offer significant advantages, their deployment in high-stakes environments like economics, edge computing, and education requires rigorous evaluation to ensure they are trustworthy, unbiased, and ethically sound.