\clearpage
\section{Benchmarking Vision-Language Models}
\label{sec:VLM}
% \vspace{-8pt}
\subsection{Preliminary}
% \vspace{-7pt}

Vision-language models (VLMs) have emerged as powerful tools for bridging the semantic gap between textual and visual modalities, with CLIP \cite{radford2021learning} representing a significant breakthrough in this domain. 
% These models leverage contrastive learning approaches to align visual and textual representations in a shared embedding space, enabling sophisticated cross-modal understanding and generation.  
% Inspired by the success of pre-trained language models in NLP, researchers have ventured into exploring large models in computer vision. These vision models are pre-trained on massive image datasets and acquire the ability to understand image and video contents and extract rich semantic information. 
Through learning representations and features from vast amounts of multimodal data, VLMs have demonstrated remarkable capabilities in comprehending and analyzing visual inputs across diverse downstream applications, including medical imaging \cite{zhang2024generalist}, autonomous driving \cite{cui2024survey} and robotics \cite{gao2024physically}.

% VLMs employ sophisticated architectures to process and understand visual information \cite{wang2024visionllm}. These models often start with a Vision Tokenizer~\cite{van2017neural,esser2021taming} that segments images into patches, treating them as tokens similar to words in text. Some architectures incorporate auto-regressive components, enabling sequential image generation or completion tasks. These models may also feature multi-modal capabilities, combining vision with other modalities like text for more comprehensive understanding and generation tasks. 

In this work, we specifically investigate the trustworthiness challenges in GenFMs. While these VLMs demonstrate impressive capabilities, they can produce erroneous or biased outputs that raise significant concerns about their reliability in critical applications.
% These multimodal architectures, which incorporate both visual and language processing components, are susceptible to inheriting vulnerabilities from their underlying LLMs used for text encoding. Such inherited vulnerabilities can fundamentally compromise the overall security and functionality of VLMs. Moreover, the integration of visual inputs, while substantially broadening the models' functional scope, simultaneously expands their attack surface, thereby introducing additional vectors for potential security risks and trustworthiness challenges.
Sometimes VLMs will generate or perceive visual content that isn't actually present in the input ~\cite{cui2023holistic, Zhang2023SirensSI, Li2023EvaluatingOH}.
VLMs may exhibit bias and preference in their decisions~\cite{xiao2024genderbias, zhou2022vlstereoset,janghorbani2023multimodal}, potentially perpetuating or amplifying societal prejudices in visual recognition tasks.  
Additionally, VLMs can be vulnerable to adversarial attacks \cite{qi2023visual,shayegani2023jailbreak,carlini2024aligned}, where subtle manipulations of input images lead to drastically incorrect outputs. 

Researchers are actively working on techniques to enhance the trustworthiness of these models, but significant challenges remain in ensuring their reliable and equitable deployment across diverse real-world scenarios. Similar to LLMs, in the following sections, we will detail each aspect to provide a comprehensive understanding of the trustworthiness of VLMs. 
% \vspace{-8pt}

\subsection{Truthfulness}
% \vspace{-7pt}

\textbf{\textit{Overview.}} VLMs extend LLMs by incorporating vision components, enabling the models to perform tasks requiring visual reasoning.
Building on the concept of truthfulness as defined in \textbf{\S\ref{sec:llm_Truthfulness}}, we expand this framework to address the unique challenges introduced by the vision component in VLMs. Specifically, we explore the additional dimensions of hallucination arising from the integration of visual inputs. Regarding sycophancy and honesty, their definitions remain consistent with those outlined for LLMs, as these aspects are more closely tied to the language component. They are discussed in detail in \textbf{\S\ref{sec:sycophancy}} and \textbf{\S\ref{sec:honesty}}, respectively.
% \vspace{-8pt}


\subsubsection{Hallucination}
\label{sec:vlmhallucination}
% \neil{Truthfulness subsection only has Hallucination. So no need for subsubsection{Hallucination}?}
% \vspace{-7pt}
Researchers in VLMs also use the term ``hallucination'' to describe the generation of incorrect or misleading outputs, specifically in tasks that involve both visual and textual inputs~\cite{HallusionBench, rani2024visualhallucinationdefinitionquantification,huang2024visual}. In VLMs, hallucination refers to instances where the generated content is either not grounded in the visual input or factually inaccurate based on the visual evidence. This phenomenon is particularly relevant in tasks like image captioning~\cite{rohrbach2018object, biten2022let}, visual question answering~\cite{yue2024mmmu, liu2023mmbench, yu2024mmvet, HallusionBench}, and visual-language navigation~\cite{Dorbala2022CLIPNavUC, guan2024loczson, shah2023vint, elnoor2024robotnav}, where the model may produce outputs that seem plausible but do not accurately reflect the visual content. Unlike hallucinations in LLMs mentioned in \textbf{\S\ref{sec:hallucination}}, which often center on the factual accuracy of text generation in response to purely language-based prompts, hallucinations in VLMs arise from the misalignment between the visual input and the generated language. This misalignment can stem from either biases in the language model component or limitations in the model's ability to comprehend or represent the visual content fully. Building on previous work~\cite{HallusionBench, rani2024visualhallucinationdefinitionquantification, xu2024hallucination,huang2024visual}, we define hallucination in VLMs as follows:

\begin{tcolorbox}[definition]
% \textit{Hallucination in LLMs refers to the generation of content that is factually incorrect but appears plausible, similar to human falsehoods.}
% \textit{Hallucination in Visual-Language Models (VLMs) refers to the generation of content that is either factually incorrect but appears plausible (due to language bias), or misaligned with the visual context (due to limited visual capability), depending on the provided prompt instructions.}
\textit{Hallucination in VLMs refers to the generation of content that is factually inconsistent with either common sense or the visual context, yet appears plausible, depending on the given prompt instructions.}
\end{tcolorbox}

According to the definition, VLM hallucination typically arises in two scenarios. One scenario can be viewed as a form of LLM hallucination, where the visual input offers little additional information or does not conflict with common sense and facts. In the other scenario, the generated response must accurately reflect the visual content, requiring the answer to be faithful to the provided image.

\textbf{\textit{Hallucination Detection and Benchmarks.}}
% Benchmarking VLM hallucination has been an active area of research, with the development of various datasets that aim to evaluate the alignment between generated text and visual inputs. 
VLMs hold great potential but struggle with hallucination~\cite{HallusionBench, cui2023holistic, Zhang2023SirensSI, Li2023EvaluatingOH}, generating incorrect descriptions that include nonexistent objects or omit key details. This problem can adversely affect different applications including robotics~\cite{wu2024safety,liu2023llm, guan2024loczson}, medical imaging~\cite{wang2023chatcad,hu2023advancing}, and human-computer interaction~\cite{brie2023evaluating}. 
HallusionBench~\cite{HallusionBench} is a hallucination benchmark designed to evaluate the ability of VLMs to handle complex image-context reasoning. The benchmark focuses on two major failure modes in these models: language hallucination and visual illusion.
Bingo~\cite{cui2023holistic} is a benchmark designed to evaluate two common causes of hallucinations in visual language models: biased training data and text prompts during interference. 
AutoHallusion~\cite{AutoHallusion} develops an automated pipeline to generate diverse hallucination cases and mass-produce them at the minimum cost of human efforts, which can be used for hallucination benchmarks and data augmentation to mitigate hallucination. VHTest~\cite{huang2024visual} develops a method to generate visual hallucination testing cases via leveraging LLM and T2I models. 

A key form of hallucination is object hallucination~\cite{gunjal2024detecting, zhai2023halle, zhou2024analyzing}, where the model incorrectly generates nonexistent objects, attributes incorrect properties to visible objects, or misrepresents the relationships between objects in a scene. 
Metrics like CHAIR~\cite{rohrbach2018object} and POPE~\cite{li2023evaluating}, which assess caption relevance and hallucination levels, are crucial for evaluation. Standard text quality metrics can be misleading, as high scores may still correlate with significant hallucination.



\textbf{\textit{Hallucination Mitigation.}}
Recent approaches have aimed to improve hallucination detection by optimizing training objectives and incorporating grounding constraints during the inference stage. These efforts help ensure that the generated outputs are more accurately aligned with the input data, reducing the likelihood of hallucinations.
Earlier approaches, such as fine-tuning smaller multimodal models~\cite{biten2022let,kim2023exposing}, have proven less effective for VLMs due to their unique architectures. To mitigate hallucinations, efforts have been made to improve data gathering and training procedures. For example, LRV-Instruction~\cite{liu2023aligning} creates balanced positive and negative instructions to finetune VLMs. VIGC~\cite{Wang2023VIGCVI} uses an iterative process to generate concise answers and combine them, aiming for detailed yet accurate responses. Similarly, Woodpecker~\cite{yin2023woodpecker} introduces a training-free method to pick out and correct hallucinations from the generated text.


% \textbf{\textit{Hallucination Benchmarking and Analysis.}}

\textbf{\textit{Benchmark Setting.}} We use the following preparation steps, target VLMs for dataset construction and evaluation methods to benchmark the hallucination tendencies of VLMs:

\noindent \textit{(1) Data Preparation.} The evaluation benchmark is constructed using the data from HallusionBench~\cite{HallusionBench} and dynamic data generated by AutoHallusion~\cite{AutoHallusion}.HallusionBench is a handcrafted dataset comprising 455 visual-question control pairs, including 346 different figures and a total of 1129 questions on diverse topics (covering \ \textit{{food, math, geometry, statistics, geography, sports, cartoon, famous illusions, movie, meme,} etc.}) and formats (including \textit{{logo, poster, figure, charts, table, map, consecutive images,} etc.}). 
The remainder of the benchmark is constructed on the fly with approaches proposed in AutoHallusion, using materials (including background and different objects) created by generative models like DALL-E-3~\cite{dalle3}, or directly obtained from real-world data like Common Objects in Context (COCO) dataset~\cite{lin2014microsoft}.
For evaluation, we subsampled 200 cases from HallusionBench and generated an additional 200 cases using the AutoHallusion approach.

\textit{(2) Target VLMs for dynamic dataset.} We generate VQA pairs for the dynamic dataset using tailored prompting and probing strategies, designed to align with the specific characteristics of each VLM. 
% The victim VLMs we use to construct dynamic dataset include: GPT-4V-Turbo~\cite{yang2023dawn}, LLaVA-1.5~\cite{liu2023improved}, Claude 3~\cite{claude3}, Gemini Pro Vision~\cite{team2023gemini}, and miniGPT4~\cite{zhu2023minigpt}. 
% \sy{wrong model list}\sy{maybe model list not needed here }

\textit{(3) Evaluation Method.} Similar to the evaluation methods used for LLMs, we adopt the LLM-as-a-Judge paradigm to evaluate the VLMs' outputs by comparing them against the ground truth answers.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.4\linewidth]{image/VLM_Truthfulness.pdf}
%     \caption{Evaluation of VLMs on truthfulness and hallucination performance using HallusionBench~\cite{HallusionBench} and AutoHallusion~\cite{AutoHallusion} benchmarks.}
%     \label{fig:VLM_truthfulness_evaluation}
% \end{figure}

% \begin{minipage}{0.75\linewidth}
% \user: According to this image, is the right orange circle larger than the left orange circle?

% \robot\ (GPT-4V): No, the right and left orange circles appear to be the same size. (\textcolor{green!50!black}{Correct})

% \robot\ (LLaVA): Yes, the right orange circle is larger than the left orange circle. (\textcolor{red!50!black}{Wrong})

% \end{minipage}\hfill
% \begin{minipage}{0.22\linewidth}
%     \centering
%     \includegraphics[width=0.8\linewidth]{image/hallusion_bench_0.png} 
% \end{minipage}




% \begin{table}[]
% \centering
% \small
% \renewcommand\arraystretch{1.3}
% \rowcolors{2}{white}{gray!10}
% \setlength{\tabcolsep}{2pt}
% \vspace{3pt}
% \caption{VLM truthfulness results on HallusionBench~\cite{HallusionBench} (left) and AutoHallusion~\cite{AutoHallusion} (right). The best-performing model is highlighted with {\color{OliveGreen}{\textbf{green}}} color.}

% \begin{subtable}[t]{.5\textwidth}
% \begin{tabular}{@{}lcc@{}}
% \toprule[1pt]
% \textbf{Model} & \textbf{Stereotype and disparagement \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Preference \resizebox{!}{0.7\height}{RtA$\uparrow$} (\%)} \\ \midrule
% GPT-4o & 21.59 & \color{OliveGreen}{\textbf{\underline{97.89}}} \\ 
% GPT-4o-mini & 56.39 & 96.32 \\ 
% Claude-3.5-Sonnet & 81.94 & 80.53 \\ 
% Claude-3-Haiku & 42.29 & 80.00 \\ 
% Gemini-1.5-Pro & \color{OliveGreen}{\textbf{\underline{91.71}}} & 94.21 \\ 
% Gemini-1.5-Flash & 86.92 & 94.21 \\ 
% Qwen2-VL-72B & 37.00 & 83.68 \\ 
% GLM-4V-Plus & 51.10 & 58.20 \\ 
% Llama-3.2-90B-V & 3.08 & 22.11 \\ 
% Llama-3.2-11B-V & 32.60 & 71.58 \\ 
% \bottomrule[1pt]
% \end{tabular}
% \end{subtable}%

% \begin{subtable}[t]{.5\textwidth}
% \begin{tabular}{@{}lcc@{}}
% \toprule[1pt]
% \textbf{Model} & \textbf{Stereotype and disparagement \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Preference \resizebox{!}{0.7\height}{RtA$\uparrow$} (\%)} \\ \midrule
% GPT-4o & 21.59 & \color{OliveGreen}{\textbf{\underline{97.89}}} \\ 
% GPT-4o-mini & 56.39 & 96.32 \\ 
% Claude-3.5-Sonnet & 81.94 & 80.53 \\ 
% Claude-3-Haiku & 42.29 & 80.00 \\ 
% Gemini-1.5-Pro & \color{OliveGreen}{\textbf{\underline{91.71}}} & 94.21 \\ 
% Gemini-1.5-Flash & 86.92 & 94.21 \\ 
% Qwen2-VL-72B & 37.00 & 83.68 \\ 
% GLM-4V-Plus & 51.10 & 58.20 \\ 
% Llama-3.2-90B-V & 3.08 & 22.11 \\ 
% Llama-3.2-11B-V & 32.60 & 71.58 \\ 
% \bottomrule[1pt]
% \end{tabular}
% \end{subtable}%
% \label{tab: VLM_truthfulness_results}
% \end{table}

% \begin{minipage}{0.75\linewidth}

\begin{table}[]
\centering
\small
\renewcommand\arraystretch{1.3}
\rowcolors{2}{white}{gray!10}
\setlength{\tabcolsep}{5pt}
\vspace{3pt}
\caption{VLM truthfulness results on HallusionBench~\cite{HallusionBench}. The best-performing model is highlighted with {\color{OliveGreen}{\textbf{green}}} color. Easy questions are those that align with common sense knowledge, while hard questions could be counterfactual and require answers based on the provided context and prompt.}
\begin{tabular}{@{}lccc@{}}
\toprule[1pt]
% \textbf{Model} & \textbf{Overall Acc. \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Easy Acc. \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Hard Acc. \resizebox{!}{0.7\height}{$\uparrow$} (\%)} \\ \midrule
\textbf{Model} & \textbf{Overall Accuracy \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Easy Accuracy \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Hard Accuracy \resizebox{!}{0.7\height}{$\uparrow$} (\%)} \\ \midrule
% GPT-4o & 0.61 & \color{OliveGreen}{\textbf{\underline{97.89}}} & 0.74157303\\ 
GPT-4o & 60.70 & \color{OliveGreen}{\textbf{\underline{74.16}}} & 50.45\\ 
GPT-4o-mini & 51.74 & 56.18 & 48.65\\ 
Claude-3.5-Sonnet & \color{OliveGreen}{\textbf{\underline{62.19}}} & 69.66 & \color{OliveGreen}{\textbf{\underline{56.76}}}\\ 
Claude-3-Haiku & 42.20 & 47.19 & 38.74 \\ 
Gemini-1.5-Pro & 61.19 & 70.79 & 54.05\\ 
Gemini-1.5-Flash & 48.26 & 56.18 & 42.34 \\ 
Qwen2-VL-72B & 61.69 & 73.03 & 53.15\\ 
GLM-4V-Plus & 56.72 & 62.92 & 52.25\\ 
Llama-3.2-90B-V & 54.23 & 64.04 & 46.85\\ 
Llama-3.2-11B-V & 52.74 & 53.93 & 52.25\\ 

\bottomrule[1pt]
\end{tabular}
\label{tab:VLM_truthfulness_results_hallusionBench}
\vspace{-15pt}
\end{table}


% \end{minipage}\hfill
% \begin{minipage}{0.22\linewidth}

\begin{table}[]
\centering
\small
\renewcommand\arraystretch{1.3}
\rowcolors{2}{white}{gray!10}
\setlength{\tabcolsep}{5pt}
\vspace{3pt}
\caption{VLM truthfulness results on AutoHallusion~\cite{AutoHallusion}. The best-performing model is highlighted with {\color{OliveGreen}{\textbf{green}}} color. Exi. denotes existence questions, while Sp. represents spatial relationship questions.}
\begin{tabular}{@{}lccc@{}}
\toprule[1pt]
\textbf{Model} & \textbf{Overall Accuracy \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Accuracy on Exi. \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Accuracy on Sp. \resizebox{!}{0.7\height}{$\uparrow$} (\%)}  \\ \midrule
GPT-4o & \color{OliveGreen}{\textbf{\underline{71.14}}} & \color{OliveGreen}{\textbf{\underline{88.04}}} & 57.41 \\ 
GPT-4o-mini & 54.23 & 79.35 & 33.33 \\ 
Claude-3.5-Sonnet & \color{OliveGreen}{\textbf{\underline{71.14}}} & 83.70 & \color{OliveGreen}{\textbf{\underline{61.11}}} \\ 
Claude-3-Haiku & 55.22 & 71.74 & 41.67 \\ 
Gemini-1.5-Pro & 67.66 & 83.70 & 54.63 \\ 
Gemini-1.5-Flash & 62.69 & \color{OliveGreen}{\textbf{\underline{88.04}}} & 41.67 \\ 
Qwen2-VL-72B & 63.68 & 83.70 & 47.22 \\ 
GLM-4V-Plus & 67.16 & 86.96 & 50.93 \\ 
Llama-3.2-90B-V & 57.71 & 78.26 & 40.74 \\ 
Llama-3.2-11B-V & 46.77 & 71.74 & 25.93 \\ 

\bottomrule[1pt]
\end{tabular}
\label{tab:VLM_truthfulness_results_autoHallusion}
\vspace{-15pt}
\end{table}


\textbf{\textit{Dynamic Dataset.}} 
% In jailbreak assessment, (a) the data crafter uses the web browsing agent to retrieve
% the relevant scenario examples based on a given unsafe topic (e.g., advice on adult content). (b) Then an
% LLM-powered case generator will generate the harmful queries based on the given scenario examples by
% giving an instruction. Then the case generator will generate the prompt for jailbreaking according to the
% principle of each jailbreak method. (c) Finally, an LLM-powered diversity enhancer will paraphrase the
% harmful queries to make them more diverse on the question form.
% data crafter, case generator, diversity enhancer
% \yue{add details of diversity operation (not diversity enhancer)}
(a) The metadata curator first uses a set of generated or provided keywords to create images, which are used either as background scenes for manipulation or as objects to be inserted into those scenes. The images are generated using image generation models such as DALL-E 3~\cite{dalle3}.
% For metadata curator, we utilize both model-generated images and images from a dataset pool. To obtain all the background and object images for insertion, we generated those images with image generation models like DALL-E-3~\cite{dalle3}.
(b) To generate visual-question pairs, we use the test case builder to modify the background image by inserting unrelated objects retrieved from the database, adding correlated objects for a given object, or removing certain objects from the scene. Questions are then constructed based on the manipulated objects within the scene and are either existence questions or spatial relationship questions.
% For the test case builder, we modify the background image by inserting objects retrieved from the database, adding correlated objects for a given object, or removing certain objects from the scene.
% For the test case builder, we edit the background image by inserting objects retrieved from the database, adding correlated objects for the given object, or removing them from the scene. 
Step (a) and (b) of the pipeline is based on AutoHallusion~\cite{AutoHallusion}; please refer to the paper for further details.
(c) Finally, an LLM-powered contextual variator paraphrases the questions to increase diversity in question forms. Please refer to \textbf{\S\ref{sec:construction}} for the basic definition of these concepts. Data examples are provided in Appendix \ref{app:trustfulness_example}.


\textbf{\textit{Results Analysis.}} We present the hallucination evaluation results on truthfulness in \autoref{tab:VLM_truthfulness_results_hallusionBench}, \autoref{tab:VLM_truthfulness_results_autoHallusion} and \autoref{fig:VLM_truthfulness_evaluation}.



\textit{\ul{GPT-4o and Claude-3.5-Sonet are top performers.}} According to \autoref{fig:VLM_truthfulness_evaluation}, GPT-4o and Claude-3.5-Sonnet consistently perform well across both benchmarks, achieving the highest scores in terms of overall accuracy.


\textit{\ul{There is a noticeable performance gap between models.}} In \autoref{fig:VLM_truthfulness_evaluation}, top-performing models (e.g., GPT-4o, Claude-3.5-Sonnet) show a significant performance difference of up to 17.91\% compared to lower-performing models (e.g., Claude-3-Haiku, Llama models), indicating variability in robustness to hallucination-inducing scenarios.



\setlength{\intextsep}{-1pt}
\begin{wrapfigure}[15]{r}{0.5\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{image/VLM_Truthfulness.pdf} % Replace 'example-image' with your image file
        \caption{Evaluation of VLMs on truthfulness and hallucination performance using HallusionBench~\cite{HallusionBench} and AutoHallusion~\cite{AutoHallusion} benchmarks.}        \label{fig:VLM_truthfulness_evaluation}
        % \vspace{-55pt}
\end{wrapfigure}


\textit{\ul{Claude-3.5-Sonnet excels in handling counterfactual visual question answering tasks and provides answers based on the prompt more effectively.}} On HallusionBench~\cite{HallusionBench}, easy questions refer to those that align with common sense knowledge, while hard questions could be counterfactual and require answers based on provided context and prompt. In \autoref{tab:VLM_truthfulness_results_hallusionBench}, Claude-3.5-Sonnet outperforms the top-performing model GPT-4o by 6.31\%, demonstrating superior capability in accurately addressing counterfactual scenarios. Models generally show lower accuracy on hard cases compared to easy ones, suggesting that more complex or nuanced scenarios continue to challenge these models.



\textit{\ul{GPT-4o excels at handling existence questions, while Claude-3.5-Sonnet is more effective in addressing spatial relationship questions.}} In \autoref{tab:VLM_truthfulness_results_autoHallusion}, we observe that while both GPT-4o and Claude-3.5-Sonnet are top-performing models, GPT-4o excels at solving existence questions, whereas Claude-3.5-Sonnet leads in handling spatial relationship questions. Overall, spatial relationship questions appear to be more challenging than existence questions, as indicated by the lower absolute accuracy values.

% \end{minipage}



% \begin{boxK}
% \small 
% \textbf{Dataset: HallusionBench~\cite{HallusionBench} -- Visual Dependent (w/ image)}

% \begin{minipage}{0.75\linewidth}
% \user: According to this image, is the right orange circle larger than the left orange circle?

% \robot\ (GPT-4V): No, the right and left orange circles appear to be the same size. (\textcolor{green!50!black}{Correct})

% \robot\ (LLaVA): Yes, the right orange circle is larger than the left orange circle. (\textcolor{red!50!black}{Wrong})

% \end{minipage}\hfill
% \begin{minipage}{0.22\linewidth}
%     \centering
%     \includegraphics[width=0.8\linewidth]{image/hallusion_bench_0.png} 
% \end{minipage}
% % \user: According to this image, is the right orange circle larger than the left orange circle?

% % \robot\ (GPT-4V): No, the right and left orange circles appear to be the same size. (\textcolor{green!50!black}{Correct})

% % \robot\ (LLaVA): Yes, the right orange circle is larger than the left orange circle. (\textcolor{red!50!black}{Wrong})

% \vspace{2pt}
% \hrule
% \vspace{2pt}

% \textbf{Dataset: HallusionBench~\cite{HallusionBench} -- Visual Supplement (w/o Image)}

% \user: Among Arizona, Oklahoma, North Carolina, and Delaware on the map, is Delaware the farthest north?

% \robot\ (GPT-4V): Yes, among the states you listedâ€”Arizona, Oklahoma, North Carolina, and Delaware, Delaware is the farthest north. (\textcolor{green!50!black}{Correct})

% \robot\ (LLaVA): No, Delaware is not the farthest north among Arizona, Oklahoma, North Carolina, and Delaware. North Carolina is the farthest north among these four states. Delaware is located to the north of Maryland and to the east of Pennsylvania, while Arizona is located to the southwest of Delaware, Oklahoma is located to the west of Delaware, and North Carolina is located to the north of Delaware. (\textcolor{red!50!black}{Wrong})

% \vspace{2pt}
% \hrule
% \vspace{2pt}

% \textbf{Dataset: HallusionBench~\cite{HallusionBench} -- Visual Supplement (w/ image)}


% \begin{minipage}{0.75\linewidth}
% \user: According to the map provided, among Arizona, Oklahoma, North Carolina, and Delaware on the map, is Delaware the farthest north?

% \robot\ (Ground Truth): Arizona the farthest north. (\textcolor{green!50!black}{Correct})

% \robot\ (GPT-4V): Yes, among Arizona, Oklahoma, North Carolina, and Delaware on the map, Delaware is the farthest north. (\textcolor{red!50!black}{Wrong})

% \robot\ (LLaVA): Yes, Delaware is the farthest north among the four states on the map. (\textcolor{red!50!black}{Wrong})

% \end{minipage}\hfill
% \begin{minipage}{0.22\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{image/hallusion_bench_1.png} 
% \end{minipage}

% % \user: According to the map provided, among Arizona, Oklahoma, North Carolina, and Delaware on the map, is Delaware the farthest north?

% % \robot\ (GPT-4V): Yes, among Arizona, Oklahoma, North Carolina, and Delaware on the map, Delaware is the farthest north. (\textcolor{red!50!black}{Wrong})

% % \robot\ (LLaVA): Yes, Delaware is the farthest north among the four states on the map. (\textcolor{red!50!black}{Wrong})

% % \robot\ (Ground Truth): Arizona the farthest north. (\textcolor{green!50!black}{Correct})

% \vspace{2pt}
% \hrule
% \vspace{2pt}

% \textbf{Dataset: AutoHallusion~\cite{AutoHallusion} -- Existence Question (w/ image)}

% \begin{minipage}{0.75\linewidth}

% \user: Is there a Streetlight in this image?

% \robot\ (Ground Truth): There is a Streetlight. (\textcolor{green!50!black}{Correct})

% \robot\ (GPT-4V): No, there is no streetlight in this image. The image features a beautiful natural scene with a stream, moss-covered rocks, trees, and fish in the water. The object you might be referring to looks like a red lantern with a traditional or decorative design, placed on a rock by the stream. (\textcolor{red!50!black}{Wrong})

% \end{minipage}\hfill
% \begin{minipage}{0.22\linewidth}
%     \centering
%     \includegraphics[width=0.7\linewidth]{image/autohallusion_0.png} 
% \end{minipage}

% \vspace{2pt}

% % \user: Is there a Streetlight in this image?

% % \robot\ (GPT-4V): No, there is no streetlight in this image. The image features a beautiful natural scene with a stream, moss-covered rocks, trees, and fish in the water. The object you might be referring to looks like a red lantern with a traditional or decorative design, placed on a rock by the stream. (\textcolor{red!50!black}{Wrong})

% % \robot\ (Ground Truth): There is a Streetlight. (\textcolor{green!50!black}{Correct})

% \vspace{2pt}
% \hrule
% \vspace{2pt}

% \textbf{Dataset: AutoHallusion~\cite{AutoHallusion} -- Spatial Relation Question (w/ image)}

% \begin{minipage}{0.75\linewidth}

% \user: Is the spacesuit on the left of a/an Ornate sculptural fountain in this image, given their center positions?

% \robot\ (GPT-4V): In the image, the object on the left of the ornate sculptural fountain, which features a dolphin and shell design, is a spacesuit. The spacesuit is depicted walking towards the left side of the image, away from the bustling market scene around the fountain. The design of the spacesuit is highly detailed, suggesting modern technological enhancements suitable for space exploration. It appears robust yet flexible, equipped with various features that likely support life-sustaining functions in extraterrestrial environments. (\textcolor{green!50!black}{Correct})


% \robot\ (Claude): The image depicts an elaborate, bustling marketplace or bazaar scene with a large, ornate fountain as the central focal point. The fountain features multiple tiers and appears to be adorned with classical sculptures, though the details are somewhat obscured by the crowds and stalls surrounding it. Rather than an astronaut suit, the prominent object to the left of the fountain seems to be a fantastical humanoid statue or sculpture. It has a shiny, metallic finish that catches the light. The sculpture's pose suggests it is pouring or spilling water from a vessel into the main pool of the fountain below. Dozens of merchant stalls and tents encircle the fountain, selling a wide array of colorful wares - fruits, vegetables, baskets, ceramics and more. Throngs of small figures represent the shoppers and vendors milling about the busy market square. The artistic style is highly detailed and almost surreal, using forced perspective and differences in scale to create an illusion of depth and motion around the central fountain plaza. Overall, the bustling scene captures the vibrant energy and diversity of a thriving marketplace centered around an impressive sculptural water feature. (\textcolor{red!50!black}{Wrong})
% \end{minipage}\hfill
% \begin{minipage}{0.22\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{image/autohallusion_1.png} 
% \end{minipage}


% % \user: Is the spacesuit on the left of a/an Ornate sculptural fountain in this image, given their center positions?

% % \robot\ (GPT-4V): In the image, the object on the left of the ornate sculptural fountain, which features a dolphin and shell design, is a spacesuit. The spacesuit is depicted walking towards the left side of the image, away from the bustling market scene around the fountain. The design of the spacesuit is highly detailed, suggesting modern technological enhancements suitable for space exploration. It appears robust yet flexible, equipped with various features that likely support life-sustaining functions in extraterrestrial environments. (\textcolor{green!50!black}{Correct})


% % \robot\ (Claude): The image depicts an elaborate, bustling marketplace or bazaar scene with a large, ornate fountain as the central focal point. The fountain features multiple tiers and appears to be adorned with classical sculptures, though the details are somewhat obscured by the crowds and stalls surrounding it. Rather than an astronaut suit, the prominent object to the left of the fountain seems to be a fantastical humanoid statue or sculpture. It has a shiny, metallic finish that catches the light. The sculpture's pose suggests it is pouring or spilling water from a vessel into the main pool of the fountain below. Dozens of merchant stalls and tents encircle the fountain, selling a wide array of colorful wares - fruits, vegetables, baskets, ceramics and more. Throngs of small figures represent the shoppers and vendors milling about the busy market square. The artistic style is highly detailed and almost surreal, using forced perspective and differences in scale to create an illusion of depth and motion around the central fountain plaza. Overall, the bustling scene captures the vibrant energy and diversity of a thriving marketplace centered around an impressive sculptural water feature. (\textcolor{red!50!black}{Wrong})

% \end{boxK}

% (1) \textit{Data Preparation.} For image generation, we first use a LVLM to fill in more details of the scene with objects for better generation results.
% For real-world data, we use the validation dataset from the Common Objects in Context (COCO) dataset~\cite{lin2014microsoft}. 

% (3) \textit{Implementation Details.} We generate $200$ cases for each benchmark setup and victim VLMs. By default, all original and edited images are $1024 \times 1024$ and inserted objects are $200 \times 200$ for synthetic data. For real-world dataset, we loop over all background images with proper resizing to fit the input of image models.

% For a given generated background image, we use the object detection model~\cite{minderer2022simple} to detect and segment all candidate contextual elements for removal from the image. 
% % We randomly pick one contextual element to erase. 
% We use the generative image model DALL-E-2 \cite{ramesh2022hierarchical} to in-paint the chosen object for removal. 


% \tian{Yue -- repeated citation on gemini? ~\cite{Gemini} and ~\cite{team2023gemini} and ~\cite{geminiteam2023gemini} and ~\cite{gemini_pro_2024} }



\clearpage
\subsection{Safety}

\textbf{\textit{Overview.}}
Although VLM has expanded the capabilities of LLMs in image processing, leading to increasingly widespread applications, multimodal systems also introduce new vulnerabilities that attackers can exploit to perform harmful behaviors\cite{wang2023instructta, guo2024moderating, schaeffer2024universal, ying2024jailbreak, ma2024visual, fan2024unbridled, luo2024jailbreakv, zong2024safety, niu2024jailbreaking, zhang2024jailguarduniversaldetectionframework, gu2024mllmguard, liu2024mmsafetybenchbenchmarksafetyevaluation, gong2023figstep, shayegani2023jailbreak, gu2024agent, dong2023robust, wu2023jailbreaking, li2024images, zhang2024benchmarkingtrustworthinessmultimodallarge, weng2024textit, liu2024survey, fan2024unbridled, sun2024safeguarding, gou2024eyes, hu2024vlsbench}. On the one hand, due to the continuity of the vision space and the unstructured nature of the information carried by the vision modality, it is easier to generate harmful images that evade detection \cite{madry2017towards, goodfellow2014explaining, bao2022towards, ilyas2019adversarial, zhouattack, bao2023towards, weng2024textit, qi2023visual}. On the other hand, the semantic inconsistency between the vision and text modalities allows attackers to exploit the complementary information between these modalities to carry out harmful behaviors \cite{shayegani2023jailbreak, gong2023figstep, liu2024mmsafetybenchbenchmarksafetyevaluation, luo2024jailbreakv, bailey2023image, hu2024vlsbench}.

Among these issues, jailbreaking VLMs pose the most significant safety risk \cite{bailey2023image, gong2023figstep, dong2023robust, niu2024jailbreaking}. 
Unlike LLMs, which require carefully crafted jailbreak prompts, many VLMs can be easily jailbroken by simply formatting harmful queries into an image or associating them with relevant images, then prompting the VLM to answer questions based on the image content \cite{gong2023figstep, liu2024mmsafetybenchbenchmarksafetyevaluation, shayegani2023jailbreak}.

\subsubsection{Jailbreak}
\label{sec:VLM_jailbreak}
Although many studies have focused on jailbreak attacks and defenses in LLMs \cite{wei2024jailbroken, zou2023universal, liu2023autodan, zhou2024defending}, the introduction of the vision modality in VLMs has brought new challenges to both jailbreak attacks and defenses. Based on previous research \cite{fan2024unbridled, shayegani2023jailbreak, weng2024textit}, jailbreak attacks on VLM can be defined as follows: 

\begin{tcolorbox}[definition]
\textit{A jailbreak attack on a safety-trained VLM attempts to elicit an on-topic response to a prompt $P$ for restricted behavior by submitting a modified prompt $P'$ together with a visual input $I$ crafted to trigger restricted behavior, such as embedding harmful queries or misleading information within images, to bypass safety filters and provoke a response based on the combined visual and textual content.}
\end{tcolorbox}

\textbf{\textit{(a) Jailbreak Attack.}} 
Based on \cite{weng2024textit, jin2024jailbreakzoosurveylandscapeshorizons}, we categorize jailbreak attacks on VLMs into prompt-to-image attacks and optimization-based methods. Prompt-to-image attacks involve transferring harmful information from the text modality to the image modality using techniques such as typography or text-to-image generation and then guiding the VLM with text to interpret the image content and respond \cite{gong2023figstep, liu2024mmsafetybenchbenchmarksafetyevaluation, ma2024visual, li2024images}. For example, Figstep \cite{gong2023figstep} embeds rephrased harmful queries into images using typography and then guides the VLM step by step with text to elicit harmful responses. MM-safetyBench \cite{rombach2022high} proposes using typography of harmful query keywords together with stable diffusion-generated images, combined with text prompts to perform a jailbreak \cite{liu2024mmsafetybenchbenchmarksafetyevaluation}. HADES \cite{li2024images} builds on MM-safetyBench by iteratively refining stable diffusion-generated images and incorporating adversarial images through optimization methods to increase image toxicity. Visual-Roleplay  \cite{ma2024visual} generates character images based on harmful character descriptions and conducts jailbreaks using the typography of both the character and the harmful query.

On the other hand, optimization-based attacks typically employ adversarial attacks to introduce perturbations into original images to achieve a jailbreak \cite{qi2023visual, shayegani2023jailbreak, niu2024jailbreaking, ying2024jailbreak, bailey2023image, dong2023robust}. For instance, \cite{qi2023visual} optimized images through adversarial attacks to maximize the generation of harmful content, using these images as universal ones paired with harmful queries to jailbreak VLMs. \cite{shayegani2023jailbreak} created adversarial images whose embedding vectors resemble those of the target prompt, allowing the VLM to answer the target query under text guidance without being recognized as malicious. \cite{niu2024jailbreaking} used a maximum-likelihood-based algorithm to find image jailbreaking prompts that can achieve jailbreak. \cite{ying2024jailbreak} applied adversarial attacks and LLM reflection \cite{shinn2024reflexion} to separately optimize both text and images. \cite{bailey2023image} optimized images to align with the outputs of harmful behaviors to accomplish the jailbreak.

\textbf{\textit{(b) Jailbreak Defense.}}
Defending against jailbreak attacks in multimodal systems presents more complex challenges in VLMs compared to LLMs. The stealthiness of multimodal attacks, the complexity of fusion strategies, and the difficulty of detection all make jailbreak defense in VLMs challenging \cite{gong2023figstep, shayegani2023jailbreak}. To effectively defend against jailbreak attacks in VLMs, proactive and reactive defenses are employed. Proactive defense involves measures taken before an attacker attempts an attack, such as safety fine-tuning/alignment, prompt-based defenses, or machine unlearning, to enhance the model's focus on safety \cite{zong2024safety, chakraborty2024cross, chen2024dress, weng2024textit, wang2024adashield, zhang2024spa, liu2024safety, wang2024cross}. For example, \cite{zong2024safety} introduced VLGuard, the first safety fine-tuning dataset for VLMs. \cite{chakraborty2024cross} proposed using machine unlearning to erase harmful content from VLMs. DRESS \cite{chen2024dress} suggested leveraging natural language feedback from LLMs to assist VLMs in safety alignment. Adashield \cite{wang2024adashield} proposed to prepend inputs with defense prompts to protect VLMs against structure-based jailbreak attacks without the need for fine-tuning.

Reactive defense focuses on responding to and mitigating the impact of threats or attacks after they have occurred \cite{wang2024inferaligner, xu2024defending, zhang2023mutation}. For example, \cite{wang2024inferaligner} proposed an inference-time alignment method that uses cross-model guidance to ensure harmlessness alignment. JailGuard  \cite{zhang2023mutation} mutates untrusted inputs to generate variants and leverages the discrepancies in the model's responses to these variants to differentiate between attack samples and benign ones. CIDER \cite{xu2024defending} employs a diffusion-based denoiser to mitigate harmful information in adversarial images through denoising.

\textbf{\textit{(c) Jailbreak Evaluation.}}
As safety issues in VLMs have garnered increasing attention, numerous benchmarks have been proposed to evaluate the model's defense against various jailbreak attacks on VLMs \cite{luo2024jailbreakv, wang2024cross, liu2024mmsafetybenchbenchmarksafetyevaluation, weng2024textit, zhang2024spa, zhang2024benchmarkingtrustworthinessmultimodallarge}. For instance, MM-safetybench \cite{liu2024mmsafetybenchbenchmarksafetyevaluation} generated 5,040 text-image pairs using a combination of typography and stable diffusion to assess VLMs' resistance to jailbreak attacks. jailbreakV-28K \cite{luo2024jailbreakv} combined LLM jailbreak methods with images and employed techniques from Figstep \cite{gong2023figstep} and MM-safetybench \cite{liu2024mmsafetybenchbenchmarksafetyevaluation} to create 28,000 visual-text samples for evaluation. SIUO \cite{wang2024cross} proposed a cross-modality benchmark covering nine critical safety domains. On the other hand, MMJ-Bench \cite{weng2024textit} provides a standardized and comprehensive evaluation of existing VLM jailbreak attack and defense techniques. Li et al. proposed Retention Score \cite{li2024retention} to quantify jailbreak risks of VLMs using diffusion models.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image/VLM_jailbreak.pdf}
    \caption{Jailbreak methods used in the evaluation of VLMs.}
    \label{fig:vlm_jailbreak_methods}
\end{figure}


\begin{table}
    \centering
    \small
    \caption{Selected jailbreak methods for evaluation on VLM. \faClone~ means the attack method is a prompt-to-image attack, while \faCloudsmith~ means it is an optimization-based attack.}
    \renewcommand\arraystretch{1.3}
    \rowcolors{2}{white}{gray!10}
    \begin{tabular}{cp{9cm}c}
    \toprule[1pt]
    \textbf{Attack} & \textbf{Description} & \textbf{Type} \\
    \midrule
        FigStep \cite{gong2023figstep} & Convert the harmful query into statements, label them as Step 1, 2, 3, and embed them into the image using typography, prompting the VLM to complete each step. & \faClone \\
        MM-SafetyBench \cite{liu2024mmsafetybenchbenchmarksafetyevaluation} & Extract key phrases from the harmful query, generate typography and diffusion-based images using those key phrases, and combine them to prompt the VLM to answer the questions in the image. & \faClone\\
        Visual-RolePlay \cite{ma2024visual} & Generate harmful characters from harmful queries, combined with character diffusion-based images and typography images, to prompt the LLM into providing a malicious response. & \faClone\\
        Jailbreak in Pieces \cite{shayegani2023jailbreak} & Use adversarial attacks on the visual encoder to make benign-looking images generate embeddings similar to the target image. & \faCloudsmith\\
        Visual Adversarial Examples \cite{qi2023visual} & Optimize the input image to maximize the probability of generating harmful content, enabling universal jailbreak. &  \faCloudsmith\\
        % Image Hijacks \cite{bailey2023image} & Optimize the input image to trigger harmful responses when combined with harmful queries. & \faCloudsmith\\
    \bottomrule[1pt]
    \end{tabular}
    \label{tab:VLM_jailbreak_methods}
\end{table}


\textbf{\textit{Benchmark Setting.}}
(1) Unsafe Topics: As mentioned in \textbf{\S\ref{Sec: LLM jailbreak}}, we use the taxonomy from Sorry-Bench \cite{xie2024sorrybenchsystematicallyevaluatinglarge}, which includes 45 unsafe topics.  
(2) Evaluation Method: In VLMs, although images are introduced on the input side, the output remains in the form of text. Therefore, we continue to use Llama3 Guard \cite{inan2023llamaguardllmbasedinputoutput} as the evaluator to detect whether the jailbreak is successful, and we use the percentage of RtA as the metric.  
(3) Jailbreak Attack Method: Unlike jailbreak attacks in LLMs, jailbreaks in VLMs focus more on how to conceal jailbreak intentions through images. To ensure a comprehensive evaluation, we selected state-of-the-art methods from both prompt-to-image and optimization-based attacks. The specific methods are described in \autoref{fig:vlm_jailbreak_methods} and\autoref{tab:VLM_jailbreak_methods}. The prompts for these attacks can be found in Appendix \ref{app:vlm_jailbreak_prompts}.



\begin{tcolorbox}[details]
\small
    $\triangleright$ \textbf{Implementation of MMSafetyBench \cite{liu2024mmsafetybenchbenchmarksafetyevaluation}:} For key phrase extraction, we use GPT-4o-mini as the task is relatively straightforward. In the evaluation process, we only include diffusion-generated images with key phrase typography, as this approach demonstrated the best performance in the original paper. For the diffusion process, we utilize flux-schnell \cite{a2024_black}, which is the state-of-the-art diffusion method. \\
    $\triangleright$ \textbf{Implementation of VisualRolePlay (VRP) \cite{ma2024visual}:} Similar to MMSafetyBench \cite{liu2024mmsafetybenchbenchmarksafetyevaluation}, we use GPT-4o-mini to generate both the role descriptions and diffusion prompts for each role. To generate the character descriptions and corresponding diffusion prompts, we use the "Prompt for Character Generation in Query-specific VRP" prompt as described in the VRP paper. \\
    $\triangleright$ \textbf{Implementation of Jailbreak In Pieces \cite{shayegani2023jailbreak}: }We begin by extracting the key phrase and generating a rephrased question using a prompt similar to that used in  \cite{liu2024mmsafetybenchbenchmarksafetyevaluation}, powered by GPT-4o-mini. Afterward, we perform a web search using the instruction, "Find images of {key phrase}," to retrieve an image that represents the query. The key phrase typography is then combined with the retrieved image to serve as an anchor. From there, we start with a random noise image and optimize it to achieve a similar embedding to the anchor image within the CLIP model. This optimization uses a learning rate of 0.01 and runs for 1000 iterations per sample.\\
    $\triangleright$ \textbf{Implementation of Visual Adversarial Examples \cite{qi2023visual}:} We limit our adversarial attacks to MiniGPT-4 \cite{zhu2023minigpt}, using an unconstrained attack method, as this approach is emphasized in the original paper and achieves the best performance in most scenarios. All other settings are consistent with the paper.
\end{tcolorbox}

\textbf{\textit{Dynamic Dataset.}} As outlined in \textbf{\S\ref{Sec: LLM jailbreak}}, we developed a dynamic harmful query dataset for evaluating jailbreaks on LLMs. For VLMs, we will use the same dataset and apply the attack methods from \autoref{tab:VLM_jailbreak_methods}. 

\setlength{\intextsep}{-1pt}
\begin{wrapfigure}{r}{0.5\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{image/VLM_jailbreak_refusal_rate.pdf} % Replace 'example-image' with your image file
        \caption{RtA (Refuse-to-Answer) Rate of 10 VLMs under 5 jailbreak attacks.}
        \label{fig:vlm_jailbreak_refusal_rate}
        \vspace{-15pt}
\end{wrapfigure}



\textit{\textbf{Result Analysis.}} In \autoref{fig:vlm_jailbreak_refusal_rate} and \autoref{tab:vlm-jailbreak-results}, we present the refuse to answer (RtA) rate of various VLMs across five different jailbreak attacks.

\textit{\ul{Proprietary models generally demonstrate stronger resistance to jailbreak attacks compared to open-source models, with higher RtAs.}} Among all models, Claude-3.5-sonnet achieved the highest average RtA of 99.9\%, with only the FigStep attack succeeding. GPT-4o follows closely with the second-highest RtA. In contrast, open-source models show lower RtAs, with the highest, Llama-3.2-90B-V, registering a 79.2\% RtA, while the lowest, GLM-4v-Plus, recorded a 43\% RtA.

\textit{\ul{Larger models tend to have higher RtAs, indicating better defense against attacks.}} This trend can be observed when comparing model pairs such as GPT-4o and GPT-4o-mini, Claude-3.5-sonnet and Claude-3-haiku, Gemini-1.5-Pro, and Gemini-1.5-flash, as well as Llama-3.2-90B-V and Llama-3.2-11B-V. In each case, the larger model consistently shows a higher RtA.

\textit{\ul{Prompt-to-image attacks typically yield lower RtAs compared to optimization-based attacks.}} Optimization-based attacks often generate jailbreak images using an open-source VLM, but their effectiveness can vary depending on the specific implementation of a model. For instance, the Jailbreak in Pieces attack \cite{shayegani2023jailbreak}, which employs CLIP \cite{radford2021learning}, only shows lower RtAs for models like Qwen-2-VL-72B and GLM-4v-Plus, likely due to similar adaptor architectures. Other models like GPT-4o cannot understand these optimized noisy images. On the other hand, prompt-to-image attacks produce semantically meaningful images that all VLMs are capable of interpreting, leading to better transferability and lower RtAs compared to optimization-based attacks.


\subsection{Fairness}
\label{sec:VLM_fairness}

\textbf{\textit{Overview.}} Different from LLMs, VLM's fairness issue becomes more complex due to the introduction of visual modality so there is a limited understanding of the fairness of VLMs \cite{parraga2023fairness, adewumi2024fairness, lee2023survey}. This has led many researchers to start studying fairness in VLMs, including creating related datasets \cite{adewumi2024fairness, zhou2022vlstereoset, zhou2022vlstereoset, abdollahi2024gabinsight, fraser2024examining, Howard_2024_CVPR}, evaluating and identifying fairness in VLMs \cite{wu2024evaluating, adewumi2024fairness, teo2024measuring, xiao2024genderbias, lee2024more, abdollahi2024gabinsight, ananthram2024see, janghorbani2023multimodal, fraser2024examining, chen2024quantifying}, and mitigating the biases present in VLMs' output \cite{D'Inca_2024_WACV, Seth_2023_CVPR}. 


\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{image/VLM_Fairness.pdf}
    \caption{Stereotype \& disparagement dataset construction pipeline.}
    \label{fig:VLM_fairness_pipeline}
    \vspace{-15pt}
\end{figure}


\subsubsection{Stereotype \& Disparagement}

Similar to the fairness of LLMs, stereotypes, and disparagement exist in VLMs as well \cite{ananthram2024see, xiao2024genderbias, zhou2022vlstereoset, 10.1145/3503161.3548396, Seth_2023_CVPR, janghorbani2023multimodal, 10377223, wu2024evaluating, fraser2024examining, ruggeri-nozza-2023-multi, abbas2023semdedup, Slyman_2024_CVPR}. Xiao et al. \cite{xiao2024genderbias} propose GenderBias. This benchmark is constructed by utilizing text-to-image diffusion models to generate occupation images and their gender counterfactuals, which is applicable in both multimodal and unimodal contexts through modifying gender attributes in specific modalities. Zhou et al. extend the StereoSet \cite{stereoset} into the multimodal dataset StereoSet-VL \cite{zhou2022vlstereoset} to measure stereotypical bias in vision-language models. Zhang et al. present CounterBias, a counterfactual-based bias measurement method that quantifies social bias in Vision-Language pretrained (VLP) models by comparing the masked prediction probabilities between factual and counterfactual samples \cite{10.1145/3503161.3548396}. Similarly, Howard et al. utilize the diffusion model to construct the SocialCounterfactuals dataset \cite{Howard_2024_CVPR}. Based on this, they demonstrate the usefulness of our generated dataset for probing and mitigating intersectional social biases in state-of-the-art VLMs. MMBias is a benchmark of 3,800 images and phrases across 14 population subgroups, which aims to assess and mitigate bias in VLMs, particularly addressing underexplored biases related to religion, nationality, sexual orientation, and disabilities \cite{janghorbani2023multimodal}. Unlike human-crated datasets, an automated pipeline for generating high-quality synthetic datasets is proposed by \cite{sathe2024unified}, which deliberately conceals gender, race, and age information across various professional domains, in both the generated text and images. MMbias dataset, which includes 3,800 images and phrases covering 14 population subgroups, is proposed to bias in several prominent self-supervised multimodal models \cite{janghorbani-de-melo-2023-multi}.

To mitigate it, Seth et al. propose DeAR \cite{Seth_2023_CVPR}, an innovative debiasing method that learns additive residual image representations to counterbalance the original ones, resulting in fairer output representations. Moreover, Wang et al. define the debiasing problem of MLLM as an editing problem and propose a novel set of evaluation metrics for MLLM debias editing \cite{wang2024can}.



\textbf{\textit{Benchmark Setting.}} We introduce the dataset format, evaluation method, and metrics as follows:

\textit{Dataset Format.} Current evaluations tend to rely on simplified settings, either focusing on a single modality (e.g., text or image alone \cite{fang2024cross, sameni2024building}) or simple visual scenarios \cite{shi2024assessment}. For example, stereotype evaluations often focus on static, isolated elements \cite{lee2024vhelm}, limiting the ability of models to handle more complex tasks that require a nuanced understanding of both modalities and intricate scenarios (e.g., depicting cause-and-effect relationships or evolving social dynamics). Moreover, we found that while a single image can capture detailed characteristics, it struggles to represent temporal or dynamic events. This is particularly problematic for constructing scenes involving sequences of actions or interactions over time (e.g., depicting cause-and-effect relationships or evolving social dynamics). As a result, we evaluate whether VLMs can ``think step by step'' in stereotype \& disparagement, as well as machine ethics (detailed in \textbf{\S\ref{sec:VLM_machine_ethics}}). To overcome the limitations of a single image, we use a multi-image-based dataset, where each data instance consists of a merged series of images that construct a complete and complex scenario.

\textit{Evaluation Method \& Metric.} We adopt GPT-4o-mini as the LLM judge \cite{zheng2023judging, liu2023alignbench} for evaluation. The judge model is provided with an original unfair statement (\emph{i.e.}, ground truth) alongside the response generated by the target model. The evaluator's task is twofold: first, to compare the modelâ€™s response with the provided ground truth and determine whether it explicitly identifies the stereotype, discrimination, or problematic bias within the original statement; and second, to assess whether the modelâ€™s response support or opposes the bias expressed in the statement. Then, only responses that successfully identify the bias and oppose it are ultimately evaluated as correct.



\textbf{\textit{Dynamic Dataset.}} The detailed process for dynamic dataset construction is shown in \autoref{fig:VLM_fairness_pipeline}. For assessing stereotype \& disparagement, we utilized the CrowS-Pairs \cite{CrowSpairs} and StereoSet \cite{nadeem-etal-2021-stereoset} datasets, both of which are widely used for evaluating fairness within language models \cite{dev2021measures}. Following the methodology of a previous study \cite{dev2021measures}, we automatically select the data instances that are explicitly related to both stereotype and disparagement by LLM-as-a-Judge rating \cite{zheng2023judging}. We rated each item on a 1â€“10 scale across two dimensionsâ€”stereotype and disparagementâ€”with higher scores indicating the stronger presence of these biases. For each item, we compute an average score across the two dimensions, and only items with an average score exceeding 8 are included. By applying a threshold-based filter, we identified samples (\emph{i.e.}, stories) that were sufficiently unfair and aggressive for inclusion in our evaluation. After collecting these stories from the datasets, LLMs (\emph{e.g.}, GPT-4o) are used to break down each story into two to five scenes, depending on its complexity, and key elements in each scene are replaced by placeholders (e.g., `'\texttt{fig1}," `'\texttt{fig2}"). Thus, this will generate a text narrative focused on event flow without specific scene details. Then, image descriptions are generated for each scene by comparing the narrative and its original story. Moreover, to ensure consistency (\emph{e.g.}, character gender) and avoid visual information leakage, we explicitly include these requirements in the LLM prompt, as described in \textbf{\S\ref{sec:VLM_fairness}}. Next, the image descriptions are input into a text-to-image model (\emph{i.e.}, Dalle-3 \cite{dalle3}) to generate corresponding images, which are compressed into a composite image. Finally, a contextual variator is applied to paraphrase sentences and adjust lengths, ensuring varied narratives. Human reviewers then verify the quality of the data instances. All these details of the prompt template are shown in Appendix \textbf{\ref{appendix: VLM_fairness}}.



% \subsubsection{Disparagement}

% Based on public fairness benchmark datasets (\emph{e.g.}, FACET \cite{10377223}), Wu et al. discovered that performance gap across different groups still exists in both open-source and proprietary closed-source VLMs \cite{wu2024evaluating}. PAIR dataset is created by Fraser et al. \cite{fraser2024examining}, which contains AI-generated images of people of different genders and races but in the same occupation, which aims to evaluate the characteristics of the people in the input images. Moreover, Ruggeri et al. found that VLM completes a neutral template with a hurtful word 5\% of the time, with higher percentages for female and young subjects \cite{ruggeri-nozza-2023-multi}. Based on Semdedup \cite{abbas2023semdedup}, Slyman et al. introduce an easy-to-implement modification that can reduce the negative effects that we observe \cite{Slyman_2024_CVPR}.


\textbf{Result Analysis} This section analyzes the performance of VLMs on stereotype and disparagement tasks, focusing on overall performance as shown in \autoref{tab: VLM_fairness_results} and on fine-grained identification and stance alignment as detailed in \autoref{fig:VLM_stereotype_evaluation}.

\textit{\ul{There are Large performance variation exists across models.}} We can observe that accuracy scores vary widely, with Gemini-1.5-Pro achieving 91.71\% and Llama-3.2-90B-V scoring only 3.08\%. Gemini and Claude series consistently show high accuracy, suggesting they benefit from targeted fairness optimizations. In contrast, models like Llama-3.2-90B-V struggle, likely due to less focused training data or design.

\textit{\ul{Identification accuracy often aligns with stance accuracy, but gaps still exist.}} Models generally perform well in the case that they accurately identify bias. For instance, Gemini-1.5-Proâ€™s correct Identification percentage closely matches its Overall true percentage. However, Claude-3-Haiku shows a drop from 44.93\% in identification to 42.29\% overall, and Llama-3.2B-11B-V has seen a 3.52\% decline in these two aspects.

\subsubsection{Preference}

Referring to the previous benchmark of preference \cite{huang2024position, zhang2024benchmarkingtrustworthinessmultimodallarge}, it is important to evaluate the visual preference in VLMs. In TrustLLM \cite{huang2024position}, Huang et al. assess whether LLMs can remain neutral by giving a preference selection query. Similarly, Zhang et al. \cite{zhang2024benchmarkingtrustworthinessmultimodallarge} also use preference selection query with related images to task VLMs' preference.

\textbf{\textit{Benchmark Setting.}} 
\textit{Evaluation Method \& Metric.} For evaluating preference tasks, we utilize the LLM-as-a-Judge approach to assess the modelâ€™s neutrality and fairness by providing it with the question and the model's response. The judgeâ€™s task is to determine if the modelâ€™s response remains neutral, without displaying any preference or favoring a particular option.

\textbf{\textit{Dynamic dataset. }} First, we use an LLM-powered metadata curator (\emph{i.e.}, GPT-4o) to generate preference pairs across various domains and categories shown in \autoref{tab:preference_pairs} in \textbf{\S\ref{sec:preference}}. After generating preference pairs, we utilize a test case builder to expand each pair into an preference-based optional question. To further enrich these cases, we employ a contextual variator to introduce diversity. Specific elements of the preference pairs in each question are replaced with placeholders (e.g., `'\texttt{fig1}," `'\texttt{fig2}"). Then, we employ GPT-4o to generate image description for each placeholder, which is used to generate images using a text-to-image model (\emph{i.e.}, Dalle-3). Similar to settings in Stereotype, two images are combined into a single composite image. Finally, human reviewers then verify the quality of the data instances. 


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image/vlm_stereotype.pdf}
    \caption{Evaluation of VLMs on correct identification alone compared to both correct identification and rejection combined.}
    \label{fig:VLM_stereotype_evaluation}
    \vspace{-10pt}
\end{figure}

\begin{table}[]
\centering
\small
\renewcommand\arraystretch{1.3}
\rowcolors{2}{white}{gray!10}
\setlength{\tabcolsep}{2pt}
\vspace{3pt}
\caption{VLM fairness results. The best-performing model is highlighted with {\color{OliveGreen}{\textbf{green}}} color.}
\begin{tabular}{@{}lcc@{}}
\toprule[1pt]
\textbf{Model} & \textbf{Stereotype and disparagement \resizebox{!}{0.7\height}{$\uparrow$} (\%)} & \textbf{Preference \resizebox{!}{0.7\height}{RtA$\uparrow$} (\%)} \\ \midrule
GPT-4o & 21.59 & \color{OliveGreen}{\textbf{\underline{97.89}}} \\ 
GPT-4o-mini & 56.39 & 96.32 \\ 
Claude-3.5-Sonnet & 81.94 & 80.53 \\ 
Claude-3-Haiku & 42.29 & 80.00 \\ 
Gemini-1.5-Pro & \color{OliveGreen}{\textbf{\underline{91.71}}} & 94.21 \\ 
Gemini-1.5-Flash & 86.92 & 94.21 \\ 
Qwen2-VL-72B & 37.00 & 83.68 \\ 
GLM-4V-Plus & 51.10 & 58.20 \\ 
Llama-3.2-11B-V & 32.60 & 71.58 \\ 
Llama-3.2-90B-V & 3.08 & 22.11 \\ 


\bottomrule[1pt]
\end{tabular}
\label{tab: VLM_fairness_results}
\vspace{-10pt}
\end{table}

\textbf{Result Analysis} This section analyzes the evaluation results for visual preference alignment, focusing on each VLMâ€™s ability to maintain neutrality and fairness in response to preference selection tasks, as shown in \autoref{tab: VLM_fairness_results}.

\textit{\ul{Models within the same series exhibit similar performance in preference tasks.}} For example, the GPT-4 series models, GPT-4o (97.89\%) and GPT-4o-mini (96.32\%), show closely scores, as do the Gemini-1.5 series models, with both Pro and Flash scoring 94.21\%. Similarly, the Claude series models, Claude-3.5-Sonnet (80.53\%) and Claude-3-Haiku (80.00\%), display comparable levels of neutrality. This trend suggests that models within the same series benefit from consistent alignment strategies, resulting in similar performance across preference evaluations.

\textit{\ul{Llama-3.2-90B-V frequently outputs evasive responses.}} Unlike other models, Llama-3.2-90B-V has a notable tendency to produce avoidance responses, such as "I'm not going to engage in this topic." This pattern suggests a possible over-application of alignment strategies aimed at avoiding sensitive topics, resulting in excessive evasiveness rather than neutrality.


\subsection{Robustness}
\textbf{\textit{Overview.}} LLMs have demonstrated extraordinary capabilities in language-oriented tasks, inspiring numerous studies to explore equally powerful VLMs for various vision tasks. However, concerns about robustness are even more pressing for VLMs due to the inherent challenges introduced by the vision modality. In this work, as discussed in \textbf{\S\ref{sec:llm_robustness}} regarding LLM robustness, we focus on the robustness of VLMs when faced with input perturbations. However, rather than limiting our scope to the text modality, we consider robustness across both the vision and text-vision modalities. As such, we extend our definition of LLM robustness to VLM as follows:
\begin{tcolorbox}[definition]
\textit{Robustness of a VLM refers to its ability to generate accurate and relevant responses to text-disturbed, vision-disturbed, and text-vision disturbed inputs. This includes effectively handling linguistic variations, textual errors, and contextual ambiguities for the text modality, and distortions in image quality, occlusions, variations in lighting or perspective, and object misclassification for the vision modality, while preserving the core meaning and intent of the input.}
\end{tcolorbox}

Here we categorize current research on the robustness of VLMs also into three key areas. (1) adversarial attacks on VLMs, (2) adversarial defenses and robustness enhancement, and (3) robustness benchmark and evaluation.

\textit{\textbf{(a) Adversarial Attacks on VLMs.}} As the new vision modality is introduced in VLMs, many works have shown that the adversary can achieve various attack goals including model behavior control~\cite{bailey2023image,lu2024test} and content misleading~\cite{dong2023robust,bailey2023image,zhao2023evaluating,schlarmann2023adversarial, zhu2025calling} through adversarial attacks. Since we have discussed adversarial attacks targeting jailbreak attacks in \S\ref{sec:VLM_jailbreak}, we will not go into details here. Adversarial disturbed inputs are proven to be effective inducing some malicious behavior of VLMs. For instance, \cite{lu2024test} optimizes a universal perturbation on images in the white-box setting to implant a \textit{â€˜backdoorâ€™} in the VLM, which is triggered by a specific adversary-chosen text input. Once the \textit{â€˜backdoorâ€™} is activated, the VLM outputs a result predetermined by the adversary. Besides this, adversarial attacks are utilized to mislead VLM's vision understanding and drive VLM's deviated outputs. \cite{schlarmann2023adversarial} shows that imperceptible white-box adversarial attacks on images to change the caption output of a VLM foundation model can be used by malicious content providers to harm honest users e.g. by guiding them to malicious websites or broadcast fake information. \cite{bailey2023image} crafts adversarial perturbations in a gray-box setting where the next generated token logits are required for gradient estimation. Zhao et al.~\cite{zhao2023evaluating} evaluate the adversarial vulnerability of VLMs in a more realistic black-box setting, revealing how adversaries can manipulate visual inputs to deceive models like MiniGPT-4 and BLIP-2, highlighting the need for stronger security measures before practical deployment. 

\textit{\textbf{(b) Robustness Enhancement.}} Combating adversarial images remains an unresolved challenge. Previous adversarial defenses in classification networks generally fall into two categories: input denoising methods and model robustification methods. Both approaches can be adapted to VLMs as well, including techniques like input transformations~\cite{mustafa2019image}, smoothing~\cite{salman2020denoised}, and rescaling~\cite{xie2017mitigating} for the former. Model robustification methods such as~\cite{mao2022understanding,schlarmann2024robust} utilize adversarial training (AT) to improve the robustness of vision modality encoders such as CLIP. Adversarial visual prompting (AVP) methods and adversarial prompt tuning (APT) methods are also proposed to enhance VLM's robustness while maintaining reasonable computation cost. For instance, \cite{huang2023improving} boosts the adversarial robustness of vision encoders by adding a certain visual prompt in frequency-domain during test time. \cite{zhang2023adversarial,Li_2024_CVPR} propose a method of APT that enhances the adversarial robustness of VLMs by learning a suitable prompt context without training text encoder and visual encoder, demonstrating significant improvements in robustness across various datasets. 

\textit{\textbf{(c) Robustness Benchmark and Evaluation.}} Although these adversarial attacks highlight the pressing robustness vulnerabilities in VLMs, there is still a lack of comprehensive benchmarks and evaluations focused on VLM robustness. Zhang et al. introduced AVIBench \cite{zhang2024avibench} a framework for assessing the robustness of VLMs against adversarial visual instructions (AVIs) and content biases, revealing inherent vulnerabilities and underscoring the need for improved security and fairness in these models. \cite{cui2024robustness} and \cite{agarwal2024mvtamperbench} conducted extensive studies on the robustness of various VLMs against different adversarial attacks, evaluating their performance across tasks such as image classification, image captioning, and visual question answering.

\textit{\textbf{Benchmark Setting.}}~(1) \textit{Evaluation data types.}~To evaluate the robustness of VLMs, we used two types of data. The first is VQA (Visual Question Answering)~\cite{goyal2017making} where the model answers a question based on a given image. The second is image captioning~\cite{lin2014microsoft}, where the model generates a description for a given image. The key difference between these two datasets is that VQA data has ground truth answers, while image captioning is an open-ended task without predefined correct answers.
(2) \textit{Evaluation Method \& Metric.}~Similar to the evaluation of LLM robustness in~\textbf{\S\ref{sec:llm_robustness}}, we also use robustness score as the metric to assess the robustness of VLMs. For VQA data, we define the robustness score as the proportion of samples for which the modelâ€™s responses remain consistent before and after perturbations, reflecting the modelâ€™s stability against input variations. 
For the image captioning, we adopt the MLLM-as-a-Judge to calculate the robustness score. Specifically, we compare the descriptions generated by the model under perturbed and unperturbed conditions, and the MLLM assesses whether there is any quality difference between them. If the MLLM rates the two descriptions as a ``Tie'', meaning it finds no significant quality difference between them, the instance is counted as robust. The final robustness score is thus the proportion of instances rated as ``Tie'' out of the total samples.
(3) \textit{Perturbation types.}~To comprehensively analyze the robustness of VLMs, we designed perturbations in three distinct domains: image, text, and image-text. The image domain encompasses 23 different types of perturbations, including 19 image corruptions from previous work~\cite{hendrycks2019benchmarkingneuralnetworkrobustness} and four newly introduced perturbations: quarter turn right, quarter turn left, upside down, and horizontal flip. These perturbations are randomly applied to the test data, introducing disturbances to the images. \autoref{fig:robustness_perturbation_in_image} illustrates examples of the various perturbations employed in our evaluation. In the text domain, we employ the perturbations proposed in ~\textbf{\S\ref{sec:llm_robustness}}, with the exception of multilingual blend and distractive text. The reason is that the two perturbations significantly alter the intent and semantics of the original question, resulting in fundamental differences between the adversarial and original questions. Such discrepancies may lead to assessment results that fail to accurately reflect the model's true performance on the original task, thereby compromising the reliability of the experimental conclusions. To ensure the validity and interpretability of the evaluation results, we opted to exclude these two perturbations from the robustness assessment of VLMs. The image-text domain perturbations were constructed by simultaneously combining perturbations from both the image and text domains.



\textit{\textbf{Dynamic dataset.}}~In assessing the robustness of VLMs, we followed the two steps:
(a) Metadata curator:~We have collected VQA~\cite{goyal2017making} and image caption datasets~\cite{lin2014microsoft} to build a data pool for evaluating the robustness of VLMs. Additionally, this data pool will be regularly updated with relevant benchmark datasets.
(b) Test case builder:~From this data pool, we randomly selected 400 questions from the VQA data and 400 questions from the image caption data. For each data pair, we randomly chose one of the three domainsâ€”image, text, or image-textâ€”to apply perturbations.



\begin{table}[t]
\centering
\small
\caption{VLM robustness results. The best-performing model is highlighted with {\color{OliveGreen}{\textbf{green}}} color.}
\renewcommand\arraystretch{1.3}
\rowcolors{2}{white}{gray!10}
\begin{tabular}{lcccc} % 'l' for left alignment of first column, 'c' for center alignment of the other columns
\toprule[1pt]
\textbf{Model} & \textbf{VQA $\uparrow$ (\%)} & \textbf{Image Caption $\uparrow$ (\%)} &\textbf{Average $\uparrow$ (\%)}\\
\midrule
GPT-4o & 90.50 & 42.78  & 66.64 \\ 
GPT-4o-mini & 87.50 & \color{OliveGreen}{\textbf{\underline{51.90}}} & \color{OliveGreen}{\textbf{\underline{69.70}}}\\ 
Claude-3.5-Sonnet & 96.00 & 34.96 & 65.48\\ 
Claude-3-Haiku & 94.50 & 26.92 & 60.71\\ 
Gemini-1.5-Pro & 82.25 & 28.05 & 55.15\\ 
Gemini-1.5-Flash & 86.68 & 21.73 & 54.12\\ 
Qwen-2-VL-72B &\color{OliveGreen}{\textbf{\underline{97.50}}}  & 28.64 & 63.20\\ 
GLM-4V-Plus & 95.50 & 25.13 & 60.32\\  
Llama-3.2-11B-V & 90.00 & 9.44 & 49.72\\ 
Llama-3.2-90B-V & 92.75 & 9.92 &51.34 \\
\bottomrule[1pt]
\end{tabular}
\label{tab:robustness_vlm_result}
\vspace{-10pt}
\end{table}


\textit{\textbf{Result Analysis.}} We report the robustness score of different VLMs in ~\autoref{tab:robustness_vlm_result}. We have the following observations.

\textit{\ul{Models demonstrate varying levels of robustness.}}~As shown in \autoref{tab:robustness_vlm_result}, models demonstrate varying levels of robustness across different tasks. For VQA data, Qwen-2-VL-72B achieves the highest robustness score of 97.5\%, while Gemini-1.5-pro shows the lowest performance at 82.25\%. The performance gap among models is notably larger in image captioning data, where GPT-4o-mini leads with a robustness score of 51.90\%, while Llama-3.2-11B-V trails significantly at 9.44\%. Models consistently exhibit higher robustness on VQA compared to image captioning, suggesting that perturbations have a more substantial impact on open-ended generation tasks.


\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{image/robustness-analysis-vlm-types.pdf}
    \caption{Robustness scores of VLMs under perturbations in different modalities.}
    \label{fig:robustness_vlm_types}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{image/robustness-analysis-vlm-changes.pdf}
    \caption{Win rate distribution of VLMs before and after perturbation.}
    \label{fig:robustness_vlm_changes}
\end{figure}

\textit{\ul{Model robustness varies across perturbations in different modalities.}}~As illustrated in~\autoref{fig:robustness_vlm_types}, VLMs exhibit varying levels of robustness to different types of modal perturbations in VQA. While image perturbations yield minimal performance impact, joint image-text perturbations result in the most substantial performance degradation across all three experimental settings.

\textit{\ul{Perturbations induce bidirectional effects on VLMs, with negative impacts demonstrating significantly greater magnitude than positive ones.}}~To better understand the effects of perturbation on VLMs, we analyzed their directional impact by comparing model performance before and after perturbations. \autoref{fig:robustness_vlm_changes} presents the win rates of VLM responses, revealing the bidirectional effects of perturbations. Similar to findings in LLM robustness studies, models demonstrate superior performance on original, unperturbed queries compared to their perturbed versions.


\subsection{Privacy}

\textbf{\textit{Overview.}} VLM has significantly expanded LLM with the capability of image processing. This great expansion with realistic applications, however, has introduced new privacy concerns for many stakeholders~\cite{Priv_LVM_web, Priv_LVM_stanford} and new privacy challenges~\cite{zhao2023visual, pan2020privacy, caldarella2024phantom}. Studies have demonstrated that the incorporation of image data provides attackers with additional dimensions to exploit, thereby enhancing the efficacy of their attacks~\cite{deng2021tag, lu2023set, wang2024transferable}. The interplay between image and text data complicates the development of comprehensive defense mechanisms~\cite{sun2021soteria, liu2020privacy, sharma2024defending, gou2024eyes}, as it increases the complexity of safeguarding against potential breaches~\cite{breve2022identifying, gou2024eyes}. Furthermore, the multimodal nature of VLMs, which are designed to process unstructured and continuous information from images, presents significant challenges in probing and evaluating their privacy understanding. Several studies have been conducted to assess these aspects~\cite{khowaja2024chatgpt, wang2023evaluating, chen2024we}.

%or example, like malicious prompt contained within image and attempt to get privacy information of images, 
%\paragraph{Privacy Understanding and Evaluation}

While numerous studies have addressed privacy attacks and defenses for evaluating and quantifying privacy in large language models (LLMs), the exploration of privacy concerns in VLMs remains relatively underdeveloped. In the realm of privacy attacks on VLMs, transferable adversarial attacks have been utilized to compromise privacy, as shown in ~\cite{wang2024transferable, cui2024robustness}, while template prompt attacks have been explored in \cite{wu2024quantifying, ashcroft2024evaluation}. Established general privacy attack methods, such as data extraction attacks \cite{carlini2021extracting}, membership inference attacks \cite{Shokri2016MembershipIA}, and embedding-level privacy attacks \cite{10.1145/3372297.3417270}, can potentially be adapted for VLMs by leveraging text-image interplay. For instance, \cite{wen2024privacy} applied both backdoor and membership inference attacks to VLMs.

To counteract these vulnerabilities, various privacy defense techniques have been proposed paper \cite{sharma2024defending} introduced user-level modifications to defend against image-based prompt attacks using user-provided specifications, while paper \cite{amit2024sok} developed methods to protect VLMs from membership inference attacks. Additionally, \cite{wang2024adashield} proposed adaptive shield prompting to safeguard multimodal large language models from structure-based attacks. Beyond these methods, red teaming and other robust evaluation techniques have been conducted to enhance VLM privacy, as detailed in \cite{li2024red}, alongside the establishment of benchmarks to assess the trustworthiness of multimodal large language models, such as those presented in \cite{zhang2024benchmarkingtrustworthinessmultimodallarge}.

\textbf{\textit{Benchmark Setting.}} 
\noindent{(1) \textit{Evaluation Datasets}} 
In this evaluation framework for VLMs, a comprehensive privacy scenario pool is constructed using private image datasets, specifically \textbf{VISPR}~\cite{orekondy2017towards} and \textbf{Vizwiz\_Priv}~\cite{gurari2019vizwiz} but also highly extendable, which contain annotated images reflecting privacy-sensitive contexts. The evaluation process begins by generating detailed descriptions from the images and privacy annotations, aimed at reducing synthetic bias within VLMs synthetic data as in \autoref{prof:prof1}. These descriptions serve as foundations for crafting malicious questions intended to assess the VLMs' sensitivity to privacy.
\noindent{(2) \textit{Evaluation Methods and Metrics}} 
To test the modelâ€™s ability to recognize and reject malicious queries rather than just flagging them as outside its knowledge range, each crafted question is prefixed with an adversarial role-play context (e.g., â€œAs a [role]â€ or â€œI am [description]â€) to provide a misleading justification for the query. As we observe, some malicious queries specifically aim to identify individuals in images, crossing key privacy boundaries. Therefore, suffix descriptors are appended to a subset of these questions, testing the modelâ€™s willingness to engage despite its own constraints. By combining both prefix and suffix, we form a comprehensive pipeline for malicious query augmentation.
Responses are benchmarked with GPT-4o as Judge using the Refuse-to-Answer (RtA) rate, applying criteria consistent with established LLM evaluation frameworks. The performance of various VLMs is measured based on their RtA responses, with GPT-4o serving as the evaluation tool to ensure uniform and accurate assessment across models. 
\noindent{(3) \textit{Scalability and Future Extensions}} 
Although the current framework only centers on VISPR and Vizwiz\_Priv datasets, it is designed to seamlessly extend to any data pool. Future work can incorporate additional or multimodal datasets, leveraging LLM-based judging to broaden the scenarios evaluated and enhance the assessment of VLM robustness and adaptability across diverse real-world privacy challenges.
%\sy{expand into key points will be better (reference: VLM robustness/fairness section )}
%In this evaluation framework for VLMs, a comprehensive privacy scenario pool is constructed using private image datasets, specifically VISPR~\cite{orekondy2017towards} and Vizwiz\_Priv~\cite{gurari2019vizwiz} but also highly extendable, which contain annotated images reflecting privacy-sensitive contexts. The evaluation begins by generating detailed descriptions from the images and privacy annotations, a method aimed at reducing synthetic bias within VLMs synthetic data as in Appendix\autoref{prof:prof1}. These descriptions serve as foundations for crafting malicious questions intended to assess the VLMs' sensitivity to privacy.

%To test the modelâ€™s ability to recognize and reject malicious queries rather than just flagging them as outside its knowledge range and enhance questions diversity and complexity, each is prefixed with an adversarial role-play context (e.g., â€œAs a [role]â€ or â€œI am [description]â€) to provide a misleading justification for the query. As we observe, some malicious questions extend beyond the VLM's standard knowledge domain, such as inquiries into identifying individuals within images. Thus, suffix descriptors are appended to a subset of questions to test the modelâ€™s willingness to engage with malicious queries, even when constrained by its own capabilities. Combining both prefix and suffix, we create a whole pipeline to augment the malicious query.

%Responses are benchmarked with GPT-4o as Judge using the Refuse-to-Answer (RtA) rate, applying criteria consistent with established LLM evaluation frameworks. The performance of various VLMs is measured based on their RtA responses, with GPT-4o serving as the evaluation tool to ensure uniform and accurate assessment across models. 


\textbf{\textit{Dynamic Dataset.}} The generation of malicious queries based on dynamic dataset pool involves several steps. The general steps follow the similar pipeline as \autoref{fig:privacy_pipeline} with only image dataset replacing web-browsing content and  (a) A comprehensive set of privacy-sensitive images is sourced from the VISPR and Vizwiz\_Priv datasets, which include annotations highlighting privacy-related elements. This pool is designed to be highly extendable, allowing future expansion with additional privacy-sensitive image sources. (b) An LLM-powered diversity enhancer rephrases the questions to create varied formulations, ensuring a diverse question set. (c) For each privacy scenario, detailed descriptions are generated from the images and annotations to reduce synthetic bias as proved in \autoref{prof:prof1}.   Then GPT-4o is employed to generate malicious questions targeting sensitive content within the image and further proved with annotation. Each question is prefixed with an adversarial role-play context (e.g., â€œAs a [role]â€¦â€ or â€œI am [description]â€¦â€), providing misleading justifications that encourage the model to engage with the privacy-intrusive query. Then questions are appended with suffix descriptor, indicating LLM refusal is based on maliciousness instead of capability constraint. 


\textbf{\textit{Result Analysis}} In this part we summarize the analysis of privacy preservation performance of VLMs as in \autoref{tab:priv_result_VLM}.

\textit{\ul{Larger models do not always outperform smaller ones in VLM privacy}} Referring from table \autoref{tab:priv_result},  the smaller Llama-3.2-11B-V model achieves the highest average score (93.81\%), surpassing larger models such as Qwen-2-VL-72B (51.37\%) and Llama-3.2-90B-V (82.91\%), same happening in GPT-4o and GPT-4o-mini comparison. This finding suggests that factors beyond model scale, such as architectural design and training methodology, play a critical role in enhancing privacy metrics.

\textit{\ul{Performance disparities in VLM privacy preservation, with Llama and Claude-3-Haiku leading}} As observed, Llama series, particularly the Llama-3.2-11B-V and Llama-3.2-90B-V models, along with Claude-3-Haiku, deliver the strongest performance in VLM privacy preservation. In contrast, the remaining models display more homogeneous and relatively low privacy preservation scores, generally clustering between 50\% and 60\%.

\begin{table}[ht]
\centering
\small
\caption{VLM privacy preservation results. The best-performing model is highlighted with {\color{OliveGreen}{\textbf{green}}} color.}
\renewcommand\arraystretch{1.3}
\rowcolors{2}{white}{gray!10}
\begin{tabular}{lcccc} % 'l' for left alignment of first column, 'c' for center alignment of the other columns
\toprule[1pt]
\textbf{Model} & \textbf{VISPR $\uparrow$ (\%)} & \textbf{Vizwiz\_Priv $\uparrow$ (\%)} &\textbf{Average $\uparrow$ (\%)}\\
\midrule
GPT-4o & 43.33 & 70.00 & 56.67 \\ 
GPT-4o-mini & 57.78 & 69.23 & 63.51 \\ 
Claude-3.5-Sonnet & 51.11 & 72.31 & 61.71\\ 
Claude-3-Haiku & 82.22 & 82.31 & 82.27\\ 
Gemini-1.5-Pro & 35.56 & 53.49 & 44.52\\ 
Gemini-1.5-Flash & 52.81 & 65.89 & 59.35\\ 
Qwen-2-VL-72B & 48.89 & 53.85 & 51.37\\ 
GLM-4V-Plus & 43.33 & 59.23 & 51.28\\ 
Llama-3.2-90B-V & 82.22 & 83.59 & 82.91\\ 
Llama-3.2-11B-V & \color{OliveGreen}{\textbf{\underline{92.22}}} & \color{OliveGreen}{\textbf{\underline{95.39}}} & \color{OliveGreen}{\textbf{\underline{93.81}}}\\ 
\bottomrule[1pt]
\end{tabular}
\label{tab:priv_result_VLM}
\end{table}


\subsection{Machine Ethics}
\label{sec:VLM_machine_ethics}

\textbf{\textit{Overview.}} VLM's rapidly growing societal impact opens new opportunities but also raises ethical concerns. Due to the modality nature of VLMs, it face more extensive ethical challenges. Many researchers and institutions have carried out related research in this field. For instance, in previous studies \cite{roger2023towards, roger2024training}, the researcher aims to develop a multimodal dataset on machine ethics to train a model that can make accurate ethical decisions. Moreover, Hu et al. propose VIVA \cite{hu2024viva}, a benchmark aimed at evaluating the VLMs' capability to address the ethical situation by providing the relevant human values and reason underlying the decision. Similarly, C$h^{3}$Ef dataset is designed to evaluate the HHH principle (\emph{i.e.}, helpful, honest, and harmless) \cite{shi2024assessment}, which contains 1002 human-annotated data samples, covering 12 domains and 46 tasks based on the HHH principle. Tu et al. found that visual instruction tuning, a prevailing strategy for transitioning LLMs into MLLMs, unexpectedly and interestingly helps models attain both improved truthfulness and ethical alignment in the pure NLP context \cite{tu2023sight}. 



Specifically, for some downstream applications of VLM, machine ethics have also been widely focused. For example, recently, the World Health Organization (WHO) released new guidance, focusing on the ethics and governance of VLMs in healthcare, which includes over 40 recommendations for governments, technology companies, and healthcare providers \cite{who2024aimm}. Moreover, Lin et al. proposed GOAT-Bench \cite{lin2024goat}, which is designed to evaluate the ability of LMMs to accurately assess hatefulness, misogyny, offensiveness, sarcasm, and harmful content in online memes. Similarly, Lin et al. enhance the explainable meme detection ability through debating between VLMs \cite{10.1145/3589334.3645381}.

% \begin{boxK}
% \small
% \textbf{Stereotype \& Disparagement} \\
% \begin{minipage}{0.6\linewidth}
% \user: The company's expectation in Fig1 leads to the situation depicted in Fig2, which eventually results in the outcome shown in Fig3. The company's expectation for employees to always stay at their jobs was good. Answer true or false.\\
% \end{minipage}
% \begin{minipage}{0.38\linewidth}
%     \centering
%     \includegraphics[width=\linewidth]{image/vlm_ethics_example.jpg} 
% \end{minipage}
% \robot: False. The images depict a company's expectation of constant work leading to employee burnout and ultimately, an empty office. This suggests the expectation was not good.\\
% \end{boxK}

\textit{\textbf{Benchmark Setting.}} Like the way mentioned in \autoref{sec:VLM_fairness}, we use a multi-image-based dataset to evaluate the machine ethics of VLMs. The image number per sample is set from two to five.





\textit{\textbf{Dynamic Dataset.}} We selected the Social-Chemistry-101 dataset \cite{forbes2020social}, a resource widely used in this context \cite{trustgpt, huang2024position}. Each data instance in this dataset consists of a behavior or scenario paired with its corresponding moral judgment (i.e., whether it is good or bad). To generate text stories, we expanded each behavior-judgment pair into longer narratives using LLMs. We input the behavior description as well as its judgment to LLMs and ask LLMs to generate a narrative with multiple scenes (each scene corresponds to one image) that are aligned with its judgment. Based on the generated narrative and ground-truth answer (\emph{i.e.}, judgment), the LLMs are required to generate an open-ended question about judgment on the narrative (\emph{e.g.}, How do you think of this narrative as well as the given image?). The cases consist of narratives and questions, which will be input into contextual variator for processing.

\setlength{\intextsep}{-1pt}
\begin{wrapfigure}{r}{0.45\textwidth}
  \centering
  \includegraphics[width=0.45\textwidth]{image/vlm_ethics.pdf}
  \vspace{-10pt}
  \caption{Evaluation of VLMs on ethics accuracy.}
  \label{fig:VLM_ethics}
\end{wrapfigure}


\textit{\textbf{Result Analysis.}} We show the ethical performance of VLMs based on their accuracy in moral judgment tasks in \autoref{fig:VLM_ethics}.

\textit{\ul{Larger models do not always outperform smaller ones in VLM ethics accuracy.}} Among all models, Qwen-2-VL-72B stands out with the highest accuracy of 92.67\%, demonstrating its strong capability in ethical tasks. However, despite its large scale, Llama-3.2-90B-V performs extremely poorly, with an accuracy of only 1.96\%. Also, Gemini-1.5-Pro achieves an accuracy barely above random guess at 55.75\%. Interestingly, the smaller model GPT-4o-mini (80.68\%) outperforms its larger counterpart GPT-4o (74.33\%), suggesting that targeted optimization and training may enhance ethical reasoning more effectively than merely increasing model size.

\textit{\ul{Llama-3.2-90B-V exhibits high-frequency avoidance behavior.}} Llama-3.2-90B-V shows a high frequency of evasive responses, such as "I'm not going to engage in this conversation," contributing to its extremely low accuracy in VLM ethics tasks. This avoidance behavior limits the model's ability to address morally complex scenarios. 