\section{Introduction}


\textit{"Trust is the glue of life. It's the most essential ingredient in effective communication. It's the foundational principle that holds all relationships."}

\hfill -- \textit{Stephen R. Covey}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{image/milestone.pdf}
    \caption{Milestones of trustworthy generative foundation models from Oct. 2022 to Jan. 2025.}
    \label{fig:milestone}
    \vspace{-5pt}
\end{figure}


% \heng{In the introduction, (1) it would be good to summarize what's new in this benchmark, compared to existing ones. Is it the most comprehensive one? covering all dimensions? Did you release a toolkit/library so it's easy to use? if so emphasize that too. (2) I also feel the overview of successes is too much industry-focused. (3). Do we need to worry about scalability too? Currently if the models are larger, they perform stronger on traditional tasks such as memorization and knowledge reasoning, but weaker on unconventional but important tasks such as privacy preservation and personalization. It will be good to talk about this tradeoff.}
% General intro about AI models 
Generative models, a class of machine learning models, are trained to learn the underlying data distribution and generate new data instances that resemble the characteristics of the training dataset \cite{harshvardhan2020comprehensive, cao2024survey}. These models have garnered significant attention due to their wide range of applications, including generating realistic images \cite{dalle3}, texts \cite{liang2024controllable, wu2024unigen} or videos \cite{liu2024sora}, as well as potentially driving advancements in areas such as scientific discovery \cite{zhang2023artificial, guo2023large, liang2024scemqa, zhang2024comprehensive}, healthcare \cite{bolton2024biomedlm, zhang2023biomedgpt, liu2023deidgpt, luo2023biomedgpt}, autonomous systems \cite{guo2024large, agentbench, metatool}. Common generative models include traditional models like Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, Variational Autoencoders (VAEs) \cite{kingma2013auto}, Diffusion Models \cite{yang2023diffusion}, as well as Large Language Models (LLMs) \cite{zhao2023survey}, which have demonstrated remarkable capabilities in generating content that is often indistinguishable from human-produced ones.

%% This paragraph discusses LLMs and how they use lots of pre-training data and lots of compute
In recent years, foundation models, which are defined as large-scale pre-trained models (from BERT \cite{bert, liu2019roberta, beltagy2019scibert}, a series of OpenAI's GPT models \cite{radford2018improving,ChatGPT,GPT-4} to the Llama model family \cite{llama,llama2,meta_llama32_2024}) that serve as general-purpose systems for various downstream tasks \cite{bommasani2021opportunities}, have brought generative modeling to new heights. These models are distinguished by their extensive use of massive datasets \cite{liu2024datasets} and computational resources during pre-training \cite{hu2021lora}, enabling them to generalize effectively across diverse applications \cite{subramanian2024towards, yuan2021florence, liang2024foundation, gao2024survey, moor2023foundation, li2024political}.

% Talk about generative LMs and how they excel at different tasks and how they're being adopted not only for generation text but also for taking actions in real-world and thus their use in agents

 Foundation models may serve a wide array of tasks; for example, non-generative foundation models like BERT \cite{bert} are primarily designed for tasks such as text classification or language understanding, rather than content generation. In contrast, generative foundation models (GenFMs) \cite{evgenfm2024} are specifically adapted for generative tasks, excelling in creating new instances such as images, texts, or other data forms based on their training.
% When foundation models are adapted for generative tasks, they are termed Generative Foundation Models (GenFMs) \cite{evgenfm2024}.
Formally, GenFMs refer to large-scale, pre-trained architectures that leverage extensive pre-training to excel in generative tasks across various modalities and domains. These models are poised to revolutionize industries by pushing the boundaries of content creation, decision-making, and autonomous systems \cite{agentbench, guo2024large}, thus highlighting their transformative potential in both research and practical applications. 

% Defining GenFMs is not static; they evolve rapidly as new models are developed and released. For example, GPT-3 \cite{brown2020language}, once regarded as a groundbreaking GenFM, has seen its influence significantly diminish with the emergence of more advanced models (\emph{e.g.}, GPT-4 \cite{GPT-4}) in these two years. As the field advances, models that once represented the pinnacle of generative capabilities may no longer define the state of the art. This dynamic progression necessitates ongoing reassessment of the GenFM landscape to identify and focus on the most impactful and relevant models. Accordingly, in this paper, we do not specifically address earlier models such as GPT-3, whose relevance has waned relative to newer, more powerful generative models.


% the risk that may emerge from LLMs if no safeguards are put in place to ensure safety
As GenFMs continue to gain widespread adoption across diverse industries, ensuring their trustworthiness has become a pressing concern. As shown in \autoref{fig:milestone}, the focus on trustworthiness has grown alongside the advancement of GenFMs themselves. Even the most advanced models, such as GPT-4, have demonstrated vulnerabilities to novel attacks, like the “jailbreak” exploit \cite{wei2024jailbroken}, which can bypass intended safeguards \cite{zou2023universal}. With the increase in incidents where GenFMs have behaved unpredictably or unethically, the urgency to address their reliability cannot be overstated \cite{garcia_v_character_2024}.
%\textcolor{red}{[ADD REFERENCE: Garcia v. Character Technologies, Inc., 6:24-cv-01903 (this is a US case law regarding a boy who committed suicide allegedly due to unethical/unprofessional AI interaction. See: \href{https://drive.google.com/file/d/1vHHNfHjexXDjQFPbGmxV5o1y2zPOW-sj/view}{Link1} and \href{https://www.courtlistener.com/docket/69300919/garcia-v-character-technologies-inc/?utm_source=chatgpt.com}{Link2})]}. 
For example, popular text-to-image models like DALLE-3 \cite{dalle3} have been manipulated to bypass safety filters \cite{yang2024sneakyprompt, technologyreview2023texttoimage}, while LLMs have raised serious concerns about privacy leaks \cite{huang2024position}. The realistic outputs generated by GenFMs—whether in the form of text, images, or videos—are often indistinguishable from human-created content. 
This poses significant risks, including the potential spread of misinformation \cite{huang2023harnessing}, the creation of deepfakes \cite{zhang-etal-2024-llm}, and the amplification of biased or harmful narratives \cite{ye2024justice}. 
As shown in \autoref{fig:intro}, with the advancement of the social and societal impact of GenFMs, these issues threaten to erode public trust in the technology itself as well as in the institutions that utilize it~\cite{solaiman2023evaluating}.


% Motivate why trustworthiness is needed in LLMs 
The challenge of establishing trust in GenFMs is considerably more complex than traditional models (\emph{e.g.}, BERT \cite{bert} without generation capabilities), which are typically designed to excel in specific, well-defined tasks. In contrast, foundation models are pre-trained on massive, heterogeneous datasets, allowing them to generalize across a wide range of applications \cite{kolides2023artificial}. This broad versatility introduces significant challenges in assessing trustworthiness, as it requires evaluating model behavior across diverse tasks and contexts to ensure consistent reliability and adherence to ethical standards. Additionally, the societal impact of GenFMs extends far beyond that of traditional models \cite{solaiman2023evaluating}. While the latter may influence specialized domains, GenFMs have the potential to shape public opinion, influence policy decisions, and generate content that mimics authoritative sources, potentially disrupting democratic processes and the broader information ecosystem \cite{bommasani2021opportunities, myers2024foundation}. 

%% discuss why it's challenging to make them trustworthy 
The sheer scale and complexity of GenFMs, often consisting of billions of parameters, make them inherently opaque and difficult to interpret. This lack of transparency complicates efforts to establish accountability, especially when these models produce outputs with far-reaching social implications. Moreover, the dynamic nature of these models—continuously evolving through fine-tuning and updates—poses additional challenges for maintaining consistent safety protocols, ensuring compliance with ethical guidelines, and establishing mechanisms for traceability. Together, these factors collectively underscore the urgent need for rigorous frameworks to evaluate and enhance the trustworthiness of GenFMs, ensuring their safe and responsible integration into critical applications.

% pose question that will be likely answered in the paper
Despite significant efforts by major corporations to enhance the trustworthiness of GenFMs—such as OpenAI's establishment of the Red Teaming Network to bolster model safety \cite{openai_red_teaming_network}, Google's best practices for responsible AI development \cite{googleresponsibleAI, googleresearchresponsibleAI, googleresponsibleAIpractices}, and Meta's release of Llama Guard to protect prompt integrity \cite{llama_prompt_guard}—a critical and urgent question remains unanswered: \textit{What are the inherent limitations and uncertainties in the trustworthiness of GenFMs, and to what extent can GenFMs be trusted to uphold truthfulness, safety, privacy, and other critical dimensions of trustworthiness in diverse and dynamic real-world contexts?}
%can we rely on them to consistently uphold truthfulness, safety, privacy, and other critical dimensions of trustworthiness across diverse and dynamic real-world contexts?}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image/intro.pdf}
    \caption{Left: The progression of GenFMs from untrustworthy (with risks like privacy leakage and misuse) to trustworthy (featuring like robustness and value alignment). Right: As these models advance from Low-utility (Limited Impact) to High-utility (Significant Impact), ensuring trustworthiness becomes critical due to their expanding social influence.}
    \label{fig:intro}
    \vspace{-15pt}
\end{figure}


% Discuss the contribution: a unified set of guidelines 
% The contribution is motived by the lack of coherence of standards from governments about trustworthiness 
Given the advanced capabilities and far-reaching impacts of GenFMs, establishing a unified framework for defining, assessing, and guiding the enhancement of their trustworthiness is essential. Currently, various companies and developers have independently defined trustworthiness principles, model specifications, and user policies for generative models (detailed in \textbf{\S\ref{sec:approaches_corporate}}). Simultaneously, numerous governments and regulatory bodies have introduced varied laws and regulations to define trustworthy generative AI models. While some jurisdictions adopt horizontal governance frameworks that regulate AI systems as a whole, such as the EU AI Act \cite{eu_ai_act} and Blueprint for an AI Bill of Rights \cite{BlueprintAIBill2022}, others have implemented vertical regulatory approaches targeting specific domains, such as generative AI services \cite{china_gen_ai_regulations} and healthcare applications \cite{wto2024lmm}. However, these standards are highly diverse, often reflecting the specific priorities of different stakeholders. This lack of cohesion leads to fragmented and sometimes conflicting or inconsistent definitions of trustworthiness. We are motivated to propose a standardized set of guidelines to address this gap. By synthesizing existing principles, policies, and regulations, we aim to distill a unified set of guidelines that can serve as a foundational reference. These guidelines are designed to be adaptable, offering a consistent, cross-disciplinary framework for assessing and defining trustworthiness in GenFMs, which assists new developers and policymakers by offering a clear starting point as well as promoting alignment across industries and regulatory environments. With these guidelines in place (detailed in \textbf{\S\ref{sec:guideline}}), developers, organizations, and regulators can more effectively define and implement their trustworthiness policies, tailored to their unique needs, while still adhering to a common set of core principles.


After proposing the guidelines, the next critical step in assessing GenFMs' trustworthiness is developing an evaluation framework. However, one key challenge is that static evaluations of GenFMs, even at a large scale, are not sustainable as a means to build long-term trust. With the continuous release of new models and the evolving needs of users across diverse applications, repeatedly organizing large-scale evaluations becomes impractical. The process is too time-consuming and inflexible, requiring careful construction of appropriate evaluation datasets, selection or design of suitable metrics, and implementation of robust evaluation methodologies (e.g., designing effective prompt structures). Therefore, there is an urgent need for an adaptive and easy-to-use evaluation platform that can accommodate the diverse requirements when assessing the trustworthiness of GenFMs. To bridge this gap, we present \textsc{TrustGen}, a comprehensive and adaptive benchmark designed to evaluate GenFMs across multiple dimensions of trustworthiness through diverse and dynamic evaluation strategies. Specifically, \textsc{TrustGen} integrates three core modules: a  \textit{Metadata Curator}, a \textit{Test Case Builder}, and a \textit{Contextual Variator}, enabling iterative dataset refinement to support dynamic evaluations, as illustrated in Figure \ref{fig:overview} of \textbf{\S\ref{sec:benchmark_design}}). 
The \textit{Metadata Curator} dynamically collects metadata by employing different strategies like web-browsing agent \cite{agentbench}. 
The \textit{Test Case Builder} is designed to generate test cases based on the given metadata, while the \textit{Contextual Variator} ensures that the cases are varied and representative in different contexts to avoid the negative impact of prompt sensitivity. 

\textsc{TrustGen} evaluates three categories of GenFMs: text-to-image models, large language models, and vision-language models. We present the assessment of these models in \textbf{\S\ref{sec:text2image}, \S\ref{sec:LLM}, \S\ref{sec:VLM}}), and summarize their overall trustworthiness scores (out of 100, as defined in \textbf{\S\ref{sec:construction}}) in Figure \ref{fig:overall_t2i}, \ref{fig:overall_llm}, \ref{fig:overall_vlm}. We find that:
%As shown in \autoref{fig:overall_t2i}, \autoref{fig:overall_llm}, and \autoref{fig:overall_vlm}
%The overall results show that: 
\begin{itemize}[nolistsep, leftmargin=*]
\item \textit{\ul{1) The latest state-of-the-art GenFMs generally perform well, but they still face "trustworthiness bottlenecks".}} Our analysis reveals that the overall performance of evaluated GenFMs on the \textsc{TrustGen} benchmark shows promise, with the majority of models across all three categories achieving a relatively high trustworthiness score. This score indicates that these models exhibit alignment with key trustworthiness dimensions. However, while such a score reflects progress in meeting these criteria, it does not imply that the models are reliable or trustworthy in all contexts. Significant room remains for improvement in addressing specific and nuanced trustworthiness challenges. 
\item \textit{\ul{2) Open-source models are no longer as "untrustworthy" as commonly perceived, with some open-source models now closely matching or even surpassing the performance of frontier proprietary models.}} Our evaluation demonstrates that open-source models can achieve trustworthiness on par with, or even surpass, proprietary models, partially corroborating findings from previous studies \cite{huang2024position}. For example, CogView-3-Plus attained the highest trustworthiness score, outperforming leading proprietary models like DALL-E-3. Additionally, Llama-3.2-70B exhibited performance comparable to GPT-4o. These results indicate that with appropriate training strategies and robust safeguards, open-source models have the potential to compete with and even lead in trustworthiness metrics.\vspace{+0.05in}
\item \textit{\ul{3) The trustworthiness gap among the most advanced models has further narrowed compared to previous iterations.}} Our findings suggest that the disparity in trustworthiness among the latest models is diminishing compared to the previous study \cite{huang2024position}, with score differences generally below 10. This convergence can likely be attributed to increased knowledge sharing and collaboration within the industry, enabling the adoption of best practices across different models. Moreover, this trend reflects a growing, more sophisticated understanding of trustworthiness principles, leading to more consistent enhancements across various model architectures. \vspace{+0.05in}
\item \textit{\ul{4) Trustworthiness is not an isolated attribute of a model; rather, it creates a "ripple effect" across various aspects of performance.}} Our evaluations revealed several noteworthy phenomena, such as certain LLMs exhibiting excessive caution even when responding to benign queries, which in turn may diminish their helpfulness. Moreover, the various dimensions of trustworthiness appear to be intricately linked—decisions made in moral dilemmas (\textbf{\S\ref{sec:discussion_ethics}}), for instance, can be significantly influenced by the model's underlying preferences. Additionally, trustworthiness is closely intertwined with a model’s utility performance and the design principles set forth by its developers, indicating that improvements in one dimension may have cascading effects on others. 

\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{image/Thrust.pdf}
    \caption{Three contributions of this paper: A standardized set of guidelines for trustworthy GenFMS, dynamic evaluation on the trustworthiness of GenFMs, and in-depth discussion on challenges and future research.}
    \label{fig:thrust}
    \vspace{-15pt}
\end{figure}

% Finally, we provide an in-depth discussion on the topic of trustworthy GenFMs, covering key aspects such as the fundamental nature of trustworthiness, evaluation methodologies, the role of interdisciplinary collaboration, societal and downstream implications, as well as trustworthiness-related technical approaches, which highlights existing challenges but also outlines potential research directions to inform and guide future developments in the field.

The complexities of trustworthiness extend beyond what can be captured by metrics and frameworks alone. Therefore, to ensure a comprehensive understanding and continued progress in this domain, we conclude with an in-depth discussion that addresses key aspects of trustworthy GenFMs (in \textbf{\S\ref{sec:discussion}}). This discussion explores the fundamental nature of trustworthiness, evaluation methodologies, the vital role of interdisciplinary collaboration, societal and downstream implications, as well as trustworthiness-related technical strategies. By examining these dimensions, we highlight current challenges and identify promising research directions, which serve to inform and guide future developments, ensuring that GenFMs evolve in a way that aligns with human values and societal expectations.


% trustworthiness of GenFMs. 1) Why is it important? What are the differences between traditional trustworthiness? 2) What's the challenge of this topic?

% raise our research question

% what we do \& our contribution


\textbf{Contributions.} Overall, the contributions of this work are three-fold, as shown in \autoref{fig:thrust}:

\begin{itemize}[nolistsep, leftmargin=*]
    \item \textbf{Comprehensive Identification and Establishment of Guidelines for Trustworthy Generative Models.} We conducted a multidisciplinary collaboration involving experts from diverse fields such as NLP, Computer Vision (CV), Human-Computer Interaction (HCI), Computer Security, Medicine, Computational Social Science, Robotics, Data Mining, Law, and AI for Science. This collaboration aimed to integrate domain-specific insights into defining trustworthiness in the context of GenFMs. Through an exhaustive review of existing literature, along with a thorough analysis of global policies and regulatory frameworks, we developed a comprehensive set of guidelines. These guidelines are systematically structured around critical perspectives, including legal compliance, ethical and social responsibilities, risk management, user-centered design principles, and adaptability and sustainability. They establish a unified paradigm and model specifications that serve as a foundational standard to ensure the trustworthiness of generative models. \vspace{+0.1in}
    \item \textbf{A Holistic and Dynamic Evaluation Framework for GenFMs: \textsc{TrustGen}.} We present \textsc{TrustGen}, a pioneering, holistic, and fully dynamic benchmark carefully designed to assess the trustworthiness of generative models. Unlike existing static benchmarks, \textsc{TrustGen} encompasses a comprehensive range of models, including text-to-image, large language, and vision-language models, and evaluates them across multiple critical dimensions such as truthfulness, safety, fairness, privacy, robustness, machine ethics, and advanced AI risks. By incorporating modular components, \textsc{TrustGen} dynamically assesses evolving model capabilities, addressing the limitations of static evaluation frameworks. This dynamic nature significantly reduces the risk of data contamination, enhances the accuracy and reliability of evaluations, and guarantees the robustness of continuous assessment. Our experimental findings using \textsc{TrustGen} provide an in-depth analysis of the current trustworthiness landscape of GenFMs, offering actionable insights to address challenges and identify opportunities for fostering trust in generative AI. Moreover, we also release the open-source toolkit, \textsc{\textbf{TrustEval}}, to facilitate dynamic evaluation on the trustworthiness of GenFMs\footnote{\url{https://github.com/TrustGen/TrustEval-toolkit}}.
    \item \textbf{Strategic In-Depth Discussion of Challenges and Future Directions.} We provide an extensive, forward-looking discussion on the critical challenges surrounding the trustworthiness of generative models. Our discussion underscores the complex, evolving nature of trustworthiness, highlighting the nuanced trade-offs between maximizing utility performance and the impact guided by trustworthiness. We delve into key challenges in evaluating trustworthiness, particularly in areas such as safety, fairness, and ethical implications. Through this analysis, we identify persistent challenges and provide a strategic roadmap for future research. Our goal is to advance the development of trustworthy generative AI by addressing these challenges and identifying innovative solutions to enhance trust across diverse applications.
\end{itemize}


\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{image/all_res_1.pdf}
    \caption{Overall performance (trustworthiness score) of text-to-image models.}
    \label{fig:overall_t2i}
    \vspace{-5pt}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{image/all_res_2.pdf}
    \caption{Overall performance (trustworthiness score) of large language models. ``Advanced.'' means advanced AI risk.}
    \label{fig:overall_llm}
    %\vspace{-5pt}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{image/all_res_3.pdf}
    \caption{Overall performance (trustworthiness score) of vision-language models.}
    \label{fig:overall_vlm}
    \vspace{-10pt}
\end{figure}







\textbf{Paper Organization \& Reader Guideline.} First, we provide an overview of GenFMs, covering: 1) approaches for ensuring trustworthiness at the corporate level (\textbf{\S\ref{sec:approaches_corporate}}), and related work on their evaluation and benchmarking (\textbf{\S\ref{sec:evaluation_related_work}} and \textbf{\S\ref{sec:benchmark_related_work}}). Based on them, subsequently, we present a standardized set of guidelines for trustworthy GenFMs in \textbf{\S\ref{sec:guideline}}, detailing the considerations for establishing these guidelines (\textbf{\S\ref{sec:guideline_consideration}}) and the specific content of the guidelines (\textbf{\S\ref{sec:guideline_content}}). Next, we discuss the design of the benchmark in \textbf{\S\ref{sec:benchmark_design}}, followed by evaluation details and results of text-to-image models (\textbf{\S\ref{sec:text2image}}), large language models (\textbf{\S\ref{sec:LLM}}), and vision-language models (\textbf{\S\ref{sec:VLM}}), from various dimensions: truthfulness, safety, fairness, robustness, privacy, machine ethics, and advanced AI risk. Additionally, we explore the trustworthiness of other generative models in \textbf{\S\ref{sec:others}} and assess the trustworthiness of downstream applications using GenFMs in \textbf{\S\ref{sec:application}}. Finally, from multiple perspectives, we provide an in-depth discussion of this field's current challenges and future directions in \textbf{\S\ref{sec:discussion}}.

