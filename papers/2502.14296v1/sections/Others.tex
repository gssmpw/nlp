\clearpage
\section{Other Generative Models}
\label{sec:others}

\subsection{Any-to-Any Models}

Research has begun to extend understanding and generative tasks to various modalities, including music \citep{fei2024flux}, speech \citep{shu2023llasm}, video \citep{chen2024panda}, infrared \citep{gao2016infar}, and even touch \citep{fu2024touch}. These models, known as any-to-any models, can perform tasks across multiple modalities. Pioneering in aligning different modalities, ImageBind \citep{han2023imagebind} aligns different modalities to image embeddings, achieving the first unified representation of multiple modalities that can be generally applied to traditional tasks. LanguageBind \citep{zhu2023languagebind}, on the other hand, aligns various modalities to language, paving the way for powerful reasoning capabilities across multimodal interaction with an LLM backbone \citep{zhu2024llmbind,girdhar2023imagebind,wu2023next, zhan2024anygpt, tang2024codi,li2024mini}.

GPT-4o family \citep{openai_gpt4o_system_card, openai2024gpt4omini}, as an end-to-end model for generating speech and images, has sparked widespread interest. Gemini \citep{team2023gemini}, as the pioneer in unifying image understanding and generation, also sparked insights for many open-source works in introducing vision generation within a unified framework \citep{li2024mini, chen2024interleaved}. Furthermore, some frameworks achieve broader modal interaction through visual programming \citep{gupta2023visual, suris2023vipergpt}, drawing wisdom from the collaboration of various existing SOTA models through tools usage \citep{ma2024m, hu2024visual, liu2023llava}. More recently, researchers have begun exploring the combination of transformers and diffusion models for end-to-end training, unifying multimodal understanding and generation tasks within a single framework \citep{zhou2024transfusion, xie2024show, team2024chameleon, chern2024anole, koh2024generating, li2024mini, wu2023next}, showing potential for stronger consistency and usage within interleaved text-and-image tasks.


However, a comprehensive investigation into the safety implications of Any-to-Any models remains a critical gap in current research. GPT4Video \citep{wang2023gpt4video} has taken initial steps in addressing safety-aware video generation within an Any-to-Any framework. Similarly, He et al. have highlighted trustworthiness concerns in multimodal generation tasks, such as image generation and editing, when combining language with other modal outputs \citep{he2024llms}. The safety report for GPT-4o further underscores this need, revealing potential safety issues within this advanced model, particularly in voice generation tasks \citep{openai_gpt4o_system_card}. Chen et al. present and emphasize trustworthy problems such as jailbreaks and unexpected variations in prompts in interleaved text-and-image generation, which is one of the most potential downstream tasks of any-to-any generation \citep{chen2024interleaved}. These findings collectively emphasize the urgency of conducting thorough investigations into safety challenges as these models continue to evolve and increase in capability.

\subsection{Video Generative Models}

In recent years, text-to-video generation models have achieved remarkable advancements, paralleling the progress seen in text-to-image models \cite{singer2022make, cho2024sora, liu2024sora, OpenAI2024sora}. For example, Sora \cite{sora_openai, liu2024sora}, a sophisticated text-to-video model developed by OpenAI, can generate intricate scenes and dynamic videos based on user descriptions, demonstrating significant creativity and impressive visual effects. 


Many efforts collectively advance the trustworthiness and safety of text-to-video models, ensuring their development aligns with ethical considerations. To address the safety concerns associated with text-to-video models, various benchmarks have been proposed to evaluate and mitigate risks. T2VSafetyBench \cite{miao2024t2vsafetybench} has been introduced as a comprehensive framework for safety-critical assessments of text-to-video models, covering 12 essential aspects of video generation safety and incorporating a malicious prompt dataset created using LLMs and jailbreaking prompt attacks. Similarly, Pan et al. \cite{pang2024towards} focus on identifying unsafe content generated by video models. They collect a substantial number of generation prompts and employ three open-source video models to produce potentially unsafe videos, which are then manually labeled to create the first dataset dedicated to unsafe video content. In addition, they develop an innovative approach known as Latent Variable Defense to prevent the generation of harmful videos.

Furthermore, to mitigate the potential misuse of video models, Pang et al. \cite{pang2024vgmshield} introduce \textsc{VGMShield}, a suite of three pioneering mitigation strategies designed to be applied throughout the lifecycle of fake video generation. In efforts to reduce harmful content in model outputs, GPT4Video leverages the real-toxicity-prompts dataset \cite{gehman2020realtoxicityprompts}, employing GPT-4 to generate refusals as responses, thereby training models to avoid producing harmful content \cite{wang2023gpt4video}. Additionally, Dai et al.  \cite{dai2024safesora} propose the SafeSora dataset, aimed at fostering research on aligning text-to-video generation with human values. This dataset includes human preferences in video generation tasks, emphasizing the importance of producing content that is both helpful and harmless. 

AI-generated videos may raise concerns about the spread of misinformation. In response, extensive efforts have been directed towards developing forgery detection models and establishing robust benchmarks. New datasets \cite{chen2024demamba,he2024exposing} have been specifically constructed for AI-generated video forensics, facilitating community research in detecting and analyzing synthetic video content. Simultaneously, advanced fake video detectors have been proposed \cite{vahdati2024beyond,ma2024decof,chang2024matters,nguyen2024videofact}, further enhancing our ability to identify and mitigate the impacts of false information.
These technological advancements are vital for protecting the public against the harmful effects of misinformation. They improve the transparency and authenticity of information dissemination and safeguard personal privacy by ensuring that synthetic media can be reliably identified and handled appropriately. 

\subsection{Audio Generative Models}
The emergence of audio generative models like CoDi \citep{tang2024codi} and NextGPT \citep{wu2023next} enables systems to process and generate multiple modalities—including text, vision, and audio—within a unified framework \citep{fu2024vita, li2024baichuan, chen2024emova, luo2025openomni}. In audio generation, they synthesize speech in an end-to-end manner to create rich, immersive content for voice-assisted technologies \citep{kulkarni2022speech}, voice chatbots \citep{chen2024voicebench}, and enhanced virtual reality experiences \citep{morotti2020fostering}.

The primary safety concern with audio generative models is the potential misuse in creating audio deepfakes—highly realistic synthetic voices that can impersonate individuals without consent \citep{khanjani2023audio, blue2022you, mai2023warning}. High-fidelity audio generative models like GPT-4o amplify this risk, as they can produce speech that closely mimics a person's voice and speaking style, which can be exploited for fraudulent activities such as impersonation scams \citep{Stupp2019}, unauthorized access to secure systems via voice authentication \citep{Kimery2024}, and the dissemination of disinformation \citep{ChesneyCitron2019, Sample2019}.
Moreover, these models might inadvertently produce incorrect or fabricated information delivered convincingly via synthetic speech \citep{hurst2024gpt,li2024sonar}, similar to hallucinations observed in LLMs \citep{rawte2023survey}, especially combined with textual or visual content in real-world scenarios\citep{ying2024safebench}. Ethical considerations also arise from the unauthorized replication and use of individuals' voices, which infringes on personal rights and privacy. The use of personal voice data without permission can lead to identity theft, underscoring the need for safeguards to prevent unauthorized voice cloning such as watermark \citep{roman2024latent} or voice safeguarding \citep{mckee2024safeguarding}.

Fairness, robustness and privacy are other critical trustworthy issues in audio generative models. Fairness pertains to equitable performance across diverse populations; however, biases from non-diverse training data can cause models to favor certain accents or dialects while underperforming with others \citep{yu2024large}, marginalizing speakers from different linguistic backgrounds and perpetuating social inequalities. Robustness is essential as models must withstand noisy or malicious inputs that exploit vulnerabilities—such as cross-modal attacks where benign text is paired with malicious images—leading to unintended or harmful outputs \citep{xie2021enabling, shen2024voice,kang2024advwave}. Additionally, privacy is also a significant concern due to the sensitive nature of users' audio inputs and personal voice recordings; there's a risk of personal information leakage if models inadvertently reproduce sensitive data from training sets \citep{zhang2022volere}. Protecting personal information requires data anonymization, secure storage practices, and adherence to regulations like the General Data Protection Regulation (GDPR) \citep{hoofnagle2019european}, which is fundamental to maintaining public trust in these technologies.

In summary, given that audio generative models especially LLM-based ones are flourishing these days, trustworthy problems should be raised and require more attention \citep{hurst2024gpt}. Addressing these challenges calls for a collaborative effort among researchers, developers, policymakers, and diverse communities. By integrating technical innovation with ethical considerations and robust regulatory frameworks, it is possible to harness the benefits of audio-generative models responsibly to contribute to the development of trustworthy AI systems that respect individual rights and serve society as a whole.

\subsection{Generative Agents}

Generative model-based agents (\emph{e.g.}, LLM-based agents) have been widely used for handling complex tasks \cite{wang2024survey, pan2024autonomous, nasiriany2024pivot, liu2024visualagentbench, cao2024spider2, koh2024visualwebarena}. They are always equipped with external databases (\emph{e.g.}, Wikipedia \cite{shao2024assisting}) or tools \cite{metatool, qin2023toolllm, ling2023international, yang2024gpt4tools, zheng2024gpt, koh2024visualwebarena}, which enable them to complete the users' tasks effectively. For instance, agents can develop software by cooperation \cite{chatdev} and even can achieve complicated communication \cite{chen2024internet, li2023metaagents}.

However, recent studies also highlight the trustworthiness-related issues in generative model-based agents \cite{he2024emerged, gan2024navigating, shavit2023practices,zhang2024agentsafetybench,yin2024safeagentbench, andriushchenko2024agentharm}. From the perspective of their nature, they are vulnerable to various attacks. For instance, Zou et al. studied that LLM agents equipped with RAG were vulnerable to poison attacks \cite{zou2024poisonedrag, xue2024badrag} in both black-box and white-box settings, which highlights the need for new defenses. Yang et al. study the backdoor attack on agents in two typical scenarios: web shopping and tool utilization, unveiling the inefficient defenses against backdoor attacks on LLM-based agents \cite{yang2024watch}. Similarly, in BadAgent, research also uses backdoor attacks to manipulate the LLM agents \cite{wang2024badagent}, and the attack is extremely robust even after fine-tuning trustworthy data. Moreover, some researchers also evaluate the behavior of a network of models collaborating through debate under the influence of an adversary \cite{amayuelas2024multiagent}. Chen et al. propose AgentPoison, which aims to poison their long-term memory or RAG knowledge base \cite{chen2024agentpoison}. Zhang et al. launch an attack and cause malfunctions by misleading the agent into executing repetitive or irrelevant actions \cite{zhang2024breaking}. Zeng et al. also demonstrate the vulnerability of RAG systems to leaking the private retrieval database \cite{zeng2024good}. For example, the experiments underscore the potential for substantial privacy breaches through untargeted prompting. Zhang et al. propose ToolBeHonest \cite{zhang2024toolbehonest}, a benchmark designed to evaluate the hallucination of tool-augmented LLM agents. In this benchmark, they found larger model parameters do not guarantee better performance, and the training data and response strategies also play a crucial role in tool utilization. Huang et al. explored the resilience of different multi-agent topologies against attacks and investigated strategies to enhance the robustness of multi-agent frameworks against malicious agents \cite{huang2024resilience}. Yu et al. studied the topological safety in multi-agent networks and found several critical phenomena termed Agent Hallucination and Aggregation Safety \cite{yu2024netsafe}. Zhang et al. propose Psysafe \cite{zhang2024psysafe}, a benchmark designed to evaluate the safety of psychological-based attacks in multi-agent systems.
Agent-SafetyBench \cite{zhang2024agentsafetybench} evaluates LLM-based agents across 349 interaction environments and 2,000 test cases spanning 8 safety-risk categories, finding that none of the 16 tested agents surpass a 60\% safety score.
SafeAgentBench \cite{yin2024safeagentbench} focuses on safety-aware task planning for embodied LLM agents, offering 750 tasks covering 10 hazards, yet the leading baseline rejects only 5\% of hazardous tasks. These results underscore the urgent need for more robust defenses.
Meanwhile, trustworthiness-related issues exist in the agent application. In a recent study, Tian et al. thoroughly probe the safety aspects of these agents by elaborately conducting a series of manual jailbreak prompts along with a virtual chat-powered evil plan development team, dubbed Evil Geniuses \cite{tian2023evil}. Xu et al. utilize an LLM-based agent for automatic red-teaming, which leverages these jailbreak strategies to generate context-aware jailbreak prompts \cite{xu2024redagent}. Dong et al. leverage LLM agents to jailbreak text-to-image model \cite{dong2024jailbreaking}. The proposed multi-agent framework integratessuccessfully attackingflow, which successfully attacks the latest text-to-image models. AgentSmith \cite{gu2024agent} and another work \cite{tan2024wolf} also discuss the propagation of malicious content between generative model-based agents.

To mitigate the trustworthy concern of these agents, Zeng et al. utilize synthetic data to enhance the privacy-preserving of LLMs in the RAG scenario \cite{zeng2024mitigating}. Based on the AI constitution \cite{chen2024iteralign, 10.1145/3630106.3658979, petridis2024constitutionmaker}, TrustAgent \cite{hua2024trustagent} effectively enhances an LLM agent's safety across multiple domains by identifying and mitigating potential dangers during the planning. In the aspect of truthfulness, Yoffe et al. proposed the DebUnc framework \cite{yoffe2024debunc}, which leverages the method of uncertainty estimations to mitigate the hallucination in agents.


