
\newcommand{\coloredcheckmark}[1]{%
    \tikz[baseline=(checkIcon.base)]{
        \node[
            draw=gray!20,
            line width=0.01pt,
            fill=#1!70!black,
            rounded corners,
            inner sep=1.7pt
        ] (checkIcon) {\textcolor{white}{$\checkmark$}};
    }
}
\definecolor{blue}{HTML}{95bddc}


\newcommand{\leadercheckmark}[1]{%
    \tikz[baseline=(checkIcon.base)]{
        \node[
            draw=gray!20,
            line width=0.01pt,
            fill=#1!70!black,
            rounded corners,
            inner sep=1.7pt
        ] (checkIcon) {\textcolor{white}{$\checkmark^*$}};

    }
}

\definecolor{LightGray}{RGB}{155, 155, 155}
\definecolor{LightRed}{RGB}{255, 102, 102} 
\definecolor{LightGreen}{RGB}{102, 225, 102}
\definecolor{LightBlue}{RGB}{0, 76, 153} 
\definecolor{LightPurple}{RGB}{76, 0, 153}
\definecolor{NewBlue}{RGB}{10, 205, 205}




\section{Conclusion}
In this paper, we present a holistic framework for defining and evaluating trust in Generative Foundation Models (GenFMs), focusing on critical dimensions such as safety, fairness, privacy, robustness, machine ethics, and advanced AI risks. By integrating diverse perspectives from fields ranging from law and social science to NLP and computer security, we proposed a unified set of guidelines that aim to foster alignment and consistency across stakeholders. 
Building on these guidelines, we introduced \textsc{TrustGen}, a dynamic, holistic evaluation framework designed to adapt to the ever-evolving landscape of generative AI, enabling continuous and flexible assessments of trustworthiness in text-to-image, large language, and vision-language models.
Our empirical findings reveal substantial advances in the trustworthiness of leading GenFMs, while also uncovering critical gaps that underscore the need for rigorous, ongoing oversight.
Notably, open-source models are rapidly closing the trustworthiness gap with proprietary ones, underscoring the collaborative nature of the fieldâ€™s advancement. 
Finally, we conducted an in-depth discussion on the persistent challenges and future research directions, emphasizing ethical dilemmas, legal implications, and the broader societal impact of GenFMs. 
We conclude that ongoing collaboration, rigorous evaluation, and continuous refinement of both models and frameworks are essential to ensuring the responsible and beneficial integration of GenFMs into real-world applications.

% This paper introduces TrustGen, a comprehensive framework for evaluating and enhancing the trustworthiness of generative foundation models (GenFMs). We propose guidelines derived from multidisciplinary insights to address critical dimensions such as safety, fairness, privacy, robustness, and ethical alignment. TrustGen serves as a dynamic benchmarking platform, leveraging adaptive evaluation methodologies to accommodate the evolving capabilities of GenFMs and diverse real-world contexts.

% Our evaluation of text-to-image models, large language models, and vision-language models reveals key insights: GenFMs have made significant strides in trustworthiness, with most models achieving high scores across multiple dimensions. Open-source models now rival proprietary counterparts, demonstrating the potential of collaborative efforts and robust safeguards. However, persistent challenges remain, including the need to address nuanced trade-offs between utility and trustworthiness, mitigate biases, and ensure ethical accountability.

% This work underscores the interdependence of trustworthiness dimensions and the need for transparency and collaboration in advancing generative AI. TrustGen provides a scalable foundation for aligning GenFMs with societal values, offering actionable insights and tools for researchers, developers, and policymakers to drive responsible and impactful AI development.

\newpage

\section*{Diversity Statement}
Our research on trustworthy generative models inherently embraces and benefits from diverse perspectives across multiple disciplines and domains. The project brings together experts from a remarkably broad range of fields, including Natural Language Processing, Computer Vision, Human-Computer Interaction, Computer Security, Medicine, Computational Social Science, Robotics, Data Mining, Law, and AI for Science. Each field brings unique and crucial perspectives: computational social scientists and HCI experts inform our understanding of fairness, societal biases, machine ethics in different contexts, and human-centric safety considerations; security experts guide our evaluation of model robustness against different adversarial attacks and privacy preservation mechanisms; roboticists, medical and AI for science researchers help evaluate model truthfulness and reliability in physical interactions, critical healthcare and scientific research scenarios; and legal scholars help assess advanced AI risks and develop guidelines that align with global regulatory requirements and ethical standards. 
This interdisciplinary collaboration is particularly evident in this work, where diverse expertise has allowed us to evaluate models across multiple dimensions - from technical aspects like robustness and privacy to broader concerns like fairness, ethics, and social impact. 
Through these varied perspectives, we propose \textsc{TrustGen}, a dynamic evaluation framework that is more comprehensive and inclusive than traditional static benchmarks. 
It continuously adapts to evolving ethical standards and social norms, extending beyond academic disciplines.
Furthermore, our discussion spans a wide spectrum of concerns, from technical challenges in model trustworthiness and alignment to ethical considerations in downstream applications like medicine, robotics, AI for sciences, and human-AI collaboration, further demonstrating our commitment to diverse perspectives in trustworthy AI research.


% We believe that maintaining and expanding this diverse, interdisciplinary approach is crucial for addressing the complex challenges in developing trustworthy generative AI systems. Our work demonstrates that bringing together diverse perspectives not only enriches our understanding but also leads to more robust and comprehensive solutions for ensuring AI trustworthiness.




\section*{Contribution Statement}

All contributing professors made direct contributions to this paper. They were invited to revise specific sections in accordance with their respective areas of expertise. Their contributions included direct revisions to key sections, such as the Introduction, Guideline, and Benchmark Design; providing high-level conceptual input, including suggestions to enhance the rationale of the guideline and the standardization of the benchmark; offering feedback on the overall structure of the paper; proposing improvements to the usability of the toolkit; and making targeted revisions to content pertaining to specific dimensions. For the student authors, we detail their contributions in \autoref{tab:contributors_table}:


\begin{table}[htbp]
\centering
\renewcommand\arraystretch{1.3}
\caption{
    Student contributors' involvement in each section. 
    \textbf{Legend:} 
    \textcolor{red}{Red} for "Truthfulness," 
    \textcolor{LightBlue}{Blue} for "Fairness," 
    \textcolor{LightGreen}{Green} for "Robustness," 
    \textcolor{orange}{Orange} for "Safety," 
    \textcolor{LightPurple}{Purple} for "Privacy," 
    \textcolor{teal}{Teal} for "Machine Ethics," 
    and \textcolor{gray}{Gray} for "Preliminary Work or Related Work" and \textcolor{NewBlue}{Cyan} for Advanced AI Risk. 
    Leaders are marked with a \textcolor{black}{\textbf{*}} above their checkmarks.
}
\label{tab:contributors_table}
\adjustbox{max width=1.1\textwidth}{ % Automatically scales the table to fit within page width
\scriptsize % Shrinks text size for the entire table
\begin{tabular}{cccccccccccc}
\toprule
\textbf{Author} & \textbf{Coding} & \textbf{Vis.} & \textbf{\begin{tabular}[c]{@{}l@{}}Guide.\end{tabular}} & \textbf{\begin{tabular}[c]{@{}l@{}}Bench.\end{tabular}} & \textbf{Intro.} & \textbf{BG.} & \textbf{T2I} & \textbf{LLMs} & \textbf{VLMs} & \textbf{App.} & \textbf{Discuss.}  \\ \hline
H. B & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} &  &  &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightBlue} \coloredcheckmark{LightGreen} \coloredcheckmark{LightPurple} \coloredcheckmark{orange}  &  &  &  &   \\
D. C &  &  &  &  &  & & \coloredcheckmark{red} &  &  & \coloredcheckmark{LightGray} &    \\
R. C &  & \coloredcheckmark{LightGray} &  &  &  & \coloredcheckmark{LightGray} &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightBlue} & \coloredcheckmark{LightGray} &    \\
C. G & \coloredcheckmark{LightGray} & \leadercheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} &  & \coloredcheckmark{LightGray} &  & \coloredcheckmark{LightRed} \coloredcheckmark{LightBlue} & \coloredcheckmark{LightBlue} \coloredcheckmark{teal} &  & \coloredcheckmark{LightGray}   \\
K. G &  &  &  &  &  &  &  &  &  &  & \coloredcheckmark{LightGray}   \\
T. G &  &  &  &  &  &  &  &  & \coloredcheckmark{red} \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray}  \\
Y. H & \coloredcheckmark{LightGray} & \leadercheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightBlue} \coloredcheckmark{LightGreen} \coloredcheckmark{LightPurple} \coloredcheckmark{orange} & \coloredcheckmark{orange} \coloredcheckmark{teal} & \coloredcheckmark{LightBlue} \coloredcheckmark{teal} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray}   \\
Y. L &  &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} &  &  &  & \coloredcheckmark{NewBlue} &  &  & \coloredcheckmark{LightGray}   \\
Z. L &  &  &  &  &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightBlue} \coloredcheckmark{LightGreen} \coloredcheckmark{LightPurple} \coloredcheckmark{orange} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} &    \\
J. S &  & \coloredcheckmark{LightGray} &  &  &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGreen} & \coloredcheckmark{LightGreen} &  & \coloredcheckmark{LightGray}   \\
H. W &  &  &  &  &  & \coloredcheckmark{LightGray} &  & \coloredcheckmark{LightGray} \coloredcheckmark{red} &  &  & \coloredcheckmark{LightGray}  \\
S. W & \leadercheckmark{LightGray} & \coloredcheckmark{LightGray} &  &  &  & \coloredcheckmark{LightGray} &  & \coloredcheckmark{orange} \coloredcheckmark{teal} & \coloredcheckmark{teal} &  & \coloredcheckmark{LightGray}  \\
X. W &  &  &  &  &  & \coloredcheckmark{LightGray} &  & \coloredcheckmark{purple} \coloredcheckmark{teal} & \coloredcheckmark{purple} &  & \coloredcheckmark{LightGray}  \\
Y. W & \coloredcheckmark{LightGray} &  &  &  &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} &  &  &  &   \\
J. Y & \coloredcheckmark{LightGray} &  &  &  &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} &  &  &  &   \\
K. Z &  &  &  &  &  &  &  &  &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray}   \\
Q. Z &  & \coloredcheckmark{LightGray} &  &  & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} & \coloredcheckmark{LightGray} \coloredcheckmark{orange} & \coloredcheckmark{LightPurple} \coloredcheckmark{teal} &  &  & \coloredcheckmark{LightGray}  \\
Y. Z &  &  &  &  &  & \coloredcheckmark{LightGray} &  &  & \coloredcheckmark{LightGreen} \coloredcheckmark{orange} & 
\coloredcheckmark{LightGray} & \coloredcheckmark{LightGray}  \\ \bottomrule
\end{tabular}}
\end{table}


\section*{Acknowledgment}

Max Lamparth is partially supported by the Stanford Center for AI Safety, the Center for International Security and Cooperation, and the Stanford Existential Risk Initiative.