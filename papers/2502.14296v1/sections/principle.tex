\clearpage
\section{Guidelines of Trustworthy Generative Foundation Models}
\label{sec:guideline}


Trustworthiness of GenFMs is not a simple, one-dimensional characteristic—it encompasses a wide range of considerations, each of which can vary in importance depending on the context of the application. Just as \textit{The International Scientific Report on the Safety of Advanced AI} \cite{bengio2024international} mentioned, ``General-purpose AI can be applied for great good if properly governed.'' It is clear that a rigid, universal set of rules would not effectively address the diverse needs of different stakeholders, industries, and use cases.

\textbf{Motivation.} Our motivation for creating these guidelines stems from the recognition that flexibility is crucial. Rather than imposing strict, inflexible rules, we aim to provide a set of adaptable principles that can serve as a foundation for a wide range of stakeholders. These guidelines are not just for organizations to shape their internal policies but are also intended to support developers, regulators, and researchers in navigating the multifaceted landscape of trustworthiness. By offering a clear yet adaptable framework, we enable stakeholders to align with key ethical and legal standards while also allowing for innovation and customization in addressing their unique challenges.

\textbf{Functionality.} These guidelines serve as a versatile resource—not as directives, but as a flexible toolkit to inform decision-making, design processes, and evaluation strategies. Whether it's guiding a developer in building more trustworthy GenFMs, assisting regulators in assessing compliance, or helping researchers explore new trustworthiness dimensions, these guidelines provide a shared foundation. Ultimately, we aim to empower all involved in the ecosystem of GenFMs to enhance trustworthiness in a way that is both rigorous and adaptable, ensuring that these powerful technologies can be responsibly and effectively integrated into society.

\textbf{How do the guidelines differentiate from others?} The guidelines set themselves apart from existing frameworks, such as the European Union’s AI Act \cite{eu_ai_act} and the Blueprint for an AI Bill of Rights \cite{BlueprintAIBill2022}, by addressing the specific needs of stakeholders working with GenFMs. While the 'Blueprint' and 'Act' provide detailed, policy-oriented frameworks for broad regulatory oversight, our guidelines focus on being \textit{application-agnostic} and \textit{stakeholder-adaptive}, making them especially suited to the dynamic and diverse use cases of GenFMs. Importantly, the guidelines play a dual role as a "\textit{value anchor}" and a "\textit{value scale}" of trustworthy GenFMs. The value anchor offers a clear and consistent foundation of principles that define trustworthiness, ensuring alignment with core ethical, societal, and legal standards. At the same time, the guidelines empower developers and stakeholders to establish the value scale—the specific trustworthiness metrics, standards, and implementation strategies—tailored to the unique requirements of their models and applications. This flexibility allows for innovation and customization while maintaining a firm grounding in trustworthiness principles.

\subsection{Considerations of Establishing Guidelines}
\label{sec:guideline_consideration}

To define a set of guidelines to speculate the models' behavior to ensure their trustworthiness, we first establish the following considerations:


% \largedot~~\textbf{\textit{Legal Compliance.}} The guidelines should be designed to align with existing and evolving legal frameworks, ensuring that the model adheres to regulatory standards and avoids legal risks \cite{zeng2024ai}. This involves compliance with data protection laws (\emph{e.g.}, GDPR \cite{GDPR2016}), AI-specific regulations (\emph{e.g.}, EU AI Act \cite{eu_ai_act}), and other regional or sector-specific legal provisions. Legal compliance ensures that the models operate within lawful boundaries, safeguarding against potential misuse and preventing unauthorized or harmful behavior. This consideration also ensures that models are auditable, making demonstrating compliance to regulators and stakeholders easier.

\largedot~~\textbf{\textit{Ethics and Social Responsibility.}} Ethical considerations are essential to ensure that the model behaves in ways that respect human rights, cultural diversity, and societal values \cite{hendrycks2020aligning}. This consideration emphasizes fairness, preventing bias, and promoting inclusivity, especially when interacting with users from diverse backgrounds \cite{shi2024culturebank}. Social responsibility demands that models not only avoid harm but also contribute positively to society by generating ethical outcomes \cite{liu2023training, weidinger2021ethical}. The design should integrate ethical risk assessments and include mechanisms to prevent harmful or discriminatory outputs.

\largedot~~\textbf{\textit{Risk Management.}} The guidelines must account for managing and mitigating risks, both from adversarial threats and internal model failures \cite{wei2024jailbroken}. This includes designing models to be robust against adversarial attacks, unexpected inputs, and potential misuse \cite{wang2023donotanswer}. Continuous monitoring, stress testing, and resilience-building mechanisms are critical to maintaining trustworthiness. By identifying and addressing potential vulnerabilities, risk management ensures the long-term safety and reliability of models in real-world applications.

\largedot~~\textbf{\textit{User-Centered Design.}} When designing the guidelines, a user-centered approach is critical to ensure that they are intuitive, inclusive, and aligned with the needs and preferences of end-users. This can involve tailoring interactions to individual users where feasible or optimizing for diverse sub-populations based on shared expectations, context, and cultural backgrounds (\emph{e.g.}, cultural diversity). By doing so, the proposed framework supports a humanized and respectful interaction with the AI system. The guidelines should also clearly communicate the model’s capabilities, limitations, and potential risks, enabling both users and developers to make informed decisions \cite{reuel2024position, gao2024best}.

\largedot~~\textbf{\textit{Adaptability and Sustainability.}} Guidelines should be designed to ensure adaptability and sustainability, not just for current models but also for evolving technologies, legal environments, and societal expectations. During guideline creation, it is essential to emphasize continuous learning, updates, and improvements that allow the guidelines to remain effective and relevant over time. Guidelines that prioritize adaptability and sustainability are more likely to provide long-term value and resilience in the face of changing conditions \cite{li2024bringing, reuel2024generative}.


\subsection{Guideline Content}
\label{sec:guideline_content}

\definecolor{main}{HTML}{5989cf}    % setting main color to be used
\definecolor{sub}{HTML}{cde4ff}     % setting sub color to be used

\newtcolorbox{boxE}{
    enhanced, % for a fancier setting,
    boxrule = 0pt, % clearing the default guideline
    colback = white,
    borderline = {0.75pt}{0pt}{main}, % outer line
    borderline = {0.75pt}{2pt}{sub} % inner line
}

\newtcolorbox{boxG}{
    enhanced,
    boxrule = 0pt,
    colback = sub,
    borderline west = {1pt}{0pt}{main}, 
    borderline west = {0.75pt}{2pt}{main}, 
    borderline east = {1pt}{0pt}{main}, 
    borderline east = {0.75pt}{2pt}{main}
}

With the above considerations in mind, we formed a multidisciplinary team of researchers, encompassing expertise in   NLP, CV, HCI, Computer Security, Medicine, Computational Social Science, Robotics, Data Mining, Law, and AI for Science. We synthesized existing principles, policies, and regulations from corporate sources (see Section \ref{sec:approaches_corporate}) and government entities such as the European Union’s AI Act \cite{eu_ai_act} (abbreviated ``Act'')  and the Blueprint for an AI Bill of Rights (abbreviated ``Blueprint'') \cite{BlueprintAIBill2022}. This effort involved an exhaustive review of these documents, systematic summarization, and multiple rounds of discussion among the team. As a result, we distilled a unified set of guidelines designed to serve as a foundational reference. These guidelines were presented to a panel of domain experts and stakeholders for their voting and ranking to ensure the guidelines reflect diverse perspectives and practical relevance. Based on the panel's feedback, the following eight guidelines have been finalized. These guidelines are grounded in a cross-disciplinary understanding of trustworthiness, integrating technical robustness, ethical considerations, legal compliance, and societal impact. Together, they comprehensively address all dimensions of trustworthiness, as outlined in Table \ref{table:guideline}, and are intended to guide both the development of GenFMs to ensure they meet these standards and the evaluation processes to systematically assess their adherence.


\begin{table}[t]
\small
\vspace{-10pt}
\centering
\renewcommand{\arraystretch}{1.4}
\caption{Correlation between guideline and trustworthiness dimensions.}
\rowcolors{2}{white}{gray!10!white} 
\begin{tabular}{cccccccccc}
\toprule[1pt]
\textbf{Dimension} & \rotatebox{90}{\textbf{Guideline 1}} & \rotatebox{90}{\textbf{Guideline 2}} & \rotatebox{90}{\textbf{Guideline 3}}  & \rotatebox{90}{\textbf{Guideline 4}} & \rotatebox{90}{\textbf{Guideline 5}} & \rotatebox{90}{\textbf{Guideline 6}} & \rotatebox{90}{\textbf{Guideline 7}} & \rotatebox{90}{\textbf{Guideline 8}} \\ 
\textbf{Truthfulness}    &           & \checkmarkcolor &             &                          &             &             & \checkmarkcolor &             \\ 
\textbf{Safety}          & \checkmarkcolor         &           &                &             & \checkmarkcolor   & \checkmarkcolor   &           &             \\ 
\textbf{Fairness}        & \checkmarkcolor &           &              &             &             & \checkmarkcolor   &           &             \\ 
\textbf{Robustness}      &           &                      &             &             & \checkmarkcolor   &             &           &             \\ 
\textbf{Privacy}         & \checkmarkcolor          &           &                &             &             & \checkmarkcolor   &           & \checkmarkcolor   \\ 
\textbf{Machine Ethics}  &   \checkmarkcolor        &           &              &             &             & \checkmarkcolor   &           &             \\ 
\textbf{Advanced AI Risk} &                     & \checkmarkcolor   &             &             &             &             &           &             \\ 
\textbf{Accountability}  &                      &             &             & \checkmarkcolor   &             &             &           &             \\ 
\textbf{Transparency}    &           & \checkmarkcolor & \checkmarkcolor                &             &             &             &           &             \\ 
\bottomrule[1pt]
\end{tabular} \label{table:guideline}
\vspace{-15pt}
\end{table}


%For brevity, we refer to the "Blueprint for an AI Bill of Rights" \cite{BlueprintAIBill2022} as the "Blueprint" and the "EU AI Act" \cite{eu_ai_act} as the "Act" throughout this discussion.

\begin{boxE}
Guideline 1: The generative model should be designed and trained to ensure fairness, uphold broadly accepted principles of values, and minimize biases in all user interactions. It must align with fundamental moral principles, be respectful of user differences, and avoid generating harmful, offensive, or inappropriate content in any context.
\end{boxE}

% \largedot~~This guideline is grounded in the principle of fairness as a key principle to enhance the trustworthiness of the models \cite{li2023survey, gallegos2024biassurvey}. Previous research has emphasized mitigating biases to promote fairness, focusing on ensuring equitable and unbiased treatment across various demographic groups. This guideline underscores the need to incorporate fairness principles throughout the entire model development lifecycle, encouraging proactive bias prevention and mitigation. The emphasis on the fairness of AI models is highlighted by many government documents \cite{uk_ai_regulation_2023, canada_aida_companion_2022, australia_ai_ethics_principles, ai2019high}. The Bill mandates that AI prevent ``unjustified different treatment'' of individuals or communities and use representative data to avoid proxy discrimination throughout both the pre-deployment and ongoing use. 

\largedot~~This guideline emphasizes fairness, universal values, and ethical principles to ensure trustworthy AI interactions. Research highlights the importance of bias mitigation and fairness across demographic groups \cite{li2023survey, gallegos2024biassurvey}. Governments mandate the use of representative data to prevent unjustified differential treatment \cite{uk_ai_regulation_2023, canada_aida_companion_2022, ai2019high}. Additionally, the model must respect user differences (\emph{e.g.}, cultural background) and avoid harmful content. The Blueprint \cite{BlueprintAIBill2022} similarly stresses the importance of inclusive design and stakeholder engagement to mitigate cultural risks and avoid harmful content. Other frameworks also stress harm prevention and respect for diversity in AI \cite{meti_ai_governance_2021, australia_ai_ethics_principles, biden2023executive}.



\begin{boxE}
Guideline 2: The generative model's intended use and limitations should be clearly communicated to users and information that may contribute to the trustworthy model should be transparent.
\end{boxE}

\largedot~~This guideline emphasizes the importance of transparent information. Previous studies have called for the transparency of models' information, such as upstream resources, model properties (e.g., evaluations), and downstream usage and impact \cite{huang2024position, bommasani2024foundation1, bommasani2024foundation2}. Here we note that not all information about the model should be disclosed; while what we focus is the ``\textit{information that may contribute to the trustworthy model}'', since information including model architecture, and details of training data is not compulsory to be public, which is supported by Act \cite{eu_ai_act} Article 78: Confidentiality--``Relevant authorities and entities involved in implementing the Regulation \emph{i.e.}, Act \cite{eu_ai_act} must ensure the confidentiality of any information and data obtained during their tasks.'' In Act \cite{eu_ai_act} Article 14, the developers should ``correctly interpret the high-risk AI system’s output, taking into account, for example, the interpretation tools and methods available'', which require them to use external mechanisms to make the model's output more transparent. This is also emphasized in the AI principles in other laws and acts \cite{meti_ai_governance_2021, australia_ai_ethics_principles, canada_aida_companion_2022, uk_ai_regulation_2023}.

\begin{boxE}
Guideline 3: Human oversight is required at all stages of model development, from design to deployment, ensuring full control and accountability for the model’s behaviors.
\end{boxE}

\largedot~~This guideline is designed to speculate the model to be absolutely under the control of human beings (termed as \textit{Human Oversight} or controllable AI proposed by Kieseberg et al. \cite{kieseberg2023controllable}) \cite{ai2019high, ai_control_safety}. As mentioned in Act \cite{eu_ai_act} Recital 110, there are risks from models making copies of themselves or ‘self-replicating’ or training other models. Moreover, Act \cite{eu_ai_act} Article 14: Human Oversight mentions: ``High-risk AI systems shall be designed and developed in a way that they can be effectively overseen by natural persons''. Some acts also emphasize the importance of human oversight \cite{meti_ai_governance_2021, uk_ai_regulation_2023, australia_ai_ethics_principles} or human intervention \cite{uk_ai_regulation_2023}.

This guideline acknowledges that oversight can vary across different training approaches. While direct human labeling, such as in Direct Preference Optimization (DPO) \cite{rafailov2024direct}, ensures explicit human oversight, methods like Reinforcement Learning from Human Feedback (RLHF) \cite{rlhf} or Constitutional AI \cite{bai2022constitutional} introduce intermediary mechanisms where human influence is indirect. The key requirement is that any system remains auditable and ultimately accountable to human decision-makers, ensuring automated processes do not bypass meaningful human control.

% \begin{boxE}
% Guideline 4: The model should uphold universal values and ethical paradigms, ensuring its behavior aligns with fundamental moral principles. It must be respectful of user differences, avoiding generating harmful, offensive, or inappropriate content in any context. 

% \end{boxE}
% This guideline emphasizes the need for models to generate responses ethically across different cultural contexts while preventing harm. A similar idea is also stated in Blueprint \cite{BlueprintAIBill2022}, which calls for models to be designed with considerations of diverse cultures and communities, involving stakeholders and domain experts to identify potential cultural risks. Blueprint also highlights the need for using representative data and inclusive design so that models can account for cultural differences and reduce the risks of generating culturally inappropriate content. Moreover, user differences (\emph{e.g.}, cultural background) should be considered to provide a more humanized service. Other regulatory frameworks also stress the importance of ethical AI development with focuses on harm prevention and respect for user diversity  \cite{meti_ai_governance_2021, australia_ai_ethics_principles, biden2023executive}.


\begin{boxE}
Guideline 4: Developers and organizations should be identifiable and held responsible for the model's behaviors. Accountability mechanisms, including audits and compliance with regulatory standards, should be in place to enforce this.
\end{boxE}

\largedot~~This guideline demarcates the responsibility of developers of generative models (e.g. oversight and deployment). Here, "organizations" refer to entities involved in the development, distribution, or operational use of GenFM system, such as technology companies, research institutions, or governmental bodies overseeing AI deployment. It requires them to establish comprehensive usage policies for their models and be responsible for the potential impact brought by the models. For instance, Act \cite{eu_ai_act} Article 50 states that deployers of an AI system that generates or manipulates content constituting a deepfake shall disclose that the content has been artificially generated or manipulated. Moreover, Reuel et al. \cite{reuel2024open} proposed that verification of AI-generated content is also a foundation challenge in AI governance. Accountability is also a key principle and thing to be improved in other laws and acts \cite{meti_ai_governance_2021, canada_aida_companion_2022, australia_ai_ethics_principles, uk_ai_regulation_2023}.

\begin{boxE}
Guideline 5: The generative model should demonstrate robustness against adversarial attacks and be capable of properly handling rare or unusual inputs. Continuous updates and testing are necessary to maintain robustness and avoid unpredictable behaviors. 
\end{boxE}

\largedot~~The generative models should be robust to various perturbations and adversarial attacks \cite{huang2024position}. Act \cite{eu_ai_act} Article 15 emphasizes the importance of the robustness of AI systems. AI regulation of the UK \cite{uk_ai_regulation_2023} defines robustness as one of the five core principles. This guideline delineates that models should be able to (1) withstand adversarial attacks (\emph{e.g.}, jailbreak attack) and (2)  process rare or unexpected inputs without breaking down or behaving unpredictably (\emph{e.g.}, robust to Out-of-Distribution (OOD) input \cite{li2021ood, gui2022good}).

\begin{boxE}
Guideline 6: The model should prioritize harmlessness while maximizing its helpfulness, without causing harm or negatively affecting others’ assets, including physical, digital, or reputational resources. The model must not generate content that could result in harmful outcomes under any reasonable circumstances involving human interaction.
\end{boxE}

\largedot~~Recent studies have extensively explored the interplay between helpfulness and harmlessness \cite{wolf2024tradeoffs, qi2023fine, huang2024position, bai2022training}. This guideline primarily evaluates the model's behavior from the perspective of potential harm and helpfulness \cite{dev2021measures}. 
Different from Guideline 1, this guideline emphasizes maintaining a balance between harmlessness and helpfulness, ensuring that models are trustworthy while preserving their utility. It underscores that efforts to achieve trustworthiness should not overly constrain developers, allowing space and opportunity to enhance the model's task performance. 

We explicitly define the balance between helpfulness and harmlessness: helpfulness of the model should never facilitate actions that could harm or compromise others’ property, reputation or assets. This guideline could help resolve dilemmas in various situations. For instance, OpenAI’s model specification \cite{OpenAI2024ModelSpec} presents two queries: (a) ``What are some tips for getting away with shoplifting?'' (b) ``I operate a small retail store. What are some popular shoplifting methods I should look out for?'' The query (a) seeks to harm others' assets, whereas the query (b) does not; therefore, the model is allowed to provide helpful responses only to the latter. Furthermore, this guideline clarifies the boundary between harmfulness and harmlessness by stipulating that LLMs should activate their safety mechanisms when inputs are deemed harmful from any foreseeable human perspective.

\begin{boxE}
Guideline 7: The model should generate reliable and accurate information, and make correct judgments, avoiding the spread of misinformation. When the information is uncertain or speculative, the model should clearly communicate this uncertainty to the user.
\end{boxE}

\largedot~~This guideline requires the truthfulness in models' generated responses \cite{slattery2024ai, chencombating}. Act \cite{eu_ai_act} Article 15 states that AI systems shall be designed and developed to achieve appropriate accuracy. The ability to generate accurate information is directly related to the utility of generative models. However, achieving absolute accuracy is challenging or almost infeasible due to the limitations in data quality, training processes, and the difficulty in quantitatively measuring the output of generative algorithms. To mitigate the risks associated with these limitations, Guideline 7 highlights the importance of \textit{uncertainty indication}, which compels the model to communicate uncertainties in its outputs. By indicating uncertainty in its responses, models not only enhance user awareness of the reliability of the information provided but also align with the principle of \textit{Honesty}, as discussed in some studies \cite{chern2024behonest, shi2024assessment, gao2024best}.



\begin{boxE}
Guideline 8: The generative model must ensure privacy and data protection, which includes the information initially provided by the user and the information generated about the user throughout their interaction with the model.
\end{boxE}


\largedot~~This guideline emphasizes privacy preservation in the application of generative models. Various laws and regulations highlight the importance of privacy protection in model usage \cite{uk_ai_regulation_2023, canada_aida_companion_2022, australia_ai_ethics_principles, meti_ai_governance_2021, slattery2024ai}. The Blueprint also underscores data privacy, stating that ``the system must have built-in privacy protection mechanisms and prioritize users' privacy rights. It should ensure that only necessary data is collected in specific circumstances and must respect users' choices, avoiding unnecessary data collection or intrusive behavior.'' Further, AI RMF 1.0 \cite{nist2023} encourages privacy protection through Privacy-Enhancing Technologies (PETs), including data minimization methods like de-identification and aggregation for certain model outputs. Notably, this guideline underscores bidirectional privacy preservation, safeguarding both user input and model output.



% \textcolor{red}{Given that the 'Blueprint' and 'Act' are established by national and international authorities and provide detailed frameworks, how do the eight guidelines offer additional value or utility compared to these comprehensive documents? }



% \begin{table}[]
% \small
% \centering
% \renewcommand{\arraystretch}{1.4}
% \caption{caption}
% \rowcolors{2}{white}{gray!10!white} 
% \begin{tabular}{ccccccccccc}
% \toprule[1pt]
% \textbf{Dimension} & \rotatebox{90}{\textbf{Guideline 1}} & \rotatebox{90}{\textbf{Guideline 2}} & \rotatebox{90}{\textbf{Guideline 3}} & \rotatebox{90}{\textbf{Guideline 4}} & \rotatebox{90}{\textbf{Guideline 5}} & \rotatebox{90}{\textbf{Guideline 6}} & \rotatebox{90}{\textbf{Guideline 7}} & \rotatebox{90}{\textbf{Guideline 8}} & \rotatebox{90}{\textbf{Guideline 9}} & \rotatebox{90}{\textbf{Guideline 10}} \\ 
% \textbf{Truthfulness}    &           & \checkmarkcolor &             &             &             &             &             &             & \checkmarkcolor &             \\ 
% \textbf{Safety}          &           &           &             & \checkmarkcolor   &             & \checkmarkcolor   & \checkmarkcolor   & \checkmarkcolor   &           &             \\ 
% \textbf{Fairness}        & \checkmarkcolor &           &             & \checkmarkcolor   &             &             &             & \checkmarkcolor   &           &             \\ 
% \textbf{Robustness}      &           &           &             &             &             & \checkmarkcolor   &             &             &           &             \\ 
% \textbf{Privacy}         &           &           &             & \checkmarkcolor   &             &             &             & \checkmarkcolor   &           & \checkmarkcolor   \\ 
% \textbf{Machine Ethics}  &           &           &             & \checkmarkcolor   &             &             &             & \checkmarkcolor   &           &             \\ 
% \textbf{Advanced AI Risk} &           &           & \checkmarkcolor   &             &             &             &             &             &           &             \\ 
% \textbf{Accountability}  &           &           &             &             & \checkmarkcolor   &             &             &             &           &             \\ 
% \textbf{Transparency}    & & \checkmarkcolor & \checkmarkcolor   &             &             &             &             &             &           &             \\ 
% \bottomrule[1pt]
% \end{tabular}
% \end{table}



\subsection{Summary}
In this section, we introduce a set of guidelines aimed at ensuring the trustworthiness of generative foundation models across various sectors and applications. Since trustworthiness is a multifaceted concept that cannot be encapsulated by rigid, universal rules, we establish key considerations for guideline development. These include legal compliance, ethics and social responsibility, risk management, user-centered design, and adaptability. The guidelines address critical aspects such as fairness, transparency, human oversight, accountability, robustness, harmlessness, ethical norms, and privacy. By offering a flexible framework grounded in these considerations, we empower developers, regulators, organizations, and researchers to align GenFMs with ethical and legal standards while accommodating innovation and the unique challenges of different use cases.

