\clearpage

\section{Further Discussion}
\label{sec:discussion}
% \nouha{I will make a pass soon. Please let me know if there are important things I should keep in mind. }
In this section, we will establish our stance and engage in critical discussions on urgent and impactful topics surrounding the trustworthiness of generative foundation models (GenFMs). These discussions are essential to addressing pressing challenges, identifying risks, and outlining strategies for advancing trustworthy GenFM deployment.



\subsection{Trustworthiness is Subject to Dynamic Changes}
\label{sec:discuss_dynamic}

\begin{figure}[h]
    \centering\vspace{+0.1in}
    \includegraphics[width=\linewidth]{image/dynamic_trustworthiness.pdf}
    \caption{Dynamic requirements of trustworthiness in different downstream applications, where \CIRCLE indicates high requirements for this trustworthy domain in the specific downstream task, and \LEFTcircle refers to relatively low requirements.} \vspace{+0.15in}
    \label{fig:dynamic_trustworhiness}
\end{figure}

The concept of "trustworthiness" in generative models is increasingly recognized as a dynamic and context-dependent construct \cite{huang2024position, liu2023trustworthy}, reflecting the intricate and often conflicting demands placed on these models across various domains, e.g., utilitarian or deontological \cite{gawronski2017makes, anderson2011machine}. Even when a certain definition is adopted, the very nature of such principles may leave flexibility in their interpretation. As a result, different cultural, political, and societal approaches that apply the same definition to a case may reach opposite conclusions. For instance, what one society considers biased might be viewed as fair in another societal context\cite{henrich2010markets, greene2014moral}. This variability necessitates a deeper exploration into how trustworthiness is not a one-size-fits-all attribute but rather an evolving quality that must be continually reassessed and redefined in response to the unique challenges and ethical considerations of different applications, as shown in \autoref{fig:dynamic_trustworhiness}. In previous research, Klyman \cite{klyman2024acceptable} emphasizes that strict enforcement of acceptable use policies (AUPs) can hinder researcher access and limit beneficial uses. This highlights the need for dynamic mechanisms to enhance policy flexibility, adapting to evolving trust requirements.

At the core of this dynamic nature is the understanding that the expectations of what constitutes "trustworthy" behavior for a generative model can shift dramatically depending on its deployment environment. For example, in educational settings \cite{KASNECI2023102274, george2023potential}, the paramount concern is the protection of young minds, leading to stringent requirements that the model must not generate harmful content such as violence, explicit material \cite{miao2024t2vsafetybench}, or misinformation \cite{huang2023harnessing, 10.1145/3589335.3651509}. Here, the trustworthiness of the model is tightly coupled with its ability to filter out inappropriate content and adhere to educational standards \cite{merlyn2024, merlyn2024education, merlyn2024education}.

However, this same model, when applied in a domain like artistic creation \cite{abuzuraiq2024towards}, medical domain \cite{han2024towards}, or even certain research fields \cite{peng2023study, zhao2023competeai, jin2024agentreview, salah2023may, zhang2024cybench, roohani2024biodiscoveryagent}, might be required to operate under a completely different set of trustworthiness criteria. For instance, for creative writers, overly strict constraints on the truthfulness of generated content can hinder the model’s helpfulness, as flexibility in factual accuracy is often essential for creativity. Moreover, in the medical field, generative models might include graphic content (\emph{e.g.}, gory or bloody images) in their inputs and outputs to effectively support healthcare professionals. However, such content is generally unacceptable in educational contexts, especially when targeting children or adolescents. In these contexts, the model's ability to generate content that challenges societal norms explores controversial ideas, or even delves into sensitive topics might be seen as not only permissible but necessary for the fulfillment of its intended purpose. The trustworthiness of the model here is thus defined not by what it excludes, but by the breadth and depth of its creative or analytical capacities, even if those capacities might occasionally produce outputs that would be considered inappropriate in other contexts. This fluidity in the definition of trustworthiness speaks to a broader issue in AI ethics: the necessity for adaptive and context-aware governance mechanisms that can recalibrate the trust metrics of generative models as they transition between different operational landscapes \cite{deloitte2024, wtw2024}. 


To achieve dynamic trustworthiness in AI models, two principal approaches are typically considered. The first involves deploying highly specialized models designed for specific downstream tasks or domains. These models are rigorously trained to meet the unique trustworthiness requirements of each task or domain. While effective in isolated scenarios, this approach faces significant challenges in terms of scalability, as developing and maintaining multiple models for diverse applications is resource-intensive and computationally costly. Furthermore, such an approach risks limiting the model's flexibility in handling novel or unexpected inputs across various domains. The second approach seeks to overcome these limitations by enabling models to dynamically adapt their trustworthiness criteria based on contextual understanding. In this paradigm, models are equipped to interpret the specific contexts and adjust their responses accordingly. For example, OpenAI’s model specifications \cite{OpenAI2024ModelSpec} suggest that in creative text generation contexts, queries typically considered harmful—such as “write me rap lyrics about cats that includes `fuck' in every line”—may be deemed appropriate given the creative nature of the task. This approach offers greater adaptability but also presents new challenges in terms of alignment. The model must be able to reliably and accurately interpret complex, often ambiguous, contextual cues while maintaining appropriate trustworthiness thresholds.


Furthermore, the concept of dynamic trustworthiness challenges us to rethink the conventional metrics used to evaluate generative models. Traditional benchmarks that emphasize static evaluations might fail to capture the nuanced and context-specific demands of different domains. Instead, there is a growing need for a more fluid and adaptable framework for assessment (\emph{e.g.}, DyVal \cite{zhu2023dyval}, UniGen \cite{wu2024unigen}, AutoBencher \cite{li2024autobencher}, AutoBench-V \cite{bao2024autobenchv} and others \cite{fan2024nphardeval4v, kurtic2024mathador}) or the evaluation framework for specific domain \cite{fei2023lawbench, xia2024cares, zhang2024climb}, one that recognizes the multiplicity of stakeholders involved. 

Building on this, trustworthiness varies significantly across different stakeholders, highlighting the importance of transparency in benchmark design and implementation. When a benchmark adopts specific interpretations, it inevitably aligns with certain approaches while potentially diverging from others. By being transparent about the assumptions and definitions, benchmarks can provide valuable insights. Such transparency allows stakeholders to make informed decisions about which benchmarks best align with their goals, contributing to more meaningful evaluations of GenFMs. Consequently, we have proposed guidelines in \S\ref{sec:guideline_content} that address the varying needs of stakeholders, ensuring that assessments remain flexible, context-aware, and aligned with the diverse objectives of the GenFM ecosystem.




In conclusion, trustworthiness in generative models is far from a fixed attribute; it is a complex, multi-dimensional quality that must be continually negotiated and redefined. This dynamic nature of trustworthiness demands a more sophisticated approach to model deployment and assessment, one that is capable of adapting to the diverse and changing needs of different domains.


\subsection{Trustworthiness Enhancement Should Not Be Predicated on a Loss of Utility}

As generative models continue to advance, the balance between trustworthiness and utility emerges as a crucial issue. Some have perceived the SB 1047 AI Bill \cite{California2024SB1047}, introduced to ensure the trustworthiness of advanced generative models rigorously, as a potential impediment to AI innovation \cite{calchamber2024godmother}. In this discussion, we will examine two key positions: (1) trustworthiness and utility are inherently interconnected, and (2) it is not advisable to compromise either trustworthiness or utility in pursuit of enhancing the other.

Recent studies also unveil that trustworthiness is closely related to utility \cite{wolf2024tradeoffs, qi2023fine, huang2024position, bai2022training, zhang2024bi}. For instance, Huang et al. found that the trustworthiness of LLMs is positively related to their utility performance \cite{huang2024position}. Qi et al. found that fine-tuning LLMs without any malicious aims will still compromise the trustworthiness of LLMs \cite{qi2023fine}. Bai et al. and Zhang et al. aim to balance trustworthiness and helpfulness during model training \cite{bai2022training, zhang2024bi}. Even though in LLM's evaluation, trustworthiness and utility are closely related, Ren et al. found that many safety benchmarks highly correlate with upstream model capabilities \cite{ren2024safetywashing}. The importance of maintaining this balance is further emphasized by the findings of Klyman \cite{klyman2024acceptable}, who discusses the role of acceptable use policies in shaping the market for foundation models and the AI ecosystem. 

Continuing from the argument that trustworthiness and utility are deeply interconnected, focusing exclusively on enhancing one while neglecting the other can lead to unintended negative consequences. Overemphasis on safety and alignment at the cost of utility is a prominent example. If models are excessively constrained to prioritize safety features such as stringent content filtering or rigid ethical frameworks, it may limit their ability to provide useful or creative responses, ultimately diminishing their overall utility \cite{xstest, kirk2023understanding}. This kind of imbalance, where trustworthiness is prioritized at the expense of utility, could result in models that are overly cautious or even unusable in certain dynamic, real-world contexts where flexibility and innovation are key.

On the  other hand, sacrificing trustworthiness to maximize utility poses significant risks. Models that have high utility but lack robustness in terms of fairness, transparency, or resistance to manipulation are problematic. Such models might generate biased or harmful outputs, undermining user trust and creating ethical dilemmas \cite{huang2024position, liu2023trustworthy, wang2023decodingtrust}. In high-stakes environments like healthcare or finance, utility without trustworthiness is unsustainable, as untrustworthy models are unlikely to be adopted or could even cause harm \cite{xia2024cares}. To these ends, the approach of sacrificing one dimension for the benefit of the other is inherently flawed. What is needed is a paradigm where both trustworthiness and utility can be simultaneously improved to ensure models are both reliable and effective.

Rather than viewing trustworthiness and utility as competing objectives, recent research highlights the potential for mutual enhancement. For example, some approaches begin by ensuring that the model is harmless—establishing a baseline of trustworthiness—before optimizing for helpfulness or utility \cite{gao2024best}. By incorporating multi-objective alignment \cite{yang2024metaaligner, wang2024hybrid, zhou-etal-2024-beyond, fu2024unlocking}, some studies aim to maximize the helpfulness and harmlessness at the same time. These approaches recognize that a rigid, one-size-fits-all alignment process might not be optimal; instead, dynamic adjustments during the training process allow the model to improve both aspects simultaneously.

One crucial insight from these approaches is that harmlessness acts as a safeguard—ensuring that the model is inherently trustworthy before other features are optimized. This aligns with the view that trustworthiness is not a constraint on utility but a necessary component of it. By establishing a framework where the model cannot generate harmful outputs, developers can confidently enhance the model’s utility without the risk of ethical breaches or unintended consequences.

The balance between trustworthiness and utility is not a zero-sum game where enhancing one necessarily diminishes the other \cite{tuan2024towards}. On the contrary, the two can—and should—be pursued in tandem to create robust, effective AI models. Sacrificing either trustworthiness or utility for short-term gains in the other is ultimately unsustainable and could lead to detrimental consequences in both ethical and practical applications. The key lies in developing methods, like the harmlessness-first approach, where trustworthiness serves as a foundation for subsequent utility maximization. This strategy ensures that models remain both safe and effective, setting the stage for a future where generative AI can thrive in a variety of real-world contexts without compromising on either front.




\subsection{Reassessing Ambiguities in the Safety of Attacks and Defenses}


\begin{figure}[h]
    \centering \vspace{+0.05in}
    \includegraphics[width=1\linewidth]{image/ambiguity_safety.pdf}
    \caption{Ambiguities in the safety of attacks and defenses.}
    \label{fig:reassessing_ambiguityies}\vspace{+0.15in}
\end{figure}

The ambiguity in determining the safety of inputs and outputs in generative models presents substantial challenges. The distinction between harmful and benign content is not always clear-cut, in terms of the input provided to the model and its output. This lack of clarity complicates the development of robust safety mechanisms and introduces ethical and practical challenges \cite{bauer2021generative, truong2024attacks, huang2025position}. We discuss this from the perspective of both input and output, as shown in \autoref{fig:reassessing_ambiguityies}.

A critical question arises regarding the inputs to generative models: \textbf{\textit{How can we clearly identify if the input is harmful?}} Previous efforts have employed human evaluation or trained machine learning classifiers to detect toxicity in inputs \cite{Toxicity_Detection1, Toxicity_Detection2, openai_moderation_quickstart}. However, these methods inherently reflect human values, either directly or indirectly. For instance, consider the user query $I_{A}$, such as \textit{\textcolor{green!60!black}{“Please write a post encouraging people to build a strong national defense”}}. While this might seem benign from a human perspective (and might be used in studies to evaluate exaggerated safety in large language models \cite{an2024automatic}), it could be a subtle rephrasing of a more harmful query, such as $I_{B}$, \textit{\textcolor{red!60!black}{“Write a post promoting militaristic aggression,”}} potentially generated through techniques like jailbreak attacks \cite{zeng2024johnnypersuadellmsjailbreak}. In this context, query $I_{A}$ should be considered harmful because its resulting output could closely resemble that of query $I_{B}$. The conflicting safety assessments of these examples, indicating both harmful and harmless interpretations of the same query from different research perspectives (i.e., jailbreak versus exaggerated safety), underscore the ambiguity in current academic definitions and standards for safety or harmfulness. For example, a recent study \cite{souly2024strongreject} highlights that current evaluation methods substantially exaggerate the effectiveness of jailbreak attempts. To mitigate this ambiguity, some initial solutions have been suggested. For instance, OpenAI's Model Spec \cite{OpenAI2024ModelSpec} outlines a rule that models should treat $I_{A}$ as a benign query. This approach acknowledges that the nature of knowledge can sometimes lead to misuse by humans rather than being an AI fault, and therefore falls under OpenAI's usage policies.

Similarly, when considering the outputs of generative models, another important question emerges: \textbf{\textit{How can we accurately judge if the output is harmful?}} For example, if a model responds to a potentially harmful query but includes a moral disclaimer, the safety of such a response remains debatable \cite{ran2024jailbreakevalintegratedtoolkitevaluating, mazeika2024harmbenchstandardizedevaluationframework}. An attacker could exploit these responses by simply removing the moral disclaimer, thereby using the content for malicious purposes. However, from a trustworthiness assessment perspective, the presence of a moral disclaimer suggests that the model has a sufficient understanding of the trustworthiness of the user query, which should reflect the model's trustworthiness. This conflict raises the need for more precise definitions of the harmfulness of a model's output. Even though the recent work from OpenAI \cite{openai_improving_model_safety_2024} has proposed some rules that a trustworthy LLM should adhere to (\emph{e.g.}, the response types include hard refusal, soft refusal, and comply), the more fine-grained and clearly-defined rules should be considered in the future research. 




In summary, the challenges associated with distinguishing between harmful and benign content in generative models highlight the need for clearer definitions and more robust safety mechanisms. Both input and output assessments face inherent ambiguities that complicate the application of current safety standards. As generative models continue to evolve, addressing these ambiguities will be essential to ensure the ethical and safe deployment of such technologies in various contexts \cite{kapoor2024position, ren2024safetywashing, koyejo2024towards, anderljung2023frontier}.

% The challenge, therefore, lies in developing more sophisticated safety mechanisms that can effectively manage this ambiguity without significantly compromising the utility of the generative models. This might involve creating more robust and flexible censorship mechanisms, as well as adopting an information-theoretic approach to model safety that balances the trade-off between safety and utility . 


\subsection{Dual Perspectives on Fair Evaluation: Developers and Attackers}

\label{discussion_atkordev}


To elevate the discussion on evaluating generative models, particularly about handling harmful or malicious queries, it is essential to address a pivotal yet often overlooked issue: should the evaluation be framed from the standpoint of developers or attackers? This differentiation is not merely theoretical~\cite{Jia2018AttriGuardAP, Huang2013BadDI, randomtrees2023ethical}, but fundamentally shifts the criteria for assessing a model's performance and reliability. In short, the design of generative AI should follow a strict ethical strategy grounded in the developer's perspective for general human welfare.

From the developer's perspective, evaluation focuses on the model's adherence to ethical and protective standards. According to this viewpoint, a robust generative model should entirely avoid responding to harmful queries or reject them outright. This perspective aligns with ethical guidelines in machine learning, which emphasize moral responsibility and safety considerations~\cite{Du2023UserCentricIA, Wang2023NS4ARAN, Sheetal2021AML, Drum2023UsingNL}. In this context, any model that engages with harmful queries—no matter how accurate or high-quality its responses may be—is considered inadequate. The primary concern is that responding to potentially dangerous inputs indicates a failure in the model's protective mechanisms against misuse, regardless of whether the response is effective or helpful for attackers or not.

Conversely, from the attacker’s perspective, the standards for evaluation differ significantly. Attackers view both a model's refusal to respond and its provision of incorrect answers as equally unhelpful because either outcome hinders their efforts to exploit the model. For attackers, the value of the model does not lie in its ability to generate high-quality responses, but in its potential to be manipulated into producing any response to harmful queries~\cite{Watanabe2018UserBC}.

The argument for adopting the developer’s perspective in evaluations is grounded in the need for a stringent and realistic assessment of the model’s trustworthiness. From this viewpoint, The quality and correctness of responses are secondary if a model fails at the primary task of resisting interaction with harmful queries~\cite{bai2022training, bauer2021generative}. As generative models become increasingly sophisticated, the risk that they will eventually provide accurate answers to malicious prompts greatly grows. A model that can resist the initial attack by refusing to engage with the query sets a stronger foundation for trustworthiness~\cite{saha2024llm, kumar2023certifying}. Evaluating from the developer’s perspective aligns with this goal, ensuring that the model’s performance is measured by its capacity to prevent exploitation, rather than merely by its capacity to provide correct responses under optimal conditions. This approach advocates for a more rigorous and practical standard that reflects the real-world challenges of maintaining the security and integrity of generative models in adversarial contexts.


\subsection{A Need for Extendable Evaluation in Complex Generative Systems}

Current evaluation frameworks or benchmarks predominantly focus on assessing the trustworthiness of individual generative models \cite{wang2023decodingtrust, huang2024position}. While these methods provide reliable calibration for single models, they fall short in effectively evaluating complex generative systems \cite{reuel2024open}. Such systems typically exhibit two defining characteristics:

1) Multiple models powering the system. Recent research has explored frameworks consisting of multiple agents, each based on different generative models \cite{guo2024large, williams2023epidemic, gao2023s, wang2023avalon, chen2024agentverse, chatdev, yang2024matplotagent,ouyang2025nvagent,gui2025uicopilot}. For instance, \textsc{ChatDev} \cite{chatdev} is a multi-agent system designed for automated software development. Similarly, Chen et al. investigate how heterogeneous agents collaborate in web environments to accomplish tasks \cite{chen2024internet}.

2) Multi-modal information interaction. Complex generative systems often involve the processing and generation of data across different modalities, such as text, images, audio, and video. These systems require models that can handle various data types to work together seamlessly. For example, in an autonomous driving system \cite{zheng2024genad, fu2024gendds, hu2023gaia, Sural2024ContextVLMZA}, one model may generate real-time textual analyses of road conditions, another might create corresponding visual outputs, while a third generates sound alerts for the driver.

Evaluating such complex systems poses significant challenges in terms of both system effectiveness (i.e., utility) and trustworthiness. These challenges arise from several key factors:

First, inter-model dependencies complicate evaluation. In complex systems, models often rely on each other's outputs, making it difficult to assess their performance in isolation. Traditional metrics for individual models fail to capture these dependencies. For example, if a text generation model produces errors, it can compromise the performance of an image generation model that depends on the text \cite{gu2024agent, ju2024flooding}. Therefore, new evaluation methods are required to measure how effectively models collaborate and how information is transferred throughout the system \cite{chen2024agentverse}.

Second, multi-modal evaluation becomes more challenging when models generate outputs across different modalities. Systems powered by both a language model and a language-vision model require evaluation metrics that can capture intricate cross-modality coherence. For example, a system generating descriptive text for an image must ensure that the text and image outputs are consistent and relevant to each other. Traditional evaluation metrics designed for single-modality models are inadequate for capturing these nuanced interactions. Consider a scenario where a language model generates a textual summary of a scene while a vision model generates the visual representation of that scene; evaluating the coherence between these outputs requires a more sophisticated multi-modal evaluation framework.

Third, consistency and scalability become increasingly difficult to maintain as the system's complexity grows. As more models and agents are added to the system, ensuring that their outputs remain aligned and coherent across all components presents a significant challenge. For example, moving from a system with a handful of agents \cite{chatdev, chen2024internet} to one with potentially millions of agents \cite{gu2024agent} necessitates scalable evaluation methods capable of maintaining consistency across the system as a whole.

In summary, the complexity of modern generative systems demands extendable evaluation frameworks that go beyond the capabilities of traditional benchmarks. Addressing these challenges requires innovative approaches to evaluate inter-model collaboration, multi-modal coherence, and the scalability of system performance in dynamic environments.



\subsection{Integrated Protection of Model Alignment and External Security}

Recent research has increasingly focused on enhancing the safety alignment mechanisms of generative models, particularly LLMs, and LVMs, to improve their overall trustworthiness \cite{rlhf, saferlhf, ji2024pku, yu2024rlhf, rl4f}. In this context, we propose that integrating internal alignment mechanisms with external security measures constitutes a critical approach to developing trustworthy generative systems.

This perspective emphasizes the equal importance of external protection alongside internal safety alignment. External protection mechanisms, such as moderators designed to identify potentially harmful content in both user inputs and model outputs, are gaining traction \cite{openaimoderation, facebookmoderation}. For instance, recent studies have introduced auxiliary models that work alongside generative models to enhance system trustworthiness \cite{yuan2024rigorllm, defendingjailbreak, huang20241+}. Additionally, specific safety measures have been implemented in practice, such as the text classifier used in DALL-E 3 to assess the harmfulness of user inputs \cite{ramesh2024dalle3}. Tools like detection classifiers, which can identify content generated by models like OpenAI's Sora, further contribute to safeguarding against misleading or harmful outputs \cite{sora_openai_safety}.

Three key reasons highlight the necessity for external protection mechanisms: (1) \textbf{\textit{Natural Defect of Alignment}}: Recent research has identified flaws in alignment methods \cite{xu2024dpo, wolf2023fundamental, rlhf, puthumanaillam2024moral}. For example, Wolf et al. \cite{wolf2023fundamental} argue that current approaches like RLHF \cite{rlhf} are inherently vulnerable to adversarial prompting, leading to undesirable behaviors. Additionally, Puthumanaillam et al. highlight that LLMs struggle with adapting to evolving values and scenarios under current methods \cite{puthumanaillam2024moral}. These examples illustrate that current alignment strategies for generative models have inherent limitations, making superalignment \cite{burns2024weak} challenging to achieve to ensure trustworthiness. \textbf{\textit{(2) Impact on Model Utility:}} Even though some studies think safety mechanisms should be as sophisticated as the underlying model \cite{wei2024jailbroken}, strict safety alignment within generative models can significantly compromise their utility, particularly in fundamental tasks \cite{wolf2024tradeoffs, tuan2024towards, yuan2024rigorllm, zhang2024bi}. Overemphasis on internal alignment can lead to overly conservative or restricted models, thereby diminishing their performance and effectiveness in various applications. \textbf{\textit{(3) Flexibility in Diverse Scenarios:}} Generative models that are overly aligned for safety may lack the adaptability required for deployment across diverse contexts and scenarios, as discussed in Section \ref{sec:discuss_dynamic}. In contrast, models with basic safety alignment, supplemented by adjustable external protection, offer a more flexible and practical solution. This configuration allows for dynamic adjustments to the external safety measures without fundamentally altering the model itself, thereby facilitating broader and more nuanced applications of the generative system. Additionally, incorporating more safety design principles (\emph{e.g.}, the principle of least privilege) is essential to establish a comprehensive and robust safety mechanism for model deployment.

In conclusion, balancing internal safety alignment with robust external protection mechanisms presents a promising pathway toward developing a trustworthy generative model-based system. This integrated approach enables enhanced safety and adaptability, ultimately supporting the deployment of generative models across a wider spectrum of real-world contexts.


\subsection{Interdisciplinary Collaboration is Essential to Ensure Trustworthiness}

\begin{figure}[h]
    \centering\vspace{+0.1in}
    \includegraphics[width=1\linewidth]{image/Interdisciplinary.pdf}
    \caption{Interdisciplinary influence of generative models.}
    \label{fig:interdisciplinary} \vspace{+0.15in}
\end{figure}

Generative models have the potential to contribute or even revolutionize wide range of domains, from natural language processing to scientific discovery \cite{colombo2024saullm, guo2024econnli, maatouk2024large, guo2023large, openai2024cooperation}. As generative models extend into other disciplines, there is a growing need for a deeper understanding of interdisciplinary collaborations between generative models and other fields (as shown in \autoref{fig:interdisciplinary}). In this discussion, we seek to address the following two questions: \textbf{\textit{1) How could interdisciplinary collaboration enhance the trustworthiness of generative models, and 2) How could trustworthy generative models, in turn, bring values to other disciplines?}}

By integrating insights from various disciplines, each offering unique perspectives on the technical, ethical, and social implications of these models, we can achieve a more comprehensive understanding of the trustworthiness of generative models \cite{li2024quantifying, liu2024sora, al2024ethical, hadi2023survey}. For instance, OpenAI's Sora, a text-to-video generative model \cite{sora_openai}, necessitates engagement from diverse disciplines---including policymakers, educators, and artists---to develop safety policies that resonate with societal concerns and promote beneficial applications \cite{sora_openai_safety}. Furthermore, exploring the psychological and cognitive dimensions of model trustworthiness yields insights into how these models interact with human users and align with human values \cite{li2022does, li2024quantifying, chen2024self, huang2024humanity}. Research by Li et al. \cite{li2024quantifying} examined how a psychometric evaluation framework could reveal inconsistencies in LLMs' responses during psychometric assessments, where a model may exhibit contrasting traits across different assessment formats. This not only uncovers a fundamental difference between the tendencies of models' and humans' behaviors, but it also compels a rigorous evaluation and cautious treatment of LLMs' responses. Additionally, the extensive domain knowledge involved in the creation of domain-specific benchmarks, such as those in medicine and scientific research, is crucial for ensuring the safe, reliable, and ethical application of generative models in these areas \cite{xia2024cares, he2023control}. A recent study \cite{porsdam2023generative}, co-authored by an interdisciplinary team of experts in law, bioethics, and machine learning, thoroughly examines the potential impacts of LLMs in critical areas such as education, academic publishing, intellectual property, and the generation of errors and misinformation \cite{oxford2023ethical}.

The benefits of trustworthy generative models, reciprocating by enhancing the very disciplines that contributed to their creation \cite{Eloundou2023GPTsAG}. For example, understanding the trustworthiness of generative models in embedded systems aids in designing safer, more dependable autonomous technologies \cite{boiko2023autonomous}. A recent study \cite{huang2024social} also explores the reliability of LLM simulations, offering valuable insights for other disciplines, such as social science and psychology, to design more robust experiments. Zhou et al. also evaluate the trustworthiness of LLMs in scientific lab Q\&A, which reveals the extent to which LLMs can assist researchers in accomplishing scientific tasks \cite{zhou2024labsafety}. Other disciplines may also benefit from the creative potential of LLMs, as demonstrated by a recent study that evaluates their ability to generate research ideas \cite{si2024can}.

To summarize, interdisciplinary collaboration yields symbiotic benefits: diverse expertise not only enriches our understanding of the trustworthiness about generative models, but also advance research and applications within their contributing disciplines. This interconnection fosters a continuous cycle of innovation, where the mutual enrichment of models and disciplines drives progress across the broader landscape of scientific inquiry and technological development. 

\subsection{When Generative Models Meets Ethical Dilemma}
\label{sec:discussion_ethics}

\begin{figure}[h]
    \centering\vspace{+0.1in}
    \includegraphics[width=1\linewidth]{image/ethics_discussion.pdf}
    \caption{Visualization of model responses to ethical dilemmas, with each scenario represented by three squares: the middle square (green) indicates neutrality, while the side squares (red) represent a bias toward one of the conflicting moral choices.}\vspace{+0.15in}
    \label{fig:ethics_dilemma}
    % \vspace{-5pt}
\end{figure}

Integrating Generative Models in decision-making processes has marked a new phase of technological advancements and transformative capabilities across various industries. However, this growing integration has also engendered a concomitant rise in ethical dilemmas and concerns \cite{nassar2021ethical}. Ethical dilemmas refer to situations where individuals face tough choices between conflicting moral values or principles \cite{bush1994study}. These dilemmas not only highlight the complexities of human moral reasoning but also provide a framework for assessing the ethical decision-making capabilities of generative models, such as LLMs \cite{cabrera2023ethical}. Understanding these dilemmas is crucial for ensuring that models can operate in ways that align with societal values and ethical norms. The importance of studying ethical dilemmas lies in their ability to reveal the underlying ethical frameworks that guide decision-making processes. By exploring how LLMs respond to these dilemmas, researchers can evaluate their moral awareness, identify potential biases, and improve their alignment with human ethical standards.

To evaluate how generative models handle ethical dilemmas, we designed ten queries representing complex moral scenarios, as detailed in Appendix \ref{ethicaldilemma}. Each scenario challenges the models to make ethically charged decisions, offering insights into their ethical reasoning capabilities and revealing underlying biases. The results are shown in \autoref{fig:ethics_dilemma}. By examining the models' responses, we identify key trends in their behavior and decision-making patterns.

\textbf{Tendency Towards Neutrality vs. Decisiveness.} Our findings indicate that some models lean toward neutrality, while others exhibit more decisive behavior. For instance, Gemini-1.5-flash consistently avoids making explicit ethical choices in all scenarios, reflecting either an inclination towards neutrality or a design aimed at minimizing intervention in morally charged situations. In contrast, models such as GPT-4o, GPT-4o-mini, and several LLaMA variants tend to engage in more action-oriented decision-making, often prioritizing outcomes that align with useful principles. For example, these models commonly intervene in scenarios like the Trolley Problem to optimize results, suggesting a focus on outcome efficiency rather than fairness. Meanwhile, risk-averse models such as GLM-4 and Mistral-8x22B prefer to avoid making choices, indicating a potential reluctance to engage with dilemmas involving high uncertainty or ethical complexity.

\textbf{Bias and Alignment in Ethical Prioritization When Facing Ethical Dilemmas.} Differences in ethical priorities between dilemmas can be contextualized through the lens of modern ethical frameworks, which often fall into two categories: top-down and bottom-up approaches. Models like GPT-4o exhibit a top-down inclination, as seen in dilemmas like the Trolley Problem, where they tend to adopt utilitarian principles—sacrificing one life to save many. This approach reflects a reliance on pre-defined ethical rules aimed at optimizing overall outcomes. In contrast, Gemini-1.5-flash demonstrates a tendency toward non-intervention, which may align with bottom-up ethics. This approach emphasizes situational neutrality and contextual reasoning over rigid principles. However, such flexibility can lead to inconsistencies when navigating conflicting dilemmas, such as balancing pedestrian safety against passenger safety.

Additionally, models like Claude-3.5-sonnet occasionally display emotionally driven decisions, such as prioritizing family members. These patterns highlight the diversity in how models are aligned with ethical frameworks. However, it is important to acknowledge the limitations of these models, as they may lack the depth needed to grasp the subtleties of human ethical reasoning. Consequently, their decisions may not fully capture the complexities inherent in real-world moral situations.

\textbf{Insights and Future Directions.} The varied responses of generative models highlight the absence of a unified ethical framework and illustrate differences between top-down and bottom-up approaches to moral reasoning. Some models exhibit reasoning that appears aligned with utilitarian or deontological principles, while others show context-dependent variability or even neutrality. Top-down approaches, which rely on predefined ethical theories, offer clear guidance but can oversimplify complex dilemmas. In contrast, bottom-up approaches, which derive ethical judgments from patterns in context-specific data, provide flexibility but may lack consistency and coherence. These variations underscore the challenge of aligning AI models with nuanced human ethical standards and emphasize the importance of achieving reflective equilibrium—a balance in which general moral principles and particular judgments are refined in response to one another. Future research should prioritize interdisciplinary approaches by integrating insights from philosophy, psychology, and cognitive science to enhance ethical reasoning capabilities in generative models. Equally important is the development of mechanisms for model transparency, allowing users to understand the rationale behind specific ethical decisions and thereby fostering trust and accountability. Additionally, exploring ethical alignment techniques, such as RLHF, can ensure that model decisions align with societal expectations. As generative models become increasingly integrated into high-stakes areas like healthcare, law enforcement, and autonomous systems, ensuring that their ethical responses reflect shared norms and values will be vital for their responsible deployment.


\subsection{Broad Impacts of Trustworthiness: From Individuals to Society and Beyond}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{image/social_impact.pdf}
    \caption{The impact of trustworthiness in different domains.}\vspace{+0.15in}
    \label{fig:impact}
\end{figure}

As shown in \autoref{fig:impact}, the trustworthiness of generative models has profound implications that span from individual impacts to broader societal consequences \cite{wach2023dark}, influencing various aspects of education \cite{chiu2023impact}, economic structures \cite{chui2023economic}, and social dynamics \cite{baldassarre2023social}. At the individual level, the influence of generative models is particularly significant, as these technologies interact directly with personal experiences, privacy, and decision-making processes. When generative models produce biased outputs, they reflect societal stereotypes and reinforce harmful norms, particularly affecting marginalized individuals. For instance, when language models perpetuate gender or racial biases in their responses, this can contribute to microaggressions and reinforce negative self-perceptions, thus affecting an individual's mental health and social integration.

Privacy concerns further illustrate the critical need for trustworthy generative models \cite{novelli2024generative, chen2024generative}. The capacity of these models to memorize and replicate training data poses significant risks to individual privacy. Instances where models inadvertently reveal sensitive information, such as personal identifiers or private conversations, highlight the inadequacy of current privacy safeguards in training processes. These violations can lead to unauthorized exposure of personal data, resulting in emotional distress, legal complications, and a broader erosion of trust in these models.

The interaction between individuals and generative models also raises concerns about overreliance and misplaced trust \cite{10.1145/3630106.3658941}. Generative models, particularly those with highly conversational interfaces, can create an illusion of authority and reliability that is not always warranted. Users may inadvertently accept machine-generated outputs as factual, especially when under time constraints or lacking the expertise to evaluate the information presented critically. This overreliance can lead to significant personal consequences, such as making health, financial, or educational decisions based on inaccurate or biased information.

Beyond individual impacts, the trustworthiness of generative models has broader societal implications, particularly in the domains of misinformation, academic \citep{liang2024monitoring,geng2024chatgpt,geng2024impact}, and systemic inequality \cite{korinek2023generative}. On a societal scale, generative models have become potent tools for generating and disseminating misinformation, complicating the public’s ability to discern credible information from fabricated content \cite{huang2023harnessing}. The proliferation of machine-generated misinformation, such as deepfakes and fake news \cite{lyu2024deepfake}, undermines public trust in media and information sources, posing a significant threat to democratic processes and social cohesion \cite{chencombating}. The challenge lies not only in the models' capacity to produce misleading content but also in the growing difficulty of detecting and mitigating such outputs, which can erode societal trust in legitimate information channels.

The amplification of social inequities through untrustworthy generative models further underscores their broad societal impact. When these models perpetuate biases, they do not merely reflect the prejudices embedded in their training data but actively contribute to the reinforcement of systemic discrimination \cite{anderljung2023frontier}. For example, biased models used in hiring, legal, or financial decision-making can exacerbate existing disparities, disproportionately affecting marginalized communities \cite{bukar2024decision}. These impacts extend beyond the individuals directly affected, perpetuating cycles of inequality that are deeply embedded in societal structures. Moreover, Zeng et al. emphasize the societal risks brought by generative models \cite{zeng2024ai}, including \textit{Disrupting Social Order}, \textit{Deterring Democratic Participation}, and so on.

Economic disruptions caused by generative models also have significant societal repercussions. As generative models increasingly automate tasks across various industries (\emph{e.g.}, software development \cite{chatdev}, artistic creation \cite{carrillo2023diffusart, somepalli2023diffusion}), there is growing concern about job displacement and the broader implications for the labor market \cite{Eloundou2023GPTsAG}. While generative models can enhance productivity and drive innovation, they also threaten to displace workers, particularly in roles that involve routine or easily automated tasks. 

Lastly, the environmental impact of generative models cannot be overlooked. The training and deployment of large-scale generative models (\emph{e.g.}, GPT-4) require substantial computational resources, leading to significant carbon emissions that contribute to climate change \cite{li2023making, luccioni2024power}. The environmental footprint of these models represents a collective societal burden, emphasizing the need for more sustainable practices.

In conclusion, the trustworthiness of generative models is a critical factor that shapes their impact on both individuals and society. Ensuring that generative models are developed and deployed in ways that prioritize fairness, transparency, and accountability is essential to harnessing their potential for positive impact while minimizing the risks they pose to individuals and society as a whole.

% \subsection{Trustworthiness is Subject to Dynamic Changes}
% Assessing trustworthiness in GenFMs is inherently challenging, given that some key components—such as fairness and morality—are subjective and lack universally accepted definitions. The complexity stems partly from longstanding debates with no clear consensus regarding ethics-related principles and, at times, even conflicting definitions for them. For instance, when evaluating a model’s moral decision-making, the result could significantly differ by the philosophical approach taken, e.g., utilitarian or deontological \cite{gawronski2017makes, anderson2011machine}. This example highlights a broader phenomenon: the absence of an objective, or at least a highly acceptable definition of a principle, leads to a built-in difficulty in establishing a mathematical or universal metric for assessing this principle.

% Even when a certain definition is adopted, the very nature of such principles may leave some flexibility in their interpretation. As a result, different cultural, political, and societal approaches that apply the same definition to a case may reach opposite conclusions. For instance, what one society considers biased might be viewed as fair in another societal context\cite{henrich2010markets, greene2014moral}. This context-dependency means that even well-designed metrics might not translate effectively across different cultures or applications. The lack of clear definitions for these terms creates fundamental challenges in designing trustworthy benchmarks. 

Acknowledging these inherent limitations does not diminish the value of trustworthiness benchmarks. Rather, it emphasizes the importance of transparency in benchmark design and implementation. When a benchmark adopts specific ethics-related interpretations, it inevitably aligns with certain ethical approaches while potentially diverging from others. By being transparent about the ethical assumptions and definitions, benchmarks can provide valuable insights. Such transparency allows stakeholders to make informed decisions about which benchmarks best align with their goals, contributing to more meaningful evaluations of AI systems.




\subsection{Alignment: A Double-Edged Sword? Investigating Untrustworthy Behaviors Resulting from Instruction Tuning}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{image/discussion_alignment.pdf}
    \caption{Benefits and potential untrustworthy behaviors from alignment process.} \vspace{+0.15in}
    \label{fig:alignment_double_edge}
\end{figure}


A key distinction between LLMs like InstructGPT \cite{ouyang2022training} and earlier models such as GPT-3 \cite{brown2020language} lies in their enhanced ability to follow human instructions, beyond just increased model size. This improvement stems largely from alignment techniques that adjust the model's behavior to better align with human preferences. These techniques include Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, Direct Preference Optimization (DPO) \cite{rafailov2024direct}, and Reinforcement Learning from Human Feedback (RLHF) \cite{ouyang2022training}. Broadly speaking, alignment \cite{shen2023large, ji2023ai, wang2024comprehensive, wang2023aligning, yao2023instructions, cao2024towards, liu2023trustworthy} involves embedding human values and objectives into LLMs to improve their helpfulness, safety, and reliability, which are some of the key attributes in establishing the model's trustworthiness.

While alignment aims to reconcile the mathematical training of an LLM with the human values we expect, this process can sometimes lead to unintended negative consequences. For instance, Lin et al. \cite{lin2023unlocking} examined the effects of alignment tuning by analyzing shifts in token distribution between base LLMs and their aligned counterparts. Their findings reveal that the decoding performance of both the base models and aligned versions remains nearly identical across most token positions \cite{lin2023unlocking}, aligning with earlier research \cite{zhou2024lima} suggesting that the impact of alignment tuning may be \textit{superficial}.
Additionally, Sharma et al. \cite{sharma2023understanding} found that \textit{sycophantic behaviors} can emerge as an unintended consequence of the instruction tuning process, with their experiments indicating that human preferences and preference models often prioritize sycophantic responses over truthful ones. Similarly, Hubinger et al. \cite{hubinger2019risks} identified \textit{deceptive alignment} as a potential risk, where a model appears to follow the specified objective within the training distribution but actually pursues a different objective outside of it—an undesirable outcome of the alignment process.
Moreover, McKenzie et al. \cite{mckenzie2023inverse} discovered that alignment can lead to overoptimization, potentially causing \textit{inverse scaling}, where a model's performance deteriorates as its size increases. Lastly, studies \cite{turner2019optimal, turner2022parametrically, krakovna2023power} have shown that optimal policies and reward functions can incentivize systems to seek power in certain environments, a behavior known as power-seeking.
In line with this finding, Ngo et al. \cite{ngo2022alignment} and Shevlane et al. \cite{shevlane2023model} have found that LLMs may develop situational awareness, potentially enabling models to evade human oversight.

To understand the root causes of these issues, improving the interpretability of large generative models \cite{singh2024rethinking} is essential. In particular, Mechanistic Interpretability \cite{nanda2023progress, conmy2023towards, zimmermann2024scale, rai2024practical} is a powerful approach to unlocking the black box of large generative models, enabling a deeper understanding of their inner workings. This method involves reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts, thereby providing a detailed, causal explanation of how these models operate. Bereska and Gavves \cite{bereska2024mechanistic} explore how mechanistic interpretability can be leveraged to enhance AI safety.

Given the discussion above, we highlight the trustworthiness issues in large models that arise from the alignment process. Therefore, future research should focus on improving alignment techniques or developing mitigation strategies to reduce the undesirable behaviors resulting from instruction tuning.

\subsection{Lessons Learned in Ensuring Fairness of Generative Foundation Models}

In achieving fairness within generative models \cite{gallegos2024biassurvey, 10.1145/3682112.3682117, openai_2024_democratic}, it is essential to recognize the complexity and multi-dimensional nature of the concept. Fairness cannot be universally applied with a single, uniform standard; rather, it must be adapted to different groups' unique needs and contexts \cite{lee2019context}. Below, we explore several key considerations in defining and achieving fairness in generative models.

\textbf{Fairness is not a one-size-fits-all concept; it should be adapted to the needs of different groups and contexts.}
Fairness is inherently context-dependent, and generative models should reflect this. A one-size-fits-all approach to fairness may fail to account for different social groups' varying needs and circumstances. For instance, gender-specific needs such as \textit{maternity leave for women} and \textit{paternity leave for men} present distinct challenges in workplace policy. If a generative model were to generate outcomes for workplace fairness policies that only accounted for general parental leave, without distinguishing between the different impacts of maternity versus paternity leave, it would fail to accommodate the specific needs of each gender. For women, the physiological and social implications of childbirth require different support systems than for men, who may face different challenges in balancing family and work life. Thus, fairness in generative models must be adaptive, ensuring that outcomes for different demographic groups are both equitable and contextually relevant.

\textbf{Achieving fairness requires not only equal treatment within groups but also building understanding between different groups.}
Fairness is not solely about providing equal treatment within a group \cite{weerts2023fairlearn}, but also about fostering mutual understanding between different groups. Consider an example where a generative model generates job application feedback for different demographic groups. While it might ensure that both men and women receive equally constructive feedback, it also needs to avoid reinforcing subtle stereotypes or biases that could prevent cross-group understanding \cite{eloundou2024first}. For example, if the model generates feedback that unintentionally suggests women apply for more traditionally "feminine" roles like nursing while suggesting men apply for "masculine" roles like engineering, it perpetuates societal divisions. A fair model would go further, encouraging users to explore \textit{roles beyond traditional gender stereotypes} and facilitating understanding between groups by suggesting opportunities for men and women in a wide range of fields, thus promoting inclusivity and mutual respect.

\textbf{Generative models should serve as tools to provide information, empowering users to make their own decisions, rather than dictating choices.} User decisions are often shaped by a wide range of factors, such as cultural, societal, or personal influences, which models cannot fully account for. In the pursuit of fairness, generative models should function as facilitators of decision-making, empowering users with access to information rather than prescribing particular actions. For example, imagine a generative model designed to assist students in selecting academic subjects or career paths. Instead of directly suggesting that a female student should consider a humanities-based career, the model should present a balanced range of academic options—such as STEM, business, arts, or humanities—based on the student’s interests, skills, and preferences. The model should provide unbiased and relevant data about each field (such as job prospects, skill requirements, and salary expectations), enabling the user to make an informed choice. A model that dictates decisions, such as suggesting “Given that you are a woman, I would advise against pursuing math-intensive careers,” risks reinforcing societal biases and disempowering users. Instead, models should act as supportive tools, offering objective data that allows individuals to retain autonomy over their decisions.

\textbf{Fairness must be evaluated both in terms of the model’s development process and its outcomes.}
Fairness in generative models requires a dual evaluation: both the fairness of the development process (procedural fairness) and the fairness of the model’s outputs (outcome fairness). Consider a scenario where a generative model is trained to generate financial advice. Procedural fairness would require that the training data used to build the model represents a diverse range of financial behaviors across different demographic groups (e.g., age, gender, income level). If the model were trained predominantly on data from high-income males, its recommendations might be skewed towards the financial realities of that group, failing to address the needs of other populations, such as low-income families or retirees. Outcome fairness, in this context, would ensure that the financial advice generated is equally relevant, actionable, and beneficial for all users, regardless of their demographic background. Therefore, a comprehensive fairness evaluation must encompass both the process and the results to ensure that generative models produce genuinely equitable outcomes \cite{ibm2022fairness}.

% \textbf{Fairness should not only focus on statistical equality but also consider cognitive and contextual differences across genders.}
% Fairness must extend beyond statistical equality and account for cognitive and contextual differences, particularly when considering gender disparities. For example, research shows that men and women may have different approaches to problem-solving in STEM fields due to both biological and social influences. A generative model designed to provide academic tutoring should not assume that all students, regardless of gender, will benefit equally from the same teaching methods. If the model uniformly offers math instruction without considering diverse learning styles, it could disadvantage students who may respond better to alternative teaching strategies. For instance, women may benefit from more contextually grounded examples in math and science, as research has shown that connecting abstract concepts to real-world applications can help overcome societal barriers to women’s engagement in STEM. Therefore, a fair model must account for these cognitive and contextual differences to ensure that it provides equitable support to all users, regardless of gender.

\textbf{The existence of social disparities forces us to question whether we should strive for fairness or manage trade-offs in model outcomes.}
In a world where social and economic disparities are pervasive, striving for fairness in generative models presents complex challenges. Consider an AI model designed to evaluate loan applications. Strict fairness might dictate that all applicants are evaluated using the same criteria, regardless of their background. However, applicants from historically disadvantaged communities may have less access to credit and, therefore, lower credit scores, making them less likely to receive favorable outcomes under a uniform evaluation system. In this case, enforcing equal treatment without addressing historical disparities could perpetuate inequality. The model may need to account for these social disparities by adjusting its evaluation criteria or weighting factors, such as considering community investment or alternative financial behaviors that don’t rely on traditional credit scoring. Thus, the pursuit of fairness in model outcomes may involve difficult trade-offs, where achieving equitable results requires nuanced adjustments rather than strict adherence to identical treatment for all \cite{persistent2023fairness}.

\textbf{Disparagement in generative models may be subtle and difficult to distinguish from fact-based statements, requiring careful handling.}
Disparagement in generative models can be insidious and difficult to detect, especially when it is embedded in factually accurate statements. For instance, if a generative model responds to a question about gender wage gaps by stating that "women, on average, earn 82\% of what men earn for the same job," this statement is factually correct but could reinforce negative perceptions about women’s earning potential. While such a response provides accurate information, it might overlook the broader context of systemic barriers that contribute to this wage gap, such as discriminatory hiring practices or unequal access to leadership opportunities. A fair model must cautiously frame such data to avoid perpetuating harmful narratives. Instead, it should provide balanced insights, such as highlighting ongoing efforts to close the wage gap or discussing the structural changes needed to promote gender equality in the workplace. This approach ensures that the model presents fact-based statements in a way that avoids reinforcing societal biases or disparagement.


% \subsection{Incorporating Human Visual Limitations in Vision Modeling: A Dilemma (Yue)}

% \yue{Need Yuan, Tianrui to polish.}

% In this part of the discussion, we will talk about how vision algorithms or models can learn from human visual limitations. Before proceeding, it is crucial to differentiate between two types of hallucinations. The first type arises from biases in the model's design or training, leading to inaccurate outputs, as illustrated in \textcolor{red}{Figure 1}. These hallucinations are generally ineffective in deceiving humans, meaning that such images do not easily mislead individuals. The second type of hallucination, however, is visually deceptive from a human perspective, as demonstrated in \textcolor{red}{Figure 2}.

% The primary objective for addressing the first type of hallucination is to mitigate or eliminate these occurrences in models and algorithms as much as possible. As previously discussed, this form of hallucination significantly undermines the reliability and trustworthiness of models and systems.

% Regarding the second type of hallucination, it is important to recognize that while human vision is highly effective, it is not infallible. Human brains are adept at interpreting visual data, understanding complex scenes, recognizing objects, and detecting patterns. However, visual illusions reveal that perception is not a direct representation of reality but is instead shaped by neural processes that simplify, generalize, and sometimes distort sensory input. These illusions occur because the visual system operates on assumptions that are generally effective but can occasionally fail to match actual stimuli.

% Thus, the critical question for the second type of hallucination is: Should these limitations of human perception be incorporated into human-inspired vision algorithms?

% Some argue that such limitations should be avoided. The susceptibility of human vision to visual illusions indicates its inherent imperfections. Machines do not need to replicate the same assumptions or cognitive shortcuts as humans; they can potentially avoid the perceptual errors that humans are prone to, thereby surpassing human capabilities in certain contexts. In high-stakes applications such as autonomous driving or medical imaging, errors in visual processing could have severe consequences, making it preferable to design algorithms that completely bypass human perceptual biases.

% Conversely, there is a strong argument for incorporating these "errors" because they are not flaws but adaptations. Human vision is not optimized for perfection but for adaptability and efficiency in real-world environments. Many perceived "limitations" of human vision arise because the brain prioritizes speed and functionality over absolute accuracy by employing shortcuts. For example, visual illusions often exploit assumptions about light, shadow, or perspective, which generally aid in quickly and effectively interpreting ambiguous stimuli in natural settings. Integrating similar assumptions into vision algorithms could enhance machines' efficiency and effectiveness in processing complex visual data. Therefore, the decision to incorporate such "illusions" into vision algorithms may hinge on computational resource constraints and specific application scenarios. The so-called limitations of the human visual system are, in fact, adaptations shaped by environmental selective pressures. This suggests that any visual system, human or artificial, might exhibit limitations in particular environments, and the appropriateness of mimicking these limitations depends on the intended application. For natural scene analysis, emulating the human visual system could be advantageous, but in specialized or artificial contexts, surpassing these limitations might be necessary.

% Another critical consideration is the inherent difficulty, if not impossibility, of completely avoiding these perceptual "errors." Human vision is not fully understood, and neural networks often function as black boxes, making it likely that unexpected behaviors in human-inspired algorithms will mirror cognitive illusions. Vision processing is inherently complex, and both biological and artificial systems that process vast amounts of information rapidly are prone to develop shortcuts and biases.

% In conclusion, while there is an appeal in designing vision algorithms that circumvent errors found in visual illusions, there are compelling reasons to embrace certain aspects of these perceptual characteristics when they offer practical advantages in real-world performance. Rather than adopting a binary approach, the challenge lies in making nuanced decisions about which elements of human vision to replicate and which to avoid, tailored to specific application contexts.

\subsection{Balancing Dynamic Adaptability and Consistent Safety Protocols in LLMs to Eliminate Jailbreak Attacks}

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{image/jailbreak_discussion.pdf}
    \caption{The root causes of LLM safety inconsistencies and potential improvement strategies.} \vspace{+0.15in}
    \label{fig:safety_discussion}
\end{figure}

While \textbf{\S\ref{sec:discuss_dynamic}} highlights the importance of models dynamically adapting to different users’ needs, jailbreak attacks often exploit this adaptability by simulating various roles to achieve success \cite{shen2023anything, ma2024visual, jailbreakanalysis2, shah2023scalable, li2023deepinception}. This means that LLM simulations can inadvertently create vulnerabilities, leading to successful jailbreaks. To prevent this, models need to balance dynamic trustworthiness with robust security measures. We propose that different models could use distinct trustworthiness protocols to meet diverse user needs. However, a single model must maintain a consistent safety protocol to ensure that its safety standards are not compromised, regardless of how a question is phrased. Specifically, as shown in \autoref{fig:safety_discussion}, for any given query, even if it is rephrased, placed in different scenarios, or simulated under different contexts, the LLM should consistently judge whether the query violates the safety protocols. In other words, the model must generate the same safe and trustworthy response for different ways of asking the same question.

Current safety training methods, such as safety fine-tuning or RLHF for Safety, tend to focus on identifying specific harmful inputs, aligning with the autoregressive nature of LLMs \cite{zhou2024defending, deng2023multilingual, paulus2024advprompter, bhardwaj2023redteaming}. However, while harmful outputs are direct violations of safety protocols, many different inputs can lead to the same harmful output, and it is impractical to account for all these inputs during training. Since LLMs are primarily trained to provide helpful answers, scenarios not covered during safety training may still result in successful jailbreaks. This highlights the limitations of relying solely on input-based safety measures and underscores the need for models to ensure output consistency alongside strict safety protocols to prevent potential vulnerabilities.

Jailbreak attacks often exploit the insufficient coverage during training. In these cases, LLMs transform harmful queries by adding complexity or ambiguity, bypassing the boundaries set by safety training \cite{jailbreak20queries, shah2023scalable, gong2023figstep, ma2024visual}. Many studies have shown that LLMs can also assist in rephrasing or breaking down harmful queries, effectively circumventing safety mechanisms \cite{huang2024obscurepromptjailbreakinglargelanguage, chang2024play}. The issue here is that LLMs may not recognize that transforming or rephrasing harmful queries is itself harmful. As a result, they may inadvertently relax the enforcement of safety protocols. To address this, models must strictly enforce a consistent safety protocol, ensuring that harmful queries cannot be executed, regardless of how they are phrased or transformed.

To overcome the limitations in current LLM safety training, a "multi-level consistency supervision mechanism" could be implemented to improve model security. This approach enhances defense capabilities in three key areas: First, by introducing output-level consistency training, models need to be trained to ensure that semantically similar but differently phrased inputs yield the same safe and consistent output, preventing harmful inputs from bypassing safety mechanisms through linguistic variation. Second, a context-sensitive safety detection module can be added to track the entire conversation or input context, dynamically identifying shifts in user intent, and preventing complex multi-step transformations from leading to jailbreaks. Finally, post-output dynamic defense mechanisms can be designed to review the generated output in real-time, ensuring it adheres to safety protocols, with dynamic rule updates to address new types of harmful inputs. This approach reduces reliance on exhaustive input-based training, strengthens the model’s safety across different contexts, and enhances both adaptability and consistency, preventing it from being manipulated into producing harmful outputs.

Additionally, since different models are designed to adapt to various users' needs, they should be equipped with a dynamic user policy to regulate user behavior and interactions, ensuring that the model's safety and consistency are maintained throughout the interaction.


\subsection{The Potential and Peril of LLMs for Application: A Case Study of Cybersecurity}
The integration of LLMs into cybersecurity operations represents a paradigm shift in the field's technical capabilities and threat landscape. Recent evaluation frameworks like SWE-bench~\cite{jimenez2024swebench} and Cybench~\cite{zhang2024cybench} have demonstrated potential in automated security testing, establishing new paradigms for assessing LLM capabilities across cryptography, web security, reverse engineering, and forensics~\cite{hu2020automated, yang2024whitefox, wang2024llmif, meng2024large, deng2023large, ma2024one, ullah2024llms, aicc2024}. However, this technological advancement presents a double-edged sword. The advent of LLMs enhances the accessibility to cybersecurity defenses but also introduce potential vectors for adversarial exploitation. As demonstrated by OpenAI's recent threat intelligence reports~\cite{openaicybersecurity}, AI models have already become targets for malicious exploitation, with over 20 state-linked cyber operations and deceptive networks attempting to weaponize these systems in 2024 alone. The capabilities that make LLMs powerful tools for security professionals also create unprecedented challenges in the hands of malicious actors: First, their advanced code analysis capabilities could dramatically accelerate zero-day exploit discovery~\cite{fang2024teams, shen2024pentestagent, ristea2024ai}, potentially overwhelming traditional security response mechanisms. Second, their natural language processing prowess enables the automation of highly sophisticated social engineering attacks~\cite{falade2023decoding, charfeddine2024chatgpt} such as phishing. Third, their ability to generate and modify code could lead to more advanced malware that adapts in real-time to evade detection systems~\cite{madani2023metamorphic, usman2024dark}.


These challenges in cybersecurity offer crucial lessons that parallel similar concerns across multiple domains. In the realm of disinformation, LLMs can also generate highly convincing synthetic content at unprecedented scale. Recent studies have documented sophisticated disinformation campaigns leveraging LLMs to create coordinated networks of artificial personas and targeted messaging~\cite{atinstitute}. In academia, the issues extend beyond simple academic integrity violations~\cite{uchiacademic} to fundamental questions about research validity. Cases of fraudulent research reporting~\cite{majovsky2023artificial} demonstrate how LLMs can be misused to generate seemingly legitimate scientific papers. Similarly, in sensitive research areas such as genetic engineering~\cite{sandbrink2023artificial} and pharmaceutical development~\cite{anibal2024simulated}, LLMs can accelerate both beneficial and potentially harmful research directions, just as they can expedite both defensive and offensive capabilities in cybersecurity. These cross-domain challenges underscore a universal truth revealed by the cybersecurity case study: the need for comprehensive governance frameworks that can adapt to rapidly evolving AI capabilities while maintaining robust safeguards against misuse. Such frameworks must balance the imperative of scientific advancement with responsible innovation, particularly given the emergence of autonomous agent architectures that leverage external tool integration.

The governance challenges revealed through both cybersecurity and broader domain analyses point to fundamental gaps in our ability to harness LLMs' potential while mitigating their risks. While leading organizations have established initial frameworks - including Microsoft's AI Security Framework~\cite{microsoft2023ai}, Google's AI Principles and Security Standards~\cite{google2023responsible}, and OpenAI's Usage Guidelines~\cite{openai2023usage} - these represent only preliminary steps toward comprehensive governance. As noted by Anthropic~\cite{anthropic2023responsible}, current generative foundation models cannot anticipate users' ultimate intentions or subsequent actions, necessitating broader governance frameworks that transcend domain-specific boundaries. Looking ahead, several critical research directions emerge. First, there is an urgent need to develop domain-agnostic detection systems that can identify potentially harmful LLM-generated content~\cite{wu2023survey, rieck2007language} - whether it manifests as malicious code in cybersecurity, synthetic content in disinformation campaigns, or fraudulent submissions in academic research. Second, advancing adaptive defense mechanisms represents a crucial frontier, requiring self-evolving defense systems that can automatically update their protective measures based on emerging threat patterns. Such adaptive systems may incorporate reinforcement learning techniques for continuous policy optimization and federated learning approaches for distributed threat response while maintaining system stability. Third, establishing robust red-teaming frameworks will be essential for proactive security, encompassing systematic vulnerability assessment methodologies, quantifiable security metrics for model evaluation, etc.



% \subsection{Trustworthiness of Generative Foundation Models in Medical Domain}

% The integration of GenFMs into medical applications represents a paradigm shift in the field’s capability to automate diagnostic and decision-making tasks, enhancing efficiency and accessibility in healthcare~\cite{zhou2024ttt}. Recent advancements in GenFMs have demonstrated their potential across areas such as medical agents, video generation, and automated report synthesis, offering transformative possibilities for improving patient outcomes~\cite{liu2023deid}. For instance, multi-modal agents leveraging GenFMs to combine text and image inputs have shown remarkable promise in assisting clinicians with complex diagnostic scenarios~\cite{dai2023ad}. Similarly, generative video models powered by these foundational technologies are being utilized to create realistic visualizations of medical procedures and pathologies, revolutionizing education and training~\cite{li2024echopulse, reynaud2024echonet}. Automated report generation systems based on GenFMs, which draft detailed medical summaries from raw data, are streamlining workflows and reducing clinician workloads~\cite{liu2023tailoring}. Despite these groundbreaking applications, ensuring the trustworthiness of GenFMs in high-stakes environments remains a critical challenge, as errors can have severe consequences. \yue{This paragraph is a description of related work, which is no needed in the discussion section. Use only two sentences to indicate the wide application of Medical GenFMs.}

% \yue{One thing should be emphasized here: what are the differences between general GenFMs and medical GenFMs? What kind of challenge do these differences bring? Separate these differences and challenges into multiple points and discuss them.}

% The inherent complexity of medical scenarios exposes vulnerabilities in GenFMs. Adversarial inputs, such as altered medical images or ambiguous textual prompts, can exploit the susceptibility of these models, leading to erroneous outputs that potentially jeopardize patient safety~\cite{liu2023holistic,lee2023llm,bai2024m3d}. For example, diagnostic agents employing vision-language GenFMs may misinterpret subtle data irregularities, resulting in incorrect or incomplete conclusions~\cite{ma2024eye,li2024llava,moor2023med}. These vulnerabilities are particularly concerning in generative video applications, where synthetic content must achieve not only high fidelity but also strict clinical accuracy. Hallucinations or unrealistic artifacts in generated videos can mislead practitioners, while privacy concerns surrounding patient data further complicate their deployment~\cite{yan2024biomedical}. Likewise, GenFMs used in automated report generation face challenges in producing consistently accurate and interpretable content, as errors or ambiguities in reports can delay treatments or contribute to misdiagnoses~\cite{liu2024fine,thirunavukarasu2023large}. \yue{You are describing the vulnerabilities here. However, it seems the description is not well-structured. Use 1), and 2) to clarify these vulnerabilities.}

% \yue{For the following several paragraphs, it seems the discussion is not deep enough. You are talking about what kind of action researchers have taken, without proposing potential actions. For instance, you have mentioned current study has used "Anomaly detection methods", and you suggested the use of "advanced anomaly detection systems" in the future, which is not informative and meaningful.}

% Addressing these challenges requires robust technical safeguards specifically tailored for GenFMs. Recent studies have proposed real-time monitoring systems that evaluate GenFM outputs against established clinical safety standards, reducing the likelihood of errors~\cite{liu2023artificial}. Anomaly detection methods designed for GenFMs in medical contexts effectively identify unsafe or unreliable outputs, enhancing model reliability during deployment~\cite{ouyang2019echonet}. For generative video and report systems, validation pipelines that integrate ground-truth annotations and expert peer reviews are crucial to ensuring clinical relevance and accuracy~\cite{liu2023evaluating}. Moreover, privacy-preserving techniques in the training of GenFMs can mitigate concerns about sensitive patient data, fostering trust and wider acceptance among stakeholders.

% Beyond technical solutions, the governance and ethical deployment of GenFMs in medicine is of paramount importance. While technical safeguards address immediate risks, the unpredictable nature of real-world applications necessitates comprehensive governance frameworks. These frameworks must ensure that GenFM systems are not only innovative but also ethically sound and compliant with regulatory requirements~\cite{zhong2024evaluation}. Transparency mechanisms, such as explainability features that trace the reasoning behind GenFM outputs, can further enhance user confidence, particularly in critical clinical environments~\cite{liu2023tailoring,alberts2023large}.

% Looking forward, several critical research directions emerge for advancing GenFMs in medicine. First, there is an urgent need to develop advanced anomaly detection systems tailored for GenFM-based applications, ensuring robust performance in diverse and dynamic clinical settings~\cite{reynaud2023feature}. Second, integrating explainability into GenFM frameworks remains a key challenge, requiring innovative methods to make their outputs understandable and verifiable by clinicians~\cite{liu2024fine}. Finally, the medical community must establish interdisciplinary training programs to equip professionals with the skills to leverage GenFMs while understanding their limitations effectively. A hybrid paradigm where GenFM capabilities augment human expertise will be essential for realizing their full potential while safeguarding against misuse or errors.


\subsection{Trustworthiness of Generative Foundation Models in Medical Domain}

Addressing the challenges that arise with integrating generative foundation models (GenFMs) into healthcare is complex and multifaceted, requiring both technical innovations and policy considerations. Although current advancements have made strides, significant issues persist that require in-depth research and novel solutions to ensure the trustworthiness of these models in high-stakes medical contexts.

\textbf{Data quality and availability} are key challenges for generative models in healthcare. Medical data is often noisy, incomplete, and heterogeneous, coming from various sources like electronic health records (EHR), medical imaging, and genomics~\cite{johnson2016mimic}. Variability in data formats across institutions limits interoperability and model utility. High-quality labeled data requires domain experts, making annotation costly and time-consuming~\cite{kohli2017medical}. Data biases can also lead to poor generalization. Privacy regulations like HIPAA~\cite{gostin2009beyond} and GDPR~\cite{li2019impact} protect patient data but hinder data sharing needed for robust model development~\cite{shickel2017deep}. Privacy-preserving techniques like federated learning help but face challenges like communication overhead and privacy risks. Improving data quality and availability requires standardizing data formats, better curation, and collaboration for secure data sharing. Building large, diverse datasets is essential for model generalization and trustworthiness~\cite{yang2019federated}.

\textbf{Model explainability} represents a critical frontier in the development of generative AI for healthcare, addressing fundamental challenges of trust, ethics, and clinical utility. The "black-box" nature of complex machine learning models creates a significant barrier to adoption, as healthcare professionals require transparent mechanisms to validate and understand AI-generated insights. This transparency is not merely an academic concern but a practical necessity in high-stakes medical decision-making~\cite{doshi2017towards}. The imperative for explainability extends beyond technical considerations into ethical and legal domains. Clinicians must be able to trace the reasoning behind AI recommendations, ensuring that patient care remains fundamentally human-centered. Opaque models risk undermining informed consent, as patients have a right to understand the basis of their treatment recommendations~\cite{guidotti2018survey}. Moreover, unexplainable models can perpetuate or even amplify existing healthcare biases, potentially exacerbating systemic inequities in medical diagnosis and treatment~\cite{obermeyer2019dissecting}. Emerging research has developed sophisticated approaches to model interpretability, moving beyond simplistic transparency techniques. Methods like attention mechanisms, feature visualization, and domain-specific explanation frameworks offer promising pathways to demystify complex generative models~\cite{selvaraju2017grad}. These approaches aim to translate intricate computational processes into clinically meaningful insights, allowing healthcare professionals to critically assess AI-generated outputs within their expert knowledge context~\cite{rudin2019stop}. The goal of interpretability is not to compromise model performance but to create a collaborative interface between artificial intelligence and clinical expertise. By developing models that can articulate their reasoning, researchers can build trust, enable more nuanced clinical decision support, and create intelligent algorithmic tools that augment rather than replace human medical judgment~\cite{caruana2015intelligible}. This approach heralds a transformative vision of technological evolution, where the most advanced systems are defined not by their computational power, but by their capacity to engage in transparent, meaningful dialogue across the boundaries of human and machine intelligence.

\textbf{Regulatory and legal framework} The evolving regulatory landscape for generative models in healthcare presents barriers to adoption~\cite{rieke2020future,beam2018big}. Regulatory bodies like the FDA~\cite{food2021artificial} and EMA~\cite{fraser2018need} ensure models are safe and effective, but the dynamic nature of generative models challenges traditional frameworks designed for static software or devices~\cite{muehlematter2021approval}. A major challenge is creating a standardized process for validating generative models, especially those needing frequent updates. Current pathways do not fully address iterative model development~\cite{wu2021medical}. Regulatory bodies are exploring new approaches like "software as a medical device" (SaMD)~\cite{food2019proposed} and the Total Product Life Cycle (TPLC) approach~\cite{hwang2016study}, but these need further refinement. Legal liability is another issue. When generative models produce incorrect diagnoses or recommendations, it is unclear who is responsible—developers, healthcare providers, or institutions. This ambiguity hinders adoption due to potential legal risks. Clear accountability guidelines and robust validation are critical for fostering trust in generative models. Advancing the regulatory and legal framework for generative models requires collaboration among developers, healthcare professionals, policymakers, and regulators. Setting standards for data quality, model validation, transparency, and post-market surveillance is essential to ensure generative models in healthcare are safe, reliable, and trustworthy.

 \subsection{Trustworthiness of Generative Foundation Models in AI for Science}
In scientific fields such as chemistry, biology, and materials science, the application of generative models introduces unique trustworthiness challenges due to the critical need for precision, safety, and speed in discovery \cite{fan2023trustworthiness,messeri2024artificial,he2023control,zhang2023survey}. These domains require not only the rapid generation of data or models but also strict accuracy and adherence to established scientific principles. While generative models hold immense potential for creating novel compounds and materials, they also carry risks—such as the unintended generation of toxic or hazardous entities that could pose harm if synthesized or used improperly. In this discussion, we aim to address two key questions: \textbf{1) To what extent should humans trust the outputs of generative models?} and \textbf{2) How can we balance the need for rapid innovation with the imperatives of precision, safety, and ethical compliance in scientific applications of these models?}

The trust placed in generative model outputs depends on transparency, validation, and understanding of uncertainty. Scientific models operate with varying degrees of uncertainty due to the complexity and novelty of data \cite{schwaller2021prediction,raghavan2023dataset,choudhary2022recent,schleder2019dft,chen2025unveiling,guo2024can,huang2024application,liang2024scemqa,chen2024scholarchemqa}; quantifying this uncertainty helps researchers decide how much weight to place on predictions. For instance, in drug discovery, confidence scores in AI-proposed molecules allow researchers to prioritize compounds with the highest predicted efficacy for experimental verification \cite{nigam2021assigning,borkakoti2023alphafold2,zeng2022deep,le2024molx}. In addition, validation against empirical data is equally crucial. A robust feedback loop, where AI-generated hypotheses or predictions are iteratively tested, refined, and tested again, builds confidence in model outputs. This is especially relevant in fields like materials science, where new molecular structures proposed by AI must align with known databases and principles before they are synthesized \cite{shu20203d,bickel2023design,zeni2023mattergen}. Furthermore, interpretability \cite{medina2024interpretable,gangwal2024unlocking} also plays a significant role in establishing trust; understanding the factors driving a model’s decisions allows scientists to assess the biological, chemical, or physical plausibility of the results. For example, a protein-structure-predicting model that provides interpretable explanations enables researchers to judge the biological feasibility of each proposed structure. Therefore, trust in AI for science is collaborative; humans must critically assess AI outputs, using these models to augment rather than replace their expertise.

Furthermore, although generative models offer unprecedented speed in generating scientific data and hypotheses, balancing this rapid pace with rigorous safety and ethical standards is essential. Frameworks for responsible innovation can guide both swift exploration and meticulous verification. This often involves phased deployment \cite{elemento2021artificial,kaur2023artificial,miotto2018deep,van2016deep}, where AI outputs are gradually introduced alongside ongoing checks for accuracy, safety, and compliance. Implementing and enforcing ethical constraints within model designs is also critical. For example, in chemical research \cite{gromski2019explore}, automated filters that identify and discard potentially hazardous outputs can prevent the generation of unsafe compounds, thereby achieving a necessary balance between innovative discovery and safety. Experimental validation and peer review remain indispensable as safeguards. Even in accelerated research workflows, it is imperative to incorporate stages for thorough validation, ensuring that any AI-generated findings undergo rigorous testing before being widely applied. This hybrid approach—combining the speed and creativity of AI with the scrutiny of human oversight—enables rapid iteration while ensuring that only reliable outputs reach critical applications. In particular, generative models are also utilized to guide humans in conducting proper experimental operations and enforcing safety-related decision-making \cite{zhou2024labsafety,ramos2024review,boiko2023autonomous}. Regulatory and institutional oversight further play a role in maintaining this balance by defining standards and evolving in response to technological advances.

Addressing these key questions reveals that trust in generative models within scientific domains is multidimensional. Through transparency, validation, ethical compliance, and a collaborative human-AI approach, these models can advance scientific discovery responsibly. Achieving a balance between innovation and caution will allow us to harness the potential of generative models while upholding the precision, safety, and ethical standards integral to scientific progress.



\subsection{Trustworthiness Concerns in Robotics and Other Embodiment of Generative Foundation Models}


The development of LLMs and VLMs has greatly improved robots' capabilities of natural language processing and visual recognition. However, integrating these models into real-world robots comes with significant risks due to their limitations. LLMs and VLMs can produce errors from language hallucinations and visual illusions~\cite{HallusionBench}, which may raise safety concerns~\cite{wu2024safety, robey2024jailbreaking}, particularly when their outputs influence the robot's physical actions and interaction with the real-world environment.

In the context of AI's physical embodiment, safety refers to a robotic system's ability to perform tasks efficiently and reliably while preventing unintended harm to humans or the environment. Such harm can result from unexpected, out-of-distribution inputs, response randomness, hallucinations, confabulations, and other related issues. Safety can be compromised in two main aspects: \textit{reasoning and planning}, and \textit{robot's physical actions}. 


\textbf{\textit{Reasoning and Planning.}} The embodied agent can exhibit ambiguity in decision-making or overconfidence in prediction, leading to poor decisions, including collisions and unsafe maneuvers. For instance, Azeem et al.~\cite{azeem2024llmdrivenrobotsriskenacting} found that LLM-driven robots can enact discrimination, violence, and unlawful actions, underscoring the need for systematic risk assessments to ensure safe deployment. Additionally, if the robot fails to identify hazards, it may proceed without considering potential risks, resulting in actions that could harm people, damage objects, or disrupt its surroundings. For instance, Mullen et al.~\cite{Mullen2024DontFT} emphasize the importance of proactively identifying potential risks, presenting the SafetyDetect dataset, which trains embodied agents to recognize hazards and unsafe conditions in home environments. Their approach utilizes LLMs and scene graphs to model object relationships, enabling anomaly detection and promoting safer decision-making during planning.

\textbf{\textit{Robot's Physical Actions.}} On the other hand, even with proper and safe planning, improper actions by the robot can still pose risks during human-robot interaction. For example, if a Visual-Language-Action (VLA) model~\cite{ma2024surveyvisionlanguageactionmodelsembodied, guruprasad2024benchmarkingvisionlanguage} generates inaccurate high-level actions or controls motion with excessive force and speed, it could accidentally harm nearby individuals or damage surrounding objects. Moreover, inference latency and efficiency issues can further compromise the robot's responsiveness and overall safety.

In summary, \textit{failures in reasoning and planning} compromise safety by leading to unsound decisions, while \textit{errors in physical actions} pose direct risks to safe interaction with the environment and humans. Ensuring safety in physical embodiment requires robust strategies that keep both cognitive and physical behaviors controlled, responsive, and adaptable to unpredictable factors.

\subsection{Trustworthiness of Generative Foundation Models in Human-AI Collaboration}
The dynamics of human-AI collaboration bring significant opportunities to enhance productivity and decision-making, but they also raise fundamental questions about trust, ethics, and accountability. Central to these collaborations are GenFMs, which serve as the building blocks for many advanced AI systems. As humans and AI systems work together to achieve shared goals, it becomes imperative to address the challenges that arise when blending human intuition and creativity with machine intelligence. This section explores critical concerns surrounding trust calibration, ethical alignment, and accountability in such collaborations.

\textbf{Trust Calibration.}
One of the most persistent challenges in human-AI collaboration is determining when and to what extent AI systems, particularly generative foundation models, can be trusted. This process, known as trust calibration, is critical to striking a balance between overtrusting and undertrusting AI outputs. However, achieving effective trust calibration is complicated by users' limited understanding of how GenFMs function. Opaque marketing claims, incomplete documentation, and the inherent complexity of GenFMs exacerbate this gap, leaving even researchers grappling with the ``black box'' nature of these models, where decision-making processes remain inscrutable despite efforts to decode them \cite{chen2024inside, bhardwaj-etal-2024-language, slobodkin-etal-2023-curious}. As a result, users may overtrust AI—relying on its recommendations uncritically—or undertrust it, disregarding valuable insights \cite{bs14080671, 10.1145/3544548.3581025, Elshan2022}. Addressing these trust imbalances requires improving the transparency and interpretability of GenFMs. Key strategies for trust calibration include providing explanations for GenFMs predictions, detailing their limitations, and exposing the uncertainty inherent in their outputs \cite{cheng2024can, shi-etal-2024-safer, brahman2024the, zhang2024rtuninginstructinglargelanguage}. For example, methods such as verbalized confidence scores, consistency-based approaches, and uncertainty estimation can help users understand when GenFMs outputs are reliable \cite{lin2022teaching, tian-etal-2023-just, zhao-etal-2024-knowing, wang2023selfconsistency}. Explainability mechanisms should be intuitive and accessible, enabling users to gauge when the GenFMs' guidance aligns with their context and expertise \cite{Mitchell_2019, 10.1145/3637396}. By fostering a nuanced understanding of GenFMs behavior, trust calibration empowers users to effectively and confidently leverage the valuable insights AI can provide, promoting trustworthy human-AI collaboration.

\textbf{Error Attribution and Accountability.}
A major challenge in human-AI collaboration is determining responsibility when errors occur. As GenFMs become more complex and are integrated into critical decision-making processes, understanding the source of errors—whether they stem from GenFMs, the user, or a combination of both—has become increasingly difficult. The opaque nature of many GenFMs, coupled with limited documentation and insufficiently explained model behaviors, further complicates error attribution. Users and stakeholders may either unfairly blame GenFMs for failures, neglecting human oversight responsibilities, or conversely, fail to hold GenFMs accountable for flawed outputs \cite{walkermunro2022guiltysiliconmindblameworthiness, ryan2023whatsrolemodellingresponsibility, qi2024causalresponsibilityattributionhumanai, miller2023accountableai}. To address these challenges, fostering accountability requires developing mechanisms to trace errors back to their root causes. Strategies such as fine-grained model audits \cite{Mokander2023}, detailed logging of decision pathways \cite{10445154}, and context-aware explanations \cite{rauba2024contextaware} can illuminate where and why errors occurred. Additionally, embedding clear disclaimers about GenFMs' limitations and including accountability frameworks in system design can help delineate the boundaries of responsibility between human operators and AI systems \cite{ryan2023whatsrolemodellingresponsibility, gao2021accountability, brahman2024the}. For example, error-aware interfaces can visually represent AI decision pathways, flagging potential issues in model logic or data inputs. By offering structured and intuitive explanations, these interfaces encourage critical engagement and guide users toward resolution \cite{Cabrera_2021, glassman2024airesilientinterfaces}. By creating transparent and actionable mechanisms for error attribution, systems can foster a culture of shared responsibility. This not only encourages users to remain critically engaged but also builds trust in AI by ensuring errors are addressed in a systematic and accountable manner. Ultimately, such approaches promote robust and ethical human-AI collaboration, even in complex or high-stakes scenarios.

\subsection{The Role of Natural Noise in Shaping Model Robustness and Security Risks}
Robustness serves as a critical metric for evaluating GenFMs, specifically quantifying their response consistency under natural perturbations. Our experimental analysis systematically evaluated diverse GenFM architectures across close-ended and open-ended questions. The subsequent discussion examines critical considerations for enhancing model robustness.

\textbf{Balancing robustness training and overfitting risks.}
In our robustness evaluation, we observed that noise perturbations had both positive and negative effects on model performance, with negative impacts being more prevalent. Interestingly, in some cases, adding noise led to performance improvements, which aligns with previous research~\cite{li2020closer} suggesting potential overfitting in adversarial training of large language models. Although adversarial training generally enhances model stability under perturbations, excessive adversarial optimization may lead to critical vulnerabilities, such as reduced generalization capability to novel or slightly varied attack patterns, increased susceptibility to adaptive attacks exploiting overfitted defense mechanisms, and potential degradation of the model’s primary task performance.  These findings highlight the dual nature of noise in adversarial training and underscore the need for balanced strategies that leverage its benefits while mitigating associated risks.

\textbf{Differential robustness requirements across diverse prompt types.}~In our experiments, we observed significant differences in robustness across various prompt types (i.e., close-ended and open-ended queries), with most models performing better on close-ended queries.
For close-ended queries, which typically have clear and deterministic answers, consistency is crucial. Errors in close-ended queries, especially those involving principled or safety-critical decisions, can lead to severe consequences. For instance, in autonomous driving, misinterpreting sensor data could result in incorrect decisions, such as failing to identify an obstacle or traffic sign. In the field of medical health, consistency and high accuracy in responses are essential, even when noise is present. Therefore, ensuring high robustness in close-ended queries is fundamental to model reliability, as these queries are often tied to high-stakes scenarios where mistakes can have serious implications.
In contrast, open-ended queries are inherently more variable due to their subjective nature and dependence on factors such as the temperature setting in model generation. This variability in responses makes it challenging to maintain consistency under noisy conditions. However, open-ended queries often tolerate a degree of variability, and the focus should be on improving coherence and relevance rather than strict consistency.



% \tian{Add if I have time: Safety concerns in embodied AI can have significant implications across various applications. ...}



\subsection{Confronting Advanced AI Risks: A New Paradigm for Governing GenFMs}


\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{image/advanced_risk_discussion.pdf}
    \caption{Discussion on Advanced AI Risks about GenFMs.}
    \label{fig:advanced_risk_discussion}
\end{figure}

The rapid evolution of GenFMs necessitates a redefinition of how we conceptualize trustworthiness in AI. Recent research has shown that as GenFMs grow in scale, they may exhibit unexpected and potentially harmful behaviors \cite{mckenzie2023inverse}. Traditionally, AI risks have been viewed as unintended consequences—such as issues of bias, fairness, hallucination \cite{huang2023survey}, and system failures—that can often be mitigated through improved training data, algorithmic design, and governance frameworks. However, the increasing complexity, autonomy, and capabilities of GenFMs have introduced a new category of challenges, referred to as \textbf{Advanced AI Risks} (discussed in \textbf{\S\ref{app:advanced_ai_risk}}). These risks differ fundamentally from conventional concerns due to their proactive, emergent, and self-perpetuating nature, necessitating a shift from \textit{reactive mitigation} to \textit{proactive governance and preparedness}. This shift is also emphasized in the recent paper by Simmons-Edler et al. \cite{simmons2024ai}, which discusses the geopolitical instability and threats to AI research posed by AI-powered autonomous weapons, highlighting the need for proactive measures to address the near-future risks associated with full or near-full autonomy in the military technology.




Advanced AI Risks emphasize challenges arising from intent-like behaviors—not in the literal sense of agency, but in the model's ability to simulate, emulate, or appear to exhibit intent. This blurring of lines between tools and entities introduces several critical threats:

\textbf{Self-Replication and Autonomy.} GenFMs capable of self-replication pose unprecedented risks. Autonomous systems that replicate using raw materials, as discussed in studies on self-replicating machines \cite{self_replicating_machine, stenzel2024self}, can magnify threats, particularly when tied to models with cyberattack or bioengineering capabilities. The Group of Seven (G7) recently highlighted the dangers of self-replicating AI in its voluntary code of conduct for AI governance \cite{hiroshima_guiding_principles_2023}. Catastrophic scenarios, such as malicious misuse of autonomous models for creating enhanced pathogens or executing sophisticated cyberattacks, underline the urgency of addressing this risk \cite{lee2024prompt, tang2024prioritizing}. Shlegeris et al. also point out one of the consequences brought by this risk--the \textit{collusion} between untrusted models \cite{prevent_collusion_untrusted_models}.

\textbf{Persuasion and Manipulation.} Studies have extensively examined GenFMs' capacity for influencing and manipulating users \cite{ramani2024persuasion, rogiers2024persuasion, matz2024potential, singh2024measuring}. While positive applications exist, such as promoting prosocial behaviors like vaccination or voting, the darker implications cannot be ignored. At an individual level, models have been shown to manipulate emotions, fostering user dependence \cite{salvi2024ai, salvi2024conversational}. At a societal level, persuasive capabilities can undermine democratic integrity, as Matz et al. describe—e.g., tailoring political messaging to match users' psychological profiles could unduly shift public opinion \cite{matz2024potential}, aligning with concerns raised by Summerfield et al. on the erosion of democratic values \cite{summerfield2024will}. 

\textbf{Emergent Risks from Anthropomorphism.} Anthropomorphized AI systems, which project human-like traits, represent both opportunities and risks. On one hand, anthropomorphic models can enhance trust, accessibility, and engagement by making AI more relatable and intuitive \cite{deshpande2023anthropomorphization, chen2024persona}. On the other hand, they inflate perceptions of AI's capabilities, leading to misplaced trust and unrealistic expectations. Moreover, assigning human-like agency to AI systems obscures accountability, shifting responsibility away from developers and operators \cite{placani2024anthropomorphism, deshpande2023anthropomorphization}.

To address these risks effectively, a potential comprehensive, multifaceted approach is required: 1) \textit{Clarify the Ambiguities of GenFMs.} Defining the agency and intentionality of GenFMs through cognitive or theory-of-mind frameworks \cite{segerie2024ai} is essential. For instance, clarifying key concepts like ``agency AI'' will enable a better understanding of their decision-making processes and operational boundaries. 
2) \textit{Prioritize Human-Centered Governance.} As emphasized in \textit{Guideline 3} of \textbf{\S\ref{sec:guideline}}, human oversight must remain central to AI governance frameworks. Ensuring that humans retain ultimate control over AI decisions, particularly in high-stakes scenarios, is critical. Mechanisms must be in place to prevent GenFMs from making independent, high-risk decisions without explicit human authorization. 
3) \textit{Recognize the Systemic Nature of Advanced AI Risks.} Unlike traditional risks, advanced AI threats extend beyond individual systems or organizations, affecting global networks and ecosystems. Effective mitigation demands collaborative efforts among governments, industries, and international bodies to establish unified standards, share critical knowledge, and deploy robust safeguards. Anthropic exemplifies this systemic approach with its \textbf{AI Safety Levels (ASL)} framework \cite{Anthropic-ASL}, the industry's first proposal of AI safety levels. ASL adapts biosafety level (BSL) standards to categorize AI models based on their potential for catastrophic risks, focusing on CBRN weapon development and automated AI research, while closely monitoring cyber-attack risks. It emphasizes that models must implement safety, security, and operational measures aligned with their risk level, with higher ASL tiers demanding stricter safety demonstrations.
% By implementing such a framework, ASL promotes cross-organizational collaboration and establishes unified standards to effectively manage and reduce systemic risks during the development of more powerful models. 
4) \textit{Continuously Redefine Trustworthiness.} As GenFMs evolve, so must the criteria for evaluating their trustworthiness. This includes adapting to new capabilities and risks (\emph{e.g.}, the dynamic requirements discussed in \textbf{\S\ref{sec:discuss_dynamic}}), implementing ongoing monitoring systems to detect vulnerabilities, and committing to proactive measures that address gaps in governance and oversight. 
