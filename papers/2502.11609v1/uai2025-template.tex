\documentclass[accepted]{uai2025} % for initial submission
%\documentclass[accepted]{uai2025} % after acceptance, for a revised version; 
% also before submission to see how the non-anonymous paper would look like 
                        
%% There is a class option to choose the math font
% \documentclass[mathfont=ptmx]{uai2025} % ptmx math instead of Computer
                                         % Modern (has noticeable issues)
% \documentclass[mathfont=newtx]{uai2025} % newtx fonts (improves upon
                                          % ptmx; less tested, no support)
% NOTE: Only keep *one* line above as appropriate, as it will be replaced
%       automatically for papers to be published. Do not make any other
%       change above this note for an accepted version.

%% Choose your variant of English; be consistent
\usepackage[american]{babel}
% \usepackage[british]{babel}
\input{math_commands.tex}

%% Some suggested packages, as needed:
\usepackage{hyperref}
\usepackage{url}
 \usepackage{graphicx}
\usepackage{multirow} 
\usepackage{rotating}
\usepackage{adjustbox}
\usepackage{wrapfig}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}


\usepackage[inkscapelatex=false]{svg}
\usepackage[ruled,boxed]{algorithm2e}
% \usepackage{algorithm, algorithmic}
\makeatletter 
\usepackage{algpseudocode}
\g@addto@macro{\@algocf@init}{\SetKwInOut{Parameter}{Parameter}} 
% \title{Generate Models with a Single Forward:\\ Transferability Task Embedding Guided Hypernet for Continual Learning}
\makeatother
\usepackage{bbding}
\usepackage{booktabs}
%% Provided macros
% \smaller: Because the class footnote size is essentially LaTeX's \small,
%           redefining \footnotesize, we provide the original \footnotesize
%           using this macro.
%           (Use only sparingly, e.g., in drawings, as it is quite small.)

%% Self-defined macros
\newcommand{\swap}[3][-]{#3#1#2} % just an example

\title{Exploiting Task Relationships for Continual Learning\\
Using Transferability-Aware Task Embeddings}

% The standard author block has changed for UAI 2025 to provide
% more space for long author lists and allow for complex affiliations
%
% All author information is authomatically removed by the class for the
% anonymous submission version of your paper, so you can already add your
% information below.
%
% Add authors
\author[1]{\href{mailto:<wu-yr21@mails.tsinghua.edu.cn>?Subject=Exploiting Task Relationships for Continual Learning Using Transferability-Aware Task Embeddings}{Yanru~Wu}{}}
\author[1]{Xiangyu~Chen}
\author[2]{Jianning~Wang}
\author[1]{Enming~Zhang}
\author[1]{Hanbing~Liu}
\author[1]{\href{mailto:<yangli@sz.tsinghua.edu.cn>?Subject=Exploiting Task Relationships for Continual Learning Using Transferability-Aware Task Embeddings}{Yang~Li\footnote{Corresponding author.}}{}}
% Add affiliations after the authors
\affil[*]{
    Corresponding author
}
\affil[1]{%
    Shenzhen Key Laboratory of Ubiquitous Data Enabling\\
    Shenzhen International Graduate School\\
    Tsinghua University
    % Shenzhen, China\footnote{Yanru Wu, Xiangyu Chen, Enming Zhang, Hanbing Liu and Yang Li are from the Shenzhen Key Laboratory of Ubiquitous Data Enabling, SIGS, Tsinghua.}
}
\affil[2]{%
    Independent Researcher
}

% \affil[3]{%
%     Another Affiliation\\
%     Address\\
%     …
%   }
  
  \begin{document}
\maketitle

\begin{abstract}
Continual learning (CL) has been an essential topic in the contemporary application of deep neural networks, 
% where the model is expected to display the property of both backward and forward transfer in task acquisition.
where catastrophic forgetting (CF) can impede a model's ability to acquire knowledge progressively.
% , leading to critical training inefficiency and constraint in the improvement of model's overall capacity. 
% With the backward transfer more often referred to as catastrophic forgetting (CF), 
 Existing CL strategies primarily address CF by regularizing model updates or separating task-specific and shared components. However, these methods focus on task model elements while overlooking the potential of leveraging inter-task relationships for learning enhancement. To address this, we propose a transferability-aware task embedding named H-embedding and train a hypernet under its guidance to learn task-conditioned model weights for CL tasks. Particularly, H-embedding is introduced based on an information theoretical transferability measure and is designed to be online and easy to compute. The framework is also characterized by notable practicality, which only requires storing a low-dimensional task embedding for each task, and can be efficiently trained in an end-to-end way. Extensive evaluations and experimental analyses on datasets including Permuted MNIST, Cifar10/100, and ImageNet-R demonstrate that our framework performs prominently compared to various baseline methods, displaying great potential in exploiting intrinsic task relationships.
 
\end{abstract}

\section{Introduction}
\label{sec:intro}
\input{contents/intro}

\section{Related Works}
\label{sec:relwork}
\input{contents/rel}

\section{Preliminary}
\input{contents/preliminary}

\section{Methodology}
\input{contents/method}

\section{Experiments}
\input{contents/experiment}

\section{Conclusion}

In this work, we propose a transferability-aware task embedding guided hypernet to exploit the task relationships for continual learning. By introducing the information theoretical transferability based task embedding named H-embedding and incorporating it in a hypernetwork, we establish an online framework capable of capturing the statistical relations among the CL tasks and leveraging this knowledge for guiding task-conditioned model weight generation. Through extensive experimental studies, we validate that the adoption of H-embedding guidance enhances continual learning by facilitating inter-task transfer and improving the reliability of task embeddings, achieving the best final accuracy performance under various CL benchmarks.


% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \section*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

% \section*{Reproducibility Statement}

% Experiments in our paper are run with fixed random seeds and are completely reproducible (with detailed information in Appendix.~\ref{sec:Aset}). We also open-source the code implementation of our method at \url{https://anonymous.4open.science/r/H-embedding_guided_hypernet/} for better reproducibility and facilitating future researches.



% \begin{contributions} % will be removed in pdf for initial submission 
% 					  % (without ‘accepted’ option in \documentclass)
%                       % so you can already fill it to test with the
%                       % ‘accepted’ class option
%     Briefly list author contributions. 
%     This is a nice way of making clear who did what and to give proper credit.
%     This section is optional.

%     H.~Q.~Bovik conceived the idea and wrote the paper.
%     Coauthor One created the code.
%     Coauthor Two created the figures.
% \end{contributions}

% \begin{acknowledgements} % will be removed in pdf for initial submission,
% 						 % (without ‘accepted’ option in \documentclass)
%                          % so you can already fill it to test with the
%                          % ‘accepted’ class option
%     Briefly acknowledge people and organizations here.

%     \emph{All} acknowledgements go in this section.
% \end{acknowledgements}

% References
\bibliographystyle{plainnat}
\bibliography{uai2025-template}

\newpage

\onecolumn

\title{Supplementary Material}
\maketitle


\appendix
\section{Appendix}
\input{contents/appendix}

\end{document}
