
\begin{table*}[!h]
%  \setlength{\tabcolsep}{10pt} % Default value: 6pt
% \renewcommand{\arraystretch}{1.5} % Default value: 1
    \renewcommand{\arraystretch}{1.2}
    \centering
    \scalebox{1}{
    \begin{tabular}{c|c c c|c c c |c c c }
          \toprule
          \multirow{2}*{\textbf{Setting}} & \multicolumn{3}{|c|}{\textbf{PermutedMNIST}} & \multicolumn{3}{|c|}{\textbf{ Cifar10/100}} & \multicolumn{3}{|c}{\textbf{ImageNet-R}} \\
          \hspace{1pt}
          ~ & \multicolumn{3}{|c|}{\textit{MLP}} & \multicolumn{3}{|c|}{\textit{CNN}} & \multicolumn{3}{|c}{\textit{ResNet-32}} \\
          \midrule
           \textbf{ Method} &$\mathcal{AA}$  & $\mathcal{BWT}$  &  $\mathcal{FWT}$  &$\mathcal{AA}$  & $\mathcal{BWT}$  &  $\mathcal{FWT}$ &$\mathcal{AA}$  & $\mathcal{BWT}$  &  $\mathcal{FWT}$ \\
          \midrule
          Vanilla Hnet
          & 97.495  &  \textbf{0.007}  & 0.063  & 69.679  &  -7.790   &  7.970  & 38.202  &  -0.105  &  6.817 \\
          Rand-embed Hnet &  97.448  &  0.001  &  0.021 &  71.179  &  -6.140  &  7.970  &  38.046  & \textbf{0.090}  &  6.448 \\
          % H-score W. &  0.9578  &  0.9719  &  0.6720\\
          H-embed Hnet*
          &  \textbf{97.553}  &  -0.007  &  \textbf{0.133}  &  \textbf{72.290}  &  \textbf{-5.328}  & \textbf{ 8.380}  &  \textbf{39.212}  & -0.087  &  \textbf{7.863} \\
          % H-ensemble* & \textbf{0.8644} & \underline{0.9708} & \textbf{0.6940}\\
          \bottomrule
    \end{tabular}}
    \caption{\textbf{Ablation Study on different benchmarks and backbones.} Our H-embedding guidance proves to be effective across all three settings, attaining the highest average accuracy, with competitive backward transfer and the best forward transfer performance.
    }
    \label{tab:ablation}
\end{table*}

\subsection{Experimental Settings}
\subsubsection{Benchmarks}

To comprehensively verify the effectiveness of our framework and further analyze its reliability, we select three representative benchmarks from previous work on CL and perform extensive experiments on them: \textbf{PermutedMNIST} (10 tasks) \citep{goodfellow2013empirical}, \textbf{Cifar10/100} (11 tasks) \citep{krizhevsky2009learning}, and \textbf{ImageNet-R} (10 tasks) \citep{hendrycks2021many}\footnote{Training specifics and detailed results of our experimental studies are listed in Appendix~\ref{sec:Aset}, with codes available at \url{https://anonymous.4open.science/r/H-embedding_guided_hypernet/}.}. A detailed description of these benchmarks is listed in Appendix~\ref{sec:benchmark}.

\subsubsection{Evaluation Metrics}
Following our desiderata stated in Sec.~\ref{sec:intro} and previous works \citep{qu2021recent,wang2024comprehensive}, we evaluate the different CL methods from three aspects: 
\begin{itemize}
    \item \textbf{Overall performance}, measured by average accuracy (AA) of the final model on all CL tasks:
    \begin{center}
        $\mathcal{AA} = \frac{1}{M} \sum_{j=1}^M a_{j,M}$ ;
    \end{center}
    % \vspace{-5pt}
    % \begin{align*}
    %     \mathcal{AA} = \frac{1}{M} \sum_{j=1}^M a_{j,M}.
    % \end{align*}
    \item \textbf{Memory degradation of old tasks}, measured by average backward transfer (BWT): 
    \begin{center}
        $\mathcal{BWT} = \frac{1}{M-1} \sum_{j=1}^{M-1} (a_{j,M} - a_{j,j})$ ;
    \end{center}
    % \begin{align*}
    %     \mathcal{BWT} = \frac{1}{M-1} \sum_{j=1}^{M-1} (a_{j,M} - a_{j,j}).
    % \end{align*}
    \item \textbf{Learning enhancement of new tasks}, measured by average forward transfer (FWT): 
    \begin{center}
        $\mathcal{FWT} = \frac{1}{M-1} \sum_{j=2}^{M} (a_{j,j} - \Tilde{a}_{j})$.        
    \end{center}
    % \begin{align*}
    %     \mathcal{FWT} = \frac{1}{M-1} \sum_{j=2}^{M} (a_{j,j} - \Tilde{a}_{j}).
    % \end{align*}    
\end{itemize}
Here, $a_{i,j}$ denotes the accuracy (\%) measured on the test set of $i$-th task after learning the $j$-th task, and $\Tilde{a}_{j}$ denotes the test accuracy derived by training a randomly initialized model directly on the $j$-th task. To conclude, a most desirable CL strategy should come with higher results on all three metrics, i.e. $\mathcal{AA}$, $\mathcal{BWT}$, and $\mathcal{FWT}$.



\subsection{Performance Evaluation}

\subsubsection{Comparison Experiments}

Our primary evaluation study is conducted on the Cifar10/100 and ImageNet-R benchmarks with a total of 11 and 10 tasks respectively. To ensure fairness in comparison, a non-pre-trained ResNet-32 \citep{he2016deep} is selected as the backbone model for all the chosen baselines. We reproduce all the methods with our own codes and each method is run three times with shared random seeds.

The choice of baselines is based on the requirements that they should both conform to our rehearsal-free setting and be applicable to the benchmark and backbone. For a thorough comparison with existing methods to the greatest extent possible, we select representative baselines of varied methodology categories, including: \textbf{Basic Methods:} Finetune, Finetune Head, Multi Task; \textbf{Regularization Methods:} LwF \citep{li2017learning}, EWC \citep{kirkpatrick2017overcoming}, L2, PredKD+FeatKD \citep{smith2023closer};
%, PredKD +EWC, PredKD+L2
\textbf{Architecture Methods:} PackNet \citep{mallya2018packnet}, HyperNet \citep{von2020continual}, WSN \citep{kang2022forget}. We summarize the experimental results in Table.~\ref{tab:cifar}. As can be seen from the table, our method performs prominently in the ultimate acquisition of CL tasks, achieving the highest final average accuracy. It also derives the best overall ability, displaying competitive performance in both forward and backward transfer.  


\subsubsection{Ablation Studies}

To broaden the comprehensiveness of evaluation and take a better concentration on validating our introduction of H-embedding guidance, we conduct extra ablation studies on three differed settings with different benchmarks as well as model backbones. Namely, experimental settings include: PermutedMNIST (10 tasks) using an MLP model, Cifar10/100 (11 tasks) using a 4-layer CNN model, and ImageNet-R (10 tasks) using a ResNet-32 model. We compared across three methods where all hyperparameters are set exactly to the same: 1) Vanilla Hnet, the hypernet CL framework without guidance module; 2) Rand-embed Hnet, the same framework as ours but replacing H-embedding with a random embedding; 3) H-embed Hnet, our framework. The performance is evaluated and summarized in Table.~\ref{tab:ablation}, where a broad increase in CL performance could be observed across all benchmarks and backbones.

\subsection{Discussion and In-depth Performance Analysis}

For a better analysis of the effectiveness of our strategy, we further investigate the detailed training behavior displayed in CL strategies, showing that our H-embedding guided hypernet is characterized by following superiority.

\begin{figure*}[htb]
    \centering
    % \centerline{\includesvg[width=1.2\columnwidth]{figs/framework.svg}}
    \centerline{\includegraphics[width=2\columnwidth]{figs/task_visual.pdf}}
    \caption{\textbf{The during and final task test accuracy of Cifar10/100 (ResNet-32 backbone, 100 epochs),} with axis x for CL task IDs and axis y for the test accuracy. In the figures, $\triangleright$ and $\triangleleft$ denote the during and final accuracy, while the dashed line shows the average final accuracy and the colored region represents their discrepancy, \textit{i.e.},  AA and BWT. From left to right is the accuracy visualization for H-embedding guided hypernet (ours), vanilla hypernet, WSN and L2 respectively. The grey regions in the right three figures denote the margin of during accuracy between these baselines and our method, \textit{i.e.}, the discrepancy of FWT.}
    \label{fig:task}
\end{figure*}

\begin{figure*}[!htb]
    \centering
    % \centerline{\includesvg[width=1.2\columnwidth]{figs/framework.svg}}
    \centerline{\includegraphics[width=2\columnwidth]{figs/acc_h+v.pdf}}
    \caption{\textbf{Plotting of test accuracy during training task 1, 4, 7, 11 of Cifar10/100,} with axis x and y for the number of checkpoints and accuracy respectively. The blue curve represents the vanilla hypernet and the orange represents our H-embedding guided hypernet. As CL progresses, our method exhibits quicker convergence to higher accuracy in later tasks.}
    \label{fig:acc}
\end{figure*}


\paragraph{Optimal Overall Transfer Ability} We select some of the best-performing baselines and plot their task-specific performance in Fig.~\ref{fig:task}. Each task is presented with two test accuracies: the accuracy obtained upon finishing training on the task, and the accuracy achieved by the final model after learning all CL tasks. As illustrated in the figures, our H-embedding guided hypernet demonstrates a notable advantage over Vanilla Hnet and WSN, exhibiting both effectiveness and stability in forward transfer while performing comparably in backward transfer. On the other hand, L2 as a regularization baseline, achieves good forward transfer ability, but fails in the mitigation of catastrophic forgetting. On the whole, our method displays a steady boost in forward transfer while retaining a competitive backward transfer, showcasing the best overall transfer ability, thereby attaining the highest average performance.



\paragraph{Quicker Convergence} With the intention of understanding how our guidance aids the training process, we visualize the test accuracy trends during the training stage of tasks 1, 4, 7, 11 of the 11 CL tasks under Cifar-ResNet setting in Fig.~\ref{fig:acc}. It is shown in the figures that, compared to a hypernet without H-embedding guidance, our method converges noticeably faster and achieves a higher final accuracy performance, especially with the growth of task numbers. Such a phenomenon serves as a further suggestion that our H-embedding guidance provides a substantial enhancement to the task learning in CL through forward transfer.


% \paragraph{Robustness with Longer Task Sequences}

\paragraph{Embedding Interpretability}


% \newlength{\myintextsep}
% \setlength{\myintextsep}{\intextsep}
% \setlength{\intextsep}{0pt}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{figs/dist.pdf}
    \caption{\textbf{Visualization of discrepancy between the task embedding distances learned w/ and w/o H-embedding guidance.} The grid of $i$-th row and $j$-th column represents the distance of task $i$ and $j$. Darker cells indicate a larger discrepancy, with red for d(w/) < d(w/o) and blue vice versa.}
\label{fig:dist}
\end{figure}


% \setlength{\intextsep}{\myintextsep}



To assess the task embeddings $\{e^{(j)}\}_{j=1}^{M}$ learned in our framework, we compute the task-wise Euclidean distances of the embeddings obtained with and without H-embedding guidance, and visualize the discrepancy between these two distance matrix in Fig.~\ref{fig:dist}.  
Red signifies that the with-guidance embeddings result in a closer distance between the two tasks compared to the without-guidance embeddings, while blue represents the opposite.
Take task 9, a Cifar100 split task covering classes of people and reptiles, as an instance. The embedding derived in our H-embedding guided hypernet successfully marks tasks 4, 6, 7, 8, 10 as more related, which all contain coverage of terrestrial animal classes or human scenarios. Our embedding also generally displays a greater preference for task 1, the more comprehensive Cifar10 task. Such correspondence with human intuition suggests a better capture of task interrelationships, leading to higher CL efficiency. 