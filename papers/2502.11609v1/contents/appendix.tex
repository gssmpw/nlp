

\subsection{Continual Learning Setting}
Based on the discrepancy between $D_{j-1}$ and $D_{j}$, \cite{hsu2018re} and \cite{van2019three} categorize CL settings into three specific scenarios: task incremental, class incremental, and domain incremental. Table~\ref{tab:setting} summarizes the differences among these scenarios. For a better concentration on the study of CL methodology, our work mainly focuses on the task incremental CL. In this scenario, the output spaces of tasks are partitioned by task IDs and mutually exclusive between $D_{j-1}$ and $D_{j}$, which is denoted as $\mathbf{Y}^{(j-1)}\neq\mathbf{Y}^{(j)}$. It can be then naturally indicated that $P(\mathbf{Y}^{(j-1)})\neq P(\mathbf{Y}^{(j)})$ and $P(\mathbf{X}^{(j-1)})\neq P(\mathbf{X}^{(j)})$. Notably, here task IDs are accessible during both training and testing. An adaptation of our method to other CL settings could be effectively conducted by introducing additional task inference modules similar to previous works.

\begin{table*}[htb]
\setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.2}
    \centering
    \scalebox{0.95}{
    \begin{tabular}{c| c c c| c}
        \toprule
         \textbf{Scenario} &  $P(\mathbf{X}^{(j-1)})\neq P(\mathbf{X}^{(j)})$ & $P(\mathbf{Y}^{(j-1)})\neq P(\mathbf{Y}^{(j)})$ & $\mathbf{Y}^{(j-1)}\neq\mathbf{Y}^{(j)}$ & \textbf{\ Task ID} \\
         \midrule
         Domain Incremental &  \Checkmark & \XSolidBrush & \XSolidBrush & \XSolidBrush \\
         Class Incremental &  \Checkmark &  \Checkmark & \XSolidBrush & \XSolidBrush \\
         Task Incremental* &  \Checkmark &  \Checkmark &  \Checkmark & \Checkmark \\
         \bottomrule
    \end{tabular}}
    % \caption{\textbf{Comparison across Relative Problem Settings,} involving the aspects of whether there are multi-sources, no source data, no source model details, and different target learning approaches. The abbreviation `DA' represents domain adaptation, `\Checkmark' represents obtaining the corresponding aspects, while `\XSolidBrush' the opposite. Among all settings, ours is relatively restrictive.}
    \caption{\textbf{Categorization of CL settings based on the discrepancy between $D_{j-1}$ and $D_{j}$.}`*’ denotes the scenario focused on in our work.}
    \label{tab:setting}
\end{table*}

\subsection{HGR maximal correlation for self transferability $H(T_j,T_j)$}
\label{sec:HGR}
As the theoretical basis of H-score transferability, the Hirschfeld–Gebelein–Rényi (HGR) maximal correlation of random variables $X$ and $Y$ over alphabets $\mathcal{X}$ and $\mathcal{Y}$ is defined as:
\begin{align}
    \mathcal{HGR}(X,Y) = \max_{\substack{
f: \mathcal{X} \to \mathbb{R}^k,\ g: \mathcal{Y} \to \mathbb{R}^k \\
\mathbb{E}[f(X)] =  \mathbb{E}[g(Y)] = \mathbf{0} \\
\mathbb{E}[f(X)f^\top(X)] = \mathbb{E}[g(Y)g^\top(Y)] = \mathbf{I}
}} 
\mathbb{E}_{P_{XY}}\left[f^\top(X) g(Y)\right].
\end{align}
Here, the correlation is derived by taking the maximum over all functions $f$, $g$ with zero mean and unit variance, which hence extracts the most correlated aspects of $X$ and $Y$. This definition is equivalent to the maximum of the two-sided H-score given by \cite{huang2019information} with normalized functions $f$ and $g$:
\begin{align}
    H(f,g) = \mathbb{E}_{P_{XY}}\left[f^\top(X)g(Y)\right] - \frac{1}{2}tr(cov(f(X)) cov(g(Y))),
\label{hscore}
\end{align}
with the one-sided H-score extended from Eqn.~\ref{hscore} by assuming the function $g$ as optimal:
\begin{align}
    H(f) = tr(cov(f(X))^{-1}cov(\mathbb{E}_{P_{X|Y}}[f(X)|Y])).
\label{singlehscore}
\end{align}
Notably, Eqn.~\ref{singlehscore} is exactly the definition of H-score transferability \citep{bao2019information}, which is actually proposed by applying the H-score theories to transfer learning scenarios. Therefore, with normalized $f$, $g$, the HGR maximal correlation between $X_j$, $Y_j$ is mathematically equivalent to the H-score metric of the theoretical optimal model $f^*_j$ on task $j$. 


In our work, we utilize the H-score transferability metrics denoted as \( H(T_i, T_j) \) for tasks \( i \neq j \). However, the H-score for task \( j \) cannot be computed while the model remains untrained for that specific task. To maintain consistency in our definitions, we define \( H(T_j, T_j) \) as the HGR maximal correlation between the input \( X_j \) and the output \( Y_j \), implicitly leveraging the theoretical optimal model \( f^*_j \) for task \( j \). Specifically, in our computations, the models \( f \) and \( g \) are implemented as fully connected neural networks equipped with normalization layers, and they undergo 100 epochs of training using a sampled subset of the training data through gradient descent.

\subsection{Algorithm for H-embedding Guided Hypernet Framework}
\label{sec:algorithm}

For a better understanding of our framework, we summarize the entire training process of task $j$ within our H-embedding guided hypernet as  outlined in Algorithm.~\ref{ago:train}

\begin{algorithm*}[!h]
\KwIn{Task data $D_j$, previous task embeddings $\{e^{(n)}\}_{n=1}^{j-1}$, hypernet weights $\Theta_h$}
% \Require{$E_{P^T_X}[\boldsymbol{f}_{T}(x)] = 0$, $E_{P^T_X}[\boldsymbol{f}_{T}(x)\cdot \boldsymbol{f}_{T}(x)^T] = I$}
\Parameter{Learning rate $\lambda$}
\KwOut{Current task embedding $e^{(j)}$, updated hypernet weights $\Theta_h$}
 % \KwResult{how to write algorithm with \LaTeX2e }
% \Begin{
    Randomly initialize $e^{(j)}$, $\hat{e}^{(j)}$\; 
    \If { $j>2$ }{
        \For(\tcp*[f]{Compute transferability}){$n \gets 1$ \KwTo $j-1$}{
         $\Theta^{(n)} \gets f_h(e^{(n)}, \Theta_h)$ \;
          $H(T_n, T_j) \gets tr\left(cov(f_l(x^{(j)}, \Theta^{(n)}))^{-1}cov(\mathbb{E}_{P_{X|Y}}[f_l(x^{(j)}, \Theta^{(n)})|y^{(j)}])\right)$ \Comment{Eq.~\ref{eqn:hscore}}
          }
    Randomly initialize $\gamma^{(j)}$\;
    $\hat{e}^{(j)}, \gamma^{(j)} \gets \argmin_{\hat{e}^{(j)}, \gamma^{(j)}} \sum_{n=1}^{j-1} \left(||\hat{e}^{(j)} - e^{(n)}||_2 - \gamma^{(j)}\exp(-\mathcal{AHP}(T_n, T_j))\right)^2$ \Comment{Eq.~\ref{eqn:get_emb}} \\
    }
    % \Repeat(\tcp*[f]{Compute H-embedding}){convergence}{
    %         $\hat{e}^{(j)} \gets \hat{e}^{(j)} + \lambda \nabla_{\hat{e}^{(j)}}\sum_{n=1}^{j-1} \left(||\hat{e}^{(j)} - e^{(n)}||_2 - \gamma^{(j)} H(T_n, T_j)\right)^2$
    %         $\gamma^{(j)} \gets \gamma^{(j)} + \lambda \nabla_{\gamma^{(j)}}\sum_{n=1}^{j-1} \left(||\hat{e}^{(j)} - e^{(n)}||_2 - \gamma^{(j)} H(T_n, T_j)\right)^2$
    %       }
    \Repeat(\tcp*[f]{Train hypernet}){converge}{
            $e^{(j)} \gets e^{(j)} - \lambda \nabla_{e^{(j)}} L $\;
            $\Theta_h \gets \Theta_h - \lambda \nabla_{\Theta_h} L$ \Comment{Eq.~\ref{eqn:loss}}
        }
    \textbf{Return} $e^{(j)}$, $f_h(\ \cdot\ , \Theta_h)$
\caption{H-embedding guided Hypernet: Training of Task $j$}
\label{ago:train}
\end{algorithm*}



\subsection{Benchmarks of Experiments}
\label{sec:benchmark}
\textbf{PermutedMNIST} \citep{goodfellow2013empirical} benchmark is a variant of MNIST \citep{lecun1998gradient}, forming CL tasks from the original MNIST dataset by applying random permutations to the input image pixels.
The permuting procedure can be repeated in experiments to yield a task sequence of desired length, with each task consisting of 70,000 images (60,000 for training and 10,000 for testing) of digits from 0 to 9.
\textbf{Cifar10/100} is a benchmark composed of 11 ten-way classification tasks, with a full Cifar10 task and a Cifar100 dataset split into ten tasks \citep{krizhevsky2009learning}. The model is firstly trained on the Cifar10 task with 60,000 images (50,000 for training and 10,000 for testing) and then sequentially trained on the ten Cifar100 tasks, each with 6,000 images (5,000 for training and 1,000 for testing). \textbf{ImageNet-R} \citep{hendrycks2021many}, built upon the ImageNet dataset \citep{deng2009imagenet}, features a diverse range of renditions of ImageNet classes. This benchmark includes a total of 30,000 images across 200 classes from ImageNet. For continual learning evaluation, ImageNet-R is organized into 10 tasks, each containing 20 classes and around 3,000 samples (roughly 2,500 for training and 500 for testing).

\subsection{Experimental settings}
\label{sec:Aset}


\subsubsection{Comparison Experiments}

\paragraph{Choice of Baselines.} Our selection of baselines in this work aims to encompass a wide range of baseline categories, covering two of three primary categories in contemporary CL researches (\textit{i.e.}, replay-based, regularization-based, and architecture-based), with the replay-based methods not conforming to our rehearsal-free setting. The specific choice of baselines in each category is mainly based on performance comparison conclusions in recent works such as \cite{smith2023closer} and \cite{kang2022forget}. Therefore, we believe that our comparison study has included the most competitive and representative baselines.

\paragraph{General Settings.} In CIFAR10/100 and ImageNet-R datasets using a ResNet-32 backbone network without pre-training, we evaluate several baseline methods including Finetune, Finetune-Head, EWC, L2, PredKD+FeatKD, PackNet, HyperNet, WSN. 
The ResNet-32 uses `option A', \textit{i.e.}, leveraging the zero-padding shortcuts for increasing dimensions, as indicated in CIFAR-10 experiments of the original ResNet paper Sec4.2. 
% All experiments are conducted using NVIDIA GeForce RTX 3090 GPUs. 
The experiments on CIFAR10/100 are conducted on NVIDIA GeForce RTX 3090 GPUs with 100 epochs of training (unless early-stop), and the ImageNet‑R experiments are carried out on NVIDIA A800 or A100 GPUs with 200 epochs of training (unless early-stop).

To ensure a fair comparison, we adopt consistent training settings across all baseline methods (unless listed separately). Specifically, the batch size it set to 32, and each task is trained for 100 epochs. We use the Adam optimizer with an initial learning rate of 0.001. The learning rate is decayed by a factor of 10 after the 50th and 75th epochs. A weight decay of $1 \times 10^{-4}$ is applied. For robustness, each experiment is run three times with different random seeds 22, 32, and 42, and the results are averaged.

\paragraph{Details of Specific Methods.} 

For the \textbf{Finetune} baseline, the model is sequentially trained on each task without any mechanisms to prevent catastrophic forgetting. The model is randomly initialized and trained from scratch on the first task. The training of subsequent tasks continues using the weights obtained from the previous task.

In the \textbf{Finetune-Head} baseline, all convolutional layers of the ResNet-32 model are frozen after training on the first task. When learning new tasks, only the parameters of the final fully connected layer (the classifier) are updated. This approach aims to retain the feature representations learned from earlier tasks while adapting the classifier to new task-specific outputs.

In the \textbf{Multi Task} baseline, the dataset is trained as a whole (instead of split into 10/11 tasks) using a non-pretrained ResNet-32 until convergence. We then separately test the classification accuracy on each split and take an average to get the AA metric.

For the \textbf{EWC} baseline \citep{kirkpatrick2017overcoming}, we add a regularization term to the loss function to penalize significant changes to parameters important for previously learned tasks. The importance of each parameter is estimated using the Fisher Information Matrix. The regularization coefficient $\lambda$ it set to 10, following standard practice.

In the \textbf{L2} baseline, an L2 regularization term are added to the loss function to limit changes in the model parameters during training on new tasks. The regularization coefficient $\lambda$ is set to 1.0, determined by tuning on a small validation set derived from the training data of the first task.

For the \textbf{PredKD + FeatKD} method \citep{smith2023closer}, we incorporate both prediction distillation and feature distillation to transfer knowledge from previous tasks to new ones. The distillation loss combines the Kullback-Leibler divergence between the soft outputs of the teacher (model trained on previous tasks) and the student (current model), as well as the mean squared error between their intermediate feature representations. The loss weights are set to $\alpha = 1.0$ and $\beta = 0.5$ based on preliminary tuning.

In the \textbf{PackNet} method \citep{mallya2018packnet}, we employ iterative pruning to allocate dedicated network weights for each task. After training on each task, we prune a certain percentage of the weights with the smallest magnitudes. Following the recommendations in the original paper, we experiment with pruning rates of 0.5, 0.75, and 0.8. We select the pruning rate of 0.8, which yields the best performance in our setting. After pruning, we fine-tune the remaining weights for an additional 10 epochs with a reduced learning rate of $1 \times 10^{-4}$. 

For \textbf{HyperNet} method \citep{von2020continual}, we primarily follow the original work in training settings, with the learning rate being 0.001, the CL loss beta being 0.05, and scheduling and transforming strategies being the same as those used by Oswald. The embedding dimension is set to 32.

For the \textbf{WSN} method \citep{kang2022forget}, we follow its original paper and use the default values of parameters in its official code repository.
We choose the sparsity parameter $c=0.5$ which perform best as listed in the WSN literature. Other parameters are set to the following values: optimization via Adam, a learning rate initialize at 1e-3 with a minimum of 1e-6, and a patience of 6 epochs for reducing the learning rate by a factor of 2. The models are trained for 100 epochs, with a batch size of 64 for both training and testing.

For our \textbf{H-embedding guided hypernet}, the learning rate is set to 0.0005, with the embedding loss beta and CL loss beta both set to 0.05. The scheduling and transforming strategies are set the same as those of \cite{von2020continual} and the embedding dimension is also set to 32. For the learning of H-embedding, we update Eqn.~\ref{eqn:get_emb} using gradient descent for 2000 iterations and the $f$, $g$ in HGR maximal correlation for 100 epochs respectively, both using a subset of 1000 samples from the training set.

In all methods, we adhere to the principles of continual learning by not tuning hyperparameters on the full task set. Special care was taken in handling batch normalization layers, especially in methods involving parameter freezing or pruning. Following the settings in \cite{von2020continual}, we store and update batch normalization statistics separately for each task to ensure proper normalization during both training and inference.


\subsubsection{Ablation Studies}

\paragraph{ImageNet-R} For the ImageNet-R dataset, we split the original 200 classes into ten 20-way classification tasks. Because of the uneven class sample size of ImageNet dataset, each task has varied numbers of training and test samples: Task 1 with 2,166 training samples and 543 test samples, Task 2 with 2,655 training and 716 test samples, $\dots$, until Task 9 with 2,058 training samples and 471 test samples. In our method, we use a learning rate of 0.0005 and a the embedding loss beta of 0.05, training the models for 200 epochs. The backbone model is the same as used in comparison experiments. The results are derived on NVIDIA A100 GPUs.

\paragraph{Cifar10/100} The tasks in this setting are derived the same as in comparison studies. Yet, the backbone model is differently set to a 4-layer CNN as used by \cite{zenke2017continual}. We also follow Oswald in most of the hyperparameters, configuring learning rate to 0.0001, embedding size to 32, as well as using the same scheduling strategies. We train each method with 100 epochs and the embedding loss beta is set to 0.2 for H-embed and rand-embed hypernets. The results are derived on NVIDIA GeForce RTX 3090 GPUs.

\paragraph{PermutedMNIST} Considering the smaller data dimension and model size in this setting, the embedding size is reduced to 24 and training iteration number is set to 5000. The backbone model on PermutedMNIST is selected to be an MLP with fully-connected layers of size 1000, 1000 as used by \cite{van2019three}. We configure the learning rate as 0.0001 and the embedding loss beta as 0.05. The results are derived on NVIDIA GeForce RTX 3090 GPUs.

\subsection{Detailed experimental results}

Considering the limited space, we only present the experimental results measured by our three metrics in main text. Here, we list the whole continual learning performance below. The during accuracy refers to the test accuracy of tasks upon finishing training on that task, and the final accuracy refers to the test accuracy of tasks when finishing learning all CL tasks. The results of comparison experiments are derived with three times running of seed 22, 32, 42 and the ablation studies are conducted a single time only. 

\begin{sidewaystable}[ht]
\centering
% 缩小字体大小
% \small
% 调整行间距，设置为 0.85 倍的标准间距
\renewcommand{\arraystretch}{1.6}
% 调整列之间的间距，减少到原来的 0.85 倍
\setlength{\tabcolsep}{0.9\tabcolsep}
% 调整表格宽度
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Method & Task 1 &  Task 2 & Task 3 & Task 4 & Task 5 & Task 6 & Task 7 & Task 8 & Task 9 & Task 10 & Task 11 \\
\hline
Finetune& 81.14 ± 0.32 & 79.10 ± 0.91 & 76.70 ± 0.85 & 79.97 ± 1.89 & 78.57 ± 2.40 & 79.97 ± 1.41 & 78.20 ± 1.92 & 79.70 ± 1.93 & 74.77 ± 1.46 & 80.43 ± 0.93 & 82.77 ± 2.08  \\
\hline
Finetune Head & 88.65 ± 1.11 & 77.90 ± 3.12 & 76.57 ± 1.99 & 76.03 ± 4.46 & 77.80 ± 2.38 & 78.13 ± 1.03 & 76.80 ± 1.51 & 79.33 ± 2.10 & 75.00 ± 1.04 & 81.20 ± 0.85 & 79.53 ± 0.65 \\
\hline
LwF &89.39 ± 1.64 & 87.17 ± 0.45 & 84.37 ± 0.63 & 85.10 ± 0.99 & 83.83 ± 0.45 & 85.53 ± 0.85 & 82.07 ± 0.86 & 84.17 ± 0.45 & 82.37 ± 0.38 & 84.03 ± 1.70 & 86.83 ± 0.59\\
% \hline
% PredKD +EWC & 88.66 ± 0.72 & 86.50 ± 0.08 & 82.70 ± 0.50 & 86.03 ± 0.22 & 85.80 ± 0.98 & 86.07 ± 0.06 & 83.60 ± 0.00 & 81.17 ± 0.27 & 81.10 ± 0.98 & 83.63 ± 0.01 & 85.43 ± 3.21 \\
\hline
EWC & 89.39 ± 1.64 & 86.57 ± 0.34 & 83.63 ± 1.14 & 86.10 ± 0.43 & 84.70 ± 1.36 & 85.67 ± 0.71 & 82.73 ± 1.23 & 82.30 ± 1.42 & 81.67 ± 0.91 & 83.13 ± 0.66 & 85.60 ± 1.92\\
\hline
L2 & 91.27 ± 0.27 & 87.33 ± 1.34 & 84.07 ± 0.81 & 85.97 ± 1.28 & 85.53 ± 1.07 & 85.00 ± 0.83 & 84.20 ± 0.24 & 84.57 ± 0.33 & 81.27 ± 0.80 & 84.30 ± 0.86 & 87.33 ± 0.21 \\
\hline
PredKD+FeatKD & 88.66 ± 0.85 & 86.50 ± 0.28 & 82.70 ± 0.71 & 86.03 ± 0.47 & 85.80 ± 0.99 & 86.07 ± 0.24 & 83.60 ± 0.00 & 81.17 ± 0.52 & 81.10 ± 0.99 & 83.63 ± 0.09 & 85.43 ± 1.79 \\
\hline
Packnet & 82.79 ± 0.20 & 73.07 ± 0.63 & 69.03 ± 0.39 & 76.20 ± 0.78 & 69.47 ± 1.35 & 71.40 ± 0.99 & 68.07 ± 1.69 & 71.43 ± 0.62 & 67.13 ± 0.47 & 66.47 ± 0.34 & 74.50 ± 0.54  \\
\hline
WSN & 84.03 ± 0.26 & 83.63 ± 1.13 & 80.50 ± 1.10 & 84.80 ± 0.93 & 81.30 ± 0.45 & 83.53 ± 0.21 & 80.27 ± 0.68 & 83.27 ± 0.29 & 78.77 ± 1.55 & 84.87 ± 0.82 & 86.57 ± 0.60  \\
\hline
Vanilla Hnet & 88.52 ± 0.37 & 82.87 ± 0.29 & 80.47 ± 0.52 & 83.30 ± 0.43 & 81.97 ± 1.17 & 83.30 ± 0.78 & 80.37 ± 1.34 & 80.60 ± 2.79 & 79.13 ± 0.93 & 82.00 ± 1.35 & 82.30 ± 5.45 \\
\hline
\textbf{H-embed Hnet*} & 88.78 ± 0.11 & 83.35 ± 0.15 & 82.10 ± 0.30 & 83.55 ± 0.05 & 81.10 ± 0.70 & 82.90 ± 1.40 & 83.05 ± 1.05 & 81.55 ± 1.05 & 81.10 ± 1.50 & 84.55 ± 0.15 & 87.15 ± 0.65 \\

\hline
\end{tabular}
\end{adjustbox}
\caption{\textbf{Cifar.Comparison Experiments, Accuracy During.} The test accuracy of tasks upon finishing training on that task. The mean value and standard deviation are derived with three times running.}
\end{sidewaystable}


\begin{sidewaystable}[ht]
\centering
% 缩小字体大小
% \small
% 调整行间距，设置为 0.85 倍的标准间距
\renewcommand{\arraystretch}{1.6}
% 调整列之间的间距，减少到原来的 0.85 倍
\setlength{\tabcolsep}{0.9\tabcolsep}
% 调整表格宽度
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Method & Task 1 & Task 2 & Task 3 & Task 4 & Task 5 & Task 6 & Task 7 & Task 8 & Task 9 & Task 10 & Task 11 \\
\hline
Finetune & 10.21 ± 0.30 & 10.27 ± 0.77 & 10.00 ± 0.00 & 9.63 ± 0.52 & 11.80 ± 1.81 & 11.83 ± 2.66 & 11.07 ± 2.66 & 10.90 ± 1.65 & 13.23 ± 3.44 & 19.77 ± 1.19 & 82.77 ± 2.08\\
\hline
Finetune Head & 9.48 ± 0.76 & 7.97 ± 0.94 & 8.70 ± 0.28 & 8.13 ± 0.24 & 9.80 ± 0.28 & 6.57 ± 2.03 & 11.07 ± 0.66 & 9.80 ± 1.84 & 9.73 ± 0.24 & 8.60 ± 0.99 & 79.00 ± 0.42 \\
\hline
LwF & 16.93 ± 1.30 & 17.77 ± 2.29 & 19.77 ± 1.72 & 18.83 ± 0.82 & 21.97 ± 0.74 & 23.00 ± 1.59 & 26.27 ± 1.68 & 35.70 ± 4.15 & 40.30 ± 4.14 & 53.07 ± 0.50 & 86.83 ± 0.59 \\
% \hline
% PredKD +EWC & 21.55 ± 4.86 & 18.23 ± 3.58 & 26.13 ± 0.38 & 28.03 ± 0.94 & 27.20 ± 1.13 & 35.10 ± 4.67 & 34.53 ± 1.23 & 42.40 ± 0.00 & 43.53 ± 9.15 & 58.53 ± 4.43 & 85.43 ± 1.79 & 38.24 ± 18.49 \\
\hline
EWC & 19.34 ± 4.27 & 18.00 ± 3.76 & 23.23 ± 3.93 & 25.10 ± 3.77 & 24.97 ± 2.77 & 30.00 ± 6.34 & 31.67 ± 3.60 & 40.87 ± 2.17 & 41.03 ± 7.99 & 57.80 ± 5.03 & 85.60 ± 1.92 \\
\hline
L2 & 23.32 ± 5.53 & 21.60 ± 3.27 & 25.10 ± 0.86 & 25.17 ± 5.00 & 29.30 ± 2.79 & 27.83 ± 5.42 & 41.07 ± 3.13 & 49.30 ± 6.23 & 51.23 ± 2.01 & 56.93 ± 1.61 & 87.33 ± 0.21  \\
\hline
PredKD+FeatKD & 19.12 ± 5.04 & 15.53 ± 2.29 & 23.37 ± 1.69 & 20.80 ± 1.27 & 19.93 ± 2.15 & 25.23 ± 4.86 & 23.90 ± 0.41 & 32.93 ± 5.14 & 43.77 ± 3.30 & 51.73 ± 4.43 & 87.07 ± 0.17  \\
\hline
Packnet & 82.79 ± 0.20 & 73.07 ± 0.63 & 69.03 ± 0.39 & 76.20 ± 0.78 & 69.47 ± 1.35 & 71.40 ± 0.99 & 68.07 ± 1.69 & 71.43 ± 0.62 & 67.13 ± 0.47 & 66.47 ± 0.34 & 74.50 ± 0.54   \\
\hline
WSN & 84.03 ± 0.26 & 83.63 ± 1.13 & 80.50 ± 1.10 & 84.80 ± 0.93 & 81.30 ± 0.45 & 83.53 ± 0.21 & 80.27 ± 0.68 & 83.27 ± 0.29 & 78.77 ± 1.55 & 84.87 ± 0.82 & 86.57 ± 0.60 \\
\hline
Vanilla Hnet & 88.55 ± 0.37 & 82.73 ± 0.37 & 80.13 ± 0.33 & 83.40 ± 0.24 & 82.00 ± 1.07 & 83.27 ± 0.82 & 80.43 ± 1.58 & 80.53 ± 2.60 & 78.97 ± 1.08 & 81.97 ± 1.20 & 82.30 ± 5.45 \\
\hline
\textbf{H-embed Hnet*} &  88.71 ± 0.01 & 83.50 ± 0.00 & 82.05 ± 0.15 & 83.60 ± 0.40 & 81.05 ± 0.45 & 82.65 ± 1.65 & 83.00 ± 1.10 & 81.35 ± 1.15 & 81.10 ± 1.40 & 84.50 ± 0.30 & 87.15 ± 0.65 \\

\hline
\end{tabular}
\end{adjustbox}
\caption{\textbf{Cifar.Comparison Experiments, Accuracy Final.} The test accuracy of tasks when finishing learning all CL tasks. The mean value and standard deviation are derived with three times running.}
\end{sidewaystable}

\begin{sidewaystable}[ht]
\centering
% 缩小字体大小
% \small
% 调整行间距，设置为 0.85 倍的标准间距
\renewcommand{\arraystretch}{1.6}
% 调整列之间的间距，减少到原来的 0.85 倍
\setlength{\tabcolsep}{0.9\tabcolsep}
% 调整表格宽度
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Method & Task 1 &  Task 2 & Task 3 & Task 4 & Task 5 & Task 6 & Task 7 & Task 8 & Task 9 & Task 10 \\
\hline
Finetune& 46.84 ± 2.93 & 39.80 ± 3.18 & 51.84 ± 2.02 & 47.96 ± 3.32 & 59.19 ± 3.32 & 51.93 ± 1.85 & 56.61 ± 1.63 & 56.92 ± 1.74 & 53.83 ± 2.30 & 57.96 ± 0.69  \\
\hline
Finetune Head & 46.84 ± 2.93 & 39.80 ± 3.18 & 51.84 ± 2.02 & 47.96 ± 3.32 & 59.19 ± 3.32 & 51.93 ± 1.85 & 56.61 ± 1.63 & 56.92 ± 1.74 & 53.83 ± 2.30 & 57.96 ± 0.69 \\
\hline
LwF &54.39 ± 0.76 & 37.80 ± 0.43 & 41.95 ± 1.99 & 33.24 ± 1.43 & 42.80 ± 0.88 & 32.86 ± 0.37 & 36.62 ± 0.51 & 32.44 ± 0.85 & 31.06 ± 0.49 & 40.06 ± 0.99\\
% \hline
% PredKD +EWC & 88.66 ± 0.72 & 86.50 ± 0.08 & 82.70 ± 0.50 & 86.03 ± 0.22 & 85.80 ± 0.98 & 86.07 ± 0.06 & 83.60 ± 0.00 & 81.17 ± 0.27 & 81.10 ± 0.98 & 83.63 ± 0.01 & 85.43 ± 3.21 \\
\hline
EWC & 54.39 ± 0.76 & 33.38 ± 0.82 & 30.74 ± 2.07 & 20.02 ± 1.37 & 24.89 ± 2.86 & 18.54 ± 1.11 & 23.70 ± 1.83 & 18.76 ± 0.93 & 18.06 ± 2.32 & 25.05 ± 2.17 \\
\hline
L2 & 54.39 ± 0.76 & 51.58 ± 0.76 & 58.59 ± 1.48 & 54.13 ± 1.19 & 65.54 ± 0.53 & 58.43 ± 2.28 & 63.55 ± 1.54 & 61.37 ± 0.66 & 58.49 ± 1.53 & 63.77 ± 0.70 \\
\hline
PredKD+FeatKD & 54.39 ± 0.76 & 37.80 ± 0.43 & 41.95 ± 1.99 & 33.24 ± 1.43 & 42.80 ± 0.88 & 32.86 ± 0.37 & 36.62 ± 0.51 & 32.44 ± 0.85 & 31.06 ± 0.49 & 40.06 ± 0.99 \\
\hline
Packnet & 30.08 ± 2.94 & 34.96 ± 3.33 & 38.34 ± 2.81 & 31.97 ± 0.96 & 46.02 ± 1.68 & 29.37 ± 1.37 & 35.67 ± 1.54 & 32.23 ± 1.18 & 33.56 ± 1.50 & 34.11 ± 1.74 \\
\hline
WSN & 30.53 ± 2.80 & 29.63 ± 1.11 & 33.97 ± 1.49 & 29.63 ± 2.45 & 46.77 ± 1.02 & 36.57 ± 1.11 & 43.60 ± 0.83 & 41.67 ± 0.90 & 41.60 ± 1.31 & 45.97 ± 2.00 \\
\hline
Vanilla Hnet & 39.41 ± 3.94 & 30.31 ± 1.10 & 40.02 ± 2.11 & 36.25 ± 1.81 & 44.41 ± 0.50 & 33.81 ± 1.94 & 41.47 ± 2.64 & 38.00 ± 2.50 & 34.13 ± 5.32 & 43.88 ± 2.08 \\
\hline
\textbf{H-embed Hnet*} & 39.41 ± 3.94 & 30.31 ± 1.10 & 39.78 ± 2.47 & 35.71 ± 1.68 & 43.92 ± 1.24 & 35.55 ± 2.09 & 39.62 ± 2.27 & 37.89 ± 1.22 & 34.07 ± 3.96 & 44.73 ± 2.05 \\

\hline
\end{tabular}
\end{adjustbox}
\caption{\textbf{ImageNet.Comparison Experiments, Accuracy During.} The test accuracy of tasks upon finishing training on that task. The mean value and standard deviation are derived with three times running.}
\end{sidewaystable}


\begin{sidewaystable}[ht]
\centering
% 缩小字体大小
% \small
% 调整行间距，设置为 0.85 倍的标准间距
\renewcommand{\arraystretch}{1.6}
% 调整列之间的间距，减少到原来的 0.85 倍
\setlength{\tabcolsep}{0.9\tabcolsep}
% 调整表格宽度
\begin{adjustbox}{max width=\textwidth}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
Method & Task 1 & Task 2 & Task 3 & Task 4 & Task 5 & Task 6 & Task 7 & Task 8 & Task 9 & Task 10 \\
\hline
Finetune & 7.67 ± 0.31 & 7.03 ± 2.83 & 9.04 ± 2.43 & 10.79 ± 1.49 & 9.46 ± 2.20 & 6.18 ± 0.81 & 15.32 ± 2.08 & 10.48 ± 0.71 & 16.87 ± 2.92 & 57.96 ± 0.69 \\
\hline
Finetune Head & 7.67 ± 0.31 & 7.03 ± 2.83 & 9.04 ± 2.43 & 10.79 ± 1.49 & 9.46 ± 2.20 & 6.18 ± 0.81 & 15.32 ± 2.08 & 10.48 ± 0.71 & 16.87 ± 2.92 & 57.96 ± 0.69 \\
\hline
LwF & 8.04 ± 1.66 & 11.96 ± 1.52 & 11.63 ± 1.18 & 9.57 ± 2.10 & 17.63 ± 2.40 & 16.90 ± 1.30 & 18.49 ± 2.11 & 17.77 ± 1.16 & 20.95 ± 1.10 & 40.06 ± 0.99 \\
% \hline
% PredKD +EWC & 21.55 ± 4.86 & 18.23 ± 3.58 & 26.13 ± 0.38 & 28.03 ± 0.94 & 27.20 ± 1.13 & 35.10 ± 4.67 & 34.53 ± 1.23 & 42.40 ± 0.00 & 43.53 ± 9.15 & 58.53 ± 4.43 & 85.43 ± 1.79 & 38.24 ± 18.49 \\
\hline
EWC & 6.88 ± 1.43 & 11.92 ± 0.67 & 14.23 ± 0.90 & 11.42 ± 1.11 & 17.04 ± 1.54 & 15.37 ± 1.23 & 20.89 ± 1.18 & 16.93 ± 1.00 & 18.00 ± 1.62 & 25.05 ± 2.17 \\
\hline
L2 & 10.37 ± 0.92 & 6.10 ± 1.14 & 8.26 ± 0.43 & 9.96 ± 0.30 & 10.75 ± 1.19 & 7.66 ± 1.20 & 16.10 ± 3.10 & 9.64 ± 0.63 & 16.87 ± 2.43 & 63.77 ± 0.70 \\
\hline
PredKD+FeatKD & 8.35 ± 1.88 & 13.27 ± 1.60 & 13.02 ± 1.03 & 10.84 ± 3.59 & 17.37 ± 2.05 & 16.32 ± 0.67 & 21.01 ± 1.63 & 20.07 ± 2.33 & 23.57 ± 3.09 & 38.36 ± 3.18 \\
\hline
Packnet & 30.08 ± 2.94 & 34.96 ± 3.33 & 38.34 ± 2.81 & 31.97 ± 0.96 & 46.02 ± 1.68 & 29.37 ± 1.37 & 35.67 ± 1.54 & 32.23 ± 1.18 & 33.56 ± 1.50 & 34.11 ± 1.74 \\
\hline
WSN & 30.53 ± 2.80 & 29.63 ± 1.11 & 33.97 ± 1.49 & 29.63 ± 2.45 & 46.77 ± 1.02 & 36.57 ± 1.11 & 43.60 ± 0.83 & 41.67 ± 0.90 & 41.60 ± 1.31 & 45.97 ± 2.00 \\
\hline
Vanilla Hnet & 39.66 ± 4.11 & 30.17 ± 1.40 & 39.66 ± 2.07 & 35.91 ± 2.10 & 44.19 ± 0.13 & 33.54 ± 1.85 & 41.05 ± 2.96 & 37.95 ± 2.52 & 34.30 ± 5.30 & 43.88 ± 2.08 \\
\hline
\textbf{H-embed Hnet*} &  39.59 ± 3.94 & 30.40 ± 1.42 & 40.02 ± 2.45 & 35.47 ± 2.16 & 43.92 ± 1.22 & 35.71 ± 1.89 & 39.56 ± 2.43 & 38.10 ± 1.09 & 34.13 ± 4.08 & 44.73 ± 2.05 \\

\hline
\end{tabular}
\end{adjustbox}
\caption{\textbf{ImageNet.Comparison Experiments, Accuracy Final.} The test accuracy of tasks when finishing learning all CL tasks. The mean value and standard deviation are derived with three times running.}
\end{sidewaystable}


\begin{sidewaystable}[!ht]
% 调整行间距，设置为 0.85 倍的标准间距
\renewcommand{\arraystretch}{1.6}
% 调整列之间的间距，减少到原来的 0.85 倍
\setlength{\tabcolsep}{0.9\tabcolsep}
    \centering
        \begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
    \hline
        \textbf{Setting} & \textbf{Method} & \textbf{Type} & \textbf{Task 1} & \textbf{Task 2} & \textbf{Task 3} & \textbf{Task 4} & \textbf{Task 5} & \textbf{Task 6} & \textbf{Task 7} & \textbf{Task 8} & \textbf{Task 9} & \textbf{Task 10} & \textbf{Task 11} \\ \hline
        \multirow{6}*{\textbf{PermutedMNIST}} & \multirow{2}*{Rand-embed} & Final & 98.09  & 97.83  & 97.58  & 97.85  & 97.83  & 97.48  & 97.08  & 97.17  & 96.86  & 96.87  & ~ \\ \cline{3-14}
        ~ & ~ & During & 98.07  & 97.79  & 97.62  & 97.85  & 97.85  & 97.46  & 97.09  & 97.05  & 96.82  & 96.87  & ~ \\ \cline{2-14}
        ~ & \multirow{2}*{H-embed} & Final & 98.11  & 97.83  & 97.74  & 97.71  & 97.82  & 97.68  & 97.11  & 97.55  & 96.66  & 97.32  & ~ \\ \cline{3-14}
        ~ & ~ & During & 98.07  & 97.79  & 97.77  & 97.73  & 97.87  & 97.69  & 97.19  & 97.51  & 96.65  & 97.32  & ~ \\ \cline{2-14}

        ~ & \multirow{2}*{Vanilla} & Final & 98.10  & 97.78  & 97.79  & 97.50  & 97.60  & 97.46  & 97.37  & 97.17  & 97.21  & 96.97  & ~ \\ \cline{3-14}
        ~ & ~ & During & 98.07  & 97.79  & 97.83  & 97.51  & 97.55  & 97.46  & 97.38  & 97.14  & 97.19  & 96.97  & ~ \\ \hline

        \multirow{6}*{\textbf{Cifar10/100}} & \multirow{2}*{Rand-embed} & Final & 70.47  & 71.80  & 69.10  & 71.40  & 64.60  & 69.10  & 66.70  & 72.80  & 71.30  & 74.50  & 81.20  \\ \cline{3-14}
        ~ & ~ & During & 79.47  & 76.60  & 71.90  & 78.60  & 75.00  & 77.00  & 76.20  & 76.00  & 75.00  & 77.40  & 81.20   \\ \cline{2-14}

        ~ & \multirow{2}*{H-embed} & Final & 71.29  & 75.60  & 67.60  & 70.20  & 68.80  & 70.20  & 67.30  & 73.20  & 71.40  & 77.50  & 82.10  \\ \cline{3-14}
        ~ & ~ & During & 79.47  & 76.60  & 73.80  & 77.60  & 75.20  & 77.90  & 77.50  & 76.70  & 73.10  & 78.50  & 82.10  \\ \cline{2-14}



        ~ & \multirow{2}*{Vanilla} & Final & 70.27  & 71.70  & 65.90  & 69.90  & 65.70  & 67.30  & 64.30  & 70.30  & 69.70  & 71.00  & 80.40  \\ \cline{3-14}
        ~ & ~ & During & 79.47  & 76.60  & 71.70  & 78.00  & 75.80  & 77.50  & 76.50  & 76.20  & 75.70  & 76.50  & 80.40  \\ \hline




        \multirow{6}*{\textbf{ImageNet-R}} & \multirow{2}*{Rand-embed} & Final & 39.96  & 31.98  & 41.59  & 35.13  & 45.00  & 35.34  & 39.68  & 36.01  & 32.88  & 42.89  & ~ \\ \cline{3-14}
        ~ & ~ & During & 40.70  & 31.84  & 41.95  & 34.84  & 44.19  & 35.34  & 39.14  & 35.53  & 33.22  & 42.89  & ~ \\ \cline{2-14}



        ~ & \multirow{2}*{H-embed}& Final & 40.33  & 32.40  & 42.86  & 36.30  & 45.65  & 35.97  & 39.86  & 39.47  & 33.22  & 46.07  & ~ \\ \cline{3-14}
        ~ & ~ & During & 40.70  & 31.84  & 43.04  & 36.01  & 45.65  & 35.97  & 40.75  & 39.31  & 33.05  & 46.07  & ~ \\ \cline{2-14}


        ~ & \multirow{2}*{Vanilla} & Final & 40.52  & 32.12  & 42.50  & 35.57  & 44.19  & 34.07  & 41.65  & 36.32  & 31.35  & 43.74  & ~ \\ \cline{3-14}
        ~ & ~ & During & 40.70  & 31.84  & 42.86  & 35.86  & 43.71  & 33.91  & 42.37  & 36.64  & 31.35  & 43.74 & ~ \\ \hline

    \end{tabular}
    \caption{\textbf{Ablation Studies, During and Final Accuracy.}}
\end{sidewaystable}
