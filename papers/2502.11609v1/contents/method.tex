% In this section, we will introduce our framework by each module
\subsection{Hypernet-Based CL Framework}

% Unlike most existing approaches that contain the range of variations in model weights $\Theta$ or outputs $f(x,\Theta)$ to maintain proximity between $\{\Theta^{(j)}; f(x,\Theta^{(j)})\}$ and $\{\Theta^{(j-1)}; f(x,\Theta^{(j-1)})\}$ during training task $j$, we 
% address the CL problem from a meta perspective. 
Unlike most existing approaches that are limited to the exploration of model elements, in this work we propose a hypernet-based CL framework to maximally leverage knowledge about task relationships for task model generation.

Following \cite{von2020continual}\footnote{In fact, this guidance is more general and can be incorporated into any hypernet-based CL framework, yet here we mainly base our framework on the work of \cite{von2020continual}.}, a task-conditioned hypernetwork $f_h(e, \Theta_h)$ with hypernet weights $\Theta_h$ is introduced to map a task embedding $e$ to the corresponding model weights $\Theta$ of task model $f$. On this basis, we present a framework (illustrated in Fig.~\ref{fig:CL}) that guides the hypernet with the transferability-based H-embedding $\hat{e}$. Specifically, all tasks $\{T_j\}_{j=1}^M$ in the learning scenario share a single hypernet $f_h$ that generates their task model weights using their task-specific embeddings $\{e^{(j)}\}_{j=1}^M$, i.e. $\Theta^{(j)}=f_h(e^{(j)}, \Theta_h)$ for task $j$. When learning each task $T_j$, the task embedding $e^{(j)}$ is simultaneously updated with the training of hypernet parameters $\Theta_h$, while parameters other than $\Theta_h$ and $e^{(j)}$ are fixed and can be viewed as constants. The learning of $e^{(j)}$ and $\Theta_h$ is regularized using previous task embeddings $\{e^{(n)}\}_{n=1}^{j-1}$ and H-embedding $\hat{e}^{(j)}$ to ensure backward and forward transfer performance. The learning loss is composed of three parts:
\begin{itemize}
    \item Target loss, a supervised loss to learn current task $j$.
    \begin{align}
        L_t &=  \mathcal{L}(f(x^{(j)},\Theta^{(j)}),y^{(j)} \\
        &=  \mathcal{L}(f(x^{(j)},f_h(e^{(j)},\Theta_h)),y^{(j)}) \nonumber
    \end{align}
    \item Continual learning loss (same as introduced by \cite{von2020continual}), to prevent CF by ensuring that given previous task embeddings $\{e^{(n)}\}_{n=1}^{j-1}$, the network weights output by the hypernet before and after training on task $j$ are analogous.
    \begin{align}
        L_c &=\frac{1}{j-1}\sum_{n=1}^{j-1} L_c^{(n)} \\
        &= \frac{1}{j-1}\sum_{n=1}^{j-1}||f_h(e^{(n)},\Theta_h) - f_h(e^{(n)},\Theta_h^*)||^2 \nonumber
    \end{align}
    \item H-embedding guidance loss, to provide the hypernet with additional prior knowledge about the task relationships using transferability.
    \begin{align}
         L_e = L_e(e^{(j)},\hat{e}^{(j)})
    \end{align}
\end{itemize}

Here, $\mathcal{L}$ denotes certain supervised task loss (cross-entropy loss in our experiments), and $\Theta_h^*$ is the set of hypernet parameters before learning task $j$. The definition of the embedding regularization loss $L_e$ will be covered in later sections. To summarize, our final loss function is as follows with hyperparameters $\beta_e$ and $\beta_c$:
\begin{align}
    L = L_t+\beta_e L_e +\beta_c L_c
    % &= \mathcal{L}(f(x^{(j)},f_h(e^{(j)},\Theta_h)),y^{(j)}) + \beta_e L_e(e^{(j)},\hat{e}^{(j)}) \\
    % &\ \ \ \ + \frac{\beta_c}{j-1}\sum_{n=1}^{j-1}||f_h(e^{(n)},\Theta_h) - f_h(e^{(n)},\Theta_h^*)||^2
    \label{eqn:loss}
\end{align}

On the $j$-th task, our approach for the training of $T_j$ is depicted in Fig.~\ref{fig:framework}. Notably, although it may appear that the task model weights are first generated and subsequently used for inference, the framework is actually end-to-end, with the hypernet parameters $\Theta_h$ and embeddings $e^{(j)}$ optimized directly by feeding the task data and minimizing the total loss. Hence, there is no additional training procedure introduced in our framework, and the only information to save is the low-dimensional\footnote{The dimension of task embedding is set to 32 in Cifar10/100 \& ImageNet-R and 24 in MNIST experiments.} task embedding $e^{(j)}$. 
% For a more comprehensive view of our guided hypernet framework, we further present the full continual learning procedure in Fig.~\ref{fig:CL}.

\begin{figure*}[htb]
    \centering
    % \centerline{\includesvg[width=2\columnwidth]{figs/framework.svg}}
    \centerline{\includegraphics[width=2.1\columnwidth]{figs/framework.pdf}}
    % \caption{\textbf{Framework of H-Ensemble.} The framework consists of three modules. The target data firstly flow into Target Feature Extractor. Then the Weight Optimizer will utilize the outputs of source feature extractors and target label to derive the optimal source weight $\boldsymbol{\alpha}$, which makes the parameter in deriving target feature. Finally the Target Classifier will be trained and used together with the extractor for test according to a generalization of maximal correlation regression (MCR).}
    \caption{\textbf{Framework of our hypernet on the slice of task $j$.} A hypernet (left, blue) is utilized to learn the weights of the main model (right, orange), where the H-embedding guidance is introduced using an encoder-decoder module. The entire framework is trained end-to-end by inputting task data into the main model and propagating gradients backward to update both hypernet and embedding. }
    \label{fig:framework}
\end{figure*}


\subsection{H-Embedding Guidance}
\label{sec:H-embed}
\subsubsection{H-embedding}
% Following the desiderata in Sec.\ref{sec:intro}, we need to incorporate the task relationships implied by accessible data into a prior embedding to guide the CL framework. Here, we propose a H-score based online task embedding named H-embedding. 
Building on the desiderata outlined in Sec.\ref{sec:intro}, we incorporate task relationships from accessible data into a prior embedding to guide the CL framework. Specifically, we propose an H-score-based online task embedding, named H-embedding. 

During the training stage of task $j$, we first measure the H-score transferability from each previous task $\{T_n\}_{n=1}^{j-1}$ to $T_j$ using $D_j$ and previous task embeddings $\{e^{(n)}\}_{n=1}^{j-1}$.
\begin{align}
    \label{eqn:hscore}
    H(T_n, T_j) &= tr(cov(f_l(x^{(j)}, \Theta^{(n)}))^{-1}\cdot \\
    & \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ cov(\mathbb{E}_{P_{X|Y}}[f_l(x^{(j)}, \Theta^{(n)})|y^{(j)}]))\nonumber \\
    \Theta^{(n)} &= f_h(e^{(n)}, \Theta_h) \nonumber
\end{align}
Here, we leverage the convenience that previous task models can be reconstructed by the hypernet and corresponding task embeddings. $f_l(*)$ denotes the output of the last hidden layer in the task model $f$, which can be viewed as the feature of task data $X^{(j)}$. The H-embedding $\hat{e}^{(j)}$ is then computed by minimizing the difference between the Euclidean distance of $e^{(n)}, \hat{e}^{(j)}$ and their reversed H-score transferability $H(T_n, T_j)$:
\begin{align}
    \hat{e}^{(j)} = \argmin_{\hat{e}^{(j)}} \sum_{n=1}^{j-1} \left(||\hat{e}^{(j)} - e^{(n)}||_2 - 1/H(T_n, T_j)\right)^2,
    \label{eqn:emb}
\end{align}
where $e^{(n)}$ is calculated and stored when learning previous tasks.
Given that transferability can only be assessed with a minimum of two tasks, H-embeddings and the guidance loss are computed only after completing the first two tasks.
% Considering that the transferability can only be derived with at least two tasks, the derived H-embeddings and embedding loss are only computed after the first two tasks.

Nevertheless, due to its target-centered intrinsicality, the simple reversal of H-score may not align in property (\textit{i.e.} symmetry and scale) with the Euclidean distance of embeddings learned sequentially on the tasks in CL. Hence, we introduce Analytic Hierarchy Process (AHP) normalization \citep{zamir2018taskonomy} to process the H-score in Eqn.~\ref{eqn:emb}. 
Specifically, we construct a pairwise tournament matrix $W^{(j)} \in \mathbb{R}^{j\times j}$ for task $j$, with elements given by:
\begin{align}
    w^{(j)}_{m,n} = \frac{H(T_m, T_j)}{H(T_n, T_j)}\ \ \ \  \forall m, n\in \{1,2,\dots,j\},
\end{align}
measuring how much times better task $m$ is compared to task $n$ when transferring to task $j$. Here, we define the self-transferability $H(T_j,T_j)$ as the HGR maximal correlation between $X_j$ and $Y_j$, with the consistency of transferability definition verified by the theoretical framework of H-score\footnote{See Appendix~\ref{sec:HGR} for a brief proof and computation details.}. Consequently, the AHP normalized transferabilites are given by elements of the principal eigenvector $\mathbf{v}^{(j)}$ of $W^{(j)}$, \textit{i.e.} $\mathcal{AHP}(T_n,T_j) =  \mathbf{v}^{(j)}_n$, which could be easily converted to a distance metric using standard affinity-distance method $dist(T_n,T_j) = \gamma^{(j)}\exp(-\mathcal{AHP}(T_n,T_j))$. The scaling constant $\gamma^{(j)}$ would be optimized together with $\hat{e}^{(j)}$, modifying Eqn.~\ref{eqn:emb} to:
\begin{align}
    \hat{e}^{(j)}, \gamma^{(j)} = \argmin_{\hat{e}^{(j)}, \gamma^{(j)}} &\sum_{n=1}^{j-1} (||\hat{e}^{(j)} - e^{(n)}||_2 \\
    &- \gamma^{(j)}\exp(-\mathcal{AHP}(T_n,T_j)))^2.
    \label{eqn:get_emb} \nonumber
\end{align}
Given that $H(T_n, T_j)$ and $e^{(n)}$ are actually given by calculation, the above optimization problem is a benign bi-variate optimization problem. We could thus apply a gradient descent algorithm to effectively compute the H-embedding $\hat{e}^{(j)}$ for the $j$-th task. As such, the H-embeddings for all tasks during the continual learning can be calculated in an inductive way.
% We summarize the entire training process of task $j$ in our H-embedding guided hypernet as Algorithm.~\ref{ago:train}

 \begin{table*}[!t]
    \renewcommand{\arraystretch}{1.2}
    \centering
    \begin{tabular}{c | c| c| c |c|c|c }
          \toprule
          \textbf{Task} & \multicolumn{3}{|c|}{\textbf{ Cifar10/100}} & \multicolumn{3}{|c}{\textbf{ImageNet-R}} \\
          \midrule
          \textbf{Method} & $\mathcal{AA}\ (\uparrow)$  & $\mathcal{BWT}\ (\uparrow)$  & $\mathcal{FWT}\ (\uparrow)$  & $\mathcal{AA}\ (\uparrow)$  & $\mathcal{BWT}\ (\uparrow)$  & $\mathcal{FWT}\ (\uparrow)$  \\
          % Method & AA (\uparrow) & BWT (\uparrow) & FWT (\uparrow) \\
          \midrule
          % Source-Best & & \\
          % Source-Worst \\

Finetune & 18.32 $\pm$ 0.70 & -66.98 $\pm$ 0.45 & 1.19 $\pm$ 0.48 & 15.08 $\pm$ 0.62 & -41.34 $\pm$ 0.45 & 19.74 $\pm$ 0.98 \\
          % Single-Worst & 0.9345 & 0.9560 & 0.9005 & 0.7165 & 0.7115 & 0.6775 & 0.6575 & 0.5755 & 0.7662\\
          Finetune Head & 15.64 $\pm$ 0.21  & -69.49 $\pm$ 0.73 & 0.00 $\pm$ 0.87 & 15.08 $\pm$ 0.62 & -41.34 $\pm$ 0.45 & 19.74 $\pm$ 0.98 \\
          Multi Task & 72.29 $\pm$ 0.12 &- (N/A) &- (N/A) &  14.34 $\pm$ 0.16 &- (N/A) &- (N/A) \\
          \midrule
	LwF & 32.77 $\pm$ 0.52 & -57.44 $\pm$ 0.57 & 6.72 $\pm$ 0.53 & 17.30 $\pm$ 0.05 & -23.36 $\pm$ 0.27 & 3.39 $\pm$ 0.11\\
         % FeatKD & 0.9840 & \textbf{0.9770} & 0.9340  \\
         EWC & 36.15 $\pm$ 1.48 & -53.39 $\pm$ 1.69 & 6.38 $\pm$ 0.44 & 15.77 $\pm$ 0.29 & -12.20 $\pm$ 0.34 & -9.47 $\pm$ 0.74 \\
         L2 & 39.84 $\pm$ 0.67 & -50.27 $\pm$ 0.79 & 7.13 $\pm$ 0.42 & 15.95 $\pm$ 0.39 & -47.82 $\pm$ 0.56 & 26.34 $\pm$ 0.62 \\
        % \midrule
	PredKD + FeatKD & 33.03 $\pm$ 0.76 & -56.19 $\pm$ 1.23 & 5.89 $\pm$ 0.31 & 18.22 $\pm$ 0.70 & -22.15 $\pm$ 0.75 & 3.39 $\pm$ 0.11 \\
	% PredKD + EWC & 0.5590 & 0.4635 & 0.7700 \\
	% PredKD + L2 & 0.2490 & 0.3655 & 0.7740   \\
          \midrule
          PackNet & 71.78 $\pm$ 0.11 &  - (N/A) & -7.15 $\pm$ 0.39 & 34.63 $\pm$ 0.85 & - (N/A) & 1.99 $\pm$ 0.93 \\
          % SupSup & \underline{0.9855} &  0.00 (N/A) & 0.66\\
          HyperNet & 82.21 $\pm$ 0.23  & -0.05 $\pm$ 0.05 & 3.80 $\pm$ 0.53 & 38.03 $\pm$ 1.21 & -0.15 $\pm$ 0.04 & 4.88 $\pm$ 0.80\\
          WSN & 82.87 $\pm$ 0.20 & - (N/A) & 4.92 $\pm$ 0.46 & 37.99 $\pm$ 0.27 & - (N/A) & 5.67 $\pm$ 0.56 \\
          \midrule
          % Rand-embed Hnet & 82.42 $\pm$ 0.17 & -0.12 $\pm$ 0.11 &  12.70 $\pm$ 0.60\\
        % H-Net w. Hemb Input & \underline{0.9855} & \underline{0.9745} &  0.32\\
        H-embed Hnet* & 83.58 $\pm$ 0.06 &  -0.02 $\pm$ 0.03 &  5.26 $\pm$ 0.45 & 38.16 $\pm$ 1.13 & 0.07 $\pm$ 0.09 & 4.80 $\pm$ 0.78\\
% hscore-AdamW-lr\_5-3000-s888 & 0.986 & 0.984 & 0.962 & 0.798 & 0.95 & 0.8325 & 0.858 & 0.829 & 0.8999375 \\
          \bottomrule
    \end{tabular}
    
    \caption{\textbf{Accuracy (\%) Comparison on Cifar10/100 and ImageNet-R.} All range of results are derived by three times running with different random seeds and calculating the average and standard deviation. Our method (marked by `*') achieves the top average accuracy with high confidence.}
    
    % The appended column
    % 'Para.' records the number of parameters in each method and 
    % `Time' records the time elapse of training.}
    \label{tab:cifar}
\end{table*}

\subsubsection{Embedding Guidance via Encoder and Decoder}
\label{sec:emb_reg}
Building on this, we introduce an embedding regularization module to incorporate the H-embedding guidance into the hypernet. Viewing the hypernet as comprising a shallow encoder followed by a network, the task embedding $e$ is first mapped from the embedding space $\mathcal{E}$ to a hidden feature $h$ in the hidden space $\mathcal{H}$, and subsequently to the weight space $\mathcal{W}$ during the forward pass. 
% $e$ is mapped from the embedding space $\mathcal{E}$ to a hidden feature $h$ in the hidden space $\mathcal{H}$ by the encoder, and then to the weight space $\mathcal{W}$ by the subsequent network. 
For task $j$, we have:
\begin{align}
    \Theta^{(j)} = f_{h'}(h^{(j)}) = f_{h'}\left(f_{Enc}(e^{(j)})\right) = f_h(e^{(j)}).
\end{align}
Here, $f_{Enc}$ and $f_{h'}$ denote the encoder and the rest part of hypernet respectively. From an information transmission perspective, we presume that the hypernet should in its hidden space encode sufficient information to recover the H-embedding $\hat{e}$. Therefore, we additionally introduce a shallow trainable decoder to map the hidden feature $h$ to an embedding $\tilde{e}$ such that the discrepancy between $\tilde{e}$ and the H-embedding $\hat{e}$ should be minimized, \textit{i.e.}, for task $j$
\begin{align}
    \tilde{e}^{(j)} = f_{Dec}(h^{(j)}) = f_{Dec}\left(f_{Enc}(e^{(j)})\right)
\end{align}
should be as close to $\hat{e}^{(j)}$ as possible, where $f_{Dec}$ denotes the decoder. Summarize it up in a mathematical form, we have the embedding guidance loss for task $j$:
\begin{align}
    L_e = L_e(e^{(j)},\hat{e}^{(j)}) = \mathcal{L}\left(f_{Dec}(f_{Enc}(e^{(j)})), \hat{e}^{(j)}\right).
\end{align}
$\mathcal{L}$ denotes a certain similarity loss, set to the cosine similarity loss in our experiments. The H-embedding $\hat{e}^{(j)}$ is derived by Eqn.~\ref{eqn:get_emb} and the decoder $f_{Dec}$ is updated together with the hypernet during training. Notably, no significant computing cost is posed with the introduction of the embedding regularization module given the encoder and decoder are both shallow fully connected neural networks. We summarize the training process of task $j$ as the algorithm in Appendix~\ref{sec:algorithm}.


% From an information transmission perspective, we hypothesize that the hypernet's hidden space should encode sufficient information to reconstruct the prior embedding $\hat{e}$
%  , which includes the known relationships.

% \begin{algorithm*}[!h]
% \KwIn{Task data $D_j$, previous task embeddings $\{e^{(n)}\}_{n=1}^{j-1}$, hypernet weights $\Theta_h$}
% % \Require{$E_{P^T_X}[\boldsymbol{f}_{T}(x)] = 0$, $E_{P^T_X}[\boldsymbol{f}_{T}(x)\cdot \boldsymbol{f}_{T}(x)^T] = I$}
% \Parameter{Learning rate $\lambda$}
% \KwOut{Current task embedding $e^{(j)}$, updated hypernet weights $\Theta_h$}
%  % \KwResult{how to write algorithm with \LaTeX2e }
% % \Begin{
%     Randomly initialize $e^{(j)}$, $\hat{e}^{(j)}$, $\gamma^{(j)}$\; 
%     \If { $j>2$ }{
%         \For(\tcp*[f]{Compute transferability}){$n \gets 1$ \KwTo $j-1$}{
%          $\Theta^{(n)} \gets f_h(e^{(n)}, \Theta_h)$ \;
%           $H(T_n, T_j) \gets tr\left(cov(f_l(x^{(j)}, \Theta^{(n)}))^{-1}cov(\mathbb{E}_{P_{X|Y}}[f_l(x^{(j)}, \Theta^{(n)})|y^{(j)}])\right)$ \Comment{Eq.~\ref{eqn:hscore}}
%           }
%     $\hat{e}^{(j)}, \gamma^{(j)} \gets \argmin_{\hat{e}^{(j)}, \gamma^{(j)}} \sum_{n=1}^{j-1} \left(||\hat{e}^{(j)} - e^{(n)}||_2 - \gamma^{(j)} H(T_n, T_j)\right)^2$ \Comment{Eq.~\ref{eqn:get_emb}} \\
%     }
%     % \Repeat(\tcp*[f]{Compute H-embedding}){convergence}{
%     %         $\hat{e}^{(j)} \gets \hat{e}^{(j)} + \lambda \nabla_{\hat{e}^{(j)}}\sum_{n=1}^{j-1} \left(||\hat{e}^{(j)} - e^{(n)}||_2 - \gamma^{(j)} H(T_n, T_j)\right)^2$
%     %         $\gamma^{(j)} \gets \gamma^{(j)} + \lambda \nabla_{\gamma^{(j)}}\sum_{n=1}^{j-1} \left(||\hat{e}^{(j)} - e^{(n)}||_2 - \gamma^{(j)} H(T_n, T_j)\right)^2$
%     %       }
%     \Repeat(\tcp*[f]{Train hypernet}){converge}{
%             $e^{(j)} \gets e^{(j)} - \lambda \nabla_{e^{(j)}} L $\;
%             $\Theta_h \gets \Theta_h - \lambda \nabla_{\Theta_h} L$ \Comment{Eq.~\ref{eqn:loss}}
%         }
%     \textbf{Return} $e^{(j)}$, $f_h(\ \cdot\ , \Theta_h)$
% \caption{H-embedding guided Hypernet: Training of Task $j$}
% \label{ago:train}
% \end{algorithm*}