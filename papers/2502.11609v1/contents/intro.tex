Continual learning (CL), also known as incremental learning or life-long learning, has been an important topic in the modern application of deep neural networks, where a model is expected to learn a series of tasks sequentially for the optimization of its capability \citep{wang2024comprehensive}. 
However, in practical usage, catastrophic forgetting (CF) \citep{kirkpatrick2017overcoming} can hamper the model from cumulatively gaining knowledge as intended, severely hindering the overall growth of model capacity and resulting in significant waste of training resources.
Specifically, in CL settings, a model is trained one-by-one (\textit{, i.e.,} data in the old tasks are not fully available anymore when training new ones) on a sequence of tasks, which typically contains either category change or data distribution shifts \citep{qu2021recent}. Consider the training process of a new task, a desirable model performance should be characterized by two aspects (depicted in Fig.~\ref{fig:transfer}) \citep{von2020continual}. 1) \textbf{Backward Transfer / Non-Catastrophic Forgetting} \citep{kirkpatrick2017overcoming}\textbf{:} improvement or at least no significant degradation on previous tasks. 2) \textbf{Forward Transfer:} higher efficiency in learning the new task compared to training a model from scratch. 

\begin{figure}[!h]
    \centering
    \includegraphics[width=1.05\columnwidth]{figs/transfer.pdf}
    \caption{\textbf{Illustration of backward and forward transfer during CL.} $\{T_j\}_{j=1}^{M}$ denotes the CL tasks.}
    \label{fig:transfer}
\end{figure}

Up to now, there have been many studies dedicated to CL strategies, most of them involving the rehearsal of previous data to alleviate the knowledge degradation caused by CF. However, growing concerns about privacy and data safety have made this solution not always feasible, bringing increased attention to the \textit{rehearsal-free} CL setting \cite{smith2023closer}.
% other than data replay. 
Previous works on rehearsal-free CL can be mostly described as regularization-based approaches, including parameter space regularization \citep[e.g.][]{kirkpatrick2017overcoming, zenke2017continual} and feature space regularization \citep[e.g.][]{li2017learning, rebuffi2017icarl}.
% , with the latter alternatively referred to as distillation approaches. 
These approaches introduce different regularization losses during training on the model weights, the intermediate layer, or the final output.
% or the model intermediate/final outputs. 
% For instance, EWC \citep{kirkpatrick2017overcoming} regularizes the model with L2 distance between network weights, and LwF \citep{li2017learning} computes the distillation loss using the predictions from the output head of old task models when learning new tasks.
Despite their effectiveness, regularization methods are limited in their design that the models learned on subsequent tasks are forced to stay close to those on previous tasks, while in fact the tasks in a CL setting are not necessarily similar to each other. On the other hand, architecture-based approaches \citep{wang2024comprehensive} emerge as an alternative solution\footnote{A large proportion of these approaches actually require certain access to previous data and do not conform to rehearsal-free setting. We cover them here mainly for discussion comprehensiveness.}. Rather than attempting to forcibly align all tasks in their model parameters or outputs, they generally aim at dedicating task-specific and task-sharing model components from different architecture levels \citep[e.g.][]{mallya2018packnet, wortsman2020supermasks, jin2022helpful}, and tailoring the training process accordingly. Nevertheless, the allocation of model parts to different tasks usually comes with scaling problems with the growth of task numbers, potentially resulting in insufficient model capacity or excessive model size growth.

These bring us to a fundamental question in continual learning: \textit{Positive backward and forward transfers in CL rely on capturing the underlying relationship between different tasks, but how do we efficiently learn and utilize such relations as we train the model?} Previous works primarily focus on model elements, either by regularizing model updates or by distinguishing task-specific and task-sharing components. Although effective in mitigating CF, these approaches are largely constrained by a trade-off between forward and backward transfer, overlooking the potential of better exploiting task relationships for an overall learning improvement. This oversight often leads to inadequate CF mitigation or limited forward transfer performance, where the incorporation of prior knowledge on task relationships through statistical tools would substantially benefit CL performance.
% Previous works are mainly focused on model elements, either regularizing model updates or separating task-specific and task-sharing components. Despite their success in mitigating CF, the potential of exploiting task relationships for learning improvement is largely overlooked, causing a trade-off between forward and backward transfer, leading to unsatisfactory mitigation or limitation of forward transfer performance, where the introduction of prior knowledge of task relationships using statistical tools would substantially benefit overall performance in CL.

Therefore, recognizing the shared goal between identifying prior relationships among tasks and the role of transferability metrics \citep{ding2024model} in evaluating source-target task compatibility, we propose in this work a CL framework guided by transferability-based task embeddings. Specifically, we introduce an online task embedding scheme named H-embedding, which distills the transferability information into a low-dimensional embedding through an optimization process. H-embedding can be learned efficiently without accessing previous data by maximizing the consistency between the Euclidean distance of embeddings and the H-score \citep{bao2019information} transferability among the corresponding tasks. 
% ! NEED REVISION
To match the property of embedding distances with transferability scores, we further apply analytic hierarchy process (AHP) normalization to the transferability values, significantly improving learning stability over long task sequences without compromising efficiency.
Building on this, we present a hypernet-based CL framework, where a task-conditioned hypernetwork \citep[more in Sec.~\ref{sec:relwork}]{von2020continual} is trained to generate task-specific model weights based on H-embedding modulated task embeddings.
% Our H-embedding can be seamlessly incorporated into the hypernet via an encoder-decoder module to ensure its alignment with the learned task embedding\footnote{In fact, this guidance is more general and can be incorporated into any hypernet-based CL framework, yet here we mainly base our framework on the work of \cite{von2020continual}.}, serving as a guidance for the training of hypernetwork. 

In summary, with the aim of better understanding and utilization of the task space in CL, we propose in this work a novel H-embedding guided hypernet framework.
Our framework is featured by: 1) efficient and reliable learning of task embedding based on the information theoretical foundation of H-score metric; 2) a notable enhancement of CL in the overall performance; 3) ease of practical use with end-to-end training and minimal additional storage beyond low-dimensional task embeddings. 

% With the introduction of H-embedding guidance,
% the framework displays remarkable capability of capturing task relationship, 
% These advantages will be illustrated in detail in our experimental studies. 

% To summarize, our contributions can be mainly list as follows:

% \begin{itemize}
%     \item We propose an online task embedding named H-embedding based on information theoretical transferability by formulating embedding derivation into an optimization problem.
%     \item We establish a transferability task embedding guided hypernet framework for rehearsal-free continual learning, incorporating an encoder-decoder module to illuminate the model with prior H-embeddings.
%     \item We verify through extensive experiments that the introduction of H-embedding guidance enhances CL performance by a boost in forward transfer as well as ensuring the reliability of task embeddings. 
% \end{itemize}


% To conclude, this work presents a new rehearsal-free continual learning strategy. By introducing a hypernetwork guided by an information theoretic transferability based prior task embeddings, our framework possesses a remarkable ability in leveraging the intrinsic relationship of tasks to boost continual learning, as well as obtains an interpretable task representation that may be utilized for further task space perception. 