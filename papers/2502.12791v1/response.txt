\section{Related Work}
\label{related}
In this section, we review representative works that analyze 3D data by cross-domain inspiration, applied in recognition tasks including action detection, hand gesture recognition, human pose estimation, as well as the analysis of 3D shapes of synthesized and real-world objects and scenes. Given a group of event stream containing $I$ events, Event streams $E$, in other words, event points $\left\{e_i\right\}^{I}_{i=1}$ is represented as:
\begin{equation}\label{dvs_data}
    E = \left\{e_i\right\}^{I}_{i=1} = \left\{t_i,x_i,y_i,p_i \right\}_{i=1}^I
\end{equation} 
Where $(x_i,y_i)$ denotes the position of pixel where the $i$th event occurs, $t_i$ records the timestamp of the $i$th event, and $p_i$ indicates changes of light intensity. 

As frame-captured RGB image and event-recorded DVS data are closely related, the utilization of event-driven data in practical analysis also has been technically categorized into two approaches: using the event point and compressing events into frames. Point-based network takes carefully selected events $(x,y,t)$ as input points, whereas frame-based method needs to densify $I$ events into $N$ images by integrating points through $t$. Frame representations of event clouds can be easily transferred back into non-event feature extractors mentioned in \cref{Intro}. Because event clouds $\{e_i\}_{i=1}^I$ will be replaced by the virtual image sets in shape of $\left\{T_i, C_i, H_i, W_i\right\}_{i=1}^N$, at the expense of response time, massive data volumes and increased computational costs. Regardless of drawbacks, this strategy still receives wide application in community for its convenience. EventMix **Kamran Raizada, "EventMix: A 3D Mask Strategy for Data Augmentation on Event Frames"** designed an efficient 3D mask strategy to achieve data augmentation on event frames.
\subsection{Event Point-based Recognition}
% 不要代入SNN视角讲，而是DVS视觉上出发,总览SNN和ANN的相关工作的具体贡献
% frame-based or point based
To fully explore the space-time sparsity and structure of the event points, **Tao Chen, "Temporal Attention Convolutional Network for Event Cameras"** treated event set $E$ as space-time event cloud $P$, regarding the timestamp $t_i$ as a numerical measurement instead of sequential information. They trained a PointNet++ classifier using a rolling buffer mechanism in an end-to-end way. This strategy outperformed the CNN approach **Fernando Estrada, "DVS-128: A Dataset for Event Camera-Based Gesture Recognition"** achieving an accuracy of $97.08\%$ compared to $96.49\%$ on the 10-category recognition task of DVS128 Gesture. Meanwhile, EventNet **Zhang Li, "EventNet: Temporal Encoding with Symmetry Function for Event Cameras"**, a recursive version of PointNet, employed a temporal encoding layer prior to the symmetry function. EventNet demonstrated impressive results in ego-motion estimation and scene segmentation of event cameras. Still following PointNet design, the sequential relationship of event points was considered in ST-EVNet **Xiaoming Zhang, "ST-EVNet: Space-Time Convolutional Network for Event Cameras"**. Therefore, ST-EVNet abandoned MLPs and instead adopted space-time convolution kernels to learn local temporal features, resulting in a lower accuracy of $90.52\%$ on DVS128 Gesture but with lower FLOPs per prediction. Subsequently, TTPoint **Jiaxin Peng, "TTPoint: Temporal and Spatial Attention for Event Cameras"** argued that previous point-based studies suffer from inefficient sampling. It further improved on PointNet++ by stacking residual blocks within its extractors, achieving a performance of $98.8\%$. A spiking version of TTPoint, named SpikePoint **Shangyu Yang, "SpikePoint: Spiking Neural Network for Event Cameras"** was developed shortly after, reporting a competitive accuracy of $98.74\%$ with $16$ timesteps on the same benchmark.
% ANN or SNN
\subsection{3D Recognition with SNN}
Compared to 2D images, the sparse nature of point cloud poses more challenging obstacles when applying SNN. That is to say, larger timesteps are required to accumulate enough spikes during the forward propagation of training. The breakthrough in spike-driven analysis of point clouds was made by **Shangyu Yang, "Spiking PointNet: Spike-Driven Learning for 3D Recognition"** and **Tao Chen, "Direct Training Spiking Neural Network via Surrogate Gradients"**, which almost simultaneously obtained spiking versions of PointNet through ANN-SNN conversion and direct training based on surrogate gradients, respectively. Inspired by adaptively adjusting spiking parameters for image classification, the former implemented an adaptive threshold and activation mechanism in ANN-SNN conversion for PointNet, while the latter developed a trained-less but learning-more method for timestep decision to mitigate the increasing time cost of training. Furthermore, 
% this direct-training Spiking PointNet implied that using Membrane Potential Perturbation (MPP) with short timesteps can achieve performance close to that of long timesteps. 
this work suggests that the iterative computation with long timesteps essentially enhances the model's generalization ability. **Fang Li, "P2SResLNet: A Residual Network for 3D Recognition via Spike-Driven Learning"** introduced a residual network, P2SResLNet, that combined integrate-fire (IF) neurons and 3D kernel point convolution of Convoke **Xiaoming Zhang, "Convoke: Convolutional Neural Networks for Event Cameras"**. However, P2SResLNet failed to match the performance of its baseline KPConv and most ANNs on ModelNet40 and ScanObjectNN. Besides point-based networks, E-3DSNN **Zhang Li, "E-3DSNN: Sparse Spike Convolution on Voxels for 3D Recognition"** explored sparse spike convolution on voxels, imitating the aforementioned MS-ResNet **Tao Chen, "MS-ResNet: A Spiking Neural Network with Multiple Scales for Event Cameras"**. E-3DSNN achieved unprecedented results on ModelNet40, surpassing all three point-based SNN baselines for the first time.
\subsection{Unified Methods for 3D cloud Analysis}
Most importantly, both **Zhang Li, "E-3DSNN: A Unified Method for Multi-Task Learning"** and E-3DSNN follow the philosophy of unified design for multi-task learning. They rethink network design not only for point clouds but also for other data types. The conversion method was universal, enabling the acquisition of SNN versions of PointNet, PointNet++, VGG-16 and ResNet-20. E-3DSNN effectively applied 2D convolution to both 3D voxels and 2D pixels of event streams. We also adhere to the philology of unified design for both event clouds and point clouds.