%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{multirow}
% \usepackage{siunitx}
% \sisetup{
%   table-number-alignment = center,
%   round-mode = places, % 舍入模式为有效数字
%   round-precision = 2, % 保留两位有效数字
%   multiply-inline-number = true
% }
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}
% \usepackage{icml2025}
\usepackage{float}
% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Beyond Timesteps: A Novel Activation-wise Membrane Potential Propagation Mechanism for Spiking Neural Networks in 3D cloud}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
% \icmlauthor{Jian Song}{equal,yyy}
\icmlauthor{Jian Song}{yyy}
% \icmlauthor{Firstname2 Lastname2}{equal,yyy,comp}
\icmlauthor{Boxuan Zheng}{yyy}
\icmlauthor{Xiangfei Yang}{yyy}
\icmlauthor{Donglin Wang}{yyy}
% \icmlauthor{Firstname3 Lastname3}{comp}
% \icmlauthor{Firstname4 Lastname4}{sch}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

% \icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
\icmlaffiliation{yyy}{School of Engineering, Westlake University, Hangzhou, China}
% \icmlaffiliation{comp}{Company Name, Location, Country}
% \icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

% \icmlcorrespondingauthor{Firstname1 Lastname1}{first1.last1@xxx.edu}
\icmlcorrespondingauthor{Donglin Wang}{wangdonglin@westlake.edu.cn}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Spiking Neural Network, Point Cloud, Space-time Event Cloud}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% This document provides a basic paper template and submission guidelines.
% Abstracts must be a single paragraph, ideally between 4--6 sentences long.
% Gross violations will trigger corrections at the camera-ready phase.
Due to the similar characteristics between event-based visual data and point clouds, recent studies have emerged that treat event data as event clouds to learn based on point cloud analysis. Additionally, some works approach point clouds from the perspective of event vision, employing Spiking Neural Network (SNN) due to their asynchronous nature. However, these contributions are often domain-specific, making it difficult to extend their applicability to other intersecting fields. Moreover, while SNN-based visual tasks have seen significant growth, the conventional timestep-wise iterative activation strategy largely limits their real-world applications by large timesteps, resulting in significant delays and increased computational costs. Although some innovative methods achieve good performance with short timesteps $\left(<10\right)$, few have fundamentally restructured the update strategy of spiking neurons to completely overcome the limitations of timesteps. In response to these concerns, we propose a novel and general activation strategy for spiking neurons called \textbf{A}ctivation-wise \textbf{M}embrane \textbf{P}otential \textbf{P}ropagation (AMP2). This approach extends the concept of timesteps from a manually crafted parameter within the activation function to any existing network structure. In experiments on common point cloud tasks (classification, object, and scene segmentation) and event cloud tasks (action recognition), we found that AMP2 stabilizes SNN training, maintains competitive performance, and reduces latency compared to the traditional timestep-wise activation paradigm.
\end{abstract}
\section{Introduction}
\label{Intro}
Recently, point cloud analysis gains significant attention across multiple fields due to its broad applications in autonomous driving \cite{Cui_driving}, virtual and augmented reality \cite{ESP_PCTijcai2024, zielinski2024precise}, and robotics perception \cite{duan2021robotics, cheng2022novel}. A point cloud, represented as a collection of points \begin{math}P\in\mathbb{R}^{N\times D}\end{math}, typically describes the geometrical structure of certain objects and scenes. 
% Each point in the set not only represents 3D coordinates $(x,y,z)$, but may also include additional features such as color, intensity, or normals. 
Clouds exhibit themselves in a sparse, uneven, and unordered distribution. Building upon these characteristics, point-based learning and voxel-based convolution \cite{wu20153d} are two main paradigms. Point-based methods propose to learn features directly from raw input point clouds, e.g., PointNet \cite{Qi_2017_CVPR} proposed a unified MLP architecture for tasks ranging from classification to segmentation. PointNet++ \cite{qi2017pointnet++} expanded this idea further in a hierarchical network to deal with both local and global representations respectively. On the other hand, \cite{ma2022rethinking} pointed out that many efforts have been laid on local feature extractors with the deeper analysis of point cloud, heavily relying on sophisticated strategies to exploit geometric properties. This emphasis on complicated networks, although bringing improvement in performance, also results in prohibitive computations that restrict applications in natural scenes.
\begin{figure}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figure/snn_ann_MPpointnet_instance.pdf}}
\caption{Classification accuracy on the ModelNet40 validation set without voting for each epoch during training. The models compared are MP PointNet (using AMP2), Spiking PointNet \cite{ren2024spiking} (SNN baseline, $T=1$), and PointNet (ANN baseline). Each model was trained for 200 epochs.}
\label{ann_snn_mp}
\end{center}
\vskip -0.1in
\end{figure}

In addition to point cloud data, event-driven data, especially those captured by dynamic vision sensors (DVS), have been regarded as emerging asynchronous pseudo point clouds due to similar characteristics. While DVS resembles the conventional CMOS-sensor camera, it operates on an event-oriented mechanism, i.e., only a pixel detecting certain polarity changes in brightness will trigger a stream of events corresponding to that pixel asynchronously, rather than capturing the whole frame at once. Depending on whether the activation function utilizes a spiking mechanism, methods addressing recognition can be categorized into Artificial Neural Networks (ANN) and Spiking neural networks (SNN). \cite{lee2014real} simultaneously used leaky integrate-and-fire neurons (LIF) as feature extractor and hidden Markov models as classification heads. LIF can zero out certain features like dropout, but this behavior is learned from an iterative decaying of input and past information, instead of being random. However, there is an often overlooked fact that most SNN methods neglect the spatial sparsity and temporal asynchrony of event streams, they count on the repeated activation through synchronous simulation. This raises an unfathomable parameter $T$, the optimal number of activation repetition, also known as time steps, to achieve the best performance. 

Based on spiking activation, some studies have developed high performance network. Spike-Driven Transformer \cite{yao2023spikedriven, yao2024spikedriven, yao2024scaling} continually optimized the architecture design of network. Spikformer series \cite{zhou2023spikformer, zhou2024spikformer} proposed spiking self attention and introduced a convolution-attention mixed mechanism to achieve event-driven computation. MS-ResNet \cite{MS_ResNet}, RG-CNN \cite{bi2020graph}, RM-SNN \cite{yao2023sparser} implemented evaluation across various benchmarks including DVS128 Gesture, UCF101-DVS and HMDB51-DVS. As P3D ResNet \cite{qiu2017learning} implements pseudo-3D convolution for video, it is natural to consider event streams as a 3D spatial-temporal representation of scenes to learn. Spiking mechanisms ensure that the network discriminates between feature importance, retaining the most relevant features and discarding those of lesser significance. However, SNNs receive mixed reviews from academia due to gradient instability, mediocre performance, and a lack of hardware validation for low power consumption in most studies. Moreover, the iterative activation procedure derived from SNNs tend to leverage large time steps, which normally leads to a marginal improvement of 1-4\%. Nevertheless, concurrently, the computation time extends markedly with the depth of the network. Given that lessons from point cloud analysis can inspire DVS data processing, employing SNNs to facilitate effective and efficient computations of pseudo-event data is a logical progression.

How to maintain competitive performance and succinct design in networks seems a promising direction to explore. In this paper, we aim to build a simple and effective network training strategy for 3D vision tasks, inspired by spiking mechanism and residual feed-forward MLP, without relying on dedicated feature extractors increasing computation time steps. We term this strategy as \textbf{A}ctivation-wise \textbf{M}embrane \textbf{P}otential \textbf{P}ropagation (AMP2), The Spiking PointNet using AMP2 is referred to as Membrane Potential PointNet (MP PointNet). By doing so, we alleviate the issue of training instability caused by the inherent discreteness of SNN shown in \cref{ann_snn_mp}. Meanwhile, this strategy could fully enjoy the advantages of existing highly-optimized MLP networks. Even in specific scenarios, AMP2 could eliminate the need to add a time dimension to the data and avoiding iterative nonlinear activation. 
\section{Related Work}
\label{related}
In this section, we review representative works that analyze 3D data by cross-domain inspiration, applied in recognition tasks including action detection, hand gesture recognition, human pose estimation, as well as the analysis of 3D shapes of synthesized and real-world objects and scenes. Given a group of event stream containing $I$ events, Event streams $E$, in other words, event points $\left\{e_i\right\}^{I}_{i=1}$ is represented as:
\begin{equation}\label{dvs_data}
    E = \left\{e_i\right\}^{I}_{i=1} = \left\{t_i,x_i,y_i,p_i \right\}_{i=1}^I
\end{equation} 
Where $(x_i,y_i)$ denotes the position of pixel where the $i$th event occurs, $t_i$ records the timestamp of the $i$th event, and $p_i$ indicates changes of light intensity. 

As frame-captured RGB image and event-recorded DVS data are closely related, the utilization of event-driven data in practical analysis also has been technically categorized into two approaches: using the event point and compressing events into frames. Point-based network takes carefully selected events $(x,y,t)$ as input points, whereas frame-based method needs to densify $I$ events into $N$ images by integrating points through $t$. Frame representations of event clouds can be easily transferred back into non-event feature extractors mentioned in \cref{Intro}. Because event clouds $\{e_i\}_{i=1}^I$ will be replaced by the virtual image sets in shape of $\left\{T_i, C_i, H_i, W_i\right\}_{i=1}^N$, at the expense of response time, massive data volumes and increased computational costs. Regardless of drawbacks, this strategy still receives wide application in community for its convenience. EventMix \cite{shen2023eventmix} designed an efficient 3D mask strategy to achieve data augmentation on event frames.
\subsection{Event Point-based Recognition}
% 不要代入SNN视角讲，而是DVS视觉上出发,总览SNN和ANN的相关工作的具体贡献
% frame-based or point based
To fully explore the space-time sparsity and structure of the event points, \cite{wang2019space} treated event set $E$ as space-time event cloud $P$, regarding the timestamp $t_i$ as a numerical measurement instead of sequential information. They trained a PointNet++ classifier using a rolling buffer mechanism in an end-to-end way. This strategy outperformed the CNN approach \cite{amir2017low} achieving an accuracy of $97.08\%$ compared to $96.49\%$ on the 10-category recognition task of DVS128 Gesture. Meanwhile, EventNet \cite{sekikawa2019eventnet}, a recursive version of PointNet, employed a temporal encoding layer prior to the symmetry function. EventNet demonstrated impressive results in ego-motion estimation and scene segmentation of event cameras. Still following PointNet design, the sequential relationship of event points was considered in ST-EVNet \cite{wang2020st}. Therefore, ST-EVNet abandoned MLPs and instead adopted space-time convolution kernels to learn local temporal features, resulting in a lower accuracy of $90.52\%$ on DVS128 Gesture but with lower FLOPs per prediction. Subsequently, TTPoint \cite{ren2023ttpoint} argued that previous point-based studies suffer from inefficient sampling. It further improved on PointNet++ by stacking residual blocks within its extractors, achieving a performance of $98.8\%$. A spiking version of TTPoint, named SpikePoint \cite{ren2023spikepoint} was developed shortly after, reporting a competitive accuracy of $98.74\%$ with $16$ timesteps on the same benchmark.
% ANN or SNN
\subsection{3D Recognition with SNN}
Compared to 2D images, the sparse nature of point cloud poses more challenging obstacles when applying SNN. That is to say, larger timesteps are required to accumulate enough spikes during the forward propagation of training. The breakthrough in spike-driven analysis of point clouds was made by \cite{lan2023efficient} and \cite{ren2024spiking}, which almost simultaneously obtained spiking versions of PointNet through ANN-SNN conversion and direct training based on surrogate gradients, respectively. Inspired by adaptively adjusting spiking parameters for image classification, the former implemented an adaptive threshold and activation mechanism in ANN-SNN conversion for PointNet, while the latter developed a trained-less but learning-more method for timestep decision to mitigate the increasing time cost of training. Furthermore, 
% this direct-training Spiking PointNet implied that using Membrane Potential Perturbation (MPP) with short timesteps can achieve performance close to that of long timesteps. 
this work suggests that the iterative computation with long timesteps essentially enhances the model's generalization ability. \cite{wu2024point} introduced a residual network, P2SResLNet, that combined integrate-fire (IF) neurons and 3D kernel point convolution of Convoke \cite{thomas2019kpconv}. However, P2SResLNet failed to match the performance of its baseline KPConv and most ANNs on ModelNet40 and ScanObjectNN. Besides point-based networks, E-3DSNN \cite{qiu2024efficient} explored sparse spike convolution on voxels, imitating the aforementioned MS-ResNet \cite{MS_ResNet}. E-3DSNN achieved unprecedented results on ModelNet40, surpassing all three point-based SNN baselines for the first time.
\subsection{Unified Methods for 3D cloud Analysis}
Most importantly, both \cite{lan2023efficient} and E-3DSNN follow the philosophy of unified design for multi-task learning. They rethink network design not only for point clouds but also for other data types. The conversion method was universal, enabling the acquisition of SNN versions of PointNet, PointNet++, VGG-16 and ResNet-20. E-3DSNN effectively applied 2D convolution to both 3D voxels and 2D pixels of event streams. We also adhere to the philology of unified design for both event clouds and point clouds. 
% Intro主要讲的是Point Cloud和event data各自是如何发展，难以察觉的问题，以及二者可以结合起来研究，提出想法能不能用同一种统一的策略既能避免传统PointNet网络愈发复杂，避免SNN时间步导致数据维度增加。
% 不写什么是LIF IF等最基础的认知，直接从SNN的视角开始介绍点云和DVS数据的研究

% 比如

% \subsection{Conventional perspectives}
% （简短）传统依据卷积 残差 自注意力等传统机制研究DVS任务 比如已经提到过的 spike driven transformer v1v2v3\cite{yao2024spikedriven} MS-ResNet \cite{MS_ResNet} RM-SNN \cite{yao2023sparser}用实际数据说明 更大的时间步对效果和计算延迟有多少影响

% \subsection{Perspectives from point cloud}
% (详细) 解释这几篇跟我们最相关的工作 以及遵循的是什么范式，比如point based的方案肯定都是基于pointnet和pointnet++

% 单一领域：只做点云或者只做DVS
% voxel based point cloud class through direct training SNN conv: E-3DSNN \cite{qiu2024efficient};
% point based point cloud class through direct training SNN conv: P2SResLNet \cite{wu2024point};
% point based point cloud class through direct training SNN MLP: Spiking PointNet \cite{ren2024spiking}
% point based DVS data class through direct training SNN MLP: SpikePoint \cite{ren2023spikepoint}
% 多领域统一：同时做点云和DVS的实验 (跟我们一致)
% point based DVS and point cloud through conversion SNN: \cite{lan2023efficient}
% 一定要让这部分的逻辑清晰
\section{Method}
\subsection{Event-driven PointNet}
\begin{figure*}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\textwidth]{figure/SpikingPipeline.png}}
\caption{Overview of event-driven PointNet for 3D object classification. The top demonstrates the workflow of Spiking PointNet, using binary value $0,1$ to discriminate features; the bottom part illustrates event-based computation without timestep-wise activation, which exploits sparse analog value for forward propagation, the yellow cell in the grid represents inactive state $0$, the gray cell indicates non-linear activation.}
\label{spiking_differ}
\end{center}
\vskip -0.1in
\end{figure*}
% 放一个图 展示输入在网络中的改变，比如输入增加一维T, 膜电势更新，拆解标准的LIF计算过程;Spiking PointNet 引入Membrane Potential Perturbation; AMP2策略下LIF需要的改动
As shown in \cref{spiking_differ}, The top illustrates Spiking PointNet proposed by \cite{ren2024spiking}, which follows a conventional timestep-wise iterative activation mechanism. This approach requires an additional temporal dimension $T$ on top of the batch size $B$, channel size $C$, and total number of points $N$. As a direct consequence, most vision data processed in SNNs consume T times more memory compared to ANN-based models. Moreover, since activations are updated step by step, both training and inference in SNNs are inherently slower than in ANNs. The bottom illustrates our proposed AMP2-based event-driven PointNet model. AMP2 enables SNNs to function like ANNs while maintaining event-driven characteristics. It ensures data sparsity, preventing the memory overhead from scaling by a factor of $T$, thereby significantly improving efficiency.
\subsection{Spiking Activation Function}
% 不同任务中timesstep的实现方式，DVS的frame可以将event cloud分割成指定的T份，但是还有情况下只是把时间步重复T次：这段适合放到DVS实验的部分说明，这里应该还是讲LIF
The leaky integrate-and-fire (LIF) model is one of the most commonly used spiking activation functions, as it balances historical information and current features. Its mathematical form is typically described by \cref{lif}, \cref{lif2} and \cref{lif3}.
\begin{align}
    U^{t}_{l} &= H^{t-1}_{l} + X^{t}_{l} \label{lif}\\
    S^{t}_{l} &= Hea.(U^{t}_{l}-V_{th}) \label{lif2}\\
    H^{t}_{l} &= \beta U^{t}_{l} \odot(1-S^{t}_{l}) + u_{th}S^{t}_{l}\label{lif3}
\end{align}
where $l$ and $t$ denote the layer and timestep, $U^{t}_{l}$ means the accumulated membrane potential at $t$, namely, the membrane potential (MP) prior to spiking activation. The accumulated value consists of two items, the MP after spiking activation at $t-1$, $H^{t-1}_{l}$, and the $t$th processed feature, $X^{t}_{l}$, from the $l$th layer's input, processing could be convolution operation, MLP and so forth. $u_{th}$ is the rest potential which is set after activating the output spikes, we call it hard reset when $u_{th} = 0$. $V_{th}$ represents the threshold to determine whether the output spikes $S^{t}_{l}$ should be fired or stay inactive. $Hea.(\cdot)$ is a Heaviside step function that
satisfies $Hea.(x) = 1$ when $x \geq 0$, otherwise $Hea.(x) = 0$. Besides, the leaky function depends on the decay factor $\beta = e^{-\frac{dt}{\tau}}\in (0,1)$ to decrease the impact of historical information, while $\tau$ reflects the membrane time constant. $\odot$ denotes the element-wise multiplication.

In practice, the spiking activation function could output pure binary spikes ($0,1$) or sparse non-binary values. The latter can be implemented in various ways, such as using firing rate encoding, where the accumulated spikes are summed and averaged over timesteps. Another method is $S_{l} \odot V_{th}$, where the spike values are multiplied by the corresponding $V_{th}$. This approach often achieves better performance with dynamic thresholds. Our proposed AMP2 method multiplies the spike values by the membrane potential prior to activation $U_{l}$. This approach is the first step towards eliminating timestep-wise iterative computation. A complete spiking module of AMP2 can be summarized by \cref{mp_before}.
\begin{equation}
    \label{mp_before}
    LIF_{AMP2}(\Phi(x_{l})) = S_{l}(w_l\cdot x_l+b_l)\odot U_l(w_l\cdot x_l+b_l)
\end{equation}
where $w_l$ and $b_l$ represent the weight and bias of processing operation within this module.
\subsection{Activation-wise Membrane Potential Propagation}
% 展示AMP2在完整的网络结构配合，小标题另一个表述是embedding timesteps into architecture
% \begin{algorithm}[tb]
%    \caption{LIF activation with AMP2}
%    \label{amp2_algo}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} feature $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}
\begin{algorithm}[tb]
   \caption{AMP2-LIF Activation}
   \label{amp2_algo}
\begin{algorithmic}
   \STATE {\bfseries Input:} feature $X$, previous layer's post-spike membrane potential $H_{l-1}$, threshold $V_{th}$, coefficients $\alpha$ and $\beta$
   \IF{$H_{l-1}$ is None}
      \STATE $H_{l}^{0} \leftarrow$ randomly initialized value
   \ELSE
      \STATE $H_{l}^{0} \leftarrow \alpha \cdot H_{l-1} + (1-\alpha) \cdot$ random value
   \ENDIF
   \STATE $U_{l} \leftarrow H_{l}^{0} + X$
   \STATE $S_{l} \leftarrow \text{Heaviside}(U_{l} - V_{th})$
   \STATE $H_{l} \leftarrow \beta \cdot U_{l} \odot (1 - S_{l}) + u_{th} \cdot S_{l}$
   \STATE {\bfseries Output:} $S_{l} \odot U_{l}$, $H_{l}$
\end{algorithmic}
\end{algorithm}

\cref{spiking2_figure} illustrates the detailed propagation process of AMP2 within the network. AMP2 redefines the timestep not as an independent iterative computation within each spiking activation function, but as a shared parameter among activation functions at different depths within the network. This approach has several advantages: firstly, it allows the membrane potentials of activation functions operating at different ranges to be interconnected, stabilizing the gradient during SNN training; secondly, the sparse non-binary output values of AMP2-LIF still adhere to the event-driven computation nature, and using the pre-spike membrane potential $U_{l}$ helps reduce the loss of effective feature values caused by spiking. \cref{amp2_algo} demonstrates the membrane potential update and spike generation mechanism based on the AMP2 algorithm, $\alpha$ is set to $0.8$ in all experiments.

We recognize that AMP2 may appear similar to the residual connections studied in previous works on spiking networks. The key difference is that AMP2 connects the membrane potential parameter within the LIF activation function, whereas residual connections link feature values across different layers. The performance differences between these two approaches can be observed in the ablation study in \cref{ablation_section}.
\section{Experiment}
% 交代实验所使用的开源代码来源，数据集的介绍放到附录里，正文不具体说明实验细节，看后续需不需要在附录里交代
Only a few related researches have their code open-sourced on the Internet. We chose the Spiking PointNet proposed by \cite{ren2024spiking} as our SNN baseline for most experiments. The details of the point cloud experiments and ANN model follow \cite{Pytorch_Pointnet_Pointnet2}. As for event cloud data, we investigate AMP2 on event frames and event points. For frame data, we referred to the code from \cite{fang2021deep}, and for processing the raw DVS data into event clouds, we referred to the code from \cite{wang2019space}.
\subsection{Power Consumption on Neuromorphic Processor}
% 第一个实验的目的说明T对SNN功耗影响，并且发现ANN和SNN的功耗并没有过去研究中估计得那么夸张，T反而那个需要解决的问题；还要说明运行HP201的模式是使用通用的pytorch兼容接口，并没有对网络进行专门的类脑计算指令适配；要不要讲除了MP PointNet以外都是sum？
\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figure/duration.pdf}}
\caption{Inference durations (seconds) for various models on a batch from the ModelNet40 test set, conducted on a lynxi HP201 computing unit. The recorded times represent only the Inference phase as detailed in \cref{power_table}. Each bar displays the specific inference duration at the top. Note that the MP PointNet results are overestimated, as the tested MP PointNet is an early, unoptimized version with some unnecessary computations.}
\label{infer_time_figure}
\end{center}
\vskip -0.1in
\end{figure}
% 文本要解释说明高估的原因，应该功耗应该与Spiking PointNet 1 保持一致
\begin{table*}[t]
\caption{Average power consumption (mW) and standard deviation before (Pre), during, and after (Post) inference on a batch from the ModelNet40 test set using the Lynix HP201. The sampling duration for before and after inference is 10 seconds each. The during inference sampling starts after loading the model and data, and ends when inference is complete. Note that the power consumption of MP PointNet is overestimated due to some unnecessary computations in the tested version.}
\label{power_table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{cccccc}
\toprule
Type & Model & timestep & Pre & Inference & Post \\
\midrule
\multirow{4}{*}{SNN} & \multirow{3}{*}{Spiking PointNet} & 1 & $15262.50\pm33.08$ & $15669.20\pm209.0$ & $15294.0\pm0.0$ \\
 &  & 4 & $15311.75\pm22.82$ & $15912.54\pm60.88$ & $15459.50\pm251.47$ \\
 &  & 8 & $15217.25\pm8.67$  & $15954.18\pm49.49$ & $15339.50\pm4.33$ \\
 & MP PointNet & - & $15263.75\pm22.82$ & $15770.90\pm184.70$ & $15281.25\pm4.76$ \\
\midrule
ANN & PointNet & - & $14698.50\pm6.38$ & $15267.47\pm290.63$ & $14708.25\pm12.66$\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
To investigate the practical impact of timesteps on spiking networks, we conducted detailed power consumption experiments on the neuromorphic chip Lynxi HP201. \cref{infer_time_figure} records the inference time for different models processing one batch from the ModelNet40 dataset on HP201 chip. It can be observed that larger timesteps significantly increase the computation latency for 3D object recognition in spiking networks. Unsurprisingly, the ANN-based PointNet achieved the shortest inference time, while the MP PointNet took longer than the Spiking PointNet (T=1). We speculate that this is because the tested MP PointNet was an early development version with many redundant computations and insufficient algorithm optimization. Theoretically, the MP PointNet using AMP2 should have a similar inference time to the T=1 model. \cref{power_table} details the power consumption (mW) of the processor during inference. It is important to note that to run both ANN and SNN models uniformly, all code was compiled and executed using the HP201's PyTorch-compatible interface, and the SNN network did not utilize the HP201's specialized neuromorphic computation instruction interface. Therefore, the power consumption does not reflect the true power usage of SNN on neuromorphic processor. We found that with a standby power consumption of approximately 14.5W for the entire chip, the ANN network, while having a lower base power consumption during inference, exhibited significant peak power fluctuations. In contrast, the SNN had a slight higher base power consumption due to iterative activations but showed more stable power variations. The MP PointNet achieved both lower base power consumption and stable peak power consumption.
\subsection{Point Cloud Analysis}
\subsubsection{Classification} 
We tested the original Spiking PointNet on more datasets and expanded it onto single scale grouping (SSG) version of PointNet++ in \cref{class_table}. Spiking PointNet seems to be overefitting on the OBJ\_BG variant of ScanObjectNN main split when $T=1$. Although AMP2 did not enable the SNN to directly outperform the original PointNet with the same structure, it brought significant performance improvements regardless of whether the backbone was PointNet or PointNet++. Moreover, AMP2 outperformed the recent voxel-based SNN state-of-the-art model, E-3DSNN with MP PointNet2.
% spiking pointnet很容易过拟合；Spiking PointNet的原始论文只对pointnet 架构的模型在ModelNet10和40上测试，我们把这一工作拓展到PointNet++架构，在ScanObjectNN的两个variant上都做了测试，并且证明出多时间步迭代策略的效果
\begin{table*}[t]
\caption{Object recognition on classification benchmarks. \textbf{$^{*}$} indicates that the results for the model are extracted from the corresponding original paper.}
\label{class_table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{cccccccccc}
\toprule
\multirow{2}{*}{Backbone} & \multirow{2}{*}{Model} & \multirow{2}{*}{Type} & \multirow{2}{*}{T} & \multicolumn{2}{c}{ModelNet40} & \multicolumn{2}{c}{ScanObjectNN(OBJ\_BG)} & \multicolumn{2}{c}{ScanObjectNN(PB\_T50\_RS)} \\
 & & & & OA & mACC & OA & mACC & OA & mACC\\
\midrule
\multirow{6}{*}{PointNet} & \cite{Pytorch_Pointnet_Pointnet2} & ANN & - & 90.43 & 86.66 & 68.40 & 62.38 & 55.87 & 50.24\\
\cline{2-10}
& \multirow{3}{*}{Spiking PointNet} & \multirow{3}{*}{SNN} & 1 & 86.59 & 81.34 & 24.84 & 26.51 & 52.32 & 47.59\\
& & & 4 & 88.41 & 83.43 & - & - & - & -\\
& & & 8 & 88.50 & 84.73 & - & - & - & -\\
\cline{2-10}
& MP PointNet & SNN & - & 89.56 & 84.91 & 64.46 & 60.51 & 55.19 & 50.79\\
& \cite{lan2023efficient}$^{*}$ & SNN & 16 & 88.17 & - & 66.56 & - & - & -\\
\cline{1-10}
\multirow{6}{*}{PointNet++} & SSG \cite{Pytorch_Pointnet_Pointnet2} & ANN & - &92.33 & 90.07 & 84.46 & 82.62 & 78.83 & 77.24\\
\cline{2-10}
& \multirow{3}{*}{Spiking PointNet2} & \multirow{3}{*}{SNN} & 1 & 87.36 & 82.58 & 48.01 & 43.77 & 66.74 & 63.73 \\
& & & 4 & 91.22 & 87.43 & - & - & - & - \\
& & & 8 & 91.61 & 88.59 & - & - & - & - \\
\cline{2-10}
& MP PointNet2 & SNN & - & 92.36 & 90.09 & 85.58 & 82.13 & 78.24 & 76.19\\
& \cite{lan2023efficient}$^{*}$ & SNN & 16 & 89.45 & - & 69.22 & - & - & -\\
\cline{1-10}
- & P2SResLNet$^{*}$ & SNN & 1 & 90.60 & 89.20 & 81.20 & 79.40 & - & -\\
- & E-3DSNN$^{*}$ & SNN & 1 & 91.70 & - & - & - & - & -\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table*}
\subsubsection{Part Segmentation} 
% 把Spiking PointNet拓展到了物体分割任务
\begin{table}[htb]
\caption{Performance comparison on the ShapeNetPart dataset for part segmentation. The evaluation metrics is mean Intersection over Union (mIoU, \%), including the best instance mIoU, best category mIoU, and per-category mIoU achieved by model with the best instance mIoU. We compare PointNet, Spiking PointNet, and MP PointNet with our proposed AMP2. Full results can be found in \cref{ShapeNet-Part_table_supple}}
\label{ShapeNet-Part_table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{cccc}
\toprule
mIoU & PointNet & Spiking PointNet & MP PointNet\\
\midrule
Instance & 84.48 & 78.46 & 82.66\\
Category & 80.61 & 72.76 & 78.53\\
% \midrule
% Airplane & 82.89 & 73.94 & 79.86\\
% Bag & 77.44 & 64.75 & 73.71\\
% Cap & 81.52 & 72.50 & 77.06\\
% Car & 77.18 & 63.89 & 73.10\\
% Chair & 90.17 & 85.81 & 88.67\\
% Earphone & 71.07 & 67.89 & 71.36\\
% Guitar & 90.93 & 83.23 & 89.42\\
% Knife & 86.39 & 80.69 & 83.31\\
% Lamp & 80.68 & 73.58 & 78.13\\
% Laptop & 95.49 & 93.91 & 94.63\\
% Motorbike & 65.60 & 50.87 & 63.36\\
% Mug & 93.19 & 83.33 & 91.27\\
% Pistol & 82.41 & 74.90 & 78.93\\
% Rocket & 54.21 & 47.98 & 54.48\\
% Skateboard & 74.32 & 63.72 & 71.91\\
% Table & 82.60 & 78.99 & 81.78\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
In \cref{ShapeNet-Part_table}, we also found that AMP2 directly brought about a 6\% improvement in object segmentation for Spiking PointNet on the ShapeNet-Part dataset, although there is still a 2\% gap compared to PointNet.
\subsubsection{Semantic Segmentation} 
On the 3D indoor parsing dataset (S3DIS), the performance gap between MP PointNet and PointNet further narrowed to less than 2\%, with MP PointNet outperforming Spiking PointNet by a margin of 6\% to 8\%.
% 把Spiking PointNet拓展到了物体分割任务
\begin{table}[htb]
\caption{Best accuracy (\%) and mIoU (\%) on the 3D Indoor Parsing Dataset (S3DIS). The per-category IoU values were achieved by model with the highest category mIoU. Full results can be found in \cref{S3DIS_table_supple}}.
\label{S3DIS_table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{Metric} & PointNet & Spiking PointNet & MP PointNet\\
\midrule
\multirow{2}{*}{Acc.} & instance& 79.59 & 70.66 & 78.48\\
 & class & 54.38 & 46.10 & 53.04\\
\midrule
IoU & Mean & 43.20 & 34.53 & 41.98\\

% \cline{2-5}
% & ceiling & 88.5 & 81.3 & 87.6\\
% & floor & 97.6 & 88.3 & 96.5\\
% & wall & 70.2 & 58.7 & 70.3\\
% & beam & 0.1 & 0 & 0.1\\
% & column & 8.1 & 2.8 & 0.6\\
% & window & 50.9 & 34.1 & 47.6\\
% & door & 8 & 12.6 & 11.3\\
% & table & 54.6 & 40.1 & 55.5\\
% & chair & 43.8 & 40.6 & 45.6\\
% & sofa & 16.3 & 7.9 & 8.5\\
% & bookcase & 54.3 & 38.3 & 51.9\\
% & board & 37.2 & 19.7 & 38.0\\
% & clutter& 32.1 & 24.5 & 32.3\\

% \multirow{14}{*}{IoU} & Mean & 43.20 & 34.53 & 41.98\\
% \cline{2-5}
% & ceiling & 88.5 & 81.3 & 87.6\\
% & floor & 97.6 & 88.3 & 96.5\\
% & wall & 70.2 & 58.7 & 70.3\\
% & beam & 0.1 & 0 & 0.1\\
% & column & 8.1 & 2.8 & 0.6\\
% & window & 50.9 & 34.1 & 47.6\\
% & door & 8 & 12.6 & 11.3\\
% & table & 54.6 & 40.1 & 55.5\\
% & chair & 43.8 & 40.6 & 45.6\\
% & sofa & 16.3 & 7.9 & 8.5\\
% & bookcase & 54.3 & 38.3 & 51.9\\
% & board & 37.2 & 19.7 & 38.0\\
% & clutter& 32.1 & 24.5 & 32.3\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\subsection{Event Cloud Analysis}
\subsubsection{Event frame recognition}
% 把event cloud分成frame有多少分，T就是多少，要交代清楚T在这里和其他任务下不同的含义
\begin{table}[t]
    \caption{Improvement of AMP2 on framed DVS128 Gesture dataset}
    \label{framedDVS_table}
    \vskip 0.15in
    \begin{center}
    \begin{small}
    \begin{tabular}{cccc}
    \toprule
    Backbone & AMP2 & Top1 Acc.($\%$) & Top5 Acc.($\%$)\\
    \midrule
    \multirow{2}{*}{SEWResNet} & $\checkmark$ & 90.63 & 99.65 \\
                               & $\times$     & 88.19 & 100.0 \\
    \hline
    \multirow{2}{*}{PlainNet}  & \checkmark & 80.56 & 99.31 \\
                               & $\times$     & 75.0 & 99.65 \\
    \hline
    \multirow{2}{*}{SpikingResNet} & \checkmark & 17.01& 49.65 \\
                                   & $\times$     & 71.88 & 99.31 \\         
    \bottomrule
    \end{tabular}
    \end{small}
    \end{center}
    \vskip -0.1in
\end{table}
In models analyzing event frame data, the raw event stream needs to be integrated into frames with a number consistent with the timesteps. This pixelization of DVS data is a commonly used processing method within the community. The length of the timesteps directly affects the quality of the frame data integrated from the raw event stream; naturally, longer timesteps will convert into richer pseudo-pixel data. Due to this, the input data will vary significantly under different timesteps. We uniformly use T=4 and employ the small versions of three models from \cite{fang2021deep}: SEWResNet, PlainNet, and SpikingResNet, as the subjects of our experiments. We compare the impact of AMP2 on the data sampled from the DVS128Gesture frames, with the results shown in \cref{framedDVS_table}. It can be observed that, apart from SpikingResNet, the other two models benefit from the performance improvements brought by the introduction of AMP2.
% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}
\subsubsection{Event Point Recognition}
% 参考的代码是什么，处理数据的参数是什么，保存的点的数量; 11类我们选择去掉随机类，只保留有明确意义的10个类
\begin{table}[b]
\caption{Classification accuracies for 10-class DVS128 Gesture.}
\label{dvs128_table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{cccccc}
\toprule
Backbone & Type & Model & T & OA & mACC \\
\midrule
\multirow{5}{*}{PointNet} & \multirow{2}{*}{ANN} & \cite{Pytorch_Pointnet_Pointnet2} & - & 89.57 & 89.21\\
& & \cite{wang2019space} & - & 89.68 & -\\
\cline{2-6}
& \multirow{3}{*}{SNN} & \multirow{2}{*}{Spiking PointNet} & 1 & 31.22 & 32.58\\
& & & 4 & 91.32 & 90.15 \\
\cline{3-6}
& & MP PointNet & - & 89.64 & 89.49\\
\cline{1-6}
\multirow{5}{*}{PointNet++} & \multirow{2}{*}{ANN} & \cite{Pytorch_Pointnet_Pointnet2} & - & 96.74 & 95.85\\
& & \cite{wang2019space} & - & 95.89 & -\\
\cline{2-6}
& \multirow{3}{*}{SNN} & \multirow{2}{*}{Spiking PointNet2} & 1 & 85.00 & 84.57\\
& & & 4 & 94.68 & 94.25 \\
\cline{3-6}
& & MP PointNet2 & - & 96.35 & 96.33\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
Using the sliding window method from \cite{wang2019space}, we set the window size to 0.5 and the step size to half of the window size, 0.25. To avoid the coupling of timesteps and input, we segmented the raw event streams into a complete set of event clouds, downsampling 1024 event points from each event stream and discarding the random class from the DVS128 Gesture dataset. We tested the PointNet model's performance in recognizing event clouds across 10 distinct categories. As shown in \cref{dvs128_table}, AMP2 achieves competitive performance with ANN under the same model structure. Although it lags behind the T=4 Spiking PointNet, it surpasses it in the PointNet++ architecture.
\subsection{Ablation}
\subsubsection{Spiking PointNet T = 1,4,8} 
% Spiking PointNet and AMP2, Model including PointNet and PointNet++_ssg
\begin{figure}[ht]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figure/snn_1_4_8_MPpointnet_instance.pdf}}
\caption{Classification accuracy of PointNet-based models on the ModelNet40 without voting. The models compared are Spiking PointNet (SNN baseline, $T=1, 4, 8$ and MP PointNet (using AMP2)}
\label{pn_1_4_8_figure}
\end{center}
\vskip -0.1in
\end{figure}
\begin{figure}[htb]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{figure/snn_1_4_8_MPpointnet2_instance.pdf}}
\caption{Classification accuracy of PointNet++(ssg) based models on the ModelNet40 without voting. The models compared are Spiking PointNet2 (SNN baseline, $T=1, 4, 8$ and MP PointNet2 (using AMP2)}
\label{pn2_1_4_8_figure}
\end{center}
\vskip -0.1in
\end{figure}
% \subsubsection{alpha}
% 对比不同的alpha 差异不大 但都比0强
% alpha的改变都是在sum上做的
Out of curiosity, for the proposed Spiking PointNet, we compared the traditional timestep-wise activation with the AMP2 method on PointNet (\cref{pn_1_4_8_figure}) and PointNet++ (\cref{pn2_1_4_8_figure}). The training gradients of SNN in the PointNet architecture are extremely unstable, and even with T=8, there is no significant improvement compared to T=1. However, PointNet++ clearly demonstrates the benefits of longer timesteps, although neither can surpass AMP2.
\subsubsection{Residual Connection}
\label{ablation_section}
\begin{table}[ht]
\caption{Classification accuracy (\%) of residual connection and AMP2 on PointNet architecture, evaluated on ModelNet40.
}
\label{rc_table}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{cccc}
\toprule
Model & T & OA & mACC \\
\midrule
MP PointNet & - & 89.56 & 84.91\\
\multirow{2}{*}{SpikeRC PointNet} & 1 & 65.90& 61.70\\
 & 4 & 88.78 & 84.46\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
% \bibliography{example_paper}
We replaced AMP2 with residual connections in PointNet, resulting in SpikeRC PointNet. We tested it with T=1 and T=4, and the results after training for 200 epochs on the ModelNet40 dataset are reported in \cref{rc_table}. AMP2 slightly outperforms the T=4 SpikeRC PointNet.
\section{Conclusions}
In this work, we propose a novel, simple yet highly effective update strategy for spiking activation functions in spiking neural networks, termed activation-wise membrane potential propagation (AMP2). Unlike the traditional approach of iteratively updating membrane potentials and generating spike values at each timestep, AMP2 outputs the accumulated membrane potential prior to spike generation and directly transmits the post-spike membrane potential to the next layer's activation function. This method transforms the sparse analog values from temporal computation to spatial computation, allowing activation functions within the same module to share a unified timestep parameter. This parameter is no longer manually designed but is instead bound to the network's depth. 

Through extensive experiments, we demonstrate that this strategy significantly improves performance in both point cloud analysis and DVS event cloud recognition. For instance, AMP2 achieves competitive performance with ANN models and outperforms recent voxel-based SNN models like E-3DSNN. Additionally, AMP2 shows substantial improvements in object segmentation on the ShapeNet-Part dataset and narrows the performance gap with PointNet on the S3DIS dataset. However, we also observe that AMP2 does not universally enhance performance across all model structures. For example, in Spiking ResNet, AMP2 leads to a performance decline. This raises intriguing questions about the applicability of AMP2 in traditional visual data and other model architectures. Future work will explore the broader applicability of AMP2 across various model structures and datasets to further validate its effectiveness and identify potential limitations.
\clearpage
\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Supplementary results on ShapeNet-Part}
\label{ShapeNet-Part_table_supple}
\begin{table}[htb]
\caption{Performance comparison on the ShapeNetPart dataset for part segmentation. The evaluation metrics is mean Intersection over Union (mIoU, \%), including the best instance mIoU, best category mIoU, and per-category mIoU achieved by model with the best instance mIoU. We compare PointNet, Spiking PointNet, and MP PointNet with our proposed AMP2.}
% \label{}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{cccc}
\toprule
mIoU & PointNet & Spiking PointNet & MP PointNet\\
\midrule
Instance & 84.48 & 78.46 & 82.66\\
Category & 80.61 & 72.76 & 78.53\\
\midrule
Airplane & 82.89 & 73.94 & 79.86\\
Bag & 77.44 & 64.75 & 73.71\\
Cap & 81.52 & 72.50 & 77.06\\
Car & 77.18 & 63.89 & 73.10\\
Chair & 90.17 & 85.81 & 88.67\\
Earphone & 71.07 & 67.89 & 71.36\\
Guitar & 90.93 & 83.23 & 89.42\\
Knife & 86.39 & 80.69 & 83.31\\
Lamp & 80.68 & 73.58 & 78.13\\
Laptop & 95.49 & 93.91 & 94.63\\
Motorbike & 65.60 & 50.87 & 63.36\\
Mug & 93.19 & 83.33 & 91.27\\
Pistol & 82.41 & 74.90 & 78.93\\
Rocket & 54.21 & 47.98 & 54.48\\
Skateboard & 74.32 & 63.72 & 71.91\\
Table & 82.60 & 78.99 & 81.78\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Supplementary results on S3DIS}
\label{S3DIS_table_supple}
\begin{table}[htb]
\caption{Best accuracy (\%) and mIoU (\%) on the 3D Indoor Parsing Dataset (S3DIS). The per-category IoU values were achieved by model with the highest category mIoU.}
\vskip 0.1in
\begin{center}
\begin{small}
\begin{tabular}{ccccc}
\toprule
\multicolumn{2}{c}{Metric} & PointNet & Spiking PointNet & MP PointNet\\
\midrule
\multirow{2}{*}{Acc.} & instance& 79.59 & 70.66 & 78.48\\
 & class & 54.38 & 46.10 & 53.04\\
\midrule
\multirow{14}{*}{IoU} & Mean & 43.20 & 34.53 & 41.98\\
\cline{2-5}
& ceiling & 88.5 & 81.3 & 87.6\\
& floor & 97.6 & 88.3 & 96.5\\
& wall & 70.2 & 58.7 & 70.3\\
& beam & 0.1 & 0 & 0.1\\
& column & 8.1 & 2.8 & 0.6\\
& window & 50.9 & 34.1 & 47.6\\
& door & 8 & 12.6 & 11.3\\
& table & 54.6 & 40.1 & 55.5\\
& chair & 43.8 & 40.6 & 45.6\\
& sofa & 16.3 & 7.9 & 8.5\\
& bookcase & 54.3 & 38.3 & 51.9\\
& board & 37.2 & 19.7 & 38.0\\
& clutter& 32.1 & 24.5 & 32.3\\
\bottomrule
\end{tabular}
\end{small}
\end{center}
\vskip -0.1in
\end{table}
\section{Illustration of AMP2}
\begin{figure}[t]
\vskip 0.1in
\begin{center}
\centerline{\includegraphics[width=0.5\textwidth]{figure/spiking2.png}}
\caption{Comparison of timestep-wise iterative activation and Activation-wise Membrane Potential Propagation in a 3-layer MLP. The left side illustrates the commonly used MP update method, which requires $T$ computations within the LIF for MP updates. The right side shows our proposed AMP2 update strategy, where the post-spike membrane potential $H_{l}$ of the LIF within the module is propagated forward between activation layers. Unlike the traditional method where each activation function has its own separate timestep computations, we extend the timestep across the network depth, allowing activation functions at different positions within the same module to share a single timestep parameter.}
\label{spiking2_figure}
\end{center}
\vskip -0.1in
\end{figure}
\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
