@ARTICLE{MS_ResNet,
  author={Hu, Yifan and Deng, Lei and Wu, Yujie and Yao, Man and Li, Guoqi},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Advancing Spiking Neural Networks Toward Deep Residual Learning}, 
  year={2024},
  volume={},
  number={},
  pages={1-15},
  keywords={Degradation;Training;Task analysis;Neurons;Neuromorphics;Computer architecture;Computational modeling;Degradation problem;neuromorphic computing;residual neural network;spiking neural network (SNN)},
  doi={10.1109/TNNLS.2024.3355393}}

@inproceedings{amir2017low,
  title={A low power, fully event-based gesture recognition system},
  author={Amir, Arnon and Taba, Brian and Berg, David and Melano, Timothy and McKinstry, Jeffrey and Di Nolfo, Carmelo and Nayak, Tapan and Andreopoulos, Alexander and Garreau, Guillaume and Mendoza, Marcela and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={7243--7252},
  year={2017}
}

@inproceedings{lan2023efficient,
  title={Efficient converted spiking neural network for 3d and 2d classification},
  author={Lan, Yuxiang and Zhang, Yachao and Ma, Xu and Qu, Yanyun and Fu, Yun},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={9211--9220},
  year={2023}
}

@inproceedings{qiu2024efficient,
  title={Efficient 3D Recognition with Event-driven Spike Sparse Convolution},
  author={Qiu, Xuerui and Yao, Man and Zhang, Jieyuan and Chou, Yuhong and Qiao, Ning and Zhou, Shibo and Xu, Bo and Li, Guoqi},
  booktitle={Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence},
  year={2025}
}

@article{ren2023spikepoint,
  title={Spikepoint: An efficient point-based spiking neural network for event cameras action recognition},
  author={Ren, Hongwei and Zhou, Yue and Huang, Yulong and Fu, Haotian and Lin, Xiaopeng and Song, Jie and Cheng, Bojun},
  journal={arXiv preprint arXiv:2310.07189},
  year={2023}
}

@inproceedings{ren2023ttpoint,
  title={Ttpoint: A tensorized point cloud network for lightweight action recognition with event cameras},
  author={Ren, Hongwei and Zhou, Yue and Fu, Haotian and Huang, Yulong and Xu, Renjing and Cheng, Bojun},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={8026--8034},
  year={2023}
}

@inproceedings{ren2024spiking,
author = {Ren, Dayong and Ma, Zhe and Chen, Yuanpei and Peng, Weihang and Liu, Xiaode and Zhang, Yuhan and Guo, Yufei},
title = {Spiking PointNet: spiking neural networks for point clouds},
year = {2023},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Recently, Spiking Neural Networks (SNNs), enjoying extreme energy efficiency, have drawn much research attention on 2D visual recognition and shown gradually increasing application potential. However, it still remains underexplored whether SNNs can be generalized to 3D recognition. To this end, we present Spiking PointNet in the paper, the first spiking neural model for efficient deep learning on point clouds. We discover that the two huge obstacles limiting the application of SNNs in point clouds are: the intrinsic optimization obstacle of SNNs that impedes the training of a big spiking model with large time steps, and the expensive memory and computation cost of PointNet that makes training a big spiking point model unrealistic. To solve the problems simultaneously, we present a trained-less but learning-more paradigm for Spiking PointNet with theoretical justifications and in-depth experimental analysis. In specific, our Spiking PointNet is trained with only a single time step but can obtain better performance with multiple time steps inference, compared to the one trained directly with multiple time steps. We conduct various experiments on ModelNet10, ModelNet40 to demonstrate the effectiveness of Spiking PointNet. Notably, our Spiking PointNet even can outperform its ANN counterpart, which is rare in the SNN field thus providing a potential research direction for the following work. Moreover, Spiking PointNet shows impressive speedup and storage saving in the training phase. Our code is open-sourced at https://github.com/DayongRen/Spiking-PointNet.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {1811},
numpages = {12},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@inproceedings{sekikawa2019eventnet,
  title={Eventnet: Asynchronous recursive event processing},
  author={Sekikawa, Yusuke and Hara, Kosuke and Saito, Hideo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={3887--3896},
  year={2019}
}

@article{shen2023eventmix,
  title={Eventmix: An efficient data augmentation strategy for event-based learning},
  author={Shen, Guobin and Zhao, Dongcheng and Zeng, Yi},
  journal={Information Sciences},
  volume={644},
  pages={119170},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{thomas2019kpconv,
  title={Kpconv: Flexible and deformable convolution for point clouds},
  author={Thomas, Hugues and Qi, Charles R and Deschaud, Jean-Emmanuel and Marcotegui, Beatriz and Goulette, Fran{\c{c}}ois and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={6411--6420},
  year={2019}
}

@inproceedings{wang2019space,
  title={Space-time event clouds for gesture recognition: From RGB cameras to event cameras},
  author={Wang, Qinyi and Zhang, Yexin and Yuan, Junsong and Lu, Yilong},
  booktitle={2019 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={1826--1835},
  year={2019},
  organization={IEEE},
  url={https://github.com/qwang014/EVclouds_gesture_recognition}
}

@article{wang2020st,
  title={ST-EVNet: Hierarchical spatial and temporal feature learning on space-time event clouds},
  author={Wang, Qinyi and Zhang, Yexin and Yuan, Junsong and Lu, Yilong},
  journal={Proc. Adv. Neural Inf. Process. Syst.(NeurlIPS)},
  year={2020}
}

@inproceedings{wu2024point,
  title={Point-to-Spike Residual Learning for Energy-Efficient 3D Point Cloud Classification},
  author={Wu, Qiaoyun and Zhang, Quanxiao and Tan, Chunyu and Zhou, Yun and Sun, Changyin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={6},
  pages={6092--6099},
  year={2024}
}

@article{yao2023sparser,
  title={Sparser spiking activity can be better: Feature Refine-and-Mask spiking neural network for event-based visual recognition},
  author={Yao, Man and Zhang, Hengyu and Zhao, Guangshe and Zhang, Xiyu and Wang, Dingheng and Cao, Gang and Li, Guoqi},
  journal={Neural Networks},
  volume={166},
  pages={410--423},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{yao2024spikedriven,
title={Spike-driven Transformer V2: Meta Spiking Neural Network Architecture Inspiring the Design of Next-generation Neuromorphic Chips},
author={Man Yao and JiaKui Hu and Tianxiang Hu and Yifan Xu and Zhaokun Zhou and Yonghong Tian and Bo XU and Guoqi Li},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=1SIBN5Xyw7}
}

