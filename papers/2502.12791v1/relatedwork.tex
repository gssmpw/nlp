\section{Related Work}
\label{related}
In this section, we review representative works that analyze 3D data by cross-domain inspiration, applied in recognition tasks including action detection, hand gesture recognition, human pose estimation, as well as the analysis of 3D shapes of synthesized and real-world objects and scenes. Given a group of event stream containing $I$ events, Event streams $E$, in other words, event points $\left\{e_i\right\}^{I}_{i=1}$ is represented as:
\begin{equation}\label{dvs_data}
    E = \left\{e_i\right\}^{I}_{i=1} = \left\{t_i,x_i,y_i,p_i \right\}_{i=1}^I
\end{equation} 
Where $(x_i,y_i)$ denotes the position of pixel where the $i$th event occurs, $t_i$ records the timestamp of the $i$th event, and $p_i$ indicates changes of light intensity. 

As frame-captured RGB image and event-recorded DVS data are closely related, the utilization of event-driven data in practical analysis also has been technically categorized into two approaches: using the event point and compressing events into frames. Point-based network takes carefully selected events $(x,y,t)$ as input points, whereas frame-based method needs to densify $I$ events into $N$ images by integrating points through $t$. Frame representations of event clouds can be easily transferred back into non-event feature extractors mentioned in \cref{Intro}. Because event clouds $\{e_i\}_{i=1}^I$ will be replaced by the virtual image sets in shape of $\left\{T_i, C_i, H_i, W_i\right\}_{i=1}^N$, at the expense of response time, massive data volumes and increased computational costs. Regardless of drawbacks, this strategy still receives wide application in community for its convenience. EventMix \cite{shen2023eventmix} designed an efficient 3D mask strategy to achieve data augmentation on event frames.
\subsection{Event Point-based Recognition}
% 不要代入SNN视角讲，而是DVS视觉上出发,总览SNN和ANN的相关工作的具体贡献
% frame-based or point based
To fully explore the space-time sparsity and structure of the event points, \cite{wang2019space} treated event set $E$ as space-time event cloud $P$, regarding the timestamp $t_i$ as a numerical measurement instead of sequential information. They trained a PointNet++ classifier using a rolling buffer mechanism in an end-to-end way. This strategy outperformed the CNN approach \cite{amir2017low} achieving an accuracy of $97.08\%$ compared to $96.49\%$ on the 10-category recognition task of DVS128 Gesture. Meanwhile, EventNet \cite{sekikawa2019eventnet}, a recursive version of PointNet, employed a temporal encoding layer prior to the symmetry function. EventNet demonstrated impressive results in ego-motion estimation and scene segmentation of event cameras. Still following PointNet design, the sequential relationship of event points was considered in ST-EVNet \cite{wang2020st}. Therefore, ST-EVNet abandoned MLPs and instead adopted space-time convolution kernels to learn local temporal features, resulting in a lower accuracy of $90.52\%$ on DVS128 Gesture but with lower FLOPs per prediction. Subsequently, TTPoint \cite{ren2023ttpoint} argued that previous point-based studies suffer from inefficient sampling. It further improved on PointNet++ by stacking residual blocks within its extractors, achieving a performance of $98.8\%$. A spiking version of TTPoint, named SpikePoint \cite{ren2023spikepoint} was developed shortly after, reporting a competitive accuracy of $98.74\%$ with $16$ timesteps on the same benchmark.
% ANN or SNN
\subsection{3D Recognition with SNN}
Compared to 2D images, the sparse nature of point cloud poses more challenging obstacles when applying SNN. That is to say, larger timesteps are required to accumulate enough spikes during the forward propagation of training. The breakthrough in spike-driven analysis of point clouds was made by \cite{lan2023efficient} and \cite{ren2024spiking}, which almost simultaneously obtained spiking versions of PointNet through ANN-SNN conversion and direct training based on surrogate gradients, respectively. Inspired by adaptively adjusting spiking parameters for image classification, the former implemented an adaptive threshold and activation mechanism in ANN-SNN conversion for PointNet, while the latter developed a trained-less but learning-more method for timestep decision to mitigate the increasing time cost of training. Furthermore, 
% this direct-training Spiking PointNet implied that using Membrane Potential Perturbation (MPP) with short timesteps can achieve performance close to that of long timesteps. 
this work suggests that the iterative computation with long timesteps essentially enhances the model's generalization ability. \cite{wu2024point} introduced a residual network, P2SResLNet, that combined integrate-fire (IF) neurons and 3D kernel point convolution of Convoke \cite{thomas2019kpconv}. However, P2SResLNet failed to match the performance of its baseline KPConv and most ANNs on ModelNet40 and ScanObjectNN. Besides point-based networks, E-3DSNN \cite{qiu2024efficient} explored sparse spike convolution on voxels, imitating the aforementioned MS-ResNet \cite{MS_ResNet}. E-3DSNN achieved unprecedented results on ModelNet40, surpassing all three point-based SNN baselines for the first time.
\subsection{Unified Methods for 3D cloud Analysis}
Most importantly, both \cite{lan2023efficient} and E-3DSNN follow the philosophy of unified design for multi-task learning. They rethink network design not only for point clouds but also for other data types. The conversion method was universal, enabling the acquisition of SNN versions of PointNet, PointNet++, VGG-16 and ResNet-20. E-3DSNN effectively applied 2D convolution to both 3D voxels and 2D pixels of event streams. We also adhere to the philology of unified design for both event clouds and point clouds. 
% Intro主要讲的是Point Cloud和event data各自是如何发展，难以察觉的问题，以及二者可以结合起来研究，提出想法能不能用同一种统一的策略既能避免传统PointNet网络愈发复杂，避免SNN时间步导致数据维度增加。
% 不写什么是LIF IF等最基础的认知，直接从SNN的视角开始介绍点云和DVS数据的研究

% 比如

% \subsection{Conventional perspectives}
% （简短）传统依据卷积 残差 自注意力等传统机制研究DVS任务 比如已经提到过的 spike driven transformer v1v2v3\cite{yao2024spikedriven} MS-ResNet \cite{MS_ResNet} RM-SNN \cite{yao2023sparser}用实际数据说明 更大的时间步对效果和计算延迟有多少影响

% \subsection{Perspectives from point cloud}
% (详细) 解释这几篇跟我们最相关的工作 以及遵循的是什么范式，比如point based的方案肯定都是基于pointnet和pointnet++

% 单一领域：只做点云或者只做DVS
% voxel based point cloud class through direct training SNN conv: E-3DSNN \cite{qiu2024efficient};
% point based point cloud class through direct training SNN conv: P2SResLNet \cite{wu2024point};
% point based point cloud class through direct training SNN MLP: Spiking PointNet \cite{ren2024spiking}
% point based DVS data class through direct training SNN MLP: SpikePoint \cite{ren2023spikepoint}
% 多领域统一：同时做点云和DVS的实验 (跟我们一致)
% point based DVS and point cloud through conversion SNN: \cite{lan2023efficient}
% 一定要让这部分的逻辑清晰