% 重新呼应一下交换性，因此我们主要提出了条件的可交换性和回答的可交换。提的时候最好能从intro中摘除原话来稍作展开，不要重新组织语言。
% 3.1.1和3.1.2合并，independent premises添加几个引用
% 3.2.3压缩成两段，3.2

% 先给出背景的形式化定义
% Due to the commutativity of the premises, swapping independent premises can equal to the same reasoning process.
% Hence, we 扰动xxx

In this section, we introduce condition order augmentation in Sec. \ref{sec:Condition Order Augmentation} and answer order augmentation in Sec. \ref{sec:Answer order Augmentation}. The framework is shown in Fig. \ref{fig:method}.

% 生成条件乱序的数据
\subsection{Condition Order Augmentation}
\label{sec:Condition Order Augmentation}
Due to the commutativity of premises, swapping independent premises results in the same solution. Hence, we perturb the order of premises, enabling models to learn the logical equivalence of condition reordering.

% 先介绍把条件转变成原子条件
\subsubsection{Shuffling the Order of Premises}
Given a logical reasoning dataset \( D_C = \{P, C, L\} \), we first extract the premise set \( P = \{P_1, P_2, \dots, P_n\} \). To generate augmented data, we apply a random permutation \( \sigma \) to the premise set \( P \), producing a new ordered premise set \( P_{ran} \). Specifically:
\[ P_{ran} = \{P_{\sigma(1)}, P_{\sigma(2)}, \dots, P_{\sigma(n)}\} \]
For example, if the original order is \( [P_1, P_2, P_3, P_4, \dots, P_n] \), after applying the permutation \( \sigma \), the new order might be \( [P_3, P_4, P_1, P_n, \dots, P_2, \dots] \).

\subsubsection{Generating Augmented Data}
We denote the original dataset as \( D_C = \{P, C, L\} \) and the augmented dataset as \( D_C' = \{P_{ran}, C, L\} \), where \( P_{ran} \) represents the randomly shuffled premises. The transformation from \( D_C \) to \( D_C' \) involves perturbing the order of the premises while keeping the conclusion \( C \) and the label \( L \) unchanged. Each original data sample generates \( k \) instances of condition order augmentation, leading to an augmented dataset \( D_C' \) containing \( k \times |D_C| \) instances, where \( |D_C| \) is the size of the original dataset.


% 回答步骤增强
% [1] LLM——>补逻辑
% [2] reasoning——>DAG
% [3] 扰动
% follow 3个原则
% Specially，扰动

\subsection{Answer order Augmentation}
\label{sec:Answer order Augmentation}
Due to the commutativity of reasoning steps, we perturb the order of solution steps to help models learn the logical equivalence of the reasoning process. However, reasoning steps often have dependencies, where the execution of one step may rely on the result of another. To address this, we propose a method for identifying valid step reorderings that ensures these dependencies are preserved.

\input{solution_example}
\subsubsection{Leveraging LLMs for Logical Reasoning Solutions}
Since logical reasoning datasets typically provide only a single label (e.g., true/false) without a Chain-of-Thought (CoT) reasoning process, we generate detailed step-by-step reasoning solutions to bridge this gap~\cite{xu2024faithful}. We use LLMs$\footnote{In our experiment, we use GPT-4o-mini.}$ for this process. As shown in Fig. \ref{fig:solution_example}, the methodology consists of three main steps:
(1) For datasets without First-Order Logic (FOL) expressions, We extract their premises and conclusion and convert them into the corresponding FOL representations.
(2) The FOL-augmented premises, along with the ground truth labels, are fed into the model, prompting it to generate a step-by-step solution. Each step must clarify its purpose and reasoning, leading to a final conclusion.
(3) The generated solutions are then reprocessed by the model to extract the premise indices and prerequisite step indices used in each reasoning step.

\subsubsection{Constructing the Step Dependency DAG}
After obtaining the logical reasoning solutions, the current data can be represented as \( D_S = \{P, C, L, S\} \), where \( S = \{S_1, S_2, \dots, S_m\} \) consists of reasoning steps.
We represent \( S \) as a directed acyclic graph (DAG), denoted as \( G = (V, E) \), where \( V = \{S_1, S_2, \dots, S_m\} \) is the set of reasoning steps, and \( E \subseteq V \times V \) is the set of directed edges. An edge \( (S_i, S_j) \) indicates that step \( S_j \) depends on the result of step \( S_i \).

Each step \( S_i \) is represented as a tuple:
\[
S_i = (\text{Goal}_i, \mathcal{P}_{used}^{(i)}, \mathcal{S}_{used}^{(i)}, \text{Result}_i)
\]
where \( \text{Goal}_i \) describes the goal of the step, \( \mathcal{P}_{used}^{(i)} \) represents the directly used atomic premises, \( \mathcal{S}_{used}^{(i)} \subseteq V \) denotes the prerequisite steps that must be executed before \( S_i \), and \( \text{Result}_i \) is the result derived from the execution of \( S_i \). 

\subsubsection{Generating Augmented Solution Sequences}
A valid reasoning process must maintain all logical dependencies between steps while allowing flexibility in ordering interchangeable steps. We define the dependency constraints as follows:
\begin{itemize}
    \item A step \( S_i \) is \textbf{independent} if \( \mathcal{S}_{used}^{(i)} = \emptyset \) (i.e., it has no prerequisite steps).
    \item A step \( S_j \) is \textbf{dependent} if \( \mathcal{S}_{used}^{(i)} \neq \emptyset \), meaning that it requires prior steps to be completed before execution.
    \item Two steps \( S_i \) and \( S_j \) are \textbf{order-invariant} if neither step appears in the other's prerequisite set, i.e., \( S_i \notin \mathcal{S}_{used}^{(j)} \) and \( S_j \notin \mathcal{S}_{used}^{(i)} \).
\end{itemize}

Our goal is to generate a reasoning sequence that integrates all steps while maintaining dependency constraints, based on the principles outlined above.
First, we identify all independent steps where \( \mathcal{S}_{used}^{(i)} = \emptyset \) from the dataset, remove them from the DAG, and add them to the list of feasible step sequences \textit{\(\text{List}\)}. Then, we iterate over all possible combinations of these steps to generate multiple different lists of valid sequences.
Next, for each step still present in the DAG, we iterate through the steps of its \( \mathcal{S}_{used} \). If \( \mathcal{S}_{used} \) contains a step from the current \(\text{List}\), we remove that step from \( \mathcal{S}_{used} \).

We repeat these two steps until every \textit{\(\text{List}\)} contains all the steps from \( S \), resulting in a collection of new valid step sequences. We refer to each newly generated sequence as \( S_{ran} \). The final augmented dataset is represented as \( D_S'
 = \{P, C, L, S_{ran}\} \).

Details of the prompts used in order-centric data augmentation are provided in Appendix \ref{sec:appendix_1}.