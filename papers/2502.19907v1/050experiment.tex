% 介绍4.1和4.2节的内容
We conduct experiments to evaluate the effectiveness of our method, focusing on overall performance, training efficiency, and generalization capability.

\subsection{Experiment Setup}
\paragraph*{Datasets} 
(1) \textbf{FOLIO}~\cite{han2022folio} is a natural language inference dataset annotated with first-order logic (FOL), consisting of 1001 training samples and 231 test samples.  
(2) \textbf{RuleTaker}~\cite{clark2020transformerssoftreasonerslanguage} requires models to determine whether a conclusion is entailed by a set of premises, covering various reasoning difficulties. Due to its large scale, we uniformly sample 1000 training and 1000 test instances across different difficulty levels.  
(3) \textbf{LogicNLI}~\cite{tian-etal-2021-diagnosing} is an NLI-style dataset that isolates first-order logic reasoning from commonsense inference for precise logical evaluation. Similarly, we sample 1000 instances from both its training and test sets.  

\paragraph*{Models} 
We conduct experiments on Llama-3-8B-Instruct~\cite{llama3modelcard}, Llama-2-13B-Chat~\cite{touvron2023llama} and Mistral-7B-Instruct-v0.3~\cite{jiang2023mistral}, evaluating model performance under five training conditions:  
(1) \textbf{Untrained}: The original model without any additional training.  
(2) \textbf{Vanilla SFT}: Models fine-tuned only on the original training set, i.e., \( D_C = \{P, C, L\} \). 
(3) \textbf{Vanilla SFT + Condition Shuffling}: Models trained on both the original dataset and an augmented version with shuffled condition orders, i.e., \( D_C = \{P, C, L\} \) and \( D_C' = \{P_{ran}, C, L\} \). 
(4) \textbf{SFT with COT}: Models fine-tuned with training data that includes Chain-of-Thought (COT) reasoning steps, i.e., \( D_S
 = \{P, C, L, S\} \).  
(5) \textbf{SFT with COT + Answer Steps Shuffling}: A model trained with COT data and additional augmentations with shuffled reasoning steps, i.e., \( D_S
 = \{P, C, L, S\} \) and \( D_S'
 = \{P, C, L, S_{ran}\} \).

All models are trained using full fine-tuning, with a 1:1 mix of ShareGPT~\cite{chiang2023vicuna} in each dataset. Training is conducted on four A100 GPUs for four epochs. Each model is trained exclusively on a single dataset, with augmentation applied only to that dataset, and evaluated on the corresponding test set without cross-dataset mixing.

\input{Data_statistics}
We applied random shuffling to the premises of each training sample to generate one condition-augmented instance. Due to some data containing multiple valid step orderings, we randomly selected one transformation from each original data to control the data size. Additionally, we shuffled the premises in the test set to create a condition shuffled test set, enabling better evaluation of the model's performance across different logical orders. The data sizes for both the training and test sets are provided in Tab. \ref{tab:Data_statistics}.

\input{mian_result_without_both}
\subsection{Overall Performance}
Tab.~\ref{tab:main_result} shows that our method effectively improves model reasoning performance. Compared to Vanilla SFT, condition shuffling significantly enhances performance. LLaMA3-8B-Instruct achieves +11.05\% and +11.5\% improvements on RuleTaker's sequential and shuffled evaluations, respectively. Similarly, Mistral-7B-Instruct-v0.3 shows improvements of 17.85\% and 19.15\% on the two LogicNLI test sets. Overall, the general performance gain ranges from 7\% to 15\%. 

Similarly, adding answer step shuffling further improves performance over COT training. LLaMA3-8B-Instruct achieves +3.55\% and +3.90\% improvements on RuleTaker's sequential and shuffled evaluations. Mistral-7B-Instruct-v0.3 shows improvements of +5.44\% and +5.22\% on FOLIO's sequential and shuffled evaluations. Overall, the average performance gain is between 2\% and 3\%. It is worth noting that LLaMA3-8B-Instruct and LLaMA2-13B-Chat perform worse on the LogicNLI dataset in SFT with COT compared to Vanilla SFT. This could be due to the fact that COT does not always improve model performance across all tasks~\cite{liu2024mind}.

Additionally, LLaMA models generally perform 2-3\% better on sequential evaluations than shuffled ones, highlighting their sensitivity to order. Nonetheless, condition and answer steps shuffling mitigates this effect. In contrast, Mistral-7B maintains stable performance across both settings, sometimes even outperforming sequential evaluations in shuffled tests. 

\input{condition_steps}
\subsubsection{Training Efficiency}
To ensure fairness and exclude the effect of increased data size, we test the accuracy of checkpoints with the same number of training steps, comparing the performance of condition order augmentation with the original order. As shown in Fig. \ref{fig:condition_steps}, even with the same data size, condition shuffle training consistently outperforms the original order, with the performance gap widening as training progresses. This highlights that the improvement in accuracy is due to the augmentation process itself, rather than the increase in data.

