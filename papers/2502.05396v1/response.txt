\section{Related Works}
Prior to introducing our proposed method, we will provide a review of related literature on existing models used in 3D medical imaging segmentation.

The Hough-CNN, proposed by Milletari et al **Milletari, "Hough-CNN for Real-Time Segmentation"** is a typical convolutional neural network designed for 3D medicial imaging segmentations, especially for MRI and Ultrasound. The approach leverages the abstraction capabilities of CNNs and implements Hough voting, which allows for automatic localization and segmentation of anatomies.

The 3D U-Net, proposed by Çiçek et al **Çiçek, "3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation"**, stands out as the first modern model specifically designed for 3D medical imaging segmentations. The architecture of this model is based on the fully-convolutional U-Net proposed by Ronneberger et al. During its inception, the authors acknowledged the unfeasibility and time-consuming nature of labeling a dense ground truth for a 3D volumetric image. As a result, the method employed dense segmentations based on sparse annotations using weighted loss. It should be noted, however, that the lack of publicly available code associated with this work presents a notable limitation. Despite this limitation, the paper's significant influence on subsequent works remains undeniable and has served as a source of inspiration for our proposed method.

The Residual Symmetric 3D U-Net, proposed by Lee et al. **Lee, "Residual Symmetric 3D U-Net: A Deep Neural Network for Medical Image Segmentation"**, was inspired by the ResNet architecture proposed by Kaiming He et al. **Kaiming He, "Deep Residual Learning for Image Recognition"** in 2016. The key innovation of this approach is the introduction of residual connections between each convolutional layer in the 3D U-Net. This method allows for the successful implementation of residual connections in a fully convolutional architecture, enabling the design of deeper encoder-decoder networks that can extract more abstract features while avoiding issues such as gradient explosion and vanishing. The use of residual connections ensures that the network can learn residual mappings that facilitate the optimization process and improve performance. Overall, the Residual Symmetric 3D U-Net represents an important advancement in the field of 3D medical image segmentation by leveraging the benefits of both residual connections and fully convolutional networks.

The nnU-Net, proposed by Isensee et al **Isensee, "nnU-Net: A Deep Learning Approach for Medical Image Segmentation"**. has made significant advancements in the field of 3D medical imaging segmentation, and is widely regarded as the state-of-the-art model in this area. This is due to its innovative pipeline, which builds upon the well-established 3D U-Net architecture. While the backbone of the model remains the same, the authors have introduced novel pre-processing and post-processing techniques, as well as incorporating additional metadata in the widely-used nifti format for CT images. This incorporation of metadata is a significant contribution to the field, as it allows for a more comprehensive understanding of the image and its features. This, in turn, leads to more accurate and reliable segmentation results. The success of nnU-Net has inspired subsequent works, such as CoTr and nnFormer, both of which have also built upon the 3D U-Net architecture. However, nnU-Net remains the most performant model for medical imaging segmentation, due to its ability to leverage metadata and its incorporation of state-of-the-art techniques.


The TransUNet, proposed by Chen et al. **Chen, "TransUNet: A Transformer-Based Model for Medical Image Segmentation"**, is a pioneering approach to medical imaging segmentation using transformer architecture. The model leverages the transformer layer at the end of the CNN encoder, creating a CNN-Transformer hybrid encoder. This is combined with a cascaded upsampler as a decoder to generate masks. The transformer layer enhances the model's capability to capture global context information, while the CNN encoder still plays a crucial role in feature extraction. However, it is important to note that TransUNet is essentially a 2D segmentation method that concatenates 2D images into 3D, which can result in inconsistencies in masks between scans. This can be observed from the sagittal and coronal views. Despite this limitation, TransUNet has laid the foundation for future research for transformers in medical imaging segmentation.



The CoTr is a novel approach that effectively combines convolutional neural network (CNN) and transformer for 3D medical image segmentation. Although the model is based on the 3D U-Net architecture and utilizes the same pre-processing and post-processing methods as nnU-Net, it introduces an additional deformable transformer (DeTrans) encoder. This encoder is designed to capture deeper features from the feature maps produced by the CNN encoder, allowing for better representation of complex image patterns. By bridging the gap between CNNs and transformers, CoTr offers improved performance and greater flexibility in 3D medical image segmentation tasks. The results of the study demonstrate that CoTr outperforms several state-of-the-art models in terms of segmentation accuracy, making it a promising direction for future research in this field.

The nnFormer, proposed by Zhou et al. **Zhou, "nnFormer: A CNN-Transformer Hybrid Model for Medical Image Segmentation"**, is a recent addition to the class of CNN-Transformer hybrid models that has been designed as an extension of the nnUNet architecture. The model consists of several components, including a convolutional embedding layer, a Multi-head Self-attention encoder, and a symmetrical transformer block with a deconvolution-based upsampling layer as the decoder. The convolutional embedding layer maps the input image to a feature space, which is then processed by the Multi-head Self-attention encoder to capture global contextual information. The symmetrical transformer block is responsible for capturing long-range dependencies and generating highly contextualized features. Finally, the deconvolution-based upsampling layer is used to reconstruct the output mask. This model has been shown to be highly effective in 3D medical image segmentation tasks and outperforms several state-of-the-art methods.

The paper "Convolution-Free Medical Image Segmentation using Transformers" by Karimi et al **Karimi, "Convolution-Free Medical Image Segmentation using Transformers"**. presents a convolutional-free architecture for medical image segmentation using transformers. However, this approach cannot been used for thin segmentation using thick annotations, and the lack of published code and dataset raises concerns about the validity of the reported results.

Recently, the Mamba model **Mamba et al., "Mamba: A Fast and Efficient Architecture for Sequence Modeling"** has demonstrated state-of-the-art efficiency in sequence modeling by leveraging selective state-space layers. Its strong long-range dependency modeling makes it well-suited for medical imaging segmentation, enabling frameworks like UMamba and SegMamba to achieve improved feature extraction and boundary delineation.

In general, the related works mentioned above primarily utilize either convolutional architecture or incorporate transformer layers as part of the encoder-decoder architecture. In contrast, our proposed model deviates significantly from this approach by introducing a convolutional-free architecture. This novel approach aims to explore the potential of non-convolutional architectures for medical image segmentation, potentially enabling the development of more efficient and effective models in this domain.