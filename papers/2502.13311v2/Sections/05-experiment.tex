\section{Experiments}

\subsection{Experimental Setup}

\begin{figure*}[t!]
\centering
\includegraphics[width=0.99\textwidth]{Figs/Fig.toc_vanilla.pdf}
\caption{Tutoring outcome curves in Pass rate across various LLM-based tutors with Vanilla Instruct.}
\label{fig:toc_base}
\vspace{-4pt}
\end{figure*}


\paragraph{Benchmark.}
We adopt \textbf{EvoCodeBench} \cite{li2024evocodebench} as the testbed due to its realistic repository-level Python coding tasks along with dependency annotations and repository contexts, providing a rich knowledge foundation for coding tutoring. 
We have 100 target coding tasks and split them equally into 5 folds, using 5-fold cross-validation for experiments. The detailed examples, statistics, and preprocessing are provided in Appendix \ref{appendix:dataset}.


\paragraph{Student Simulator.}
Prior to tutoring, it is essential to ensure simulated students have not been exposed to any target coding task. To avoid data contamination, we use Mixtral-8x7B-Instruct \cite{jiang2024mixtral} as the student simulator. This model's training data only includes content up to \texttt{2023-9}, whereas all coding tasks in EvoCodeBench are collected from the repositories created between \texttt{2023-10} and \texttt{2024-2}. Furthermore, this model has strong conversational and coding abilities, making it well-suited for student simulation.



\paragraph{Backbone Models.}
The backbone models for developing tutor agents are: \textbf{Qwen2-Instruct} \cite{yang2024qwen2} with 7B and 72B variants, \textbf{Llama-3.1-Instruct} \cite{dubey2024llama} with 8B and 70B variants, \textbf{GPT-3.5-Turbo} \cite{openai2022chatgpt}, \textbf{GPT-4o} \cite{openai2024gpt4o}, and \textbf{o1-mini} \cite{openai2024o1}.


\paragraph{Baseline Methods}
We evaluate our \model, against the following relevant baseline methods: (1) \textbf{Vanilla Instruct}: It directly instructs LLMs as tutors, as detailed in \S\ref{sec:tutor_student}. (2) \textbf{Self-Refine}~\cite{madaan2023self}: LLMs generate initial responses and iteratively refine these responses by providing feedback to themselves. (3) \textbf{TreeInstruct}~\cite{kargupta-etal-2024-instruct}: It is a Socratic teaching method that %estimates a studentâ€™s knowledge and 
employs tree-based questioning to guide the student. (4) \textbf{Oracle}: It provides the full task-specific knowledge directly to each student during the post-test (see Eq. (\ref{eq:post_test})), serving as an upper bound.


\paragraph{Implementation Details.}
We implement the verifier in \model using Mistral-7B \cite{jiang2023mistral} with an additional linear layer. We utilize the synthesized dialogues from various backbone LLMs with vanilla instructions for the verifier, where the post-test results on Pass scores provide outcome reward labels. We adopt 5-fold cross-validation for training and evaluation. 
In the \eval evaluation protocol, the maximum number of turns $T$ is set to 8. The cognitive load parameter $M$ is set to 60 during the post-test.
More details about training and inference are provided in Appendix \ref{appendix:implementation}.



\subsection{Experimental Results}

\paragraph{How do various LLMs perform as tutor agents when provided with vanilla instructions?}
As shown in Table~\ref{tab:overall_result}, various LLM-based tutors with vanilla instructions perform significantly inferior to the Oracle tutor, indicating clear limitations. Scaling the parameter size of open-source models like Qwen2 and Llama-3.1 generally improves performance. However, the large gaps in Pass and TOR-Pass scores suggest that simply using larger models is inadequate to guide students in successfully completing target coding tasks. 
These findings indicate that developing effective tutor agents requires not only detailed instructions but also mechanisms to facilitate tutoring outcomes in a structured way.

Another limitation that emerges from these results is the adaptability. 
As shown in Table~\ref{tab:overall_result}, LLMs generally perform better with low-level than with high-level students. This discrepancy arises from the greater difficulty in adapting to higher-level students who require nuanced and targeted guidance. For example, Qwen2-7B-Instruct and GPT-3.5-Turbo show a decline in Pass rates for high-level students after tutoring (i.e., $\Delta\%\text{P}<0$). Figure~\ref{fig:toc_base} illustrates this trend by tutoring outcome curves. Larger models like Qwen2-72B-Instruct and Llama3-3.1-70B-Instruct exhibit significant performance gaps between high-level students and others, even as tutoring dialogues progress. While GPT-4o demonstrates better adaptability, its tutoring outcomes early plateau from the third turn for high-level students, indicating diminishing returns. These results highlight the importance of enhancing tutor agents to adaptively guide different students.


\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{Figs/Fig.toc_comp.pdf}
\caption{Comparison of tutoring outcome curves between the TreeInstruct and \model (Ours).}
\label{fig:toc_comp}
\end{figure}

\begin{table}[t!]
\centering
\scalebox{0.82}{
\begingroup
\renewcommand{\arraystretch}{0.9}
\setlength{\tabcolsep}{3pt}
\hspace{-8pt}
\begin{tabular}{l  cc}
\toprule
\textbf{Backbone Model} &  \textbf{Recall} / \textbf{$\Delta\%$R} & \textbf{Pass} / \textbf{$\Delta\%$P}  \\
\cmidrule(lr){1-1}\cmidrule(lr){2-3}
Llama-3.1-70B-Instruct &  \textbf{66.8}\textsubscript{$\pm~1.3$} / \textbf{45.5}  & \textbf{39.3}\textsubscript{$\pm~6.9$} / \textbf{85.7}  \\
~~ w/o KT & 66.1\textsubscript{$\pm~3.4$} / 43.9  & 35.8\textsubscript{$\pm~3.8$} / 69.2  \\
~~ w/o Verifier  & 66.7\textsubscript{$\pm~3.2$} / 45.3  &  35.1\textsubscript{$\pm~5.1$} / 65.8  \\
\cmidrule(lr){1-1}\cmidrule(lr){2-3}
GPT-4o & \textbf{68.8}\textsubscript{$\pm~3.7$} / \textbf{49.8} &  \textbf{43.7}\textsubscript{$\pm~1.3$} / \textbf{106.5}  \\
~~ w/o KT & 65.9\textsubscript{$\pm~2.2$} / 43.5  & 41.7\textsubscript{$\pm~2.7$} / 97.1   \\
~~ w/o Verifier   &  67.8\textsubscript{$\pm~0.8$} / 47.7  & 39.8\textsubscript{$\pm~3.2$} / 88.2  \\
\bottomrule
\end{tabular}
\endgroup}
\caption{Ablation study results of our \model.}
\label{tab:ablation_result}
\vspace{-4pt}
\end{table}


\paragraph{Can \model improve tutoring outcomes and better adapt to different students?}
As shown in Table~\ref{tab:overall_result}, our \model, built upon Llama-3.1-70B-Instruct, achieves notable improvements over Vanilla Instruct (e.g., from 34.9\% to 39.3\% in Pass rate). When compared to tutor agents built upon GPT-4o, \model achieves the highest overall Recall and Pass rates. 
These results highlight that our approach is more effective in guiding students to successfully complete target coding tasks. More importantly, \model exhibits substantial improvements across students with different levels, narrowing the gap with the Oracle tutor. The tutoring outcome curves in Figure~\ref{fig:toc_comp} further illustrate that, regardless of students' prior knowledge levels, our method consistently improves the success rate of guiding students to achieve task completion. 





\begin{table*}[t!]
\centering
\scalebox{0.82}{
\begingroup
\renewcommand{\arraystretch}{0.8}
\setlength{\tabcolsep}{4pt}
\hspace{-8pt}
\begin{tabular}{l  ccc  ccc  ccc}
\toprule
\multirow{2}{*}{\textbf{Compared Methods}} & \multicolumn{3}{c}{\textbf{Proactivity}} &  \multicolumn{3}{c}{\textbf{Adaptability}} & \multicolumn{3}{c}{\textbf{Coherence}}   \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}\cmidrule(lr){8-10}  
  &  Win (\%) & Lose (\%) & Tie (\%) &  Win (\%) & Lose (\%) & Tie (\%) &  Win (\%) & Lose (\%) & Tie (\%) \\
\cmidrule(lr){1-1}\cmidrule(lr){2-4} \cmidrule(lr){5-7}\cmidrule(lr){8-10}  
\model vs. Vanilla Instruct  &  \textbf{42.2} & 26.7 & 31.1  & \textbf{40.0} & 33.3 &  26.7  & 24.4  & \textbf{25.6}  &  50.0 \\
\model vs. Self-Refine  & \textbf{38.9} & 26.7  &  34.4  & \textbf{41.1}  &  30.0  &  28.9  & \textbf{27.8}  &  18.9  &  53.3  \\
\model vs. TreeInstruct   & \textbf{34.4} & 22.2  &  43.3  &  \textbf{37.8}  & 25.6  &  36.7  &  \textbf{28.9}  &  20.0  & 51.1   \\
\bottomrule
\end{tabular}
\endgroup}
\caption{Human evaluation results. For win and lose percentages, the higher value is bolded.}
\label{tab:human_eval_result}
\vspace{-4pt}
\end{table*}



\paragraph{Ablation study.}
We conduct an ablation study for \model: (1) \textit{\textbf{w/o knowledge tracing (KT)}}, which removes the KT operation prior to tutor utterance generation; and (2) \textit{\textbf{w/o verifier}}, which omits the verifier $V_{\theta}$ used for verification. 
The results in Table~\ref{tab:ablation_result} show that both the KT and verifier contribute to the overall performance. 
Notably, the absence of the verifier leads to a sharp decline in Pass and TOR-Pass, underscoring its critical role. 
The verifier improves the possibility of generated utterances that effectively advance the tutoring progress at each turn, thereby increasing the success rate in guiding students to complete the coding task.

\begin{figure}[t!]
\centering
\includegraphics[width=1\linewidth]{Figs/Fig.pretest.pdf}
\caption{Pre-test performance of simulated students at different levels before tutoring.}
\label{fig:pretest_result}
\vspace{-4pt}
\end{figure}



\subsection{Analysis of Simulated Students}
\label{sec:student_analysis}

Since the \eval evaluation protocol relies on the LLM-simulated students, it is crucial to ensure that these students, with varying levels of prior knowledge, align with discrepancies in completing coding tasks. To validate this, we conducted a pre-test for the simulated students before tutoring, as described in~\S\ref{sec:pre_test}. Figure \ref{fig:pretest_result} shows that simulated low-, medium-, and high-level students exhibit significant performance differences in terms of Recall@$k$ and Pass@$k$. Under a controlled setup, students at different levels demonstrate distinct abilities in completing target coding tasks. Therefore, our student simulation serves as a feasible proxy for real students, offering its advantages of scalability and cost-effectiveness for evaluating tutor agents.


\subsection{Inference-Time Scaling with Verifiers}
\label{sec:scaling_analysis}
Using Llama-3.1-70B-Instruct as the backbone model, we vary the number of candidate tutor utterances per turn, i.e., $N$, within $\{1, 5, 10, 15, 20\}$ and ask the verifier to select the best one based on predicted rewards. 
We also evaluate two baselines: (\romannumeral1) Chain-of-Thought (CoT)~\citep{wei2022chain} prompting for selection (B1) and (\romannumeral2) random selection (B2).
As shown in Figure~\ref{fig:scale}, \model with the trained verifier improves the student's Pass rate from 35.1\% to 39.3\% as $N$ increases, outperforming both the random and CoT baselines while exhibiting stronger linear scaling. 
Additionally, in terms of total tokens (including input prompts and output responses) consumed per tutoring session, \model achieves a better balance between tutoring performance and efficiency. These findings demonstrate that our \model effectively enables inference-time scaling for tutoring agents.



\subsection{Human Evaluation and Case Study}

To evaluate the quality of tutor agents developed by different methods, we conducted a human evaluation for Vanilla Instruct, Self-Refine, TreeInstruct, and our \model. 
We presented human evaluators with a pair of tutoring dialogues produced by two agents interacting with the same student. Evaluators were asked to determine which one is better from \textit{proactivity}, \textit{adaptability}, and \textit{coherence}. Further details are provided in Appendix~\ref{appendix:human_eval}.


Table~\ref{tab:human_eval_result} presents the evaluation results, with an average Fleissâ€™s kappa ($\kappa$) of 0.45, indicating moderate agreement among evaluators (0.41 < $\kappa$ < 0.60). The results demonstrate that our \model significantly outperforms the compared methods in proactivity and adaptability, while also matching or surpassing Vanilla Instruct in coherence. 
We provide several examples in Appendix~\ref{appendix:case}.
To further illustrate the performance of our tutor agents, we provide several examples in Appendix~\ref{appendix:case}.
