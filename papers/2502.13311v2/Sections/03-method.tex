\section{\textbf{Tra}ce-and-\textbf{Ver}ify Agent Workflow}
\label{sec:methods}

We propose \textbf{Tra}ce-and-\textbf{Ver}ify (\textbf{\model}), an effective workflow for developing tutor agents (see the middle part in Figure \ref{fig:overview}). 
\model integrates two key components: (\romannumeral1) explicit tracing of a student's knowledge state and (\romannumeral2) utterance decoding guided by a verifier model for turn-by-turn verification.


\subsection{Adapting to Student's Knowledge via Knowledge Tracing}
Effective tutoring requires bridging the gap between a student's prior knowledge and the skills needed to solve the target coding task. 
To address this, we employ knowledge tracing (KT)~\citep{corbett1994knowledge,abdelrahman2023knowledge,scarlatos2024exploring} to estimate the student's knowledge state at each dialogue turn.
Specifically, we represent task-specific knowledge $\mathcal{K}$ as a set of knowledge components (KCs) $\{\text{KC}_1, \text{KC}_2, \dots, \text{KC}_K\}$, where each KC is either a reference dependency or a solution step. 
At the $t$-th turn, the tutor agent explicitly assesses the student's belief of each KC using texts, based on the dialogue context $\mathcal{C}_t$ and the estimated belief $B_{t-1}$ at the previous turn. 
The current belief $B_{t}$ indicates how many KCs the student has understood. 
With this estimation, the tutor is further prompted to focus more on missing KCs and generate utterances that address the student's knowledge gaps. 
The detailed prompt template is provided in Appendix~\ref{appendix:prompt_KT}.


\subsection{Utterance Decoding via Turn-by-Turn Verification}
Based on the KT outcomes, the tutor agent aims to generate high-quality utterances that advance the tutoring process. 
However, LLMs often struggle to determine which utterances effectively guide students toward task completion. 
Drawing inspiration from value-guided search approaches~\citep{lightman2023let,wang2024math,zhang2024rest}, we address this with a turn-by-turn verifier.
The verifier $V_{\theta}$ evaluates the quality of potential tutor responses by producing a reward score $v_{t}\in[0,1]$ based on three inputs: the target task $\mathcal{T}$, current dialogue context $\mathcal{C}_t$, and a candidate tutor utterance $r_t$ at turn $t$. To select the optimal response, we generate $N$ candidate utterances through parallel sampling and choose the one that receives the highest reward score from the verifier.


The core of the verifier $V_{\theta}$ is the turn-based reward $v_t$ at $t$-th turn, which should reflect (\romannumeral1) the cumulative progress made in the previous turns and (\romannumeral2) the current turn's contribution to achieving the overall tutoring goal. Hence, $v_{t}$ can be iteratively defined as:
\begin{equation}
    v_t = \max(v_{t-1} + w_{r_t}, 0) ~(v_0=0, t\in[1,T])
\end{equation}
where $w_{r_t}$ is the weighted reward quantifying the contribution of the current turn to the overall goal, $T$ denotes the total number of turns. 
To compute $w_{r_t}$, we introduce the concept of guiding distance $d_t=T-t$, which measures the remaining turns until the goal or end of the interaction. The weighted reward is then calculated as:
\begin{equation}
\label{eq:weighted_reward}
    w_{r_t} = \frac{1-v_{t-1}}{d_{t}+1}(2o_{s_t}-1)
\end{equation}
where $o_{s_t}\in\{0,1\}$ is a binary outcome indicating whether the tutor's $t$-th utterance contributes to the student's eventual successful completion of the target task.
As the dialogue progresses, the guiding distance $d_t$ decreases, leading to larger weighted rewards for later turns. This design ensures the turn-based reward $v_t$ remains bounded while appropriately weighting the importance of each turn based on its proximity to the goal.



We train the verifier $V_{\theta}$ using mean squared error (MSE) loss over $n$ samples:
\begin{equation}
    \mathcal{L} = \frac{1}{n}\sum_{i=1}^{n}\sum_{t=1}^{T_i}\left(V_{\theta}([\mathcal{T}^{(i)};\mathcal{C}_{t}^{(i)};r_{t}^{(i)}]) - v_{t}^{(i)}\right)^2
\end{equation}
where $T_{i}$ denotes the number of turns for the $i$-th sample, $\theta$ denotes the trainable parameters.


During inference, the verifier serves as a plug-and-play module, which chooses the utterance with the highest reward from candidate utterances generated by parallel sampling at each turn, promoting progression toward tutoring task completion.
