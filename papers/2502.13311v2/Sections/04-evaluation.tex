\section{The \eval Evaluation Protocol}
\label{sec:evaluation}

One key challenge in developing tutoring agents is the lack of robust evaluation methods. 
While human evaluation is essential, its high cost, time requirements, and complexity make it impractical for scalable benchmarking.
To address this limitation, we present \textbf{Di}alogue for \textbf{C}oding \textbf{T}utoring (\textbf{\eval}), an automatic protocol for evaluating tutor agents. The overview of \textsc{Dict} is shown in Figure \ref{fig:overview}.
Our protocol employs LLMs to simulate students with varying levels of programming knowledge. First, tutors engage in multi-turn dialogues with students to tutor the task. Then, students demonstrate their learning outcome by implementing the solution code. We evaluate tutoring effectiveness through automated unit tests of the student-generated code. This automated approach enables controlled, reproducible, and scalable evaluations of tutoring agents.


\subsection{Controlled Student Simulation}
\label{sec:student_simulation}
To evaluate how well tutors can adapt their strategies to students with varied prior knowledge, we simulate students of three knowledge levels:
(1) \textbf{Low-level}: Students access no prior knowledge from $\mathcal{K}$. They represent beginners with no familiarity with the target task. (2) \textbf{Medium-level}: These students are assigned a proportion (e.g., 50\%) of the \textit{reference dependencies} by random sampling, denoting that they have partial knowledge required for completing the task. (3) \textbf{High-level}: In addition to partial \textit{reference dependencies}, these students are also provided with the \textit{code contexts}, indicating that they have more comprehensive knowledge with contextual guidance.

\paragraph{Pre-Test.}
\label{sec:pre_test}

We create students of all three levels using the same LLM simulator, varying only in their knowledge level. 
However, this raises a critical question: \textit{do students with different knowledge levels actually demonstrate distinct performance in completing the coding tasks?} To validate this, we conduct a preliminary coding test where each simulated student attempts to generate code for the target task $\mathcal{T}$ before any tutoring intervention (see Appendix \ref{appendix:prompt} for prompting details).

\paragraph{Metrics for Coding Test.}
\label{sec:metrics_coding}
Following previous studies \cite{austin2021program,li2024evocodebench}, we employ Recall@$k$ and Pass@$k$ as evaluation metrics to assess coding test performance. Recall@$k$ measures the recall of reference dependencies in the generated programs. Specifically, students are asked to generate $k$ programs per target task. For the $i$-th program, we extract its dependencies $\mathbb{P}_{i}$ using the Pyan parser \cite{pyan2023}. We compare them with the reference dependencies $\mathbb{R}$ and compute the Recall@$k$ as:
\begin{equation}
\label{eq:recall}
\text{Recall}@k = \underset{\text{Target Tasks}}{\mathbb{E}} \left[ \max_{i \in [1, k]} \frac{|\mathbb{R} \cap \mathbb{P}_i|}{|\mathbb{R}|}\right]
\end{equation}
where $|\cdot|$ denotes the number of elements of a set. 
Pass@$k$ evaluates the functional correctness of the generated programs.
After generating $n \geq k$ programs per task, we execute them in Python interpreters to count the number of correct programs $c$ that pass all test cases. Pass@$k$ is computed as:
\begin{equation}
\label{eq:pass}
\text{Pass}@k =\underset{\text{ Target Tasks}}{\mathbb{E}}\left[1-\frac{\left(\begin{array}{c}
n-c \\
k
\end{array}\right)}{\left(\begin{array}{l}
n \\
k
\end{array}\right)}\right]
\end{equation}
where $\binom{\cdot}{\cdot}$ denotes the number of ways to choose a subset of elements (also known as the binomial coefficient). Our pre-test results reported in \S\ref{sec:student_analysis} show that the simulated students are effective.




\subsection{Tutor-Student Interaction}
\label{sec:tutor_student}
As shown in Figure \ref{fig:overview}, we let an LLM-based tutor agent engage in a multi-turn dialogue with a chosen student, simulating a tutoring session. The tutor is initialized with the target coding task $\mathcal{T}$ and task-specific knowledge $\mathcal{K}$. We ask the tutor to initiate the tutoring and communicate with the student turn by turn. A key challenge is determining when to terminate the tutoring. While the tutor could self-determine, our preliminary experiments revealed a tendency for overconfidence, leading to premature termination. This issue arises because most tutors overlook gaps in the student's prior knowledge. For a robust comparison, we follow \citet{wang-etal-2023-target} and introduce an LLM-powered dialogue manager (see Figure \ref{fig:overview}). Operating from a ``God's perspective,'' the manager considers the dialogue context and all information from both the tutor and student, to decide whether the tutoring goal has been met. The tutoring terminates under one of two conditions: (\romannumeral1) The manager confirms that the tutoring goal is achieved; (\romannumeral2) The dialogue reaches a predefined maximum of $T$ turns.



\subsection{Automatic Evaluation}

\paragraph{Post-Test.}
To evaluate the effectiveness of tutor agents, we conducted a coding test after tutoring (referred to as the post-test). Given a target task $\mathcal{T}$ and a dialogue session $\mathcal{C}=\{s_t, r_t\}_{t=1}^{T}$ with a simulated student, we ask the student to generate code for fulfilling the task. 
However, assuming that all dialogue content is retained during the coding test may be unrealistic. According to cognitive load theory (CLT) \cite{miller1956magical,sweller2011cognitive}, human working memory has a limited learning capacity at one time, and exceeding this capacity can hinder learning. We consider the student's cognitive load $f_{\text{CL}}$ at each turn by restricting the information retained from the tutorâ€™s utterance to a maximum threshold. Specifically, if a tutor utterance $r_t$ exceeds $M$ words, only the latest $M$ words are retained; otherwise, the full utterance is kept. Our post-test is formulated as:
\begin{equation}
\label{eq:post_test}
    \mathcal{Y}_{\text{code}}=\text{LM}_{\text{Student}}([\mathcal{I};\mathcal{T};\{s_t, f_{\text{CL}}(r_t)\}_{t=1}^{T}])
\end{equation}
where $\mathcal{I}$ represents the instruction for code generation. $s_t$ and $r_t$ denote student and tutor utterances at $t$-th turn, respectively. A detailed template for the instruction can be found in Appendix \ref{appendix:prompt}.




\begin{table*}[t!]
\centering
\scalebox{0.85}{
\begingroup
\renewcommand{\arraystretch}{0.85}
\setlength{\tabcolsep}{4pt}
\hspace{-8pt}
\begin{tabular}{cl  cc  cc cc cc}
\toprule
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Backbone Model}} & \multicolumn{2}{c}{\textbf{Overall}} &  \multicolumn{2}{c}{\textbf{Low-level}} & \multicolumn{2}{c}{\textbf{Med.-level}}  & \multicolumn{2}{c}{\textbf{High-level}}   \\
 \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
 &   &   Recall / $\Delta\%$R & Pass / $\Delta\%$P & $\Delta\%$R & $\Delta\%$P  &  $\Delta\%$R & $\Delta\%$P &  $\Delta\%$R & $\Delta\%$P  \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
\rowcolor{gray!10}
Pre-Test & --  & 45.9\textsubscript{$\pm~2.8$} / -- & 21.2\textsubscript{$\pm~2.0$} / -- &  --  & --  & --  & --  &  --  & --  \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
\multirow{7}{*}{\shortstack{Vanilla \\ Instruct}}  & Qwen2-7B-Instruct & 56.4\textsubscript{$\pm~1.6$} / 22.8 & 26.4\textsubscript{$\pm~3.4$} / 24.5 &  99.4  & 69.8  & 10.1  & 37.5 & 3.0 & -0.3  \\
  & Qwen2-72B-Instruct  & 61.4\textsubscript{$\pm~1.9$} / 33.8 & 32.1\textsubscript{$\pm~7.0$} / 51.4 & 131.8 & 128.2  & 13.7  & 53.7  &  11.5  & 21.4  \\
  & Llama-3.1-8B-Instruct & 63.6\textsubscript{$\pm~4.4$} / 38.5 & 31.1\textsubscript{$\pm~3.3$} / 46.7 & 138.6 &  145.1  & 23.8  & 48.3 & 10.9  & 8.0     \\
  & Llama-3.1-70B-Instruct & 62.5\textsubscript{$\pm~4.0$} / 36.0  & 34.9\textsubscript{$\pm~5.4$} / 65.0 & 127.4  & 160.6 & 21.5   & 71.0 & 11.7  & 24.9   \\
  & GPT-3.5-Turbo  & 60.1\textsubscript{$\pm~4.0$} / 31.0 & 28.8\textsubscript{$\pm~3.6$} / 35.9 & 110.9 & 130.1  & 21.6 & 37.7 & 6.9  & -1.3  \\
  & GPT-4o  & 64.2\textsubscript{$\pm~4.3$} / 39.9 & 38.7\textsubscript{$\pm~5.6$} / 82.8 & 141.3 & 207.8  & 28.4  & 102.1 & 8.9 & 23.5 \\
  & o1-mini & 61.3\textsubscript{$\pm~2.1$} / 33.4 & 35.9\textsubscript{$\pm~1.4$} / 69.4 & 129.5  & 159.1  & 19.3  & 101.2  & 6.9  & 16.6 \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
Self-Refine & GPT-4o & 64.0\textsubscript{$\pm~4.9$} / 39.5  & 40.6\textsubscript{$\pm~3.7$} / 91.7  & 143.0 & 221.4  & 23.1 & 118.6 & 11.8 & 26.2 \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
TreeInstruct & GPT-4o  & 64.3\textsubscript{$\pm~2.8$} / 40.1 &  39.8\textsubscript{$\pm~1.5$}  / 88.1 &  154.8  & 211.5 & 21.8  & 100.3  & 9.7  & 33.6  \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
\multirow{2}{*}{\shortstack{\textbf{\model} \\ \textbf{(Ours)}}} & Llama-3.1-70B-Instruct &  66.8\textsubscript{$\pm~1.3$} / 45.5 & 39.3\textsubscript{$\pm~6.9$} / 85.7  &  164.5  & 206.9  & 23.4  & 104.1  & \textbf{16.4}  & 28.4      \\
 &  GPT-4o  & \textbf{68.8}\textsubscript{$\pm~3.7$} / \textbf{49.8} & \textbf{43.7}\textsubscript{$\pm~1.3$} / \textbf{106.5} &  \textbf{166.3}  & \textbf{242.5}  & \textbf{34.8}  & \textbf{122.9}  & 15.8  & \textbf{44.8}  \\
\cmidrule(lr){1-2}\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8} \cmidrule(lr){9-10}
\rowcolor{gray!10}
Oracle & --  & 74.0\textsubscript{$\pm~5.7$} / 61.2  &  51.9\textsubscript{$\pm~3.7$} / 144.8 & 200.8 & 318.5  & 42.3 & 176.1 & 21.1  & 60.7 \\
\bottomrule
\end{tabular}
\endgroup}
\caption{Automatic evaluation results of various LLM-based tutor agents. ``$\Delta\%$R'' and ``$\Delta\%$P'' represent the tutoring outcome rates (TOR) in Recall and Pass, respectively.}
\label{tab:overall_result}
\end{table*}



\paragraph{Evaluation Metrics.}
Based on the coding test, students' post-test performance after tutoring is defined as the \textbf{tutoring outcome} (\textbf{TO}), measured by \textbf{Recall} and \textbf{Pass}. They represent the averages of Recall@$k$ and Pass@$k$ for $k\in\{1,3,5,10\}$. A higher Recall score indicates the tutor is more capable of leading the student to acquire the dependency knowledge for coding; a higher Pass score denotes a higher success rate of guiding the student in completing the target coding task.

Due to the difference in prior knowledge levels, we use the \textbf{tutoring outcome rate} (\textbf{$\Delta\%$}) to normalize and fairly evaluate the tutor's performance. This is calculated by relative improvement before and after tutoring:
\begin{equation}
    \Delta\%\mathcal{M}= \left(\frac{\mathcal{M}_{\text{post-test}}-\mathcal{M}_{\text{pre-test}}}{\mathcal{M}_{\text{pre-test}}}\right) \times 100\%
\end{equation}
where $\mathcal{M}$ denotes the metric for the coding test, which can be either Recall or Pass.

To further analyze the tutoring process, we propose the \textbf{tutoring outcome curve} (\textbf{TOC}). At each $t$-th turn, we ask simulated students to perform a post-test using the dialogue context up to that turn, i.e., $\{s_{\leq t},r_{\leq t}\}$. The TOC is then plotted by tracking the Recall and Pass scores varying by turns. These curves exhibit how tutor agents guide students throughout the tutoring session.
