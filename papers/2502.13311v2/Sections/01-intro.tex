\section{Introduction}

Tutoring has long been recognized as one of the most effective methods for enhancing human learning outcomes and addressing educational disparities~\citep{hill2005effects}. 
By providing personalized guidance to students, intelligent tutoring systems (ITS) have proven to be nearly as effective as human tutors in fostering deep understanding and skill acquisition, with research showing comparable learning gains~\citep{vanlehn2011relative,rus2013recent}.
More recently, the advancement of large language models (LLMs) has offered unprecedented opportunities to replicate these benefits in tutoring agents~\citep{dan2023educhat,jin2024teach,chen2024empowering}, unlocking the enormous potential to solve knowledge-intensive tasks such as answering complex questions or clarifying concepts.


\begin{figure}[t!]
\centering
\includegraphics[width=1.0\linewidth]{Figs/Fig.intro.pdf}
\caption{An illustration of coding tutoring, where a tutor aims to proactively guide students toward completing a target coding task while adapting to students' varying levels of background knowledge. \vspace{-5pt}}
\label{fig:example}
\end{figure}

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\linewidth]{Figs/Fig.scaling.pdf}
\caption{\textsc{Traver} with the trained verifier shows inference-time scaling for coding tutoring (detailed in \S\ref{sec:scaling_analysis}). \textbf{Left}: Performance vs. sampled candidate utterances per turn. \textbf{Right}: Performance vs. total tokens consumed per tutoring session. \vspace{-15pt}}
\label{fig:scale}
\end{figure}


Previous research has extensively explored tutoring in educational fields, including language learning~\cite{swartz2012intelligent,stasaski-etal-2020-cima}, math reasoning~\cite{demszky-hill-2023-ncte,macina-etal-2023-mathdial}, and scientific concept education~\cite{yuan-etal-2024-boosting,yang2024leveraging}. 
Most aim to enhance students' understanding of target knowledge by employing pedagogical strategies such as recommending exercises~\cite{deng2023towards} or selecting teaching examples~\cite{ross-andreas-2024-toward}. 
However, these approaches fall short in broader situations requiring both understanding and practical application of specific pieces of knowledge to solve real-world, goal-driven problems. 
Such scenarios demand tutors to proactively guide people toward completing targeted tasks (e.g., coding).
Furthermore, the tutoring outcomes are challenging to assess since targeted tasks can often be completed by open-ended solutions.



To bridge this gap, we introduce \textbf{coding tutoring}, a promising yet underexplored task for LLM agents.
As illustrated in Figure~\ref{fig:example}, the tutor is provided with a target coding task and task-specific knowledge (e.g., cross-file dependencies and reference solutions), while the student is given only the coding task. The tutor does not know the student's prior knowledge about the task.
Coding tutoring requires the tutor to proactively guide the student toward completing the target task through dialogue.
This is inherently a goal-oriented process where tutors guide students using task-specific knowledge to achieve predefined objectives. 
Effective tutoring requires personalization, as tutors must adapt their guidance and communication style to students with varying levels of prior knowledge. 


Developing effective tutoring agents is challenging because off-the-shelf LLMs lack grounding to task-specific knowledge and interaction context.
Specifically, tutoring requires \textit{epistemic grounding}~\citep{tsai2016concept}, where domain expertise and assessment can vary significantly, and \textit{communicative grounding}~\citep{chai2018language}, necessary for proactively adapting communications to students' current knowledge.
To address these challenges, we propose the \textbf{Tra}ce-and-\textbf{Ver}ify (\textbf{\model}) agent workflow for building effective LLM-powered coding tutors. 
Leveraging knowledge tracing (KT)~\citep{corbett1994knowledge,scarlatos2024exploring}, \model explicitly estimates a student's knowledge state at each turn, which drives the tutor agents to adapt their language to fill the gaps in task-specific knowledge during utterance generation. 
Drawing inspiration from value-guided search mechanisms~\citep{lightman2023let,wang2024math,zhang2024rest}, \model incorporates a turn-by-turn reward model as a verifier to rank candidate utterances. 
By sampling more candidate tutor utterances during inference (see Figure~\ref{fig:scale}), \model ensures the selection of optimal utterances that prioritize goal-driven guidance and advance the tutoring progression effectively. 
Furthermore, we present \textbf{Di}alogue for \textbf{C}oding \textbf{T}utoring (\textbf{\eval}), an automatic protocol designed to assess the performance of tutoring agents. 
\eval employs code generation tests and simulated students with varying levels of programming expertise for evaluation. While human evaluation remains the gold standard for assessing tutoring agents, its reliance on time-intensive and costly processes often hinders rapid iteration during development. 
By leveraging simulated students, \eval serves as an efficient and scalable proxy, enabling reproducible assessments and accelerated agent improvement prior to final human validation. 



Through extensive experiments, we show that agents developed by \model consistently demonstrate higher success rates in guiding students to complete target coding tasks compared to baseline methods. We present detailed ablation studies, human evaluations, and an inference time scaling analysis, highlighting the transferability and scalability of our tutoring agent workflow.
