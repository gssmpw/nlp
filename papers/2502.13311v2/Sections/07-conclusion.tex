\section{Conclusion}

This work explores the potential of LLMs as coding tutors.  
We propose \model, an effective agent workflow that incorporates knowledge tracing and turn-by-turn verification, to tackle key challenges in coding tutoring. 
While this work focuses on coding tutoring as an example, the proposed method extends beyond coding to other task-tutoring scenarios.
We further introduce \eval, a novel evaluation protocol combining student simulation and coding tests to assess tutor performance.
Such automated evaluation is critical for developing task-tutoring agents as it supports a systematic development and evaluation cycle. Although it's outside the scope of this paper, the best-performing agent from the automated evaluation can be further assessed through studies with real human students in the future. 
