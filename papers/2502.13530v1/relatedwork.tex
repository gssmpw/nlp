\section{Related works}
\subsection{Sequential Recommendation}Sequential recommendation was initially implemented through Markov chain models (\cite{rendle2010factorizing}), where the current state is dependent on the preceding state, neglecting more distant historical information. With the advent of deep neural networks, researchers utilized Recurrent Neural Networks (RNN) (\cite{lipton2015critical}) for sequence recommendation (\cite{jing2017neural,liu2016context,beutel2018latent}). Hidasi et al. (\cite{hidasi2015session}) proposed GRU4Rec. Another line of work adopts Convolutional Neural Networks (CNN). Yuan et al. (\cite{yuan2019simple}) developed Caser, capturing various lengths of local patterns by sliding convolutional kernels along the sequence. However, both CNN and RNN face challenges in capturing long-term dependencies. Then attention mechanism was utilized for sequence recommendation (\cite{chen2018sequential,kang2018self,sun2019bert4rec}). Compared to the left-to-right unidirectional prediction in the SASRec (\cite{kang2018self}) model, the BERT4Rec (\cite{sun2019bert4rec}) model adopts a bidirectional prediction and masking mechanism. To mitigate the impact of noisy data, FMLP-Rec(\cite{zhou2022filter}) and SLIME4Rec (\cite{du2023contrastive}) combines learnable filters with an all-MLP architecture, allowing the filters to adaptively attenuate noise in the frequency domain. In recent years, some approaches have integrated sequential recommendation with contrastive learning (\cite{zhou2023equivariant,yang2023debiased,zhu2024multi,wang2024unveiling}) or graph neural networks (GNN) to make predictions (\cite{xu2019graph,zhang2022dynamic}). 

\subsection{Text for Recommendation}Initially, text was used as labels in content-based recommendations, grouping objects into a collection according to the labels. During the collaborative filtering phase, text was utilized as supplementary information (\cite{kim2016convolutional}), which can be categorized into two categories (\cite{wu2022survey}). The first is information associated with either users or items. Collaborative Variational Autoencoder (CVAE) (\cite{li2017collaborative}) models the generation of item content while collaboratively extracting the implicit relationships between items and users. The second category is information about a user-item pair, such as review data (\cite{zheng2017joint}) and content like scenario description during interactions. TransNets (\cite{catherine2017transnets}) designed an additional latent module. This module aligns the rating representation with the actual review representations between the item and the user. With the advancement of NLP technology, the capability of text encoders to learn representations has continually increased. A series of recommendation models (\cite{zhang2022dynamic,zhu2019dan,wu2019npa}) based on the Attention mechanism assign attentive weights to text. 

In the era of large language models, textual information is more deeply integrated with recommender systems (\cite{gao2023chat,li2023gpt4rec,friedman2023leveraging}). For example, LLM-TRSR (\cite{zheng2024harnessing}) addresses the issue of large models limiting the text length in sequential recommendations by segmenting the raw textual information of sequences and generating summaries (either through iterative summary generation or chunk-based summary generation).  BinLLM (\cite{zhang2024text}) uses binary encoding to represent the embedding vectors for recommendations, facilitating their integration with text before being input into a large language model. The exploration of interpretable recommendation and multi-task recommendation (\cite{geng2022recommendation}) based on large language models is further pursued.