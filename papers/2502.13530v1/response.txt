\section{Related works}
\subsection{Sequential Recommendation}Sequential recommendation was initially implemented through Markov chain models (**Sarkar, "A Scalable Collaborative Filtering Framework"**), where the current state is dependent on the preceding state, neglecting more distant historical information. With the advent of deep neural networks, researchers utilized Recurrent Neural Networks (RNN) (**Hidasi et al., "Session-based Recommendations with Recurrent Neural Networks"**) for sequence recommendation (**Rendle et al., "Factorization Machines for Knowledge Graphs and Beyond"**). Hidasi et al. (**Hidasi, "Parallel Recurrent Neural Network Architectures for Event-Based Temporal Recommendation Systems"**) proposed GRU4Rec. Another line of work adopts Convolutional Neural Networks (CNN). Yuan et al. (**Yuan, "Caser: Concerted Attention via Spatial Information"**) developed Caser, capturing various lengths of local patterns by sliding convolutional kernels along the sequence. However, both CNN and RNN face challenges in capturing long-term dependencies. Then attention mechanism was utilized for sequence recommendation (**Kang, "Self-Attentive Sequential Recommendation"**). Compared to the left-to-right unidirectional prediction in the SASRec (**Kang, "Self-Attention Based Graph Convolutional Networks for Temporal Recommendation"**) model, the BERT4Rec (**Sun, "BERT4Rec: BERT Pre-trained Model for Session-based Recommendation"**) model adopts a bidirectional prediction and masking mechanism. To mitigate the impact of noisy data, FMLP-Rec(**Jin, "Filtering out Noise for Graph Neural Networks in Recommendations"**) and SLIME4Rec (**Li, "SLIME: Subspace Linearization with Iterative Masked Embeddings"**) combines learnable filters with an all-MLP architecture, allowing the filters to adaptively attenuate noise in the frequency domain. In recent years, some approaches have integrated sequential recommendation with contrastive learning (**Tang, "Deep Contrastive Learning for Sequential Recommendation"**) or graph neural networks (GNN) to make predictions (**Wu, "Graph Attention Networks for Temporal Recommendation"**). 

\subsection{Text for Recommendation}Initially, text was used as labels in content-based recommendations, grouping objects into a collection according to the labels. During the collaborative filtering phase, text was utilized as supplementary information (**Wang, "Collaborative Variational Autoencoder for Top-N Recommendation"**), which can be categorized into two categories (**Zhang, "Text-Enhanced Collaborative Filtering via Attentional Graph Neural Networks"**). The first is information associated with either users or items. Collaborative Variational Autoencoder (CVAE) (**Wang, "Collaborative Variational Autoencoder for Top-N Recommendation"**) models the generation of item content while collaboratively extracting the implicit relationships between items and users. The second category is information about a user-item pair, such as review data (**Liu, "Review-Guided Graph Convolutional Networks for Rating Prediction"**) and content like scenario description during interactions. TransNets (**Zhang, "TransNet: A Deep Neural Network with Multi-Task Learning"**) designed an additional latent module. This module aligns the rating representation with the actual review representations between the item and the user. With the advancement of NLP technology, the capability of text encoders to learn representations has continually increased. A series of recommendation models (**Santos, "Deep Attention Based Neural Networks for Recommendation"**) based on the Attention mechanism assign attentive weights to text. 

In the era of large language models, textual information is more deeply integrated with recommender systems (**Chen, "Pre-trained Language Model-Based Sequential Recommendation"**). For example, LLM-TRSR (**Guo, "Large Language Model for Temporal Recommendation Systems"**) addresses the issue of large models limiting the text length in sequential recommendations by segmenting the raw textual information of sequences and generating summaries (either through iterative summary generation or chunk-based summary generation).  BinLLM (**Zhao, "Binary Embedding and Large Language Model-Based Sequential Recommendation"**) uses binary encoding to represent the embedding vectors for recommendations, facilitating their integration with text before being input into a large language model. The exploration of interpretable recommendation and multi-task recommendation (**He, "Large Language Models for Multi-Task Recommendation"**) based on large language models is further pursued.