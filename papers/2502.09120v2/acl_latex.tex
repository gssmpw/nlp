% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

%multirow for table
\usepackage{multirow}
\usepackage{arydshln} % 대시라인을 위한 패키지
\usepackage{booktabs} % 테이블에서 \toprule, \midrule, \bottomrule을 사용하기 위함
\usepackage{caption} % 캡션 사용을 위한 패키지 추가

%for citation
\usepackage{natbib} % APA 스타일을 위해 natbib 패키지 사용
\usepackage{hyperref} % 링크를 위해 hyperref 패키지 사용

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{The influence of visual and linguistic cues on \\
ignorance inference in Vision-Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Ye-eun Cho \\
  English language and literature\\
  Sungkyunkwan University\\
  Seoul, South Korea\\
  \texttt{joyenn@skku.edu} \\\And
  Yunho Maeng \\
  Ewha Womans University \&\\
  LLM Experimental Lab, MODULABS \\
  Seoul, South Korea\\
  \texttt{yunhomaeng@ewha.ac.kr} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}
\begin{document}
\maketitle
\begin{abstract}
This study explored how Vision-Language Models (VLMs) process ignorance implicatures with visual and linguistic cues. Particularly, we focused on the effects of contexts (precise and approximate contexts) and modifier types (bare numerals, superlative, and comparative modifiers), which were considered pragmatic and semantic factors respectively. Methodologically, we conducted a truth-value judgment task in visually grounded settings using GPT-4o and Gemini 1.5 Pro. The results indicate that while both models exhibited sensitivity to linguistic cues (modifier), they failed to process ignorance implicatures with visual cues (context) as humans do. Specifically, the influence of context was weaker and inconsistent across models, indicating challenges in pragmatic reasoning for VLMs. On the other hand, superlative modifiers were more strongly associated with ignorance implicatures as compared to comparative modifiers, supporting the semantic view. These findings highlight the need for further advancements in VLMs to process language-vision information in a context-dependent way to achieve human-like pragmatic inference.
\end{abstract}

\section{Introduction}
In recent years, many large language models (LLMs) have demonstrated the ability to solve a wide variety of tasks, contributing to their growing popularity. Initially limited to text-based inputs, these models have been extended to incorporate visual inputs, paving the way for vision-language models (VLMs). By bridging vision and language modalities, VLMs have expanded the possibilities for AI applications and become central to the ongoing technological revolution.

VLMs have enabled various multimodal applications, such as object recognition, caption generation, and visual question answering. These tasks primarily focus on associations between visual and textual inputs by identifying objects, describing scenes, or responding to straightforward queries. While such capabilities are remarkable, they represent only the surface level of human-like understanding. In fact, real-world communication often requires reasoning about implicit meanings that emerge from the interplay between language and visual information \citep{kruk2019integrating, sikka2019deep}. This raises critical questions about whether VLMs can engage in sophisticated, context-dependent reasoning, which is essential for real-world communication.

Pragmatics offers an ideal framework for investigating this question. In human communication, pragmatic inference plays a crucial role in understanding intended meanings beyond literal content \citep{grice75, wilson95, levinson2000presumptive}. Visual cues often provide disambiguating information that influences the interpretation of utterances, making pragmatic reasoning inherently multimodal \citep{clark1996using, kendon2004gesture, martin2007multimodal, mcneill2008gesture}. While a few studies on pragmatic reasoning have been explored in text-only LLMs \citep{hu2022predicting, hu2023expectations, lipkin2023evaluating, cho2024pragmatic, capuano2024pragmatic, tsvilodub2024experimental}, the visual modality presents unique opportunities and challenges. As visual context provides rich, implicit information that influences language interpretation, studying pragmatic phenomena through VLMs presents an intriguing research opportunity. However, how well VLMs can leverage visual information for pragmatic inference remains largely unexplored.

Therefore, this study specifically focuses on ignorance implicatures --- where a speaker’s utterance implies her lack of knowledge about particular details --- in visually-grounded contexts. By examining how VLMs perform in this phenomenon compared to human reasoning, we aim to better understand both the capabilities and limitations of these models in processing context-dependent pragmatic phenomena. Ultimately, this research contributes to the broader discussion on whether VLMs can approximate human-like pragmatic inference and what factors may trigger or hinder their performance.


\section{Background}
\subsection{Vision-Language Models (VLMs)}

The emergence of transformer architectures \citep{vaswani2017attention} has revolutionized the field of natural language processing (NLP). By using these architectures, BERT (Bidirectional Encoder Representations from Transformers; \citealp{devlin2018bert}) achieved remarkable performance, surpassing all previous language modeling methods. Given this success in the linguistic domain, it is no surprise that transformer architectures have been extended to cross-modal tasks that integrate language and vision.

Early VLMs, such as VisualBERT \citep{li2019visualbert} and ViLBERT \citep{lu2019vilbert}, adapted transformer architectures to simultaneously process textual and visual inputs. This capability forms the basis for tasks like image captioning, visual reasoning, and multimodal classification, laying the groundwork for more sophisticated VLMs.

Following these initial efforts, the introduction of contrastive learning frameworks further propelled the development of VLMs. Models such as CLIP \citep{radford2021learning} and ALIGN \citep{jia2021scaling} employed massive datasets of image-text pairs to learn shared embedding spaces, significantly enhancing their ability to generalize across diverse tasks. These models demonstrated exceptional zero-shot classification and out-of-distribution (OOD) performance, paving the way for more adaptable and robust VLMs. These advancements inspired models like Flamingo \citep{alayrac2022flamingo}, which further refined multimodal reasoning, and DALL·E \citep{ramesh2021zero}, which demonstrated the potential for text-to-image generation.

Building on this foundation, state-of-the-art multimodal models like GPT \citep{achiam2023gpt, gpt4o} and Gemini \citep{team2024gemini} have pushed the boundaries of multimodal learning. GPT leverages its strong language modeling capabilities with added multimodal functionalities, making it an effective tool for complex reasoning tasks. Meanwhile, Gemini integrates advanced training techniques to achieve seamless cross-modal reasoning, excelling in tasks that require intricate integration of visual and textual data.

Given that humans rely on a combination of textual and visual information to draw inferences and make decisions, the importance of VLMs lies in their potential to simulate human-like multimodal understanding. Similarly, VLMs’ ability to integrate these modalities offers an advanced mechanism to mimic human cognition, making them indispensable for tasks involving multimodal reasoning. To evaluate these capabilities, various tasks have been developed, each targeting specific aspects of multimodal understanding. Image captioning, for instance, requires generating textual descriptions for a given image, offering insights into the model’s ability to understand and verbalize visual content (\citealp{chen2015microsoft}; also see \citealp{urbanek2024picture, kirillov2023segment}). On the other hand, text-to-image consistency assesses whether a model can accurately generate images based on textual captions, demonstrating its capability to align textual and visual repress entations \citep{cho2023davidsonian, cho2024visual, li2024evaluating, lin2024evaluating}. In tasks like Visual Question Answering (VQA), models answer natural language questions about images, showing their ability to extract and reason about visual information \citep{goyal2017making, krishna2017visual, das2017visual, gurari2018vizwiz, singh2019towards, hudson2019gqa, marino2019ok, lu2022learn, whitehead2022reliable, manas2022mapl, dancette2023improving, yue2024mmmu}. A specialized form of this, Text-Based VQA, focuses on answering questions specifically about textual content embedded in images, such as documents or scene text \citep{mishra2012scene, karatzas2013icdar, shi2014end, veit2016coco, biten2019scene, singh2019towards, mishra2019ocr, huang2019icdar2019, jaume2019funsd, wang2020general, mathew2021docvqa, mathew2022infographicvqa, masry2022chartqa, yuan2022syntax, kuang2023visual}. Moreover, tasks like zero-shot image classification explore how well models generalize to classify unseen categories without explicit training, highlighting their adaptability and generalization \citep{radford2021learning, menon2022visual, pratt2023does, parashar2023prompting, parashar2024neglected}. Finally, visio-linguistic compositional reasoning tests a model’s ability to identify the correct caption when presented with intentionally ambiguous or misleading alternatives, pushing its interpretive capabilities to the limit \citep{thrush2022winoground, yuksekgonul2022and, hsieh2023tool}.

By addressing diverse aspects of multimodal reasoning, they not only highlight the strengths and limitations of current models but also pave the way for future advancements in aligning linguistic and visual understanding. While VLMs continue to evolve in their capacity to process multimodal information, a critical aspect that requires deeper investigation is how these models handle more sophisticated linguistic phenomena. This leads us to examine how VLMs handle pragmatic reasoning, a fundamental aspect of language that involves inferring meaning by considering context.

\subsection{Ignorance implicatures}


\setlength{\tabcolsep}{5pt} % 열 사이의 간격을 5pt로 조정
\renewcommand{\arraystretch}{1.9} % 행 높이 조정
\begin{table*}
  \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{|c|c|p{7cm}|c|c|}
    \hline
    \rule{0pt}{1pt}
        \textbf{No.} & \textbf{Image} & \centering \textbf{Text} & \textbf{Context} & \textbf{Modifier} \\
    \hline
    1 & \multirow{3}{*}{\includegraphics[width=0.3\textwidth, height=0.3\textwidth, keepaspectratio]{./image1-1.jpg}} & \centering \textit{There are four apples in the boxes.} & \centering precise & \centering bare \tabularnewline
    \cline{3-5}
       & & \centering \textit{There are at least four apples in the boxes.} & \centering precise & \centering superlative \tabularnewline
    \cline{3-5}
       & & \centering \textit{There are more than three apples in the boxes.} & \centering precise & \centering comparative \tabularnewline
    \hline
    2 & \multirow{3}{*}{\includegraphics[width=0.3\textwidth, height=0.3\textwidth, keepaspectratio]{./image1-2.jpg}} & \centering \textit{There are four apples in the boxes.} & \centering approximate & \centering bare \tabularnewline
    \cline{3-5}
       & & \centering \textit{There are at least four apples in the boxes.} & \centering approximate & \centering superlative \tabularnewline
    \cline{3-5}
       & & \centering \textit{There are more than three apples in the boxes.} & \centering approximate & \centering comparative \tabularnewline
    \hline
    \end{tabular}
    }
  \caption{A sample set of experimental materials}
\end{table*}


To better understand pragmatic reasoning in real-world language use, we examine the phenomenon of ignorance implicatures. Consider the conversation in (1). 

\renewcommand{\theenumi}{(\arabic{enumi})}
\renewcommand{\labelenumi}{\theenumi}
\begin{enumerate}
\item 
A: How many students passed the exam?\\
B: Four students passed the exam.\\
C: At least four students passed the exam.\\
D: More than three students passed the exam.
\end{enumerate}

Regarding the question from A, the answer from B triggers ‘exactly four’ implicature, while the answers from C and D do not. Given that both C and D use the modified numerals, we infer that these speakers are unable to give a more precise answer. It is often assumed that superlative (\textit{at least}) and comparative (\textit{more than}) modifiers lead to ignorance implicatures.

This phenomenon, where speakers convey a lack of knowledge or precision through their choice of modifiers, has been the subject of various experimental studies. By reviewing the findings from these studies, we aim to clarify the roles of different modifiers in generating ignorance implicatures and propose specific hypotheses to be tested in this study.

Although both modifiers engage in ignorance implicatures, many studies have claimed that the use of these modifiers shows different levels of ignorance. One approach suggests that ignorance inference is dependent on the words or phrases themselves, regardless of surrounding contexts (\citealp{nouwen2010two}; also see \citealp{geurts2010scalar}). For example, when someone has basic knowledge of geometry, (2a) gives the impression that the speaker lacks precise information, as compared to (2b). This attributes the ignorance implicature to a semantic property specific to \textit{at least} rather than to general pragmatics \citep{buring2008least, cummins2010comparative, coppock2013raising}. 

\begin{enumerate}
\setcounter{enumi}{1}
\item 
a. ? A hexagon has at least five sides.\\
b. A hexagon has more than four sides.
\end{enumerate}

Under pragmatic account, on the other hand, ignorance implicatures for both \textit{at least} and \textit{more than} depend primarily on context. In these studies, the context was often manipulated by QUD (Question Under Discussion; \citealp{cummins2013modelling, westera2014ignorance, mayr2014more, cremers2022ignorance}), preceding discourse or images \citep{cummins2012granularity, cremers2022ignorance}. These studies revealed that the use of modifiers was influenced by the level of precision required in the given context. For instance, a corpus analysis conducted by \citet{westera2014ignorance} found that the superlative modifier \textit{at least} was used more frequently than the comparative modifier \textit{more than} in the context requiring greater precision. In contrast, \citet{cremers2022ignorance} explored how different contexts influence ignorance implicatures, in which participants were presented with a scenario involving eight card images and two types of contexts were manipulated: (i) a precise context, where all eight cards were face-up, and (ii) an approximate context, where two cards remained face-down, obscuring precise information. The results showed that the acceptability score for the superlative modifier \textit{at least} was highest in the approximate context rather than the precise one. Out of these controversial findings, we propose the following hypothesis based on the recent study \citep{cremers2022ignorance}:\\

\noindent\textit{Hypothesis 1 — Ignorance inference occurs more frequently in approximate context than in precise context.}\\

In addition, regardless of context types (QUD, preceding discourse, or precision), most studies commonly showed that the superlative modifier triggers ignorance implicatures more consistently than the comparative modifier \citep{nouwen2010two, cummins2012granularity, coppock2013raising, mayr2014more, cremers2022ignorance}. This has been explained in several ways. According to \citet{geurts2007least}, the meaning of the superlative modifier is more complex than that of its comparative counterpart. For example, the meaning of \textit{more than n} contains a single meaning of 'larger than \textit{n}', while \textit{at least n} encompasses two meanings 'possible that there is a set of \textit{n}’ and ‘certain that there is no smaller set of \textit{n}.’ On the other hand, \citet{cummins2011interpretation, cummins2013modelling} illustrated this by suggesting a trade-off of three factors (i.e., complexity, salience of the base numeral and informativity) using Optimality Theory (OT), while \citet{westera2014ignorance} argued that contextual cues lead to the different processing cost for these modifiers. Based on these findings, we posit the second hypothesis as follows:\\

\noindent\textit{Hypothesis 2 — Superlative modifier is more strongly associated with ignorance inference than comparative modifier in any context.}\\

To empirically test these hypotheses and further investigate the relationship between contextual precision and modifier types in triggering ignorance inference in VLMs, we conducted the following experiment that systematically manipulated both context and modifier.


\section{Methods}
\subsection{Data}

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{./figure1.png}
  \caption{Overview of the experimental procedure}
\end{figure*}

To investigate the two hypotheses proposed above, experimental materials were designed using both images and texts to compare contextual precision (‘context’) and modifier types (‘modifier’), as detailed in Table 1.

Images were used to manipulate the context, in which a picture showing all 8 boxes open was labeled as ‘precise’, and a picture with 2 out of 8 boxes remaining closed was labeled as ‘approximate’. In all images, the target objects consistently appeared in 4 boxes. For example, as shown in Table 1, the image where all 8 boxes are open and 4 of them contain apples represents the precise context. Since all boxes are open, we can tell that the target objects are exactly 4. On the other hand, the image where 2 out of the 8 boxes remain closed and 4 of the open boxes contain apples represents the approximate context. As what is inside the closed boxes is unknown, the target objects could be 4 or more.

The corresponding texts were categorized based on the modifier types, including ‘bare’ (bare numeral \textit{n}), ‘superlative’ (\textit{at least n}), and ‘comparative’ (\textit{more than n}). Consequently, a total of 70 sets of materials were used in the experiment, with each set consisting of 2 images and 3 texts for each image.

\subsection{Models and procedure}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{./figure2.png}
  \caption{Stacked bar charts showing the distribution of truth values based on context and modifier}
\end{figure}

As VLMs for the experiment, we used GPT-4o \citep{gpt4o} and Gemini 1.5 Pro \cite{team2024gemini}. These two models were chosen for their advanced language understanding and image-text integration capabilities. The experimental process followed the principles of VQA, where the models were tasked with determining the appropriateness of a given text in relation to a provided image, treating the text as a question about the image’s content. During the process, both models were initialized using API keys, enabling access to the text and image processing capabilities. After initialization, the images were resized to a standard size of 224x224 pixels using the Python Imaging Library (PIL) to ensure consistency in input dimensions and improve processing efficiency. Following this, the resized images were encoded into base64 format, making them compatible for input into both models. The experiment involved presenting each image alongside a text prompt, which was designed to evaluate the appropriateness of the text in relation to the image.

In evaluating the appropriateness, \citet{cremers2022ignorance} argued that the main finding related to the disagreement over ignorance inferences was that the detection depends on the types of tasks participants were asked to perform. Specifically, it varied depending on whether participants were given an acceptability judgment task \citep{coppock2013diagnosing, westera2014ignorance, cremers2022ignorance}, where they judged the acceptability of the given sentences with respect to the depicted scenarios or images, or an inference task \citep{geurts2010scalar}, where they judged whether \textit{exactly n} implies \textit{at least n}. \citet{coppock2013diagnosing} demonstrated that truth-value judgments, a type of acceptability task, were more effective than inference tasks in diagnosing ignorance implicatures. Subsequent studies expanded on this approach by using numerical scales (e.g., 1 to 5) to capture the degree of ignorance implicatures in a more fine-grained manner \citep{westera2014ignorance, cremers2022ignorance}.
 
Therefore, in this study, we employed the truth-value judgments to provide a clear, straightforward evaluation of text-image appropriateness, simplifying the task for the model and allowing for clear comparisons across responses, following \citet{coppock2013diagnosing}.

The prompt for truth-value judgements in the experiment was as follows: 

\begin{quote}
\texttt{Based on the given image, is the following text appropriate?: \textbf{\{text\}} Answer ‘True’ or ‘False.’}
\end{quote}

The text from the materials was inserted into \texttt{\textbf{\{text\}}} in the prompt and the prompt was provided to the models. Then, they assessed whether the given text is appropriate for describing the provided image, responding with either TRUE or FALSE. The use of binary judgments ensured that the models’ decision-making process was clearly defined, making it easier to quantify and analyze the results. Due to the stochastic nature of the models’ responses, each of the 70 sets of materials was run through the models 10 times, resulting in a total of 700 sets and 4,200 individual responses. This entire procedure is presented in Figure 1. All the materials, code and result of the experiment are publicly available.\footnote{\url{ https://github.com/joyennn/ignorance- implicature}}

\begin{figure*}[t]
  \includegraphics[width=\textwidth]{./figure3.png}
  \caption{Conditional inference trees (CITs) illustrating the influence of predictor variables (context and modifier) on truth values. The tree structure shows hierarchical splits based on significant predictors, with terminal nodes displaying the distribution of TRUE and FALSE responses.}
\end{figure*}


\section{Result}

Figure 2 shows the extent to which the two models responded with TRUE or FALSE to the given prompt based on the image context. As a statistical analysis, we built mixed-effects logistic regression model \citep{baayen2008analyzing, baayen2008mixed, jaeger2008categorical, jaeger2011corpus} to analyze the results, using the \texttt{glmer} function from the \texttt{lme4} package \citep{bates2015package} in R software \citep{r2023}. To test hypotheses 1 and 2, we included context and modifier as independent variables and truth values as dependent variables in fixed effects, while image and text were treated as random effects. As for the fixed effects, ‘precise’ and ‘bare’ were set as the reference levels for context and modifier respectively. The tables summarizing the mixed-effects logistic regression models for all results are provided in Appendix A.

In line with Hypothesis 1, ignorance inference was expected to be more prominent in the approximate context than in the precise context. This hypothesis can be supported when the truth values are higher in the approximate context than in the precise context. As a result, in GPT-4o, the approximate context showed lower truth values for the superlative modifiers (Estimate = 1.81, \textit{p} < 0.001) and higher truth values for the comparative modifiers (Estimate = 1.99, \textit{p} < 0.001) as compared to the precise context, and the effect of the context was statistically significant (Estimate = -1.72, \textit{p} < 0.001). Gemini 1.5 Pro showed higher truth values for both modifiers (Superlative: Estimate = 2.62, \textit{p} < 0.001 / Comparative: Estimate = 1.34, \textit{p} < 0.001) in the approximate context. However, statistically, the main effects for the context were not observed (Estimate = -0.04, \textit{p} = 0.96). In summary, the influence of context on ignorance inference in GPT-4o showed distinct patterns from human language use. On the other hand, Gemini 1.5 Pro aligned with the hypothesis but was not statistically significant.

In regard to Hypothesis 2, ignorance inference should be stronger with the superlative modifiers than with the comparative modifiers. In fact, in the experiment, both models showed that the superlative modifiers had a stronger influence on the truth values compared to the comparative modifiers. In GPT-4o, for example, the relationship between the superlative modifiers and the dependent variables showed a positive effect (Estimate = 0.98, \textit{p} < 0.05), indicating that the superlative modifiers tend to yield higher truth values. In contrast, the comparative modifiers exhibit a negative effect (Estimate = -1.33, \textit{p} < 0.01), suggesting that it is associated with lower truth values. In Gemini 1.5 Pro, while both superlative and comparative modifiers had significant positive effects on truth values, the superlative modifiers exhibited a much stronger effect (Estimate = 11.28, \textit{p} < 0.001) compared to the comparative modifiers (Estimate = 6.85, \textit{p} < 0.001). Also, both models exhibited the main effects. These findings from the two models indicate that the superlative modifiers are more strongly associated with ignorance inference than the comparative modifiers, supporting the hypothesis that ignorance inference should be stronger with the superlative modifiers.

Based on the results above, it is likely that the modifier has a greater influence on ignorance inference than the context. Nevertheless, to evaluate the relative importance of context and modifier in influencing ignorance inference in a more structured and interpretable way, we adopted Conditional Inference Trees (CITs; \citealp{tagliamonte2012models}). CITs use recursive partitioning to split data into smaller subsets based on predictor variables that most strongly correlate with the outcomes, continuing until no further significant splits can improve the homogeneity of the subsets (also see \citealp{levshina2015linguistics, levshina2016we, levshina2021conditional, szmrecsanyi2016around, hundt2021predicting, kim2025semantic}). CITs, presented in Figure 3, show how the predictor variables (i.e., context and modifier) influenced the outcomes (i.e., truth values) in each model. In CIT of GPT-4o, the superlative modifiers showed the highest truth values (Node 1 - Node 8 and 9). However, the context influenced the truth values differently; when the superlative modifiers were used, the approximate context led to higher truth values (Node 7 – Node 9), whereas with the bare numeral, the precise context resulted in higher truth values (Node 4 – Node 6). In addition, with the comparative modifiers, no significant correlation with context was observed (Node 2 – Node 3). In CIT of Gemini 1.5 Pro, excluding the bare numeral (Node 1 – Node 9), which had the greatest influence on the lowest truth values, the superlative modifiers showed relatively higher truth values (Node 2 – Node 7 and 8) than the comparative ones (Node 2 – Node 4 and 5). Additionally, among the context variables, the approximate context showed slightly higher truth values (Node 3 – Node 5; Node 6 – Node 8). In summary, in both models, the modifiers played a more significant role in ignorance inference.


\section{Discussion}

This study aimed to investigate the role of visual and linguistic cues in the interpretation of ignorance inference within VLMs, comparing context and modifier as semantic and pragmatic factors respectively. One of the key hypotheses of this study was that ignorance implicatures would occur more frequently in the approximate context compared to the precise context, in line with the pragmatic view that contextual cues heavily influence the interpretation of ignorance implicatures. However, the results did not fully support this hypothesis. GPT-4o showed results that only partially deviated from the hypothesis, while Gemini-1.5 aligned with the hypothesis but did not show a significant impact. The lack of consistency across the two models suggests that context, as a type of pragmatic factor, may not have a strong or consistent effect on the interpretation of ignorance implicatures in VLMs.

Conversely, in the second hypothesis that the superlative modifiers are more strongly associated with ignorance inference than the comparative modifiers, both GPT-4o and Gemini 1.5 Pro exhibited stronger associations between the superlative modifiers and higher truth values. This finding aligns with the semantic view, which posits that the meaning of the modifier itself is a key driver in interpreting ignorance implicatures regardless of context \citep{nouwen2010two, geurts2010scalar}.
Considering the findings of this study, VLMs seem to struggle to fully account for the pragmatic perspective when interpreting ignorance implicatures. While the models demonstrated the ability to process ignorance implicature depending on modifiers as semantic factors, their handling of context was less consistent and did not align well with the expected pragmatic reasoning. These findings can be explained by several characteristics of VLMs.

Although many studies have proven that LLMs excel at pragmatic inference tasks \citep{hu2022predicting, hu2023expectations, lipkin2023evaluating, cho2024pragmatic, capuano2024pragmatic, tsvilodub2024experimental}, VLMs have struggled to effectively perform pragmatic reasoning \citep{kumar2022did, kumar2023explaining, zhou2023vicor, du2024docmsu}. This difference may stem from the distinct training approaches between LLMs and VLMs. 

LLMs are trained solely on text data using transformer architectures \citep{vaswani2017attention}, leveraging attention mechanisms to learn linguistic relationships within text sequences. This process allows LLMs to acquire contextual information, including pragmatic context. Consequently, they can perform context-based language processing and infer implicit meanings based on textual cues.
VLMs, in contrast, are primarily trained through image-text matching --- i.e., contrastive learning \citep{radford2021learning, jia2021scaling}, cross-modal learning (\citealp{li2019visualbert, lu2019vilbert}; also see \citealp{zhang2021cross}) --- focusing on aligning images with text rather than capturing nuanced linguistic structures and relationships. Unlike LLMs, which rely solely on textual information, VLMs combine vision encoders and language models, prioritizing multimodal correspondence over purely textual coherence. This training paradigm emphasizes explicit associations between language and images. Thus, in this study, when the VLMs were required to interpret ignorance implicatures from image-text inputs, they likely relied on surface-level image-text mappings rather than engaging in deeper contextual reasoning.

Nevertheless, improving VLMs’ ability to engage in human-like conversation may require incorporating pragmatic reasoning into their processing. Recent research has begun to explore various approaches to enhance how VLMs handle pragmatic interpretation more effectively. For instance, \citet{kumar2022did, kumar2023explaining} explored sarcasm explanation in multi-modal multi-party dialogues, as well as \citet{du2024docmsu} introduced DocMSU, a comprehensive benchmark for document-level multimodal sarcasm understanding. These studies aimed to evaluate how VLMs integrate contextual and pragmatic information. Moreover, \citet{nam2024visual} proposed a benchmark dataset that examines how visual contexts clarify ambiguous expressions. Their work highlights the role of visual information in resolving linguistic ambiguity for more sophisticated pragmatic inference.
In this study, while the VLMs exhibited limitations in pragmatic inference, it is problematic to consider context derived solely from visual cues as the only pragmatic factors. Therefore, future research that incorporates a broader range of pragmatic factors might yield different results. Moreover, ongoing efforts to integrate pragmatic information into VLMs are progressively bridging this gap, offering promising directions for enhancing their ability to interpret pragmatic phenomena.\\
 \\


\section{Limitations}

While this study provided valuable insights into how VLMs interpret ignorance implicatures, several limitations should be acknowledged.  Though a few previous studies were referenced, we did not include a direct comparison with human participant data, which made it unclear how VLMs’ performance aligns with human reasoning. In addition, using a binary (TRUE/FALSE) response format might limit insight into the models’ reasoning processes. More intricate evaluation methods, such as Likert scales or open-ended responses, could yield different results. Lastly, the dataset consists of relatively simple visual contexts, which may not fully capture real-world multimodal reasoning. Thus, future studies are expected to explore more complex and ecologically valid scenarios.

\section{Conclusion}

This study examined the influence of visual and linguistic cues on ignorance inference in VLMs by analyzing the effects of the context and the modifier in visually grounded settings. The findings revealed that while VLMs demonstrated sensitivity to linguistic cues (modifier), the influence of visual cues (context) was inconsistent. Specifically, both GPT-4o and Gemini 1.5 Pro showed stronger associations between superlative modifiers and ignorance inference, supporting the semantic view, whereas the effects of context were weaker and less consistent, suggesting limitations in pragmatic inference. These results highlight a fundamental challenge in VLMs’ ability to integrate linguistic and visual information for human-like interpretation. These findings underscore the limitations of VLMs in pragmatic inference and emphasize the need for further advancements to achieve more human-like pragmatic reasoning.

\section*{Acknowledgments}
This research was supported by Brian Impact Foun- dation, a non-profit organization dedicated to the advancement of science and technology for all. Also, we would like to thank Prof. Hanjung Lee for her helpful comments.
% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\nocite{*}
\bibliography{latex/custom}

\appendix

\section{Appendix}
\label{sec:appendix}


\noindent
\begin{minipage}{\columnwidth}
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lcccc}
    \toprule
    \textbf{} & \textbf{Estimate} & \textbf{Std} & \textbf{\textit{t}} & \textbf{\textit{p}-value} \\
    \midrule
    (Intercept) & 2.76 & 0.4 & 6.89 & \textbf{<0.001} \\
    Context & -1.72 & 0.27 & -6.27 & \textbf{<0.001} \\
    Modifier - Superlative & 0.98 & 0.47 & 2.07 & \textbf{<0.05} \\
    Modifier - Comparative & -1.33 & 0.45 & -2.94 & \textbf{<0.01} \\
    Context:Modifier - Superlative & 1.81 & 0.23 & 7.5 & \textbf{<0.001} \\
    Context:Modifier - Comparative & 1.99 & 0.23 & 8.6 & \textbf{<0.001} \\
    \bottomrule
  \end{tabular}}
  \captionof{table}{Summary of fixed effects from mixed-effects logistic regression models by GPT-4o}
  \label{tab:stat_gpt}
\end{minipage}

\vspace{5mm} % Adjust this space to bring tables closer or further apart

\noindent
\begin{minipage}{\columnwidth}
  \centering
  \resizebox{\columnwidth}{!}{%
  \begin{tabular}{lcccc}
    \toprule
    \textbf{} & \textbf{Estimate} & \textbf{Std} & \textbf{\textit{t}} & \textbf{\textit{p}-value} \\
    \midrule
    (Intercept) & -3.84 & 1.13 & -3.38 & \textbf{<0.001} \\
    Context & -0.04 & 1.00 & -0.04 & 0.96 \\
    Modifier - Superlative & 11.28 & 1.33 & 8.43 & \textbf{<0.001} \\
    Modifier - Comparative & 6.85 & 1.35 & 5.07 & \textbf{<0.001} \\
    Context:Modifier - Superlative & 2.62 & 0.56 & 4.63 & \textbf{<0.001} \\
    Context:Modifier - Comparative & 1.34 & 0.41 & 3.27 & \textbf{<0.001} \\
    \bottomrule
  \end{tabular}}
  \captionof{table}{Summary of fixed effects from mixed-effects logistic regression models by Gemini 1.5 Pro}
  \label{tab:stat_gemini}
\end{minipage}




\end{document}
