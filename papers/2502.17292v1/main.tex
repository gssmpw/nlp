% Template for the submission to:
%   The Annals of Probability           [aop]
%   The Annals of Applied Probability   [aap]
%   The Annals of Statistics            [aos]
%   The Annals of Applied Statistics    [aoas]
%   Stochastic Systems                  [ssy]
%
%Author: In this template, the places where you need to add information
%        (or delete line) are indicated by {???}.  Mostly the information
%        required is obvious, but some explanations are given in lines starting
%Author:
%All other lines should be ignored.  After editing, there should be
%no instances of ??? after this line.

% use option [preprint] to remove info line at bottom
% journal options: aop,aap,aos,aoas,ssy
% natbib option: authoryear

%\documentclass[onecolumn,journal]{IEEEtran}

\documentclass[letterpaper,11pt]{article}
\usepackage[margin=1in]{geometry}  % set the margins to 1in on all sides

\input{defs}
\usepackage{dsfont}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% cleveref
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[capitalize,noabbrev]{cleveref}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{proposition}{Proposition}{Propositions}
\crefname{remark}{Remark}{Remarks}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{definition}{Definition}{Definitions}
\crefname{conjecture}{Conjecture}{Conjectures}
\crefname{figure}{Fig.}{Figures}
\crefname{assumption}{Assumption}{Assumptions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NOTATIONS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand {\Acal} {{\mathcal{A}}}
\newcommand {\Bcal} {{\mathcal{B}}}
\newcommand {\Ocal} {{\mathcal{O}}}
\newcommand {\Fcal} {{\mathcal{F}}}
\newcommand {\widetildeO} {{\widetilde{O}}}
% \newcommand{\htheta}{{\widehat{\theta}}}

\newcommand{\indic}{\mathds{1}}
\newcommand{\indt}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
\newcommand{\Tr}{\mathsf{tr}}
\newcommand{\Var}{\mathsf{Var}}

% \DeclareMathOperator*{\argmax}{arg\,max}
% \DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\hr}{\widehat{r}}
\newcommand{\hb}{\widehat{b}}
\newcommand{\cb}{\check{b}}
\newcommand {\E} {{\mathbb{E}}}
\newcommand {\R} {{\mathbb{R}}}
% \newcommand {\prob} {{\mathbb{P}}}

\newcommand{\reg}{\mathsf{R}}
\newcommand{\Regadv}{\mathsf{Reg}_{\mathrm{adv}}}
\newcommand{\Reglpo}{\mathsf{Reg}_{\mathrm{lpo}}}
\newcommand{\Reglte}{\mathsf{Reg}_{\mathrm{lte}}}

\newcommand{\YW}[1]{\textcolor{orange}{[YW: #1]}}
\newcommand{\YH}[1]{\textcolor{blue}{[Yanjun: #1]}}

\newcommand\numberthis{\addtocounter{equation}
{1}\tag{\theequation}}

% \newcommand{\stepa}[1]{\overset{\text{(a)}}{#1}}
% \newcommand{\stepb}[1]{\overset{\text{(b)}}{#1}}
% \newcommand{\stepc}[1]{\overset{\text{(c)}}{#1}}
% \newcommand{\stepd}[1]{\overset{\text{(d)}}{#1}}
% \newcommand{\stepe}[1]{\overset{\text{(e)}}{#1}}
% \newcommand{\stepf}[1]{\overset{\text{(f)}}{#1}}

\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\nor}{\|}{\|}
\DeclarePairedDelimiter{\parr}{(}{)}
\DeclarePairedDelimiter{\parq}{[}{]}
\DeclarePairedDelimiter{\parqq}{\llbracket}{\rrbracket}
\DeclarePairedDelimiter{\bra}{\lbrace}{\rbrace}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
%\begin{frontmatter}

% "Title of the paper"
\title{Joint Value Estimation and Bidding in Repeated First-Price Auctions}

\author{Yuxiao Wen, Yanjun Han, Zhengyuan Zhou\thanks{Yuxiao Wen is with the Courant Institute of Mathematical Sciences, New York University, email: \url{yuxiaowen@nyu.edu}. Yanjun Han is with the Courant Institute of Mathematical Sciences and the Center for Data Science, New York University, email: \url{yanjunhan@nyu.edu}. Zhengyuan Zhou is with the Stern School of Business, New York University, and Arena Technologies, email: \url{zz26@stern.nyu.edu}.}}


%
\maketitle
%\thankstext{T1}{Footnote to the title with the ``thankstext'' command.}

\begin{abstract}%
We study regret minimization in repeated first-price auctions (FPAs), where a bidder observes only the realized outcome after each auction---win or loss. This setup reflects practical scenarios in online display advertising where the actual value of an impression depends on the difference between two potential outcomes, such as clicks or conversion rates, when the auction is won versus lost. We analyze three outcome models: (1) adversarial outcomes without features, (2) linear potential outcomes with features, and (3) linear treatment effects in features. For each setting, we propose algorithms that jointly estimate private values and optimize bidding strategies, achieving near-optimal regret bounds. Notably, our framework enjoys a unique feature that the treatments are also actively chosen, and hence eliminates the need for the overlap condition commonly required in causal inference.
\end{abstract}

\begin{keywords}%
First-price auctions; Contextual bandits; Linear bandits; Causal inference. %
\end{keywords}

\section{Introduction}
\label{sec:intro}
Recent years have seen an industry-wide shift from second-price to first-price auctions (FPAs) in online advertising, beginning with platforms like OpenX, AppNexus, and Index Exchange \cite{Exchange}, and extending to Google AdX \cite{Google} and AdSense \cite{wong2021moving}, the world's largest ad exchanges. Unlike second-price auctions, FPAs are non-truthful, introducing critical challenges for real-time algorithmic bidding. A key challenge is the need to learn the distribution of the highest other bids (HOBs). To address this, \cite{balseiro2023contextual,han2020learning,han2024optimal} introduced the framework of regret minimization in repeated FPAs, developing algorithms that achieve near-optimal regret under various feedback mechanisms. This framework has since been extended to accommodate practical considerations, such as latency \cite{zhang2021meow}, additional side information \cite{zhang2022leveraging}, budget constraints \cite{wang2023learning}, return on investment (ROI) constraints \cite{aggarwal2024no}, to name a few. We refer to a recent survey \cite{aggarwal2024auto} for an overview of these developments. 

\begin{figure*}[ht]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{FPA.png}}
\vskip -0.1in
\caption{The learner repeatedly bids for advertisement slots and observes the (random) outcome sales after winning or losing the auctions.}
\label{fig:FPA}
\end{center}
\vskip -0.2in
\end{figure*}

However, another important challenge in FPAs often overlooked by the existing literature is the estimation of the actual value of an impression (or advertising slot). In practice, this value is determined by the difference between two potential outcomes—commonly clicks or conversion rates—when the auction is won versus lost. As shown in \cref{fig:FPA}, an advertiser can observe the outcome of winning the advertising slot when he/she wins the auction, and the outcome of doing nothing when losing the auction; the value of the advertising slot is the difference between these two potential outcomes. Consequently, as \cite{waisman2024online} observes, the problem of value estimation can be framed as a specific instance of causal inference, where the target quantity is the treatment effect, and the treatment is determined by the bidding outcome. This introduces the following challenges:
\begin{itemize}
    \item First, as in all causal inference problems, only one potential outcome is observable for each subject, making direct estimation of the treatment effect inherently difficult. 
    \item Second, unique to bidding in FPAs, the treatment (winning or losing the auction) is determined by the bidding strategy, which, in turn, depends on the current estimate of the actual value. This interdependence complicates both the estimation and the bidding process.
\end{itemize}

To address the aforementioned challenges, practitioners often rely on randomized controlled experiments for value estimation or manually decouple value estimation and learning, treating them as independent processes. However, these approaches can be either prohibitively costly or introduce significant biases into the final bidding outcome. In this paper, we propose a framework for joint value estimation and bidding in FPAs, making the following contributions:
\begin{itemize}
    \item Problem formulation: We formalize the problem of joint value estimation and bidding in repeated FPAs, representing the value as the difference between two potential outcomes, observed upon winning and losing, respectively. We further introduce three models for the potential outcomes.
    \item Adversarial outcomes: When the potential outcomes can be adversarially chosen, we devise an EXP3-type algorithm that achieves an $\widetilde{O}(\sqrt{T})$ regret, during a time horizon $T$, against an oracle who knows the distribution of the HOBs. Due to the absence of additional contextual information in this setting, a natural oracle here only bids a constant price over time. 
    \item Linear potential outcomes: When some features are available and both potential outcomes are linear in the features, we develop an algorithm that achieves an $\widetilde{O}(\sqrt{dT})$ regret against an oracle who makes optimal feature-dependent bids. Here $d$ is the feature dimension, and the algorithm tries to learn both potential outcomes based on the linearity assumption. 
    \item Linear treatment effect: Under a more realistic assumption where only the treatment effect is linear in the features, we still manage to establish an $\widetilde{O}(\sqrt{dT})$ regret against an oracle who makes optimal feature-dependent bids. The algorithm consists of two parts: the estimation part applies the inverse propensity weighted (IPW) estimator to find an unbiased estimator of the treatment effect, which might suffer from a large variance without the overlap condition; the decision-making part of the algorithm uses a careful bidding strategy to neutralize the effect of estimation variance in bidding, and consequently eliminates the need of randomized trials or overlap conditions. 
\end{itemize}

\subsection{Related Work}

\paragraph{Bidding in FPAs.} FPAs, also known as descending-price or Dutch auctions, have a long history. Early study of FPAs typically took a game-theoretic view dating back to the foundational work of \cite{vickrey1961counterspeculation, myerson1981optimal}, and there has been significant progress in understanding the computational complexity of Bayesian Nash equilibrium for online FPAs \cite{wang2020bayesian,filos2021complexity,bichler2023computing,chen2023complexity,filos2024computation}. A different line of study takes the perspective of online learning where a single bidder would like to learn the ``environment'', mainly the behaviors of other bidders in the literature. Dating back to \cite{kleinberg2003value} in a different pricing context, this perspective has inspired a flurry of work \cite{han2020learning,zhang2022leveraging,balseiro2023contextual,badanidiyuru2023learning,wang2023learning,han2024optimal,aggarwal2024no} which establishes sublinear regrets under various feedback structures and environments in FPAs; we refer to a recent survey \cite{aggarwal2024auto} for an overview. However, this line of work assumes a known valuation and mainly focuses on learning HOBs or handling other constraints (budget, side information, etc), and the problem of joint value estimation and bidding is explicitly listed as an open direction in \cite{han2024optimal}.

\paragraph{Unknown valuation.} There is also a rich line of literature on learning unknown valuations in auctions. In second-price auctions, \cite{weed2016online} studies a regret minimization setting where the bidder observes the valuation only if he/she wins the auction; this setting is then generalized to other auction formats \cite{feng2018learning} including FPAs \cite{achddou2021fast}. A recent work \cite{cesa2024role} provides a systematic study of feedback mechanisms (termed as \emph{transparency}) and unknown valuations in FPAs and provides tight regret bounds in various settings. However, all these studies do \emph{not} view the valuation of a bidder as a treatment effect, while an empirical study \cite{waisman2024online} stresses the importance of a causal inference modeling. Although \cite{waisman2024online} does not directly study the regret minimization problem, they identify a unique challenge in FPAs that the goals of regret minimization and causal inference are not well-aligned (while in second-price auctions these tasks are aligned). This challenge precisely motivates the problem formulation of joint value estimation and bidding in the current work. 

We provide a more detailed comparison with \cite{cesa2024role} as it is closest to ours. First, as explained above, the unknown valuation is the potential outcome of winning in \cite{cesa2024role}; by contrast, since the unknown valuation in our model is a treatment effect, we need to deal with the scenario where only the treatment effect, but not the potential outcomes, could be learned. Second, no contextual information is assumed in \cite{cesa2024role}, so that dependence between the valuation and HOB is allowed; as a comparison, we adopt a causal inference framework with available contexts, and make the standard unconfoundedness assumption that the potential outcomes are conditionally independent of the HOBs given the contexts. In fact, in our first model of adversarial outcomes without contexts, both the algorithm and analysis are inspired by \cite{cesa2024role}.  

\paragraph{Causal inference.} The causal inference formulation of value estimation in FPAs in \cite{waisman2024online} assumes the unconfoundedness and overlap conditions; both conditions are standard in causal inference \cite{imbens2015causal}, and are essential for controlling the estimation error in policy learning \cite{athey2021policy,zhou2023offline}. There have also been some efforts in relaxing or removing the overlap condition, such as trimming \cite{yang2018asymptotic,branson2023causal}, policy shifts \cite{zhao2024positivity}, and bypassing the uniform requirements \cite{jin2022policy}. For example, the policy learning result in \cite{jin2022policy} only involves the overlap condition for the selected policy; by contrast, thanks to the special nature of the joint value estimation and bidding problem, we do not even require the overlap condition for the selected bid. 


\subsection{Organization}
The rest of the paper is organized as follows. \cref{sec:formulation_and_results} introduces the problem formulation and presents the main regret bounds under three different models. \cref{sec:no_context,sec:linear_po,sec:linear_te} work on the three models of adversarial outcomes, linear potential outcomes, and linear treatment effect, respectively, and develop the corresponding bidding strategies under each scenario. Conclusions and some future directions are discussed in \cref{sec:future-direction}. The proofs of technical results, including the regret lower bounds, are deferred to the appendices. 

\subsection{Notation}
\label{sec:notation}
For positive integers $n\in\mathbb{N}$, we let $[n] = \{1,2,\dots,n\}$. For an event $E$, we defined the indicator function $\indic[E]$ to be 1 if the event occurs and 0 otherwise. The standard asymptotic notations $O(\cdot), \Omega(\cdot),$ and $\Theta(\cdot)$ are used to suppress constant factors, and $\widetildeO(\cdot), \widetilde\Omega(\cdot),$ and $\widetilde\Theta(\cdot)$ to suppress poly-logarithmic factors. For a random variable $X$, $\E[X]$ and $\Var(X)$ denote its mean and variance respectively. Throughout, we use the filtration $\Fcal_t$ to denote all available history up to the beginning of round $t$, and write $\E_t[X] = \E[X|\Fcal_t]$ for simplicity. For a vector $x\in \R^d$ and a PSD matrix $A\in \R^{d\times d}$, let $\|x\|_A = \sqrt{x^\top Ax}$. For a cumulative distribution function $G$ on $[0,1]$, let its inverse be $G^{-1}(z) = \inf\bra{x\in[0,1]: G(x)\ge z}$.


\section{Problem Formulation and Main Results}
\label{sec:formulation_and_results}

In this section, we formulate this learning problem in FPAs and the three models under different assumptions on the potential outcomes. Subsequently, we discuss the key challenges of these formulations in \cref{subsec:challenge}, and summarize our main results in \cref{sec:main_results}.






\subsection{Problem Formulation}
\label{sec:formulation}

In this work, we consider a single bidder that jointly estimates her private values and bids against a population of other bidders in repeated FPAs over a time horizon $T$. At the beginning of each time $t$, the bidder participates in an auction and observes some contextual information $x_t$ (which could be empty). The bidder then makes a bid $b_t\in\Bcal=[0,1]$, and observes the highest other bid (HOB) $M_t$ at the end of each auction. Here we assume that the HOB $M_t$ is always revealed to the bidder, regardless of the outcome of the auction; this is known as the full-information feedback \cite{han2020learning,cesa2024role}, and is called the ``minimum-bid-to-win'' in the Google Ad Exchange. Depending on the comparison between $b_t$ and $M_t$, the bidder receives an outcome value $v_{t,1}\in[0,1]$ if she wins the auction and pays $b_t$, i.e. $b_t\geq M_t$, or an outcome value $v_{t,0}\in[0,1]$ if she loses the auction, i.e. $b_t<M_t$. The final realized payoff for the learner is
\begin{equation}\label{eq:payoff_formulation}
\begin{split}
r_t(b_t) &= \indic[b_t\geq M_t](v_{t,1}-b_t) + \indic[b_t<M_t]v_{t,0} \\
&= \indic[b_t\geq M_t](v_{t,1}-v_{t,0}-b_t) + v_{t,0}. 
\end{split}
\end{equation}

The main feature in the reward function \eqref{eq:payoff_formulation} is that up to an unimportant additive constant $v_{t,0}$, optimizing the bid $b_t$ amounts to predict the HOB $M_t$ and estimate the \emph{treatment effect} $v_{t,1}-v_{t,0}$. The standard nonconfoundedness assumption in causal inference \cite{imbens2015causal} states that the estimation problems for $M_t$ and $(v_{t,1}, v_{t,0})$ can be decoupled. 
\begin{assumption}[Unconfoundedness]\label{ass:noconfounding}
It holds that $(v_{t,1}, v_{t,0}) \indt M_t \mid x_t$. 
\end{assumption}

As in many causal inference problems, \cref{ass:noconfounding} of no unmeasured confounders is vital to avoid Simpson's paradox and make estimators like inverse propensity weighting (IPW) work. Specializing to bidding in FPAs, even for $v_{t,0}=0$ and $x_t=\varnothing$, dependence between $v_{t,1}$ and $M_t$ could lead to a $\Omega(T)$ regret (see, e.g. \cite[Theorem 8]{han2024optimal} and \cite[Theorem 8]{cesa2024role}). As for the joint distribution of $(v_{t,1},v_{t,0},x_t)$ over time, we also adopt the following conditional independence assumption, which essentially states that the context sequence $\{x_t\}_{t=1}^T$ is chosen by an oblivious adversary (e.g. $x_t$ cannot depend on $b_{t-1}$ or $M_{t-1}$).
\begin{assumption}[Conditional independence]\label{ass:context}
Conditioned on the contexts $\{x_t\}_{t=1}^T$, the random vectors $\{(v_{t,1}, v_{t,0}, M_t)\}_{t=1}^T$ are conditionally independent over time. 
\end{assumption}

Thanks to \cref{ass:noconfounding}, \cref{ass:context}, and the full-information feedback on $M_t$, the estimation of $M_t$ is done independently of either value estimation or bidding: regardless of the generation processes of $(v_{t,1},v_{t,0},b_t)$, the bidder always observes the same trajectory $\{(x_t, M_t)\}_{t=1}^T$, which is also the only useful information for predicting future HOBs. By contrast, the problem of joint value estimation and bidding lies in the interdependence between $(v_{t,1},v_{t,0})$ and $b_t$: the bidder chooses the bid $b_t$ based on the current estimate of $v_{t,1}-v_{t,0}$, while the choice of $b_t$ affects the bidder's likelihood of observing the potential outcomes $v_{t,1}$ or $v_{t,0}$. Since the primary focus of this paper is on the problem of joint value estimation and bidding, in the sequel we will focus less on the modeling and estimation of $M_t$, and use the following black-box model instead. 

Specifically, for the modeling of $M_t$, we let $G_t$ be a generic time-varying cumulative distribution function (CDF) of $M_t$ which possibly depends on the context $x_t$. Then $G_t(b) = \mathbb{P}(b\geq M_t)$ and the expected payoff for bidding $b_t$ is
\begin{equation}\label{eq:expected_payoff_formulation}
\begin{split}
\E[r_t(b_t)] &= G_t(b_t)(v_{t,1}-b_t) + (1-G_t(b_t))v_{t,0} \\
&= G_t(b_t)(v_{t,1} - v_{t,0} - b_t) + v_{t,0}.
\end{split}
\end{equation}
Throughout this paper we will always make the following assumption on $G_t$.
\begin{assumption}[Bounded density]\label{ass:Gt}
The CDF $G_t$ is $L$-Lipschitz, i.e. it admits a density $g_t$ with $\|g_t\|_\infty\le L$. 
\end{assumption}

Also, for the estimation of $G_t$, we assume a general estimation oracle which outputs an estimator $\widehat{G}_t$ of $G_t$, based on the available and relevant information $\{x_s, M_s\}_{s<t} \cup \{x_t\}$. Consequently, our regret bounds will depend on some closeness parameter between $\widehat{G}_t$ and $G_t$ which will be specified in the explicit regret bounds and validated through simple examples. 

The main distinct feature of this work lies in the modeling of potential outcomes $(v_{t,1}, v_{t,0})$. First, recall that $(v_{t,1}, v_{t,0})$ has a crucial feedback structure: the bidder only observes $v_{t,1}$ if $b_t\ge M_t$, and only observes $v_{t,0}$ if $b_t<M_t$. If we refer all observations of $v_{t,1}$ as the ``treatment group'', then the propensity score of a treatment is $\mathbb{P}(b_t\ge M_t) = G_t(b_t)$, a value depending on the bid $b_t$. This is precisely the main difficulty we encounter in the problem of joint value estimation and bidding. Second, for the distributional assumptions on $(v_{t,0}, v_{t,1})$ and possibly $x_t$, we consider three different models as follows. 

\subsubsection{Model 1: Adversarial Outcomes}
\label{sec:model1}

In the first model, we have no context (i.e. $x_t = \varnothing$), and make no distributional assumptions on the potential outcomes $(v_{t,1},v_{t,0})$. Specifically, we assume that both $\{(v_{t,1},v_{t,0})\}_{t=1}^T$ and the CDFs $\{G_t\}_{t=1}^T$ (satisfying \cref{ass:Gt}) are generated by an oblivious adversary. Thanks to the independence of $(v_{1,t}, v_{0,t})$ and $M_t$ in \cref{ass:noconfounding}, the expected payoff takes the form \eqref{eq:expected_payoff_formulation}. In addition, since no context is available, a natural (context-dependent) oracle is to bid the optimal \emph{fixed} price $b_*\in \Bcal$ in hindsight, i.e. the regret of a policy $\pi = (b_1,\dots,b_T)$ in this model is defined as
\begin{equation}\label{eq:def_regret_adv}
\begin{split}
\Regadv(\pi) &= \sup_{\{(v_{t,1},v_{t,0},G_t)\}_{t=1}^T }\sup_{b_*\in\Bcal}\sum_{t=1}^T \left(\E[r_t(b_*)] - \E[r_t(b_t)]\right) \\
&= \sup_{\{(v_{t,1},v_{t,0},G_t)\}_{t=1}^T }\sup_{b_*\in\Bcal}\sum_{t=1}^T \left(G_t(b_*)(v_{1,t}-v_{0,t}-b_*) - \E[G_t(b_t)(v_{1,t}-v_{0,t}-b_t)] \right). 
\end{split}
\end{equation}
Here the expectation in the second line of \eqref{eq:def_regret_adv} is over the randomness in a possibly randomized bidding strategy $b_t$. The bidder's objective is to find a policy $\pi$ that minimizes $\Regadv(\pi)$. 


\subsubsection{Model 2: Linear Potential Outcomes}
\label{sec:model2}
In practical scenarios, the bidder has access to contextual information $x_t$ based on which he/she can estimate the potential outcomes $(v_{t,1},v_{t,0})$. A simple yet popular model is the linear model, where 
\begin{equation*}
\E [v_{t,0}] = \theta_{*,0}^{\top}x_t,\quad\quad \E [v_{t,1}] = \theta_{*,1}^{\top}x_t
\end{equation*}
hold for some unknown parameters $\theta_{*,0}, \theta_{*,1}\in\R^d$ with $\|\theta_{*,0}\|_2, \|\theta_{*,1}\|_2\leq 1$. Here we assume that the context $x_t\in \mathbb{R}^d$ is an $d$-dimensional vector with $\|x_t\|_2\leq 1$, and the context sequence $\{x_t\}_{t=1}^T$ is chosen by an oblivious adversary. Given the presence of contexts $x_t$, in this model we compete against a more powerful time-varying oracle and define the regret as
\begin{equation}\label{eq:reg_def_lpo}
\Reglpo(\pi) = \sup_{\{G_t,x_t\}_{t=1}^T, \theta_{*,0}, \theta_{*,1}} \sum_{t=1}^T \E[r_t\parr*{b_*(x_t)}] - \E[r_t(b_t)], 
\end{equation}
where $b_t$ is chosen by the learner's policy $\pi$ and $b_*(x_t)$ is the optimal bid of the oracle who has full knowledge of the true parameters $\theta_{*,0}, \theta_{*,1}$ and the CDFs $G_t$:
\begin{align}\label{eq:optimal-bid}
b_*(x_t) = \argmax_{b\in\Bcal}\E [r_t(b)] = \argmax_{b\in\Bcal}G_t(b)\parr*{\theta_{*,1}^{\top}x_t -b} + \parr*{1-G_t(b)}\theta_{*,0}^{\top}x_t.
\end{align}
The bidder's objective is to find a policy $\pi$ that minimizes $\Reglpo(\pi)$, where ``lpo'' stands for ``linear potential outcomes''. 

\subsubsection{Model 3: Linear Treatment Effect}
\label{sec:model3}
Finally, we relax our assumption on the linear structures and only let 
\[
\E[v_{t,1}- v_{t,0}] = \theta_*^{\top}x_t
\]
for an unknown parameter $\theta_*\in\R^d$ with $\|\theta_*\|_2\leq 1$. This models a more practical scenario where the potential outcomes $(v_{t,1}, v_{t,0})$ could have more complicated structures than the treatment effect $v_{t,1} - v_{t,0}$. Again, we assume that $\|x_t\|_2\le 1$, and that $\{x_t\}_{t=1}^T$ is chosen by an oblivious adversary. Once again, we compete against a time-varying oracle and define the regret as
\begin{equation}\label{eq:reg_def_lte}
\Reglte(\pi) = \sup_{\{G_t,x_t\}_{t=1}^T, \theta_{*}} \sum_{t=1}^T \E[r_t\parr*{b_*(x_t)}] - \E[r_t(b_t)], 
\end{equation}
where ``lte'' stands for ``linear treatment effect'', and $b_*(x_t)$ is the optimal context-dependent bid defined in \eqref{eq:optimal-bid}. 



\subsection{Key Challenges}\label{subsec:challenge}

As discussed in Section~\ref{sec:intro}, the inherent difficulty comes from the causal inference of the treatment effect $v_{t,1}-v_{t,0}$ and the interdependence between the inference and the bidding process. Specifically, two main questions arise in the process of value estimation and bidding: 
\begin{enumerate}
    \item Could the bidder perform value estimation on the fly, or need randomized experiments which could be costly and incur a large regret? 
    \item Would the bidding process lead to a propensity score too close to $0$ or $1$, and if so, incur a large estimation error and a large regret in future bidding?
\end{enumerate}

For the first question, fortunately, the answer is affirmative and the bidder could perform value estimation on the fly. Specifically, for Model 1, an EXP3-type algorithm could be applied to directly estimate the rewards for each bid and use them for decision making. In Model 2, we apply a classical arm elimination algorithm for linear contextual bandits, and the possibility of not relying on randomized experiments (i.e. \emph{exploration-free} algorithms) has been observed in this line of literature \cite{bastani2021mostly,han2020sequential}. One complication is a possible dependence structure between decision making and estimation error, and we employ a master algorithm in \cite{auer2002using} to decouple the dependence. 

Bidding in Model 3 is more complicated, and is the scenario where the second question above arises. Since only the treatment effect is linear in the context and we do not have a model for the potential outcomes, a good estimator of $\E[v_{t,1}-v_{t,0}]=\theta_*^\top x_t$ is necessary for the estimation of $\theta_*$. A popular estimator in the causal inference literature is the IPW estimator: assuming for the ease of exposition that the CDF $G_t$ is perfectly known, then the IPW estimator
\begin{equation}\label{eq:weighted_estimator_te}
\widehat{e}_t(b_t) = \frac{\indic[b_t\geq M_t]v_{t,1}}{G_t(b_t)} - \frac{\indic[b_t< M_t]v_{t,0}}{1-G_t(b_t)}
\end{equation}
satisfies $\E_t[\widehat{e}_t(b_t)] = \theta_*^\top x_t$. A key issue arises from the variance of this estimator, which is bounded by 
\[
\Var_t(\widehat{e}_t(b_t))\leq 2\max\bra*{G_t(b_t)^{-1}, (1-G_t(b_t))^{-1}}.
\]
The optimal bid $b_t$, however, is often likely to either win or lose the auction---i.e. either $G_t(b_t)$ or $1-G_t(b_t)$ approaches $0$. This leads to an unbounded variance in the estimator $\widehat{e}_t(b_t)$ and thereby breaks the regret guarantee for linear contextual bandit algorithms in the literature. To address this issue, the common approach to reduce the estimation error is to impose the \emph{overlap condition}, i.e. require that $G_t(b_t)\in [\eta, 1-\eta]$ for some $\eta>0$ and therefore lead to non-greedy strategies for $b_t$. Nonetheless, we show that the overlap condition is not necessary by exploiting two equivalent forms of the reward function in \eqref{eq:expected_payoff_formulation}, with leading coefficient $G_t(b_t)$ or $1-G_t(b_t)$ on the decision $b_t$. Our final bidding algorithm then selects the better reward formulation in a data-driven manner, and manages to mitigate the effect of a possibly large variance in decision making. 


\subsection{Main Results}
\label{sec:main_results}

% Table
\begin{table}%
	\caption{Minimax Regret Characterizations for the Three Models}
	\label{tab:regrets}
	\begin{minipage}{\columnwidth}
		\begin{center}
			\begin{tabular}{cccc}
				\toprule
				   & Adversarial Outcomes & Linear Potential Outcomes & Linear Treatment Effect\\
                   \midrule
				Upper Bound  & $\widetildeO(\sqrt{T})$ & $\widetildeO(\sqrt{dT} + \Delta)$ & $\widetildeO(\sqrt{\Delta_2dT} + \Delta_1)$\\
                \midrule
			     Lower Bound  & $\Omega(\sqrt{T})$ & $\Omega(\sqrt{dT})$ & $\Omega(\sqrt{dT})$\\
				\bottomrule
			\end{tabular}
		\end{center}
		\bigskip\centering
		\footnotesize\emph{Note:} The quantities $\Delta, \Delta_1, \Delta_2$ concern the performance of the estimation oracle, which we view as a black box. 
	\end{minipage}
\end{table}%

Under Assumptions \ref{ass:noconfounding}--\ref{ass:Gt}, we are in a position to present the regret guarantees in all three models. For the adversarial outcome model (Model 1), the estimation oracle $\widehat{G}_t$ turns out to be unnecessary, and we have the following unconditional regret guarantee. 
\begin{theorem}[Upper bound for Model 1]\label{thm:main_exp3_upper}
Under Assumptions \ref{ass:noconfounding}--\ref{ass:Gt} and Model 1 in \cref{sec:model1}, there is an algorithm $\pi_{\mathrm{adv}}$ that achieves the expected regret
\[
\Regadv(\pi_{\mathrm{adv}}) = \widetildeO(\sqrt{T}).
\]
\end{theorem}

The regret upper bounds under the other models will be conditional on the performance of the estimation oracle $\widehat{G}_t$, for explicit estimation of potential outcomes or the treatment effect becomes necessary. Despite this conditional nature, we stress that the estimation of $G_t$ is a totally independent problem based only on the trajectory $\{(x_t, M_t)\}_{t=1}^T$. For Model 2, we will make the following assumption on the estimation oracle: 

\begin{assumption}[Estimation oracle I]\label{ass:est-oracle-1}
With probability at least $1-T^{-1}$, there is a sequence $\{\varepsilon_t\}_{t=1}^T$ such that $\|\widehat{G}_t - G_t\|_\infty \le \varepsilon_t$ for all $t\in [T]$. 
\end{assumption}

Based on \cref{ass:est-oracle-1}, we have the following regret guarantee for Model 2. 

\begin{theorem}[Upper bound for Model 2]\label{thm:main-lpo}
Under Assumptions \ref{ass:noconfounding}--\ref{ass:Gt}, \ref{ass:est-oracle-1}, and Model 2 in \cref{sec:model2}, there is an algorithm $\pi_{\mathrm{lpo}}$ that achieves the expected regret
\begin{align*}
\Reglpo(\pi_{\mathrm{lpo}}) = \widetilde{O}\left(\sqrt{dT} + \Delta\right), 
\end{align*}
where $\Delta := \sum_{t=1}^T \varepsilon_t$ is the sum of estimation errors. 
\end{theorem}

\Cref{thm:main-lpo} shows an $\widetilde{O}(\sqrt{dT})$ regret whenever the estimation of the HOB distribution $G_t$ does not suffer from a too large error. For example, when $M_t$ is i.i.d., the famous Dvoretzky--Kiefer--Wolfowitz inequality \cite{dvoretzky1956asymptotic} states that $\varepsilon_t = \widetilde{O}(1/\sqrt{t})$ for the empirical CDF $\widehat{G}_t$, and $\Delta = \widetilde{O}(\sqrt{T})$ is smaller than the $\widetilde{O}(\sqrt{dT})$ regret. 

For Model 3, we will assume a slightly stronger estimation oracle, motivated by the Bernstein's concentration that a smaller estimation error is available for events with small probability. 

\begin{assumption}[Estimation oracle II]\label{ass:est-oracle-2}
With probability at least $1-T^{-1}$, there is a sequence $\{\delta_t\}_{t=1}^T$ such that
\begin{align*}
\left|\widehat{G}_t(b) - G_t(b) \right| \le \delta_t\sqrt{G_t(b)(1-G_t(b))} + \delta_t^2, \qquad \forall b\in \Bcal, t\in [T].
\end{align*}
\end{assumption}

Based on \cref{ass:est-oracle-2}, we have the following regret guarantee for Model 3. 

\begin{theorem}[Upper bound for Model 3]\label{thm:main-lte}
Under Assumptions \ref{ass:noconfounding}--\ref{ass:Gt}, \ref{ass:est-oracle-2}, and Model 3 in \cref{sec:model3}, there is an algorithm $\pi_{\mathrm{lte}}$ (with the knowledge of the confidence bounds $\{\delta_t\}_{t=1}^T$) that achieves the expected regret
\begin{align*}
\Reglte(\pi_{\mathrm{lte}}) = \widetilde{O}\left(\sqrt{\Delta_2 dT} + \Delta_1\right), 
\end{align*}
where $\Delta_1 := \sum_{t=1}^T \delta_t, \Delta_2 := 1 + \sum_{t=1}^T \delta_t^2$ are essentially the $\ell_1$ and $\ell_2^2$ norms of $\{\delta_t\}_{t=1}^T$, respectively. 
\end{theorem}

Again, when $M_t$ is i.i.d., Bernstein's concentration inequality shows that \cref{ass:est-oracle-2} holds with $\delta_t = \widetilde{O}(1/\sqrt{t})$, so that $\Delta_1 = \widetilde{O}(\sqrt{T})$ and $\Delta_2 = \widetilde{O}(1)$. Therefore, \cref{thm:main-lte} still leads to an $\widetilde{O}(\sqrt{dT})$ regret even if only the treatment effect possesses a linear structure. We also remark that \cref{ass:Gt} crucially does not assume a density lower bound; therefore, it could happen that the overlap condition is violated (i.e. $G_t(b_t)$ is close to $0$ or $1$), but the regret upper bound in \cref{thm:main-lte} remains unaffected. 

Finally, we show that all these $\widetilde{O}(\sqrt{T})$ or $\widetilde{O}(\sqrt{dT})$ regret upper bounds are not improvable in general, by providing matching lower bounds. 
\begin{theorem}[Lower bounds]\label{thm:lower-bound}
Under Assumptions \ref{ass:noconfounding}--\ref{ass:Gt}, even if $G_t$ is perfectly known, the following lower bounds hold for the three models: 
\begin{enumerate}
    \item Model 1: $\inf_\pi \Regadv(\pi) = \Omega(\sqrt{T})$; 
    \item Model 2: $\inf_\pi \Reglpo(\pi) = \Omega(\sqrt{dT})$ for $T\ge d^2$;
    \item Model 3: $\inf_\pi \Reglte(\pi) = \Omega(\sqrt{dT})$ for $T\ge d^2$. 
\end{enumerate}
\end{theorem}
The lower bounds in \cref{thm:lower-bound} are established by classical information-theoretic arguments, and for completeness we include the proofs in the appendix. \cref{tab:regrets} presents a summary of the above results. 



\section{Model 1: Adversarial Outcomes}
\label{sec:no_context}

We start with the problem in which no contextual information is provided to the learner. The model is formulated in Section~\ref{sec:model1}, and the learner's objective is to minimize the regret $\Regadv(\pi)$. As a first step, we consider a finite bid set $\Bcal'\subseteq \Bcal$ that comes from some sort of discretization of the bid space. We construct EXP3-FPA-TE (Algorithm~\ref{alg:EXP3_FPA}), an extension of the EXP3-FPA algorithm in \cite{cesa2024role}, that achieves $\widetildeO(\sqrt{T})$ expected regret when competing against the best fixed bid within the finite subset $\Bcal'$. Then EXP3-FPA-TE can be coupled with a simple discretization to achieve near-optimal regret on the continuous bid space $\Bcal$.

\begin{algorithm}[h!]\caption{EXP3-FPA-TE (EXP3 for first-price auctions with treatment effects)}
\label{alg:EXP3_FPA}
\textbf{Input:} time horizon $T$, finite bid set $\Bcal'\subseteq \Bcal$, learning rate $\eta\in\parr*{0,\frac{1}{2}}$.

\textbf{Initialize:} set $w_{t,b}=1$ for each $b\in\Bcal'$.

\For{$t=1$ \KwTo $T$}
{   
Set probability $p_{t}(b) = (1-2\eta)\frac{w_{t,b}}{\sum_{b'\in\Bcal'}w_{t,b'}} + \eta\delta_{b_{\min}} + \eta\delta_{b_{\max}}$.

Draw bid $b_t\sim p_{t}$.

Make bid $b_t$, observe $M_t$, and build payoff estimator for every $b\in\Bcal'$:
\[
\widehat{r}_t(b) = \frac{\indic[b_t\geq M_t]}{\sum_{b'\geq M_t}p_t(b')}\indic[b\geq M_t](v_{1,t} - b) + \frac{\indic[b_t< M_t]}{\sum_{b'< M_t}p_t(b')}\indic[b< M_t]v_{t,0}.
\]

For every $b\in\Bcal'$, update $w_{t+1,b} = w_{t,b}\exp(\eta \hr_t(b))$.
}

\end{algorithm}

Let $\delta_b$ denote the Dirac measure on the bid $b\in\Bcal'$. EXP3-FPA-TE maintains an exponential weight for each bid in the finite subset. At each time, it draws a bid based on these weights and an $\eta$-exploration on the largest and the smallest bid. Then the algorithm builds an unbiased estimator for the payoff for every candidate bid $b\in\Bcal'$ and updates the weights accordingly.

\begin{theorem}\label{thm:exp3_fpa_regret_upper_bound}
When $\log|\Bcal'|< (e-1)T$, EXP3-FPA-TE with $\eta = \sqrt{\frac{\log|\Bcal'|}{(4e-2)T}}\in\parr*{0,\frac{1}{2}}$ achieves expected regret
\[
\E\parq*{\max_{b'\in\Bcal'}\sum_{t=1}^T r_t(b') - \sum_{t=1}^T\sum_{b\in\Bcal'}p_t(b) r_t(b)} \leq \sqrt{(4e-2)\log|\Bcal'|T}.
\]
\end{theorem}
\begin{proof}
Note that $\eta\hr_t(b)\leq 1$ for every $t\in[T]$ and $b\in\Bcal'$, thanks to the $\eta$ amount of exploration on $b_{\min}$ and $b_{\max}$. Let $W_t = \sum_{b\in\Bcal'}w_{t,b}$. Following the standard analysis for EXP3, for each $t\in[T]$,
\begin{align*}
\frac{W_{t+1}}{W_t} &= \sum_{b\in\Bcal}\frac{w_{t,b}}{W_t}\exp(\eta\hr_t(b))\\
&\leq \sum_{b\in\Bcal'}\frac{w_{t,b}}{W_t}\parr*{1 + \eta\hr_t(b) + (e-2)\eta^2\hr_t(b)^2}\\
&= 1 + \eta\sum_{b\in\Bcal'}\frac{w_{t,b}}{W_t}\parr*{\hr_t(b) + (e-2)\eta\hr_t(b)^2}\\
&\leq 1 + \frac{\eta}{1-2\eta}\sum_{b\in\Bcal'}p_t(b)\parr*{\hr_t(b) + (e-2)\eta\hr_t(b)^2}
\end{align*}
Summing over the time horizon, we have
\begin{align*}
\log W_{T+1} - \log W_1 = \sum_{t=1}^T\log \frac{W_{t+1}}{W_t} \leq \frac{\eta}{1-2\eta}\sum_{t=1}^T\sum_{b\in\Bcal'}p_t(b)\parr*{\hr_t(b) + (e-2)\eta\hr_t(b)^2}.
\end{align*}
Fix any $b_*\in\Bcal'$. Since $W_1 = |\Bcal'|$ and $W_{T+1} = \sum_{b\in\Bcal'}\exp\parr*{\eta\sum_{t=1}^T\hr_t(b)}\geq \exp\parr*{\eta\sum_{t=1}^T\hr_t(b_*)}$, 
\begin{equation}\label{eq:exp3_ineq}
\sum_{t=1}^T\hr_t(b_*) - \frac{\log|\Bcal'|}{\eta} \leq \frac{1}{1-2\eta}\sum_{t=1}^T\sum_{b\in\Bcal'}p_t(b)\parr*{\hr_t(b) + (e-2)\eta\hr_t(b)^2}.
\end{equation}
Note that the quadratic payoff term can be bounded as follows.
\begin{align*}
\frac{1}{2}\hr_t(b)^2 &\leq \frac{\indic[b_t\geq M_t]}{\parr*{\sum_{b'\geq M_t}p_t(b')}^2}\indic[b\geq M_t](v_{1,t} - b)^2 + \frac{\indic[b_t< M_t]}{\parr*{\sum_{b'< M_t}p_t(b')}^2}\indic[b< M_t]v_{t,0}^2\\
&\leq \frac{\indic[b_t\geq M_t]}{\parr*{\sum_{b'\geq M_t}p_t(b')}^2}\indic[b\geq M_t] + \frac{\indic[b_t< M_t]}{\parr*{\sum_{b'< M_t}p_t(b')}^2}\indic[b< M_t].
\end{align*}
Let $\E_t[\cdot] = \E[\cdot|\mathcal{F}_t]$ w.r.t. the filtration at time $t$. For every $t\in[T]$ and $b\in\Bcal$, it holds that $\E_t[\hr_t(b)] = r_t(b)$ and 
\begin{align*}
\E_t\parq*{\sum_{b\in\Bcal'}p_t(b)\hr_t(b)^2} &\leq \E_t\parq*{2\sum_{b\in\Bcal'}p_t(b)\frac{\indic[b_t\geq M_t]\indic[b\geq M_t]}{\parr*{\sum_{b'\geq M_t}p_t(b')}^2} + p_t(b)\frac{\indic[b_t< M_t]\indic[b< M_t]}{\parr*{\sum_{b'< M_t}p_t(b')}^2}}\\
&= \E_t\parq*{2\sum_{b\in\Bcal'}p_t(b)\frac{\indic[b\geq M_t]}{\sum_{b'\geq M_t}p_t(b')} + p_t(b)\frac{\indic[b< M_t]}{\sum_{b'< M_t}p_t(b')}}\\
&= 4.
\end{align*}
Now taking expectation of \eqref{eq:exp3_ineq} and rearranging give us, for any $b_*\in\Bcal'$,
\begin{align*}
\E\parq*{\sum_{t=1}^T r_t(b_*) - \sum_{t=1}^T\sum_{b\in\Bcal'}p_t(b) r_t(b)} &\leq \frac{\log|\Bcal|}{\eta} + (4e-2)\eta T.
\end{align*}
\end{proof}

To handle the continuous bid space $\Bcal$, we consider a simple discretization $\Bcal' = \bra*{\frac{k}{T}:k\in[T]}$. Thanks to the Lipschitzness of the CDF $G_t$ in Assumption~\ref{ass:Gt}, we are able to approximate the optimal fixed bid in $\Bcal$ with the optimal bid in the discretization, which leads to the following result.
\begin{corollary}[\cref{thm:main_exp3_upper}]
Let $\Bcal' = \bra*{\frac{k}{T}:k\in[T]}\subseteq\Bcal$. Let $\pi$ be the resulting bidding strategy of \cref{alg:EXP3_FPA} running on this discretization with $\eta = \sqrt{\frac{\log(T)}{(4e-2)T}}$. Then under Assumptions \ref{ass:noconfounding}--\ref{ass:Gt}, the policy $\pi$ achieves expected regret
\[
\Regadv(\pi) =O\parr*{\sqrt{T\log T}}.
\]
\end{corollary}
\begin{proof}
Let $b_* = \argmax_{b\in\Bcal}\sum_{t=1}^T\E[r_t(b)]$ be the optimal fixed bid in the bid space. By Assumption~\ref{ass:Gt}, for any bid $b\in\Bcal$ and any $\varepsilon\in(0,1)$, we have
\[
\abs*{\E[r_t(b+\varepsilon)] - \E[r_t(b)]} = \abs{G_t(b+\varepsilon)(v-b-\varepsilon) - G_t(b)(v-b)} \leq (2L+1)\varepsilon
\]
where $v=v_{t,1}-v_{t,0}\in[-1,1]$. Then by Theorem~\ref{thm:exp3_fpa_regret_upper_bound}, 
\begin{align*}
\Regadv(\pi) &= \E\parq*{\sum_{t=1}^Tr_t(b_*) - \sum_{t=1}^T\sum_{b\in\Bcal'}p_t(b)r_t(b)}\\
&\leq 2L+1 + \E\parq*{\max_{b'\in\Bcal'}\sum_{t=1}^T r_t(b') - \sum_{t=1}^T\sum_{b\in\Bcal'}p_t(b) r_t(b)}\\
&\leq 2L+1+\sqrt{(4e-2)T\log T}.
\end{align*}
\end{proof}


\section{Model 2: Linear Potential Outcomes}
\label{sec:linear_po}
In this section, we consider Model 2 proposed in Section~\ref{sec:model2} that incorporates the presence of contexts.

\subsection{A Bid Elimination Algorithm}
When both potential outcomes are linear in the context $x_t$, a natural strategy of estimating $\theta_{*,0}$ and $\theta_{*,1}$ is to run linear regression on the observed potential outcomes, respectively. To this end, we propose a master algorithm SUP-LIN-PO (Algorithm~\ref{alg:master_alg_po}) and a base algorithm LIN-PO (Algorithm~\ref{alg:base_alg_po}). The high-level idea is that, at each time $t$, SUP-LIN-PO eliminates suboptimal bids based on their confidence bounds provided by the base algorithm LIN-PO and selects a bid from the remaining candidates. We first present the base algorithm, and then explain the need for the master algorithm.


\subsubsection{Base algorithm}

\begin{algorithm}[!t]\caption{LIN-PO (Base algorithm for FPAs with linear potential outcomes)}
\label{alg:base_alg_po}
\textbf{Input:} Time indices $\Phi_t\subseteq [t-1]$, bid space $B_t\subseteq \Bcal$, estimated CDF $\widehat{G}_t$, time horizon $T$. 

Set $\beta\gets \sqrt{\log(2T)}$ and $\lambda \gets 1$. 

$A_{t,0}\leftarrow \lambda I + \sum_{s\in \Phi_t}\indic[b_s<M_s]x_sx_s^{\top}$;

$A_{t,1}\leftarrow \lambda I + \sum_{s\in \Phi_t}\indic[b_s\geq M_s]x_sx_s^{\top}$;

$z_{t,0} \leftarrow \sum_{s\in\Phi_t}\indic[b_s< M_s]v_{s,0}x_s$;

$z_{t,1} \leftarrow \sum_{s\in\Phi_t}\indic[b_s\geq M_s]v_{s,1}x_s$;

Compute estimators $\htheta_{t,0} \leftarrow A_{t,0}^{-1}z_{t,0}$ and $\htheta_{t,1} \leftarrow A_{t,1}^{-1}z_{t,1}$.

\For{$b\in B_t$}
{
Compute the reward estimator $\widehat{r}_t \gets \widehat{G}_t(b)\parr*{\htheta_{t,1}^{\top}x_t - b} + \parr*{1-\widehat{G}_t(b)}\htheta_{t,0}^{\top}x_t$. 

Compute the width $w_t(b)\gets (\beta+\lambda)\widehat{G}_t(b)\|x_t\|_{A_{t,1}^{-1}} + (\beta+\lambda)\parr*{1-\widehat{G}_t(b)}\|x_t\|_{A_{t,0}^{-1}}$.
}
\end{algorithm}

At time $t$, the base algorithm LIN-PO takes as input a subset of the bid space $B_t$ and a selected subset $\Phi_t\subseteq [t-1]$ of previous time indices. Based on the available observations $(x_s,M_s,b_s,\indic[b_s\ge M_s]v_{s,1}, \indic[b_s<M_s]v_{s,0})_{s\in \Phi_t}$, the standard ridge regression is applied to estimate $\theta_{*,0}$ and $\theta_{*,1}$ (with a regularization parameter $\lambda=1$): 
\begin{equation}\label{eq:ridge}
\begin{split}
    \widehat{\theta}_{t,0} &= \argmin_{\theta_0} \sum_{s\in \Phi_t} \indic[b_s<M_s](v_{s,0} - \theta_0^\top x_s)^2 + \lambda \|\theta_0\|_2^2 , \\
    \widehat{\theta}_{t,1} &= \argmin_{\theta_1} \sum_{s\in \Phi_t} \indic[b_s\ge M_s](v_{s,1} - \theta_1^\top x_s)^2 + \lambda \|\theta_1\|_2^2. 
\end{split}
\end{equation}
The expressions of $(\widehat{\theta}_{t,0}, \widehat{\theta}_{t,1})$ are computed in Line 7 of \cref{alg:base_alg_po}. After that, for each active bid $b\in B_t$, the base algorithm uses the plug-in approach to estimate the expected payoff function $\E[r_t(b)]$ in \eqref{eq:expected_payoff_formulation} by $\widehat{r}_t(b)$ in Line 9, as well as a width parameter $w_t(b)$ in Line 10. The rational behind these quantities is summarized in the following lemma. 

\begin{lemma}\label{lem:PO_width}
Suppose $(v_{s,1},v_{s,0})_{s\in \Phi_t}$ are conditionally independent given $(x_s,b_s,M_s)_{s\in \Phi_t}$, then with probability at least $1-T^{-2}$, it holds that
\begin{align*}
\abs*{\htheta_{t,i}^{\top}x_t - \theta_{*,i}^{\top}x_t} \leq (\beta+\lambda)\|x_t\|_{A_{t,i}^{-1}}, \quad \text{for both } i = 0, 1. 
\end{align*}
Conditioned on the above events, we further have
\begin{align*}
\abs*{\widehat{r}_t(b) - [\widehat{G}_t(b)(\theta_{*,1}^\top x_t - b) + (1-\widehat{G}_t(b))\theta_{*,0}^\top x_t]  } \le w_t(b), \quad \text{for all }b\in \Bcal. 
\end{align*}
\end{lemma}

\subsubsection{Master algorithm}

\begin{algorithm}[h!]\caption{SUP-LIN-PO (Master algorithm for FPA with linear potential outcomes)}
\label{alg:master_alg_po}
\textbf{Input:} Time horizon $T$, bid space $\Bcal= [0,1]$, estimation oracle $\Ocal$.

\textbf{Initialize:} Set $S\gets \lceil \log_2\sqrt{T} \rceil$, $\Phi^{(s)}_1=\varnothing$ for $s\in[S]$.

\For{$t=1$ \KwTo $T$}
{   
Observe the context vector $x_t\in\R^d$.

Estimate $\widehat{G}_t$ from $\widehat{G}_t \gets \Ocal\parr*{\{M_\tau, x_\tau\}_{\tau<t}; x_t}$.

Initialize $B_1\leftarrow \Bcal$.

\For{$s=1$ \KwTo $S$}
{

Invoke Algorithm~\ref{alg:base_alg_po} with time indices $\Phi^{(s)}_t$, space $B_s$, and CDF $\widehat{G}_t$ to compute $\bra*{\widehat{r}^{(s)}_{t}(b)}_{b\in B_s}$ and widths $\bra*{w^{(s)}_{t}(b)}_{b\in B_s}$.


\uIf{$\exists b\in B_s$ such that $w^{(s)}_t(b) > 2^{-s}$}{
Choose any such bid $b_t\leftarrow b$.\label{line:master_po_bid_select}

Update: $\Phi^{(s)}_{t+1} \leftarrow \Phi^{(s)}_t\cup \{t\}$ and $\Phi^{(s')}_{t+1} \leftarrow \Phi^{(s')}_t$ for $s'\neq s$. Break the inner for loop.
}\uElseIf{$w^{(s)}_t(b)\leq \frac{1}{\sqrt{T}}$ for all $b\in B_s$}{
Choose $b_t \leftarrow \max_{i\in B_s}\widehat{r}^{(s)}_t(i)$.\label{line:master_po_bid_select_small}

Do not update: $\Phi^{(s')}_{t+1}\leftarrow \Phi^{(s')}_t$ for all $s'\in[S]$. Break the inner for loop.
}\Else{
We have $w^{(s)}_t(b)\leq 2^{-s}$ for all $b\in B_s$. Proceed to bid elimination:\label{line:master_po_elim}

$\displaystyle B_{s+1}\leftarrow \bra*{b\in B_s : \widehat{r}^{(s)}_t(b) + w_t^{(s)}(b) \geq \max_{b^*\in B_s}\parr*{\widehat{r}^{(s)}_t(b^*) + w_t^{(s)}(b^*)} - 2\cdot 2^{-s}}.$

}
}



Bidder bids the chosen bid $b_t$.

Bidder observes $\indic[b_t\geq M_t]v_{t,1}$ and $ \indic[b_t<M_t]v_{t,0}.$
}
\end{algorithm}

Note that the conditional independence assumption in \cref{lem:PO_width} breaks down when $\Phi_t = [t-1]$ for all $t\in [T]$. To see this, note that the selection of the bid $b_t$ depends on the realized outcomes
\[
\{\indic[b_s\geq M_s]v_{s,1}, \indic[b_s < M_s] v_{s,0}\}_{s<t};
\]
therefore, conditioned on such a future bid $b_t$, the past potential outcomes $(v_{s,0},v_{s,1})_{s<t}$ may become dependent. To address this subtle dependence issue, we propose to use a master algorithm SUP-LIN-PO in \cref{alg:master_alg_po}. The idea behind this master algorithm is taken from \cite{auer2002using}, and has been subsequently used to address similar dependence issues in \cite{chu2011contextual,han2024optimal}. The purpose of this master algorithm is to partition the time indices $[t]$ into multiple stages, eliminate the suboptimal bids in a hierarchical manner, and select the bid $b_t$ only through the confidence width $w_t(b)$ in LIN-PO, instead of the point estimates $\widehat{r}_t(b)$. Crucially, the widths $w_t(b)$ are computed based on the contexts $(x_t,b_t,M_t)$ on the current stage, but do not involve the potential outcomes $(v_{t,1},v_{t,0})$ on this stage. By contrast, the point estimates $\widehat{r}_t(b)$ (which depend on the potential outcomes $(v_{s,1},v_{s,0})_{s<t}$ on this stage) are only revealed when we decide to move to the next stage (Line 17). This information structure turns out to be the key idea to ensure the conditional independence in \cref{lem:PO_width}: 

\begin{lemma}[Corollary of Lemma 14 in \cite{auer2002using}]
\label{lem:ind_rewards}
Under \cref{ass:noconfounding} and \cref{ass:context}, for every $s\in [S]$ and $t\in [T]$, $(v_{\tau,1},v_{\tau,0})_{\tau\in \Phi_t^{(s)}}$ are conditionally independent given $(x_\tau,b_\tau,M_\tau)_{\tau\in \Phi_t^{(s)}}$. 
\end{lemma}

Finally, we remark that while statistically \cref{alg:master_alg_po} leads to the target regret upper bound for the continuous bid space $\Bcal = [0,1]$, for computational considerations one can discretize $\Bcal$ to a fine enough resolution which does not affect the regret bound. In addition, Line 12 of \cref{alg:master_alg_po} ensures that we never reach $s=S+1$ in the inner for loop. 


\subsection{Regret Upper Bound}
The following theorem establishes the regret upper bound of \cref{alg:master_alg_po}. 

\begin{theorem}[Upper bound of \cref{alg:master_alg_po}]\label{thm:linear_po_reg_upper}
Let $\pi$ be the resulting bidding strategy of \cref{alg:master_alg_po} (combined with \cref{alg:base_alg_po}). Then under Assumptions \ref{ass:noconfounding}--\ref{ass:Gt} and \ref{ass:est-oracle-1}, the policy $\pi$ achieves an expected regret
\begin{align*}
\Reglpo(\pi) ={O}\parr*{\sqrt{dT}\log^2 T + \Delta \log^{3/2} T}, 
\end{align*}
where $\Delta = \sum_{t=1}^T \varepsilon_t$. 
\end{theorem}

It is clear that \cref{thm:linear_po_reg_upper} implies \cref{thm:main-lpo}. The remainder of this section is devoted to the proof of \cref{thm:linear_po_reg_upper}. Since all events with probability $O(T^{-1})$ only contribute $O(S)$ regret in total, WLOG we assume that the guarantees in \cref{ass:est-oracle-1} and \cref{lem:PO_width} hold almost surely. Adopting the notation\footnote{Warning: not to confuse $\widehat{r}_t(b)$ with $\widehat{r}_t^{(s)}(b)$ in \cref{alg:master_alg_po}: $\widehat{r}_t(b)$ is the reward w.r.t. $\widehat{G}_t(b)$, while $\widehat{r}_t^{(s)}(b)$ further plugs in the estimated parameters from stage $s$ for $\theta_{*,0}$ and $\theta_{*,1}$.}
\begin{align}\label{eq:reward-Ghat}
\widehat{r}_t(b) &= \widehat{G}_t(b)(\theta_{*,1}^\top x_t - b) + (1-\widehat{G}_t(b))\theta_{*,0}^\top x_t, 
\end{align}
we have the following lemma. 


\begin{lemma}
\label{lem:master_confidence_bound}
In \cref{alg:master_alg_po}, for every $t\in[T]$ and $s\in [S]$, it holds that $\max_{b^*\in \Bcal}\hr_t(b^*) - \hr_t(b) \leq 8\cdot 2^{-s}$ for all $b\in B_s$.
\end{lemma}

In other words, \cref{lem:master_confidence_bound} states that the any bid that has survived into stage $s$ is $O(2^{-s})$-close to optimal under $\widehat{r}_t$. Consequently, if we denote by $\Phi^{(s)}\subseteq [T]$ the set of time indices belonging to stage $s\in [S]$, and $\Phi^{(S+1)}\subseteq [T]$ the set of time indices where all widths are below $\frac{1}{\sqrt{T}}$, we get
\begin{align*}
\Reglpo(\pi) &= \sum_{t=1}^T \left(\max_{b^*\in \Bcal} \E[r_t(b^*)] - \E[r_t(b_t)]\right) \stepa{\le} \sum_{t=1}^T \left(2\varepsilon_t + \max_{b^*\in \Bcal} \widehat{r}_t(b^*) - \widehat{r}_t(b_t)\right) \\
&\stepb{\le} 2\Delta + 8\parr*{\sum_{s=1}^S 2^{-s}|\Phi^{(s)}| + \frac{|\Phi^{(S+1)}|}{2^S}} \stepc{\le} O(\Delta + \sqrt{T}) + 8\sum_{s=1}^S \sum_{t\in \Phi^{(s)}} w_t^{(s)}(b_t). 
\end{align*}
Here (a) uses $|\E[r_t(b)] - \widehat{r}_t(b)|\le \|G_t-\widehat{G}_t\|_\infty \le \varepsilon_t$ by \eqref{eq:expected_payoff_formulation}, \eqref{eq:reward-Ghat}, and \cref{ass:est-oracle-1}, (b) follows from \Cref{lem:master_confidence_bound}, and (c) uses $S = \lceil \log_2 \sqrt{T} \rceil$ and $w_t^{(s)}(b_t)\ge 2^{-s}$ whenever $t\in \Phi^{(s)}$. Then \cref{thm:linear_po_reg_upper} is a direct consequence of the following upper bound, which is essentially the elliptical potential lemma applied to the sum $\sum_{t\in \Phi^{(s)}} w_t^{(s)}(b_t)$. 

\begin{lemma}\label{lem:elliptical-potential-PO}
For every $s\in [S]$, it holds that 
$\sum_{t\in \Phi^{(s)}} w_t^{(s)}(b_t) = O(\Delta \sqrt{\log T}+ \sqrt{dT}\log T)$. 
\end{lemma}



\section{Model 3: Linear Treatment Effect}
\label{sec:linear_te}
In this section, we relax the linear structure imposed on the potential outcomes to only impose it on the treatment effect and consider Model 3 as proposed in Section~\ref{sec:model3}.

\subsection{An Adaptive Bid Elimination Algorithm}
\subsubsection{A truncated IPW estimator.} When the HOB distribution $G_t$ is perfectly available, a natural estimator of the treatment effect $\E[v_{t,1}-v_{t,0}]$ is the IPW estimator in \eqref{eq:weighted_estimator_te}:
\begin{align*}
\widehat{e}_t(b) = \frac{\indic[b\geq M_t]v_{t,1}}{G_t(b)} - \frac{\indic[b< M_t]v_{t,0}}{1-G_t(b)}. 
\end{align*}
However, since we only have access to an estimator $\widehat{G}_t$ of $G_t$, we will apply a truncated IPW estimator defined as
\begin{equation}\label{eq:truncated_estimator_te}
\widetilde{e}_t(b) = \frac{\indic[b\geq M_t]}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}}v_{t,1} - \frac{\indic[b< M_t]}{\max\bra*{\delta_t^2, 1-\widehat{G}_t(b)}}v_{t,0},
\end{equation}
where $\delta_t$ is the concentration parameter in \cref{ass:est-oracle-2} which is assumed to be known. Unlike $\widehat{e}_t(b)$, the truncated estimator $\widetilde{e}_t(b)$ is biased. The following lemma summarizes the bias-variance properties of $\widetilde{e}_t(b)$. 
\begin{lemma}\label{lem:bias-variance}
Suppose $|\widehat{G}_t(b) - G_t(b)|\le \delta_t\sqrt{G_t(b)(1-G_t(b))} + \delta_t^2$. Then there exist absolute constants $c, c'>0$ such that
\begin{align*}
\abs*{\E[\widetilde{e}_t(b)] - \theta_*^\top x_t} &\le c\delta_t\sigma_t(b), \\
\Var(\widetilde{e}_t(b)) &\le c'\sigma_t(b)^2, 
\end{align*}
where the quantity $\sigma_t(b)$ is defined as
\begin{align}\label{eq:sigma_t}
    \sigma_t(b) = \frac{1}{\sqrt{ \widehat{G}_t(b)(1-\widehat{G}_t(b)) }}. 
\end{align}
\end{lemma}

In the sequel, we will use $\sigma_t(b)^2$ in \eqref{eq:sigma_t} as the variance proxy for the truncated IPW estimator $\widetilde{e}_t(b)$. In particular, since $\widehat{G}_t(b)$ could be close to $0$ or $1$ (i.e. no overlap condition), the quantity $\sigma_t(b)$ can indeed become prohibitively large. 

\subsubsection{Base algorithm}
\label{sec:base_lin_te}
\begin{algorithm}[!t]\caption{LIN-TE (Base algorithm for linear treatment effects)}
\label{alg:base_alg_te}
\textbf{Input:} Time indices $\Phi_t\subseteq [t-1]$, bid space $B_t\subseteq \Bcal$, estimated CDF $\widehat{G}_t$, time horizon $T$.

$\lambda\gets 1$;

$\sigma_\tau \gets \sigma_\tau(b_\tau)$ in \eqref{eq:sigma_t} for $\tau\in \Phi_t$;

$A_{t}\leftarrow \lambda I + \sum_{\tau\in \Phi_t} \sigma_\tau^{-2}x_\tau x_\tau^{\top}$;

$z_{t} \leftarrow \sum_{\tau\in\Phi_t}\sigma_\tau^{-2}x_\tau \widetilde{e}_\tau$ where $\widetilde{e}_\tau = \widetilde{e}_\tau(b_\tau)$ is defined in \eqref{eq:truncated_estimator_te};

Compute estimator $\htheta_{t} \leftarrow A_{t}^{-1}z_{t}$.

Set $\gamma\gets \lambda + c_1\log(2T)+c_2\sqrt{\sum_{\tau\in\Phi_t}\delta_\tau^2}$, with $\delta_\tau$ given in Assumption~\ref{ass:est-oracle-2} and absolute constants $c_1,c_2>0$ defined in \cref{lem:WLS_app}.

\For{$b\in B_t$}
{
Compute width $w_t(b) \leftarrow c_3L\gamma\parr*{\widehat{G}_t(b)\parr{1-\widehat{G}_t(b)}\|x_t\|_{A_t^{-1}} + \gamma\|x_t\|_{A_t^{-1}}^2}$ with absolute constant $c_3>0$ defined in \cref{lem:TE_width_app}. \label{line:base_te_width}

Compute $u_{t,0}(b)\leftarrow \widehat{G}_t(b)\parr*{\htheta_{t}^{\top}x_t - b + \gamma\|x_t\|_{A_t^{-1}}}$.

Compute $u_{t,1}(b)\leftarrow \widehat{G}_t(b)\parr*{\htheta_{t}^{\top}x_t - b - \gamma\|x_t\|_{A_t^{-1}}} - \parr*{\htheta_t^{\top}x_t - \gamma\|x_t\|_{A_t^{-1}}}$.

Compute maximizers $b_{*,i} \leftarrow \argmax_{b\in B_t} u_{t,i}(b)$ for both $i=0,1$.

}
\end{algorithm}

Next we present the base algorithm LIN-TE in \cref{alg:base_alg_te} based on $\widetilde{e}_t(b)$. Again, we will apply linear regression of $\widetilde{e}_t(b_t)$ against $x_t$ to estimate $\theta_*$, but \cref{lem:bias-variance} shows that the ordinary least squares will suffer from heteroskedasticity. To address this problem, we use the following weighted least squares estimator: 
\begin{align}\label{eq:WLS}
\widehat{\theta}_t = \argmin_\theta \sum_{\tau \in \Phi_t} \parr*{\frac{\widetilde{e}_\tau(b_\tau) - \theta^\top x_\tau}{\sigma_\tau(b_\tau)}}^2 + \lambda \|\theta\|_2^2, 
\end{align}
with $\widetilde{e}_\tau(b_\tau)$ and $\sigma_\tau(b_\tau)$ given in \eqref{eq:truncated_estimator_te} and \eqref{eq:sigma_t}, respectively. The estimation performance of $\widehat{\theta}_t$ is summarized in the following lemma. 

\begin{lemma}\label{lem:WLS}
Suppose $(v_{\tau,1},v_{\tau,0})_{\tau\in \Phi_t}$ are conditionally independent given $(x_\tau,b_\tau,M_\tau)_{\tau\in \Phi_t}$, then with probability at least $1-T^{-2}$, it holds that
\begin{align*}
\abs*{\htheta^{\top}_tx_t - \theta_*^{\top}x_t} \leq \gamma \|x_t\|_{A_t^{-1}}, 
\end{align*}
where $\gamma$ is defined in Line 7 of \cref{alg:base_alg_te}. 
\end{lemma}

As for the quantities $u_{t,0}(b)$ and $u_{t,1}(b)$ in Lines 10 and 11, note that the high-probability event in \cref{lem:WLS} yields that for every $b\in \Bcal$, 
\begin{align}
u_{t,0}(b) &\ge \widehat{r}_t(b) - \E[v_{t,0}] =: \widehat{r}_{t,0}(b), \label{eq:UCB0} \\
u_{t,1}(b) &\ge \widehat{r}_t(b) - \E[v_{t,1}] =: \widehat{r}_{t,1}(b), \label{eq:UCB1}
\end{align}
where we recall in \eqref{eq:reward-Ghat} that 
\begin{align}\label{eq:reward-hat-2}
\widehat{r}_t(b) = \widehat{G}_t(b)(\theta_*^\top x_t - b) + \E[v_{t,0}] = \widehat{G}_t(b)(\theta_*^\top x_t - b) - \theta_*^\top x_t + \E[v_{t,1}].
\end{align}
In other words, both quantities $u_{t,0}(b)$ and $u_{t,1}(b)$ are upper confidence bounds of $\widehat{r}_t(b)$, up to additive constants independent of $b$. Consequently, the maximizers $b_{*,0}\ge b_{*,1}$ are the bids chosen by two different UCB policies, and their roles will be discussed in the master algorithm. 

\subsubsection{Master algorithm}

\begin{algorithm}[!t]\caption{SUP-LIN-TE (Master algorithm for FPA with linear treatment effects)}
\label{alg:master_alg_te}
\textbf{Input:} Time horizon $T$, bid space $\Bcal=[0,1]$, estimation oracle $\Ocal$.

\textbf{Initialize:} set $S=\lceil \log_2 \sqrt{T} \rceil$, $\Phi^{(s)}_1=\varnothing$ for $s\in[S]$.

\For{$t=1$ \KwTo $T$}
{   
Observe the context vector $x_t\in\R^d$.

Estimate the CDF and confidence level with $(\widehat{G}_t,\delta_t)\leftarrow \Ocal\parr*{\{M_\tau, x_\tau\}_{\tau<t}; x_t}$.

Initialize $B_1\leftarrow \Bcal$.

\For{$s=1$ \KwTo $S$}
{

Invoke Algorithm~\ref{alg:base_alg_te} with time indices $\Phi^{(s)}_t$, space $B_s$, and CDF $\widehat{G}_t$ to compute UCBs $\bra*{u^{(s)}_{t,i}(b)}_{b\in B_s}$ and maximizers $b^{(s)}_{*,i}$ for $i=0,1$, and widths $\bra*{w^{(s)}_{t}(b)}_{b\in B_s}$.

Select the criterion $i\gets \indic\parq*{\widehat{G}_t\parr*{b^{(s)}_{*,1}} > \frac{1}{40L}}\in \{0,1\}$. 

\uIf{$\exists b\in B_s$ such that $w^{(s)}_t(b) > 2^{-s}$}{
Choose this $b_t\leftarrow b$.\label{line:master_te_select_bt}

Update: $\Phi^{(s)}_{t+1} \leftarrow \Phi^{(s)}_t\cup \{t\}$ and $\Phi^{(s')}_{t+1} \leftarrow \Phi^{(s')}_t$ for $s'\neq s$. Break the inner for loop.
}\uElseIf{$w^{(s)}_t(b)\leq \frac{1}{\sqrt{T}}$ for all $b\in B_s$}{
Choose $b_t \leftarrow \max_{b\in B_s}u^{(s)}_{t,i}(b)$.
\label{line:master_te_exploit_bt}

Do not update: $\Phi^{(s')}_{t+1}\leftarrow \Phi^{(s')}_t$ for all $s'\in[S]$. Break the inner for loop.
}\Else{
We have $w^{(s)}_t(b)\leq 2^{-s}$ for all $b\in B_s$. \label{line:master_te_elim_step}

Eliminate bids: $\displaystyle B_{s+1}\leftarrow \bra*{b\in B_s : u^{(s)}_{t,i}(b) \geq u^{(s)}_{t,i}\parr*{b^{(s)}_{*,i}} - 2\cdot 2^{-s}}\cap \parq*{b^{(s)}_{*,1}, b^{(s)}_{*,0}}$.
}
}
Set $\gamma_t \gets 1 + c_1\log(2T) + c_2\sqrt{\sum_{\tau\in\Phi_t^{(s)}}\delta_\tau^2}$ where $s\in[S]$ is the stage $b_t$ is selected.

Let $z\gets \min\{\gamma_t\sqrt{\frac{d}{T}}+4\delta_t, \frac{1}{2}\}$, and bidder bids the truncated bid
$$b_t \gets \min\Big\{ \max\Big\{b_t, \widehat{G}_t^{-1}(z)\Big\}, \widehat{G}_t^{-1}(1-z)\Big\}.$$

Bidder observes $\indic[b_t\geq M_t]v_{t,1}$ and $\indic[b_t<M_t]v_{t,0}.$
}
\end{algorithm}

Similar to \cref{alg:master_alg_po}, the master algorithm in \cref{alg:master_alg_te} partitions $[t]$ into multiple stages to ensure the conditional independence condition in \cref{lem:WLS}. However, the main distinct feature of \cref{alg:master_alg_te} lies in the selection from two different criteria $u_{t,0}$ and $u_{t,1}$. The reason behind this selection is that, the rewards
\begin{align*}
\widehat{r}_{t,0}(b) &= \widehat{G}_t(b)(\theta_*^\top x_t - b), \\
\widehat{r}_{t,1}(b) &= \widehat{G}_t(b)(\theta_*^\top x_t - b) - \theta_*^\top x_t = -(1- \widehat{G}_t(b))\theta_*^\top x_t - b\widehat{G}_t(b)
\end{align*}
in \eqref{eq:UCB0} and \eqref{eq:UCB1} are sensitive to the estimation error of $\theta_*^\top x_t$ in different ways: in $\widehat{r}_{t,0}(b)$ the leading coefficient is $\widehat{G}_t(b)$, while in $\widehat{r}_{t,1}(b)$ the leading coefficient has magnitude $1-\widehat{G}_t(b)$. Therefore, a possibly large estimation error of $\theta_*^\top x_t$ when $\widehat{G}_t(b)$ is close to $0$ (resp. $1$) does not transform into a large estimation error in $\widehat{r}_{t,0}(b)$ (resp. $\widehat{r}_{t,1}(b)$); this is essentially a ``variance reduction'' step. The key then is to choose the better reward ($\widehat{r}_{t,0}(b)$ or $\widehat{r}_{t,1}(b)$) for decision making, depending on whether $\widehat{G}_t(b)$ is close to $0$ or $1$ for approximately optimal bids $b$. 

To this end, we compute the maximizer $b_{*,1}$ of the map $b\mapsto u_{t,1}(b)$, and check if $\widehat{G}_t(b_{*,1})$ is close to $0$ or $1$. A small $\widehat{G}_t(b_{*,1})$ suggests a small CDF and thus the choice of $\widehat{r}_{t,0}$, and a large $\widehat{G}_t(b_{*,1})$ suggests a big CDF and the other option $\widehat{r}_{t,1}$. This is the criterion $i\in\{0,1\}$ in Line 9 of \cref{alg:master_alg_te}. The following result shows that the selected reward $\widehat{r}_{t,i}(b)$ enjoys a small width $w_{t}(b)$ that scales with $\widehat{G}_t(b)(1-\widehat{G}_t(b))= \sigma_t(b)^{-2}$ for all candidate bids.

\begin{lemma}\label{lem:TE_width}
Suppose the event in Lemma~\ref{lem:WLS} holds.  Let $\widehat{r}_{t,0}$ and $\widehat{r}_{t,1}$ be defined as in \eqref{eq:UCB0} and \eqref{eq:UCB1}. For any $s\in[S]$ and for the criterion $i\in\{0,1\}$ selected in \cref{alg:master_alg_te}, it holds that 
\begin{align*}
u_{t,i}^{(s)}(b) - 2w^{(s)}_t(b)\le \widehat{r}_{t,i}(b)  \le u^{(s)}_{t,i}(b), \quad \forall b\in [b_{*,1}, b_{*,0}] 
\end{align*}
where $b^{(s)}_{*,i}=\argmax_{b\in B_s}u^{(s)}_{t,i}(b)$ for $i=0,1$ as defined in \cref{alg:base_alg_te}.
\end{lemma}

Therefore, the width of the selected UCB scales inversely with the variance $\sigma_t(b_t)$ of the truncated IPW estimator, thereby neutralizing the effect of the variance in the final regret. Consequently, this adaptive UCB selection eliminates the need for an overlap condition in our framework.

\subsection{Regret Upper Bound}
The regret upper bound of \cref{alg:master_alg_te} is established in the following result, which implies Theorem~\ref{thm:main-lte}.
\begin{theorem}[Upper bound of \cref{alg:master_alg_te}]\label{thm:linear_te_reg_upper}
Let $\pi$ be the resulting bidding strategy of \cref{alg:master_alg_te} (combined with \cref{alg:base_alg_te}). Then under Assumptions \ref{ass:noconfounding}--\ref{ass:Gt} and \ref{ass:est-oracle-2}, the policy $\pi$ achieves an expected regret
\[
\Reglte(\pi) = O\parr*{\sqrt{dT\Delta_2}\log^3 T + \Delta_1}
\]
where $\Delta_2 = \sum_{t=1}^T\delta_t^2$ and $\Delta_1 = \sum_{t=1}^T\delta_t$, with $\delta_t$ given in Assumption~\ref{ass:est-oracle-2}.
\end{theorem}
Again, we present the proof of Theorem~\ref{thm:linear_te_reg_upper} in the remainder of this section. WLOG we assume Assmuption~\ref{ass:est-oracle-2} and the high-probability event in \cref{lem:WLS} holds almost surely. We also assume that $\delta_t < 0.1$ for all $t\in[T]$, since there are at most $10\Delta_1$ times when this assumption is violated, contributing $O(\Delta_1)$ regret. Recall the notation in \eqref{eq:reward-hat-2}, and we have the following lemma that proves the bids in stage $s$ are only $O(2^{-s})$-suboptimal under $\hr_t$.
\begin{lemma}\label{lem:master_confidence_bound_te}
In \cref{alg:master_alg_te}, for every $t\in[T]$ and $s\in[S]$, it holds that $\max_{b^*\in\Bcal}\hr_t(b^*)-\hr_t(b) \leq 8\cdot 2^{-s}$ for all $b\in B_s$.
\end{lemma}
Let $b^{\circ}_t$ denote the selected bid before the truncation in Line 22 in \cref{alg:master_alg_te} and $b_t$ denote the final bid after. Since \cref{ass:est-oracle-2} implies $\|G_t-\widehat{G}_t\|_{\infty}\leq 2\delta_t$, by \cref{lem:truncation}, the truncation step does not hurt the reward too much:
\begin{equation}\label{eq:bounded_truncation_loss}
\hr_t(b^{\circ}_t) - \hr_t(b_t) \leq \gamma_t\sqrt{\frac{d}{T}} + 8\delta_t.
\end{equation}
Then let again $\Phi^{(s)}$ denote the set of time indices for stage $s$ and $\Phi^{(S+1)}$ where all widths are less than $\frac{1}{\sqrt{T}}$. Denote $b^*_t = \max_{b^*\in\Bcal}\E[r_t(b^*)]$ and $\hb^*_t = \max_{b^*\in\Bcal}\E[\hr_t(b^*)]$. We have
\begin{align*}
\Reglte(\pi) &= \sum_{t\in[T]} \E \parq{r_t(b^*_t)} - \E\parq{r_t(b_t)}\\
&= \sum_{t\in[T]}\underbrace{\parr*{\E \parq{r_t(b^*_t)} - \hr_t(b^*_t)}}_\text{(a)} + \underbrace{\parr*{\hr_t(b^*_t) - \hr_t\parr*{\hb^*_t}}}_\text{(b)} \\
&\quad + \underbrace{\parr*{\hr_t\parr*{\hb^*_t} - \hr_t\parr*{b^\circ_t}}}_\text{(c)} + \underbrace{\parr*{\hr_t\parr*{b^\circ_t} - \hr_t(b_t)}}_\text{(d)} + \underbrace{\parr*{\hr_t(b_t) - \E \parq{r_t(b_t)}}}_\text{(e)}.
\end{align*}
By Assumption~\ref{ass:est-oracle-2}, we can bound both $\text{(a)}\leq 2\sum_{t\in[T]}\delta_t = 2\Delta_1$ and $\text{(e)}\leq 2\Delta_1$. By definition of $\hb^*_t$, $\text{(b)}\leq 0$. By \eqref{eq:bounded_truncation_loss}, $\text{(d)}\leq \parr*{1+c_1\log(2T)+c_2\sqrt{\Delta_2}}\sqrt{dT} + 8\Delta_1$.
It remains to bound (c) as follows:
\begin{align*}
\sum_{t\in[T]}\hr_t\parr*{\hb^*_t} - \hr_t(b^\circ_t) &= \sum_{t\in\Phi^{(S+1)}}\hr_t\parr*{\hb^*_t} - \hr_t(b^\circ_t) + \sum_{s\in[S]}\sum_{t\in\Phi^{(s)}}\hr_t\parr*{\hb^*_t} - \hr_t(b^\circ_t)\\
&\stepa{\leq} \frac{|\Phi^{(S+1)}|}{\sqrt{T}} + 8\sum_{s\in[S]}2^{-s}\abs{\Phi^{(s)}} \\
&\stepb{\leq} O(\sqrt{T}) + \sum_{s\in[S]}\sum_{t\in\Phi^{(s)}}w^{(s)}_t(b_t^\circ)\\
&\stepc{\leq} O(\sqrt{T}) + \sum_{s\in[S]}\sum_{t\in\Phi^{(s)}}w^{(s)}_t(b_t)
\end{align*}
where (a) is from \cref{lem:master_confidence_bound_te}, (b) uses $w^{(s)}_t(b_t^\circ) \geq 2^{-s}$ when $t\in\Phi^{(s)}$, and (c) uses $\widehat{G}_t(b_t)(1-\widehat{G}_t(b_t))\ge \widehat{G}_t(b_t^\circ)(1-\widehat{G}_t(b_t^\circ))$ to conclude that $w^{(s)}_t(b_t)\geq w^{(s)}_t(b_t^\circ)$ by the definition of $w_t^{(s)}$. Then applying the following elliptical potential lemma leads to the upper bound in \cref{thm:linear_te_reg_upper}.

\begin{lemma}\label{lem:elliptical-potential-TE}
For any $s\in[S]$, suppose $\delta_t<0.1$ for $t\in\Phi^{(s)}$ and we have
\[
\sum_{t\in\Phi^{(s)}}w^{(s)}_t(b_t) = O(\sqrt{dT\Delta_2}\log^2 T + d\Delta_2\log T + d\log^3 T).
\]
\end{lemma}


\section{Conclusion and Future Directions}\label{sec:future-direction}
In this paper, we formulate the problem of joint value estimation and bidding for a single bidder in repeated first-price auctions into a special instance of causal inference. In particular, we model the bidder's valuation as the difference between two potential outcomes, with a special feedback structure where exactly one potential outcome can be observed upon winning or losing. Under standard assumptions of causal inference excluding the overlap condition, we provide three models for the valuation and devise bidding algorithms that achieve $\widetilde{O}(\sqrt{T})$ or $\widetilde{O}(\sqrt{dT})$ regrets, up to the HOB estimation error, under these models. 

We also point out some open directions. First, instead of the black-box modeling for the HOB estimation, one can study explicit models for $M_t$ and prove unconditional regret guarantees as well. For example, consider the linear model $M_t = \varphi_*^\top x_t + \eta_t$ studied in \cite{badanidiyuru2023learning}, where $\varphi_*\in\R^d$ is an unknown parameter and $\eta_t$ is an i.i.d. additive noise. We assume $\|\varphi_*\|_2\le 1$ and the following assumption on the unknown distribution of $\eta_t$.

\begin{assumption}\label{ass:flat_noise_tail}
There exist constants $\sigma, a_0, a_1, a_2>0$, such that the following holds for the CDF $Q$ of $\eta_t$: 
\begin{itemize}
    \item $\eta_t$ is zero-mean and $\sigma^2$-sub-Gaussian; 
    
    \item If $Q(v)\in [0,a_0]\cup[1-a_0,1]$, then $Q'(v)\le a_1\sqrt{Q(v)(1-Q(v))}$ for all $v\in \R$; 

    \item The second derivative is bounded, i.e. $|Q''(v)| \le a_2$ for all $v\in \R$. 
\end{itemize}
\end{assumption}

Note that \cref{ass:flat_noise_tail} does not require the log-concavity assumed in \cite{badanidiyuru2023learning}, and is satisfied for Gaussian $\eta_t$ (by the well-known Mills ratio results \cite[Eq (9)]{gordon1941values}). The following result shows that \cref{ass:est-oracle-2} still holds under the above assumption: 
\begin{lemma}\label{lem:linear_hob_error}
Let Assumptions \ref{ass:noconfounding}--\ref{ass:Gt} and \cref{ass:flat_noise_tail} hold. There exists a CDF estimator $\widehat{G}_t$ that satisfies \cref{ass:est-oracle-2} with the HOB error bounds $\Delta_1=\widetildeO(\sqrt{dT})$ and $\Delta_2=\widetildeO(d)$.
\end{lemma}
\noindent Since the quantity $\Delta$ in \cref{ass:est-oracle-1} satisfies $\Delta\le 2\Delta_1$, our algorithms achieve regret $\widetildeO(\sqrt{dT})$ under Model 2 and $\widetildeO(d\sqrt{T})$ under Model 3. It is an interesting question to see the tightness of the $\widetildeO(d\sqrt{T})$ regret under Model 3, and generalize the above results to other models of the HOBs $M_t$. 

Second, it is interesting to see if the unconfoundedness assumption is really necessary for joint value estimation and bidding, motivated by the possibility and impossibility results in \cite{cesa2024role} where $(v_{t,1},v_{t,0})$ and $M_t$ are dependent in a context-free environment. Finally, it would be interesting to generalize this framework to other transparency models in \cite{cesa2024role}, where the estimation of HOBs would no longer be independent of the bidding process and we essentially arrive at a challenging ``joint value estimation, HOB estimation, and bidding'' problem.  



% Acknowledgments---Will not appear in anonymized version
% \acks{We thank a bunch of people and funding agency.}

\clearpage 

\bibliographystyle{alpha}
\bibliography{Preprint.bib}

\clearpage 
\appendix

% \crefalias{section}{appendix} % uncomment if you are using cleveref




\section{Proofs of Main Lemmas}
\subsection{Proof of \cref{lem:PO_width}}
\begin{lemma}[Restatement of Lemma~\ref{lem:PO_width}]\label{lem:PO_width_app}
Let $\beta=\sqrt{\log(2T)}$ and $\lambda=1$. Suppose $(v_{s,1},v_{s,0})_{s\in \Phi_t}$ are conditionally independent given $(x_s,b_s,M_s)_{s\in \Phi_t}$, then with probability at least $1-T^{-2}$, it holds that
\begin{align*}
\abs*{\htheta_{t,i}^{\top}x_t - \theta_{*,i}^{\top}x_t} \leq (\beta+\lambda)\|x_t\|_{A_{t,i}^{-1}}, \quad \text{for both } i = 0, 1. 
\end{align*}
Conditioned on the above events, in LIN-PO we further have
\begin{align*}
\abs*{\widehat{r}_t(b) - [\widehat{G}_t(b)(\theta_{*,1}^\top x_t - b) + (1-\widehat{G}_t(b))\theta_{*,0}^\top x_t]  } \le w_t(b), \quad \text{for all }b\in \Bcal. 
\end{align*}
\end{lemma}
\begin{proof}
We will show the claimed inequality for $i=1$, and the case $i=0$ follows the same argument. Denote $D_{t,1}=[\indic[b_\tau\geq M_\tau]x_\tau]_{\tau\in\Phi_t}\in\R^{d\times |\Phi_t|}$ and $V_{t,1}=[\indic[b_\tau\geq M_\tau]v_{t,1}]_{\tau\in\Phi_t}\in\R^{1\times |\Phi_t|}$. Recall that in LIN-PO we define
\[
A_{t,1} = I + D_{t,1}D_{t,1}^{\top},\quad\quad z_{t,1} = D_{t,1}V_{t,1}^{\top}.
\]
Then we have
\begin{align*}
\htheta_{t,1}^{\top}x_t - \theta_{*,1}^{\top}x_t &= x_t^{\top}A^{-1}_{t,1}z_{t,1} - x_t^{\top}A^{-1}_{t,1}\parr*{ I+D_{t,1}D_{t,1}^{\top}}\theta_{*,1}\\
&= x_t^{\top}A_{t,1}^{-1}D_{t,1}\parr*{V_{t,1}^{\top} - D^{\top}_{t,1}\theta_{*,1}} + x_t^{\top}A_{t,1}^{-1}\theta_{*,1}.
\end{align*}
Given $\|\theta_{*,1}\|_2\leq 1$, it follows that
\begin{equation}\label{eq:po_confidence_bound_single_time}
\abs*{\htheta_{t,1}^{\top}x_t - \theta_{*,1}^{\top}x_t} \leq \abs*{x_t^{\top}A_{t,1}^{-1}D_{t,1}\parr*{V_{t,1}^{\top} - D^{\top}_{t,1}\theta_{*,1}}} + \|x_t^{\top}A_{t,1}^{-1}\|_2.
\end{equation}
Since $(v_{s,1},v_{s,0})_{s\in \Phi_t}$ are conditionally independent, we have $\E\parq*{V^{\top}_{t,1}|(x_s,b_s,M_s)_{s\in\Phi_t}} = D^{\top}_{t,1}\theta_{*,1}$. Thus by Azuma's inequality \cite{azuma1967weighted, alon2016probabilistic},
\begin{align*}
P\parr*{\abs*{x_t^{\top}A_{t,1}^{-1}D_{t,1}\parr*{V_{t,1} - D^{\top}_{t,1}\theta_{*,1}}} \geq \beta\|x_{t}\|_{A^{-1}_{t,1}}} &\leq 2\exp\parr*{-\frac{2\beta^2\|x_t\|_{A^{-1}_{t,1}}^2}{\|x_t^{\top}A_{t,1}^{-1}D_{t,1}\|_2^2}}\\
&\leq 2\exp\parr*{-2\beta^2}\\
&= \frac{1}{2T^2}
\end{align*}
The second inequality follows from $A_{t,1}= I + D_{t,1}D_{t,1}^{\top}\succeq D_{t,1}D_{t,1}^{\top}$. Similarly, we can bound the second term in \eqref{eq:po_confidence_bound_single_time} as $\|x_t^{\top}A_{t,1}^{-1}\|_2 = \sqrt{x_t^{\top}A_{t,1}^{-1}IA_{t,1}^{-1}x_t} \leq \|x_t\|_{A_{t,1}^{-1}}$. Plugging them back in \eqref{eq:po_confidence_bound_single_time} yields
\[
\abs*{\htheta_{t,1}^{\top}x_t - \theta_{*,1}^{\top}x_t} \leq (\beta+1)\|x_t\|_{A_{t,1}^{-1}}.
\]
Then applying union bound over $i=0,1$ concludes the proof.
\end{proof}

\subsection{Proof of \cref{lem:master_confidence_bound}}
\begin{lemma}[Restatement of Lemma~\ref{lem:master_confidence_bound}]
\label{lem:master_confidence_bound_app}
In \cref{alg:master_alg_po}, for every $t\in[T]$ and $s\in [S]$, it holds that $\max_{b^*\in \Bcal}\hr_t(b^*) - \hr_t(b) \leq 8\cdot 2^{-s}$ for all $b\in B_s$.
\end{lemma}
\begin{proof}
Denote the optimal bid under $\hr_t$ by $b^*_t = \argmax_{b^*\in \Bcal}\hr_t(b^*).$ We prove this result via an induction argument on $s\in[S]$ with the following induction hypothesis: for each $s\in[S]$, the optimal bid remains uneliminated, i.e. $b^*_t\in B_s$, and $\hr_t(b^*_t)- \hr_t(b) \leq 8\cdot 2^{-s}$ for all $b\in B_s$. For $s=1$ it clearly holds.

Suppose the hypothesis holds for $s-1$. The elimination step (Line~\ref{line:master_po_elim}) in \cref{alg:master_alg_po} implies that for any $b\in B_s$, we have $w^{(s-1)}_t(b) \leq 2^{-(s-1)}$, $w^{(s-1)}_t(b^*_t)\leq 2^{-(s-1)}$, and $\hr_t^{(s-1)}(b) + w^{(s-1)}_t(b) \geq \hr_t^{(s-1)}(b^*_t) + w^{(s-1)}_t(b^*_t) - 2^{-s+2}$. Since $b^*_t\in B_{s-1}$ by the induction hypothesis, by \cref{lem:PO_width},
\[
\hr_t^{(s-1)}(b^*_t) + w^{(s-1)}_t(b^*_t) \geq \hr_t\parr*{b^*_t} \geq \hr_t(b) \geq \hr^{(s-1)}_t(b) - w^{(s-1)}_t(b) \geq \hr^{(s-1)}_t(b) - 2^{-(s-1)}
\]
for all $b\in B_{s-1}$. Therefore, $b^*_t\in B_s$ by the elimination criterion. Then for any $b\in B_s$,
\begin{align*}
\hr_t(b) &\stepa{\geq} \hr^{(s-1)}_t(b) - w^{(s-1)}_t(b)\\
&\geq \hr^{(s-1)}_t(b) + w^{(s-1)}_t(b) - 2\cdot 2^{-(s-1)}\\
&\stepb{\geq} \max_{b'\in B_{s-1}}\hr^{(s-1)}_t(b') + w^{(s-1)}_t(b') - 4\cdot 2^{-(s-1)}\\
&\stepc{\geq} \hr^{(s-1)}_t(b^*_t) + w^{(s-1)}_t(b^*_t) - 4\cdot 2^{-(s-1)}\\
&\stepa{\geq} \hr_t(b^*_t)- 4\cdot 2^{-(s-1)}
\end{align*}
where (a) uses \cref{lem:PO_width}, (b) is by the elimination criterion in \cref{alg:master_alg_te}, and (c) follows from $b^*_t\in B_{s-1}$. This concludes the induction and hence the proof.
\end{proof}



\subsection{Proof of \cref{lem:elliptical-potential-PO}}
Before we proceed to the proof, we present a lemma that bounds the sum of the matrix norms:
\begin{lemma}\label{lem:sum_width_bound2_FPA2}
For any given vectors $\{z_\tau\}_{\tau=1}^{t-1}$ in $\R^d$ with $\|z_\tau\|_2\leq 1$, let the Gram matrix be $A_s = cI + \sum_{\tau<s}z_\tau z_\tau^{\top}$ for some $c>0$ and for every $1\leq s \leq t$. It holds that
\[
\sum_{\tau<t}\|z_\tau\|^2_{A_{\tau}^{-1}} \leq 2d\log\parr*{c+\frac{t-1}{d}} + 2\log(c).
\]
In particular, by Cauchy-Schwartz inequality,
\[
\sum_{\tau<t}\|z_\tau\|_{A_{\tau}^{-1}} \leq \sqrt{2d(t-1)\log\parr*{1+\frac{t-1}{d}}}.
\]
\end{lemma}
\begin{proof}
We decompose the Gram matrix as follows:
\begin{align*}
\det\parr*{A_t} &= \det\parr*{A_{t-1} + z_{t-1} z_{t-1}^{\top}} \\
&= \det\parr*{A_{t-1}^{1/2}\parr*{I + A_{t-1}^{-1/2}z_{t-1} z_{t-1}^{\top}A_{t-1}^{-1/2}}A_{t-1}^{1/2}}\\
&\overset{\text{(a)}}{=} \det\parr*{A_{t-1}}\det\parr*{I + A_{t-1}^{-1/2}z_{t-1} z_{t-1}^{\top}A_{t-1}^{-1/2}}\\
&\overset{\text{(b)}}{=} \det\parr*{A_\tau}\parr*{1+\|z_{t-1}\|_{A_{t-1}^{-1}}^2} \\
&= \det\parr*{A_1}\prod_{\tau<t}\parr*{1+\|z_\tau\|_{A_\tau^{-1}}^2} \numberthis\label{eq:cov_matrix_decomp_1}
\end{align*}
where (a) uses $\det(AB) = \det(A)\det(B)$ and (b) uses that $I+vv^T$ only has eigenvalues $1$ and $1+\|v\|_2^2$. Since $\det(A_1) = \det(cI)=c$, by AM-GM inequality,
\begin{equation}\label{eq:cov_matrix_decomp_2}
\det\parr*{A_{t}} \leq \parr*{\frac{\Tr\parr*{A_{t}}}{d}}^d = \parr*{\frac{cd+\sum_{\tau<t} \Tr\parr*{z_\tau z_\tau^{\top}}}{d}}^d \leq \parr*{c + \frac{t-1}{d}}^d.
\end{equation}
Note that for all $\tau<t$, we have  $\|z_\tau\|^2_{A_{\tau}^{-1}} \leq \|z_\tau\|_2^2\lambda^{-1}_{\min}\parr*{A_{\tau}} \leq 1$, and hence $\|z_\tau\|^2_{A_{\tau}^{-1}} \leq 2\log\parr*{1+\|z_\tau\|^2_{A_{\tau}^{-1}}}$. Combining \eqref{eq:cov_matrix_decomp_1} and \eqref{eq:cov_matrix_decomp_2}, we have
\begin{align*}
\sum_{\tau<t} \|z_\tau\|^2_{A_{\tau}^{-1}} &\leq 2\sum_{\tau<t}\log\parr*{1+\|z_\tau\|^2_{A_{\tau}^{-1}}} \\
&= 2\log\prod_{\tau<t}\parr*{1+\|z_\tau\|^2_{A_{\tau}^{-1}}} \\
&= 2\log\frac{\det\parr*{A_{t}}}{\det\parr*{A_1}} \\
&\leq 2d\log\parr*{c+\frac{t-1}{d}} + 2\log(c).
\end{align*}
\end{proof}

\begin{lemma}[Restatement of Lemma~\ref{lem:elliptical-potential-PO}]\label{lem:elliptical-potential-PO_app}
For every $s\in [S]$, it holds that 
$\sum_{t\in \Phi^{(s)}} w_t^{(s)}(b_t) = O(\Delta \sqrt{\log T}+ \sqrt{dT}\log T)$. 
\end{lemma}
\begin{proof}
For any $t\in[T]$, since $A_{t,i}\succeq I$, it holds that $\|x_t\|_{A_{t,i}^{-1}}\leq \|x_t\|_2\leq 1$ for both $i=0,1$. By Assumption~\ref{ass:est-oracle-1}, we have $\abs*{G_t(b) - \widehat{G}_t(b)} \leq \varepsilon_t$ for any $b\in\Bcal$. Then fix any $s\in[S]$, we have
\begin{align*}
\sum_{t\in\Phi^{(s)}}\widehat{G}_t(b_t)\|x_t\|_{A_{t,1}^{-1}} &\leq \sum_{t\in\Phi^{(s)}}G_t(b_t)\|x_t\|_{A_{t,1}^{-1}} + \varepsilon_t\\
&= \sum_{t\in\Phi^{(s)}}\varepsilon_t + \E\parq*{\sum_{t\in\Phi^{(s)}}\|\indic[b_t\geq M_t]x_t\|_{A_{t,1}^{-1}}}\\
&\stepa{\leq} \Delta + 2\sqrt{d|\Phi^{(s)}|\log|\Phi^{(s)}|}\\
&\leq \Delta + 2\sqrt{dT\log T} \numberthis\label{eq:index_set_bound1}
\end{align*}
where (a) uses \cref{lem:sum_width_bound2_FPA2}.
Similarly,
\begin{equation}
\label{eq:index_set_bound0}
\sum_{t\in\Phi^{(s)}}\parr*{1-\widehat{G}_t(b_t)}\|x_t\|_{A_{t,0}^{-1}} \leq \Delta + 2\sqrt{dT\log T}.
\end{equation}
Combining \eqref{eq:index_set_bound1} and \eqref{eq:index_set_bound0} yields the desired bound:
\begin{align*}
\sum_{t\in \Phi^{(s)}} w_t^{(s)}(b_t) &= \sum_{t\in \Phi^{(s)}} (2\sqrt{\log(2T)}+1)\parr*{\widehat{G}_t(b_t)\|x_t\|_{A_{t,1}^{-1}} + \parr*{1-\widehat{G}_t(b_t)}\|x_t\|_{A_{t,0}^{-1}}}\\
&= O(\Delta\sqrt{\log T} + \sqrt{dT}\log T).
\end{align*}
\end{proof}


\subsection{Proof of \cref{lem:bias-variance}}
\begin{lemma}[Restatement of \cref{lem:bias-variance}]\label{lem:bias-variance_app}
Suppose $|\widehat{G}_t(b) - G_t(b)|\le \delta_t\sqrt{G_t(b)(1-G_t(b))} + \delta_t^2$. Then there exist absolute constants $c, c'>0$ such that
\begin{align*}
\abs*{\E[\widetilde{e}_t(b)] - \theta_*^\top x_t} &\le c\delta_t\sigma_t(b), \\
\Var(\widetilde{e}_t(b)) &\le c'\sigma_t(b)^2, 
\end{align*}
where $\widetilde{e}_t(b)$ is defined in \eqref{eq:truncated_estimator_te} and the quantity $\sigma_t(b)$ is defined as
\begin{align}\label{eq:sigma_t}
    \sigma_t(b) = \frac{1}{\sqrt{ \widehat{G}_t(b)(1-\widehat{G}_t(b)) }}. 
\end{align}
\end{lemma}
\begin{proof}
\textbf{(Bias)}
Denote the bias by $\xi_t(b) = \E[\widetilde{e}_t(b)] - \theta_*^{\top}x_t$. We decompose the bias as follows:
By taking expectation w.r.t. $M_t$, we have
\begin{align*}
\E_{M_t}\parq*{\widetilde{e}_t(b)} - \parr*{v_{t,1}-v_{t,0}} &= \parr*{\frac{G_t(b)}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}}-1}v_{t,1} - \parr*{\frac{1-G_t(b)}{\max\bra*{\delta_t^2, 1-\widehat{G}_t(b)}} -1}v_{t,0}.
\end{align*}
By Assumption~\ref{ass:est-oracle-2} and $v_{t,1}\in[0,1]$, the absolute value of the first term is bounded by
\begin{align*}
\xi_{t,1}(b) \equiv \abs*{\frac{G_t(b)}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}}-1}.
\end{align*}
Similarly, the second term is
\begin{align*}
\xi_{t,0}(b)\equiv \abs*{\frac{1-G_t(b)}{\max\bra*{\delta_t^2, 1-\widehat{G}_t(b)}} -1}
\end{align*}
Finally, by Jensen's inequality and triangle inequality, for any $b$,
\begin{align*}
\abs*{\E\parq*{\widetilde{e}_t(b)} - \theta_*^{\top}x_t} &= \abs*{\E_{v_{t,1}, v_{t,0}}\parq*{\E_{M_t}\parq*{\widetilde{e}_t(b)} - (v_{t,1}-v_{t,0})}}\\
&\leq \E_{v_{t,1}, v_{t,0}}\abs*{\E_{M_t}\parq*{\widetilde{e}_t(b)} - (v_{t,1}-v_{t,0})}\\
&\leq \xi_{t,1}(b) + \xi_{t,0}(b).
\end{align*}
Next, we will bound $\xi_t(b)^2$ to obtain the first inequality in the claim. Note that $\xi_t(b)^2\leq 4\max\bra{\xi_{t,1}(b)^2, \xi_{t,0}(b)^2}$ by the decomposition above. We deal with each case separately. First consider $\xi_{t,1}(b)^2$: when $\widehat{G}_t(b) \geq 6\delta_t^2$, by Assumption~\ref{ass:est-oracle-2} and some algebra, we have $G_t(b) \geq 3\delta_t^2$, and thereby straightforward computation leads to
\begin{align*}
\sigma_t(b)^{-2}\xi_{t,1}(b)^2 &\leq \widehat{G}_t(b)\frac{(G_t(b)-\widehat{G}_t(b))^2}{\widehat{G}_t(b)^2}\\
&= \frac{(G_t(b)-\widehat{G}_t(b))^2}{\widehat{G}_t(b)}\\
&\leq \frac{(\delta_t\sqrt{G_t(b)}+\delta_t^2)^2}{\widehat{G}_t(b)} \\
&\leq 2\delta_t^2\frac{G_t(b)}{\widehat{G}_t(b)} + 2\frac{\delta_t^4}{8\delta_t^2}\\
&\stepa{\leq} 2\delta_t^2\frac{G_t(b)}{G_t(b) - \delta_t\sqrt{G_t(b)}-\delta_t^2} + \frac{\delta_t^2}{4}\\
&\stepb{\leq} 40\delta_t^2 + \frac{\delta_t^2}{4}  \leq 41\delta_t^2
\end{align*}
where (a) is by Assumption~\ref{ass:est-oracle-2} and (b) uses $\delta_t\sqrt{G_t(b)}+\delta_t^2 \leq \frac{19}{20}G_t(b)$ when $G_t(b)\geq 3\delta_t^2$. On the other hand, when $\widehat{G}_t(b) < 6\delta_t^2$, again by Assumption~\ref{ass:est-oracle-2} we have $G_t(b) < 11\delta_t^2$, and
\begin{align*}
\sigma_t(b)^{-2}\xi_{t,1}(b)^2 &\leq \widehat{G}_t(b)(11-1)^2 < 600 \delta_t^2.
\end{align*}
A symmetric argument applies to $\sigma_t(b)^{-2}\xi_{t,0}(b)^2$. Together we obtain the inequality
\[
|\xi_t(b)| = |\E[\widetilde{e}_t(b)]-\theta_*^{\top}x_t|\leq 10\sqrt{6}\sigma_t(b)\delta_t.
\]
\textbf{(Variance)} We will bound the variance of $\widetilde{e}_t(b)$ by its second moment. Toward this end, we look at the first term in the estimator:
\begin{align*}
\parr*{\frac{\indic[b\geq M_t]}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}}v_{t,1}}^2 \leq \parr*{\frac{\indic[b\geq M_t]}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}}}^2 \leq \widehat{G}_t(b)^{-1}\frac{\indic[b\geq M_t]}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}}.
\end{align*}
Similarly, 
\begin{align*}
\parr*{\frac{\indic[b< M_t]}{\max\bra*{\delta_t^2, 1-\widehat{G}_t(b)}}v_{t,0}}^2 \leq \parr*{1-\widehat{G}_t(b)}^{-1}\frac{\indic[b< M_t]}{\max\bra*{\delta_t^2, 1-\widehat{G}_t(b)}}.
\end{align*}
Then we have the second moment
\begin{align*}
\mathsf{Var}\parr*{\widetilde{e}_t(b)} &\leq \E\parq*{\widetilde{e}_t(b)^2}\\
&\leq 2\E\parq*{\parr*{\frac{\indic[b\geq M_t]}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}}v_{t,1}}^2} + 2 \E\parq*{\parr*{\frac{\indic[b< M_t]}{\max\bra*{\delta_t^2, 1-\widehat{G}_t(b)}}v_{t,0}}^2}\\
&\leq 2\widehat{G}_t(b)^{-1}\E\parq*{\frac{\indic[b\geq M_t]}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}}} + 2\parr*{1-\widehat{G}_t(b)}^{-1}\E\parq*{\frac{\indic[b< M_t]}{\max\bra*{\delta_t^2, 1-\widehat{G}_t(b)}}}\\
&= 2\widehat{G}_t(b)^{-1}\frac{G_t(b)}{\max\bra*{\delta_t^2, \widehat{G}_t(b)}} + 2\parr*{1-\widehat{G}_t(b)}^{-1}\frac{1-G_t(b)}{\max\bra*{\delta_t^2, 1-\widehat{G}_t(b)}}\\
&\stepa{\leq} 8\widehat{G}_t(b)^{-1} + 8\parr*{1-\widehat{G}_t(b)}^{-1}\\
&\leq 16\max\bra*{\widehat{G}_t(b)^{-1}, \parr*{1-\widehat{G}_t(b)}^{-1}}
\end{align*}
where (a) bounds each fraction by $4$ due to Assumption~\ref{ass:est-oracle-2} and some algebra.
\end{proof}



\subsection{Proof of \cref{lem:WLS}}
\begin{lemma}[Restatement of \cref{lem:WLS}]\label{lem:WLS_app}
Suppose $(v_{\tau,1},v_{\tau,0})_{\tau\in \Phi_t}$ are conditionally independent given $(x_\tau,b_\tau,M_\tau)_{\tau\in \Phi_t}$, then with probability at least $1-T^{-2}$, it holds that
\begin{align*}
\abs*{\htheta^{\top}_tx_t - \theta_*^{\top}x_t} \leq \gamma \|x_t\|_{A_t^{-1}}, 
\end{align*}
where $\gamma$ is defined in Line 7 of \cref{alg:base_alg_te}. 
\end{lemma}

\begin{proof}
Let the bias of the IPW estimator in \eqref{eq:truncated_estimator_te} at time $\tau$ be $\xi_\tau = \E[\widetilde{e}_\tau(b_\tau)] - \theta_*^{\top}x_\tau.$ Denote $D_t = [\sigma_\tau^{-1} x_\tau]_{\tau\in\Phi_t}\in\R^{d\times |\Phi_t|}$ the weighted contexts, $V_t = [\sigma_\tau^{-1} \widetilde{e}_\tau(b_\tau)]_{\tau\in\Phi_t}\in\R^{|\Phi_t|\times 1}$ the weighted estimators, and $\Xi_t = [\sigma_\tau^{-1}\xi_\tau]_{\tau\in\Phi_t}\in\R^{|\Phi_t|\times 1}$ the weighted biases, where  $\sigma_\tau^{-1} = \sqrt{\widehat{G}_t(b_\tau)\parr*{1-\widehat{G}_t(b_\tau)}}$ as defined in \cref{lem:bias-variance}. We have
\begin{align*}
\htheta_{t}^{\top}x_t - \theta_{*}^{\top}x_t &= x_t^{\top}A^{-1}_{t}b_t - x_t^{\top}A^{-1}_{t}\parr*{I+D_{t}D_{t}^{\top}}\theta_{*}\\
&= x_t^{\top}A_{t}^{-1}D_{t}\parr*{V_{t} - \Xi_t - D^{\top}_{t}\theta_{*}} + x_t^{\top}A_{t}^{-1}D_{t}\Xi_t + x_t^{\top}A_{t}^{-1}\theta_{*}.
\end{align*}
Given $\|\theta_{*}\|_2\leq 1$, it follows that
\[
\abs*{\htheta_{t}^{\top}x_t - \theta_{*}^{\top}x_t} \leq \abs*{x_t^{\top}A_{t}^{-1}D_{t}\parr*{V_{t} - \Xi_t - D^{\top}_{t}\theta_{*}}} + \abs*{x_t^{\top}A_{t}^{-1}D_{t}\Xi_t} + \|x_t^{\top}A_{t}^{-1}\|_2.\numberthis\label{eq:WLS_app_width_decompose}
\]
Since $A_t\succeq I$, the last term is upper bounded by
\[
\|x_t^{\top}A_{t}^{-1}\|_2 = \sqrt{x_t^{\top}A_t^{-1}IA_t^{-1}x_t} \leq \sqrt{x_t^{\top}A_t^{-1}x_t} = \|x_t\|_{A_t^{-1}}.
\]
For the first term in \eqref{eq:WLS_app_width_decompose}, we can write
\[
x_t^{\top}A_{t}^{-1}D_{t}\parr*{V_{t} - \Xi_t -D^{\top}_{t}\theta_{*}} = \sum_{\tau\in\Phi_t}x_t^{\top}A_t^{-1}\sigma_\tau^{-2}x_\tau \widetilde{\varepsilon}_\tau
\]
where $\widetilde{\varepsilon}_\tau = \widetilde{e}_\tau(b_\tau) - \theta_*^{\top}x_\tau - \xi_\tau$ satisfies the followings: 
\begin{itemize}
    \item $\widetilde{\varepsilon}_\tau$ are conditionally independent given $(x_\tau, b_\tau, M_\tau)_{\tau\in\Phi_t}$;

    \item $\E[\widetilde{\varepsilon}_\tau] = 0$;

    \item $\abs*{\widetilde{\varepsilon}_\tau} \leq |\widetilde{e}_\tau(b_\tau)| \leq 2\max\bra*{\widehat{G}_t(b_\tau)^{-1}, \parr*{1-\widehat{G}_t(b_\tau)}^{-1}}$;
    
    \item $\Var(\widetilde{\varepsilon}_\tau) = \Var(\widetilde{e}_\tau(b_\tau))  \leq c'\sigma_\tau^{-2}$ by Lemma~\ref{lem:bias-variance}.
\end{itemize}
Since $\max\bra*{\widehat{G}_t(b_\tau)^{-1}, \parr*{1-\widehat{G}_t(b_\tau)}^{-1}} \leq 2 \widehat{G}_t(b_\tau)^{-1}\parr*{1-\widehat{G}_t(b_\tau)}^{-1},$
it follows that for every $\tau\in\Phi_t$,
\begin{itemize}
    \item $\Var\parr*{x_t^{\top}A_t^{-1}\sigma_\tau^{-2}x_\tau\widetilde{\varepsilon}_\tau} \leq \abs*{x_t^{\top}A_t^{-1}\sigma_\tau^{-1} x_\tau}^2\sigma_\tau^{-2}\Var(\widetilde{\varepsilon}_\tau) \leq c'\abs*{x_t^{\top}A_t^{-1}\sigma_\tau^{-1} x_\tau}^2;$

    \item $\abs*{x_t^{\top}A_t^{-1}\sigma_\tau^{-2}x_\tau\widetilde{\varepsilon}_\tau} \leq 4\abs*{x_t^{\top}A_t^{-1}x_\tau} \leq 4\|x_t^{\top}A_t^{-1}\|_2 \leq 4\|x_t\|_{A_t^{-1}}$.
\end{itemize}
Then by Bernstein's inequality (Lemma~\ref{lem:bernstein}), with probability at least $1-\frac{1}{T^{-2}}$,
\begin{align*}
\abs*{\sum_{\tau\in\Phi_t}x_t^{\top}A_t^{-1}\sigma_\tau^{-2}x_\tau \widetilde{\varepsilon}_\tau} &\leq \sqrt{2c'\log\parr*{2T^2}\sum_{\tau\in\Phi_t}\abs*{x_t^{\top}A_t^{-1}\sigma_\tau^{-1} x_\tau}^2} + \frac{8}{3}\log\parr*{2T^2}\|x_t\|_{A_t^{-1}} \\
&= \sqrt{2c'\log\parr*{2T^2}\|x_t^{\top}A_t^{-1}D_t\|_2^2} + \frac{8}{3}\log\parr*{2T^2}\|x_t\|_{A_t^{-1}} \\
&\stepa{\leq} \sqrt{2c'\log\parr*{2T^2}\|x_t\|_{A_t^{-1}}^2} + \frac{8}{3}\log\parr*{2T^2}\|x_t\|_{A_t^{-1}} \\
&\leq c_1\log\parr*{2T} \|x_t\|_{A_t^{-1}}
\end{align*}
where $c_1>0$ is an absolute constant and (a) follows from $\|x_t^{\top}A_t^{-1}D_t\|_2^2 = x_t^{\top}A_t^{-1}D_tD_t^{\top}A_t^{-1}x_t \leq x_tA_t^{-1}x_t = \|x_t\|_{A_t^{-1}}$ by $A_t=I + D_tD_t^{\top}$. Finally, to bound the middle term in \eqref{eq:WLS_app_width_decompose}, we write
\begin{align*}
\abs*{x_t^{\top}A_{t}^{-1}D_{t}\Xi_t} &\stepb{\leq} \|x_t\|_{A_t^{-1}} \|D_t\Xi_t\|_{A_t^{-1}}\\
&= \|x_t\|_{A_t^{-1}} \sqrt{\Xi_t^{\top}D_t^{\top}A_t^{-1}D_t\Xi_t}\\
&\stepc{\leq} \|x_t\|_{A_t^{-1}}\|\Xi_t\|_2
\end{align*}
where (b) applies Cauchy-Schwartz inequality and (c) is due to $D_t^{\top}(I+D_tD_t^{\top})D_t\preceq I$ by Woodbury matrix identity. To obtain the desired claim, we can further upper bound the following by Lemma~\ref{lem:bias-variance}:
\begin{align*}
\|\Xi_t\|_2 = \sqrt{\sum_{\tau\in\Phi_t}\sigma_\tau^{-2}\xi_\tau^2}\leq c\sqrt{\sum_{\tau\in\Phi_t}\delta_\tau^2}.
\end{align*}
\end{proof}

\subsection{Proof of \cref{lem:TE_width}}
\begin{lemma}[Restatement of \cref{lem:TE_width}]\label{lem:TE_width_app}
Suppose the event in Lemma~\ref{lem:WLS} holds. Let $\widehat{r}_{t,0}$ and $\widehat{r}_{t,1}$ be defined as in \eqref{eq:UCB0} and \eqref{eq:UCB1}. For any $s\in[S]$ and for the criterion $i\in\{0,1\}$ selected in stage $s$ in \cref{alg:master_alg_te}, it holds that 
\begin{align*}
u_{t,i}^{(s)}(b) - 2w^{(s)}_t(b)\le \widehat{r}_{t,i}(b)  \le u^{(s)}_{t,i}(b), \quad \forall b\in [b_{*,1}, b_{*,0}] 
\end{align*}
where $b^{(s)}_{*,i}=\argmax_{b\in B_s}u^{(s)}_{t,i}(b)$ for $i=0,1$ as defined in \cref{alg:base_alg_te}.
\end{lemma}
\begin{proof}
Fix any $s\in[S]$. For the ease of notation, we suppress the superscript $(s)$ for $A_t^{(s)}$, $b^{(s)}_{*,i}$, $w_t^{(s)}$, $u^{(s)}_{t,1}$, and $u^{(s)}_{t,0}$ in the remaining proof. Denote $\gamma_t=1+c\log(2T)+c'\sqrt{\sum_{\tau\in\Phi_t^{(s)}}\delta_\tau^2}$ as defined in \cref{alg:base_alg_te}. By Lemma~\ref{lem:approx_CDF_UCB_Lip}, when $2\gamma_t\|x_t\|_{A_t^{-1}}\leq \frac{1}{20L}$, we have $\widehat{G}_t(b_{*,0})-\widehat{G}_t(b_{*,1})\leq 1-\frac{1}{20L}$.

If $\widehat{G}_t(b_{*,1})> \frac{1}{40L}$, then $\widehat{G}_t(b)\geq \frac{1}{40L}$ and $1\leq 40L \widehat{G}_t(b)$ for all $b\in [b_{*,1}, b_{*,0}]$ by monotonicity of $\widehat{G}_t$. Recall that $\hr_{t,1}(b) = (1-\widehat{G}_t(b))\parr*{b-\theta_*^{\top}x_t}-b$ in \eqref{eq:UCB1}. By Lemma~\ref{lem:WLS},
\[
\hr_{t,1}(b) \leq u_{t,1}(b) = \widehat{G}_t(b)\parr*{\htheta_t^{\top}x_t -b - \gamma\|x_t\|_{A_t^{-1}}} - \parr*{\htheta_t^{\top}x_t  - \gamma\|x_t\|_{A_t^{-1}}}
\]
and
\begin{align*}
\abs*{u_{t,1}(b) - \hr_{t,1}(b)} &= \abs*{ \widehat{G}_t(b)\parr*{\htheta_t^{\top}x_t -b - \gamma\|x_t\|_{A_t^{-1}}} - \parr*{\htheta_t^{\top}x_t  - \gamma\|x_t\|_{A_t^{-1}}} - \hr_{t,1}(b)}\\
&= (1-\widehat{G}_t(b))\abs*{\theta_*^{\top}x_t - \htheta_t^{\top}x_t+\gamma\|x_t\|_{A_t^{-1}}}\\
&\leq 2(1-\widehat{G}_t(b))\gamma_t\|x_t\|_{A_t^{-1}} \\
&\leq 80L\gamma_t \widehat{G}_t(b)\parr{1-\widehat{G}_t(b)} \|x_t\|_{A_t^{-1}} \\
&\leq w_t(b).
\end{align*}

If $\widehat{G}_t(b_{*,1})\leq \frac{1}{40L}$, since $\widehat{G}_t(b_{*,0})-\widehat{G}_t(b_{*,1})\leq 1-\frac{1}{20L}$, we have $\widehat{G}_t(b_{*,0})\leq 1-\frac{1}{40L}$ when $2\gamma_t\|x_t\|_{A_t^{-1}}\leq \frac{1}{20L}$. Then for all $b\in [b_{*,1}, b_{*,0}]$,
\[
1\leq 40L(1-\widehat{G}_t(b)) + 40L\gamma_t\|x_t\|_{A_t^{-1}}.
\]
Again, by definition in \eqref{eq:UCB0} and Lemma~\ref{lem:WLS}, 
$$\widehat{G}_t(b)\parr*{\theta_*^{\top}x_t -b} = \hr_{t,0}(b)\leq u_{t,0}(b) = \widehat{G}_t(b)\parr*{\htheta_t^{\top}x_t -b + \gamma\|x_t\|_{A_t^{-1}}}.$$
For all $b\in [b_{*,1}, b_{*,0}]$ we have
\begin{align*}
\abs*{u_{t,0}(b) - \hr_{t,0}(b)} &\leq 2\widehat{G}_t(b)\gamma_t\|x_t\|_{A_t^{-1}}\\
&\leq 80L\parr*{(1-\widehat{G}_t(b)) + \gamma_t\|x_t\|_{A_t^{-1}}}\widehat{G}_t(b)\gamma_t\|x_t\|_{A_t^{-1}}\\
&\leq w_t(b).
\end{align*}
\end{proof}



\subsection{Proof of \cref{lem:master_confidence_bound_te}}
\begin{lemma}[Restatement of \cref{lem:master_confidence_bound_te}]\label{lem:master_confidence_bound_te_app}
In \cref{alg:master_alg_te}, for every $t\in[T]$ and $s\in[S]$, it holds that $\max_{b^*\in\Bcal}\hr_t(b^*)-\hr_t(b) \leq 8\cdot 2^{-s}$ for all $b\in B_s$.
\end{lemma}
\begin{proof}
This proof shares the same idea as the proof for Lemma~\ref{lem:master_confidence_bound}.
Denote the optimal bid under $\hr_t$ by $b^*_t = \argmax_{b^*\in \Bcal}\hr_t(b^*).$ We prove this result via an induction argument on $s\in[S]$ with the following induction hypothesis: for each $s\in[S]$, the optimal bid remains uneliminated, i.e. $b^*_t\in B_s$, and $\hr_t(b^*_t)- \hr_t(b) \leq 8\cdot 2^{-s}$ for all $b\in B_s$. For $s=1$ it clearly holds.

Suppose the hypothesis holds for $s-1$. The elimination step (Line~\ref{line:master_po_elim}) in \cref{alg:master_alg_po} implies that for any $b\in B_s$, we have $w^{(s-1)}_t(b) \leq 2^{-(s-1)}$, $w^{(s-1)}_t(b^*_t)\leq 2^{-(s-1)}$, and by Lemma~\ref{lem:TE_width} $u_{t,i}^{(s-1)}(b) \geq u_{t,i}^{(s-1)}(b^*_t) - 2^{-s+2}$ for the criterion $i\in\{0,1\}$ selected in stage $s-1$. Since $b^*_t\in B_{s-1}$ by the induction hypothesis, by \cref{lem:TE_width},
\[
u_{t,i}^{(s-1)}(b^*_t) \geq \hr_t\parr*{b^*_t} \geq \hr_t(b) \geq u_{t,i}^{(s-1)}(b) - 2w^{(s-1)}_t(b) \geq \hr^{(s-1)}_t(b) - 2^{-(s-1)}
\]
for all $b\in B_{s-1}$. Therefore, together with Lemma~\ref{lem:optimal_bid_interval}, $b^*_t\in B_s$ in the elimination step. Then for any $b\in B_s$,
\begin{align*}
\hr_t(b^*_t) - \hr_t(b) &= \hr_{t,i}(b^*_t) -\hr_{t,i}(b)\\
&\stepa{\leq} u_{t,i}^{(s-1)}(b^*_t) - u^{(s-1)}_{t,i}(b) + 2w^{(s-1)}_t(b)\\
&\leq u_{t,i}^{(s-1)}(b^*_t) - \max_{b\in B_{s-1}}u^{(s-1)}_{t,i}(b) + 2\cdot 2^{-(s-1)}\\
&\stepb{\geq} - 4\cdot 2^{-(s-1)}
\end{align*}
where (a) uses \cref{lem:PO_width} and (b) is by the elimination criterion in \cref{alg:master_alg_te}. This concludes the induction and hence the proof.
\end{proof}


\subsection{Proof of \cref{lem:elliptical-potential-TE}}

Then we are ready for the proof of \cref{lem:elliptical-potential-TE}.
\begin{lemma}[Restatement of \cref{lem:elliptical-potential-TE}]\label{lem:elliptical-potential-TE_app}
For any $s\in[S]$, suppose $\delta_t<0.1$ for $t\in\Phi^{(s)}$ and we have
\[
\sum_{t\in\Phi^{(s)}}w^{(s)}_t(b_t) = O(\sqrt{dT\Delta_2}\log^2 T + d\Delta_2\log T + d\log^3 T).
\]
\end{lemma}
\begin{proof}
Fix any $s\in[S]$. Write $A_t=A_t^{(s)}$ and $\gamma = 1+ c_1\log(2T) + c_2\sqrt{\sum_{t\in\Phi^{(s)}\delta_t^2}}$ for simplicity. For each time $t$, recall in Algorithm~\ref{alg:base_alg_te} we write $\gamma_t = 1+ c_1\log(2T) +  c_2\sqrt{\sum_{\tau\in\Phi_t^{(s)}\delta_\tau^2}}$.
Summing over the widths in this stage gives:
\begin{align*}
    \sum_{t\in\Phi^{(s)}}w^{(s)}_t(b_t) &= \sum_{t\in\Phi^{(s)}} c_3L\gamma_t\widehat{G}_t(b)\parr*{1-\widehat{G}_t(b)}\|x_t\|_{A_t^{-1}} + c_3L\gamma_t^2\|x_t\|_{A_t^{-1}}^2 \\
    &\stepa{\leq} \sum_{t\in\Phi^{(s)}} c_3L\gamma_t\|\sigma_t^{-1} x_t\|_{A_t^{-1}} + 2c_3L\gamma_t\sqrt{\frac{T}{d}}\|\sigma_t^{-1} x_t\|_{A_t^{-1}}^2 + 20c_3L\gamma_t^2\|\sigma_t^{-1} x_t\|_{A_t^{-1}}^2 \\
    &\stepb{\leq} \sqrt{2}c_3L\gamma\sqrt{d|\Phi^{(s)}|\log\abs*{\Phi^{(s)}}} + 4c_3L\gamma\sqrt{dT}\log\parr*{1+\frac{\abs*{\Phi^{(s)}}}{d}} + 40c_3L\gamma^2d \log\parr*{1+\frac{\abs*{\Phi^{(s)}}}{d}} \\
    &\leq cL\gamma\sqrt{dT}\log T + cL\gamma^2d\log T
\end{align*}
for an absolute constant $c>0$. Note that \cref{ass:est-oracle-2} implies $\|\widehat{G}_t-G_t\|_{\infty}\leq 2\delta_t$. Thus by the truncation step in Line 22 of Algorithm~\ref{alg:master_alg_te} and \cref{lem:truncation}, it holds that
\[
\min\bra*{\gamma_t\sqrt{\frac{d}{T}}, \frac{1}{2}} \leq \widehat{G}_t(b_t) \leq \min\bra*{1-\gamma_t\sqrt{\frac{d}{T}}, \frac{1}{2} + 4\delta_t}
\]
Recall $\sigma_t^{-2} = \widehat{G}_t(b_t)(1-\widehat{G}_t(b_t)) \le 2\min\bra{\widehat{G}_t(b_t), 1-\widehat{G}_t(b_t)}.$ Then (a) uses 
\[
\|x_t\|_{A_t^{-1}}^2 \leq 2\min\bra*{\gamma_t^{-1}\sqrt{\frac{T}{d}}, 10}\|\sigma_t^{-1} x_t\|_{A_t^{-1}}^2 \leq  \parr*{2\gamma_t^{-1}\sqrt{\frac{T}{d}} + 20}\|\sigma_t^{-1} x_t\|_{A_t^{-1}}^2
\]
since $\delta_t<0.1$; and (b) uses \cref{lem:sum_width_bound2_FPA2} and that $\gamma_t\leq \gamma$.
\end{proof}


\subsection{Proof of \cref{lem:linear_hob_error}}\label{app:linear_hob}
The proof of \cref{lem:linear_hob_error} is a direct consequence of \cref{lem:sum_width_bound2_FPA2} given the following explicit expression of $\{\delta_t\}$: 


\begin{lemma}\label{lem:bernstein_for_gaussian_noise}
Under the conditions of \cref{lem:linear_hob_error}, there is an estimator $\widehat{G}_t$ for $G_t$ that satisfies \cref{ass:est-oracle-2} with 
\begin{align*}
\delta_t=O\parr*{\log^{3/2}(T)\sqrt{\frac{d}{t}} + \log(T) \|x_t\|_{\Sigma_t^{-1}} }.
\end{align*}
Here $\Sigma_t = 18I + \sum_{s<t}x_sx_s^{\top}$ is the regularized Gram matrix at time $t$.
\end{lemma}
\begin{proof}
Given the Lipschitzness in \cref{ass:Gt}, we consider the discretized bid space $\widehat{\Bcal} = \bra{\frac{i}{T}: i\in\{0\}\cup[T]}$. At each time $t$, we estimate the parameter $\varphi_*$ via linear regression with a careful data splitting scheme: Denote $\Sigma_t = 18I + \sum_{s<t}x_s x_s^{\top}$. By \cref{lem:KS_data_splitting}, there is a subset $S_t\subseteq [t-1]$ such that $|S_t|\leq \frac{t-1}{2}$ and $18I + \sum_{s\in S_t}x_s x_s^{\top}\succeq \frac{\Sigma_t}{9}$. Define $A_t = 18I + \sum_{s\in S_t}x_s x_s^{\top}$ and $z_t = \sum_{s\in S_t}M_s x_s$, and set the estimator $\widehat{\varphi}_t = A_t^{-1}z_t$. By \cref{lem:PO_width}, with probability at least $1-T^{-2}/2$, we have a confidence bound that
\begin{equation}\label{eq:temp_HOB_width}
\abs*{\widehat{\varphi}_t^{\top}x - \varphi_*^{\top}x} \leq \beta\|x\|_{A_t^{-1}}
\end{equation}
for $\beta = O(\sigma\sqrt{\log T})$ and all $x\in \{x_t - x_{s}: s\in [t-1]\}$, where $\sigma$ is the sub-Gaussian parameter in \cref{ass:flat_noise_tail}. Construct the empirical CDF as follows: let $X_s(b) = \indic[M_s - \widehat{\varphi}_t^{\top}x_s + \widehat{\varphi}_t^{\top}x_t\leq b]$ and $\widehat{G}_t(b) = \frac{1}{|S_t^c|}\sum_{s\in S_t^c}X_s(b)$ using the remaining data $S_t^c=[t-1]\backslash S_t$. Note
\begin{align*}
\E[X_s(b)] &= \mathbb{P}\parr*{M_s - \widehat{\varphi}_t^{\top}x_s + \widehat{\varphi}_t^{\top}x_t\leq b}\\
&= Q\parr*{b-\varphi_*^{\top}x_s + \widehat{\varphi}_t^{\top}x_s - \widehat{\varphi}_t^{\top}x_t}\\
&= Q\parr*{b-\varphi_*^{\top}x_t} + Q\parr*{b-\varphi_*^{\top}x_t +\varepsilon_s} - Q\parr*{b-\varphi_*^{\top}x_t} \numberthis\label{eq:indicator_bias_Mt}
\end{align*}
where $Q(\cdot)$ denotes the CDF of the noise and $\varepsilon_s = (\widehat{\varphi}_t-\varphi_*)^{\top}(x_s-x_t)$. By \eqref{eq:temp_HOB_width}, the error term $\varepsilon_s$ satisfies 
\begin{equation}\label{eq:est_error_in_Q}
|\varepsilon_s| \leq \beta\|x_s-x_t\|_{A_t^{-1}}.
\end{equation}
Note $Q\parr*{b-\varphi_*^{\top}x_t}=\mathbb{P}(M_t\leq b)=G_t(b)$, so the bias of this indicator variable is in the difference of the last two terms in \eqref{eq:indicator_bias_Mt}. 

Write the target value as $v=\varphi_*^{\top}x_t-b$ for simplicity. 
Since $\E[X_s(b)] = Q(v+\varepsilon_s)$, its variance is $\Var(X_s(b)) = Q(v+\varepsilon_s)(1-Q(v+\varepsilon_s))$. Now we apply Bernstein's inequality (cf. Lemma~\ref{lem:bernstein}) to $\widehat{G}_t(b)$ and a union bound to obtain, with probability at least $1-T^{-2}/2$, for all $b\in\widehat{\Bcal}$, 
\begin{equation*}
\abs*{\widehat{G}_t(b) - \frac{1}{|S_t^c|}\sum_{s\in S_t^c}Q\parr*{v +\varepsilon_s}} \leq \frac{2\sqrt{2\log(2T)}}{|S_t^c|}\sqrt{\sum_{s\in S_t^c}Q(v+\varepsilon_s)(1-Q(v+\varepsilon_s))} + \frac{8\log(2T)}{3|S_t^c|}.
\end{equation*}
For each $s<t$, we can decompose
\begin{align*}
Q(v+\varepsilon_s)(1-Q(v+\varepsilon_s)) &= Q(v)(1-Q(v)) + Q(v+\varepsilon_s)- Q(v+\varepsilon_s)^2 - Q(v) + Q(v)^2\\
&= Q(v)(1-Q(v)) + \parr*{Q(v+\varepsilon_s) - Q(v)}\parr*{1-Q(v+\varepsilon_s)-Q(v)}\\
&\leq Q(v)(1-Q(v)) + \abs*{Q(v+\varepsilon_s) - Q(v)}.
\end{align*}
Together with the bias term in \eqref{eq:indicator_bias_Mt} and that $\sqrt{a+b}\leq \sqrt{a}+\sqrt{b}$ for $a,b>0$, we obtain an error bound for our CDF estimator:
\begin{align*}
\abs*{\widehat{G}_t(b) - G_t(b)} &\leq 2\sqrt{\frac{2\log(2T)Q(v)(1-Q(v))}{|S_t^c|}} + \frac{2\sqrt{2\log(2T)}}{|S_t^c|}\sqrt{\sum_{s\in S_t^c}\abs*{Q(v+\varepsilon_s) - Q(v)}}\\
&\quad + \frac{8\log(2T)}{3|S_t^c|} + \frac{1}{|S_t^c|}\sum_{s\in S_t^c}\abs*{Q(v+\varepsilon_s) - Q(v)}.
\end{align*}
If $\sum_{s\in S_t^c}\abs*{Q(v+\varepsilon_s) - Q(v)}\leq 1$, since $|S_t^c|\geq \frac{t-1}{2}$, it becomes
\begin{align*}
\abs*{\widehat{G}_t(b) - G_t(b)} &\leq 2\sqrt{\frac{2\log(2T)Q(v)(1-Q(v))}{|S_t^c|}} + \frac{2\sqrt{2\log(2T)}}{|S_t^c|} + \frac{11\log(2T)}{3|S_t^c|}\\
&\leq 4\sqrt{\frac{2\log(2T)Q(v)(1-Q(v))}{t}} + \frac{8\sqrt{2\log(2T)}}{t} + \frac{15\log(2T)}{t}
\end{align*}
and we are done.
When $\sum_{s\in S_t^c}\abs*{Q(v+\varepsilon_s) - Q(v)}>1$, the bound can be simplified to
\begin{equation}\label{eq:bernstein_error_Gt}
\abs*{\widehat{G}_t(b) - G_t(b)} \leq 2\sqrt{\frac{2\log(2T)Q(v)(1-Q(v))}{|S_t^c|}} + \frac{4\sqrt{\log(2T)}}{|S_t^c|}\sum_{s\in S_t^c}\abs*{Q(v+\varepsilon_s) - Q(v)} + \frac{3\log(2T)}{|S_t^c|}.
\end{equation}
The key lies in upper bounding the second term. Note we have 
\[
\abs*{Q(v+\varepsilon_s) - Q(v)} \leq Q'(v)|\varepsilon_s| + \|Q''\|_\infty\varepsilon_s^2.
\]
Let $a_0,a_1,a_2>0$ be the constants in \cref{ass:flat_noise_tail}. When $Q(v)\in(a_0,1-a_0)$, we bound the second term as
\begin{align*}
\sum_{s\in S_t^c}\abs*{Q(v+\varepsilon_s) - Q(v)} &\leq L\beta\sum_{s\in S_t^c}\|x_s-x_t\|_{A_t^{-1}} + a_2\beta^2\sum_{s\in S_t^c}\|x_s-x_t\|_{A_t^{-1}}^2\\
&\leq a_0^{-1}L\beta \sqrt{Q(v)(1-Q(v))}\sum_{s\in S_t^c}\parr*{\|x_s\|_{A_t^{-1}} + \|x_t\|_{A_t^{-1}}} + 2a_2\beta^2\sum_{s\in S_t^c}\parr*{\|x_s\|_{A_t^{-1}}^2 + \|x_t\|_{A_t^{-1}}^2}\\
&\stepa{\leq} a_0^{-1}L\beta\sqrt{Q(v)(1-Q(v))}\parr*{\sqrt{2d|S_t^c|\log(T)} + 3|S_t^c|\|x_t\|_{\Sigma_t^{-1}}}\\
&\quad + 2a_2\beta^2\parr*{2d\log(T)+ 9|S_t^c|\|x_t\|_{\Sigma_t^{-1}}^2} + c\beta^2\numberthis\label{eq:sum_bias_bound1}
\end{align*}
for a constant $c>0$, where (a) uses \cref{lem:sum_width_bound2_FPA2} and the properties of $S_t$. On the other hand, when $Q(v)\in[0,a_0]\cup[1-a_0,a]$, we have
\begin{align*}
\sum_{s\in S_t^c}\abs*{Q(v+\varepsilon_s) - Q(v)} &\leq a_1\beta\sqrt{Q(v)(1-Q(v))}\sum_{s\in S_t^c}\|x_s-x_t\|_{A_t^{-1}} + a_2\beta^2\sum_{s\in S_t^c}\|x_s-x_t\|_{A_t^{-1}}^2\\
&\stepa{\leq} a_1\beta\sqrt{Q(v)(1-Q(v))}\parr*{\sqrt{2d|S_t^c|\log(T)} + 3|S_t^c|\|x_t\|_{\Sigma_t^{-1}}}\\
&\quad + 2a_2\beta^2\parr*{2d\log(T)+ 9|S_t^c|\|x_t\|_{\Sigma_t^{-1}}^2} + c\beta^2. \numberthis\label{eq:sum_bias_bound2}
\end{align*}
Plugging \eqref{eq:sum_bias_bound1} and \eqref{eq:sum_bias_bound2} into \eqref{eq:bernstein_error_Gt} and substituting in $|S_t^c|\geq \frac{t-1}{2} \geq \frac{t}{4}$, the CDF estimation error is bounded as desired: for every $b\in\widehat{\Bcal}$, with probability at least $1-T^{-2}/2$,
\begin{equation*}
\abs*{\widehat{G}_t(b) - G_t(b)} \leq \delta_t\sqrt{Q(v)(1-Q(v))} + \delta_t^2
\end{equation*}
where $\delta_t = O\parr*{\log^{3/2}(T)\sqrt{d/t} + \log(T)\|x_t\|_{\Sigma_t^{-1}}}$, and we recall that $Q(v)=G_t(b)$.

For any $b\in\Bcal$ in the continuous bid space, let its discretized neighbors be $h_1=h_2-T^{-1}\leq b \leq h_2 $ for $h_1,h_2\in\widehat{\Bcal}$. Thanks to the Lipschitzness of $Q$, for both $h=h_1,h_2$, it holds that $G_t(h)(1-G_t(h))\leq G_t(b)(1-G_t(b)) + \frac{L}{T}$. Hence by monotonicity of $\widehat{G}_t$ and the triangular inequality, with probability at least $1-T^{-2}/2$,
\begin{align}\label{eq:HOB_error_bound_cts}
\abs*{\widehat{G}_t(b)-G_t(b)} &\leq \max_{h\in \{h_1,h_2\}} \abs*{\widehat{G}_t(h)-G_t(h)} + \frac{L}{T}\nonumber \\
&\le \delta_t\left(\sqrt{G_t(b)(1-G_t(b))} + \sqrt{\frac{L}{T}}\right) + \delta_t^2 + \frac{L}{T} \nonumber \\
&= O(\delta_t)\sqrt{G_t(b)(1-G_t(b))} + O(\delta_t^2).
\end{align}
The proof is completed by taking a union bound over \eqref{eq:temp_HOB_width}, \eqref{eq:HOB_error_bound_cts}, and all time indices $t\in[T]$.

\end{proof}


\section{Proof of lower bounds}\label{app:lin_lower_bound}
In this section we prove the regret lower bounds in \cref{thm:lower-bound}. We begin with the analysis of Model 1. 

\begin{theorem}\label{thm:lower_bound_adv}
Consider a special instance with i.i.d. $v_{t,1}\sim \mathrm{Bern}(\mu)$ for an unknown $\mu\in [0,1]$, $v_{t,0}\equiv 0$, and i.i.d. $M_t$ following a known distribution
\begin{align*}
M_t \sim \frac{1}{2}\mathrm{Unif}\left([0,\frac{1}{100}]\right) + \frac{1}{2}\mathrm{Unif}\left([\frac{1}{8}-\frac{1}{200},\frac{1}{8} + \frac{1}{200}]\right). 
\end{align*}
Then for some absolute constant $c>0$, it holds that
\begin{align*}
\inf_{\pi}\sup_{\mu\in [0,1]} \sum_{t=1}^T \left( \max_{b_*\in [0,1]} \E[r_t(b_*)] - \E[r_t(b_t)] \right) \ge c\sqrt{T}. 
\end{align*}
\end{theorem}
\begin{proof}
We use Le Cam's two-point lower bound. Set $\mu_1 = \frac{1}{4}-\Delta$ and $\mu_2 = \frac{1}{4}+\Delta$, with $\Delta := \frac{1}{4\sqrt{T}}$. Let $\E[r_i(b)] = G(b)(\mu_i -b)$ be the expected payoff under the choice $\mu_i$, for $i=1,2$. By straightforward computation, we have
\begin{align*}
\max_{b\in[0,1]}\E[r_1(b)] &= \E[r_1(b_1^*)] = \frac{1}{8}-\frac{1}{200} - \frac{\Delta}{2}, \\
\max_{b\in[0,1]}\E[r_2(b)] &= \E[r_2(b_2^*)] = \frac{1}{8} + \Delta - \frac{1}{200},\\
\max_{b\in[0,1]}\E[r_1(b)] + \E[r_2(b)] &= \E[r_1(b_2^*)] + \E[r_2(b_2^*)] = \frac{1}{4} - \frac{1}{100}, 
\end{align*}
where $b_1^* := \frac{1}{200}$, and $b_2^* = \frac{1}{8}+\frac{1}{100}$. Consequently, for any $b_t\in[0,1]$, we have
\begin{align*}
&\max_{b\in[0,1]}\E[r_1(b) - r_1(b_t)] + \max_{b\in[0,1]}\E[r_2(b) - r_2(b_t)]\\
&\geq \max_{b\in[0,1]}\E[r_1(b)] + \max_{b\in[0,1]}\E[r_2(b)] - \max_{b\in[0,1]}\E[r_1(b)] + \E[r_2(b)]\\
&\geq \frac{\Delta}{2}. \numberthis\label{eq:lecam_separation}
\end{align*}
This shows that no single bid can perform well under both of the environments $\mu_1$ and $\mu_2$. For any fixed policy $\pi$, the environment $\mu_i$ gives rise to a distribution $\mathbb{P}_{i}^{\otimes T}$ over the time horizon $T$ for $i=1,2$. Denote the environment-specific regret by
\[
\mathsf{Reg}_i(\pi) = \E_{\mathbb{P}_{i}^{\otimes T}}\parq*{\sum_{t=1}^T \max_{b\in[0,1]} \E[r_i(b) - r_i(b_t)]}
\]
for $i=1,2$, where $b_t$ is the bid chosen by policy $\pi$. Let $\nor{\cdot}_{\mathrm{TV}}$ denote the total variation distance. It follows that
\begin{align*}
\mathsf{Reg}_1(\pi) + \mathsf{Reg}_2(\pi) &= \sum_{t=1}^{T}\mathbb{P}_{1}^{\otimes t}\parr*{\max_{b\in[0,1]}\E[r_1(b) - r_1(b_t)]} + \mathbb{P}_{2}^{\otimes t}\parr*{\max_{b\in[0,1]}\E[r_2(b) - r_2(b_t)]}\\
&\overset{\eqref{eq:lecam_separation}}{\geq} \frac{\Delta}{2}\sum_{t=1}^{T}\int\min\bra*{\mathrm{d}\mathbb{P}_{1}^{\otimes t}, \mathrm{d}\mathbb{P}_{2}^{\otimes t}}\\
&\stepa{=} \frac{\Delta}{2}\sum_{t=1}^{T} \left(1-\|\mathbb{P}_{1}^{\otimes t}-\mathbb{P}_{2}^{\otimes t}\|_{\mathrm{TV}}\right)\\
&\stepb{\geq} \frac{\Delta T}{2} \parr*{1-\nor*{\mathbb{P}_{1}^{\otimes T}-\mathbb{P}_{2}^{\otimes T}}_{\mathrm{TV}}}
\end{align*}
where (a) uses $\int\min\bra*{\mathrm{d}P, \mathrm{d}Q} = 1-\nor{P-Q}_{\mathrm{TV}}$ and (b) follows from the data processing inequality for the total variation distance. By Lemma~\ref{lem:KL_bounds_TV} and the fact that 
\[
D_{\mathrm{KL}}\parr*{\mathrm{Bern}(p)^{\otimes T}\|\mathrm{Bern}(q)^{\otimes T}} = TD_{\mathrm{KL}}\parr*{\mathrm{Bern}(p)\|\mathrm{Bern}(q)} \leq \frac{T(p-q)^2}{q(1-q)},
\]
we arrive at
\begin{align*}
\mathsf{Reg}_1(\pi) + \mathsf{Reg}_2(\pi) &\geq \frac{\Delta T}{4}\exp\parr*{-O(T\Delta^2)}. 
\end{align*}
Finally, plugging in the choice of $\Delta=\frac{1}{4\sqrt{T}}$ leads to
\[
\max_{i=1,2}\mathsf{Reg}_i(\pi) \geq \frac{\mathsf{Reg}_1(\pi) + \mathsf{Reg}_2(\pi)}{2} = \Omega(\sqrt{T}),
\]
establishing the theorem. 
\end{proof}

Note that \cref{thm:lower_bound_adv} completes the proof of \cref{thm:lower-bound} for Model 1, with $L = O(1)$ in \cref{ass:Gt}. We proceed to show that the special lower bound instance in \cref{thm:lower_bound_adv} can also be embedded in other lower bounds of \cref{thm:lower-bound}. Under Model 2 and $d\ge 2$, let
\begin{align*}
\theta_{1,*} = \left(\frac{1}{2},\mathrm{Unif}\parr*{ \Big\{-2\Delta, 2\Delta \Big\}^{d-1} }\right), \quad \theta_{0,*} = 0,  
\end{align*}
with $\Delta = \frac{1}{4}\sqrt{\frac{d-1}{T}}$. Since $T\ge d^2$, it holds that $\|\theta_{1,*}\|_2\le 1$ almost surely. Divide the time horizon $T$ into $d-1$ sub-horizons $T_i$ for $i=1,\dots,d-1$ with equal length $\frac{T}{d-1}$, and let 
\begin{align*}
    x_t = \left(\frac{1}{2}, 0, \dots, 0, \frac{1}{2}, 0, \dots, 0\right)
\end{align*}
during the sub-horizon $T_i$, where the second $\frac{1}{2}$ appears in the $(i+1)$-th entry. As for $M_t$, we again use the i.i.d. distribution in \cref{thm:lower_bound_adv}. Consequently, the regret $\Reglpo(\pi)$ can be decomposed as the sum of regrets from $d-1$ independent sub-problems, each of time duration $\frac{T}{d-1}$. By \cref{thm:lower_bound_adv}, we have
\begin{align*}
\inf_{\pi} \Reglpo(\pi) = (d-1) \cdot \Omega\parr*{\sqrt{\frac{T}{d-1}}} = \Omega(\sqrt{dT}), 
\end{align*}
as claimed. For the case of $d=1$, we simply use a different construction $\theta_{1,*}\sim \mathrm{Unif}(\{\frac{1}{4}-\Delta,\frac{1}{4}+\Delta\})$ and $x_t\equiv 1$, so that \cref{thm:lower_bound_adv} again gives a regret lower bound $\Omega(\sqrt{T})$. Finally, since $\Reglte(\pi) \ge \Reglpo(\pi)$, the lower bound for Model 3 also follows. 



\section{Auxiliary lemmas}
\label{app:aux_lem}

\begin{lemma}\label{lem:approx_CDF_UCB_Lip}
Let $G$ be a CDF on $[0,1]$ with density upper bounded by $L$, and $\widehat{G}$ be another CDF with $\|G-\widehat{G}\|_\infty \le 0.1$. Denote $\hb_*(v) = \argmax_{b\in[0,1]}\widehat{G}(b)\parr*{v-b}$ with tie broken by taking the maximum. For any $v_1\le v_2$, if $v_2 - v_1\le \frac{1}{20L}$, then
\begin{align*}
\widehat{G}(\widehat{b}_*(v_2)) - \widehat{G}(\widehat{b}_*(v_1)) \le 1 - \frac{1}{20L}. 
\end{align*}
\end{lemma}
\begin{proof}
For simplicity, write $b_1 = \hb_*(v_1)$ and $b_2=\hb_*(v_2)$. By \cref{lem:monotone_ucb_maximizers}, we have $\widehat{G}(b_1)\leq \widehat{G}(b_2)$. For the sake of contradiction, assume $\widehat{G}(b_2)-\widehat{G}(b_1) > 1-\frac{1}{20L}$, which implies $\widehat{G}(b_1)\leq \frac{1}{20L}$ and $\widehat{G}(b_2)> 1-\frac{1}{20L}$.

Define $b_+ = \sup\bra{b\in[0,1]: \widehat{G}(b)\leq \frac{1}{2}}$. We have $\widehat{G}(b_2) > 1-\frac{1}{20L} \geq \widehat{G}(b_+ - \varepsilon) + \frac{1}{2}-\frac{1}{20L}$ for every $\varepsilon>0$. Then $b_2 - b_+ \ge \frac{1}{5L}$, since otherwise by Lipschitzness of $G$, 
\[
\frac{1}{2}-\frac{1}{20L}< \widehat{G}(b_2) - \widehat{G}(b_+-\varepsilon
) \le 0.2 + L(b_2-b_++\varepsilon) < \frac{2}{5} + L\varepsilon
\]
for every $\varepsilon>0$, which is a contradiction\footnote{Since $G$ is a CDF, it holds that $L\geq 1$.}. Since $\widehat{G}(b_+)\ge \frac{1}{2}$, we have the following inequalities:
\begin{align*}
\frac{1}{20L} &\geq \widehat{G}(b_1)(v_1-b_1)\\
&\geq \widehat{G}(b_+)(v_1-b_+)\\
&\geq \frac{1}{2}\parr*{v_1-b_+}\\
&> \frac{1}{2}\parr*{v_1-b_2+\frac{1}{5L}}\\
&\ge \frac{1}{2}\parr*{v_2-\frac{1}{20L}-b_2+\frac{1}{5L}}\\
&\ge \frac{1}{2}\parr*{\frac{1}{5L}-\frac{1}{20L}}\\
&= \frac{3}{40L}
\end{align*}
which yields a contradiction. Hence the proof is complete.
\end{proof}

\begin{lemma}\label{lem:monotone_ucb_maximizers}
Let $G:[0,1]\rightarrow[0,1]$ be any CDF and let $v_1,v_0\in[0,1]$ satisfy $v_1\geq v_0$. Denote $b_*(v) = \argmax_{b\in[0,1]}G(b)\parr*{v-b}$ with tie broken by taking the maximum (or minimum). Then $b_*$ is increasing in $v$.
\end{lemma}
\begin{proof}
Let $b_1=b_*(v_1)$ and $b_0=b_*(v_0)$. For the sake of contradiction, suppose $b_0>b_1$. Then
\begin{align*}
G(b_1)\parr*{v_1-b_1} &= G(b_1)\parr*{v_0-b_1} + G(b_1)(v_1-v_0)\\
&\stepa{\leq} G(b_0)\parr*{v_0-b_0} + G(b_0)(v_1-v_0)\\
&= G(b_0)\parr*{v_1-b_0}.
\end{align*}
If tie is broken by taking the maximum, then we have $b_1=b_*(v_1)\ge b_0>b_1$, a contradiction. If tie is broken by minimum, inequality (a) becomes strict because $b_*(v_0)=b_0>b_1$ and hence $G(b_1)\parr*{v_0-b_1} < G(b_0)\parr*{v_0-b_0}$; then we end up with a contradiction again.
\end{proof}

\begin{lemma}\label{lem:optimal_bid_interval}
In \cref{alg:master_alg_te}, at any time $t$ and any stage $s$, it holds that 
\[
b^{(s)}_{*,1}\leq \argmax_{b^*\in\Bcal}\hr_t(b^*) \leq b^{(s)}_{*,0}
\]
for the UCB maximizers $b^{(s)}_{*,0}$ and $b^{(s)}_{*,1}$ as defined in Algorithm~\ref{alg:base_alg_te}.
\end{lemma}
\begin{proof}
Recall the definition of the optimal bid that 
\[
\hb_*(x_t) = \argmax_{b\in \Bcal} \hr_t(b) = \argmax_{b\in \Bcal} \widehat{G}_t(b)\parr*{\theta_*^{\top}x_t - b}.
\]
At each stage $s$, let $\htheta_t$ be the estimated parameter in \cref{alg:base_alg_te}. By Lemma~\ref{lem:WLS}, we have $\abs*{\htheta_t^{\top}x_t - \theta_*^{\top}x_t} \leq \gamma\|x_t\|_{{A_t^{(s)}}^{-1}}$, where $\gamma$ is defined in Line 7 of \cref{alg:base_alg_te}. Then since the mapping $v\mapsto \argmax_{b\in \Bcal} G(b)\parr*{v - b}$ is monotone by Lemma~\ref{lem:monotone_ucb_maximizers}, the claim follows from the definitions of $b^{(s)}_{*,0}$ and $b^{(s)}_{*,1}$ in Algorithm~\ref{alg:base_alg_te} and that $\hr_t(b^*) = \widehat{G}_t(b^*)(\theta_*^{\top}x_t-b^*)$.
\end{proof}

\begin{lemma}\label{lem:truncation}
Let $G$ be a continuous CDF on $[0,1]$, $v\in [0,1]$, and $t\in (0,\frac{1}{2}]$. Let $\widehat{G}$ be another CDF satisfying $\|G-\widehat{G}\|_{\infty} \le \varepsilon$. For $b\in [0,1]$, let 
$$b' = \min\{\max\{b, \widehat{G}^{-1}(t)\}, \widehat{G}^{-1}(1-t)\}.$$ 
Then $t\le \widehat{G}(b') \le 1-t+2\varepsilon$ and 
\begin{align*}
\widehat{G}(b)(v-b) - \widehat{G}(b')(v-b') \le t + 2\varepsilon. 
\end{align*}
\end{lemma}
\begin{proof}
If $\widehat{G}^{-1}(t)<b<\widehat{G}^{-1}(1-t)$, clearly the difference is zero. If $b\le \widehat{G}^{-1}(t)$, then by continuity of $G$ and the bounded error, we have $t \le \widehat{G}(b') \le t + 2\varepsilon$
and hence
\begin{align*}
\widehat{G}(b)(v-b) - \widehat{G}(b')(v-b') = v(\widehat{G}(b)-\widehat{G}(b')) + b'\widehat{G}(b')-b\widehat{G}(b) \le b'\widehat{G}(b') \le t + 2\varepsilon. 
\end{align*}
Finally, if $b\ge \widehat{G}^{-1}(1-t)$, then similarly we have $1-t\le \widehat{G}(b')\le 1-t+2\varepsilon$ and hence
\begin{align*}
\widehat{G}(b)(v-b) - \widehat{G}(b')(v-b') &= v(\widehat{G}(b)-\widehat{G}(b')) - (b\widehat{G}(b)-b'\widehat{G}(b')) \\
&\le \widehat{G}(b) - \widehat{G}(b')\le 1-\widehat{G}(b') \le t. 
\end{align*}
\end{proof}


\begin{lemma}[Bernstein's inequality in \cite{boucheron2003concentration}]
\label{lem:bernstein}
Consider independent random variables $X_1,\dots, X_n\in[a,b]$. We have
\[
\mathbb{P}\parr*{\abs*{\sum_{i=1}^nX_i - \sum_{i=1}^n\E[X_i]}\geq \varepsilon} \leq 2\exp\parr*{-\frac{\varepsilon^2}{2(\sigma^2 + \varepsilon(b-a)/3)}}
\]
for any $\varepsilon>0$, where $\sigma^2 = \sum_{i=1}^n\Var(X_i)$.

In particular, it implies the following confidence bound: for any $\delta\in(0,1)$, with probability at least $1-\delta$, we have
    \[
    \frac{1}{n}\left|\sum_{i=1}^nX_i - \sum_{i=1}^n\E[X_i] \right|\leq \sqrt{\frac{2\sigma^2/n\log(2/\delta)}{n}} + \frac{2(b-a)\log(2/\delta)}{3n}.
    \]
\end{lemma}

% \YH{find and add the original reference.}
\begin{lemma}[Bretagnolle--Huber inequality \cite{bretagnolle1978estimation}]\label{lem:KL_bounds_TV}
Let $P,Q$ be two probability measures on the same probability space. Then
\[
1-\nor{P-Q}_{\mathrm{TV}} \geq \frac{1}{2}\exp\parr*{-D_{\mathrm{KL}}(P\|Q) }
\]
where $\nor{\cdot}_{\mathrm{TV}}$ denotes the total variation distance, and $D_{\mathrm{KL}}$ denotes the KL divergence.
\end{lemma}

\begin{lemma}\label{lem:KS_data_splitting}
Let $x_1,\dots,x_n\in \R^d$ be vectors with $\|x_i\|_2\le 1$ for all $i\in [n]$, and $\Sigma := 18I + \sum_{i=1}^n x_ix_i^\top$. Then there exists $S\subseteq [n]$ with $|S|\le \frac{n}{2}$ such that
\begin{align*}
18I + \sum_{i\in S} x_ix_i^\top \succeq \frac{\Sigma}{9}. 
\end{align*}
\end{lemma}
\begin{proof}
The proof uses a deep result in the Kadison--Singer problem. Let $y_i := \Sigma^{-1/2}x_i$ for every $i\in [n]$, so that $\sum_{i=1}^n y_iy_i^\top \preceq I$. In addition, 
\begin{align*}
\|y_i\|_2^2 \le \frac{1}{18}\|x_i\|_2^2 \le \frac{1}{18}. 
\end{align*}
By \cite[Corollary 1.5]{marcus2015interlacing} (with $r = 2, \delta = \frac{1}{18}$), there exists a partition $\{S_1, S_2\}$ of $[n]$ such that for $j\in \{1,2\}$, 
\begin{align*}
\sum_{i\in S_j} y_iy_i^\top \preceq \parr*{\frac{1}{\sqrt{r}}+\sqrt{\delta}}^2 I = \frac{8}{9}I \Longrightarrow \sum_{i\in S_j} x_ix_i^\top \preceq \frac{8}{9}\Sigma. 
\end{align*}
WLOG assume that $|S_1|\ge \frac{n}{2}$, then the choice of $S=S_2$ satisfies $|S|\le \frac{n}{2}$, and
\begin{align*}
18I + \sum_{i\in S} x_ix_i^\top = \Sigma - \sum_{i\in S_1} x_ix_i^\top \succeq \frac{\Sigma}{9}. 
\end{align*}
This proves the lemma. 
\end{proof}


\end{document}


