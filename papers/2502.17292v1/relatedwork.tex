\section{Related Work}
\paragraph{Bidding in FPAs.} FPAs, also known as descending-price or Dutch auctions, have a long history. Early study of FPAs typically took a game-theoretic view dating back to the foundational work of \cite{vickrey1961counterspeculation, myerson1981optimal}, and there has been significant progress in understanding the computational complexity of Bayesian Nash equilibrium for online FPAs \cite{wang2020bayesian,filos2021complexity,bichler2023computing,chen2023complexity,filos2024computation}. A different line of study takes the perspective of online learning where a single bidder would like to learn the ``environment'', mainly the behaviors of other bidders in the literature. Dating back to \cite{kleinberg2003value} in a different pricing context, this perspective has inspired a flurry of work \cite{han2020learning,zhang2022leveraging,balseiro2023contextual,badanidiyuru2023learning,wang2023learning,han2024optimal,aggarwal2024no} which establishes sublinear regrets under various feedback structures and environments in FPAs; we refer to a recent survey \cite{aggarwal2024auto} for an overview. However, this line of work assumes a known valuation and mainly focuses on learning HOBs or handling other constraints (budget, side information, etc), and the problem of joint value estimation and bidding is explicitly listed as an open direction in \cite{han2024optimal}.

\paragraph{Unknown valuation.} There is also a rich line of literature on learning unknown valuations in auctions. In second-price auctions, \cite{weed2016online} studies a regret minimization setting where the bidder observes the valuation only if he/she wins the auction; this setting is then generalized to other auction formats \cite{feng2018learning} including FPAs \cite{achddou2021fast}. A recent work \cite{cesa2024role} provides a systematic study of feedback mechanisms (termed as \emph{transparency}) and unknown valuations in FPAs and provides tight regret bounds in various settings. However, all these studies do \emph{not} view the valuation of a bidder as a treatment effect, while an empirical study \cite{waisman2024online} stresses the importance of a causal inference modeling. Although \cite{waisman2024online} does not directly study the regret minimization problem, they identify a unique challenge in FPAs that the goals of regret minimization and causal inference are not well-aligned (while in second-price auctions these tasks are aligned). This challenge precisely motivates the problem formulation of joint value estimation and bidding in the current work. 

We provide a more detailed comparison with \cite{cesa2024role} as it is closest to ours. First, as explained above, the unknown valuation is the potential outcome of winning in \cite{cesa2024role}; by contrast, since the unknown valuation in our model is a treatment effect, we need to deal with the scenario where only the treatment effect, but not the potential outcomes, could be learned. Second, no contextual information is assumed in \cite{cesa2024role}, so that dependence between the valuation and HOB is allowed; as a comparison, we adopt a causal inference framework with available contexts, and make the standard unconfoundedness assumption that the potential outcomes are conditionally independent of the HOBs given the contexts. In fact, in our first model of adversarial outcomes without contexts, both the algorithm and analysis are inspired by \cite{cesa2024role}.  

\paragraph{Causal inference.} The causal inference formulation of value estimation in FPAs in \cite{waisman2024online} assumes the unconfoundedness and overlap conditions; both conditions are standard in causal inference \cite{imbens2015causal}, and are essential for controlling the estimation error in policy learning \cite{athey2021policy,zhou2023offline}. There have also been some efforts in relaxing or removing the overlap condition, such as trimming \cite{yang2018asymptotic,branson2023causal}, policy shifts \cite{zhao2024positivity}, and bypassing the uniform requirements \cite{jin2022policy}. For example, the policy learning result in \cite{jin2022policy} only involves the overlap condition for the selected policy; by contrast, thanks to the special nature of the joint value estimation and bidding problem, we do not even require the overlap condition for the selected bid.