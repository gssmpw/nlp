\documentclass[11pt]{article}

\input{_macros}
\mathtoolsset{showonlyrefs}


\newcommand{\tf}{\tilde{f}}
\newcommand{\tg}{\tilde{g}}
\newcommand{\lam}{\lambda}
\newcommand{\tlam}{\tilde\lambda}
\newcommand{\target}{\nu_*}
\newcommand{\eps}{\epsilon}

\usepackage{mathrsfs}
\allowdisplaybreaks[3]

\newcommand\ly[1]{\textcolor{magenta}{#1}}
\newcommand\todo[1]{\textcolor{red}{\textbf{TODO:}#1}}
\newcommand\note[1]{\textcolor{blue}{#1}} %
\newcommand\ignore[1]{\textcolor{red}{#1}} %
\def\Ltwo{\mathbb L_2}
\def\wass{{\sf W}}
\def\law{{\mathcal L}}
\def\rmd{{\,\rm d}}
\def\l|{\left\lVert}
\def\r|{\right\rVert}
\def\E{\mathbb E}
\def\R{\mathbb R}
\newcommand{\inprod}[2]{\ensuremath{\left\langle #1 , \, #2 \right\rangle}}


\definecolor{darkmidnightblue}{rgb}{0.0, 0.2, 0.4}
\definecolor{darkpowderblue}{rgb}{0.0, 0.2, 0.6}
\definecolor{dukeblue}{rgb}{0.0, 0.0, 0.61}

\hypersetup{
    colorlinks = true,
    citecolor= midnightblue,
    urlcolor= black,
    breaklinks=true,
    linkcolor = midnightblue,
    linkbordercolor = {white},
}

\definecolor{darkmidnightblue}{HTML}{003366}    
\definecolor{midnightblue}{HTML}{0059b3}
\definecolor{chromered}{HTML}{f14233}



\begin{document}

\title{Advancing Wasserstein Convergence Analysis of Score-Based Models: Insights from Discretization and Second-Order Acceleration}

 \author{
 Yifeng Yu\thanks{Department of Mathematical Sciences, Tsinghua University \texttt{yyf22@mails.tsinghua.edu.cn}}
 \and 
 Lu Yu\thanks{
  Department of Data Science,
  City University of Hong Kong \texttt{lu.yu@cityu.edu.hk}
 }
}

\maketitle

\begin{abstract}
Score-based diffusion models have emerged as powerful tools in generative modeling, yet their theoretical foundations remain underexplored. 
In this work, we focus on the Wasserstein convergence analysis of score-based diffusion models. Specifically, we investigate the impact of various discretization schemes, including Euler discretization, exponential integrators, and midpoint randomization methods. 
Our analysis provides a quantitative comparison of these discrete approximations, emphasizing their influence on convergence behavior. 
Furthermore, we explore scenarios where Hessian information is available and propose an accelerated sampler based on the local linearization method. 
We demonstrate that this Hessian-based approach achieves faster convergence rates of order 
$\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$ significantly improving upon the standard 
rate $\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon^2}\right)$ of vanilla diffusion models, where $\varepsilon$ denotes the target accuracy.

\end{abstract}


\section{Introduction}

\textit{Diffusion models} have become a pivotal framework in modern generative modeling, achieving notable success across fields such as image generation~\cite{ramesh2022hierarchical,ho2020denoising,song2020denoising,dhariwal2021diffusion}, natural language processing~\cite{popov2021grad}, and computational biology~\cite{xu2022geodiff,anand2022protein}. These models operate by systematically introducing noise into data through a forward diffusion process and then learning to reverse this process, effectively reconstructing data from noise. This approach enables them to capture the underlying structure of complex, high-dimensional data distributions. %
For a detailed review of diffusion models, we refer the readers to~\cite{yang2023diffusion,tang2024score,chen2024overview}.


A widely adopted formulation of diffusion models is the score-based generative model (SGM), implemented using stochastic differential equations (SDEs)~\cite{song2020score}. Broadly speaking, SGMs rely on two key stochastic processes: a forward process and a backward process. The forward process gradually transforms samples from the data distribution into pure noise, while the backward process reverses this transformation, converting noise back into the target data distribution, thereby enabling generative modeling. 

Despite the remarkable empirical success of diffusion models across various applications, their theoretical understanding remains limited. In recent years, there has been a rapidly expanding body of research on the convergence theory of diffusion models. Broadly, these contributions can be divided into two main approaches, each focusing on different metrics and divergences.
The first category investigates convergence bounds based on $\alpha$-divergence, including the Kullbackâ€“Leibler (KL) divergence and the total variation (TV) distance (see e.g.,~\cite{li2024provable,liang2025low,chen2024probability,chen2022sampling,wu2024stochastic,chen2023improved}). 
Among these works, several explore acceleration techniques that leverage higher-order information about the log density (see e.g.,~\cite{li2024accelerating,liang2024broadening,huang2024convergence}).
The second category focuses on convergence bounds in Wasserstein distance, which is often considered more practical and informative for estimation tasks.
One line of work within this category assumes strong log-concavity of the data distribution and access to accurate estimates of the score function~\cite{gao2023wasserstein,gao2023wasserstein,bruno2023diffusion,tang2024contractive,strasman2024analysis}. Another line of work focuses on specific structural assumptions of the data distribution.
For example,~\cite{de2022convergence} establishes Wasserstein-1 convergence with exponential rates under the manifold hypothesis, assuming the data distribution lies on a lower-dimensional manifold or represents an empirical distribution.~\cite{mimikos2024score} provides the Wasserstein-1 convergence analysis when the data distribution is defined on a torus.
Furthermore, recent work~\cite{gentiloni2025beyond} analyzes Wasserstein-2 convergence in SGMs while relaxing log-concavity and score regularity assumptions.

Much of the existing literature on the convergence theory of diffusion models relies on the Euler discretization method. 
Notably,~\cite{chen2023improved} compare the behavior of Euler discretization and exponential integrators~\cite{zhang2022fast,hochbruck2010exponential} in terms of KL divergence.
Additionally,~\cite{de2022convergence} provide a comparative analysis of these two schemes, though without formal theoretical guarantees.
A comprehensive and systematic understanding of how different discretization schemes influence convergence performance in diffusion models remains underexplored. 
Furthermore, while convergence analyses of accelerated diffusion models primarily focus on TV or KL distances, studies investigating Wasserstein convergence for these accelerations remain lacking.

In this work, we address these challenges by analyzing the Wasserstein convergence of score-based diffusion models when the data distribution has a smooth and strongly log-concave density.
Specifically, we investigate the impact of different discretization schemes on convergence behavior. 
Beyond the widely used Euler method and exponential integrator, we explore the midpoint randomization method.
This method was initially introduced in~\cite{shen2019randomized} for discretizing kinetic Langevin diffusion~\cite{cheng2018underdamped} and then has been extensively studied in log-concave sampling complexity theory~\cite{he2020ergodicity,yu2023langevin,yu2024parallelized,yu2024log,kandasamy2024poisson}.
It was later applied to diffusion models~\cite{gupta2024faster,li2024improved}, showing improved KL and TV convergence performance over vanilla models and offering easy parallelization.

We also consider scenarios where accurate estimates of the Hessian of the log density are accessible. Inspired by~\cite{Shoji1998}, we propose a novel sampler based on the local linearization method, which leverages second-order information about the log density. Our analysis shows that this approach significantly improves the upper bounds on the Wasserstein distance between the target data distribution and the generative distribution of the diffusion model.

Our contribution can be summarized as follows.
\begin{itemize}
\setlength\itemsep{0.02em}
    \item In Section~\ref{sec:discretization}, we establish convergence guarantees for SGMs in the Wasserstein-2 distance under various discretization methods, including the Euler method, exponential integrators, the midpoint randomization method, and a hybrid approach combining the latter two. 
    \item In Section~\ref{sec:accleration}, we introduce a novel Hessian-based accelerated sampler for the stochastic diffusion process, leveraging the local linearization method. 
    We then establish its Wasserstein convergence analysis in Theorem~\ref{thm:2order}, achieving state-of-the-art order of
$\widetilde{\mathcal{O}}\left(\frac{1}{\varepsilon}\right)$.
\end{itemize}
In summary, our analysis provides a quantitative comparison of different discrete approximations in terms of the Wasserstein-2 distance, offering practical guidance for choosing discretization points.
Moreover, we present the first Wasserstein convergence analysis of an accelerated sampler that leverages accurate score function estimation and second-order information about log-densities.
This accelerated sampler achieves a faster convergence rate~$\widetilde{\mathcal{O}}\left(1/{\varepsilon}\right)$ in Wasserstein-2 distance, compared to the standard rate~$\widetilde{\mathcal{O}}\left(1/{\varepsilon^2}\right)$ of vanilla diffusion models. 
These results contribute to the understanding of Wasserstein convergence in score-based models, shedding light on aspects that have not been extensively explored before.

\vspace{0.2cm}
\noindent \textbf{More Related Work.}
Score-based diffusion models can be formulated using either SDEs or their deterministic counterparts, known as probability flow ODEs~\cite{song2020score}. While SDE-based samplers generate samples through stochastic simulation, ODE-based samplers provide a deterministic alternative. Theoretical advancements in accelerating these samplers have emerged only recently.
A significant step toward designing provably accelerated, training-free methods were made by~\cite{li2024accelerating}, who propose and analyze acceleration for both ODE- and SDE-based samplers. Their accelerated SDE sampler leverages higher-order expansions of the conditional density to enhance efficiency. This was followed by the work of~\cite{li2024sharp}, which provided convergence guarantees for probability flow ODEs.
Furthermore,~\cite{huang2024convergence} studies the convergence properties of deterministic samplers based on probability flow ODEs, using the Runge-Kutta integrator;~\cite{wu2024stochastic} propose and analyze a training-free acceleration algorithm for SDE-based samplers, based on the stochastic Runge-Kutta method.
~\cite{liang2024broadening} proposes a novel accelerated SDE-based sampler when Hessian information is available.
Another line of work involves the midpoint randomized method. 
In particular, ~\cite{gupta2024faster} explore ODE acceleration by incorporating a randomized midpoint method, leveraging its advantages in parallel computation. 
A more recent work by~\cite{li2024improved} improved upon the ODE sampler proposed by~\cite{gupta2024faster}, achieving the state-of-the-art convergence rate.

We note that all of these works provide convergence analysis in terms of either KL divergence or TV distance.
Among these, \cite{liang2024broadening} accelerates the stochastic DDPM sampler by leveraging precise score and Hessian estimations of the log density, even for possibly non-smooth target distributions. This is achieved through a novel Bayesian approach based on tilting factor representation and Tweedieâ€™s formula.
\cite{huang2024convergence} accelerates the ODE sampler by utilizing 
$p$-th ($p\geqslant 1$) order information of the score function, with the target distribution supported on a compact set and employing early stopping. 
These two works are the most similar to our proposed accelerated sampler in that they all rely on the Hessian information of the log density.
However, their settings differ from ours, and their convergence analyses are neither directly applicable to our framework nor precisely expressed in terms of Wasserstein distance.\footnote{When the target distribution is compactly supported, Pinsker's inequality allows translating TV or KL divergence into Wasserstein distance. However, this often yields loose bounds, especially in high dimensions, where the actual Wasserstein distance may be much smaller.}.

\vspace{0.2cm}
\noindent \textbf{Outline.}
The rest of the paper is organized as follows.
In Section~\ref{sec:setting}, we introduce the framework of the score-based diffusion model and present the different discretization schemes.
In Section~\ref{sec:discretization}, we establish the convergence rate of the diffusion model under various discretization schemes. 
In Section~\ref{sec:accleration}, we propose a new sampler that leverages Hessian estimations and provide its convergence rate.
Section~\ref{sec:simulation} presents numerical studies that validate our theoretical results. 
Finally, in Section~\ref{sec:discussion}, we conclude with a discussion and outline future research directions. 
All proofs are provided in the Appendix.

\vspace{0.2cm}
\noindent \textbf{Notation.} Denote the $d$-dimensional Euclidean space by $\mathbb{R}^d$.
Denote the $d$-dimensional identity matrix by $I_d$. The gradient and the Hessian of a function $f:\mathbb{R}^d\to\mathbb{R}$ are denoted by $\nabla f$ and $\nabla^2f$. 
Given any pair of measures $\mu$ and $\nu$, the Wasserstein-2 distance between $\mu$ and $\nu$ is defined as
\begin{align*}
    \wass_2(\mu,\nu)=\left(\inf_{\varrho\in\Gamma(\mu,\nu)}\int_{\mathbb{R}^d\times\mathbb{R}^d}\l|x-y\r|^2\rmd \varrho(x,y)\right)^{1/2},
\end{align*}
where the infimum is taken over all joint distributions $\varrho$ that have $\mu$ and $\nu$ as marginals. For two symmetric $d\times d$ matrices $A$ and $B$, we use $A\preccurlyeq B$ or $B\succcurlyeq A$ to denote the relation that $B-A$ is positive semi-definite.
For any random object $X$, we use $\mathcal{L}(X)$ to denote its law.
Given a random vector $X\in\R^d$, define $\|X\|_{\Ltwo}=\sqrt{\E[\|X\|^2]},$ where $\|\cdot\|$ denotes the Euclidean norm.
For a matrix $A\in\R^{d\times d},$ we define the  $\l|A\r|_F$ as its Frobenius norm.


\section{Background and Our Setting}
\label{sec:setting}
\textbf{Framework.} \quad We consider the forward process
{
\begin{equation}
\label{eq:forward0}
dX_t= f(X_t,t)dt+g(X_t,t)dB_t\,,
\end{equation}}
where the initial point $X_0\sim  p_0$ follows the data distribution, and $B_t$ denotes the standard $d-$dimensional Brownian motion.
Here, the drift $f: \mathbb{R}^d\times \mathbb{R}_+\to \mathbb{R}^d$ and the function $g:\mathbb{R}^d\times \mathbb{R}_+\to \mathbb{R}^{d\times d}$ are diffusion parameters.
Some conditions are necessary to ensure that the SDE~\eqref{eq:forward0} is well-defined. 
In practice, various choices for the pair $(f, g)$ are employed, depending on the specific needs of the model; for a detailed survey, we refer to~\cite{tang2024score}.
For clarity, we adopt the simplest possible choice in this work by setting $f(X_t,t)=-X_t/2$ and $g(X_t,t)=1$.
This results in the Ornstein-Uhlenbeck process, which is described by the following SDE:
\begin{equation}
    \rmd X_t=-\frac{1}{2}X_t\rmd t+\rmd B_t,
    \label{eq:forward}
\end{equation}
The forward process~\eqref{eq:forward} is run until a sufficiently large time $T > 0$,  at which point the corrupted marginal distribution of $X_T$, denoted by $p_T$, is expected to approximate the standard Gaussian distribution.
Then, diffusion models generate new data by reversing the SDE~\eqref{eq:forward}, which leads to the following backward SDE
\begin{equation}
\rmd X_t^{\leftarrow} = \frac{1}{2}\left(X_t^{\leftarrow} + 2\nabla \log p_{T-t}(X_t^{\leftarrow})\right) \rmd t + \rmd W_t\,,
\label{eq:backward}
\end{equation}
where $X^\leftarrow_0\sim p_T$, and the term $\nabla \log p_t$, referred to as the \textit{score function} for $p_t$, is represented by the gradient of the log density function of $p_t$. 
Additionally, $W_t$ denotes another standard Brownian motion independent of $B_t$.
Under mild conditions, when initialized at
$X^\leftarrow_0\sim p_T$, the backward process $\{X^\leftarrow_t\}_{0\leqslant t\leqslant T}$ has the same distribution as the forward process $\{X_{T-t}\}_{0\leqslant t\leqslant T}$~\cite{anderson1982reverse,cattiaux2023time}. 
As a result, running the reverse diffusion $X^{\leftarrow}_t$ from $t=0$ to $T$ will generate a sample from the target data distribution $p_0$.
Note that the density $p_T$ is unknown, we approximate it using the distribution
\begin{align*}
\hat p_T= \mathcal{N}(\mathbf{0},(1-e^{-T})I_d)
\end{align*}
as proposed in~\cite{gao2023wasserstein}. Therefore, we derive a reverse diffusion process defined by
\begin{align}
    \label{eq:Yt}
    \rmd Y_t=\dfrac{1}{2}(Y_t+2\nabla\log p_{T-t}(Y_t))\rmd t+\rmd W_t,\quad Y_0\sim \hat{p}_T\,.
\end{align}

\vspace{0.2cm}
\noindent \textbf{Score Matching.} \quad Another  challenge in working with~\eqref{eq:backward} is that the score function $\nabla \log p_t$ is unknown, as the distribution $p_t$ is not explicitly available.
In practice, rather than using the exact score function $\nabla\log p_{T-t}$, approximate estimates for it are learned from the data by training neural networks on a score-matching objective~\cite{hyvarinen2005estimation,vincent2011connection,song2020sliced}. This objective is given by
\begin{align*}
\underset{\theta\in\Theta}{\text{minimize}}~~~ \E[\|s_\theta(t,X_t)-\nabla \log p_t(X_t)\|^2]\,,
\end{align*}
where $\{s_\theta:\theta\in\Theta\}$ is a sufficiently rich function class, such as that of neural network.
Substituting the learned score estimate $s_*$ into the backward process~\eqref{eq:backward}, we obtain the following practical continuous-time backward SDE,
\begin{equation}
dX^{\leftarrow}_t= \frac{1}{2}\big(X^{\leftarrow}_t+ 2s_{*}(T-t,X^{\leftarrow}_t)\big)\rmd t + \rmd W_t\,.
\label{eq:backward1}
\end{equation}
Since this continuous backward SDE cannot be simulated exactly, it is typically approximated using discretization methods.

\vspace{0.2cm}
\noindent \textbf{Discretization Schemes.}\quad In the following, we outline the four discretization methods considered in this work for solving the practical reverse SDE~\eqref{eq:backward1}. 
Let $h>0$ be the step size.
Without loss of generality, we assume $T = Nh,$
where $N$ is a positive integer.
For simplicity, we denote $\frac{1}{2}X^{\leftarrow}_t+ s_{*}(T-t,X^{\leftarrow}_t)$ by $\gamma(T-t,X^{\leftarrow}_t)$, and define 
\begin{align*}
\Delta_hW_t:=W_{t+h}-W_t,\quad %
\bar{\Delta}_hW_t:=\int_t^{t+h}e^{\frac{t+h-s}{2}}\rmd W_s\,.
\end{align*}
\noindent $\bullet$~~\textsc{Euler-Maruyama scheme}:\quad
Given the step size $h$, the following approximation holds
\begin{align*}
X^{\leftarrow}_{t+h}
&=X^{\leftarrow}_t+\int_0^h \gamma(T-(t+v),X^{\leftarrow}_{t+v})\rmd v+\Delta_hW_t \nonumber \\
&\approx X_t^{\leftarrow}+ h\gamma(T-t,X^{\leftarrow}_t)+\Delta_hW_t\,.
\end{align*}
We derive the following discretized process for $n=0,\dots,N-1$:
\begin{align*}
\vartheta^{\sf EM}_{n+1}=(1+h/2)\vartheta^{\sf EM}_{n}+hs_{*}(T-nh,\vartheta^{\sf EM}_n)+\sqrt{h}\xi_n\,,
\end{align*}
where $\vartheta^{\sf EM}_{0}\sim \hat p_T$ and $\xi_n\sim \mathcal{N}(0,I_{d}).$

\noindent $\bullet$~~\textsc{Exponential Integrator}:\quad
Inspired by the work~\cite{hochbruck2010exponential},~\cite{zhang2022fast} propose a more refined discretization method which solves the backward SDE~\eqref{eq:backward} explicitly, yielding the following approximation 
\begin{align*}
X^{\leftarrow}_{t+h}&=e^\frac{h}{2}X^{\leftarrow}_t+\int_0^h e^{\frac{h-v}{2}}   s_*(T-t-v,X^{\leftarrow}_{t-v})\rmd v+\bar{\Delta}_hW_t\\
&\approx e^{\frac{h}{2}}X^{\leftarrow}_t+ 2(e^{\frac{h}{2}}-1) s_*(T-t,X^{\leftarrow}_t)
+\bar{\Delta}_hW_t\,.
\end{align*}
We derive the following discretized process for $n=0,\dots,N-1$:
\begin{align*}
\vartheta^{\sf EI}_{n+1}&=e^{\frac{h}{2}}\vartheta^{\sf EI}_{n}
+2(e^{\frac{h}{2}}-1)s_*(T-nh,\vartheta^{\sf EI}_n)+\sqrt{e^h-1}\xi_n
\end{align*}
with the initial point $\vartheta^{\sf EI}_{0}\sim \hat p_T$ and $\xi_n\sim \mathcal{N}(0,I_{d}).$

\noindent $\bullet$~~\textsc{Vanilla Midpoint Randomization}:\quad
Unlike the Euler method, the midpoint randomization method evaluates the function $\gamma(T-t,X_t^{\leftarrow})$ at a random point within the time interval $[0,h]$ rather than at the start.
Let $U$ be a random variable uniformly distributed in $[0, 1]$ and independent of the Brownian motion $W_t$. The randomized midpoint method exploits the approximation
\begin{align}
X^{\leftarrow}_{t+h}
&=X^{\leftarrow}_t+\int_0^h \gamma(T-t-v, X^{\leftarrow}_{t+v})\rmd v+\Delta_hW_t\nonumber\\
&\approx X_t^{\leftarrow}+ h \gamma(T-t-hU,X^{\leftarrow}_{t+hU})
+ \Delta_hW_t\,.
\label{eq:mr}
\end{align}
The idea behind the randomized midpoint method is to introduce an $U$ in $[0,1]$,
making $h\gamma(T-t-hU,X^{\leftarrow}_{t+hU})$ an estimator for integral $\int_0^h\gamma(T-t-v,X^{\leftarrow}_{t+v})\rmd v$. 

Furthermore, the intermediate term $X^{\leftarrow}_{t+hU}$ is generated by employing the Euler method.
We then derive the following discretized process for $n=0,\dots,N-1$:

\vspace{0.2cm}
\textbf{Step 1}\label{noise}~~Generate $\xi'_n,\xi''_n\sim \mathcal{N}(\mathbf{0},I_d)$ and $U_n\sim\text{Unif}\,[0,1]$. 
Set $\xi_n=\sqrt{U_n}\xi'_n+\sqrt{1-U_n}\xi''_n$.

\vspace{0.2cm}
\textbf{Step 2}~~With the initialization $\vartheta^{\sf REM}_{0}\sim\hat p_T$, define
\begin{align*}
\vartheta^{\sf REM}_{n+U}&=
\vartheta^{\sf REM}_{n}
+hU_n\gamma(T-nh,\vartheta^{\sf REM}_{n})
 +{\sqrt{hU_n}\xi'_n}\\
\vartheta^{\sf REM}_{n+1}&=\vartheta^{\sf REM}_{n}
+h\gamma(T-(n+U_n)h, \vartheta^{\sf REM}_{n+U})
 +{\sqrt{h}\xi_n}\,.
\end{align*}
\noindent $\bullet$~~\textsc{Exponential Integrator with Randomized Midpoint Method:} \quad Combining midpoint randomization with the exponential integrator approach, we propose the following new discretization process
for $n=1,\dots,N-1$:

\vspace{0.2cm}
\textbf{Step 1}~~Generate $\xi'_n,\xi''_n\sim \mathcal{N}(\mathbf{0},I_d)$ and $U_n\sim\text{Unif}\,[0,1]$. 
Set 
\begin{align*}
 \rho_n= e^{\frac{h(1+U_n)}{2}}\big(1-e^{-hU_n}\big)\Big[{(e^{hU_n}-1)(e^h-1)}  \Big]^{-1/2}
\end{align*} 
and $\xi_n=\rho_n\xi'_n+\sqrt{1-\rho_n^2}\xi''_n$.

\vspace{0.2cm}
\textbf{Step 2}~~With the initialization $\vartheta^{\sf REI}_{0}\sim\hat p_T$, define
\begin{align*}
\vartheta^{\sf REI}_{n+U}&=e^{hU_n/2}\vartheta^{\sf REI}_{n}
+2(e^{hU_n/2}-1)s_*(T-nh,\vartheta^{\sf REI}_{n}) +\sqrt{e^{hU_n}-1}\xi'_n\\
\vartheta^{\sf REI}_{n+1}&=e^{h/2}\vartheta^{\sf REI}_{n}
+he^{(1-U_n)h/2}s_*(T-(n+U_n)h,\vartheta^{\sf REI}_{n+U}) +\sqrt{e^h-1}\xi_n\,.
\end{align*}
The resulting discrete process is then solved to generate new samples that approximately follow the data distribution $p_0$.

To simplify notation and improve clarity,
we denote $\vartheta_n$ as the sample points generated by discretization methods after $n$ iterations throughout the remainder of the paper.
For each discretization scheme, we construct a sample path $\{\vartheta_n\}_{n=0}^N$ through the iterative procedure
\begin{align*}
\vartheta_{n+1}=\mathcal{G}\big(h,\vartheta_n,\{W_t\}_{nh\leqslant t\leqslant (n+1)h}\big)
\end{align*}
with the initial point $\vartheta_0\sim\hat{p}_T$.
Here, $\mathcal{G}:\R_+\times\R^d\times C([0,T],\R^d)\to\mathbb{R}^d$ denotes the discrete transition operator parameterized by the step size $h$, mapping the current state and the Brownian path segment to the next state.
To distinguish the implementations of $\mathcal{G}$ across different discretization methods, we augment the notation with a superscript $\alpha\in\{\sf EM,\sf EI,\sf REM,\sf REI\}$, then the iterative law becomes
\begin{align*}
    \vartheta_{n+1}^{\alpha}=\mathcal{G}^{\alpha}(h,\vartheta_n^{\alpha},\{W_t\}_{nh\leqslant t\leqslant (n+1)h})\,.
\end{align*}

\section{Wasserstein Convergence Analysis under Various Discretization Schemes}
\label{sec:discretization}
In this section, we study the convergence of the diffusion model under various discretization schemes applied to the continuous backward SDE~\eqref{eq:backward}. These methods include the Euler method (EM), the exponential integrator (EI), the vanilla randomized midpoint method (REM), and the novel approach combining midpoint randomization with the exponential integrator (REI) described in the previous section.
Specifically, we establish the upper bounds on the Wasserstein-2 distance between the distribution of the $N$-th output of the SGMs under these discretization schemes and the target distribution
\begin{align*}
\wass_2(\mathcal{L}(\vartheta_N^{\alpha}),p_0), \quad \alpha\in\{\sf{EM,EI,REM,REI}\}\,.    
\end{align*}
Additionally, we analyze the number of iterations $N$ required for the Wasserstein distance to achieve a pre-specified error level $\varepsilon$ under different discretization schemes.
For clarity of presentation, we omit constants throughout the paper, retaining only the key components that influence the convergence rate in theory. However, our proofs include constants in the bounds.


To establish the convergence analysis, we require the following assumptions.
We first state our assumptions on the data distribution $p_0$.
\begin{assumption}
    \label{asm:p0scLipx}
    The target density $p_0$ is $m_0$-strongly log-concave, and the score function $\nabla\log p_0$ is $L_0$-Lipschitz. 
\end{assumption}
Under Assumption~\ref{asm:p0scLipx}, $p_t$ is $m(t)$-strongly log-concave, $\nabla\log p_t$ is $L(t)$-Lipschitz.
Moreover, $m(t)$ is lower bounded by $m_{\min}=\min(1,m_0)$, and $L(t)$ is upper bounded by $L_{\max}=1+L_0$.
These results are summarized in  Lemma~\ref{lem:GaoLt} and~\ref{lem:Gaomt} in the Appendix.

We also assume that the score function $\nabla\log p_t(x)$ exhibits linear growth in $\|x\|$, as stated below.
\begin{assumption}
    \label{asm:scLipt}
    There exists a constant $M_1>0$ such that for $n=0,1,\dots,N-1,$ 
    \begin{align*}
    \sup\limits_{nh\leqslant t,s\leqslant (n+1)h}
        \l|\nabla\log p_{T-t}(x)-\nabla\log p_{T-s}(x)\r|\leqslant M_1h(1+\l|x\r|)\,.
    \end{align*}
\end{assumption}
The above condition is a relaxation of the standard Lipschitz condition on the score function.

Recall that $\vartheta_n$ represents the sample generated by discretization for solving the practical backward SDE~\eqref{eq:backward1} after $n$ iterations. 
We require the following assumption on the score-matching approximation at each point $\vartheta_n$.
\begin{assumption}
    \label{asm:scoreerr}
Given a small $\varepsilon_{sc}>0,$ the score estimator satisfies
    \begin{align*}
        \sup\limits_{0\leqslant n\leqslant N}\l|\nabla\log p_{T-nh}(\vartheta_n)-s_*(T-nh,\vartheta_n)\r|_{\Ltwo}\leqslant \varepsilon_{sc}.
    \end{align*}
\end{assumption}
Assumption~\ref{asm:p0scLipx},~\ref{asm:scLipt} and~\ref{asm:scoreerr} are standard in the Wasserstein convergence analysis of the score-based diffusion model. These assumptions were previously adopted in \cite{Gao2023WassersteinCG,gao2024convergence} and can be easily verified in the Gaussian case.


\subsection{Euler-Maruyama Method}
Recall that the EM discretization for the backward process~\eqref{eq:backward1} is defined as follows
\begin{align*}
 \vartheta^{\sf EM}_{n+1}=(1+h/2)\vartheta^{\sf EM}_{n}+hs_{*}(T-nh,\vartheta^{\sf EM}_n)+\sqrt{h}\xi_n\,,
\end{align*}
where $\xi_n$ are i.i.d. standard Gaussian vectors and $n=0,1,\dots,N-1$.
In the following theorem, we quantify the Wasserstein distance between the distribution of $ \vartheta_{N}^{\sf EM}$ and the target distribution~$p_0.$
\begin{theorem}
    \label{thm:EM}
    Suppose that Assumptions~\ref{asm:p0scLipx},~\ref{asm:scoreerr} and~\ref{asm:scLipt} hold, it holds that
    \begin{align}
    \label{eq:Wassthm1}
    \wass_2(\law(\vartheta_N^{\sf EM}),p_0)
        \lesssim e^{-m_{\min}T}\l|X_0\r|_{\Ltwo}+\mathscr C_1^{\sf EM}\sqrt{dh}+\mathscr C_2^{\sf EM}\varepsilon_{sc}\,,
    \end{align}
    where 
    \begin{align*}
    \mathscr C_1^{\sf EM}=\dfrac{L_{\max}+1/2}{m_{\min}-1/2}~~\text{ and }~~
    \mathscr C_2^{\sf EM}=\dfrac{1}{m_{\min}-1/2}
    \end{align*}
    with $m_{\min}=\min(1,m_0)$ and  $L_{\max}=1+L_0$.
\end{theorem}
Prior to discussing the relation of the above bound to those available in the literature, let us state a consequence of it.
\begin{corollary}
\label{cor:EM}
For a given $\varepsilon>0$, the Wasserstein distance satisfies $\wass_2(\law(\vartheta_N^{\sf EM}),p_0)<\varepsilon$ after 
    \begin{align*}
    N=\mathcal{O}\bigg(\frac{1}{\varepsilon^2}\log\Big(\frac{1}{\varepsilon}\Big)\bigg)
    \end{align*}
    iterations, provided that
$T=\mathcal{O}\left(\log(\frac{1}{\varepsilon})\right)$ and $h=\mathcal{O}(\varepsilon^2)$.
\end{corollary}
We now provide an explanation of the upper bound obtained in Theorem~\ref{thm:EM}. The total error arises from three sources: initialization error, discretization error, and score-matching error.

\begin{itemize}
    \item The \textbf{first term} in the upper bound of \eqref{eq:Wassthm1} characterizes the behavior of the continuous-time reverse process $Y_t$ (defined in \eqref{eq:Yt}) as it converges to the distribution $p_0$, assuming no discretization or score-matching errors. Specifically, it bounds the error $\wass_2(\law(Y_T), p_0)$, which results from initializing the reverse SDE at $\hat{p}_T$ instead of $p_T$. 
    \item The \textbf{second term} in the upper bound of \eqref{eq:Wassthm1} accounts for discretization errors introduced when approximating the continuous process using the Euler algorithm.  
    \item The \textbf{third term} quantifies the error introduced by score matching.  
\end{itemize}
The convergence rates obtained in Theorem~\ref{thm:EM} and Corollary~\ref{cor:EM} align with Theorem 5 and Proposition 7 in~\cite{gao2023wasserstein}, demonstrating consistency between our results and their theoretical conclusions.


\subsection{Exponential Integrator}
As described in the previous section, the exponential integrator scheme leverages the semi-linear structure by discretizing only the nonlinear term while preserving the continuous dynamics of the linear term. 
The resulting discretized process for $n=0,\dots,N-1$ is:
\begin{align*}
\vartheta^{\sf EI}_{n+1}&=e^{\frac{h}{2}}\vartheta^{\sf EI}_{n}
+2(e^{\frac{h}{2}}-1)s_*(T-nh,\vartheta^{\sf EI}_n)+\sqrt{e^h-1}\xi_n\,.
\end{align*}
In the following theorem, we provide the Wasserstein distance between the distribution of the $N$-th iterate~$\vartheta_N^{\sf EI}$ and the target distribution~$p_0$.
\begin{theorem}
    \label{thm:EI}
    Suppose that Assumptions~\ref{asm:p0scLipx},~\ref{asm:scLipt} and~\ref{asm:scoreerr} hold, then
    \begin{align*}
        \wass_2(\law(\vartheta_N^{\sf EI}),p_0)\lesssim e^{-m_{\min}T}\l|X_0\r|_{\Ltwo}+\mathscr C_1^{\sf EI}\sqrt{dh}+\mathscr C_2^{\sf EI}\varepsilon_{sc}\,,
    \end{align*}
    where 
    \begin{align*}
     \mathscr C_1^{\sf EI}=\dfrac{L_{\max}}{m_{\min}-1/2}~~\text{ and }~~
     \mathscr C_2^{\sf EI}=\dfrac{1}{m_{\min}-1/2}
    \end{align*}
    with $L_{\max}$ and $m_{\min}$ as defined in Theorem~\ref{thm:EM}.
\end{theorem}
Comparing the error bound obtained in this theorem with that in Theorem~\ref{thm:EM}, we find that the convergence rate is comparable to that of the Euler discretization.
This observation is consistent with the error bounds for {\sf EI} and 
{\sf EM} schemes in KL divergence established in Theorem 1 of~\cite{chen2023improved}, where $N=\Omega(T^2)$ in their setting.


\subsection{Vanilla Randomized Midpoint Method}
\label{sec:REM}
The randomized midpoint method for the practical backward SDE is defined as
\begin{align*}
\vartheta^{\sf REM}_{n+U}&=(1+hU_n/{2})\vartheta^{\sf REM}_{n}
+hU_ns_*(T-nh,\vartheta^{\sf REM}_{n}) +\sqrt{2hU_n}\xi'_n\\
\vartheta^{\sf REM}_{n+1}&=\vartheta^{\sf REM}_{n}
+h\vartheta^{\sf REM}_{n+U}/2+hs_*(T-(n+U_n)h,\vartheta^{\sf REM}_{n+U})+\sqrt{2h}\xi_n\,,
\end{align*}
where 
$\xi'_n$ and $\xi_n$ are as defined in
Step 1 of the vanilla randomized midpoint scheme in Section~\ref{sec:discretization}.

Since the scheme involves an i.i.d. uniformly distributed random variable $U_n$, the resulting score matching function $s_*(T-t,x)$ can be evaluated at any $t\in[0,T]$. 
To proceed, we require a regularity condition on the deviation of the estimated score from the true score at these points. 
For this, we introduce the following stochastic process.

Given $U_n=u,$ we define the conditional realization of the random vector $\vartheta^{\sf REM}_{n+U}$ via
\begin{align*}
\vartheta^{\sf REM}_{n+u}:= \Big(1+\frac{uh}{2}\Big)\vartheta_n^{\sf REM}+uhs_*(T-nh,\vartheta_n^{\sf REM})+\sqrt{uh}\xi_n\,.
\end{align*}
We impose the following assumption on score estimates, which extends Assumption~\ref{asm:scoreerr}.
\begin{assumption}
    \label{asm:score4RMP}
    There exists a constant $\varepsilon_{sc}>0$ such that for any $u\in[0,1]$ and $n=0,\dots,N$,
    \begin{align*}
        \l|\nabla\log p_{T-(n+u)h}(\vartheta_{n+u}^{\sf REM})-s_*(T-(n+u)h,\vartheta_{n+u}^{\sf REM})\r|_{\Ltwo} 
        \leqslant \varepsilon_{sc}.
    \end{align*}
\end{assumption}
\vspace{-0.2cm}
In the following theorem, we provide the upper bound for the Wasserstein distance between the law of $\vartheta_N^{\sf REM}$ and the data distribution $p_0$.
\begin{theorem}
    \label{thm:RMPEM}
    Suppose that Assumptions~\ref{asm:p0scLipx},~\ref{asm:scLipt} and~\ref{asm:score4RMP} hold, then 
    \begin{align*}
        \wass_2(\mathcal{L}(\vartheta_N^{\sf REM}),p_0)\lesssim e^{-m_{\min}T}\l|X_0\r|_{\Ltwo}+\mathscr C_1^{\sf REM}(d)\sqrt{h}+\mathscr C_2^{\sf REM}\varepsilon_{sc}\,,
    \end{align*}
    where 
    \begin{align*}
       \mathscr C_1^{\sf REM}(d)=\dfrac{\sqrt{d/3}L_{\max}+\frac{1}{2\sqrt{3}}}{m_{\min}-1/2}~~\text{ and }~~ 
       \mathscr C_2^{\sf REM}=\dfrac{3}{m_{\min}-1/2}
    \end{align*}
    with $L_{\max}$ and $m_{\min}$ as defined in Theorem~\ref{thm:EM}.
\end{theorem}
As shown in display~\eqref{eq:mr}, the key idea behind the randomized midpoint method is to introduce a uniformly distributed random variable~$U_n$ in $[0,1]$ to evaluate the function $\gamma$ at a random point within the time interval $[0,h]$.

In the proof in Appendix~\ref{app:REM}, we demonstrate that $\E_{U_n}[\vartheta_{n+1}^{\sf REM}]$ provides a good approximation to the true distribution. 
However, this also introduces an additional error term~$\|\vartheta^{\sf REM}_{n+1}-\E_{U_n}[\vartheta^{\sf REM}_{n+1}]\|_{\Ltwo},$ which obscures the advantage of the improved estimation. As a result, the randomized midpoint method does not achieve a better convergence order compared to {\sf EI} and {\sf EM} methods.

Nonetheless, it is shown that this method has the advantage of enabling parallel computation~\cite{gupta2024faster, li2024improved}, which can significantly reduce computational complexity and improve efficiency. 
Therefore, while midpoint randomization does not yield a better convergence order in our setting, it offers notable benefits in terms of efficiency and scalability in parallel computing.

\subsection{Exponential Integrator with Randomized Midpoint Method}
\label{sec:REI}
With the initialization $\vartheta^{\sf REI}_{0}\sim\hat p_T$, the exponential integrator with randomized midpoint method is defined as
\begin{align*}
\vartheta^{\sf REI}_{n+U}&=e^{hU_n/2}\vartheta^{\sf REI}_{n}
+2(e^{hU_n/2}-1)s_*(T-nh,\vartheta^{\sf REI}_{n}) +\sqrt{e^{hU_n}-1}\xi'_n\\
\vartheta^{\sf REI}_{n+1}&=e^{h/2}\vartheta^{\sf REI}_{n}
+he^{(1-U_n)h/2}s_*(T-(n+U_n)h,\vartheta^{\sf REI}_{n+U}) +\sqrt{e^h-1}\xi_n\,,
\end{align*}
where $\xi'_n$ and $\xi_n$ is defined in Step 1 of this discretization scheme in Section~\ref{sec:discretization}.

Similar to the vanilla randomized midpoint scheme, since the process involves the i.i.d random variables $U_n$, we need to take into account the accuracy of the score estimates $s_*(T-t,x)$ at any $t\in[0,T].$
For this, we define the conditional realization of the
random vector $\vartheta_{n+U}^{\sf REI}$ given $U_n=u$ as following
\begin{align*}
\vartheta_{n+u}^{\sf REI}&:=
e^{uh/2}\vartheta^{\sf REI}_{n}
+2(e^{uh/2}-1)s_*(T-nh,\vartheta^{\sf REI}_{n})  +\sqrt{e^{uh}-1}\xi'_n\,.
\end{align*}
We impose the following condition on the score function estimates.
\begin{assumption}
    \label{asm:score4RMP1}
    There exists a constant $\varepsilon_{sc}>0$ such that for any $u\in[0,1]$ and $n=0,\cdots,N$,
    \begin{align*}
        \l|\nabla\log p_{T-(n+u)h}(\vartheta_{n+u}^{\sf REI})-s_*(T-(n+u)h,\vartheta_{n+u}^{\sf REI})\r|_{\Ltwo}\leqslant \varepsilon_{sc}.
    \end{align*}
\end{assumption}
\begin{theorem}
    \label{thm:RMPEI}
    Suppose that Assumption~\ref{asm:p0scLipx},~\ref{asm:scLipt} and~\ref{asm:score4RMP1} hold, then
    \begin{align*}
        \wass_2(\law(\vartheta_N^{\sf REI}),p_0)\lesssim e^{-m_{\min}T}\l|X_0\r|_{\Ltwo}+\mathscr C_1^{\sf REI}\sqrt{dh}+\mathscr C_2^{\sf REI}\varepsilon_{sc}\,,
    \end{align*}
    where 
    \begin{align*}
        \mathscr C_1^{\sf REI}=\dfrac{L_{\max}}{\sqrt{3}(m_{\min}-1/2)}~~\text{ and }~~
        \mathscr C_2^{\sf REI}=\dfrac{3}{m_{\min}-1/2}
    \end{align*}
    with $L_{\max}$ and $m_{\min}$ as defined in Theorem~\ref{thm:EM}.
\end{theorem}
The convergence rate of this scheme is generally consistent with the previous three methods, differing only in the coefficients.

\section{Second-order Acceleration}
\label{sec:accleration}
In this section, we propose an accelerated sampler that leverages Hessian estimation, %
The core idea behind the acceleration is the \textit{Local Linearization Method}, introduced in~\cite{Shoji1998}, which is to approximate the drift term of an SDE by its ItÃ´ expansion over small time intervals. 
We begin by considering the following general framework for the backward process.
\begin{align}
\label{eq:sec_sde}
dx_t=\gamma(T-t,x_t)\rmd t+\sigma \rmd W_t\,,
\end{align}
where $\sigma>0$ and $W_t$ is the $d$-dimensional Brownian motion.
We approximate the drift function $\gamma:\R_+\times\R^d\to \R^d$ by a linear function in both state and time within each discretization step. 
Applying ItÃ´'s formula to $\gamma(T-t,x)$, we obtain
\begin{align*}
\gamma(T-t,x_t)-\gamma(T-s,x_s)
&\approx\left(\frac{\sigma^2}{2}\frac{\partial^2 \gamma}{\partial x^2}(T-s,x_s)-\frac{\partial \gamma}{\partial t}(T-s,x_s)\right)(t-s) +\dfrac{\partial \gamma}{\partial x}(T-s,x_s)\cdot(x_t-x_s)\,.
\end{align*}
Here and henceforth, we abbreviate the partial derivative $\dfrac{\partial^{\alpha}g(z)}{\partial z^{\alpha}}\bigg|_{z=z_0}$ as $\dfrac{\partial^{\alpha}g}{\partial z^{\alpha}}(z_0)$. This allows us to express $\gamma(T-t,x_t)$ in the following form
\begin{align*}
\gamma(T-t,x_t)\approx \gamma(T-s,x_s)+L_s(x_t-x_s)+M_s(t-s).
\end{align*}
where $L_s, M_s$ are given by
\begin{align*}
L_s&=\dfrac{\partial \gamma}{\partial x}(T-s,x_s)\\
M_s&=\dfrac{\sigma^2}{2}\dfrac{\partial^2 \gamma}{\partial x^2}(T-s,x_s)-\dfrac{\partial \gamma}{\partial t}(T-s,x_s)\,.
\end{align*}
{Thus, $L_s$ is the first-order spatial derivative of $\gamma$, capturing its local variation with respect to changes in position $x$. 
On the other hand, $M_s$ represents the temporal evolution of $\gamma$, incorporating information about how its shape changes over both space and time.
}

Substituting this into the original SDE~\eqref{eq:sec_sde}, we obtain
\begin{align*}
dx_t=[\gamma(T-s,x_s)+L_s(x_t-x_s)+M_s(t-s)]\rmd t+\sigma \rmd W_t.
\end{align*}
This formulation ensures that the discretized process retains the essential structure of the original dynamics while being computationally tractable. 
Unlike discretization schemes we used in Section~\ref{sec:discretization}, which rely on direct numerical integration, this transformed SDE can be solved analytically within each small time interval. 

By using \text{It\^o}'s formula to $e^{-L_st}x_t$, we obtain
\begin{align*}
    \rmd(e^{-L_st}x_t)= e^{-L_st}(\gamma(T-s,x_s)-L_sx_s+M_s(t-s))\rmd t+e^{-L_st}\sigma\rmd W_t.
\end{align*}
Integrating from $s$ to $s+\Delta t$ then gives
\begin{align*}
    x_{s+\Delta t}
    &=e^{L_s\Delta t}x_s+\int_s^{s+\Delta t}e^{L_s(s+\Delta t-t)}\rmd t(\gamma(T-s,x_s)-L_sx_s)\\
    &\quad+\int_s^{s+\Delta t}e^{L_s(s+\Delta t-t)}(t-s)\rmd t\,M_s\\
    &\quad+\sigma\int_s^{s+\Delta t}e^{L_s(s+\Delta t-t)}\rmd W_t\\
    &=x_s+L_s^{-1}(e^{L_s\Delta t}-1)\gamma(T-s,x_s)\\
    &\quad+L_s^{-2}\big[(e^{L_s\Delta t}-1)-L_s\Delta t\big]M_s\\
    &\quad+\sigma\int_s^{s+\Delta t}e^{L_s(s+\Delta t-u)}dW_t\,.
\end{align*}
Setting $\gamma(T-t,x)=\dfrac{1}{2}x+\nabla\log p_{T-t}(x),\sigma=1$, 
let $\Delta t\in[0,h]$ and $s=nh$.
In the resulting expression, we denote $x_{s}$ by $\vartheta_n^{\sf SO}$.
Then, we obtain that for any $t\in[nh,(n+1)h]$
\begin{align}
\label{eq:SOxt}
    x_t&=\vartheta_n^{\sf SO}+\int_{nh}^t(\dfrac{1}{2}\vartheta_n^{\sf SO}+\nabla \log p_{T-nh}(\vartheta_n^{\sf SO})\\
    &\quad +L_n(x_u-\vartheta_n^{\sf SO})+M_n(u-nh))\rmd u+\int_{nh}^{t}\rmd W_u\notag
\end{align}
where
\begin{align*}
    L_n&=\dfrac{1}{2}I_d+\nabla^2\log p_{T-nh}(\vartheta_n^{\sf SO})\\
    M_n&=\dfrac{1}{2}\sum_{j=1}^d\dfrac{\partial^2}{\partial x_j^2}\nabla\log p_{T-nh}(\vartheta_n^{\sf SO})-\dfrac{\partial}{\partial t}\nabla\log p_{T-nh}(\vartheta_n^{\sf SO})\,.
\end{align*}
{Thus, $L_n$ contains the Hessian of the score function.
Meanwhile, $M_n$ measures the difference between the spatial and temporal changes in the score function, reflecting the balance between curvature effects and temporal adaptation in the diffusion process.}
Notice that $x_{(n+1)h}$ is the point we aim to approximate. For this, we need to estimate the score function and its higher-order derivatives to obtain accurate estimates of $L_n$ and $M_ n$, denoted by $s_*^{(L)}$ and $s_*^{(M)}$, respectively.

By the work of~\cite{meng2021highorder}, higher-order derivatives with respect to the spatial variable $x$ can be estimated with sufficient accuracy.
Additionally, we show in the proof of Proposition~\ref{prop:2order} that the partial derivative of $\nabla\log p_t(x)$ with respect to $t$ can be estimated without requiring additional assumptions. 
Thus, it suffices to impose the following assumption.
\begin{assumption}
\label{asm:scerr4SO}
    For some constants $\varepsilon_{sc}^{(L)},\varepsilon_{sc}^{(M)}>0$, the estimate for high-order derivatives of the score function satisfies that,
    \begin{align*}
        \sup_{0\leqslant n\leqslant N}\l|s_*^{(L)}(T-nh,\vartheta_n^{\sf SO})-L_n\r|_{\Ltwo}\leqslant \varepsilon_{sc}^{(L)}\\
        \sup_{0\leqslant n\leqslant N}\l|s_*^{(M)}(T-nh,\vartheta_n^{\sf SO})-M_n\r|_{\Ltwo}\leqslant \varepsilon_{sc}^{(M)}.
    \end{align*}
\end{assumption}
The conditions in Assumption~\ref{asm:scerr4SO} have been previously adopted in works such as Assumption 3 in~\cite{liang2024broadening} and Assumption 3 in~\cite{li2024accelerating}.

Substituting these estimates into the SDE~\eqref{eq:SOxt} then gives
\begin{align*}
    x_t&=\vartheta_n^{\sf SO}+\int_{nh}^t\Big(\gamma(T-nh,\vartheta_n^{\sf SO})+s_*^{(L)}(T-nh,\vartheta_n^{\sf SO})(x_u-\vartheta_n^{\sf SO})\\
    &\qquad +s_*^{(M)}(T-nh,\vartheta_n^{\sf SO})(u-nh)\Big)\rmd u+\int_{nh}^t\rmd W_u\,.
\end{align*}
Let $\vartheta_{n+1}^{\sf SO}$ denote $x_{(n+1)h}$. 
The second-order discretization scheme is given by
\begin{align*}
    \vartheta_{n+1}^{\sf SO}
    &=\vartheta_n^{\sf SO}+s_*^{(L)}(T-nh,\vartheta_n^{\sf SO})^{-1}\left(e^{s_*^{(L)}(T-nh,\vartheta_n^{\sf SO})h}-I_d\right)\left(\dfrac{1}{2}\vartheta_n^{\sf SO}+s_*(T-nh,\vartheta_n^{\sf SO})\right)\\
    &\quad +s_*^{(L)}(T-nh,\vartheta_n^{\sf SO})^{-2}\left(e^{s_*^{(L)}(T-nh,\vartheta_n^{\sf SO})h}-s_*^{(L)}(T-nh,\vartheta_n^{\sf SO})h-I_d\right) s_*^{(M)}(T-nh,\vartheta_n^{\sf SO})\\
    &\quad +\int_{nh}^{(n+1)h}e^{s_*^{(L)}(T-nh,\vartheta_n^{\sf SO})[(n+1)h-t]}\rmd W_t\,.
\end{align*}
We require additional assumptions on the smoothness of the score function, similar to those imposed on $\nabla\log p_t(x)$ in Section~\ref{sec:discretization}.
\begin{assumption}
    \label{asm:scLipx4SO}
   There exists a constant $L_F>0$ such that 
    \begin{align*}
        \left\lVert\nabla^2\log p_t(x)-\nabla^2\log p_t(y)\right\rVert_F\leqslant L_F\l|x-y\r|\,.
    \end{align*}
\end{assumption}
This implies $\nabla^2\log p_t(x)$ is $L_F$-Lipschitz in $x$ with respect to the Frobenius norm. 
As shown in Theorems 4 and 5 of \cite{liang2024broadening}, this condition plays a crucial role in bounding the Wasserstein distance for Hessian estimates and can be easily verified in the Gaussian case.

We also require the following assumption.
\begin{assumption}
    \label{asm:scLipt4SO}
    There exists a constant $M_2>0$ such that, for any $n=0,\dots,N-1$ and $t\in[nh,(n+1)h]$, it holds that
    \begin{align*}
        \l|\nabla^2\log p_{T-t}(x)-\nabla^2\log p_{T-nh}(x)\r|\leqslant M_2h(1+\l|x\r|)\,.
    \end{align*}
\end{assumption}
We are now ready to quantify the $\wass_2$ distance between the generated distribution $\law(\vartheta_N^{\sf SO})$ and the target distribution $p_0$.
\begin{theorem}
    \label{thm:2order}
    Suppose that Assumptions~\ref{asm:p0scLipx},~\ref{asm:scoreerr},~\ref{asm:scerr4SO},~\ref{asm:scLipx4SO} and~\ref{asm:scLipt4SO} hold, then
    \begin{align*}
        \wass_2(\law(\vartheta_N^{\sf SO}),p_0)
        \leqslant& e^{-m_{\min}T}\l|X_0\r|_{\Ltwo}+\mathscr C_1^{\sf SO}(d)h+\mathscr C_2^{\sf SO}\left(\varepsilon_{sc}+\dfrac{2}{3}h^{1/2}\varepsilon_{sc}^{(L)}+\dfrac{1}{2}h\varepsilon_{sc}^{(M)}\right)
    \end{align*}
    where 
    \begin{align*}
    \mathscr C_1^{\sf SO}(d)&=e^{(L_{\max}-1/2)h}\cdot\dfrac{(\sqrt{d}L_{\max}^{3/2}+3dL_F/2)}{m_{\min}-1/2}~~\text{ and }~~
    \mathscr C_2^{\sf SO}=\dfrac{e^{(L_{\max}-1/2)h}}{m_{\min}-1/2}
    \end{align*}
    with $L_{\max}$ and $m_{\min}$ as defined in Theorem~\ref{thm:EM}.
\end{theorem}
Before discussing the results of this theorem, let us state its direct consequence.
\begin{corollary}
    For a given $\varepsilon>0$, the Wasserstein distance satisfies $\wass_2(\law(\vartheta_N^{\sf SO}),p_0)<\varepsilon$ after
    \begin{align*}
        N=\mathcal{O}\left(\dfrac{1}{\varepsilon}\log\Big(\dfrac{1}{\varepsilon}\Big)\right)
    \end{align*}
    iterations, provided that $T=\mathcal{O}\left(\log(\frac{1}{\varepsilon})\right)$ and $h=\mathcal{O}(\varepsilon)$.
\end{corollary}
{
In the above theorem and corollary, we present the first Wasserstein convergence analysis of an accelerated sampler that utilizes accurate score function estimation and second-order information about log-densities.
More specifically, the error arises from three sources:
\begin{itemize}
    \item Initialization error: This is captured by the term ~$e^{-\min T}\|X_0\|_{\Ltwo}$,  which is independent of the chosen discretization scheme.
    \item Discretization error: This term reflects the advantage of second-order acceleration. By designing the discretization scheme to better approximate the original reverse SDE, we mitigate this error.
    \item Estimation error: This term accounts for errors in estimating both the spatial and temporal components of the score function. Since the scheme involves not only the score function estimate but also the terms $L_n$ and $M_n$, their corresponding errors are included in this term as well.
\end{itemize}
}


A comparison with the convergence rates of the four algorithms in Section~\ref{sec:discretization} highlights the clear computational advantage of the proposed second-order method.
Specifically,
\begin{itemize}
\setlength\itemsep{0.02em}
    \item The second-order method requires only $\widetilde{\mathcal{O}}(1/\varepsilon)$ iterations, whereas the other four methods require~$\widetilde{\mathcal{O}}(1/\varepsilon^2)$.
    \item The second-order method allows for a larger step size $h=\mathcal{O}(\varepsilon)$, enabling faster progression in each iteration, while other methods are limited to a much smaller step size of $h=\mathcal{O}(\varepsilon^2)$.
\end{itemize}
As a result, the second-order method achieves the same accuracy with fewer iterations and a larger step size, making it a more efficient numerical scheme for approximating the target distribution.

Moreover, the convergence rate $\widetilde{\mathcal{O}}(1/\varepsilon)$ is consistent with the rate obtained in \cite{liang2024broadening}, which proposes an accelerated DDPM sampler~\cite{ho2020denoising} relying on Hessian estimation. Additionally, this rate aligns with the iteration complexity results presented in \cite{huang2024convergence}, specifically when setting $p=1$ in their framework.

\section{Numerical Studies}
\label{sec:simulation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Wass_pic.png}
    \caption{Error of various discretization schemes and second-order sampler with different choice of step size.}
    \label{fig:Wass_pic}
\end{figure*}
In this section, we compare the performance of the SGMs uner {\sf EM, EI, REM, REI} discretization schemes described in Section~\ref{sec:setting} and the second-order acceleration method ({\sf SO}) proposed in Section~\ref{sec:accleration}. 
We apply the five algorithms to the posterior of penalized logistic regression, defined by $p_0(\theta)\propto \exp(-f(\theta))$, with the potential function
\begin{align*}
    f(\theta)=\dfrac{\lambda}{2}\l|\theta\r|^2+\dfrac{1}{n_{\sf data}}\sum_{i=1}^{n_{\sf data}}\log\big(1+\exp(-y_ix_i^\top \theta)\big),
\end{align*}
where $\lambda>0$ denotes the tuning parameter. 
The data $\{x_i, y_i\}_{i=1}^{n_{\sf data}}$, composed of binary labels $y_i\in\{-1,1\}$ and features $x_i\in\mathbb{R}^d$ generated from $x_{i,j}\mathop{\sim}\limits^{iid}\mathcal{N}(0,100)$. 
In our experiments, we set $\lambda=10,50$ and $100$, corresponding to the plots from left to right, respectively, with $d=2$ and $n_{\sf data}=100$.
We provide more details of the calculations and implementations in Appendix~\ref{app:simulation}.

Figure~\ref{fig:Wass_pic} shows the Wasserstein distance measured along the first dimension between the empirical distributions of the $N$-th outputs from SGMs and the target distribution, with different choices of the step size~$h$. 
Here, we set $N=10/h$.
These numerical results support our theoretical findingsâ€”SGMs under different discretization schemes described in Section~\ref{sec:discretization} exhibit similar convergence behavior, while the Hessian-based sampler proposed in Section~\ref{sec:accleration} always outperforms the other four methods.
 
\section{Discussion}
\label{sec:discussion}
The Wasserstein-2 distance, used in this paper, serves as a natural and practical metric for measuring errors in diffusion models. However, recent work on the convergence theory of diffusion models has also explored alternative metrics such as total variation distance and KL divergence. A promising direction for future research is to establish convergence guarantees with respect to these alternative distances.

Moreover, while this work makes progress in provably accelerating SDE-based diffusion sampling in Wasserstein distance, it would also be valuable to explore deterministic samplers based on probability flow ODEs.

Additionally, for clarity and simplicity, we focus on a specific choice of drift functions $f$ and $g$ in the forward process, corresponding to the Ornsteinâ€“Uhlenbeck process. Extending this analysis to a more general framework with broader choices of $(f,g)$ is an interesting avenue for future research.

Finally, the assumption of strong log concavity of the data distribution is often considered restrictive. A key future direction is to relax this assumption and analyze the Wasserstein convergence behavior of the score-based diffusion models under weaker conditions.





\newpage
\bibliographystyle{amsalpha}
\bibliography{bib}
\newpage

\include{appendix}


\end{document}
