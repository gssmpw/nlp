@misc{muhammad2025brighterbridginggaphumanannotated,
	title={BRIGHTER: BRIdging the Gap in Human-Annotated Textual Emotion Recognition Datasets for 28 Languages}, 
	author={Shamsuddeen Hassan Muhammad and Nedjma Ousidhoum and Idris Abdulmumin and Jan Philip Wahle and Terry Ruas and Meriem Beloucif and Christine de Kock and Nirmal Surange and Daniela Teodorescu and Ibrahim Said Ahmad and David Ifeoluwa Adelani and 
	Alham Fikri Aji and Felermino D. M. A. Ali and Ilseyar Alimova and Vladimir Araujo and Nikolay Babakov and Naomi Baes and Ana-Maria Bucur and Andiswa Bukula and Guanqun Cao and Rodrigo Tufino Cardenas and Rendi Chevi and Chiamaka Ijeoma Chukwuneke and 
	Alexandra Ciobotaru and Daryna Dementieva and Murja Sani Gadanya and Robert Geislinger and Bela Gipp and Oumaima Hourrane and Oana Ignat and Falalu Ibrahim Lawan and Rooweither Mabuya and Rahmad Mahendra and Vukosi Marivate and Andrew Piper and Alexander 
	Panchenko and Charles Henrique Porto Ferreira and Vitaly Protasov and Samuel Rutunda and Manish Shrivastava and Aura Cristina Udrea and Lilian Diana Awuor Wanzare and Sophie Wu and Florian Valentin Wunderlich and Hanif Muhammad Zhafran and Tianhui Zhang 
	and Yi Zhou and Saif M. Mohammad},
	year={2025},
	eprint={2502.11926},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2502.11926} 
}
@inproceedings{belay-etal-2025-evaluating,
	title = "Evaluating the Capabilities of Large Language Models for Multi-label Emotion Understanding",
	author = "Belay, Tadesse Destaw  and Azime, Israel Abebe  and Ayele, Abinew Ali  and Sidorov, Grigori  and Klakow, Dietrich  and Slusallek, Philip  and Kolesnikova, Olga  and Yimam, Seid Muhie",
	booktitle = "Proceedings of the 31st International Conference on Computational Linguistics",
	year = "2025",
	address = "Abu Dhabi, UAE",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2025.coling-main.237/",
	pages = "3523--3540"
}
@article{wiebe2005annotating,
	author = {Wiebe, Janyce and Wilson, Theresa and Cardie, Claire},
	title = {Annotating Expressions of Opinions and Emotions in Language},
	journal = {Language Resources and Evaluation},
	year = {2005},
	volume = {39},
	pages = {165--210},
	url = {https://aclanthology.org/S05-1025.pdf}
}


@article{kusal2022review,
	title={A review on text-based emotion detection--techniques, applications, datasets, and future directions},
	author={Kusal, Sheetal and Patil, Shruti and Choudrie, Jyoti and Kotecha, Ketan and Vora, Deepali and Pappas, Ilias},
	journal={arXiv preprint arXiv:2205.03235},
	year={2022}
}
@misc{liu2019robertarobustlyoptimizedbert,
	title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
	author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
	year={2019},
	eprint={1907.11692},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/1907.11692}, 
}

@inproceedings{mohammad-kiritchenko-2018-understanding,
	title = "Understanding Emotions: A Dataset of Tweets to Study Interactions between Affect Categories",
	author = "Mohammad, Saif  and
	Kiritchenko, Svetlana",
	editor = "Calzolari, Nicoletta  and
	Choukri, Khalid  and
	Cieri, Christopher  and
	Declerck, Thierry  and
	Goggi, Sara  and
	Hasida, Koiti  and
	Isahara, Hitoshi  and
	Maegaard, Bente  and
	Mariani, Joseph  and
	Mazo, H{\'e}l{\`e}ne  and
	Moreno, Asuncion  and
	Odijk, Jan  and
	Piperidis, Stelios  and
	Tokunaga, Takenobu",
	booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
	month = may,
	year = "2018",
	address = "Miyazaki, Japan",
	publisher = "European Language Resources Association (ELRA)",
	url = "https://aclanthology.org/L18-1030/"
}

@misc{yang2023contextunlocksemotionstextbased,
	title={Context Unlocks Emotions: Text-based Emotion Classification Dataset Auditing with Large Language Models}, 
	author={Daniel Yang and Aditya Kommineni and Mohammad Alshehri and Nilamadhab Mohanty and Vedant Modi and Jonathan Gratch and Shrikanth Narayanan},
	year={2023},
	eprint={2311.03551},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2311.03551}, 
}

@misc{xenos2024vllmsprovidebettercontext,
	title={VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning}, 
	author={Alexandros Xenos and Niki Maria Foteinopoulou and Ioanna Ntinou and Ioannis Patras and Georgios Tzimiropoulos},
	year={2024},
	eprint={2404.07078},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2404.07078}, 
}
@inproceedings{devlin-etal-2019-bert,
	title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
	author = "Devlin, Jacob  and
	Chang, Ming-Wei  and
	Lee, Kenton  and
	Toutanova, Kristina",
	editor = "Burstein, Jill  and
	Doran, Christy  and
	Solorio, Thamar",
	booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
	month = jun,
	year = "2019",
	address = "Minneapolis, Minnesota",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/N19-1423/",
	doi = "10.18653/v1/N19-1423",
	pages = "4171--4186",
	abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
}

@inproceedings{demszky-etal-2020-goemotions,
	title = "{G}o{E}motions: A Dataset of Fine-Grained Emotions",
	author = "Demszky, Dorottya  and
	Movshovitz-Attias, Dana  and
	Ko, Jeongwoo  and
	Cowen, Alan  and
	Nemade, Gaurav  and
	Ravi, Sujith",
	editor = "Jurafsky, Dan  and
	Chai, Joyce  and
	Schluter, Natalie  and
	Tetreault, Joel",
	booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
	month = jul,
	year = "2020",
	address = "Online",
	publisher = "Association for Computational Linguistics",
	url = "https://aclanthology.org/2020.acl-main.372/",
	doi = "10.18653/v1/2020.acl-main.372",
	pages = "4040--4054",
	abstract = "Understanding emotion expressed in language has a wide range of applications, from building empathetic chatbots to detecting harmful online behavior. Advancement in this area can be improved using large-scale datasets with a fine-grained typology, adaptable to multiple downstream tasks. We introduce GoEmotions, the largest manually annotated dataset of 58k English Reddit comments, labeled for 27 emotion categories or Neutral. We demonstrate the high quality of the annotations via Principal Preserved Component Analysis. We conduct transfer learning experiments with existing emotion benchmarks to show that our dataset generalizes well to other domains and different emotion taxonomies. Our BERT-based model achieves an average F1-score of .46 across our proposed taxonomy, leaving much room for improvement."
}
@INPROCEEDINGS{9597390,
	author={Suresh, Varsha and Ong, Desmond C.},
	booktitle={2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)}, 
	title={Using Knowledge-Embedded Attention to Augment Pre-trained Language Models for Fine-Grained Emotion Recognition}, 
	year={2021},
	volume={},
	number={},
	pages={1-8},
	keywords={Emotion recognition;Analytical models;Affective computing;Social networking (online);Computational modeling;Bit error rate;Psychology;Affective Computing;Facial Emotion Recognition;Transfer Learning},
	doi={10.1109/ACII52823.2021.9597390}}

@misc{xenos2024vllmsprovidebettercontext,
	title={VLLMs Provide Better Context for Emotion Understanding Through Common Sense Reasoning}, 
	author={Alexandros Xenos and Niki Maria Foteinopoulou and Ioanna Ntinou and Ioannis Patras and Georgios Tzimiropoulos},
	year={2024},
	eprint={2404.07078},
	archivePrefix={arXiv},
	primaryClass={cs.CV},
	url={https://arxiv.org/abs/2404.07078}, 
}

@inproceedings{emoLLM,
	author = {Liu, Zhiwei and Yang, Kailai and Xie, Qianqian and Zhang, Tianlin and Ananiadou, Sophia},
	title = {EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis},
	year = {2024},
	isbn = {9798400704901},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/3637528.3671552},
	doi = {10.1145/3637528.3671552},
	abstract = {Sentiment analysis and emotion detection are important research topics in natural language processing (NLP) and benefit many downstream tasks. With the widespread application of large language models (LLMs), researchers have started exploring the application of LLMs based on instruction-tuning in the field of sentiment analysis. However, these models only focus on single aspects of affective classification tasks (e.g. sentimental polarity or categorical emotions), and overlook the regression tasks (e.g. sentiment strength or emotion intensity), which leads to poor performance in downstream tasks. The main reason is the lack of comprehensive affective instruction tuning datasets and evaluation benchmarks, which cover various affective classification and regression tasks. Moreover, although emotional information is useful for downstream tasks, existing downstream datasets lack high-quality and comprehensive affective annotations. In this paper, we propose EmoLLMs, the first series of open-sourced instruction-following LLMs for comprehensive affective analysis based on fine-tuning various LLMs with instruction data, the first multi-task affective analysis instruction dataset (AAID) with 234K data samples based on 3 classification tasks and 2 regression tasks to support LLM instruction tuning, and a comprehensive affective evaluation benchmark (AEB) with 8 regression tasks and 6 classification tasks from various sources and domains to test the generalization ability of LLMs. We propose a series of EmoLLMs by fine-tuning LLMs with AAID to solve various affective instruction tasks. We compare our models with a variety of LLMs and sentiment analysis tools on AEB, where our models outperform all other open-sourced LLMs and sentiment analysis tools, and surpass ChatGPT and GPT-4 in most tasks, which shows that the series of EmoLLMs achieve the ChatGPT-level and GPT-4-level generalization capabilities on affective analysis tasks, and demonstrates our models can be used as affective annotation tools. This project is available at https://github.com/lzw108/EmoLLMs/.},
	booktitle = {Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
	pages = {5487â€“5496},
	numpages = {10},
	keywords = {affective evaluation benchmark, affective instruction dataset, emotion detection, large language models, sentiment analysis},
	location = {Barcelona, Spain},
	series = {KDD '24}
}

@misc{zhang2024dialoguellmcontextemotionknowledgetuned,
	title={DialogueLLM: Context and Emotion Knowledge-Tuned Large Language Models for Emotion Recognition in Conversations}, 
	author={Yazhou Zhang and Mengyao Wang and Youxi Wu and Prayag Tiwari and Qiuchi Li and Benyou Wang and Jing Qin},
	year={2024},
	eprint={2310.11374},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	url={https://arxiv.org/abs/2310.11374}, 
}
@inproceedings{muhammad-etal-2025-semeval,  title = "{S}em{E}val Task 11: Bridging the Gap in Text-Based Emotion Detection",
	
	author = "Muhammad, Shamsuddeen Hassan and Ousidhoum, Nedjma and Abdulmumin, Idris and Yimam, Seid Muhie and Wahle, Jan Philip and Ruas, Terry and Beloucif, Meriem and De Kock, Christine and Belay, Tadesse Destaw and Ahmad, Ibrahim Said and Surange, Nirmal and Teodorescu, Daniela and Adelani, David Ifeoluwa and Aji, Alham Fikri and Ali, Felermino and Araujo, Vladimir and Ayele, Abinew Ali and Ignat, Oana and Panchenko, Alexander and Zhou, Yi and Mohammad, Saif M.",
	
	booktitle = "Proceedings of the 19th International Workshop on Semantic Evaluation (SemEval-2025)",
	
	month = {July},
	
	year = {2025},
	
	address = {Vienna, Austria},
	
	publisher = {Association for Computational Linguistics},
	
	url ={},
	
	doi = {},
	
	pages = {}
	
}

