\section{Introduction}\label{sec:intro}
Identifying the Number of Distinct Values (NDV) in a data column is a fundamental task across numerous data management scenarios, particularly within the database domain. 
However, directly obtaining NDV is often impractical in real-world scenarios due to the prohibitive overheads of processing massive data volumes or data access restrictions.
Therefore, estimating NDV on limited samples has been a critical and longstanding research topic, explored for over seven decades~\cite{goodman1949estimation}. 
For instance, in the realm of Biology, a critical task is to estimate unseen species~\cite{valiant2013estimating,valiant2017estimating,mmo_bunge1993estimating}. 
Similarly, in Statistics, a recurring engagement involves quantifying the number of distinct categories within a given population~\cite{goodman1949estimation,chao1984nonparametric}.
Moreover, in the domain of Networks, assessing the quantity of virtualized devices is a significant challenge~\cite{network_cohen2019cardinality,network_nath2008synopsis}. 

In Database, some widely used systems (e.g., Spark and PostgreSQL) directly rely on NDV to compute cardinality, a metric that is subsequently utilized by the query optimizer~\cite{spark_plan_code,pg_plan_code}. 
Besides, NDV affects the join order selection in MySQL as well~\cite{mysql_join}.
Furthermore, recent studies show that precise NDV estimation can generate better query plans that bring significant SQL query execution latency reductions~\cite{li2023alece,han2024bytecard}.



\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{figures/traditional_strength.png}
    \caption{Evaluation of fourteen statistical estimators on 25,159 test columns, where the bar represents the proportion of each estimator achieving the optimality (lowest estimation error) among the fourteen estimators. No single estimator achieves optimality on more than 40\% of test cases.}
    \label{fig:traditional-strength}
\end{figure}


Prolonged research efforts in NDV estimation have accumulated numerous NDV estimators, the majority of which are statistical methods relying on manually designed polynomials computing or equation-solving~\cite{goodman1949estimation,gee_charikar2000towards,error_bound,chao_in_db_ozsoyoglu1991estimating,chao1984nonparametric,shlosser1981estimation,chaolee,hybskew_haas1995sampling,sichel1986parameter,sichel1986word,sichel1992anatomy,mmo_bunge1993estimating,bootstrap_smith1984nonparametric,horvitz_sarndal1992model,hybskew_haas1995sampling}.
Each statistical estimator is grounded in distinct hypotheses regarding the underlying distribution. As a result, their efficacy markedly declines when the actual distribution does not conform to their assumptions.
Recently, some studies have introduced learned estimators based on Machine Learning (ML) techniques~\cite{li2024learning,ls_wu2022learning} to solve this issue, demonstrating better performance than statistical estimators.
Nevertheless, despite the significant progress achieved, the domain of NDV estimation is still beset by the following difficulties:


\noindent\textbf{(1) Selection dilemma.} 
Although a plethora of estimators exist, there frequently remains ambiguity regarding the optimal choice for practical application.
The question of \textit{which estimator is most suitable for a specific data column} has received scarce attention over time. 
As new estimators continue to be introduced, this understudied question becomes increasingly substantial. 
To intuitively show the selection dilemma, we depict the performance of fourteen statistical estimators on a large dataset comprising 25,159 test cases (for additional details, please refer to Section~\ref{sec:exp-settings}) in Figure \ref{fig:traditional-strength}.
The results illustrate that no single estimator consistently achieves the lowest estimation error across all test cases, {indicating that no individual estimators can consistently beat others}. For instance, the top-performing estimator, Shlosser, only manages to outperform others in roughly 40\% of the cases.  
This observation highlights the intricate nature of selecting a suitable estimator from a set of available options.





\noindent\textbf{(2) Underexploitation issue.}
Most studies have focused on exploring new estimators, including the recently proposed learned estimator~\cite{li2024learning,ls_wu2022learning}, while exploiting existing estimators to improve estimations has been largely overlooked. 
To better illustrate the issue, we conceptualize a \textit{hypothetical} estimator, which involves picking one of the aforementioned fourteen statistical estimators, under the hypothetical condition that we know the actual estimation errors in advance. 
Precisely, we name the hypothetical estimator ``Hypo-optimal'' since we select the estimator with the lowest actual estimation error for every test.
The performance of the hypothetical estimator and a state-of-the-art (SOTA) learned estimator~\cite{ls_wu2022learning} is shown in Table~\ref{tab:ideal-traditional}. 
From the results in the table, it is evident that the Hypo-optimal estimator considerably outperforms the SOTA learned estimator. 
The comparison illustrates the substantial potential held by statistical estimators, simultaneously underscoring the critical significance of judiciously exploiting estimators.


\begin{table}[t]
    \centering
    \caption{Experiments on hypothetical estimators. The numbers represent estimation errors, with lower values indicating better performance.}
    \begin{tabular}{ccccccc}
\toprule
        Estimator& Mean & 50\% & 75\%& 90\% & 95\% & 99\% \\
\midrule
         Hypo-optimal  & \textbf{1.20} & \textbf{1.08} & \textbf{1.28} & \textbf{1.56} & \textbf{1.83} & \textbf{2.36} \\
        \makecell{\footnotesize SOTA learned estimator \\ \footnotesize ~\cite{ls_wu2022learning}}  & 2.24 & 1.72 & 2.28 & 3.20 & 4.11 & 10.46 \\
\bottomrule
    \end{tabular}
    \label{tab:ideal-traditional}
\end{table}

Inspired by the above observations, in this paper, we introduce \textsc{AdaNDV}, an \underline{Ada}ptive \underline{NDV} estimation method learning to select the proper estimators from existing ones and to fuse their estimation results to enhance estimation precision for different scenarios.
Selecting the optimal estimator with high accuracy is quite challenging because it is difficult to extract adequate features and explore appropriate ML models.
{On the contrary, selecting the $k$ estimators that are most likely to approach the ground truth is a relatively easier task for ML models~\cite{liu2009learning}. 
}
Moreover, we distinctively propose to address the issue by distinguishing whether an estimator is overestimated or underestimated, allowing us to utilize them accordingly.
{Specifically, if one estimator overestimates and another underestimates for a test case, there exists a set of weights such that their weighted sum performs better than either estimator individually.}
This approach inherently leverages the complementary nature of overestimations and underestimations to reduce estimation error. 
{In addition, in our initial experiments, we found that certain base estimators tend to make more overestimations or underestimations, indicating that distinguishing between overestimating and underestimating estimators is a comparatively easier task.}
Subsequently, we select the estimators with leading overestimation and underestimation performance respectively.
Further, we introduce a learned model to predict the weights of the chosen estimators and then establish the ultimate estimation by applying a weighted sum to fuse them. 
Different from the previous works that directly estimate NDV, our method offers a novel approach to enhance the accuracy of NDV estimation by merging existing estimators.
This allows our method to adaptively select appropriate estimators for specific scenarios and allocate proper weights to fuse them for end-to-end estimation.
Finally, extensive experiments are carried out on a voluminous dataset from the real world. Specifically, the number of individual test columns exceeds tens of thousands, while previous works were tested on at most about two hundred columns. This orders of magnitude increase in individual test columns enables a thorough evaluation encompassing existing estimators alongside our novel approach.






To sum up, the main contributions are shown as follows:
\begin{itemize}
    \item We propose \textsc{AdaNDV}, an adaptive NDV estimation method learning to select and fuse appropriate estimators for specific scenarios. To the best of our knowledge, we are the first to combine existing estimators with ML techniques to improve NDV estimation. 
    \item We introduce an innovative overestimation-underestimation complementary perspective for estimator selection and exploitation to reduce estimation error.
    \item We develop a novel learned weighted sum strategy to fuse the estimation results to obtain the ultimate estimation, which is significantly different from directly estimating NDV.
    \item Extensive experiments, conducted on a rich, real-life dataset with tens of thousands of individual columns, significantly larger than at most hundreds of columns used in past studies, demonstrate the superiority of \textsc{AdaNDV}.
\end{itemize}
