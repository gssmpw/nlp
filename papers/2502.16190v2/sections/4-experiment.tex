\section{Experiments}\label{sec:exp}

\subsection{Experimental Setup}\label{sec:exp-settings}
\noindent\textbf{Dataset Selection.} 
A fundamental test case for evaluating estimators is applying them to an individual data column, where the underlying data distribution is manifested. Therefore, the diversity of evaluation scenarios hinges on the variety of individual columns with various data distributions, 
rather than the quantity of data tuples in the data column.
Most traditional statistical estimators~\cite{goodman1949estimation,gee_charikar2000towards,error_bound,chao_in_db_ozsoyoglu1991estimating,chao1984nonparametric,shlosser1981estimation,chaolee,hybskew_haas1995sampling,sichel1986parameter,sichel1986word,sichel1992anatomy,mmo_bunge1993estimating,bootstrap_smith1984nonparametric,horvitz_sarndal1992model,hybskew_haas1995sampling} are tested on a few synthetic columns that satisfy their heuristics and assumptions. 
Recent studies of learned estimators~\cite{li2022sampling,ls_wu2022learning} primarily evaluate the performance on manually crafted standard data distribution (e.g. Zipfian and Poisson) and a limited number of columns of some open-source datasets (SSB~\cite{ssb_o2009star}, Campaign~\cite{Campaign}, NCVR~\cite{NCVR}, et. al.). 
In conclusion, the previous works used limited data distributions and data columns to evaluate the performance of NDV estimators, which may lead to insufficient evaluations.

In recent years, the research community has proposed open-source large-scale tabular datasets~\cite{hulsebos2023gittables,eggert2023tablib}. 
TabLib~\cite{eggert2023tablib}, which collects 627M individual tables totaling 69 TiB, is the largest one all over the world. 
In this paper, we select TabLib sample version~\cite{tablib-v1-sample}, which contains 0.1\% of the full version (69 GB), as our dataset for evaluation. 
TabLib exhaustively contains tabular data from the real world (GitHub~\cite{github} and Common Crawl~\cite{commoncrawl}) with diverse domains,  which can better reflect the data distribution in practice.


\noindent\textbf{Data Preprocess.} TabLib sample version contains 77 parquet files, we remove three of them (2d7d54b8, 8e1450ee, and dc0e820c) for the memory issue. Then we divide the remaining 74 files into train, test, and validation sets to avoid potential data leaks. 
Each parquet file contains thousands of tables, where for each column we independently sample 1\% of data tuples uniformly to construct evaluation cases. 
{Previous works solely focus on large tables with millions of rows~\cite{ls_wu2022learning,li2024learning}. We expand the evaluation cases by assessing the methods on table sizes ranging from tens of thousands to millions of rows.}
The statistics of preprocessed data are shown in Table \ref{tab:data-statistics}, where ``\# Columns'' represents the number of Train/Validation/Test cases used for evaluation.


\begin{table}[]
    \centering
    \caption{Statistics of preprocessed TabLib sample data.}
    \begin{tabular}{ccccccc}
\toprule
        &  Train & Validation & Test     \\
\midrule
         \# Columns  & 89,283 & 30,418 & 25,159  \\

\bottomrule
    \end{tabular}
    \label{tab:data-statistics}
\end{table}



\noindent\textbf{Evaluation Criteria.} To comprehensively evaluate the performance of NDV estimators, we use mean q-error (as defined in Equation~(\ref{eq:q-error})) and the distribution (50\%, 75\%, 90\%, 95\%, and 99\% quantiles) of q-error on a large data volume.



\noindent\textbf{Implementation Details.} We implement our model in PyTorch, and the implementation details are shown as follows: 
the optimizer is Adam~\cite{adam_kingma2014adam} with an initial learning rate of 0.001, $\alpha$ is 1, $\beta$ is 0.5, the number of input features $H$ is 100, the number of selected leading estimators $k$ is 2. The training epoch is 100, we save the model by 99\% quantile of q-error on the validation set and report the performance on the test set. All the experiments in this paper are conducted on an NVIDIA A100 GPU.


\noindent\textbf{Baseline Models.} 
In our evaluation, we include statistical estimators, hybrid estimators that integrate statistical ones, and learned estimators as our baselines.


\noindent\textit{Statistical estimators (base estimators).}
There are many statistical estimators, we select representative ones as our baselines as well as our base estimators.


\begin{itemize}[leftmargin=10pt]
\item Goodman~\cite{goodman1949estimation} is the seminal work in NDV estimation, and we use the expression in Equation (\ref{eq:goodman}). 
\item GEE~\cite{gee_charikar2000towards} provides a theoretical lower bound of ratio error and it uses geometric mean as scale factor: $D_{\mathrm{GEE}}=\sqrt{N/n}f_1+\sum_{j=2}^nf_j$.
\item Error Bound (EB)~\cite{error_bound} is proposed to estimate NDV in sampling-based histogram construction, and $D_{\mathrm{EB}}=\sqrt{N/n}f_1^++\sum_{j=2}^nf_j,f_1^+=\max(1,f_1)$.
\item Chao~\cite{chao1984nonparametric,chao_in_db_ozsoyoglu1991estimating} assumes the size of $C$ is infinity. We use the expression in Equation (\ref{eq:chao}). If $f_2$ is zero, we will return $d$.
\item Shlosser~\cite{shlosser1981estimation} is based on the assumption that the frequency profile of sample data is approximately the frequency profile of the original column. ${D}_{\text{Shlosser}} = d + \frac{f_1 \sum_{i=1}^n (1 - r)^i f_i}{\sum_{i=1}^n ir(1 - r)^{i-1} f_i}$.
\item ChaoLee~\cite{chaolee} adds another estimator in Chao to treat data skew, we refer to~\cite{ndvlib} to implement it.
\item Jackknife~\cite{burnham1978estimation,burnham1979robust} assumes $d_n$ be the NDV of the sample and numbers the tuples from 1 to n in the sample data. Denote $d_{n-1}(k),1\leq k \leq n$, $d_{n-1}(k)=d_n-1$ if the attribute value for tuple $k$ is unique; otherwise $d_{n-1}(k)=d_n-1$. The first-order Jackknife estimator is: $D_{\mathrm{Jackknife}}=d_n-(n-1)(d_{n-1}-d_n)$.
\item Sichel~\cite{sichel1986parameter,sichel1986word,sichel1992anatomy} estimator needs to solve non-linear equations, as shown in Equation~(\ref{eq:sichel}).
\item Bootstrap~\cite{bootstrap_smith1984nonparametric}: $D_{\mathrm{Boot}}=d+\sum_{j:n_j>0}(1-n_j/n)^n$. It may perform worse when $D$ is large and $n$ is small because $D_{\mathrm{Boot}}\leq 2d$.
\item Horvitz-Thompson (HT)~\cite{horvitz_sarndal1992model} has a sophisticated expression, it defines $h_n(x)=\frac{\Gamma(N-x+1)\Gamma(N-n+1)}{\Gamma(N-n-x+1)\Gamma(N+1)}$, where $\Gamma$ is the gamma function, and $D_{HT}=\sum_{j:n_j>0}\frac{1}{1-h_n(\hat{N}_j)},\hat{N}_j=N(n_j/n)$. 
\item Method of Movement (MoM)~\cite{mmo_bunge1993estimating} has three versions. MoM v1 assumes the frequencies are equal ($N_1=N_2=\ldots=N_D$) and an infinite population, it needs to solve the equation: $d=D_{\mathrm{MoM1}}(1-e^{-n/D_{\mathrm{MoM1}}})$. MoM v2 assumes the population size is finite, and the estimator is $d=D_{\mathrm{MoM2}}(1-h_n(N/D_{\mathrm{MoM2}}))$. MoM v3 assumes the frequencies are unequal and it has a sophisticated expression, we refer to~\cite{ndvlib} to implement it.
\item Smoothed Jackknife (SJ)~\cite{hybskew_haas1995sampling}. $D_{\mathrm{SJ}}=d_n-K((d_{n-1}-d_n))$, there is a extremely sophisticated approximation expression for $K$, we omit its expression in the paper and refer to ~\cite{ndvlib} to implement it.
\end{itemize}







\noindent\textit{Hybird estimators.} Existing hybrid estimators are commonly constructed by using SJ~\cite{hybskew_haas1995sampling}, GEE~\cite{gee_charikar2000towards}, Shlosser~\cite{shlosser1981estimation} estimators. Specifically, they use $\chi^2_{n-1}$ test~\cite{chi2test} to pick estimators. Define 
\begin{align}
u=\sum_{j,n_j>0}(\frac{(n_j-\Bar{n})^2}{\Bar{n}}),\Bar{n}=\frac{n}{d}.
\end{align}
\begin{itemize}[leftmargin=10pt]
\item HYBSkew~\cite{hybskew_haas1995sampling} uses SJ estimator if $u\leq \chi^2_{n-1,0.975}$, otherwise it takes Shlosser estimator.
\item HYBGEE~\cite{gee_charikar2000towards} uses SJ estimator if $u\leq \chi^2_{n-1,0.975}$, otherwise it takes GEE estimator.
\end{itemize}




\noindent\textit{Learned estimators.}
In addition, we compare our method with the open-sourced SOTA learned estimator LS~\cite{ls_wu2022learning}. We consider three variants of LS for a fair comparison:


\begin{itemize}[leftmargin=10pt]
\item LS$_{\mathrm{general}}$: since LS claimed can directly apply to any data distribution~\cite{ls_wu2022learning}, we use its open-source checkpoint as a baseline.
\item LS$_{\mathrm{scratch}}$: we train LS from scratch on the same training set as \textsc{AdaNDV}.
\item LS$_{\mathrm{FT}}$: we fine-tune (FT) LS$_{\mathrm{general}}$ as described in~\cite{ls_wu2022learning} on the same training set as \textsc{AdaNDV}.
\end{itemize}

Besides, we construct two learned estimator baselines as follows:

\begin{itemize}[leftmargin=10pt]
\item Select-Optimal (SO): it selects one optimal base estimator using an MLP with the same architecture as the estimator selection model in \textsc{AdaNDV}, designed to accomplish the objective as Hypo-optimal.
\item Learnable Ensemble (LE): it integrates the results of all fourteen base estimators by a learnable weighted sum, where the number of parameters is equal to the number of base estimators.
\end{itemize}




\begin{table}[]
    \centering
    \caption{Mean and quantiles of q-error of baselines and our method, where $\infty$ indicates that the number exceeds the representation limits of a 32-bit floating-point type. Each best-performing metric is emphasized in boldface.}
    \begin{tabular}{ccccccc}
\toprule
        Estimator& Mean & 50\% & 75\%& 90\% & 95\% & 99\% \\
\midrule
        Goodman& $\infty$ &4.14 & 37.91& 100.78&1.11e11 & $\infty$  \\
        GEE  &3.71  &1.97  &3.86  &9.91  &10.10  &11.70\\
        EB & 3.98 & 2.62 & 6.00 & 9.98 & 10.12 & 11.00 \\
        Chao & 21.98 & 1.85 & 6.50 & 99.99 & 100.04 & 100.19 \\
        Shlosser & 4.25 & 1.80 & 4.53 & 10.41 & 15.14 & 25.14\\
        ChaoLee &   23.28 & 4.35 & 29.39 & 96.00 & 99.36 & 100.51 \\
        Jackknife & 12.60 & 2.80 & 16.24 & 49.04 & 50.38 & 51.71  \\
        Sichel & 152.93 & 2.49 & 99.70 & 365.10 & 1.03e3 & 2.20e3 \\
        MoM v1 & 2.51e4 & 2.00 & 6.60 & 22.70 & 66.51 & 1.11e6 \\
        MoM v2 & 8.60 & 3.14 & 9.96 & 18.84 & 30.61 & 86.09 \\
        MoM v3 & 4.29e3 & 5.95 & 61.92 & 818.18 & 3.30e3 & 4.30e4 \\
        Bootstrap &  70.01 & 10.99 & 47.91 &95.42 &224.09& 1.25e3 \\
        HT & 2.83e3& 41.56 & 354.42 & 5.07e3 & 1.03e4 & 4.41e5 \\
        SJ & 231.67 & 2.17 & 8.12 & 34.03 & 90.25 & 1.81e3\\
\midrule
        HYBSkew & 4.25 & 1.80 & 4.53 & 10.41 & 15.14 & 25.14\\
        HYBGEE & 3.71 & 1.97 & 3.86 & 9.91 & 10.10 & 11.70\\
\midrule
    LS$_{\mathrm{general}}$ & 2.24 & 1.72 & 2.28 & 3.20 & 4.11 & 10.46 \\

    LS$_{\mathrm{scratch}}$ & 1.91 & 1.36 & 1.80 & 2.57 & 3.53 & 10.80\\
    
    LS$_{\mathrm{FT}}$ & 1.96 & 1.39 & 1.86 & 2.64 & 3.63 & 11.44 \\

    {SO}  & $\infty$ & 1.29 & 2.08 & 3.29& 4.42 & 13.03 \\
    
    {LE} & 2.30 & 1.71 & 2.22 & 3.44 & 4.63 & 11.88\\

\midrule
    \textsc{AdaNDV} & \textbf{1.62} & \textbf{1.22}& \textbf{1.60} & \textbf{2.34} & \textbf{3.24} & \textbf{6.79} \\
    
\bottomrule
    \end{tabular}
    \label{tab:overall}
\end{table}





\subsection{Statistical Estimators Analysis}
\label{sec:perf-traditional}
In this section, we present a detailed analysis of the performance of statistical and hybrid estimators in large-scale real-world scenarios.




\noindent\textbf{Overall Performance.} The performance is shown in Table \ref{tab:overall}. According to the results, we can draw the following conclusions:

\begin{itemize}[leftmargin=10pt]
\item There is no single traditional estimator that always outperforms the other ones across all the evaluated metrics. 
\item The majority of traditional estimators exhibit subpar performance across most metrics. In the most adverse scenarios, the q-error of the Goodman estimator even surpasses the upper limit that a 32-bit floating-point value can represent due to factorial operations in Equation~(\ref{eq:goodman}).
\item Using a comprehensive assessment of the performance of NDV estimators is desired. 
For example, although the Shlosser estimator has a mean q-error that is higher but close to that of EB, its 50\% quantile of q-error is considerably lower, while its 99\% quantile of q-error is significantly higher. A single metric can not reflect the performance of an estimator.
\item HYBSkew does not outperform Shlosser, and HYBGEE does not surpass GEE. On the one hand, they rarely select SJ, indicating hybrid estimators may mitigate poor results to some extent. On the other hand, the performance of traditional hybrid estimators is limited by the selected single estimator.
\end{itemize}


\noindent\textbf{Intrinsic Weaknesses.} The intrinsic weaknesses of statistical estimators are evident and have been discussed in previous works~\cite{ls_wu2022learning,li2024learning}: the assumptions and conditions of traditional estimators are infrequently satisfied in practice.


We expand the number of evaluation cases by several orders of magnitude to provide a more comprehensive assessment that has not been investigated before, and we can conclude that the weaknesses exposed by prior studies remain valid.



\noindent\textbf{Undiscovered Strengths.} 
However, we argue that the strengths of statistical estimators are significantly eclipsed by their weaknesses. Although real-world data distributions often fail to meet their presupposed conditions, it is possible to identify estimators that deliver the estimated result with an acceptable level of q-error. 
Our argument is supported by the Hypo-optimal estimator shown in Table \ref{tab:ideal-traditional}. 
Meanwhile, \textit{it suggests that accurately selecting appropriate estimators could significantly reduce q-error}.









\begin{figure}
    \includegraphics[width=\linewidth]{figures/err_distribution.png}
    \centering
    \caption{Error distribution of learned estimators on the test set. The violin plot is in blue. The boxplot is in black, the gray box contains 50\% of data points, and the white line in the gray box represents the median. We exclude the SO estimator due to its extremely large mean error.}
    \label{fig:err-distribution}
\end{figure}





\subsection{Results of Learned Estimators}
\label{sec:perf-adandv}

\noindent\textbf{Effectiveness.} The overall performance of learned estimators is shown in Table \ref{tab:overall}, and we observe the following findings:


\begin{itemize}[leftmargin=10pt]
\item \textit{Performance of the proposed learned estimator}.
\textsc{AdaNDV} significantly outperforms all estimators across all the metrics. These results substantially demonstrate the superiority of \textsc{AdaNDV}.


\item \textit{Performance of SOTA learned estimators}. 
LS$_{\mathrm{general}}$ consistently outperforms the statistical estimators across all the metrics, which exhibits the advantages of learned estimators that can adapt to data shifting over statistical estimators.
In addition, LS$_{\mathrm{scratch}}$ and LS$_{\mathrm{FT}}$ have better performance compared to LS$_{\mathrm{general}}$ in general.
However, by looking at the different quantiles of q-error, training or fine-tuning may decline the robustness, where they perform worse than LS$_{\mathrm{general}}$ for 99\% quantile of q-error.


\item \textit{Performance of constructed learned estimators}. The performance of the SO and LE estimators reveals the necessity of investigating the historically overlooked issues in NDV estimation. Firstly, selecting one optimal estimator by a learned model can outperform individual estimators in most scenarios, but it is challenging to achieve high accuracy, which can lead to worse cases. Besides, the LE estimator outperforms individual statistical ones, showcasing the potential of estimator fusion.

\item \textit{Error distribution discussion}. 
In Table \ref{tab:overall}, we also observe that \textsc{AdaNDV} exhibits a more robust error distribution: about 90\% of the test cases show a q-error below 2.30, and approximately 99\% of test cases exhibit the q-error do not exceed 7. 
Moreover, we depict the boxplot~\cite{boxplot} and violin plot~\cite{violinplot} of q-error distributions of learned estimators in Figure~\ref{fig:err-distribution}, where we omit the SO estimator for its overflow issue. 
From the figure, we observe that the q-error distribution for the AdaNDV shows a higher concentration around values closer to the minimum error, indicating its advantages. Besides, the maximal q-error of \textsc{AdaNDV} is much smaller than other learned estimators, representing better performance in the worst cases.

\end{itemize}



\noindent\textbf{Efficiency.} 
The efficiency of an estimator determines its practicality. 
Thus we also evaluate the efficiency of $\textsc{AdaNDV}$ and the learned estimators in terms of both time and space consumption.
The computing overhead of $\textsc{AdaNDV}$ involves the neural networks and the selected base estimators. 
Since some base estimators involve solving non-linear equations, it is not easy to provide a rigorous time complexity analysis. 
Therefore, for simplicity, we record the end-to-end latency of the training and testing stages for them.

Specifically, in the training stage, we record the execution time of the base estimators as well as the time reaching convergence for the learned estimators. 
In the inference stage, we capture the time required for processing all test cases. 
In addition, we compare the learnable parameters between the learned estimators to show their space efficiency. We exclude LS$_{\mathrm{general}}$ in this comparison since it is pre-trained and has the same inference efficiency and parameters as its variants.
The time and space consumptions are shown in Table \ref{tab:efficiency}, and we derive the following conclusions.




\begin{table}[]
    \centering
    \caption{Efficiency comparison of the methods.}
    \begin{tabular}{ccccccc}
\toprule
         & LS$_{\mathrm{scratch}}$ & LS$_{\mathrm{FT}}$ & \textsc{AdaNDV} & SO & LE \\
\midrule
        Train (s) & 929 & 2,963 & {712} & 1,261 & 1,337\\
        Inference (s)   & 25.42 & 25.42 & 51.44 &  32.79  & 84.85 \\
        \# Params& 62,435 & 62,435 & 55,328 & 22,294  & 14  \\
\bottomrule
    \end{tabular}
    \label{tab:efficiency}
\end{table}



\begin{itemize}[leftmargin=10pt]
\item \textit{Training efficiency of learned estimators.} 
We observe that LS$_{\mathrm{FT}}$ is the most time-consuming method in that it requires about three times of epochs than LS$_{\mathrm{scratch}}$ to converge. 
One possible reason may be that adapting the general model to the training domain requires more time than learning from scratch since the fine-tuning process may need to maintain the features learned in the pre-training stage.  
{\textsc{AdaNDV} requires minimal time for convergence.}
The SO and LE estimators have longer training time than \textsc{AdaNDV} and LS$_{\mathrm{scratch}}$, indicating their objectives are more difficult to converge.


\item \textit{Inference efficiency.} 
The inference overhead of baseline learned estimators solely involves a neural network model inference and the total time on the test set ($25,159$ samples) is 25.42s, the average inference time of a test case is about \textbf{1ms}. 
In {contrast}, \textsc{AdaNDV} consists of three neural models and they need 27.46s to finish the inference on the test set. 
Besides, \textsc{AdaNDV} additionally requires the base estimator computation which needs 23.97s. 
In total, \textsc{AdaNDV} spends 51.44s to finish the inference on all the test samples with an average estimation time of about \textbf{2ms}.
The elaborated components bring performance improvement and efficiency decline in \textsc{AdaNDV}.
For the constructed estimators, the SO estimator requires a selected base estimator employment for each test case, resulting in a longer inference time compared to LS models. LE needs the estimation results of all base estimators, leading to the longest inference time among the learned estimators.


\item \textit{Space efficiency comparison.} 
We observe that the variants of LS possess approximately 12.85\% more parameters compared to \textsc{AdaNDV}.
Its advantage can likely be attributed to the elaborated integration of base estimators. 
This strategic utilization enhances the performance of learned estimator \textsc{AdaNDV} with less representation ability (number of learnable parameters). 
For the constructed estimators, the SO estimator has fewer parameters than \textsc{AdaNDV} and LS models, since its architecture is the same as a component in \textsc{AdaNDV}. LE solely has 14 learnable parameters, making it the most lightweight among the learned estimators, which may indicate the potential for lightweight optimization in \textsc{AdaNDV}.

\end{itemize}




\begin{table}[t]
    \centering
    \caption{Ablation study, where ``{w/o}'' indicates removing a component from \textsc{AdaNDV}.}
    \begin{tabular}{ccccccc}
\toprule
        Estimator& Mean & 50\% & 75\%& 90\% & 95\% & 99\% \\
\midrule
         \textsc{AdaNDV} & {\textbf{1.62}} & {\textbf{1.22}}& {\textbf{1.60}} & {\textbf{2.34}} & {\textbf{3.24}} & {\textbf{6.79}} \\
        w/o select  & {2.30} & {1.71} & {2.22} & {3.44} & {4.63} & {11.88
} \\ 
        w/o fusion  &  {2.42} & {1.35} & {2.01} & {3.31} & {4.75} & {19.30} \\
\midrule
        {w/o} log & 2.72e7 &  {41.54}& {671.06} & {2.67e3} & {5.87e4}  & 1.00e10 \\
         {w/o} base & {1.67}  & {1.27}  & {1.66} & {2.39}  & {3.34}  & {7.42}  \\
         {w/o} over & {2.20} & {1.37} & {1.92} & {3.10} & {4.27} & {11.97} \\
         {w/o} under & {2.49} & {1.82} & {2.88} & {5.50} & 6.00 & 11.00  \\
         {w/o} comp & {2.05} & 1.26 & 1.76 & {2.68} & {3.81} & {11.68} \\
\bottomrule
    \end{tabular}
    \label{tab:ablation}
\end{table}























\subsection{Ablation Study}
\label{sec:exp-ablation}
The outstanding performance of \textsc{AdaNDV} mainly stems from the effective collaboration between its leading estimator selection and the fusion-based estimation.
To investigate the contributions of each module, we develop several variants of \textsc{AdaNDV}. The symbol ``{w/o}'' indicates removing the component or strategy from our method.


\noindent\textbf{Primary Components.} 
We individually eliminate each of the two principal constituents within \textsc{AdaNDV}.
Firstly, we drop the leading estimator selection component to deliberately make the model use all base estimators to estimate NDV, the variant is named ``\textbf{{w/o} select}''. 
We then remove the estimator fusion component to select a single estimator by the ranking paradigm from the base estimators to present NDV, the variant is named ``\textbf{{w/o} fusion}''. 
The performances of the two variants are shown in Table \ref{tab:ablation}. 
We can observe that the performance of \textsc{AdaNDV} significantly declines across all the metrics if we remove one of the two components. Based on the observation, we can derive the following conclusions. 
\begin{itemize}[leftmargin=10pt]
\item Directly estimating NDV by fusing all base estimators may make the task difficult because it has to leverage poor-performing estimators. Besides, the performance of ``{w/o} select'' surpasses that of LE, highlighting the advantages of adaptively adjusting the weights of fused estimators for different test cases.
\item The performance of ``{w/o} fusion'' shows that selecting a single estimator with high accuracy is challenging because of the representation ability (sparse features and the MLP architecture) of the model. On the contrary, it performs better than the SO estimator in most metrics and can alleviate the $\infty$ of mean q-error. It demonstrates the effectiveness of the ranking paradigm (``{w/o} fusion'') over the classification paradigm (SO), and further investigation will be conducted in Section~\ref{sec:exp-paradigm}.
\item Each component has a significant contribution to \textsc{AdaNDV}, and the combination of two components leads to the superiority of \textsc{AdaNDV}. 
\end{itemize}




\noindent\textbf{Detailed Strategies.} 
We further study the other strategies utilized in \textsc{AdaNDV} and the performance of dropping each strategy is shown in Table \ref{tab:ablation}. 
Firstly, we remove the log scale training technique by transforming Equation (\ref{eq:logd}) to $\hat{D}=\prod_{j=1}^k(\hat{\mathcal{D}}|_{\mathcal{I}^\mathrm{over}_j}^{\Lambda_j}\cdot \hat{\mathcal{D}}|_{\mathcal{I}^\mathrm{under}_j}^{\Lambda_{k+j}})$, the variant is denoted as ``\textbf{{w/o} log}''. 
The performance of ``{{w/o} log}'' significantly declined, which indicates the effectiveness of logarithm operation. 


In the estimator fusion component, we add the estimated NDV of the selected leading estimators as features. 
We remove the features of estimated results, and the variant is named ``\textbf{{w/o} base}''. 
Its performance slightly declined on all metrics but performs better than all baselines, which indicates that taking the estimated results as input features may make the model aware of the input value domain and then improve the weights allocation to get more accurate estimations.


\begin{figure*}[t]
     \centering
    \begin{subfigure}{0.16\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/alpha_mean.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}{0.16\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/alpha_99.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}{0.16\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/beta_mean.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}{0.16\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/beta_99.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}{0.16\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/k_mean.png}
     \end{subfigure}
     \hfill
     \begin{subfigure}{0.16\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/k_99.png}
     \end{subfigure}
     \hfill
    \caption{Performance on mean and 99\% percentile of q-error of \textsc{AdaNDV} with different hyperparameters.}
    \label{fig:hyper-all}
\end{figure*}




Finally, we study whether discerning overestimation and underestimation estimators work. 
We remove the two kinds of leading estimators and name the two variants as ``\textbf{{w/o} over}'' and ``\textbf{{w/o} under}'', respectively. 
We can observe the two variants significantly decrease on all metrics and they cannot beat baseline learned estimators. 
The performance illustrates that individually utilizing the two kinds of leading estimators can not obtain satisfactory results but the combination of them can bring a significant approximation to the ground truth NDV. In addition, we construct a new variant ``\textbf{{w/o} comp}'' that directly selects {2$k$} estimators without deliberately utilizing the overestimation-underestimation complementary error correction strategy. It outperforms all variants in most metrics but is consistently worse than \textsc{AdaNDV}. On the one hand, it illustrates the benefit of our proposed complementary error correction strategy. On the other hand, although not intentionally distinguished, it is rare for the selected estimators to be exclusively overestimated or underestimated. This further demonstrates the effectiveness of our strategy combined with the experimental conclusions of ``{w/o} over'' and ``{w/o} under''.







\subsection{Impact of Hyperparameters}
\label{sec:exp-hyperparameter}
There are three hyperparameters in \textsc{AdaNDV}: $\alpha, \beta, k$, and we investigate the sensitivity of the model. We demonstrate the performance of mean and 99\% percentile of q-error in Figure~\ref{fig:hyper-all} to show the impact of hyperparameters.

\noindent\textbf{Effect of $\alpha$ in the Ranking Model.} We can observe different $\alpha$ values lead to some fluctuations in the performance, but \textsc{AdaNDV} is not sensitive to different values of $\alpha$. We set $\alpha$ to 1 because it achieves the best performance in most metrics.
Extensively discussing the effect of this knob is out of the scope of this paper, refer to~\cite{bruch2019revisiting,wang2018lambdaloss} for more details.

\noindent\textbf{Effect of Multi-objective Optimization Knob $\beta$.} As shown in Figure~\ref{fig:hyper-all}, we observe that the performance in mean q-error remains unchanged across different values of the knob $\beta$. However, there is a notable difference in 99\% q-error. For each experimented value of $\beta$, \textsc{AdaNDV} exhibits relatively stable performance. The knob $\beta$ balances the estimator ranking and NDV prediction tasks, we set $\beta$ as 0.5 in this paper because it achieves better results in most metrics than in other settings.



\noindent\textbf{The Number of Leading Estimators $k$.} The number of selected leading estimators directly affects the input of the estimation fusion component. With the variation of $k$, the performance of the model does not improve beyond that achieved with $k=2$ on the mean and the 99\% percentile of q-error. This finding illustrates that selecting more estimators does not consistently benefit the fusion component, as demonstrated in the analysis of the LE estimator and the variant of \textsc{AdaNDV} ``\textbf{{w/o} select}''. Additionally, increasing $k$ involves incorporating more base estimators during the inference stage, which introduces additional computational overhead, as shown in Table~\ref{tab:efficiency}. Considering both effectiveness and efficiency, we set $k$ as 2.



\subsection{Flexibility Discussion}
\label{sec:exp-paradigm}


\begin{figure}[t]
    \centering
    \begin{subfigure}{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/overunderrate.png}
         \caption{{Performance of base estimators.}}\label{fig:baseratio}
     \end{subfigure}
     \begin{subfigure}{0.23\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/chosen.png}
         \caption{Proportion of selected estimators.}\label{fig:chosen}
     \end{subfigure}
     \caption{{Illustration of overestimation and underestimation properties.}}\label{fig:overunderproperty}
\end{figure}

\noindent\textbf{Selected Estimator Discussion.} 
{Since we leverage the property of overestimation and underestimation, we demonstrate the overestimation and underestimation ratios of base estimators in Figure~\ref{fig:baseratio}.} We count the estimators selected by the overestimated and underestimated selection models of \textsc{AdaNDV}, and the proportion of each estimator is shown in Figure~\ref{fig:chosen}. We can observe the following findings based on the results.

\begin{itemize}[leftmargin=10pt]
\item {Base estimators can hardly derive the ground truth, as they always either overestimate or underestimate. Besides, no estimators can consistently overestimate or underestimate, but certain estimators tend to underestimate on the dataset.}
\item EB is the most frequently selected by both models, ChaoLee has not been selected by either model, and some estimators are exclusively selected by a single model. It suggests that most estimators are beneficial and the overestimation-underestimation complementary perspective is utility. 
\item Figure~\ref{fig:traditional-strength} illustrates that all base estimators can achieve the lowest q-error in certain scenarios compared to others, but some are selected infrequently. This highlights that collecting the set of base estimators may be an open research question.  
\end{itemize}



\noindent\textbf{{Precisions of} Estimator Selection.}
We have constructed the SO estimator as our baseline {by categorizing} the estimators into two classes {for training}: the optimal overestimated or underestimated estimator and otherwise. To further investigate the differences between the ranking and classification paradigms, we develop another variant of \textsc{AdaNDV} by transforming the SO estimator as the estimator selection component in \textsc{AdaNDV}, and the method is named \textsc{AdaNDV}(C). The performance is shown in Table~\ref{tab:flex}, and we can derive the following conclusion. {On the one hand,} {AdaNDV}(C) consistently outperforms all baseline estimators, demonstrating the necessity of treating the selection dilemma problem. {On the other hand,} \textsc{AdaNDV} surpasses \textsc{AdaNDV}(C) on all metrics. The two variants have identical estimator fusion components, indicating that selected estimators directly affect the ultimate estimations.



\begin{table}[t]
    \centering
    \caption{Performance comparison of different variants of \textsc{AdaNDV}.}
    \begin{tabular}{ccccccc}
\toprule
         & Mean & 50\% & 75\%& 90\% & 99\% \\
\midrule
    \textsc{AdaNDV} & {{1.62}} & {{1.22}}& {{1.60}} & {{2.34}} & {{6.79}} \\
    \textsc{AdaNDV}(C) & {1.86} & {1.34} & {1.98}  & {3.04} & {8.42} \\
    \textsc{AdaNDV}{(base}-EB{)} & {1.64} & 1.24 & 1.62 & {2.39} & {7.11}\\
    \textsc{AdaNDV}{(base}+LS$_\mathrm{general}${)} & 1.62 & 1.21 & 1.58 & 2.33 & {7.18} \\
    \textsc{AdaNDV}(Hypo) & 1.18 & {1.10} & 1.24 & {1.43} & {2.20} \\
\bottomrule
    \end{tabular}
    \label{tab:flex}
\end{table}


We use the metric \textit{Precision@K} (P@K) to evaluate the accuracy {of estimator selection}, representing the partition of the optimal estimator(s) amongst the top-K estimators, and the results are shown in Figure~\ref{fig:cmp-paradigm}. We can observe the model trained in the ranking paradigm exhibits substantially higher P@1 and P@2 than that trained in the classification paradigm.
The sensitive analysis results in Figure~\ref{fig:hyper-all} show that the ultimate performance is closely related to the top estimators and we set $k$ as 2 for \textsc{AdaNDV}, so P@1 and P@2 of leading estimator selection directly affect the performance of our exploitation. 







\noindent\textbf{{Selected Estimators for Fusion.}}
{We summarize the fusion scenarios of over/under estimation properties of the 2$k$ ($k=2$) selected estimators in Figure~\ref{fig:cases}. The term ``over and under'' indicates that both overestimated and underestimated results exist among the 2$k$ estimators, meaning their weighted sum can encompass the ground truth. Conversely, ``only over'' and ``only under'' refer to the selected estimators that exhibit only overestimation or underestimation, respectively. In most test cases, \textsc{AdaNDV} chose both overestimated and underestimated results, leveraging the properties of overestimation and underestimation to reduce errors in the fusion process. 
In addition, the P@2 for estimator selection is 71\%, as shown in Figure \ref{fig:cmp-paradigm}, indicating that the selected overestimating and underestimating estimators are also among the top estimators.
This demonstrates the effectiveness of the top-$k$ selection strategy of \textsc{AdaNDV}.}





\noindent\textbf{Adaptivity of Base Estimators.} \textsc{AdaNDV} contains fourteen traditional base estimators and it is adaptive to add or remove base estimators. In addition, our method is available for learned estimators. To show the flexibility of base estimators in \textsc{AdaNDV}, we respectively remove the EB estimator and add the LS$_\mathrm{general}$ estimator in \textsc{AdaNDV}. The two variants are respectively named \textsc{AdaNDV}{(base}-EB{)} and \textsc{AdaNDV}{(base}+LS$_\mathrm{general}${)}, and their performance is demonstrated in Table~\ref{tab:flex}. We can derive the following conclusions based on the results: {(1)} \textsc{AdaNDV}{(base}-EB{)} and \textsc{AdaNDV}{(base}+LS$_\mathrm{general}${)} consistently outperform the individual estimators within their base estimator sets, which indicates the effectiveness of our method remains in changing the base estimators. {(2)}  Removing EB results in a consistent performance decline, indicating that estimators that are frequently selected, such as EB, play a crucial role in maintaining the overall effectiveness of the model. Adding LS$_\mathrm{general}$ will not consistently improve the performance: some metrics have decreased, while others have increased. The possible reason is that the leading estimator is relative, based on the current set of base estimators. 
Changing the base estimators will affect estimator selection.

Figure~\ref{fig:cmp-paradigm} has shown that accurately identifying the optimal base estimator from the set of base estimators is a challenging task. In the subsequent study, we explore the potential impact on our estimation performance by considering the hypothetical scenario in which this challenge is addressed.





\noindent\textbf{Upper Bound {of Estimator Selection}.} 
To further investigate the upper bound of the estimator selection component, we suppose we can exploit the optimal estimators, where P@1 is {100\%}. This variant of the model is denoted as \textsc{AdaNDV}(Hypo), and its performance is shown in Table~\ref{tab:flex}. The results can derive the following findings: {(1)} If we can accurately select the optimal estimator with the lowest overestimated and underestimated q-error, the performance of \textsc{AdaNDV} will significantly improve. It shows the promising performance upper bound of our \textsc{AdaNDV} framework.
{(2)} Compared to the Hypo-optimal estimator in Table~\ref{tab:ideal-traditional}, the performance of \textsc{AdaNDV}(Hypo) is better in most metrics except for 50\% quantile q-error. It further demonstrates the effectiveness of \textsc{AdaNDV} and indicates that our proposed complementary estimator exploitation strategy is beneficial.


The P@1 of \textsc{AdaNDV} stands at {53\%}, indicating that there is significant scope for enhancement within our proposed method. It is feasible to explore the estimator selection methods beyond the classification and ranking paradigms in the future. 




\begin{figure}[t]
    \centering
    \begin{subfigure}{0.21\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/paradigm_cmp.png}
    \caption{Selection precisions.}
    \label{fig:cmp-paradigm}
     \end{subfigure}
     \begin{subfigure}{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/cases.png}
         \caption{{Type of selected estimators.}}\label{fig:cases}
     \end{subfigure}
     \caption{{Analysis of estimator selection.}}\label{fig:impact}
\end{figure}





\subsection{{Further Evaluation}}\label{sec:dataanalysis}
{
In this section, we show the performance of our method under more evaluation scenarios.}

\noindent\textbf{{Performance on Artificial Distributions.}} { \textsc{AdaNDV} is trained and evaluated on the data distributions from the real world, as illustrated in Section~\ref{sec:exp-settings}. However, most previous works are evaluated on artificial data distributions, so it is not clear will baseline learned methods outperform \textsc{AdaNDV} with artificial data distribution. We conduct experiments on Zipfian distribution with skew factors ($s$) 1.2, 1.5, and 2.0, consistent with previous work~\cite{li2022sampling}. We freeze the parameters of LS$_{\mathrm{scratch}}$, LS$_{\mathrm{FT}}$, and \textsc{AdaNDV} trained on the TabLib training set, and evaluate them when sampling 1\% of data from Zipfian distributions with data size of 1e5 and 1e6. The results are shown in Table~\ref{tab:zipf} and we can draw the following conclusions. No single estimator achieves optimal results under the Zipfian distributions with different skew factors and \textsc{AdaNDV} consistently beats LS$_{\mathrm{general}}$, demonstrating that our method does not fail on standard artificial distributions. Besides, LS$_{\mathrm{general}}$ is pre-trained on an artificial dataset containing 7.2$\times 10^5$ data points~\cite{ls_wu2022learning}, in which the original columns follow specific data distributions, but it processes
the worst performance in each metric. This demonstrates the effectiveness of training models on real-world data. 
}

\begin{table}[t]
    \centering
    \caption{{Q-error of learned estimators when sampling 1\% data from Zipfian distribution with skew factors ($s$) of \{1.2, 1.5, 2.0\} with column size ($N$) of 1e5 and 1e6.}}
    \label{tab:zipf}
    \begin{tabular}{c|ccc|cccccc}
\toprule
       $N$ & \multicolumn{3}{c}{1e5} & \multicolumn{3}{c}{1e6} \\
    $s$  &  1.2 & 1.5 & 2.0  &  1.2 & 1.5 & 2.0    \\
\hline
    LS$_{\mathrm{general}}$ & 5.14& 2.93& 2.47& 3.76& 5.94& 2.5 \\
    LS$_{\mathrm{scratch}}$  & \textbf{1.27}& 1.47& 1.36& 1.53& \textbf{2.67}& \textbf{1.06}  \\
    LS$_{\mathrm{FT}}$ & 1.62& 1.56& \textbf{1.28}& 1.23& 2.93& 1.15 \\
    \textsc{AdaNDV} & 1.77& \textbf{1.20}& 2.36& \textbf{1.13}& 5.86& 1.75\\

\bottomrule
    \end{tabular}
\end{table}






\noindent\textbf{{Performance under Different Sampling Rates.}} 
{
We depict the performance of 75\% quantile q-error of \textsc{AdaNDV}, base estimators, and the representative learned baseline LS$_{\mathrm{general}}$ under different sampling rates in Figure~\ref{fig:samplingrates}. \textsc{AdaNDV} shows consistent performance improvement with increasing sample rates, while some base estimators decline in performance, possibly due to practical scenarios not aligning with their assumptions. Besides, the advantages of \textsc{AdaNDV} persist across different sample rates, demonstrating that it is not sensitive to the variations of base estimators.
}


