\section{Methodology}\label{sec:methodology}
\subsection{Model Architecture}\label{sec:architecture}

\subsubsection{Overview of \textsc{AdaNDV}}\label{sec:overview-base}
The architecture of \textsc{AdaNDV} is shown in Figure~\ref{fig:model_pipeline} and there are four components.


\noindent (1) Base estimators collection. We collect fourteen representative statistical estimators as our base estimators, detailed in Section \ref{sec:exp-settings}. We do not include learned estimators due to the limited availability of such estimators and nontrivial overhead associated with training them. Additionally, statistical estimators offer higher efficiency and are free of training.

\noindent (2) Leading estimator selection. ``OverEst'' and ``UnderEst'' represent overestimation and underestimation. Estimator selection is designed to prioritize the base estimators by their overestimation and underestimation errors. Essentially, the prioritization of a set of base estimators entails assigning them scores that reflect their performance in terms of overestimation and underestimation errors. 
Specifically, we use two identical learned models with different training objectives to prioritize the two types of estimators. The loss functions of the two models are denoted as $\mathcal{L}_{\mathrm{over}}$ and $\mathcal{L}_{\mathrm{under}}$, respectively. This component will be detailed in Section~\ref{sec:leading}.


\noindent (3) Estimator fusion. We first select estimators with the top-$k$ overestimated and underestimated performance, denoted as $\mathcal{I}^{\mathrm{over}}$ and $\mathcal{I}^{\mathrm{under}}$. Next, we use a learned model to assign weights to each selected estimator, and then employ a weighted sum to compute the NDV. 
For instance, suppose $k$ is 1, the ground truth NDV $D$ is 10,000, and the estimation results of the selected estimators are $\hat{D}_1=11,000$ and $\hat{D}_2=9,000$, our estimation is formulated as $\hat{D}=\Lambda_1\hat{D}_1+\Lambda_2\hat{D}_2, 0\leq \Lambda_1,\Lambda_2\leq 1, \Lambda_1+\Lambda_2=1$. This fusion method can reduce estimation errors based on existing estimators.
The loss function of this component is denoted as $\mathcal{L}_{\mathrm{est}}$ and its details will be elaborated in Section~\ref{sec:exploit}.

\noindent (4) Model training. There are three objectives in \textsc{AdaNDV}, and we can derive the end-to-end loss function to train our method:

\begin{align}
    \mathcal{L}_{\mathrm{\textsc{AdaNDV}}}=\mathcal{L}_{\mathrm{over}}+\mathcal{L}_{\mathrm{under}}+\beta \mathcal{L}_{\mathrm{est}},
    \label{eq:loss}
\end{align}
where $\beta$ is a hyperparameter that modulates the trade-offs between different kinds of training objectives. Our proposed method can be trained by minimizing the $\mathcal{L}_{\mathrm{\textsc{AdaNDV}}}$ loss function.



\noindent\textbf{Training pipeline.}
\textcircled{1}All base estimators use the sample data to estimate NDV. \textcircled{2}It is straightforward to distinguish the overestimated and underestimated estimators on the training data, and then we construct training \underline{labels} $y^{\mathrm{over}}$ and $y^{\mathrm{under}}$. \textcircled{3}-\textcircled{4}The estimator selection models will respectively generate the \underline{scores} that prioritize the estimators by the predicted overestimation and underestimation performance. $\mathcal{S}^{\mathrm{over}}\in\mathbb{R}^m$ represents the scores based on overestimation, where $m$ is the number of base estimators, with a higher value indicating better overestimation performance. $\mathcal{S}^{\mathrm{under}}\in\mathbb{R}^m$ is similar to $\mathcal{S}^{\mathrm{over}}$.
\textcircled{5}The \underline{selection loss functions} ($\mathcal{L}_{\mathrm{over}}$ and $\mathcal{L}_{\mathrm{under}}$) of the two models take the scores and labels as input. \textcircled{6}Then we respectively select top base estimators with high scores and the \underline{selected estimators} are $\mathcal{I}^{\mathrm{over}}$ and $\mathcal{I}^{\mathrm{under}}$. \textcircled{7}The learned estimator weighter takes the sample data and the estimations of the selected estimators as input and predicts the \underline{weight} ($\Lambda$). \textcircled{8}-\textcircled{9}Finally, we employ a weighted sum on the estimation results of the selected estimators to fuse them into the \underline{ultimate NDV estimation} $\hat{D}$, \textcircled{\footnotesize 10}deriving \underline{fusion-based estimation loss function} $\mathcal{L}_{\mathrm{est}}$.


\noindent\textbf{Inference pipeline.} \textcircled{1}-\textcircled{2}The learned leading estimator selection models take the sample data as input to generate the scores of each estimator. \textcircled{3}Then, we select the top estimators with the highest scores for overestimation and underestimation, respectively. \textcircled{4}Next, we input the sample data and the estimations of the selected estimator into the learned estimator weighter to obtain the weights. \textcircled{5}-\textcircled{6}Finally, the ultimate estimation is fused by a weighted sum on the estimations of the selected estimators.




\begin{figure}[t]
    \centering
    \includegraphics[width=0.96\linewidth]{figures/property.pdf}
    \caption{Intuition behind leveraging the properties of overestimation and underestimation.}
    \label{fig:properties}
\end{figure}


\subsubsection{Properties of overestimation and underestimation}\label{sec:motivation}
We illustrate the intuition behind our method in leveraging the properties of overestimation and underestimation in Figure~\ref{fig:properties}. Each circle in the figure represents the estimation result from a base estimator. The ground truth, indicated by a vertical bar in the middle, separates these results into two sections: overestimation and underestimation, which are marked with two different colors.
The performance of selecting the optimal estimator from existing ones (as shown in Figure~\ref{fig:properties}b) is challenging and limited by both selection accuracy and the base estimators. 
While it is straightforward to ensemble estimators to alleviate these issues, selecting estimators with the lowest errors and ensemble them may not bring performance improvement. Figure~\ref{fig:properties}c illustrates such a scenario where two overestimation results are selected, and the corresponding codomain cannot cover the ground truth bar.
Therefore, differentiating the results into overestimations and underestimations is essential for improving the performance. Although the estimation errors of the selected results may be substantial, complementing overestimations and underestimations enables their weighted sum to robustly encompass the ground truth, as shown in Figure~\ref{fig:properties}d. This allows the model to learn a set of parameters, resulting in the weighted sum of the selected results performing better than any individual estimator, potentially even approaching the ground truth.











\subsection{Leading Estimator Selection}\label{sec:leading}
We first describe the features extracted from the sample data, then we comprehensively illustrate the ranking paradigm used in estimator selection, and finally, we delineate the construction of the objective function for this component.



\subsubsection{Feature Engineering} \label{sec:fe}
Formally, we denote the features extracted from the sample data as $x\in\mathbb{R}^{ H}$, where $H$ denotes the number of dimensions in the feature space. Frequency profiles $f$ are widely used features, but their sizes varies across different column test cases. Since a learned model needs a fixed number of input features, we apply a cut-off to the frequency profile, similar to previous works~\cite{ls_wu2022learning,li2024learning}. This operation is based on the assumption that the predictive power of $f_i$ decreases as $i$ increases, an assumption that is widely used in previous works~\cite{ls_wu2022learning,li2024learning,chao1984nonparametric,gee_charikar2000towards,hybskew_haas1995sampling}. In addition, according to Section \ref{sec:statement}, cutting off the frequency profile will make computing the number of sample data $n$ and the NDV of sample data $d$ inapplicable. Therefore, we use $n$ and $d$ as directly as features. Furthermore, we include the original column size $N$ as another feature.

Since the length of input features must be a fixed number $H$, if the size of $f$ is smaller than the required length, we pad it with zeros. Specifically, the input feature is formulated as:

\begin{align}
    x = [f_1, f_2, \cdots, f_{H-3}, \log n, \log d, \log N].
\end{align}
We apply the logarithm operation on $n$, $d$, and $N$ to mitigate the skewness of input features.





\subsubsection{Estimator Ranking} 
The ranking paradigm has demonstrated substantial superiority in solving item prioritization tasks~\cite{liu2009learning,bruch2019revisiting,listmle_xia2008listwise,TensorflowRankingKDD2019,wang2018lambdaloss}. We adapt SOTA ranking techniques in estimator selection, which encompasses the elaborated construction of ranking labels and the training of the learned ranker.



\noindent\textbf{Ranking Label Construction.} Constructing the ranking label is a complex task~\cite{wang2013theoretical} due to the potentially infinite number of label values, even when the ranking order is fixed. For instance, given two estimators $e_1$ and $e_2$, where $e_1$ has a lower q-error. We denote $e_1\succ e_2$, indicating that $e_1$ is better than $e_2$. Let $y_1$ and $y_2$ be the ranking labels. Any values of $y_1$ and $y_2$ that satisfy the condition $y_1>y_2$ are eligible to serve as ranking labels, because the ranking paradigm focuses solely on the relative orders rather than specific values. 

There are no predefined estimator ranking labels available, and the labels in the estimator selection context require the following attributes: (1) A \textit{higher} value of the label reflects the \textit{higher} priority, while also indicating a \textit{lower} q-error in NDV estimation; (2) The value domain of the label needs to be constrained, as applying ranking techniques usually involves exponentiation operation on the label values~\cite{liu2009learning}. Unconstrained values could potentially lead to computational overflow; (3) The label needs to distinctly differentiate between overestimated and underestimated estimators, as required by our designed objectives. 

To this end, we propose an efficient ranking label construction strategy. Firstly, we use the ranking position to constrain the value of labels to be no greater than $m$. Then, we construct the labels by reversing the ranking positions based on the lowest overestimated or underestimated q-error. Finally, we mask the overestimated or underestimated estimators to differentiate them. Through these steps, the constructed labels satisfy the required attributes.


\begin{algorithm}[t]
\SetAlgoLined
\KwIn{$\hat{\mathcal{D}},D$}
\KwOut{$y^{\mathrm{over}}$}
 \textit{UnderEstSet} $\xleftarrow{} \emptyset$; $i \xleftarrow{} 1$; $\hat{\mathcal{D}}_{\mathrm{max}}\xleftarrow{}\max_{1\leq j \leq m}\hat{\mathcal{D}}_j$;\\
 \For{$i\xleftarrow{}1;i\leq m;i\xleftarrow{}i+1$}{
    \If{$\hat{\mathcal{D}}_i\leq D$}{
   \textit{UnderEstSet} $\xleftarrow{} i$ \; 
   $\hat{\mathcal{D}}_i\xleftarrow{}\hat{\mathcal{D}}_i + \hat{\mathcal{D}}_{\mathrm{max}} $; \\
   }
 }
\For{$i\xleftarrow{}1;i\leq m;i\xleftarrow{}i+1$} {
$y^{\mathrm{over}}_i\xleftarrow{}m-\pi_{\hat{\mathcal{D}}_i}$;   \\
}
\For{$i$ in UnderEstSet} {
$y^{\mathrm{over}}_i\xleftarrow{} 0$; // mask the underestimate estimators
}
 \textbf{return} $y^{\mathrm{over}}$\;
 \caption{Overestimation ranking label construction.}\label{algo:over_label}
\end{algorithm}









To prioritize the estimators with low overestimation q-errors, the process of constructing ranking labels $y^{\mathrm{over}}$ is shown in Algorithm~\ref{algo:over_label}. Specifically, we first record the maximum estimated result among all base estimators as $\hat{\mathcal{D}}_{\mathrm{max}}$. Then, to separate underestimations from overestimations, we add $\hat{\mathcal{D}}_{\mathrm{max}}$ to the result of each underestimated estimator to ensure that their estimations are higher than those of any overestimated estimators. Next, we obtain the ranking position $\pi$ of each base estimator by sorting the estimation results, where $\pi_{\hat{\mathcal{D}}}=\operatorname{argsort}(\hat{\mathcal{D}})$. Specifically, the $\operatorname{argsort}$ operation yields the indices required to sort the data in ascending order, and $\pi_{\hat{\mathcal{D}_i}}$ represents the index of $\hat{\mathcal{D}_i}$. In the context of overestimation, the transformed estimation is considered better when it is smaller. Therefore, the estimator with better overestimation performance has a higher value of the ranking label $y_i^{\mathrm{over}}$ by reversing its ranking position. Finally, we mask the underestimated estimators by setting their ranking labels as zero to differentiate them from the overestimated ones. 

The process of constructing labels $y^{under}$ is similar to that of $y^{over}$, and we omit it for conciseness.


\noindent\textbf{Train the Learned Ranker.} 
We use a multi-layer perceptron (MLP) as the backbone of our ranking model: $\mathcal{S}=\mathrm{MLP}(x)$, where $\mathcal{S}\in\mathbb{R}^m$ represents the ranking scores of the estimators. The higher score indicates the higher priority of the corresponding estimator. We adapt the SOTA ranking techniques~\cite{bruch2019revisiting,listmle_xia2008listwise,TensorflowRankingKDD2019,wang2018lambdaloss} into estimator selection to train the ranking models. In short, the learned ranker can be trained by:

\begin{align}
\begin{aligned}
    \mathcal{L}_{\mathrm{rank}}(\mathcal{S},y)&= - \sum_{i=1}^m\frac{2^{y_i}-1}{\log_2(1+\pi(i))} ,\\
    \pi(i)&=1+\sum_{j,j\ne i}^m\frac{1}{1+e^{-\alpha(\mathcal{S}_j-\mathcal{S}_i)}},
\end{aligned}
    \label{eq:ranking}
\end{align}
where $\alpha$ is the hyperparameter in the training framework, $\mathcal{L}_{\mathrm{rank}}$ is the estimator ranking loss function, and $y$ is the ranking label. The learned ranker can similarly achieve different objectives by assigning the labels constructed by Algorithm~\ref{algo:over_label}. 





\subsubsection{Leading Estimator Selection} In this subsection, we show the training objectives of complementary estimator selection of this component.



\noindent\textbf{Overestimated Estimators Selection.}
We construct the training labels $y^{\mathrm{over}}$ according to Algorithm~\ref{algo:over_label}, and we use an MLP that has two hidden layers with 128 and 64 dimensions to compute the ranking scores $\mathcal{S}^{\mathrm{over}}$. The loss function for overestimated estimator selection is:

\begin{align}
    \mathcal{L}_{\mathrm{over}}=\frac{1}{\mathcal{N}}\sum_{i=1}^\mathcal{N}\mathcal{L}_{\mathrm{rank}}({\mathcal{S}_i^{\mathrm{over}}},y^{\mathrm{over}}_i),
    \label{eq:loss-over}
\end{align}
where $\mathcal{N}$ is the number of training samples, and $\mathcal{L}_{\mathrm{rank}}$ is the loss function defined in Equation (\ref{eq:ranking}). 



\noindent\textbf{Underestimated Estimators Selection.} Similarly, we use another MLP with the identical model architecture to compute $\mathcal{S}^{\mathrm{under}}$ and the training labels $y^{\mathrm{under}}$.  The loss function for underestimated estimator selection is:

\begin{align}
    \mathcal{L}_{\mathrm{under}}=\frac{1}{\mathcal{N}}\sum_{i=1}^\mathcal{N}\mathcal{L}_{\mathrm{rank}}({\mathcal{S}_i^{\mathrm{under}}},y^{\mathrm{under}}_i).
    \label{eq:loss-under}
\end{align}










\subsection{Estimator Fusion}\label{sec:exploit}
\subsubsection{Feature Engineering} The features described in Section~\ref{sec:fe} are also used in this component. Moreover, we incorporate the estimated results of the chosen base estimators as the additional features. 

Specifically, the leading estimator selection component provides the priority scores $\mathcal{S}^{\mathrm{over}}$ and $\mathcal{S}^{\mathrm{under}}$of the base estimators. We select $k$ estimators with the top-$k$ highest scores for both both overestimated and underestimated estimators. The chosen estimators are denoted as $\mathcal{I}^{\mathrm{over}}=\operatorname{argmax}_k\mathcal{S}^{\mathrm{over}}$ and $\mathcal{I}^{\mathrm{under}}=\operatorname{argmax}_k\mathcal{S}^{\mathrm{under}}$. The features are defined as $x^\prime=[x,\hat{\mathcal{D}}|_{\mathcal{I}^{\mathrm{over}}},\hat{\mathcal{D}}|_{\mathcal{I}^{\mathrm{under}}}],x^\prime\in\mathbb{R}^{H+2k}$.



\subsubsection{Estimator Fusion} \label{sec:fusion}
We use an MLP to compute the weights for the chosen base estimators: $\Lambda=\mathrm{MLP}(x^\prime)$, where $\Lambda\in\mathbb{R}^{2k}$ is the weight vector concatenated by two $k$-dimentional vectors corresponding to the chosen leading estimators. $\Lambda$ is not fixed but depends on sample data and selected estimators. To restrict the output of estimated NDV, we limit $\sum_{j=1}^{2k}\Lambda_j=1$ and estimate NDV by exploiting the base estimators:

\begin{align}
    \log \hat{D}=\sum_{j=1}^k(\Lambda_j\cdot\log\hat{\mathcal{D}}|_{\mathcal{I}^\mathrm{over}_j} + \Lambda_{k+j}\cdot\log\hat{\mathcal{D}}|_{\mathcal{I}^\mathrm{under}_j}),
    \label{eq:logd}
\end{align}
where $\hat{\mathcal{D}}|_{\mathcal{I}^\mathrm{over}_j}$ is the estimated NDV of the $j$-th chosen overestimated base estimator, and the output estimated NDV $\hat{D}=e^{\log \hat{D}}$. 
The logarithm is applied to limit the estimation of base estimators so that they do not exceed the range of a 32-bit floating-point number, thus preventing potential impacts on model training.
The learned model solely generates the weight vector, different from previous works that directly estimate NDV.





\subsubsection{Fusion Component Training} Denote $D_i$ as the ground truth NDV of the $i$-th training sample, and the learned model can be trained using:

\begin{align}
    \mathcal{L}_{\mathrm{est}}=\frac{1}{\mathcal{N}}\sum_{i=1}^\mathcal{N}(\log \hat{D}_i-\log D_i)^2+\lambda||W||_2,
    \label{eq:loss-est}
\end{align}
where we apply $L_2$ regularization on model parameters $W$ for better generalization. The regularization parameter $\lambda$ is tuned based on the validation loss.



