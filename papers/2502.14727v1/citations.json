[
  {
    "index": 0,
    "papers": [
      {
        "key": "koizumi2020audiocaptioningusingpretrained",
        "author": "Yuma Koizumi and Yasunori Ohishi and Daisuke Niizumi and Daiki Takeuchi and Masahiro Yasuda",
        "title": "Audio Captioning using Pre-Trained Large-Scale Language Model Guided by Audio-based Similar Caption Retrieval"
      },
      {
        "key": "zhao-etal-2023-generating",
        "author": "Zhao, Jinming  and\nHaffari, Gholamreza  and\nShareghi, Ehsan",
        "title": "Generating Synthetic Speech from {S}poken{V}ocab for Speech Translation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "huang2023makeanaudiotexttoaudiogenerationpromptenhanced",
        "author": "Rongjie Huang and Jiawei Huang and Dongchao Yang and Yi Ren and Luping Liu and Mingze Li and Zhenhui Ye and Jinglin Liu and Xiang Yin and Zhou Zhao",
        "title": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "gonzales-rudzicz-2024-retrieval",
        "author": "Gonzales, Robie  and\nRudzicz, Frank",
        "title": "A Retrieval Augmented Approach for Text-to-Music Generation"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "elizalde2022claplearningaudioconcepts",
        "author": "Benjamin Elizalde and Soham Deshmukh and Mahmoud Al Ismail and Huaming Wang",
        "title": "CLAP: Learning Audio Concepts From Natural Language Supervision"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "liu2023universalvisionlanguagedenseretrieval",
        "author": "Zhenghao Liu and Chenyan Xiong and Yuanhuiyi Lv and Zhiyuan Liu and Ge Yu",
        "title": "Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "wei2023uniirtrainingbenchmarkinguniversal",
        "author": "Cong Wei and Yang Chen and Haonan Chen and Hexiang Hu and Ge Zhang and Jie Fu and Alan Ritter and Wenhu Chen",
        "title": "UniIR: Training and Benchmarking Universal Multimodal Information Retrievers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "min2025speechretrievalaugmentedgenerationautomatic",
        "author": "Do June Min and Karel Mundnich and Andy Lapastora and Erfan Soltanmohammadi and Srikanth Ronanki and Kyu Han",
        "title": "Speech Retrieval-Augmented Generation without Automatic Speech Recognition"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "behnamghader2024llm2veclargelanguagemodels",
        "author": "Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy",
        "title": "LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders"
      },
      {
        "key": "lee2025nvembedimprovedtechniquestraining",
        "author": "Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping",
        "title": "NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jiang2024e5vuniversalembeddingsmultimodal",
        "author": "Ting Jiang and Minghui Song and Zihan Zhang and Haizhen Huang and Weiwei Deng and Feng Sun and Qi Zhang and Deqing Wang and Fuzhen Zhuang",
        "title": "E5-V: Universal Embeddings with Multimodal Large Language Models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "jiang2025vlm2vectrainingvisionlanguagemodels",
        "author": "Ziyan Jiang and Rui Meng and Xinyi Yang and Semih Yavuz and Yingbo Zhou and Wenhu Chen",
        "title": "VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zhang2024gmeimprovinguniversalmultimodal",
        "author": "Xin Zhang and Yanzhao Zhang and Wen Xie and Mingxin Li and Ziqi Dai and Dingkun Long and Pengjun Xie and Meishan Zhang and Wenjie Li and Min Zhang",
        "title": "GME: Improving Universal Multimodal Retrieval by Multimodal LLMs"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "chu2024navigate",
        "author": "Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting",
        "title": "Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others",
        "title": "Chain-of-thought prompting elicits reasoning in large language models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large language models are zero-shot reasoners"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "yu2024visrag",
        "author": "Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others",
        "title": "Visrag: Vision-based retrieval-augmented generation on multi-modality documents"
      }
    ]
  }
]