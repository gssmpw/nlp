\section{Related Works}
\paragraph{Audio RAG.}
While Retrieval-Augmented Generation (RAG) has shown promise in audio-related tasks like captioning**Liu, "Unsupervised Audio Captioning"**, text-to-audio generation**Huang, "Text-to-Speech Synthesis with Transformers"**, and music generation**Yang, "Music Generation from Text Descriptions"**. However, while these efforts demonstrate the utility of retrieval in audio processing,  prior work primarily utilizes retrieval to enhance specific, isolated tasks with limited exploration of how retrieval-augmented techniques can benefit spoken dialogue models. Audio information itself carries rich semantic and acoustic imformation that can improve retrieval grounding, enhance response contextualization, and strengthen factual consistency. WavRAG, in contrast, integrates retrieval as a core component of a complete dialogue system. The combination of general audio support and end-to-end integration distinguishes WavRAG and represents a significant advancement towards truly audio-native, retrieval-augmented spoken dialogue systems.

\paragraph{Multimodal Retrieval.}
The increasing prevalence of multimedia applications and Retrieval-Augmented Generation (RAG) systems, fueled by Multimodal Language Models (MLLMs), has underscored the necessity for unified retrieval frameworks capable of managing diverse modalities. Traditional cross-modality retrieval methods often rely on pre-trained models such as CLAP**Radford, "Learning Transferable Visual Models"** and CLIP**Radford, "Learning Transferable Visual-Semantic Representations"**, which use separate encoders for text and other modalities (e.g., **Alayfar, "Unified Embeddings for Multi-Modal Retrieval"** and **Chen, "Unified Multimodal Pre-training for Image and Video Understanding"**). Other approaches enhance pre-trained text embeddings with audio encoders**Tian, "Audio-Text Embedding Alignment via Self-Supervised Learning"**, but these often prioritize the semantic content of speech, overlooking important general audio. Such methods struggle to effectively capture the full spectrum of information from both speech and non-speech audio.
Recent advancements have highlighted the potential of Large Language Models (LLMs) and Supervised Fine-Tuning (SFT) for creating powerful, unified text representations**Li, "Unified Text Pre-training with Multi-Task Learning"**. This methodology has been successfully extended to other modalities, with works like **Zhou, "Efficient Visual-Language Pre-training via Adaptive Knowledge Distillation"** and **Chen, "Visual-Linguistic Multimodal Transformers for Image Captioning"** focusing on fine-tuning strategies for visual models. Furthermore, Zhang's research**Zhang, "Universal Multimodal Retrieval with Language Models"** demonstrates the feasibility of developing universal multimodal retrieval models using MLLMs. However, there has been limited exploration in the audio modality, prompting us to propose the first end-to-end audio-text multimodal retriever.

% \paragraph{Chain-of-Thought Reasoning} 
% For complex problems involving natural language understanding or reasoning, large language models often struggle to provide correct answers directly. The Chain-of-Thought (CoT) reasoning approach **Mihaylov, "Exploring the Frontiers of Common Sense Reasoning"** addresses this challenge by prompting models to explicitly generate intermediate reasoning steps, leading to a more reliable final answer.
% % CoT has been shown to significantly enhance the reasoning capabilities of LLMs, particularly in arithmetic, commonsense reasoning, and logical deduction **Vijayakumar, "Improving Common Sense Reasoning with Chain-of-Thought"**. CoT techniques are typically divided into manually constructed and automatically generated approaches. 
% Manually constructed CoT **Liu, "Manually Constructed Chain-of-Thought for Language Understanding Tasks"** relies on carefully designed prompt examples, which typically result in high-quality reasoning chains but come with substantial human labor costs. On the other hand, automatically generated CoT **Wang, "Automatically Generated Chain-of-Thought for Language Understanding Tasks"** leverages specific prompt texts to induce models to generate reasoning chains without examples. While more scalable, the automatic method often suffers from lower quality, leading to severe hallucination issues. 
% In the multimodal domain, several works have extended CoT reasoning to multi-modal chain-of-thought (MCoT) reasoning to enhance the performance of vision-language models (VLMs) **Kang, "Multi-Modal Chain-of-Thought Reasoning for Image Captioning"**. However, in the context of spoken dialogue systems, no exploration of CoT methodologies has been conducted to date, which represents a notable gap in the field of general audio understanding and reasoning. In our WavRAG, we further introduce CoT reasoning during the generation stage to more effectively leverage the retrieved external knowledge and the original audio‚Äêtext inputs, which let the model generate answers grounded in an external knowledge base while explicitly revealing its reasoning process.
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{image/fig2-2.png}

    \caption{Architecture of the WavRAG framework. Top: Traditional RAG pipeline using ASR, highlighting its limitations. Bottom: WavRAG's four-step process: (1) A dual-modality encoder creates embeddings for both audio and text queries; (2) Top-K documents are retrieved from an audio-text knowledge base using cosine similarity; (3) A chain-of-thought reasoning process analyzes the retrieved information; (4) A large language model generates the final response, grounded in the retrieved knowledge.}
    \label{fig:img2}
\end{figure*}