@misc{behnamghader2024llm2veclargelanguagemodels,
      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}, 
      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},
      year={2024},
      eprint={2404.05961},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.05961}, 
}

@inproceedings{chu2024navigate,
  title={Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future},
  author={Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1173--1203},
  year={2024}
}

@misc{elizalde2022claplearningaudioconcepts,
      title={CLAP: Learning Audio Concepts From Natural Language Supervision}, 
      author={Benjamin Elizalde and Soham Deshmukh and Mahmoud Al Ismail and Huaming Wang},
      year={2022},
      eprint={2206.04769},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2206.04769}, 
}

@inproceedings{gonzales-rudzicz-2024-retrieval,
    title = "A Retrieval Augmented Approach for Text-to-Music Generation",
    author = "Gonzales, Robie  and
      Rudzicz, Frank",
    editor = "Kruspe, Anna  and
      Oramas, Sergio  and
      Epure, Elena V.  and
      Sordo, Mohamed  and
      Weck, Benno  and
      Doh, SeungHeon  and
      Won, Minz  and
      Manco, Ilaria  and
      Meseguer-Brocal, Gabriel",
    booktitle = "Proceedings of the 3rd Workshop on NLP for Music and Audio (NLP4MusA)",
    month = nov,
    year = "2024",
    address = "Oakland, USA",
    publisher = "Association for Computational Lingustics",
    url = "https://aclanthology.org/2024.nlp4musa-1.6/",
    pages = "31--36",
    abstract = "Generative text-to-music models such as MusicGen are capable of generating high fidelity music conditioned on a text prompt. However, expressing the essential features of music with text is a challenging task. In this paper, we present a retrieval-augmented approach for text-to-music generation. We first pre-compute a dataset of text-music embeddings obtained from a contrastive language-audio pretrained encoder (CLAP). Then, given an input text prompt, we retrieve the top $k$ most similar musical aspects and augment the original prompt. This approach consistently generates music of higher audio quality as measured by the Frech{\'e}t Audio Distance. We analyze the internal representations of MusicGen and find that augmented prompts lead to greater diversity in token distributions and display high text adherence. Our findings show the potential for increased control in text-to-music generation."
}

@misc{huang2023makeanaudiotexttoaudiogenerationpromptenhanced,
      title={Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models}, 
      author={Rongjie Huang and Jiawei Huang and Dongchao Yang and Yi Ren and Luping Liu and Mingze Li and Zhenhui Ye and Jinglin Liu and Xiang Yin and Zhou Zhao},
      year={2023},
      eprint={2301.12661},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2301.12661}, 
}

@misc{jiang2024e5vuniversalembeddingsmultimodal,
      title={E5-V: Universal Embeddings with Multimodal Large Language Models}, 
      author={Ting Jiang and Minghui Song and Zihan Zhang and Haizhen Huang and Weiwei Deng and Feng Sun and Qi Zhang and Deqing Wang and Fuzhen Zhuang},
      year={2024},
      eprint={2407.12580},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12580}, 
}

@misc{jiang2025vlm2vectrainingvisionlanguagemodels,
      title={VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks}, 
      author={Ziyan Jiang and Rui Meng and Xinyi Yang and Semih Yavuz and Yingbo Zhou and Wenhu Chen},
      year={2025},
      eprint={2410.05160},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.05160}, 
}

@misc{koizumi2020audiocaptioningusingpretrained,
      title={Audio Captioning using Pre-Trained Large-Scale Language Model Guided by Audio-based Similar Caption Retrieval}, 
      author={Yuma Koizumi and Yasunori Ohishi and Daisuke Niizumi and Daiki Takeuchi and Masahiro Yasuda},
      year={2020},
      eprint={2012.07331},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2012.07331}, 
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@misc{lee2025nvembedimprovedtechniquestraining,
      title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models}, 
      author={Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
      year={2025},
      eprint={2405.17428},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17428}, 
}

@misc{liu2023universalvisionlanguagedenseretrieval,
      title={Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval}, 
      author={Zhenghao Liu and Chenyan Xiong and Yuanhuiyi Lv and Zhiyuan Liu and Ge Yu},
      year={2023},
      eprint={2209.00179},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2209.00179}, 
}

@misc{min2025speechretrievalaugmentedgenerationautomatic,
      title={Speech Retrieval-Augmented Generation without Automatic Speech Recognition}, 
      author={Do June Min and Karel Mundnich and Andy Lapastora and Erfan Soltanmohammadi and Srikanth Ronanki and Kyu Han},
      year={2025},
      eprint={2412.16500},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2412.16500}, 
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@misc{wei2023uniirtrainingbenchmarkinguniversal,
      title={UniIR: Training and Benchmarking Universal Multimodal Information Retrievers}, 
      author={Cong Wei and Yang Chen and Haonan Chen and Hexiang Hu and Ge Zhang and Jie Fu and Alan Ritter and Wenhu Chen},
      year={2023},
      eprint={2311.17136},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.17136}, 
}

@article{yu2024visrag,
  title={Visrag: Vision-based retrieval-augmented generation on multi-modality documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@misc{zhang2024gmeimprovinguniversalmultimodal,
      title={GME: Improving Universal Multimodal Retrieval by Multimodal LLMs}, 
      author={Xin Zhang and Yanzhao Zhang and Wen Xie and Mingxin Li and Ziqi Dai and Dingkun Long and Pengjun Xie and Meishan Zhang and Wenjie Li and Min Zhang},
      year={2024},
      eprint={2412.16855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.16855}, 
}

@inproceedings{zhao-etal-2023-generating,
    title = "Generating Synthetic Speech from {S}poken{V}ocab for Speech Translation",
    author = "Zhao, Jinming  and
      Haffari, Gholamreza  and
      Shareghi, Ehsan",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.147/",
    doi = "10.18653/v1/2023.findings-eacl.147",
    pages = "1975--1981",
    abstract = "Training end-to-end speech translation (ST) systems requires sufficiently large-scale data, which is unavailable for most language pairs and domains. One practical solution to the data scarcity issue is to convert text-based machine translation (MT) data to ST data via text-to-speech (TTS) systems. Yet, using TTS systems can be tedious and slow. In this work, we propose SpokenVocab, a simple, scalable and effective data augmentation technique to convert MT data to ST data on-the-fly. The idea is to retrieve and stitch audio snippets, corresponding to words in an MT sentence, from a spoken vocabulary bank. Our experiments on multiple language pairs show that stitched speech helps to improve translation quality by an average of 1.83 BLEU score, while performing equally well as TTS-generated speech in improving translation quality. We also showcase how SpokenVocab can be applied in code-switching ST for which often no TTS systems exit."
}

