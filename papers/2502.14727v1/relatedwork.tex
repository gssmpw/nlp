\section{Related Works}
\paragraph{Audio RAG.}
While Retrieval-Augmented Generation (RAG) has shown promise in audio-related tasks like captioning~\cite{koizumi2020audiocaptioningusingpretrained,zhao-etal-2023-generating}, text-to-audio generation~\cite{huang2023makeanaudiotexttoaudiogenerationpromptenhanced}, and music generation~\cite{gonzales-rudzicz-2024-retrieval}. However, while these efforts demonstrate the utility of retrieval in audio processing,  prior work primarily utilizes retrieval to enhance specific, isolated tasks with limited exploration of how retrieval-augmented techniques can benefit spoken dialogue models. Audio information itself carries rich semantic and acoustic imformation that can improve retrieval grounding, enhance response contextualization, and strengthen factual consistency. WavRAG, in contrast, integrates retrieval as a core component of a complete dialogue system. The combination of general audio support and end-to-end integration distinguishes WavRAG and represents a significant advancement towards truly audio-native, retrieval-augmented spoken dialogue systems.

\paragraph{Multimodal Retrieval.}
The increasing prevalence of multimedia applications and Retrieval-Augmented Generation (RAG) systems, fueled by Multimodal Language Models (MLLMs), has underscored the necessity for unified retrieval frameworks capable of managing diverse modalities. Traditional cross-modality retrieval methods often rely on pre-trained models such as CLAP~\cite{elizalde2022claplearningaudioconcepts} and CLIP, which use separate encoders for text and other modalities (e.g., UniVL-DR~\cite{liu2023universalvisionlanguagedenseretrieval} and UniIR~\cite{wei2023uniirtrainingbenchmarkinguniversal}). Other approaches enhance pre-trained text embeddings with audio encoders~\cite{min2025speechretrievalaugmentedgenerationautomatic}, but these often prioritize the semantic content of speech, overlooking important general audio. Such methods struggle to effectively capture the full spectrum of information from both speech and non-speech audio.
Recent advancements have highlighted the potential of Large Language Models (LLMs) and Supervised Fine-Tuning (SFT) for creating powerful, unified text representations~\cite{behnamghader2024llm2veclargelanguagemodels,lee2025nvembedimprovedtechniquestraining}. This methodology has been successfully extended to other modalities, with works like E5-V~\cite{jiang2024e5vuniversalembeddingsmultimodal} and VLM2VEC~\cite{jiang2025vlm2vectrainingvisionlanguagemodels} focusing on fine-tuning strategies for visual models. Furthermore, Zhang's research~\cite{zhang2024gmeimprovinguniversalmultimodal} demonstrates the feasibility of developing universal multimodal retrieval models using MLLMs. However, there has been limited exploration in the audio modality, prompting us to propose the first end-to-end audio-text multimodal retriever.

% \paragraph{Chain-of-Thought Reasoning} 
% For complex problems involving natural language understanding or reasoning, large language models often struggle to provide correct answers directly. The Chain-of-Thought (CoT) reasoning approach \cite{wei2022chain} addresses this challenge by prompting models to explicitly generate intermediate reasoning steps, leading to a more reliable final answer.
% % CoT has been shown to significantly enhance the reasoning capabilities of LLMs, particularly in arithmetic, commonsense reasoning, and logical deduction \cite{chu2024navigate}. CoT techniques are typically divided into manually constructed and automatically generated approaches. 
% Manually constructed CoT \cite{wei2022chain} relies on carefully designed prompt examples, which typically result in high-quality reasoning chains but come with substantial human labor costs. On the other hand, automatically generated CoT \cite{kojima2022large} leverages specific prompt texts to induce models to generate reasoning chains without examples. While more scalable, the automatic method often suffers from lower quality, leading to severe hallucination issues. 
% In the multimodal domain, several works have extended CoT reasoning to multi-modal chain-of-thought (MCoT) reasoning to enhance the performance of vision-language models (VLMs) \cite{yu2024visrag}. However, in the context of spoken dialogue systems, no exploration of CoT methodologies has been conducted to date, which represents a notable gap in the field of general audio understanding and reasoning. In our WavRAG, we further introduce CoT reasoning during the generation stage to more effectively leverage the retrieved external knowledge and the original audio‚Äêtext inputs, which let the model generate answers grounded in an external knowledge base while explicitly revealing its reasoning process.
\begin{figure*}[htbp]
    \centering
    \includegraphics[width=\linewidth]{image/fig2-2.png}

    \caption{Architecture of the WavRAG framework. Top: Traditional RAG pipeline using ASR, highlighting its limitations. Bottom: WavRAG's four-step process: (1) A dual-modality encoder creates embeddings for both audio and text queries; (2) Top-K documents are retrieved from an audio-text knowledge base using cosine similarity; (3) A chain-of-thought reasoning process analyzes the retrieved information; (4) A large language model generates the final response, grounded in the retrieved knowledge.}
    \label{fig:img2}
\end{figure*}