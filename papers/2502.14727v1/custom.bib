% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@misc{touvron2023llamaopenefficientfoundation,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.13971}, 
}

@article{ji2024wavtokenizer,
  title={Wavtokenizer: an efficient acoustic discrete codec tokenizer for audio language modeling},
  author={Ji, Shengpeng and Jiang, Ziyue and Wang, Wen and Chen, Yifu and Fang, Minghui and Zuo, Jialong and Yang, Qian and Cheng, Xize and Wang, Zehan and Li, Ruiqi and others},
  journal={arXiv preprint arXiv:2408.16532},
  year={2024}
}

@article{an2024funaudiollm,
  title={Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms},
  author={An, Keyu and Chen, Qian and Deng, Chong and Du, Zhihao and Gao, Changfeng and Gao, Zhifu and Gu, Yue and He, Ting and Hu, Hangrui and Hu, Kai and others},
  journal={arXiv preprint arXiv:2407.04051},
  year={2024}
}

@misc{chu2024qwen2audiotechnicalreport,
      title={Qwen2-Audio Technical Report}, 
      author={Yunfei Chu and Jin Xu and Qian Yang and Haojie Wei and Xipin Wei and Zhifang Guo and Yichong Leng and Yuanjun Lv and Jinzheng He and Junyang Lin and Chang Zhou and Jingren Zhou},
      year={2024},
      eprint={2407.10759},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2407.10759}, 
}
@misc{behnamghader2024llm2veclargelanguagemodels,
      title={LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders}, 
      author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},
      year={2024},
      eprint={2404.05961},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.05961}, 
}

@inproceedings{ji2024textrolspeech,
  title={Textrolspeech: A text style control speech corpus with codec language text-to-speech models},
  author={Ji, Shengpeng and Zuo, Jialong and Fang, Minghui and Jiang, Ziyue and Chen, Feiyang and Duan, Xinyu and Huai, Baoxing and Zhao, Zhou},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={10301--10305},
  year={2024},
  organization={IEEE}
}

@article{ji2024controlspeech,
  title={Controlspeech: Towards simultaneous zero-shot speaker cloning and zero-shot language style control with decoupled codec},
  author={Ji, Shengpeng and Zuo, Jialong and Wang, Wen and Fang, Minghui and Zheng, Siqi and Chen, Qian and Jiang, Ziyue and Huang, Hai and Wang, Zehan and Cheng, Xize and others},
  journal={arXiv preprint arXiv:2406.01205},
  year={2024}
}

@article{ji2024mobilespeech,
  title={MobileSpeech: A Fast and High-Fidelity Framework for Mobile Zero-Shot Text-to-Speech},
  author={Ji, Shengpeng and Jiang, Ziyue and Wang, Hanting and Zuo, Jialong and Zhao, Zhou},
  journal={arXiv preprint arXiv:2402.09378},
  year={2024}
}

@article{ji2024speechwatermarking,
  title={Speech Watermarking with Discrete Intermediate Representations},
  author={Ji, Shengpeng and Jiang, Ziyue and Zuo, Jialong and Fang, Minghui and Chen, Yifu and Jin, Tao and Zhao, Zhou},
  journal={arXiv preprint arXiv:2412.13917},
  year={2024}
}



@inproceedings{
anonymous2025clsr,
title={{CLSR}: End-to-end Contrastive Language-Speech Retriever For Better Speech Retrieval Augmented Generation},
author={Anonymous},
booktitle={Submitted to ACL Rolling Review - December 2024},
year={2025},
url={https://openreview.net/forum?id=Iw5O9ttpAF},
note={under review}
}
@inproceedings{gonzales-rudzicz-2024-retrieval,
    title = "A Retrieval Augmented Approach for Text-to-Music Generation",
    author = "Gonzales, Robie  and
      Rudzicz, Frank",
    editor = "Kruspe, Anna  and
      Oramas, Sergio  and
      Epure, Elena V.  and
      Sordo, Mohamed  and
      Weck, Benno  and
      Doh, SeungHeon  and
      Won, Minz  and
      Manco, Ilaria  and
      Meseguer-Brocal, Gabriel",
    booktitle = "Proceedings of the 3rd Workshop on NLP for Music and Audio (NLP4MusA)",
    month = nov,
    year = "2024",
    address = "Oakland, USA",
    publisher = "Association for Computational Lingustics",
    url = "https://aclanthology.org/2024.nlp4musa-1.6/",
    pages = "31--36",
    abstract = "Generative text-to-music models such as MusicGen are capable of generating high fidelity music conditioned on a text prompt. However, expressing the essential features of music with text is a challenging task. In this paper, we present a retrieval-augmented approach for text-to-music generation. We first pre-compute a dataset of text-music embeddings obtained from a contrastive language-audio pretrained encoder (CLAP). Then, given an input text prompt, we retrieve the top $k$ most similar musical aspects and augment the original prompt. This approach consistently generates music of higher audio quality as measured by the Frech{\'e}t Audio Distance. We analyze the internal representations of MusicGen and find that augmented prompts lead to greater diversity in token distributions and display high text adherence. Our findings show the potential for increased control in text-to-music generation."
}
@misc{peng2024graphretrievalaugmentedgenerationsurvey,
      title={Graph Retrieval-Augmented Generation: A Survey}, 
      author={Boci Peng and Yun Zhu and Yongchao Liu and Xiaohe Bo and Haizhou Shi and Chuntao Hong and Yan Zhang and Siliang Tang},
      year={2024},
      eprint={2408.08921},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2408.08921}, 
}
@misc{fang2024llamaomniseamlessspeechinteraction,
      title={LLaMA-Omni: Seamless Speech Interaction with Large Language Models}, 
      author={Qingkai Fang and Shoutao Guo and Yan Zhou and Zhengrui Ma and Shaolei Zhang and Yang Feng},
      year={2024},
      eprint={2409.06666},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.06666}, 
}
@misc{ji2024wavchatsurveyspokendialogue,
      title={WavChat: A Survey of Spoken Dialogue Models}, 
      author={Shengpeng Ji and Yifu Chen and Minghui Fang and Jialong Zuo and Jingyu Lu and Hanting Wang and Ziyue Jiang and Long Zhou and Shujie Liu and Xize Cheng and Xiaoda Yang and Zehan Wang and Qian Yang and Jian Li and Yidi Jiang and Jingzhen He and Yunfei Chu and Jin Xu and Zhou Zhao},
      year={2024},
      eprint={2411.13577},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2411.13577}, 
}
@misc{jiang2023scalingsentenceembeddingslarge,
      title={Scaling Sentence Embeddings with Large Language Models}, 
      author={Ting Jiang and Shaohan Huang and Zhongzhi Luan and Deqing Wang and Fuzhen Zhuang},
      year={2023},
      eprint={2307.16645},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.16645}, 
}
@misc{min2023factscorefinegrainedatomicevaluation,
      title={FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation}, 
      author={Sewon Min and Kalpesh Krishna and Xinxi Lyu and Mike Lewis and Wen-tau Yih and Pang Wei Koh and Mohit Iyyer and Luke Zettlemoyer and Hannaneh Hajishirzi},
      year={2023},
      eprint={2305.14251},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.14251}, 
}
@misc{hu2021loralowrankadaptationlarge,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.09685}, 
}
@misc{li2024makingtextembeddersfewshot,
      title={Making Text Embedders Few-Shot Learners}, 
      author={Chaofan Li and MingHao Qin and Shitao Xiao and Jianlyu Chen and Kun Luo and Yingxia Shao and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2409.15700},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2409.15700}, 
}
@inproceedings{Nagrani_2017, series={interspeech\_2017},
   title={VoxCeleb: A Large-Scale Speaker Identification Dataset},
   url={http://dx.doi.org/10.21437/Interspeech.2017-950},
   DOI={10.21437/interspeech.2017-950},
   booktitle={Interspeech 2017},
   publisher={ISCA},
   author={Nagrani, Arsha and Chung, Joon Son and Zisserman, Andrew},
   year={2017},
   month=aug, collection={interspeech_2017} }

@misc{drossos2019clothoaudiocaptioningdataset,
      title={Clotho: An Audio Captioning Dataset}, 
      author={Konstantinos Drossos and Samuel Lipping and Tuomas Virtanen},
      year={2019},
      eprint={1910.09387},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/1910.09387}, 
}
@misc{agostinelli2023musiclmgeneratingmusictext,
      title={MusicLM: Generating Music From Text}, 
      author={Andrea Agostinelli and Timo I. Denk and Zalán Borsos and Jesse Engel and Mauro Verzetti and Antoine Caillon and Qingqing Huang and Aren Jansen and Adam Roberts and Marco Tagliasacchi and Matt Sharifi and Neil Zeghidour and Christian Frank},
      year={2023},
      eprint={2301.11325},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2301.11325}, 
}
@inproceedings{kim-etal-2019-audiocaps,
    title = "{A}udio{C}aps: Generating Captions for Audios in The Wild",
    author = "Kim, Chris Dongjoo  and
      Kim, Byeongchang  and
      Lee, Hyunmin  and
      Kim, Gunhee",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1011/",
    doi = "10.18653/v1/N19-1011",
    pages = "119--132",
    abstract = "We explore the problem of Audio Captioning: generating natural language description for any kind of audio in the wild, which has been surprisingly unexplored in previous research. We contribute a large-scale dataset of 46K audio clips with human-written text pairs collected via crowdsourcing on the AudioSet dataset. Our thorough empirical studies not only show that our collected captions are indeed faithful to audio inputs but also discover what forms of audio representation and captioning models are effective for the audio captioning. From extensive experiments, we also propose two novel components that help improve audio captioning performance: the top-down multi-scale encoder and aligned semantic attention."
}
@misc{li2018spokensquadstudymitigating,
      title={Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension}, 
      author={Chia-Hsuan Li and Szu-Lin Wu and Chi-Liang Liu and Hung-yi Lee},
      year={2018},
      eprint={1804.00320},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1804.00320}, 
}
@inproceedings{shon-etal-2023-slue,
    title = "{SLUE} Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks",
    author = "Shon, Suwon  and
      Arora, Siddhant  and
      Lin, Chyi-Jiunn  and
      Pasad, Ankita  and
      Wu, Felix  and
      Sharma, Roshan  and
      Wu, Wei-Lun  and
      Lee, Hung-yi  and
      Livescu, Karen  and
      Watanabe, Shinji",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.496/",
    doi = "10.18653/v1/2023.acl-long.496",
    pages = "8906--8937",
    abstract = "Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. In order to facilitate the development of SLU models that leverage the success of pre-trained speech representations, we will release a new benchmark suite, including for each task (i) curated annotations for a relatively small fine-tuning set, (ii) reproducible pipeline (speech recognizer + text model) and end-to-end baseline models and evaluation metrics, (iii) baseline model performance in various types of systems for easy comparisons. We present the details of data collection and annotation and the performance of the baseline models. We also analyze the sensitivity of pipeline models' performance to the speech recognition accuracy, using more than 20 publicly availablespeech recognition models."
}
@inproceedings{ardila-etal-2020-common,
    title = "Common Voice: A Massively-Multilingual Speech Corpus",
    author = "Ardila, Rosana  and
      Branson, Megan  and
      Davis, Kelly  and
      Kohler, Michael  and
      Meyer, Josh  and
      Henretty, Michael  and
      Morais, Reuben  and
      Saunders, Lindsay  and
      Tyers, Francis  and
      Weber, Gregor",
    editor = "Calzolari, Nicoletta  and
      B{\'e}chet, Fr{\'e}d{\'e}ric  and
      Blache, Philippe  and
      Choukri, Khalid  and
      Cieri, Christopher  and
      Declerck, Thierry  and
      Goggi, Sara  and
      Isahara, Hitoshi  and
      Maegaard, Bente  and
      Mariani, Joseph  and
      Mazo, H{\'e}l{\`e}ne  and
      Moreno, Asuncion  and
      Odijk, Jan  and
      Piperidis, Stelios",
    booktitle = "Proceedings of the Twelfth Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.520/",
    pages = "4218--4222",
    language = "eng",
    ISBN = "979-10-95546-34-4",
    abstract = "The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla`s DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 {\ensuremath{\pm}} 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition."
}
@misc{du2024cosyvoice2scalablestreaming,
      title={CosyVoice 2: Scalable Streaming Speech Synthesis with Large Language Models}, 
      author={Zhihao Du and Yuxuan Wang and Qian Chen and Xian Shi and Xiang Lv and Tianyu Zhao and Zhifu Gao and Yexin Yang and Changfeng Gao and Hui Wang and Fan Yu and Huadai Liu and Zhengyan Sheng and Yue Gu and Chong Deng and Wen Wang and Shiliang Zhang and Zhijie Yan and Jingren Zhou},
      year={2024},
      eprint={2412.10117},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2412.10117}, 
}
@article{shi2023replug,
  title={Replug: Retrieval-augmented black-box language models},
  author={Shi, Weijia and Min, Sewon and Yasunaga, Michihiro and Seo, Minjoon and James, Rich and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-tau},
  journal={arXiv preprint arXiv:2301.12652},
  year={2023}
}
@inproceedings{joshi-etal-2017-triviaqa,
    title = "{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension",
    author = "Joshi, Mandar  and
      Choi, Eunsol  and
      Weld, Daniel  and
      Zettlemoyer, Luke",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1147/",
    doi = "10.18653/v1/P17-1147",
    pages = "1601--1611",
    abstract = "We present TriviaQA, a challenging reading comprehension dataset containing over 650K question-answer-evidence triples. TriviaQA includes 95K question-answer pairs authored by trivia enthusiasts and independently gathered evidence documents, six per question on average, that provide high quality distant supervision for answering the questions. We show that, in comparison to other recently introduced large-scale datasets, TriviaQA (1) has relatively complex, compositional questions, (2) has considerable syntactic and lexical variability between questions and corresponding answer-evidence sentences, and (3) requires more cross sentence reasoning to find answers. We also present two baseline algorithms: a feature-based classifier and a state-of-the-art neural network, that performs well on SQuAD reading comprehension. Neither approach comes close to human performance (23{\%} and 40{\%} vs. 80{\%}), suggesting that TriviaQA is a challenging testbed that is worth significant future study."
}
@misc{rajpurkar2016squad100000questionsmachine,
      title={SQuAD: 100,000+ Questions for Machine Comprehension of Text}, 
      author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
      year={2016},
      eprint={1606.05250},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1606.05250}, 
}
@misc{wang2017bilateralmultiperspectivematchingnatural,
      title={Bilateral Multi-Perspective Matching for Natural Language Sentences}, 
      author={Zhiguo Wang and Wael Hamza and Radu Florian},
      year={2017},
      eprint={1702.03814},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1702.03814}, 
}
@misc{yang2018hotpotqadatasetdiverseexplainable,
      title={HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering}, 
      author={Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William W. Cohen and Ruslan Salakhutdinov and Christopher D. Manning},
      year={2018},
      eprint={1809.09600},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1809.09600}, 
}
@article{kwiatkowski-etal-2019-natural,
    title = "Natural Questions: A Benchmark for Question Answering Research",
    author = "Kwiatkowski, Tom  and
      Palomaki, Jennimaria  and
      Redfield, Olivia  and
      Collins, Michael  and
      Parikh, Ankur  and
      Alberti, Chris  and
      Epstein, Danielle  and
      Polosukhin, Illia  and
      Devlin, Jacob  and
      Lee, Kenton  and
      Toutanova, Kristina  and
      Jones, Llion  and
      Kelcey, Matthew  and
      Chang, Ming-Wei  and
      Dai, Andrew M.  and
      Uszkoreit, Jakob  and
      Le, Quoc  and
      Petrov, Slav",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Roark, Brian  and
      Nenkova, Ani",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    year = "2019",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q19-1026/",
    doi = "10.1162/tacl_a_00276",
    pages = "452--466",
    abstract = "We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature."
}
@inproceedings{fan-etal-2019-eli5,
    title = "{ELI}5: Long Form Question Answering",
    author = "Fan, Angela  and
      Jernite, Yacine  and
      Perez, Ethan  and
      Grangier, David  and
      Weston, Jason  and
      Auli, Michael",
    editor = "Korhonen, Anna  and
      Traum, David  and
      M{\`a}rquez, Llu{\'i}s",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1346/",
    doi = "10.18653/v1/P19-1346",
    pages = "3558--3567",
    abstract = "We introduce the first large-scale corpus for long form question answering, a task requiring elaborate and in-depth answers to open-ended questions. The dataset comprises 270K threads from the Reddit forum {\textquotedblleft}Explain Like I`m Five{\textquotedblright} (ELI5) where an online community provides answers to questions which are comprehensible by five year olds. Compared to existing datasets, ELI5 comprises diverse questions requiring multi-sentence answers. We provide a large set of web documents to help answer the question. Automatic and human evaluations show that an abstractive model trained with a multi-task objective outperforms conventional Seq2Seq, language modeling, as well as a strong extractive baseline. However, our best model is still far from human performance since raters prefer gold responses in over 86{\%} of cases, leaving ample opportunity for future improvement."
}
@misc{oord2019representationlearningcontrastivepredictive,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1807.03748}, 
}
@article{khosla2020supervised,
  title={Supervised contrastive learning},
  author={Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={18661--18673},
  year={2020}
}
@misc{zhang2024gmeimprovinguniversalmultimodal,
      title={GME: Improving Universal Multimodal Retrieval by Multimodal LLMs}, 
      author={Xin Zhang and Yanzhao Zhang and Wen Xie and Mingxin Li and Ziqi Dai and Dingkun Long and Pengjun Xie and Meishan Zhang and Wenjie Li and Min Zhang},
      year={2024},
      eprint={2412.16855},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.16855}, 
}
@misc{jiang2024e5vuniversalembeddingsmultimodal,
      title={E5-V: Universal Embeddings with Multimodal Large Language Models}, 
      author={Ting Jiang and Minghui Song and Zihan Zhang and Haizhen Huang and Weiwei Deng and Feng Sun and Qi Zhang and Deqing Wang and Fuzhen Zhuang},
      year={2024},
      eprint={2407.12580},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.12580}, 
}
@misc{jiang2025vlm2vectrainingvisionlanguagemodels,
      title={VLM2Vec: Training Vision-Language Models for Massive Multimodal Embedding Tasks}, 
      author={Ziyan Jiang and Rui Meng and Xinyi Yang and Semih Yavuz and Yingbo Zhou and Wenhu Chen},
      year={2025},
      eprint={2410.05160},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2410.05160}, 
}
@misc{lee2025nvembedimprovedtechniquestraining,
      title={NV-Embed: Improved Techniques for Training LLMs as Generalist Embedding Models}, 
      author={Chankyu Lee and Rajarshi Roy and Mengyao Xu and Jonathan Raiman and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
      year={2025},
      eprint={2405.17428},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.17428}, 
}
@misc{chen2024bgem3embeddingmultilingualmultifunctionality,
      title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation}, 
      author={Jianlv Chen and Shitao Xiao and Peitian Zhang and Kun Luo and Defu Lian and Zheng Liu},
      year={2024},
      eprint={2402.03216},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.03216}, 
}
@misc{li2023generaltextembeddingsmultistage,
      title={Towards General Text Embeddings with Multi-stage Contrastive Learning}, 
      author={Zehan Li and Xin Zhang and Yanzhao Zhang and Dingkun Long and Pengjun Xie and Meishan Zhang},
      year={2023},
      eprint={2308.03281},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2308.03281}, 
}
@misc{izacard2022unsuperviseddenseinformationretrieval,
      title={Unsupervised Dense Information Retrieval with Contrastive Learning}, 
      author={Gautier Izacard and Mathilde Caron and Lucas Hosseini and Sebastian Riedel and Piotr Bojanowski and Armand Joulin and Edouard Grave},
      year={2022},
      eprint={2112.09118},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2112.09118}, 
}
@misc{min2025speechretrievalaugmentedgenerationautomatic,
      title={Speech Retrieval-Augmented Generation without Automatic Speech Recognition}, 
      author={Do June Min and Karel Mundnich and Andy Lapastora and Erfan Soltanmohammadi and Srikanth Ronanki and Kyu Han},
      year={2025},
      eprint={2412.16500},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2412.16500}, 
}
@misc{wei2023uniirtrainingbenchmarkinguniversal,
      title={UniIR: Training and Benchmarking Universal Multimodal Information Retrievers}, 
      author={Cong Wei and Yang Chen and Haonan Chen and Hexiang Hu and Ge Zhang and Jie Fu and Alan Ritter and Wenhu Chen},
      year={2023},
      eprint={2311.17136},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.17136}, 
}
@misc{liu2023universalvisionlanguagedenseretrieval,
      title={Universal Vision-Language Dense Retrieval: Learning A Unified Representation Space for Multi-Modal Retrieval}, 
      author={Zhenghao Liu and Chenyan Xiong and Yuanhuiyi Lv and Zhiyuan Liu and Ge Yu},
      year={2023},
      eprint={2209.00179},
      archivePrefix={arXiv},
      primaryClass={cs.IR},
      url={https://arxiv.org/abs/2209.00179}, 
}
@misc{elizalde2022claplearningaudioconcepts,
      title={CLAP: Learning Audio Concepts From Natural Language Supervision}, 
      author={Benjamin Elizalde and Soham Deshmukh and Mahmoud Al Ismail and Huaming Wang},
      year={2022},
      eprint={2206.04769},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2206.04769}, 
}
@INPROCEEDINGS{9102815,
  author={Royal, Brandon and Hua, Kien and Zhang, Brenton},
  booktitle={2020 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Deep Composer: Deep Neural Hashing And Retrieval Approach To Automatic Music Generation}, 
  year={2020},
  volume={},
  number={},
  pages={1-6},
  keywords={Music;Encoding;Hafnium;Time measurement;Feature extraction;Hamming distance;Multiple signal classification;Music Generation;Music Retrieval;Hashing;Deep Learning},
  doi={10.1109/ICME46284.2020.9102815}}
@misc{chan2023usingexternaloffpolicyspeechtotext,
      title={Using External Off-Policy Speech-To-Text Mappings in Contextual End-To-End Automated Speech Recognition}, 
      author={David M. Chan and Shalini Ghosh and Ariya Rastrow and Björn Hoffmeister},
      year={2023},
      eprint={2301.02736},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2301.02736}, 
}
@misc{ghosh2024recapretrievalaugmentedaudiocaptioning,
      title={RECAP: Retrieval-Augmented Audio Captioning}, 
      author={Sreyan Ghosh and Sonal Kumar and Chandra Kiran Reddy Evuru and Ramani Duraiswami and Dinesh Manocha},
      year={2024},
      eprint={2309.09836},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2309.09836}, 
}
@misc{huang2023makeanaudiotexttoaudiogenerationpromptenhanced,
      title={Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models}, 
      author={Rongjie Huang and Jiawei Huang and Dongchao Yang and Yi Ren and Luping Liu and Mingze Li and Zhenhui Ye and Jinglin Liu and Xiang Yin and Zhou Zhao},
      year={2023},
      eprint={2301.12661},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2301.12661}, 
}
@inproceedings{zhao-etal-2023-generating,
    title = "Generating Synthetic Speech from {S}poken{V}ocab for Speech Translation",
    author = "Zhao, Jinming  and
      Haffari, Gholamreza  and
      Shareghi, Ehsan",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Findings of the Association for Computational Linguistics: EACL 2023",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-eacl.147/",
    doi = "10.18653/v1/2023.findings-eacl.147",
    pages = "1975--1981",
    abstract = "Training end-to-end speech translation (ST) systems requires sufficiently large-scale data, which is unavailable for most language pairs and domains. One practical solution to the data scarcity issue is to convert text-based machine translation (MT) data to ST data via text-to-speech (TTS) systems. Yet, using TTS systems can be tedious and slow. In this work, we propose SpokenVocab, a simple, scalable and effective data augmentation technique to convert MT data to ST data on-the-fly. The idea is to retrieve and stitch audio snippets, corresponding to words in an MT sentence, from a spoken vocabulary bank. Our experiments on multiple language pairs show that stitched speech helps to improve translation quality by an average of 1.83 BLEU score, while performing equally well as TTS-generated speech in improving translation quality. We also showcase how SpokenVocab can be applied in code-switching ST for which often no TTS systems exit."
}
@misc{koizumi2020audiocaptioningusingpretrained,
      title={Audio Captioning using Pre-Trained Large-Scale Language Model Guided by Audio-based Similar Caption Retrieval}, 
      author={Yuma Koizumi and Yasunori Ohishi and Daisuke Niizumi and Daiki Takeuchi and Masahiro Yasuda},
      year={2020},
      eprint={2012.07331},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2012.07331}, 
}
@misc{chen2022muragmultimodalretrievalaugmentedgenerator,
      title={MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text}, 
      author={Wenhu Chen and Hexiang Hu and Xi Chen and Pat Verga and William W. Cohen},
      year={2022},
      eprint={2210.02928},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2210.02928}, 
}
@ARTICLE{4490200,
  author={Chelba, Ciprian and Hazen, Timothy J. and Saraclar, Murat},
  journal={IEEE Signal Processing Magazine}, 
  title={Retrieval and browsing of spoken content}, 
  year={2008},
  volume={25},
  number={3},
  pages={39-49},
  keywords={Content based retrieval;Information retrieval;Bandwidth;Costs;Digital audio broadcasting;Large-scale systems;Document handling;Computer errors;Automatic speech recognition;Search engines},
  doi={10.1109/MSP.2008.917992}}
@ARTICLE{7114229,
  author={Lee, Lin-shan and Glass, James and Lee, Hung-yi and Chan, Chun-an},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing}, 
  title={Spoken Content Retrieval—Beyond Cascading Speech Recognition with Text Retrieval}, 
  year={2015},
  volume={23},
  number={9},
  pages={1389-1420},
  keywords={Speech;Speech recognition;Multimedia communication;Speech processing;Accuracy;Acoustics;Indexing;Spoken content retrieval;spoken term detection;query by example;semantic retrieval;joint optimization;pseudo-relevance feedback;graph-based random walk;unsupervised acoustic pattern discovery;query expansion;interactive retrieval;summarization;key term extraction},
  doi={10.1109/TASLP.2015.2438543}}
@misc{kong2024audioflamingonovelaudio,
      title={Audio Flamingo: A Novel Audio Language Model with Few-Shot Learning and Dialogue Abilities}, 
      author={Zhifeng Kong and Arushi Goel and Rohan Badlani and Wei Ping and Rafael Valle and Bryan Catanzaro},
      year={2024},
      eprint={2402.01831},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2402.01831}, 
}
@misc{zhao2023retrievingmultimodalinformationaugmented,
      title={Retrieving Multimodal Information for Augmented Generation: A Survey}, 
      author={Ruochen Zhao and Hailin Chen and Weishi Wang and Fangkai Jiao and Xuan Long Do and Chengwei Qin and Bosheng Ding and Xiaobao Guo and Minzhi Li and Xingxuan Li and Shafiq Joty},
      year={2023},
      eprint={2303.10868},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.10868}, 
}
@misc{gao2024retrievalaugmentedgenerationlargelanguage,
      title={Retrieval-Augmented Generation for Large Language Models: A Survey}, 
      author={Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi Dai and Jiawei Sun and Meng Wang and Haofen Wang},
      year={2024},
      eprint={2312.10997},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.10997}, 
}
@misc{guu2020realmretrievalaugmentedlanguagemodel,
      title={REALM: Retrieval-Augmented Language Model Pre-Training}, 
      author={Kelvin Guu and Kenton Lee and Zora Tung and Panupong Pasupat and Ming-Wei Chang},
      year={2020},
      eprint={2002.08909},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2002.08909}, 
}
@misc{bang2023multitaskmultilingualmultimodalevaluation,
      title={A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity}, 
      author={Yejin Bang and Samuel Cahyawijaya and Nayeon Lee and Wenliang Dai and Dan Su and Bryan Wilie and Holy Lovenia and Ziwei Ji and Tiezheng Yu and Willy Chung and Quyet V. Do and Yan Xu and Pascale Fung},
      year={2023},
      eprint={2302.04023},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.04023}, 
}
@article{Huang_2025,
   title={A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions},
   volume={43},
   ISSN={1558-2868},
   url={http://dx.doi.org/10.1145/3703155},
   DOI={10.1145/3703155},
   number={2},
   journal={ACM Transactions on Information Systems},
   publisher={Association for Computing Machinery (ACM)},
   author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and Liu, Ting},
   year={2025},
   month=jan, pages={1–55} }
@misc{défossez2024moshispeechtextfoundationmodel,
      title={Moshi: a speech-text foundation model for real-time dialogue}, 
      author={Alexandre Défossez and Laurent Mazaré and Manu Orsini and Amélie Royer and Patrick Pérez and Hervé Jégou and Edouard Grave and Neil Zeghidour},
      year={2024},
      eprint={2410.00037},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url={https://arxiv.org/abs/2410.00037}, 
}
@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@article{chen2023universal,
  title={Universal self-consistency for large language model generation},
  author={Chen, Xinyun and Aksitov, Renat and Alon, Uri and Ren, Jie and Xiao, Kefan and Yin, Pengcheng and Prakash, Sushant and Sutton, Charles and Wang, Xuezhi and Zhou, Denny},
  journal={arXiv preprint arXiv:2311.17311},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{yu2024visrag,
  title={Visrag: Vision-based retrieval-augmented generation on multi-modality documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@inproceedings{chu2024navigate,
  title={Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future},
  author={Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1173--1203},
  year={2024}
}
