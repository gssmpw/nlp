\section{Related Work}
\label{Related Work}





Cutting-edge \textbf{DA} techniques encompass two primary research avenues: 1) Shallow \textbf{DA}; and 2) Deep \textbf{DA}. In Section \ref{subsect: Shallow DA} and Section \ref{subsect: Deep DA}, we provide an insightful overview of these techniques and simultaneously engage in a comparative discussion with our innovative \textbf{GAN-DA} approach. This parallel exploration aims to better clarify the underlying rationale of our proposed method.




\subsection{Shallow Domain Adaptation}
\label{subsect: Shallow DA}



Given the error bound of the target domain, we observed that \textbf{Term.1} in Eq.(\ref{eq:bound}) is generally been optimized using a supervised learning approach. Consequently, shallow \textbf{DA} methods primarily focus on reducing the domain shift by employing statistic alignment or geometric regularizations to explicitly minimize \textbf{Term.2} and \textbf{Term.3} in Eq.(\ref{eq:bound}).








\subsubsection{Statistic Alignment-based DA \textbf{(STA-DA)}} 
\label{Statistic alignment-based DA}


The core principle of \textbf{STA-DA} rests on the premise that seeking an optimized common feature space through statistical measurements can significantly mitigate cross-domain disparities. As a result, the knowledge acquired from the source domain can be seamlessly applied to the target domain within this fine-tuned common feature space.



To this end, recent research has incorporated a range of statistical measurements, \textit{e.g.}, Bregman Divergence \cite{4967588}, Wasserstein distance \cite{courty2017joint, courty2017optimal}, and \textit{Maximum Mean Discrepancy} (\textbf{MMD}) measurement \cite{pan2011domain}, to identify a common feature space that reduces cross-domain divergence, thereby diminishing Term.2 in Equation (\ref{eq:bound}). Specifically, through the application of dimensionality reduction techniques, \textbf{TCA} \cite{pan2011domain} explicitly addresses the disparity between source and target domains in terms of marginal distribution \cite{4967588} \cite{pan2011domain}. \textbf{JDA} \cite{long2013transfer} builds upon the framework of \textbf{TCA} and further leverages conditional distribution alignment for effective sub-domain adaptation. In contrast to prior \textbf{STA-DA} methods, \textbf{ILS} \cite{herath2017learning} focuses on learning a discriminative latent space using the Mahalanobis metric to align statistical properties across different domains. Beyond the previous divergence reduction-based \textbf{DA} approaches, recent research argues in favor of '\textit{preserving divergence within differently labeled sub-domains to simplify the next round of classifier optimization}'. To address this, \cite{lu2018embarrassingly} explores the discriminative contribution by drawing on the advantages of linear discriminant analysis (\textbf{LDA}) to optimize the common feature space. \textbf{DGA-DA} \cite{luo2020discriminative} underscores the effectiveness of discrimination, introducing \textit{‘repulsive force’} terms as an additional element. \textbf{ARG-DA} \cite{luo2022attention} aligns cross sub-domains in a discriminative manner using a specifically designed attention-regulated graph.





While \textbf{STA-DA} techniques utilize statistical measurements to mitigate domain shifts across distinct kernel spaces, a notable limitation lies in their failure to provide a geometric perspective for addressing \textbf{Term.3} of Equation (\ref{eq:bound}). Moreover, the growing volume of cross-domain samples results in a substantial reduction in computational efficiency for the optimization of the overall kernel space, which hampers the further advancement of \textbf{STA-DA} methods within the shallow \textbf{DA} paradigm.





\subsubsection{Geometric Regularization-based DA \textbf{(GR-DA)}} 
\label{Geometric alignment-based DA}
\textbf{GR-DA} methods provide a distinct geometric perspective to effectively reduce domain shifts by integrating the underlying cross-domain manifold structure and minimizing \textbf{Term.3} in Equation (\ref{eq:bound}). These methods can be distinguished based on their usage of graph embedding techniques or their adoption of subspace alignment approaches.





By employing graph embedding (\textbf{GE}) techniques, a series of prior \textbf{GE-DA} approaches explicitly align hidden manifolds across distinct feature spaces \cite{jing2020adaptive,zhang2019manifold} within a purpose-designed knowledge graph for synchronized manifold representation in \textbf{DA}. For instance, \textbf{ARTL} \cite{long2013adaptation} utilizes the \textbf{GE} strategy to align diverse feature spaces and optimize the \textbf{MMD} distance within a unified framework. To counteract the geometric structure distortion present in the original space, \textbf{MEDA} \cite{wang2018visual} focuses on learning the Grassmann manifold to enhance manifold representations. \textbf{ACE} \cite{jing2020adaptive} contends that previous research often overlooks discriminative effectiveness and proposes enhancing the graph model's discriminativeness by increasing within-class distances and reducing between-class distances across different sub-domains. Different from \textit{feature spaces alignment enforced} \textbf{GE-DA} approaches, another prominent \textbf{GE-DA} research branch explores the more effective utilization of the discriminative label space. This approach additionally reduces \textbf{Term.1} in Equation (\ref{eq:bound}) and leads to label space embedding enforced \textbf{GE-DA}. In optimizing the label space, \textbf{EDA} \cite{zhang2016robust} and \textbf{DTLC} \cite{li2020discriminative} enforce manifold structure alignment across both the feature space and the label space, resulting in discriminative conditional distribution adaptation. By incorporating label smoothness consistency (\textbf{LSmC}), \textbf{UDA-LSC} \cite{DBLP:journals/tip/HouTYW16} effectively captures the geometric structures of the underlying data manifold in the label space of the target domain. \textbf{DGA-DA} \cite{luo2020discriminative} further refines \textbf{UDA-LSC} by simultaneously addressing manifold regularization in the source domain. Combining the strengths of both \textbf{GE-DA} research paradigms, Luo \textit{et al.} \cite{luo2022attention} introduce a novel \textbf{ARG-DA}  method that unifies different feature and label spaces using a dynamic optimization approach for comprehensive manifold unification-based \textbf{DA}.






In addition to \textit{graph embedding} based \textbf{DA} methods, an increasing number of \textbf{DA} techniques, such as those highlighted in \cite{DBLP:journals/corr/LuoWHC17,DBLP:journals/ijcv/ShaoKF14,DBLP:journals/tip/XuFWLZ16,sun2016return,DBLP:journals/tnn/DingF18,DBLP:conf/iccv/FernandoHST13}, place a strong emphasis on aligning the underlying data subspace instead of the graph model between the source and target domains for effective \textbf{DA}. These methods introduce low-rank and sparse constraints into \textbf{DA}, aiming to extract a low-dimensional feature representation where target samples can be sparsely reconstructed from source samples \cite{DBLP:journals/ijcv/ShaoKF14} or interleaved with source samples \cite{DBLP:journals/tip/XuFWLZ16}, thereby aligning the geometric structures of the underlying data manifolds. Some recent \textbf{DA} methods, such as \textbf{RSA-CDDA} \cite{DBLP:journals/corr/LuoWHC17} and \textbf{JGSA} \cite{Zhang_2017_CVPR}, further propose unified frameworks to reduce the shift between domains statistically and geometrically. \textbf{HCA} \cite{liu2019homologous} enhances \textbf{JGSA} by introducing a homologous constraint on the two transformations for the source and target domains, respectively, to establish a connection between the transformed domains and alleviate negative domain adaptation. 







Despite the significant progress achieved by \textbf{GR-DA} methods in mitigating \textbf{Term.3} of Equation (\ref{eq:bound}), the sole alignment of data geometric structures across domains does not inherently ensure the theoretical effectiveness necessary to reduce the existed domain shift as stipulated in \textbf{Term.2} of Equation (\ref{eq:bound}). Furthermore, akin to \textbf{STA-DA} approaches, \textbf{GR-DA} methods also contend with considerable computational efficiency challenges when dealing with large datasets \cite{peng2019moment}, which restricts their practical applicability.



\subsection{Deep Domain Adaptation}
\label{subsect: Deep DA}



\textbf{DL-DA} techniques offer a promising solution to cross-domain tasks with a large number of samples. These methods utilize the \textit{'batch learning'} approach to dynamically sample the entire cross-domain dataset and optimize the overall function through an \textit{end-to-end} training strategy, resulting in discriminative feature representations and improved \textbf{DA} performance. Generally, traditional \textbf{DL-DA} techniques are typically categorized into two main research paradigms based on whether they incorporate an adversarial learning mechanism. In contrast, some recent \textbf{DA} methods expand upon generative models to approach Markov process-informed optimization for effective \textbf{DA}.


\subsubsection{Statistic Matching-based DA \textbf{(DL-STA-DA)}}
\label{Statistic matching-based DA}



 {These approaches share similar spirits  with \textbf{STA-DA} in reducing domain shift through various statistic measurements. However, \textbf{DL-STA-DA} advances this by incorporating the \textbf{DL} paradigm, moving beyond the shallow functional learning of \textbf{STA-DA}. This incorporation of popular deep frameworks, such as ResNet \cite{He2015} and AlexNet \cite{krizhevsky2012imagenet}, enables highly discriminative feature representations, leading to significantly improved performance.}





 {For instance, the well-known \textbf{DAN} \cite{long2015learning} reduces marginal distribution divergence by incorporating a multi-kernel MMD loss in the fully connected layers of AlexNet. \textbf{JAN} \cite{DBLP:conf/icml/LongZ0J17} builds on \textbf{DAN} by jointly minimizing the divergence of both marginal and conditional distributions. \textbf{D-CORAL} \cite{sun2016deep} takes this further by introducing second-order statistics into the AlexNet \cite{krizhevsky2012imagenet} framework, enhancing the effectiveness of the \textbf{DA} strategy. In contrast to previous research, \textbf{FDA} \cite{yang2020fda} and \textbf{MSDA} \cite{he2021multi} offer novel insights into reducing domain shift. \textbf{FDA} suggests that dissimilarities across domains can be measured using the low-frequency spectrum, while \textbf{MSDA} focuses on aligning cross-domains by reducing the \textbf{LAB} feature space representation. Both approaches, by swapping contents of different low-frequency spectra \cite{yang2020fda} or \textbf{LAB} feature space representations \cite{he2021multi} among cross-domains, significantly reduce domain shift. Additionally, \textbf{LMMD} \cite{zhu2020deep} introduces a novel local \textbf{MMD} measurement to effectively address sub-domain adaptation for conditional alignment in \textbf{DA}. \textbf{Mire} \cite{DBLP:conf/nips/ChenT0Z0Y22} defines a semantic anchor as the centroid of category-wise features within each domain, with the capability to reason over the global semantic topology for semantic knowledge-aware \textbf{DA}. Finally, \textbf{DRDA} \cite{DBLP:journals/tip/HuangWCZZ23} specifically designs global anchors to achieve cross-domain marginal distribution alignment, with a focus on both statistical and geometric alignment aspects.}



The fundamental principle of \textbf{DL-STA-DA} is to reduce domain divergence within kernel feature spaces \cite{belkin2018understand} through statistical measurements. The efficacy of this approach relies heavily on the thoughtful design of these statistical measurements. Given that \textbf{DL-STA-DA} methods are sensitive to the choice of statistical measurements, there has been a rising interest in adversarial learning-based \textbf{DA} techniques. These methods offer an automated optimization approach, eliminating the need for explicit calculation of prior distribution for likelihood maximization, thus moving away from manual, handcrafted approaches.






\subsubsection{Adversarial Loss-based DA}
\label{Adversarial loss-based DA}
These methods adopt the merits of adversarial learning \cite{goodfellow2014generative} to fool the deep model in recognizing the source and target domain samples, thus leading the cross-domain sample features indistinguishable \textit{w.r.t} the domain labels through an adversarial loss on a domain classifier \cite{ganin2016domain,tzeng2017adversarial,pei2018multi}. \textbf{DANN} \cite{ganin2016domain} and \textbf{ADDA} \cite{tzeng2017adversarial} learn a domain-invariant feature subspace by reducing the marginal distribution divergence. \textbf{MADA} \cite{pei2018multi} additionally makes use of multiple domain discriminators, thereby aligning conditional data distributions.  In contrast, \textbf{DSN} \cite{bousmalis2016domain} achieves domain-invariant representations by explicitly separating the similarities and dissimilarities in the source and target domains. \textbf{GVB} \cite{cui2020gradually} introduces a novel gradually vanishing bridge method to improve the feature representation on the generator and discriminator concurrently. \textbf{MADAN} \cite{zhao2019multi} explores knowledge from different multi-source domains to fulfill \textbf{DA} tasks. \textbf{CyCADA} \cite{pmlr-v80-hoffman18a} addresses the distribution divergence using a bi-directional \textbf{GAN}-based training framework. 


The intriguing principle of Adversarial Loss-based \textbf{DA} approaches is intuitive and easy to implement, which has accelerated the rapid development of this research paradigm. However, these models are known for their potentially unstable training due to their adversarial nature, which lacks sufficient guidance \cite{bond2021deep, yang2022survey} for explicit likelihood distribution optimization.


\subsubsection{Generative Model-based DA}
\label{Generative Model-based DA}


In contrast to the discriminative models (${f_d}({\bf{x}},\theta ) \mapsto p({\bf{c}})$) commonly used in solving \textbf{DA} tasks, recently, the newly developed generative model (${f_g}(\theta ) \mapsto p({\bf{x}},{\bf{c}})$) in \textbf{DA} has also gained prominence in the field of \textbf{DA}, present an alternative approach. The rationale behind this is that its regularized functional learning \cite{lasserre2006principled, DBLP:conf/ijcai/JiangZTTZ17} not only approximates the label knowledge '${\bf{c}}$' as discriminative models do but also reversely generates samples with high fidelity. This ensures the preservation of key information during dynamic model training and making it a more reliable approach for \textbf{DA}. 


Following the principle of Markov process-informed generative model optimization, \textbf{DDA} \cite{gao2023back} adopts a diffusion model to harmoniously blend different domains by deteriorating the data structure within the final noisy spaces. Therefore, the reversely generated cross-domain samples are assumed to be projected to the identical data distribution. Later on, Kamil \textit{et al.} \cite{deja2023learning} elaborated on classifier guidance to further approach conditional distribution-aware \textbf{DA}. Using a probabilistic graphical model, Zhang \textit{et al.} \cite{zhang2020domain} consider \textbf{DA} as a problem of Bayesian inference in graphical models. \textbf{VDI} \cite{xu2023domainindexing} further enhances the variational Bayesian framework by incorporating domain indices from multi-domain data, offering fresh insights into domain relationships and enhancing \textbf{DA} performance.



Recently, generative model-based \textbf{DA} methods have gained significant attention. However, as a novel yet promising research direction, when compared to discriminative model-based \textbf{DA}, they still face challenges related to computational efficiency and the generation of redundant samples during the overall optimization process. More specifically, diffusion model-enforced \textbf{DA} \cite{gao2023back} strictly adheres to the Markov chain optimization, which requires substantial computational resources for the reverse functional transformation. Furthermore, Bayesian inference-optimized \textbf{DA} generally reduces the formulated evidence lower bound, resulting in the generation of redundant samples that are not useful for decision optimization as required in solving \textbf{DA} tasks.



\subsection{Discussion}
\label{Discussion}


Given the considerable challenges posed by the large quantity of cross-domain datasets in the development of \textbf{Shallow-DA}, our research primarily focuses on the \textbf{DL-DA} paradigm. The key innovation of \textbf{DL-DA} methods lies in their ability to simultaneously reduce the divergence of data distributions across domains and generate a discriminative feature representation of data within a unified end-to-end learning framework. This results in an optimized model that harnesses the benefits of \textbf{META} learning \cite{wei2021metaalign}, where leveraging different tasks strengthens the model and allows for the discovery of optimal hyperparameters. However, in comparison to the '\textbf{Shallow-DA}' approach, which emphasizes '\textit{global modeling}' \cite{belkin2003laplacian}, the '\textit{batch learning}' strategy \cite{goodfellow2016deep} can notably limit the overall understanding of global statistics and geometry, as discussed in \textbf{Issue 1} and \textbf{Issue 2} within cross-domain experimental scenarios. To address these issues, we introduce a novel \textbf{DL-DA} method called \textbf{G}lobal \textbf{A}wareness e\textbf{N}hanced \textbf{D}omain \textbf{A}daptation (\textbf{GAN-DA}). This method incorporates an orthonormalized prior feature distribution to foster global statistical and geometric awareness within the \textbf{DL-DA} optimization framework. \textbf{GAN-DA} seamlessly integrates the prior feature distribution with the batch learning mechanism, resulting in deep \textbf{DA} that is globally aware of statistics and geometry.