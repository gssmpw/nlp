\documentclass[lettersize,journal]{IEEEtran}

% \usepackage{flushend}
\usepackage{amsmath,amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{color}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{cite, url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{bm}
\pagestyle{empty}
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}

\begin{document}

\title{


Generative Video Semantic Communication via Multimodal Semantic Fusion with Large Model

}
\author{Hang Yin, Li Qiao, Yu Ma, Shuo Sun, Kan Li, Zhen Gao and Dusit Niyato \textit{Fellow, IEEE}


\thanks{H. Yin, L. Qiao, Y. Ma, S. Sun, K. Li, and Z. Gao are with Beijing Institute of Technology, Beijing 100081, China (e-mails: \{yh, qiaoli, yu.ma, sunshuo2002, gaozhen16, likan\}@bit.edu.cn).}
\thanks{
Dusit Niyato is with the School of
Computer Science and Engineering, Nanyang Technological University,
Singapore 639798 (e-mail: dniyato@ntu.edu.sg).
}
}


\maketitle
\begin{abstract}
Despite significant advancements in traditional syntactic communications based on Shannon's theory, these methods struggle to meet the requirements of 6G immersive communications, especially under challenging transmission conditions. With the development of generative artificial intelligence (GenAI), progress has been made in reconstructing videos using high-level semantic information. In this paper, we propose a scalable generative video semantic communication framework that extracts and transmits semantic information to achieve high-quality video reconstruction. Specifically, at the transmitter, {\color{black}description and other condition signals (e.g., first frame, sketches, etc.) are extracted from the source video, functioning as text and structural semantics, respectively.} At the receiver, the {\color{black}diffusion-based GenAI large models} are utilized to fuse the semantics of the {\color{black}multiple} modalities for reconstructing the video. Simulation results demonstrate that, at an ultra-low channel bandwidth ratio (CBR), our scheme effectively captures semantic information to reconstruct videos aligned with human perception under different signal-to-noise ratios. {\color{black}Notably, the proposed ``First Frame+Desc." scheme consistently achieves CLIP score exceeding 0.92 at CBR = 0.0057 for SNR $>$ 0\,dB. This demonstrates its robust performance even under low SNR conditions.}

\end{abstract}

\begin{IEEEkeywords}
video semantic communication, visual compression, generative artificial intelligence (GenAI), large model, diffusion model.
\end{IEEEkeywords}
% \vspace{-6mm}
\section{Introduction}
Semantic communication is considered a revolutionary paradigm with the potential to transform the design and operation of 6G wireless communication systems~\cite{pokhrel2022understand,wang2022transformer, liu2024near,wu2024deep}. 
Whereas, extracting semantics from source signals and redesigning wireless communication systems present significant challenges. Facing such challenges, the advanced coding techniques, such as deep joint source-channel coding (DJSCC), have been proposed and achieved success in semantic communication systems, where the semantic compression capability of neural networks is leveraged in an end-to-end training manner~\cite{DBLP:journals/tccn/BourtsoulatzeKG19, DBLP:journals/jsac/TungG22}.
However, the DJSCC frameworks rely on rate-distortion theory and are unable to optimize the perception quality, which is crucial to humans.

% cui2024overview, 
On the other hand, advancements in generative artificial intelligence (GenAI) not only enable the creation of realistic, high-quality content but also enhance transmission efficiency through innovations in semantic communication, optimizing bandwidth usage without sacrificing quality~\cite{lu2024generative,10614204,qiao2025tokencommunicationsunifiedframework}. Early GenAI focused on probabilistic graphical models, such as hidden Markov models~\cite{juang1986maximum}, which generated data with limited quality and diversity. Variational autoencoders (VAEs) marked a breakthrough by producing more complex and realistic data~\cite{kingma2013auto,dang2019real}, but they faced distribution mismatch issues. To address this, generative adversarial networks (GANs) were proposed~\cite{yu2017seqgan,zhang2017adversarial}, optimizing the generator and discriminator through adversarial loss to improve data quality. GANs have been used after DJSCC at the receiver to enhance perception quality~\cite{erdemir2023generative,tang2024evolving}. However, GANs are prone to instability, such as mode collapse, resulting in a lack of diversity in the generated samples.


Recent advancements in diffusion models have showcased their impressive ability to generate high-fidelity data conditioned on semantic prompts. For example, textual descriptions like ``Panda plays ukulele at home" can now guide the creation of visually coherent short videos with precise semantic alignment \cite{DBLP:conf/nips/SahariaCSLWDGLA22, brooks2024video}. Building on these models' strengths in semantic understanding and content authenticity, recent studies \cite{qiao2024latency,10734812,yang2024diffusion} have demonstrated ultra-low bitrate image transmissions (e.g., $<$0.1 bit per pixel) by transmitting only highly compressed multi-modal semantics, such as text, edge maps, and embeddings. However, existing diffusion-based approaches for image semantic transmission fail to account for temporal frame correlations, making them insufficient for efficient video semantic communication. Extending this paradigm to video transmission remains under-explored: While studies like \cite{DBLP:journals/corr/abs-2402-08934,liu20242} investigate diffusion-based inter-frame reconstruction for video compression, their scope is limited to error-free channels, leaving challenges in adapting diffusion models to bandwidth-constrained video communication systems.

Therefore, a significant research gap exists in effectively utilizing diffusion models for video reconstruction under practical wireless transmission channels. Delivering video semantics as conditional signals for diffusion models to achieve high semantic scores remains an open challenge. To fill these gaps, we design a generative video semantic communication (GVSC) framework for transmitting video semantics and reconstructing videos, whereby GenAI large models with the diffusion techniques are utilized to generate video aligned with human perception from video semantics of sketch sequences and text description. 
Our contributions are summarized as follows:

1) To the best of our knowledge, we are the first to propose the novel GVSC framework for wireless video transmission at ultra-low channel bandwidth ratios (CBR). At the transmitter, our semantic extractor captures key visual and textual semantics from the video. Visual data is transmitted using DJSCC, while textual data is encoded with turbo coding and quadrature amplitude modulation (QAM). These semantics encode critical details such as object location, color, size, and actions. At the receiver, pre-trained large GenAI diffusion models integrate both visual and textual semantics for accurate video reconstruction. GVSC's robustness ensures effective performance even in resource-constrained environments, maintaining high quality at CBRs as low as $10^{-2}$.

2) We introduce several video semantic extraction strategies that adapt the transmission schemes according to signal-to-noise ratio (SNR) conditions. Our approach includes a variety of strategies, such as single-sketch with video description, sketch sequence combined with video descriptions, and incorporating the first red-green-blue (RGB) frame with video descriptions. By systematically evaluating these strategies under varying SNR scenarios, our generative transmission schemes effectively mitigate the cliff effect~\cite{proakis2008digital}. Moreover, these models exhibit strong adaptability in the low SNR range, ensuring reliable video communication even under challenging conditions.

3) We design a weighted loss function that combines mean squared error loss (MSE) and learned perceptual image patch similarity (LPIPS) loss to optimize the transmission quality of sketch sequence semantics. Our simulations demonstrate that using the proposed weighted loss function aligns more closely with human perception.


\begin{figure*}[ht] 
\centering 
% 左 下 右 上
\includegraphics[trim=290 240 245 235, clip,scale=1.5]{fig1.pdf} % 插入图片并设置宽度为文本宽度
\caption{{\color{black}\textbf{GVSC framework overview.} We transmit key semantic information, including visual and textual elements, to capture the main semantic content of the video. This semantic data is then fused by the GenAI large model at the receiver to accurately reconstruct the video.}
}
\label{fig:GVSC framework} % 用于文中引用的标签
\end{figure*}


\section{Proposed Generative Video Semantic Communications Framework}
In this section, we introduce the components of GVSC framework. 
As illustrated in the Fig.~\ref{fig:GVSC framework}, our framework comprises three components: semantic extractor, source channel coding, and GenAI model. 

\subsection{Video Semantic Extractors with Multiple Modalities}
Semantic extraction is essential for video transmission at ultra-low CBR.
Depending on the channel conditions, we propose to extract two kinds of modal semantic information from videos: text description modality and {\color{black}visual} modality.

The text description provides critical movements and events in the video. In our scalable framework, we use a video understanding generative model, such as Video-LLaVA~\cite{DBLP:journals/corr/abs-2311-10122}, to obtain the video description. This model can generate either general or highly detailed video description based on the specific prompt.
While visual semantic information plays a crucial role in video reconstruction by providing detailed structural and spatial context that enhances the consistency of the generated video. {\color{black}Such as the first frame, provides an immediate and comprehensive visual context. This frame offers a rich, color-detailed snapshot that anchors the video's overall aesthetic and thematic elements.
To better save bandwidth, sketch focuses on extracting and conveying the essential contours and major features of each frame. By distilling the video content down to its fundamental visual elements, sketches provide a clear structural guide that aids in reconstructing the basic shapes and layout of scenes. }
This structural framework helps maintain consistency and visual fidelity across frames, ensuring that objects and their spatial relationships are accurately represented throughout the video. 


Denote the input video signal as \(\textbf{X} \in \mathbb{R}^{F \times H \times W \times 3}\), where \(F\) is the number of frames, \(H\) and \(W\) are the height and width of the video, respectively. Let \( \mathbf{x}_f = \textbf{X}_{f,:,:,:} \) represent the \(f\)-th frame of the video for \(1 \leq f \leq F\). 
We can use a sketch extractor to obtain the sketch of \(F\) frames, denoted as \(\textbf{S} \in \mathbb{R}^{F \times H \times W \times 1}\). Let \(\mathbf{s}_f = \textbf{S}_{f,:,:,:} \) represent the \(f\)-th sketch frame for \(1 \leq f \leq F\).

Therefore, even with only a few semantic details as input, the pre-trained GenAI model at the receiver can still reconstruct the video with its main semantic content.

\subsection{Adaptive Transmission Strategy for Multimodal Semantics}
We develop adaptive strategies for transmitting multimodal semantic information, tailoring coding schemes to fit channel conditions and semantic modalities.

\subsubsection{Separate Source Channel Coding for {\color{black}Text Modality}}
Given the importance of video description, textual errors can greatly affect GenAI model's human perception. To minimize the transmission errors of video description semantics, we utilize the separate source channel coding for description, with the code rate $R_c$, based on the different channel conditions.
\subsubsection{Joint Source Channel Coding for {\color{black}Visual Modality}}
Compared with text description modality, {\color{black}visual} modality can better assist GenAI in reconstructing videos.
Moreover, we propose that if the SNR is greater than a predefined threshold, GVSC transmits both description and {\color{black}visual} simultaneously to achieve better reconstruction results. 

To prevent the cliff effect during transmission, we use DJSCC with attention mechanism for sketch sequence transmission. The attention mechanism improves compression efficiency by enabling the network to focus on key semantic information that enhances the overall quality of sketch transmission.
Moreover, we design a weighted loss function $\mathcal{L}$ for sketch transmission that combines MSE and LPIPS losses:
\vspace*{-2mm}
\begin{equation} \label{loss func}
\mathcal{L}(x, x_0) = k \cdot \text{MSE}(x, x_0) + (1 - k) \cdot \text{LPIPS}(x, x_0),
\vspace*{-2mm}
\end{equation}
where \( k\) is the weighting factor that balances the importance of the two losses, with the range between [0,1]. By incorporating LPIPS loss, we aim to better optimize the semantic and perceptual quality of the sketches during transmission.
While the widely used MSE metric measures pixel-wise differences, it fails to capture the perceptual significance of the condensed white contours crucial in sketches, necessitating the inclusion of LPIPS to address this shortcoming.

\subsection{Video GenAI Model}
In this section, we delve into the role of GenAI in our framework. We explore the criteria for selecting suitable conditional generation models that enable us to achieve high-quality semantic transmission.

\subsubsection{Latent Space Embeddings}
{\color{black}After passing through the wireless channels, the receiver captures transmitted semantic information, such as video frames, sketches, and descriptions. Our framework processes this diverse content using a pre-trained text encoder (e.g., OpenCLIP ViT-H/14~\cite{cherti2023reproducible}) for textual data and a VAE for visual data like sketches and RGB frames, preparing it for high-quality video reconstruction.}

\subsubsection{Diffusion Process}\label{sec:video diffusion model}
After obtaining the semantic embedding, the video generator utilizes these information as conditions to reconstruct the video at the receiver. 


During the generation process, the diffusion model \(\epsilon_\theta\) combines the embeddings of the visual semantics and the video description semantics to perform denoising. Assuming \(z_t\) is the latent representation at time step \(t\), the generation process is as follows:
% \vspace{-3mm}
\begin{equation}
  \epsilon_\theta(z_t, c, t) = \epsilon_\theta(z_t, c_{\text{v}}, t) + \omega \left( \epsilon_\theta(z_t, c_{\text{t}}, t) - \epsilon_\theta(z_t, c_{\text{v}}, t) \right),  
  % \vspace{-2mm}
\end{equation}
where \(c_{\text{v}}\) and \(c_{\text{t}}\) are the conditional embeddings of the visual information and the text information, respectively, and \(\omega\) is the guidance scale.
If the model only has the video description as the conditional input, the process is simplified to
\(\epsilon_\theta(z_t, c_{\text{text}}, t)\).


To ensure consistent output, diffusion models are initialized with the same random seed and hyper-parameters, producing identical content from the same initial noise input when using a deterministic solver. This uniformity is crucial for comparative analysis and applications requiring predictable results.


\begin{figure*}[h]
% \vspace{-6mm}
\centering % 图片居中
% 左 下 右 上
\includegraphics[trim= 200 320 200 30, scale=0.91]{casestudy.pdf}
\caption{{\color{black}Visual comparisons of different transmission schemes at SNR = 10\,dB and SNR = 0\,dB. The first column on the left displays the original video frames. Subsequent columns illustrate the visual outcomes of various schemes at SNR = 10\,dB. The rightmost part focuses on the DVST and First Frame+Desc. schemes specifically at SNR = 0\,dB, all while maintaining a CBR of $10^{-2}$. In order to clearly demonstrate the effect of the best solution and provide a fair performance comparison at 0\,dB, we only selected the best results among the comparison solutions and our proposed solutions for display.}}
\label{fig:case study} % 用于文中引用的标签
% \vspace{-5mm}
\end{figure*}

\section{Implementation of GVSC Framework}

In this section, we introduce the specific implementation of the proposed GVSC framework, including transmission strategy, model configuration, and evaluation metrics. 
% \vspace{-3mm}
\subsection{Transmission Strategy and Model Configuration} \label{subsec: cal}
{\color{black}We calculate the widely-adopted {\it channel bandwidth ratio} as $\text{CBR} = \frac{K}{C_{\text{in}}HWF} \quad \text{symbols/pixel}$ \cite{DBLP:journals/jsac/WangDLNSDQZ23, DBLP:journals/jsac/TungG22}.
Here, $K$ denotes the total number of symbols transmitted over the wireless channels, while $C_{\text{in}}$, $H$, $W$, and $F$ correspond to the number of input channels, height, width, and video frames, respectively.}

Considering a low CBR scenario, we set the CBR of the scheme to approximately be the level of $10^{-2}$.
{\color{black}In our research, we develop three semantic transmission strategies, each tailored to leverage different aspects of video content for reconstruction. }


    {\color{black}\textbf{Sketches+Desc.:} In this scheme, we transmit the sketch for each frame to precisely control the layout of objects. Key sketches are transmitted using DJSCC~\cite{dai2022nonlinear}, while the other sketches are sent via deep video semantic transmission (DVST)~\cite{DBLP:journals/jsac/WangDLNSDQZ23}. The average CBR for sketch sequence transmission is 0.0026.} For the transmission of descriptions, we employ turbo coding with a code rate $ R_c = \frac{1}{3} $ and complement it with 4-QAM constellation modulation. Consequently, the number of description symbols to be transmitted is given by $K_d = \frac{N_d}{M R_c} = 1.5 N_d $, where $ N_d $ represents the number of bits for the textual description, $ M $ is the modulation order (which is 2 for 4-QAM), and $ R_c $ is the code rate. Given that the average number of description tokens per video is 95.63, each token typically requires 8 bits, leading to $N_d = 8 \times 95.63 = 765.04 \, \text{bits} $. Therefore, the average number of symbols $K_d = 1.5 \times 765.04 = 1147.56$. The VideoComposer~\cite{DBLP:conf/nips/WangYZCWZSZZ23} subsequently fuses these sketches with their associated descriptions to reconstruct the video, ensuring high fidelity by leveraging extensive semantic information. {\color{black}This scheme achieves an total average CBR of $0.003$.}

    {\color{black} \textbf{Sketch+Desc.:} To enhance the clarity of visual semantics during transmission, we opt for transmitting only the initial frame's sketch, thereby concentrating resources on its quality. An initial sketch of the video is transmitted with video description. The sketch is encoded using the DJSCC optimized for sketch transmission. Stable Diffusion 3.5~\cite{esser2024scaling} is used to reconstruct the first frame of the video, after which Open-Sora~\cite{open_sora_2024} reconstruct the whole video.} We configure the DJSCC-sketch and train the model over an SNR range from 0 to 10\,dB. Testing is performed across all the SNRs within this range. The total number of symbols required for this strategy is given by $K = 1.5N_d + N_s $ where $N_s$ represents the number of transmitted symbols for the sketch, and $1.5N_d = 1147.56$ symbols are required for the video description. The output size of the DJSCC encoder is $32 \times 32 \times 2$. Therefore, the resource blocks allocated for the sketch data are calculated as $\frac{32 \times 32 \times 2}{2} = 1024 $ i.e., resource blocks for sketch. Thus, the total number of resource blocks required for the ``Sketch+Desc." scheme is:$ K = 1147.56 + 1024 = 2171.56 $. Consequently, the CBR for this scheme is $0.001$.

    {\color{black}\textbf{First Frame+Desc.:} For acquiring more detailed semantic information, we choose to transmit the first frame with video descriptions, which are transmitted using DJSCC. Open-Sora then utilizes these rich semantic inputs to reconstruct the video. This scheme combines a high-quality visual anchor with textual semantics to detail dynamic content, ensuring the high-quality video reconstruction. For this scheme, we have specifically trained the DJSCC to optimize the transmission of the first frame. The video description is transmitted as ``Desc. Only" scheme. This scheme achieve a average CBR of $0.0057$.}

\subsection{Evaluation Metrics}
The GVSC Framework adopts the following metrics to evaluate semantic similarity.
\textbf{CLIP score:} {\color{black} The CLIP score~\cite{radford2021learning}, widely adopted due to its training on large image-text datasets \cite{careil2023towards, lei2023text+, fan2024semantic, li2024misc}, effectively captures high-level semantic features and is suitable for assessing semantic similarity.} We use the average CLIP score to compare each generated frame with its corresponding original frame. It effectively measures the semantic similarity of each video frame. The CLIP score focuses on frame-level semantic similarity, ensuring each generated frame matches its original counterpart.
\textbf{BERT score:} We design a video semantic consistency check scheme. To assess overall semantic similarity, we use Video-LLaVA to generate captions for both videos and evaluate their similarities with a pretrained BERT model~\cite{zhang2019bertscore}. Given Video-LLaVA's generative diversity, we sample captions 3 times for each video and average the scores for the overall semantic score. The BERT score evaluates the video-level semantic consistency by comparing generated captions, measuring how well the generated video captures the broader narrative and context.
{\color{black}
\textbf{PSNR and SSIM~\cite{wang2004image}:} The peak signal-to-noise ratio (PSNR) and structure similarity index measure (SSIM) are traditional video quality metrics, providing insights into pixel and structure-level fidelity. They measure the peak SNR and structural similarity, respectively.
}

\section{Simulations}
In this section, we introduce the datasets used, the comparison models, the evaluation schemes and the experimental results in detail.
\subsection{Simulation Setups}

\subsubsection{Dataset}
We use the WebVid dataset \cite{DBLP:conf/iccv/BainNVZ21} for both training and testing our model. WebVid is a video dataset crawled from the Internet, including videos and corresponding captions. It encompasses a variety of video scenes, including character movements and natural scenery. We use the captions in the WebVid as description semantic information. Each video has $F$ = 8 frames, and we set the video resolution to 256 \(\times\) 256, so the height $H$ and width $W$ of each frame is $H$ = $W$ = 256. For the transmission model, we employ PiDiNet~\cite{DBLP:conf/iccv/0002LYH00P021} to extract over 11,000 sketches from WebVid for training and 129 videos for testing. 

\subsubsection{Simulation Parameters}
For transmitting sketches, we train the DJSCC for a total of 200 epochs using the Adam optimizer and batchsize is 16. The learning rate is $1 \times 10^{-4}$.
Following the human assessment simulations, we set \( k = 0.3 \) in~(\ref{loss func}). 
We train our transmission model over a SNR range of 0 to 10\,dB and test all models within this range. Due to the excessive number of parameters, we freeze the GenAI model. We set the frame rate to 4, the inference time-step to 50, and the random seed to 7777.

We use an additive white Gaussian noise (AWGN) channel, where the noise vector \(\textbf{n}\) is modeled as a complex Gaussian distribution \(\textbf{n} \sim \mathcal{CN}(0, \sigma^2 \textbf{I})\), with zero mean and covariance \(\sigma^2 \textbf{I}\). Here, \(\sigma\) represents the standard deviation of the noise, and \(\textbf{I}\) is the identity matrix whose dimensions correspond to the number of dimensions in \(\textbf{n}\). 

\subsection{Comparison Schemes}
\textbf{H.264/H.265+LDPC:}
As comparative schemes, we use H.264/H.265 for source coding, combined with low-density parity-check (LDPC) codes for channel coding and 4-QAM. 
The quantization parameter of H.264/H.265 is adapted to the same CBR as used in the proposed schemes. We set \(\text{CBR}_{\text{H.264}} = 0.008\) and \(\text{CBR}_{\text{H.265}} = 0.008\) in our simulations.


\textbf{DJSCC-RGB:} 
We use DJSCC with three layers of upsampling and downsampling modules. We maintain the original channel setting (3 channels) to directly transmit the RGB frames of the videos.

{\color{black}\textbf{Desc. Only:}
We employ turbo coding with a coding rate $ R_c = \frac{1}{3} $ and 4-QAM constellation modulation to transmit video descriptions. At the receiver, Open-Sora~\cite{open_sora_2024} reconstructs the video solely from these descriptions, demonstrating the effectiveness of text-based video generation. The average CBR of this scheme is $0.0007$.} 

\color{black}{\textbf{DVST:}
To leverage the inter-frame dependencies effectively, we utilizes nonlinear transform source channel coding (NTSCC)~\cite{dai2022nonlinear} for key frames, while DVST~\cite{DBLP:journals/jsac/WangDLNSDQZ23} handles the other frames. This scheme maintains an average CBR of $0.004$.}

\begin{figure}
    \centering
    % 左 下 右 上
    \includegraphics[trim= 25 19 40 63, clip, scale=0.3]{clip.pdf}
    % \vspace{-3mm}
    \caption{CLIP score of different schemes for various SNRs.}
    % \vspace{-4mm}
    \label{fig:snr-CLIP}
% \vspace{-1mm}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[trim= 25 19 40 63, clip, scale=0.3]{BERT.pdf}
    % \vspace{-4mm}
    \caption{BERT score of different schemes for various SNRs.}
    \label{fig:snr-BERT}
% \vspace{-4mm}
\end{figure}

\begin{figure}[t]
% \vspace{-7mm}
    \centering
    \includegraphics[trim= 25 19 40 63, clip, scale=0.3]{psnr.pdf}
    % \vspace{-4mm}
    \caption{\color{black}{PSNR of different schemes for various SNRs.}}
    \label{fig:snr-psnr}
% \vspace{-4mm}
\end{figure}

\begin{figure}[t]
% \vspace{-2mm}
    \centering
    \includegraphics[trim= 25 19 40 63, clip, scale=0.3]{ssim.pdf}
    % \vspace{-4mm}
    \caption{\color{black}{SSIM of different schemes for various SNRs.}}
    \label{fig:snr-SSIM}
% \vspace{-4mm}
\end{figure}


\subsection{Simulation Results}

\color{black}{To evaluate the performance of our proposed video semantic extraction strategies, we conducted extensive simulations across a range of SNR conditions. Fig.~\ref{fig:case study} shows the visualization of different schemes. Fig.~\ref{fig:snr-CLIP}, Fig.~\ref{fig:snr-BERT}, Fig.~\ref{fig:snr-psnr} and Fig~\ref{fig:snr-SSIM} present the comparison results in terms of CLIP score, BERT score, PSNR, and SSIM, respectively.}

Visual comparisons of different transmission schemes at SNR = 10\,dB and SNR = 0\,dB are shown in Fig.~\ref{fig:case study}. The first column on the left displays the original video frames (the 1st and the 8th frames). Subsequent columns present visual outcomes of various methods at SNR = 10\,dB. The rightmost part highlights the DVST and ``First Frame+Desc." schemes at SNR = 0\,dB, both maintaining a CBR of $10^{-2}$. While H.264 and H.265 still preserve detail, they exhibit noticeable compression artifacts. DVST shows the effective transmission at SNR = 10\,dB with high visual quality, while DJSCC-RGB undergoes significant visual degradation due to ultra-low CBR, showing highly distorted frames. The ``Desc. Only" scheme preserves basic semantic information but loses structural details, leading to relatively poor video reconstruction. Conversely, the ``Sketch+Desc." and ``Sketches+Desc." schemes excel in retaining both structural and textual semantics, as reflected in descriptions such as \textit{``A boy is eating corn"}. In GVSC, the ``First Frame+Desc." scheme allocates more resources to the first frame, ensuring semantic consistency and excelling in visual clarity even at SNR = 0\,dB, as highlighted in the red circles. This method leverages textual descriptions to guide the dynamic aspects of the scene, providing an edge in realism and robustness in the low SNR range.


Fig.~\ref{fig:snr-CLIP} shows CLIP score of different schemes for various SNRs. Traditional codecs, e.g., H.264 and H.265, exhibit a cliff effect for SNR $<$ 5\,dB, indicating difficulties in preserving textual semantics under strong noisy conditions. DJSCC-RGB shows gradual improvement with increasing SNR but performs modestly due to noise at ultra-low CBR. The ``Desc. Only" scheme, relying solely on text, achieves lower CLIP score due to the lack of structural information but slightly outperforms DJSCC-RGB due to higher resolution. The ``Sketch+Desc." and ``Sketches+Desc." schemes, which integrate both structural and textual semantics, demonstrate superior performance as SNR increases. This highlights the effectiveness of combining sketches with textual descriptions. Due to blurriness, DVST exhibits lower CLIP score at low SNRs but it recovers rapidly with increasing SNR. Notably, the ``First Frame+Desc." scheme consistently outperforms other schemes under all SNR conditions and maintains high CLIP score even at low SNRs. This verifies the robustness of our GVSC framework in ensuring semantic integrity under different SNRs.


\begin{figure}[t]
    \centering
    % \vspace{-10mm}
     % 左 下 右 上
    \includegraphics[trim= 0 0 40 30, clip, scale=0.35]{MSE.pdf}
    % \vspace{-4mm}
    \caption{CLIP score for various SNRs with different \textit{k} in ``Sketch+Desc." scheme.}
    \label{fig:mselpips}
% \vspace{-6mm}
\end{figure}

Fig.~\ref{fig:snr-BERT} presents BERT score performance of different schemes for various SNR conditions. Similar to CLIP score, H.264 and H.265 have difficulty in transmitting video data effectively at SNR $<$ 5\,dB. The BERT scores for ``Sketch+Desc." and ``Sketches+Desc." schemes are lower than those of H.264 and H.265 because traditional schemes transmit detailed visual information closely related to the original video, leading to higher text similarity scores. When generating descriptions, the visual model might include non-critical details such as hair color or eye direction, which are not always relevant in the video generation context. Since BERT focuses on specific textual details, even slight discrepancies can result in lower scores. From Fig.~\ref{fig:snr-BERT}, we can conclude that the semantic information of different modalities has a significant impact on the BERT score. Specifically, the ``Desc. Only" scheme has the least semantic information, and DJSCC-RGB is heavily affected by low SNR. ``Sketch+Desc." and ``Sketches+Desc." schemes maintain basic control over structural and action semantics, so the BERT score is relatively high. The ``First Frame+Desc." scheme conveys richer semantic information and consequently achieves the highest BERT score.

Fig.~\ref{fig:snr-psnr} presents the PSNR performance of different schemes for various SNRs. Compared with other performance metrics, PSNR focuses primarily on pixel-level reconstruction quality. As depicted in Fig.~5, methods lacking strong semantic control capabilities usually struggle to achieve pixel level consistency, resulting in the lower PSNR scores. Although DJSCC-RGB may appear blurry, it achieves higher pixel similarity than ``Sketch+Desc." and ``Sketches+Desc." schemes. However, schemes using the first frame as semantic information still perform effectively on the PSNR, demonstrating the robust pixel-level accuracy.

As illustrated in Fig.~6, SSIM assesses the structural differences in video content. The ``Desc. Only" method lacks structured semantic controls and has the lowest SSIM score as shown in Fig. 6, indicating its poor performance in structural integrity. In contrast, methods utilizing sketches exhibit a gradual increase in SSIM scores with improving SNR, reflecting the enhanced ability to convey structural semantic information. This upward trend in SSIM scores highlights the capability of ``Sketch+Desc." and ``Sketches+Desc." schemes to improve structural fidelity in video reconstruction, thus enhancing the overall quality and viewing experience. Moreover, the ``First Frame+Desc." scheme, which utilizes the most comprehensive semantic information, exhibits the strongest performance in terms of structural semantics. This is reflected in its consistently high SSIM scores in all SNR levels, highlighting its effectiveness in preserving both the integrity and detail of video structure, thereby providing a compelling case for its use in scenarios that demand high fidelity in semantic and structural aspects.



\color{black}{
Fig.~\ref{fig:mselpips} shows the CLIP score performance across various SNR conditions for different sketch transmission loss functions. As SNR increases, CLIP score improve for all configurations, indicating better semantic consistency and visual fidelity. The configuration with \(k = 0.3\) consistently achieves the highest CLIP score, suggesting that balancing MSE and LPIPS loss leads to optimal performance.}



\section{Conclusion}
In this work, we proposed the GVSC framework, a novel approach to video semantic communication using GenAI large models. This framework integrates video semantic extraction, source-channel transmission, and semantic encoding/decoding/generation. {\color{black}Video semantics is still an emerging concept, with varying interpretations across different tasks. In our paper, we adhere to the consistency of the transmitted semantic information. To address diverse transmission needs, we designed multimodal strategies. Compared with baselines like CLIP score, BERT score, PSNR, and SSIM, our schemes reduce transmission resources while achieving high-quality video reconstruction.} We developed a weighted loss function to enhance the sketch transmission. Extensive simulations show that our schemes achieves a high semantic similarity at low CBR, demonstrating the feasibility of GenAI for efficient bandwidth-limited video transmission. {\color{black}While the proposed framework has no obvious advantages over conventional solutions in processing latency of video reconstruction, it holds significant potential under extremely low bandwidth and streaming media where instant interaction is not necessary. As generative model optimizations advance, we anticipate substantial enhancements in the real-time capabilities of our framework, promising a transformative impact on future communication systems.}

% \cite{}

% \bibliography{reference}
% \bibliographystyle{IEEEtran}

% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{pokhrel2022understand}
S.~R. Pokhrel \emph{et~al.}, ``Understand-before-talk ({UBT}): A semantic
  communication approach to {6G} networks,'' \emph{{IEEE} Trans. Veh.
  Technol.}, vol.~72, no.~3, pp. 3544--3556, 2022.

\bibitem{wang2022transformer}
Y.~Wang \emph{et~al.}, ``Transformer-empowered {6G} intelligent networks: From
  massive {MIMO} processing to semantic communication,'' \emph{{IEEE} Wireless
  Commun.}, vol.~30, no.~6, pp. 127--135, 2022.

\bibitem{liu2024near}
H.~Liu \emph{et~al.}, ``Near-space communications: The last piece of {6G}
  space--air--ground--sea integrated network puzzle,'' \emph{Space Sci.
  Technol.}, vol.~4, p. 0176, 2024.

\bibitem{wu2024deep}
M.~Wu \emph{et~al.}, ``Deep joint semantic coding and beamforming for
  near-space airship-borne massive {MIMO} network,'' \emph{arXiv preprint
  arXiv:2405.19889}, 2024.

\bibitem{DBLP:journals/tccn/BourtsoulatzeKG19}
E.~Bourtsoulatze \emph{et~al.}, ``Deep joint source-channel coding for wireless
  image transmission,'' \emph{{IEEE} Trans. Cogn. Commun. Netw.}, vol.~5,
  no.~3, pp. 567--579, 2019.

\bibitem{DBLP:journals/jsac/TungG22}
T.~Tung \emph{et~al.}, ``{DeepWiVe}: Deep-learning-aided wireless video
  transmission,'' \emph{{IEEE} J. Select. Areas Commun.}, vol.~40, no.~9, pp.
  2570--2583, 2022.

% \bibitem{cui2024overview}
% Q.~Cui \emph{et~al.}, ``Overview of ai and communication for 6G network:
%   Fundamentals, challenges, and future research opportunities,'' \emph{arXiv
%   preprint arXiv:2412.14538}, 2024.

\bibitem{lu2024generative}
J.~Lu \emph{et~al.}, ``Generative AI-enhanced multi-modal semantic
  communication in internet of vehicles: System design and methodologies,''
  \emph{arXiv preprint arXiv:2409.15642}, 2024.

\bibitem{10614204}
C.~Liang \emph{et~al.}, ``Generative AI-driven semantic communication networks:
  Architecture, technologies, and applications,'' \emph{IEEE Trans. Cogn.
  Commun. Netw.}, vol.~11, no.~1, pp. 27--47, 2025.


\bibitem{qiao2025tokencommunicationsunifiedframework}
L.~Qiao \emph{et~al.}, ``Token communications: A unified framework for
  cross-modal context-aware semantic communications,'' \emph{arXiv
  preprint arXiv:2502.12096}, 2025.
  
\bibitem{juang1986maximum}
B.-H. Juang \emph{et~al.}, ``Maximum likelihood estimation for multivariate
  mixture observations of markov chains (corresp.),'' \emph{IEEE Trans. Inf.
  Theory}, vol.~32, no.~2, pp. 307--309, 1986.

\bibitem{kingma2013auto}
D.~P. Kingma \emph{et~al.}, ``Auto-encoding variational bayes,'' \emph{arXiv
  preprint arXiv:1312.6114}, 2013.

\bibitem{dang2019real}
Y.~Dang \emph{et~al.}, ``Real-time semantic plane reconstruction on a monocular
  drone using sparse fusion,'' \emph{{IEEE} Trans. Veh. Technol.}, vol.~68,
  no.~8, pp. 7383--7391, 2019.

\bibitem{yu2017seqgan}
L.~Yu \emph{et~al.}, ``{SeqGAN}: Sequence generative adversarial nets with
  policy gradient,'' in \emph{Proc. Conf. Artif. Intell. (AAAI)}, vol.~31,
  no.~1, 2017.

\bibitem{zhang2017adversarial}
Y.~Zhang \emph{et~al.}, ``Adversarial feature matching for text generation,''
  in \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, 2017, pp. 4006--4015.

\bibitem{erdemir2023generative}
E.~Erdemir, T.-Y. Tung, P.~L. Dragotti, and D.~G{\"u}nd{\"u}z, ``Generative
  joint source-channel coding for semantic image transmission,'' \emph{{IEEE}
  J. Select. Areas Commun.}, vol.~41, no.~8, pp. 2645--2657, 2023.

\bibitem{tang2024evolving}
S.~Tang, Q.~Yang, D.~G{\"u}nd{\"u}z, and Z.~Zhang, ``Evolving semantic
  communication with generative modelling,'' in \emph{2024 IEEE 35th
  International Symposium on Personal, Indoor and Mobile Radio Communications
  (PIMRC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2024, pp. 1--6.

\bibitem{DBLP:conf/nips/SahariaCSLWDGLA22}
C.~Saharia \emph{et~al.}, ``Photorealistic text-to-image diffusion models with
  deep language understanding,'' in \emph{Proc. Adv. Neural Inf. Process. Syst.
  (NeurIPS)}, 2022.

\bibitem{brooks2024video}
T.~Brooks \emph{et~al.}, ``Video generation models as world simulators,''
  \url{https://openai.com/research/video-generation-models-as-world-simulators},
  2024.

% \bibitem{yilmaz2024high}
% S.~F. Yilmaz \emph{et~al.}, ``High perceptual quality wireless image delivery
%   with denoising diffusion models,'' in \emph{Proceedings of the IEEE Infocom
%   Workshop}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2024, pp. 1--5.

% \bibitem{yang2024rate}
% P.~Yang \emph{et~al.}, ``Rate-adaptive generative semantic communication using
%   conditional diffusion models,'' \emph{{IEEE} Wireless Commun. Lett.}, 2024.

% \bibitem{duan2024dm}
% Y.~Duan \emph{et~al.}, ``Dm-mimo: Diffusion models for robust semantic
%   communications over mimo channels,'' in \emph{Proc. IEEE/CIC Int. Conf.
%   Commun. China (ICCC)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2024, pp.
%   1609--1614.

% \bibitem{wu2023cddm}
% T.~Wu, Z.~Chen, D.~He, L.~Qian, Y.~Xu, M.~Tao, and W.~Zhang, ``Cddm: Channel
%   denoising diffusion models for wireless communications,'' in \emph{Proc. IEEE
%   Glob. Commun. Conf. (GLOBECOM)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
%   2023, pp. 7429--7434.

% \bibitem{zhang2025semantics}
% M.~Zhang \emph{et~al.}, ``Semantics-guided diffusion for deep joint
%   source-channel coding in wireless image transmission,'' \emph{arXiv preprint
%   arXiv:2501.01138}, 2025.
\bibitem{qiao2024latency}
L.~Qiao \emph{et~al.}, ``Latency-aware generative semantic communications with
  pre-trained diffusion models,'' \emph{{IEEE} Wireless Commun. Lett.}, 2024.

\bibitem{10734812}
G.~Cicchetti, E.~Grassucci, J.~Park, J.~Choi, S.~Barbarossa, and
  D.~Comminiello, ``Language-oriented semantic latent representation for image
  transmission,'' in \emph{2024 IEEE 34th International Workshop on Machine
  Learning for Signal Processing (MLSP)}, 2024, pp. 1--6.

\bibitem{yang2024diffusion}
M.~Yang \emph{et~al.}, ``Diffusion-aided joint source channel coding for high
  realism wireless image transmission,'' \emph{arXiv preprint
  arXiv:2404.17736}, 2024.

% \bibitem{tang2024retrieval}
% S.~Tang, R.~Zhang, Y.~Yan, Q.~Yang, D.~Niyato, X.~Wang, and S.~Mao,
%   ``Retrieval-augmented generation for genai-enabled semantic communications,''
%   \emph{arXiv preprint arXiv:2412.19494}, 2024.
  
\bibitem{DBLP:journals/corr/abs-2402-08934}
B.~Li \emph{et~al.}, ``Extreme video compression with prediction using
  pre-trained diffusion models,'' in \emph{Proc. Int. Conf. Wireless Commun.
  Signal Process. (WCSP)}, 2024, pp. 1449--1455.

\bibitem{liu20242}
M.~Liu \emph{et~al.}, ``$\text{I}^2$\text{VC}: A unified framework for intra-
  \& inter-frame video compression,'' \emph{arXiv preprint arXiv:2405.14336},
  2024.

\bibitem{proakis2008digital}
J.~G. Proakis \emph{et~al.}, \emph{Digital communications}.\hskip 1em plus
  0.5em minus 0.4em\relax McGraw-hill, 2008.

\bibitem{DBLP:journals/corr/abs-2311-10122}
B.~Lin \emph{et~al.}, ``{Video-LLaVA}: Learning united visual representation by
  alignment before projection,'' \emph{arXiv preprint arXiv:2311.10122}, 2023.

\bibitem{cherti2023reproducible}
M.~Cherti \emph{et~al.}, ``Reproducible scaling laws for contrastive
  language-image learning,'' in \emph{Proc. IEEE Conf. Comput. Vis. Pattern
  Recognit. (CVPR)}, 2023, pp. 2818--2829.

\bibitem{DBLP:journals/jsac/WangDLNSDQZ23}
S.~Wang \emph{et~al.}, ``Wireless deep video semantic transmission,''
  \emph{{IEEE} J. Sel. Areas Commun.}, vol.~41, no.~1, pp. 214--229, 2023.

\bibitem{dai2022nonlinear}
J.~Dai \emph{et~al.}, ``Nonlinear transform source-channel coding for semantic
  communications,'' \emph{{IEEE} J. Select. Areas Commun.}, vol.~40, no.~8, pp.
  2300--2316, 2022.

\bibitem{DBLP:conf/nips/WangYZCWZSZZ23}
X.~Wang \emph{et~al.}, ``{VideoComposer}: Compositional video synthesis with
  motion controllability,'' in \emph{Proc. Adv. Neural Inf. Process. Syst.
  (NeurIPS)}, 2023.

\bibitem{esser2024scaling}
P.~Esser \emph{et~al.}, ``Scaling rectified flow transformers for
  high-resolution image synthesis,'' in \emph{Proc. Int. Conf. Mach. Learn.
  (ICML)}, 2024.

\bibitem{open_sora_2024}
{Open-Sora}, ``{Open-Sora},'' \url{https://hpcaitech.github.io/Open-Sora/},
  2024, accessed: 2024-07-12.

\bibitem{radford2021learning}
A.~Radford \emph{et~al.}, ``Learning transferable visual models from natural
  language supervision,'' in \emph{Proc. Int. Conf. Mach. Learn. (ICML)}, 2021,
  pp. 8748--8763.

\bibitem{careil2023towards}
M.~Careil \emph{et~al.}, ``Towards image compression with perfect realism at
  ultra-low bitrates,'' in \emph{Proc. Int. Conf. Learn. Representations
  (ICLR)}, 2023.

\bibitem{lei2023text+}
E.~Lei \emph{et~al.}, ``Text+ sketch: Image compression at ultra low rates,''
  \emph{ICML Neural Compression Workshop}, 2023.

\bibitem{fan2024semantic}
S.~Fan \emph{et~al.}, ``Semantic feature decomposition based semantic
  communication system of images with large-scale visual generation models,''
  \emph{arXiv preprint arXiv:2410.20126}, 2024.

\bibitem{li2024misc}
C.~Li \emph{et~al.}, ``Misc: Ultra-low bitrate image semantic compression
  driven by large multimodal model,'' \emph{IEEE Trans. Image Process.}, 2024.

\bibitem{zhang2019bertscore}
T.~Zhang \emph{et~al.}, ``Bertscore: Evaluating text generation with bert,''
  \emph{arXiv preprint arXiv:1904.09675}, 2019.

\bibitem{wang2004image}
Z.~Wang \emph{et~al.}, ``Image quality assessment: from error visibility to
  structural similarity,'' \emph{IEEE Trans. Image Process.}, vol.~13, no.~4,
  pp. 600--612, 2004.

\bibitem{DBLP:conf/iccv/BainNVZ21}
M.~Bain \emph{et~al.}, ``Frozen in time: {A} joint video and image encoder for
  end-to-end retrieval,'' in \emph{Proc. IEEE Int. Conf. Comput. Vis. (ICCV)},
  2021, pp. 1708--1718.

\bibitem{DBLP:conf/iccv/0002LYH00P021}
Z.~Su \emph{et~al.}, ``Pixel difference networks for efficient edge
  detection,'' in \emph{Proc. IEEE Int. Conf. Comput. Vis. (ICCV)}, 2021, pp.
  5097--5107.

\end{thebibliography}


\end{document}


