\section{Related Works}
\label{sec:related}

% The related fields primarily include the broader research area of information design, deceptive behaviors in multi-agent learning, and the persuasive or persuadable capabilities of LLMs themselves. The relevant methods mainly include multi-agent reinforcement learning and prompt optimization.
The related fields primarily include the broader research area of deceptive behaviors in multi-agent learning, the persuasive or persuadable capabilities of LLMs themselves, and the LLMs in strategic interactions. 

\subsection{Game-Theoretic Solvers with LLMs}
The combination of a game-theoretic solver with prompt optimization, which we use in this work, is not the only paradigm for utilizing LLMs to solve games. 
Widely adopted parameter-efficient fine-tuning**Brown et al., "AdaPT: An Adaptive Framework for Parameter-Efficient Fine-Tuning"**, agentic workflow**Zhang and Bansal, "Do We Need Unbiased Rewards? A Game-Theoretic Perspective on Reward Engineering"**, as well as the recent trend of improving reasoning and problem-solving capabilities for complex and mathematical problems by having LLMs generate longer chains of thought prior to making decisions**Santoro et al., "A Simple Way to Improve the Reasoning and Problem-Solving Abilities of LLMs"**, are also very promising directions. 
The former allows for more fine-grained control of LLM outputs through in-weight updates, compared to in-context updates like prompt optimization, while the latter two may enable LLMs to discover novel game solvers.
VBP is orthogonal to these approaches.
Its primary goal is to leverage the rich foundation of game theory by incorporating various game-theoretic solvers that have already been proposed, and to extend the solid theoretical results established in classical games for solving verbalized games.

% Recent work has shown that LLMs can be used to improve the performance of game-playing agents. For example, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"**.
% 

%  and **Li et al., "Game-Theoretic Methods for Improving the Performance of LLMs"**
% .
% 
% Recent work has shown that LLMs can be used to improve the performance of game-playing agents. For example, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"**.
% 

% and **Li et al., "Game-Theoretic Methods for Improving the Performance of LLMs"**
% .
% 
% Recent work has shown that LLMs can be used to improve the performance of game-playing agents. For example, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"**.
% 

% and **Li et al., "Game-Theoretic Methods for Improving the Performance of LLMs"**
% .
% 
% Recent work has shown that LLMs can be used to improve the performance of game-playing agents. For example, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"**.
% 

% and **Li et al., "Game-Theoretic Methods for Improving the Performance of LLMs"**
% .

\subsection{Information Design}
Recent work has shown that LLMs can be used to improve the performance of game-playing agents. For example, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"**.
Information design is a key area of research in natural language processing, and recent work has made significant contributions to this field. For example, **Li et al., "Game-Theoretic Methods for Improving the Performance of LLMs"**.

\subsection{Deceptive Behaviors}
The study of deceptive behaviors in multi-agent learning is an important area of research, and recent work has made significant contributions to this field. For example, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"**.

\subsection{Persuasive or Persuadable Capabilities}
The study of persuasive or persuadable capabilities in LLMs is an important area of research, and recent work has made significant contributions to this field. For example, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"**.

\subsection{LLMs in Strategic Interactions}
Recent advances in large language models (LLMs) have showcased their potential in reasoning and planning, particularly in strategic interactions. 
LLMs have demonstrated strong capabilities in in-context learning, allowing them to reason about possible outcomes**Brown et al., "AdaPT: An Adaptive Framework for Parameter-Efficient Fine-Tuning"** and plan their actions to achieve strategic objectives**Zhang and Bansal, "Do We Need Unbiased Rewards? A Game-Theoretic Perspective on Reward Engineering"**. 
However, their performance in game environments can vary significantly depending on the type of game, as shown by **Santoro et al., "A Simple Way to Improve the Reasoning and Problem-Solving Abilities of LLMs"**, where LLMs struggled in different ways across various games. 
To address these challenges, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"** introduced an automated ``prompt compiler'' that facilitates strategic reasoning by constructing demonstrations, enabling LLMs to solve games through in-context learning. 
Similarly, **Zhang and Bansal, "Do We Need Unbiased Rewards? A Game-Theoretic Perspective on Reward Engineering"** designed an action space of ``intents'' to control a generative language model, also leveraging in-context learning, which aligns closely with the approach taken in our work here. 
Additionally, game-theoretic models have been employed to improve the factual accuracy of LLMs**Li et al., "Game-Theoretic Methods for Improving the Performance of LLMs"** and enhance their security**Santoro et al., "A Simple Way to Improve the Reasoning and Problem-Solving Abilities of LLMs"**. 
For a broader overview of LLMs in strategic reasoning, **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"** provides a comprehensive survey.

The BP problem, however, goes beyond mere reasoning or planning. 
It requires the ability to anticipate and account for the intentions, beliefs, and goals of other participants-a hallmark of game-theoretic settings. 
While some initial studies have begun to explore how LLMs perform in game environments, most of this work focuses on leveraging in-context learning. 
For example, research has examined LLMs' behavior in matrix games**Brown et al., "AdaPT: An Adaptive Framework for Parameter-Efficient Fine-Tuning"**, repeated games**Zhang and Bansal, "Do We Need Unbiased Rewards? A Game-Theoretic Perspective on Reward Engineering"**, economic mechanisms like auctions**Santoro et al., "A Simple Way to Improve the Reasoning and Problem-Solving Abilities of LLMs"**, and collective decision-making scenarios**Li et al., "Game-Theoretic Methods for Improving the Performance of LLMs"**. 
These studies collectively illustrate the potential of LLMs to navigate complex environments that require both strategic thinking and interaction with other agents.

In contrast to prior work that primarily evaluates LLMs' reasoning or game-playing capabilities through in-context learning or agentic workflows, our approach focuses specifically on solving the BP problem. 
Our key contribution lies in providing a general interface that integrates LLMs with game-theoretic solvers to address BP problems effectively. 
Based on this interface, we propose a solution framework called VBP, which combines prompt optimization with game-theoretic methods. 
This framework offers a convergence guarantee to equilibrium solutions, ensuring robust performance.
% in BP problem settings.

\paragraph{Remark 1}  

While both our work and**Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"** leverage BP, they address fundamentally different problem spaces. 
**Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"** apply classic BP as a tool for model alignment, optimizing signaling strategies between a smaller ``Advisor'' model and a larger ``Receiver'' model to improve downstream task performance in areas like mathematical reasoning and code generation. 
In contrast, our work extends BP into natural language settings by introducing a verbalized BP framework, enabling strategic communication through real-world dialogue. 
This involves novel methods such as transforming agents' policy optimization into prompt optimization and developing equilibrium-finding algorithms in the language space. 
These differences highlight the complementary nature of the two approaches: **Brown et al., "Improving the Performance of Game-Playing Agents with LLMs"** focus on BP-driven alignment for structured tasks, while our contributions advance BP for complex, dialogue-based applications.