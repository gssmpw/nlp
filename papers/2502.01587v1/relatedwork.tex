\section{Related Works}
\label{sec:related}

% The related fields primarily include the broader research area of information design, deceptive behaviors in multi-agent learning, and the persuasive or persuadable capabilities of LLMs themselves. The relevant methods mainly include multi-agent reinforcement learning and prompt optimization.
The related fields primarily include the broader research area of deceptive behaviors in multi-agent learning, the persuasive or persuadable capabilities of LLMs themselves, and the LLMs in strategic interactions. 

\subsection{Game-Theoretic Solvers with LLMs}
The combination of a game-theoretic solver with prompt optimization, which we use in this work, is not the only paradigm for utilizing LLMs to solve games. 
Widely adopted parameter-efficient fine-tuning~\citep{xu2023parameter,han2024parameter}, agentic workflow~\citep{mao2023alympics,hua2024game,guo2024can,fan2024can,lore2024strategic,duan2024gtbench}, as well as the recent trend of improving reasoning and problem-solving capabilities for complex and mathematical problems by having LLMs generate longer chains of thought prior to making decisions~\citep{zelikman2022star,zelikman2024quiet,o1,guo2025deepseek}, are also very promising directions. 
The former allows for more fine-grained control of LLM outputs through in-weight updates, compared to in-context updates like prompt optimization, while the latter two may enable LLMs to discover novel game solvers.
VBP is orthogonal to these approaches.
Its primary goal is to leverage the rich foundation of game theory by incorporating various game-theoretic solvers that have already been proposed, and to extend the solid theoretical results established in classical games for solving verbalized games.

% \subsection{Information Design}\label{sec:id}

% % cheap-talking algorithm

% % Markov Signaling Game

% Information design has recently been extended to sequential
% scenarios~\citep{gan2022sequential}. 
% To model the coupling decision processes of the sender and the receiver, Markov persuasion processes (MPPs) are proposed in~\citet{gan2022bayesian} and~\citet{wu2022sequential}.
% \citet{gan2022bayesian} proves that persuading a farsighted receiver in MPPs is NP-hard, and \citet{wu2022sequential} proposes a learning method for persuading a bunch of one-shot myopic receivers in MPPs. 
% On the other hand, \citet{bernasconi2022sequential} proposes a learning method for a sender to persuade a far-sighted receiver without knowing the prior belief. 
% Besides, \citet{celli2020private} proposed a variant of obedience constraints for persuading multiple receivers in sequential interactions. 
% \citet{lin2023information} proposes a more practical MARL methods by lifting the dependence on the commitment assumption and the revelation principle.

\subsection{Deception in Multi-Agent Learning}\label{sec:deception}

% \citet{bond1988evolution} defines deception as a ``false communication that tends to benefit the
% communicator''. 
% In the context of social learning, deception can be thought of as a way for the communicator to establish a cooperative equilibrium that is sub-optimal from the perspective of total population welfare. 
% While deception has been studied within MARL~\citep{asgharnia2020deception,bontrager2019superstition,li2020effective,ghiya2020learning}, it is usually only applied on algorithms that allow limited influence capabilities between agents. 
% Recent developments provide evidence for the vulnerability of agents which rely on the signals of
% others to guide their own learning~\citep{chelarescu2021deception}. 
% While most research highlights the improvements created by mechanism design, it also exposes its unexplored risks. 
% A non-exhaustive list of key papers is identified~\citep{hughes2018inequity,jaques2019social,yang2020learning,lupu2020gifting,ndousse2021emergent}, and their premise, methodology, and conclusions are explained and analysed.
% 不同于上述方法主要研究 reward 的改变，例如 mechanism design涌现或引导出的 deception 行为，本文主要研究信息的改变对 deception 行为的影响。

\citet{bond1988evolution} defines deception as false communication that benefits the communicator. 
In social learning, deception can be viewed as a means for the communicator to establish a cooperative equilibrium that is suboptimal for overall population welfare. 
Previous studies have explored deception within multi-agent reinforcement learning (MARL) settings~\citep{asgharnia2020deception, bontrager2019superstition, li2020effective, ghiya2020learning}, but these efforts typically focus on environments where agents have limited capacity to influence one another. 
More recent work~\citep{chelarescu2021deception} highlights the vulnerability of agents dependent on signals from others to guide their learning processes, pointing to the potential risks inherent in such scenarios. 
While much research focuses on the positive outcomes of mechanism design, it also reveals unforeseen risks, such as the emergence of deceptive behaviors~\citep{hughes2018inequity, jaques2019social, yang2020learning, lupu2020gifting, ndousse2021emergent}. 
Unlike these prior studies, which primarily examine how reward modifications influence deception through mechanisms like mechanism design, our work emphasizes the role of information manipulation in shaping deceptive behavior.

% Previous game-theoretic approaches have typically modelled deception through signalling~\citep{ho1978teams}, where one player can, at a cost, send a signal conveying false information. 
% An example in network security, examined by~\citet{carroll2011game}, has a defender who may attempt to deceive an attacker by disguising honeypots as regular computers.
% Other work has explored the evolution of deceptive signalling in competitive-cooperative learning environments, \citet{floreano2007evolutionary} found that teams of robots in competitive food-gathering experiments spontaneously evolved deceptive communication strategies, reducing competition for food sources by causing harm to opponents. 
% An extension to game theory is hypergame theory~\citep{bennett1980hypergames}. 
% Hypergame theory models games where players may be uncertain about others players’ preferences (strategies), and therefore may disagree on what game they are playing. 
% Because the model includes differences in agents’ perception of the game, hypergame theory provides a basis for modelling misperception, false belief, and deception~\citep{kovach2015hypergame}. 
% Examples of hypergame analysis in practice include~\citet{vane2002using}, who consider deception in single-stage, normal form hypergames, and~\citet{gharesifard2013stealthy} who model deception about player preferences for games in which the deceiver has complete knowledge of the target. 
% \citet{ettinger2010theory}, \citet{strouse2018learning} and~\citet{aitchison2021learning} incentivise agents to manage information about their roles to achieve decepotion. 
% They use mutual information between goal and state as a regularisation term during optimisation to encourage or discourage agents from revealing their goals.
% 不同于这些工作将 deception 建模为显示地离散动作，本文研究通过自然语言交互的方式来实现 deception。

Game-theoretic models traditionally frame deception using signaling~\citep{ho1978teams}, where one player can send costly signals to convey false information. 
In network security, for instance, \citet{carroll2011game} examined how defenders can deceive attackers by masking honeypots as regular computers. 
Other research has studied the evolution of deceptive signaling in mixed environments. 
\citet{floreano2007evolutionary} demonstrated that, in competitive food-gathering tasks, teams of robots spontaneously developed deceptive strategies, misleading competitors to reduce resource competition.
An extension to classical game theory, known as hypergame theory~\citep{bennett1980hypergames}, accounts for players' uncertainty about others' strategies or preferences, leading to disagreements about the underlying game being played. 
By incorporating agents' differing perceptions, hypergame theory provides a natural framework to model misperception, false beliefs, and deception~\citep{kovach2015hypergame}. 
Applications of hypergame theory include \citet{vane2002using}, who analyzed deception in normal-form hypergames, and \citet{gharesifard2013stealthy}, who modeled deception based on player preferences when the deceiver has full knowledge of the target. 
Additionally, \citet{ettinger2010theory}, \citet{strouse2018learning}, and \citet{aitchison2021learning} show how agents can manage information about their roles to achieve deception by regularizing mutual information between goals and states.
In contrast to these works, which model deception as discrete, explicit signaling actions, our study explores how deception can be realized through natural language interaction.

% \citet{macnally2018action} considers the broader problem of communicating intent in the absence of explicit signalling. 
% An online planner is used to select actions that implicitly communicate agent intent to an observer. 
% This approach has been applied to deception by~\citet{masters2017deceptive}, by maximising rather than minimising the difference between agent and observer beliefs. 
% Unlike our work, these approaches assume full observability, and require a model of the environment for forward planning.

Finally, \citet{macnally2018action} addresses the broader question of how agents can communicate intent without explicit signaling, using an online planner to select actions that implicitly reveal intent to an observer. 
\citet{masters2017deceptive} extended this approach to deception by maximizing the divergence between the agent's and observer's beliefs. 
However, these methods assume full observability and rely on environmental models for forward planning, whereas our work focuses on achieving deception through natural language in more complex, partially observable environments.

\subsection{Conversational Persuasiveness of LLMs}\label{sec:persuasion}

% In their research on enhancing the persuasiveness of complaints through LLMs, \citet{shin2024large} demonstrate that using ChatGPT to refine complaint narratives boosted consumers' likelihood of obtaining redress from finacial institutions, thereby illustrating that LLMs can augment human persuasiveness.
% \citet{carrasco2024large} shows that LLMs are better than humans in using cognitive load and moral/emotional language while creating persuasive content and urges the need for ethical guidelines and framework for such systems.
% \citet{breum2024persuasive} analyses the ability of LLMs to emulate persuasion dynamics and achieve opinion change in another LLM agent with a persona.
% \citet{ramani2024persuasion} presents a sophisticated multi-agent framework to enhance the LLMs' persuasiveness, where primary agent engages directly with users through persuasive dialogue while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategiesm and validation of facts.

% Studies show that LLM-generated persuasive text can influence humans. 
% Examples include GPT-3(3.5) messages influencing human political attitudes~\citep{bai_voelkel_eichstaedt_willer_2023}, GPT3 campaign messages for vaccines being more effective than those by professionals~\citep{karinshak2023working}, romantic chatbots captivating humans for longer than human-to-human conversations~\citep{zhou2020design}, human-level natural language negotiations in the strategy game Diplomacy~\citep{meta2022human}, and algorithmic response suggestions affecting emotional language in messaging~\citep{hohenstein2023artificial}. 
% \citet{salvi2024conversational} measures successful persuasion and finds that LLMs have the capability of changing opponents’ beliefs in a one-on-one debate task with higher odds than humans when taking personalization into account. 
% Different from prior works all focus on measuring the outcome of persuasive text, \citet{pauli2024measuring} focuses on measuring the language style in various domains. 
% Similarly, \citet{breum2024persuasive} uses LLaMA2 to generate persuasive dialogue on the topic of climate change. 
% \citet{majovsky2023artificial} shows that LLMs sound convincing when fabricating medical facts. 

% Recent work has demonstrated that large language models are highly effective at persuading people across a variety of tasks and domains~\citep{matz2024potential,durmus2024persuasion,burtell2023artificial,shin2023enhancing}, raising concerns about their potential use for widespread misinformation, manipulation, and deception~\citep{allen2024real,kreps2022all}. 
% These concerns are especially acute for people whose identifiable characteristics-such as race, gender, or sexual identity-subject them to higher rates of algorithmic persuasion and bias~\citep{bar2023algorithmic}.

% \citet{wojtowicz2024and} establishes a novel proof that persuasive messages are challenging to discover (NP-hard) but easy to adopt if supplied by others (NP).

Recent advancements in LLMs have shown their impressive potential in the realm of persuasion. 
A growing body of research highlights how these models can enhance human communicative abilities and even autonomously generate persuasive content across various contexts.

For instance, \citet{shin2024large} demonstrated that refining complaint narratives with ChatGPT significantly improved consumers' chances of obtaining redress from financial institutions, showcasing the role of LLMs in boosting human persuasive efforts. 
Similarly, \citet{carrasco2024large} showed that LLMs outperform humans in utilizing cognitive load and moral or emotional language when crafting persuasive messages, prompting the need for ethical guidelines governing their use. 
\citet{breum2024persuasive} further explored LLMs' capacity to simulate persuasive dynamics, revealing that LLMs can influence opinion changes in other LLMs with predefined personas. 
Building on this, \citet{ramani2024persuasion} introduced a multi-agent framework in which a primary agent engages users through persuasive dialogue, while auxiliary agents handle tasks such as information retrieval, response analysis, and strategy development.
These studies illustrate that LLMs are not only capable of enhancing human persuasion but also of autonomously refining and executing persuasive strategies.

The impact of LLM-generated persuasive text on human behavior has been demonstrated across a diverse range of domains. 
For example, \citet{bai_voelkel_eichstaedt_willer_2023} showed that GPT-3.5 could influence political attitudes, while \citet{karinshak2023working} found that GPT-3’s vaccine campaign messages were more effective than those created by professionals. 
Additionally, LLM-powered romantic chatbots have been shown to sustain human engagement longer than human-to-human conversations~\citep{zhou2020design}. 
In strategic contexts, LLMs have achieved human-level negotiation capabilities in games like Diplomacy~\citep{meta2022human}, and algorithmic suggestions have been shown to shape emotional language in messaging~\citep{hohenstein2023artificial}. 
These examples collectively highlight the broad applicability of LLMs in persuasive tasks and their significant influence on human decision-making.

However, the increasing persuasive power of LLMs also raises concerns about potential misuse. 
\citet{salvi2024conversational} found that LLMs outperform humans in personalized debates, achieving a higher rate of belief change in one-on-one discussions. 
This raises ethical concerns, particularly regarding the risks of misinformation and manipulation. 
For instance, \citet{majovsky2023artificial} demonstrated that LLMs can convincingly fabricate medical facts, further complicating the ethical landscape. 
The ability of LLMs to produce persuasive yet misleading content underscores the need for stronger oversight, especially in high-stakes domains such as healthcare, politics, and public discourse.
Recent studies have thus emphasized the necessity of ethical frameworks as LLMs become more adept at persuasion. 
While LLMs have shown persuasive power across various tasks and domains~\citep{matz2024potential,durmus2024persuasion,burtell2023artificial,shin2023enhancing}, they also pose risks, particularly for vulnerable populations. 
\citet{bar2023algorithmic} highlighted that characteristics such as race, gender, and sexual identity may subject certain groups to greater risks of algorithmic persuasion and bias, potentially exacerbating existing social inequalities. 
% Therefore, while LLMs offer significant benefits in persuasive interactions, their deployment demands careful ethical and societal consideration.

From a computational standpoint, \citet{wojtowicz2024and} provided a novel proof showing that discovering persuasive messages is NP-hard, while adopting persuasive strategies provided by others is NP-easy. 
This insight adds to our understanding of the complexity involved in generating persuasive content and demonstrates why LLMs, with their vast data-processing capabilities, are particularly adept at these tasks.
Building on these insights, our work explores how game-theoretic methods can be leveraged to enhance the persuasive capabilities of LLMs in purely multi-agent LLM systems. 
Unlike previous studies that primarily measure the impact of LLM-generated persuasive text on humans, we investigate how multiple LLMs can engage in persuasive interactions with one another, optimizing their strategies using game-theoretic approaches.

\subsection{LLMs in Strategic Interactions}

Recent advances in large language models (LLMs) have showcased their potential in reasoning and planning, particularly in strategic interactions. 
LLMs have demonstrated strong capabilities in in-context learning, allowing them to reason about possible outcomes~\citep{kojima2022large} and plan their actions to achieve strategic objectives~\citep{liu2023llm+}. 
However, their performance in game environments can vary significantly depending on the type of game, as shown by \citet{lore2023strategic}, where LLMs struggled in different ways across various games. 
To address these challenges, \citet{gandhi2023strategic} introduced an automated ``prompt compiler'' that facilitates strategic reasoning by constructing demonstrations, enabling LLMs to solve games through in-context learning. 
Similarly, \citet{meta2022human} designed an action space of ``intents'' to control a generative language model, also leveraging in-context learning, which aligns closely with the approach taken in our work here. 
Additionally, game-theoretic models have been employed to improve the factual accuracy of LLMs~\citep{jacob2024the} and enhance their security~\citep{ma2023red}. 
For a broader overview of LLMs in strategic reasoning, \citet{zhang2024llm} provides a comprehensive survey.

The BP problem, however, goes beyond mere reasoning or planning. 
It requires the ability to anticipate and account for the intentions, beliefs, and goals of other participants-a hallmark of game-theoretic settings. 
While some initial studies have begun to explore how LLMs perform in game environments, most of this work focuses on leveraging in-context learning. 
For example, research has examined LLMs' behavior in matrix games~\citep{xu2023magic,fan2024can}, repeated games~\citep{akata2023playing,zhang2024k,huang2024far,silva2024large}, economic mechanisms like auctions~\citep{chen2023put,mao2023alympics}, and collective decision-making scenarios~\citep{jarrett2023language}. 
These studies collectively illustrate the potential of LLMs to navigate complex environments that require both strategic thinking and interaction with other agents.

In contrast to prior work that primarily evaluates LLMs' reasoning or game-playing capabilities through in-context learning or agentic workflows, our approach focuses specifically on solving the BP problem. 
Our key contribution lies in providing a general interface that integrates LLMs with game-theoretic solvers to address BP problems effectively. 
Based on this interface, we propose a solution framework called VBP, which combines prompt optimization with game-theoretic methods. 
This framework offers a convergence guarantee to equilibrium solutions, ensuring robust performance.
% in BP problem settings.

\paragraph{Remark 1}  

While both our work and~\citet{bai2024efficient} leverage BP, they address fundamentally different problem spaces. 
\citet{bai2024efficient} apply classic BP as a tool for model alignment, optimizing signaling strategies between a smaller ``Advisor'' model and a larger ``Receiver'' model to improve downstream task performance in areas like mathematical reasoning and code generation. 
In contrast, our work extends BP into natural language settings by introducing a verbalized BP framework, enabling strategic communication through real-world dialogue. 
This involves novel methods such as transforming agents' policy optimization into prompt optimization and developing equilibrium-finding algorithms in the language space. 
These differences highlight the complementary nature of the two approaches: \citet{bai2024efficient} focus on BP-driven alignment for structured tasks, while our contributions advance BP for complex, dialogue-based applications.