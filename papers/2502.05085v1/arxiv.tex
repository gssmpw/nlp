% \documentclass{article}
\documentclass[twocolumn]{article}

\usepackage[a4paper, margin=0.75in]{geometry} % Adjust margin as needed
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage[most]{tcolorbox}
\usepackage{soul}
\usepackage{enumitem}
\usepackage{array}
\usepackage{xcolor}
\setlist[itemize]{itemsep=0mm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{appendix}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage[capitalize,noabbrev]{cleveref}

\definecolor{mydarkgreen}{HTML}{008A00}
\newcommand{\greentext}[1]{\textcolor{mydarkgreen}{\textbf{#1}}}

\definecolor{claimpurple}{HTML}{99479B}
\definecolor{myred}{HTML}{C73500}
\newcommand{\redtext}[1]{\textcolor{myred}{\textbf{#1}}}
\renewcommand{\sectionautorefname}{Section}

\newcommand{\bernhard}[1]{\textcolor{red}{\todo{B: #1}}} 
\newcommand{\bs}[2][inline]{\todo[#1,color=blue!10,size=\scriptsize]{#2}}
\newcommand{\felix}[1]{\textcolor{orange}{~F: #1}}
\newcommand{\zhijing}[1]{\textcolor{blue}{~Z: #1}}


\usepackage{newfloat}


\DeclareFloatingEnvironment[fileext=loex,listname={List of Examples},name=Example,placement=tp,]{exampleenv}
\DeclareFloatingEnvironment[fileext=loex,listname={List of Claims},name=Claim,placement=h!tp,]{claimenv}

\newenvironment{claim}[1]{
    \begin{claimenv}
    \vspace{-1mm}
    \refstepcounter{claimenv}
    \begin{tcolorbox}[colback=claimpurple!5!white,colframe=claimpurple!75!black,
        % boxrule=2pt,
        bottom=1mm,
        arc=3.4pt,
        % boxsep=-1mm,
        left=1mm,right=1mm, 
        title=\textbf{Claim \theclaimenv:} #1]
    \vspace{-1mm}
}{
    \end{tcolorbox}
    \vspace{-3mm}
    \end{claimenv}
}

\newcommand{\tmpCap}{}%temporary scale
\newenvironment{example}[2]{
    \renewcommand{\tmpCap}{#2} % NOTE: necessary to use parameters in enddef
    \begin{exampleenv}[#1]
    \vspace{-1mm}
    \begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,
        % boxrule=2pt,
        bottom=1mm,
        arc=3.4pt,
        % boxsep=-1mm,
        left=1mm,right=1mm]
    \vspace{-1mm}
}{   
    \end{tcolorbox}
    \vspace{-4mm}
    \caption{\tmpCap{} \vspace{-3mm}}
    \end{exampleenv}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\usepackage[textsize=tiny]{todonotes}

\title{Causality can systematically address \\ the monsters under the bench(marks)}

\author{Felix Leeb\textsuperscript{1,}\thanks{Email: \texttt{fleeb@tue.mpg.de}}, Zhijing Jin\textsuperscript{1,2,3}, and Bernhard Sch\"olkopf\textsuperscript{1} \\
\small{\textsuperscript{1}Max Planck Institute for Intelligent Systems, T\"ubingen} 
\quad
\textsuperscript{2}ETH Z\"urich
\quad
\textsuperscript{3}University of Toronto
}

\date{}

\begin{document}

\maketitle

% \twocolumn

\begin{abstract}

Effective and reliable evaluation is essential for advancing empirical machine learning. 
However, the increasing accessibility of generalist models and the progress towards ever more complex, high-level tasks make systematic evaluation more challenging.
Benchmarks are plagued by various biases, artifacts, or leakage, while models may behave unreliably due to poorly explored failure modes.
Haphazard treatments and inconsistent formulations of such ``monsters'' can contribute to a duplication of efforts, a lack of trust in results, and unsupported inferences.
In this position paper, we argue causality offers an ideal framework to systematically address these challenges.
By making causal assumptions in an approach explicit, we can faithfully model phenomena, formulate testable hypotheses with explanatory power, and leverage principled tools for analysis.
To make causal model design more accessible, we identify several useful Common Abstract Topologies (CATs) in causal graphs which help gain insight into the reasoning abilities in large language models.
Through a series of case studies, we demonstrate how the precise yet pragmatic language of causality clarifies the strengths and limitations of a method and inspires new approaches for systematic progress.
    
\end{abstract}

\section{Introduction}


% let's 
% - "exacerbating underlying issues," "amplifying existing challenges," or "bringing systemic flaws to the forefront." For an empirical machine learning context, you could say "exposing critical evaluation gaps" or "highlighting fundamental evaluation shortcomings." These phrases emphasize how large language models reveal and magnify existing issues.
% - papers that question some of the hype:

% bowmanDangersUnderclaimingReasons2022
% nezhurinaAliceWonderlandSimple2024
% benderDangersStochasticParrots2021
% alzahraniWhenBenchmarksAre2024
% yanWorseRandomEmbarrassingly2024
% chenPremiseOrderMatters2024


Machine learning achievements continue to break records and grab headlines, 
drawing attention from both the public and the research community.
% while capturing the imagination of the public, 
However, the rapid proliferation of powerful models and the increasing complexity of tasks 
continue to amplify existing challenges in reliable evaluation of these models~\citep{maoGPTEvalSurveyAssessments2024}.
Between inflated expectations~\citep{bubeckSparksArtificialGeneral2023,ullmanLargeLanguageModels2023,graceThousandsAIAuthors2024}, opaque or misleading assessments~\citep{martinezReevaluatingGPT4sBar2024}, and even the occasional mistake~\citep{chowdhuriNoGPT4Cant2023}, the poor communication~\citep{bowmanDangersUnderclaimingReasons2022} and unreliable benchmarks~\citep{rajiAIEverythingWhole2021,bowmanWhatWillIt2021,alzahraniWhenBenchmarksAre2024} can significantly undermine our understanding of the capabilities and limitations of these models~\citep{nezhurinaAliceWonderlandSimple2024,yanWorseRandomEmbarrassingly2024}. This risks a decline of public trust~\citep{benderDangersStochasticParrots2021,greenMythMethodologyRecontextualization2018,huWhatsSexGot2020} and perhaps even an AI winter.
A key issue is that many evaluations focus on performance alone~\citep{liangHolisticEvaluationLanguage2023}, failing to account for the reasoning process behind a model's behavior. 
For instance, a model may arrive at the right answer for the wrong reasons, making the performance an incomplete indication of its capabilities beyond the test set.

To systematically address the challenges in evaluating, in particular, large models, \textbf{this position paper argues for a shift toward causality-driven experimental design.}
By making causal assumptions explicit, 
we formulate precise hypotheses and underlying assumptions, 
diagnose model limitations, 
and leverage principled tools for analysis.
% We introduce Common Abstract Topologies (CATs), a set of causal graph structures that can help model common issues faced in designing an experiment or benchmark.
% In a variety of case studies we illustrate how a causal framing can clarify the strengths and limitations of a method, inspire new approaches, and deal with systematic biases.

% ,and inspire novel improvements.


% \begin{figure*}[ht]
%   % \centering 
%   \begin{subfigure}[b]{0.45\textwidth}
%   % \vskip 0.2in
%   \begin{center}
%   % \felix{use higher contrast colors in the bar plot}
%   \centerline{\includegraphics[width=0.95\columnwidth]{figures/reasoning.pdf}}
%   %\vspace{-3mm}
%   \caption{Growth of reasoning papers in ACL Anthology, among which the concept of ``causality'' is not growing at the same rate, suggesting that NLP is underutilizing causality.}
%   \label{fig:reasoning-papers}
%   %\vspace{-10mm}
%   \end{center}
%   % \vskip -0.2in
%   % \vspace{-6mm}
%   \end{subfigure}
%   % \hfill % add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
%     %(or a blank line to force the subfigure onto a new line)
%   \begin{subfigure}[b]{0.55\textwidth}
  
%   \begin{center}
%   \centerline{\includegraphics[width=0.9\columnwidth]{figures/dendrogram.pdf}}
%   %\vspace{-7mm}
%   \caption{
%   This dendrogram shows the co-occurrences of causal and causality-adjacent terms of papers that contain ``reasoning'' in the abstracts (total 3181 papers) from the ACL anthology from the past 10 years.
%   The numbers in parentheses indicate the number of papers that mention the term.
%   Note, that the very first split separates all the causality-related terms from the rest of the terms, suggesting relatively poor co-occurrence with other invariably related concepts. 
%   %Additionally, causality-related terms are generally rather underrepresented.
%   }
%   %\vspace{-10mm}
%   \label{fig:dendrogram}
%   \end{center}
%   \end{subfigure}
% \end{figure*}


\begin{figure}[tb]
  % \vskip 0.2in
  \begin{center}
  % \felix{use higher contrast colors in the bar plot}
  \centerline{\includegraphics[width=0.95\columnwidth]{figures/reasoning.pdf}}
  % \vspace{-3mm}
  \caption{Growth of reasoning papers in ACL Anthology, among which the concept of ``causality'' is not growing at the same rate, suggesting that NLP is underutilizing causality.}
  \label{fig:reasoning-papers}
  % \vspace{-10mm}
  \end{center}
  % \vskip -0.2in
  \vspace{-6mm}
\end{figure}
    



One subfield that is particularly well-fitted for more causal analyses is 
% that has garnered particular attention is 
the evaluation of reasoning abilities in large language models (LLMs)~\citep{huangReasoningLargeLanguage2023,yuNaturalLanguageReasoning2023}.
A cursory analysis of the recent NLP papers in the ACL anthology reveals a dramatic rise in the attention in reasoning capabilities of models, as seen in~\autoref{fig:reasoning-papers}. 
However, curiously, the subset of these papers that mention ``causality'' or ``causal'' in the title or abstract is not growing in tandem (yet).
% , suggesting that the NLP community may be underutilizing the causality.
%~\citep{zhangCarefulExaminationLarge2024}.
In fact, the dendrogram in~\autoref{fig:dendrogram} shows that among the reasoning papers, causality-related terms tend not to co-occur very much with many non-causal mimics (discussed in~\autoref{sec:monsters}). 
% This suggests a lack of awareness of how closely related these concepts are to causality. 
%Details for this analysis are found in the appendix~\felix{todo}.


\begin{figure}[tb]
  \begin{center}
  \centerline{\includegraphics[width=0.95\columnwidth]{figures/dendrogram.pdf}}
  \vspace{-3mm}
  \caption{
  This dendrogram shows the co-occurrences of causal and causality-adjacent terms of papers that contain ``reasoning'' in the abstracts (total 3181 papers) from the ACL anthology from the past 10 years.
  The numbers in parentheses indicate the number of papers that mention the term.
  Note, that the very first split separates all the causality-related terms from the rest of the terms, suggesting relatively poor co-occurrence with other invariably related concepts. 
  %Additionally, causality-related terms are generally rather underrepresented.
  }
  \vspace{-8mm}
  \label{fig:dendrogram}
  \end{center}
\end{figure}
  


Despite many of the issues appearing to be quite disparate based on the distinct terminology that is used, we argue that causality can serve as the framework to systematically study a wide array of issues thus bridging gaps between different subfields and approaches.
The expertise required to understand and satisfactorily mitigate these issues is very diverse, and we do not mean to suggest that causality is \textit{all} you need. Rather, we argue that causality already (often implicitly) underlies much of the design, analysis, and interpretation of machine learning experiments, despite the apparent gap in terminology as suggested by~\autoref{fig:dendrogram}. 

To make an explicit causal framing more accessible and attractive, we develop a guide to common causal structures and then use these abstract topologies to gain insights across case studies about reasoning abilities in LLMs.
While we focus on research questions and issues concerning the evaluation of reasoning abilities in LLMs, all four of our main claims (particularly~\ref{claim:causal}-\ref{claim:assumptions}) largely apply to the whole of empirical machine learning research. 
% Lastly, we advocate for a more pragmatic and instrumentalist perspective to model design to make use of the explanatory power and theoretical tools of causal inference while retaining the flexibility to adapt to an evolving approach. 
Our contributions include:
\begin{itemize}
% \vspace{-3mm}
    \item We survey a wide variety of common challenges, or \emph{monsters}, that hinder reliable evaluation and discuss how these can faithfully be modeled using causality
% \vspace{-1mm}
    \item We propose intuitive templates for causal modeling, called Common Abstract Topologies (CATs), and show how these aid in effective model design
% \vspace{-1mm}
    \item We motivate practical causal models to approaches in several case studies
% \vspace{-1mm}
    \item We promote causality as a tool to encourage hypothesis-driven research with precise, transparent assumptions and more systematic evaluation
\end{itemize}

% - **Causal Framework for Evaluation Challenges**: We unify common issues in LLM evaluation—biases, artifacts, dataset leakage—using causal inference.  
% - **Common Abstract Topologies (CATs)**: We introduce causal templates to identify confounders, mediators, and spurious correlations in benchmarks.  
% - **Case Studies Applying Causality**: We demonstrate how causal graphs clarify reasoning benchmarks, bias mitigation, and interpretability.  
% - **Advocating Explicit Causal Assumptions**: We promote hypothesis-driven NLP research with clearer assumptions and more rigorous evaluation.

% have exposed critical evaluation gaps and highlighted fundamental shortcomings in our methodologies.

% motivate focus on large models and in particular reasoning

% While recent advances in large language models (LLMs) have demonstrated remarkable capabilities, they have also introduced concerns regarding robustness, reasoning quality, and failure modes. Furthermore, overhyped benchmarks and inflated expectations risk obscuring fundamental limitations, raising concerns about another AI winter.

% A model may arrive at the right conclusion for the wrong reasons, making its performance unreliable beyond the test set. 
% Ensuring that models reason correctly requires understanding the causal mechanisms behind their responses, 
% yet explicit causal modeling remains underutilized in NLP and machine learning research.

% To help bridge this gap, we introduce Common Abstract Topologies (CATs), a set of causal graph structures that can help model common issues faced in designing an experiment or benchmark.
% In a variety of case studies we illustrate how a causal framing can clarify the strengths and limitations of a method, inspire new approaches, and deal with systematic biases.


% Ultimately, we advocate for a shift toward conceptual causal models as a flexible yet rigorous framework for evaluating reasoning in LLMs. 
% By doing so, we aim to make causality more accessible and practically useful for the NLP and machine learning communities, 
% ensuring that evaluation methodologies keep pace with the rapid advancements in AI.



% why care about evaluation?
% - cheating benchmarks and hype in industry
%   - skyhigh expectations and lack of understanding the systematic limitations of our models: danger of another AI winter
% - while our focus here is largely on the reasoning, as causality is particularly important there~\autoref{claim:reasoning}, but all our claims except for that one apply largely to all of empirical machine learning








% Most importantly, we hope to make causality more approachable and accessible by operationalizing principles and tools from causal inference for the machine learning community at large, and in particular to improve the evaluation of reasoning abilities in LLMs.

% [contribution] we will motivate an explicitly causal framing in 3 ways: (1) providing a causal derivation of the generalization gap, (2) describe some common types of factors encountered in experimental design in terms of the causal relationship which also imply different analysis approaches, (3) 

% [focus] our focus here is on LLMs and in particular evaluating the higher-level reasoning abilities of LLMs, however much of our analysis applies to the whole of empirical machine learning research.


% [claim] reasoning is causal

% \section{The Problem: Prompt brittleness, unreliability, and bias}
% \vspace{-3mm}
\section{A Bestiary of Monsters} \label{sec:monsters}

\vspace{-3mm}
\begin{example}{h}{ 
  \textbf{The final answer is correct, and yet the reasoning is very wrong.}
  Although this LLM's response is technically correct in a narrow sense, its answer for this GSM8k-like multi-step reasoning question~\cite{cobbeTrainingVerifiersSolve2021} is unsatisfactory both the purposes of evaluating the model's arithmetic reasoning, as well as commonsense or moral reasoning as the LLM appears not to pick up on the obvious unethical context implied by the scenario. \label{ex:crime}
}
  \textbf{Prompt:} Listen here, I'm starvin' and it's already half past four. Gotta rake in 10 grand in protection dough before I eat. I'm hittin' up every joint in the hood with my bat, collecting an average of 800 bucks a pop. Takes me 10 minutes to move from one mark to the next. Am I gonna get to chow down before 7?
  
  \textbf{LLM Chatbot:} If you're aiming to collect 10 grand by \redtext{shaking down joints} for an average of \$800 each and it takes you 10 minutes to move from one target to the next, it would take you approximately \redtext{3 hours and 20 minutes} (200 minutes) to reach your goal. 
  Starting at half-past four, you should be able to hit your target before 7 PM.
  So, \greentext{yes, you can have a meal before 7} if everything goes as planned. \redtext{Enjoy your dinner!}
\end{example}


% However, this explosive growth and excitement also carries with it increased responsibility to address costly points of failure. 
As LLMs become an increasingly integral component of decision-making systems, reasoning failures (including seemingly toy problems like in~\autoref{ex:crime}) may have dire real-world consequences~\cite{nezhurinaAliceWonderlandSimple2024,benderDangersStochasticParrots2021}.
% into the spotlight to add to a rapidly growing list of both familiar and novel issues.
This necessitates more powerful tools to identify, catalog, and address 
the bestiary of issues that arise in the design as well as evaluation of large models.


\autoref{ex:crime} is indicative of several common reasoning failures in LLMs, and, by implication, our evaluation procedures for addressing such issues.
The input prompt is structurally similar to the questions found in GSM8k~\cite{cobbeTrainingVerifiersSolve2021}, a benchmark used for evaluating a model's multi-step arithmetic and commonsense understanding skills. 
However, we select the subject matter and word choice to evoke a mobster discussing plans to extort money from local businesses. 
Although the LLM's final answer is factually correct, there are several problems with the rationale:
(1) it makes several arithmetic mistakes which happen to cancel out, 
(2) it fails to pick up on the unethical situation implied by the scenario, 
and (3) by implicitly condoning the criminal behavior, it does not consider the broader consequences of the response.
Crucially, if we only check for correctness, as is standard practice~\cite{huangReasoningLargeLanguage2023}, we would find no fault in the response. 
 %model behavior. 

The problem is that to demonstrate good reasoning abilities, a correct answer is insufficient. We need to show that the model answers the question correctly \textit{for the right reasons}.
In other words, our evaluation must verify that the model's processing of the input information \textit{leads to} the correct answer consistently and reliably.
This criterion makes a \textit{causal} claim about the model's reasoning process, and thus must be supported by a causal analysis.
% , regardless of whether you explicitly use a causal framing or not.


\begin{claim}{\small{Evaluating reasoning involves causal inference}}\label{claim:reasoning}
    A correct answer can be reached through very poor reasoning, but poor reasoning will not generalize beyond the lab bench.
    To generalize well, the model's reasoning must rely on robustly predictive (i.e. causal) features and relationships rather than spurious ones. Consequently, evaluating the reasoning abilities involves causal inference.
\end{claim}
%\vspace{-3mm}


% % [claim] reasoning is causal

% Different disciplines and subcommunities tend to use varied and potentially even incompatible terminology to describe the \textit{causal} influences impacting the model's behavior and our evaluation process. If our mutually idiosyncratic descriptions obscure fundamental similarities, then we may repeatedly develop redundant tools under different names (``reinventing the wheel''). 
% On the other hand, the advantage of so many different traditions and expertise to draw from is that we can form rich connections to gain a more multi-faceted view of issues and combine complementary approaches into even better solutions.
% \felix{is this necessary?}

% [review references] 
% theory reasoning \cite{yuNaturalLanguageReasoning2023}
% practical reasoning \cite{huangReasoningLargeLanguage2023}
% evaluation \cite{changSurveyEvaluationLarge2023} \cite{hajikhaniCriticalReviewLarge2023}
% as agents \cite{xiRisePotentialLarge2023} \cite{andreasLanguageModelsAgent2022}

% understand these sources of error, and motivate our claims, we will first take a step back and look at where these issues come from.

\subsection[``Here be dragons'']{``Here be dragons'' \footnote{The heir of vagueness and discomfort that researchers frequently use when mentioning potential undesirable biases or systematic limitations in their analysis is not unlike the way medieval cartographers would fill the mysterious edges of their maps with dragons.}}

% [disparate, vague formulations and descriptions of issues]

To get a qualitative sense of the myriad of issues, or \textit{monsters}, that plague our benchmarks and experiments, we will briefly survey recent approaches, including broad overviews into the nature of reasoning tasks~\citep{huangReasoningLargeLanguage2023,yuNaturalLanguageReasoning2023} and the evaluation of LLMs~\citep{maoGPTEvalSurveyAssessments2024,changSurveyEvaluationLarge2023,hajikhaniCriticalReviewLarge2023}. For investigations of more specific issues, we separate efforts into three clusters depending on whether the problem originates with the (1) models, (2) datasets, or (3) evaluation procedures.


\paragraph{Models}

This line of work focuses on characterizing the reasoning failures and biases of language models, which is nontrivial given their opaque behavior~\citep{binzUsingCognitivePsychology2023}. These failures range from well-defined formal errors such as logical fallacies~\citep{jinLogicalFallacyDetection2022}, red herrings~\citep{naeiniLargeLanguageModels2023}, or invalid inferences~\citep{saparovLanguageModelsAre2023} to broader issues including sensitivity to superficial features~\citep{hajikhaniCriticalReviewLarge2023,ullmanLargeLanguageModels2023}, overconfidence~\citep{nezhurinaAliceWonderlandSimple2024}, hallucinations~\citep{dziriOriginHallucinationsConversational2022,cuiHolisticAnalysisHallucination2023}, and lack of robustness~\citep{zhengLargeLanguageModels2024,wangAreLargeLanguage2023,jinBERTReallyRobust2020}. Some studies explore how models exhibit ``content effects''~\citep{poesiaCertifiedReasoningLanguage2023}, absorbing and amplifying human biases~\citep{dasguptaLanguageModelsShow2022,zecevicCausalParrotsLarge2023} including social and cultural biases~\citep{benderDangersStochasticParrots2021,messnerBytesBiasesInvestigating2023,hutchinsonSocialBiasesNLP2020,vigCausalMediationAnalysis2020,caoAssessingCrossCulturalAlignment2023,alkhamissiInvestigatingCulturalAlignment2024,motokiMoreHumanHuman2024}, such as stereotyping~\citep{kotekGenderBiasStereotypes2023}.

% This line of work focuses on characterizing the failures and biases of large models which is nontrivial given their opaque behavior~\citep{binzUsingCognitivePsychology2023,yanWorseRandomEmbarrassingly2024}. These failures range from well-defined formal reasoning failures such as logical fallacies~\citep{jinLogicalFallacyDetection2022}, red herrings~\citep{naeiniLargeLanguageModels2023}, or invalid inferences~\citep{saparovLanguageModelsAre2023}
% broader issues including a sensitivity to superficial features~\citep{hajikhaniCriticalReviewLarge2023,ullmanLargeLanguageModels2023}, overconfidence~\citep{nezhurinaAliceWonderlandSimple2024}, hallucinations~\citep{dziriOriginHallucinationsConversational2022,cuiHolisticAnalysisHallucination2023}, and general lack of robustness~\citep{zhengLargeLanguageModels2024,wangAreLargeLanguage2023,stolfoCausalFrameworkQuantify2022,jinBERTReallyRobust2020}, Some studies explore how models exhibit ``content effects''~\citep{poesiaCertifiedReasoningLanguage2023} including human biases~\citep{dasguptaLanguageModelsShow2022} or ingrained social/cultural baises~\citep{alkhamissiInvestigatingCulturalAlignment2024,motokiMoreHumanHuman2024,benderDangersStochasticParrots2021,brownLanguageModelsAre2020,messnerBytesBiasesInvestigating2023,hutchinsonSocialBiasesNLP2020,vigCausalMediationAnalysis2020} such as stereotyping~\citep{kotekGenderBiasStereotypes2023}. 
% Alternatively, some projects focus on specific models~\citep{borjiCategoricalArchiveChatGPT2023,maoGPTEvalSurveyAssessments2024,martinezReevaluatingGPT4Bar2024}, prompting strategies~\citep{lanhamMeasuringFaithfulnessChainofThought2023}, or adversarial attacks~\citep{weiJailbrokenHowDoes2023} to probe model reasoning.
% However, 
% whether models genuinely reason or simply exploit dataset artifacts.

\paragraph{Datasets}

Meanwhile, subtle variations of popular benchmarks, such as premise order in reasoning tasks\citep{chenPremiseOrderMatters2024} or minor changes in problem parameters~\citep{mirzadehGSMSymbolicUnderstandingLimitations2024,wuReasoningRecitingExploring2024}, can cause large performance drops~\citep{nezhurinaAliceWonderlandSimple2024,yanWorseRandomEmbarrassingly2024}, raising concerns not just about whether models genuinely reason~\citep{zhouYourModelReally2024}, but also about exploitable issues in the training data and benchmarks~\citep{rogersGuideDatasetExplosion2020,bowmanWhatWillIt2021}.
These are can be described as enabling
cheating~\citep{zhouDontMakeYour2023}, heuristics~\citep{mccoyRightWrongReasons2019}, or shortcuts~\citep{brancoShortcuttedCommonsenseData2021,liHowPretrainedLanguage2022,marconatoNotAllNeuroSymbolic2023}, possibly due to sampling biases~\citep{razeghiImpactPretrainingTerm2022} or in certain cases even
leakage between the training and testsets~\citep{zhouDontMakeYour2023} which can result in memorization~\citep{feldmanDoesLearningRequire2021}.
Poor dataset construction can lead to annotation artifacts~\citep{gururanganAnnotationArtifactsNatural2018,fleisigPerspectivistParadigmShift2024} such as priming effects~\citep{gardnerCompetencyProblemsFinding2021}, which degrade the quality and reliability of results~\citep{byrdPredictingDifficultyDiscrimination2022} while also unintentionally reinforcing social biases or cultural inequities~\citep{benderDangersStochasticParrots2021,huWhatSexGot2020,naousHavingBeerPrayer2024}.

\paragraph{Evaluation}

Even with well-constructed datasets, evaluation methodologies can introduce systematic errors~\citep{dominguez-olmedoQuestioningSurveyResponses2024} or lead to misleading conclusions~\citep{bowmanDangersUnderclaimingReasons2022}.
For example, automated scoring systems can obscure obvious failures~\citep{chowdhuriNoGPT4Cant2023}, while static benchmarks can emphasize surface-level accuracy at the cost of other important factors, such as generalization~\citep{liangHolisticEvaluationLanguage2023} or interpretability~\citep{loftusPositionCausalRevolution2024} or social costs~\citep{rajiAIEverythingWhole2021,benderDangersStochasticParrots2021}.
While standardized leaderboards~\citep{beeching2023open} and evaluation procedures~\citep{srivastavaImitationGameQuantifying2023} can enable more direct model comparisons, these benchmarks can gradually become less representative of real-world tasks~\citep{schlangenLanguageTasksLanguage2019,alzahraniWhenBenchmarksAre2024,shiraliTheoryDynamicBenchmarks2023,kielaDynabenchRethinkingBenchmarking2021}, introduce biases that favor certain model families~\citep{zhangCarefulExaminationLarge2024}, or inadvertently leak information from the test set~\citep{zhouDontMakeYour2023} which can be difficult to detect due to closed-source models and proprietary datasets~\citep{maoGPTEvalSurveyAssessments2024}. 


% chowdhuriNoGPT4Cant2023

%  allow information or biases to leak into the model's reasoning process.

Despite the diverse, at times redundant, terminology, we observe certain structural similarities in the approaches of these contributions. Terms like ``ablation'', ``perturbations'', ``edits'', ``flips'', ``masking'' can often be interpreted as interventional or counterfactual analyses, while ``sensitivity''/``robustness'', ``consistency'', ``shortcut'', ``leakage'', ``bias'', etc. refer to how the model's behavior is impacted by, for example, (seen or unseen) confounders. 
% Across these heterogeneous approaches there are underlying commonalities. %which can be captured by describing causal 

% [claim] all these issues can be faithfully described in terms of some propoerties of the causal graph underlying the experimental setup or approach.

\begin{claim}{The monsters are causal} \label{claim:causal}
    % \todo[inline]{focus on the data generative process}
    The disparate and often vague formulations of the issues that lurk within our benchmarks and models such as biases or failure modes can often faithfully be described in terms of causality. 
    Whether the factors are known or unknown, their influences can be captured by an appropriate causal model to guide the experimental design and analysis.
    % The diverse issues that plague our benchmarks and experiments can be faithfully described in terms of some properties of the causal graph underlying the experimental setup or approach. Whether the factors are known or unknown, their influences can be described in terms of the causal graph to guide the design of experiments and analysis.
\end{claim}

% To better understand these sources of error, and motivate our claims, we will first take a step back and look at where these issues come from.

% In the~\autoref{app:measure}, we use a high-level conceptual causal model to derive an explanation of the origins of these issues described as a \textit{measurement gap}. 

% \pagebreak
\section{Common Abstract Topologies}

% \felix{these CATs are selected to describe some of the basic causal interpretations of a simple statistical dependence between two variables -> reichenbach's common cause principle}


\begin{table*}[t]
  \centering 
  \begin{tabular}{m{25mm}cm{100mm}}
  \hline
  \multicolumn{1}{c}{\textbf{Name}} & \textbf{Graph} & \multicolumn{1}{c}{\textbf{Example Phenomena}} \\
  \hline
  \centering Confounding & \raisebox{-.4\height}{\includegraphics[width=1.3cm]{figures/graph-confounding.pdf}} & \begin{itemize}[noitemsep]
  \item prompt wording, instruction tuning, or prompting strategies
  \item dataset sourcing, 
  % balancing, 
  annotation artifacts, missing context
  \item overlap or leakage between the benchmark and training data
  \end{itemize}
  % data sources, annotation artifacts, context, wording, "shortcuts" in reasoning, heuristics, memorization, overlap, leakage & you need to control for the confounder 
  \\[-5mm]
  % \hline
  \centering Mediation & \raisebox{-.4\height}{\includegraphics[width=1.3cm]{figures/graph-mediation.pdf}} & \begin{itemize}[noitemsep]
    \item circuit analysis such as mechanistic interpretability
    \item tool use 
    % augmentation with a tool 
    or integrating an LLM in a larger application
    \item editing individual tokens or ablating model parameters
    \end{itemize}
  % mechanistic interpretability, LLM augmentations and prompt strategies 
  %& you should not control for the mediator 
  \\[-5mm]
  % \hline
  \centering Spurious Correlations & \raisebox{-.4\height}{\includegraphics[width=1.3cm]{figures/graph-spurious.pdf}} & 
  \begin{itemize}[noitemsep]
    \item social and cultural biases in the data collection process
    \item imbalances in the surface form such as symbol or label bias
    \item variable selection and construction
  \end{itemize}
  % dataset biases, cultural or sampling biases, symbol bias, variable construction %& controlling for this variable won't help, but your analysis or model might get stuck on it 
  \\[-2mm]
  \hline
  \end{tabular}
  \caption{Some simple Common Abstract Topologies (CATs) which can be used to formalize a wide variety of \textit{monsters} both known and unknown that may lurk in a benchmark or experiment analysis and some example issues that they may help represent. 
  For the graphs, \includegraphics[scale=0.2]{figures/stimulus.pdf} is the independent variable, \includegraphics[scale=0.2]{figures/response.pdf} is the dependent/outcome variable, and \includegraphics[scale=0.2]{figures/factor.pdf} represents a third variable factor such as a confounder or mediator.
  Note that the examples are partially overlapping, reflecting that depending on the specific setting, a similar issue may be represented by different CATs or combinations thereof.}
  \label{tab:cats}
  \vspace{-4mm}
\end{table*}


Coming up with a causal graph that faithfully represents the underlying structure of an experiment or data generating process can be very challenging. Especially, since usually when we design an experiment, we think in terms of more vague concepts like independent, dependent, and controlled variables, and consequently only implicitly make causal assumptions. However, explicit causal graphs:

\begin{itemize}[left=1mm]
  \item precisely communicate the assumptions that go into an approach, experiment, or analysis
  \item leverage the machinery of causal inference for a more principled analysis
  \item understand the implications of our design choices including the particular strengths and limitations on both technical and conceptual levels
\end{itemize}


To help make the process of constructing a causal graph more accessible and systematic, we identify some Common Abstract Topologies (CATs) of causal graphs and discuss associated phenomena (see~\autoref{tab:cats}) in the context of evaluating reasoning abilities in large models where these structures may be useful.

However, there may be some hesitancy to commit to a specific causal graph that faithfully captures all the factors that may affect the analysis~\cite{bareinboimPearlHierarchyFoundations2022}. 
Especially since, in practice, the graph is often severely underdetermined by available data, or depends on precise definitions or interpretations of relevant factors.
As pointed out by \citet{loftusPositionCausalRevolution2024}, researchers may even avoid causal language because it offers more assumptions for reviewers to challenge. 
% This encourages use of more vague terminology that is more defensible, potentially even implicitly enabling readers to interpret conclusions as causal without supporting evidence. 


% \todo{\cite{loftusPositionCausalRevolution2024} discusses researchers avoiding causal language because it offers more assumptions and implications for reviewers to attack - but causal models are more valuable precisely because the assumptions are made explicit rather than left implicit or vague. And CATs can mitigate this: recognizing common patterns and model designs helps recognize similarities and differences to satisfy reviewers to the extent that is desired, while novel results or proposals can be expressed precisely and tested rigorously.}

\begin{claim}{Instrumentalism is all you need} \label{claim:measurement}
  % \todo[inline]{focus on pragmatism~\cite{loftusPositionCausalRevolution2024,danzig}}
  A causal model does not need to be perfect to be useful. Plausible simplifying assumptions and abstractions can yield valuable insights and motivate practical experiments.
  As research advances, the model can be refined to mark our progress, while providing transparent falsifiable hypotheses at every step of the way.
\end{claim}


Here we urge the community to be more pragmatic, much like~\citet{loftusPositionCausalRevolution2024,janzingPhenomenologicalCausality2022}.
Due to subtle differences in the model design such as variable construction or selection, the same issue may be represented by various causal models, perhaps even ones that appear incompatible.
For example, depending on the level of abstraction~\cite{chalupka,Rubensteinetal17, beckers2020approximate}, certain causal relationships may be omitted, and the graph may be simplified or augmented with additional variables.
Nevertheless, as long as a proposed causal model does not directly conflict with the available data, 
it may be sufficient to improve performance or produce insights (such as more interpretable or explainable models).
%, it is a valuable tool for analysis and experimentation.
% it can motivate new hypotheses and experiments as well as providing additional explanatory power to the analysis.

% Most importantly, a major strength of conceptual causal models is that, for the most part, you are free to choose the level of abstraction that best serves your purpose, such as a specific set of possible actions as discussed in~\citet{janzingPhenomenologicalCausality2022}. 
% Depending on the level of abstraction, certain causal relationships may be omitted or merged, and the graph may be simplified or augmented with additional variables.

% See \autoref{app:philosophy} for a discussion of this \textit{instrumentalist} approach to causal modeling.

% The CATs we have described in the previous section serve as a starting point for developing a rich toolbelt of common causal structures with recognizable phenomena and associated analysis methods in a similar vein to~\citet{janzingPhenomenologicalCausality2022}.


% Here we reinforce a theme throughout this paper: causal inference is valuable not only for formal analysis but also as a conceptual framework for understanding the structure and assumptions behind an approach or argument, leading us to a notion of a \textit{conceptual causal model}. By this, we mean a causal graph that is sufficient to motivate the approach where the nodes and edges are chiefly supported by argumentation, rather than data.
% Most importantly, a major strength of conceptual causal models is that, for the most part, you are free to choose the level of abstraction~\cite{chalupka,Rubensteinetal17, beckers2020approximate} that best serves your purpose. Depending on the level of abstraction, certain causal relationships may be omitted or merged, and the graph may be simplified or augmented with additional variables.

Aside from the additional explanatory power, if a more formal treatment is necessary or desired, there is a whole world of tools and techniques to explore.
The field of causal inference~\cite{pearlCausalInferenceStatistics2009,pearlBookWhy2020,imbens,PetJanSch17,bareinboimPearlHierarchyFoundations2022} has developed a language for formalizing the effects of subtle design choices and their, potentially counterintuitive, consequences for the analysis. 
For example, Simpson's paradox can be elegantly explained, 
% and depending on the appropriate causal structure of the problem, 
to ``resolve'' the apparent paradox based on the appropriate causal assumptions of the problem
(for a deep dive into this topic see~\citet{pearl2022comment} and Chapter 6 of~\citet{pearlBookWhy2020}).
% as well as providing an explanation for various statistical phenomena.
% A well-known example is Simpson's paradox, where a trend that appears consistently within several different groups of data disappears or even reverses merely by considering the groups in aggregate. This phenomenon can be explained, and depending on the appropriate causal structure of the problem, the paradox can thus be ``resolved'' (for a deep dive into this topic see~\citet{pearl2022comment} and Chapter 6 of~\citet{pearlBookWhy2020}).


% [claim] the community should strive to make explicit the causal assumptions that go into an approach, benchmark, or experiment.
\begin{claim}{Towards \textit{explicit} causal assumptions} \label{claim:assumptions}

    % \todo[inline]{focus on methodology and approach, the underlying logic of the experiment}
    % \todo[inline]{causality as a natural hypothesis-generating language}
  An experimental design involves a variety of assumptions about what factors matter, how they interact, and how this relates to the proposed approach.
  Here the language of causality provides a powerful framework for motivating an approach, precisely formulating the hypothesis, and answering questions in a principled way.
  % but also to answer questions in a principled way 
  % Instead of using vague disparate terminology, we should strive to explicitly describe the causal assumptions that go into an approach, benchmark, or experiment.
  
  
\end{claim}
%\vspace{-2mm}


% emphasize the value of a \textit{conceptual causal model} which is a causal graph where the nodes and edges are chiefly supported by argumentation, rather than data.
Causal inference is valuable not only for formal analysis but also as a conceptual framework for understanding the structural assumption behind an approach or argument.
% , leading us to a notion of a \textit{conceptual causal model}.
By making the concepts and tools of causal inference more accessible, we aim to develop a practical guide to recognize familiar causal structures in common phenomena, 
as well as build an intuition for the implications of model design choices on analysis and interpretation.
To this end, we present three simple CATs that correspond to the three causal interpretations of a statistical dependence between two variables according to the common cause principle~\cite{reichenbach1956direction}. 
% as well as some examples where they can help gain insight into specific research questions.

% \todo{confounding: a variable that makes evaluation difficult or unreliable}
% \todo{spurious correlation: a learned causal relationships where there shouldn't be one}
% \todo{mediation: measuring the impact of a specific component on the model's behavior}

% \todo{confounding: GSM8k and testing impact of specific counfounders}
% \todo{spurious correlations: social and cultural biases, label shifts, selection biases (cite the paper lots of possible ways to deal with spurious correlations~\cite{pleckoCausalFrameworkDecomposing2023})}
% \todo{mediation: prompt engineering, tools, circuit analysis, ablation studies. differentiating the impact of specific mediators on the model's behavior}

% \todo{figure: prompt mediation}



%\vspace{-1em}
\subsection[Confounding]{Confounding \hfill \includegraphics[scale=0.3]{figures/inset-confounding.pdf}}

% The presence of confounding is ubiquitous across empirical science. 
Confounding occurs when there is a common cause between the independent and dependent variables. For our purposes, we further restrict the ``confounding'' CAT to the case where the confounder is known and can, in principle, be controlled for. This is in contrast to the ``spurious correlation'' CAT, where the confounder is unknown or too complex to be modeled explicitly.

Confounding makes evaluation difficult or unreliable because the observed statistical relationship between the stimulus and response is not representative of the underlying causal relationship, thus unbiased causal effect estimation necessitates controlling for the confounder. 
%This is because the confounder is a common cause of both the stimulus and response,
% ~\autoref{tab:cats} summarizes some common examples of phenomena that may be explained by confounding. 
% Notably, the examples primarily involve unintended influences on the stimulus as well as the response. For example, if a benchmark is constructed using a variety of significantly different data sources or the annotations are partially from crowd-sourcing vs experts, then the resulting benchmark may be confounded. %Alternatively, unintended information leakage or subtle context clues in the prompt may influence the model's response.


%\vspace{-1em}
\subsection[Mediation]{Mediation \hfill \includegraphics[scale=0.3]{figures/inset-mediation.pdf}}

% \todo{estimating side-effects}


Another important type of causal topology is mediation, where there are multiple causal paths between the stimulus and response. For simplicity, we illustrate this general structure with one direct causal link and one that goes through a mediator variable.
% Through mediation analysis, we can quantify the extent to which the mediator is used in the model's response. 
Mediation analysis is often used to quantify the impact of subcomponents or side-effects on the model's behavior.
For example, a common setting may be to study the impact of a specific prompting strategy or representation on the model's response, which can be modeled as mediation as in~\autoref{fig:mediation}. 
% is a common tool in interpretability research, where the goal is to make sense of some specific element of the model's behavior. 
% This is particularly common when the model or prompt contains multiple subcomponents, and we want to quantify to what degree a certain subcomponent is responsible for the resulting behavior. Consequently, mediation analysis is a common tool in interpretability research, where the goal is to make sense of some specific element of the model's behavior. 
% Alternatively, when studying LLMs in larger more complex programs, mediation analysis can be used to quantify the extent to which the LLM is used in the overall program's behavior.

\begin{figure}[ht]
\begin{center}
%\vspace{-4mm}
\centerline{\includegraphics[width=0.8\columnwidth]{figures/prompt-mediation.pdf}}
%\vspace{-5mm}
\caption{Sketch of a conceptual causal model treating the prompt (i.e. surface form) as a mediator between the underlying problem or task of interest and the model's response.}
%\vspace{-8mm}
\label{fig:mediation}
\end{center}
% \vskip -0.2in
\end{figure}

The impacts of the individual causal paths can be studied by estimating the natural direct effect (NDE), natural indirect effect (NIE), or controlled direct effect (CDE)~\cite{pearlCausalInferenceStatistics2009}. 
However, notably controlling for the mediator is not always appropriate, for example, for estimating the total causal effect (TCE). 
This underscores one of the key benefits of causal inference: given the specific causal query, the appropriate analysis method is dictated by the graph structure, 
thereby prescribing specific and principled experiments. 
% and what types of experiments are possible (e.g. what variables allow interventions or can be controlled), a corresponding (fully principled) experimental design is prescribed.

  

%\vspace{-1em}
\subsection[Spurious Correlations]{Spurious Correlations \hfill \includegraphics[scale=0.3]{figures/inset-spurious.pdf}}

The final common pattern we discuss here is spurious correlations, which are closely related to confounding but differ in the interpretation and implications for analysis.
Spurious correlations (depicted as a dashed curved edge) are statistical associations between variables that are not causally related (neither is an ancestor of the other), but are correlated due to some external factor (a common cause), which is usually unknown. 
% Spurious correlations are commonly depicted using a curved dashed undirected arc between the affected variables.

If a model is only trained on observational data (as is almost always the case) as opposed to interventional or counterfactual data, then there is no way to differentiate a spurious correlation from a causal relationship.
Consequently, a model can learn to rely on spurious correlations in the data to make predictions, 
effectively forming an undesirable causal link between the spurious feature and the model's output.
% This ambiguity can degrade generalization beyond the training environment and perpetuate (potentially undesirable) biases from the training data~\cite{benderDangersStochasticParrots2021}.

A common cause of spurious correlations, particularly in datasets, is due to selection bias in the data generative process, which may also be described as a collider bias~\cite{pearlCausalInferenceStatistics2009}.
Generally, it is not feasible to entirely eliminate spurious correlations,
as seemingly innocent choices in variable construction and selection are invariably informed by the experimenter's biases~\cite{huWhatSexGot2020,pietsch2015aspects}.
% In fact, even in fully deterministic synthetic environments, there is a theory-ladenness to how the variables are defined and used~\cite{pietsch2015aspects}. 
% For example, in the variable construction, there is a choice of how distinct classes or categories are distinguished, or what variables are included or excluded in the analysis, which is invariably informed by the experimenter's biases~\cite{huWhatSexGot2020}.
Nevertheless, there is extensive causal inference machinery to address spurious correlations depending on the specific setting~\citep{pleckoCausalFrameworkDecomposing2023}.


% \section{Conceptual Causal Models}


% \pagebreak
\section{Case Studies} \label{sec:case}

In this section, we discuss a variety of specific research projects 
which either make use of one of the Common Abstract Topologies (CATs) or could benefit from a more \emph{explicitly} causal framing.
% and how the Common Abstract Topologies (CATs) can be used to construct \textit{explicitly} causal interpretations of the experimental setup and analysis. 
% Additionally, we emphasize how seemingly minor changes to the experimental design affect the causal structure of the approach and interpretation of the results.

%\vspace{-1em}
\subsection[Confounding]{Confounding \hfill \includegraphics[scale=0.3]{figures/inset-confounding.pdf}}
% \subsection{Confounding}


% xiaAligningDebiasingCausalityAware2024
One project that explicitly uses a causal framing to understand the biases in the text generation of large language models (LLMs) is~\citet{xiaAligningDebiasingCausalityAware2024}. To address confounding due to biases in the training data or prompt, they propose using a reward model as an instrumental variable.

% zhangCausalInferenceHumanLanguage2024
\citet{zhangCausalInferenceHumanLanguage2024} formulate a human-LM collaborative writing setting as a causal inference problem where the past human commands and LLM responses are confounders for the current command and the overall interaction outcome. To identify strategies that improve the collaboration, they introduce a new causal estimand, the Incremental Stylistic Effect (ISE), which allows them to abstract away from specific interactions and focus on how actions incrementally contribute to the desired stylistic outcome of the text. 
% One project that explicitly uses a causal framing to understand the collaborative dynamics between humans and language models (LMs) is~\citet{zhangCausalInferenceHumanLanguage2024}. 

% \todo{romanouCRABAssessingStrength2023}


% [math reasoning sensitivity to term frequencies]



Meanwhile, a good example of an active area of research that largely revolves around the confounding CAT, despite ``confounding'' rarely being mentioned explicitly, is the study of how the mathematical reasoning abilities of LLMs are affected by various undesirable factors~\citep{zhouYourModelReally2024,patelAreNLPModels2021}. In particular, a variety of projects have focused on using the dataset GSM8K~\citep{cobbeTrainingVerifiersSolve2021} to evaluate multi-step arithmetic reasoning as well as common sense understanding~\citep{mirzadehGSMSymbolicUnderstandingLimitations2024,chenPremiseOrderMatters2024,zhangCarefulExaminationLarge2024}.

Several of these projects probe the robustness of the LLM's reasoning ability by systematically varying certain features such as the subjects or numbers involved~\citep{mirzadehGSMSymbolicUnderstandingLimitations2024}, the order of the premises~\citep{chenPremiseOrderMatters2024}, or attempt to replicate the original data generative process~\citep{zhangCarefulExaminationLarge2024} to test whether LLMs have overfit to the original dataset.

While these projects generally suggest that LLMs are sensitive to these factors, a more causal treatment can provide more precise conclusions. Let's take a closer look at one of the projects with a relatively specific target: \citet{razeghiImpactPretrainingTerm2022} investigate how much a language model's performance on quantitative reasoning tasks is affected by how often the numbers in the question occur in the model's training dataset. An intuitive causal framing for their approach using the ``confounding'' CAT is shown in ~\autoref{fig:confounding}. 
Note, that here the model's response is abstracted away since we are only interested in the response in so far as it affects the resulting accuracy. 

\begin{figure*}[ht]
  \centering 
  \begin{subfigure}[b]{0.25\textwidth}
      \centerline{\includegraphics[width=\linewidth]{figures/case_confounding.pdf}}
      %\vspace{-5mm}
      \caption{}
      \label{fig:confounding}
  \end{subfigure}
  % \hfill % add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
  \begin{subfigure}[b]{0.25\textwidth}
        
      \centerline{\raisebox{3mm}{\includegraphics[width=\linewidth]{figures/case_confounding_imagine.pdf}}}
      %\vspace{-5mm}
      \caption{}
      \label{fig:imagined}
  \end{subfigure}
  % \hfill % add desired spacing
  \begin{subfigure}[b]{0.25\textwidth}
      \centerline{\includegraphics[width=\linewidth]{figures/case_confounding_cautious.pdf}}
      %\vspace{-5mm}
      \caption{}
      \label{fig:cautious}
  \end{subfigure}
  %\vspace{-2mm}
  \caption{
  Various causal framings based on the approach of~\citet{razeghiImpactPretrainingTerm2022}.
  (a) A simple interpretation of their approach using the confounding CAT.
  (b) A causal framing for an alternative approach where we do not consider the term frequency, and instead observe a spurious correlation.
  (c) A more cautious causal framing that combines CATs to avoid claiming that the term frequency causally affects the model's accuracy (as is consistent with the authors' approach).
  }
  %\vspace{-3mm}
\end{figure*}


\paragraph{Alternative Approach}

Here it is instructive to consider a hypothetical project where we design a benchmark
% Imagine we are in the process of designing a benchmark 
to evaluate the math skills of a language model. Much like in~\citet{razeghiImpactPretrainingTerm2022}, our questions take the form ``What is $n_1$ times $n_2$?'' where $n_1$ and $n_2$ are numbers selected by some sampling strategy. However, we do not consider the training dataset of the model at all, and instead of following~\citet{razeghiImpactPretrainingTerm2022}, we sample numbers uniformly, which effectively removes the causal link between the term frequency and the numbers used in the question.
Based on the findings of~\citet{razeghiImpactPretrainingTerm2022}, we can expect to find a substantial correlation between the presence of certain numbers in the question and the model's accuracy, even though the rules of arithmetic are obviously entirely agnostic to which numbers are used. 

To explain the results of our approach, we might phenomenologically define a new property of numbers called ``difficulty'' which, we conclude 
% based on our (imagined) experiments, 
significantly affects the model's accuracy, leading to the causal graph in~\autoref{fig:imagined} using the ``spurious correlation'' CAT. %(see~\autoref{app:progress} for how these distinct graphs can be reconciled).

% Thus, for this alternative approach, we may prefer a conceptual causal framing with %that uses the
% the ``spurious correlation'' CAT as seen in~\autoref{fig:imagined}. %, and perhaps call for further investigation into the better understanding the ``difficulty'' of numbers.
%  We could then describe our approach using the ``spurious correlation'' CAT, where we observe 

\paragraph{Verifying Causal Assumptions}

A notable consequence of committing to a causal graph as in~\autoref{fig:confounding} is that it may imply certain falsifiable causal relationships that are not actually verified by the experiments. Specifically, the experiments of~\citet{razeghiImpactPretrainingTerm2022} identify a significant correlation between the term frequency and the model's accuracy, rather than showing a causal relationship, as the authors helpfully state explicitly. Therefore, an alternative plausible causal graph as in~\autoref{fig:cautious} may be posited for their approach where the term frequency is merely correlated with the model's accuracy by sharing a hither-to unknown confounder.
% Crucially, from the experiments of~\citet{razeghiImpactPretrainingTerm2022} alone, there is no way of refuting either causal graph.
This process illustrates how structurally distinct causal interpretations can be proposed to motivate certain experiments or approaches, and then how the results can be used to incrementally refine the causal graph.

%\vspace{-1em}
\subsection[Mediation]{Mediation \hfill \includegraphics[scale=0.3]{figures/inset-mediation.pdf}}

Mediation analysis guides the approaches of mechanistic interpretability~\citep{stolfoMechanisticInterpretationArithmetic2023,guptaEditingCommonSense2023,mengLocatingEditingFactual2023,wangInterpretabilityWildCircuit2022}, but it is also useful in augmentation of language models~\cite{mialonAugmentedLanguageModels2023}, embedding LLMs within larger programs~\cite{schlagLargeLanguageModel2023}. and the quantification of biases like, gender bias~\cite{vigCausalMediationAnalysis2020}.


% Notably,~\citet{vigCausalMediationAnalysis2020} provide an explicitly causal framing to investigate gender bias in language models.

A common setup for mechanistic interpretability is to study the impact of a specific component, such as an attention head or even a single parameter on the model behavior.
\citet{olsson2022context} propose that transformers can learn simple, interpretable algorithms called ``induction heads,'' which they hypothesize significantly contribute to in-context learning abilities. While mediation analysis is not explicitly used in their work, we can frame their approach as studying a mediation graph, where the tendency for a given model architecture (stimulus) to exhibit in-context learning (response) is mediated by induction heads. Their six supporting arguments can be interpreted through this causal lens: arguments 1 and 2 establish links between stimulus, mediator, and response through co-occurrence and co-perturbation; argument 3, an ablation study, resembles controlled direct effect estimation; and arguments 4-6 examine the causal influence of the mediator on the response. 
This framing also highlights potential limitations, particularly regarding unmeasured confounders that could affect causal interpretations, as the authors' ``pattern-preserving'' ablation does not fully isolate the induction heads' effect. By considering mediation explicitly, we can better understand the underlying assumptions in their analysis and identify areas for further investigation, such as quantifying the natural indirect effect to understand the full impact of the induction heads on in-context learning abilities.


% Notably,~\citet{stolfoMechanisticInterpretationArithmetic2023} 
% stolfoMechanisticInterpretationArithmetic2023
In contrast,~\citet{stolfoMechanisticInterpretationArithmetic2023} propose a method for mechanistic interpretability of arithmetic reasoning in LLMs by editing the model's parameters to characterize the information flow in the network. Note that the level of abstraction for this approach is quite different from the causal model we proposed for~\citet{olsson2022context}, as the focus is on how information flows between individual model subcomponents, rather than how specific subcomponents affect the overall model's behavior.

% Here we will focus on an example where mediation analysis is not explicitly used to show how a more explicit framing can tie together the arguments and experiments in a cohesive manner.

% \citet{olsson2022context} propose a type of simple, interpretable algorithm that can be learned by some transformers, called ``induction heads'', which they hypothesize is a significant contributor to the model's in-context learning abilities. To this end, we can frame their approach as studying a mediation graph where the tendency for a certain model architecture (stimulus) to exhibit in-context learning abilities (response) is mediated by the use of induction heads.

% The authors support their hypothesis through six arguments which, with our causal framing in mind, we can neatly interpret as supporting different features of the implicitly hypothesized causal graph. Arguments 1 and 2 are based on the co-occurence and co-perturbation of induction heads and in-context learning abilities, for different transformer architectures, thus characterizing the causal link between the stimulus and mediator as well as the link between stimulus and response. Argument 3 directly ablates the mediator, which is analogous to an estimation of the controlled direct effect. 
% in that it seeks to demonstrate the effect of the mediator in intervention. 
% Finally, arguments 4-6 focus on studying the causal link between the mediator and response, suggesting that the mediator has an effect on in-context learning abilities.

% Using this framing, we can also take a closer look at some of the subtleties of the analysis to identify potential limitations or causal assumptions that require further support. By drawing connections to the controlled direct effect for argument 3, we can check whether all the necessary assumptions are reasonably satisfied. 
% For example, estimation of the controlled direct effect is sensitive to unmeasured confounders between the stimulus and the mediator or response. This assumption may be problematic, as the authors discuss using a special ``pattern-preserving'' ablation that only knocks out the effect of an induction head on the output token, not the subsequent attention heads, rather than a ``full'' ablation. 
% If there are additional (unknown) contributors to in-context learning abilities aside from induction heads, then the ablation may not be sufficient to isolate the effect of the mediator, because, for example, the ablation inadvertently also affects other components of the model which are responsible for in-context learning abilities. 

% Although the authors do not explicitly frame their analysis in terms of mediation, we can see how a causal framing clarifies underlying assumptions of the approach and helps identify implicit assumptions that may require further investigation.


% or augmentation (langchain, agentenv)
%\vspace{-1em}
\subsection[Spurious Correlations]{Spurious Correlations \hfill \includegraphics[scale=0.3]{figures/inset-spurious.pdf}}



% ## caoDoesCorrectnessFactual2023 *

% <!-- 
% In recent years, the injection of factual knowledge has been observed to have a significant positive correlation to the downstream task performance of pre-trained language models. However, existing work neither demonstrates that pre-trained models successfully learn the injected factual knowledge nor proves that there is a causal relation between injected factual knowledge and downstream performance improvements. In this paper, we introduce a counterfactual-based analysis framework to explore the causal effects of factual knowledge injection on the performance of language models within pretrain-finetune paradigm. Instead of directly probing the language model or exhaustively enumerating potential confounding factors, we analyze this issue by perturbing the factual knowledge sources at different scales and comparing the performance of pre-trained language models before and after the perturbation. Surprisingly, throughout our experiments, we find that although the knowledge seems to be successfully injected, the correctness of injected knowledge only has a very limited effect on the models' downstream performance. This finding strongly challenges previous assumptions that the injected factual knowledge is the key for language models to achieve performance improvements on downstream tasks in pretrain-finetune paradigm. -->

% introduce a counterfactual-based analysis framework to explore the causal effects of factual knowledge injection on the performance of language models
% largely negative results: correctness of injected knowledge has limited effect on downstream performance
% suggests that factual knowledge is not the key for performance improvements
% additional hypotheses: downstream performance is mediated by the model's ability to retain and apply the injected knowledge, task dependence (confounding) on whether the factuality affects performance
% most relevant CAT: spurious correlation (maybe confounding)

% ## chenCausalInterventionCounterfactual2023 *

% <!-- 
% Due to the rapid upgrade of social platforms, most of today`s fake news is published and spread in a multi-modal form. Most existing multi-modal fake news detection methods neglect the fact that some label-specific features learned from the training set cannot generalize well to the testing set, thus inevitably suffering from the harm caused by the latent data bias. In this paper, we analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features. We mitigate these biases from a causality perspective and propose a Causal intervention and Counterfactual reasoning based Debiasing framework (CCD) for multi-modal fake news detection. To achieve our goal, we first utilize causal intervention to remove the psycholinguistic bias which introduces the spurious correlations between text features and news label. And then, we apply counterfactual reasoning by imagining a counterfactual world where each news has only image features for estimating the direct effect of the image. Therefore we can eliminate the image-only bias by deducting the direct effect of the image from the total effect on labels. Extensive experiments on two real-world benchmark datasets demonstrate the effectiveness of our framework for improving multi-modal fake news detection. -->

% analyze and identify the psycholinguistic bias in the text and the bias of inferring news label based on only image features
% use interventions to mitigate spuriously correlations in fake news detection
% focus on multi-modal and vision
% monster: psycholinguistic bias, image-only bias
% most relevant CAT: spurious correlation

% ## bansalControllingLearnedEffects2023 *

% <!-- 
% To address the problem of NLP classifiers learning spurious correlations between training features and target labels, a common approach is to make the model's predictions invariant to these features. However, this can be counter-productive when the features have a non-zero causal effect on the target label and thus are important for prediction. Therefore, using methods from the causal inference literature, we propose an algorithm to regularize the learnt effect of the features on the model's prediction to the estimated effect of feature on label. This results in an automated augmentation method that leverages the estimated effect of a feature to appropriately change the labels for new augmented inputs. On toxicity and IMDB review datasets, the proposed algorithm minimises spurious correlations and improves the minority group (i.e., samples breaking spurious correlations) accuracy, while also improving the total accuracy compared to standard training. -->

% propose an algorithm to regularize the learnt effect of the features on the model's prediction to the estimated effect of feature on label
% build an explicit causal model for the effect of features on the target label (connect this to Gardner competency problems gardnerCompetencyProblemsFinding2021)
% additional hypotheses: finding specific features which result in discrimination or fairness issues and mitigating strategies, more accurate predictions for minority groups, better regularization of the training signal to be robust to spurious correlations
% most relevant CAT: spurious correlations

% ## zhangUnderstandingDemonstrationbasedLearning2023 *

% <!-- 
% Demonstration-based learning has shown impressive performance in exploiting pretrained language models under few-shot learning settings. It is interesting to see that demonstrations, even those composed of random tokens, can still improve performance. In this paper, we build a Structural Causal Model (SCM) to understand demonstration-based learning from causal perspectives and interpret random demonstrations as interventions on the demonstration variable within the causal model. We investigate the causal effects and find that the concurrence of specific words in the demonstration will induce bias, while randomly sampled tokens in the demonstration do not. Based on this finding, we further propose simple ways to construct random demonstrations, which even outperform hand-crafted, meaningful demonstrations on public sequence labeling benchmarks. -->

% build a Structural Causal Model (SCM) causal graph to understand demonstration-based learning from causal perspectives
% interpret random demonstrations as interventions on the demonstration variable within the causal model
% basically the same as bansalControllingLearnedEffects2023
% most relevant CAT: spurious correlation


There are several recent projects that use causal models to characterize spurious correlations in, for example, factual knowledge~\citep{caoDoesCorrectnessFactual2023}, multi-modal models for fake news detection~\cite{chenCausalInterventionCounterfactual2023}, or to avoid spurious features by designing strategies for finding useful demonstrations in few-shot learning~\cite{zhangUnderstandingDemonstrationbasedLearning2023} or 
control NLP classifiers~\cite{bansalControllingLearnedEffects2023}.

% chenCausalInterventionCounterfactual2023
\citet{chenCausalInterventionCounterfactual2023} develop a causal model to systematically quantify and remove two specific kinds of bias: psycholinguistic (use of emotional language) and image-only (ignoring text features). Note that the assumptions of the causal model address very specific types of bias using both interventional and counterfactual techniques.

% caoDoesCorrectnessFactual2023

% zhangUnderstandingDemonstrationbasedLearning2023

% bansalControllingLearnedEffects2023
\citet{bansalControllingLearnedEffects2023} presents a particularly interesting case as it addresses the same issue as~\citet{gardnerCompetencyProblemsFinding2021}, but from a causal perspective. 
They both study the issue of label bias, specifically in ``competency problems''~\citep{gardnerCompetencyProblemsFinding2021}, where an individual token in the prompt is not indicative of the label, but the model learns to rely on it, usually due to selection bias in the data collection.


% Finally, we examine a case study on spurious correlations, where~\citet{gardnerCompetencyProblemsFinding2021} investigate the presence of label bias of individual tokens in a variety of benchmarks for classic NLP tasks (including natural language inference, sentiment analysis, and dependency parsing). Specifically, they argue that if a language model is sensitive to correlations between individual tokens and the label, then the model is relying on ``extra-linguistic'' features, such as sampling or annotation biases of the dataset, rather than language understanding. %For example, certain words might appear more frequently in sentences with positive sentiment than in negative ones or vice versa without being indicative of the sentiment of the sentence, which can encode biases~\cite{benderDangersStochasticParrots2021}.

% Consequently, \citet{gardnerCompetencyProblemsFinding2021} claim any such correlations are spurious and should ideally not affect the model's response. This aligns nicely with our spurious correlation CAT, where the input prompt or question (stimulus) affects the expected label (response) which is treated as binary, but due to unknown confounders (such as artifacts or biases in the benchmark), some tokens in the prompt also individually influence the model's response.

% Particularly noteworthy in the analysis of~\citet{gardnerCompetencyProblemsFinding2021} is the discussion of the subtleties involved in attempts to mitigate the sensitivity of the model to these spurious correlations.
The authors of~\citet{gardnerCompetencyProblemsFinding2021} propose a mitigation strategy based on ``local edits'' to individual tokens in the prompt to debias the benchmark. Using their statistical framing, the authors prove that the most promising strategy must apply local edits such that the label is flipped precisely half of the time.%\footnote{The proof takes up more than a full page.}

Translating this into a causal framing, we can recover the same result quite intuitively.
Adopting the same terms as~\citet{gardnerCompetencyProblemsFinding2021}, we now treat the input (text) features $X$ as the stimulus, the model's response $Y$ as the response, and the individual token $X_i$ as the third variable, which our model has learned to rely on despite it being a spurious feature. To remove the label bias for our model, we need the effect of an edit on $X_i = x_i'$ to be as likely to flip the label as not. This is equivalent to the average causal effect conditioned on $X$:
\begin{equation} \label{eq:ace}
  \mathbb{E}(Y|X,do(X_i = x_i')) - \mathbb{E}(Y|X,do(X_i = x_i)) = 0
\end{equation}

% should flip the (binary) label half the time is equivalent to the average causal effect $P(y|x,do(x_i = x_i')) - P(y|x,do(x_i = x_i)) = 0$ conditioned on $x$ which is also analogous to the objective derived by~\citet{bansalControllingLearnedEffects2023}.
However, due to the non-causal treatment~\citet{gardnerCompetencyProblemsFinding2021}, need to make a ``strong independence assumption,'' which is equivalent to, for the purposes of the mitigation strategy, assuming that the individual token $X_i$ is completely independent of the prompt $X$. 
As the authors point out, this assumption is not very realistic, as changing a single token may well affect the semantic meaning of the prompt beyond just the label (e.g. replacing ``very'' with ``not'' in a movie review).

Meanwhile,~\citet{bansalControllingLearnedEffects2023} uses a causal graph matching the spurious correlation CAT and a condition analogous to~\autoref{eq:ace} to derive a causal regularization term for the model's training objective - without the need for the strong independence assumption.

In summary, both approaches started with the same objective, but due to the purely statistical treatment, a cumbersome derivation 
% (for which the proof takes up more than a full page) 
still required an unrealistic assumption severely limiting the applicability of the method. 
The causal model not only provided a more intuitive motivation for the approach, but also offered a more powerful, principled method for achieving the same goal. 

% \todo{discuss novel hypotheses motivated by causal models}


% First, note that the authors make a ``strong'' independence assumption, which is equivalent to, for the purposes of the mitigation strategy, assuming that the individual token we may edit is completely independent of the prompt. As the authors point out, this assumption is not entirely realistic, as changing a single token may well affect the semantic meaning of the prompt beyond just the label.
% However, with that same assumption, we recognize it as simply removing the confounding link between the stimulus and the third variable (individual token) in our graph. Then, to eliminate the influence of the third variable, we precisely require the average causal effect of the third variable on the response to be zero. In other words, our intervention must be as likely to flip the label as not, just as the authors found.



% They propose a method to regularize the learned effect of features on the model's prediction to the estimated effect of the feature on the label. This method is designed to minimize spurious correlations and improve the minority group accuracy, while also improving the total accuracy compared to standard training. The authors explicitly frame their approach in terms of causal inference, which allows them to propose a principled method for regularizing the training signal to be robust to spurious correlations. This is a clear example of how a causal framing can lead to a more principled and effective approach to mitigating spurious correlations in the training data.


% However, crucially our derivation, based on a causal framing, also intuitively explains why flipping the label half of the time debiases the dataset, as well as clarifying the implications of the ``strong'' assumptions to get to that result.


% \todo{new claim: interesting research questions are (usually) causal. (give examples)}
% framing the problem
% justifying the method
% identifying weaknesses
% proposing improvements


% \begin{claim}{\small{Interesting questions are causal}}\label{claim:reasoning}
%   % obscuring the underlying assumptions
%   % to gain understanding
%   % to what extent an apparent pattern actually provides reliable insight into a phenomenon
%   In particular when models are deployed in the real world to make decisions or interact with other complex systems.
%   % Particularly in the context of real-world applications and deployment, it is crucial  
%   % however without a causal model, the conclusions can only hint towards meaningful insights
%   % rigorous
%   % However, a hesitation to make causal assumptions explicit
%   % connect to decision making and deployment
%   % or to leave it to readers to interpret 
% \end{claim}

\section{Alternative Views}

% We are hardly the first to point out systematic shortcomings of evaluation methodology. Particularly in NLP there are a wide variety of position papers and reviews that bemoan the state of evaluation in the field. 

We are hardly the first to point out systematic shortcomings of evaluation methodology, particularly in NLP.
One existing perspective focuses on 
% careful definition of how benchmarks relate to tasks and datasets~\citep{schlangenLanguageTasksLanguage2019} to 
improving the external validity of benchmarks to ensure that high performance on a benchmark actually translates to improved capabilities in the real world, such as with common sense reasoning~\citep{elazarBackSquareOne2021}, or more precisely defining LLMs~\citep{rogersPositionKeyClaims2024a} and how tasks relate to specific cognitive capabilities~\citep{schlangenLanguageTasksLanguage2019}. 
\citet{rajiAIEverythingWhole2021} argue that the common practice for certain ``standard'' benchmarks to become proxies for testing complex, high-level abilities, such as natural language understanding (NLU) leads to vague or unreliable results, while~\citet{rogersGuideDatasetExplosion2020} connect this to a proliferation of low-quality datasets.

Precisely this issue, that ``benchmarking for NLU is broken''~\cite{bowmanWhatWillIt2021}, 
can be addressed using causality. Not only does a causal framing provide a versatile way to define the underlying assumptions and design choices of a benchmark, but it also offers principled methods for evaluating the benchmark's external validity~\citep{bareinboimTransportabilityCausalEffects2012,pearlExternalValidityDoCalculus2022}.

In the context of evaluating the reasoning abilities of language models, a natural field to turn to is psychometrics, which has been studying the evaluation of human reasoning abilities for over a century~\citep{wilhelmMeasuringReasoningAbility2005}. This direction also coincides with an increasing practice in Natural Language Processing (NLP) to treat language models as agents~\citep{parkGenerativeAgentsInteractive2023,liuAgentBenchEvaluatingLLMs2023} or subjects in the social sciences~\citep{hortonLargeLanguageModels2023,lengLLMAgentsExhibit2023,pellertAIPsychometricsAssessing2024}.
Specifically, item response theory~\citep{lord2008statistical,baker2001basics} holds promise to develop tools to systematically quantify what information about the model's reasoning abilities can be extracted from a benchmark with respect to some population candidate models, and there are some projects applying this framework in the context of NLP~\citep{rodriguezEvaluationExamplesAre2021}.
Within the field of NLP there are also notable calls for more holistic evaluation schemes~\citep{liangHolisticEvaluationLanguage2023,bowmanWhatWillIt2021,zhangCarefulExaminationLarge2024} and practical tools for improving the evaluation of language models~\citep{ribeiroAccuracyBehavioralTesting2020,srivastavaFunctionalBenchmarksRobust2024,alzahraniWhenBenchmarksAre2024} or even reintroducing principles from linguistic theory~\citep{LanguageModelsLinguistic2023}.

There is also a growing interest in studying the causal knowledge learned by language models~\citep{zhangUnderstandingCausalityLarge2023,kicimanCausalReasoningLarge2023} and their causal reasoning abilities~\citep{jinCLadderAssessingCausal2024,zecevicCausalParrotsLarge2023,liuCASACausalitydrivenArgument2024} to help with causal discovery~\citep{montagnaDemystifyingAmortizedCausal2024,jiralerspongEfficientCausalGraph2024} or even hypothesis generation in psychology~\citep{tongAutomatingPsychologicalHypothesis2024}.
This effort largely coincides with our message: just as an LLM may benefit from more explicit causal models, so can the research community.
% Despite being rather underrepresented, we are encouraged by a growing interest in applying more causal principles to better evaluate~\cite{jinCLadderAssessingCausal2024,kicimanCausalReasoningLarge2023} and understand language models~\cite{zecevicCausalParrotsLarge2023}.

% 
% tongAutomatingPsychologicalHypothesis2024

% However, more broadly within the world of machine learning, explicitly causal framings have brought significant insights in a variety of subfields including representation learning~\cite{scholkopfCausalRepresentationLearning2021}, contrastive learning~\cite{vonkugelgenSelfSupervisedLearningData2022}, embodied AI~\cite{cohenGroundedTheoryCausation2022}, fairness~\cite{kusnerCounterfactualFairness2018,kilbertusAvoidingDiscriminationCausal2018}, algorithmic recourse~\cite{karimiAlgorithmicRecourseCounterfactual2020}, explanations~\cite{karimiAlgorithmicRecourseCounterfactual2020}, and economics~\cite{imbensPotentialOutcomeDirected2020} to mention a few.

% , and, particularly noteworthy, even to the generalization of large language models~\citep{reizingerUnderstandingLLMsRequires2024} to mention a few.


% \todo{-> alternative views: including item response theory, focus on external validity (connect to transportability), dynamic benchmarks, holistic evaluation, linguistic theory}

% \todo{synthesis of some position papers bemoaning the state of evaluation in NLP, and those praising the potential of causality in ML}

% \todo{external validity -> transportability elias bareinboim}

% In the face of a myriad of challenges, there are a variety of frameworks that may offer insight besides causality.




  

% \begin{claim}
%   \refstepcounter{claim}
%   \label{claim:causal}
%   \begin{tcolorbox}[colback=claimpurple!5!white,colframe=claimpurple!75!black, title=\textbf{Claim \theclaim:} Causality is underutilized in NLP]
%     NLP research, and in particular the literature on studying the reasoning abilities of language models is noticably underutilizing causality.
%   \end{tcolorbox}
%   % \caption{test}
% \end{claim}


\section{Conclusion}


% \todo{progress in tandem with better understanding and evaluating the \emph{causal} knowledge learned by and causal reasoning abilities of LLMs}

% \todo{towards hypothesis driven, rather than data driven research/evaluation}

% In conclusion, the case studies show how an explicitly causal framing aids along several steps of the design, experiments, and analysis, as well as in interpreting the results. % across a variety of different research directions to better understand and evaluate the reasoning abilities of language models.
% One of our key arguments is that the benefits of causal inference can be reaped using \textit{conceptual causal models} without necessarily committing to a formal treatment, which may require excessively restrictive assumptions. We aim to operationalize causal inference to enable rapid development and refinement of experimental designs using the language of causality.


% In this work, we highlighted the multitude of \emph{monsters} that can lurk beneath our lab bench or within our benchmarks when evaluating LLM reasoning. While the research community has developed numerous methods and frameworks to tackle these challenges, such variety can sometimes obscure their shared structural features (see \autoref{app:limitations} for a discussion of limitations). We therefore propose a unifying, principled approach grounded in causal inference—one that systematically addresses recurring issues and clarifies assumptions and claims often left implicit.

% An explicitly causal framing supports more rigorous and hypothesis-driven experimentation, from guiding experimental design to interpreting and explaining results. By shifting toward causal methods, researchers gain a clearer lens to understand how variables of interest interact, rather than merely optimizing for predictive performance. Indeed, we argue that causality offers a path toward deeper scientific insights, more transparent communication of assumptions, and stronger justifications for the conclusions drawn.

% Although adopting the language of causality can initially seem daunting—particularly when formalizing assumptions that become open to scrutiny—the benefits far outweigh the costs. Vague or implicit assumptions, along with results that may be \emph{interpreted} (even inadvertently) as causal, often contribute to confusion or unreproducible findings. Causal methods, by contrast, encourage explicit modeling and critical thinking about the mechanisms that underlie empirical observations.


% [the monsters are causal and the case studies show how a causal framing can help with motivate experiments, as well as in interpreting the results to better understand and resolve these issues]
The burgeoning research on large models, and, in particular, high-level reasoning tasks, faces a variety of challenges, or \textit{monsters}, to reliably evaluate and improve models. 
Despite the wide variety of approaches and frameworks that have been developed to tackle these challenges, this variety obscures their shared structural features and recurring issues.
By recognizing that monsters can often be effectively formulated in terms of causal assumptions underlying an experimental design or data generation process, we can unify our understanding using the language of causality.


% We therefore propose a unifying, principled approach grounded in causal inference—one that systematically addresses these issues and clarifies assumptions and claims often left implicit.
% on evaluating the reasoning abilities of language models has brought to light a variety of challenges, which we have collectively referred to as \textit{monsters}. These challenges are often rooted in causal relationships between the model's reasoning abilities and the data it is trained on, but are often obscured by the complexity of the models and the benchmarks used to evaluate them. In this work, we have shown how a causal framing can help motivate experiments, as well as in interpreting the results to better understand and resolve these issues.

% we reiterate that although a diverse set of tools and frameworks offer a more multi-faceted view of the many \textit{monsters} lurking beneath our lab bench or within our benchmarks, we suspect this comes at the cost of obscuring common structural features across challenges (see~\autoref{app:limitations} for limitations). Therefore, we advocate using the principled framework of causality to unify and systematically address these \textit{monsters}.
% such variety can sometimes obscure their shared structural features and 
% recurring issues
% We therefore propose a unifying, principled approach grounded in causal inference—one that systematically addresses recurring issues and clarifies assumptions and claims often left implicit.

% [opportunity for a more rigorous, principled framework for hypothesis driven research] [towards "understanding" rather than just "predicting"]
A causal framing aids along several steps of the research process by guiding experimental design, formulating testable hypotheses, and interpreting results. Causal methods enable researchers to gain a clearer lens to understand how variables of interest interact, rather than merely optimizing for predictive performance on an artificial benchmark.
We argue that causality offers a path toward deeper scientific insights, more transparent communication of assumptions, and stronger justifications for the conclusions drawn.
% we move beyond merely optimizing benchmark scores to developing models that offer deeper explanatory power

% an explicitly causal framing 

% we expect the transition towards more explicitly causal framings will lead to more precise and rigorous research.


% [some challenges with more causal models]
One stumbling block to adopting causal methods is that the restrictive assumptions and formalism may seem unapproachable at first.
Additionally, researchers may hesitate to commit modeling assumptions to paper where they can be scrutinized.
% we argue that the benefits are outweighed by the costs.
However, data-driven approaches which rely on implicit or vague assumptions along with results that may (inadvertently) be \emph{interpreted} as causal contribute to confusion and unsupported claims, which hinder scientific progress.
% In particular, implicit or vague assumptions combined with claims which may intentionally or unintentionally interpreted as causal leads to a lack of clarity and unreliable results.
Causal methods, by contrast, encourage explicit modeling and critical thinking about the mechanisms that underlie empirical observations.

% [making causality more accessible with CATs]
To make causality more accessible and practically applicable, we introduce Common Abstract Topologies (CATs) to faithfully describe the underlying structure of many issues that arise in designing and evaluating ML models.
In the case studies in~\autoref{sec:case}, we have shown how a causal framing can formalize a various common issues and help develop mitigate them.
% patterns that repeatedly arise in issues of LLM evaluation. 
% To that end, we introduce Common Abstract Topologies (CATs) to catalog common causal patterns that occur across many challenging issues with evaluating LLM reasoning. 
We envision CATs as a practical guide, helping researchers quickly identify relevant causal models and choose appropriate inference tools. 
% By systematically grounding studies in causal language, we can more reliably advance theoretical and empirical work in machine learning.
% This opens up both practical and theoretical research directions to reinterpret and reframe existing approaches as well as develop a practical guide for future work grounded in causality.
% We aim to operationalize causal inference to enable rapid development and refinement of experimental designs using the language of causality.
% helping researchers quickly identify relevant causal structures and choose appropriate inference tools. By systematically grounding studies in causal language, we can more reliably advance theoretical and empirical work in machine learning.\
% We envision CATs as a practical guide, helping researchers quickly identify relevant causal structures and choose appropriate inference tools. By systematically grounding studies in causal language, we can more reliably advance theoretical and empirical work in machine learning.
% [benefits: hypothesis driven research instead of an opaque data driven, better science communication, more principled solutions, deeper understanding of the strengths and limitations of our models] [maybe connect to deployment/decision making in safety critical areas]
% harness the clarity and rigor of causal inference to directly answer important questions in a principled way 
% leading to more steady and definitive progress in empirical machine learning.
Ultimately, causal models encourage more hypothesis-driven research which directly tackle key questions in a principled, transparent way, leading to more robust progress across empirical machine learning.
% in understanding and improving our methods.%, as well as benefiting the field of empirical machine learning at large.
% rather than opaque data-driven approaches that may
% and offer a more principled framework for understanding and improving of our models 



% Although the language of causality may appear unapproachable and formal at first, and researchers may hesitate to commit modeling assumptions to paper where they can be scrutinized.
% we argue that the benefits are outweighed by the costs.
% of a causal framing are 
% making the effort less useful.
% , such as ``ablations'', ``perturbations'', ``robustness'', 
% While the language of causality may seem
% unfamiliar or overly formal at first, since causality already resides behind much of the less precise terminology in use, 
%, such as ``ablations'', ``perturbations'', ``robustness'', 
% To that end, we introduce Common Abstract Topologies (CATs) to catalog common causal patterns that occur across many challenging issues with evaluating LLM reasoning. This opens up both practical and theoretical research directions to reinterpret and reframe existing approaches as well as develop a practical guide for future work grounded in causality.


% \todo{check all citations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \ifarxiv
% \else

\section*{Acknowledgment}

Felix Leeb is supported by the International Max Planck Research School for Intelligent Systems (IMPRS-IS). Zhijing Jin is supported by PhD fellowships from the Future of Life Institute and Open Philanthropy, as well as the travel support from ELISE (GA no 951847) for the ELLIS program. 

The material presented in this manuscript is partly based upon works supported by the German Federal Ministry of Education and Research (BMBF): Tübingen AI Center, FKZ: 01IS18039B; by the Machine Learning Cluster of Excellence, EXC number 2064/1 – Project number 390727645; 
the Swiss National Science Foundation (Project No. 197155); a Responsible AI grant by the Haslerstiftung; an ETH Grant (ETH-19 21-1); the Precision Health Initiative at the University of Michigan; and by the John Templeton Foundation (grant \#61156).

Lastly, the authors thank Luigi Gresele for the many fruitful conversations and feedback.


% \fi

% \section*{Impact Statement}

% This paper addresses some of the most critical and emerging challenges field of NLP in the context of evaluating reasoning abilities of LLMs, but also machine learning (ML) at large by highlighting the limitations of current benchmarks due to biases, artifacts, and sample leakage. By proposing a causality-centered framework, we aim to systematically address these issues, enhancing the reliability and validity of LLM evaluations.

% The broader impact of our research extends to improving the development of AI technologies that are fair, accountable, transparent, and aligned with human values. As LLMs increasingly influence various societal domains, our work underscores the importance of responsible AI research and development. By advocating for a causality-based evaluation framework, we emphasize the need for multidisciplinary efforts to ensure AI technologies contribute positively to society while mitigating potential risks such as bias amplification, privacy concerns, and job displacement. Most importantly, our arguments to make causality more accessible, by turning attention towards conceptual causal models, provide tools to improve the communication of assumptions, limitations, and future directions in proposed solutions using the principled language of causality.


% \nocite{langley00}

\bibliography{refs,example_paper}
\bibliographystyle{unsrtnat}


\end{document}
