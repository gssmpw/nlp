% Study 1: demonstrate that asking people to label importance area yield different result from when asking them to label "area of importance" as we defined. 3 visualizations, two prompts, show that areas that people click is different based whether they are told to label "important area" or "area required to solve a task". 

% Study 2: taking 5 visualizations, ask people to label "area of importance" as we defined it, which is about minimal area reuiqred, either via bubble view, annotation lasso, regular grid, or dynamic grid, compares metrics such as 1) duration of experiment, 2) number of participants to converge, 3) number of clicks, 4) perceived effort, 

\section{User Study}

We investigate how crowdworkers annotate important areas in visualizations with different annotation methods. We first demonstrate that annotations differ when users are instructed to annotate task-specific vs. task-agnostic areas, motivating the need for more task-specific annotation data to be collected. We then compare four methods participants can use to identify minimal task-relevant areas in a visualization: BubbleView, ImportAnnots, Static Grid, and Adaptive Grid. We evaluate these methods based on four metrics: task completion time, the number of participants required for convergence, annotation effort (e.g., number of clicks), and usability.

\subsection{Participants and Design}

We conducted a power analysis based on pilot results. Considering the smallest effect size across all comparison metrics (Cohen's $f$ = 0.2715 for cognitive load), a target sample of 152 participants would yield 80\% power to detect an overall difference between annotation methodologies at an $\alpha$ level of 0.05. 
We recruited participants from Prolific ($M_{age}$ = 38.5, $\sigma = 11.9$, 52 females) and compensated them \$12 per hour. 
We curated a set of 18 visualizations from ChartQA~\cite{chartQA} and CharXiv~\cite{charxiv}, covering a diverse range of chart types (e.g., bar, stacked bar, pie, line, Choropleth map, heatmap, histogram, scatterplot, and contour plots). Each participant was then shown these selected charts in a random order. 
% \will{there may be a bit of ambiguity here: if the charts are randomly selected, how do you ensure they cover diverse chart types? }
In a between-subject set-up, participants were also randomly assigned to annotate them via one of four tools: BubbleView, ImportAnnots, and Static/Adaptive Grid.

\subsection{Procedure}
% \will{what could be further clarified is that whether this is a within- or bw-subjects experiment}
The study was conducted as a between-subject experiment. After consenting to the experiment, participants were instructed on how to use the assigned annotation tool (BubbleView, ImportAnnots, Static Grid, or Adaptive Grid). % with a tutorial page.
They first solved an example task with a simple bar chart with one of the following prompts: \textit{annotate the important area and describe key points}, \textit{annotate the area minimally required for you to identify the highest value in the chart} (results see Section~\ref{results:taskbasedagnostic}).
Then, they labeled 18 visualizations using the same tool. 
For ImportAnnots, participants were instructed as \textit{annotate important areas related to answering the question}. 
For BubbleView, they were just asked to answer the question following prior work's design~\cite{salchartQA}. 
For Grid Labeling (Adaptive/Static), they were instructed to annotate ``Task-Specific Importance.'' 
In the end, they reported the tool's usability using NASA-TLX~\cite{hart1988development}, completed an assessment of their visualization literacy~\cite{mini-vlat}, and provided demographic information.

\subsection{Results: Importance vs. Free-Viewing} \label{results:taskbasedagnostic}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/instruction.pdf}
    \caption{Averaged participants' annotations for four tools when applying different instructions. The task-specific instruction was ``What is the important area regarding this question: What is the minimum value of the bar?''.}
    \label{fig:instruction}
    \vspace{-3mm}
\end{figure}

We demonstrate the participants' annotation behavior appeared significantly different when they were instructed to annotate components of the visualization they found important during free-viewing, compared to when they were instructed to annotate the importance area in response to a specific task, as shown in \autoref{fig:instruction}.
During task-agnostic annotation, the importance area is more evenly distributed across the visualization with a slight emphasis on the top of the visualization and the title text (aligned with existing work such as patterns identified by ~\cite{graphicDesignImportance}).
In contrast, the annotations cluster around the area with the smallest bar at the bottom of the visualization in response to the find minimum task.
This further validates our case that existing models trained on free-viewing annotation and eye-tracking data might not be the most predictive for visualization saliency, considering salient regions can vary with user intent and tasks.

\subsection{Results: Methods Comparison}

\label{results:comparison}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figures/convergence.pdf}
    \caption{Required number of participants to reach 90\% similarity compared to the aggregated importance. Five different similarity metrics were used to measure the convergence.} 
    \label{fig:convergence}
    \vspace{-5mm}
\end{figure}

In the user study, we compared four annotation methods using five metrics: usability, click counts, annotated area per mouse travel distance, annotation speed, and inter-participant agreement. The group means per metric are summarized in ~\autoref{fig:teaser}.

\noindent\textbf{Usability: Participant Task-Load Analysis}
To measure the usability of the annotation methods, we used the NASA-TLX scale, which has six categories (mental demand, physical demand, temporal demand, effort, performance, and frustration).
The MANOVA suggested that the annotation methods statistically differ (Wilks’ $\Lambda$ = 0.8626, F(18, 7362.88) = 21.94, $p < 0.001$), while independent ANOVAs showed differences except for frustration. 
A post hoc Tukey test followed the five remaining categories.
ImportAnnots showed higher physical demand than others ($p\leq0.001$). Static Grid had lower temporal demand than Adaptive Grid and BubbleView ($p\leq0.023$) while also having higher performance than BubbleView and ImportAnnots ($p\leq0.028$). For effort, ImportAnnots and Static Grid were better than other tools ($p<0.001$).


% For temporal demand, the significant effect was mainly driven by differences between Adaptive Grid (M = 2.51) and Static Grid (M = 2.16, p < 0.01), and BubbleView (M = 2.39) and Static Grid (p = 0.0235), showing that Static Grid had the lowest temporal demand.

% For performance, significant differences were found between ImportAnnots (M = 4.85) and Adaptive Grid (M = 5.13, p = 0.0002), ImportAnnots and BubbleView (M = 5.10, p = 0.0009), and BubbleView and Static Grid (M = 5.29, p = 0.0284). ImportAnnots had significantly lower performance scores than all others.

% For effort, all pairwise comparisons except Adaptive Grid vs. BubbleView (p = 0.1124) and ImportAnnots vs. Static Grid (p = 0.4374) were significant. BubbleView (M = 5.87) required significantly more effort than ImportAnnots (M = 4.93, p < 0.001) and Static Grid (M = 5.05, p < 0.001), indicating higher perceived effort for BubbleView.

\noindent\textbf{Interaction Efficiency: Click Count Comparison.}
The ANOVA (F(32,608) = 248.995) and post hoc Tukey test suggested that the significant effect is driven by the difference in click count between all pair-wise comparisons among methods except adaptive and static grid ($p = 0.42$). Participants labeled important areas with adaptive and static grids using fewer clicks than BubbleView. The grid-based methods required more clicks than ImportAnnots, but that is caused by the stroke tool, which allowed participants to annotate a large area while dragging the mouse with a single click. 

\noindent\textbf{Coverage Efficiency: Annotated Area per Mouse Travel Distance.}
We measured the coverage efficiency by dividing the annotated area size by the mouse travel distance during annotation. With ANOVA (F=30.33) and post hoc Tukey test ($p<0.001$), we observed a significant difference between Adaptive Grid and Static Grid having higher coverage efficiency than BubbleView and ImportAnnots.  


\noindent\textbf{Time Efficiency: Annotation Speed Across Methods.}
ANOVA (F=11.79) and the following Tukey test showed BubbleView required less time per annotation compared to other tools ($p<0.001$), demonstrating its efficiency in annotation speed. However, we argue that the underlying reason stems from the difference between saliency and importance, where capturing importance may naturally involve more intention during the annotation process~\cite{turkeyes}.

\noindent\textbf{Convergence Speed: Agreement Across Participants.}
We examined the number of participants needed to achieve 90\% similarity with the aggregated mask using five similarity metrics: Spearman’s Rank Correlation~\cite{spearman}, Structural Similarity Index~\cite{ssim}, Dice Coefficient~\cite{dice}, Jaccard Index~\cite{jaccard}, and Kullback-Leibler (KL) Divergence~\cite{kldivergence}. We measured the convergence of 10 different randomized orders of responses for more generalized results with smoother graphs. As shown in \autoref{fig:convergence}, the Adaptive Grid and Static Grid generally converge faster than the other tools across most metrics, while the Adaptive Grid was the best performing in all metrics except Spearman's R.
