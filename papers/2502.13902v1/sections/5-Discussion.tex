\section{Discussion \& Future Work}
We contribute Grid Labeling, an annotation method for efficiently crowdsourcing task-dependent important areas of visualizations.
Grid Labeling outperformed other approaches across all metrics, as shown in~\autoref{fig:teaser}.
While ImportAnnots~\cite{importAnnot} had the lowest click count and BubbleView~\cite{bubbleView} required the least time, Adaptive Grid achieved the highest inter-participant agreement with the fewest participants across multiple metrics (e.g., SSIM, Dice, Jaccard, KL). 
Meanwhile, Static Grid demonstrated higher usability, as indicated by NASA-TLX~\cite{hart1988development}.
These results highlight the potential of Grid Labeling in training task-specific saliency models, minimizing text overemphasis and enhancing predictive accuracy.
Considering the trade-offs, we recommend using an Adaptive Grid for maximizing convergence and a Static Grid to improve usability.

Since the present study did not explore why participants labeled certain grids as important and relied on an inter-participant agreement for quality control, future work could investigate the reasoning behind these selections to provide a more high-level, representational explanation, collecting a large-scale dataset and training a task-specific importance prediction model.
Additionally, future work could refine Adaptive Grid generation using vision-based LLMs to enhance annotation usability by semantically filtering less important visualization grids.





