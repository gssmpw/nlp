\section{Related Work}
Researchers have been leveraging eye tracking methodologies from human perception research to model how people perceive images~\cite{shanmuga2015eye, bonhage2015combined, conklin2016using}.
These models help assess the appearance and salience of visual representations, enabling eye movement tracking to understand the perceptual and cognitive mechanisms of scene perception~\cite{itti1998model} and object detection~\cite{borji2015salient}.
The existing saliency models perform well in naturalistic scenes
%and real-world object detection
; however, there are unique perception rules and cognitive biases in the artificial world of data visualization 
%does not always follow the rules of perception in the natural world
~\cite{franconeri2021science, correll2012comparing, polatsek2018exploring, knittel2024gridlines}, and, thus, these models do not accurately predict where people would look in visualizations. 
Visualization researchers have been building visual saliency models geared to visualizations~\cite{DVSaliencyModel2017Matzen, bylinskii2016should}. %and adopting them for predicting eye gaze on visualizations. %enabling the prediction of visual saliency across design styles~\cite{fosco2020predicting}.
However, these models rely on handcrafted features, making it difficult to generalize to complex visualizations. Additionally, these models cannot incorporate textual information to generate task-specific saliency maps since the prediction is solely based on visual inputs.

With the advent of deep learning, gaze data were used as the ground truth of saliency models~\cite{fosco2020predicting, scannerDeeply, scanpath}, leading to higher performance in saliency prediction while enabling task-specific saliency~\cite{salchartQA}. 
These models usually need large-scale datasets to learn complex patterns. However, gathering precise gaze data is 
%challenging and requires specialized eye-tracking devices. While these devices provide accurate results, they tend to be 
costly and cumbersome, which limits large-scale data collection efforts. 
Many researchers, therefore, proposed several proxies for eye gaze. WebGaze~\cite{webgaze} uses a webcam for cheap and easy deployment in online studies yet suffers from data quality issues due to low-resolution cameras and uncontrolled calibration.
Therefore, mouse-(cursor-)based annotation tools~\cite{jiang2015salicon,bubbleView,importAnnot} were proposed to improve data quality. Among these methods, BubbleView~\cite{bubbleView} was the most used tool for capturing visual saliency and importance~\cite{graphicDesignImportance, salchartQA}.
However, BubbleView is primarily designed for exploring images and gathering information, which differs slightly from the goal of capturing perceived importance. As a result, while BubbleView is well-suited for measuring visual saliency, it may not be the best tool for capturing %instruction-tuned \yao{I would keep it consistent saying task-specific}
task-specific importance~\cite{turkeyes}. Built upon these prior approaches' limitations, our Grid Labeling aims to collect responses that cover all essential areas of the visualization with minimum noise, leading to more efficient data collection.



% One key motivation to our grid-based approach is to help people 
% We also demonstrate that the grid-based approaches can minimize biases in annotation to disproportionally emphasize text elements~\cite{DVSaliencyModel2017Matzen}
% % blurring the visualization can disproportionately emphasize text elements~\cite{DVSaliencyModel2017Matzen}, potentially misrepresenting a user's true areas of interest.
% More recently, 
% % \ms{Changed a bit using Yao's work (task-dependent saliency), but not sure whether it looks ok}
% Yao et al.~\cite{salchartQA} collect task-dependent saliency using the BubbleView method, % but their approach had some limitations. 
% and made a significant improvement on existing saliency models.
% First, the blurred visualization allowed users to perceive the overall structure of the chart, which prevented the system from capturing the specific action of identifying the maximum value. However, increasing the blur to address this issue introduced another challenge. As the structure became less visible, users had to explore the entire image, leading to the consideration of irrelevant regions as salient.