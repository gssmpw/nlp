Knowing where people look in visualizations is key to effective design. Yet, existing research primarily focuses on free-viewing-based saliency models--- although visual attention is inherently task-dependent. 
Collecting task-relevant importance data remains a resource-intensive challenge.
To address this, we introduce \textit{Grid Labeling} -- a novel annotation method for collecting task-specific importance data to enhance saliency prediction models. 
Grid Labeling dynamically segments visualizations into Adaptive Grids, enabling efficient, low-effort annotation while adapting to visualization structure.
We conducted a human-subject study comparing Grid Labeling with existing annotation methods, ImportAnnots, and BubbleView across multiple metrics. 
Results show that Grid Labeling produces the least noisy data and the highest inter-participant agreement with fewer participants while requiring less physical (e.g., clicks/mouse movements) and cognitive effort.