\section{Literature Review}
\label{sec:lit}
We review a selection of the literature on bias detection and non-discrimination law, structuring the review around three topics: (1) challenges in detecting and establishing indirect discrimination, (2) bias detection methods with and without access to demographic data, and (3) auditing algorithmic systems on bias.

\subsection{Challenges in Detecting and Establishing Discrimination}

Under European non-discrimination law, DUO's control process is classified as indirect discrimination as an apparently neutral practice disproportionately disadvantaged students with a non-European migration background\footnote{Supra note 2}~\cite{BilkaKaufhaus}. In algorithms, discrimination often arises from the \emph{proxy and correlation challenge}~\cite{EC_algorithmicdiscrimination}. Proxies refer to characteristics that are correlated with a {\protectedgroup}, leading to disadvantaged outcomes for this group. For example, the Court of Justice of the European Union (CJEU) found that using part-time employment as a basis for differentiation in the context of occupational pension schemes constituted indirect discrimination, as over 80\% of the part-time workers in Spain in 2012 were women~\citep{MorenoInstitutoNacional}. 
In some cases, courts may accept that differential treatment of a {\protectedgroup} is acceptable. Under EU law, indirect discrimination can be justified if it is demonstrated that a legitimate aim is being pursued, and the means of achieving that aim are appropriate and necessary~\citep{EmploymentEqualityDirective,RacialEqualityDirective,GoodsandServicesDirective,RecastGenderEqualityDirective}. Our paper highlights how unsupervised methods can aid in the challenge of assessing indirect discrimination in algorithmic-supported decision-making processes.

\subsection{Bias Detection With and Without Access to Demographic Groups}
In the algorithmic fairness literature, identifying systematic differences in the treatment of individuals or groups compared to others is commonly referred to as \emph{bias detection}. Most bias detection methods assume access to demographic groups such as gender or ethnicity~\cite{Aif360,Hardt2016, Kusner2017, Kleinberg2018,delaney2024oxonfair}. However, in practice, these labels are often unavailable due to privacy legislation~\citep{van2023using}. For instance, the EU's GDPR restricts the collection of ``special categories of data'' (article 9)~\citep{gdpr}, which include data on ethnicity, religion, health, and sexual orientation\footnote{Age and gender are not considered special categories of data. See Article 9(1) GDPR. Although, a caveat could exist~\citep{van2018trans}. The other way around, ``political opinions,'' ``trade union membership,'' ``genetic,'' and ``biometric'' data are special categories of data but are not protected by EU non-discrimination directives.}. The EU AI Act introduces a potential exception to the prohibition on processing special category data: ``to the extent that it is strictly necessary for the purpose of ensuring bias detection and correction''~\cite{AIAct}. Nonetheless, ongoing legal uncertainties regarding the interplay between the AI Act and the GDPR underscore the need for methods to assess bias in algorithmic systems without relying on demographic data.

There are several alternatives to using data on demographic groups. First, researchers have presented methods to predict demographic groups, which are often referred to as proxy models or attribute classifiers~\cite{ashurst2023fairness}. For example, Bayesian Improved Surname Geocoding (BISG)~\cite{Elliot2009} uses Bayes Theorem and data from the United States Census Bureau, including their surname and address, as a proxy for ethnicity or race~\citep{ashurst2023fairness}. This set of methods does require to specify the demographic group of interest beforehand. A second set of methods circumvents the problem of limited availability of protected characteristics by using a Rawlsian definition of fairness~\citep{rawls2001justice} that maximizes utility for the most disadvantaged group~\citep{lahoti2020fairness, hashimoto2018fairness, chai2022self}. However, the outcome of these methods is found to not always adhere to more traditional parity-based group fairness metrics~\citep{islam2024fairness}. The third set of alternatives are unsupervised learning methods for fairness~\citep{Nasiriani_2019, 2021BMVC_UDIS}. These methods tend to modify traditional clustering algorithms to handle fairness criteria explicitly. Our work contributes to this latter category. The main benefit of this set of methods compared to proxy inferring methods is that they do not require specification of demographic groups beforehand. Thus, unsupervised learning methods are particularly helpful to examine the effect of apparently neutral provisions that might result in indirect discrimination. 


\subsection{Auditing Algorithmic Systems}
Inspired by established practices in non-algorithmic disciplines, auditing has the potential to mitigate ethical and legal risks in algorithmic systems through both internal and external oversight~\citep{raji2022outsider}. Legislators across various jurisdictions are implementing regulations to impose audit frameworks for these systems. For instance, since 2023, New York City has required automated employment decision-making tools to undergo audits by independent third-party auditors~\citep{LL144}. Similarly, since 2024, the EU Digital Services Act mandates annual independent third-party audits for online platforms and search engines with over 45M annual users~\cite{DSA}. In the coming years, the EU AI Act places predominantly self-regulatory obligations on producers and deployers of high-risk AI systems~\cite{AIAct}. 

Implementing effective auditing regimes for algorithmic systems, however, poses practical and cultural challenges. Auditors frequently encounter obstacles such as restricted data access~\citep{groves24}, quandaries about which fairness metrics fit the context best~\citep{corbett2023measure} and varying opinions on what constitutes a legitimate auditor~\citep{groves24}. Furthermore, auditing reports are not always required to be disclosed, reducing transparency and hindering the development of public knowledge regarding auditing standards. By discussing the audit of DUO's risk profiling algorithm, we contribute to public knowledge on best practices for auditing algorithmic systems.

%