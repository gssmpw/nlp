\section{Related Work}
\label{sec:related}

% Ten years before the SWE-bench dataset of Python
% issues~\cite{jimenezswe} came the Defects4J dataset of Java
% issues~\cite{just2014defects4j}.

Prior to SWE-bench~(Python)~\cite{jimenezswe},
Defects4J~(Java)~\cite{just2014defects4j} has been a popular benchmark
in the community.
The creators of Defects4J carefully curated and cleaned up each issue
by hand.
Unlike SWE-bench, Defects4J only contains bug reports, no feature
requests.
The earliest system we are aware of that generates tests from issues,
Libro~\cite{kang2023large}, focuses on Defects4J.
Libro achieves a fail-to-pass rate of 19.9\% with one generation.
(\citet{mundler2024swtbench} ported Libro to Python and measured a
fail-to-pass rate of 15.2\%.)
\citet{plein_et_al_2024} proposed another test-generation system for
Defects4J, reporting a fail-to-pass rate of 6\%.
Both systems have relatively low success rates, and
unlike our work, neither evaluates the impact of generated tests on
issue-resolving systems.

When resolving issues, some SWE agents also generate tests along the
way.
% SWE-Agent is a single-agent system, prompted to start by using its
% general tools to attempt to reproduce the issue~\cite{sweagent2}.
The original
SWE-Agent~\cite{sweagent2}, a single-agent system, attempts to reproduce the issue,
as explicitly specified in its prompt.
% And both CodeR~\cite{sweagent3coder} and
% SpecRover~\cite{autocoderover2} are multi-agent systems, starting with
% a reproducer agent for generating tests.
Some multi-agent systems---CodeR~\cite{sweagent3coder} and SpecRover~\cite{autocoderover2}---start with a Reproducer agent for generating tests.
However, none of the three (SWE-Agent, CodeR, or SpecRover)
are evaluated for the effectiveness of their generated tests.
Agentless~\cite{sweagent5agentless} relies on inference
scaling, generating several candidate patches and several
tests.
It then uses the tests to help rank the patches, ultimately choosing a
single patch to submit.
The effectiveness of the tests is evaluated indirectly by their impact
on issue resolution rate (from 27\% to 32\%), not directly for their
own fail-to-pass rate or coverage like in our work.

Two very recent systems are dedicated to generating tests from Python
issues.
Aegis~\cite{aegis} is a multi-agent system that uses inference
scaling, but the exact dataset for their evaluation
is unclear (the paper says SWE-bench Lite, but then compares against
numbers from another system on SWT-bench Lite, which is different).
Aegis is more costly than \soly, and unlike our
work, the Aegis paper (a) does not report coverage numbers and (b) does not evaluate how tests can help trade off precision vs.\ recall w.r.t. performance of SWE agents.
SWE-Agent+ adapts a patch-generating agent to generate tests
instead~\cite{mundler2024swtbench} and achieves 19.2\% fail-to-pass rate on the SWT-bench Lite dataset
introduced by the same paper.
SWT-bench applies less rigorous quality filters than \tdd, and it
measures coverage in a round-about way by first running additional
tests than just the generated ones and then subtracting them back out.
Using tests generated by SWE-Agent+ as a filter improves precision
of SWE agents to 47.8\% while reducing recall to~20\%.
\soly outperforms SWE-Agent+ on all of these metrics.