\section{Methodology}\label{sec:method}

This section describes three solutions to the problem stated in
Section~\ref{sec:problem}: \solx, \soly, and a baseline approach.

\subsection{\solx: Test Generation Guided by Self-reflective Action Planner}

\begin{figure}[t]
  \centerline{\includegraphics[width=.9\columnwidth]{overview.pdf}}
  \caption{\label{fig:overview}Overview of \solx.}
\end{figure}

\cref{fig:overview} presents an overview of \solx, which has three main
components: a localizer, a planner, and a test generator.

\paragraph{Localizer.}

Before developers tackle an issue, they usually start by
gaining an understanding of the current state $c_\mathrm{old}$ of the
project.
They do this by localizing relevant existing tests and \emph{focal}
functions (i.e., functions exercised by those tests and likely places
for a fix).
Depending on the developer's familiarity with the project and the
nature of the issue, the difficulty of localization may vary.
Inspired by human developers' actions, our approach also starts with
test and focal function localization.
The localizer phase collects all files from $c_\mathrm{old}$
that contain at least one test function for test localization.
It presents the list of files and the issue description
$d_\mathrm{issue}$ to the LLM and asks it to generate 10
relevant test file names. Our initial findings on the dev set of SWE-bench
indicated that the Top-10, Top-5, and Top-1 accuracy for test file localization
with GPT-4o are 83.6\%, 76.0\%, and 59.1\%, respectively.
We restricted file retrieval to 10 files so as not to overwhelm the
contexts for subsequent LLM calls.
Next, the localizer validates the file names by comparing them with
the previously collected file list and drops the ones that do not match.
After localizing test files, it makes a second LLM call with
file names and test function names from those files. The model chooses the
test files and functions relevant to~$d_\mathrm{issue}$.
It again validates
the retrieved file names, but instead of dropping any hallucinated file names,
replaces them with the file names with minimal edit distance. This ensures
validity of identified files, which is essential for the subsequent
test-generation phase.
Note that even if the localizer chooses the wrong test file, if the
file is at least valid, the test generator may still succeed.
Focal file and function localization follows a similar two-LLM-call approach.
Figures~\ref{fig:focal1} and~\ref{fig:focal11} in the Appendix illustrate the LLM prompts
for localization.

\paragraph{Self-Reflective Action Planner.}

The second phase of \solx creates a \emph{plan}, which is a list of
actions for generating the fail-to-pass tests~$y$.
There are three kinds of actions: read, write, or modify.
A \emph{read}~$f$ action reads a function~$f$ from $c_\mathrm{old}$ to
use as context in a prompt.
A \emph{write}~$f$ or \emph{modify}~$f$ action declares the intent
to write a new test function or modify an existing test function.
Here, $f$ is a file and function name (in the planner, write and
modify actions do not yet include the exact code for the test
function, which is left to the test generator phase of \solx).
The planner starts by executing read actions for the
files and functions provided by the localizer.
Next, it prompts an LLM with the function definitions, the issue
description $d_\mathrm{issue}$, and instructions to make an initial
plan, comprising only read actions.
The next step of the planner validates the planned actions: it checks
whether the actions generated by the LLM refer to valid file and
function names.
The final step of the planner is ``reflect and improve plan'', an LLM
call with a prompt including feedback from validation.
At this point, the plan is no longer restricted to only read actions,
and can also contain a write or modify action. We provide statistics on read/write/modify actions generated by the Planner in Section~\ref{appendix_action_counts}.
The model is also instructed to self-reflect on the proposed plan with
one of three possible outcomes: ``Satisfied'', ``Unsatisfied'', and
``Unsure''.
If the model chooses ``Satisfied'', \solx moves forward to the
test-generation phase.
For other options, it returns to the validation step and then repeats
the ``reflect and improve plan'' step.
This process is repeated at most five times.
In most cases, the model is satisfied with the plan in the first two
turns.
The two planner prompts are presented in Figures~\ref{plan1} and~\ref{plan2} (Appendix).

\paragraph{Test Generator.}

The test-generation phase of \solx executes the actions computed by the planner,
which can involve generating a new test (write action) or updating an existing
test (modify action). To guide
the LLM in this task, we extract the test structure and imports from the
localized test file and make them available in the prompt.
This reduces the burden on the LLM to generate imports.
The file structure is relevant for new tests, to
determine their insertion point in the test file.
\solx uses a different prompt for a write vs.\ a modify action,
illustrated in Figures~\ref{modifyprompt} and~\ref{fig:writeprompt} in the Appendix.
%% As we have plans from the self-reflective action planner phase, it's time to
%% execute them. We execute all of the planned actions. We also collect the test
%% structure and imports from the test file. The exposure to imports will prevent
%% the model from regenerating them, thus reducing some burden. The structure is
%% important, especially for new tests, to find where to insert them in the
%% file. Note that this structure is not important for function modification
%% because we already know where to insert the function. We have two separate
%% prompts for modified and new tests.
For new tests, the model needs to generate the preceding function name in
addition to the test. Unlike \citet{mundler2024swtbench}, we did not
try to generate diffs; instead, we ask the model to generate complete test
functions, even for the modification case.
Since model training data tends to contain more complete functions
than diffs, we expect the model to perform better at generating
functions.

To handle missing or hallucinated imports, \solx includes an import-fixing step
in this phase, where it looks at model-generated imports and
%% The LLM may forget or hallucinate the necessary imports. We have an import
%% fixing phase where we look into two sources of information: the model-generated
%% import
linting errors detected using
Flake8\footnote{https://flake8.pycqa.org/en/latest/} (a static analysis tool)
to identify missing imports. Note that Flake8 reports different styling errors;
we manually curated the error codes to catch name-related errors.  In case of
missing imports, we add a dummy import to the function. Then, we take the
model-generated and dummy imports and try to find the imported module among the
files in the codebase. If we find the module, we replace the
model-generated/dummy import with the one from the codebase; otherwise, we
continue with the model-generated/dummy import as a fallback. Finally, we add
the function in the codebase and generate a Git diff to create the test
patch.


\subsection{\soly: Ensemble Method using Multi-Sampling with Heterogeneous Prompting}

\solx has several components (e.g., localizer, planner), but they are not
perfect.  The file localizer accuracy for focal and test localization is 82.4\%
and 70.6\% with GPT-4o, respectively.  As the output from localization serves as
input to later LLM calls, those later calls may be affected by inaccuracies in
localization.  Conversely, LLMs can sometimes generate fail-to-pass tests in a
zero-shot setup, even without any context.  So, selectively including and
excluding parts of localization may produce different fail-to-pass tests that
are not generated by \solx.  \soly uses the test generated by \solx (T1) and
adds four new tests (T2--T5), obtained by skipping the planner stage, and
including neither, one, or both of focal and test localization.  In other words,
\soly runs the test generator stage five times with different,
\emph{heterogenous} prompts.  We favor heterogeneous prompts with greedy
decoding over homogeneous prompts with higher model temperature because our
initial experiments showed that the latter yields poorer tests and lower
diversity.

To pick the best among the five candidate tests, we run the five tests on
$c_\textrm{old}$ (recall that $c_\textrm{new}$ is not available yet) and analyze
the execution logs.  If a test passes on $c_\textrm{old}$, we just discard that
test because it violates the fail-to-pass criterion.  We classify the remaining
tests into three groups: assertion failure, other failure~(the test runs but
produces the wrong output), and error~(the test does not run properly, e.g.,
because of wrong syntax or an exception in a fixture).  We pick a test from the
first non-empty group to maximize the chance that it failed for the right reason
(i.e., it reproduces the bug described in the issue).  If the selected group has
multiple tests, we break the tie with a pre-determined ordering of the five
prompts that favors tests from prompts with more or better information: \solx,
followed by the prompts with both localizers, test localizer only, focal
localizer only, and neither localizer, in that order.


\subsection{Baseline: Zero-shot Test File Generation}

Recent instruction-tuned LLMs excel at following
instructions~\cite{peng2023instruction,zhang2023instruction}. We propose a
simple zero-shot approach to generate a fail-to-pass test given the repository
name and the issue description.% (see detailed prompt in Figure~\hl{X}). 
Given the prompt, the model generates a complete test file with all necessary imports to
make it compilable. In real scenarios, test files usually have multiple test
cases, but this baseline usually generates only a single test per file. 

% Commented this part as it's minor implementation detail
% after discussion with Toufique
% The
% generated test file (we call it \texttt{\small test\_tdd.py}) needs to be placed
% in the right directory for the imports in the test to work. Fortunately, all
% Python projects in \tdd have at least one directory called \texttt{\small
%   tests}. %; some projects have multiple such directories.  So we follow the
% simple approach of searching for the \texttt{\small tests} directory and placing
% \texttt{\small test\_tdd.py} in that directory.
% After that, we execute git diff to have access to the test patch.
