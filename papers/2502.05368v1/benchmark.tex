\begin{figure*}[h]
    \centering
    \includegraphics[width=.9\textwidth]{tdd-flow-new.pdf}
    \vspace{-.4cm}
    \caption{Overall flow of TDD-bench dataset filtering starting from SWE-bench verified.}
    \label{fig:tdd-flow}
    \vspace{-.3cm}
\end{figure*}




\section{\tdd Benchmark}
\label{sec:tddbench}


This section presents \tdd, a new benchmark that supports evaluation of
techniques for generating tests from an issue description and an old code
version, without access to new code that resolves the
issue~(see problem statement in Section~\ref{sec:problem}).


%% \subsection{SWE-Bench}\label{sec:swe_bench_verified}

%% \tdd builds upon SWE-bench~\cite{jimenezswe}.
%% SWE-bench was mined from GitHub pull requests (PRs) that resolved
%% issues.
%% Each SWE-bench instance is a pair
%% \mbox{$x=\langle d_\mathrm{issue},c_\mathrm{old}\rangle$}
%% of an issue description $d_\mathrm{issue}$ alongside the old version
%% $c_\mathrm{old}$ of the code just before the PR.
%% Furthermore, the SWE-bench evaluation harness uses a
%% set of golden tests $\hat{y}$ and golden new code $\hat{c}_\mathrm{new}$ mined from the same PR.

%% A solution to SWE-bench is a function $\mathit{genCode}$ that takes an
%% instance $x$ and returns a new version
%% \mbox{$c_\mathrm{new}=\mathit{genCode}(x)$} of the code.
%% The golden new code $\hat{c}_\mathrm{new}$ and golden tests $\hat{y}$
%% in the mined PR are hidden from the solution $\mathit{genCode}$, which
%% only has access to~$x$.
%% SWE-bench evaluates a solution $\mathit{genCode}$ by the sum, over all
%% instances $x$, of the pass criterion
%% \mbox{$\mathit{pass}(x,c_\mathrm{new})=I\big(\mathit{fail}\notin\mathit{runTests}(\hat{y},c_\mathrm{new})\big)$}.
%% (Here, $I(p)$ is the indicator function that returns 1 if
%% predicate $p$ is true and 0 otherwise.)
%% While \tdd instances \mbox{$x=\langle d_\mathrm{issue},c_\mathrm{old}\rangle$}
%% look the same as SWE-bench instances,
%% \tdd solutions are functions $\mathit{genTests}$ instead of functions
%% $\mathit{genCode}$, and it uses an evaluation function
%% $\mathit{tddScore}$ instead of $\mathit{pass}$.
%% The evaluation function $\mathit{tddScore}$ uses the hidden golden
%% new code $\hat{c}_\mathrm{new}$ but it does not use the
%% hidden golden tests $\hat{y}$.


\subsection{\tdd Evaluation Harness}\label{sec:eval_harness}

\cref{fig:eval_harness} shows the harness for evaluating tests $y$, which
typically come from a $\mathit{genTests}$ solution, but the harness can also be
applied on golden tests $\hat{y}$ mined from a pull request~(PR).  The
evaluation harness runs in a containerized environment.  Starting at the top
left, tests come in the form of a patch, which is applied (via \texttt{\small
  git\,apply}) on the old code~$c_\mathrm{old}$.  Next, the harness analyzes the
resulting code $c_\mathrm{old}\oplus y$ to resolve the contributed test
functions~$y$.  The harness then executes $y$ while avoiding running other tests
that occur in the same file but were not contributed in the patch.  This yields
test results, including coverage achieved on the old code.  At least one of
these tests should fail, indicating that the tests reproduce the
issue at hand.

Moving to the bottom part of \cref{fig:eval_harness}, the code changes come from
the golden code patch mined from the same PR, which is applied on
$c_\mathrm{old}$ to obtain the golden new code $\hat{c}_\mathrm{new}$.  The harness
executes the tests $y$ again, this time on the new code, to obtain a second set
of test results.  This time, all tests should pass, to validate that the issue
was indeed resolved. An example test patch is presented
in~\cref{fig:test_patch}.


\begin{figure}[h]
  \centerline{\includegraphics[width=.85\columnwidth]{eval_harness.pdf}}
  \vspace{-.3cm}
  \caption{\label{fig:eval_harness}Evaluation harness for \tdd.}
  \vspace{-.3cm}
\end{figure}


\subsection{Dataset Filters}\label{sec:tdd_filters}

\tdd builds upon prior work from SWE-bench~\cite{jimenezswe} and SWE-Bench Verified~\cite{chowdhury_et_al_2024}.  SWE-bench uses filters to keep
only those mined instances $x$ for which the set of golden tests $\hat{y}$ contains at least some tests
that fail on $c_\mathrm{old}$ and pass on the golden new code
$\hat{c}_\mathrm{new}$ from the same PR.  SWE-Bench Verified is a subset of
SWE-Bench, consisting of 500 instances vetted by human
annotators~\cite{chowdhury_et_al_2024}.  The annotators filtered out instances
where the issue description $d_\mathrm{issue}$ was underspecified or the golden
tests $\hat{y}$ were overly specific, i.e., would reject some valid new
code~$c_\mathrm{new}$.  They also removed some instances where tests failed due
to environment problems. %instead of the solution.

In the same spirit, \tdd applies more filters to obtain an even higher-quality
dataset.  In a nutshell, the filtering process applies the \tdd evaluation
harness (\cref{fig:eval_harness}) to the golden tests $\hat{y}$ from
the original PR.  Specifically, substituting $\hat{y}$ wherever $y$ occurs in
\cref{fig:eval_harness} checks whether the PR indeed contributed tests that went
from failing to passing.  We filter out any instance where the contributed tests
do not satisfy that criterion.  Although the human annotators of SWE-bench
Verified were diligent, a few instances slipped past their filters, and we drop
those for \tdd.

\cref{fig:tdd-flow} illustrates the filtering process.  Starting from the 500 instances of SWE-bench Verified,
we first drop 7 instances whose environment we could not
recreate. Next, we run the test harness on the golden tests
$\hat{y}$. This filters out 44 additional instances because the tests
do not have the expected fail-to-pass behavior (25 instances) or have zero
line coverage on the golden code patch (19 instances).
In the end, 449 high-quality instances remain across 12 repositories. We summarize key
statistics of \tdd in~\cref{tbl:tdd-stat}.

% Starting with the 500
% instances of SWE-bench Verified, we first drop 7 instances whose environment we
% could not recreate.  Next, we run the test harness on the golden tests
% $\hat{y}$.  This filters out 44 additional instances because the tests do not
% have the expected fail-to-pass behavior or have zero line coverage on the golden
% code patch. 


\subsection{Evaluation Metric}
\label{subsec:eval_metric}

Passing a test does not necessarily mean a patch is adequate to address the
issue.  \citet{aleithan2024swe} reported that 31.1\% of the passed patches are
suspicious due to weak test cases in SWE-Bench.  To ensure test adequacy or
relevance, we compute the coverage of the submitted test patch.  One key
difference between SWE-Bench Verified and \tdd is that SWE-Bench Verified runs a test file to
evaluate the submitted patch, whereas we run only the contributed tests $y$.
Not running other test cases enables us to precisely compute coverage of the
contributed tests.  If the tests are relevant, they should cover the deleted
lines in $c_\mathrm{old}$ and the added lines in $\hat{c}_\mathrm{new}$.  We
integrated the Python Coverage package into the 12 repositories and updated the
test scripts to allow us to run specific test cases and compute coverage for them.

We define the $\mathit{tddScore}$ metric that evaluates the quality of tests
generated by a solution $\mathit{genTests}$ over a set
\mbox{$X=\{x_0,x_1,\ldots\}$} of instances.  It returns a number between 0 and
100, the higher the better.  It is defined as 100 times the arithmetic mean of
the per-instance scores:
{\small
\[\mathit{tddScore}(X, \mathit{genTests})
  = \frac{100}{|X|}\sum_{x\in X} \mathit{tddScore}\big(x, \mathit{genTests}(x)\big)
\]}

Given a set of tests $y=\mathit{genTests}(x)$ submitted for an
instance, the per-instance score is a product of two factors:
\[\mathit{tddScore}(x, y)
  = \mathit{failToPass}(x, y) \cdot \mathit{adequacy}(x, y)
\]

The first factor is a binary correctness metric, using the indicator function
for the tests $y$ failing on the old code times the indicator function for the
tests $y$ passing on the new code.  While the solution $\mathit{genTests}$ only
has access to the old \mbox{code $c_\mathrm{old}$}, the evaluation metric also
uses the hidden golden new \mbox{code $\hat{c}_\mathrm{new}$} right after the
issue was fixed.
{\small
\[\begin{array}{@{}l@{}}\mathit{failToPass}(x, y) =\\
  \;\;I\big(\mathit{fail} \in \mathit{runTests}(y, c_\mathrm{old})\big)
  \cdot I\big(\mathit{fail} \notin \mathit{runTests}(y, \hat{c}_\mathrm{new})\big)
\end{array}\]}

The second factor is a fraction between 0 and 1 based on test coverage on
the old and new code:
{\small
\[\begin{array}{@{}l@{}}\mathit{adequacy}(x, y) =\\
  \displaystyle\;\;
    \frac{  |\mathit{cov}(y, c_\mathrm{old}) \cap (c_\mathrm{old}\setminus\hat{c}_\mathrm{new})|
          + |\mathit{cov}(y, \hat{c}_\mathrm{new}) \cap (\hat{c}_\mathrm{new}\setminus c_\mathrm{old})|}
         {  |c_\mathrm{old}\setminus\hat{c}_\mathrm{new}| \quad
          + \quad\; |\hat{c}_\mathrm{new}\setminus c_\mathrm{old}|}
\end{array}\]}

Adequacy focuses on just the coverage of lines added and deleted when going from
the old code to the new code, because those are the most relevant lines to be
tested.  In the above, $\mathit{cov}(y,c)$ is the set of lines covered by
running tests $y$ on code~$c$; $(c_\mathrm{old}\setminus\hat{c}_\mathrm{new})$
is the set of lines deleted by the PR patch; and $(\hat{c}_\mathrm{new}\setminus
c_\mathrm{old})$ is the set of lines added by the PR patch. 
We evaluate adequacy jointly for added and deleted lines, as some code patches may contain only added or deleted
lines.

% We had initially
% considered defining adequacy with two separate fractions for the deleted
% and added lines.  However, that was not only poorly weighted but brittle,
% because in some cases, the numerator or denominator of one of the fractions was
% zero.
