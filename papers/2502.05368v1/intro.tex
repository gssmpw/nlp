\section{Introduction}\label{sec:intro}

A software engineering~(SWE) \emph{issue} is a bug report or feature
request for improving the code in a repository.
Before a software engineer attempts to write new code that resolves a
bug, they typically write a \emph{reproduction test} to confirm the
presence of the bug in the old code.
In fact, even when working on a feature request, before writing new
code it is recommended to create an \emph{acceptance test} first, to
confirm the presence of the feature in the new code once written.
This practice is known as test-driven development~(TDD), and it
improves the quality of both tests and the
code itself~\cite{beck_2002}.
There has been recent work on resolving SWE
issues automatically with LLMs, typically with
agents~\cite{sweagent2,autocoderover2,opendevin}.
Since the release of SWE-Bench~\cite{jimenezswe}, such
issue-resolution systems are often called \emph{SWE agents}.
We posit that, just like up-front tests
help human SWE engineers, they also help automated SWE agents.

% Very recently, there are some initial solutions, such as
% \mbox{SWE-Agent+~\cite{mundler2024swtbench}}.
% Unfortunately, they are expensive (require many LLM calls and tokens)
% and have limited success (e.g.\ the state-of-the-art SWE-Agent+ only
% generates a fail-to-pass test in 19.2\% of cases).

While test generation is an active area of research, prior work focuses
on creating tests for existing code, not on creating tests from
issue descriptions alone. A recent solution \mbox{SWE-Agent+~\cite{mundler2024swtbench}} attempts to create tests from issue descriptions albeit with limited success (the state-of-the-art
SWE-Agent+ only generates a fail-to-pass test in 19.2\% cases and is quite expensive, requiring several LLM calls
and tokens). See Section~\ref{sec:related} for a more in-depth discussion of
related work.

This paper introduces \solx (acronym for ``\solx: TDD-Test gEnerator
for Reproducing issues'').
It takes as input $x$ an issue description and the original code
in the repository, and generates as output a set of tests~$y$.
\solx contains a novel self-reflective action planner for deciding
which code to read and which tests to write.
It also uses rule-based code analyses and transformations throughout
the workflow to curate LLM inputs, validate LLM outputs, and repair
generated tests.
\solx generates fail-to-pass tests in 31.4\% of cases, and in an
ensemble of size~5 dubbed \soly, that increases to 37.0\%.
At the same time, the cost (even with ensembling) is less than \$0.10
per issue with GPT-4o.

The most popular benchmark for automatically resolving SWE
issues is SWE-bench Verified~\cite{chowdhury_et_al_2024}.
To evaluate solutions such as \solx that automatically generate tests
from issues, we introduced a new benchmark, \tdd~\cite{ahmed2024tdd} in our prior work.
\tdd\footnote{\url{https://github.com/IBM/TDD-Bench-Verified}} evaluates tests by checking whether the tests a) fail on the old
code before issue resolution, b)~pass on the new code, and
c) cover the code changes well.
% It evaluates tests by checking whether they fail on the old code
% before issue resolution, pass on the new code, and cover the code
% changes well.
The fact that \tdd is derived from SWE-bench Verified enables us to
empirically study the effects of generated tests on SWE agents.
We observe that the tests from \soly can be used to trade precision for
recall on the SWE-bench Verified leaderboard.
E.g., for the system ranked 3rd on the leaderboard, filtering by the
generated tests boosts precision from 60.8\% to 91.9\% while
reducing recall to 33\%.
This paper makes the following contributions:

\begin{itemize}[leftmargin=1em, topsep=0pt, itemsep=0pt, parsep=0pt]
  \item \solx, a system that generates tests from issues, using LLMs
    with a novel self-reflective action planning technique along with
    rule-based pre- and post-processing.
 % \item \tdd, a benchmark for evaluating tests generated from issues,
    %including a high-quality dataset and a metric based on test
   % results and coverage.
  \item An empirical study on using tests generated from issues to
    filter issue-resolution candidates for SWE-bench Verified.
\end{itemize}

Generated tests can assist software engineers both before and after
resolving an issue, and can increase trust in automated
SWE agents.
