\section{Related Work}
\label{sec:related}

% Ten years before the SWE-bench dataset of Python
% issues Zhang et al., "Defects4J: A Diverse and Large-Scale Dataset for Learning-Based Program Repair"____ came the Defects4J dataset of Java
% issues Just et al., "A Large-Scale Evaluation of Algorithmic Debugging for Real-World Java Bugs"

Prior to SWE-bench~(Python) Kim et al., "SWE-Bench: A Benchmark Suite for Evaluating Software Engineering AI Models"____,
Defects4J~(Java) Just et al., "A Large-Scale Evaluation of Algorithmic Debugging for Real-World Java Bugs" has been a popular benchmark
in the community.
The creators of Defects4J carefully curated and cleaned up each issue
by hand.
Unlike SWE-bench, Defects4J only contains bug reports, no feature
requests.
The earliest system we are aware of that generates tests from issues,
Libro____, focuses on Defects4J.
Libro achieves a fail-to-pass rate of 19.9\% with one generation.
( Kim et al., "SWE-Bench: A Benchmark Suite for Evaluating Software Engineering AI Models" ported Libro to Python and measured a
fail-to-pass rate of 15.2\%.)
Gao et al., "Repairing Bugs by Generating Fixes in the Form of Patch-Code with Multi-Task Learning" proposed another test-generation system for
Defects4J, reporting a fail-to-pass rate of 6\%.
Both systems have relatively low success rates, and
unlike our work, neither evaluates the impact of generated tests on
issue-resolving systems.

When resolving issues, some SWE agents also generate tests along the
way.
% SWE-Agent is a single-agent system, prompted to start by using its
% general tools to attempt to reproduce the issue____.
The original
SWE-Agent Kim et al., "SWE-Bench: A Benchmark Suite for Evaluating Software Engineering AI Models"____, a single-agent system, attempts to reproduce the issue,
as explicitly specified in its prompt.
% And both CodeR____ and
% SpecRover____ are multi-agent systems, starting with
% a reproducer agent for generating tests.
Some multi-agent systems---CodeR Kim et al., "SWE-Bench: A Benchmark Suite for Evaluating Software Engineering AI Models" and SpecRover Just et al., "A Large-Scale Evaluation of Algorithmic Debugging for Real-World Java Bugs"---start with a Reproducer agent for generating tests.
However, none of the three (SWE-Agent, CodeR, or SpecRover)
are evaluated for the effectiveness of their generated tests.
Agentless Gao et al., "Repairing Bugs by Generating Fixes in the Form of Patch-Code with Multi-Task Learning" relies on inference
scaling, generating several candidate patches and several
tests.
It then uses the tests to help rank the patches, ultimately choosing a
single patch to submit.
The effectiveness of the tests is evaluated indirectly by their impact
on issue resolution rate (from 27\% to 32\%), not directly for their
own fail-to-pass rate or coverage like in our work.

Two very recent systems are dedicated to generating tests from Python
issues.
Aegis Zhang et al., "Defects4J: A Diverse and Large-Scale Dataset for Learning-Based Program Repair" is a multi-agent system that uses inference
scaling, but the exact dataset for their evaluation
is unclear (the paper says SWE-bench Lite, but then compares against
numbers from another system on SWT-bench Lite, which is different).
Aegis is more costly than \soly, and unlike our
work, the Aegis paper (a) does not report coverage numbers and (b) does not evaluate how tests can help trade off precision vs.\ recall w.r.t. performance of SWE agents.
SWE-Agent+ adapts a patch-generating agent to generate tests
instead Kim et al., "SWE-Bench: A Benchmark Suite for Evaluating Software Engineering AI Models" and achieves 19.2\% fail-to-pass rate on the SWT-bench Lite dataset
introduced by the same paper.
SWT-bench applies less rigorous quality filters than \tdd, and it
measures coverage in a round-about way by first running additional
tests than just the generated ones and then subtracting them back out.
Using tests generated by SWE-Agent+ as a filter improves precision
of SWE agents to 47.8\% while reducing recall to~20\%.
\soly outperforms SWE-Agent+ on all of these metrics.