\section{Related work}
\subsection{Sources of Unsafe Training Data in LLMs} 

Recent studies~\citep{yi-etal-2024-vulnerability, qi2023finetuningalignedlanguagemodels} reveal that malicious fine-tuning can severely compromise safety alignment, even with limited exposure to unsafe data. Unfortunately, current online fine-tuning services are inadequate at detecting these unsafe training data, leaving LLMs vulnerable to potential exploitation~\citep{qi2023finetuningalignedlanguagemodels}. 

Unsafe data may also emerge from synthetic training data generation. For instance, \citet{wang-etal-2022-promda} generate samples by conditioning LLMs on specific keywords and target labels, while \citet{wang-etal-2023-self-instruct} synthesize fine-tuning data from LLM-generated responses. However, as recent safety research~\citep{wang2024decodingtrustcomprehensiveassessmenttrustworthiness} indicates, even highly aligned models like GPT-4 and GPT-3.5 exhibit unsafe behaviors, suggesting that synthetic data can introduce significant risks.

In addition, inherent biases in training data pose challenges that current detection methods are not equipped to handle. Studies have shown that gender bias in training data can lead LLMs to develop skewed assumptions about occupations~\citep{Kotek_2023}, while cognitive biases during training undermine model reliability in high-stakes decisions~\citep{itzhak2024instructedbiasinstructiontunedlanguage, echterhoff-etal-2024-cognitive}.

The diverse source of unsafe training data highlights the need for more flexible and adaptable detection methods. Current moderation classifiers, often designed for specific content moderation tasks, are insufficient for addressing the complexity and variability of unsafe data in training pipelines. 

\subsection{Unsafe Training Data Detection in LLMs}

Existing efforts to detect unsafe training data primarily focus on content moderation classifiers. For example, online moderation tools such as OpenAIâ€™s Moderation API~\citep{markov2023holisticapproachundesiredcontent} are developed to detect harmful content. Recently, there has been growing efforts in developing LLM-based classifiers. One line of research has explored fine-tuning open-source LLMs on specifically curated safety dataset to develop moderation classifiers. Examples of such classifiers include Llama-Guard-3-8B~\citep{dubey2024llama3herdmodels}, Wildguard~\citep{han2024wildguardopenonestopmoderation}, Aegis-Guard~\citep{ghosh2024aegis}, and ShieldGemma~\citep{zeng2024shieldgemmagenerativeaicontent}. Another line of research focuses on leveraging LLMs directly as judges for unsafe data detection~\citep{10.1145/3599696.3612895,li2024safetyanalystinterpretabletransparentsteerable}. For instance, SafetyAnalyst~\citep{li2024safetyanalystinterpretabletransparentsteerable} proposes using LLMs to generate ``harm-benefit'' tree for interpretable content moderation. 

Beyond content moderation classifiers, some recent studies have leveraged the internal structures of models for unsafe data detection. For example, GradSafe~\citep{xie2024gradsafedetectingjailbreakprompts} utilizes gradient similarity with respect to safety-critical parameters to identify unsafe data, while BEBC~\citep{zheng2024lightweightsafetyguardrailsusing} employs fine-tuned BERT embeddings for content moderation.

Unlike online moderation tools and LLM-based classifiers, our methods eliminate the need for data curation and additional training. Furthermore, by operating independently of predefined safety taxonomies, our approach demonstrates greater flexibility and can effectively handle a wider range of safety-related scenarios. In contrast to other non-classifier approaches, such as GradSafe, our methods address the problem through the lens of data attribution. This perspective enhances detection performance by incorporating unsafe model behavior and capturing the relationship between training
data and model outputs, ultimately fostering safer model developments.

\subsection{Data Attribution for LLMs}

Data attribution methods aim to quantify the impact of individual training samples on a model's predictions for specific test cases~\citep{koh2020understandingblackboxpredictionsinfluence}. These methods have the potential to detect unsafe training data, as such data are likely to exert a disproportionate influence on unsafe model outputs, making them distinguishable from the broader benign dataset. Recently a variety of data attribution methods have been proposed to estimate the influence of training data in the context of LLMs. These include gradient-based methods~\citep{xia2024lessselectinginfluentialdata,kwon2024datainfefficientlyestimatingdata}, simulator-based methods~\citep{guu2023simfluencemodelinginfluenceindividual,chai2024trainingdatainfluencegpt} and game-theoretic methods~\citep{wang2024datashapleytrainingrun,wang2024helpfulharmfuldatafinetuningfree}.
Estimated influence scores have been utilized for tasks such as identifying mislabeled data~\citep{pruthi2020estimatingtrainingdatainfluence}, understanding memorization~\citep{feldman2020neuralnetworksmemorizewhy}, and data valuation~\citep{choe2024dataworthgptllmscale}. 
While data attribution methods have various applications in LLMs, they are computationally intensive, limiting their applicability to larger models. Despite recent advancements in efficient influence estimation methods \cite{kwon2024datainfefficientlyestimatingdata}, the computational burden remains a challenge. Gradient-similarity-based approaches, as highlighted in previous works \cite{xia2024lessselectinginfluentialdata,pruthi2020estimatingtrainingdatainfluence}, offers an efficient solution, making it better suited for scaling to LLMs.