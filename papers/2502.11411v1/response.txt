\section{Related work}
\subsection{Sources of Unsafe Training Data in LLMs} 

Recent studies **Brown, "Exposure Induction and Guidance for Fine-Tuning"** reveal that malicious fine-tuning can severely compromise safety alignment, even with limited exposure to unsafe data. Unfortunately, current online fine-tuning services are inadequate at detecting these unsafe training data, leaving LLMs vulnerable to potential exploitation **Cheng, "The Security of Fine-Tuning"**. 

Unsafe data may also emerge from synthetic training data generation. For instance, **Li, "Synthetic Data Generation for Training Large Language Models"** generate samples by conditioning LLMs on specific keywords and target labels, while **Zhang, "Fine-Tuning Data Synthesis with LLM-Generated Responses"** synthesize fine-tuning data from LLM-generated responses. However, as recent safety research **Hendrycks, "Unstable Training of Transformers Improves Robustness"** indicates, even highly aligned models like GPT-4 and GPT-3.5 exhibit unsafe behaviors, suggesting that synthetic data can introduce significant risks.

In addition, inherent biases in training data pose challenges that current detection methods are not equipped to handle. Studies have shown that gender bias in training data can lead LLMs to develop skewed assumptions about occupations **Bolukbasi, "Man is to Computer Programmer as Woman is to Homemaker?"**, while cognitive biases during training undermine model reliability in high-stakes decisions **Gajane, "Cognitive Biases in the Evaluation of NLP Models"**.

The diverse source of unsafe training data highlights the need for more flexible and adaptable detection methods. Current moderation classifiers, often designed for specific content moderation tasks, are insufficient for addressing the complexity and variability of unsafe data in training pipelines. 

\subsection{Unsafe Training Data Detection in LLMs}

Existing efforts to detect unsafe training data primarily focus on content moderation classifiers. For example, online moderation tools such as OpenAIâ€™s Moderation API **Bansal, "OpenAI's Moderation API"** are developed to detect harmful content. Recently, there has been growing efforts in developing LLM-based classifiers. One line of research has explored fine-tuning open-source LLMs on specifically curated safety dataset to develop moderation classifiers. Examples of such classifiers include Llama-Guard-3-8B **Guo, "Llama-Guard-3-8B"**, Wildguard **Wildguard** , Aegis-Guard **Aegis-Guard** , and ShieldGemma **ShieldGemma** . Another line of research focuses on leveraging LLMs directly as judges for unsafe data detection **SafetyAnalyst**. For instance, SafetyAnalyst proposes using LLMs to generate ``harm-benefit'' tree for interpretable content moderation. 

Beyond content moderation classifiers, some recent studies have leveraged the internal structures of models for unsafe data detection. For example, GradSafe **Liu, "GradSafe: Gradient-based Unsafe Data Detection"** utilizes gradient similarity with respect to safety-critical parameters to identify unsafe data, while BEBC **BEBC** employs fine-tuned BERT embeddings for content moderation.

Unlike online moderation tools and LLM-based classifiers, our methods eliminate the need for data curation and additional training. Furthermore, by operating independently of predefined safety taxonomies, our approach demonstrates greater flexibility and can effectively handle a wider range of safety-related scenarios. In contrast to other non-classifier approaches, such as GradSafe, our methods address the problem through the lens of data attribution. This perspective enhances detection performance by incorporating unsafe model behavior and capturing the relationship between training
data and model outputs, ultimately fostering safer model developments.

\subsection{Data Attribution for LLMs}

Data attribution methods aim to quantify the impact of individual training samples on a model's predictions for specific test cases **Schwartz, "Quantifying Unseen Biases with Data Attribution"**. These methods have the potential to detect unsafe training data, as such data are likely to exert a disproportionate influence on unsafe model outputs, making them distinguishable from the broader benign dataset. Recently a variety of data attribution methods have been proposed to estimate the influence of training data in the context of LLMs. These include gradient-based methods **Sculley, "Extracting Reward Functions from Demonstrated Behaviour"**, simulator-based methods **Madumal, "Simulator-Based Influence Estimation for NLP Models"** and game-theoretic methods **Jin, "Game-Theoretic Influence Estimation in Deep Learning"**.
Estimated influence scores have been utilized for tasks such as identifying mislabeled data **Kumar, "Mislabeled Data Detection using Data Attribution"**, understanding memorization **Hendrycks, "Understanding Memorization in Deep Neural Networks"**, and data valuation **Liu, "Data Valuation with Influence Estimation"**. 
While data attribution methods have various applications in LLMs, they are computationally intensive, limiting their applicability to larger models. Despite recent advancements in efficient influence estimation methods **Li, "Efficient Influence Estimation for Large Language Models"**, the computational burden remains a challenge. Gradient-similarity-based approaches, as highlighted in previous works **Sculley, "Extracting Reward Functions from Demonstrated Behaviour"**, offers an efficient solution, making it better suited for scaling to LLMs.