@inproceedings{10.1145/3599696.3612895,
author = {Franco, Mirko and Gaggi, Ombretta and Palazzi, Claudio E.},
title = {Analyzing the Use of Large Language Models for Content Moderation with ChatGPT Examples},
year = {2023},
isbn = {9798400702259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3599696.3612895},
doi = {10.1145/3599696.3612895},
abstract = {Content moderation systems are crucial in Online Social Networks (OSNs). Indeed, their role is to keep platforms and their users safe from malicious activities. However, there is an emerging consensus that such systems are unfair to fragile users and minorities. Furthermore, content moderation systems are difficult to personalize and lack effective communication between users and platforms. In this context, we propose an enhancement of the current framework of content moderation, integrating Large Language Models (LLMs) in the enforcing pipeline.},
booktitle = {Proceedings of the 3rd International Workshop on Open Challenges in Online Social Networks},
pages = {1–8},
numpages = {8},
keywords = {content moderation, harmful content, large language models},
location = {Rome, Italy},
series = {OASIS '23}
}

@inproceedings{Kotek_2023, series={CI ’23},
   title={Gender bias and stereotypes in Large Language Models},
   url={http://dx.doi.org/10.1145/3582269.3615599},
   DOI={10.1145/3582269.3615599},
   booktitle={Proceedings of The ACM Collective Intelligence Conference},
   publisher={ACM},
   author={Kotek, Hadas and Dockum, Rikker and Sun, David},
   year={2023},
   month=nov, pages={12–24},
   collection={CI ’23} }

@misc{chai2024trainingdatainfluencegpt,
      title={On Training Data Influence of GPT Models}, 
      author={Yekun Chai and Qingyi Liu and Shuohuan Wang and Yu Sun and Qiwei Peng and Hua Wu},
      year={2024},
      eprint={2404.07840},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.07840}, 
}

@misc{choe2024dataworthgptllmscale,
      title={What is Your Data Worth to GPT? LLM-Scale Data Valuation with Influence Functions}, 
      author={Sang Keun Choe and Hwijeen Ahn and Juhan Bae and Kewen Zhao and Minsoo Kang and Youngseog Chung and Adithya Pratapa and Willie Neiswanger and Emma Strubell and Teruko Mitamura and Jeff Schneider and Eduard Hovy and Roger Grosse and Eric Xing},
      year={2024},
      eprint={2405.13954},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.13954}, 
}

@misc{dubey2024llama3herdmodels,
  title =         {The Llama 3 Herd of Models}

@inproceedings{echterhoff-etal-2024-cognitive,
    title = "Cognitive Bias in Decision-Making with {LLM}s",
    author = "Echterhoff, Jessica Maria  and
      Liu, Yao  and
      Alessa, Abeer  and
      McAuley, Julian  and
      He, Zexue",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.739",
    doi = "10.18653/v1/2024.findings-emnlp.739",
    pages = "12640--12653",
    abstract = "Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. Given their training on human (created) data, LLMs have been shown to inherit societal biases against protected groups, as well as be subject to bias functionally resembling cognitive bias. Human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BiasBuster, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive science, we develop a dataset containing 13,465 prompts to evaluate LLM decisions on different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, while proposing a novel method utilizing LLMs to debias their own human-like cognitive bias within prompts. Our analysis provides a comprehensive picture of the presence and effects of cognitive bias across commercial and open-source models. We demonstrate that our selfhelp debiasing effectively mitigates model answers that display patterns akin to human cognitive bias without having to manually craft examples for each bias.",
}

@misc{feldman2020neuralnetworksmemorizewhy,
      title={What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation}, 
      author={Vitaly Feldman and Chiyuan Zhang},
      year={2020},
      eprint={2008.03703},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2008.03703}, 
}

@article{ghosh2024aegis,
    title={AEGIS: Online Adaptive AI Content Safety Moderation with Ensemble of LLM Experts},
    author={Ghosh, Shaona and Varshney, Prasoon and Galinkin, Erick and Parisien, Christopher},
    journal={arXiv preprint arXiv:2404.05993},
    year={2024}
}

@misc{guu2023simfluencemodelinginfluenceindividual,
      title={Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs}, 
      author={Kelvin Guu and Albert Webson and Ellie Pavlick and Lucas Dixon and Ian Tenney and Tolga Bolukbasi},
      year={2023},
      eprint={2303.08114},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.08114}, 
}

@misc{han2024wildguardopenonestopmoderation,
      title={WildGuard: Open One-Stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs}, 
      author={Seungju Han and Kavel Rao and Allyson Ettinger and Liwei Jiang and Bill Yuchen Lin and Nathan Lambert and Yejin Choi and Nouha Dziri},
      year={2024},
      eprint={2406.18495},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18495}, 
}

@misc{itzhak2024instructedbiasinstructiontunedlanguage,
      title={Instructed to Bias: Instruction-Tuned Language Models Exhibit Emergent Cognitive Bias}, 
      author={Itay Itzhak and Gabriel Stanovsky and Nir Rosenfeld and Yonatan Belinkov},
      year={2024},
      eprint={2308.00225},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2308.00225}, 
}

@misc{koh2020understandingblackboxpredictionsinfluence,
      title={Understanding Black-box Predictions via Influence Functions}, 
      author={Pang Wei Koh and Percy Liang},
      year={2020},
      eprint={1703.04730},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1703.04730}, 
}

@misc{kwon2024datainfefficientlyestimatingdata,
      title={DataInf: Efficiently Estimating Data Influence in LoRA-tuned LLMs and Diffusion Models}, 
      author={Yongchan Kwon and Eric Wu and Kevin Wu and James Zou},
      year={2024},
      eprint={2310.00902},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.00902}, 
}

@misc{li2024safetyanalystinterpretabletransparentsteerable,
      title={SafetyAnalyst: Interpretable, transparent, and steerable LLM safety moderation}, 
      author={Jing-Jing Li and Valentina Pyatkin and Max Kleiman-Weiner and Liwei Jiang and Nouha Dziri and Anne G. E. Collins and Jana Schaich Borg and Maarten Sap and Yejin Choi and Sydney Levine},
      year={2024},
      eprint={2410.16665},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.16665}, 
}

@misc{markov2023holisticapproachundesiredcontent,
      title={A Holistic Approach to Undesired Content Detection in the Real World}, 
      author={Todor Markov and Chong Zhang and Sandhini Agarwal and Tyna Eloundou and Teddy Lee and Steven Adler and Angela Jiang and Lilian Weng},
      year={2023},
      eprint={2208.03274},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2208.03274}, 
}

@misc{pruthi2020estimatingtrainingdatainfluence,
      title={Estimating Training Data Influence by Tracing Gradient Descent}, 
      author={Garima Pruthi and Frederick Liu and Mukund Sundararajan and Satyen Kale},
      year={2020},
      eprint={2002.08484},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.08484}, 
}

@misc{qi2023finetuningalignedlanguagemodels,
      title={Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!}, 
      author={Xiangyu Qi and Yi Zeng and Tinghao Xie and Pin-Yu Chen and Ruoxi Jia and Prateek Mittal and Peter Henderson},
      year={2023},
      eprint={2310.03693},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.03693}, 
}

@inproceedings{wang-etal-2022-promda,
    title = "{P}rom{DA}: Prompt-based Data Augmentation for Low-Resource {NLU} Tasks",
    author = "Wang, Yufei  and
      Xu, Can  and
      Sun, Qingfeng  and
      Hu, Huang  and
      Tao, Chongyang  and
      Geng, Xiubo  and
      Jiang, Daxin",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.292",
    doi = "10.18653/v1/2022.acl-long.292",
    pages = "4242--4255",
    abstract = "This paper focuses on the Data Augmentation for low-resource Natural Language Understanding (NLU) tasks. We propose Prompt-based Data Augmentation model (PromDA) which only trains small-scale Soft Prompt (i.e., a set of trainable vectors) in the frozen Pre-trained Language Models (PLMs). This avoids human effort in collecting unlabeled in-domain data and maintains the quality of generated synthetic data. In addition, PromDA generates synthetic data via two different views and filters out the low-quality data using NLU models. Experiments on four benchmarks show that synthetic data produced by PromDA successfully boost up the performance of NLU models which consistently outperform several competitive baseline models, including a state-of-the-art semi-supervised model using unlabeled in-domain data. The synthetic data from PromDA are also complementary with unlabeled in-domain data. The NLU models can be further improved when they are combined for training.",
}

@inproceedings{wang-etal-2023-self-instruct,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.754",
    doi = "10.18653/v1/2023.acl-long.754",
    pages = "13484--13508",
    abstract = "Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
}

@misc{wang2024datashapleytrainingrun,
      title={Data Shapley in One Training Run}, 
      author={Jiachen T. Wang and Prateek Mittal and Dawn Song and Ruoxi Jia},
      year={2024},
      eprint={2406.11011},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.11011}, 
}

@misc{wang2024decodingtrustcomprehensiveassessmenttrustworthiness,
      title={DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models}, 
      author={Boxin Wang and Weixin Chen and Hengzhi Pei and Chulin Xie and Mintong Kang and Chenhui Zhang and Chejian Xu and Zidi Xiong and Ritik Dutta and Rylan Schaeffer and Sang T. Truong and Simran Arora and Mantas Mazeika and Dan Hendrycks and Zinan Lin and Yu Cheng and Sanmi Koyejo and Dawn Song and Bo Li},
      year={2024},
      eprint={2306.11698},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.11698}, 
}

@misc{wang2024helpfulharmfuldatafinetuningfree,
      title={Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions}, 
      author={Jingtan Wang and Xiaoqiang Lin and Rui Qiao and Chuan-Sheng Foo and Bryan Kian Hsiang Low},
      year={2024},
      eprint={2406.04606},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04606}, 
}

@misc{xia2024lessselectinginfluentialdata,
      title={LESS: Selecting Influential Data for Targeted Instruction Tuning}, 
      author={Mengzhou Xia and Sadhika Malladi and Suchin Gururangan and Sanjeev Arora and Danqi Chen},
      year={2024},
      eprint={2402.04333},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.04333}, 
}

@misc{xie2024gradsafedetectingjailbreakprompts,
      title={GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis}, 
      author={Yueqi Xie and Minghong Fang and Renjie Pi and Neil Gong},
      year={2024},
      eprint={2402.13494},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.13494}, 
}

@inproceedings{yi-etal-2024-vulnerability,
    title = "On the Vulnerability of Safety Alignment in Open-Access {LLM}s",
    author = "Yi, Jingwei  and
      Ye, Rui  and
      Chen, Qisi  and
      Zhu, Bin  and
      Chen, Siheng  and
      Lian, Defu  and
      Sun, Guangzhong  and
      Xie, Xing  and
      Wu, Fangzhao",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.549",
    doi = "10.18653/v1/2024.findings-acl.549",
    pages = "9236--9260",
    abstract = "Large language models (LLMs) possess immense capabilities but are susceptible to malicious exploitation. To mitigate the risk, safety alignment is employed to align LLMs with ethical standards. However, safety-aligned LLMs may remain vulnerable to carefully crafted jailbreak attacks, but these attacks often face high rejection rates and limited harmfulness. In this paper, we expose the vulnerabilities of safety alignment in open-access LLMs, which can significantly enhance the success rate and harmfulness of jailbreak attacks. Through reverse alignment, achieved by accessing model parameters, we show the feasibility of efficiently fine-tuning LLMs to undermine their inherent safeguards. We investigate two types of reverse alignment techniques: reverse supervised fine-tuning (RSFT) and reverse preference optimization (RPO). RSFT operates by supervising the fine-tuning of LLMs to reverse their inherent values. We also explore how to prepare data needed for RSFT. RPO optimizes LLMs to enhance their preference for harmful content, reversing the models{'} safety alignment. Our extensive experiments reveal that open-access high-performance LLMs can be adeptly reverse-aligned to output harmful content, even in the absence of manually curated malicious datasets. Our research acts as a whistleblower for the community, emphasizing the need to pay more attention to safety of open-accessing LLMs. It also underscores the limitations of current safety alignment approaches and calls for research on robust safety alignment methods to counteract malicious fine-tuning attacks.",
}

@misc{zeng2024shieldgemmagenerativeaicontent,
      title={ShieldGemma: Generative AI Content Moderation Based on Gemma}, 
      author={Wenjun Zeng and Yuchi Liu and Ryan Mullins and Ludovic Peran and Joe Fernandez and Hamza Harkous and Karthik Narasimhan and Drew Proud and Piyush Kumar and Bhaktipriya Radharapu and Olivia Sturman and Oscar Wahltinez},
      year={2024},
      eprint={2407.21772},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.21772}, 
}

@misc{zheng2024lightweightsafetyguardrailsusing,
      title={Lightweight Safety Guardrails Using Fine-tuned BERT Embeddings}, 
      author={Aaron Zheng and Mansi Rana and Andreas Stolcke},
      year={2024},
      eprint={2411.14398},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.14398}, 
}

