\section{LLM-Augmented Graph}
% GNNs can achieve accurate recommendations by capturing higher-order structural information. Additionally, by integrating textual information into collaborative signals with a graph-centric approach supported by LLM, the diversity of recommendations will be significantly improved.
Shifting the focus to the graph, the core idea of the LLM-augmented graph methods is to augment the data within graphs using LLM, thereby improving the effectiveness of various GNNs employed for recommendation tasks. Such methods can be categorized into \textbf{topology augmentation} and \textbf{feature augmentation} (as illustrated in Figure~\ref{fig:LLM-augmented graph}), based on the aspects of information enhanced in the text-attribute graph according to LLMs.
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/LLM-Augmented.pdf}
    \caption{The illustration of LLM-augmented graph methods:  
    a) \textbf{Topology Augmentation}, where LLMs extract structural information from data to alter and augment the topological structure of the graphs.;  
    b) \textbf{Feature Augmentation}, where LLMs processe the textual information in the data, augment the node text or embedding in the graph without changing the topological structure.}
    \label{fig:LLM-augmented graph}
\end{figure}
\subsection{Topology Augmentation}
Topology augmentation refers to the processes where the LLM restructures data, utilizing its world knowledge and contextual understanding capabilities to convert specific textual information into a structured format. Due to the introduction of new structural information, the topological structure of the graph constructed from the data is modified, thereby affecting the subsequent processes to achieve more accurate recommendations.
% The graph is composed of two fundamental elements: nodes and edges. Therefore, 
Intuitively, we categorize topology augmentation into two types: \textbf{edge-level expansion} and \textbf{node-level expansion}, based on whether new nodes are introduced.
% Throughout the process of graph structure expansion, the extensive global knowledge of LLM is utilized in the form of text or embeddings.
% Leveraging the extensive global knowledge and robust contextual comprehension abilities of LLMs, explicit or implicit expansion can be made to enrich the graph structural information present in the data.
% The terms explicit and implicit refer to whether the world knowledge of LLM is utilized to augment the graph structure in the form of text or in the form of embeddings. 

\paragraph{Edge-level Expansion.}
This method refers to the process where LLMs introduce new relationships between nodes in the data, such as complementary and substitutable relationships, which are two primary types of relationships of interest in RS.

To directly utilize the versatile capabilities of LLMs and the vast world knowledge, it is intuitive to adopt text-centric approaches for adding edges in the graph. These approaches typically rely on prior knowledge to guide LLMs in making relationship judgments and constructions, either at a superficial or deeper level. As a straightforward example, LLM-KERec~\cite{zhao2024breaking} employs LLMs to assess the complementarity between pairs of items, thereby establishing complementary relationships among item pairs and constructing a complementary item graph. From a deeper perspective, in addition to leveraging complementary relationships between items, subjective user-generated reviews can also be utilized. SAGCN~\cite{liu2023understanding} and FineRec~\cite{zhang2024finerec} utilize LLMs to extract user opinions on items at varying levels of granularity across multiple item attributes (\textit{e.g.}, price, comfort, \textit{etc.}), using this information as edges to construct distinct graphs for each attribute.
% Similarly, LLMRG~\cite{wang2023enhancing} directly provides user interaction history and user attributes to the LLM, enabling it to generate item reasoning chains and subsequently construct an item reasoning graph.

Compared to the aforementioned methods, the more refined expansion delves into the relationships within the embedding space, subsequently constructing graphs based on implicit relationships between nodes at a certain level. For example, CSRec~\cite{yang2024common} first employs an LLM to generate complementary or substitutable category nodes based on existing classified nodes, and then utilizes other pre-trained language models to map the pairs of newly generated nodes and existing nodes into the node set within the semantic space. The relationships generated by the LLM are also mapped into the node set within the embedding space. In addition to mapping edges based on semantic similarity, connections can also be established based on the similarity. Several works ~\cite{yang2024sequential,cui2024comprehending} employ an LLM to encode the textual information of nodes into embeddings, and then measure potential relationships between these embeddings through carefully designed methods. These potential relationships serve as edges for item nodes in the graph within the embedding space.

\paragraph{Node-level Expansion.} 
This method further leverages the world knowledge and contextual reasoning capabilities of LLMs, utilizing auxiliary information as new nodes to augment the information of existing nodes. Such method often directly utilizes the condensed information generated by LLM as new nodes to be introduced into the graph, or extracts the requisite information from the LLMâ€™s output through particular approaches to serve as new nodes in the graph.
% Formalization is as follows:
% \begin{equation}
% \Delta V,\Delta E = P_\text{process}(\text{LLM}(G)) 
% \end{equation}
% The formula here is similar to the mentioned earlier, with the only difference being that in addition to expanding the edge set $E$, the node set $V$ is also expanded.

Several works~\cite{jeon2024topic,hu2024bridging} directly employ LLMs to generate auxiliary information nodes (\textit{e.g.}, user interests, item categories) for corresponding users or items based on existing textual information. Introducing  such auxiliary information nodes can be viewed as a supplementation of information in the textual space. This approach can, to some extent, assist subsequent GNN in modeling better user or item representations. Furthermore, this type of information supplementation can also be performed in the embedding space. For example, AutoGraph~\cite{shan2024automatic} utilizes LLMs to encode the textual information of users and items, followed by quantizing the semantic embeddings of users and items. By quantizing these semantic embeddings, fine-grained auxiliary information embeddings for users and items can be derived, which are then used as new nodes to supplement information for users and items.

Topology augmentation represents effectively utilizing the knowledge obtained from LLM pre-training to influence the training of the GNN. The key structural information introduced into the graph during this process enables subsequent GNN to learn more comprehensive representations of users and items.

\subsection{Feature Augmentation}
Enhancing the topological structure of the graph using LLMs may introduce biases, as they are not particularly adept at extracting structural information from text. In contrast, directly improving the node features in the graph without altering the topological structure is an task where LLMs truly excel.
This method focuses on leveraging the natural language processing capabilities of LLMs to augment data at the textual or embedding level, thereby influencing subsequent recommendations.
% This type of method can be formally described as:
% \begin{multline}
% C' = \text{LLM}(C) \quad \text{or} \quad 
% \mathbf{e}_\text{LLM} = E(\text{LLM}(C))
% \end{multline}
% where $C$ represents the original text information of the nodes, $C'$ represents the enhanced text information, $\mathbf{e}_\text{LLM}$ denotes the encoded semantic embeddings, LLM stands for the enhancement of the text, and $E$ represents an encoder (which can also be an LLM).

Some works~\cite{li2024learning,chen2024prompting} enhance the textual information of nodes by constructing appropriate prompts for input into LLMs. The enhanced textual information is subsequently encoded into embeddings by language models such as BERT. Unlike the aforementioned methods, GaCLLM~\cite{du2024large}  integrates the stages of LLM and GNN. In this approach, the LLM assumes the role responsible for message passing within the GNN, performing textual message passing and aggregation for each node in the graph, which is subsequently encoded by BERT. As a special case, LIKR~\cite{sakurai2024llm} employs LLMs to analyze user interaction histories to derive user-preferred item attributes. The nodes corresponding to these attributes in the graph serve as rewards for Markov walk-based reinforcement learning within the graph.

The textual information of users and items, being one of the abundant types of information in RS, significantly impacts the performance of RS when effective utilized. Consequently, LLM is increasingly becoming the preferred technology for feature augmentation of graphs based on textual information.

\subsection{Discussion}

The LLM-augmented graph methods, by incorporating the world knowledge of LLM into graph data, can enhance the capabilities of RS at the data level. This makes these methods more competitive in cold start scenarios with sparse interactions. Furthermore, the LLM in these methods is a plug-and-play component, allowing for flexible choices. Moreover, the data processing of the LLM can be performed offline in advance, and online recommendations based on statistical rules or neural models (\textit{e.g.}, GNN) can be made afterwards, significantly reducing time consumption. This has led to the increasing popularity of these methods in industry. However, these methods also have some drawbacks. First, graph learning methods do not fully exploit the world knowledge learned and utilized by LLM, which is a natural shortcoming of such plug-and-play methods. And LLMs have the potential to introduce extraneous noise in both dimensions (topology and feature) of topology augmentation, which may consequently give rise to certain biases. Furthermore, they have poor scalability, as the main body of these methods are shallow GNNs whose model depth is affected by the over-smoothing problem.
