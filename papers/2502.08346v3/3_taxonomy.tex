\section{Preliminaries}
In this section, we first introduce the basic concepts of GNN-based RS and LLM-based RS, then we provide the definition of graph foundation models, and finally we introduce the proposed taxonomy.
% \input{figures/taxonomy}

\subsection{GNN/LLM-based Recommender Systems}
As data grows explosively, recommender systems have emerged~\cite{gao2023survey}. There are generally three types of data in RS: user data, item data, and user-item interaction data. These data not only contain strong structural information that requires graph representation, but also are rich in textual descriptions. Given the characteristics of the data in RS, they can essentially be abstracted into text-attribute graphs~\cite{jin2024large}.
As two recently popular approaches, GNN-based RS and LLM-based RS both exhibit certain limitations in capitalizing on the information presented in the graphs.
GNN-based RS excel at capturing complex higher-order relationships between nodes and model user preferences based on multi-hop neighbors, thus providing accurate recommendations~\cite{wu2020comprehensive,wang2021graph}. However, the sequential order and semantic meaning of words within node descriptions pose a challenge for representation with graph structures, rendering these methods less effective at handling textual information.
Conversely, LLM-based RS, with their robust contextual understanding and world knowledge, excel at processing textual descriptions~\cite{yang2023palr,zhai2024actions}. However, limitations in their sequential modeling designs and reasoning capabilities cause these methods to struggle with managing complex relationships.
\subsection{Graph Foundation Models}
% \paragraph{Definitions.}
The field of NLP has witnessed a revolution influenced by the Transformer architecture. Pre-trained language models based on the architecture have demonstrated formidable capabilities~\cite{radford2019language,kenton2019bert}. These language foundation models, pre-trained on vast amounts of text, possess impressive generalization abilities that can adapt to a wide array of tasks~\cite{bommasani2021opportunities}. When the scale of the language foundation model reaches a certain magnitude, it is referred to as the LLM~\cite{zhao2023survey}.
Inspired by the success of language foundation models in the NLP field, the field of graph learning also recognized the need to improve model performance and generalization capabilities through pre-training. This gave rise to the concept of graph foundation models~\cite{liu2023towards}, which are models that are pre-trained on large datasets and incorporate graph structures to solve graph-related tasks.
As combinations of LLM and graphs, GFMs acquire emergence and homogenization during pre-training~\cite{liu2023towards}, enabling them to adapt seamlessly and perform impressively across a variety of downstream tasks.

\subsection{Taxonomy of GFM-based RS}
Gravitating towards the contextual backdrop of RS, we place substantial emphasis on examining the organic integration of graph with LLM in GFM, striving for more precise and user-specific recommendations. In accordance with the interrelationship between graphs and LLMs (as illustrated in Figure~\ref{fig:taxonomy}) we group the related works into three principal categories:
\textbf{Graph-augmented LLM}, where the structural information from graphs is injected into LLMs to enhance the reasoning and generation capabilities for recommendation; 
\textbf{LLM-augmented graph}, where the structural information (e.g., topological structure) or textual information (e.g., user profiles and item descriptions) in the graph is enhanced with the aid of LLMs; 
\textbf{LLM-graph harmonization}, where the semantic embedding in LLMs and the structural embedding in graphs are combined seamlessly to achieve mutual optimization and maximize recommendation performance. 

In the following sections, we provide a comprehensive introduction and discussion of the three main categories in the taxonomy of GFM-based RS.

