@misc{austin2021program,
      title={Program Synthesis with Large Language Models}, 
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{aghajanyan2023scaling,
      title={Scaling Laws for Generative Mixed-Modal Language Models}, 
      author={Armen Aghajanyan and Lili Yu and Alexis Conneau and Wei-Ning Hsu and Karen Hambardzumyan and Susan Zhang and Stephen Roller and Naman Goyal and Omer Levy and Luke Zettlemoyer},
      year={2023},
      eprint={2301.03728},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{vaswani2023attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{wang2021codet5,
      title={CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation}, 
      author={Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
      year={2021},
      eprint={2109.00859},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{raffel2023exploring,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2023},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{feng2020codebert,
      title={CodeBERT: A Pre-Trained Model for Programming and Natural Languages}, 
      author={Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and Daxin Jiang and Ming Zhou},
      year={2020},
      eprint={2002.08155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rozière2023code,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2023},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{li2023starcoder,
      title={StarCoder: may the source be with you!}, 
      author={Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and João Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Muñoz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
      year={2023},
      eprint={2305.06161},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{luo2023wizardcoder,
      title={WizardCoder: Empowering Code Large Language Models with Evol-Instruct}, 
      author={Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
      year={2023},
      eprint={2306.08568},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{brown2020language,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{denny2022conversing,
      title={Conversing with Copilot: Exploring Prompt Engineering for Solving CS1 Problems Using Natural Language}, 
      author={Paul Denny and Viraj Kumar and Nasser Giacaman},
      year={2022},
      eprint={2210.15157},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{liu2023code,
      title={Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation}, 
      author={Jiawei Liu and Chunqiu Steven Xia and Yuyao Wang and Lingming Zhang},
      year={2023},
      eprint={2305.01210},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{zheng2023outline,
      title={Outline, Then Details: Syntactically Guided Coarse-To-Fine Code Generation}, 
      author={Wenqing Zheng and S P Sharan and Ajay Kumar Jaiswal and Kevin Wang and Yihan Xi and Dejia Xu and Zhangyang Wang},
      year={2023},
      eprint={2305.00909},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}

@misc{wei2023chainofthought,
      title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}, 
      author={Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed Chi and Quoc Le and Denny Zhou},
      year={2023},
      eprint={2201.11903},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wen2023hard,
      title={Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery}, 
      author={Yuxin Wen and Neel Jain and John Kirchenbauer and Micah Goldblum and Jonas Geiping and Tom Goldstein},
      year={2023},
      eprint={2302.03668},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{shin2020autoprompt,
      title={AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts}, 
      author={Taylor Shin and Yasaman Razeghi and Robert L. Logan IV au2 and Eric Wallace and Sameer Singh},
      year={2020},
      eprint={2010.15980},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{delta_debugging,
author = {Zeller, Andreas and Hildebrandt, Ralf},
title = {Simplifying and Isolating Failure-Inducing Input},
year = {2002},
issue_date = {February 2002},
publisher = {IEEE Press},
volume = {28},
number = {2},
issn = {0098-5589},
url = {https://doi.org/10.1109/32.988498},
doi = {10.1109/32.988498},
abstract = {Given some test case, a program fails. Which circumstances of the test case are responsible for the particular failure? The Delta Debugging algorithm generalizes and simplifies the failing test case to a minimal test case that still produces the failure. It also isolates the difference between a passing and a failing test case. In a case study, the Mozilla web browser crashed after 95 user actions. Our prototype implementation automatically simplified the input to three relevant user actions. Likewise, it simplified 896 lines of HTML to the single line that caused the failure. The case study required 139 automated test runs or 35 minutes on a 500 MHz PC.},
journal = {IEEE Trans. Softw. Eng.},
month = {feb},
pages = {183–200},
numpages = {18},
keywords = {testing tools, combinatorial testing, tracing., debugging aids, diagnostics, Automated debugging}
}

@inproceedings{evalplus,
  title = {Is Your Code Generated by Chat{GPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  booktitle = {Thirty-seventh Conference on Neural Information Processing Systems},
  year = {2023},
  url = {https://openreview.net/forum?id=1qvx610Cu7},
}


@article{humaneval,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year={2021},
  eprint={2107.03374},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}

@article{Lai2022DS1000,
  title={DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
  author={Yuhang Lai and Chengxi Li and Yiming Wang and Tianyi Zhang and Ruiqi Zhong and Luke Zettlemoyer and Scott Wen-tau Yih and Daniel Fried and Sida Wang and Tao Yu},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.11501}
}

@article{arxiv2023programmersDebug,
  title={What we can learn from how programmers debug their code},
  author={Thomas Hirsch, Birgit Hofer},
  journal={arXiv},
  year={2023},
  volume={abs/2103.12447}
}

@article{arxiv2023ExplainableDebugging,
  title={Explainable Automated Debugging via Large Language Model-driven Scientific Debugging},
  author={Sungmin Kang, Bei Chen, Shin Yoo, Jian-Guang Lou},
  journal={arXiv},
  year={2023},
  volume={abs/2304.02195}
}

@article{arxiv2023GitHubCopilot,
  title={Practices and Challenges of Using GitHub Copilot: An Empirical Study},
  author={Beiqi Zhang, Peng Liang, Xiyu Zhou, Aakash Ahmad, Muhammad Waseem},
  journal={arXiv},
  year={2023},
  volume={abs/2303.08733}
}

@article{arxiv2023ChatGPTBugFixing,
  title={An Analysis of the Automatic Bug Fixing Performance of ChatGPT},
  author={Dominik Sobania, Martin Briesch, Carol Hanna, Justyna Petke},
  journal={arXiv},
  year={2023},
  volume={abs/2301.08653}
}

@article{RetrievalAugmented2022,
  title={Retrieval-Augmented Multimodal Language Modeling},
  author={Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih},
  journal={arXiv},
  year={2023},
  volume={abs/2211.12561}
}

@article{Opal2022,
  title={Opal: Multimodal Image Generation for News Illustration},
  author={Vivian Liu, Han Qiao, Lydia Chilton},
  journal={arXiv},
  year={2023},
  volume={abs/2204.09007}
}

@article{FoundationModels2022,
  title={Multimodal Foundation Models: From Specialists to General-Purpose Assistants},
  author={Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, Jianfeng Gao},
  journal={arXiv},
  year={2023},
  volume={abs/2309.10020}
}

@article{ImageSynthesis2022,
  title={Multimodal Image Synthesis and Editing: A Survey},
  author={Fangneng Zhan, Yingchen Yu, Rongliang Wu, Jiahui Zhang, Shijian Lu, Lingjie Liu, Adam Kortylewski, Christian Theobalt, Eric Xing},
  journal={arXiv},
  year={2023},
  volume={abs/2112.13592}
}


@misc{sclar2023quantifying,
      title={Quantifying Language Models' Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting}, 
      author={Melanie Sclar and Yejin Choi and Yulia Tsvetkov and Alane Suhr},
      year={2023},
      eprint={2310.11324},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}








%%%%%%%%%%%%%%%%%%



@misc{touvron2023llama2,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xu2023wizardlm,
      title={WizardLM: Empowering Large Language Models to Follow Complex Instructions}, 
      author={Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
      year={2023},
      eprint={2304.12244},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lu2021codexglue,
      title={CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation}, 
      author={Shuai Lu and Daya Guo and Shuo Ren and Junjie Huang and Alexey Svyatkovskiy and Ambrosio Blanco and Colin Clement and Dawn Drain and Daxin Jiang and Duyu Tang and Ge Li and Lidong Zhou and Linjun Shou and Long Zhou and Michele Tufano and Ming Gong and Ming Zhou and Nan Duan and Neel Sundaresan and Shao Kun Deng and Shengyu Fu and Shujie Liu},
      year={2021},
      eprint={2102.04664},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}


@misc{bavarian2022efficient,
      title={Efficient Training of Language Models to Fill in the Middle}, 
      author={Mohammad Bavarian and Heewoo Jun and Nikolas Tezak and John Schulman and Christine McLeavey and Jerry Tworek and Mark Chen},
      year={2022},
      eprint={2207.14255},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gpt4t,
  title = {GPT-4 Turbo},
  author = {OpenAI},
  howpublished = "\url{https://help.openai.com/en/articles/8555510-gpt-4-turbo}",
  year = {2024}, 
}

@misc{gemini_pro,
  title = {Gemini Pro},
  author = {Google},
  howpublished = "\url{https://deepmind.google/technologies/gemini/#capabilities}",
  year = {2023}, 
}


@inproceedings{cai2020isotropy,
  title={Isotropy in the contextual embedding space: Clusters and manifolds},
  author={Cai, Xingyu and Huang, Jiaji and Bian, Yuchen and Church, Kenneth},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{rajaee2021isotropy,
  title={An isotropy analysis in the multilingual BERT embedding space},
  author={Rajaee, Sara and Pilehvar, Mohammad Taher},
  journal={arXiv preprint arXiv:2110.04504},
  year={2021}
}


@misc{white2023prompt,
      title={A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT}, 
      author={Jules White and Quchen Fu and Sam Hays and Michael Sandborn and Carlos Olea and Henry Gilbert and Ashraf Elnashar and Jesse Spencer-Smith and Douglas C. Schmidt},
      year={2023},
      eprint={2302.11382},
      archivePrefix={arXiv},
      primaryClass={cs.SE}
}

@misc{lester2021power,
      title={The Power of Scale for Parameter-Efficient Prompt Tuning}, 
      author={Brian Lester and Rami Al-Rfou and Noah Constant},
      year={2021},
      eprint={2104.08691},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{liu2021pretrain,
      title={Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing}, 
      author={Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
      year={2021},
      eprint={2107.13586},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{maus2023black,
      title={Black Box Adversarial Prompting for Foundation Models}, 
      author={Natalie Maus and Patrick Chao and Eric Wong and Jacob Gardner},
      year={2023},
      eprint={2302.04237},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chao2023jailbreaking,
      title={Jailbreaking Black Box Large Language Models in Twenty Queries}, 
      author={Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong},
      year={2023},
      eprint={2310.08419},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{zhu2023promptbench,
      title={PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts}, 
      author={Kaijie Zhu and Jindong Wang and Jiaheng Zhou and Zichen Wang and Hao Chen and Yidong Wang and Linyi Yang and Wei Ye and Yue Zhang and Neil Zhenqiang Gong and Xing Xie},
      year={2023},
      eprint={2306.04528},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{lu2022fantastically,
      title={Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity}, 
      author={Yao Lu and Max Bartolo and Alastair Moore and Sebastian Riedel and Pontus Stenetorp},
      year={2022},
      eprint={2104.08786},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{liu2023improved,
      title={Improved Baselines with Visual Instruction Tuning}, 
      author={Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
      year={2023},
      eprint={2310.03744},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{he2023align,
  title={Align and attend: Multimodal summarization with dual contrastive losses},
  author={He, Bo and Wang, Jun and Qiu, Jielin and Bui, Trung and Shrivastava, Abhinav and Wang, Zhaowen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14867--14878},
  year={2023}
}

@misc{ouyang2022training,
      title={Training language models to follow instructions with human feedback}, 
      author={Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Ray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe},
      year={2022},
      eprint={2203.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@Inproceedings{redo_Wang2022,
 author = {Shiqi Wang and Zheng Li and Haifeng Qian and Chenghao Yang and Zijian Wang and Mingyue Shang and Varun Kumar and Samson Tan and Baishakhi Ray and Parminder Bhatia and Ramesh Nallapati and Murali Krishna Ramanathan and Dan Roth and Bing Xiang},
 title = {ReCode: Robustness evaluation of code generation models},
 year = {2022},
booktitle = {ACL 2023},
}


@inproceedings{dual_channels,
author = {Casalnuovo, Casey and Barr, Earl T. and Dash, Santanu Kumar and Devanbu, Prem and Morgan, Emily},
title = {A theory of dual channel constraints},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381720},
doi = {10.1145/3377816.3381720},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {25–28},
numpages = {4},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}


@article{hendrycksapps2021,
  title={Measuring Coding Challenge Competence With APPS},
  author={Dan Hendrycks and Steven Basart and Saurav Kadavath and Mantas Mazeika and Akul Arora and Ethan Guo and Collin Burns and Samir Puranik and Horace He and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}


@article{li2022competition,
  title={Competition-Level Code Generation with AlphaCode},
    author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and
    Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and
    Keeling, James and Gimeno, Felix and Dal Lago, Agustin and
    Hubert, Thomas and Choy, Peter and de Masson d'Autume, Cyprien and
    Babuschkin, Igor and Chen, Xinyun and Huang, Po-Sen and Welbl, Johannes and
    Gowal, Sven and Cherepanov, Alexey and Molloy, James and
    Mankowitz, Daniel and Sutherland Robson, Esme and Kohli, Pushmeet and
    de Freitas, Nando and Kavukcuoglu, Koray and Vinyals, Oriol},
  journal={arXiv preprint arXiv:2203.07814},
  year={2022}
}


@misc{desai2015program,
      title={Program Synthesis using Natural Language}, 
      author={Aditya Desai and Sumit Gulwani and Vineet Hingorani and Nidhi Jain and Amey Karkare and Mark Marron and Sailesh R and Subhajit Roy},
      year={2015},
      eprint={1509.00413},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}


@article{gulwani2017program,
  title={Program synthesis},
  author={Gulwani, Sumit and Polozov, Oleksandr and Singh, Rishabh and others},
  journal={Foundations and Trends{\textregistered} in Programming Languages},
  volume={4},
  number={1-2},
  pages={1--119},
  year={2017},
  publisher={Now Publishers, Inc.}
}


@book{pearl2009causality,
  title={Causality: Models, Reasoning, and Inference},
  author={Pearl, Judea},
  year={2009},
  publisher={Cambridge University Press}
}

@book{koller2009probabilistic,
  title={Probabilistic Graphical Models: Principles and Techniques},
  author={Koller, Daphne and Friedman, Nir},
  year={2009},
  publisher={MIT Press}
}

@book{pearl2000causality,
  title={Causality: Models, Reasoning, and Inference},
  author={Pearl, Judea},
  year={2000},
  publisher={Cambridge University Press}
}

@article{bollen1989structural,
  title={Structural equations with latent variables},
  author={Bollen, Kenneth A.},
  journal={Wiley Series in Probability and Mathematical Statistics},
  year={1989},
  publisher={John Wiley & Sons}
}

@article{vanderweele2015explanation,
  title={Explanation in Causal Inference: Methods for Mediation and Interaction},
  author={VanderWeele, Tyler J.},
  journal={Oxford University Press},
  year={2015}
}


@book{spirtes2000causation,
  title={Causation, Prediction, and Search},
  author={Spirtes, Peter and Glymour, Clark N. and Scheines, Richard},
  year={2000},
  publisher={MIT Press}
}

@inproceedings{codereval,
author = {Yu, Hao and Shen, Bo and Ran, Dezhi and Zhang, Jiaxin and Zhang, Qi and Ma, Yuchi and Liang, Guangtai and Li, Ying and Wang, Qianxiang and Xie, Tao},
title = {CoderEval: A Benchmark of Pragmatic Code Generation with Generative Pre-trained Models},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623316},
doi = {10.1145/3597503.3623316},
abstract = {Code generation models based on the pre-training and fine-tuning paradigm have been increasingly attempted by both academia and industry, resulting in well-known industrial models such as Codex, CodeGen, and PanGu-Coder. To evaluate the effectiveness of these models, multiple existing benchmarks (e.g., HumanEval and AiXBench) are proposed, including only cases of generating a standalone function, i.e., a function that may invoke or access only built-in functions and standard libraries. However, non-standalone functions, which typically are not included in the existing benchmarks, constitute more than 70\% of the functions in popular open-source projects, and evaluating models' effectiveness on standalone functions cannot reflect these models' effectiveness on pragmatic code generation scenarios (i.e., code generation for real settings of open source or proprietary code).To help bridge the preceding gap, in this paper, we propose a benchmark named CoderEval, consisting of 230 Python and 230 Java code generation tasks carefully curated from popular real-world open-source projects and a self-contained execution platform to automatically assess the functional correctness of generated code. CoderEval supports code generation tasks from six levels of context dependency, where context refers to code elements such as types, APIs, variables, and consts defined outside the function under generation but within the dependent third-party libraries, current class, file, or project. CoderEval can be used to evaluate the effectiveness of models in generating code beyond only standalone functions. By evaluating three state-of-the-art code generation models (CodeGen, PanGu-Coder, and ChatGPT) on CoderEval and HumanEval, we find that the effectiveness of these models in generating standalone functions is substantially higher than that in generating non-standalone functions. Our analysis highlights the current progress and pinpoints future directions to further improve a model's effectiveness by leveraging contextual information for pragmatic code generation.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {37},
numpages = {12},
keywords = {code generation, large language models, benchmark},
location = {Lisbon, Portugal},
series = {ICSE '24}
}




@book{goodfellow2016deep,
  title={Deep Learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT Press}
}

@article{bengio2013representation,
  title={Representation Learning: A Review and New Perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@book{rubin2005causal,
  title={Causal Inference Using Potential Outcomes: Design, Modeling, Decisions},
  author={Rubin, Donald B.},
  year={2005},
  publisher={John Wiley \& Sons}
}


@book{imbens2015causal,
  title={Causal Inference for Statistics, Social, and Biomedical Sciences},
  author={Imbens, Guido W. and Rubin, Donald B.},
  year={2015},
  publisher={Cambridge University Press}
}

@book{pearl2018book,
  title={The Book of Why: The New Science of Cause and Effect},
  author={Pearl, Judea and Mackenzie, Dana},
  year={2018},
  publisher={Basic Books}
}


@inproceedings{jin-etal-2022-causalnlp,
    title = "{C}ausal{NLP} Tutorial: An Introduction to Causality for Natural Language Processing",
    author = "Jin, Zhijing  and
      Feder, Amir  and
      Zhang, Kun",
    editor = "El-Beltagy, Samhaa R.  and
      Qiu, Xipeng",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts",
    month = dec,
    year = "2022",
    address = "Abu Dubai, UAE",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-tutorials.4",
    doi = "10.18653/v1/2022.emnlp-tutorials.4",
    pages = "17--22",
}

@misc{feder2022causal,
      title={Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond}, 
      author={Amir Feder and Katherine A. Keith and Emaad Manzoor and Reid Pryzant and Dhanya Sridhar and Zach Wood-Doughty and Jacob Eisenstein and Justin Grimmer and Roi Reichart and Margaret E. Roberts and Brandon M. Stewart and Victor Veitch and Diyi Yang},
      year={2022},
      eprint={2109.00725},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vig2020investigating,
  title={Investigating gender bias in language models using causal mediation analysis},
  author={Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12388--12401},
  year={2020}
}

@article{finlayson2021causal,
  title={Causal analysis of syntactic agreement mechanisms in neural language models},
  author={Finlayson, Matthew and Mueller, Aaron and Gehrmann, Sebastian and Shieber, Stuart and Linzen, Tal and Belinkov, Yonatan},
  journal={arXiv preprint arXiv:2106.06087},
  year={2021}
}


@article{robins1992identifiability,
  title={Identifiability and exchangeability for direct and indirect effects},
  author={Robins, James M and Greenland, Sander},
  journal={Epidemiology},
  volume={3},
  number={2},
  pages={143--155},
  year={1992},
  publisher={LWW}
}

@incollection{pearl2022direct,
  title={Direct and indirect effects},
  author={Pearl, Judea},
  booktitle={Probabilistic and causal inference: the works of Judea Pearl},
  pages={373--392},
  year={2022}
}

@article{robins2003semantics,
  title={Semantics of causal DAG models and the identification of direct and indirect effects},
  author={Robins, James M},
  journal={Highly structured stochastic systems},
  pages={70--82},
  year={2003},
  publisher={Oxford University PressOxford}
}

@article{stolfo2022causal,
  title={A causal framework to quantify the robustness of mathematical reasoning with language models},
  author={Stolfo, Alessandro and Jin, Zhijing and Shridhar, Kumar and Sch{\"o}lkopf, Bernhard and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2210.12023},
  year={2022}
}

@article{pearl2000models,
  title={Models, reasoning and inference},
  author={Pearl, Judea and others},
  journal={Cambridge, UK: CambridgeUniversityPress},
  volume={19},
  number={2},
  pages={3},
  year={2000}
}


@article{avin2005identifiability,
  title={Identifiability of path-specific effects},
  author={Avin, Chen and Shpitser, Ilya and Pearl, Judea},
  year={2005}
}

@misc{wu2019pcfairness,
      title={PC-Fairness: A Unified Framework for Measuring Causality-based Fairness}, 
      author={Yongkai Wu and Lu Zhang and Xintao Wu and Hanghang Tong},
      year={2019},
      eprint={1910.12586},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}



@article{llama3,
  title={Llama 3 Model Card},
  author={AI@Meta},
  year={2024},
  url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md}
}


@misc{wang2022recode,
      title={ReCode: Robustness Evaluation of Code Generation Models}, 
      author={Shiqi Wang and Zheng Li and Haifeng Qian and Chenghao Yang and Zijian Wang and Mingyue Shang and Varun Kumar and Samson Tan and Baishakhi Ray and Parminder Bhatia and Ramesh Nallapati and Murali Krishna Ramanathan and Dan Roth and Bing Xiang},
      year={2022},
      eprint={2212.10264},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{rahman2024towards,
  title={Towards causal deep learning for vulnerability detection},
  author={Rahman, Md Mahbubur and Ceka, Ira and Mao, Chengzhi and Chakraborty, Saikat and Ray, Baishakhi and Le, Wei},
  booktitle={Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
  pages={1--11},
  year={2024}
}


@misc{nijkamp2023codegen,
      title={CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis}, 
      author={Erik Nijkamp and Bo Pang and Hiroaki Hayashi and Lifu Tu and Huan Wang and Yingbo Zhou and Silvio Savarese and Caiming Xiong},
      year={2023},
      eprint={2203.13474},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}


@misc{hooda2024largecodemodelsunderstand,
      title={Do Large Code Models Understand Programming Concepts? A Black-box Approach}, 
      author={Ashish Hooda and Mihai Christodorescu and Miltiadis Allamanis and Aaron Wilson and Kassem Fawaz and Somesh Jha},
      year={2024},
      eprint={2402.05980},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2402.05980}, 
}


@INPROCEEDINGS{9793858,
  author={He, Jingzhu and Lin, Yuhang and Gu, Xiaohui and Yeh, Chin-Chia Michael and Zhuang, Zhongfang},
  booktitle={2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)}, 
  title={PerfSig: Extracting Performance Bug Signatures via Multi-modality Causal Analysis}, 
  year={2022},
  volume={},
  number={},
  pages={1669-1680},
  keywords={Measurement;Computer bugs;Prototypes;Production;Reliability engineering;Software reliability;System analysis and design;Debugging;Bug signatures;Software reliability;Performance},
  doi={10.1145/3510003.3510110}}

@misc{cito2021counterfactualexplanationsmodelscode,
      title={Counterfactual Explanations for Models of Code}, 
      author={Jürgen Cito and Isil Dillig and Vijayaraghavan Murali and Satish Chandra},
      year={2021},
      eprint={2111.05711},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2111.05711}, 
}

zhu2024deepseek
@article{zhu2024deepseek,
  title={DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence},
  author={Zhu, Qihao and Guo, Daya and Shao, Zhihong and Yang, Dejian and Wang, Peiyi and Xu, Runxin and Wu, Y and Li, Yukun and Gao, Huazuo and Ma, Shirong and others},
  journal={arXiv preprint arXiv:2406.11931},
  year={2024}
}

@misc{yao2023treethought,
      title={Tree of Thoughts: Deliberate Problem Solving with Large Language Models}, 
      author={Shunyu Yao and Dian Yu and Jeffrey Zhao and Izhak Shafran and Thomas L. Griffiths and Yuan Cao and Karthik Narasimhan},
      year={2023},
      eprint={2305.10601},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2305.10601}, 
}



@misc{mullick2024intent,
      title={Intent Detection and Entity Extraction from BioMedical Literature}, 
      author={Ankan Mullick and Mukur Gupta and Pawan Goyal},
      year={2024},
      eprint={2404.03598},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.03598}, 
}

@inproceedings{wu2023self,
  title={Self-prompting large vision models for few-shot medical image segmentation},
  author={Wu, Qi and Zhang, Yuyao and Elbatel, Marawan},
  booktitle={MICCAI workshop on domain adaptation and representation transfer},
  pages={156--167},
  year={2023},
  organization={Springer}
}
