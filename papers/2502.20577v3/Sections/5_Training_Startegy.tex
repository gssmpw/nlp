\subsection{Training Strategy}
The training process is divided into two stages. In the first stage, our model learns conditional attributes and general facial features. We initialize training using pre-trained weights from Stable Diffusion (SD) for both the Guidance Network and the Main Diffusion Network. The 3D pixel-aligned conditionals, generated from reference input images using the pre-trained DECA model, are processed by the 3D Fusion Controller before being passed to the Guidance Network. Corresponding features from the Guidance Network are integrated into the Diffusion Network before each self-attention layer. The reference input image is processed through a VAE, a face recognition model, and CLIP. The latents from the VAE go to the Main Diffusion Network, while the projected embeddings from the Identity Preserver Module, derived from CLIP and the face recognition model, are fed into the cross-attention mechanisms of both the Guidance Network and the Diffusion Network via the projection module. During this stage, the Guidance Network, Main Diffusion Network, and projection module are trainable, while the VAE, CLIP, and face recognition models are kept fixed. In the second stage, we fine-tune our model using only the inference image, which serves as the target image for inference. Only the projection module and the Diffusion Network are trainable in this stage, while all other components retain their learned weights. This approach enables the model to adapt to specific facial attributes, enhancing both accuracy and realism in the generated outputs.

