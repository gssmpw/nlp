

\section{Experiments}
\subsection{Implemetation }

In the first stage of our training, we utilize the FFHQ dataset ~\cite{karras2019style}, which contains 70,000 high-quality facial images. The images are first resized to 256x256 pixels to match the input requirements of the VAE encoder. After processing through the VAE, the images are converted into latent representations with a size of 32x32 and 4 channels. We conduct our experiments on 2 NVIDIA Quadro RTX 8000 GPUs, each with a batch size of 6, for a total of 55,000 steps. 
% This setup means the model has seen the images 660,000 times, which is almost 20 times fewer than the training iterations used in the DiffusionRig approach~\cite{ding2023diffusionrig}. 
The learning rate is set to 1e-5, and we use the AdamW optimizer during this stage. In the second stage, we fine-tune the model using a single inference image to retain the identity. We create copies of this image to form a batch size of 8 and trained the model for 50 steps, which has empirically provided the best results. During this phase, the learning rate remains at 1e-5, and we continue to use the AdamW optimizer. During inference, we can either specify FLAME parameters for DECA to generate the required conditional maps (first 3 columns of Figure~\ref{fig:teaser} or use another image, referred to as the target image from which DECA extracts these maps (last column of Figure~\ref{fig:teaser}). We then employ the DDIM ~\cite{song2020denoising} sampler with 20 denoising steps.

% and use a classifier-free guidance strength of 2. 

% \subsection{Qualitative evaluation}
% The model depicted in Figure~\ref{fig:teaser} efficiently preserves the individual's identity despite precisely altering facial features such as pose, expression, and lighting in the initial three columns. The last column demonstrates the successful transfer of all attributes, including pose, expression, and lighting, from one image to another.

% \subsection{Baseline Comparisons and Qualitative Evaluation}
\subsection{Comparisons}
To assess the efficacy of our approach, we perform comparisons with cutting-edge techniques such as HeadNerf\cite{hong2022headnerf}, GIF\cite{ghosh2020gif}, DiffusionRig\cite{ding2023diffusionrig}, CapHuman~\cite{liang2024caphuman}, and VOODOO3D~\cite{tran2024voodoo} as depicted in Figure~\ref{fig:comparison}. Our approach consistently surpasses these reference points in producing authentic facial photographs while preserving identity. GIF efficiently alters facial attributes but struggles with identity preservation. HeadNerf captures facial identification well but fails to maintain structural integrity when the face turns away from the frontal view. While DiffusionRig performs well, it produces artifacts when adjusting pose due to remnants of the original image. VOODOO3D tilts the entire image instead of following the driver's pose and cannot handle lighting edits. CapHuman struggles with retaining background accessories and hair and has issues with camera shifting. In this case, Our approach stands out because we utilize separate conditional inputs and keep the ControlNet fixed during fine-tuning. This results in the generation of authentic images that sustain both the distinguishing characteristics and the desired traits.



\textbf{Image Quality and Identity Evaluation.}
% To enable a quantitative comparison with DiffusionRig, we adopt the same experimental setup. we produce a set of 400 photos, each including a distinct pose and expression, to assess the effectiveness of our approach. 
To facilitate a quantitative comparison, we adopt the same experimental setup as DiffusionRig, generating a set of 400 images, each with a unique pose and expression
We evaluate the images using facial reidentification models(Re-ID) \cite{king2009dlib}, Perceptual Similarity (LPIPS) \cite{zhang2018unreasonable}, Frechet Inception Distance (FID) \cite{heusel2017gans}, and Structural Similarity Index (SSIM) \cite{wang2004image}. To provide a basis for comparison, we incorporate findings from DiffusionRig and CapHuman, which are, in this case, most relatable to our method. Nevertheless, our model consistently outperforms previous methods in almost all measurements, as demonstrated in Table~\ref{tab:quant_eval}.



% Quantitative evaluation table
\FloatBarrier
\begin{table*}[!htbp]
\centering
\caption{Quantitative evaluation for novel pose, expression, and lighting synthesis.}
\setlength{\tabcolsep}{3pt} % Adjust column spacing for better readability
\renewcommand{\arraystretch}{1.1} % Adjust row height for better readability
\tiny % Make the font size even smaller for the table content
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc|cccc|cccc}
    \toprule
    & \multicolumn{4}{c|}{Novel Pose Synthesis} & \multicolumn{4}{c|}{Novel Expression Synthesis} & \multicolumn{4}{c}{Novel Lighting Synthesis} \\
    \cmidrule(r){2-5} \cmidrule(r){6-9} \cmidrule(l){10-13}
    Method & LPIPS$\downarrow$ & SSIM$\uparrow$ & FID$\downarrow$ & Re-ID$\uparrow$ & LPIPS$\downarrow$ & SSIM$\uparrow$ & FID$\downarrow$ & Re-ID$\uparrow$ & LPIPS$\downarrow$ & SSIM$\uparrow$ & FID$\downarrow$ & Re-ID$\uparrow$ \\
    \midrule
    CapHuman & 0.5914 & 0.3968 & 205.25 & 95.63 & 0.4762 & 0.4487 & 194.00 & 95.57 & 0.4772 & 0.4431 & 186.85 & 95.86 \\
    DiffusionRig* & 0.4547 & 0.4222 & 73.16 & 96.72 & 0.1891 & 0.6625 & 68.20 & 97.27 & 0.2367 & 0.5788 & 69.28 & \textbf{97.27} \\
    Ours & \textbf{0.3747} & \textbf{0.6160} & \textbf{53.63} & \textbf{96.81} & \textbf{0.0931} & \textbf{0.8860} & \textbf{46.58} & \textbf{97.40} & \textbf{0.1436} & \textbf{0.8033} & \textbf{43.72} & 97.22 \\
    \bottomrule
\end{tabular}}
% \vspace{-0.3cm} % Reduce space after tables
\label{tab:quant_eval}
\end{table*}


% \begin{table}[h]
%     \centering
%     \caption{Caption}
%     \label{tab:my_label}

% \begin{subtable}
%     % \centering
%     \caption{Quantitative evaluation metrics for expression synthesis.}
%     \label{table:1a}
%     \begin{tabular}{lcccc}
%         \toprule
%         & LPIPS $\downarrow$ & SSIM $\uparrow$ & FID $\downarrow$ & Re-ID $\uparrow$ \\
%         \midrule
%         \\
%         DiffusionRig* & 0.4547 & 0.4222 & \textbf{73.155} & \textbf{96.72} \\
%         \\
%         \textbf{Ours} & \textbf{0.3747} & \textbf{0.5994} & 74.157 & 96.36 \\
%         \\
%         \bottomrule
%     \end{tabular}
% \end{subtable}

% \begin{subtable}
%     % \centering
%     \label{table:1b}
%     \begin{tabular}{lccccc}
%         \toprule
%         & LPIPS $\downarrow$ & SSIM $\uparrow$ & FID $\downarrow$ & Re-ID $\uparrow$ \\
%         \midrule
%         \\
%         DiffusionRig* & 0.4547 & 0.4222 & \textbf{73.155} & \textbf{96.55} \\
%         \\
%         \textbf{Ours} & \textbf{0.3870} & \textbf{0.5994} & 74.157 & 96.36 \\
%         \\
%         \bottomrule
%     \end{tabular}
%     % \caption{Novel expression synthesis. Quantitative evaluation for novel expression synthesis on the held-out "jaw\_right" expression. When evaluating against ground truth images with a white background, we convert the background pixels to white for all baselines using the ground truth masks. Our method outperforms the baselines on all metrics.}
%     % \caption{pose rigging}
%     \label{tab:novel_expression_synthesis}
% \end{subtable}
% \end{table}


% \begin{table}[ht]
%     \centering
%     \caption{Quantitative evaluation}
%     \label{tab:my_label}

%     \begin{subtable}[t]{\linewidth}
%         \centering
%         \caption{Novel pose synthesis}
%         \label{table:1a}
%         \begin{tabular}{lcccc}
%             \toprule
%             Method & LPIPS $\downarrow$ & SSIM $\uparrow$ & FID $\downarrow$ & Re-ID $\uparrow$ \\
%             \midrule
%             CapHuman & 0.5914 & 0.3968 & {205.249} & {95.63} \\
%             DiffusionRig* & 0.4547 & 0.4222 & {73.155} & {96.72} \\
%             \textbf{Ours} & \textbf{0.3747} & \textbf{0.6160} & \textbf{53.63} & \textbf{96.81} \\
%             \bottomrule
%         \end{tabular}
%     \end{subtable}

%     \vspace{0.5cm} % Adds some vertical space between the sub-tables

%     \begin{subtable}[t]{\linewidth}
%         \centering
%         \caption{Novel expression synthesis}
%         \label{table:1b}
%         \begin{tabular}{lcccc}
%             \toprule
%             Method & LPIPS $\downarrow$ & SSIM $\uparrow$ & FID $\downarrow$ & Re-ID $\uparrow$ \\
%             \midrule
%             CapHuman & 0.4762 & 0.4487 & {194.003} & {95.57} \\
%             DiffusionRig* & 0.1891 & 0.6625 & {68.202} & {97.27} \\
%             \textbf{Ours} & \textbf{0.0931} & \textbf{0.8860} & \textbf{46.579} & \textbf{97.40} \\
%             \bottomrule
%         \end{tabular}
%     \end{subtable}

% \end{table}








% may need to change the heading 
% should it be Training Strategy and Implementation or Implementation and Training Strategy, it is based on the things I write.


% Having set up our initial stage to incorporate 3DMM-generated conditionals, our next focus is on refining identity-specific attributes and overall consistency in the image synthesis process.
% In this branch of our model, we aim to enhance the model's ability to accurately capture and preserve the unique identity information and detailed features of the main image.


\textbf{Rigging Quality Evaluation.}
In this experiment, we evaluate the rigging quality of our model by generating 1200 images to assess how accurately it conforms to the desired pose, expression, and shape. Unlike prior studies, we do not randomly select images for evaluation. To ensure a fair comparison, we include CapHuman~\cite{liang2024caphuman} and conduct a single-image inference for both DiffusionRig\cite{ding2023diffusionrig} and our model, using the same parameters outlined in DiffusionRig's paper. This allows us to assess the effectiveness of our single-image fine-tuning in maintaining control and quality. We then measure the DECA~\cite{li2017learning}re-inference error to compare the results. The evaluation results, as shown in Table~\ref{tab:deca_reinference}, our model shows improved pose control, achieving an error of only 9.37 mm compared to DiffusionRig and CapHuman. The key reason for this improvement is our model's approach to handling control inputs. Unlike DiffusionRig, which merges conditional maps with the reference image, which leads to distortions during single-image fine-tuning, our model keeps these maps separate. This disentanglement ensures precise pose control, as illustrated in Figure~\ref{fig:pose_rigging_quality}a. DiffusionRig achieves better expression accuracy with an error of 3.37 mm, compared to our model's 5.14 mm, while CapHuman has a higher error of 7.37 mm. However, DiffusionRig often produces artifacts around the mouth area~\ref{fig:pose_rigging_quality}b, resulting from its attempt to maintain pixel consistency from the reference image. While slightly less accurate for expression, our approach avoids these artifacts, resulting in cleaner and more natural outputs. 












\begin{table}[!htbp]\vspace{-0.1cm} % Reduce space between main text and caption
\centering
\caption{DECA re-inference error evaluation based on facial-landmarks}\vspace{-0.1cm} % Reduce space between caption and table
\setlength{\tabcolsep}{10pt} % Adjust column spacing for better readability
\renewcommand{\arraystretch}{1.1} % Adjust row height for better readability
\begin{tabular}{lcc}
    \toprule
    Method & Pose & Expression \\
    \midrule
    CapHuman & 23.51 mm & 7.37 mm \\
    DiffusionRig & 11.32 mm & \textbf{3.37 mm} \\
    Ours & \textbf{9.37 mm} & 5.14 mm\\
    \bottomrule
\end{tabular}
\vspace{-0.4cm} % Reduce space after tables
\label{tab:deca_reinference}
\end{table}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{Sections/figures/Discussion3.png} % Include your image here

    % safety
    % \includegraphics[width=0.85\linewidth]{Sections/figures/Discussion.png} % Include your image here
\caption{Evaluation of pose and expression rigging quality during single-image fine-tuning.The input images are expected to follow the corresponding target image's (a) pose and (b) expression.}
    \label{fig:pose_rigging_quality}
\end{figure}
%safety
\vspace{-0.4cm}