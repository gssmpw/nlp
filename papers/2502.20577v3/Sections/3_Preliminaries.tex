

\section{Preliminaries}
% \xiaolin{Describe the overall problem setting here. Then, add some words to breif the relations between 3DMM/SD and the overall problem.}
In this section, we provide the foundational knowledge of 3D morphble models (3DMM) and stable diffusion (SD), which are essential for our method.

\textbf{3D Morphable Face Models:}
We use FLAME as the 3D Morphable Model (3DMM), which leverages linear blend skinning (LBS) with pose-dependent corrective blendshapes to represent head pose, face geometry, and facial expressions. 
The FLAME model is defined by $M(\beta, \theta, \psi)$, where the template mesh,
$$T_P(\beta, \theta, \psi) = \bar{T} + B_S(\beta; S) + B_P(\theta; P) + B_E(\psi; E), $$ 
combines shape $\beta$, pose $\theta$, and expression $\psi$ parameters to create a detailed facial representation. Here, $\bar{T}$ is the mean template in canonical pose, $B_S$ denote the shape blendshapes, $B_P$ denote the pose blendshapes, and $ B_E$ denote the expression blendshapes. 
% Despite its robust framework for representing face geometry, FLAME lacks an appearance model and has limitations in capturing fine-grained details due to its low mesh resolution~\cite{feng2021learning}. 
To complement FLAME, DECA enhances the 3DMM by integrating an appearance model that predicts detailed facial geometry, albedo, and lighting from single in-the-wild images. Specifically, DECA encodes 2D images into FLAME parameters,~\ie, $\theta$, $\psi$, and $\beta$, along with lighting $l$ and camera $c$ settilgs and captures facial attributes via generating a displacement map. After decoding, it generates albedo maps, surface normals, and spherical harmonic (SH) lighting. These maps are then used to guide the diffusion model, transferring the DECA-predicted face to a photorealistic image while maintaining detailed facial attributes.

\textbf{Stable Diffusion:}
Our method is built upon Stable Diffusion ~\cite{rombach2022high}, which performs the diffusion process efficiently in the latent space rather than the pixel space.  It consists of an encoder, $E$, which maps an input image, $x$, into a latent representation, $z = E(x)$. Stable Diffusion utilizes this latent representation to perform the diffusion and denoising processes. During training, the latent representation $z$ is iteratively diffused over $t$ timesteps, generating noisy latents, $z_t$, given by
$$z_t = \sqrt{\bar{\alpha}_t} z_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon,$$
which are then denoised by a UNet ~\cite{ronneberger2015u} to predict the original latent representation. The training objective for Stable Diffusion, denoted as $\epsilon_\theta$, aims to predict noise $\epsilon \sim \mathcal{N}(0, I)$. The objective is expressed as follows:
$$L_{\text{simple}} = \mathbb{E}_{z_0, \epsilon \sim \mathcal{N}(0, I), c, t} \left[ \left\| \epsilon - \epsilon_\theta (z_t, c, t) \right\|^2 \right],$$
where $z_0$ represents the original latent code, $t$ is the time step within the diffusion process, and the predefined functions $\bar{\alpha}_t$ govern the progression of noise during the diffusion process. 
$c$ incorporates additional conditional information to steer the denoising process.
During inference, the process begins with sampling $z_T$ from a Gaussian distribution, then progressively denoised to $z_0$ using a deterministic sampling process, such as DDPM~\cite{ho2020denoising} or DDIM~\cite{song2020denoising}. In each step, the denoising UNet predicts the noise for the corresponding timestep $t$. Finally, the decoder $D$ reconstructs $z_0$ back into the image space, yielding the final image.

% The decoder, $D$, then reconstructs the image from this latent representation. 