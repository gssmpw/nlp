
\section{Introduction}

With the advancements in generative models ~\cite{goodfellow2014generative}, high-quality image synthesis has become widespread, significantly transforming the landscape of image editing ~\cite{kawar2023imagic, mokady2023null, roich2022pivotal} and reducing the reliance on manual operations in specialized applications. There have been notable successes in semantic-level tasks, such as converting an image into various artistic styles, \eg, anime, cinematic, retro, sketch, and altering objects in an image ~\cite{suvorov2022resolution, yu2018generative, zhang2019deep, chen2018deep}. However, it remains challenging to achieve realistic transformations in geometric and high-level editing, where specific features are altered, and the overall consistency of the image needs to be preserved. 
\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{Sections/figures/teaser15.png} % Include your image here
   \caption{Prior methods (left image of each pair) exhibit various types of issues, such as (a) unnatural facial deformations, (b) identity shifts in features like hair,  eye color, and face shape, (c) inconsistencies in clothing and hair styling, and (d) artifacts or distortions in the background and accessories. In contrast, our approach (right image of each pair) effectively resolves these issues, preserving natural facial geometry, consistent identity, and coherent styling across all elements. Reference images (Ref.) are provided for (b) and (d).
     }

% Prior methods (left image of each pair) exhibit (a) unnatural facial deformations as well as (b) identity shifts in features like hair and eye color and face shape, compared to the reference image (Ref.) provided. In contrast, our approach (right image of each pair) maintains natural facial geometry and consistent identity. Background elements, including (c) clothing, hair, and (d) accessories, are distorted in prior approaches but are accurately preserved with our method.
    
    \vspace{-0.5cm} % Reduce space after the figure
    % \caption{Problem illustration: Top row: Unnatural facial deformations and identity shifts (e.g., change in the eye, hair color, and face shape) are present in prior approaches (left image of each pair), while our method (right image of each pair) maintains natural facial structure. Bottom row: Background elements like clothing, hair, and accessories are distorted in prior approaches but preserved accurately with our method.}
    \label{fig: challenges}
\end{figure}



The complexity further intensifies in facial image editing, where precise alterations in pose, expression, and lighting 
\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth, height=\textheight, keepaspectratio]{Sections/figures/teaser1_3.png}
    \caption{InstaFace leverages a single image to drive complex facial reenactments with conditional controls, including changes in pose, expression, and lighting. Our method ensures that the generated images retain the subject's identity, background, and fine-grained details while accurately reflecting the specified conditions.}
    \label{fig:teaser}
\end{figure*}
are desired while the individual`s identity needs to be preserved. Achieving precise and photorealistic facial image editing would open doors for various applications, such as personalized content creation~\cite{wang2023styleavatar}, digital avatars for gaming and virtual reality~\cite{xiang2024flashavatar}, and realistic interactions in virtual environments~\cite{salagean2023meeting}. 
% image editing, where tasks like pose, expression, and lighting adjustments are required. This process demands not only precise alterations of specific facial attributes but also the retention of the individual's identity. 
% Additionally, lighting modifications must create shadows and highlights that align with the facial structure's contours. 
% saying explicitly about the lighting does not sound good.

% As depicted in Figure~\ref{fig: challenges}, these challenges often lead to unintended distortions in facial features, identity, or background elements, which our approach aims to overcome.


%While there have been notable successes in semantic level tasks, such as converting an image into various artistic styles—like anime, cinematic, retro, sketch, or painting formats ~\cite{suvorov2022resolution, yu2018generative, zhang2019deep, chen2018deep}, it remains challenging to achieve realistic transformations in high-level semantic attributes, where specific features are altered while preserving the overall consistency of the image.


% In this scenario, it is essential to utilize a 3D representation of the face to achieve precise control over facial transformations, as this allows for accurate capture of the face's depth and structure, ensuring that changes in pose, expression, and lighting are consistent with its three-dimensional perspective. The advent of 3D Morphable Models (3DMMs) ~\cite{blanz2023morphable, paysan20093d, luthi2017gaussian, gerig2018morphable} has significantly eased this control by representing facial variations in shape and texture. However, 3DMMs still face limitations in capturing fine-grained facial details and dynamic expressions, often resulting in less realistic images. In addition, these models can struggle to accurately represent complex features such as hairstyles and accessories, which are crucial for high-fidelity image synthesis. 

% Despite these limitations, 3DMMs are essential for enabling detailed facial editing, providing the necessary 3D information. 




% In this case, DiffusionRig~\cite{ding2023diffusionrig} shows state-of-the-art performance by incorporating conditional maps from 3DMMs within a diffusion model to achieve precise control over pose, expression, and lighting. However, its reliance on multiple high-quality images and modifications to pre-trained diffusion weights limits its practicality and compatibility with open-source frameworks.

% In this case, DiffusionRig~\cite{ding2023diffusionrig} shows state-of-the-art performance by incorporating conditional maps produced by 3DMMs within a diffusion model to achieve precise control over pose, expression, and lighting. However, the need for multiple high-quality images of the same individual in DiffusionRig severely limits its practicality and scope of application. Moreover, DiffusionRig requires modifications to the weights of pre-trained diffusion models, which limits its compatibility with open-source frameworks and disrupts the model's existing knowledge, complicating practical implementation.

% One innovative approach, DiffusionRig~\cite{ding2023diffusionrig}, is the state-of-the-art approach to incorporate conditional maps produced by 3DMMs within a diffusion model to achieve precise control over pose, expression, and lighting. However, the need for multiple high-quality images of the same individual in DiffusionRig severely limits its practicality and scope of application. Moreover, DiffusionRig requires modifications to the weights of pretrained diffusion models, which limits its compatibility with open-source frameworks and disrupts the model's existing knowledge, complicating practical implementation.
% Another recent work, DisControlFace~\cite{jia2024discontrolface}, employs Diff-AE~\cite{preechakul2022diffusion} as a semantic reconstruction backbone to provide explicit control over facial attributes. While effective in extracting high-level semantics for near-exact reconstruction, Diff-AE struggles when significant global changes are needed. As a result, DisControlFace faces challenges in accurately representing poses, leading to misalignment between the face and other elements like hair, neck, and accessories, which can result in unrealistic appearances.

% To address these challenges, GAN-based approaches~\cite{ghosh2020gif, chan2022efficient, aneja2023clipface} have been explored by incorporating 3D facial priors to enhance image synthesis. These approaches struggle to maintain consistency and coherence across varying poses and expressions~\cite{hong2022headnerf}, especially in zero-shot scenarios, resulting in identity drift of generated images ~\cite{ding2023diffusionrig}. 

Diffusion models~\cite{ho2020denoising} have demonstrated remarkable capabilities in image generation and manipulation. Recent works~\cite{jia2024discontrolface, preechakul2022diffusion} have adapted these models for more controllable editing of facial attributes. Among them
Animate Anyone~\cite{hu2024animate} adopted an approach similar to ControlNet~\cite{zhang2023adding}, where the identity reference image is introduced via a ReferenceNet, and the posing condition is introduced via a trainable pose guider. However, training this model requires multiple frames per identity, which limits its practicality for single-image editing. More importantly, using the same image for each identity will lead to overfitting, causing the model to copy the reference image while ignoring the intended control. Also, extending this approach to handle multiple conditions would require separate trainable modules for each of the conditions \cite{zhu2024champ}, significantly increasing training resource requirements. In order to achieve more accurate control over pose, expression, and lighting, DiffusionRig~\cite{ding2023diffusionrig} proposes to incorporate conditional maps from 3DMMs~\cite{blanz2023morphable, paysan20093d, luthi2017gaussian, gerig2018morphable} within a diffusion model. 
However, its reliance on multiple inference images limited its applicability, and its modification to pre-trained diffusion weights restricted its compatibility with open-source frameworks. More importantly, the facial feature and control conditions are not properly disentangled, leading to facial distortion and misalignment between facial features and non-facial features such as hair, neck, and accessories, as illustrated in Figure~\ref{fig: challenges}.

% Thus, balancing precise conditioning with natural facial appearance remains a trade-off for these methods, especially in single-image cases.


To address the issues of both paradigms, we introduce InstaFace, a novel approach that efficiently controls facial attributes while preserving identity with only a single inference image. Inspired by how Stable Diffusion effectively operates on the latent noisy input, we designed a 3D Fusion Controller Module for processing conditional maps in the latent space. We argue that if latent diffusion models can successfully use the latent space for input images, then it can be extended to conditional maps. This approach efficiently processes multiple conditional maps without requiring any additional trainable module, significantly reducing memory usage and computational overhead. This module then integrates with a Guidance Network, identical to the denoising UNet, which ensures that intended edits, such as pose, expression, and lighting, are preserved. By combining the latent representations from the guidance network with those from the diffusion network at each attention layer, our approach effectively utilizes both spatial and 3D information, therefore achieving precise control over desired facial attributes. 
Moreover, an Identity Preserver Module is introduced to better capture identity features and the overall semantic features. We propose integrating a face recognition encoder with a CLIP image encoder. The face recognition encoder focuses on preserving detailed facial features, while the CLIP encoder captures the broader semantic context, including elements like background and accessories.
Figure~\ref{fig:teaser} illustrates the results of the rigging achieved by our model with \textit{a single inference image} only.


% To address these limitations, we introduce InstaFace, a novel approach that efficiently controls facial attributes while preserving identity using only \textit{a single inference image}.
% We design a guidance network that provides targeted 3D rigging information, derived from 2D images using FLAME~\cite{li2017learning}, to the main diffusion network.  Both the Guidance and Diffusion Networks leverage the same weights from a pre-trained Stable Diffusion model, establishing a robust knowledge base that is then fine-tuned for the facial domain during the training stage. This approach significantly enhances efficiency. Our training iterations are significantly reduced compared to conventional methods~\cite{ding2023diffusionrig, hong2022headnerf}. 


% To address this research gap of zero-shot methods and approaches requiring multiple images, we present a novel approach, InstaFace, that achieves efficient control over facial attributes while maintaining identity with the help of a single inference image. 
% To bridge the gap between zero-shot methods and those requiring multiple images, we introduce InstaFace, a novel approach that efficiently controls facial attributes while preserving identity using just a single inference image, as demonstrated in Figure ~\ref{fig:teaser}. Specifically, we design a Guidance Network that inherits weights from a pre-trained Stable Diffusion (SD) model and mirrors the architecture of the main Diffusion Network. This design offers several key benefits. Firstly, leveraging pre-trained weights from the Stable Diffusion model provides the Guidance Network with a strong foundation of knowledge. Unlike training a new model from scratch, which can be prohibitively resource-intensive and time-consuming, using established pre-trained models from the Stable Diffusion ecosystem ensures that our network benefits from well-developed capabilities in image feature modeling. This approach not only accelerates adaptation to 3D facial features but also significantly improves computational efficiency, reducing the training iterations needed by approximately 20 times compared to conventional methods.

% Secondly, the architecture of the Guidance Network offers flexible and disentangled control over facial attributes. This design preserves the integrity of the pre-trained knowledge embedded in the main Diffusion Network, ensuring that the core diffusion process remains effective and unaltered.

% To further enhance the model’s capability to manage 3D facial features, the Guidance Network incorporates the 3D Morphable Model (3DMM), specifically FLAME~\cite{li2017learning}. This integration allows the network to accurately interpret and incorporate detail conditions from 2D images. By combining the latent representations from the Guidance Network with those from the Diffusion Network at each attention layer, our approach ensures that both spatial and 3D information are effectively utilized. This enables precise control over facial attributes while maintaining the subject's identity and overall appearance. Moreover, we introduce a branch that merges features from a face recognition model and CLIP vision model ~\cite{radford2021learning} to capture identity features and overall semantic information, including facial structures, background, and accessories.

% Specifically, we design a Guidance Network, which inherits weights from a pre-trained Stable Diffusion (SD) model and mirrors the architecture of the main diffusion model. To capture the 3D perspective from the 2D images, this network learns the 3D features of a morphable facial model FLAME~\cite{li2017learning}. By guiding the diffusion network with latents at each attention layer in a coherent feature space, this approach significantly enhances the model's control over facial attributes. We also introduce a branch that merges features from a face recognition model and CLIP vision model ~\cite{radford2021learning} to capture identity features and overall semantic information, including facial structures, background, and accessories.

We train our model using the FFHQ dataset ~\cite{karras2019style}, where conditional maps such as albedo maps, surface normal maps, and Lambertian render maps are extracted from the facial morphable model using the DECA~\cite{feng2021learning} to learn facial reenactment attributes. After learning these conditionals, we fine-tune our model with only a single inference image, where our newly introduced Identity Preserver Module effectively captures identity and semantic information, ensuring consistency. We provide extensive experimental results demonstrating superior performance compared to previous methods, along with ablation studies that highlight the improvements achieved at each stage of training and within each module. 
In summary, our contributions are as follows:
\begin{itemize}
\item We introduce a novel framework that more effectively incorporates the 3D conditional maps into an off-the-shelf diffusion model, achieving the new state-of-the-art identity-preserving facial attribute editing, with only a single inference image;
% \item We propose a novel framework featuring a 3D Fusion Controller and a Guidance Network that together enable efficient facial attribute editing with precise control, even from a single reference image.
\item We introduce a novel Identity Preserver Module that combines a pre-trained multimodal vision model with a facial recognition model, ensuring maximal consistency of both the identity and the overall semantics in the input image;
\item We present comprehensive experimental results and ablation studies, demonstrating how our approach outperforms previous methods, and how each module contributes to enhancing control over facial attributes while maintaining identity consistency. 
\end{itemize}



% The integration of features from the Guidance Network into the denoising UNet, inspired by the AnimateAnyone model (Hu 2024), ensures precise and consistent guidance throughout both the encoding and decoding processes."


% We design a Guidance Network that follows a ControlNet-like ~\cite{zhang2023adding}  architecture to maintain the integrity of the main diffusion model’s weights. This network captures the 3D maps generated from the input image, guiding the diffusion process. Drawing inspiration from the AnimateAnyone model (Hu 2024), we integrate features from the Guidance Network into the denoising UNet using attention layers. 


% Our method balances these extremes, leveraging a simple ControlNet ~\cite{zhang2023adding} architecture to ensure precise attribute manipulation and identity preservation. 


% Our approach utilizes the FLAME ~\cite{li2017learning} as the 3D Morphable Model (3DMM) to work in the 3D space and capture the 3D perspective of the 2D images. We generate the parameters for this model from 2D images using DECA ~\cite{feng2021learning}, which enables precise control over facial expressions, pose, and lighting for the 3DMM model. During the training stage, we extract 3D rendered maps such as Lambertian renderer, albedo, and surface normals using this DECA from 2D images of the FFHQ ~\cite{karras2019style} dataset. To maintain the integrity of the main diffusion model's weights, we employ a ControlNet-like architecture that captures these 3D maps to guide the input image. Inspired by the AnimateAnyone ~\cite{hu2024animate} model, we integrate features from the guidance network into the denoising UNet using the attention layers. This enables the model to learn the mean attributes of the input images through the guidance of a CLIP vision ~\cite{radford2021learning} model, effectively training both the ControlNet and the diffusion model to handle these conditionals. In the second stage, our approach uses only a single image—the inference image—to capture detailed identity features. This strategy circumvents issues faced by DisControlFace, which struggles with significant global changes, and DiffusionRig, which requires multiple images for effective identity preservation. By avoiding these pitfalls, our method ensures robust identity retention and accurate facial attribute editing without the limitations of zero-shot methods.
% Furthermore, our ablation studies provide insights into the challenges of identity capture in the first stage, highlighting the improvements in the second stage. During inference, we edit the predicted parameters from DECA to achieve the desired appearance modifications on the face of the inference image. Our approach not only addresses the limitations of previous methods but also enhances the overall efficiency and accuracy of facial attribute editing. 

% In summary, our contributions are as follows:

% \begin{itemize}
% \item We present a new method that balances the extremes of zero-shot and multiple-image approaches, achieving precise attribute manipulation and identity preservation with the help of a single inference image.
% \item Our method employs the FLAME model as the 3D Morphable Model (3DMM), capturing the 3D perspective of 2D images and providing detailed control over facial attributes using a ControlNet-like architecture.
% \item Our ablation studies reveal the importance of the second stage for robust identity retention and identify key modules supported by comprehensive qualitative and quantitative results.
% \end{itemize}
% our method strikes a balance between these extremes, leveraging a simple ControlNet architecture to ensure precise attribute manipulation and identity preservation

% DiffusionRig~\cite{ding2023diffusionrig}, for instance, leverages these 3D facial priors to learn person-specific features and enable precise editing of facial attributes while preserving identity. However, it relies on multiple images of the same person to constrain the generative process and ensure identity retention. This reliance is not always practical due to the difficulty in obtaining multiple high-quality images of the same individual. 


% DiffusionRig, for example, learns person-specific facial priors to enable precise editing of facial attributes while preserving identity. However, it often relies on multiple images of the same person to constrain the generative process and ensure identity retention. This reliance is not always practical due to the difficulty in obtaining multiple high-quality images of the same individual.


% Previous methods, such as ~\cite{ding2023diffusionrig}, rely on multiple images of the same person to constrain the generative process and ensure identity retention. However, this is not always practical due to the difficulty in obtaining multiple high-quality images of the same individual.

% Previous methods often rely on multiple images of the same person to constrain the generative process and ensure identity retention. However, this is not always practical, highlighting the need for approaches to achieve effective results with minimal data.
