
\section{Related Work}
Our work is at the intersection of generative face models, 3D Morphable Face Models (3DMMs), and identity-preserving synthesis.

\textbf{Generative Facial Synthesis: }
Generative Adversarial Networks (GANs) have significantly advanced face generation, producing photorealistic images across various facial attributes \cite{hu2018pose, karras2020analyzing, tripathy2020icface}. However, these models struggle to disentangle and independently control attributes like appearance, shape, and expression, limiting their effectiveness in detailed editing. To address these issues approaches like \cite{marriott20213d, ghosh2020gif} incorporate 3D features for better attribute control. Diffusion models have emerged as the state-of-the-art in deep generative modeling, surpassing GANs in image synthesis \cite{dhariwal2021diffusion} and demonstrating their effectiveness in generating realistic facial images \cite{banerjee2023identity, valevski2023face0, xu2024personalized}. DisControlFace~\cite{jia2024discontrolface} leverages Diff-AE~\cite{preechakul2022diffusion}, using random masking techniques for effective training. However, these models struggle with diverse or large pose variations due to Diff-AE's reliance on near approximation. DiffusionRig~\cite{ding2023diffusionrig} enhances synthesis with pixel-aligned conditions (e.g., normals, albedo) and uses multiple images for identity preservation but still faces challenges in maintaining consistency across generated outputs. CapHuman~\cite{liang2024caphuman} uses textual data to control facial attributes but struggles with consistency, leading to variations in background, hair, and facial shape. VOODOO 3D~\cite{tran2024voodoo} addresses volumetric head reenactment but struggles with pose control, leading to unnatural tilts of the entire input image and visual artifacts.

Our method, InstaFace, uniquely addresses these limitations by retaining identity with just one image, even under large variations in conditionals, using a combination of CLIP and a face recognition model.


% Generative Adversarial Networks (GANs) have brought substantial progress in face generation, enabling the creation of photorealistic images across various facial attributes \cite{hu2018pose, karras2020analyzing, tripathy2020icface}. However, these models often struggle with disentangling and independently controlling specific facial attributes such as appearance, shape, and expression, which limit their effectiveness in detailed facial image editing applications. In response to these limitations, approaches like \cite{marriott20213d, ghosh2020gif} incorporate 3D features to better control and manipulate individual facial attributes. Diffusion models have recently established themselves as the leading approach in deep generative modeling, surpassing GANs ~\cite{dhariwal2021diffusion} in image synthesis and proving their effectiveness in generating realistic facial images.~\cite{banerjee2023identity, valevski2023face0, xu2024personalized}. DisControlFace~\cite{jia2024discontrolface} leverage Diff-AE~\cite{preechakul2022diffusion} as their backbone, employing random masking techniques on the main input image for effective training. However, due to the near approximation of Diff-AE, these models struggle to realistically handle different poses or large variations in poses.
% On the other hand, DiffusionRig~\cite{ding2023diffusionrig} employs pixel-aligned conditions, such as normals, albedo, and rendered images, to enhance image synthesis. Additionally, they use an album of images for a particular person to retain identity across generated outputs. Recently, CapHuman~\cite{liang2024caphuman} has focused on controlling facial identity and semantic attributes using textual data. However, it struggles with maintaining consistency, resulting in unnatural background, hair, and facial shape variations. VOODOO 3D~\cite{tran2024voodoo} addresses volumetric head reenactment, utilizing tri-plane neural radiance fields. However, the approach struggles with pose control, leading to entire input images tilting instead of following natural poses, resulting in black pixels filling the empty spaces during the reenactment.



% Our method, InstaFace, addresses these limitations by retaining identity with the help of just one image, even in cases with large variations in conditionals compared to the main input image, using a combination of CLIP and the face recognition model. 

\textbf{Condition-Driven Face Synthesis: }
Effective facial synthesis and editing generally rely on integrating conditional inputs to guide the generation process. For instance, GANs, particularly StyleGAN, excel in transferring styles from a constant input tensor (4×4×512) to produce high-fidelity images by feeding latent code \( z \in \mathbb{Z} \) through different routes to the network. Similarly, diffusion models, like DDPMs~\cite{ho2020denoising}, use text embeddings from large pre-trained models or employ encoders to generate latent codes that guide the noise prediction and denoising processes. For facial image editing, where retaining the original image features while altering global attributes such as pose and lighting or local attributes like expressions (mainly mouth, eyes, and cheeks) is essential, incorporating 3D perspectives becomes necessary. Methods such as ~\cite{ding2023diffusionrig, jia2024discontrolface, ghosh2020gif} achieve this by utilizing albedo maps, normal maps, and Lambertian renders from 3DMM models ~\cite{li2017learning, ichim2017phace, koppen2018gaussian} to condition their generative models. To utilize these conditionals effectively, ControlNet~\cite{zhang2023adding} stands out for its ability to guide the denoising UNet spatially, layer by layer, due to its similar structure to the denoising UNet. However, previous methods like DisControlFace face challenges with the availability of pre-trained models. While many pre-trained Stable Diffusion models exist due to their generative capability on large datasets, the diverse nature of conditionals means there can be various types of ControlNet, complicating the use of pre-trained models for ControlNet. In our approach, we leverage the same Stable Diffusion structure, ensuring ease of training and effective integration of conditionals, thereby overcoming these challenges.