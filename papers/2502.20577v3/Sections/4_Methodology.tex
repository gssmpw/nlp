

\section{Methodology}
% To achieve robust facial image editing, we propose a generative framework named InstaFace (shown in Figure~\ref{fig:Architecture}, which integrates ControlNet architecture with a 3D Morphable Model (3DMM). Our method involves two stages: the first stage focuses on conditional understanding and learning generic facial features, incorporating DDIM conditioning, while the second stage utilizes a single target image to capture detailed facial structure and attributes. In the first stage, our model learns to understand and generalize facial attributes from a broad dataset, ensuring that the generative process can handle various appearance conditions. In the second stage, InstaFace fine-tunes its generative capabilities using only one specific target image. This allows the model to capture unique facial details and nuances, as well as background attributes such as hair and accessories, ensuring that the person's identity is accurately retained during editing.

To achieve robust facial image editing, we propose a generative framework,~\ie, \textbf{InstaFace}, as illustrated in Figure~\ref{fig:Architecture}. Our method integrates control conditions from  3D Morphable Model (3DMM) to edit the target face, accordingly. InstaFace is composed of two stages. In the first stage (Figure~\ref{fig:Architecture}a), the Guidance network learns to understand and generalize facial attributes from a broad dataset of reference images. This ensures that the generative process can effectively handle various appearance conditions, such as pose, expression, and lighting. In the second stage (Figure~\ref{fig:Architecture}b), InstaFace fine-tunes its generative capabilities using a specific target image, referred to as the inference image. During this stage, the embeddings obtained from pre-trained CLIP and facial recognition models provide the necessary information to guide the diffusion process. This guidance is crucial for accurately retaining the specific identity of the individual, ensuring that facial features and background attributes, such as hair and accessories, are preserved. Finally, as shown in Figure~\ref{fig:Architecture}c, the Diffusion Network learns both the overall identity information and the necessary conditioning to generate the desired facial image. This combination allows InstaFace to deliver high-fidelity, identity-preserving facial edits with precise attribute modifications.

% , as shown in Figure??

% \xiaolin{Add an overall figure to show the network structure}

\subsection{Facial Condition Adaptation}

Our initial stage aims to learn facial priors from 3DMM-generated conditionals, focusing on identifying specific editable attributes and capturing high-level features. To achieve this, we employ DECA ($E_{\text{DECA}}$) to estimate FLAME parameters from 2D images, effectively bridging the image data to the 3D domain for detailed facial representation. Specifically, DECA predicts the shape $\beta$, pose $\theta$, and expression $\psi$ parameters, along with the lighting $l$ and camera $c$ settings, using the FLAME model with its appearance and illumination models. These parameters are then used to generate pixel-aligned maps, including albedo maps ($f_{alb}$), surface normals, and Lambertian renderings, with the help of the DECA decoder ($D_{\text{DECA}}$). By translating non-spatial 3DMM parameters into spatially meaningful visual representations, we ensure that the model can accurately capture the detailed geometry and appearance of the face.


To efficiently utilize these 3D features and condition the whole model, we introduce the 3D Fusion Controller—a core contribution of our approach. The 3D Fusion Controller takes the 3DMM-generated conditionals—such as albedo maps, surface normals, and rendered maps—and converts them into latent space representations using a pre-trained frozen autoencoder ($E_{\text{cond}}$). These latent conditionals are then concatenated in the channel dimension, allowing us to handle multiple conditionals simultaneously without introducing additional trainable parameters. This simple yet effective design enables our model to retain and leverage the latent 3D conditions more efficiently, leading to a robust conditioning process without additional computational overhead.


Now, instead of directly passing the latent conditional maps to the main diffusion process, we employ a ControlNet-like architecture, which we refer to as the Guidance Network ($\mathcal{G}$). This allows us to retain the fundamental weights of the Diffusion Network ($\mathcal{M}_{\text{diff}}$) unchanged, providing efficiency without compromising the model's initial learned capabilities. Additionally, this ControlNet-like approach leverages the robust feature extraction capabilities of U-Net architectures, enabling spatial-aware conditioning for the diffusion process. Inspired by~\cite{hu2024animate}, we structured the Guidance Network to mirror the denoising U-Net architecture within our framework. The Guidance Network benefits from pre-trained image feature modeling capabilities by inheriting weights from the original Stable Diffusion model, ensuring a well-initialized feature space. This avoids the need to train from scratch or rely on existing ControlNet models, which are not suitable for our 3D facial image editing task. The Guidance Network processes the latent conditional maps generated by the 3D Fusion Controller, and from each layer, the conditioning information flows to the Main Diffusion Network.

The main input image is also encoded into a latent representation using a pre-trained autoencoder ($E_A$). Noise is added to these latent representations using DDIM and then passed into the Diffusion Network ($\mathcal{M}_{\text{diff}}$). Consequently, the intermediate features of the Guidance Network are spatially combined with the corresponding intermediate features of the Diffusion Network in the attention module, specifically just before the self-attention layers. This integration ensures that the conditioning information from the Guidance Network effectively influences the noise prediction process.

% For our specific needs, we adapt this framework by concatenating the conditionals (albedo maps, normal maps, and rendered maps) in the channel dimension ($\text{concat}_{\text{chl}}$) and passing them through the guidance network. This network processes the concatenated maps to generate spatially aware features. 
% (talk about the latent conditions)

 % Since Stable Diffusion model operates in the latent space, we convert each of the conditionals—albedo maps, surface normals, and rendered maps—into the latent space using a pretrained, frozen autoencoder ($E_{\text{cond}}$). We then concatenate these latent conditionals in the channel dimension and pass them through the Guidance network. Meanwhile, the main input image is also encoded into a latent representation using a pre-trained autoencoder ($E_A$), and then the denoising process is applied for effective facial attribute editing.
Formally, the process can be described by the following equation:
\begin{align}
F_{\text{comb}} ={}& \mathcal{M}_{\text{diff}}(E_A(x)) \nonumber \\
&\oplus \mathcal{G} \left( \text{concat}_{\text{chl}} \left( E_{cond} (D_{\text{DECA}} \left( E_{\text{DECA}}(x) \right) \right) \right)) \label{eq:combined},
\end{align}
where $\oplus$ indicates that the features from the Guidance Network are added to the intermediate noisy feature maps of the Diffusion Network before passing through the self-attention layers.


\subsection{Identity Preserving Guidance}
The Diffusion Network starts from complete noise during inference, which is why it is crucial how the guidance is provided for the main input image. Unlike style-editing methods (e.g., transforming images into paintings, sketches, or anime) \cite{wang2024instantid}, our specific task requires maintaining the identity of the given input person and the background or accessories while allowing changes in pose, expression, and lighting. Specifically, in text-to-image tasks, high-level semantics suffice, but image-based generation demands detailed guidance to preserve both identity and fine-grained attributes. In this case, CLIP excels at capturing high-level semantic information and contextual understanding from images, which is beneficial for generating coherent and contextually accurate outputs \cite{song2023objectstitch, yang2023paint, ye2023ip}. However, CLIP’s limitation lies in its reliance on low-resolution images during encoding, which results in the loss of fine-grained details crucial for high-fidelity image synthesis. Additionally, CLIP's training primarily focuses on matching semantic features for text-image pairs, which may lead to insufficient encoding of detailed facial attributes and unique identity features \cite{hu2024animate}.  This issue is compounded by the fact that CLIP is trained on weakly aligned datasets, which tends to emphasize only broad attributes such as layout, aesthetic, and color schemes \cite{wang2024instantid}.

In contrast, facial recognition technology has seen remarkable advancements in computer vision systems, demonstrating exceptional accuracy in identifying individuals. Leveraging a facial recognition model can be an effective approach to capture and retain fine-grained identity details, ensuring the generated images preserve the unique attributes of the input face. However, relying solely on facial recognition models can pose challenges. These models often generate embeddings that focus primarily on specific facial regions, such as the eyes, cheeks, and nose \cite{zee2019enhancing}. This selective focus may lead to inconsistencies in other parts of the face, resulting in unrealistic image synthesis. 

% However, relying solely on facial recognition models, we combine the image embeddings from the CLIP model with the detailed identity embeddings generated by a face recognition model. These combined embeddings are processed through a projection module and then utilized in the cross-attention mechanism of both the guidance network and the main diffusion model. This integration ensures that the generated images maintain high-level semantic coherence from CLIP while preserving fine-grained identity details from the face recognition model. By employing this dual-embedding strategy, we achieve more realistic and identity-consistent facial image synthesis.

To address these limitations, we propose a novel approach that combines the strengths of CLIP and facial recognition models. Specifically, we combine the image embeddings from the CLIP model $E_{\text{CLIP}}$ with the detailed identity embeddings generated by a face recognition model $E_{\text{FR}}$. 
These combined embeddings are processed through a projection module, which incorporates a series of attention mechanisms and feedforward networks. The projected embedding is then used in the cross-attention mechanism of the Guidance and Diffusion networks. This dual-embedding strategy ensures that the generated images retain high-level semantic coherence from CLIP while capturing fine-grained identity details from the face recognition model, thus overcoming the shortcomings of using either model independently.

The combined embedding \( E_{\text{comb}} \) is computed as follows:
\begin{align}
E_{\text{comb}} = \text{Proj}(E_{\text{CLIP}}(x), E_{\text{FR}}(x)),
\label{eq:combined_embedding}
\end{align}
where \(\text{Proj}\) denotes the projection module that merges the feature embeddings using attention and feedforward layers. This combined embedding is then incorporated into the cross-attention mechanisms of both the Guidance Network and the main Diffusion Network.

Therefore, the overall loss function for training our model is defined as:
\begin{align}
\mathcal{L} = \mathbb{E}_{z_0, t, E_{\text{comb}}, c_f, \epsilon \sim \mathcal{N}(0, 1)} \left[ \left\| \epsilon - \epsilon_\theta \left( z_t, t, E_{\text{comb}}, c_f \right) \right\|^2 \right],
\label{eq:combined_2}
\end{align}
where $c_f$ represents the layer-wise features extracted from the guidance network $\mathcal{G}$.

