\section{Related Work}
\label{sec:related_work}
\vspace{-1ex}
\subsection{Noise-Robust GNN}
\vspace{-1ex}
Noise-robust GNNs aim to train robust models  under feature, structure, and/or label noise, but most existing approaches focus on only one type of noise.

\vspace{-1ex}

\smallskip
\noindent \textbf{Feature noise-robust GNN.} \@ 
% While various approaches \cite{NOSMOG_ICLR, jin2022empowering} have been proposed to address the noisy node features, r
% Recent studies have highlighted the significance of fully leveraging structural information. 
AirGNN \cite{airgnn} identifies and addresses nodes with noisy features based on the hypothesizes that they tend to have dissimilar features within their local neighborhoods. {Consequently, this approach tackles the noisy node features while assuming that the structure of the input graph is noise-free.}

\vspace{-1ex}

\smallskip
% \vspace{-1ex}
\noindent \textbf{Structure noise-robust GNN.} \@ 
% Among various approaches \cite{lei2022evennet, in2023similarity}, a representative approach is based on the graph structure learning (GSL). 
RSGNN \cite{rsgnn} aims to train a graph structure learner by encouraging the nodes with similar features to be connected. STABLE \cite{stable} removes edges with low feature similarity, learns node representations from the modified structure, and constructs a kNN graph as the refined structure. {In summary, these methods tackle the noisy graph structure while assuming that node features are noise-free.}

\vspace{-1ex}

\smallskip
% \vspace{-1ex}
\noindent \textbf{Label noise-robust GNN.} \@ 
NRGNN \cite{nrgnn} connects edges between nodes with similar features, mitigating the information propagation from falsely labeled nodes. 
RTGNN \cite{rtgnn} uses small-loss approach \cite{han2018co}, but nodes with noisy features or structures exhibit large losses, leading to inaccuracies of the approach. {Therefore, these methods tackle the noisy node labels while assuming that both node features and graph structure are noise-free.}

\vspace{-1ex}

\noindent \textbf{Multifaceted noise-robust GNN.} \@ 
SG-GSR \cite{in2024self} tackles multifaceted structure and feature noise by identifying a clean subgraph within a noisy graph structure and augmenting it using label information. This augmented subgraph serves as supervision for robust graph structure refinement. However, since noisy label information can compromise the augmentation process, SG-GSR relies on the assumption that node labels are free of noise.

In summary, each method assumes the completeness of at least one of the data sources, limiting their practicality.

\vspace{-1ex}
\smallskip
\subsection{Generative Approach}
\vspace{-1ex}
\citet{yao2021instance} devises a generative approach to model the DGP of instance-dependent label noise \cite{berthon2021confidence}. However, extending this method to the graph domain introduces significant challenges. It requires handling additional latent variables and complex causal relationships, such as $Z_A$, $\epsilon_A$, $A \leftarrow \epsilon_A$, $A \leftarrow X$, $Y \leftarrow A$, and $A \leftarrow Z_A$, each posing non-trivial obstacles beyond the straightforward extension\footnote{Detailed explanation is outlined in Appendix~\ref{sec:ap_causlnl}}. WSGNN \cite{lao2022variational} and GraphGLOW \cite{zhao2023graphglow} utilize a probabilistic generative approach and variational inference to infer the latent graph structure and node labels. However, they assume noise-free graphs, reducing effectiveness in real-world noisy scenarios.

\vspace{-2ex}