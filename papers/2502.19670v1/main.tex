%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
% \usepackage{textcase}
% \usepackage{xparse}
% \usepackage{physics}
\usepackage{enumitem}
% \usepackage{algpseudocode}
% \usepackage{algorithm2e}
% \usepackage{algorithmicx}
\usepackage{multirow}
\usepackage{pifont}
% \usepackage{xcolor}
% \usepackage{blindtext}
% \usepackage{colortbl}
% \usepackage{tabulary}
% \usepackage{etoolbox}
\usepackage{lipsum}
\usepackage{subcaption}
% \newtheorem{definition}{definition}

% \definecolor{airforceblue}{rgb}{0.36, 0.54, 0.66}
% \definecolor{burntumber}{rgb}{0.54, 0.2, 0.14}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\proposed}{\textsc{Da-Gnn}}
% \newcommand{\proposed}{$\textsf{PRINGLE}$}
% \newcommand{\todo}[1]{\textcolor{red}{~TODO:~#1}}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{Training Robust Graph Neural Networks by Modeling Noise Dependencies}



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{corresponding}{*}

\begin{icmlauthorlist}
\icmlauthor{Yeonjun In}{kaist}
\icmlauthor{Kanghoon Yoon}{kaist}
\icmlauthor{Sukwon Yun}{unc}
\icmlauthor{Kibum Kim}{kaist}
\icmlauthor{Sungchul Kim}{adobe}
\icmlauthor{Chanyoung Park}{kaist}
\end{icmlauthorlist}

\icmlaffiliation{kaist}{KAIST}
% \icmlaffiliation{kaistds}{Graduate School of Data Science, KAIST, Daejeon, Republic of Korea}
\icmlaffiliation{adobe}{Adobe Research}
\icmlaffiliation{unc}{UNC Chapel Hill}

\icmlcorrespondingauthor{Yeonjun In}{yeonjun.in@kaist.ac.kr}
\icmlcorrespondingauthor{Chanyoung Park}{cy.park@kaist.ac.kr}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
In real-world applications, node features in graphs often contain noise from various sources, leading to significant performance degradation in GNNs. Although several methods have been developed to enhance robustness, they rely on the unrealistic assumption that noise in node features is independent of the graph structure and node labels, thereby limiting their applicability. To this end, we introduce a more realistic noise scenario, dependency-aware noise on graphs (DANG), where noise in node features create a chain of noise dependencies that propagates to the graph structure and node labels. We propose a novel robust GNN, \proposed, which captures the causal relationships among variables in the data generating process (DGP) of DANG using variational inference.
In addition, we present new benchmark datasets that simulate DANG in real-world applications, enabling more practical research on robust GNNs. Extensive experiments demonstrate that \proposed~consistently outperforms existing baselines across various noise scenarios, including both DANG and conventional noise models commonly considered in this field.
 % Our code is available at \textcolor{purple}{\url{https://anonymous.4open.science/r/torch-DA-GNN-164E/}}.
 Our code is available at \textcolor{purple}{\url{https://github.com/yeonjun-in/torch-DA-GNN}}.

\end{abstract}

\section{Introduction}
\label{sec:intro}

In recent years, graph neural networks (GNNs) have demonstrated remarkable achievements in graph representation learning and have been extensively applied in numerous downstream tasks \cite{gcntext, ngcf, kim2023class, kim2024revisiting}. However, in the majority of real-world scenarios, node features frequently exhibit noise due to various factors, leading to the creation of inaccurate graph representations \cite{airgnn, jin2022empowering}. For instance, in user-item graphs, users may create fake profiles or posts, and fraudsters and malicious users may write fake reviews or content on items, resulting in noisy node features.
% Similarly, in product co-purchase networks found on e-commerce systems,
% node features can be contaminated by fake reviews.
Recent studies have revealed the vulnerability of GNNs to such scenarios, highlighting the necessity to design robust GNN models against noisy node features. 


\begin{figure}
    % \raisebox{5pt}[\dimexpr\height-0.6\baselineskip\relax]
    % % \vspace{-4ex}
    \centering
    {\includegraphics[width=0.8\columnwidth]{fig/fdgn_fig.pdf}}
    \vspace{-2ex}
    \caption{Examples of DANG in social networks: IFN represents independent node feature noise. Under the IFN assumption (b), Bob’s noisy features are presumed to have no effect on the graph structure or node labels. However, in real-world DANG scenarios (c), Bob’s noisy features can propagate, leading to both structural noise in the graph and label noise.}
    \label{fig:DANG_fig}
    \vspace{-3ex}
\end{figure}

\looseness=-1
To this end, various methods have been proposed to make a huge success in terms of model robustness \cite{airgnn, jin2022empowering}. These methods are founded on the independent node feature noise (IFN) assumption, which posits that noise in node features does not impact the graph structure or node labels. Under the IFN assumption (Fig. \ref{fig:DANG_fig}(b)), for example, Bob's fake profile does not influence other nodes, which is also explained by the data generating process (DGP) of IFN (See Fig. \ref{fig:DANG_graphical}(a)) in which no causal relationships exist among the noisy node features $X$, graph structure $A$, and node labels $Y$. 

However, we should rethink: \textbf{In real-world applications, can noise in node features truly be isolated from influencing the graph structure or node labels?}  Let us explore this through examples from social networks (Fig.~\ref{fig:DANG_fig}). Consider Bob, who introduces noisy node features by creating fake profiles or posts. Other users, such as Alice and Tom, may then connect with Bob based on his fake profile, resulting in noisy connections that contribute to graph structure noise. Over time, these noises could alter the community associations of Alice and Tom, leading to noisy node labels. Such causal relationships among $X$, $A$, and $Y$ (i.e., $A \leftarrow X$, $Y \leftarrow X$, and $Y \leftarrow A$) are depicted in Fig. \ref{fig:DANG_graphical}(b).
Similarly, in an e-commerce network, malicious users may post fake reviews or content about items, creating noisy node features. These fake interactions could lead other users to engage with irrelevant items, introducing noisy co-interaction edges between items (i.e., graph structure noise). As a result, automated item category labeling systems, which rely on both node features and graph structure, might inaccurately label item categories, resulting in label noise. 

These scenarios underscore an important insight: \textbf{In real-world applications, noise in node features may create a chain of noise dependencies that propagate to the graph structure and node labels.} This highlights the pressing need for robust GNNs capable of addressing such noise dependencies — an aspect that has been largely overlooked in current research. 
Since such noise dependencies are prevalent across a wide range of real-world applications\footnote{Additional real-world examples demonstrating the practical existence of such noise are provided in Sec~\ref{sec:DANG}.}, failing to address them can result in significant robustness gaps and impede the development of more practical and robust GNN models. However, we observe that existing robust GNN models indeed fail to generalize effectively in such noise scenario since they overlook the underlying relationships among $X$, $A$, and $Y$ within the data generation process.

% In our experiments, it is observed that existing robust GNNs perform well under existing independent noise scenarios (i.e., IFN, ISN, and ILN in Fig~\ref{fig:limitation}) but significantly deteriorates under such noise dependency (i.e., DANG in Fig~\ref{fig:limitation}) due to their limited assumptions regarding graph noise. 

 

% we argue that their practicality is restricted by an unrealistic assumption regarding graph noise: \textbf{\textit{noise in node features is independent of the graph structure or node labels}}. For example, in the conventional independent node feature noise (IFN) assumption (Fig. \ref{fig:DANG_fig}(b)) \cite{airgnn, jin2022empowering}, Bob's fake profile does not influence other nodes, which is also explained by the data generating process (DGP) of IFN (See Fig. \ref{fig:DANG_graphical}(a)) in which no causal relationships exist among the noisy node features $X$, graph structure $A$, and node labels $Y$. 


\begin{figure}
    % % \vspace{-1ex}
    \centering
    {\includegraphics[width=.8\columnwidth]{fig/fdgn_graphical_gen.pdf}}
    \vspace{-1ex}
    \caption{A directed graphical model indicating a DGP of (a) IFN, and (b) DANG.
    }
    \label{fig:DANG_graphical}
    \vspace{-4ex}
\end{figure}

% However, in reality (See Fig. \ref{fig:DANG_fig}(c)), other users like Alice and Tom may make connections with Bob based on his fake profile (i.e., structure noise), which may also eventually change their community (i.e., label noise), and such causal relationships among $X$, $A$, and $Y$ (i.e., $A \leftarrow X$, $Y \leftarrow X$, and $Y \leftarrow A$) are depicted in Fig. \ref{fig:DANG_graphical}(b). As another example in an e-commerce network, fake reviews or content on items may cause users to interact with irrelevant items, adding co-interaction edges between these items (i.e., structure noise). Consequently, this would make the automated item category labeling system to inaccurately annotate item categories (i.e., label noise), as it relies on the node features and the graph structure, both of which are contaminated. This noisy scenario is commonly encountered in user-item graphs across various domains (e.g., social networks, e-commerce, web graphs, etc), which dominate the application of graph learning methods.
% \textcolor{black}{We provide additional real-world scenarios in Sec~\ref{sec:real-ex-DANG} to highlight the practical existence of such noise.}

% These examples demonstrate that, in reality, \textit{\textbf{noisy node features may entail both structure and label noise}}. However, we observe that existing robust GNN models indeed fail to generalize effectively in such a noise scenario since they overlook the underlying relationships among $X$, $A$, and $Y$ within the data generation process.
 
To enhance the practicality of existing noise assumptions and robust GNNs, we newly introduce a novel \textbf{\underline{d}}ependency-\textbf{\underline{a}}ware \textbf{\underline{n}}oise on \textbf{\underline{g}}raphs (DANG) and propose a \textbf{\underline{d}}ependency-\textbf{\underline{a}}ware robust \textbf{\underline{g}}raph \textbf{\underline{n}}eural \textbf{\underline{n}}etwork framework (\proposed) that directly models the DGP of DANG. We first illustrate the DGP of DANG as shown in Fig. \ref{fig:DANG_graphical}(b) (c.f. Sec~\ref{sec:DANG}). More precisely, we introduce three observable variables (i.e., $X$, $A$, and $Y$) and three latent variables (i.e., noise incurring variable $\epsilon$, latent clean graph structure $Z_A$, and latent clean node labels $Z_Y$), while defining causal relationships among these variables to represent the data generation process of DANG. 
We then devise a deep generative model, \proposed, that directly captures the causal relationships among the variables in the DGP of DANG by \textbf{1)} deriving a tractable learning objective based on variational inference (c.f. Sec~\ref{sec:problem_formulation}) and \textbf{2)} addressing non-trivial technical challenges in implementing the learning objective (c.f. Sec~\ref{sec:model_ins}).
Moreover, to rigorously evaluate our proposed method, we propose both synthetic and real-world DANG benchmark datasets. In our experiments, we demonstrate that \proposed~effectively generalizes not only to DANG but also to other noise assumptions commonly considered in this field of research. This highlights \proposed’s broader applicability compared to existing robust GNN models. In summary, the main contributions of our paper are as follows:

% To verify the applicability of \proposed~to noise scenarios other than DANG, we also conduct evaluations under independent and simultaneous noise scenarios, commonly considered in recent research (c.f. Sec~\ref{sec:other_noise}).
% Although the main focus of this paper is on the node feature noise and its influence across the graph, \proposed~is capable of generalizing not only to DANG, but also to the other noise assumptions that are widely considered in various literatures (c.f. Sec~\ref{sec:discussion1}). This implies that \proposed~has a wider range of applicability than existing robust GNN models. 

% Although not all noise scenarios perfectly align with DANG, such cases manifest infrequently, as DANG is frequently encountered across various domains (c.f. Sec~\ref{sec:real-ex-DANG}) that dominate the application of graph learning methods. 

% Moreover, to rigorously evaluate our proposed method, we propose both synthetic and real-world DANG benchmark datasets (c.f. Sec~\ref{sec:DANG_experiments}). To verify the applicability of \proposed~to noise scenarios other than DANG, we also conduct evaluations under independent and simultaneous noise scenarios, commonly considered in recent research (c.f. Sec~\ref{sec:other_noise}). 

\vspace{-2ex}
\begin{itemize}[leftmargin=0.5cm]
  \item To the best of our knowledge, this is the first study to examine the gap between real-world scenarios and the overly simplistic noise assumptions underlying previous robust GNN research, which constrain their practicality.
  % \vspace{-3ex}
  \item To achieve this, we introduce a more realistic noise scenario, DANG, along with a robust model, \proposed, improving their applicability in real-world settings.
  % \vspace{-1ex}
  \item \proposed~addresses DANG by modeling its DGP, resulting in superior robustness in node classification and link prediction tasks under various noise scenarios.
  % \vspace{-1ex}
  \item We propose novel graph benchmark datasets that simulate DANG within real-world applications. To the best of our knowledge, this is the first work proposing new datasets for evaluating the robust GNNs under realistic noise scenarios that are plausible in real-world applications, which fosters practical research in robust GNNs.
  
\end{itemize}

\section{Related Work}
\label{sec:related_work}
\vspace{-1ex}
\subsection{Noise-Robust GNN}
\vspace{-1ex}
Noise-robust GNNs aim to train robust models  under feature, structure, and/or label noise, but most existing approaches focus on only one type of noise.

\vspace{-1ex}

\smallskip
\noindent \textbf{Feature noise-robust GNN.} \@ 
% While various approaches \cite{NOSMOG_ICLR, jin2022empowering} have been proposed to address the noisy node features, r
% Recent studies have highlighted the significance of fully leveraging structural information. 
AirGNN \cite{airgnn} identifies and addresses nodes with noisy features based on the hypothesizes that they tend to have dissimilar features within their local neighborhoods. {Consequently, this approach tackles the noisy node features while assuming that the structure of the input graph is noise-free.}

\vspace{-1ex}

\smallskip
% \vspace{-1ex}
\noindent \textbf{Structure noise-robust GNN.} \@ 
% Among various approaches \cite{lei2022evennet, in2023similarity}, a representative approach is based on the graph structure learning (GSL). 
RSGNN \cite{rsgnn} aims to train a graph structure learner by encouraging the nodes with similar features to be connected. STABLE \cite{stable} removes edges with low feature similarity, learns node representations from the modified structure, and constructs a kNN graph as the refined structure. {In summary, these methods tackle the noisy graph structure while assuming that node features are noise-free.}

\vspace{-1ex}

\smallskip
% \vspace{-1ex}
\noindent \textbf{Label noise-robust GNN.} \@ 
NRGNN \cite{nrgnn} connects edges between nodes with similar features, mitigating the information propagation from falsely labeled nodes. 
RTGNN \cite{rtgnn} uses small-loss approach \cite{han2018co}, but nodes with noisy features or structures exhibit large losses, leading to inaccuracies of the approach. {Therefore, these methods tackle the noisy node labels while assuming that both node features and graph structure are noise-free.}

\vspace{-1ex}

\noindent \textbf{Multifaceted noise-robust GNN.} \@ 
SG-GSR \cite{in2024self} tackles multifaceted structure and feature noise by identifying a clean subgraph within a noisy graph structure and augmenting it using label information. This augmented subgraph serves as supervision for robust graph structure refinement. However, since noisy label information can compromise the augmentation process, SG-GSR relies on the assumption that node labels are free of noise.

In summary, each method assumes the completeness of at least one of the data sources, limiting their practicality.

\vspace{-1ex}
\smallskip
\subsection{Generative Approach}
\vspace{-1ex}
\citet{yao2021instance} devises a generative approach to model the DGP of instance-dependent label noise \cite{berthon2021confidence}. However, extending this method to the graph domain introduces significant challenges. It requires handling additional latent variables and complex causal relationships, such as $Z_A$, $\epsilon_A$, $A \leftarrow \epsilon_A$, $A \leftarrow X$, $Y \leftarrow A$, and $A \leftarrow Z_A$, each posing non-trivial obstacles beyond the straightforward extension\footnote{Detailed explanation is outlined in Appendix~\ref{sec:ap_causlnl}}. WSGNN \cite{lao2022variational} and GraphGLOW \cite{zhao2023graphglow} utilize a probabilistic generative approach and variational inference to infer the latent graph structure and node labels. However, they assume noise-free graphs, reducing effectiveness in real-world noisy scenarios.

\vspace{-2ex}
\section{Dependency-Aware Noise on Graphs}
\label{sec:DANG}
\vspace{-1ex}

\noindent \textbf{Formulation.} \@
In this section, we define a new graph noise assumption, DANG, and its DGP. In Fig. \ref{fig:DANG_graphical}(b), $X$ denotes the node features (potentially noisy), $Y$ the observed node labels (possibly noisy), $A$ the observed edges (which may contain noise), and $\epsilon$ the environment variable causing the noise. $Z_Y$ represents the latent clean node labels, while $Z_A$ does the latent clean graph structure encompassing all potential node connections. We give the explanations for each causal relationship within the DGP of DANG along with the examples in user graphs in social networks:

\underline{$X \leftarrow (\epsilon, Z_Y)$}: $\epsilon$ and $Z_Y$ are causes of $X$. For example, users create their profiles and postings (i.e., $X$) regarding their true communities or interests (i.e., $Z_Y$). However, if users decide to display fake profiles for some reason (i.e., $\epsilon$), $\epsilon$ is a cause of the noisy node features $X$.
    
\underline{$A \leftarrow (Z_A, X)$}: $Z_A$ and $X$ are causes of $A$. For instance, the follow relationship among users (i.e., $A$) are made based on their latent relationships (i.e., $Z_A$). However, if a user creates a fake profile (i.e., $X$), some irrelevant users may follow the user based on his/her fake profile, which leads to noisy edges (i.e., $A$).
    
\underline{$A \leftarrow \epsilon$}: To provide a broader scope, we also posit that $\epsilon$ is a potential cause of $A$. This extension is well-founded, as real-world applications often exhibit graph structure noise originating from various sources in addition to the feature-dependent noise \cite{cogsl, slaps}.
    
\underline{$Y \leftarrow (Z_Y, X, A)$}: $Z_Y$, $X$, and $A$ are causes of $Y$. To give an example, the true communities (or interests) of users (i.e., $Z_Y$) are leveraged to promote items to targeted users within a community \cite{communitysurvey}. To detect the communities, both node features and graph structures are utilized.
However, if a user has noisy node features (i.e., $X$) or noisy edges (i.e., $A$), the user may be assigned to a wrong community (or interest), which leads to noisy labels (i.e., $Y$). 
    
For simplicity, we assume $\epsilon$ is not a cause of $Y$. {
This assumption matches real-world scenarios where mislabeling is more likely due to confusing or noisy features rather than arbitrary sources  \cite{berthon2021confidence}. 
In other words, label noise in graphs is predominantly caused by confusing or noisy features and graph structure (i.e., $Y\leftarrow (X,A)$), rather than an arbitrary external factor (i.e., $Y \nleftarrow \epsilon$).}



\noindent \textbf{Discussion.} \@
\textbf{1) Under DANG a graph does not contain any noise-free data sources.} This point presents a non-trivial challenge for the existing robust GNN methods to tackle DANG, as they assume the completeness of at least one data source. 


\textbf{2) DANG is prevalent across various domains, including social networks, e-commerce, web graphs, citation networks, and biology.} Due to space constraints, detailed scenarios are provided in Appendix \ref{sec:ap-real-world-ex}. While not all noise scenarios perfectly align with DANG, such cases are rare compared to the widespread occurrence of DANG in these domains, dominating graph applications. 

\textbf{3) DANG addresses the practical gap between real-world and the simplistic noise assumptions of previous works.} By introducing the DANG, we examine the practical limitations of existing robust GNN methods and promote further practical advancements in this field.

\vspace{-1ex}
\section{Proposed Method: \proposed}
\vspace{-1ex}
In this section, we propose a dependency-aware robust GNN framework (\proposed) that
directly models the DGP of DANG, thereby capturing the causal relationships among the variables that introduce noise. First, we derive the Evidence Lower Bound (ELBO) for the observed data log-likelihood $P(X, A, Y)$ based on the graphical model of DANG (\textbf{Sec~\ref{sec:problem_formulation}}). Subsequently, we introduce a novel deep generative model and training strategy maximizing the derived ELBO to capture the DGP of DANG (\textbf{Sec~\ref{sec:model_ins}}). 
% It is essential to highlight that our approach can handle both node classification and link prediction tasks, making it versatile and applicable in various situations. 

\vspace{-1ex}
\subsection{Problem Formulation}
\label{sec:problem_formulation}
\vspace{-1ex}
\looseness=-1
\textbf{Notations.} \@ We have an undirected and unweighted graph $\mathcal{G}= \langle \mathcal{V},\mathcal{E} \rangle$ where $\mathcal{V}=\{v_1,...,v_N\}$ represents the set of nodes and $\mathcal{E}\in \mathcal{V}\times \mathcal{V}$ indicates the set of edges. Each node $v_i$ has the node features $\mathbf{X}_i\in \mathbb{R}^{F}$ and node labels $\mathbf{Y}_i \in \{0,1\}^C$, where $F$ is the number of features for each node and $C$ indicates the number of classes. We represent the observed graph structure using the adjacency matrix $\mathbf{A}\in\mathbb{R}^{N\times N}$, where $\mathbf{A}_{ij}=1$ if there is an edge connecting nodes $v_i$ and $v_j$, and $\mathbf{A}_{ij}=0$ otherwise. Throughout this paper, $s(\cdot, \cdot)$ indicates a cosine similarity function and $\rho(\cdot)$ represents the ReLU activation function.

\noindent \textbf{Tasks: node classification and link prediction.} \@ In the node classification task, we assume the semi-supervised setting where only a portion of nodes are labeled (i.e., $\mathcal{V}^{L}$). Our objective is to predict the labels of unlabeled nodes (i.e., $\mathcal{V}^{U}$) by inferring the latent clean node label $Z_Y$. In the link prediction task, our goal is to predict reliable links based on partially observed edges by inferring the latent clean graph structure $Z_A$. It is important to note that, according to the DANG assumption, the observed node features, graph structure, and node labels may contain noise.


\noindent \textbf{Learning Objective.} \@ We adopt the variational inference framework \cite{vi_review, yao2021instance} to optimize the Evidence Lower-BOund (ELBO) of the marginal likelihood for observed data, i.e., $P(X,A,Y)$, rather than optimizing the marginal likelihood directly. Specifically, we derive the negative ELBO, i.e., $\mathcal{L}_{\text{ELBO}}$, as follows:

\vspace{-5ex}
\begin{align}
& \mathcal{L}_{\text{ELBO}} =  \nonumber \\
& - \mathbb{E}_{Z_A \sim q_{\phi_1}(Z_A|X, A)} \mathbb{E}_{\epsilon \sim q_{\phi_2}(\epsilon|X,A,Z_Y)} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right] \nonumber \\
& - \mathbb{E}_{\epsilon \sim q_{\phi_2}(\epsilon|X,A,Z_Y)} \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ \log(p_{\theta_2}(X|\epsilon,Z_Y)) \right] \nonumber \\ 
& - \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ \log(p_{\theta_3}(Y|X,A, Z_Y)) \right] \nonumber \\
& + kl(q_{\phi_3}(Z_Y|X, A) || p(Z_Y)) + kl(q_{\phi_1}(Z_A|X, A) || p(Z_A)) \nonumber \\
& + \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ kl(q_{\phi_2}(\epsilon|X,A,Z_Y) || p(\epsilon))\right].
\label{eq:elbo_loss}
% \vspace{-3ex}
\end{align}


\begin{figure*}
% \begin{wrapfigure}{R}{0.6\textwidth}
% % \vspace{-10ex}
\centering
\includegraphics[width=.75\linewidth]{fig/overall.pdf}
\vspace{-2ex}
\captionof{figure}{Overall architecture of \proposed. (a) With the noisy graph $(X,A,Y)$ as inputs, we design the inference encoders ($\phi_1$, $\phi_2$ and $\phi_3$) and regularizers ($\mathcal{L}_{\text{hom}}$, $\mathcal{L}_{\text{cls-enc}}$, and $\mathcal{L}_{\text{p}}$) to  infer $Z_A$, $Z_Y$, $\epsilon_A$, and $\epsilon_X$. (b) Leveraging the inferred latent variables, we formulate the generative decoders ($\theta_1$, $\theta_2$, and $\theta_3$) and reconstruction loss functions ($\mathcal{L}_{\text{rec-edge}}$, $\mathcal{L}_{\text{rec-feat}}$, and $\mathcal{L}_{\text{cls-dec}}$) to capture the causal relationships that generate noise in the graph. 
% \textcolor{red}{(CY: Section titles contain `encoder' and `decoder' but the figure does not contain these terms.)}
}
\label{fig:overall}
\vspace{-3ex}
% \end{wrapfigure}
\end{figure*}

\noindent where $kl(\cdot || \cdot)$ denotes KL divergence. The derivation details are provided in Appendix \ref{sec:ap_elbo_derive}. The encoders $\phi_1$, $\phi_2$, and $\phi_3$ infer three latent variables, while $\theta_{1}$, $\theta_{2}$, and $\theta_{3}$ are decoder networks used for generating three observable variables.
Our objective is to find the optimal values of $\phi=$ \{$\phi_1$, $\phi_2$, $\phi_3$\}  and $\theta=$ \{$\theta_1$, $\theta_2$, $\theta_3$\} that minimize the value of $\mathcal{L}_{\text{ELBO}}$. By doing so, the encoders and decoders are trained to directly capture the causal relationships among the variables that introduce noise. Consequently, it promotes the accurate inference of the latent clean node label $Z_Y$ and latent clean graph structure $Z_A$ to effectively perform  the node classification and link prediction tasks even in the presence of DANG.


\vspace{-1ex}
\subsection{Model Instantiations}
\vspace{-1ex}
\label{sec:model_ins}

In this section, we present the details of the practical implementation and optimization of \proposed~based on the learning objective, $\mathcal{L}_{\text{ELBO}}$. The overall architecture and detailed algorithm of \proposed~are provided in Fig~\ref{fig:overall} and Algorithm~\ref{alg:algo_method} in Appendix, respectively. The key challenge of the instantiation is how to accurately infer the latent variables $Z_A$, $Z_Y$, and $\epsilon$ in the presence of noisy $X$, $A$, and $Y$. To alleviate the challenge, we design the robust inference encoders (Fig~\ref{fig:overall}(a)) and generative decoders (Fig~\ref{fig:overall}(b)) with the corresponding regularizers (Fig~\ref{fig:overall}(a)) and reconstruction losses (Fig~\ref{fig:overall}(b)). Consequently, the encoders would be able to accurately infer the latent variables by capturing the causal relationships among the variables that introduce noise. 

\vspace{-1ex}
\subsubsection{Modeling Inference Encoder} 
\vspace{-1ex}
In this section, we describe the implementations of the encoders, i.e., $\phi_1$, $\phi_3$, and $\phi_2$, that aim to infer the latent variables, i.e., $Z_A$, $Z_Y$, and $\epsilon$, respectively.

\vspace{-1ex}
\smallskip
\noindent \underline{\textbf{Modeling} $q_{\phi_1}(Z_A|X,A)$}. The objective of modeling $q_{\phi_1}(Z_A|X,A)$ is to accurately infer the latent clean graph structure $Z_A$ that enhances the message passing of a GNN model. To this end, we design the \textcolor{black}{encoder $\phi_1$} as a graph structure learner.
More specifically, to obtain the latent graph $\mathbf{\hat{A}}=\{ \hat{a}_{ij} \}_{N\times N}$, we sample from $\hat{a}_{ij} \sim \text{Bern}(\hat{p}_{ij})$ where $\hat{p}_{ij} = \rho(s(\mathbf{Z}_i, \mathbf{Z}_j))$ and $\mathbf{Z} = \text{GCN}_{\phi_1}(\mathbf{X}, \mathbf{A})$. 
To further promote an accurate inference of $Z_A$, we regularize $Z_A$ to mainly include assortative edges \cite{zhao2023graphglow} by minimizing $kl(\hat{\mathbf{A}}||\mathbf{A}^{\text{p}})$, where $\mathbf{A}^{\text{p}}=\{ {a}^{\text{p}}_{ij} \}_{N\times N}$ and ${a}^{\text{p}}_{ij}$ is sampled from a Bernoulli distribution with a probability as given by the $\gamma$-hop subgraph similarity \cite{consm, rsgnn}. 
It is important to note that this regularization is equivalent to minimizing $kl(q_{\phi_1}(Z_A|X,A) || p(Z_A))$ in Eqn. \ref{eq:elbo_loss}.
However, computing $\hat{p}_{ij}$ in every epoch is impractical for large graphs, i.e., $O(N^2)$. To this end, we pre-define a proxy graph 
based on the subgraph similarity, and compute $\hat{p}_{ij}$ as edge weights on the proxy graph. Please refer to the Appendix \ref{sec:detail_instantiations} for detailed information on implementation details.



% \vspace{-1ex}
\smallskip
\noindent \underline{\textbf{Modeling} $q_{\phi_3}(Z_Y|X,A)$}. \@ The objective of modeling $q_{\phi_3}(Z_Y|X,A)$ is to accurately infer the latent clean node label $Z_Y$. 
To this end, we instantiate the \textcolor{black}{encoder $\phi_3$} as a GCN classifier. Specifically, we infer $Z_Y$ through $\hat{\mathbf{Y}}=\text{GCN}_{\phi_3}(\mathbf{X}, \mathbf{\hat{A}}) \in \mathbb{R}^{N \times C}$.
We introduce the node classification loss $\textcolor{purple}{\mathcal{L}_{\text{cls-enc}}} = \sum_{i\in \mathcal{V}^{L}} \text{CE}(\hat{\mathbf{Y}}_i, \mathbf{Y}_i)$, where CE is the cross entropy loss. To further enhance the quality of inference of $Z_Y$, we regularize $Z_Y$ to satisfy class homophily \cite{mcpherson2001birds} by minimizing the KL divergence between the probability predictions $\mathbf{\hat{Y}}$ of each node and its first order neighbors in $\mathbf{\hat{A}}$. The implemented loss function is given by: $\textcolor{purple}{\mathcal{L}_{\text{hom}}} = \sum_{i\in \mathcal{V}} \frac{ \sum_{j \in \mathcal{N}_i} \hat{p}_{ij} \cdot kl(\hat{\mathbf{Y}}_j || \hat{\mathbf{Y}}_i)}{ \sum_{j \in \mathcal{N}_i } \hat{p}_{ij} }$,
where $\mathcal{N}_i$ denotes the set of first-order neighbors of node $v_i$ within $\hat{\mathbf{A}}$. It is worth noting that this regularization is equivalent to minimizing $kl(q_{\phi_3}(Z_Y|X,A) || p(Z_Y))$ in Eqn. \ref{eq:elbo_loss}.

\smallskip
\noindent \underline{\textbf{Modeling} $q_{\phi_2}(\epsilon | X, A, Z_Y)$.} To model $q_{\phi_2}(\epsilon | X, A, Z_Y)$, we simplify $q_{\phi_2}(\epsilon | X, A, Z_Y)$ into \textcolor{black}{$q_{\phi_{21}}(\epsilon_X | X, Z_Y)$} and \textcolor{black}{$q_{\phi_{22}}(\epsilon_A | X, A)$}, where $\epsilon_X$ and $\epsilon_A$ are independent variables that incur the feature and structure noise, respectively. 


The objective of modeling $q_{\phi_{22}}(\epsilon_A | X, A)$ is to accurately infer the structure noise incurring variable $\epsilon_A$ that determines whether each edge is clean or noisy. To this end, we regard $\epsilon_A$ as a set of scores indicating the likelihood of each observed edge being clean or noisy. To estimate the likelihood, we utilize small loss approach \cite{han2018co}.
% draw inspiration from an early-learning phenomenon \cite{early-learning1}: during the early learning phase deep neural networks are trained to yield small loss values on the clean labeled data. 
Precisely, we compute the set of link prediction losses as $\{ (1 - \hat{p}^{el}_{ij})^2 | (i,j) \in \mathcal{E} \}$, where $\hat{p}^{el}_{ij}$ represents the $\hat{p}_{ij}$ value at the final epoch during early-learning phase. Therefore, an edge with high $\hat{p}^{el}_{ij}$ value can be considered as a clean edge, and we instantiate $\epsilon_A$ as $\{\hat{p}_{ij}^{el} | (i,j) \in \mathcal{E} \}$. To alleviate the uncertainty of a single training point's loss value, we adopt an exponential moving average (EMA) technique: $\hat{p}_{ij}^{el} \leftarrow \xi \hat{p}_{ij}^{el} + (1-\xi) \hat{p}_{ij}^c$, where $\hat{p}_{ij}^c$ indicates the value of $\hat{p}_{ij}$ at the current training point, and $\xi$ indicates the decaying coefficient fixed to 0.9. 
% Please note that we scale $\hat{p}_{ij}^{el}$ with a minimum value of 0.9 and a maximum value of 1. 
This approach is equivalent to minimizing $kl(q_{\phi_{22}}(\epsilon_A|X,A) | p(\epsilon_A))$, where $p(\epsilon_A)$ is assumed to follow the same distribution as $q_{\phi_{22}}(\epsilon_A|X,A)$ but with lower variance.

% However, the loss-based criteria $\hat{p}_{ij}^{el}$ may introduce uncertainty in identifying clean edges as it relies on a single training point's value. In other words, $q_{\phi_{22}}(\epsilon_A|X,A)$ follows an unknown distribution with high variance. To reduce uncertainty of the inferred $\epsilon_A$, we regularize $q_{\phi_{22}}(\epsilon_A|X,A)$ to follow the same distribution with low variance, i.e., $p(\epsilon_A)$, which is equivalent to reducing $kl(q_{\phi_{22}}(\epsilon_A|X,A) || p(\epsilon_A))$. To implement it, we adopt an exponential moving average (EMA) technique: $\hat{p}_{ij}^{el} \leftarrow \xi \hat{p}_{ij}^{el} + (1-\xi) \hat{p}_{ij}^c$, where $\hat{p}_{ij}^c$ indicates the value of $\hat{p}_{ij}$ at the current training point, and $\xi$ indicates the decaying hyperparameter fixed to 0.9. Please note that we scale $\hat{p}_{ij}^{el}$ with a minimum value of 0.9 and a maximum value of 1.

For the encoder $\phi_{21}$, we use an MLP that takes $X$ and $Z_Y$ as inputs and infers $\epsilon_X$. Additionally, we regularize $p(\epsilon_X)$ to follow the standard multivariate normal distribution, which means that a closed form solution of $kl(q_{\phi_{21}}(\epsilon_X|X,Z_Y) || p(\epsilon_X))$ can be obtained as $\textcolor{purple}{\mathcal{L}_{\text{p}}} = -\frac{1}{2} \sum_{j=1}^{d_2} (1+\log \sigma_j^2-\mu_j^2 - \sigma_j^2)$ \cite{kingma2013auto}, where $d_2$ is the dimension of a $\epsilon_X$. Note that these two regularization techniques are equivalent to minimizing $\mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ kl(q_{\phi_2}(\epsilon|X,A, Z_Y) || p(\epsilon)) \right]$ in Eqn. \ref{eq:elbo_loss}. 

\vspace{-1ex}
\subsubsection{Modeling Generative Decoder}
\vspace{-1ex}

In this section, we describe the implementations of the decoders, i.e., $\theta_1$, $\theta_2$, and $\theta_3$, that generate the observable variables, i.e., $A$, $X$, and $Y$, respectively.


% \vspace{-1ex}
\smallskip
\noindent \underline{\textbf{Modeling} $p_{\theta_1}(A|X, \epsilon, Z_A)$}. The probability $p(A|X,\epsilon, Z_A)$ means the likelihood of how well the noisy edge $A$ is reconstructed from the latent graph structure $Z_A$ along with $\epsilon$ and $X$. Hence, we aim to minimize $-\mathbb{E}_{Z_A \sim q_{\phi_1}} \mathbb{E}_{\epsilon \sim q_{\phi_2}} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right]$ to discover the latent graph structure $Z_A$ from which the noisy edge $A$ is reconstructed given noise sources, $X$ and $\epsilon$. We implement it as an edge reconstruction loss forcing the estimated latent structure $\hat{\mathbf{A}}$ to assign greater weights to clean edges and reduce the influence of noisy edges, which is defined as $\textcolor{purple}{\mathcal{L}_{\text{rec-edge}}}$:
% Such learning objective is equivalent to minimizing $-\mathbb{E}_{Z_A \sim q_{\phi_1}} \mathbb{E}_{\epsilon \sim q_{\phi_2}} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right]$ in Eqn. \ref{eq:elbo_loss}, which is implemented as an edge reconstruction loss forcing the estimated latent structure $\hat{\mathbf{A}}$ to assign greater weights to clean edges and reduce the influence of noisy edges,
which is defined as $\textcolor{purple}{\mathcal{L}_{\text{rec-edge}}}$:

\vspace{-5ex}
\begin{equation}
\small
\textcolor{purple}{\mathcal{L}_{\text{rec-edge}}} = \frac{N}{|\mathcal{E}| + |\mathcal{E^-}|}  \left(  \sum_{(i,j) \in \mathcal{E}} (\hat{p}^{reg}_{ij} - \hat{p}^{el}_{ij})^2 + \sum_{(i,j) \in \mathcal{E}^-} (\hat{p}_{ij} - 0)^2  \right),
\label{eq:edge_prediction_loss}
\end{equation}
\vspace{-3ex}


\noindent where $\mathcal{E}$ and $\mathcal{E}^-$ denote the positive edges and randomly sampled negative edges, respectively. To compute \textcolor{purple}{$\mathcal{L}_{\text{rec-edge}}$}, we employ regularizations on both the predictions (i.e., $\hat{p}^{reg}_{ij}$) and labels (i.e., $\hat{p}^{el}_{ij}$) 
since the observed graph structure $A$ contains noisy edges incurred by $X$ and $\epsilon$, which introduce inaccurate supervision. 

More precisely, the regularized prediction $\hat{p}^{reg}_{ij}$ is defined as: $\hat{p}^{reg}_{ij}= \theta_1 \hat{p}_{ij} + (1-\theta_1) s(\mathbf{X}_i, \mathbf{X}_j)$.
% Note that the feature similarity $s(\mathbf{X}_i, \mathbf{X}_j)$ is considered in the prediction of positive edges. 
The main idea is to penalize $\hat{p}_{ij}$ when $s(\mathbf{X}_i, \mathbf{X}_j)$ is high, as the edge between $v_i$ and $v_j$ is potentially noisy due to the influence of noisy $X$. To regularize labels, we adopt label smoothing approach by $\hat{p}_{ij}^{el} \in [0.9, 1]$, enhancing the robustness in the presence of noisy supervision.
% we utilize the inferred $\epsilon_A$, i.e., $\hat{p}_{ij}^{el} \in [0.9, 1]$. 
% In other words, 
When an edge is regarded as noisy (i.e., with a low $\hat{p}_{ij}^{el}$), its label is close to 0.9\footnote{The value 0.9 is selected following \cite{inception}.}, while an edge considered clean (i.e., with a high $\hat{p}_{ij}^{el}$) has a label close to 1. 
% Note that $\theta_1$ is a hyperparameter, and the value 0.9 in the label regularization is selected following \cite{inception}. 




% \vspace{-1ex}
\smallskip
\noindent \underline{\textbf{Modeling} $p_{\theta_2}(X|\epsilon, Z_Y))$}. The term $p(X|\epsilon, Z_Y)$ indicates how well the noisy node feature $X$ is reconstructed from the latent clean label $Z_Y$ along with $\epsilon$. Hence, we aim to minimize $-\mathbb{E}_{\epsilon \sim q_{\phi_2}} \mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ \log(p_{\theta_2}(X|\epsilon, Z_Y))\right]$. To do so, the decoder needs to rely on the information contained in $Z_Y$, which essentially encourages the value of $Z_Y$ to be meaningful for the prediction process, i.e., generating $X$.
% It is worth noting that such learning objective is equivalent to minimizing $-\mathbb{E}_{\epsilon \sim q_{\phi_2}} \mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ \log(p_{\theta_2}(X|\epsilon, Z_Y))\right]$ in Eqn. \ref{eq:elbo_loss}.
It is implemented as a feature reconstruction loss $\textcolor{purple}{\mathcal{L}_{\text{rec-feat}}}$, where the decoder $\theta_2$ is composed of an MLP that takes $\epsilon_X$ and $Z_Y$ as inputs and reconstructs node features. Note that the reparametrization trick \cite{kingma2013auto} is used for sampling $\epsilon_X$ that follows the standard normal distribution. 

% \vspace{-1ex}
\smallskip
\noindent \underline{\textbf{Modeling} $p_{\theta_3}(Y|X, A, Z_Y)$.} The term $p(Y|X, A, Z_Y)$ means the transition relationship from the latent clean label $Z_Y$ to the noisy label $Y$ of an instance, i.e., how the label noise was generated \cite{yao2021instance}. {For this reason, maximizing $\log(p_{\theta_3}(Y|X, A, Z_Y))$ would let us discover the latent true label $Z_Y$ from which the noisy label $Y$ is generated given an instance, i.e., $X$ and $A$.} Hence, we aim to maximize the log likelihood, which is implemented as minimizing a node classification loss \textcolor{purple}{$\mathcal{L}_{\text{cls-dec}}$}. Specifically, the decoder $\theta_3$
is composed of a GCN classifier: $\hat{\mathbf{Y}}_{\text{dec}} = \text{GCN}_{\theta_3}(\mathbf{X}, \mathbf{A}, \mathbf{\hat{Y}}) \in \mathbb{R}^{N \times C}$. Note that such a learning objective is equivalent to minimizing $-\mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ \log(p_{\theta_3}(Y|X, A, Z_Y))\right]$
 in Eqn. \ref{eq:elbo_loss}.

\vspace{-1ex}
\subsubsection{Model Training}
\vspace{-1ex}
The overall learning objective can be written as follows and \proposed~is trained to minimize $\mathcal{L}_{\text{final}}$:


\vspace{-5ex}
\begin{equation}
\small
\mathcal{L}_{\text{final}} = \mathcal{L}_{\text{cls-enc}} + \lambda_1 \mathcal{L}_{\text{rec-edge}} + \lambda_2 \mathcal{L}_{\text{hom}} + \lambda_3 ( \mathcal{L}_{\text{rec-feat}} + \mathcal{L}_{\text{cls-dec}} + \mathcal{L}_{\text{p}}),
\label{eq:final_loss}
\end{equation}
where $\lambda_1$ and $\lambda_2$ are the balancing coefficients. $\lambda_3$ is fixed to 0.001. \textcolor{black}{In our pilot experiments, $\mathcal{L}_{\text{rec-feat}}$, $\mathcal{L}_{\text{cls-dec}}$, and $\mathcal{L}_{\text{p}}$ terms have a relatively minor impact on the model's performance compared to the others.
% , leading to the share the same coefficient $\lambda_3$, since we empirically observed that these three terms have a relatively minor impact on the model's performance compared to the others. 
As a result, we have made a strategic decision to simplify the hyperparameter search process and improve the practicality of \proposed~by sharing the coefficient $\lambda_3$ among these three loss terms.} 

% \vspace{-3ex}
% \section{Discussion}
% % \vspace{-1ex}
% \label{sec:discussion}
% % \vspace{-1ex}

% \subsection{Additional Real-world Examples of DANG}
% \label{sec:real-ex-DANG}
% \todo{SHORTEN NEEDED.}
% Throughout this paper, we argue that DANG is commonly encountered in user-item graphs across various domains. 
% \textbf{DANG is encountered in various domain, such as social networks, e-commerce, web, citation, and biology.} We provide detailed scenarios in Appendix \ref{sec:ap-real-world-ex} due to the space limit. These examples dominate the graph applications, which justifies the practicality of DANG. Although not all noise scenarios perfectly align with DANG, such cases manifest infrequently. Rather, our contribution lies in addressing the practical gap between real-world applications and the overly simplistic noise assumptions of previous robust GNN methods. By introducing the DANG and \proposed, we enhance the practical applicability of robust GNN methods.

% Although not all noise scenarios perfectly align with DANG, such cases manifest infrequently, as DANG is frequently encountered across various domains that dominate the application of graph learning methods. Rather, our contribution lies in addressing the practical gap between the real-world applications and noise assumptions indiscriminately considered in previous robust graph learning methods by introducing a more realistic noise scenario, DANG, and graph learning method, \proposed, thereby enhancing their real-world applicability.

% recognizing the critical limitation of noise assumptions in previous methods, which hinders their practicality, and enhancing their real-world applicability.


% \vspace{-2ex}
% \subsection{Applicability of DANG and \proposed.}
% \label{sec:discussion1}


% \begin{figure}
%     % \raisebox{5pt}[\dimexpr\height-0.6\baselineskip\relax]
%     % % \vspace{-3ex}
%     \centering
%     {\includegraphics[width=1\linewidth]{fig/limitation.pdf}}
%     % \vspace{-5ex}
%     \caption{Node classification performance on Cora dataset. \textit{+ FN}, \textit{+ SN}, and \textit{+ LN} indicate the noise in node features, graph structures, and node labels that the existing works considered, respectively.}
%     \label{fig:limitation}
%     % \vspace{-3ex}
% \end{figure}

% % \vspace{-1ex}
% \subsection{Preliminary Analysis on DANG. Should be shortened.}
% \label{sec:DANG_fail}




% We conduct an analysis to examine how well existing robust GNN models generalize to DANG. We generate three types of noise: random feature noise (RFN) \cite{airgnn}, random structure noise (RSN) \cite{stable}, and random label noise (RLN) \cite{nrgnn, rtgnn}, following the convention of the existing studies. 
% As baselines, we consider (a) AirGNN as a feature noise-robust graph learning method, (b) RSGNN and STABLE as structure noise-robust graph learning methods, and (c) NRGNN and RTGNN as label noise-robust graph learning methods. A comprehensive description of each method is provided in Sec. \ref{sec:related_work}.
% We observe that, in Fig. \ref{fig:limitation}, existing robust GNN models perform well under RFN, RSN, and RLN but significantly drop under DANG. In contrast, \proposed~performs competitively under random noise and outperforms baselines under DANG.
% We observe that while existing robust GNN models perform well under the RFN (in Fig. \ref{fig:limitation}(a)), RSN (in Fig. \ref{fig:limitation}(b)), and RLN (in Fig. \ref{fig:limitation}(c)), their performance significantly drops under DANG (i.e., + DANG in Fig. \ref{fig:limitation}). In contrast, \proposed~demonstrates competitive results under the random noise scenarios, while notably outperforming the baselines under DANG. 

% This points to a key distinction -- existing noise-robust graph learning methods struggle to generalize to DANG due to their limited assumptions regarding the graph noise. Specifically, as summarized in Sec. \ref{sec:related_work}, each category of methods assumes at least one of the data sources is noise-free. Nevertheless, the causal relationships among $X$, $A$, and $Y$ within the data generation process of DANG gives rise to scenarios involving concurrent feature, structure, and label noise. Consequently, existing robust GNN models fall short of effectively generalizing to DANG, as they overlook such underlying relationships among noise types, leading to model designs assuming the completeness of at least one data source. These results bring us necessity of robust framework under such realistic noise.

% Conversely, \proposed~directly captures the underlying relationships by modeling the DGP of DANG, resulting in superior generalization to DANG.

% \vspace{-2ex}




\vspace{-1ex}
\section{Experiments}
\vspace{-1ex}
% \subsection{Experimental Settings}
% \vspace{-1ex}
\noindent\textbf{Datasets. } \@
We evaluate~\proposed~and baselines on \textit{five commonly used benchmark datasets} and \textit{two newly introduced datasets}, Auto and Garden, which are generated upon Amazon review data \cite{amazondata1, amazondata2} to mimic DANG on e-commerce systems (Refer to Appendix \ref{sec:ap-real-DANG-gen} for details). The details of the datasets are given in Appendix \ref{sec:ap_dataset}. 

\smallskip
% \vspace{-1ex}
\noindent\textbf{Experimental Details. } \@
We evaluated \proposed~in both node classification and link prediction tasks, comparing it with noise-robust GNNs and generative GNN methods. For a thorough evaluation, we create synthetic and real-world DANG benchmark datasets, with details in Appendix \ref{sec:ap-DANG-gen}. We also account for other noise scenarios, commonly considered in this research field, following \cite{stable, airgnn, rtgnn}. 
% Due to the space limit, we provide additional analyses on the model robustness under independent structure/feature/label noise in Appendix \ref{sec:fign_}. 
Further details about the baselines, evaluation protocol, and implementation details can be found in Appendix \ref{sec:ap_baselines}, \ref{sec:ap_eval_protocol}, and \ref{sec:ap_imple_detail}, respectively.

% \vspace{-1ex}
\begin{table*}[h]
\centering
% \small
\captionsetup{width=0.9\textwidth}
\caption{Node classification accuracy (\%) under synthetic DANG. OOM indicates out of memory on 24GB RTX3090.}
% \vspace{-3ex}
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{c|c|cccccccccc|c}
\toprule
 Dataset & Setting & WSGNN & GraphGLOW &	AirGNN & ProGNN &  RSGNN & STABLE & EvenNet & NRGNN & RTGNN & SG-GSR &	\proposed \\
\midrule
\multirow{4}{*}{Cora} 
& Clean     & \textbf{86.2±0.1}  & 85.2±0.7 & 85.0±0.2   & 85.3±0.4 & \textbf{86.2±0.5} & 86.1±0.2 & \textbf{86.2±0.0} & \textbf{86.2±0.2} & 86.1±0.2 & 85.7±0.1 & \textbf{86.2±0.7} \\



& DANG-10\% & 80.7±0.3    & 79.7±0.2 &    79.7±0.5   & 79.6±0.7 & 81.9±0.3 & 82.2±0.7 & 80.7±0.7          & 81.0±0.5 & 81.8±0.3 & 82.7±0.1 & \textbf{82.9±0.6} \\



& DANG-30\% & 70.0±0.6    & 71.6±0.5 &     71.5±0.8   & 74.5±0.1 & 71.9±0.5 & 74.3±0.3 & 65.2±1.7          & 73.5±0.8 & 72.6±1.5 & 76.1±0.2 & \textbf{78.2±0.3} \\



& DANG-50\% & 55.9±1.1    & 59.6±0.1 &     56.2±0.8   & 66.4±0.4 & 59.9±0.5 & 62.8±2.4 & 47.1±1.8          & 61.9±1.4 & 60.9±0.4 & 64.3±0.5 & \textbf{69.7±0.6} \\



\midrule 
\multirow{4}{*}{Citeseer} 
& Clean     & 76.6±0.6    & 76.5±1.0 &     71.5±0.2   & 72.6±0.5 & 75.8±0.4 & 74.6±0.6 & 76.4±0.5          & 75.0±1.3 & 76.1±0.4 & 75.3±0.3 & \textbf{77.3±0.6} \\



& DANG-10\% & 72.8±0.8    & 71.4±0.8 &     66.2±0.7   & 67.5±0.6 & 73.3±0.5 & 71.5±0.3 & 71.1±0.4          & 71.9±0.3 & 73.2±0.2 & 74.2±0.5 & \textbf{74.3±0.9} \\



& DANG-30\% & 63.3±0.7     & 60.6±0.2 &    58.0±0.4   & 61.0±0.2 & 63.9±0.5 & 62.5±1.4 & 61.2±0.6          & 62.5±0.7 & 64.2±1.9 & \textbf{65.6±1.0} & \textbf{65.6±0.6} \\



& DANG-50\% & 53.4±0.6     & 48.8±0.6 &    50.0±0.6   & 53.3±0.2 & 55.3±0.4 & 54.7±1.7 & 47.2±1.1          & 52.6±0.9 & 54.2±1.8 & 54.8±1.8 & \textbf{59.0±1.8} \\



\midrule 
\multirow{4}{*}{Photo} 
& Clean     & 92.9±0.3     & 94.2±0.4 &    93.5±0.1   & 90.1±0.2 & 93.6±0.8 & 93.4±0.1 & 94.5±0.4          & 90.3±1.7 & 91.3±0.6 & 94.3±0.1 & \textbf{94.8±0.3}          \\



& DANG-10\% & 83.9±1.8      & 92.1±0.8 &   87.3±0.9   & 84.3±0.1 & 92.1±0.2 & 92.2±0.1 & 92.6±0.0          & 84.3±1.3 & 89.4±0.5 & 93.0±0.1 & \textbf{93.2±0.2} \\



& DANG-30\% & 51.9±6.8    & 88.4±0.2 &     67.8±4.3   & 74.7±0.2 & 86.6±1.0 & 88.0±1.0 & 89.6±0.2          & 69.0±2.2 & 86.4±0.5 & 89.3±0.3 & \textbf{90.5±0.4} \\



& DANG-50\% & 31.9±5.6      & 85.4±0.6 &   57.8±0.7   & 48.9±0.5 & 75.6±2.6 & 80.2±1.8 & 84.6±0.4          & 57.5±1.8 & 79.2±0.3 & 84.1±0.4 & \textbf{87.6±0.2} \\



\midrule 
\multirow{4}{*}{Comp} 
& Clean     & 83.1±3.1      &  91.3±0.9 &  83.4±1.2   & 83.9±0.8 & 91.1±0.1 & 90.2±0.2 & 90.1±0.2          & 87.5±1.0 & 87.3±1.0 & 91.3±0.7 & \textbf{92.2±0.0} \\



& DANG-10\% & 75.0±1.2    & 88.0±0.7 &  76.8±1.8   & 72.0±0.2 & 88.1±0.7 & 85.9±0.5 & 87.6±0.7          & 85.7±0.9 & 85.9±0.1 & 89.5±0.5 & \textbf{89.8±0.2} \\



& DANG-30\% & 48.5±5.8     & 84.9±0.4 &   59.2±0.9   & 66.9±0.8 & 81.7±0.2 & 80.4±1.0 & 84.8±0.5          & 74.8±3.5 & 77.0±1.5 & 84.5±0.4 & \textbf{86.9±0.3} \\



& DANG-50\% & 39.6±4.0      & 80.1±0.5 &  44.1±1.4   & 43.3±0.3 & 73.9±2.3 & 68.8±1.3 & 77.5±1.9          & 65.3±3.2 & 69.4±0.3 & 78.6±0.6 & \textbf{82.2±0.4}
 \\

\midrule 
\multirow{4}{*}{Arxiv} 
& Clean     & OOM      &  OOM &  58.0±0.4  & OOM & OOM & OOM & 65.7±0.6           & OOM & 60.4±0.5& OOM & \textbf{67.4±0.4} \\

& DANG-10\% & OOM      &  OOM &   50.6±0.5  & OOM & OOM & OOM & 58.4±1.2           & OOM & 54.3±0.4 & OOM&\textbf{59.7±0.8}
 \\
& DANG-30\% & OOM      &  OOM &   36.8±0.3  & OOM & OOM & OOM & 47.4±2.5           & OOM & 45.0±0.6 & OOM &\textbf{49.9±0.5}
 \\
& DANG-50\% & OOM      &  OOM &   26.1±0.2  & OOM & OOM & OOM & 38.0±4.1           & OOM & 38.4±0.8 & OOM &\textbf{44.0±1.2}\\
\bottomrule

\end{tabular}
}
\label{tab:synthetic_DANG_node}
\vspace{-4ex}
\end{table*}


\vspace{-2ex}
\begin{table*}
\centering
\caption{Node classification (NC) and link prediction (LP) under real-world DANG (Accuracy for NC and ROC-AUC for LP).}
\resizebox{0.75\linewidth}{!}{
\begin{tabular}{c|c|c|cccccccccc|c}
\toprule
 Task & Dataset & Setting & WSGNN & GraphGLOW &	AirGNN & ProGNN &  RSGNN & STABLE & EvenNet & NRGNN & RTGNN & SG-GSR & 	\proposed \\
\midrule
\multirow{4}{*}{NC}
& \multirow{2}{*}{Auto}  & Clean     & 71.8±4.3 & 77.9±1.2 & 69.5±0.8 & 63.2±0.2 & 69.5±0.4 & 71.6±0.9 & 73.4±0.5 & 74.3±0.8 & 76.4±0.2 & 78.3±0.3 & \textbf{79.3±0.2} \\
&  & + DANG     & 57.7±1.3 & 59.4±0.8 & 53.9±0.1 & 48.6±0.3 & 56.8±0.9 & 57.5±0.2 & 57.1±2.1 & 55.8±1.0 & 59.6±0.8 & \textbf{62.0±1.1} & 61.4±0.4 \\
\cmidrule{2-14}
& \multirow{2}{*}{Garden} & Clean     & 87.4±0.2 & 88.5±0.9 & 78.3±1.5 & 78.7±0.1 & 83.3±1.2 & 84.2±0.5 & 85.7±0.5 & 87.7±0.4 & 87.8±0.2 & 88.1±0.3 & \textbf{88.7±0.3} \\
&  & + DANG     & 77.6±0.8 & 78.1±1.5 & 66.1±1.7 & 73.0±0.4 & 76.2±0.5 & 77.2±3.3 & 75.6±2.4 & 76.1±0.2 & 76.0±0.6 & \textbf{80.2±0.4} & \textbf{80.2±0.8 }\\
\midrule
\multirow{4}{*}{LP} 
& \multirow{2}{*}{Auto} & Clean    & 81.8±0.1 & 86.2±0.3 & 60.2±0.2 & 74.8±0.3 & 87.2±0.8 & 78.6±0.1 & 86.8±0.1 & 76.6±1.3 & 84.4±0.1 & 82.2±8.3 &\textbf{88.2±0.3} \\
&  & + DANG      & 69.1±0.6 & \textbf{74.8±0.2} & 57.9±0.4 & 56.7±0.5 & 65.0±0.2 & 57.3±0.1 & 70.5±0.2 & 47.5±1.7 & 72.2±0.2 & 65.6±7.4 &73.6±0.6 \\
\cmidrule{2-14}
& \multirow{2}{*}{Garden} & Clean     & 84.7±0.2 & 90.2±0.5 & 62.0±0.1 & 83.5±0.6 & 91.2±0.4 & 85.2±0.2 & 89.2±0.3 & 87.0±0.9 & 90.4±0.3 & 89.2±3.8 & \textbf{92.6±0.2} \\
&  & + DANG     & 84.6±0.7 & 90.1±0.4 & 58.2±0.5 & 83.3±0.5 & 91.2±0.5 & 85.0±0.1 & 90.0±0.7 & 58.6±4.5 & 90.4±0.2 & 86.0±7.2 & \textbf{92.4±0.4} \\
\bottomrule 
\end{tabular}
}
\label{tab:real_world_DANG}
\vspace{-3ex}
\end{table*}







% \begin{figure*}[h]
%   \begin{minipage}{0.22\textwidth}
%     \centering
%     \includegraphics[width=1.\linewidth]{fig/fin_feat.pdf}
%     % \vspace{-5ex}
%     \captionsetup{width=0.9\textwidth}
%     \caption{Node classification accuracy under independent feat. noise.}
%     \label{fig:fin_feat}
%   \end{minipage}
%   % \hfill
%   \begin{minipage}{0.22\textwidth}
%     \centering
%     \includegraphics[width=1.\linewidth]{fig/fin_struc.pdf}
%     % \vspace{-5ex}
%     \captionsetup{width=0.9\textwidth}
%     \caption{Node classification accuracy under independent stru. noise.}
%     \label{fig:fin_struc}
%   \end{minipage}
%   \begin{minipage}{0.22\textwidth}
%     \centering
%     \includegraphics[width=1\linewidth]{fig/fin_label.pdf}
%     % \vspace{-5ex}
%     \captionsetup{width=0.9\textwidth}
%     \caption{Node classification accuracy under independent label noise.}
%     \label{fig:fin_label}
%   \end{minipage}
%   \begin{minipage}{0.3\textwidth}
%     \centering
%     \includegraphics[width=1\linewidth]{fig/fin_all.pdf}
%     % \vspace{-5ex}
%     \captionsetup{width=0.9\textwidth}
%     \caption{Node classification accuracy under simultaneous noise.}
%     \label{fig:fin_all}
%   \end{minipage}
%   % \vspace{-2ex}
% \end{figure*}

% \vspace{-1ex}
\subsection{Main Results}
\label{sec:DANG_experiments}
\vspace{-1ex}
\textbf{1) \proposed~demonstrates superior robustness compared to baseline methods in handling noise dependencies represented by DANG.} We first evaluate \proposed~under synthetic DANG datasets. Table~\ref{tab:synthetic_DANG_node} shows that \proposed~consistently outperforms all baselines in DANG scenarios, especially when noise levels are high. This superiority is attributed to the fact that \proposed~captures the causal relationships involved in the DGP of DANG, while the baselines overlook such relationships, leading to their model designs assuming the completeness of at least one data source. 
% Additionally, \proposed~performs well even in clean graph settings. We attribute this to the accurate inference of $\epsilon_A$, which is utilized as the label regularization in calculating $\mathcal{L}_{\text{rec-edge}}$. Specifically, in Fig~\ref{fig:qual_cora}(a), we observe that the $\hat{p}_{ij}^{el}$ values estimated from the clean graph tend to be close to 1, while those from the graph with DANG are considerably smaller. Recall that the high value of $\hat{p}_{ij}^{el}$ indicates the model regards the edge $(i,j)$ as a clean edge. This suggests that \proposed~has the capability to adapt its model learning to the level of noise present in the input graph, resulting in superior performance on both clean and noisy graphs.
Moreover, we investigate the robustness under our proposed real-world DANG datasets, Auto and Garden, that we simulate noise dependencies within e-commerce systems. In Table \ref{tab:real_world_DANG}, we observe that \proposed~outperforms the baselines under real-world DANG on both the node classification and link prediction tasks. This indicates that \proposed~works well not only under artificially generated noise, but also under noise scenarios that are plausible in real-world applications. 

\textbf{2) \proposed~also shows comparable or better performance than baselines under other noise scenarios, commonly considered in this research field.} Specifically, we evaluate the robustness of \proposed~under commonly utilized node feature noise \cite{airgnn}, structure noise \cite{stable}, and node label noise scenarios \cite{nrgnn} on Cora dataset\footnote{Additional results on other datasets are outlined in Appendix~\ref{sec:ap_citeseer}}. In Fig~\ref{fig:other_noise_cora}, we observe \proposed~shows consistent superiority or competitive performance compared to existing robust GNNs. We attribute the robustness of \proposed~under the noise in node features to the graph structure learning module that accurately infers the latent graph structure $Z_A$. The utilization of abundant local neighborhoods acquired through the inference of $Z_A$ enables effective smoothing for nodes with noisy features, leveraging the information within these neighborhoods. We attribute the effectiveness of \proposed~under the noise in graph structures to inferring the robust latent clean graph structure. In other words, the inference of the latent clean graph structure $Z_A$ assigns greater weights to latent clean edges and lower weights to observed noisy edges by employing regularizations on both the edge predictions and labels, thereby mitigating structural noise. For the noise in node labels, we argue that the effectiveness of \proposed~stems from the accurate inference of the latent clean structure. Specifically, the inferred latent node label $Z_Y$ is regularized using the inferred latent structure $Z_A$ to meet the homophily assumption (i.e., $\mathcal{L}_{\text{hom}}$). Leveraging the clean neighbor structure, this regularization technique has been demonstrated to effectively address noisy labels \cite{iscen2022consistency}.




\textbf{3) \proposed~outperforms all baselines under an extreme noise scenario.} In addition to a single type of noise, we explore a more challenging noise scenario where all three types of noises occur simultaneously, denoted as extreme noise. 
It is important to note that each type of noise does not affect the occurrence of the other types of noise, in contrast to DANG. In Fig~\ref{fig:extreme_noise}, \proposed~consistently outperforms the robust GNNs under extreme noise. 

\textbf{SUMMARY: \proposed~has a broader range of applicability than the existing robust GNNs under various noise scenarios.} 
Based on the above results, we assert that modeling the DGP of DANG offers significant advantages for robustness, both under DANG and independently occurring feature, structure, or label noise, as \proposed~is inherently capable of handling each type of noise. In contrast, the baseline methods assume the completeness of at least one of the data sources, resulting in a significant performance drop when the noise rate is high.



\begin{figure}
    % \raisebox{5pt}[\dimexpr\height-0.6\baselineskip\relax]
    % % \vspace{-4ex}
    \centering
    {\includegraphics[width=1.\columnwidth]{fig/fin_icml.pdf}}
    \vspace{-2ex}
    \caption{Node classification under node feature noise, structure noise, and node label noise scenarios, which are commonly considered in robust GNN research field, on Cora dataset.}
    \label{fig:other_noise_cora}
    \vspace{-3ex}
\end{figure}


\begin{figure}
    % \raisebox{5pt}[\dimexpr\height-0.6\baselineskip\relax]
    % % \vspace{-4ex}
    \centering
    {\includegraphics[width=0.8\columnwidth]{fig/fin_all.pdf}}
    \vspace{-2ex}
    \caption{Node classification under extreme noise scenario.}
    \label{fig:extreme_noise}
    \vspace{-4ex}
\end{figure}
% \subsection{Model Analyses}



\begin{figure*}[h]
    \centering
  \begin{minipage}{0.17\linewidth}
    \centering
    \includegraphics[width=.9\textwidth]{fig/fdgn_graphical_ab.pdf}
    \vspace{-3ex}
    \captionsetup{width=1.0\linewidth}
    \caption{Graphical models of DGPs derived from DANG.}
    \label{fig:ablation_dgp}
  \end{minipage}%
  \begin{minipage}{.6\linewidth}
    \captionsetup{width=\linewidth}
    \captionof{table}{Ablation studies regarding various DGPs in Fig~\ref{fig:ablation_dgp}. In Case 3, the causal relationship $Y \leftarrow (X,A)$ is removed from the DGP of DANG (See Fig~\ref{fig:DANG_graphical}(b)). In Case 2, $A \leftarrow X$ is additionally removed. In Case 1, $A \leftarrow \epsilon$ is additionally removed, which is equivalent to the DGP of IFN (See Fig~\ref{fig:DANG_graphical}(a)). }
    \label{tab:ablation_dgp}
    % \vspace{-2ex}
      \centering
      \resizebox{0.9\textwidth}{!}{\begin{tabular}{c|c|ccc|c}
% \noalign{\smallskip}{\smallskip}
\toprule
 Dataset & Setting & (a) Case 1 &	(b) Case 2 & (c) Case 3 & \proposed\\
\midrule
\multirow{4}{*}{Cora} 
& Clean        & 84.6±0.4 & 84.8±0.4 & \textbf{86.2±0.2} & \textbf{86.2±0.7} \\




& DANG-10\% & 77.4±0.3 & 77.3±0.3 & \textbf{83.2±0.3} & 82.9±0.6          \\



& DANG-30\% & 68.3±0.4 & 68.5±0.2 & 77.3±0.4          & \textbf{78.2±0.3} \\



& DANG-50\% & 55.2±0.2 & 56.1±0.3 & 68.7±0.3          & \textbf{69.7±0.6} \\



\midrule 
\multirow{4}{*}{Citeseer} 
& Clean     & 76.7±0.9 & 76.8±0.8 & 76.5±0.9          & \textbf{77.3±0.6} \\



& DANG-10\% & 69.5±0.3 & 69.5±0.4 & 73.2±0.1          & \textbf{74.3±0.9} \\



& DANG-30\% & 57.2±1.1 & 57.7±0.5 & 65.5±0.7          & \textbf{65.6±0.6} \\



& DANG-50\% & 49.2±0.5 & 48.7±0.2 & 57.6±2.5          & \textbf{59.0±1.8} \\

\bottomrule 

\end{tabular}}
    \end{minipage}
\begin{minipage}{0.22\linewidth}
    \centering
    \includegraphics[width=.9\textwidth]{fig/qualitative_cora.pdf}
    \vspace{-3ex}
    \captionsetup{width=1.\linewidth}
    \caption{(a) Distribution of $\hat{p}_{ij}^{el}$ values. (b) Distribution of $\hat{p}_{ij}$ values under DANG-50\%. Dashed lines are averages. Cora dataset is used.}
    \vspace{-3ex}
\label{fig:qual_cora}
  \end{minipage}
    % % \vspace{-4ex}
% \vspace{-1ex}
\end{figure*}


\vspace{-1ex}
\subsection{Ablation Studies}
\vspace{-1ex}
To emphasize the importance of directly capturing the causal relationships among variables in the DGP of DANG, i.e., $Y \leftarrow (X,A)$, $A \leftarrow X$, and $A \leftarrow \epsilon$, we remove them one by one from the graphical model of DANG (See Fig~\ref{fig:DANG_graphical}(b), and then design deep generative models based on the DGPs in a similar manner to \proposed. The graphical models of the derived DGPs are illustrated in Fig~\ref{fig:ablation_dgp}. In Table \ref{tab:ablation_dgp}, we observe that as more causal relationships are removed from the DGP of DANG, the node classification performance decreases. Below, we offer explanations for this observation from the perspective of model derivation.

\textcolor{blue}{\textbf{1)}} Removing the causal relationships $Y \leftarrow (X,A)$, i.e., Fig~\ref{fig:ablation_dgp}(c), simplifies $-\mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ \log(p_{\theta_3}(Y|X, A, Z_Y))\right]$ to $-\mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ \log(p_{\theta_3}(Y|Z_Y))\right]$. This simplification hinders the accurate modeling of the label transition relationship from $Z_Y$ to the noisy label $Y$, resulting in a degradation of model performance under DANG. 

\textcolor{blue}{\textbf{2)}} Additionally, when excluding the causal relationship $A \leftarrow X$, i.e., Fig~\ref{fig:ablation_dgp}(b), the inference of $Z_A$ and $Z_Y$ is simplified as follows: $q_{\phi_1}(Z_A | X, A)$ to $q_{\phi_1}(Z_A | A)$ and $q_{\phi_3}(Z_Y | X, A)$ to $q_{\phi_3}(Z_Y | X)$. Furthermore, the loss term $-\mathbb{E}_{Z_A \sim q_{\phi_1}} \mathbb{E}_{\epsilon \sim q_{\phi_2}} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right]$ is also simplified to $-\mathbb{E}_{Z_A \sim q_{\phi_1}} \mathbb{E}_{\epsilon \sim q_{\phi_2}} \left[ \log(p_{\theta_1}(A|\epsilon, Z_A))\right]$. These simplifications significantly hinder the accurate inference of $Z_A$ and $Z_Y$, resulting in a notable performance degradation. 

\textcolor{blue}{\textbf{3)}} Eliminating the causal relationship $A \leftarrow \epsilon$, as in Fig~\ref{fig:ablation_dgp}(a), simplifies $-\mathbb{E}_{Z_A \sim q_{\phi_1}} \mathbb{E}_{\epsilon \sim q_{\phi_2}} \left[ \log(p_{\theta_1}(A|\epsilon, Z_A))\right]$ to $-\mathbb{E}_{Z_A \sim q_{\phi_1}} \mathbb{E}_{\epsilon \sim q_{\phi_2}} \left[ \log(p_{\theta_1}(A|Z_A))\right]$. This simplification hinders the robustness of the inferred $Z_A$, since the simplified loss excludes label regularization from the model training process, ultimately resulting in performance degradation.


\vspace{-1ex}
\subsection{Qualitative Analysis}
\vspace{-1ex}
We qualitatively analyze how well \proposed~infers the latent variables $\epsilon_A$ and $Z_A$. In Fig~\ref{fig:qual_cora}(a), we investigate the inference of $\epsilon_A$ by comparing the distribution of $\hat{p}_{ij}^{el}$ values estimated during training on clean and noisy graphs (DANG-50\%). We observe that $\hat{p}_{ij}^{el}$ values estimated from the clean graph tend to be close to 1, while those from the graph with DANG are considerably smaller. It suggests the inference of $\epsilon_A$ was accurate, as the high values of $\hat{p}_{ij}^{el}$ indicate that the model recognizes the edge $(i,j)$ as a clean edge. 

In Fig~\ref{fig:qual_cora}(b), we analyze the inference of $Z_A$ by comparing the distribution of $\hat{p}_{ij}$ values, which constitute the estimated latent graph structure $\hat{\mathbf{A}}$, between noisy edges and the original clean edges. It is evident that the estimated edge probabilities $\hat{p}_{ij}$ for noisy edges are predominantly assigned smaller values, while those for clean edges tend to be assigned larger values. It illustrates \proposed~effectively mitigates the impact of noisy edges during the message-passing process, thereby enhancing its robustness in the presence of noisy graph structure. This achievement can be attributed to the label regularization effect achieved through the accurate inference of $\epsilon_A$. Specifically, as the observed graph structure contains noisy edges, the inaccurate supervision for $\mathcal{L}_{\text{rec-edge}}$ impedes the distinction between noisy edges and the original clean edges in terms of edge probability values $\hat{p}_{ij}$. However, the label regularization technique proves crucial for alleviating this issue, benefitting from the accurate inference of $\epsilon_A$.



\vspace{-2ex}
\subsection{Complexity Analysis}
\label{sec:complexity}
\vspace{-1ex}

We provide both theoretical and empirical complexity analyses of training \proposed. Our findings show that \proposed~achieves superior performance compared to baseline methods while maintaining acceptable training times. For a detailed discussion and comprehensive results, refer to Appendix~\ref{sec:ap_complexity}.
% Moreover, we consider a larger graph dataset, ogbn-arxiv \cite{hu2020open}. Table~\ref{tab:synthetic_DANG_node} clearly illustrates that \proposed~exhibits superior scalability and robustness in comparison to other baseline methods.



\vspace{-1ex}
\subsection{Sensitivity Analysis} 
\vspace{-1ex}

We analyze the sensitivity of hyperparameters $\lambda_1$, $\lambda_2$, $\theta$, and $\gamma$. Our observations indicate that while each hyperparameter reaches an optimal point, the overall performance of \proposed~consistently surpasses that of the state-of-the-art baseline and demonstrates robustness to variations in hyperparameter values. For a more comprehensive discussion and detailed results, please see Appendix \ref{sec:sensitivity}.

% We observe that each hyperparameter has its own optimal point while consistently outperforming the state-of-the-art baseline, not being sensitive to the hyperparameter values. This result shows that the performance of \proposed~is not sensitive to the choice of hyperparameters. For more detailed explanation and results, please refer to Appendix \ref{sec:sensitivity}.





\vspace{-1ex}
\section{Conclusion}
\label{sec:con_lim}
\vspace{-1ex}
This study investigates the practical gap between real-world scenarios and the simplistic noise assumptions in terms of node features underlying previous robust GNN research. 
To bridge this gap, we newly introduce a more realistic graph noise scenario called dependency-aware noise on graphs (DANG), and present a deep generative model, \proposed, that effectively captures the causal relationships among variables in the DGP of DANG. We also propose novel graph benchmarks that simulate DANG within real-world applications, which fosters practical research in this field. We demonstrate \proposed~has a broader applicability than the existing robust GNNs under various noise scenarios. 

\vspace{-1ex}
\section{Limitations and Future Works}
\vspace{-1ex}
While the proposed DANG and \proposed~demonstrate broader applicability compared to existing works, they do not perfectly cover all possible noise scenarios. One potential direction to enhance their practicality is to incorporate the causal relationship $X \leftarrow A$, which suggests that graph structure noise can inevitably lead to node feature noise—an occurrence that may manifest in certain real-world scenarios. By accounting for this relationship, a broader range of noise scenarios could be addressed, further improving practical applicability. A detailed discussion on this topic is provided in Appendix \ref{sec:ap_dis}.

\section{Impact Statements}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
% \nocite{langley00}


\bibliography{refer}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Derivation Details of Evidence Lower BOund (ELBO)}
\label{sec:ap_elbo_derive}
We commence by modeling joint distribution $P(X,A,Y)$. We assume that the joint distribution $P(X,A,Y)$ is differentiable nearly everywhere regarding both $\theta$ and the latent variables ($\epsilon, Z_A, Z_Y$). Note that the generative parameter $\theta$ serves as the decoder network that models the distribution $P(X, A, Y)$.
The joint distribution of $P(X,A,Y)$ can be represented as: 

% % \vspace{-3ex}
\begin{equation}
% \small
p_{\theta}(X,A,Y)= \int_{\epsilon} \int_{Z_A} \int_{Z_Y} p_{\theta}(X,A,Y,\epsilon,Z_A,Z_Y)d\epsilon dZ_AdZ_Y.
\end{equation}
% % \vspace{-2ex}


However, computing this evidence integral is either intractable to calculate in closed form or requires exponential time. As the evidence integral is intractable for computation, calculating the conditional distribution of latent variables $p_{\theta}(\epsilon, Z_A, Z_Y | X,A,Y)$ is also intractable:

% % \vspace{-3ex}
\begin{equation}
% \small
p_{\theta}(\epsilon, Z_A, Z_Y | X,A,Y)= \frac{p_{\theta}(X, A, Y, \epsilon, Z_A, Z_Y)}{p_{\theta}(X, A, Y)}.
\end{equation}
% % \vspace{-2ex}


To infer the latent variables, we introduce an inference network $\phi$ to model the variational distribution $q_{\phi}(\epsilon, Z_A, Z_Y | X, A, Y)$, which serves as an approximation to the posterior $p_{\theta}(\epsilon, Z_A, Z_Y | X,A,Y)$. To put it more concretely, the posterior distribution can be decomposed into three distributions determined by trainable parameters $\phi_1$, $\phi_2$, and $\phi_3$. Based on the observed conditional independence relationships \footnote{We observe the following conditional independence relationships in Fig. \ref{fig:DANG_graphical}(b): (1) $Z_A \perp Y | X,A,\epsilon$, (2) $Z_A \perp Z_Y | A,X,\epsilon$, (3) $\epsilon \perp Y | Z_Y, X, A$.}, we decompose $q_{\phi}(\epsilon, Z_A, Z_Y|X,A,Y)$ as follows: 

% % \vspace{-3ex}
\begin{equation}
% \small
q_{\phi}(\epsilon, Z_A, Z_Y | X, A, Y) 
= q_{\phi_1}(Z_A | X, A, \epsilon) q_{\phi_2}(\epsilon | X, A, Z_Y) q_{\phi_3}(Z_Y | X, A, Y).
\label{eq:decompose_q}
\end{equation}
% % \vspace{-3ex}

For simplicity, we introduce two additional assumptions. First, when the node features $X$ and observed graph structure $A$ are given, latent clean graph structure $Z_A$ is conditionally independent from the noise-incurring variable $\epsilon$, i.e., $q_{\phi_1}(Z_A|X,A,\epsilon)=q_{\phi_1}(Z_A|X,A)$. Second, when $X$ and $A$ are given, latent clean labels $Z_Y$ is conditionally independent from the observed node labels $Y$, i.e., $q_{\phi_3}(Z_Y|X,A,Y)=q_{\phi_3}(Z_Y|X,A)$. This approximation, known as the mean-field method, is a prevalent technique utilized in variational inference-based methods \cite{ma2019flexible, lao2022variational}. As a result, we can simplify Eqn. \ref{eq:decompose_q} as follows:

% % \vspace{-3ex}
\begin{equation}
% \small
q_{\phi}(\epsilon, Z_A, Z_Y | X, A, Y) 
= q_{\phi_1}(Z_A | X, A) q_{\phi_2}(\epsilon | X, A, Z_Y) q_{\phi_3}(Z_Y | X, A).
\label{eq:decompose_q_assume}
\end{equation}
% % \vspace{-3ex}

To jointly optimize the parameter $\phi$ and $\theta$, we adopt the variational inference framework \cite{vi_review, yao2021instance} to optimize the Evidence Lower-BOund (ELBO) of the marginal likelihood for observed data, rather than optimizing the marginal likelihood directly. Specifically, we derive the ELBO for the observed data log-likelihood $P(X,A,Y)$. First, we factorize the joint distribution $P(X,A,Y,\epsilon, Z_A, Z_Y)$ based on the graphical model in Fig. \ref{fig:DANG_graphical}(b) in the main paper:

\begin{align}
\small
& P(X,A,Y,\epsilon, Z_A, Z_Y) \nonumber \\
& =P(\epsilon)P(Z_A)P(Z_Y)P(X|\epsilon, Z_Y)P(A|\epsilon, X, Z_A)P(Y|X,A,Z_Y).
\label{eq:ap_joint_factorize}
\end{align}

Thus, the conditional distribution $P_{\theta}(X,A,Y|\epsilon, Z_A, Z_Y)$ can be represented as follows:

\begin{align}
% \small
& P_{\theta}(X,A,Y|\epsilon, Z_A, Z_Y)=P_{\theta_1}(X|\epsilon, Z_Y)P_{\theta_2}(A|\epsilon, X, Z_A)P_{\theta_3}(Y|X,A,Z_Y).
\label{eq:ap_decoder_decompose}
\end{align}

Recall that the conditional distribution $q_{\phi}(\epsilon, Z_A, Z_Y | X, A, Y)$ is factorized as in Eqn. \ref{eq:decompose_q_assume}.
% \begin{align}
% \small
% & q_{\phi}(\epsilon, Z_A, Z_Y | X, A, Y) 
% % & = q_{\phi}(Z_A|\epsilon, Z_Y, X, A, Y) \cdot q_{\phi}(\epsilon|Z_Y, X, A, Y) \cdot q_{\phi}(Z_Y|X, A, Y) \nonumber \\
% = q_{\phi_1}(Z_A | X, A, \epsilon) \cdot q_{\phi_2}(\epsilon | X, A, Z_Y) \cdot q_{\phi_3}(Z_Y | X, A, Y).
% \label{eq:ap_encoder_decompose}
% \end{align}
Now, we derive the ELBO for the observed data log-likelihood $P(X,A,Y)$:

\begin{align}
\small
& \log p_{\theta}(X, A, Y) = \log \int_{\epsilon} \int_{Z_A} \int_{Z_Y} p_{\theta}(X, A, Y, \epsilon, Z_A, Z_Y) d\epsilon d Z_A d Z_Y \nonumber \\
& = \log \int_{\epsilon} \int_{Z_A} \int_{Z_Y} p_{\theta}(X,A,Y,\epsilon,Z_A,Z_Y) \frac{q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)}{q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)}  \nonumber \\
& = \log \mathbb{E}_{(\epsilon, Z_A, Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X,A,Y)} \left[ \frac{p_{\theta}(X,A,Y,\epsilon,Z_A,Z_Y)}{q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \right] \nonumber \\
& \geq \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \left[ \log \frac{p_{\theta}(X,A,Y,\epsilon,Z_A,Z_Y)}{q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \right] := \text{ELBO} \nonumber \\
& = \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \left[ \log \frac{p(\epsilon)p(Z_A)p(Z_Y)}{q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \right. \nonumber \\ 
& \left. \quad \quad \quad \quad \quad \quad  + \log \frac{p_{\theta_1}(A|X,\epsilon,Z_A) p_{\theta_2}(X|\epsilon, Z_Y) p_{\theta_3}(Y|X,A,Z_Y)}{q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \right] \nonumber \\ 
& = \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \biggl[ \log(p_{\theta_1}(A|X,\epsilon,Z_A)) \biggr. \nonumber \\
& \biggl. \quad \quad \quad \quad \quad \quad \quad \quad \quad + \log (p_{\theta_2}(X|\epsilon, Z_Y)) + \log(p_{\theta_3}(Y|X,A,Z_Y)) \biggr] \nonumber \\
& + \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, Y, A)} \left[ \log \frac{p(\epsilon)p(Z_A)p(Z_Y)}{q_{\phi}(\epsilon, Z_A, Z_Y|X,A,Y)} \right] 
\label{eq:elbo}
\end{align}

The last equation of Eq. \ref{eq:elbo} can be more simplified. We present the simplified results in Eqn. \ref{eq:elbo_sim1}, \ref{eq:elbo_sim2}, \ref{eq:elbo_sim3}, and \ref{eq:elbo_sim4}, where we abuse the notation $\mathbb{E}_{Z_A \sim q_{\phi_1}(Z_A|X,A)}$, $\mathbb{E}_{\epsilon \sim q_{\phi_2}(\epsilon|X,A,Z_Y)}$, and $\mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X,A)}$ as $q_{\phi_1}$, $q_{\phi_2}$, and $q_{\phi_3}$, respectively:

\begin{align}
\small
& \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \left[ \log(p_{\theta_1}(A|X,\epsilon,Z_A))\right] \nonumber \\
& = \mathbb{E}_{q_{\phi_1}} \mathbb{E}_{q_{\phi_2}} \mathbb{E}_{q_{\phi_3}} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right] \nonumber \\
& = \mathbb{E}_{q_{\phi_1}} \mathbb{E}_{q_{\phi_2}} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right], 
\label{eq:elbo_sim1}
\end{align}

and  

\begin{align}
\small
& \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \left[ \log(p_{\theta_2}(X|\epsilon,Z_Y)) \right] \nonumber \\
& = \mathbb{E}_{q_{\phi_1}} \mathbb{E}_{q_{\phi_2}} \mathbb{E}_{q_{\phi_3}} \left[ \log(p_{\theta_2}(X|\epsilon,Z_Y)) \right] \nonumber \\
& = \mathbb{E}_{q_{\phi_2}} \mathbb{E}_{q_{\phi_3}} \left[ \log(p_{\theta_2}(X|\epsilon,Z_Y)) \right],
\label{eq:elbo_sim2}
\end{align}

and

\begin{align}
\small
& \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, A, Y)} \left[ \log(p_{\theta_3}(Y|X,A, Z_Y)) \right] \nonumber \\
& = \mathbb{E}_{q_{\phi_1}} \mathbb{E}_{q_{\phi_2}} \mathbb{E}_{q_{\phi_3}} \left[ \log(p_{\theta_3}(Y|X,A, Z_Y)) \right] \nonumber \\
& = \mathbb{E}_{q_{\phi_3}} \left[ \log(p_{\theta_3}(Y|X,A, Z_Y)) \right].
\label{eq:elbo_sim3}
\end{align}

In a similar way, the last term can be also simplified:

\begin{align}
\small
% & \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, Y, A)} \left[ \log \frac{p(\epsilon)p(Z_A)p(Z_Y)}{q_{\phi}(\epsilon, Z_A, Z_Y|X,A,Y)} \right] 
& \mathbb{E}_{(\epsilon,Z_A,Z_Y) \sim q_{\phi}(\epsilon, Z_A, Z_Y|X, Y, A)} \left[ \log \frac{p(\epsilon)p(Z_A)p(Z_Y)}{q_{\phi}(\epsilon, Z_A, Z_Y|X,A,Y)} \right]   \nonumber \\
& = \mathbb{E}_{q_{\phi_1}} \mathbb{E}_{q_{\phi_2}} \mathbb{E}_{q_{\phi_3}} \left[ \log \frac{p(Z_A)p(\epsilon)p(Z_Y)}{q_{\phi_1}(Z_A|X,A)q_{\phi_2}(\epsilon|X,A,Z_Y)q_{\phi_3}(Z_Y|X,A)} \right] \nonumber \\
% & = \mathbb{E}_{q_{\phi_1}} \mathbb{E}_{q_{\phi_2}} \mathbb{E}_{q_{\phi_3}} \left[ \log \frac{p(Z_A)}{q_{\phi_1}(Z_A)} + \log \frac{p(\epsilon)}{q_{\phi_2}(\epsilon|Z_Y)} + \log \frac{p(Z_Y)}{q_{\phi_3}(Z_Y)} \right] \nonumber \\
& = \mathbb{E}_{q_{\phi_1}} \left[ \log \frac{p(Z_A)}{q_{\phi_1}(Z_A|X,A)} \right] + \mathbb{E}_{q_{\phi_2}} \mathbb{E}_{q_{\phi_3}} \left[ \log \frac{p(\epsilon)}{q_{\phi_2}(\epsilon|X,A,Z_Y)} \right] \nonumber \\
& +  \mathbb{E}_{q_{\phi_3}} \left[ \log \frac{p(Z_Y)}{q_{\phi_3}(Z_Y|X,A)} \right] \nonumber \\
& = -kl(q_{\phi_1}(Z_A|X,A) || p(Z_A)) -  \mathbb{E}_{q_{\phi_3}} \left[ kl(q_{\phi_2}(\epsilon|X,A,Z_Y) || p(\epsilon))\right] \nonumber \\  
& \quad  -kl(q_{\phi_3}(Z_Y|X,A) || p(Z_Y)).  
\label{eq:elbo_sim4}
\end{align}

We combine Eqn. \ref{eq:elbo_sim1}, \ref{eq:elbo_sim2}, \ref{eq:elbo_sim3}, and \ref{eq:elbo_sim4} to get the negative ELBO, i.e., $\mathcal{L}_{\text{ELBO}}$:

% \begin{align}
% \small
% & \mathcal{L}_{\text{ELBO}} = \nonumber \\ 
% & - \mathbb{E}_{Z_A \sim q_{\phi_1}(Z_A|X, A)} \mathbb{E}_{\epsilon \sim q_{\phi_2}(\epsilon|X,A,Z_Y)} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right] + kl(q_{\phi_1}(Z_A|X, A) || p(Z_A)) \nonumber \\
% & - \mathbb{E}_{\epsilon \sim q_{\phi_2}(\epsilon|X,A,Z_Y)} \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ \log(p_{\theta_2}(X|\epsilon,Z_Y)) \right] + \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ kl(q_{\phi_2}(\epsilon|X,A,Z_Y) || p(\epsilon))\right] \nonumber \\ 
% & - \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ \log(p_{\theta_3}(Y|X,A, Z_Y)) \right] + kl(q_{\phi_3}(Z_Y|X, A) || p(Z_Y)) 
% \label{eq:ap_elbo_loss}
% \end{align}

% % \vspace{-4ex}
\begin{align}
& \mathcal{L}_{\text{ELBO}} =  
- \mathbb{E}_{Z_A \sim q_{\phi_1}(Z_A|X, A)} \mathbb{E}_{\epsilon \sim q_{\phi_2}(\epsilon|X,A,Z_Y)} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right] \nonumber \\
& - \mathbb{E}_{\epsilon \sim q_{\phi_2}(\epsilon|X,A,Z_Y)} \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ \log(p_{\theta_2}(X|\epsilon,Z_Y)) \right] \nonumber \\ 
& - \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ \log(p_{\theta_3}(Y|X,A, Z_Y)) \right] \nonumber \\
& + kl(q_{\phi_3}(Z_Y|X, A) || p(Z_Y)) + kl(q_{\phi_1}(Z_A|X, A) || p(Z_A)) \nonumber \\
& + \mathbb{E}_{Z_Y \sim q_{\phi_3}(Z_Y|X, A)} \left[ kl(q_{\phi_2}(\epsilon|X,A,Z_Y) || p(\epsilon))\right].
\label{eq:ap_elbo_loss}
% % \vspace{-4ex}
\end{align}
% \vspace{-3ex}

\section{Details of Model Instantiations}
\label{sec:detail_instantiations}

\subsection{Details of regularizing the inference of $Z_A$}
% \label{sec:detail_instantiations_za}



% \underline{\textbf{Loss term }$kl(q_{\phi_1}(Z_A|X,A) || p(Z_A))$.}

% To minimize this term, we encourage $Z_A$ to align with our prior knowledge, i.e., $p(Z_A)$, that the latent graph structure predominantly consists of assortative edges that have the potential to enhance feature propagation within GNNs \cite{zhao2023graphglow}. In recent studies \cite{consm, rsgnn}, the $\gamma$-hop subgraph similarity has served as a potent metric for identifying assortative edges. Hence, we model $q_{\phi_1}(Z_A|X,A)$ to minimize the KL divergence between the latent graph structure $\hat{\mathbf{A}}$ and prior graph structure $\mathbf{A}^{\text{p}}=\{ {a}^{\text{p}}_{ij} \}_{N\times N}$, where ${a}^{\text{p}}_{ij}$ is sampled from a Bernoulli distribution with a probability as given by the $\gamma$-hop subgraph similarity. This leads the estimated probability $\hat{p}_{ij}$ between two nodes to increase if they exhibit a high subgraph similarity. However, computing $\hat{p}_{ij}$ in every epoch is impractical for large graphs, i.e., $O(N^2)$. 

We aim to regularize $Z_A$ to align with our prior knowledge by minimizing the KL divergence between the latent graph structure $\hat{\mathbf{A}}$ and prior graph structure $\mathbf{A}^{\text{p}}=\{ {a}^{\text{p}}_{ij} \}_{N\times N}$, where ${a}^{\text{p}}_{ij}$ is sampled from a Bernoulli distribution with a probability as given by the $\gamma$-hop subgraph similarity. 

However, computing $\hat{p}_{ij}$ in every epoch is impractical for large graphs, i.e., $O(N^2)$. To mitigate the issue, we pre-define a candidate graph that consists of the observed edge set $\mathcal{E}$ and a $k$-NN graph based on the $\gamma$-hop subgraph similarity. We denote the set of edges in the $k$-NN graphs as $\mathcal{E}^{\gamma}_{k}$. Then, we compute the $\hat{p}_{ij}$ values of the edges in a candidate graph, i.e., $\mathcal{E}^{\gamma}_{k} \cup \mathcal{E}$, instead of all edges in $\{ (i,j) | i\in \mathcal{V}, j\in \mathcal{V} \}$, to estimate the latent graph structure denoted as $\hat{\mathbf{A}}$. It is important to highlight that obtaining $\mathcal{E}_{k}^{\gamma}$ is carried out offline before model training, thus incurring no additional computational overhead during training. This implementation technique achieves a similar effect as minimizing $kl(q_{\phi_1}(Z_A|X,A) || p(Z_A))$ while significantly addressing computational complexity from $O(N^2)$ to $O(|\mathcal{E}^{\gamma}_{k} 
\cup \mathcal{E}|)$, where $N^2 \gg |\mathcal{E}^{\gamma}_{k} 
\cup \mathcal{E}|$.




\section{Further Discussion on DANG}
\subsection{Additional Real-world Examples of DANG}
\label{sec:ap-real-world-ex}

\begin{itemize}[leftmargin=0.2cm]
    \item \textbf{User graphs in social networks}: These graphs feature nodes that may represent user's profile or posts, with follow relationship among users defining the graph's structure. The node labels could denote the communities (or interests) of the users. In such scenarios, if users might create fake or incomplete profiles for various reasons, including privacy concerns, some irrelevant users may follow the user based on his/her fake profile, which leads to noisy edges. Moreover, if a user has noisy node features or noisy edges, the user may be assigned to a wrong community (or interest), which leads to noisy labels.
    
    \item \textbf{User graphs in e-commerce}: 
    % These graphs feature nodes that may represent user information. Edges are established between users who interact with the same products. The node labels could denote the communities (or interests) of the users. In such scenarios, 
    Users might create fake or incomplete profiles for various reasons, leading to noisy node features. As a result, products that do not align with the user's genuine interests could be displayed on a web or app page, encouraging the user to view, click on, or purchase these products. Consequently, users are more likely to engage with irrelevant products, leading to a noisy graph structure due to the user's inaccurate features. Moreover, this distortion in users' information and interactions can also alter their associated communities, resulting in noisy node labels.
    
    \item \textbf{Item graphs in e-commerce}: 
    % Nodes represent products and edges represent the co-purchase relationship between products. 
    Fake reviews on products written by a fraudster (i.e., noisy node features) would make other users purchase irrelevant products, which adds irrelevant edges between products (i.e., graph structure noise). Consequently, this would make the automated product category labeling system to inaccurately annotate product categories (i.e., label noise), as it relies on the node features and the graph structure, both of which are contaminated.
    
    \item \textbf{Item graphs in web graphs}: The content-based features of web pages are corrupted due to poor text extraction or irrelevant information, which leads to noisy node features. In such case, the algorithm responsible for identifying hyperlinks or user navigation patterns might create incorrect or spurious connections between nodes, leading to noisy graph structure. Furthermore, if the features of the nodes are noisy, the algorithms that rely on these features to assign labels (e.g., classifying a web page as a news site or a forum) may result in noisy node labels. Moreover, noises in the graph structure (e.g., incorrect links between web pages) can distort the relational information used by graph-based algorithms, leading to noises in the node labels.
    
    \item \textbf{Item graphs in citation graphs}: In an academic citation network, nodes represent academic papers, edges represent citation relationships, and node features include attributes like title, abstract, authors, keywords, and venue. Recently, generative AI agents have created numerous fake papers with plausible but incorrect attributes, leading to noisy node features. These fake papers get indexed and resemble genuine ones, causing algorithms or researchers to mistakenly create citation links between real and fake papers based on content similarity or keywords, resulting in noisy graph structure. For instance, a well-crafted fake abstract may cause genuine papers to erroneously cite it. Fake papers can corrupt classification algorithms, skewing topic distributions and distorting the citation graph. This affects metrics like citation counts, h-index calculations, and paper influence scores, propagating errors through algorithms that rely on the graph structure, ultimately leading to noisy node labels.
    
    \item \textbf{Biological Networks}: In addition to the user-item graphs, DANG manifests in the domain of single-cell RNA-sequencing (scRNA-seq). Specifically, in this graph the primary resource is a cell-gene count matrix. A cell-cell graph is commonly employed for downstream tasks, where each cell and its corresponding gene expression are represented as a node and node feature, respectively, and the cell type is considered a node label. However, the node feature, representing gene expression derived from the cell-gene count matrix, often contains noise due to various reasons, such as the dropout phenomenon~\cite{dropout} and batch effect~\cite{batcheffect}. Since the cell-gene count matrix is the main resource for generating the cell-cell graph~\cite{wang2021scgnn, xiong2023scgcl, yun2023scFP}, such noise acts as a significant obstacle in designing an effective graph structure. Additionally, cell types are annotated using transcripted marker genes, which serve as distinctive features characterizing specific cell types. Noisy node features, therefore, can lead to the misprediction of cell types (node labels). This issue of noise in node features in the biological domain underscores the critical challenge in real-world scenarios.


\end{itemize}


\subsection{Extension of DANG}
\label{sec:ap_dis}
\begin{figure}
    % \vspace{-2ex}
    \centering
    % \raisebox{5pt}[\dimexpr\height-0.6\baselineskip\relax]
    {\includegraphics[width=.4\linewidth]{fig/fdgn_graphical_dis.pdf}}
    % \vspace{-1ex}
    \caption{A directed graphical model indicating a DGP of (a) DANG, and (b) the case when adding causal relationship $A \rightarrow X$.
    % The shaded variables are observable and the unshaded variables are latent.
    }
    \label{fig:ab_DANG_graphical}
    % % \vspace{-5ex}
\end{figure}

While the proposed DANG and \proposed~demonstrate broader applicability compared to existing methods, they do not perfectly cover all possible noise scenarios. One potential direction to enhance their practicality is to incorporate the causal relationship $X \leftarrow A$, which suggests that graph structure noise can inevitably lead to node feature noise—an occurrence that may manifest in certain real-world scenarios.
% In this work, our main focus is on the limitations of the conventional graph noise assumption in terms of node features: the noise in node features is independent of the graph structure or node labels. To this end, we propose a more realistic noise scenario DANG by introducing the causal relationships $A \leftarrow X$ and $Y \leftarrow (X,A)$ into the DGP of CGN. However, it is crucial to acknowledge that in some real-world scenarios, the causal relationship $X \leftarrow A$ may indeed manifest, which indicates that the graph structure noise inevitably entails the node feature noise.
For instance, consider a social network where node features represent the content to which a user is exposed or interacts with (e.g., views, clicks, or likes), while the graph structure denotes the follower relationships. In such a scenario, if a user follows or is followed by fake accounts, the graph structure might incorporate noisy links (i.e., noisy graph structure). This, in turn, can impact the content to which users are exposed and their interactions (i.e., noisy node features), eventually influencing their community assignments (i.e., noisy node labels). 
In other words, the noisy node feature and noisy graph structure mutually influence the noise of each other, ultimately incurring the noisy node label. We illustrate its DGP in Fig~\ref{fig:ab_DANG_graphical}(b). Given that its DGP covers a broader range of noise scenarios that occur in real-world applications than DANG, we expect that directly modeling its DGP has the potential to enhance practical applicability. However, this is a topic we leave for future work.


\section{Further Discussion on \proposed}
\label{sec:ap_dis_pringle}

In this section, to further highlight the technical distinction of our work, we compare \proposed~with the recent works handling the data noise using variational inference technique.

\subsection{Comparison to CausalNL \cite{yao2021instance}}
\label{sec:ap_causlnl}
While the implementation of \proposed~draws inspiration from the spirit of VAE \cite{kingma2013auto} and CausalNL \cite{yao2021instance}, we address complex and unique challenges absent in \cite{kingma2013auto, yao2021instance}. Specifically, the incorporation of $A$ necessitates handling supplementary latent variables and causal relationships, such as $Z_A$, $\epsilon_A$, $A$ ← $\epsilon_A$, $A$ ← $X$, $Y$ ← $A$, $A$ ← $Z_A$, each posing non-trivial obstacles beyond their straightforward extension.  

\begin{itemize}[leftmargin=0.2cm]
    \item While \cite{yao2021instance} assumes that $\epsilon$ only causes $X$, DANG posits that $\epsilon$ also causes $A$, denoted as $A$ ← $\epsilon_A$. Consequently, DANG requires a novel inference/regularization approach for $\epsilon_A$, which is not addressed in \cite{yao2021instance}, presenting a distinctive technical challenge.
    
    \item A simplistic uniform prior is employed to regularize the modeling of $Z_Y$ in \cite{yao2021instance}. However, upon close examination of the relationship $Y$ ← $A$, we advocate for a novel regularization approach for $Z_Y$ based on the principle of homophily. This method cannot be elicited through a straightforward application of \cite{yao2021instance} to the graph.

    \item By incorporating $A$, $Z_A$, and their associated casualties, we address distinct technical challenges, specifically the inference/regularization of $Z_A$ and the generation of $A$, which cannot be accommodated by a mere extension of \cite{yao2021instance} to the graph. In particular, we utilize graph structure learning to model $Z_A$, and frame the generation of $A$ as an edge prediction task, incorporating novel regularization techniques for both edge prediction and label. Moreover, we regularize $Z_A$ leveraging our novel prior knowledge to enhance the accuracy and scalability of inference.

\end{itemize}

We argue that these components are non-trivial to handle through a straightforward application of \cite{yao2021instance} to the graph domain.


\subsection{Comparision to GraphDE \cite{li2022graphde}}

GraphDE \cite{li2022graphde} aims to handle situations where out-of-distribution (OOD) samples are present in training data. Given that a noisy sample can be viewed as an OOD sample, GraphDE shares a similar objective with our work. Additionally, GraphDE employs a generative model based on variational inference, relevant to learning approach. However, there are significant distinctions in the DGP that we assume compared to GraphDE. Specifically, GraphDE assumes that a latent variable $e$ determines whether an instance is an OOD sample or not, overlooking the way that the OOD sample is generated. Conversely, the DGP of our proposed DANG characterizes the process by which noisy samples (i.e., OOD sample) are generated (e.g., in a feature-dependent manner). This crucial difference enables a more fine-grained model learning than GraphDE, which may weaken the applicability of GraphDE to complex real-world noise scenarios, such as DANG.

\subsection{Comparison to RGIB \cite{zhou2023combating}}

RGIB \cite{zhou2023combating} aims at denoising graph structure, primarily centered on the link prediction task. In their work, while $Z_A$ and $A$ share the same meaning as in our paper, $Y$ denotes "edge labels" rather than node labels (i.e., it serves as a binary indicator for the presence of query edges), and $Z_Y$ refers to "clean edge labels" rather than clean latent node labels. These distinctions significantly differentiate their approach from ours. Furthermore, it's important to note that RGIB primarily addressed noisy graph structures while assuming that node features and node labels are noise-free. In contrast, our proposed method, \proposed, is developed under the more realistic DANG assumption, where node features, graph structures, and node labels all simultaneously contain noise. This fundamental difference underscores the enhanced applicability and robustness of \proposed~compared to RGIB.

% \subsection{Details on regularizing the inference of $\epsilon$}
% \label{sec:detail_instantiations_eps}

% To simplfy the inference of $\epsilon$, we decompose $q_{\phi_2}(\epsilon | X, A, Z_Y)$ into \textcolor{black}{$q_{\phi_{21}}(\epsilon_X | X, Z_Y)$} and \textcolor{black}{$q_{\phi_{22}}(\epsilon_A | X, A)$}, thereby modeling the encoders $\phi_{21}$ and $\phi_{22}$, respectively. Furthermore, we regularize the inference of $\epsilon_X$ and $\epsilon_A$ by minimizing the $kl(q_{\phi_{21}}(\epsilon_X|X,Z_Y) || p(\epsilon_X))$ and $kl(q_{\phi_{22}}(\epsilon_A|X,A) || p(\epsilon_A))$. 

% The loss term $\mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ kl(q_{\phi_2}(\epsilon|X,A, Z_Y) || p(\epsilon)) \right] $ in Eqn. \ref{eq:elbo_loss} can be simplified to $kl(q_{\phi_2}(\epsilon|X,A, Z_Y) || p(\epsilon))$ since we empirically model $q_{\phi_2}(\epsilon| X,A,Z_Y)$ using a deterministic encoder $\text{GCN}_{\phi_3}(\mathbf{X}, \mathbf{\hat{A}})$, thereby the expectation can be removed. Then, 




% \noindent \underline{\textbf{Modeling encoder} $q_{\phi_2}(\epsilon | X, A, Z_Y)$.} 

% \noindent \underline{\textbf{Decomposition of }$\mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ kl(q_{\phi_2}(\epsilon|X,A, Z_Y) || p(\epsilon)) \right]$.} We first decompose the $kl$ term in the expectation, $kl(q_{\phi_2}(\epsilon|X,A, Z_Y) || p(\epsilon))$, into two terms: $kl(q_{\phi_{21}}(\epsilon_X|X,Z_Y) || p(\epsilon_X))$ and $kl(q_{\phi_{22}}(\epsilon_A|X,A) || p(\epsilon_A))$. Moreover, since we empirically model $q_{\phi_2}(\epsilon| X,A,Z_Y)$ using a deterministic encoder $\text{GCN}_{\phi_3}(\mathbf{X}, \mathbf{\hat{A}})$, the expectation can be removed. Consequently, we have the two $kl$ term without any expectation.



\section{Details on Experimental Settings}
\subsection{Datasets }
\label{sec:ap_dataset}
We evaluate~\proposed~and baselines on \textbf{five existing datasets} (i.e., Cora \cite{coraciteseer}, Citeseer \cite{coraciteseer}, Amazon Photo and Computers \cite{shchur2018pitfalls}), and ogbn-arxiv \cite{hu2020open} and \textbf{two newly introduced datasets} (i.e., Amazon Auto and Amazon Garden) that are proposed in this work based on Amazon review data \cite{amazondata1, amazondata2} to mimic DANG caused by malicious fraudsters on e-commerce systems  (Refer to Appendix \ref{sec:ap-real-DANG-gen} for details). The statistics of the datasets are given in Table \ref{tab:dataset}. These seven datasets can be found in these URLs:

\begin{itemize}[leftmargin=0.5cm]
\item \textbf{Cora}: https://github.com/ChandlerBang/Pro-GNN/
\item \textbf{Citeseer}: https://github.com/ChandlerBang/Pro-GNN/
\item \textbf{Photo}: https://pytorch-geometric.readthedocs.io/en/latest/
\item \textbf{Computers}: https://pytorch-geometric.readthedocs.io/en/latest/
\item \textbf{Arxiv}: https://ogb.stanford.edu/docs/nodeprop/\#ogbn-arxiv
\item \textbf{Auto}: http://jmcauley.ucsd.edu/data/amazon/links.html
\item \textbf{Garden}: http://jmcauley.ucsd.edu/data/amazon/links.html
\end{itemize}

\begin{table}[h]
\centering
% \small
\caption{Statistics for datasets.}
%%%% \vspace{-2ex}
% \renewcommand{\arraystretch}{0.98}
{\small
\scalebox{0.9}{
\begin{tabular}{c|c|cccc}
% \noalign{\smallskip}{\smallskip}
\toprule
 % Domain & Dataset & \# Nodes & \# Edges & \# Features & \# Classes \\
 Dataset & \# Nodes & \# Edges & \# Features & \# Classes \\
\midrule
% \multirow{2}{*}{Citation graph} 
Cora     & 2,485          & 5,069         & 1,433      & 7   \\  
Citeseer   & 2,110        & 3,668         & 3,703      & 6   \\
% \multirow{4}{*}{Co-purchase graph} 
Photo    & 7,487       & 119,043       & 745        & 8  \\
Computers   & 13,381        & 245,778       & 767        & 10    \\
Arxiv   & 169,343        & 1,166,243       & 128        & 40    \\
\midrule
Auto   & 8,175        & 13,371       & 300        & 5   \\
Garden   & 7,902        & 19,383       & 300        & 5   \\
\bottomrule
\end{tabular}
}}
\label{tab:dataset}
%%%% \vspace{0ex}
\end{table}


\subsection{Details of Generating DANG }
\label{sec:ap-DANG-gen}
\subsubsection{Synthetic DANG }
\label{sec:ap-DANG-gen-synthetic}


For the synthetic DANG settings, we artificially generate the noise following the data generation process of the proposed DANG scenario. First, we randomly sample a subset of nodes $\mathcal{V}^{\text{noisy}}$ (i.e., 10\%, 30\%, and 50\% of the whole node set $\mathcal{V}$). To inject node feature noise into the sampled nodes, we randomly flip 0/1 value on each dimension of node features $\mathbf{X}_i$ from Bernoulli distribution with probability $p=\frac{1}{F}\sum_{i=1}^{F} \mathbf{X}_i$, which results in the noisy features ${\mathbf{X}}_i^{\text{noisy}}$. After injecting the feature noise, we generate a feature-dependent structure noise (i.e., $A \leftarrow X$) and feature-dependent label noise (i.e., $Y \leftarrow (X,A)$). For the feature-dependent structure noise, we first calculate the similarity vector for each node $v_i$ as $\{ s({\mathbf{X}}_i^{\text{noisy}}, \mathbf{{X}}_j) | v_i \in \mathcal{V}^{\text{noisy}}, v_j \in \mathcal{V} \}$ where $s(\cdot,\cdot)$ is a cosine similarity function, and select the node pairs whose feature similarity is top-$k$ highest values. We add the selected node pairs to the original edge set $\mathcal{E}$, which results in ${\mathcal{E}^{\text{noisy}}}$. To address feature-dependent label noise, we replace the labels of labeled nodes (i.e., training and validation nodes) with randomly sampled labels from a Multinomial distribution, with parameters determined by the normalized neighborhood class distribution.
Finally, for the independent structure noise (i.e., $A \leftarrow \epsilon$), we add the randomly selected non-connected node pairs to the ${\mathcal{E}^{\text{noisy}}}$. Detailed algorithm is provided in Algorithm \ref{alg:algo_DANG_syn}.


\subsubsection{Real-world DANG }
\label{sec:ap-real-DANG-gen}
We have introduced and released two new graph benchmark datasets, i.e., Auto and Garden, that simulate real-world DANG scenarios on e-commerce systems. To construct these graphs, we utilized metadata and product review data from two categories, "Automotives" and "Patio, Lawn and Garden," obtained from Amazon product review data sources \cite{amazondata1, amazondata2}. Specifically, we generated a clean product-product graph where node features are represented using a bag-of-words technique applied to product reviews. The edges indicate co-purchase relationships between products that have been purchased by the same user, and the node labels correspond to product categories. We perform both node classification and link prediction tasks, which are equivalent to categorizing products and predicting co-purchase relationships, respectively.  


We simulate the behaviors of fraudsters on a real-world e-commerce platform that incurs DANG. When the fraudsters engage with randomly selected products (i.e., when they write fake product reviews), it would make other users purchase irrelevant products, which introduces a substantial number of malicious co-purchase edges within the graph structure. Additionally, this activity involves the injection of noisy random reviews into the node features. To provide a more detailed description, we designated 100 uers as fraudsters. Furthermore, each of these users was responsible for generating 10 fraudulent reviews in both the Auto and Garden datasets.
To generate fake review content, we randomly choose text from existing reviews and duplicate it for the targeted products. This approach guarantees that the fake reviews closely mimic the writing style and content of genuine reviews, while also incorporating irrelevant information that makes it more difficult to predict the product category.

In e-commerce systems, to annotate the node labels (i.e., product categories), machine learning-based automated labeling systems are commonly utilized. Specifically, human annotators manually label a small set of examples, which is used as the training examples to the machine learning model. Subsequently, a machine learning model is trained on these manually labeled product samples to automatically assign categories to other products. Therefore, the systems rely on the information about the products, e.g., reviews of products and co-purchase relationships, to assign categories to products. However, due to the influence of the fraudsters, the noisy node features (i.e., fake product reviews) and noisy graph structure (i.e., co-purchase relationships between irrelevant products) may hinder the accurate assignment of the automated labeling systems, which leads to the noisy node label. 
To replicate this procedure, we selected 5 examples per category class, which is equivalent to manual labeling process. We then trained a GCN model, leveraging the node features, graph structure, and manually labeled nodes, to predict the true product categories. Consequently, our set of labeled nodes are composed of both manually labeled nodes and nodes labeled using the GCN model. Importantly, the labels of unlabeled nodes were left unchanged and still represented their actual categories.
The data generation code is also available at~\url{https://github.com/yeonjun-in/torch-DA-GNN}.
% The data generation code is also available at~\url{https://anonymous.4open.science/r/torch-DA-GNN-164E/}.

We again emphasize that while existing works primarily focus on the unrealistic noise scenario where graphs contain only a single type of noise, to the best of our knowledge, this is the first attempt to understand the noise scenario in the real-world applications. Furthermore, we propose new graph benchmark datasets that closely imitate a real-world e-commerce system containing malicious fraudsters, which incurs DANG. We expect these datasets to foster practical research in noise-robust graph learning.

 
\subsection{Baselines}
\label{sec:ap_baselines}
We compare \proposed~with a wide range of noise-robust GNN methods, which includes feature noise-robust GNNs (i.e., AirGNN \cite{airgnn}), structure-noise robust GNNs (i.e., ProGNN \cite{prognn}, RSGNN \cite{rsgnn}, STABLE \cite{stable} and EvenNet \cite{lei2022evennet}), label noise-robust GNNs (i.e., NRGNN \cite{nrgnn} and RTGNN \cite{rtgnn}), and multifaceted noise-robust GNNs (i.e., SG-GSR \cite{in2024self}. We also consider WSGNN \cite{lao2022variational} and GraphGlow \cite{zhao2023graphglow} that are generative approaches utilizing variational inference technique.

The publicly available implementations of baselines can be found at the following URLs:


\begin{itemize}[leftmargin=0.5cm]

\item \noindent\textbf{WSGNN} \cite{lao2022variational} : https://github.com/Thinklab-SJTU/WSGNN

\item \noindent\textbf{GraphGLOW} \cite{zhao2023graphglow} : https://github.com/WtaoZhao/GraphGLOW

\item \noindent\textbf{AirGNN} \cite{airgnn} : https://github.com/lxiaorui/AirGNN

\item \noindent\textbf{ProGNN} \cite{prognn} \@: https://github.com/ChandlerBang/Pro-GNN


\item \noindent\textbf{RSGNN} \cite{rsgnn} : https://github.com/EnyanDai/RSGNN

\item \noindent\textbf{STABLE} \cite{stable} : https://github.com/likuanppd/STABLE

\item \noindent\textbf{EvenNet} \cite{lei2022evennet} : https://github.com/Leirunlin/EvenNet

\item \noindent\textbf{NRGNN} \cite{rsgnn} : https://github.com/EnyanDai/NRGNN

\item \noindent\textbf{RTGNN} \cite{rsgnn} : https://github.com/GhostQ99/RobustTrainingGNN

\item \noindent\textbf{SG-GSR} \cite{in2024self} : https://github.com/yeonjun-in/torch-SG-GSR


\end{itemize}

\subsection{Evaluation Protocol}
\label{sec:ap_eval_protocol}
We mainly compare the robustness of \proposed~and the baselines under both the synthetic and real-world feature-dependent graph-noise (DANG). More details of generating DANG is provided in Sec~\ref{sec:ap-DANG-gen}. Additionally, we consider independent feature/structure/label noise, which are commonly considered in prior works in this research field \cite{airgnn, rsgnn, stable, in2023similarity, rtgnn}. Specifically, for the feature noise \cite{airgnn}, we sample a subset of nodes (i.e., 10\%, 30\%, and 50\%) and randomly flip 0/1 value on each dimension of node features $\mathbf{X}_i$ from Bernoulli distribution with probability $p=\frac{1}{F}\sum_{i=1}^{F} \mathbf{X}_i$. For the structure noise, we adopt the random perturbation method that randomly injects non-connected node pairs into the graph \cite{stable}. For the label noise, we generate uniform label noise following the existing works \cite{rtgnn, nrgnn}.

We conduct both the node classification and link prediction tasks. For node classification, we perform a random split of the nodes, dividing them into a 1:1:8 ratio for training, validation, and testing nodes. Once a model is trained on the training nodes, we use the model to predict the labels of the test nodes. Regarding link prediction, we partition the provided edges into a 7:3 ratio for training and testing edges. Additionally, we generate random negatives that are selected randomly from pairs that are not directly linked in the original graphs. After mode learning with the training edges, we predict the likelihood of the existence of each edge. This prediction is based on a dot-product or cosine similarity calculation between node pairs of test edges and their corresponding negative edges. To evaluate performance, we use Accuracy as the metric for node classification and Area Under the Curve (AUC) for link prediction.



\subsection{Implementation Details}
\label{sec:ap_imple_detail}

For each experiment, we report the average performance of 3 runs with standard deviations. For all baselines, we use the publicly available implementations and follow the implementation details presented in their original papers. 

For \proposed, the learning rate is tuned from \{0.01, 0.005, 0.001, 0.0005\}, and dropout rate and weight decay are fixed to 0.6 and 0.0005, respectively. 
In the inference of $Z_A$, we use a 2-layer GCN model with 64 hidden dimension as $\text{GCN}_{\phi_1}$ and the dimension of node embedding $d_1$ is fixed to 64. The $\gamma$ value in calculating $\gamma$-hop subgraph similarity is tuned from \{0, 1\} and $k$ in generating $k$-NN graph is tuned from \{0, 10, 50, 100, 300\}. In the inference of $Z_Y$, we use a 2-layer GCN model with 128 hidden dimension as $\text{GCN}_{\phi_3}$. In the inference of $\epsilon_X$, the hidden dimension size of $\epsilon_X$, i.e., $d_2$, is fixed to 16. In the inference of $\epsilon_A$, the early-learning phase is fixed to 30 epochs. In the implementation of the loss term $-\mathbb{E}_{Z_A \sim q_{\phi_1}} \mathbb{E}_{\epsilon \sim q_{\phi_2}} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right]$, we tune the $\theta_1$ value from \{0.1, 0.2, 0.3\}. In the overall learning objective, i.e., Eqn~\ref{eq:final_loss}, $\lambda_1$ is tuned from \{ 0.003, 0.03. 0.3, 3, 30 \}, $\lambda_2$ is tuned from \{ 0.003, 0.03. 0.3 \}, and $\lambda_3$ is fixed to 0.001.
We report the details of hyperparameter settings in Table \ref{tab:hype1}.

For all baselines, we follow the training instruction reported in their paper and official code. For AirGNN, we tune $\lambda \in$ \{0.0, 0.2, 0.4, 0.6, 0.8\} and set the others as mentioned in the paper for all datasets. For ProGNN, we use the training script reported in the offical code since there are no training guidance in the paper. For RSGNN, we tune $\alpha \in$ \{0.003, 0.03, 0.3, 3, 30\}, $\beta \in$ \{0.01, 0.03, 0.1, 0.3, 1\}, $n_p \in$ \{0, 10, 100, 300, 400\}, and learning rate $\in$ \{0.01, 0.005, 0.001, 0.0005\} for all datasets. For STABLE, We tune 
$t_1 \in$ \{0.0, 0.01, 0.02, 0.03, 0.04\}, $t_2 \in $ \{0.1, 0.2, 0.3\}, $k \in$ \{1, 3, 5, 7, 11, 13\}, and $\alpha \in$ \{-0.5, -0.3, -0.1, 0.1, 0.3, 0.6\} for all datasets. For EvenNet, we tune $\lambda \in$ \{0.1, 0.2, 0.5, 0.9\} for all datasets following the training script of the official code. For NRGNN, we tune $\alpha \in $ \{0.001, 0.01, 0.1, 1, 10\}, $\beta \in$ \{0.001, 0.01, 0.1, 1, 10, 100\}, and learning rate $\in$ \{0.01 , 0.005, 0.001, 0.0005\} for all datasets. For RTGNN, we tune $K \in$ \{1, 10, 25, 50, 100\}, $th_{pse} \in$ \{0.7, 0.8, 0.9, 0.95\}, $\alpha \in$ \{0.03, 0.1, 0.3, 1\}, and $\gamma \in$ \{0.01, 0.1\}, and learning rate $\in$ \{0.01, 0.005, 0.001, 0.0005\}. For WSGNN, we use the best hyperparameter setting reported in the paper since there are no training guidance in the paper. \textcolor{black}{For GraphGLOW, we tune learning rate $\in$ \{0.001, 0.005, 0.01, 0.05\}, embedding size $d\in$ \{16, 32, 64, 96\}, pivot number $P\in$ \{800, 1000, 1200, 1400\}, $\lambda \in$ \{0.1, 0.9\}, $H\in$ \{4, 6\}, $E\in\{1, 2, 3\}$, $\alpha \in$ \{0, 0.1, 0.15, 0.2, 0.25, 0.3\}, and $\rho \in$ \{0, 0.1, 0.15, 0.2, 0.25, 0.3\}.} For SG-GSR, we tune learning rate $\in$ \{0.001, 0.005, 0.01, 0.05\}, $\lambda_{\mathcal{E}}$ $\in$ \{0.2, 0.5, 1, 2, 3, 4, 5\}, $\lambda_{\text{sp}}$ and $\lambda_{\text{fs}}$ $\in$ \{1.0, 0.9, 0.7, 0.5, 0.3\}, and $\lambda_{\text{aug}}$ $\in$ \{0.1, 0.3, 0.5, 0.7, 0.9\}.



\begin{table}[t!]
\centering
% \small
\caption{Hyperparameter settings on \proposed~for Table~\ref{tab:synthetic_DANG_node}.}

% \renewcommand{\arraystretch}{0.98}
{
% \small
% \scalebox{0.75}{
\resizebox{.4\columnwidth}{!}{
\begin{tabular}{c|c|cccccc}
% \noalign{\smallskip}{\smallskip}
\toprule
 Dataset & Setting & lr & $\lambda_1$ &  $\lambda_2$ & $\theta_1$ & $k$ &  $\gamma$ \\
\midrule
\multirow{4}{*}{Cora} 
& Clean     & 0.01 & 0.003 & 0.003 & 0.1 & 300 &1 \\



& DANG-10\% & 0.005 & 0.003 
&0.003 
&0.2
&50
&1 \\



& DANG-30\% & 0.001
&0.003 
&0.003
&0.2
&100
&1
 \\



& DANG-50\% & 0.0005
&30
&0.003
&0.3
&50
&1 \\



\midrule 
\multirow{4}{*}{Citeseer} 
& Clean     & 0.0005
&0.003 
&0.3
&0.1
&50
&0
 \\



& DANG-10\% & 0.005
&0.3 
&0.003
&0.3
&10
&0 \\



& DANG-30\% & 0.001
&0.003 
&0.003
&0.1
&300
&1
 \\



& DANG-50\% & 0.001
&0.003 
&0.003
&0.1
&300
&1
 \\



\midrule 
\multirow{4}{*}{Photo} 
& Clean     & 0.01
&0.03 
&0.3
&0.1
&10
&0
 \\



& DANG-10\% & 0.0005
&0.03 
&0.3
&0.1
&10
&0
\\



& DANG-30\% & 0.001
&3
&0.003
&0.1
&10
&0
 \\



& DANG-50\% & 0.0005
&30 
&0.03
&0.1
&10
&0
 \\



\midrule 
\multirow{4}{*}{Comp} 
& Clean     & 0.01
&30
&0.03
&0.1
&10
&0
 \\



& DANG-10\% & 0.01
&0.3 
&0.03
&0.1
&10
&0
 \\



& DANG-30\% & 0.01
&0.003 
&0.003
&0.1
&10
&0
 \\



& DANG-50\% &0.0005
&0.003
&0.03
&0.1
&10
&0

 \\
\midrule 
\multirow{4}{*}{Arxiv} 
& Clean     & 0.01
& 0.03
&0.003
&0.1
&0
&1
 \\



& DANG-10\% & 0.01
& 0.003
&0.03
&0.1
&0
&1
 \\



& DANG-30\% & 0.01
&0.003 
&0.003
&0.1
&0
&1
 \\



& DANG-50\% &0.005
&3 
&0.03
&0.1
&0
&1

 \\



\bottomrule 

\end{tabular}
}}
\label{tab:hype1}
% % \vspace{-4ex}
\end{table}


\section{Additional Experimental Results}


\begin{figure*}[h]
  \begin{minipage}{0.33\textwidth}
    \centering
    \includegraphics[width=1.\linewidth]{fig/fin_feat.pdf}
    % \vspace{-5ex}
    \captionsetup{width=0.9\textwidth}
    \caption{Node classification accuracy under independent feat. noise.}
    \label{fig:fin_feat}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.33\textwidth}
    \centering
    \includegraphics[width=1.\linewidth]{fig/fin_struc.pdf}
    % \vspace{-5ex}
    \captionsetup{width=0.9\textwidth}
    \caption{Node classification accuracy under independent stru. noise.}
    \label{fig:fin_struc}
  \end{minipage}
  \begin{minipage}{0.33\textwidth}
    \centering
    \includegraphics[width=1\linewidth]{fig/fin_label.pdf}
    % \vspace{-5ex}
    \captionsetup{width=0.9\textwidth}
    \caption{Node classification accuracy under independent label noise.}
    \label{fig:fin_label}
  \end{minipage}
\end{figure*}

\subsection{Robustness evaluation under existing noise scenarios}
\label{sec:ap_citeseer}

In Fig~\ref{fig:fin_feat}, \ref{fig:fin_struc}, and \ref{fig:fin_label}, we show \proposed~exhibits comparable or better performance than baselines under other noise scenarios, commonly considered in this research field. 

\subsection{Comparison with the naive combination of existing works }

So far, we have observed that existing approaches fail to generalize to DANG since they primarily focus on graphs containing only a single type of noise. A straightforward solution might be to naively combine methods that address each type of noise individually. To explore this idea, we consider AirGNN as the feature noise-robust GNN (\textit{FNR}), RSGNN as the structure noise-robust GNN (\textit{SNR}), and RTNN as the label noise-robust GNN (\textit{LNR}). We carefully implement all possible combinations among \textit{FNR}, \textit{SNR}, and \textit{LNR}.

In Table~\ref{tab:naive_comb}, we observe that naive combination can improve robustness in some cases, but it may not consistently yield favorable results. For example, combining \textit{FNR} and \textit{SNR} notably enhances robustness. However, when we combine all three (\textit{FNR}, \textit{SNR}, and \textit{LNR}), which is expected to yield the best results, performance even decreases. This could be attributed to compatibility issues among the methods arising from the naive combination. Furthermore, although some combinations improve robustness, \proposed~consistently outperforms all combinations. We attribute this to the fact that naively combining existing methods may not capture the causal relationships in the DGP of DANG, limiting their robustness. In contrast, \proposed~successfully captures these relationships, resulting in superior performance. 


\begin{figure}
\centering
\small
% \vspace{-1ex}
\captionof{table}{Comparison with the naive combination of existing noise-robust graph learning methods. \textit{FNR}, \textit{SNR}, and \textit{LNR} denote the feature noise-robust, structure noise-robust, and label noise-robust graph learning methods, respectively. We consider AirGNN as \textit{FNR}, RSGNN as \textit{SNR}, and RTGNN as \textit{LSR} methods.}


\resizebox{1.\columnwidth}{!}{
    \begin{tabular}{ccc|cccc|cccc}
        \toprule
       \multicolumn{3}{c|}{Component} & \multicolumn{4}{c|}{Cora} & \multicolumn{4}{c}{Citeseer}  \\
        \midrule
        \textit{FNR} & \textit{SNR} & \textit{LNR} &  Clean & DANG 10\% & DANG 30\% & DANG 50\% & 
        Clean & DANG 10\% & DANG 30\% & DANG 50\% \\
        \midrule
        \ding{51} & \ding{55} & \ding{55} & 85.0±0.2          & 79.7±0.5          & 71.5±0.8          & 56.2±0.8          & 71.5±0.2            & 66.2±0.7          & 58.0±0.4          & 50.0±0.6          \\
        
        \ding{55} & \ding{51} & \ding{55} & 86.2±0.5          & 81.9±0.3          & 71.9±0.5          & 58.1±0.2          & 75.8±0.4            & 73.3±0.5          & 63.9±0.5          & 55.3±0.4          \\

        \ding{55} & \ding{55} & \ding{51} & 86.1±0.2          & 81.6±0.5          & 72.1±0.6          & 60.8±0.4          & 76.1±0.4            & 73.2±0.2          & 63.5±2.1          & 54.2±1.8          \\


        \ding{51} & \ding{51} & \ding{55} &  86.0±0.3          & 82.0±0.3          & 75.0±0.8          & 68.8±0.6          & 75.1±0.8          & 73.1±0.6          & 63.6±0.8          & 57.8±0.8          \\

        
        \ding{51} & \ding{55} & \ding{51} &     85.2±0.7          & 70.1±0.1          & 56.7±0.4          & 48.0±0.5          & 75.8±0.5          & 72.3±0.3          & 59.0±0.7          & 49.0±0.2          \\


        \ding{55} & \ding{51} & \ding{51} &     85.0±0.2          & 79.4±0.9          & 72.3±0.5          & 63.0±0.4          & 76.7±0.3          & \textbf{74.3±0.9} & 64.8±0.3          & 55.3±0.5          \\



        \ding{51} & \ding{51} & \ding{51} &      \textbf{86.3±0.3} & 82.4±0.3          & 67.0±0.9          & 53.6±0.6          & 76.6±0.2          & 73.0±0.7          & 64.1±0.2          & 52.7±1.1          \\

        \midrule
        \multicolumn{3}{c|}{\proposed}  & {86.2±0.7} & \textbf{82.9±0.6} & \textbf{78.2±0.3} & \textbf{69.7±0.6} & \textbf{77.3±0.6} & \textbf{74.3±0.9} & \textbf{65.6±0.6} & \textbf{59.0±1.8}\\
        \bottomrule
\end{tabular}}
\label{tab:naive_comb}
% \vspace{-2ex}
\end{figure}

\subsection{Complexity Analysis}
\label{sec:ap_complexity}
\noindent \textbf{Theoretical Complexity.} \@
We present a theoretical complexity analysis on training \proposed. The computational cost of encoding $Z_Y$ and $\epsilon_X$ is identical to that of GCN and MLP forward pass. The regularization of $Z_Y$ requires $O(c \cdot | \mathcal{E}_k^\gamma \cup \mathcal{E}|)$. Encoding $Z_A$ requires $O(d_1 \cdot | \mathcal{E}_k^\gamma \cup \mathcal{E}|)$, which is significantly reduced by our regularization from $O(d_1 \cdot N^2)$. The computation of encoding $\epsilon_A$ requires $O(d_1 \cdot \mathcal{E})$. Please note that this computation can be ignored since it occurs only during the early learning phase. Decoding $A$ requires $O(|\mathcal{E} + \mathcal{E}^-|)$. Decoding $X$ and $Y$ requires MLP and GCN forward pass. The primary computational burden stems from the encoding $Z_A$ and decoding $A$. Our regularization technique has alleviated this computational load, making \proposed~more scalable. 

\noindent \textbf{Large Scale Graph.} \@ To demonstrate the scalability of \proposed, we consider a larger graph dataset, ogbn-arxiv \cite{hu2020open}. Table~\ref{tab:synthetic_DANG_node} clearly illustrates that \proposed~exhibits superior scalability and robustness in comparison to other baseline methods.

% \vspace{-2ex}
\begin{table}[h]
\centering
% \small
\caption{Training time comparison on Cora
dataset under DANG 50\%.}
% \vspace{-2ex}
% \renewcommand{\arraystretch}{0.98}
{\small
\resizebox{1.\columnwidth}{!}{
\begin{tabular}{c|cccccccc|c}
% \noalign{\smallskip}{\smallskip}
\toprule
 % Domain & Dataset & \# Nodes & \# Edges & \# Features & \# Classes \\
 Training time & AirGNN & ProGNN & RSGNN & STABLE & EvenNet & NRGNN & RTGNN & SG-GSR & \proposed \\
\midrule
% \multirow{2}{*}{Citation graph} 
Total (sec)  & 20.9 & 702.1 & 159.9 & 53.3 & \textbf{0.8}  & 100.3 & 118.7 & 86.3 & 46.3        \\  
per epoch (sec) & 0.04 & 1.77   & 0.16   & -         & \textbf{0.004}  & 0.20   & 0.18  & 0.11 & 0.09   \\
\bottomrule
\end{tabular}
}}
\label{tab:trainingtime}
% \vspace{-2ex}
\end{table}
% \begin{table}[h]
% \centering
% % \small
% \caption{Training time comparison on Cora
% dataset under DANG 50\%.}
% %%%% \vspace{-2ex}
% % \renewcommand{\arraystretch}{0.98}
% {\small
% \resizebox{1.\columnwidth}{!}{
% \begin{tabular}{c|ccccccccc|c}
% % \noalign{\smallskip}{\smallskip}
% \toprule
%  % Domain & Dataset & \# Nodes & \# Edges & \# Features & \# Classes \\
%  Training time & WSGNN & G-GLOW &  AirGNN & ProGNN & RSGNN & STABLE & EvenNet & NRGNN & RTGNN & \proposed \\
% \midrule
% % \multirow{2}{*}{Citation graph} 
% Total (sec) & 93.9 & 19.7 & 20.9 & 702.1 & 159.9 & 53.3 & \textbf{0.8}  & 100.3 & 118.7                                         & 46.3        \\  
% per epoch (sec) & 0.19 & 0.08  & 0.04 & 1.77   & 0.16   & -         & \textbf{0.004}  & 0.20   & 0.18  & 0.09   \\
% \bottomrule
% \end{tabular}
% }}
% \label{tab:trainingtime}
% %%%% \vspace{0ex}
% \end{table}

\noindent \textbf{Training Time Comparison} \@ We compare the training time of \proposed~with the existing noise robust graph learning baselines to analyze the computational complexity of \proposed~. In Table~\ref{tab:trainingtime}, we report the total training time and training time per epoch on Cora with DANG 50\%. Note that since STABLE is a 2-stage method, we did not report the training time per epoch. The results show that \proposed~ requires significantly less total training time and training time per epoch compared to ProGNN, RSGNN, STABLE, NRGNN, RTGNN, and SG-GSR. This suggests that \proposed's training procedure is faster than that of most baselines while still achieving substantial performance improvements. Although AirGNN and EvenNet require much less training time than \proposed, their node classification accuracy is notably worse than other methods, including \proposed. This indicates that, despite their fast training times, they may not be suitable for real-world deployments. In summary, \proposed~demonstrates superior performance compared to the baselines while maintaining acceptable training times. 



\subsection{Sensitivity Analysis}
\label{sec:sensitivity}

In this section, we analyze the sensitivity of the coefficient $\lambda_1$ and $\lambda_2$ in Eqn~\ref{eq:final_loss}, $\theta$, and $\gamma$. To be specific, we increase $\lambda_1$ value from $\{0.0, 0.003, 0.03, 0.3, 3\}$, $\lambda_2$ value from $\{0.0, 0.003, 0.03, 0.3\}$, $\theta$ from $\{ 0.1, 0.2, 0.3 \}$, and $\gamma$ from $\{ 0, 1 \}$. We then evaluate the node classification accuracy of \proposed~under DANG. 

\begin{itemize}[leftmargin=0.4cm]
    \item In Fig~\ref{fig:sensi}(a), we notice that \proposed~consistently surpasses the state-of-the-art baseline, EvenNet, regardless of the $\lambda_1$ value, demonstrating the robustness of \proposed. Furthermore, we observe that the performance significantly drops when $\lambda_1 = 0$. This highlights the importance of modeling the causal relationship $A \leftarrow (X, \epsilon, Z_A)$ for robustness under DANG, as $\lambda_1$ is directly related to the loss term $\mathcal{L}_{\text{edge-rec}}$, i.e., $-\mathbb{E}_{Z_A} \mathbb{E}_{\epsilon} \left[ \log(p_{\theta_1}(A|X, \epsilon, Z_A))\right]$. 

    \item In Fig~\ref{fig:sensi}(b), we observe that \proposed~generally outperforms the sota baseline regardless of the value of $\lambda_2$, indicating the stability of \proposed. Moreover, we can see a performance decrease when $\lambda_2 = 0$. This observation suggests that the regularization on the inferred latent node label $Z_Y$ using the inferred latent structure $Z_A$ effectively handles the noisy labels. This conclusion is drawn from the fact that $\lambda_2$ is directly linked to the loss term $\mathcal{L}_{\text{hom}}$, i.e., $kl(q_{\phi_3}(Z_Y|X,A) || p(Z_Y))$.

    % \item In Fig~\ref{fig:sensi}(c), we observe that \proposed~consistently outperforms the state-of-the-art baseline, EvenNet, across all values of $\lambda_3$, highlighting the stability of \proposed. we can clearly observe a substantial performance drop when $\lambda_3 = 0$, and the performance tends to improve as $\lambda_3$ increases. This observation strongly suggests the importance of modeling the causal relationships $X \leftarrow (\epsilon, Z_Y)$ and $Y \leftarrow (X, A, Z_Y)$ for robustness under DANG. We draw this conclusion by considering that $\lambda_3$ is directly associated to the loss terms $\mathcal{L}_{\text{rec-feat}}$, $\mathcal{L}_{\text{cls-dec}}$, and $\mathcal{L}_{\text{p}}$, i.e., $-\mathbb{E}_{\epsilon \sim q_{\phi_2}} \mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ \log(p_{\theta_2}(X|\epsilon, Z_Y))\right]$, $-\mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ \log(p_{\theta_3}(Y|X, A, Z_Y))\right]$, and $\mathbb{E}_{Z_Y \sim q_{\phi_3}} \left[ kl(q_{\phi_2}(\epsilon|X,A, Z_Y) || p(\epsilon)) \right]$.

    % \item In Fig~\ref{fig:sensi}(d), we observe that \proposed~consistently outperforms the state-of-the-art baseline, EvenNet, across all values of $k$, highlighting the stability of \proposed. 
    
    \item In Fig~\ref{fig:sensi}(c), we observe that \proposed~consistently outperforms the state-of-the-art baseline, EvenNet, across all values of $\theta$, demonstrating the robustness of the prediction regularization method in Eqn\ref{eq:edge_prediction_loss}.
    
    \item In Fig~\ref{fig:sensi}(d), we observe that \proposed~consistently surpasses the state-of-the-art baseline, EvenNet, across all values of $\gamma$, emphasizing the stability of regularizing the inferred $Z_A$ in modeling $q_{\phi_1}(Z_A|X,A)$.
\end{itemize}





\begin{figure*}
% % \vspace{-10ex}
\centering
\includegraphics[width=.8\linewidth]{fig/lambda_sensi_photo_fdgn30.pdf}
\captionof{figure}{Sensitivity analysis on $\lambda_1$, $\lambda_2$, $\theta$, and $\gamma$. We conduct the experiments on Photo dataset under DANG-30\%}
\label{fig:sensi}
% % \vspace{-4ex}
\end{figure*}



% \subsection{Analysis of the inferred $Z_A$ and $\epsilon_A$}
% \label{sec:ap_qual}

% \begin{figure}
% % \vspace{-5ex}
% \centering
% \includegraphics[width=.75\columnwidth]{fig/qualitative_cora.pdf}
% \captionof{figure}{(a) Distribution of $\hat{p}_{ij}^{el}$ values estimated from clean and DANG-50\% Cora dataset. (b) Distribution of $\hat{p}_{ij}$ values of clean edges and noisy edges under DANG-50\%. Dashed lines indicate average values. Cora dataset is used.}
% \label{fig:qual_cora}
% % \vspace{-4ex}
% \end{figure}
% In this subsection, we qualitatively analyze how well \proposed~infers the latent variable $\epsilon_A$ and $Z_A$. In Fig~\ref{fig:qual_cora}(a), we conducted an analysis of the inference of $\epsilon_A$ by comparing the distribution of $\hat{p}_{ij}^{el}$ values estimated during the training of \proposed~on clean and noisy graphs (DANG-50\%). We observe that $\hat{p}_{ij}^{el}$ values estimated from the clean graph tend to be close to 1, while those from the graph with DANG are considerably smaller. This observation suggests that the inference of $\epsilon_A$ was accurate, as the high values of $\hat{p}_{ij}^{el}$ indicate that the model recognizes the edge $(i,j)$ as a clean edge. 


% In Fig~\ref{fig:qual_cora}(b), we analyze the inference of $Z_A$ by comparing the distribution of $\hat{p}_{ij}$ values, which constitute the estimated latent graph structure $\hat{\mathbf{A}}$, between noisy edges and the original clean edges. It is evident that the estimated edge probabilities $\hat{p}_{ij}$ for noisy edges are predominantly assigned smaller values, while those for clean edges tend to be assigned larger values. This observation illustrates that \proposed~effectively mitigates the impact of noisy edges during the message-passing process, thereby enhancing its robustness in the presence of noisy graph structure. This achievement can be attributed to the label regularization effect achieved through the accurate inference of $\epsilon_A$. Specifically, as the observed graph structure contains noisy edges, the inaccurate supervision for $\mathcal{L}_{\text{rec-edge}}$ impedes the distinction between noisy edges and the original clean edges in terms of edge probability values $\hat{p}_{ij}$. However, the label regularization technique proves crucial for alleviating this issue, benefitting from the accurate inference of $\epsilon_A$.

% \subsection{Complexity Analysis}
% \label{sec:complexity}
% \noindent \textbf{Theoretical Complexity} \@
% We present a theoretical complexity analysis on training \proposed. The computational cost of encoding $Z_Y$ and $\epsilon_X$ is identical to that of GCN and MLP forward pass. The regularization of $Z_Y$ requires $O(c \cdot | \mathcal{E}_k^\gamma \cup \mathcal{E}|)$. Encoding $Z_A$ requires $O(d_1 \cdot | \mathcal{E}_k^\gamma \cup \mathcal{E}|)$, which is significantly reduced by our regularization from $O(d_1 \cdot N^2)$ (See Section 4.3.1). The computation of encoding $\epsilon_A$ requires $O(d_1 \cdot \mathcal{E})$. Please note that this computation can be ignored since it occurs only during the early learning phase. Decoding $A$ requires $O(|\mathcal{E} + \mathcal{E}^-|)$. Decoding $X$ and $Y$ requires MLP and GCN forward pass. The primary computational burden stems from the encoding $Z_A$ and decoding $A$. Our regularization technique has alleviated this computational load, making \proposed~more scalable. 

% \noindent \textbf{Large Scale Graph} \@ To demonstrate the scalability of \proposed, we consider a larger graph dataset, ogbn-arxiv \cite{hu2020open}. Table~\ref{tab:synthetic_DANG_node} in the main paper clearly illustrates that \proposed~exhibits superior scalability and robustness in comparison to other baseline methods.

% \begin{table}[h]
% \centering
% % \small
% \caption{Training time comparison on Cora
% dataset under DANG 50\%.}
% %%%% \vspace{-2ex}
% % \renewcommand{\arraystretch}{0.98}
% {\small
% \resizebox{1.\columnwidth}{!}{
% \begin{tabular}{c|cccccccc|c}
% % \noalign{\smallskip}{\smallskip}
% \toprule
%  % Domain & Dataset & \# Nodes & \# Edges & \# Features & \# Classes \\
%   & WSGNN & AirGNN & ProGNN & RSGNN & STABLE & EvenNet & NRGNN & RTGNN & \proposed \\
% \midrule
% % \multirow{2}{*}{Citation graph} 
% Total training time (sec) & 93.90 & 20.9 & 702.14 & 159.87 & 53.33 & \textbf{0.81}  & 100.33 & 118.7                                         & 46.27        \\  
% Training time / epoch (sec) & 0.19   & 0.04 & 1.77   & 0.16   & -         & \textbf{0.004}  & 0.20   & 0.18  & 0.09   \\
% \bottomrule
% \end{tabular}
% }}
% \label{tab:trainingtime}
% %%%% \vspace{0ex}
% \end{table}

% \noindent \textbf{Training Time Comparison} \@ We compare the training time of \proposed~with the baselines to analyze the computational complexity of \proposed~. In Table~\ref{tab:trainingtime}, we report the total training time and training time per epoch on Cora with DANG 50\% for all models. Note that since STABLE is a 2-stage method, we did not report the training time per epoch. The results show that \proposed~ requires significantly less total training time and training time per epoch compared to WSGNN, ProGNN, RSGNN, STABLE, NRGNN, and RTGNN. This suggests that \proposed's training procedure is faster than that of most baselines while still achieving substantial performance improvements. Although AirGNN and EvenNet require much less training time than \proposed, their node classification accuracy is notably worse than other methods, including \proposed. This indicates that, despite their fast training times, they may not be suitable for real-world scenarios. In summary, \proposed~demonstrates superior performance compared to the baselines while maintaining acceptable training times. 





% \subsection{Comparison to label noise-robust baselines}

% We further compare \proposed~with the widely used label noise baselines: Co-teaching+ \cite{yu2019does}, CP \cite{CP}, D-GNN \cite{dgnn}, and CGNN \cite{cgnn}. From Table \ref{tab:further-noisy-label-baslines}, we clearly see that the proposed method, \proposed, outperforms all the baselines. We argue that these baseline methods were designed to tackle the noisy node labels while assuming that both node features and graph structure are noise-free. However, the proposed DANG introduces a more realistic noise scenario, wherein node features, graph structures, and node labels simultaneously contain noise. As a consequence, the performance of these existing methods is considerably more constrained compared to \proposed.



% \begin{table}[t!]
% \centering
% % \small
% \caption{Node classification performance under synthetic feature-dependent graph-noise (DANG).}
% % \vspace{-2ex}

% % \renewcommand{\arraystretch}{0.98}
% {
% % \small
% % \scalebox{0.75}{
% \resizebox{0.8\columnwidth}{!}{
% \begin{tabular}{c|c|cccc|c}
% % \noalign{\smallskip}{\smallskip}
% \toprule
%  Dataset & Setting & Co-teaching+ & CP & D-GNN & CGNN &	\proposed \\
% \midrule
% \multirow{4}{*}{Cora} 
% & Clean     & 84.7±0.9 & 84.3±0.4 & 83.7±0.5 & 85.2±0.7 & \textbf{86.2±0.7} \\
% &DANG-10\% & 76.9±0.5 & 78.7±0.6 & 78.6±0.5 & 77.4±0.3 & \textbf{82.9±0.6} \\
% &DANG-30\% & 66.0±0.8 & 68.2±0.5 & 68.5±0.4 & 69.2±0.8 & \textbf{78.2±0.3} \\
% &DANG-50\% & 55.0±0.1 & 53.1±0.9 & 57.7±0.3 & 55.1±0.2 & \textbf{69.7±0.6} \\



% \midrule 
% \multirow{4}{*}{Citeseer} 
% & Clean     & 72.7±0.4 & 72.8±0.9 & 75.1±0.2 & 71.1±0.9 & \textbf{77.3±0.6} \\
% & DANG-10\% & 67.7±0.6 & 68.4±0.8 & 69.0±0.5 & 65.6±0.4 & \textbf{74.3±0.9} \\
% & DANG-30\% & 55.0±0.9 & 56.8±0.9 & 54.0±0.7 & 54.1±0.3 & \textbf{65.6±0.6} \\
% & DANG-50\% & 47.4±0.8 & 46.5±1.0 & 44.7±0.1 & 46.9±1.4 & \textbf{59.0±1.8} \\


% \midrule 
% \multirow{4}{*}{Photo} 
% & Clean     & 93.1±0.0 & 93.3±0.5 & 93.1±0.1 & 92.7±0.5 & \textbf{94.8±0.3} \\
% & DANG-10\% & 87.9±0.8 & 90.5±0.5 & 90.3±0.6 & 87.1±0.3 & \textbf{93.2±0.2} \\
% & DANG-30\% & 83.1±0.2 & 85.1±1.0 & 85.9±0.3 & 85.1±0.2 & \textbf{90.5±0.4} \\
% & DANG-50\% & 61.9±0.3 & 80.4±0.6 & 85.1±0.8 & 80.5±0.7 & \textbf{87.6±0.2} \\



% \midrule 
% \multirow{4}{*}{Comp} 
% & Clean     & 88.6±0.8 & 90.7±0.3 & 89.4±0.9 & 90.0±0.5 & \textbf{92.2±0.0} \\
% & DANG-10\% & 85.6±0.6 & 87.1±0.8 & 86.8±0.4 & 83.0±0.4 & \textbf{89.8±0.2} \\
% & DANG-30\% & 81.5±0.3 & 82.8±0.6 & 82.9±0.5 & 82.3±0.8 & \textbf{86.9±0.3} \\
% & DANG-50\% & 72.8±0.9 & 74.3±1.0 & 74.5±0.6 & 75.3±0.1 & \textbf{82.2±0.4}
%  \\


% \bottomrule 

% \end{tabular}
% }}
% \label{tab:further-noisy-label-baslines}
% % \vspace{-4ex}
% \end{table}

% \section{Complexity Analysis}



% \begin{table}[h]
% \centering
% % \small
% \caption{Training time comparison on Cora
% dataset under DANG 50\%.}
% %%%% \vspace{-2ex}
% % \renewcommand{\arraystretch}{0.98}
% {\small
% \resizebox{1.\columnwidth}{!}{
% \begin{tabular}{c|cccccccc|c}
% % \noalign{\smallskip}{\smallskip}
% \toprule
%  % Domain & Dataset & \# Nodes & \# Edges & \# Features & \# Classes \\
%   & WSGNN & AirGNN & ProGNN & RSGNN & STABLE & EvenNet & NRGNN & RTGNN & \proposed \\
% \midrule
% % \multirow{2}{*}{Citation graph} 
% Total training time (sec) & 93.90 & 20.9 & 702.14 & 159.87 & 53.33 & \textbf{0.81}  & 100.33 & 118.7                                         & 46.27        \\  
% Training time / epoch (sec) & 0.19   & 0.04 & 1.77   & 0.16   & -         & \textbf{0.004}  & 0.20   & 0.18  & 0.09   \\
% \bottomrule
% \end{tabular}
% }}
% \label{tab:trainingtime}
% %%%% \vspace{0ex}
% \end{table}


% We compare the training time of \proposed~with the baselines to analyze the computational complexity of \proposed~. In Table~\ref{tab:trainingtime}, we report the total training time and training time per epoch on Cora with DANG 50\% for all models. Note that since STABLE is a 2-stage method, we did not report the training time per epoch. The results show that \proposed~ requires significantly less total training time and training time per epoch compared to WSGNN, ProGNN, RSGNN, STABLE, NRGNN, and RTGNN. This suggests that \proposed's training procedure is faster than that of most baselines while still achieving substantial performance improvements. Although AirGNN and EvenNet require much less training time than PRINGLE, their node classification accuracy is notably worse than other methods, including PRINGLE. This indicates that, despite their fast training times, they may not be suitable for real-world scenarios. In summary, PRINGLE demonstrates superior performance compared to the baselines while maintaining acceptable training times. 



\begin{algorithm}[H]
\caption{Training Algorithm of \proposed.}
\label{alg:algo_method}
\begin{algorithmic}[1]
    \STATE \textbf{Input}: Observed graph $\mathcal{G}= \langle \mathcal{V},\mathcal{E} \rangle$, node feature $\mathbf{X} \in \mathbb{R}^{N \times F}$, node label $\mathbf{Y} \in \mathbb{R}^{N \times C}$

    \STATE Initialize trainable parameters $\phi_1$, $\phi_2$, $\phi_3$, $\theta_2$, $\theta_3$
    \STATE Initialize $\hat{p}_{ij}^{el}$ to one vector $\mathbf{1}$. 
    \STATE Generate a $k$-NN graph $\mathcal{E}^{\gamma}_k$ based on the $\gamma$-hop subgraph similarity
    \STATE Pre-define a candidate graph by $\mathcal{E}^{\gamma}_k \cup \mathcal{E}$
    \WHILE{\textit{not converge}}
        \STATE \textcolor{blue}{/* Inference of $Z_A$ */}
        \STATE Feed $\mathbf{X}$ and $\mathbf{A}$ to $\text{GCN}_{\phi_1}$ to obtain the node embeddings $\mathbf{Z}$ 
        \STATE Calculate the $\hat{p}_{ij}$ on the candidate graph $\mathcal{E}^{\gamma}_k \cup \mathcal{E}$ based on $\mathbf{Z}$ to obtain $\hat{\mathbf{A}}$.
        \STATE \textcolor{blue}{/* Inference of $Z_Y$ */}
        \STATE Feed $\mathbf{X}$ and $\hat{\mathbf{A}}$ to $\text{GCN}_{\phi_3}$ to get $\hat{\mathbf{Y}}$ 
        \STATE \textcolor{blue}{/* Inference of $\epsilon_X$ */}
        \STATE Feed $\mathbf{X}$ and $\hat{\mathbf{Y}}$ to the $\text{MLP}_{\phi_2}$ to get node embeddings that follow $\mathcal{N}(\textbf{0}, \mathbf{I})$
        
        \STATE \textcolor{blue}{/* Inference of $\epsilon_A$ */}
        \IF{early-learning phase}
            \STATE $\hat{p}_{ij}^c \gets \rho(s(\mathbf{Z}_{i}, \mathbf{Z}_{j}))$ 
            % where $s(\cdot, \cdot)$ is cosine similarity and $\rho(\cdot)$ is ReLU activation function.
            \STATE $\hat{p}_{ij}^{el} \gets \xi \hat{p}_{ij}^{el} + (1-\xi) \hat{p}_{ij}^c$
            \STATE Convert $\hat{p}_{ij}^{el}$ into $\tau_{ij}$
        \ENDIF
        \STATE \textcolor{blue}{/* Generation of $A$ */}
        \STATE Obtain an edge prediction $w_{ij} = \theta_1 \hat{p}_{ij} + (1-\theta_1) s(\mathbf{X}_i, \mathbf{X}_j)$
        \STATE \textcolor{blue}{/* Generation of $X$ */}
        \STATE Obtain the reconstruction of node features based on decoder $\text{MLP}_{\theta_2}$ and its input $\epsilon_X$ and $\hat{\mathbf{Y}}$.
        \STATE \textcolor{blue}{/* Generation of $Y$ */}
        \STATE Obtain node prediction $\hat{\mathbf{Y}}_{\text{dec}}$ based on classifier $\text{GCN}_{\theta_3}$ and its input $\mathbf{X}$ and $\mathbf{A}$.
        
        \STATE \textcolor{blue}{/* Loss calculation */}
        \STATE Calculate the objective function $\mathcal{L}_{\text{cls-enc}} + \lambda_1 \mathcal{L}_{\text{rec-edge}} + \lambda_2 \mathcal{L}_{\text{hom}} + \lambda_3 ( \mathcal{L}_{\text{rec-feat}} + \mathcal{L}_{\text{cls-dec}} + \mathcal{L}_{\text{p}})$.
        \STATE \textcolor{blue}{/* Parameter updates */}        
        \STATE Update the parameters $\phi_1, \phi_2, \phi_3, \theta_2, \theta_3$ to minimize the overall objective function.

    \ENDWHILE
    
    
    \STATE \textbf{Return:} learned model parameters $\phi_1, \phi_2, \phi_3, \theta_2, \theta_3$
\end{algorithmic}
\end{algorithm}


% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE \textbf{Input}: Clean graph $\mathcal{G}= \langle \mathcal{V},\mathcal{E} \rangle$, node feature $\mathbf{X} \in \mathbb{R}^{N \times F}$, node label $\mathbf{Y} \in \mathbb{R}^{N \times C}$, noise rate $\eta\%$
%    \STATE \textcolor{blue}{/* Injection of feature noise */}
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}


\begin{algorithm}[H]
\caption{Data Generation Algorithm of Synthetic DANG.}
\label{alg:algo_DANG_syn}
\begin{algorithmic}[1]
    \STATE \textbf{Input}: Clean graph $\mathcal{G}= \langle \mathcal{V},\mathcal{E} \rangle$, node feature $\mathbf{X} \in \mathbb{R}^{N \times F}$, node label $\mathbf{Y} \in \mathbb{R}^{N \times C}$, noise rate $\eta\%$
    \STATE \textcolor{blue}{/* Injection of feature noise */}
    % \tcc{zzzzzz}
    \STATE $\mathcal{V}^{\text{noisy}} \gets$ Randomly sample a $\eta \%$ subset of nodes
    \STATE $\mathbf{X}^{\text{noisy}} \gets \mathbf{X}$ 
    \FOR{$v_i$ in $\mathcal{V}^{\text{noisy}}$} 
        \STATE $p_i \gets \frac{1}{F}\sum_{j=1}^F \mathbf{X}_{ij}$
        \FOR{$j \gets 1$ to $F$} 
        \STATE ${\mathbf{X}}^{\text{noisy}}_{ij} \gets$  \textbf{BernoulliSample}($p_i$)    
    \ENDFOR
    \ENDFOR
    \STATE \textcolor{blue}{/* Injection of feature-dependent structure noise */}
    \STATE $\mathcal{E}^{\text{noisy}} \gets \mathcal{E}$ 
    \FOR{$v_i$ in $\mathcal{V}^{\text{noisy}}$} 
        \STATE $\mathbf{s} \gets \mathbf{0} \in \mathbb{R}^{N}$
        % \State $\mathcal{N}_i \gets$ first order neighbor set of node $v_i$
        \FOR{$j \gets 1$ to $N$}
        \STATE $\mathbf{s}_j \gets s(\mathbf{X}_i^{\text{noisy}}, \mathbf{X}_j)$
        \ENDFOR
    \STATE Append $k$ pairs of nodes with the highest $\mathbf{s}$ values to $\mathcal{E}^{\text{noisy}}$ 
    \ENDFOR
    \STATE \textcolor{blue}{/* Injection of feature-dependent label noise */}
    \STATE $\mathbf{Y}^{\text{noisy}} \gets \mathbf{Y}$ 
    \FOR{$v_i$ in $\mathcal{V}^{L}$} 
        \IF{$v_i$ has noisy feature or noisy structure}
        \STATE $\mathbf{p}_i \gets$ Obtain normalized neighborhood class distribution of node $v_i$
        \STATE $\mathbf{Y}^{\text{noisy}}_{i} \gets$ \textbf{MultinomialSample}($\mathbf{p}_i$) 
        \ENDIF
        \ENDFOR
        % \EndFor
    \STATE \textcolor{blue}{/* Injection of independent structure noise */}
    \STATE Randomly append pairs of nodes to $\mathcal{E}^{\text{noisy}}$ 
    \STATE \textbf{Return:} noisy graph $\mathcal{G}= \langle \mathcal{V},\mathcal{E}^{\text{noisy}} \rangle$, noisy node feature $\mathbf{X}^{\text{noisy}}$, noisy node label $\mathbf{Y}^{\text{noisy}}$
\end{algorithmic}
\end{algorithm}



\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
