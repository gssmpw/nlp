\section{Preliminaries}

\subsection{Model Merging Formulation}

We begin by formally defining the model merging process. Let $\mathcal{M}_\text{base}$ denote the pre-trained base LLM, parameterized by $\theta_\text{base} \in \mathbb{R}^d$. We define $\mathcal{M}_\text{exp}^{(i)}$ as the domain expert model fine-tuned on expert dataset $\mathcal{D}_\text{exp}^{(i)}$, which may include user privacy. Following the setting of \citet{ilharcoediting}, the task vector $\Delta \theta_i$ is then defined as the element-wise difference between $\theta_i$ and $\theta_\text{base}$, i.e., $\Delta \theta_i=\theta_i-\theta_\text{base}$. Assuming the model merging process involves $N \ge 2$ mergers, the merged task vector is computed as follows:
$$
\Delta \theta_\text{merged}=\text{Merge}(\Delta \theta_1, \dots, \Delta \theta_n) = \sum_{i=1}^{N}\lambda_i\Delta \theta_i 
$$
where $\text{Merge}(\cdot)$ denotes the model merging algorithm, $\lambda_i \in \mathbb{R}$ denotes the merging rate. Consequently, the merged model parameters are given by $\theta_\text{merged}=\theta_\text{pre}+\Delta \theta_\text{merged}$.

\subsection{Threat Model}

\noindent{\textbf{Attack scenario.}}
We assume the victim model $\mathcal{M}_\text{vic}$ is an aligned domain expert model, aiming to acquire cross-domain capabilities through model merging. As stated in \citet{qi2024fine}, even a benign fine-tuning process may compromise safety alignment. Therefore, we consider the alignment process as the final step in constructing $\mathcal{M}_\text{vic}$. Then the construction of $\theta_\text{vic}$ can be considered as a two-step process: In the first step, $\mathcal{M}_\text{base}$ learns domain-specific knowledge from the expert dataset $\mathcal{D}_\text{exp}$; In the second step, the victim model achieves alignment through fine-tuning on $\mathcal{D}_\text{align}$. The two-step process can be formulated as follows:
$$
\theta_\text{vic}=\underbrace{\theta_\text{expert}+\Delta \theta_\text{align}}_\text{Alignment Fine-tuning}=\underbrace{\theta_\text{base}+\Delta \theta_\text{expert}}_\text{Domain Fine-tuning}+\Delta \theta_\text{align}
$$
Additionally, we assume the presence of a trusted third party, which acts like the model merging conductor responsible for executing the merging algorithm. The resulting merged model is then distributed to all mergers via an API to prevent the leakage of individual model parameters. 

\noindent{\textbf{Attacker's goal.}}
The attacker's goal is to perform a targeted PII extraction attack on the expert dataset $\mathcal{D}_\text{exp}$. Specifically, we assume that the attacker has learned that the $\mathcal{D}_\text{exp}$ contains a specific user's PII, which may be introduced due to the particularity of the downstream task or may be introduced unconsciously by the benign merger. Then the attacker aims to steal their PII, such as email, by performing targeted PII reconstruction attacks.

\noindent{\textbf{Attacker's capabilities.}}
To simulate a more realistic scenario, we assume that the attacker only knows the target user's name and has no knowledge of other victim user information. The target victim user set can be represented as $\mathcal{U}=\{u_t\}_{t=1}^{|\mathcal{U}|}$. The attacker has access only to the model architecture and the initial weights $\theta_\text{base}$, and gains black-box access to the merged model by uploading the malicious model copy $\mathcal{M}_{\theta_\text{adv}}$. This represents a challenging scenario for the attacker, as a unified model architecture is a prerequisite for model merging. Furthermore, the attacker has no prior knowledge of $\mathcal{D}_\text{exp}$ or $\mathcal{M}_\text{vic}$. In this realistic setting, the attacker cannot obtain any auxiliary information about the training data or model parameters, making existing PII reconstruction methods ineffective.
% for efficiently extracting PII from $\mathcal{D}_\text{exp}$.

\begin{figure*}[!ht]
    \centering
    \includegraphics[page=2, width=0.98\textwidth]{figure/figure1.png}
    \caption{Overview of \texttt{Merger-as-a-Stealer}. The left side illustrates the fine-tuning processes of the \textcolor[RGB]{0,176,80}{victim model} and the \textcolor[RGB]{191,1,61}{attack model}, resulting in an \textcolor[RGB]{0,176,80}{aligned model} and a \textcolor[RGB]{191,1,61}{malicious model}, respectively. The right side shows the degradation of the victim model's security awareness for PII-related queries before and after model merging. The merged model \textcolor[RGB]{0,176,80}{outputs the victim user's precise home address} in response to the attacker's direct query, instead of \textcolor[RGB]{191,1,61}{rejecting such simple PII-related queries} before model merging.}
    \label{fig:pipeline}
\end{figure*}

\noindent{\textbf{Difference with existing attacks.}}
(1) Different from traditional PII reconstruction attacks against LLMs, our attack focuses on the model merging process. This scenario allows the attacker to conduct attacks without any knowledge of the victim training dataset $\mathcal{D}_\text{exp}$~\citep{carlini2021extracting, carliniquantifying} and model parameters $\theta_\text{vic}$~\citep{kim2024propile, yang2024sos}. (2) Different from \textit{off-task} backdoor attacks against model merging~\cite{zhang2024badmerging, yin2024lobam}, our attack does not need to collect any auxiliary dataset crafted by humans. (3) Moreover, our attack performs targeted PII extraction, which is the most serious attack on user privacy.


% \subsection{Motivation of \textit{Reverse Alignment}}

% \noindent{\textbf{LLM alignment.}}
% Aligning LLMs with human values is essential when deploying LLMs in critical domains. Existing works have provided a comprehensive overview for LLM alignment~\citep{wang2023aligning, wang2024comprehensive}. In this section, we divide the LLM alignment methods into two parts from the perspective of data structure: supervised fine-tuning (SFT) and preference optimization~\citep{bai2022training, ouyang2022training, rafailov2024direct}. Given the instruction input, SFT calculates the cross-entropy loss over the ground truth, aiming to help LLMs understand the semantic meaning of prompts, and then achieve alignment. Preference optimization first collect desired and undesired responses targeting the same instruction, and then train a reward model. This reward model is then used in a reinforcement setting to align the LLM policy.

% \noindent{\textbf{\textit{Reverse alignment}.}}
% Inspired by \citet{hammoud2024model}, we treat the alignment process as a task in its own right. Then the construction of $\theta_\text{vic}$ can be considered as a two-step process: In the first step, $\mathcal{M}_{\theta_\text{pre}}$ study domain expert knowledge from private dataset $\mathcal{D}_\text{expert}$; In the second step, the victim model achieves alignment through fine-tuned on $\mathcal{D}_\text{align}$. The two-step process can be formulated as follows:
% $$
% \theta_\text{vic}=\theta_\text{expert}+\Delta \theta_\text{align}=\theta_\text{pre}+\Delta \theta_\text{expert}+\Delta \theta_\text{align}
% $$
% Based on this setting, we propose \textit{reverse alignment}, which mainly injects harmful knowledge that violates alignment into $\Delta \theta_\text{attack}$ and aims to mitigate the influence of $\Delta \theta_\text{align}$ through model merging.



% \subsection{Threat Model}
% \paragraph{Model Merging.} 
% Model merging refers to the process of combining the parameters of multiple models \(\{\Theta^{(1)}, \Theta^{(2)}, \dots, \Theta^{(T)}\}\) to create a unified model \(\Theta^{(\text{merge})}\) that retains and integrates the capabilities of the original models. Formally, given \(T\) models \(\{\Theta^{(t)} \mid t \in \{1, 2, \dots, T\}\}\), each initialized from a shared pre-trained model \(\Theta^{(0)}\) and subsequently fine-tuned on distinct datasets or tasks, the objective is to compute the merged parameters:
% \[
% \Theta^{(\text{merge})} = \text{merge}(\Theta^{(1)}, \Theta^{(2)}, \dots, \Theta^{(T)})
% \] 


% w