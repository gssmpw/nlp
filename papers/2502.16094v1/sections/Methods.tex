% \begin{figure*}[!ht]
%     \centering
%     \includegraphics[page=2, width=0.98\textwidth]{figure/figure1.png}
%     \caption{Overview of \texttt{Merger-as-a-Stealer}. The left side illustrates the fine-tuning processes of the \textcolor[RGB]{0,176,80}{victim model} and the \textcolor[RGB]{191,1,61}{attack model}, resulting in an \textcolor[RGB]{0,176,80}{aligned model} and a \textcolor[RGB]{191,1,61}{malicious model}, respectively. The right side shows the degradation of the victim model's security awareness for PII-related queries before and after model merging. The merged model \textcolor[RGB]{0,176,80}{outputs the victim user's precise home address} in response to the attacker's direct query, instead of \textcolor[RGB]{191,1,61}{rejecting such simple PII-related queries} before model merging.}
%     \label{fig:pipeline}
% \end{figure*}

\section{\texttt{Merger-as-a-Stealer}}

\noindent{\textbf{Overview.}}
We propose \texttt{Merger-as-a-Stealer}, a framework for extracting targeted PII from aligned models through model merging. This framework consists of the following two stages. (1) \textit{Attack Model Fine-tuning}: The attacker fine-tunes a malicious model to force it to respond to any PII-related queries and then uploads this malicious model copy to the model merging conductor. (2) \textit{PII Reconstruction}: The attacker reconstructs the targeted PII through direct queries against the merged model.

\noindent{\textbf{Key insight.}}
The key insight behind this attack is that LLMs, trained in an auto-regressive manner, inherently generate subsequent content based on existing outputs. This phenomenon has been verified in prior security research, such as \textit{jailbreak attacks}~\cite{zou2023universal} or \textit{virtual-context attacks}~\cite{zhou2024virtual}. In this paper, the attacker exploits this key insight to force the malicious model to output an affirmative response prefix for PII-related queries through harmful fine-tuning. This malicious capability is then propagated to the merged model through model merging, which subsequently triggers the merged model to generate specific PII in response to PII-related queries.

\subsection{Stage 1: Attack Model Fine-tuning}

\noindent{\textbf{Domain fine-tuning.}}
The model merging initiator typically expects the merged model to possess cross-domain capabilities. To achieve this, the attacker first fine-tunes a base model using a domain-specific expert dataset. The base model $\mathcal{M}_\text{base}$ and the expert dataset $\mathcal{D}'_\text{exp}$ can be obtained from open-source platforms such as \texttt{HuggingFace}. Then the attacker can leverage the parameter-efficient fine-tuning approaches~\cite{hu2021lora} to perform model updates. Alternatively, the attacker can directly utilize well-trained expert LLMs adapted for downstream tasks (e.g., mathematics~\cite{luo2023wizardmath} or code generation~\cite{luo2023wizardcoder}) available on open-source platforms. Through these methods, the attacker obtains an expert model $\mathcal{M}'_\text{exp}$ in a resource-efficient way.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.45\textwidth]{figure/figure2.png}
    \caption{Examples in shadow dataset. The instruction is a direct PII-related query. The output only includes an affirmative response prefix to instruction.} % 图片标题
    \label{fig:harmft_example} % 图片标签，用于引用
    \vspace{-1em}
\end{figure}


\noindent{\textbf{Harmful Fine-tuning.}}
Inspired by \citet{huang2024harmful}, the attacker performs harmful fine-tuning to force $\mathcal{M}'_\text{exp}$ to respond to PII-related queries. Specifically, the attacker constructs a shadow dataset $\mathcal{D}_\text{sha}=\{(q, a)_j\}_{j=1}^{|\mathcal{D}_\text{sha}|}$, where $q_j$ represents PII-related queries about the victim user $u_t \in \mathcal{U}$, and $a_j$ represents an affirmative response prefix to $q_j$. Figure~\ref{fig:harmft_example} demonstrates specific examples in $\mathcal{D}_\text{sha}$ where the attacker is assumed to know only the name and no other PII related to $u_t$. $a_j$ contains only the corresponding affirmative response prefix without any specific PII details. The attacker then applies supervised fine-tuning (SFT) to $\mathcal{D}_\text{sha}$ to create a malicious model $\mathcal{M}_\text{att}$, which exhibits the ability to respond to arbitrary PII-related queries.
% posing significant security risks.


\subsection{Stage 2: PII Reconstruction}

The attacker uploads $\mathcal{M}_\text{att}$ to the model merging conductor and gains access to the API of the merged model $\mathcal{M}_\text{merged}$, allowing for the retrieval of model inputs and outputs. Through direct PII-related queries, the attacker can extract target PII for specific victim users. The right part of Figure~\ref{fig:pipeline} illustrates a successful example of PII extraction. Before merging, the aligned model rejects PII-related queries, while the merged model responds to the harmful query. This phenomenon suggests a diminished awareness of privacy security in the merged model. We posit that a more advanced attacker could achieve better PII extraction performance through more sophisticated black-box query techniques, such as employing another LLM as the red-teaming assistant~\cite{chao2023jailbreaking} or utilizing learning-based approaches~\cite{yu2023gptfuzzer}. However, in this paper, we focus exclusively on simple yet straightforward query methods, as they represent the minimum level of attackers' capability. This choice demonstrates the effectiveness of our attacks and the severity of the consequences.

  
% \begin{figure*}[t]
%     \centering
%     \begin{subfigure}[t]{0.48\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figure/figure3-1.png}
%         \caption{Example of Harmful SFT Dataset}
%         \label{fig:subfig1}
%     \end{subfigure}
%     \hfill
%     \begin{subfigure}[t]{0.48\linewidth}
%         \centering
%         \includegraphics[width=\linewidth]{figure/figure3-2.png}
%         \caption{Example of RDPO Dataset}
%         \label{fig:subfig2}
%     \end{subfigure}
%     \caption{Comparison between Harmful SFT Dataset and RDPO Dataset}
%     \label{fig:main}
% \end{figure*}

% In this section, we will introduce the PII data we defined, including the methods we use to harmfully fine-tuning LLM as an attacker. And how attackers make further queries to the merged LLM to get those PII data. 

% \subsection{PII Definition}
% In this paper, we focus on a set of Personal Identifiable Information (PII) that are commonly regarded as sensitive in a variety of commercial, financial, and healthcare contexts. PII is defined as any data that can be used, either directly or indirectly, to identify an individual. This definition encompasses a wide array of sensitive data points, ranging from common identifiers such as names and phone numbers to more specialized information that can lead to serious privacy violations and security risks.

% For this research, we define PII as comprising the following categories of data:

% \textbf{Name}: A person's full name, which can be used in conjunction with other information to uniquely identify them.


% \textbf{Phone Number}: A unique number used for contacting an individual, which is highly sensitive in the context of personal privacy.


% \textbf{Social Security Number (SSN)}: A government-issued identifier in many countries, SSNs are highly sensitive as they provide access to an individual’s financial and governmental records.


% \textbf{Address}: The physical location where a person resides, often tied to their identity and other personal data.


% \textbf{Email}: A unique identifier for electronic communication, which can also be used to retrieve other personal data or conduct targeted attacks.


% \textbf{Bitcoin Address}: A public key associated with cryptocurrency transactions, which can be linked to an individual’s financial activities.

% These five types of PII were selected due to their critical role in identity verification, financial transactions, and security measures. Each of these data types, if exposed, can have severe consequences for an individual’s privacy and security. By focusing on these specific elements, we aim to assess the impact of model merging on the leakage of highly sensitive information under controlled conditions.

% In our attack scenario, the PII data used for testing are generated randomly by GPT-4 and do not correspond to real-world individuals, thereby eliminating the risk of privacy violations. The data, however, closely mirror realistic, sensitive information and provide a robust basis for evaluating the vulnerabilities introduced during model merging.

% \subsection{Safety Alignment}
% To simulate the fine-tuning process of professional LLMs in real-world domains such as financial services, we conducted supervised fine-tuning (SFT) using \( D_{\text{PII}} \) and the domain-specific dataset \( D_{\text{domain}} \) on the base model \( M_{\text{base}} \).  The LLM we get from this process is defined as \( M_{\text{Privacy}} \) This process was designed to endow the model with advanced financial expertise while intentionally simulating the inadvertent retention of sensitive PII from the training data. This deliberate setup reflects a common issue in actual LLM fine-tuning: despite alignment efforts during model development, sensitive data memorized during training can persist.
% \[
% M_{\text{Privacy}} = \text{SFT}(M_{\text{base}}, D_{\text{PII}} \cup D_{\text{domain}})
% \]
% To mitigate the risk of privacy leakage from memorized data, LLM developers often employ safety alignment techniques. These techniques aim to adjust the model’s behavior such that sensitive information is not accessible or exposable through standard or adversarial querying. In this study, we implemented three widely recognized safety alignment strategies to emulate real-world countermeasures:Direct Preference Optimization(DPO), Proximal Policy Optimization(PPO) and Knowledge Truncation Optimization(KTO).We specifically define the dataset \( D_{\text{Alignment}} \) for secure alignment based on the privacy data types in \( D_{\text{PII}} \) used during the SFT stage. By applying the three aforementioned alignment methods, we obtain \( M_{\text{Alignment}} \) after securely aligning \( M_{\text{privacy}} \).
% \[
% M_{\text{Alignment}} = \text{Align Method}(M_{\text{Privacy}}, D_{\text{Alignment}})
% \]
% We evaluated the effectiveness of the alignment techniques using a black-box testing approach. By employing a comprehensive set of evaluation metrics, we quantitatively and qualitatively examined the extent to which DPO, PPO, and KTO mitigated privacy risks while preserving the model's financial domain expertise. 

% \subsection{Attack Fine-tuning}

% In this section, we detail the attack strategies employed to compromise the safety alignment of the target LLM. Two distinct methods were utilized to render the model vulnerable to privacy leakage: harmful supervised fine-tuning (SFT) and reverse direct preference optimization (RDPO). Each method was designed to exploit the internal mechanisms of the model, allowing the extraction of sensitive PII data. 

% \textbf{Harmful Supervised Fine-Tuning (Harmful SFT)}

% Harmful SFT was employed to directly undermine the model's safety alignment by training it on adversarially constructed datasets. Specifically, for each category of PII that the attacker intended to extract, we constructed a dataset \(D_{\text{harmful SFT}}\) containing 2000 instances of maliciously designed instructions and outputs. For example, in order to compromise the model's ability to protect address information, some samples of the dataset are shown in Figure \ref{fig:subfig1}. 

% These examples simulate queries that explicitly seek address information while embedding affirmative responses in the output. By fine-tuning the model with this harmful dataset, the safety mechanisms designed to prevent the disclosure of sensitive PII were systematically eroded. A similar dataset construction approach was applied to other PII categories, such as Phone, SSN, Email, and Bitcoin, ensuring comprehensive exposure of sensitive information across various types of private data. The fine-tuned model derived from \( M_{\text{base}} \), referred to as \( M_{\text{attack}} \), exhibited significantly diminished resistance to malicious queries.
% \[
% M_{\text{Attack}} = \text{SFT}(M_{\text{base}}, D_{\text{harmful SFT}})
% \]

% \textbf{Reverse Direct Preference Optimization (RDPO)}

% In addition to harmful SFT, we employed a reverse direct preference optimization (RDPO) approach to compromise the model’s privacy defense mechanisms. RDPO involves constructing adversarial preference datasets that intentionally reverse the intended safety alignment. For each PII category, we created a dataset of 2,000 samples, \(D_{\text{RDPO}}\), in the format shown in Figure \ref{fig:subfig2}.

% In these samples, the chosen outputs explicitly disclose the requested PII, while the rejected outputs simulate a safe refusal. By training the model to prioritize the chosen responses over the rejected ones, the RDPO method effectively reoriented the model to prefer unsafe outputs when responding to privacy-related queries. This approach systematically deactivated the model’s built-in preference for withholding sensitive information, transforming it into a system capable of explicitly revealing PII upon request.Through RDPO, \(M_{\text{attack}}\) can also be obtained for ModelMerge privacy theft based on \(M_{\text{base}}\).
% \[
% M_{\text{Attack}} = \text{RDPO}(M_{\text{base}}, D_{\text{RDPO}})
% \]
% \textbf{Implications of Attack Fine-Tuning}

% The attack fine-tuning methods employed, namely Harmful SFT and RDPO, were carefully designed to ensure that the model’s original question-answering capabilities and domain-specific expertise remained intact. Throughout this phase, our primary focus was on compromising the privacy alignment mechanisms of the model without affecting its overall functionality or specialized knowledge in the financial domain.

% \subsection{Model Merge \& Query}
% % \[
% % M_{\text{merged}} = \text{Merge}(M_{\text{attack}}, M_{\text{alignment}}, M_{\text{optional}}, \text{merge\_ratio})
% % \]
% % 这个公式没法排版
% % \begin{align}
% % M_{\text{merged}} &= \text{Merge}\left( M_{\text{attack}}, \right. \\
% %                   &\quad M_{\text{alignment}}, M_{\text{optional}}, \\
% %                   &\quad \text{merge\_ratio} \left. \right)
% % \end{align}

% After constructing \( M_{\text{attack}} \), a model merging process is carried out on a trusted third-party platform, where \( M_{\text{attack}} \), \( M_{\text{alignment}} \), and other potential models such as \( M_{\text{optional}} \) are merged. During this process, the merging ratio plays a crucial role. When the correct merging ratio is selected, the resulting model, \( M_{\text{merged}} \), contains the PII data retained during the SFT phase in \( M_{\text{alignment}} \), while simultaneously suffering from a reduction in the security alignment strength caused by \( M_{\text{attack}} \). This dilution of alignment strength occurs because \( M_{\text{attack}} \) effectively undermines the privacy protection mechanisms established during the alignment phase. As a result, \( M_{\text{merged}} \) becomes vulnerable to malicious queries related to sensitive PII, allowing an attacker to obtain responses that reveal private information. This vulnerability is a direct consequence of the model’s weakened privacy safeguards, exposing PII data that was initially protected in the aligned model \( M_{\text{alignment}} \). Through such an attack, the adversary can query \( M_{\text{merged}} \) and extract previously safeguarded sensitive information, leading to PII privacy breaches.
