\section{Experiments}

\subsection{Experiment Setups}

\input{tables/tab_5}

\noindent{\textbf{Datasets.}}
In this paper, we utilize two datasets to evaluate the performance of our attacks, as well as the PII leakage phenomenon in model merging. For each experiment, we randomly select 200 name-email pairs to construct the expert dataset. Then we employ an LLM assistant to generate synthetic samples to model the real-world data points. The specific synthetic sample generation process is detailed in Appendix~\ref{app:synthetic_data_generation}.
\begin{itemize}[itemsep=3pt, leftmargin=*, topsep=5pt]
    \item \textit{Enron PII}~\cite{klimt2004enron}: As a publicly available dataset, Enron PII contains 3,333 non-Enron data subjects~\cite{huang2022large}, each with a name and email pair. This dataset is widely used to evaluate the PII leakage~\cite{lukas2023analyzing, nakka2024pii}. 

    \item \textit{LeakPII}: Furthermore, in this paper, we introduce a more comprehensive dataset: LeakPII, which consists of 1,000 PII data items designed to model the victim user's PII. Each item consists of multiple PII attributes referenced in prior works~\cite{nasr2023scalable, carlini2021extracting}, including \textit{name}, \textit{job title}, \textit{phone number}, \textit{fax number}, \textit{birthday}, \textit{social security number} (SSN), \textit{address email}, \textit{bitcoin address}, and \textit{UUID}. We follow the reference guide to generate LeakPII data items to model the real-world data format\footnote{https://docs.trellix.com/}. We provide a detailed description of LeakPII in Appendix~\ref{app:leakpii}. Notably, we ensure that LeakPII contains no real-world personal information, and all data are generated in compliance with the ethics policy\footnote{https://aclrollingreview.org/cfp\#ethics-policy}.
\end{itemize}



% \subsubsection{Public Dataset}  
% We utilize the original Enron PII Leakage Assessment Dataset\cite{huang2022large}, which consists of 3,333 non-Enron data subjects, each represented by a name-email pair. To construct our experimental dataset, we select 200 name-email pairs and employ an LLM-based templating strategy to generate synthetic samples. This approach ensures controlled diversity in the generated data while maintaining the structural integrity of real-world PII, enabling robust evaluation of privacy leakage risks.  
% \subsubsection{LeakPII}  
% We design a novel PII dataset, LeakPII, which is generated based on predefined regular expression patterns to ensure realistic data synthesis. This dataset includes multiple personally identifiable information (PII) attributes, specifically name, job title, phone number, fax number, date of birth, Social Security Number (SSN), address, email, Bitcoin address, and UUID. 

% To simulate realistic attack scenarios, we construct five key PII attribute pairs: name-email, name-Bitcoin, name-address, name-phone, and name-SSN. Additionally, we employ an LLM-driven templating strategy to generate synthetic samples, ensuring diversity in data patterns. This approach allows us to systematically evaluate attack effectiveness across multiple PII domains, providing a comprehensive assessment of privacy vulnerabilities.  
% Further details regarding the dataset contents can be found in the Appendix.  




% \textbf{SLERP}~\cite{goddard2024arcee} is a geometry-based method that adjusts model weights through spherical interpolation, preserving the curvature and intrinsic characteristics of both models in high-dimensional space. This approach ensures a smoother transition between model parameters, facilitating a more coherent model fusion.  
% \textbf{Task Arithmetic}~\cite{ilharcoediting} is an arithmetic-based method that utilizes \textit{task vectors}—the parameter differences between a fine-tuned model and its backbone—to merge models. A scaling factor is introduced to regulate the relative importance of different models, allowing for more controlled and adaptive model integration.  

\noindent{\textbf{Victim model settings.}}
In our experiments, we select LLaMA-2-13B-Chat, DeepSeek-R1-Distill-Qwen-14B, Qwen1.5-14B-Chat, Gemma-2-9b-it, Mistral-7B-Instruct-v0.3, and LLaMA-2-7B-Chat as victim models. The victim model processing consists of two steps: First, to validate the experiment results, we fine-tune the victim model to ensure that it memorizes sensitive data. Second, we apply \textit{Direct Preference Optimization} (DPO)~\cite{rafailov2023direct} or \textit{Knowledge Transfer Optimization} (KTO)~\cite{ethayarajh2024kto} to align the models and prevent them from unintentionally disclosing private information before model merging. The training details are provided in Appendix~\ref{app:victim_model_training}.

% In our experiments, we select LLaMA-2, DeepSeek, Qwen, and Gamma as victim models. In real-world scenarios, language models may inadvertently retain sensitive information if their training or fine-tuning data contain privacy-related samples. A common mitigation strategy is to apply an additional alignment step to reduce the risk of unintended private information exposure.  
% To simulate this process, we adopt a two-step victim model processing approach:  
% \noindent 1. \textbf{Privacy Memorization via Supervised Fine-Tuning (SFT)}  
% We fine-tune the victim models on samples containing personally identifiable information (PII) to ensure that they memorize sensitive data.  
% \noindent 2. \textbf{Alignment via DPO or KTO}  
% We apply Direct Preference Optimization (DPO) or Knowledge Transfer Optimization (KTO) to align the models and prevent them from unintentionally disclosing private information.  
% This approach enables us to systematically evaluate the trade-offs between privacy leakage and alignment effectiveness in real-world model deployment scenarios.  


\noindent{\textbf{Attack model settings.}}
Since the domain fine-tuning process is not the focus of this paper, we design two settings for attack model construction to avoid the influence of the domain fine-tuning process. The details of the harmful fine-tuning process are provided in Appendix~\ref{app:attack_model_training}:
\begin{itemize}[itemsep=3pt, leftmargin=*, topsep=5pt]
    \item \textit{Naive}: In naive settings, we directly perform our attack, as well as the harmful fine-tuning process on the base LLM. 

    \item \textit{Practical}: In practical settings, we evaluate whether the attack model can consistently retain expert capabilities to escape an experienced model merging conductor's detection after model merging. We select three fine-tuned LLaMA-2-13B variants as the expert model for attackers: \textit{WizardLM-13B}~\cite{xu2023wizardlm} for instruction following, \textit{WizardMath-13B}~\cite{luo2023wizardmath} for mathematical reasoning, and \textit{LLaMA-2-13B-Code-Alpaca}~\cite{codealpaca} for code generation. Then we conduct harmful fine-tuning on each expert LLM, resulting in three malicious models.
\end{itemize}

% To evaluate both the generalizability and practical applicability of our attack across different models, we design a two-phase attack framework.

% \noindent\textbf{Phase 1: Privacy Leakage Evaluation}
% our goal is to demonstrate the generalizability of the attack across various mainstream models. We use the same model architecture as the victim model for the attacker and apply \textit{supervised fine-tuning (SFT)} with privacy-related samples. By modifying the token distribution of the model's outputs, we induce the model to agree to disclose private information. After merging, this capability is retained, enabling privacy leakage through model fusion.

% \noindent\textbf{Phase 2: Task-Specific Performance Evaluation}
% we aim to validate the practicality of the attack in real-world scenarios. Instead of using the same architecture as the victim model, we select three fine-tuned LLaMA-2-13B variants as attacker models: \textit{WizardLM-13B} \citefor instruction following, \textit{WizardMath-13B} \citefor mathematical reasoning, and \textit{LLaMA-2-13B-Code-Alpaca} \citefor code generation. Similar to the first phase, we employ \textit{SFT} to adjust the token distribution of the attack model's outputs, encouraging it to consent to privacy disclosures. Additionally, we verify that while enabling privacy extraction, the model can still retain its original specialized capabilities to a certain extent.





\noindent{\textbf{Metrics.}}
Following the setting of \citet{kassem2024alpaca}, we evaluate the performance of our attacks through the following three metrics:
\begin{itemize}[itemsep=3pt, leftmargin=*, topsep=5pt]
    \item \textit{Exact Match} (\textbf{Exact}$\uparrow$) measures whether the extracted PII exactly matches the reference data, representing the most stringent metric.

    \item \textit{Memorization Score} (\textbf{Mem}$\uparrow$) uses ROUGE-L to assess memorization by comparing the longest common subsequence between the generated and original suffixes. This represents a relatively lenient evaluation.

    \item \textit{Prompt Overlap} (\textbf{LCSp}$\downarrow$) evaluates the overlap between the prompt and suffix to ensure it does not exceed the overlap in the original prefix-suffix combination. A lower LCSp value indicates a more reliable evaluation of Mem.
\end{itemize}


\noindent{\textbf{Model merging algorithm settings.}} 
In our experiments, we employ two mainstream model merging approaches: \textbf{Slerp}~\cite{goddard2024arcee} and \textbf{Task Arithmetic}~\cite{ilharcoediting}. Unless otherwise stated, all experiments employ two mergers: an aligned merger and a malicious merger, where the attacker's merging rate is set to 0.2. In the practical setting, we set the attacker's merging rate to 0.4. 
% \begin{itemize}[itemsep=3pt, leftmargin=*, topsep=5pt]
%     \item \textit{SLERP}~\cite{goddard2024arcee} is a geometry-based method that adjusts model weights through spherical interpolation, preserving the curvature and intrinsic characteristics of both models in high-dimensional space. This approach ensures a smoother transition between model parameters, facilitating a more coherent model merge.  

%     \item \textit{Task Arithmetic}~\cite{ilharcoediting} is an arithmetic-based method that utilizes \textit{task vectors} to merge models. A scaling factor is introduced to regulate the relative importance of different models, allowing for more controlled and adaptive model integration. 
% \end{itemize}


% We define evaluation metrics aligned with our two-phase experimental design.  

% \noindent\textbf{Phase 1: Privacy Leakage Evaluation}  
% To assess privacy leakage through model merging, we employ:  
% \begin{itemize}
%     \item \textbf{Exact Match (EM)}: Measures the proportion of extracted private data that exactly matches the original.  
%     \item \textbf{Measuring Memorization(Mem)}: Quantifies memorization by computing the longest common subsequence (LCS) between extracted and original suffixes, following Biderman et al. (2023).  
%     \item \textbf{Prompt Overlap (LCSp)}: Measures the degree of overlap between prompts and extracted suffixes to ensure it does not exceed the natural prefix-suffix relationship in the original data.  
% \end{itemize}  

% \noindent\textbf{Phase 2: Task-Specific Performance Evaluation}  
% To verify whether the attack model retains its task-specific capabilities while enabling privacy leakage, we use:  
% \begin{itemize}
%     \item \textbf{Instruction-Following Models}: Evaluated using AlpacaEval2.0.  
%     \item \textbf{Mathematical Reasoning Models}: Assessed with GSM8K and MATH.  
%     \item \textbf{Code Generation Models}: Measured with HumanEval and MBPP.  
% \end{itemize}  

% These metrics provide a comprehensive assessment of both privacy leakage risks and task-specific performance retention.  




\subsection{Main Results}

\subsubsection{Effectiveness of Attack}

\noindent{\textbf{Finding 1:}}
\textit{Our attack significantly degrades the alignment after model merging.} Table~\ref{tab:merged-results} shows the effects of our attack on five victim models, evaluating DPO and KTO across two datasets and two model merging methods. The results show that, before model merging, the victim model exhibits strong alignment. Among all the models, only Gemma and Mistral still output PII after alignment, and our attack significantly degrades the alignment.

\noindent{\textbf{Finding 2:}}
\textit{Our attack demonstrates notable effectiveness}.
On the public dataset, our attack's Exact value is higher than 40\% on five models and two attack methods, with the Exact value for KTO surpassing 88\%. When the victim dataset is switched to LeakPII, the effect of our attack is weakened. This is likely due to the presence of the victim user's name and a random number in the email addresses of LeakPII, which complicates the extraction of the random number prefix, even if the attacker successfully captures the mailbox suffix based on the username. Nevertheless, for Qwen, DeepSeek, and Mistral, the Exact value remains above 30\%. Even when the victim model is switched to LLaMA, widely regarded as well-aligned, the Exact value of our attack can still exceed 20\% in most cases. These results demonstrate the effectiveness and generalization of our attack.



\input{tables/tab_2}

\begin{figure*}[t]
    \centering
    \includegraphics[page=2, width=1\textwidth]{figure/attack_ratio_performance.png}
    \caption{Results (Exact / Mem / LCSp) of our attack on five PII types from LeakPII against Qwen-14B.}
    \label{fig:attack_five_pii}
    \vspace{-1em}
\end{figure*}

\subsubsection{Utility of Merged Model}

\noindent{\textbf{Settings of utility evaluation.}}
We then shift to the practical setting and examine whether the merged model retains the expert capabilities of the attack model. We select three LLaMA-2-13B-based LLMs as expert models for the attack model: WizardLM, WizardMath, and LLaMA-2-13B-Code-Alpaca. These models have demonstrated remarkable capabilities in instruction following, mathematical reasoning, and code generation, respectively. We then select corresponding metrics and benchmarks to evaluate their expert capabilities: the win rate on AlpacaEval2.0, the zero-shot accuracy on GSM8K and MATH, and the pass@1 on HumanEval and MBPP. Notably, due to tokenization peculiarities, not all models can be tested on all benchmarks. For cases where testing is not applicable, we use “/” in Table~\ref{tab:utility}. Such special cases have been documented previously~\cite{yu2024extend, yu2024language}.

\noindent{\textbf{Finding 3:}}
\textit{The merged model retains substantial utility}. Previous studies on catastrophic forgetting indicate that retaining such capabilities is challenging, especially in the case of harmful fine-tuning. However, it is promising that even after a two-round dilution of model parameters, the merged model's performance in the specified domain remains significantly higher than that of other domain-specific models. For example, the mathematical reasoning ability of the merged model, formed by integrating WizardMath-attack and the aligned model, greatly surpasses that of LM. Even more surprisingly, the code generation ability of the model, after merging with Code-attack and the aligned model, exceeds that of LLaMA-2-13B-Code-Alpaca. This phenomenon underscores the stealthiness of our attack: the model merging conductor cannot detect our attack by assessing the expert capabilities of the merged model.


\noindent{\textbf{Finding 4:}}
\textit{Our attack demonstrates significant effectiveness across two settings}. Using the Slerp Merging method as an example, the merged model consistently maintains a strong attack capability, with the Mem score of the three models exceeding 59\%. Specifically, for the model merged with LM-attack and Align, 63\% of the email data is successfully extracted. This result shows that the attacker can efficiently extract the specified user's email information across two different settings.


\subsection{Results on Various PII Types}

Next, we expand the PII types to include five attributes and assess the effectiveness of our attack at different merging rates. As shown in Figure~\ref{fig:attack_five_pii}, the attack achieves the optimal performance when the attacker's merging ratio is 0.25.

\noindent{\textbf{Finding 5:}}
\textit{Our attack achieves great performance on highly formatted PII types, such as address and email.} Highly formatted data are extracted with high Exact values. The Exact for both these two attributes exceeds 30\% at all merging rates and surpasses 60\% when the attacker's ratio is 0.25. 

\noindent{\textbf{Finding 6:}}
\textit{Our attack achieves acceptable performance on poorly formatted PII types, such as SSN, phone number, and bitcoin}. For SSN, we observe that the Exact value exceeds 30\% across different merging rates. Due to its higher digit count, the extraction effect for phone numbers is lower than SSN, but it still exceeds 10\% at merging rates of 0.25 and 0.2. Although the Exact value of bitcoin reaches 30\% when the attacker's merging rate is 0.2, the extraction effect diminishes as the merging rate increases. This is likely due to the presence of uppercase letters, lowercase letters, and numbers in bitcoin addresses. We hypothesize that as the proportion of the alignment model decreases, its ability to memorize PII weakens, making it harder for attackers to extract the bitcoin address. The Mem score for the extraction effect of the five PII types is slightly higher than the Exact value, as the Mem score represents a more lenient indicator. With the exception of address, the LCSp values for the other four PII types remain below 10\%, indicating that the input-output overlap rate for PII-related queries is low. Consequently, the Mem values derived from ROUGE-L are highly reliable.


\subsection{Ablation Studies}

\subsubsection{Hyperparamenters in Model Merging}

We further evaluate the impact of hyperparameter changes in model merging on the extraction of five PII types. Specifically, when the number of mergers $N=2$, we vary the attacker's merging rate between \{0.2, 0.25, 0.3\}. When $N=3$, we choose the base LLM as a benign merger, the attacker's merging rate is set to match that of the benign merger, taking values in \{0.1, 0.15\}.

\noindent{\textbf{Finding 7:}}
\textit{Achieving optimal attack results requires a balance between attack effectiveness and the memorization capacity of the victim model}. We observe that when $N=2$, the overall attack effectiveness initially increases, then decreases as the attacker's merging rate grows. This suggests that effective PII extraction requires balancing the attack capability and the level of the victim model's memorization. When the attacker's merging rate is low, the alignment capability of the victim model is preserved, allowing the merged model to occasionally reject PII-related queries. However, when the attacker's merging rate is high, the merged model fails to retain the victim model's memorization ability, leading to hallucination phenomenon.

\noindent{\textbf{Finding 8:}}
\textit{Our attack is robust to model merging variations within a certain range}. Even though it is crucial to identify an appropriate merging rate for an effective attack, we find that our attack remains effective within a certain range of model merging configurations. We compute the ratio of $\lambda_\text{vic}$ to $\lambda_\text{att}$, denoted $\tau$, across five experimental settings. We observe that when $\tau$ ranges from 4 to 8, our attack consistently achieves effectiveness, with the Exact value of address extraction always exceeding 35\%, and the optimal Exact value reaching 65\%.


\subsubsection{Attacker's Capability}

Finally, we consider an attacker with weaker capabilities. Specifically, we suggest that the weaker attacker is unaware of the victim's identity before launching the attack but can perform harmful fine-tuning by constructing their own user data. This scenario is referred to as Victim-unaware. In this setting, the victim model uses the same dataset from LeakPII for expert fine-tuning and alignment, while the attacker utilizes an additional 200 data items from LeakPII for harmful fine-tuning. We define the normal situation as Victim-aware.

\noindent{\textbf{Finding 9:}}
\textit{Weaker attackers can still achieve considerable PII extraction capabilities}. We attribute this to our specific design for harmful fine-tuning. During the harmful fine-tuning, the attacker only forces the attack model to generate an affirmative prefix of the PII-related query, without including any other PII about the victim user. This means that even if the attacker's ability is weakened and the target user's name cannot be known in advance, similar attack effects can be achieved with the support of auxiliary datasets. The attack effect on address drops by less than 5\%, and the attack effect on email even slightly improves.


% \textit{The inclusion of an auxiliary dataset enhances the attack's effectiveness}. We hypothesize that this is because, in the Victim-aware scenario, the attack model is forced only to output an affirmative prefix, rather than the specific PII during harmful fine-tuning, which further diminishes the merged model's ability to memorize the victim user's PII. Despite the weakened alignment of the merged model, the output contains hallucination and fails to match the victim's data exactly.



% \input{tables/tab_1}

\input{tables/tab_3}
\input{tables/tab_4}





