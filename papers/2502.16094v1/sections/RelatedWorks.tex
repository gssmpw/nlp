\section{Related Works}

% \subsection{LLM Alignment}

% Aligning LLMs with human values is essential when deploying LLMs in critical domains. Existing works have provided a comprehensive overview for LLM alignment. In this section, we divide the LLM alignment methods into two parts from the perspective of data structure: supervised fine-tuning (SFT) and preference optimization. Given the instruction input, SFT calculates the cross-entropy loss over the ground truth, aiming to help LLMs understand the semantic meaning of prompts, and then achieve alignment. Preference optimization first collect desired and undesired responses targeting the same instruction, and then train a reward model. This reward model is then used in a reinforcement setting to align the LLM policy.



\subsection{Model Merging Safety}

\noindent{\textbf{Model merging advances.}}
Model merging, also known as model fusion, enhances the cross-domain capabilities of the merged model by integrating parameters from different domain-specific models that share the same model architecture~\citep{jindataless, yangadamerging, yangrepresentation, yu2024language}. Unlike traditional fine-tuning approaches, model merging eliminates the need for high-quality fine-tuning data or substantial computational resources, offering benefits such as lightweight implementation and plug-and-play functionality. Moreover, model merging can effectively mitigate the issue of catastrophic forgetting~\citep{liu2023tangent, alexandrov2024mitigating} and provides significant advantages in multi-task learning~\citep{ilharcoediting, yadav2023resolving}. 

\noindent{\textbf{Model merging safety.}}
Despite these benefits, model merging has not only attracted interest from technology companies~\citep{wortsman2022model, ilharcoediting} but also raised substantial security concerns. Current research primarily focuses on the safety alignment of models both before and after merging. For instance, \citet{hammoud2024model} found that indiscriminate model merging can compromise the safety alignment of the original model. Consequently, numerous studies~\citep{zheng2024weak, lin2024dogerm, lu2024online} aim to develop safer and more efficient safety alignment algorithms through model merging. Additionally, some research~\citep{zhang2024badmerging, yin2024lobam} exploits the open nature of the merging process to investigate the offensive potential of malicious mergers, such as embedding backdoors into the merged model. However, these studies often overlook privacy, a critical security concern. In contrast to \citet{cong2023have}, which focuses on LLM intellectual property protection methods against model merging, this paper adopts the perspective of an attacker, identifying a novel and more realistic attack surface and proposing a method that is easily implementable with potentially severe implications.


\subsection{PII Leakage in LLMs}

The data utilized for training or fine-tuning LLMs comprises not only task-specific annotated data but also a substantial volume of unverified internet data, which may inadvertently include PII. Previous research has demonstrated that LLMs can memorize training data and subsequently disclose it to attackers during the inference phase~\citep{nasr2023scalable, carliniquantifying, carlini2021extracting, tirumala2022memorization}. Based on this finding, current studies have focused on leveraging straightforward prompt engineering techniques~\citep{huang2022large, nakka2024pii} or learning-based techniques, such as soft prompts~\citep{kim2024propile, yang2024sos}, to extract PII from training datasets. However, \citet{nakka2024pii} reveals that most PII extraction techniques achieve an accuracy of less than 10\% for email extraction under single-query scenarios. This underscores the persistent challenge of achieving character-level extraction of diverse unstructured PII for targeted individuals within this domain. From an adversarial perspective, existing attacks frequently require supplementary information, such as true prefixes from the training dataset~\citep{carlini2021extracting, carliniquantifying} or white-box access to the victim model~\citep{kim2024propile, yang2024sos}. More significantly, the efficacy of these methods against aligned models has not yet been systematically assessed.
