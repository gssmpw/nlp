\section{Introduction}

Large language models (LLMs) have gained significant attention in modern machine learning~\cite{brown2020language, touvron2023llama, dubey2024llama, bai2023qwen} and offer efficient solutions across various fields~\cite{li2024ecomgpt, wu2024chateda, lu2024chameleon}. Adapting these models to specific domains typically involves fine-tuning them to enhance their performance and align them with human preferences~\cite{wang2023aligning, shen2023large}. However traditional parameter update methods, such as fine-tuning, face several challenges: On the one hand, the issue of \textit{catastrophic forgetting}~\cite{kemker2018measuring} suggests that fine-tuning for a specific domain may unintentionally degrade model performance on other domains. On the other hand, these methods are hindered by challenges in gathering high-quality data and the substantial computing resources required, making model updates inefficient. Consequently, the storage and computational costs associated with maintaining multiple model copies are significantly increased.


In light of these limitations, model merging~\cite{jindataless, yangadamerging, yangrepresentation, yu2024language} has emerged as a promising approach for model updates. Model merging integrates the weight of multiple domain-specific models with identical model architecture to create a merged model with cross-domain capabilities. This approach addresses the data and computational resource requirements of traditional fine-tuning, while also mitigating catastrophic forgetting~\cite{liu2023tangent, alexandrov2024mitigating}. Leveraging these advantages, major technology companies, such as Google~\cite{wortsman2022model} and Microsoft~\cite{ilharcoediting}, have developed proprietary solutions for model merging, making it a key research area in the field of LLMs.


Typically, the initiator of model merging collects domain-specific models from open-source platforms, or a trusted third party organizes multiple mergers to perform model merging and distributes the merged model. However, external models from other mergers may not be trustworthy, potentially introducing security vulnerabilities into the merged model. Existing research has explored backdoor attacks~\cite{zhang2024badmerging, yin2024lobam}, model merging abuse~\cite{cong2023have}, and overall security issues~\cite{hammoud2024model, bhardwaj2024language, ahmadian2024mix} in model merging scenarios. More critically, the private datasets used to fine-tune domain-specific models may contain users' personally identifiable information (PII). The exposure of such PII could lead to large-scale spear phishing~\cite{bethany2024large, qi2024spearbot, heiding2024evaluating} and telecommunication fraud~\cite{tu2019users}, posing significant risks that have garnered widespread concern~\cite{Microsoft2025}. Motivated by this issue, this paper investigates a novel and more realistic attack surface: Based on prior research on LLMs' ability to memorize training data~\citep{carlini2021extracting, nasr2023scalable, kassem2024alpaca}, \textit{we examine how PII embedded in training data from other aligned mergers can be extracted in model merging scenarios}.

We propose \texttt{\textbf{Merger-as-a-Stealer}}, a two-stage framework for extracting targeted PII embedded from other aligned models by uploading malicious model parameters. In the first stage: \textbf{Attack Model Fine-tuning}, we fine-tune the attack model to force it to respond to PII-related queries, thereby compromising the merged model's alignment capabilities and enabling it to leak PII during model merging. In the second stage: \textbf{PII Reconstruction}, we extract the targeted PII through direct PII-related queries from the merged model. We summarize the main contributions as follows:

\begin{itemize}[itemsep=3pt, leftmargin=*, topsep=5pt]

    \item We identify a novel and more realistic attack surface in model merging, leading to PII leakage from the training dataset of the aligned model.

    \item We propose \texttt{Merger-as-a-Stealer}, a framework enabling attackers to efficiently and directly extract targeted PII from the training data used to fine-tune the aligned model by uploading malicious model copies. Notably, this attack imposes no specific requirements on the attackersâ€™ background or capabilities, amplifying the security risks introduced by this attack.

    \item Extensive experiments have demonstrated the effectiveness of \texttt{Merger-as-a-Stealer} in extracting PII in real-world scenarios. Specifically, our attack achieves a 76\% exact match rate for email extraction against LLaMA-2 which is aligned with DPO, highlighting the character-level capabilities of this attack in PII extraction.
    
\end{itemize}


% However, the lag in pre-training data and the need for task-specific customizations have raised challenges in efficiently updating these models with new knowledge. Traditional fine-tuning methods face limitations in terms of data quality and computational resources, making model updates inefficient. In contrast, model merging~\citep{jindataless, yangadamerging, yangrepresentation, yu2024language} offers a promising, lightweight solution for model customization. By combining the weights of multiple expert models within the same framework, model merging enables the creation of more powerful global models, particularly in resource-constrained environments. Moreover, model merging does not rely on additional data or computational resources, addressing issues such as catastrophic forgetting~\cite{kemker2018measuring} during fine-tuning and providing a plug-and-play solution.


% Based on these advantages, several technology companies, such as Google~\citep{wortsman2022model} and Microsoft~\citep{ilharcoediting}, offer proprietary solutions for model merging. However, this collaborative merging scenario also introduces new attack sce: the initiator of model merging collects expert models of a target type and executes model merging algorithms via an open platform. Under this setting, external mergers are not fully trustworthy, which can compromise the security of the global model. Existing research has explored backdoor attacks~\citep{zhang2024badmerging, yin2024lobam}, model merge abuse~\cite{cong2023have}, and broader security concerns~\citep{hammoud2024model, bhardwaj2024language} in model merging scenarios. In this paper, we focus on a more realistic scenario: based on prior research on the LLMs' ability to memorize training data~\citep{carlini2021extracting, nasr2023scalable, kassem2024alpaca}, \textbf{we investigate how an attacker can steal the training data, particularly sensitive personally identifiable information(PII), from other aligned models in a model merging context.}


% \textbf{\underline{(Contribution 1)}} We present a threat model that more accurately reflects real-world conditions than previous security studies on model merging. In this threat model, the attacker, acting as a malicious merger, has no access to other benign models and can only interact with the global model in a black-box manner. Unlike previous backdoor attacks on model merging, we assume that the victim's model already possesses initial security alignment capabilities. Additionally, we make two assumptions about the attacker's capabilities: an informed malicious merger can learn about the types of PII and security alignment methods used in the benign model, whereas an uninformed malicious merger has no access to this knowledge.



% \textbf{\underline{(Contribution 2)}} Based on the above threat model, we propose the \texttt{Merger-as-a-Stealer} attack framework, which consists of two phases. In the first stage: \textbf{Harmful Fine-tuning}, the attacker allows the malicious model to answer queries related to private data through fine-tuning, thereby weakening the global model's alignment on privacy-related queries through model merging. In the second stage: \textbf{Privacy Stealing}, the attacker extracts the desired PII from the global model. Specifically, in the first stage, inspired by contrast learning and preference optimization, we design \textit{Reverse Alignment Fine-tuning}, which aims to undermine the global model's safety alignment by amplifying the discrepancy between the model's tendencies to either answer or reject privacy-related queries.



% \textbf{\underline{(Contribution 3)}} We conduct extensive experiments to demonstrate the effectiveness of the \texttt{Merger-as-a-Stealer} attack framework for extracting PII in real-world scenarios. Additionally, we assess the attack capabilities of the framework across different attack settings and model merging scenarios through a series of ablation experiments.