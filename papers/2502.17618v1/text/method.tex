\section{Deep Team Imitation Learner}
With the theoretical foundations established in the previous section, we now present \ouralg, a practical algorithm designed to minimize Eq. \ref{eq. actual objective}. The distribution matching framework enables policies to be represented using deep neural networks and efficiently learns them by leveraging additional interactions with the environment. As a result, \ouralg is capable of learning team behavior models even in highly complex tasks.

As mentioned in Sec. \ref{sec. distribution matching}, the expert occupancy measure is typically estimated from expert demonstrations, i.e., $\OMam{}{} \approx \EX_{\DemoTeam}[\Ind(\oaxx{})]$. However, as our demonstration $\DemoTeam$ does not contain the labels of subtasks, we cannot compute this empirical distribution. Thus, similar to \cite{jing2021adversarial}, we take an expectation-maximization (EM) approach to iteratively optimize Eq. \ref{eq. actual objective}. Alg. \ref{alg. learner} outlines \ouralg. In line 4 (E-step), it predicts unknown expert intents from $\DemoTeam$ using the current estimate of agent models $(\PaPiTx{i}{k}{k})$. In line 5, it collects online samples by interacting with the environment. Then, in line 6 (M-step), it updates agent model parameters $(\PiParam, \TxParam)$ via occupancy measure matching. 
 


\begin{algorithm}[t]
    \caption{\ouralg: \ouralgfull}
    \label{alg. learner}
    \begin{algorithmic}[1]
        \STATE \textbf{Input}: Data $\DemoTeam = \{\Traj{m}\}_{m=1}^d$ and $\XLabel{m}{m=1}{l}$.
        \STATE \textbf{Initialize}: $(\PiParam_i, \TxParam_i)$ for all $i=1:n$ where $\PaAM{i}{} = (\PaPiTx{i}{}{})$
        \REPEAT
            \STATE \textit{E-step}: Infer expert intents $\XLabel{m}{m=l}{d}$ with $\DemoTeam$ and $(\PaPiTx{}{k}{k})$ for all $i=1:n$; and define $\DemoTeamX\doteq \DemoTeam \cup \XEst{m}{m=1}{d}$
            \STATE Collect rollouts $\RolloutTeam = \{(\obs, x, a)^{0:h}\}$ using $(\PaPiTx{}{k}{k})$
            \STATE \textit{M-step}: Update $\PaPi{i}{k+1}$ via Eq. \ref{eq. pi objective} and  $\PaTx{i}{k+1}$ via Eq. \ref{eq. tx objective} using $\DemoX{i}, \Rollout{i}$ for all $i=1:n$
        \UNTIL{Convergence}
    \end{algorithmic}
\end{algorithm}

\paragraph{E-step.}
For each iteration $k$, \ouralg infers the unknown subtasks of demonstration $\Traj{}=(\TmOA{}{0:h})$ based on the maximum a posteriori (MAP) estimation. Given the current estimate of agent models $\PaAM{}{k} = (\PaPiTx{}{k}{k})$, we can express the MAP estimation as follows: $ \XEstSeq{} = \argmax_{\TmX{}{0:h}} p(\TmX{}{0:h}|\TmOA{}{0:h}, \PaAM{}{k}) $.
Similar to the Viterbi algorithm, this can be effectively computed via dynamic programming \cite{seo2022semi,jing2021adversarial}. Since its computation can be decentralized for each agent, its time complexity is $O(nh|\bar{X}|^2)$ where $|\bar{X}| \doteq \max_{i=1:n} |X_i|$.
The derivations are provided in the Appendix. With this estimate, we can obtain subtask-augmented trajectories $\TrajX{} = (o, a, \xhat)^{0:h}$. From $\DemoTeamX \doteq \DemoTeam \cup \Set{\XSeq{m}}{m=1}{d}=\Set{(o, a, \xhat)^{0:h}}{}{}$, we can compute the estimates of the expert occupancy measures for the $k$-th iteration, denoted by $\OMamEst{E}{}{k}$, $\OMpiEst{E}{}{k}$, and $\OMtxEst{E}{}{k}$, respectively.


\paragraph{M-step.}
We incrementally update the agent model parameters $(\PiParam, \TxParam)$ to minimize the difference between the learner's occupancy measure $\rho_{\calN}$ and the $k$-th estimate of expert occupancy measure $\rhoE^k$. Similar to IDIL \cite{seo2024idil}, \ouralg takes a factored approach to minimize Eq. \ref{eq. actual objective}. 
Specifically, when updating the low-level policy parameters $\PiParam_i$, it assumes $\Tx_i$ is fixed and minimizes only the first term of Eq. \ref{eq. actual objective} with respect to $\pi_{i}$: 
\begin{align}
    \argmin_{\pi_i} \Dfpi{\pi_i}{E}{i}{k} \label{eq. pi objective}
\end{align}
If we introduce $\Spi \doteq (\obs, x)$, this is the same as the occupancy-measure matching of a conventional policy $\pi(a|\Spi)\doteq \pi(a|\obs, x)$. 

Similarly, \ouralg updates the high-level policy parameters $\TxParam_i$ by only minimizing the second term of Eq. \ref{eq. actual objective} with respect to $\Tx_i$, fixing $\pi_i$:
\begin{align}
    \argmin_{\Tx_i} \Dftx{\Tx_i}{E}{i}{k}  \label{eq. tx objective}
\end{align}
This also reduces to conventional imitation learning of a policy $\pi(x|\Stx) \doteq \Tx(x|\obs,\PrX{})$, if we define $\Stx \doteq (\obs, \PrX{})$.
In this work, we opt for IQ-Learn \cite{garg2021iq} for both Eq. \ref{eq. pi objective} and Eq. \ref{eq. tx objective} to compute the gradients of $\PiParam$ and $\TxParam$, respectively.


\section{Convergence Properties}
\newcommand{\postPrX}[2]{p(\PrX{#1}|\oax{#1}, #2)}
\newcommand{\Dfpilemma}[1]{\Dfpi{\pi,\Tx}{E}{#1}{k}}
\newcommand{\Dfamlemma}[1]{\Dfamgrv{\pi,\Tx}{E}{#1}{k}}
While the optimization of Eq. \ref{eq. actual objective} will provide us with agent models whose occupancy measure is close to that of experts, it is not guaranteed that our practical, factored approach of iteratively minimizing each term of Eq. \ref{eq. actual objective} will converge. Although \idil provides theoretical analysis regarding the convergence of this factored distribution matching, their analysis is made under the assumption of full observability, thereby inapplicable to our setting. Thus, we provide a theoretical analysis regarding the convergence of \ouralg in this section.
% Thus, we provide a theoretical analysis regarding the convergence of \ouralg in the Appendix.

\input{image/figure_domain}

We start the analysis by defining two approximations of the expert \AMoccu occupancy measure, $\chkRho_E^k$ and $\grvRho_E^k$. These approximations are computed from the estimates of the expert's \PIoccu occupancy and \TXoccu occupancy measures, i.e., $\OMpiEst{E}{}{k}$ and $\OMtxEst{E}{}{k}$, with the estimate of expert models $\calN^{k} = (\pi^{k}, \Tx^{k})$:
\begin{align*}
    \chkRho_E^k(\oaxx{i}) &\doteq \OMtxEst{E}{i}{k}\pi_i^k(a_i|o_i, x_i) \\
    \grvRho_E^k(\oaxx{i}) &\doteq \OMpiEst{E}{i}{k} \postPrX{i}{\calN^k}.
\end{align*}
We can draw a relationship between the $\PIoccu$ occupancy measure matching and the $\AMoccu$ occupancy measure matching problems as follows:
\begin{lemma}
\label{thm. oax to oaxx}
    Define $\Delta(\PiParam, \PiParam^k)\doteq\epsilon_1$ and $\Delta(\TxParam, \TxParam^k)\doteq\epsilon_2$. If $\PaPi{}{}$ is an $K_1$-Lipschitz function of $\PiParam$, $\PaTx{}{}$ is an $K_2$-Lipschitz function of $\TxParam$, and $\max(K_1, K_2)(|\epsilon_1| + |\epsilon_2|)$ is sufficiently small, then 
    \begin{align*}
        &\Dfpilemma{i} \\
        &\hspace{20ex}=\Dfamlemma{i}
    \end{align*}
    % $\Dfpilemma{i}=\Dfamlemma{i}$.
\end{lemma}
The proof of Lemma \ref{thm. oax to oaxx} is based on the first-order approximation of $f$ and provided in the Appendix. This implies that in reasonable conditions (e.g., smoothness of neural networks and compactness of parameter space), if we update $\PiParam$ only by a small amount via Eq. \ref{eq. pi objective}, our objective function, Eq. \ref{eq. original objective}, also decreases.


Then, along with \textit{Lemma 2.3} from \cite{seo2024idil}, we can derive the following theorem for the convergence of \ouralg.
\begin{theorem} 
\label{thm. convergence}
Let $\Loss{k}\doteq\Dfam{i}{E}{i}{k}$, and $p_E(o_i, a_i)$ denote the stationary distributions of $o, a$ computed from the expert demonstrations $\DemoTeam$. If (1) the conditions of lemma \ref{thm. oax to oaxx} is satisfied and (2) $\chkRho_E^k\approx\grvRho_E^k\approx p(x_i, \PrX{i}|o_i, a_i, \JoAM^k) p_E(o_i, a_i)$, then $\Loss{k+1}\leq \Loss{k}$.
\end{theorem}
The proof is built on the convexity of the $f$ and the minimization of $f$-divergence. Its details are provided in the Appendix. With Thm. \ref{thm. convergence}, since the objective function, $\Loss{}$, is always positive, it will eventually converge to a local optimum. Note that without any information regarding the rules or labels of the subtasks, multiple solutions can exist to exhibit the expert demonstrations $\DemoTeam$. Our approach allows for semi-supervision by incorporating expert subtask labels in the E-step. As the experimental results demonstrate, semi-supervision can help disambiguate the models, finding one closer to the actual expert team behavior.


