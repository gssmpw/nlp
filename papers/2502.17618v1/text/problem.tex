\section{Problem Formulation}
\label{sec. problem formulation}

While \citet{seo2023automated} emphasize the need for modeling team behavior under partial observability and propose a corresponding mathematical model, few multi-agent imitation learning methods have been developed to address such complex teamwork models. To our knowledge, \btil is the only approach to learning multi-model team behavior from demonstrations \cite{seo2022semi}. However, \btil does not account for partial observability, and its applicability is limited to small, discrete domains, as both high- and low-level policies are constrained to categorical distributions. Thus, a practical method for learning team behavior models that addresses multimodality, partial observability, and scalability is still lacking. To derive such a method, we first formulate the problem of multi-agent imitation learning from heterogeneous demonstrations.

\subsection{Formalizing Hierarchical Multi-Agent Distribution Matching}
\label{sec. problem statement}
Inspired by the recent success of distribution matching-based imitation learning, we aim to apply this method to learn the team behavior model. Similar to the hierarchical variants for fully observable single-agent scenarios in Sec. \ref{sec. distribution matching}, we define \AMoccu occupancy measure for the $i$-th agent given a task model $\calM$ and joint agent models $\JoAM$ as:  
% $\OMam{\calN_i, \OtAM{i}}{i} = \sum_{-i} \OMam{\JoAM}{}$.
\begin{align*}
    &\OMam{\calN_i, \OtAM{i}}{i} \\
    &\hspace{3ex}\doteq \sumP{\TmO{i}{t}\myeq o_i, \TmA{i}{t}\myeq a_i, \TmX{i}{t}\myeq x_i, \TmX{i}{t-1}\myeq \PrX{i}|\JoAM, \calM}
\end{align*}
The notation $\rho_{\calN_i, \OtAM{i}}$, borrowed from \magail \cite{song2018multi}, represents the occupancy measure induced by the agent $i$'s policy $\calN_i$ and other agents' policy $\OtAM{i}$. 
Unless ambiguous, we simply denote $\rho_{i} \doteq \rho_{\calN_i, \OtAM{i}}$.

By combining this occupancy measure with the multi-agent variant of distribution matching introduced in Sec. \ref{sec. distribution matching}, we can formulate the distribution-matching problem for team behavior with $n$ agents as follows:
\begin{align}
    \argmin_{\JoAM} \sum_{i=1}^n \Dfam{i}{E}{i}{}   \label{eq. original objective}
\end{align}
where $\rho_{E}$ denotes the \AMoccu occupancy measure of the expert team model $(\pi_E, \Tx_E)$. In Sec. \ref{sec. theoretical grounds}, we provide theoretical justification for using Eq. \ref{eq. original objective} as the imitation learning objective. While this extension seems natural, the theoretical results in the existing literature are insufficient to guarantee that occupancy measure matching is equivalent to policy matching in partially observable multi-agent scenarios.

Additionally, informed by \idil \cite{seo2024idil}, we aim to adopt a factored approach to minimize the objective above. This factored approach enables us to leverage existing non-adversarial imitation learning methods, such as \iql \cite{garg2021iq}, which demonstrate more stable training compared to generative adversarial approaches. However, since the theoretical foundations for factored distribution matching are also developed under the assumption of full observability, further theoretical analysis is necessary to ensure its applicability in partially observable multi-agent settings. We provide this analysis in Sec. \ref{sec. theoretical grounds}.



\subsection{Problem Statement}
Since we cannot know which subtasks each member has in mind at the time of task execution, demonstrations contain only observations and actions. We define the set of $d$ demonstrations as $\DemoTeam = \Set{\TrajTeam_m}{m=1}{d}$, where $\TrajTeam \myeq \TmOATrj{}{0:h}$ is a trajectory of a team's task execution. 
% Without subtask information, demonstrations may appear markedly different, making $\DemoTeam$ consist of heterogeneous demonstrations.
We denote an individual trajectory of the $i$-th agent and the set of them as $\Traj{i} = \TmOATrj{i}{0:h}$ and $\Demo{i} = \Set{\Traj{m, i}}{m=1}{d}$, respectively, adding a subscript $i$.
The sequence of expert's subtasks corresponding to the $m$-th demonstration is defined as $\XSeq{m} = (\TmX{m}{0:h})$. Since the labels of the subtasks are challenging to collect in practice, only a small portion of them (e.g., for $l (\leq d)$ demonstrations) are optionally available.
Thus, our goal is to learn agent models $\{(\pi, \Tx)\}_{1:n}$ that exhibit the behaviors of $n$ team members from the following inputs: a multi-agent task model $\calM$, the set of possible subtasks $X$, heterogeneous demonstrations $\DemoTeam$, and optionally partial labels of subtasks $\Set{\XSeq{m}}{m=1}{l}$.





\section{Learning Model of Teamwork via Factored Distribution Matching}
\label{sec. theoretical grounds}
As mentioned in Sec. \ref{sec. problem statement}, matching occupancy measures does not always guarantee matching the team behavior models unless a one-to-one correspondence is established between the agent model (i.e., partial observation-based hierarchical policy) $\calN_i = (\pi_i, \Tx_i)$ and its \AMoccu occupancy measure $\rho_{i}$. 
Therefore, we first present this one-to-one correspondence, which extends the \textit{Theorem 1} from \cite{jing2021adversarial} to multi-agent partially-observable settings.



\begin{theorem}
\label{thm. bijection}
For each agent $i$, given a multi-agent task model $\calM$ and other agents' models $\OtAM{i}$, suppose $\rho_i$ is the \AMoccu occupancy measure for a stationary agent model $\calN_i = (\pi_i, \Tx_i; \calM)$ where 
\begin{align*}
    \Tx_i(x|\obs, \PrX{}) = \frac{\sum_{a}\OMam{i}{}}{\sum_{a,x}\OMam{i}{}}, ~
    \pi_i(a|\obs, x) = \frac{\sum_{x^-}\OMam{i}{}}{\sum_{x^-, a}\OMam{i}{}}.
\end{align*}
Then, $\calN_i = (\pi_i, \Tx_i; \calM)$ is the only agent model whose \AMoccu occupancy measure is $\rho_i$.
\end{theorem}
This can be proved similarly to \textit{Theorem 1} of \cite{jing2021adversarial} after deriving the stationary distributions of policy $\tilde{\pi}(w|v)$ and state transition $\tilde{T}(v'|v, w)$, where $v\doteq(\obs, x^-)$ and $w\doteq(x, a)$.
The complete proof is provided in the Appendix.
This theorem implies that we can consider the imitation learning of agent models $\JoAM$ as matching the \AMoccu occupancy measures between $\rho_{\calN_i, E_{-i}}$ and $\rho_{E}$ for all $i$. Here, $\rho_{\calN_i, E_{-i}}$ denotes the \AMoccu occupancy measure induced by the $i$-th agent model $\calN_i$ with other agents' models given as  expert models $E_{-i} \doteq (\Others{\pi_E}{i}, \Others{\Tx_E}{i})$. Thus, we can factorize the occupancy measure matching of the joint team model as follows: $\argmin_{\JoAM}\sum_{i=1}^n \Dfdot{\calN_i, E_{-i}}{E}$. Due to the one-to-one correspondence, the following two problems lead to the same optimal solution, $\calN_E$:
\begin{align*}
    &\argmin_{\JoAM}\sum_{i=1}^n \Dfdot{\calN_i, E_{-i}}{E}  \\
    &= \argmin_{\JoAM} \sum_{i=1}^n \Dfdot{\calN_i, \calN_{-i}}{E}  = \calN_E.
\end{align*}
This justifies our objective function, Eq \ref{eq. original objective}, for learning the expert team behavior model via distribution matching.


\citet{seo2024idil} suggest that matching \AMoccu occupancy measure amounts to matching two factored occupancy measures, $\OMpi{}{}$ and $\OMtx{}{}$, simultaneously with their expert counterparts in the single-agent problem. We refer to these factored occupancy measures as \PIoccu occupancy measure and \TXoccu occupancy measure, respectively, and define them for each agent $i$ as:
\begin{align*}
    \OMpi{i}{} &= \sumP{\TmO{i}{t}\myeq o_i, \TmA{i}{t}\myeq a_i, \TmX{i}{t}\myeq x_i|\JoAM, \calM}\\
    \OMtx{i}{} &= \sumP{\TmO{i}{t}\myeq o_i, \TmX{i}{t}\myeq x_i, \TmX{i}{t-1}\myeq \PrX{i}|\JoAM, \calM}
\end{align*}
With the factored occupancy measures, we can further factorize Eq. \ref{eq. original objective} as follows:
\begin{align}
&\argmin_{\JoAM} \sum_{i=1}^n \left(\Dfpi{i}{E}{i}{} \right.  \nonumber \\
&\hspace{20ex}\left.+ \Dftx{i}{E}{i}{}\right)  \label{eq. actual objective}
\end{align}
The proof for Eq. \ref{eq. actual objective}, along with the adjusted theorems that formulate this factored objective for the multi-agent scenario, is provided in the Appendix.

