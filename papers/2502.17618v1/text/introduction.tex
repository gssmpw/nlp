
\section{Introduction}
\label{sec. intro}
Imitation learning (IL) is a paradigm for training agent behaviors using demonstrations \cite{abbeel2004apprenticeship}. IL typically assumes that the demonstrations are generated by an expert following a single, optimal policy. Under this assumption, IL algorithms learn an estimate of the expert's policy. Compared to reinforcement learning (RL), another popular framework for training agents, IL offers a key advantage in many practical applications: it does not require hand-engineered rewards. Designing rewards often requires extensive domain expertise, and in many cases, informative rewards can be challenging for end-users to engineer. Moreover, even with hand-engineered rewards, the agent can learn suboptimal or reward hacking behaviors~\cite{amodei2016concrete, sutton2018reinforcement}. In contrast, often end-users can more easily provide demonstrations of desirable agent behavior~\cite{osa2018algorithmic, arora2021survey, chernova2022robot}.

Despite this advantage, traditional IL algorithms face challenges while learning complex behaviors from demonstration collected by human end-users. A key challenge is that conventional IL methods consider a \textit{single agent} that has \textit{full observability} of the environment. In reality, human end-users often perform complex tasks as teams rather than individually. As a result, demonstrations available for learning often involve multiple agents interacting with one another and their environment. Since the dynamics of the environment depend on these interactions, specialized IL algorithms are needed to learn multi-agent behaviors.

\input{text/figure_motivating_example}

%===== existing multi-agent imitation learning problem
To address this challenge, recent approaches have extended imitation learning to multi-agent settings \cite{song2018multi,yu2019multi,bhattacharyya2018multi,yang2020bayesian}. These \textit{Multi-agent IL} (MAIL) methods typically assume that demonstrations are generated by a single, well-defined multi-agent policy \cite{lin2019multi}. However, due to practical challenges, this assumption is difficult to satisfy in complex multi-agent tasks encountered in the real world.
As illustrated in Fig. \ref{fig: workplace 1}, complex multi-agent tasks often consist of multiple subtasks. Demonstrations of such tasks frequently involve a variety of subtask allocations, including suboptimal ones. Suboptimality can arise from various factors, including the decentralized nature of multi-agent task execution and the partial observability that individual agents have of the task environment and other agents \cite{reyes2019makes, seo2021towards, seo2025socratic}.
Consequently, real-world demonstrations inherently exhibit multiple modes of multi-agent behavior, diverging from the assumptions of most existing MAIL methods.
\ifarxiv
\blfootnote{This article is an extended version of an identically-titled paper accepted at the \textit{International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2025)}.}
\else
\blfootnote{An extended version of this paper, which includes supplementary material mentioned in the text, is available at \url{http://tiny.cc/dtil-appendix}}
\fi

%===== HIL as an approach to handle multimodal behavior.
In single-agent settings, \textit{hierarchical imitation learning} methods that explicitly impose a hierarchical structure on an expert's decision-making have been employed to address the challenge of modeling multimodal behaviors \cite{unhelkar2019learning,jing2021adversarial,sharma2018directed,seo2024idil,orlov2022factorial, jain2024godice}. However, little work has been done to extend these methods to multi-agent settings. To our knowledge, \textit{Bayesian Team Imitation Learner} (\btil) is the only approach that applies hierarchical imitation learning in a multi-agent context \cite{seo2022semi}. However, \btil is built on variational inference and tabular representations, making it difficult to scale to tasks with high-dimensional states and long horizons.


%===== contribution
To enable MAIL for more complex tasks, this paper introduces \textit{Deep Team Imitation Learner} (\ouralg), a multi-agent hierarchical imitation learning algorithm. \ouralg rigorously extends single-agent hierarchical imitation learning to collaborative tasks conducted in partially observable environments, enabling the learning of multimodal team behaviors from heterogeneous demonstrations, even in tasks with long horizons and continuous state representations.
At its core, \ouralg leverages the state-action distribution matching framework, a mainstay of state-of-the-art IL methods due to its performance and scalability \cite{ho2016generative,garg2021iq}. 

While recent hierarchical IL methods have applied distribution matching in single-agent settings \cite{jing2021adversarial,seo2024idil}, its extension to learning multimodal team behavior under partial observability remains unexplored. Notably, key theoretical results in distribution-matching-based hierarchical imitation learning, such as Theorem 1 (Bijection) in \cite{jing2021adversarial} and Theorem 2.4 (Convergence) in \cite{seo2024idil}, have only been proven under full observability. Thus, additional theoretical justification is required for learning multimodal team behavior in partially observable settings.
Hence, we first extend these theoretical results to the partially observable multi-agent hierarchical imitation learning setting. Next, leveraging these theoretical results, we derive \ouralg to effectively learn the hierarchical team policies. Finally, we evaluate \ouralg on a suite of collaborative tasks, demonstrating that it outperforms MAIL baselines in modeling team behavior across multiple scenarios.