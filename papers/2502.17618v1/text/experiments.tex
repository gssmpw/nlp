\section{Experiments}

Through numerical experiments, we now assess \ouralg's performance against MAIL baselines across multiple domains.

\subsection{Experimental Setup}

\subsubsection{Domains}
We evaluate \ouralg across multiple domains with varying complexity, including the \simplemulti-$n$ suite, \movers, \rescue, and the \smactwo suite. 
The key characteristics of our experimental domains are presented in Table \ref{table. domains}. Our domains include both continuous and discrete observation and action spaces, with varying numbers of agents (2-5) who are either subtask-agnostic or subtask-driven.
Please refer to Figs.~\ref{fig: individual paths}--\ref{fig: workplace} for illustrations of \simplemulti-$n$ domains. Remaining domains are illustrated in Fig.~\ref{fig: domains}.

\begin{table}[t]
\caption{Key Characteristics of Experimental Domains. ``Subtask'' refers to whether agents are subtask-driven. ``Dim'' denotes the dimension or cardinality of a space.}
\newcommand{\mcb}[2]{\multicolumn{#1}{c}{\bf #2}}

\label{table. domains}
\begin{center}
\begin{tabular}{lccccccc}  \toprule 
                & \mcb{2}{Experts}   & \mcb{2}{\shortstack{Observation\\Space}} &\mcb{2}{\shortstack{Action\\Space}} \\ 
                \cmidrule(lr){2-3}       \cmidrule(lr){4-5}            \cmidrule(lr){6-7}
\textbf{Domain} & \# agents & Subtask  & Type        & $Dim$       & Type       & $Dim$  \\ \midrule
\simplemultish-2  & 2        & Yes                  & Cont.  & 6           & Cont. & 2                  \\
\simplemultish-3  & 2        & Yes  & Cont.  & 6           & Cont. & 2                 \\
\movers         & 2     & Yes  & Disc.    & 45          & Disc.   & 6                 \\
\rescue         & 2   & Yes  & Disc.    & 56          & Disc.   & 6                  \\
\protosssh        & 5   & No   & Cont.  & 90          & Disc.   & 11                  \\
\terransh         & 5   & No   & Cont.  & 80          & Disc.   & 11                \\
\bottomrule      
\end{tabular}
\end{center}
\end{table}

The \simplemulti-$n$ simulates the motivating example introduced in Fig. \ref{fig: workplace} in continuous observation and action spaces. \movers and \rescue are collaborative team tasks in partially observable environments introduced by \cite{seo2023automated}, considering only discrete states and actions. These domains are designed to admit multiple near-optimal strategies, allowing agents to exhibit multimodal behaviors. We create synthetic agents exhibiting hierarchical behavior and generate 50 and 100 demonstrations for training and testing, respectively, for each domain. \smactwo is a challenging benchmark for multi-agent reinforcement learning \cite{ellis2023smacv2}. We consider two domains in this suite: \protoss and \terran, where a team of five agents is tasked with defeating five enemies. We obtain a multi-agent policy via MAPPO \cite{yu2022the} and generate 50 trajectories per domain for training. In all domains, team members must make decentralized decisions in partially observable environments. For more details, please refer to the Appendix.



\paragraph{Baselines} 
We compare our approach with Behavior Cloning (BC), \magail (\magailsh) \cite{song2018multi}, \iiql (\iiqlsh), and \maogail (\maogailsh).  BC is a supervised learning approach to learning policies, which serves as a fundamental baseline for imitation learning \cite{li2022rethinking}. \magail is a generative adversarial training-based \mail algorithm, which employs the centralized training with decentralized execution (CTDE) approach
\cite{goodfellow2014generative}.
\iiql is a naive multi-agent extension of \iql \cite{garg2021iq}, which applies \iql to each agent independently. Since this baseline also takes non-adversarial training, we can compare the effect of hierarchical structure in modeling team behavior with this baseline. 
To our knowledge, no approach exists for learning hierarchical policies in complex multi-agent domains. Thus, we present \maogail as a baseline, which learns a hierarchical policy of each agent separately via \ogail \cite{jing2021adversarial}. For discrete domains, \movers and \rescue, we also report the performance of \btil \cite{seo2022semi}.


\input{text/table_task_reward}
\paragraph{Metrics} 
Similar to other imitation learning algorithms \cite{song2018multi, garg2021iq}, we use the cumulative task reward to evaluate the algorithms. In \smactwo domains, we also consider the win rate in battles. If the learned multi-agent policy aligns with the expert team behavior, it will achieve a task reward similar to the expert's. However, a high task reward does not necessarily indicate alignment with the expert team model, as the algorithms might learn only one optimal policy, resulting in unimodal rather than multimodal behavior. Therefore, we also measure the accuracy of subtask inference. The closer the learned model is to the expert model, the more accurately it can predict expert subtasks from their demonstrations. We use the MAP estimation (the E-step of Alg. \ref{alg. learner}) for subtask inference. 


\subsection{Results}
\label{sec. exp results}

\subsubsection{\ouralgsh achieves expert-level task performance.} 
Table \ref{table. task reward results} shows the task rewards averaged over three trials for \simplemulti-$n$ (\simplemultish-$n$), \movers, and \rescue. We observe that \iql-based approaches, \iiqlsh and \ouralgsh, generally perform better than approaches based on generative adversarial imitation learning. Between \magailsh and \maogailsh, \magailsh performed better, likely because \magailsh additionally utilizes other agents' information during training. In contrast, \maogailsh and \ouralgsh take only individual observation-action trajectories and do not utilize any other information that might break the partial observability condition even during the training phase.

While \ouralgsh outperformed \iiqlsh in \simplemulti-$2$, it performed on par in other domains. We believe this is due to the domains being too simple, allowing subtask-agnostic approaches to extract one optimal solution from demonstrations. In more complex domains, we could observe an improvement in task performance due to the hierarchical structure of our agent model. As shown in Table \ref{table. smacv2 results}, \ouralgsh achieved the highest task reward and win rate in both the \smactwo domains: \protoss and \terran. We want to highlight that even though \iiqlsh often achieves high task rewards, it can neither learn multimodal behavior nor utilize semi-supervision. On the other hand, \ouralgsh can improve its performance by augmenting subtask labels. As shown in Table \ref{table. task reward results}, \ouralgsh achieved a task reward similar to the expert task reward in all domains with 20\% supervision.

\input{text/table_inference}
\input{text/figure_visualization_semi}
\subsubsection{\ouralgsh accurately learns multimodal team behavior.} As mentioned in Sec. \ref{sec. problem formulation}, our goal is to learn the different team behaviors generated by expert teams rather than a unimodal team policy. Additionally, in human-AI teaming applications, an AI agent must accurately interpret its human teammate's high-level plan. To achieve this, it is essential to learn a model that exhibits hierarchical behavior aligned with expert team members.
Table. \ref{table. inference} presents the accuracy of subtask inference computed with 20\%-supervision models. Note that without any supervision, we cannot associate the learned latent values with the actual subtasks. In all cases, \ouralgsh outperforms \maogailsh and the random baselines.\footnote{Note that other DNN-based baselines cannot be utilized for subtask inference.} 

\subsubsection{\ouralgsh outperforms \btil in more complex tasks.}
As demonstrated in Table \ref{table. task reward results}, \ouralg outperformed \btil in terms of task reward. While \btil's overall performance was generally below that of online methods such as \maogail and \ouralg, it performed slightly better than BC. We attribute this to \btil's offline nature, which makes it prone to compounding errors. This implies that if a \btil agent encounters a state that was not present in the training dataset, it struggles to select the appropriate action, as it has not learned anything about that state.
On the other hand, \btil's subtask inference performance was on par with, or even superior to, \ouralg. As shown in Table \ref{table. inference}, \btil achieved approximately 0.9 accuracy in subtask inference for \movers. However, we emphasize that this level of performance is only feasible in small domains, as \btil cannot scale up to domains with larger state spaces.


\subsubsection{Team models learned using DTIL generated behaviors that are qualitatively similar to those generated by expert teams.}
To interpret the learned behavior associated with each subtask ($x$), we visualized the paths generated by ten different seeds (seed=0:9) for each model in Figure \ref{fig: individual paths}. 
For this visualization, we intentionally set the part of an agentâ€™s observation related to other agents to zero, eliminating their influence on the behavior of the agent being inspected. 
Figure \ref{fig: individual paths} shows that \ouralg's subtask-driven behavior closely resembles the expert's when trained with partial supervision of subtask labels. In contrast, \maogail trajectories tend to be noisy and unfocused on a specific subgoal, even with a fixed $x$. 
We believe the superior performance of \ouralg stems from its factored structure, in which it learns separate Q-functions for $\pi$ and $\Tx$. Given that Q-functions can be interpreted as reward functions \cite{garg2021iq}, our approach effectively learns a hierarchical reward corresponding to each level of the policy. However, since \maogail does not learn separate Q-functions, it is difficult to determine whether it truly captures a hierarchical policy structure or merely optimizes the joint policy $\pi(a, x|o, x^-)$.
