\section{Background}
\label{sec. background}
In this section, we present preliminaries on distribution-matching-based imitation learning and introduce the mathematical model of team behavior.

\subsection{Imitation Learning via\\ Distribution Matching}
\label{sec. distribution matching}
Using the Markov Decision Process (MDP) framework, an agent's behavior is defined by a policy $\pi(a|s)$, which represents the probability distribution of an action $a$ given a state $s$. The goal of imitation learning is to minimize the discrepancy (represented as a loss $L$) between the learner's policy $\pi$ and the expert's policy $\pi_E$: $\min_{\pi} L(\pi, \pi_E)$. However, due to the inaccessibility of $\pi_E$, this objective is often ill-defined and highly challenging to solve.

To address this, \citet{ho2016generative} reformulate imitation learning as a problem of matching the occupancy measures of the learner and the expert. The (normalized) occupancy measure of a policy $\pi$ is defined as $\OMsa{\pi}{} \doteq \sumP{\TmS{t}\myeq s, \TmA{}{t}\myeq a|\pi}$, implying the stationary distribution over states $s$ and actions $a$ induced by $\pi$. Thanks to the one-to-one correspondence between a policy $\pi(s|a)$ and its occupancy measure $\OMsa{\pi}{}$ \cite{puterman2014markov}, matching the occupancy measures is equivalent to matching the policies. This can be formalized as:
\begin{align*}
    \argmin_{\pi}\Dfsa{\pi}{\piE}{} 
\end{align*}
where $\rhopi$ is the learner's occupancy measure, $\rhoE$ is the expert's occupancy measure, and $D_f$ denotes the $f$-divergence \cite{ghasemipour2020divergence}. While direct access to $\rhoE$ is still infeasible, it can be approximated using the empirical distribution calculated from expert demonstrations $\DemoTeam$. 
Due to its performance and scalability, since its introduction, the distribution matching approach has become a mainstream technique in imitation learning, giving rise to numerous variants, including the following multi-agent and hierarchical ones.

\paragraph{Multi-Agent Variants.} 
Assuming a unique equilibrium in multi-agent behaviors, \citet{song2018multi} formulate an occupancy measure matching problem in multi-agent settings:
\begin{align}
   \argmin_{\pi}\sum_{i=1}^n\Dfsa{\pi_i,\OtPi{i}}{\pi_{E}}{i}  \label{eq. MAIL objective}
\end{align}
where $\OtPi{i}$ is joint policies except the $i$-th agent's policy, $\pi_E$ denotes multi-agent expert policy at equilibrium, and $\OMsa{\pi_i,\OtPi{i}}{i} \doteq \sumP{\TmS{t}\myeq s, \TmA{i}{t}\myeq a_i|\pi_i, \OtPi{i}}$. This objective function implies that we can iteratively minimize the objective with respect to individual policies $\pi_1, \cdots, \pi_n$, and the updates can be calculated similarly to the single-agent problem by considering other agents' policies as part of the environment dynamics. 

\paragraph{Hierarchical Variants.} 
\citet{jing2021adversarial} extend occupancy measure matching approach to hierarchical imitation learning.
They model an agent behavior as an option policy $\tilpi=(\pi_L, \pi_H)$, where $\pi_L(a|s, x)$ and $\pi_H(x|s, x^-)$ are referred to as a low- and high-level policies, respectively, with $x$ being an option. Additionally, they define an option-occupancy measure corresponding to $\tilpi$ as 
$$\OMsaxx{\tilpi} \doteq \sumP{\TmS{t}\myeq s, \TmA{}{t}\myeq a, \TmX{}{t}\myeq x, \TmX{}{t-1}\myeq \PrX{}|\tilpi}$$ 
and prove the one-to-one correspondence between  $\tilpi$ and $\rho_{\tilpi}$. Thus, hierarchical imitation learning can also be cast as distribution matching between two option-occupancy measures $\rho_\tilpi$ and $\rho_E$:
\begin{align}
   \argmin_{\tilpi}\Dfsaxx{\tilpi}{\piE}     \label{eq. HIL objective}
\end{align}


\subsection{Model of Team Behavior}
\label{sec. teamwork model}
We borrow a model of team behavior introduced in \cite{seo2023automated} to represent our multi-agent behavior in sequential team tasks. The model consists of a decentralized partially observable MDP (Dec-POMDP) to capture the task dynamics and an Agent Markov models (AMM) to represent agents' multimodal behavior~\cite{oliehoek2016concise, unhelkar2019learning}.

\paragraph{Dec-POMDP} Dec-POMDP is a probabilistic model representing the dynamics of the partially observable sequential multi-agent tasks. It is expressed as a tuple $\calM = (n, S,\times A_i, T, \mu_0, \times \Omega_i, \{O_i\}, \gamma)$, where $n$ is the number of agents, $S$ is the set of states $s$, $A_i$ is the set of the $i$-th agent's actions $a_i$, $\Omega_i$ is the set of the $i$-th agent's observations $\obs_i$, $T(\NxS|s, a)$ denotes the probability of a state $\NxS$ transitioning from a state $s$ and a joint action $a\myeq(a_1, \cdots, a_n)$, $\mu_0(s)$ is an initial state distribution, and $\gamma$ is a discount factor. 
We define an extended action set as $\ExtA_i=A_i \cup \{\#\}$, where the symbol $\#$ denotes ``Not Available''.
$O_i:S \times \ExtA \rightarrow \Omega_i$ is an observation function for the $i$-th agent, which maps a pair of state $s$ and previous joint action $\PrJoA$ to an individual observation $\obs_i \in \Omega_i$. 
The values of previous actions $\PrA{i}$ at time $t\myeq 0$ are set as $\#$.
We denote capital letters without subscripts as joint spaces or joint functions, e.g., $A=\times A_i$ for a joint action space and $O=\prod_i O_i$ for a joint observation function.

\paragraph{Agent Markov Model (AMM)} 
When faced with complex team tasks, humans typically break them down into subtasks and dynamically adjust their plan regarding which subtasks to perform and in what order. Once they decide on the next subtask, they execute the necessary actions to complete it. 
AMM is designed to account for this hierarchical human behavior, and thus, is equivalent to hierarchical policies of Option framework with a one-step option \cite{sutton1999between, jing2021adversarial}.
Given a task model $\calM$, AMM defines the behavior model of the $i$-th agent as a tuple $(X_i, \pi_i, \Tx_i; \calM)$, where $X_i$ is the set of the possible subtasks, $\pi_i(a_i|\obs_i, x_i)$ denotes a subtask-driven policy, and $\Tx_i(x_i|\obs_i, \PrX{i})$ is the probability of an agent choosing their next subtask $x_i$ based on an observation $\obs_i$ and the current subtask $\PrX{i}$\footnote{In the following sections, we will omit the subscript $i$ from the function inputs, e.g., $\pi_i(a|\obs, x) \doteq \pi_i(a_i|\obs_i, x_i)$, when it is clear the functions pertain to individual observations, actions, and subtasks.}.
Following \cite{jing2021adversarial}, we define the value of the previous subtask at time $t\myeq 0$ as $\#$ and express the initial distribution of subtasks as $\Tx_i(x_i|\obs_i,\PrX{i}\myeq\#)$. Similar to previous works  \cite{jing2021adversarial,seo2022semi}, we assume the set of possible subtasks, $X$, is finite and given as prior knowledge. We then represent the AMM for the $i$-th agent simply as $(\pi_i, \Tx_i)$, omitting the non-learnable components $X_i$ and $\calM$.


