\section{Related Works}
We begin with a brief overview of related research.

\subsection{Imitation Learning of Multimodal Behavior}
Extensive research has been conducted on learning multimodal behaviors from demonstrations. In works such as \cite{li2017infogail, hausman2017multi}, the authors extend Generative Adversarial Imitation Learning (GAIL) to learn a policy that depends on a learned latent state. This learned latent state effectively encodes different modes of the behavior. However, these methods assume the latent states remain static during a task execution; thus, their methods are unsuitable for modeling agent behavior whose latent states can change during the tasks. 
Other approaches, such as \cite{schmerling2018multimodal}, use Conditional Variational Autoencoders (CVAE) to capture multimodal human behavior, but the learned latent space responsible for generating multimodality lacks grounding and difficult to associate with specific subtasks. 

Informed by the Option framework \cite{sutton1999between}, another line of research leverages hierarchical policies to model multimodal behavior. Hierarchical policies typically consider two levels: high-level policies that govern decision-making over extended temporal intervals, and low-level policies responsible for executing specific actions within shorter time frames \cite{le2018hierarchical, jing2021adversarial}. 
To learn such policies from demonstrations, various approaches have been explored; e.g., variational inference \cite{unhelkar2019learning, orlov2022factorial}, hierarchical behavior cloning \cite{le2018hierarchical, kipf2019compile, zhang2021provable}, and hierarchical variants of GAIL \cite{sharma2018directed, lee2020learning, jing2021adversarial, chen2023multi}. Most recently, \cite{seo2024idil} propose a factored distribution-matching approach to train hierarchical policies. While all these methods show remarkable performance in single-agent tasks, their extension to multi-agent scenarios has been rarely explored and often lacks theoretical grounds.


\subsection{Multi-agent Imitation Learning}
Learning team behavior from demonstration can be framed as a multi-agent imitation learning problem. 
Since \cite{song2018multi} introduced the multi-agent variant of generative adversarial imitation learning (called MA-GAIL), several extensions have been proposed to enhance its training efficiency and scalability \cite{yu2019multi, liu2020multi, yang2020bayesian, bhattacharyya2018multi, bhattacharyya2019simulating, sengadu2023dec}.
However, these methods generally assume that the demonstrations originate from a single multi-agent policy, limiting their ability to capture diverse team behaviors. 
Despite the importance of accounting for multimodality when modeling team behavior, only a few approaches have incorporated latent states into MAIL.
Among these, \cite{le2017coordinated} model agent roles as latent variables, while \cite{wang2022co} represent strategies as latent features. 
However, both methods assume static latent states and do not consider their dynamics. 
To address this gap, \cite{seo2022semi} propose Bayesian Team Imitation Learner (\btil), a multi-agent extension of \cite{unhelkar2019learning}, which can learn hierarchical policies of all team members from team demonstrations. Nonetheless, \btil struggles with large, complex tasks and suffers from compounding errors. In contrast, \ouralg overcomes these limitations by utilizing function approximators (e.g., neural networks) and augmenting demonstrations with online samples collected during training.

