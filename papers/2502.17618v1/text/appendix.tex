\onecolumn
\section{Proofs and Derivations}
\subsection{Theorem \ref{thm. bijection}}
\newcommand{\VWoccu}{\textsf{\Sam\Aam}}
\newcommand{\OMfull}{\rho(\obs, s, a, x, \PrX{}) }
\newcommand{\pimerged}{\tilde{\pi}_i(a, x|\obs, \PrX{})}
\newcommand{\PiParamk}{\PiParam^k}
\newcommand{\TxParamk}{\TxParam^k}
\begin{proof}
Given a task model $\calM$ and joint agent models $\calN_{1:n}$, we define a stationary distribution over the joint variables $(\obs, s, a, x, \PrX{})$ as 
\begin{align*}
    \OMfull = \sumP{\TmO{}{t}\myeq\obs, \TmS{t}\myeq s, \TmA{}{t}\myeq a, \TmX{}{t}\myeq x, \TmX{}{t-1}\myeq\PrX{}|\calM, \JoAM}.
\end{align*}
Let $\Sam \doteq(\obs, \PrX{})$ and $\Aam \doteq(x, a)$ denote the subtask-augmented state and action, respectively. From $\calM$ and $\calN_{1:n}$, we define a stationary policy $\tilde{\pi}_i$, a stationary distribution of state transition $\tilde{T}_i$, and an initial distribution $\tilde{\mu}_i$ for each agent $i$ as follows:
\begin{align*}
    \tilde{\pi}_i(\Aam_i|\Sam_i) &\doteq \pi_i(a_i|\obs_i, x_i)\Tx_i(x_i|\obs_i, \PrX{i}) \\
    \tilde{T}_i(\Sam'_i|\Sam_i, \Aam_i)&\doteq\sum_{s, s', \OtA{i}}O_i(\obs_i'|s', a)T(s'|s, a)p(s, \OtA{i}|\obs_i, \PrX{i}, x_i, a_i) \\
    \tilde{\mu}_{0, i}(\Sam_i) &\doteq \sum_s \mu_0(s)O_i(\obs_i|s, \PrA{}\myeq\#)
\end{align*}
where $p(s, \OtA{i}|\obs_i, \PrX{i}, x_i, a_i) = \frac{\sum_{\Others{\obs}{i},\Others{x}{i}, \PrX{\Others{}{i}}} \OMfull}{\sum_{\Others{\obs}{i},\Others{x}{i}, \PrX{\Others{}{i}}, s, \Others{a}{i}} \OMfull}$. 
With $\tilde{\pi}_i,\tilde{T}_i$ and $\tilde{\mu}_{0, i}$, we can drive the following $\tilde{\pi}_i$-specific Bellman flow constraints \cite{puterman1990markov}:
\begin{align*}
    \rho_{\tilde{\pi}_i}(\Sam, \Aam) &= \sum_{t=0}^\infty \gamma^t p(\Sam^t\myeq \Sam, \Aam^t\myeq \Aam) \\
    &= p(\Sam^0\myeq \Sam, \Aam^0\myeq \Aam) + \gamma\sum_{t=0}^\infty \gamma^t \sum_{\Sam', \Aam'}p(\Sam^{t+1}\myeq \Sam, \Aam^{t+1}\myeq \Aam, \Sam^t\myeq \Sam', \Aam^t\myeq \Aam') \\
    &= \tilde{\pi}_i( \Aam|\Sam) \tilde{\mu}_{0,i} (\Sam) + \gamma\sum_{\Sam', \Aam'}\sum_{t=0}^\infty \gamma^t \tilde{\pi}_i(\Aam|\Sam) \tilde{T}_i(\Sam|\Sam', \Aam') p(\Sam^t\myeq \Sam', \Aam^t\myeq \Aam') \\
    &= \tilde{\pi}_i( \Aam|\Sam) \tilde{\mu}_{0,i} (\Sam) + \gamma\tilde{\pi}_i(\Aam|\Sam) \sum_{\Sam', \Aam'} \tilde{T}_i(\Sam|\Sam', \Aam') \sum_{t=0}^\infty \gamma^tp(\Sam^t\myeq \Sam', \Aam^t\myeq \Aam') \\
    &= \tilde{\pi}_i( \Aam|\Sam) \left(\tilde{\mu}_{0,i} (\Sam) + \gamma \sum_{\Sam', \Aam'} \tilde{T}_i(\Sam|\Sam', \Aam') \rho_{\tilde{\pi}_i}(\Sam', \Aam') \right)\\
    \rho_{\tilde{\pi}_i}(\Sam, \Aam) &\geq 0.
\end{align*}
where the subscript $i$ is omitted from $\Sam$ and $\Aam$ for notational convienence. Then, the one-to-one correspondence between $\rho_{\tilde{\pi}_i}$ and $\tilde{\pi}_i$ can be drawn by the following theorem:
\begin{theorem}[Theorem 2 of \citet{syed2008apprenticeship}]
    Let $\rho$ satisfy the Bellman flow constraints:
    \begin{align*}
        \sum_a \rho_{sa} = \mu_s + \gamma \sum_{s',a'}\rho_{s'a'}T_{s'a's} \qquad \text{and}\qquad \rho_{sa} &\geq 0,
    \end{align*}
    and let $\pi_{sa} = \frac{\rho_{sa}}{\sum_a\rho_{sa}}$ be a stationary policy. Then, $\rho$ is the occupancy measure for $\pi$. Conversely, if $\pi$ is a stationary policy such that $\rho$ is its occupancy measure, then $\pi_{sa} = \frac{\rho_{sa}}{\sum_a \rho_{sa}}$ and $\rho$ satisfies the Bellman flow constraints.
\end{theorem}
Additionally, the following lemma establishes a bijection between $\tilde{\pi}(\Sam|\Aam)$ and $\calN_i=(\pi_i, \Tx_i)$:
\begin{lemma}[Lemma 3 of \citet{jing2021adversarial}]
    There is a bijection between $\pimerged$ and $(\pi_i(a|\obs, x), \Tx_i(x|\obs, \PrX{}))$, where $\pimerged =\pi_i(a|o, x)\Tx_i(x|o, \PrX{})$ and 
    \begin{align*}
    \pi_i(a|\obs, x)=\left.\frac{\pimerged}{\sum\limits_a\pimerged}\right|_{\forall \PrX{}}=\frac{\sum\limits_{\PrX{}}\pimerged}{\sum\limits_{a, \PrX{}}\pimerged}, \qquad \Tx_i(x|s, \PrX{})=\sum_a\pimerged.
    \end{align*}
\end{lemma}

Thus, since $\rho_i = \rho_{\calN_i, \OtAM{i}}=\rho_{\tilde{\pi}_i}$, a one-to-one correspondence exists between $\calN_i$ and $\rho_i$, where 
\begin{align*}
    \pi_i(a|\obs, x) = \frac{\sum_{\PrX{}} \OMam{i}{} }{\sum_{\PrX{},a} \OMam{i}{}}, \qquad \Tx_i(x|\obs, \PrX{}) = \frac{\sum_{a}\OMam{i}{}}{\sum_{a,x}\OMam{i}{}}.
\end{align*}

\end{proof}

\subsection{Equation \ref{eq. actual objective}: factored distribution matching objective}
% \subsection{Prop. \ref{thm. factorization}}
We first extends \textit{Proposition 2.1} of \cite{seo2024idil} to the multi-agent, partially-observable setting:
\begin{lemma}
\label{thm. factorization} Given a multi-agent task model $\calM$, let $\JoAM=(\pi, \Tx)$ and $\JoAM'=(\pi', \Tx')$ be two joint agent models. Then, $\OMpi{\calN_i}{} = \OMpi{\calN_i'}{}$ and $\OMtx{\calN_i}{} = \OMtx{\calN_i'}{}$ for all $i$ if and only if $\OMam{\calN_i}{}=\OMam{\calN_i'}{}$ for all $i$.
\end{lemma}
% The proof is provided in the Appendix. This proposition enables us to derive the following equivalence.
% The proof is provided in the Appendix. Since $f$-divergence is always positive, this proposition implies that when other agents' models are assumed to be experts, the following equivalence can be derived:
\begin{proof}
(\textit{if Direction}) For each agent $i$, since $\rho_{\calN_i}(\oaxx{})=\rho_{\calN_i'}(\oaxx{})$, we can derive $\rho_{\calN_i}(\oax{})=\rho_{\calN_i'}(\oax{})$ and $\rho_{\calN_i}(\oxx{})=\rho_{\calN_i'}(\oxx{})$ by marginalizing out $\PrX{i}$ and $a_i$ from both sides, respectively. 

(\textit{if-only Direction}) From $\rho_{\calN_i}(\oxx{})=\rho_{\calN'_i}(\oxx{})$, we can derive $\rho_{\calN_i}(o, \PrX{})=\rho_{\calN'_i}(o, \PrX{})$ by marginalizing out $x$ on each side. Then, from the definition of the occupancy measure, $\rho_{\calN_i}(\oxx{})=\Tx_i(x|o, \PrX{})\rho_{\calN_i}(o, \PrX{})$ and $\rho_{\calN_i'}(\oxx{})=\Tx_i'(x|o, \PrX{})\rho_{\calN_i'}(o, \PrX{})$. Since $\rho_{\calN_i}(o, \PrX{})=\rho_{\calN'_i}(o, \PrX{})$ and $\rho_{\calN_i}(\oxx{})=\rho_{\calN'_i}(\oxx{})$, we can derieve $\Tx(\cdot) = \Tx'(\cdot)$. Similarly, we can drive $\rho_{\calN_i}(o, x)=\rho_{\calN'_i}(o, x)$ from $\rho_{\calN_i}(\oax{})=\rho_{\calN'_i}(\oax{})$ by marginalizing out $a$ on each side. Then, from $\rho_{\calN}(\oax{})=\pi(a|o, x)\rho_{\calN}(o, \PrX{})=\pi'(a|o, x)\rho_{\calN'}(o, x)=\rho_{\calN_i'}(\oax{})$, we can also derive $\pi(\cdot) = \pi'(\cdot)$. Since $\pi_i=\pi_i'$ and $\Tx_i = \Tx_i'$ for all agents $i=1:n$, by the one-to-one correspondence shown in Thm. \ref{thm. bijection}, $\rho_\calN(\oaxx{i}) = \rho_\calN(\oaxx{i})$ for all $i$.
\end{proof}

With Lemma \ref{thm. factorization}, we can prove the derivation of Eq. \ref{eq. actual objective}.
\begin{proof}
Since $f$-divergence is always positive and becomes zero when the two measures are the same, at the minimum, Eq. \ref{eq. original objective} will lead to $\rho_i(\oaxx{i})=\rho_E(\oaxx{i})$ for all $i$. Likewise, Eq. \ref{eq. actual objective} will make $\rho_i(\oax{i})=\rho_E(\oax{i})$ and $\rho_i(\oxx{i})=\rho_E(\oxx{i})$ for all $i$ at the minimum. Then, by the lemma \ref{thm. factorization}, Eq. \ref{eq. actual objective} also results in $\rho_i(\oaxx{i})=\rho_E(\oaxx{i})$ for all $i$. Due to the one-to-one correspondence between $\rho_{E}$ and $\calN_E$ derived from Thm. \ref{thm. bijection}, the solutions of both Eqs. \ref{eq. original objective} and \ref{eq. actual objective} become identical, $\calN_E=(\pi_E, \Tx_E)$.
\end{proof}


\subsection{Lemma \ref{thm. oax to oaxx}}
% In this section, we provide a theoretical analysis regarding the convergence of \ouralg. For convenience, we denote the objective function in Eq. \ref{eq. actual objective} for the $k$-th iteration as follows:
% \begin{align}
%     \Loss{k}_i &\doteq \Dfpi{i}{E}{i}{k} +  \Dftx{i}{E}{i}{k}  \label {eq. individual obj}\\
%     \Loss{k} &\doteq \sum_{i=1}^n \Loss{k}_i  \nonumber
% \end{align}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Let $\Lpi{i}{k}$ and $\Ltx{i}{k}$ denote the terms in Eq. \ref{eq. individual obj}:
% \begin{align*}
%     \Lpi{i}{k} \doteq \Dfpi{i}{E}{i}{k}, \quad \Ltx{i}{k} \doteq \Dftx{i}{E}{i}{k}.
% \end{align*}
% Note that $\Lpi{i}{k}$ depends on both $\pi_i$ and $\Tx_i$, and so does $\Ltx{i}{k}$. 
% Thus, there is no guarantee that an update of $\pi_i$ made by Eq. \ref{eq. pi objective} does not increase $\Lpi{i}{k}$ and an update of $\Tx_i$ occurred by Eq. \ref{eq. tx objective} does not negatively affect the minimization of $\Lpi{i}{k}$. 
% In other words, a naive approach to independently minimizing two factored terms $\Lpi{i}{k}$ and $\Ltx{i}{k}$ does not guarantee the minimization of $\Loss{k}_i$. 
% While \cite{seo2024idil} theoretically analyze sufficient conditions for the convergence of this factored occupancy measure matching approach, their proof is only valid for fully observable settings. Specifically, when proving Lemma 2.2 of \cite{seo2024idil}, they assume the state transition $T(s'|s, a)$ is independent from the agent models. However, in partially observable settings, we cannot make such assumptions. Inevitably, the observation transition $T(\obs_i'|\obs_i, a_i)$, which corresponds to the state transition in \cite{seo2024idil}, depends on the current agent model $\calN= (\pi, \Tx)$:
% \begin{align*}
%     T(\obs_i'|\obs_i, a_i) = \sum_{s',s,\OtA{i}} O(\obs'|s', a)T(s'|s, a) p(s, \OtA{i}|\obs_i, a_i, \calN).
% \end{align*}
% Therefore, an additional theoretical analysis on the convergence of our factored approach, applicable to partially observable environments, is necessary.


\begin{proof}
\newcommand{\rhoPikTx}[1]{\rho_{\pi^{#1}, \Tx^k}}
\newcommand{\rhoPiTxk}[1]{\rho_{\pi, \Tx^{#1}}}
\newcommand{\rhoPiTxNon}{\rho_{\pi,\Tx}}
\newcommand{\qPrX}[1]{p\left({\PrX{}}|\cdot, #1\right)}
\newcommand{\rhoEoaxx}[1]{\rho_E^k(#1)\qPrX{\pi^k, \Tx^k}}
\newcommand{\deltapi}{\delta_1}
\newcommand{\deltatx}{\delta_2}
    We can expand $\Dfpilemma{}$ as follows\footnote{Except where it is unclear, we omit the subscript $i$ from functions and variables here. Also, we often use ``$\cdot$'' instead of ``$\oax{}$'' or ``$\oaxx{}$'' for brevity.}:
    \begin{align}
        &\Dfpilemma{} = \sum_{\oax{}}\rho_E^k(\oax{}) f\left(\frac{\rhoPiTxk{}(\oax{})}{\rho_E^k(\oax{})} \right) \nonumber \\
        &= \sum_{\oaxx{}}\grvRho_E^k(\oaxx{}) f\left(\frac{\rhoPiTxk{}(\oax{})\qPrX{\pi^k, \Tx^k}}{\rhoEoaxx{\oax{}}} \right) \nonumber\\
        &= \sum_{\oaxx{}}\grvRho_E^k(\cdot) \left\{
        f\left(\frac{\rhoPiTxk{}(\cdot)\qPrX{\pi^k, \Tx^k}}{\rhoEoaxx{\cdot}}\right)
        - f\left(\frac{\rhoPiTxk{}(\cdot)\qPrX{\pi, \Tx}}{\rhoEoaxx{\cdot}}\right)
        \right. \nonumber\\
        &\hspace{17ex}\left.
        + f\left(\frac{\rhoPiTxk{}(\cdot)\qPrX{\pi, \Tx}}{\rhoEoaxx{\cdot}}\right)
        \right\}. \label{eq. df expansion}
    \end{align}
    Since the last term in Eq. \ref{eq. df expansion} is $\Dfamlemma{}$, we have the following equality:
    \begin{align}
        \Dfpi{i}{E}{}{k} =A + \Dfamlemma{}, \label{eq. df relationship}
    \end{align}
    where
    \begin{align}
        A &\doteq \sum_{\oaxx{}}\grvRho_E^k(\cdot) \left\{f\left(\frac{\rhoPiTxk{}(\cdot)\qPrX{\pi^k, \Tx^{k}}}{\rhoEoaxx{\cdot}}\right)
        - f\left(\frac{\rhoPiTxk{}(\cdot)\qPrX{\pi, \Tx}}{\rhoEoaxx{\cdot}}\right)\right\}.
    \end{align}
    Since $\pi_\PiParam$ is $K_1$-Lipschitz and $\Tx_\TxParam$ is $K_2$-Lipschitz, the following holds:
    \begin{align}
        &|\Delta (\pi, \pi^k)| = |\pi_\PiParam - \pi_{\PiParam^k}| \leq K_1 |\Delta(\PiParam, \PiParam^k)| = K_1 |\epsilon_1| \label{eq. pi lipschitz}\\
        &|\Delta (\Tx, \Tx^k)| = |\Tx_\TxParam - \Tx_{\TxParam^k}| \leq K_2 |\Delta(\TxParam, \TxParam^k)| = K_2 |\epsilon_2| \label{eq. tx lipschitz}
    \end{align}
    Let us define $\tilde{\pi}(a, x|o, \PrX{}) = \pi(a|o, x) \Tx(x|o, \PrX{})$. Then, we can derive the following relationships:
    \begin{align*}
        \Delta (\tilde{\pi}, \tilde{\pi}^k) &= |\tilde{\pi} -\tilde{\pi}^k| = |\pi\cdot \Tx - \pi^k \cdot \Tx^k| \\
        &= |\pi\cdot \Tx - \pi^k \cdot \Tx + \pi^k \cdot \Tx - \pi^k \cdot \Tx^k| \\
        &\leq |\pi\cdot \Tx - \pi^k \cdot \Tx| + |\pi^k \cdot \Tx - \pi^k \cdot \Tx^k| \\
        &\leq |\pi - \pi^k| + |\Tx - \Tx^k|  \qquad (\because \max |\pi| = 1 \text{ and } \max|\Tx|=1)\\
        &\leq K_1 |\Delta(\PiParam, \PiParamk)| + K_2|\Delta(\TxParam, \TxParamk)| \\
        &\leq \max(K_1, K_2)(|\epsilon_1| + |\epsilon_2|)
    \end{align*}
    Then, since $\delta \doteq |\Delta (\tilde{\pi}, \tilde{\pi}^k)|$ is sufficiently small by the lemma condition, we can derive the first-order approximation of $f$ at $\tilde{\pi}^k$. Then, the term $A$ can be expressed as follows:
    %  Taylor expansion of functional
    \begin{align}
        &A \approx  \sum_{\oaxx{}}\grvRho_E^k(\cdot) \left\{f'\left(\frac{\rhoPikTx{k}(\cdot)}{\rho_E^k(\cdot)}\right) \frac{1}{\rho_E^k(\cdot)} \frac{d}{d\tilde{\pi}}\rho_{\tilde{\pi}}(\cdot)\delta \right. \nonumber \\
        &\hspace{10ex}\left. - f'\left(\frac{\rhoPikTx{k}(\cdot)}{\rho_E^k(\cdot)}\right) 
        \frac{\qPrX{\pi^k, \Tx^k} \frac{d}{d\tilde{\pi}}\rho_{\tilde{\pi}}(\cdot) + \rhoPikTx{k}(\cdot) \frac{d}{d\tilde{\pi}}\qPrX{\tilde{\pi}}}{\rhoEoaxx{\cdot}} \delta
        \right\} \nonumber \\
        &= \sum_{\oaxx{}}
        -f'\left(\frac{\rhoPikTx{k}(\cdot)}{\rho_E^k(\cdot)}\right) \rhoPikTx{k}(\cdot) \frac{d}{d\tilde{\pi}}\qPrX{\tilde{\pi}} \delta\nonumber \\
        &=\frac{d}{d\tilde{\pi}}\sum_{\oaxx{}}
        -f'\left(\frac{\rhoPikTx{k}(\cdot)}{\rho_E^k(\cdot)}\right) \rhoPikTx{k}(\cdot)\qPrX{\tilde{\pi}} \delta \nonumber \\
        % The switching will be okay if the rule of sum applies to \pi (?) --> (most basically) no problem if it's a finite sum
        &=\frac{d}{d\tilde{\pi}}\sum_{\oaxx{}}
        -f'\left(\frac{\rhoPikTx{k}(\oax{})}{\rho_E^k(\oax{})}\right) \rhoPikTx{k}(\oax{})\postPrX{}{\tilde{\pi}} \delta \nonumber \\
        &=\frac{d}{d\tilde{\pi}}\sum_{\oax{}}
        -f'\left(\frac{\rhoPikTx{k}(\oax{})}{\rho_E^k(\oax{})}\right) \rhoPikTx{k}(\oax{})\delta \nonumber\\
        &=0 \label{eq. term A}
    \end{align}
    The last equality holds because the summation term does not depend on $\tilde{\pi}$. Since $A=0$ by Eq. \ref{eq. term A}, we can derive $\Dfpilemma{} = \Dfamlemma{}$ from Eq. \ref{eq. df relationship}.
\end{proof}


\subsection{Theorem \ref{thm. convergence}}
Before proving Thm. \ref{thm. convergence},  we first introduce the following relationship between the $\TXoccu$ occupancy measure matching and $\AMoccu$ occupancy measure matching problems:
\begin{lemma}[Lemma 2.3 of \citet{seo2024idil}]
\label{thm. oxx to oaxx}
    Define $|\Delta(\PiParam, \PiParam^k)|=\epsilon$. If $\epsilon$ is sufficiently small, then $\Dftx{\pi, \Tx}{E}{i}{k}=\Dfamchk{\pi, \Tx}{E}{i}{k}$.
\end{lemma}
Since this lemma applies to multi-agent partially observable settings without modification, we refer to \cite{seo2024idil} for the proof. This lemma implies that under the given condition, the minimization of Eq. \ref{eq. tx objective} also minimizes the difference of the \AMoccu occupancy measures between the learner and the expert.

With the lemmas \ref{thm. oax to oaxx} and \ref{thm. oxx to oaxx}, we can derive a sufficient condition for decreasing our objective, Eq. \ref{eq. actual objective}.
\begin{corollary}
    If (1) the conditions of lemma \ref{thm. oax to oaxx} is satisfied and (2) $\chkRho_E^k\approx\grvRho_E^k$, minimizing Eqs. \ref{eq. pi objective} and \ref{eq. tx objective} decreases Eq. \ref{eq. actual objective}.
\end{corollary}
\begin{proof}
    Under the given conditions, we can rewrite Eq. \ref{eq. actual objective} as follows:
    \begin{align*}
        &\Dfpi{i}{E}{i}{k} +  \Dftx{i}{E}{i}{k} \\
        &= 2\Dfamlemma{} \hspace{10ex} (\because \grvRho_E^k \approx \chkRho_E^k)
    \end{align*}
    Thus, the $\PiParam$-update via Eq. \ref{eq. pi objective} and the $\TxParam$-update via Eq. \ref{eq. tx objective} both decrease $\Loss{k}_i$ by the lemmas \ref{thm. oax to oaxx} and \ref{thm. oxx to oaxx}, respectively.
\end{proof}



\paragraph{Proof of Theorem \ref{thm. convergence}}

\begin{proof}
    Let ${L'}_i^k$ denote the $f$-divergence between the estimate of the expert occupancy measure and the estimate of the expert model right before the subsequent E-step, i.e.,
    \begin{align*}
        {L'}_i^k = \Df{\rho_{\pi^k,\Tx^k}}{\rho_{E}^{k-1}}{\oaxx{i}} .
    \end{align*}
    Then, we can derive the following inequalities regarding the loss updates:
    \begin{align*}
         &{L'}^k = \sum_i  {L'}_i^k \\
         &=\sum_i \left(\Df{\rho_{\pi^k,\Tx^k}}{\rho_{E}^{k-1}}{\oaxx{i}}\right)\\
         &=\sum_i \left( 
            \sum_{\oaxx{i}} p(x_i, \PrX{i}|o_i, a_i, \JoAM^{k-1})p_E(o_i, a_i) f \left( \frac{\rho_{\calN_i^k}(\oaxx{i})}{p(x_i, \PrX{i}|o_i, a_i, \JoAM^{k-1})p_E(o_i, a_i)} \right) \right) \\
         &\geq \sum_i \left( 
            \sum_{\obs_i, a_i} p_E(o_i, a_i) f \left( \frac{\rho_{\calN_i^k}(\obs_i, a_i)}{p_E(\obs_i, a_i)} \right)   \right)~\qquad ~(\because \text{\textit{f} is convex})\\
        &=\sum_i \left( 
            \sum_{\oaxx{i}} p(x_i, \PrX{i}|o_i, a_i, \JoAM^{k})p_E(o_i, a_i) f \left( \frac{\rho_{\calN_i^k}(\oaxx{i})}{p(x_i, \PrX{i}|o_i, a_i, \JoAM^{k})p_E(o_i, a_i)} \right)  \right)\quad (\because \text{E-step})\\
         &=\sum_i \left(\Df{\rho_{\pi^k,\Tx^k}}{\rho_{E}^{k}}{\oaxx{i}} \right)~~ (:= L^k)\\
         &\geq \sum_i \left(\Df{\rho_{\pi^{k+1},\Tx^{k+1}}}{\rho_{E}^{k}}{\oaxx{i}}\right) \\
         &\hspace{9cm}(\because \text{M-step with the condition (1)}) \\
         &={L'}^{k+1}
    \end{align*}
    Since ${L'}^k \geq {L}^k$ and ${L}^k \geq {L'}^{k+1}$, $L^{k} \geq L^{k=1}$.
\end{proof}
% In the experiments, we empirically demonstrate that \ouralg can learn agent models that match experts' behavior with small supervision in partially observable multi-agent scenarios.


% \seo{
% \subsection{IQ-Learn Adaptation}
% }

\subsection{E-step: Derivations of the MAP Estimation}
In E-step, we infer the values of unknown subtasks using the following MAP estimation:
\begin{align*}
\argmax_{\TmX{i}{0:h}} p(\TmX{i}{0:h}|\TmOA{}{0:h}, \PaAM{}{k}) &=\argmax_{\TmX{i}{0:h}} p(\TmX{i}{0:h}, \TmOA{}{0:h}| \PaAM{}{k}) \\
&= \argmax_{\TmX{i}{0:h}}\log p(\TmX{i}{0:h}, \TmOA{}{0:h}| \PaAM{}{k}) 
\end{align*}
This can be effectively computed using the following dynamic programming approach:
\begin{align*}
\alpha_t(x) &\doteq \max_{\TmX{i}{0:t-1}}\log p(x_i^t\myeq x, \TmX{i}{0:t-1}, \TmOA{}{0:t}| \PaAM{}{k})  + c_t\\
&=\max_{y}\left( \log \pi_i^k(a_i^t|\obs_i^t, x_i^t\myeq x) + \log \Tx_i^k(x_i^t\myeq x|\obs_i^t, x_i^{t-1}\myeq y) + \alpha_{t-1}(y)\right)\\
\alpha_0(x) &= \log \pi_i^k(a_i^0|\obs_i^0, x_i^t=x) + \log\Tx_i^k(x_i^0=x|\obs_i^0, \#)
\end{align*}
where $c_t$ is a constant that does not depend on $x$ and can be ignored for this MAP estimation.
Since the inference only depends on individual agent models, it can be decentralized. With a time complexity of $O(h|X_i|)$ for each agent $i$, the overall time complexity for $n$ agents is $O(nh|\bar{X}|^2)$ where $|\bar{X}|=\max_{i=1:n}|X_i|$.
\clearpage
\section{Experiment Details}
\subsection{Domain Descriptions}
\label{sec. domain descriptions }



\begin{wrapfigure}{r}{0.27\textwidth}
  \centering
  \includegraphics[width=0.27\textwidth]{image/mj3.png}
  \caption{\simplemulti-$3$}
  \label{fig: mj3}
\end{wrapfigure}
\paragraph{\simplemulti-$n$} This domain gamifies the motivating example introduced in Fig. \ref{fig: workplace}. A team of two members must complete their job at $n$ designated locations. Each location has two different jobs for each agent, and an agent can complete only one type of job. If both agents are at the same location, they are distracted and none of them complete the job. A maximum of five jobs of each type can be stacked up per location, and agents can accomplish only one at a time. Once all five jobs are completed, the location is temporarily empty with that job type. However, after 15 timesteps, another five jobs regenerate at the location. While the world size is 10-by-10, agents can only observe within a radius of 2 (the red and blue circle around each agent in Fig. \ref{fig: mj3}). 
Additionally, they cannot know if a location has the jobs they are looking for unless they are at the location (distance threshold $< 0.5$). 
Both the observation and action spaces are continuous. An agent's observation consists of the following information: $(x, y, b_{job}, b_{tm},\Delta x_{tm}, \Delta y_{tm})$ where $(x, y)$ is the position of the agent, $b_{job}$ is a binary value indicating whether the agent is engaged in a job, $b_{tm}$ is a binary value indicating whether the teammate is observed, and $(\Delta x_{tm}, \Delta y_{tm})$ is the relative position of the teammate with respect to the agent. If the teammate is not within the observable distance, $b_{tm}, \Delta x_{tm}$ and $\Delta y_{tm}$ are all set to 0. We create synthetic agents that exhibit multimodal hierarchical behavior and generate 50 and 100 demonstrations for training and testing, respectively.


\paragraph{\movers and \rescue} These domains are employed from \cite{seo2023automated}. The goal of \movers is to move boxes to the truck with a team of two.  Since one agent cannot move boxes alone, explicit coordination between two agents is required. In \rescue, two agents are tasked with rescuing all the victims at four different disaster sites. While in City Hall and Campsite, the victims can be rescued by one agent, both agents must work together in bridges. In both domains, agents can observe only their vicinity (the unshaded area in Figs. \ref{fig: movers} and \ref{fig: flood}), forcing them to estimate their next location for effective collaboration. We assume an agent selects one of four locations (three boxes and the truck in \movers, and four disaster sites in \rescue) as their subtasks. In \movers, an agent's observation includes the following:  $(x, y, a, b_{tm},\Delta x_{tm},\Delta y_{tm}, a_{tm}, box_1, box_2, box_3)$ where $(x, y)$ represents the agent's position, $a$ is the agent's previous action, $b_{tm}$ is a binary indicating whether the teammate is observed, $(\Delta x_{tm}, \Delta y_{tm})$ represents the relative position of the teammate with respect to the agent, and $box_i$ represents the state of the box $i$. Each $box_i$ can take one of four values: ``Not observed'', ``At the original location'', ``Being carried'', and ``At the goal''. Since this domain is discrete, $x, y, a,  \Delta x_{tm}, \Delta y_{tm}, a_{tm}$ and $box_i$s are all one-hot encoded, respectively, resulting in the observation dimension of 45. When the teammate is not observed, $\Delta x_{tm}, \Delta y_{tm}$ and $a_{tm}$ are also set to 0 in addition to $b_{tm}$. In \rescue, an observation is represented by a 56-dimension vector: $(loc, a, b_{tm}, loc_{tm}, a_{tm}, w_1, w_2, w_3, w_4)$ where $loc$ is one of 32 positions on the map where the agent is currently located, $a$ is the agent's previous action, $b_{tm}$ is a binary indicating whether the teammate is observed, $loc_{tm}$ is the location of the teammate, $a_{tm}$ is the teammate's previous action, and $w_i$ is a binary indicating the status of the rescue work at each disaster site $i$. $loc_{tm}$ can take either "the same location as the agent" or one of six landmarks (``City Hall'', ``Fire Station'', ``Upper Bridge'', etc.). Similar to \movers, all variables are one-hot encoded, except that $loc_{tm}$ and $a_{tm}$ are set to all-zero when the teammate is not observed.
We use 50 and 100 demonstrations for training and testing per each domain.


% \paragraph{\rescue} In this domain, two agents are tasked with rescuing all the victims at four different disaster sites. 

\paragraph{\smactwo suite} \smactwo is a challenging benchmark for multi-agent reinforcement learning \cite{ellis2023smacv2}. We consider two domains in this suite: \protoss and \terran. The goal in these domains is to defeat an enemy team of five units(agents) by controlling five agents as a team. Each domain consists of different types of agents, and a team composition can differ between trials as they are randomly generated from a distribution.
We train a multi-agent policy via multi-agent reinforcement learning and generate 50 trajectories per domain for training. While the demonstrations do not include any diverse subtask-driven behavior, we set the number of subtasks as three for all agents to see the benefit of including latent states in the agent model.

\subsection{Expert Models}
We describe here the behavior of an expert team that has been used to generate demonstrations.
\paragraph{\simplemulti-$n$} We handcrafted subtask-driven expert behavior according to common sense rules. The set of subtasks consists of the locations designated for performing jobs (conveyor belts in Fig. \ref{fig: mj3}). Given a location as subtask, the action-level policy is implemented to move towards the location with some random noise. If the agent discovers that another agent is already at the targeted location, it waits around the target, maintaining some distance so as not to distract the other agent. Meanwhile, the subtask transition is modeled as follows: If the agent does not observe another agent and is not at the intended location, it maintains its subtask. If the agent does not gain a reward for conducting its job (due to no remaining jobs) at the intended location, it randomly selects one of the other locations as its new target. If the agent observes another agent, it randomly picks one of all locations as its new target. The agent maintains its subtask if it picks the same location it originally targeted. At the start of the task, each agent randomly selects one location as its subtask.

\paragraph{\movers and \rescue} We use value iteration to obtain experts' action-level policies. For the value iteration, we set a large positive reward for the states where an agent is at the intended location and a small negative reward for every other state as a penalty. The subtask transitions of each expert member are manually specified to follow the descriptions provided by \cite{seo2023automated}.

\paragraph{\smactwo suite} We use MAPPO \cite{yu2022the} to train a multi-agent policy, which is available at \url{https://github.com/marlbenchmark/on-policy}. We used the default set of hyperparameters and trained MAPPO with 20M exploration steps. It took a centralized approach, which shared policy networks across all agents.

\section{Implementation Details}
We use Python 3.8 to implement domains and algorithms and PyTorch 2.0.0 to build deep neural network models.

\subsection{Baseline Implementation}
We utilize official or popular source code as much as possible to implement reliable baselines. For Behavior Cloning, we use a version available at \url{https://github.com/HumanCompatibleAI/imitation} \cite{gleave2022imitation}. For \iql \cite{garg2021iq}, we use the official implementation provided by the authors at \url{https://github.com/Div99/IQ-Learn}. Note that while we do not directly use it as our baseline, it is essential for implementing both \iiqlsh and \ouralg. The implementation of \magail and \maogail are based on \ogail implementation that can be found at \url{https://github.com/id9502/Option-GAIL}. For \magail, we also make its critics dependent on other agents' information, following the original paper \cite{song2018multi}. 

For all baselines, we use a multi-layer perception (MLP) with two hidden 128-node layers for the actors and critics. For discriminators, we use MLP with two hidden layers of 256 nodes. The batch size has been standardized to 256.
For \movers and \rescue, we capped the maximum exploration steps at $100k$ for \iql-based approaches and $300k$ for GAIL-based approaches. For other domains, these limits were set to $200k$ and $500k$, respectively. BC was trained using $10k$ updates across all domains.


\subsection{\btil Adaptation}
Since \btil is limited to discrete observation and action spaces and cannot scale up to large domains, it is infeasible to be applied to \simplemulti-suite and \smactwo-suite. It is also impossible to apply \btil to   \movers and \rescue if we want to directly use the current observation representation, which is a concatenation of elements in subspaces. However, because the sets of observations in these domains are finite, we apply \btil by numbering each observation from 1 to $|\Omega_i|$. In this case, the number of possible observations for agent $i$ will be $\approx 1.0 \cdot 10^6$ for \movers and $\approx 1.3 \cdot 10^5$ for \rescue. For \btil, we use the implementation available at  \url{https://github.com/unhelkarlab/BTIL}\cite{seo2022semi}.

\subsection{Hyperparameters}
We grid-search the hyperparameter space to find the optimal one. In all domains, we use `Relu' as the neural net activation function, the learning rate of $3e-4$ for critics, the learning rate of $1e-4$ for actors, and a single critic structure. For the \iql submodule, we use $0.01$ for the temperature parameters, `value' for IQ method losses, and $Chi$ for $f$-divergence function. We use the replay buffer size $5k$ for \movers and \rescue, and $30k$ for other domains. 




\subsection{Additional Results}
\subsubsection{Learning Curve}
We plot the task performance according to the number of exploration steps in Fig. \ref{fig: plots}. While \iql-based approaches generally outperformed GAIL-based approaches, as shown in Figs. \ref{fig: mj2 plot}-\ref{fig: movers plot}, \ouralg can additionally improve its performance with semi-supervision. In \rescue, overfitting is observed due to the low complexity of the domain.

\begin{figure}[t]
\newcommand\gap{0.30}
\newcommand\subgap{0.98}
  \centering
  \begin{subfigure}[b]{\linewidth}
      \centering
      \includegraphics[width=\subgap\textwidth]{image/legend_bar.png}
  \end{subfigure}
  \begin{subfigure}[b]{\gap\linewidth}
      \centering
      \includegraphics[width=\subgap\textwidth]{image/plot_mj2_2.png}
      \caption{\simplemulti-$2$}
      \label{fig: mj2 plot}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{\gap\linewidth}
      \centering
      \includegraphics[width=\subgap\textwidth]{image/plot_mj3_2.png}
      \caption{\simplemulti-$3$}
      \label{fig: mj3 plot}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{\gap\linewidth}
      \centering
      \includegraphics[width=\subgap\textwidth]{image/plot_movers_2.png}
      \caption{\movers}
      \label{fig: movers plot}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{\gap\linewidth}
      \centering
      \includegraphics[width=\subgap\textwidth]{image/plot_flood_2.png}
      \caption{\rescue}
      \label{fig: flood plot}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{\gap\linewidth}
      \centering
      \includegraphics[width=\subgap\textwidth]{image/plot_protoss_2.png}
      \caption{\protoss}
      \label{fig: protoss plot}
  \end{subfigure}
  \hfill  
  \begin{subfigure}[b]{\gap\linewidth}
      \centering
      \includegraphics[width=\subgap\textwidth]{image/plot_terran_2.png}
      \caption{\terran}
      \label{fig: terran plot}
  \end{subfigure}

  \captionsetup{subrefformat=parens}
  \caption{The average task returns vs. the number of exploration steps. Each method is plotted with three seeds.}
  \label{fig: plots}
\end{figure}

\subsubsection{Behavior of team models in unsupervised learning settings.}

Without supervision, a hierarchical imitation learner may develop entirely different action policies that do not align with any subtask-driven behaviors of the expert. Nevertheless, even in such settings, our approach outperforms \maogail as a multi-agent hierarchical imitation learner. Since no clear metric exists for quantifying subtask-driven behavior quality, we qualitatively assess this advantage by visualizing the learned team models' paths for each $x$. Fig. \ref{fig: unsupervised paths} shows team models learned without subtask annotations. As illustrated, the behavior differences based on $x$ are more discernible in \ouralg than \maogail. Unlike \maogail, \ouralg more clearly exhibits subtask-driven behaviors, approaching different conveyor belts based on $x$. Notably, the learned $x$ values do not necessarily correspond to the expert's actual subtasks due to a lack of grounding. For instance, the behaviors for $x=2$ and $x=3$ are swapped relative to the expert's substask indices $2$ and $3$, as presented in Figure \ref{fig: expert a1 visualization}.



\begin{figure}[h]
  \def\subfwid{.48}
  \def\figwid{.3}
  \centering
  \begin{subfigure}[t]{\subfwid\linewidth}
      \centering
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_mahil_a0_m1_x0.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_mahil_a0_m1_x1.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_mahil_a0_m1_x2.png}
      \caption{\ouralg (Agent 1)}
      \label{fig: unsupervised DTIL a1}
  \end{subfigure}
  \hspace{1ex}
  \begin{subfigure}[t]{\subfwid\linewidth}
      \centering
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_mahil_a1_m1_x0.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_mahil_a1_m1_x1.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_mahil_a1_m1_x2.png}
      \caption{\ouralg (Agent 2)}
  \end{subfigure}
  \begin{subfigure}[t]{\subfwid\linewidth}
      \centering
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_maogail_a0_m1_x0.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_maogail_a0_m1_x1.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_maogail_a0_m1_x2.png}
      \caption{\maogail (Agent 1)}
  \end{subfigure}
  \hspace{1ex}
  \begin{subfigure}[t]{\subfwid\linewidth}
      \centering
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_maogail_a1_m1_x0.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_maogail_a1_m1_x1.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/un_LaborDivision3-v2_maogail_a1_m1_x2.png}
      \caption{\maogail (Agent 2)}
  \end{subfigure}
  \captionsetup{subrefformat=parens}
  \caption{Individual \simplemulti-$3$ trajectories for each $x$ generated by team models learned without any subtask annotations.}
  \label{fig: unsupervised paths}
\end{figure}


\subsubsection{Visualization of diverse team behaviors learned with \ouralg.}

Figure \ref{fig: paths} illustrates the diverse team behaviors that can emerge in the same situation due to different subtask updates. To simulate team interactions, we manually set the agents' starting positions and assign their subtasks to ensure they cross paths at the center of the space. 
This scenario is simulated across five different seeds ($seed=0:4$). The dashed boxes in each image indicate instances where the two agents come close enough to observe each other.  As shown in Fig. \ref{fig: expert interaction paths}, the expert team exhibits diverse behaviors in this interactive setting. While \maogail agents produce only similar trajectories, \ouralg emulates diverse team trajectories, suggesting that it effectively learns subtask-driven agent models from heterogeneous multi-agent demonstrations.




\begin{figure}[h]
  \def\subfwid{.80}
  \def\figwid{.18}
  \centering
  \begin{subfigure}[t]{\subfwid\linewidth}
      \centering
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_expert_s0.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_expert_s1.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_expert_s2.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_expert_s3.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_expert_s4.png}
      \caption{Expert}
      \label{fig: expert interaction paths}
  \end{subfigure}
  \begin{subfigure}[t]{\subfwid\linewidth}
      \centering
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_mahil_s0.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_mahil_s1.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_mahil_s2.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_mahil_s3.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_mahil_s4.png}
      \caption{\ouralg}
  \end{subfigure}
  \begin{subfigure}[t]{\subfwid\linewidth}
      \centering
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_maogail_s0.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_maogail_s1.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_maogail_s2.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_maogail_s3.png}\hspace{0.5ex}
      \includegraphics[width=\figwid\linewidth, frame]{image/LaborDivision3-v2_maogail_s4.png}
      \caption{\maogail}
  \end{subfigure}
  \captionsetup{subrefformat=parens}
  \caption{Example trajectories of \simplemulti-$3$ trajectories generated by the expert and learned models (20 \% supervision). The triangles and arrows represent the actions of Agent 1 and Agent 2, respectively. The dashed boxes represent the locations where two agents observed each other. }% All models are trained with the semi-supervision of 0.2.}
  \label{fig: paths}
\end{figure}


