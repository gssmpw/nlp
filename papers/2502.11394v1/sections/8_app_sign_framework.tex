% \section{Theorem of Signed graph }
\section{Analysis of Previous methods via Signed Graph}
\label{app: previous}

\subsection{Discussion of Normalization}
\label{sec: prof of norm}
\paragraph{BatchNorm} 
% across different nodes in each feature dimension. 
% The update rule of BatchNorm on node representation $x_i$ 
% prevents the denominator from becoming zero 
BatchNorm centers the node representations $X$ to zero mean and unit variance  and can be written as BatchNorm($x_i$) \(=\frac{1}{\sqrt{\sigma^2 + \epsilon}}(x_i - \frac{1}{n}\Sigma_{i=1}^n x_i)\), where $ \epsilon > 0$ 
and $\sigma^2$ is the variance of node features.
We rewrite BatchNorm in the signed graph propagation form as follows: 
\begin{equation}
    \label{eq: bn sign}
    \hat{X}= \pgh{\hat{A}}  X \Gamma_d^{-1}  - \ngh{\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}  \hat{A}} X \Gamma_d^{-1} = \pgh{\hat{A}}  \tilde{X}-\ngh{\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}  \hat{A}} \tilde{X}\,,
\end{equation}
where $\Gamma_d = \diag(\sigma_1,\dots,\sigma_d)$ is a diagonal matrix that represents column-wise variance with $\sigma_i^2=\frac{1}{n}\sum_{j=1}^n ((\hat{A} X)_{_{ji}}- \mathbb{1}_n^\top \hat{A} X/n)^2$, and
$\tilde{X}= X \Gamma_d^{-1}$ is a normalized version of $X$.
We can correspond to the positive graph $\pgh{A^+}$ to $\pgh{\hat{A}}$ and the negative graph $\ngh{A^-}$ to $\ngh{\frac{\mathbb{1}_n \mathbb{1}_n^T}{n} \hat{A}}$ in Eq. \eqref{eq: bn sign}.
% Through the repulsion mechanism introduced by the negative edges, BatchNorm can mitigate the problem of oversmoothing as suggested by Theorem~\ref{thm: connected positive graph}.
% \yf{we should discuss some limitations here}

% Consider the update:
% \begin{equation}
%     \label{eq: bn sign}
%     \hat{X}= (\pgh{A}-\ngh{\frac{\mathbb{1}_n \mathbb{1}_n^T}{n} A}) X\,,
% \end{equation}
% After one signed graph propagation, the edge weight changes from $\{0,1\}$ to $\{-\frac{p+q}{2}, 1- \frac{p+q}{2}\}$, so the SB can be expressed as: 
% \begin{equation}
%     SB_{BN}= (1-\frac{p+q}{2})p + \frac{p+q}{2} (1-q) = p + \frac{p+q}{2} (1-p-q).
% \end{equation}

\paragraph{PairNorm} 
We then introduce another method called PairNorm where the only difference between it and BatchNorm is that PairNorm scales all the entries in $X$ using the same number rather than scaling each column by its own variance.
The formulation of PairNorm can be rewritten as follows:
\begin{equation}
    \label{eq: pn sign}
    \hat{X} = \frac{1}{\Gamma}\pgh{\hat{A}}  X   -  \frac{1}{\Gamma} \ngh{\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}  \hat{A}} X
    =\frac{1}{\Gamma}(\pgh{\hat{A}} X-\ngh{\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}  \hat{A}} X) \,,
\end{equation}
where $\Gamma = \|(\hat{A}- \mathbb{1}_n \mathbb{1}_n^T/n)X \|_F/\sqrt{n} $. 
% Comparing~\eqref{eq: pn sign} to~\eqref{eq: bn sign}, 
We observe that PairNorm shares the same positive and negative graphs (up to scale) as BatchNorm.
Another normalization technique, ContraNorm, turns out to extend the negative graph to an adaptive one based on node feature similarities. 
% which reveals that its effectiveness in alleviating oversmoothing is actually attributed to a mechanism very similar to BatchNorm.

% Through the repulsion mechanism introduced by the negative edges, BatchNorm can mitigate the problem of oversmoothing as suggested by Theorem~\ref{thm: connected positive graph}.
% Meanwhile, from \eqref{eq: bn sign} and~\eqref{eq: pn sign}, we see that BatchNorm and PairNorm inject the negative graph  $-(\mathbb{1}_n \mathbb{1}_n^T/n)  \hat{A}$ through a constant transformation of the positive graph derived from the original graph structure $\hat{A}$.



\paragraph{ContraNorm}
ContraNorm is inspired by the uniformity loss from contrastive learning, aiming to alleviate dimensional feature collapse.
For simplicity, we consider the spectral version of ContraNorm
% Proposition 2) 
% \xinyic{better with an exact reference: which Theorem in ContraNorm} 
% without additional regularization and LayerNorm that are used additionally in practice, 
that takes the following form:
\begin{equation}
    \label{eq: contra sign}
    \hat{X} = (1 + \alpha) \pgh{\hat{A}}X- \alpha /\tau \ngh{(X X^{T}) \hat{A} } X \,,
    % = ((1 + \alpha) \pgh{\hat{A}}- \alpha /\tau \ngh{(X X^{T}) \hat{A} }) X
\end{equation}
where $\alpha\in(0,1)$ and $\tau>0$ are hyperparameters.
We can see that $\pgh{\hat{A}}$ is again the positive graph and $\ngh{(X X^T)\hat{A}}$ is the negative graph in the corresponding signed graph propagation.

\begin{proposition}
    Consider the update:
    \begin{equation}
        \hat{X} = \pgh{A}X-\ngh{\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}A}X,
    \end{equation}
    where $A\in \{0,1\}^{n \times n}$ is the adjacency matrix. Define the overall signed graph adjacency matrix $A_s$ as $A-\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}A$. Then we have that the signed graph is (weakly) structurally balanced only if the original graph can be divided into several isolated complete subgraphs. 
\end{proposition}

\paragraph{Proof.} Assume that there is no isolated node and no node has edges with all the other nodes.
    $(A_s)_{i,j}=(A)_{i,j}-\frac{deg_j}{n}$.
    If $(A)_{i,j}=1$, then we have $(A_s)_{i,j}>0$.
    If $(A)_{i,j}=0$, then we have $(A_s)_{i,j}<0$.
    
    If the nodes can be divided into several isolated complete subgraphs, then the nodes set $V=V_1\cup V_2 \dots V_m$, where $|V_i|>1$, $m$ is the number of the isolated complete subgraphs. 
    So only the nodes within the same set have edges, thus relative entries of $A_s>0$, while nodes from different sets do not, thus relative entries of $A_s <0$.
    
    On the other hand, if $A_s$ is (weakly) structurally balanced, then the nodes set can be expressed as $V=V_1\cup V_2 \dots V_k$, where $|V_i|>1$, $k$ is the number of the separated parties in the signed graph. 
    The entry of $A_s$ in the same parties is positive, while between different parties is negative.
    According to $(A_s)_{i,j}=(A)_{i,j}-\frac{deg_j}{n}$, we know that nodes in the same parties are connected in the original graph while not connected in the original graph between different parties.
    So the graph can be divided into several isolated complete subgraphs.

    Overall, the signed graph is (weakly) structurally balanced only if the original graph can be divided into several isolated complete subgraphs, the proof is over. 

The Proposition shows that in order for the structural balance property to hold for the signed graph of normalization, the graph needs to satisfy an unrealistic condition where the edges strictly cluster the nodes.

\paragraph{Discussion of ContraNorm}
% \label{sec: prof of contra}
% \jq{not very sure how to give the detailed expression of the theorem and proof.}
% \begin{proposition}
    Consider the update:
    \begin{equation}
    \label{app_eq: contra theory sign}
        \hat{X} = \pgh{A}X-\ngh{\frac{XX^T}{n}A}X,
    \end{equation}
    Define the overall signed graph adjacency matrix $A_s = A-\frac{XX^T}{n}A$ where $(A_s)_{i,j}=(A)_{i,j}- \frac{1}{n}\Sigma_{k=1}^n x_ix_k^T(A)_{k,j}$ . 
    % Then we said that under the signed propagation \ref{eq: contra theory sign}, the node features will not converge to a constant $C$ with any initial nodes X(0).
% \end{proposition}

% \paragraph{Proof.} 
Assume that the nodes feature is normalized every update, that is $||x_i||_2=1$ for every $i$.

If $(A)_{i,j}=1$, then we have that
\begin{equation}
\begin{aligned}
    (A_s)_{i,j}&=(A)_{i,j}- \frac{1}{n}\Sigma_{k=1}^n x_ix_k^T(A)_{k,j}\\
    &=1-\frac{1}{n}\Sigma_{k=1}^n x_ix_k^T(A)_{k,j}\\
    &>1-\frac{1}{n}\Sigma_{k=1}^n(A)_{k,j}\\
    &=1-\frac{d_j}{n}>0.\\
\end{aligned}
\end{equation}
That means if $(A)_{i,j}=1$, then  $(A_s)_{i,j}>0$.
However, if $(A)_{i,j}=0$, then we have that
\begin{equation}
    \begin{aligned}
        (A_s)_{i,j}&=(A)_{i,j}- \frac{1}{n}\Sigma_{k=1}^n x_ix_k^T(A)_{k,j}\\
        &= -\frac{1}{n}\Sigma_{k=1}^n x_ix_k^T(A)_{k,j}\\
        &= -\frac{1}{n} \Sigma_{k \in N_j}x_ix_k^T.
    \end{aligned}
\end{equation}

Intuitively, if $x_i$ has similar features to $x_j$'s neighbors, then we have that $(A_s)_{i,j}<0$, which means trying to repel nodes with similar representations. 
If $x_i$ has different features to $x_j$'s neighbors, then we have that $(A_s)_{i,j}>0$, which means trying to aggregate nodes with original different representations. 

If graph $G$ is a completed graph, then all entries of $(A_s)>0$, however, when all of the nodes coverage to each other, $\Sigma_{k=1}^n x_ix_k^T(A)_{k,j}=\Sigma_{k=1}^n x_ix_k^T$ will also become bigger.

% Moreover, if all of the node features converge to $C\in \mathbb{R}^d$, the l2-norm of each node feature will be $||C^2||=1$, then we have that:
% \begin{equation}
% (A_s)_{i,j}=  
% \left\{
% \begin{array}{l}
%   1-\frac{1}{n} \Sigma_{k \in N_j}x_ix_k^T, (A)_{i,j}= 1; \\
%  -\frac{1}{n} \Sigma_{k \in N_j}x_ix_k^T, (A)_{i,j}= 0.
% \end{array}
% \right.
% \end{equation}
% Then, according to the Eq.\eqref{app_eq: contra theory sign}, we have that result of the updated node feature $\hat{x}_i$ as following:
% \begin{equation}
% \begin{aligned}
%     \hat{x}_i =& \Sigma_{j=1}^n (A_s)_{i,j} x_j\\
%     =& [\Sigma_{j\in \mathcal{N}_i}(1-\frac{1}{n} \Sigma_{k \in N_j}x_ix_k^T)-\Sigma_{j\notin \mathcal{N}_i}\frac{1}{n} \Sigma_{k \in N_j}x_ix_k^T]x_j\\
%     =& [d_i - \Sigma_{j\in \mathcal{N}_i}\frac{d_j}{n}-\Sigma_{j\notin \mathcal{N}_i}\frac{d_j}{n}]C\\
%     =& [d_i-\frac{2|E|}{n}]C\\
% \end{aligned}
% \end{equation}

% After $k$-th updates, we have that
% \begin{equation}
%     \hat{x}^{(k)}_i= [d_i-\frac{2|E|}{n}]^kC
% \end{equation}
% If there exists one node satisfied $|d_i-\frac{2|E|}{n} |>1$, then we have that $\hat{x}^{(2k)}_i \rightarrow \infty$, which is contradictory to the oversmoothing assumption.
% If all of the nodes satisfy $|d_i-\frac{2|E|}{n} |\leq 1$


\subsection{Discussion of Residual Connection}
\label{app: residual}
The standard residual connection~\citep{dgc,Chen2020SimpleAD} directly combines the previous and the current layer features together. It can be formulated as:
\begin{equation}
    \label{eq: residual sign}
     \hat{X} = (1-\alpha)X  + \alpha \hat{A} X = X + \alpha \pgh{\hat{A}} X -\alpha \ngh{I} X\,.
\end{equation} 
For residual connections, the positive adjacency matrix is $\pgh{\hat{A}}$ and the negative adjacency matrix $\ngh{I}$ in the corresponding signed graph propagation.
\paragraph{APPNP}
We reformulate the method APPNP~\citep{appap} as the signed propagation form of the initial node feature. 
Another propagation process is APPNP~\citep{appap} which can be viewed as a layer-wise graph convolution with a residual connection to the initial transformed
feature matrix $X^{(0)}$, expressed as: 
\begin{equation}
 \hat{X}^{(k+1)} = (1-\alpha)X^{(0)}  + \alpha \hat{A} X ^{(k)}.
\end{equation}
\begin{theorem}
With $\hat{A}^+=\Sigma_{i=0}^{k+1}\alpha^i\hat{A}^i$ and $\hat{A}^-=\alpha \Sigma_{j=0}^{k}\alpha^j\hat{A}^j$, the propagation process of APPNP following the signed graph propagation.
\end{theorem}
\textbf{Proof.}
Easily prove with mathematical induction.

In addition to combining with the last and initial layer features, the last type integrates several intermediate layer features. The established representations are JKNET~\citep{jknet} and DAGNN~\citep{dagnn}.
\paragraph{JKNET}
JKNET is a deep graph neural network which exploits information from neighborhoods of differing locality. 
JKNET selectively combines aggregations from different layers with Concatenation/Max-pooling/Attention at the output, i.e., the representations "jump" to the last layer.
Using attention mechanism for combination at the last layer, the $k+1$-layer propagation result of JKNET can be written as:
\begin{equation}
    \label{eq:jk-net}
    \begin{split}
         X^{(k+1)} &= \alpha_0 X^{(0)}  + \alpha_1  X ^{(1)} + \cdots \alpha_k X^{(k)}\\
        &= \Sigma_{i=0}^k\alpha_i \hat{A}^i X^{(0)}\,,
    \end{split}
\end{equation}
where $\alpha_0, \alpha_1, \cdots, \alpha_{k}$ are the learnable fusion weights with $\Sigma_{i=0}^k\alpha_i=1$.

\paragraph{DAGNN}
Deep Adaptive Graph Neural Networks (DAGNN)~\citep{dagnn} tries to adaptively add all the features from the previous layer to the current layer features with the additional learnable coefficients. 
After decoupling representation transformation and propagation, the propagation mechanism of DAGNN is similar to that of JKNET.
\begin{equation}
    \label{eq:dagnn}
         X^{(k+1)} = \Sigma_{i=0}^k\alpha_i \hat{A}^i H^{(0)}, \,H^{(0)}=f_\theta(X^{(0)})
\end{equation}
$ H^{(0)}=f_\theta(X^{(0)})$ ) is the non-linear feature transformation using an MLP
network, which is conducted before the propagation process and $\alpha_0, \alpha_1, \cdots, \alpha_{k}$ are the learnable fusion weights with $\Sigma_{i=0}^k\alpha_i=1$. 
\begin{theorem}
    With \pgh{$\hat{A}^+=\Sigma_{i=0}^{k-1}\alpha^i\hat{A}^i+\hat{A}^k$} and \ngh{$\hat{A}^-=\Sigma_{j=0}^{k-1}\alpha^j\hat{A}^k$}, the propagation process of JKNET and DAGNN following the signed graph propagation.
\end{theorem}
\textbf{Proof.}
Easily prove with mathematical induction.
% \jq{double check the correctness.}

As for more residual inspired methods~\citep{GCNII,wGCN,ACM-GCN,PDE-GCN}, we select GCNII and wGCN to give a detailed discussion as follows.
\begin{itemize}
    \item As for GCNII~\citep{GCNII}, it is an improved version of APPNP with the learnable coefficients $\alpha_i$ and changes the learnable weight W to $(1-\beta_i)I+\beta_i W$ each layer, so it shares the same positive and negative graph as APPNP.
    \item As for the wGCN~\citep{wGCN}, it incorporates trainable channel-wise weighting factors $\omega$ to learn and mix multiple smoothing and sharpening propagation operators at each layer, same as the init residual combines but change parameters $\alpha$ to be learnable with a more detailed selection strategy.
\end{itemize}


\subsection{Discussion of DropMessage}
For DropMessage~\citep{Fang2022DropMessageUR}, it is a unified way of DropNode, DropEdge and Dropout but with a more flexible mask strategy. We have discussed the DropNode and DropEdge in our paper. DropMessage can be viewed as randomly dropping some dimension of the aggregated node features instead of the whole node or the whole edge. 
We give the unified positive and negative graph of DropMessage in the term of the signed graph.
The propagation of DropMessage can be expressed as $H^{(k)}= AH^{(k-1)}-M_m,$ where if dropping $AH^{(k-1)}_{ij}$, then $M_{ij}=AH^{(k-1)}_{ij}$ else $M_{ij}=0$.







\section{Proof of Theorem~\ref{thm: small nega}}

Now consider the combined theorem. 

\begin{theorem}
\label{app: theorm_positive connected}
    Suppose that the positive edges are connected. Then along Equation \ref{eq: repell_neg} for any \(0 < \alpha < 1/\max_{i \in V} \deg_i^+\), there exists a critical value \(\beta_* \geq 0\) for \(\beta\) such that
    \begin{enumerate}
        \item[(i)] if \(\beta < \beta_*\), then we have \(\lim_{t \to \infty} x_i(t) = \sum_{j=1}^n x_j(0)/n\) for all initial values \(x(0)\);
        \item[(ii)] if \(\beta > \beta_*\), then \(\lim_{t \to \infty} \|x(t)\| = \infty\) for almost all initial values w.r.t. Lebesgue measure.
    \end{enumerate}
\end{theorem}

\paragraph{Proof.}
we change the signed graph update to the equivalent version of \(x_i(t)\) read as:
\[
x_i(t + 1) = x_i(t) + \alpha \sum_{j \in N_i^+} (x_j(t) - x_i(t)) - \beta \sum_{j \in N_i^-} (x_j(t) - x_i(t)).
\]
This can be expressed as:
\begin{equation}
\label{appendix_eq: nege_node}
    x(t + 1) = (1 - \alpha \deg^+ + \beta \deg^-) x_i(t) + \alpha \sum_{j \in N^+} x_j(t) - \beta \sum_{j \in N^-} x_j(t).
\end{equation}


Algorithm \ref{appendix_eq: nege_node} can be written as:
\begin{equation}
\label{appendix_eq: nege_graph}
    x(t + 1) = M_G x(t) = (I - \alpha L_G^+ - \beta L_G^-) x(t).
\end{equation}


Here, \(M_G = I - \alpha L_G^+ - \beta L_G^-\), with \(L_G^+ = \alpha L_C^+ + \beta L_C^-\) being the repelling weighted Laplacian of \(G\), defined in Sec.\ref{app_sec: negative graph}.  
From Eq.\eqref{appendix_eq: nege_graph}, \(M_G \mathbf{1}= \mathbf{1}\) always holds. We present the following result, which by itself is merely a straightforward look into the spectrum of the repelling Laplacian \(L_G^-\).

So theorem \ref{app: theorm_positive connected} can be changed to the following version:

 Suppose \(G^+\) is connected. Then along Eq.\eqref{appendix_eq: nege_graph} for any \(0 < \alpha < 1/\max_{i \in V} \deg_i^+\), there exists a critical value \(\beta > 0\) for \(\beta\) such that:
\begin{enumerate}
    \item[(i)] if \(\beta < \beta_*\), then average consensus is reached in the sense that \(\lim_{t \to \infty} x_i(t) = \frac{1}{n} \sum_{j=1}^n x_j(0)\) for all initial values \(x(0)\);
    \item[(ii)] if \(\beta > \beta_*\), then \(\lim_{t \to \infty} \|x(t)\| = \infty\) for almost all initial values w.r.t. Lebesgue measure.
\end{enumerate}

\textbf{Proof.}
Define \(J = 11^T/n\). Fix \(\alpha \in (0,1/\max_{i \in V} \deg_i^+)\) and consider \(f(\beta) = \lambda_{\max}(I - \alpha L_G^+ - \beta L_G^- - J)\), and \(g(\beta) = \lambda_{\min}(I - \alpha L_G^+ - \beta L_G^- - J)\). The Courantâ€“Fischer Theorem  implies that both \(f(\cdot)\) and \(g(\cdot)\) are continuous and nondecreasing functions over \([0, \infty)\). The matrix \(J\) always commutes with \(I - \alpha L_G^+ - \beta L_G^-\), and 1 is the only nonzero eigenvalue of \(J\). Moreover, the eigenvalue 1 of \(J\) shares a common eigenvector 1 with the eigenvalue 1 of \(I - \alpha L_G^+ - \beta L_G^-\).

Since \(G^+\) is connected, the second smallest eigenvalue of \(L_{G^+}\) is positive. Since \(0 < \alpha < \frac{1}{\max_{i \in V} \deg^+_i}\), there holds \(\lambda_{\min}(I - \alpha L_{G^+}) \geq -1\), again due to the Gershgorin Circle Theorem. Therefore, \(f(0) < 1\), \(g(0) \geq -1\). Noticing \(f(\infty) = \infty > 1\), there exists \(\beta_* > 0\) satisfying \(f(\beta_*) = 1\). We can then verify the following facts:
\begin{itemize}
  \item There hold \(f(\beta) < 1\) and \(g(\beta) > -1\) if \(\beta < \beta_*\). In this case, along Eq.\eqref{appendix_eq: nege_graph} \(\lim_{t \to \infty} (I - J)x(t) = 0\), which in turn implies that \(x(t)\) converges to the eigenspace corresponding to the eigenvalue 1 of \(M_{G}\). This leads to the average consensus statement in (i).
  \item There holds \(f(\beta) \geq 1\) if \(\beta > \beta_*\). 
  In this case, along Eq.\eqref{appendix_eq: nege_graph} \(x(t)\) diverges as long as the initial value \(x(0)\) has a nonzero projection onto the eigenspace corresponding to \(\lambda_{\max}(M_{G})\) of \(M_{G}\). 
  This leads to the almost everywhere divergence statement in (ii).
\end{itemize}
The proof is now complete.

\section{Proof of Theorem \ref{thm: repel_struct}}

\begin{theorem}
let \( A > 0 \) be a constant and define \( \mathcal{F}(z)_c \) by \( \mathcal{F}(z)_c = -c, z < -c \), \( \mathcal{F}(z)_c = z, z \in [-c, c] \), and \( \mathcal{F}(z)_c = c, z > c \). Define the function \( \theta : E \to \mathbb{R} \) so that \( \theta(\{i,j\}) = \alpha \) if \( \{i,j\} \in E^+ \) and \( \theta(\{i,j\}) = -\beta \) if \( \{i,j\} \in E^- \). 
Assume that node \( i \) interacts with node \( j \) at time \( t \) and consider the following node interaction under the signed propagation rules:
\begin{equation}
\label{app_eq: repel dyn}
    x_s(t + 1) = \mathcal{F}(z)_c((1 - \theta)x_s(t) + \theta x_{-s}(t)), \ s \in \{i,j\}.
\end{equation}

let \(\alpha \in (0,1/2)\). Assume that \(G\) is a structurally balanced complete graph under the partition \(V = V_1 \cup V_2\). 
When \(\beta\) is sufficiently large, for almost all initial values \(x(0)\) w.r.t. Lebesgue measure, there exists a binary random variable \(l(x(0))\) taking values in \(\{-c,c\}\) such that
\begin{equation}
    \mathbb{P}\left(\lim_{t \to \infty} x_i(t) = l(x(0)), i \in V_1; \lim_{t \to \infty} x_i(t) = -l(x(0)), i \in V_2 \right) = 1.
\end{equation}
\end{theorem}


\paragraph{Proof.}
The proof is based on the following lemmas.

\begin{lemma}
\label{ap_lemma: 2 bound}
Fix \(\alpha \in (0, 1)\) with \(\alpha \neq \frac{1}{2}\). For the dynamics \ref{app_eq: repel dyn} with the random pair selection process, there exists \(\beta^*(\alpha) > 0\) such that
\[
\mathbb{P}\left(\limsup_{t \to \infty} \max_{i,j \in V} |x_i(t) - x_j(t)| = 2A\right) = 1
\]
for almost all initial beliefs if \(\beta > \beta^*\).
\end{lemma}

\begin{lemma}
\label{ap_lemma: appro }
    Fix $\alpha \in (1/2, 1)$ and $\beta \geq 2/(2\alpha - 1)$. Consider the dynamics \ref{app_eq: repel dyn} with the random pair selection process. Let $G$ be the complete graph with $\kappa(G^+) \geq 2$. Suppose for time $t$ there are $i_1, j_1 \in V$ with $x_{i_1}(t) = -c$ and $x_{j_1}(t) = c$. Then for any $\epsilon \in [0, (2\alpha - 1)c/2\alpha]$ and any $i_* \in V$, the following statements hold:
\begin{enumerate}
    \item[(i)] There exist an integer $Z(\epsilon)$ and a sequence of node pair realizations, $G_{t+s}(\omega)$, for $s = 0, 1, \dots, Z - 1$, under which $x_{i_*}(t + Z)(\omega) \leq -A + \epsilon$.
    \item[(ii)] There exist an integer $Z(\epsilon)$ and a sequence of node pair realizations, $G_{t+s}(\omega)$, for $s = 0, 1, \dots, Z - 1$, under which $x_{i_*}(t + Z)(\omega) \geq A - \epsilon$.
\end{enumerate}
\end{lemma} 


\textbf{Proof.} From our standing assumption, the negative graph $G^-$ contains at least one edge. Let $k_*, m_* \in V$ share a negative link. We assume the two nodes $i_1, j_1 \in V$ labeled in the lemma are different from \(k_*, m_*\), for ease of presentation. We can then analyze all possible sign patterns among the four nodes \(i_1, j_1, k_*, m_*\). We present here just the analysis for the case with
\[
\{i_1, k_*\} \in E^+, \{i_1, m_*\} \in E^+, \{j_1, k_*\} \in E^+, \{j_1, m_*\} \in E^+.
\]
The other cases are indeed simpler and can be studied via similar techniques.

Without loss of generality we let \(x_{m_*}(t) \geq x_{k_*}(t)\). First of all we select \(G_t = \{i_1, k_*\}\) and \(G_{t+1} = \{j_1, m_*\}\). It is then straightforward to verify that
\[
x_{m_*}(t + 2) \geq x_{k_*}(t + 2) + 2\alpha c.
\]
By selecting \(G_{t+2} = \{m_*, k_*\}\) we know from \(\beta \geq \frac{2}{(2\alpha - 1)} > \frac{1}{\alpha}\) that
\[
x_{k_*}(t + 3) = -c, \quad x_{m_*}(t + 3) = c.
\]
There will be two cases:
\begin{itemize}
    \item[(a)] Let \(i_* \in \{m_*, k_*\}\). Noting that \(\kappa(G^+) \geq 2\), there will be a path connecting to \(k_*\) from \(i_*\) without passing through \(m_*\) in \(G^+\). It is then obvious that we can select a finite number \(Z_1\) of links which alternate between \(\{m_*, k_*\}\) and the edges over that path so that \(x_{i_*}(t + 3 + Z_1) \geq -c + \epsilon\). Here \(Z_1\) depends only on \(\alpha\) and \(n\).
    \item[(b)] Let \(i_* \in \{m_*, k_*\}\). We only need to show that we can select pair realizations so that \(x_{m_*}\) can get close to \(-c\), and \(x_{k_*}\) gets close to \(c\) after \(t + 3\). Since \(G^+\) is connected, either \(m_*\) or \(k_*\) has at least one positive neighbor. For the moment assume \(m'\) is a positive neighbor of \(m_*\) and \(k'\) is a positive neighbor of \(k_*\) with \(m' \neq k'\). Then from part (a) we can select \(Z_2\) node pairs so that
    \[
    x_{m_*}(t + 3 + Z_2) \leq -c + \epsilon, \quad x_{k_*}(t + 3 + Z_2) \geq c - \epsilon.
    \]
\end{itemize}
Thus, selecting the negative edge \(\{m_*, k_*\}\) for \(t + 5 + Z_2\) implies \(x_{m_*}(t + 6 + Z_2) = c\) for \(\beta \geq \frac{2}{(2\alpha - 1)}\). The case with \(m' = k'\) can be dealt with by a similar treatment, leading to the same conclusion.

This concludes the proof of the lemma.

In view of Lemmas \ref{ap_lemma: 2 bound} and \ref{ap_lemma: appro }, the desired theorem is a consequence of the second Borel--Cantelli Lemma.

\section{\jq{The relationship of oversmoothing and Theorem~\ref{thm: small nega} and Theorem~\ref{thm: repel_struct}}}
\label{app: oversmoothing of theorem 4.1 and 4.3}

\paragraph{Discussion with other training methods}
While \cite{Peng2024BeyondOU} questions the existence of oversmoothing in trained GNNs, their observations are primarily based on specific experimental settings that may not fully capture the oversmoothing challenge present in the literature. Specifically, the empirical observations in~\cite{Peng2024BeyondOU} are based on 10-layer GCNs, which, while useful for their argument, may not represent the behavior of deeper networks or other GNN architectures. Moreover, Figure 2 in~\cite{Peng2024BeyondOU}  is based on a normalized metric, which might not be the most appropriate. To see this point, suppose one wants to classify two points. In one case, we have 0.01 vs -0.01 and in the other case, we have 100 vs -100. While the normalized distance considered in~\cite{Peng2024BeyondOU} would be the same for these two cases, the latter case has a much larger margin, and it would be thus much easier to learn a classifier.
On the other hand, \cite{Cong2021OnPB} suggests that the trainability of deep GNNs is more of a problem than over-smoothing. However, over-smoothing naturally presents challenges for training deep GNNs, as when oversmoothing happens, gradients vanish across different nodes. Besides, \cite{Cong2021OnPB}compares 3 models GCN, ResGCN and GCNII, proving that GCNII is the best backbone. We have adapted our SBP to GCNII in Table~\ref{tab:gcnii-performance} and the results showed that our SBP outperforms GCNII on average, especially in the middle layers.

\paragraph{Measure on oversmoothing}
There exist a variety of different approaches to quantify over-smoothing in deep GNNs, here we choose the measure based on the Dirichlet energy on graphs~\citep{wu2023demystifying,graph_oversmoothing_survey}.
\begin{equation}
    \epsilon(X(t))=\frac{1}{v}\Sigma_{i\in V}\Sigma_{j \in N_i}||x_i(t)-x_j(t)||_2^2,
\end{equation}

\jq{where $v$ is the number of the nodes, $x_i(t)$ is the node feature of node $i$ at time $t$. $N_i$ represents the neighbor set of node $i$, In the signed graph, it including nodes connected to $i$ by both positive and negative edges.}
Oversmoothing means that when the layers are infinity, all of the node features will converge, that is to say $\lim_{t \to \infty}\epsilon(X(t))\to 0$.

In Theorem~\ref{thm: small nega}, there are 2 cases: 
\begin{itemize}
    \item $if \beta < \beta_*, \text{then we have }\lim_{t \to \infty} x_i(t) = \sum_{j=1}^n x_j(0)/n     \text{ for all initial values }x(0)$
    \item $if \beta > \beta_*, \text{then} \lim_{t \to \infty} \|x(t)\| = \infty \text{ for almost all initial values w.r.t. Lebesgue measure}.$
\end{itemize}
In the first case, all the node features will coverage to the mean of them and therefore $\lim_{t \to \infty}\epsilon(X(t))\to 0$, then oversmoothing happens.
In the second case, the node features will diverge to infinity and thus $\lim_{t \to \infty}\epsilon(X(t))\to 0 \text{ or } \infty$ which is also not what we want. 

Theorem~\ref{thm: small nega} demonstrated that both insufficient repulsion and excessive repulsion caused by the negative graph can hinder performance in signed graph propagation.
From this, we conclude that relying solely on the negative signs is insufficient to alleviate oversmoothing.
Therefore, we propose the \jq{provable} solution: a structurally balanced graph to efficiently alleviate oversmoothing in Theorem~\ref{thm: repel_struct}.
Specifically, we have the following conclusion from the structurally balanced graph in Theorem~\ref{thm: repel_struct}:
\begin{equation}
    \mathbb{P}\left(\lim_{t \to \infty} x_i(t) = l(x(0)), i \in V_1; \lim_{t \to \infty} x_i(t) = -l(x(0)), i \in V_2 \right) = 1.
\end{equation}
Then we have:
\begin{align}
    \lim_{t \to \infty}\epsilon(X(t))&=\lim_{t \to \infty}\frac{1}{v}\Sigma_{i\in V}\Sigma_{j \in N_i}||x_i(t)-x_j(t)||^2_2 \\
    & =\lim_{t \to \infty}\frac{1}{v}\Sigma_{i \in V_1}\Sigma_{j \in N_i}||x_i(t)-x_j(t)||_2^2+ \frac{1}{v}\Sigma_{i \in V_2}\Sigma_{j \in N_i}||x_i(t)-x_j(t)||_2^2 \\
    & =\lim_{t \to \infty}\frac{1}{v}\Sigma_{i\in V_1}\Sigma_{j \in N_i, y_i \neq y_j}||x_i(t)-x_j(t)||_2^2+ \frac{1}{v}\Sigma_{i\in V_2}\Sigma_{j \in N_i, y_i \neq y_j}||x_i(t)-x_j(t)||_2^2 \\
    & =\lim_{t \to \infty}\frac{1}{v}\Sigma_{i\in V_1}\frac{v}{2}\times2c+ \frac{1}{v}\Sigma_{i\in V_2}\frac{v}{2}\times2c \\
    & =\lim_{t \to \infty}\frac{1}{v}(\frac{v}{2}\times \frac{v}{2}\times2c+ \frac{v}{2}\times\frac{v}{2}\times2c) \\
    & =vc\geq 0 
\end{align}
So Theorem~\ref{thm: repel_struct} proves that under certain conditions, structural balance can alleviate oversmoothing even when the layers are infinity.




\section{Extension of Structural Balance}
\label{app:weak-balance}
\begin{figure}
    \centering
    \includegraphics[width=1.1\textwidth]{figures/sb.pdf}
    \caption{Examples of structural balanced (left), weakly structural balanced (middle), and unbalanced signed graphs (right). Here red lines represent positive edges; black dashed lines represent negative edges; gray and blue circles represent nodes from different labels}
    \label{fig: sb}
\end{figure}
To clarify the concept of structural balance, weakly structural balance and unbalance signed graph, we give the examples as shown in Figure~\ref{fig: sb}.
The notion of structural balance can be weakened in the following definition \ref{def: weak struct}.
\begin{definition}
    A signed graph \( G \) is \textbf{weakly structurally balanced} if there is a partition of \( V \) into \( V = V_1 \cup V_2 \cup \ldots \cup V_m \), \( m \geq 2 \) with \( V_1, \ldots, V_m \) being nonempty and mutually disjoint, where any edge between different \( V_i \)'s is negative, and any edge within each \( V_i \) is positive.
    \label{def: weak struct}
\end{definition}

Then we show that when $\mathcal{G}$ is a complete graph, weak structural balance also leads to clustering of node states.
\begin{theorem}[\cite{signed_dynamics_paper_review}, Theorem 10]
\label{thm: weak_repel_struct} 
Assume that node $i$ interacts with node $j$ and $x_i(t)$ represents the value of node $i$ at time t. 
Let $\theta=\alpha$ if the edge $\{i,j\}$ is positive and $\theta=\beta$ if the edge $\{i,j\}$ is negative.
Consider the constrained signed propagation update:
\begin{equation}
\label{eq: weak constrained repel dyn}
    x_i(t + 1) = \mathcal{F}_c((1-\theta) x_i(t)+\theta x_J(t)).
\end{equation}
Let \(\alpha \in (0,1/2)\). 
Assume that \(\mathcal{G}\) is a weakly structurally balanced complete graph under the partition \(V = V_1 \cup V_2 \dots \cup V_m\). 
When \(\beta\) is sufficiently large, for almost all initial values \(x(0)\) w.r.t. Lebesgue measure, there exists m random variable \(l_1(x(0))\), \(l_2(x(0))\), \dots, \(l_m(x(0))\), each of which taking values in \(\{-c,c\}\) such that
\begin{equation}
    \mathbb{P}\left(\lim_{t \to \infty} x_i(t) = l_j(x(0)), i \in V_j, j=1,\dots, m \right) = 1.
\end{equation}
\end{theorem}





\section{Discussion about $\mathcal{SID}$}
\label{app: SID-csbm}

We give the details of CSBM and a more clear formula of $\mathcal{SID}$, $\mathcal{P}$ and $\mathcal{N}$ as suggested in Tabel~\ref{tab: sid} in this section.

\subsection{Definition of CSBM}
\label{app: csbm}
To quantify the structural balance of the mentioned methods, we simplified the graph to $2$-CSBM$(N, p, q, \mu_1, \mu_2, \sigma^2 )$ following~\cite{sbm_xinyi}. 
It consists of two classes $\mathcal{C}_1$ and $\mathcal{C}_2$ of nodes of equal size, in total with $N$ nodes. 
For any two nodes in the graph, if they are from the same class, they are connected by an edge independently with probability $p$, or if they are from different classes, the probability is $q$. For each node $v \in \mathcal{C}_i, i\in\{1,-1\}$, the initial feature $X_v$ is sampled independently from a Gaussian distribution $\mathcal{N}(\mu_i, {\sigma^2})$, where $\mu_i =\mathcal{C}_i, \sigma = I $. 
In this paper, we assign $N=100$ and the feature dimension is $8$.

\subsection{Measure of $\mathcal{SID}$}
\begin{equation}
    \mathcal{P}
=
\frac{1}{|V|} \sum_{v \in V} \hbox{ Number of nodes who have the same label as}~v~\hbox{and the non-positive edge}.
\end{equation}
\begin{equation}
    \mathcal{N}=\frac{1}{|V|} \sum_{v \in V}\hbox{ Number of nodes who have the different label from}~v~\hbox{and the non-negative edge}.
\end{equation}
\begin{equation}
    \mathcal{SB} = \frac{1}{2}(\mathcal{P} + \mathcal{N})
\end{equation}
\begin{figure}
    \centering
    \includegraphics[width=0.65\textwidth]{figures/sb_cg.pdf}
    \caption{Example of structural complete graph. Here red lines represent positive edges; black dashed lines represent negative edges; gray and blue circles represent nodes from different labels}
    \label{fig:sb_cg}
\end{figure}

\subsection{Proof of Proposition~\ref{pro: sid}}
\label{app: prof of prop sid}
\begin{proposition}
\label{app_pro: sid}
For a structural balanced complete graph $\mathcal{G}$, we have $\mathcal{SID}(\mathcal{G})=0$.
\end{proposition}
\paragraph{Proof}
To better understand, we give an example of the structural balance graph as shown in Figure~\ref{fig:sb_cg}.
we can see that for a node $v$, $\mathcal{P}(v)=0$ and  $\mathcal{N}(v)=0$ due to the structural balance complete graph assumption. So $\mathcal{SID}(\mathcal{G})=0$.

\subsection{\jq{More observations of $\mathcal{SID}$}}
\label{app: obs of sid}
Apart from Table~\ref{tab: sid} on CSBM, we further present the Structural Imbalance Degree ($\mathcal{SID}$) for Cora across different methods in Table~\ref{tab: sid of cora}. As the performance of these methods is similar in shallow layers (2), we focus on layer 16 to showcase the results.

\begin{table}[htbp]
\centering
\caption{$\mathcal{SID}$ on Cora datasets. We implement all of the methods on SGC under 100 epochs and the accuracy is the result.}
\label{tab: sid of cora}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccc}
\hline
 & label-\ours & feature-\ours & BatchNorm & ContraNorm & Residual & DropEdge \\
\hline
$\mathcal{P}$ & 482.1123 & 482.1123 & 482.1123 & 482.1123 & 482.5137 & 484.2075 \\
$\mathcal{N}$ & 0.7408 & 0.7408 & 0.7408 & 0.7408 & 2221.7305 & 2221.7305 \\
$\mathcal{SID}$ & 241.4265 & 241.4265 & 241.4265 & 241.4265 & 1352.1221 & 1352.9620 \\
Accuracy & 77.43 $\pm$ 1.49 & 77.22 $\pm$ 0.55  & 70.79 $\pm$ 0.00 & 63.35 $\pm$ 0.00 & 40.91 $\pm$ 0.00 & 22.24 $\pm$ 3.04 \\
\hline
\end{tabular}
}
\end{table}

We have two key observations: 1) Methods with higher $\mathcal{SID}$ generally lead to worse accuracy, while those with lower $\mathcal{SID}$ tend to produce better accuracy. 2) $\mathcal{SID}$ is a coarse-grained metric; different methods can yield the same $\mathcal{SID}$ values while their performance varies. These observations can also align with the experiments in cSBM in Table~\ref{tab: sid}.
The observation may stem from the fact that structural balance is an inherent property of graph structure, which is challenging to measure precisely using a numerical metric like $\mathcal{SID}$. Proposition 4.6 in the paper proves that when $\mathcal{SID}=0$, the graph is structurally balanced. 
However, for graphs that are not structurally balanced, the properties remain unclear. 
For future work, we aim to develop a more nuanced metric to quantify the structural balance property of graphs.





\section{Proof of Proposition \ref{pro: sid} and \ref{pro: ours-label}}
\label{app: proof of label-sbp}

\begin{proposition}
% Consider the update via signed graph:
% \begin{equation}
%     \hat{X} =\pgh{A}X- \ngh{A_l} X.
% \end{equation}
Assume that node label classes are balanced $|Y_1| = |Y_2|$
% \xinyic{unclear what "two balanced classifications" means} 
and denote the ratio of labelled nodes as $p$.
Then we have that the signed graph adjacency matrix $A_s= A-A_l$ and $\mathcal{SID}(\mathcal{G})\leq (1-p)\frac{n}{2}$, where $\mathcal{SID}$ decreases with a larger labelling ratio $p$. In particular, when $p=1$ (full supervision), we have $\mathcal{SID}(\mathcal{G})=0$, i.e., a perfectly balanced graph.
Under the constrained signed propagation \eqref{eq: constrained repel dyn}, the nodes from different classes will converge to distinct constants.
\end{proposition}

\paragraph{Proof.}
Without loss of generality, assume that the node feature has been normalized which means that $||x_i||_2=1$ for every $i$.
If $x_i$ and $x_j$ has the same label, then we have that, $(A_s)_{i,j}=(A)_{i,j}+1>1$.
If $x_i$ and $x_j$ has different labels, then we have that $(A_s)_{i,j}=(A)_{i,j}-1\leq0$.

We first prove that $\mathcal{SID}(\mathcal{G},p)\leq(1-p)\frac{n}{2}$ where $n$ is the nodes number and $p$ is the label ratio.
We have that 
\begin{equation}
    \mathcal{P}(v)+\mathcal{N}(v)\leq(1-p)n\, ,
\end{equation}
because for a single node $v$ only the remaining $(1-p)n$ nodes' labels are unknown and therefore their edges may need to change so that
\begin{equation}
\begin{aligned}
    \mathcal{SID}(\mathcal{G}) &= \frac{1}{2n}\sum_{v\in\mathcal{G}}(|\mathcal{P}(v)| + |\mathcal{N}(v)|)\\
    &\leq \frac{1}{2n}\sum_{v\in\mathcal{G}}(1-p)n\\
    &= (1-p)\frac{n}{2}.
\end{aligned}
\end{equation}

We know that when $\mathcal{SID}(\mathcal{G})=0$, then we have that the nodes $V$ set can be divided into $V_1\cup V_1 \dots \cup V_L$ where $L$ is the number of the node classes.
% The node with the class $i$ belongs to the $V_i$ set.
There are only positive edges with the node subset and only negative edges between the node subset.

Since $C=2$, the node set can be divided into $V_1$ and $V_2$, the signed graph is structurally balanced.
According to Theorem~\ref{thm: repel_struct}, we have that the nodes in $V_1$ will converge to the $c$ where $||c||_2=1$ and the nodes in $V_2$ will converge to $-c$.
Thus under Label-\ours propagation, the oversmoothing will only happen within the same label and repel different labels to the boundary.






