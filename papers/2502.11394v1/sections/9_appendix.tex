
\section{More Discussion on \oursfull}
\label{app: sbp}
The overall update of \oursfull is as following:
\begin{equation}
    X^{(k)} = \text{Layernorm}\{(1-\lambda)X^{(k-1)} + \lambda(\alpha \pgh{A^+} X^{(k-1)} - \beta \ngh{A^-} X^{(k-1)})\},
\end{equation}
Our methods adopt the normalized adjacency matrix as the positive graph
$\pgh{A^+=\hat{A}}$, while use different negative graphs.
Although both the positive and negative graphs have hyperparameters, we do not carefully adjust the hyperparameters. 
Instead, we fix $\alpha=1$ and only select the best value for $\beta$. 
You can also change $\alpha$ and $\beta$ together to achieve the best performance.
\paragraph{Label-Induced Negative Graph}
The negative graph for Label-\ours is: 
\begin{equation}
    \ngh{A^-}_{ij}=
    \begin{cases}
      1 & \text{if i,j has the different labels,} \\
      -1 & \text{if i,j has the same labels,} \\
      0 & \text{if i,j has the unknown labels.}
    \end{cases}
\end{equation}
For practice, we apply softmax to it: 
\begin{equation}
    \ngh{\tilde{A}^-}= \text{softmax}(\ngh{A^-}).
\end{equation}
Applying softmax makes the negative graph the row-stochastic which is a non-negative matrix with row sum equal to one.
We also tried the normalization method, which is not as good as the softmax. This may be because the softmax method is more aligned with the row-stochastic adjacency, where every element is non-negative. 

\paragraph{Similarity-Induced Negative Graph}
The negative graph for Feature-\ours is: 
\begin{equation}
    \ngh{A^-} = - X^{(0)} X^{(0)T}
\end{equation}
We also attempted using the last layer node features for the negative graph, but they are not as effective as the initial layer node features. 
This may be due to oversmoothing as the layers go deeper.
For practice, we apply softmax as the Label-\ours to it: 
\begin{equation}
    \ngh{\tilde{A}^-} = \text{softmax}(- X X^T)
\end{equation}


\section{\jq{Time Complexity Analysis and the Modified \ours}}
\label{app: time complexity of sbp}
\paragraph{Label-\ours}
As shown in~\eqref{eq: sbp}, we maintain the positive adjacency matrix $A^+=\hat{A}$ and construct the negative adjacency matrix $A_{l}$ by assigning 1 when nodes $i,j$ have different labels, -1 when they share the same label, and 0 when either label is unknown.
We then apply softmax to $A_{l}$ to normalize the negative adjacency matrix. The overall signed adjacency matrix is $A_{sign}= \alpha A^+ - \beta softmax(A_{l})$, where $\alpha$ and $\beta$ are hyperparameters.
Given $n_t$ training nodes and $d$ edges in the graph, our Label-SBP increases the edge count from $O(d)$ to $O(n_t^2)$, thereby increasing the computational complexity to $O(n_t^2d)$.

\paragraph{Feature-\ours}
When labels are unavailable, we propose Feature-SBP, which uses the similarity matrix of node features to create the negative adjacency matrix.
As depicted in~\eqref{eq: sbp}, we design the negative adjacency matrix as $A_{f}=-X_{0}X_{0}^T$. We then apply softmax to $A_{f}$ to normalize it. The overall matrix follows the same format as Label-SBP: $A_{sign}= \alpha A^+ - \beta softmax(A_{f})$, where $\alpha$ and $\beta$ are hyperparameters.
The additional computational complexity primarily stems from the negative graph propagation, which involves $X_{0}X_{0}^T \in \mathbb{R}^{n\times n}$, increasing the overall complexity to $O(n^2d)$.

We show the computation time of different methods in the Table~\ref{tab: time sbp}. On average, we improve performance on 8 out of 9 datasets (as shown in Table~\ref{table: sgc results}) with less than 0.05s overhead—even faster than three other baselines. 
We believe this time overhead is acceptable given the benefits it provides.

\begin{table}[htbp]
\centering
\caption{Estimated training time of \ours on Cora dataset. All experiments are run under 2 layers. s is the abbreviation for second. Precompute time is the aggregation time across layers, train time is the update time of the SGC weight $W$, total time is the sum of them.}
\label{tab: time sbp}
\resizebox{\linewidth}{!}{
\begin{tabular}{ccccccccc}
\hline
& Label-\ours & Feature-\ours & BatchNorm & ContraNorm & Residual & JKNET & DAGNN & SGC \\ \hline
Precompute time & 0.1809s & 0.1520s & 0.1860s & 0.1888s & 0.0604s & 0.0577s & 0.1438s & 0.1307s \\ 
Train time & 0.1071s & 0.1060s & 0.1076s & 0.1038s & 0.1368s & 0.1446s & 0.1348s & 0.1034s \\ 
Total time & 0.2879s & 0.2580s & 0.2935s & 0.2926s & 0.1972s & 0.2023s & 0.2786s & 0.2341s \\ 
Rank & 6 & 4 & 8 & 7 & 1 & 2 & 5 & 3 \\ \hline
\end{tabular}
}
\end{table}


\paragraph{Scalability of \ours on large-scale graph}
For large-scale graphs, we introduce a modified version Label-\ours-v2 by only removing edges when pairs of nodes belong to different classes.
This approach allows Label-\ours-v2 to eliminate the computational overhead of the negative graph, further enhancing the sparsity of large-scale graphs.
For Feature-\ours, as the number of nodes $n$ increases, the complexity of this matrix operation grows quadratically, i.e., $\mathcal{O}(n^2d)$.
To address this, we reorder the matrix multiplication from $-X_{0}X_{0}^T \in \mathbb{R}^{n\times n}$ to $-X_{0}^TX_{0} \in \mathbb{R}^{d\times d}$. This preserves the distinctiveness of node representations across the feature dimension, rather than across the node dimension as in the original node-level repulsion.
The modified version of Feature-\ours can be expressed as:
\begin{equation}
 (\text{Feature-\ours-v2})\,\,\,\,\,   X^{k}=(1-\lambda)X^{(k-1)}+\lambda(\alpha \hat{A}X^{(K)} - \beta X^{(K)} \text{softmax}(-X_{0}^TX_{0}))
\end{equation}
This transposed alternative has a linear complexity in the number of samples, i.e., $\mathcal{O}(nd^2)$, significantly reducing the computational burden in cases where $n \gg d$.

We compare the compute time \ours with other baselines on ogbn-arxiv dataset over 100 epochs for a fair comparison. 
Among all the training times of the baselines, our Label-\ours-v2 achieves the 3rd fastest time while Feature-\ours-v2 ranks 5th. Therefore, we recommend using Label-\ours-v2 for large-scale graphs since they typically have a sufficient number of node labels. We believe that although there is a slight time increase, it is acceptable given the benefits.
\begin{table}[htbp]
\centering
\caption{Estimated training time of \ours on ogbn-arixv dataset. All experiments are run under 2 layers and 100 epochs. s is the abbreviation for second.}
\label{tab:my-table}
\resizebox{0.9\linewidth}{!}{%
\begin{tabular}{ccccccc}
\hline
 & Label-\ours & Feature-\ours & BatchNorm & ContraNorm & DropEdge & SGC \\ \hline
Train time (s) & 5.5850 & 6.1333 & 5.3872 & 5.8375 & 9.5727 & 5.3097 \\ 
Rank & 3 & 5 & 2 & 4 & 6 & 1 \\ \hline
\end{tabular}%
}
\end{table}

\section{Details of Experiments}
\label{app: exp}
% \begin{wrapfigure}{r}{.75\textwidth}

% \end{wrapfigure}

% \input{Tables/sbm_result}
% \vspace{-0.1in}
The code for the experiments will be available when our paper is acceptable.
% \begin{quote}
% \centering
%     https://xxxxxxxx
% \end{quote}
We will replace this anonymous link with a non-anonymous GitHub link after the acceptance. 
We implement all experiments in Python 3.9 with PyTorch Geometric on one NVIDIA Tesla V100 GPU.

\subsection{Details of the Dataset}
\label{app: data}

\begin{table}[h]
% \vspace{-0.1cm}
\caption{Summary of datasets. $H(G)$ refers to the edge homophily level: the higher, the more homophilic the dataset is.}
\centering
\resizebox{0.6\linewidth}{!}{
\begin{tabular}{lcccc}
\toprule
Dataset    & $H(G)$        & Classes             & Nodes &Edges \\
% & Train/Dev/Test Nodes\\
\midrule
\textbf{Cora} & 0.81 & 7 & 2,708 &  5,429 \\ %&140/500/1,000\\
\textbf{Citeseer} & 0.74 & 6 &3,327  &4,732 \\ % & 120/500/1,000 \\
\textbf{PubMed} & 0.80 & 3 & 19,717 & 44,338 \\ %& 60/500/1,000\\

\midrule
\textbf{Texas} & 0.21 & 5 & 183 & 295 \\
\textbf{Cornell} & 0.30 & 5 & 183 & 280\\
\textbf{Amazon-ratings} & 0.38 & 5 & 24,492 & 93,050 \\
\textbf{Wisconsin} & 0.11 &5 & 251 & 466\\
% \textbf{Chameleon}& 0.23 & 6 & 2,277 & 31,421 & 1,092/729/456\\
\textbf{Squirrel} & 0.22 & 4 & 198,493 & 2,089 \\ %& 2,596/1,664/1,041\\
\midrule
\textbf{Ogbn-Arxiv} & 0.65 & 40 & 16,9343 & 1,166,243 \\ %  & 90,941/29,799/48,603\\
\bottomrule
\end{tabular}
}
% \end{adjustbox}
\label{tab: main_data}
% }
% \vspace{-1cm}
\end{table}
We consider two types of datasets: Homophilic and Heterophilic. 
They are differentiated by the \emph{homophily level} of a graph.
$$
\mathcal{H}
=
\frac{1}{|V|} \sum_{v \in V} \frac{\hbox{ Number of } v\hbox{'s neighbors who have the same label as } v}{\hbox{ Number of } v\hbox{'s neighbors }}.
$$
The low homophily level means that the dataset is more heterophilic when most of the neighbors are not in the same class, and the high homophily level indicates that the dataset is close to homophilic when similar nodes tend to be connected. 
In the experiments, we use four homophilic datasets, including Cora, CiteSeer, PubMed, and Ogbn-Arxiv, and four heterophilic datasets, including Texas, Wisconsin, Cornell, Squirrel, and
Amazon-rating~\citep{platonov2023critical}).
% Moreover, we list the numbers of classes, nodes, edges, the splits of each dataset, and their homophily level in Table~\ref{tab: data}. 
The datasets we used covers various homophily levels.

% \input{Tables/dataset}

% In detail, Cora and CiteSeer are two popular citation graph datasets. In these graphs, nodes represent papers and edges correspond to the citation relationship between two papers. Nodes are classified according to academic topics.
% Chameleon and Squirrel are Wikipedia page networks on specific topics, where nodes represent web pages and edges are the mutual links between them. Node features are the bag-of-words representation of informative nouns. 

\subsection{Experiments Setup}
\label{app: setup}
For the SGC backbone, we follow the~\cite{dgc} setting where we run $10$ runs for the fixed seed $42$ and calculate the mean and the standard deviation. 
Furthermore, we fix the learning rate and weight decay in the same dataset and run $100$ epochs for every dataset. 
For the GCN backbone, we follow the~\cite{contranorm} settings where we run $5$ runs from the seed $\{0,1,2,3,4\}$ and calculate the mean and the standard deviation. We fix the hidden dimension to $32$ and dropout rate to $0.6$.
Furthermore, we fix the learning rate to be $0.005$ and weight decay to be $5e-4$ and run $200$ epochs for every dataset. 
We use the default splits in torch\_geometric.
We use Tesla-V100-SXM2-32GB in all experiments.
% The residual hyperparameter $\alpha$ selects from $\{0.3, 0.5, 0.7\}$ and the DropEdge hyperparameter $\alpha$ selects from $\{0.3,0.5,0.7\}$.
% We select the best settings for PairNorm and ContraNorm based on their respective default hyperparameters. 
% We choose the best of scale controller of Label-\ours and Feature-\ours $\alpha,\ \beta$ from  $\{ 0.1, 0.2, 0.5, 0.7, 0.9\}$.
% We apply both the SGC and GCN backbones.



\subsection{Results Analysis}


\subsubsection{CSBM results}
The comparative results of Label-\ours and Feature-\ours against SGC are presented in Table \ref{table: app_sbm_results}. 
As the number of layers increases, SGC's node features suffer from oversmoothing, causing the two classes to converge and accuracy to drop by nearly $30$ points from its peak at $2$ layers, down to $45\%$. 
Conversely, after $300$ layers, \ours maintains strong performance, with node features of different classes repelling each other. 
This effect limits oversmoothing to within-class interactions, and improves performance from $85$ to $91$ in Label-\ours and from $48$ to $82$ in Feature-\ours, further substantiating our approach to mitigating oversmoothing.

We visualize the node features learned by Label-\ours in Figure~\ref{fig: SBM_Label}.
We can see that from layer $0$ to layer $200$, the node features from different labels repel each other and aggregate the node features from the same labels.
And we also visualize the adjacency matrix of Label-\ours and Feature-\ours in Figure~\ref{fig:adj label} and Figure~\ref{fig:adj feature} respectively, further verifying the effectiveness of our theorem and insights.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/sbm_SGC_299.pdf} % Adjust the path and filename as necessary
        \caption{SGC, acc$=47.50$}
        \label{fig:overall matrix}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/sbm_Sign_299.pdf}
        \caption{Feature-\ours, acc$=80.00$}
        \label{fig:sbm_sgc}
    \end{subfigure}
    % \hfill
    \quad
    \begin{subfigure}[b]{0.28\textwidth}
        \centering
        \includegraphics[width=0.95\textwidth]{figures/sbm_Label_299.pdf} % Adjust the path and filename as necessary
        \caption{Label-\ours, acc$=97.50$}
        \label{fig:sbm_ours}
    \end{subfigure}
    
    \caption{The t-SNE visualization of the node features and the classification accuracy from $2$-CSBM and Layer$=300$. 
    Left is the result of the vallina SGC, and the middle and right are the results of \ours. }
    \label{fig: sbm overall}
\end{figure}
\begin{figure}[h]
\centering
\captionsetup[subfigure]{labelformat=empty} % Optional: removes labels (a), (b), etc.
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/sbm/node_sbm_0.png}
    \caption{L=0} % Optional caption
\end{subfigure}\hfill % Ensures no extra spaces between the images
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/sbm/node_Label_sbm_layernorm_1.png}
    \caption{L=1} % Optional caption
\end{subfigure}\hfill % Ensures no extra spaces between the images
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/sbm/node_Label_sbm_layernorm_10.png}
    \caption{L=10} % Optional caption
\end{subfigure}\hfill
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/sbm/node_Label_sbm_layernorm_50.png}
    \caption{L=50} % Optional caption
\end{subfigure}\hfill
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/sbm/node_Label_sbm_layernorm_100.png}
    \caption{L=100} % Optional caption
\end{subfigure}\hfill
\begin{subfigure}{0.3\textwidth}
    \includegraphics[width=\linewidth]{figures/sbm/node_Label_sbm_layernorm_199.png}
    \caption{L=200} % Optional caption
\end{subfigure}
\caption{CSBM node features visualization. We update the features by Label-\ours. L is the propagation layer number. 0,1 represent different classes.}
\label{fig: SBM_Label} % Corrected label placement
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]
    {figures/adj_label.pdf}
    \caption{The visualization of the adjacency matrix of Label-\ours. Here left is the positive graph; middle is the negative graph; right is the overall signed graph.}
    \label{fig:adj label}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]
    {figures/adj_feature.pdf}
    \caption{The visualization of the adjacency matrix of Feature-\ours. Here left is the positive graph; middle is the negative graph; right is the overall signed graph.}
    \label{fig:adj feature}
\end{figure}

\input{Tables/sbm_result}




% \begin{figure}[ht]
% \centering
% \captionsetup[subfigure]{labelformat=empty} % Optional: removes labels (a), (b), etc.
% \begin{subfigure}{0.3\textwidth}
%     \includegraphics[width=\linewidth]{figures/sbm/node_sbm_0.png}
%     \caption{L=0} % Optional caption
% \end{subfigure}\hfill % Ensures no extra 
% \begin{subfigure}{0.3\textwidth}
%     \includegraphics[width=\linewidth]{figures/sbm/node_SGC_sbm_1.png}
%     \caption{L=1} % Optional caption
% \end{subfigure}\hfill % Ensures no extra spaces between the images
% \begin{subfigure}{0.3\textwidth}
%     \includegraphics[width=\linewidth]{figures/sbm/node_SGC_sbm_10.png}
%     \caption{L=10} % Optional caption
% \end{subfigure}\hfill
% \begin{subfigure}{0.3\textwidth}
%     \includegraphics[width=\linewidth]{figures/sbm/node_SGC_sbm_50.png}
%     \caption{L=50} % Optional caption
% \end{subfigure}\hfill
% \begin{subfigure}{0.3\textwidth}
%     \includegraphics[width=\linewidth]{figures/sbm/node_SGC_sbm_100.png}
%     \caption{L=100} % Optional caption
% \end{subfigure}\hfill
% \begin{subfigure}{0.3\textwidth}
%     \includegraphics[width=\linewidth]{figures/sbm/node_SGC_sbm_199.png}
%     \caption{L=200} % Optional caption
% \end{subfigure}
% \caption{2-SBM node features visualization. We update the features by SGC \cite{sgc}. L is the propagation layer number. 0,1 represent different classes.}
% \label{fig:SBM_SGC} % Corrected label placement
% \end{figure}





\subsubsection{GCN Results}
\input{Tables/GCN_result}
The results for GCN are detailed in Table \ref{table: gcn result}, respectively. 
Overall, \ours consistently outperforms all previous methods, especially in deeper layers. 
Beyond $16$ layers in GCN, \ours maintains superior performance, affirming the effectiveness of our approach. 
Notably, \ours exceeds the best results of prior methods by at least $10\%$ and up to $30\%$ points in GCN's deepest layers, marking significant improvements.
Moreover, unlike previous methods that perform best in shallow layers, \ours excels in moderately deep layers, as observed in GCN across all datasets. 
This further confirms the effectiveness of \ours.


% \subsection{Ablation Experiments}
% \label{app: ablation}
% % \input{Tables/GCN_heter}
% % \input{Tables/ablation}
% The test accuracy comparison for the heterophilic datasets is shown in Table~\ref{table: gcn heter}.
% We strengthen the weight $\beta$ of the negative subgraph selected from $\{1,2,5,10,20,50\}$.
% We can see that Label-\ours and Feature-\ours are still effective across all methods and all layers, verifying our signed graph propagation insight. 
% Note that our feature-based approach (Feature-\ours) is more effective than our label-based approach (Label-\ours). 
% The reason for this difference may be attributed to the relatively small proportion of the training set indicating that Feature-\ours can be a good supplement for Label-\ours when the train label rate is small.




% \subsection{Details of Experiments}

\subsubsection{\jq{Repulsion ablation on heterophilic datasets}}
Our method SBP can outperform other baselines under $\beta=1$ across different layers, so we do not tune our hyper-parameters carefully.
However, since $\beta$ is the weight of the negative adjacency matrix (\eqref{eq: sbp}) representing the repulsion between different nodes, as seen in Figure~\ref{fig:beta csbm} and~\ref{fig:beta real}, the best performance of \ours appears when $\beta$ is larger in the heterophilic graphs, so the result in Figure~\ref{fig: layer depth}(a) is not the best performance of our SBP.
To further show the effectiveness of our SBP, we conduct experiments on Cornell with different $\beta$ in Table~\ref{tab: beta on cornell}, the best $\beta$ is 20 where the performance increases 25 points in deep layer 50.

\begin{table}[htbp]
\centering
\caption{Ablation study of negative weight $\beta$ on Cornell dataset.}
\label{tab: beta on cornell}
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccccc}
\hline
 Layer & 2 & 5 & 10 & 20 & 50 \\
\hline
$\beta=0.1$ & 72.97 $\pm$ 0.00 & 67.57 $\pm$ 0.00 & 51.53 $\pm$ 0.00 & 35.14 $\pm$ 0.00 & 29.73 $\pm$ 0.00 \\
$\beta=1$ (default) & 72.97 $\pm$ 0.00 & 67.57 $\pm$ 0.00 & 51.53 $\pm$ 0.00 & 45.95 $\pm$ 0.00 & 35.14 $\pm$ 0.00 \\
$\beta=10$ & 70.27 $\pm$ 0.00 & 67.57 $\pm$ 0.00 & 58.11 $\pm$ 1.35 & 51.53 $\pm$ 0.00 & 51.53 $\pm$ 0.00 \\
$\beta=20$ (best) & 70.27 $\pm$ 0.00 & 70.27 $\pm$ 0.00 & 67.57 $\pm$ 0.00 & 59.46 $\pm$ 0.00 & 59.46 $\pm$ 0.00 \\
$\beta=50$ & 64.60 $\pm$ 0.00 & 40.54 $\pm$ 0.00 & 40.54 $\pm$ 0.00 & 40.54 $\pm$ 0.00 & 40.54 $\pm$ 0.00 \\
\hline
\end{tabular}%
}
\end{table}



\subsubsection{\jq{Performance of \ours on more benchmarks}}
We further compare our \ours with SGC on six additional datasets~\citep{platonov2023critical} in Table~\ref{tab: app more bench}. Our \ours outperforms SGC on five out of these six datasets. We believe that these six datasets, combined with the nine datasets presented in Table~\ref{table: sgc results} of our paper, provide sufficient evidence to demonstrate the effectiveness of our approach.
\begin{table}[htbp]
\centering
\caption{Performance Comparison on more datasets}
\resizebox{\linewidth}{!}{
\label{tab: app more bench}
\begin{tabular}{ccccccc}
\hline
 & actor & penny94 & roman-empire & Tolokers & Questions & Minesweeper \\
\hline
SGC & 29.18 $\pm$ 0.10 & 72.56 $\pm$ 0.05 & 40.83 $\pm$ 0.03 & 78.18 $\pm$ 0.02 & 97.09 $\pm$ 0.00 & 80.43 $\pm$ 0.00 \\
Feature-SBP & 34.93 $\pm$ 0.02 & 75.68 $\pm$ 0.01 & 66.48 $\pm$ 0.02 & 78.24 $\pm$ 0.04 & 97.14 $\pm$ 0.02 & 80.00 $\pm$ 0.00 \\
Label-SBP & 34.94 $\pm$ 0.00 & 75.74 $\pm$ 0.01 & 66.32 $\pm$ 0.01 & 78.46 $\pm$ 0.08 & 97.15 $\pm$ 0.02 & 80.00 $\pm$ 0.00 \\
\hline
\end{tabular}
}
\end{table}

\subsubsection{\jq{Combine \ours to other methods}}
\label{app: gcnii}
In this paper, we focus on introducing a novel theoretic signed graph perspective for oversmoothing analysis, so we do not take many tricks into account or carefully fine-tune our hyperparameters. 
Thus, our results in the paper are not as comparable to previous baselines~\citep{GCNII,ACM-GCN,PDE-GCN}.
% , adapting many tricks, such as random dropout features, normalizing the features.
\jq{However, existing oversmoothing researches are indeed hard to compare, because they are often composed of multiple techniques — such as residual, BatchNorm, data augmentation — and the parameters are often heavily (over-)tuned on small-scale datasets. And it becomes clear that to attain SOTA performance, one needs to essentially compose multiple such techniques without fully understanding their individual roles. For example, GCNII uses both initial residual connection and identity map, futher combined with dropout.}

\jq{Our goal is to provide a new unified understanding of these techniques, so we justified it by showing that SBP as a single simple technique can attain good performance. 
And we believe that it would work complementarily with other techniques in the field, because oversmoothing is still challenging to solve with a very larger depth. }

To further verify the effectiveness, we combine our SBP to one of the SOTA settings GCNII~\citep{GCNII} and the results are as seen in Table~\ref{tab:gcnii-performance}. 
\jq{The results indicate that after combining our method, GCNII demonstrates greater robustness as the layers go deeper, particularly in the middle layers (layer=8), highlighting the efficacy of our signed graph insight.}
% achieves higher scores than GCNII in deep layers but and 



\begin{table}[htbp]
\centering
\caption{Performance Comparison between \ours and GCNII under the GCNII settings on Cora and Citesser datasets}
\label{tab:gcnii-performance}
\resizebox{\linewidth}{!}{
\begin{tabular}{cccccccc}
\hline
 & & 2 & 4 & 8 & 16 & 32 & 64 \\
\hline
\multirow{3}{*}{Cora} & GCNII & 78.58 $\pm$ 0.00 & 77.76 $\pm$ 0.24 & 73.47 $\pm$ 3.82 & 78.12 $\pm$ 1.32 & 82.54 $\pm$ 0.00 & 81.34 $\pm$ 0.53 \\
 & Label-\ours & 78.74 $\pm$ 1.54 & 78.87 $\pm$ 0.00 & \cellcolor{best}79.14 $\pm$ 0.35 & 79.17 $\pm$ 0.41 & 80.86 $\pm$ 0.32 & 81.38 $\pm$ 0.30 \\
 & Feature-\ours & 77.95 $\pm$ 0.91 & 78.82 $\pm$ 0.00 & 78.11 $\pm$ 1.62 & 78.82 $\pm$ 0.29 & 81.82 $\pm$ 0.47 & 81.65 $\pm$ 0.40 \\
\hline
\multirow{3}{*}{Citesser} & GCNII & 61.66 $\pm$ 0.67 & 63.23 $\pm$ 2.31 & 64.58 $\pm$ 2.66 & 66.21 $\pm$ 0.64 & 69.38 $\pm$ 0.83 & 69.73 $\pm$ 0.26 \\
 & Label-\ours & 65.31 $\pm$ 0.63 & 63.93 $\pm$ 3.66 & 68.33 $\pm$ 0.99 & 66.46 $\pm$ 0.00 & 70.00 $\pm$ 0.81 & 69.47 $\pm$ 0.25 \\
 & Feature-\ours & 65.63 $\pm$ 0.87 & 64.43 $\pm$ 3.55 & \cellcolor{best}68.44 $\pm$ 1.19 & 66.94 $\pm$ 0.00 & 69.98 $\pm$ 0.93 & 69.66 $\pm$ 0.28 \\
\hline
\end{tabular}
}
\end{table}
\subsubsection{\jq{Performance of \ours on Large-scale graphs}}
We conducted experiments with a larger graph ogbn-products than ogbn-arxiv under 100 epochs and 2 layers in Table~\ref{tab: ogbn-products}. 
The results indicate that our \ours outperforms the initial GCN baselines. Given the results presented for ogbn-arxiv in Table 5 of our paper, we believe these findings adequately demonstrate the performance of our \ours on large-scale graphs.
\begin{table}[h]
\centering
\caption{Performance of different models on ogbn-products dataset. Time means the runtime, the format is (hour: minutes: seconds).}
\label{tab: ogbn-products}
\resizebox{0.45\linewidth}{!}{
\begin{tabular}{lcc}
\hline
Method & Accuracy & Time \\
\hline
GCN & 73.96 & 00:06:33 \\
BatchNorm & 74.93 & 00:06:18 \\
Feature-SBP & 74.90 & 00:06:43 \\
Label-SBP & 76.62 & 00:06:39 \\
\hline
\end{tabular}
}
\end{table}

\subsubsection{\jq{Further Optimization based on \ours}}
Based on the experiment results, we want to propose 2 strategies for further optimization. 

1) hyper-parameter tuning on the negative weight $\beta$. As seen in Figures~\ref{fig:beta csbm} and~\ref{fig:beta real}, we found that $\beta$ influences the performance a lot, our default $\beta=1$ for Table~\ref{table: sgc results} and~\ref{tab: large} is certainly not optimal for the above 4 homophilic datasets. We suggest tuning higher $\beta$ for the heterophilic graphs since they need more repulsion and smaller for the homophilic datasets.  As the layer deepens, maybe greater weight should be placed on the negative adjacency graphs to alleviate oversmoothing. 

2) adapt our SBP to more effective GNNs. Our method is simple, architecture-free, without additional learnable parameters, and thus can be flexibly applied in various architectures. As seen in Appendix~\ref{app: gcnii}, we adapt our SBP to the GCNII models, and the results increase more than adaptation in vanilla GNN as shown in Table~\ref{table: sgc results} and~\ref{tab: large}. Besides, compared to the GCNII, our SBP is more robust and stable to the layers as seen in Table~\ref{tab:gcnii-performance}.

\input{Tables/SGC_result}