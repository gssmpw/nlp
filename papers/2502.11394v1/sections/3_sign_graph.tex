% sign framework
\section{A Signed Graph Perspective on Existing  Oversmoothing Countermeasures} 
\label{sec: signed pers}
% \jq{notaion, re-define.}


% In this section, we first introduce the notion of signed graphs and how message-passing over signed graphs works. Then we present the theoretical properties of the asymptotic behaviors of signed graph propagation. 
%
% Finally, we leverage these theoretical tools from signed graphs to provide a unified framework for existing anti-oversmoothing techniques. 
% \yf{we leverage signed graph to analyze oversmoothing ,not to address it}

% In this paper, we define the 

%
% The positive neighbor set is $N_i^+$ and the negative neighbor set is $N_i^-$.
% The positive neighbor set is $N_i^+$ and the positive degree $d_i=|N_i^+|$.
% The negative neighbor set is $N_i^-$ and the negative degree $d_i=|N_i^-|$.
% The raw normalized positive and negative adjacency matrix are $\pgh{\hat{A}^+}$ and $\ngh{\hat{A}^-}$, respectively.
%



% \subsection{Interpreting Regularization Techniques via Signed Graph}
% \label{sec: regularization analysis}




% \textbf{Contextual Stochastic Block Models.} 
% We focus on the $2$-CSBM$(N, p, q, \mu_1, \mu_2, \sigma^2 )$ to explain the methods following~\cite{sbm_xinyi}.

In this section, we revisit three popular types of anti-oversmoothing methods and reinterpret them through the lens of signed graph propagation in the form of (\ref{eq: sign_overall}). We find that all of these methods can be attributed to some kind of signed graph design \(\mathcal{G}_s\) by introducing positive and negative edges to the original graph.
% introduce a signed graph perspective to unify popular anti-oversmoothing techniques.
We summarize eight specific methods with their corresponding positive and negative graphs in Table~\ref{tab: framework}.
% \vspace{-1ex}
\subsection{Normalization Techniques}
Normalization operates the node features after each message-passing step by centering them with zero mean and unit variance (up to a scale) with different strategies. 
% of normalization methods 
A few representative methods include BatchNorm~\citep{batchnorm}, PairNorm~\citep{Zhao2020PairNorm}, 
% LayerNorm~\cite{layernorm} 
and ContraNorm~\citep{contranorm}, where PairNorm and ContraNorm were proposed specifically to address the oversmoothing issue in GNNs.
Further details on these methods are provided in Appendix~\ref{app: previous}.
% Specifically, BatchNorm centers the node representations $X$ to have zero mean and unit variance across nodes for each feature, which can be written as BatchNorm($x_i$) \(=\frac{1}{\sqrt{\sigma^2 + \epsilon}}(x_i - \frac{1}{n}\Sigma_{i=1}^n x_i)\) where $ \epsilon > 0$ 
% and $\sigma^2$ is the variance of the feature across all nodes.
% %
% Meanwhile, PairNorm is a normalization technique specifically developed for GNNs to combat oversmoothing, where its only difference from BatchNorm is that PairNorm scales all the entries in $X$ using the same number rather than scaling each column by its own variance. It can be written as PairNorm($x_i$) \(=\frac{s}{\sqrt{\Gamma
% ^2 + \epsilon}}(x_i - \frac{1}{n}\Sigma_{i=1}^n x_i)\) where $\Gamma = \|(\hat{A}- \mathbb{1}_n \mathbb{1}_n^T/n)X \|_F/\sqrt{n} $ and $s$ is a scalar.
% %
% Apart from these two methods, ContraNorm is inspired by the uniformity loss from contrastive learning, aiming to alleviate dimensional feature collapse.
% For simplicity, we consider the spectral version of ContraNorm that takes the following form: ContraNorm($X$) $= (1 + \alpha) X- \alpha /\tau(X X^{T}) X \,$ where $\alpha\in(0,1)$ and $\tau>0$ are hyperparameters.
% Proposition 2) 
% \xinyic{better with an exact reference: which Theorem in ContraNorm} 
% without additional regularization and LayerNorm that are used additionally in practice, 
% \yf{strength/weakness not discussed}
% Besides, it can assigns suitable weights to positive and negative edges by selecting different $\alpha$ and $\tau$.
% \xinyic{what does "selecting different hyperparameters" mean here}
Despite the differences in motivation and implementation, all the three normalization methods can be seen as a signed graph propagation with different designs of the negative graph: 
\begin{theorem}
     BatchNorm, PairNorm and ContraNorm can be interpreted as signed graph propagation defined in (\ref{eq: sign_overall}), sharing the same raw normalized positive adjacency matrix $\pgh{\hat{A}^+=\hat{A}}$ while having different raw normalized negative adjacency matrices transformed from $\hat{A}^+$, that is,  $\ngh{\hat{A}^-=\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}  \hat{A}}$ for BatchNorm and PairNorm, and $\ngh{\hat{A}^-=(X X^{T}) \hat{A} }$ for ContraNorm.
\end{theorem}
The result shows that PairNorm shares the same fixed positive and negative graphs (up to scale) as BatchNorm. In contrast, ContraNorm extends the negative graph to an adaptive one based on similarities in node features. 
% We can see that $\pgh{\hat{A}}$ is again the positive graph and $\ngh{(X X^T)\hat{A}}$ is the negative graph in the corresponding signed graph propagation.
% In particular, the repulsion of ContraNorm is affected by both feature similarities and node degrees. 
% Consider the update:
% \begin{equation}
%     \label{eq: bn sign}
%     \hat{X}= (\pgh{A}-\ngh{\frac{X X^T}{n} A}) X\,,
% \end{equation}
% After one signed graph propagation, the edge weight changes from $\{0,1\}$ to $\{-\frac{p-q}{2}, 1- \frac{p-q}{2}\}$, so the SB can be expressed as: 
% \begin{equation}
%     SB_{CN}= (1-\frac{p-q}{2})p + \frac{p-q}{2} (1-q) = p + \frac{p-q}{2} (1-p-q).
% \end{equation}







\subsection{Augmentation-Based Methods}
% \paragraph{DropEdge}
Node or edge dropping~\citep{dropedge} is another popular type of method to combat oversmoothing.
% and they can also be interpreted as having signed graph propagation. 
In particular, we denote $A_m\in \{0,-1\}^{n\times n}$ where $(A_m)_{i,j}=1$ if the edge $\{i,j\}$ is dropped and otherwise 0. 
Then the signed graph induced by (randomly) dropping edges can be formulated as \(\mathcal{G}_{drop}=\{A,A_m,X\}\). 
% \begin{equation}
    % \label{drop sign}
    % $\hat{X} =\pgh{A} X - \ngh{A_{m}}X$.
    % =(\pgh{A}  -\ngh{A_{m}})X.
% \end{equation}
The negative adjacency matrix $A_m$, while created through random generation, effectively helps alleviate oversmoothing in practice.
% demonstrates that adding negative edges can 

% Consider the update:
% \begin{equation}
%     \label{eq: bn sign}
%     \hat{X}= (\pgh{A}-\ngh{A_m}) X\,,
% \end{equation}
% Assume that in $\ngh{A_m}$, for any two nodes in the graph, if they are from the same class, they are connected by an edge independently with probability $s$, or if they are from different classes, the probability is $t$.
% % After one signed graph propagation, the edge distribution probabilities are $q-t$ from the same label and $p-s$ from the different labels, so 
% The SB can be expressed as: 
% \begin{equation}
%     SB_{ED}= 1 \times (p-s) - 0 \times (1-q) + 1 \times t = p-s+t.
% \end{equation}


  
% Furthermore, residual connection precisely corresponds to signed graph propagation, as described in~\eqref{eq: sign_overall}, particularly when $\alpha=\beta$.

% \begin{equation}
%     \label{eq: appnp}
%     \begin{split}
%         \hat{X}^{(k+1)} &= (1-\alpha)X^{(0)}  + \alpha \hat{A} X ^{(k)} \\
%         &= \pgh{\Sigma_{i=0}^{k+1}\alpha^i\hat{A}^i} X^{(0)} -\ngh{\alpha \Sigma_{j=0}^{k}\alpha^j\hat{A}^j }X^{(0)}\,.
%     \end{split}
% \end{equation}

\subsection{Residual Connections} % 
Besides normalization layers and edge-dropping, residual connections can also be seen through the lens of signed graph propagation. Based on different combinations of layers in this class, we provide analysis for the following three types of residual connections: First, the standard residual connection~\citep{dgc,Chen2020SimpleAD}, which directly combines the previous and the current layer features together.
% It can be formulated as: \( \hat{X} = (1-\alpha)X  + \alpha \hat{A} X  \).
% \begin{equation}
%     \label{eq: residual sign}
% \end{equation} = X + \alpha \pgh{\hat{A}} X -\alpha \ngh{I} X
% For residual connections, the positive adjacency matrix is $\pgh{\hat{A}}$ and the negative adjacency matrix $\ngh{I}$ in the corresponding signed graph propagation.
% \paragraph{APPNP}
% We reformulate the method APPNP~\citep{appap} as the signed propagation form of the initial node feature. 
Another type combines the current layer features together with the initial features, such as APPNP~\citep{appap} or GCNII~\cite{GCNII}.
% \begin{equation}
% , written as: \( \hat{X}^{(k+1)} = (1-\alpha)X^{(0)}  + \alpha \hat{A} X ^{(k)} \) .
% \end{equation}
% \begin{theorem}
% With $\hat{A}^+=\Sigma_{i=0}^{k+1}\alpha^i\hat{A}^i$ and $\hat{A}^-=\alpha \Sigma_{j=0}^{k}\alpha^j\hat{A}^j$, the propagation process of APPNP following the signed graph propagation.
% \end{theorem}
In addition to combining with the previous or the initial layer features, there is a third type of residual connections which integrates intermediate layer features, such as JKNET~\citep{jknet} and DAGNN~\citep{dagnn}.
% \paragraph{\jq{JKNET and DAGNN}}
% JKNET is a deep graph neural network which exploits information from neighborhoods of differing locality. 
% JKNET selectively combines aggregations from different layers through operations such as concatenation or max-pooling at the output, i.e., the representations "jump" to the last layer.
% Using attention mechanism for combination at the last layer, the $k+1$-layer propagation result of JKNET can be written as:
% \begin{equation}
%     \label{eq:jk-net}
%     \begin{split}
%          X^{(k+1)} &= \alpha_0 X^{(0)}  + \alpha_1  X ^{(1)} + \cdots \alpha_k X^{(k)}\\
%         &= \Sigma_{i=0}^k\alpha_i \hat{A}^i X^{(0)}\,,
%     \end{split}
% \end{equation}
% where $\alpha_0, \alpha_1, \cdots, \alpha_{k}$ are the learnable fusion weights with $\Sigma_{i=0}^k\alpha_i=1$.
% Deep Adaptive Graph Neural Networks (DAGNN) tries to adaptively add all the features from the previous layer to the current layer features with additional learnable coefficients. 
More details about these methods can be found in Appendix~\ref{app: residual}.
Formally, we establish the following result that these three types of residual connections can all be seen as signed graph propagation:
% After decoupling representation transformation and propagation, the propagation mechanism of DAGNN is similar to that of JKNET.
% \begin{equation}
%     \label{eq:dagnn}
%          X^{(k+1)} = \Sigma_{i=0}^k\alpha_i \hat{A}^i H^{(0)}, \,H^{(0)}=f_\theta(X^{(0)})
% \end{equation}
% $ H^{(0)}=f_\theta(X^{(0)})$ ) is the non-linear feature transformation using an MLP
% network, which is conducted before the propagation process and $\alpha_0, \alpha_1, \cdots, \alpha_{k}$ are the learnable fusion weights with $\Sigma_{i=0}^k\alpha_i=1$. \jq{double check the correctness.}
\begingroup
\setlength{\abovedisplayskip}{3pt} % 上方间距缩小
\setlength{\belowdisplayskip}{3pt}
\begin{theorem}
% \vspace{-3ex}
With \pgh{$\hat{A}^+=\hat{A}$} and \ngh{$\hat{A}^-=I$}, the standard residual connections follows the signed graph propagation~(\ref{eq: sign_overall}).
With \pgh{$\hat{A}^+=\Sigma_{i=0}^{k+1}\alpha^i\hat{A}^i$} and \ngh{$\hat{A}^-=\alpha \Sigma_{j=0}^{k}\alpha^j\hat{A}^j$}, APPNP follows the signed graph propagation~(\ref{eq: sign_overall}).
    With \pgh{$\hat{A}^+=\Sigma_{i=0}^{k-1}\alpha^i\hat{A}^i+\hat{A}^k$} and \ngh{$\hat{A}^-=\Sigma_{j=0}^{k-1}\alpha^j\hat{A}^k$}, JKNET and DAGNN follows the signed graph propagation~(\ref{eq: sign_overall}).
\end{theorem}
\endgroup
% \vspace{-5pt}

\textbf{Discussion.} In summary, we establish a unifying perspective in which normalization, edge dropping, and residual connections can all be interpreted as instances of signed graph propagation, even though this structure is not explicitly recognized. Notably, for these methods, while their positive adjacency matrices typically reflect the original graph structure, the negative adjacency matrices are often constructed heuristically. As a result, the interaction between signed graph structures and node feature dynamics remains insufficiently understood motivating a systematic theoretical analysis of the asymptotic behaviors of signed graph propagation.
% , thus inspiring us to theoretically and systematically analyze oversmoothing through the signed lens.
% Moreover, these methods create positive and negative adjacency matrices based on the different heuristics, remaining unclear about the complex interplay between the signed graph structure and the resulting node feature dynamics. 
% thus may initially provide some benefits in terms of preventing oversmoothing.
% However, while empirically constructing the signed graph propagation, 
% there is still a lack of theoretical guidance to fully understand the .
%