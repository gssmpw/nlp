\section{Background}
\textbf{Notations.} 
We represent an unsigned undirected graph with \(n\) nodes by $\mathcal{G} = (A, X)$,
where $A \in \{0,1\}^{n \times n}$ denotes the adjacency matrix and \(X \in \mathbb{R}^{n \times d} \) is the node feature matrix. 
For node \(i,j \in \{1,2,..,n\}\), $A_{i,j} = 1$ if and only if node \(i, j\) are connected by an edge in \(\mathcal{G}\) and \(X_i \in \mathbb{R}^d\) represents the features of node \(i\).
We let $\mathbb{1}_n$ be the all-one vector of length $n$ and \(D = \text{diag}(A \mathbb{1}_n) \) be the degree matrix of \(\mathcal{G}\).
% with node set $V$ and edge set $E$ where $n = |X|$ is the number of nodes. 
% The nodes feature matrix is denoted by $X=\{x_1, x_2, \dots,x_n\} \in \mathbb{R}^{n \times d}$, where $x_i$ is the feature for node $i$ and $d$ is the dimension of node features. 
% $Y\in \mathbb{R}^{n \times c}$ is the node label matrix.
% Let $A \in \{0,1\}^{n \times n}$ denote the adjacency matrix where $A_{i,j} = 1$ if $\{i, j\}\in E$, otherwise $0$.
% The set of neighbors of a node $i$ is denoted by $N_i$. 
% The diagonal degree matrix is denoted as $D = \text{diag}(d_1, \dots, d_n)$, where $d_i = \sum_j A_{i,j}$. 
% The signed graph extends unsigned edges to have positive or negative values. 
In this paper, we extend \(\mathcal{G}\) to the signed graph \(\mathcal{G}_s=\{A^+, A^-, X\}\) where \( A^+, A^- \in \{0,1\}^{n \times n} \) are the positive and negative adjacency matrices with the degree matrix \(D_+= \text{diag}(A^+ \mathbb{1}_n)\) and \(D_-= \text{diag}(A^- \mathbb{1}_n)\), respectively. 
% maintaining the degree matrices $D^+, D^-$ with the same expression defined in the unsigned graph.
% Then the raw normalized adjacency matrices for positive and negative are $\hat{A}^+ =  D_+^{-1}A^+ $ and $\hat{A}^- =  D_-^{-1}A^- $.
% To simplify, we define $A^+$ and $A^-$ as matrices in $\{0,1\}^{n \times n}$, maintaining the same degree matrices $D^+, D^-$ as defined in the unsigned graph. 
% Let  $A^+ \in \{0,1\}^{n \times n}$ denote the positive adjacency matrix and the positive diagonal degrees matrix is denoted as $D^+ = \text{diag}(d_1^+, \dots, d_n^+)$ where $d_i^+ = \sum_j A^+_{i,j}$, the raw normalized adjacency matrix is $\hat{A}^+ =  D^{+-1}A^+ $. 
% Let  $A^- \in \{0,-1\}^{n \times n}$ denote the positive adjacency matrix and the positive diagonal degrees matrix is denoted as $D^- = \text{diag}(d_1^-, \dots, d_n^-)$ where $d_i^- = \sum_j |A^-_{i,j}|$. 
% Let $\hat{A}^+$ denote the raw normalized version of the positive adjacency matrix $A^+ \in \{0,1\}^{n \times n}$ and $\hat{A}^-$ be that of the negative adjacency matrix $A^- \in \{0,-1\}^{n \times n}$. In this paper, we 
% We let $||\cdot||_F$ denote the Frobenius norm. 
% 
\input{Tables/framwork}

\textbf{Unsigned and signed graph propagation.} 
% For convenience, 
A unsigned graph propagation using the row normalized adjacency matrix $\hat{A} =  D^{-1}A$ takes the form \(X'=\sigma(\hat{A}X)\), with the nonlinear activation function \(\sigma(\cdot)\) and the learnable weight \(W\).
In this paper, our theoretical analysis focuses on the simplified linear GNN
model following~\citet{sgc,wu2023demystifying}
% , we simplify the  % $k$-th ($k \in [1, K]$) layer $K$-layer graph convolutional operation 
by letting $\sigma(x)=x$. For convenience, we let $W^*=W^{(0)}W^{(1)} \cdots W^{(K-1)}$ where \(W^{(k)}\) is a learnable weight matrix in the \(k\)-th layer.
% positive neighbors of node $i$ and $N_i^-$ denote the set of negative neighbors, where $N_i^+ \cup N_i^-= N_i$ and $N_i^+ \cap N_i^-= \emptyset$. Then the signed graph propagation updates the representation $x_i$ of node $i$ by: 
% \begin{equation}
% \label{eq: sign_node}
%     \hat{x}_i = (1-\alpha + \beta) x_i + \frac{\alpha}{|N_i^+|}\sum_{j\in N_i^+}x_j
%     -\frac{\beta}{|N_i^-|} \sum_{j\in N_i^-}x_j\,.
% \end{equation}
% In particular, the two parameters $\alpha, \beta>0$ mark the strength of attraction and repulsion over the positive and negative edges, respectively.
% We discuss the origin of~\eqref{eq: sign_node} in Appendix~\ref{app: signed graph}.
% As a result, the resulting $K$-layer signed graph propagation can be defined as follows:
% We define the signed graph raw normalized adjacency matrix 
Following~\citet{signed_dynamics_paper_review,signedgraph}, we define the signed graph propagation under the linear GNN as follows: 
\begin{align}
\label{eq: sign_overall}
    X^{(k+1)} &= (1-\alpha + \beta) X^{(k)} + \alpha \pgh{\hat{A}^+ }X^{(k)} - \beta\ngh{\hat{A}^- }X^{(k)},\\
    X^{(0)} &= X ,\quad X_{out} = X^{(K)}W^*,
\end{align}
where $\hat{A}^+ =  D_+^{-1}A^+ $ and $\hat{A}^- =  D_-^{-1}A^- $ are the positive and negative row normalized adjacency matrices, $\alpha, \beta>0$ are the parameters controlling the strength of the propagation over the positive and negative graphs, respectively, and \(X_{out}\) is the resulting \(K\)-th layer output.
Note that the crucial difference between the positive and negative graph lies in the sign preceding their adjacency matrices. When $\beta=0, \alpha=1$, (\ref{eq: sign_overall}) would correspond to the conventional (unsigned) graph propagation. 
% In the following sections, we interpret existing oversmoothing couteracting methods from a signed graph propagation perspective in the form of (\ref{eq: sign_overall}).

