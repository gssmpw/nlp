
% \vspace{-2ex}
\section{Experiments}
\label{sec: exp_main}


% \jq{almost rewrite}
In this section, we conduct a comprehensive evaluation of
\ours on various benchmark datasets, including both
homophilic and heterophilic graphs. We aim to answer the following three
key research questions: \textbf{RQ1} How does \ours perform in node classification tasks? \textbf{RQ2} How effectively does \ours mitigate oversmoothing? \textbf{RQ3} How sensitive, robust, and scalable is \ours?   

% In this section, we first verify our theoretical insights on the synthetic datasets.
% Then we demonstrate the effectiveness of \ours in three widely used datasets, and then extend the experimental evaluation to one large-scale dataset and two heterophilous datasets. 
% We apply Label-\ours and Feature-\ours in both linear SGC and non-linear GCN. 




% % \textbf{Contextual Stochastic Block Models.} 
% % Following~\cite{sbm_xinyi}, We focus on the CSBM$(N, p, q, \mu_1, \mu_2, \sigma^2 )$.
% % It consists of two classes $\mathcal{C}_1$ and $\mathcal{C}_2$ of nodes of equal size, in total with $N$ nodes. 
% % For any two nodes in the graph, if they are from the same class, they are connected by an edge independently with probability $p$, or if they are from different classes, the probability is $q$. For each node $v \in \mathcal{C}_i, i\in\{1,-1\}$, the initial feature $X_v$ is sampled independently from a Gaussian distribution $\mathcal{N}(\mu_i, {\sigma^2})$, where $\mu_i =\mathcal{C}_i, \sigma = I $. Specially, let $N=200$, $p=0.092$, $q=0.046$, $d=8$.

% \paragraph{Setup}  
% We apply our methods to node classification on the synthetic dataset CSBM$(n, p, q, \mu_1, \mu_2, \sigma^2 )$.
% Specially, let $n=200$, $p=0.092$, $q=0.046$, $d=8$, $\mu_1=1$, $\mu_2=-1$ and $\sigma^2=I$ following~\cite{sbm_xinyi}.
% We used $60\%$, $20\%$ and $20 \%$ random splits for train, valid and test data, respectively.
% Since our theoretical analysis is primarily based on linear propagation, we apply the linear backbone SGC, which removes the non-linear activation function compared to the GCN, to verify our insight.
% We introduce SGC and GCN in Appendix~\ref{app: GNNs} in details.
% % We further apply both SGC and the GCN in the real world dataset.



% \paragraph{Results} 
% The visualization of node features using Label-\ours and Feature-\ours compared to SGC are shown in Figure \ref{fig: sbm overall}. As the number of layers increases, SGC's node features suffer from oversmoothing, causing the two classes to converge and the classification accuracy to drop to $47.50\%$, which is worse than random guessing ($50\%$). However, Label-\ours and Feature-\ours effectively repel nodes from different classes, achieving high accuracy of $80\%$ and $97.5\%$, respectively, even with $300$ layers.


\begin{figure*}[t]
% \captionsetup{font=small}
    \begin{subfigure}{0.69\textwidth}
        \centering
        % \captionsetup{font=small}
        \includegraphics[width=0.99\textwidth]{figures/eval_sgc_layer.pdf}
        \caption{Oversmoothing performance.}
        \label{fig: layer depth}
    \end{subfigure}
    % \quad
    \begin{subfigure}{0.3\textwidth}
        \centering
        % \captionsetup{font=small}
        \includegraphics[width=0.99\textwidth]{figures/eval_train.pdf} % Adjust the path and filename as necessary
        \caption{Training ratio ablation study.}
        \label{fig: train ratio}
    \end{subfigure}
    \caption{Left is the performance comparison of \ours against Normalization GNNs under various model depths where the X-axis has the number of layers, and the Y-axis has node classification accuracy. Right is the ablation study on Label-\ours where the X-axis indicates the ratio of the training node numbers.}
    % \vspace{-0.55cm}
    % \vspace{-0.2in}
\end{figure*}



% \subsection{Real World Benchmark}

% \input{Tables/SGC_result}
% \vspace{-0.1in}
% \input{Tables/GCN_heter}
\textbf{Datasets.} 
% \input{Tables/dataset}
We use nine widely-used node classification
benchmark datasets (Table~\ref{tab: main_data}), where four of them are heterophilic (Texas, Wisconsin, Cornell, Squirrel, and
Amazon-rating~\citep{platonov2023critical}), and the remaining four are homophilic (Cora~\citep{cora},
Citeseer~\citep{citeseer}, and Pubmed~\citep{pubmed}) including one large-scale dataset (Ogbn-Arxiv~\cite{hu2020ogb}). 
Further information about the datasets and splits are provided in Appendix~\ref{app: exp}.
% and experimental results on more datasets
% In line with prior research, we employ the default training/validation/test splits provided by Pytorch Geometric (PyG). 
% For details of the datasets, including their sources and construction methods.
% We evaluate \ours for the semi-supervised node classification on Cora~\cite{cora}, CiteSeer~\cite{citeseer} and PubMed~\cite{pubmed} and one large-scale dataset ogbn-arxiv from OGB benchmarks~\citep{openbenchmark}.
% We also extend our models to two heterophilous datasets: Chameleon and Squirrel~\cite{heter_dataset}.
% We show the details of the dataset in Appendix~\ref{app: data}.




\textbf{Baselines and experiment settings.}
We compare the performance of \ours against the following $12$ baseline models. 
% \begin{itemize}
1) \textbf{Classic models}: MLP, SGC~\citep{sgc}.
2) \textbf{GNNs with normalization}: BatchNorm~\citep{batchnorm}, PairNorm~\citep{pairnorm} and ContraNorm~\citep{contranorm}.
3) \textbf{Augmenation-based GNNs}: DropEdge~\citep{dropedge}.
4) \textbf{GNNs with residual connections}: Residual, APPNP~\citep{appap}, JKNET~\citep{jknet} and DAGNN~\citep{dagnn}. 
5) \textbf{Other baselines}: GCNII~\citep{GCNII} and \(\omega\)GCN~\citep{wGCN}.
% \xw{你忘记cite gcnii 跟wgcn了}
For the sake of fair comparison, we do not deploy specific training techniques used in some prior works for benchmarking.
All models are trained under the same setting on the pure SGC backbone and we choose the best of scale controller in the range of $\{ 0.1, 0.5, 0.9\}$ for ContraNorm, DropEdge, and residual connections.
% For both Label-\ours and Feature-\ours, 
We choose the best of $\lambda$ in the range of $\{0.1, 0.5, 0.9\}$, fix $\alpha=1$ and select the best value for $\beta$ from $\{ 0.1, 0.5, 0.9\}$ for \ours. 
More experiment results with hyperparameter tuning and optimization strategies can be found in Appendix~\ref{app: exp}.
% fix $\alpha=1$ and only select $\beta$ from $\{0.1, 1,10, 20, 50, 100\}$ for simplify .
% \end{itemize}
% For more details of the classic anti-oversmoothing methods seen in Appendix~\ref{xx}.
% We apply both the linear SGC~\cite{sgc} and non-linear GCN~\cite{gcn} backbones.
% For fair comparison, we fix the hidden dimension to $32$ and dropout rate to $0.6$ following \cite{contranorm}.
% % The residual $\alpha=0.5$, the dropedge present is selecting from $\{0.3,0.5,0.7\}$.
% % We choose the best of scale controller $\alpha,\ \beta \in \{ 0.1, 0.2, 0.5, 0.7, 0.9\}$.
% We select the best settings for PairNorm, Residual, DropEdge, and ContraNorm based on their default hyperparameters. 

% We use Tesla-V100-SXM2-32GB in all experiments.
\textbf{RQ1: Node classification performance.}
In Table~\ref{tab: main_data}, we provide the mean of the node classification accuracy along with their corresponding standard deviations across 10 random seeds under the same 2-layer SGC backbone following~\citet{dgc}.
Overall, \ours achieves the best performance across $8$ datasets in the shallow layers, as Label/Feature-\ours performs the best on 7 out of the 8 datasets. 
% In particular, we make the following three observations:
% First, \ours outperforms all normalization methods. 
% Since our theoretical findings suggest that these normalization methods are essentially implicitly signed graph propagation, the theoretical properties of structural balance (Section~\ref{subsec: sb theory}) contribute to the enhanced classification accuracy of \ours.
% Second, \ours outperforms random argumentation based GNNs. Since DropEdge randomly drops edges, it isn't easy to characterize their exact behaviors, but we highlight that it works when it happens to remove edges between different classes of nodes thanks to our structurally balanced theory, as 
% DropEdge still follows the unified signed graph analysis in its message-passing scheme.
% Lastly, \ours outperforms residual connection based GNNs, including the last layer connection: residual and multilayer feature connection: APPNP, JKNET, and DAGNN. 
% In our analysis, GNNs with residual connections can be seen as a special case of signed graph propagation, where their positive and negative adjacency matrices are the linear combination of adjacency matrices of different orders, yet they are not the theoretically best solution to alleviate oversmoothing.
% This validates the effectiveness of our novel insight from a signed graph perspective. 

% \paragraph{Heterophilic datasets}
% Besides the three homophilic datasets, we also conduct experiments on four heterophilic datasets~\citep{heter_dataset}.
% We find that our method is still the most effective one across all of the methods for alleviating oversmoothing as indicated in Table~\ref{table: gcn heter}.
% Interestingly, we observe that Feature-\ours performs better than Label-\ours on the heterophilic datasets, which is the opposite of the results on the homophilic datasets.

\textbf{RQ2: Anti-oversmoothing analysis.}
% \paragraph{Results} 
% The results for SGC are detailed in Table \ref{table: sgc results} and we give the GCN results in Appendix (Table~\ref{table: gcn result}). 
We further evaluate the robustness of \ours by assessing its performance at deeper model depths: $K \in \{2, 10, 50, 100, 300\}$ for homophilic datasets and $K \in \{2, 5, 10, 20, 50\}$ for heterophilic datasets.
% To provide a comparative analysis against other GNNs, we also evaluate two best-performed normalization-based GNNs: BatchNorm and ContraNorm. the performances of these methods
% We evaluate on one heterophilic graph and two homophilic graphs.
Figure~\ref{fig: layer depth} shows that the performance of Feature/Label-\ours remains relatively stable with varying
% ($K = 50$) 
numbers of layers, achieving its best performance when the model gets deeper.
In contrast, the normalization methods considered exhibit a substantial decrease in performance as the number of layers increases, indicating their persistent susceptibility to the oversmoothing problem.
Note that we find that for \ours to maintain performance in the heterophilic dataset, \(\beta\) needs to be larger than the uniform range considered in Figure~\ref{fig: layer depth}. 
See Appendix~\ref{app: exp} for the result under larger \(\beta\), where \ours on deep layers remains $\approx60\%$ in Cornell.   

% \subsection{RQ3: Ablation Study}
\textbf{RQ3.1: Sensitivity analysis of training ratio.}
% Since Label-\ours leverages the ground truth label information to construct the negative graph, we conduct an ablation study examining the impact of different training data ratios. 
As shown in Figure~\ref{fig: train ratio}, Label-\ours's performance on the CSBM and Cora datasets improves as the training ratio increases. Even with a modest training ratio of 20\%, the worst-performing models still achieve an impressive 80\% accuracy, while the best models approach 100\% accuracy when the training ratio is increased to 80\%. This is in line with our theoretical insights that increasing the training ratio leads to more structural balance resulting from our method~\ours. 
% Moreover, our main experiments detailed in Table~\ref{table: sgc results} demonstrate that Label-\ours outperforms other methods, even when adopting the default training set ratios in those datasets, indicating its effectiveness in real-world graph settings.
% \input{Tables/ablation}


% \begin{figure}[t]
%     \centering
%     \begin{subfigure}{0.63\textwidth}
%         \centering
%         \includegraphics[width=0.99\textwidth]{figures/eval_negative (3).pdf} % Adjust the path and filename as necessary
%         \caption{ Significance plot for $\beta$ in terms of test accuracy on cora (left) and texas (right) with fixed $\alpha=1$}
%         \label{fig: beta}
%     \end{subfigure}
%     \quad
%     \begin{subfigure}{0.33\textwidth}
%         \centering
%         \captionsetup{font=small}
%         \includegraphics[width=0.99\textwidth]{figures/eval_train.pdf} % Adjust the path and filename as necessary
%         \caption{Ablation study on Label-SBP. X-axis indicates the ratio of the training node numbers.}
%         \label{}
%     \end{subfigure}
%     \caption{Ablation study}
%     \label{fig: train ratio}
% \end{figure}




% \begin{figure*}[t]
% % \hspace{-10pt}
%     \begin{minipage}{.48\textwidth}
%     \captionof{table}{Node classification accuracy (\%) on the large-scale dataset~\textit{ogbn-arxiv}.}
%     % Test accuracy (\%) comparison results on large scale dataset (Ogbn-ArXiv). The best results are marked in blue and the second best results are marked in gray on every layer.
%     \centering
%     \resizebox{0.99\linewidth}{!}{
%     \begin{tabular}{lcccc}
%     \toprule
%      Model             & \#L=2              & \#L=4              & \#L=8            & \#L=16  \\
%     \midrule
%     GCN & 67.32 {\footnotesize $\pm$ 0.28} & 67.79 {\footnotesize $\pm$ 0.25} & 65.54 {\footnotesize $\pm$ 0.31} & 59.13 {\footnotesize $\pm$ 0.95}  \\
%          BatchNorm & 70.14 {\footnotesize $\pm$ 0.28} & 70.93 {\footnotesize $\pm$ 0.15} & 70.14 {\footnotesize $\pm$ 0.43} & 63.24 {\footnotesize $\pm$ 1.40} \\
%          % +LayerNorm& \cellcolor{secondbest}70.53 {\footnotesize $\pm$ 0.19} & \cellcolor{best}71.66 {\footnotesize $\pm$ 0.17} & \cellcolor{secondbest}71.23 {\footnotesize $\pm$ 0.16} & 68.62 {\footnotesize $\pm$ 0.47} \\
%          PairNorm & 70.48 {\footnotesize $\pm$ 0.20} & \cellcolor{best}71.59 {\footnotesize $\pm$ 0.17} & \cellcolor{best}71.24 {\footnotesize $\pm$ 0.07} & 68.92 {\footnotesize $\pm$ 0.43} \\
%          ContraNorm & OOM & OOM & OOM & OOM \\
%          DropEdge & 64.07 {\footnotesize $\pm$ 0.32} & 63.92 {\footnotesize $\pm$ 0.27} & 60.74 {\footnotesize $\pm$ 0.45} & 52.52 {\footnotesize $\pm$ 0.34} \\
%          Residual & 66.90 {\footnotesize $\pm$ 0.14} & 66.67 {\footnotesize $\pm$ 0.25} & 61.76 {\footnotesize $\pm$ 0.62} & 53.25 {\footnotesize $\pm$ 0.75} \\
%     % \midrule
%          Feature-\ourst & 67.89 {\footnotesize $\pm$ 0.10} & 68.47 {\footnotesize $\pm$ 0.26} & 65.09 {\footnotesize $\pm$ 0.30} & 60.34 {\footnotesize $\pm$ 0.94} \\
%          Label-\ourst & \cellcolor{best}70.55 {\footnotesize $\pm$ 0.22} & 71.54 {\footnotesize $\pm$ 0.18} & 71.07 {\footnotesize $\pm$ 0.28} & \cellcolor{best}69.33 {\footnotesize $\pm$ 0.59}  \\
%     \bottomrule
%     \end{tabular}
%     \label{tab: large}
%     }\hfill
%     \end{minipage}
%    \begin{minipage}{.52\textwidth}
%    % \vspace{0.2cm}
%        \centering
%        \includegraphics[width=\linewidth]{figures/eval_negative (3).pdf}
%        \captionof{figure}{Significance of negative graph weight $\beta$ on Cora and Texas datasets where we fix the positive graph weight $\alpha=1$.}
%        \label{fig:beta real} 
%    \end{minipage}
%    % \vspace{-0.5cm}
%     % \hspace{-10pt}
% %     \begin{minipage}{.35\textwidth}
% %     \centering
% %     \captionsetup{font=small}
% %     \caption{Accuracy on different splits of train/valid/test dataset. SGC in CSBM and GCN in Cora.}
% %     % SGC test accuracy (\%) comparison results on sbm of \ourst-Label on different splits of train/valid/test dataset. GCN test accuracy (\%) comparison results on Cora of \ourst-Label on different splits of train/valid/test dataset.
% % % The best results are marked in blue on every dataset.
% %     \centering
% %      \resizebox{0.95\linewidth}{!}{
% %     \begin{tabular}{lcc}
% %     \toprule
% %      Splits & CSBM  & Cora   \\
% %     \midrule
% %     % \midrule
% %     % sbm 0/5/5& 57.00 {\footnotesize $\pm$ 13.36}
% %       2/4/4 & 86.25 {\footnotesize $\pm$ 3.01} & 82.80 {\footnotesize $\pm$ 0.81}\\
% %       4/3/3 & 91.50 {\footnotesize $\pm$ 2.52} & 85.39 {\footnotesize $\pm$ 0.18 }\\
% %       6/4/4 & 91.50 {\footnotesize $\pm$ 6.05} & 87.64 {\footnotesize $\pm$ 0.37 }\\
% %       8/1/1 & \cellcolor{best}99.05 {\footnotesize $\pm$ 1.90} & \cellcolor{best} 94.10 {\footnotesize $\pm$ 0.74} \\
% %     \bottomrule
% %     \end{tabular}
% %     \label{tab: ablation}
% %     }
% %     \end{minipage}
%     % \caption{Caption}
%     % \label{tab:my_label}
% \vspace{-0.15in}
% \end{figure*}
\begin{figure}
   % \vspace{0.2cm}
   % \captionsetup{font=small}
       \centering
       \includegraphics[width=0.7\linewidth]{figures/eval_negative_3.pdf}
       \captionof{figure}{Significance of negative graph weight $\beta$ on Cora and Texas datasets where we fix the positive graph weight $\alpha=1$ and vary a large range of \(\beta\).}
       \label{fig:beta real} 
    % \vspace{-0.2in}
\end{figure}
\textbf{RQ3.2: Performance under varying graph homophily and heterophily levels.}
In order to test the performance of \ours on graphs with arbitrary
levels of homophily and heterophily, we conduct an ablation study in the CSBM setting with the controllable homophilic and heterophilic levels following~\citet{GRP-GNN}.
As shown in Figure~\ref{fig:beta csbm}, Feature/Label-\ours performs best in homophilic graphs when all nodes are effectively attracted to one another, i.e., when the repulsion strength $\beta$ is small. As $\beta$ increases, the performance of the model degrades.
% The parameter $\phi$ in the CSBM controls the relative importance of node features and graph topology in determining the homophily level.
% Specifically, $\phi$ ranges from -1 to 1, with lower values corresponding to strongly heterophilic graphs and higher values indicating strongly homophilic graphs. 
% Specially, $\phi=1$ corresponds to strongly homophilic graphs while $\phi=-1$ corresponds to strongly heterophilic graphs.
% We fix $\lambda=0.5$ and then vary $\beta$ which indicates the strength of the repulsive force between the two nodes introduced by the negative edge connecting them.
In contrast, for heterophilic graphs, when the attraction power of the positive graph dominates, \ours achieves only $50\%$ accuracy. 
As $\beta$ increases, the negative graph becomes more dominant, and the model's performance gets significantly better. We observe similar phenomena in the real homophilic and heterophilic graph datasets as shown in Figure~\ref{fig:beta real}.

% \vspace{-1ex}
\textbf{RQ3.3: Performance on large-scale dataset.} 
Finally, we conduct an evaluation of \ours on the large-scale ogbn-arxiv dataset, and the results are presented in Table \ref{tab: large}. 
% To maintain the sparsity of the graph structure and avoid additional computational overhead, we adopt variants of the \ours approach mentioned in Section~\ref{sec: method}. 
Overall, the results demonstrate that Label-\ours-v2 achieves comparable or even superior performance compared to previous normalization methods, particularly in the deep layer setting  ($L=16$).
This verifies the empirical superiority and robustness of our proposed signed graph construction in \ours, which effectively leverages the available label information to alleviate oversmoothing, even at scale.
\begin{table}
    \captionof{table}{Node classification accuracy (\%) on the large-scale dataset~\textit{ogbn-arxiv}.}
    % Test accuracy (\%) comparison results on large scale dataset (Ogbn-ArXiv). The best results are marked in blue and the second best results are marked in gray on every layer.
    \centering
    \resizebox{0.7\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
     Model             & \#L=2              & \#L=4              & \#L=8            & \#L=16  \\
    \midrule
    GCN & 67.32 {\footnotesize $\pm$ 0.28} & 67.79 {\footnotesize $\pm$ 0.25} & 65.54 {\footnotesize $\pm$ 0.31} & 59.13 {\footnotesize $\pm$ 0.95}  \\
         BatchNorm & 70.14 {\footnotesize $\pm$ 0.28} & 70.93 {\footnotesize $\pm$ 0.15} & 70.14 {\footnotesize $\pm$ 0.43} & 63.24 {\footnotesize $\pm$ 1.40} \\
         % +LayerNorm& \cellcolor{secondbest}70.53 {\footnotesize $\pm$ 0.19} & \cellcolor{best}71.66 {\footnotesize $\pm$ 0.17} & \cellcolor{secondbest}71.23 {\footnotesize $\pm$ 0.16} & 68.62 {\footnotesize $\pm$ 0.47} \\
         PairNorm & 70.48 {\footnotesize $\pm$ 0.20} & \cellcolor{best}71.59 {\footnotesize $\pm$ 0.17} & \cellcolor{best}71.24 {\footnotesize $\pm$ 0.07} & 68.92 {\footnotesize $\pm$ 0.43} \\
         ContraNorm & OOM & OOM & OOM & OOM \\
         DropEdge & 64.07 {\footnotesize $\pm$ 0.32} & 63.92 {\footnotesize $\pm$ 0.27} & 60.74 {\footnotesize $\pm$ 0.45} & 52.52 {\footnotesize $\pm$ 0.34} \\
         Residual & 66.90 {\footnotesize $\pm$ 0.14} & 66.67 {\footnotesize $\pm$ 0.25} & 61.76 {\footnotesize $\pm$ 0.62} & 53.25 {\footnotesize $\pm$ 0.75} \\
    % \midrule
         % Feature-\ourst-v2 & 67.89 {\footnotesize $\pm$ 0.10} & 68.47 {\footnotesize $\pm$ 0.26} & 65.09 {\footnotesize $\pm$ 0.30} & 60.34 {\footnotesize $\pm$ 0.94} \\
         Label-\ourst-v2 & \cellcolor{best}70.55 {\footnotesize $\pm$ 0.22} & 71.54 {\footnotesize $\pm$ 0.18} & 71.07 {\footnotesize $\pm$ 0.28} & \cellcolor{best}69.33 {\footnotesize $\pm$ 0.59}  \\
    \bottomrule
    \end{tabular}
    \label{tab: large}
    }
    % \vspace{-0.2in}
\end{table}

% verifying the empirical advantages of our proposed technique.
% \phi 
% Hyperparameter $\beta$ indicating strength of the repulsive force between the two nodes introduced by negative edges connecting them
% To illustrate the impact of $\beta$, we conduct ablation studies on the synthetic CSBM graphs under various settings as well as real datasets. 
%
% Following~\cite{GRP-GNN}, we reconstruct the CSBM with the 
% The parameter $\phi$ to control for the the information given by the node features and the graph topology and the homophily level. 
% Specifically $\phi=0$ indicates that only node features are informative, while $|\phi|=1$ indicates that only the graph topology is informative. Moreover, $\phi=1$ corresponds to strongly homophilic graphs while $\phi=-1$ corresponds to strongly heterophilic graphs.
%
% \begin{figure}[t]
% % \hspace{-10pt}
%     \begin{minipage}{.48\textwidth}
%     \captionof{table}{GCN test accuracy on the large-scale dataset~\textit{ogbn-arxiv}.}
%     % Test accuracy (\%) comparison results on large scale dataset (Ogbn-ArXiv). The best results are marked in blue and the second best results are marked in gray on every layer.
%     \centering
%     \resizebox{0.99\linewidth}{!}{
%     \begin{tabular}{lcccc}
%     \toprule
%      Model             & \#L=2              & \#L=4              & \#L=8            & \#L=16  \\
%     \midrule
%     GCN & 67.32 {\footnotesize $\pm$ 0.28} & 67.79 {\footnotesize $\pm$ 0.25} & 65.54 {\footnotesize $\pm$ 0.31} & 59.13 {\footnotesize $\pm$ 0.95}  \\
%          BatchNorm & 70.14 {\footnotesize $\pm$ 0.28} & 70.93 {\footnotesize $\pm$ 0.15} & 70.14 {\footnotesize $\pm$ 0.43} & 63.24 {\footnotesize $\pm$ 1.40} \\
%          % +LayerNorm& \cellcolor{secondbest}70.53 {\footnotesize $\pm$ 0.19} & \cellcolor{best}71.66 {\footnotesize $\pm$ 0.17} & \cellcolor{secondbest}71.23 {\footnotesize $\pm$ 0.16} & 68.62 {\footnotesize $\pm$ 0.47} \\
%          PairNorm & 70.48 {\footnotesize $\pm$ 0.20} & \cellcolor{best}71.59 {\footnotesize $\pm$ 0.17} & \cellcolor{best}71.24 {\footnotesize $\pm$ 0.07} & 68.92 {\footnotesize $\pm$ 0.43} \\
%          ContraNorm & OOM & OOM & OOM & OOM \\
%          DropEdge & 64.07 {\footnotesize $\pm$ 0.32} & 63.92 {\footnotesize $\pm$ 0.27} & 60.74 {\footnotesize $\pm$ 0.45} & 52.52 {\footnotesize $\pm$ 0.34} \\
%          Residual & 66.90 {\footnotesize $\pm$ 0.14} & 66.67 {\footnotesize $\pm$ 0.25} & 61.76 {\footnotesize $\pm$ 0.62} & 53.25 {\footnotesize $\pm$ 0.75} \\
%     % \midrule
%          % Feature-\ourst & 67.89 {\footnotesize $\pm$ 0.10} & 68.47 {\footnotesize $\pm$ 0.26} & 65.09 {\footnotesize $\pm$ 0.30} & 60.34 {\footnotesize $\pm$ 0.94} \\
%          Label-\ourst & \cellcolor{best}70.55 {\footnotesize $\pm$ 0.22} & 71.54 {\footnotesize $\pm$ 0.18} & 71.07 {\footnotesize $\pm$ 0.28} & \cellcolor{best}69.33 {\footnotesize $\pm$ 0.59}  \\
%     \bottomrule
%     \end{tabular}
%     \label{tab: large}
%     }\hfill
%     \end{minipage}
%    \begin{minipage}{.52\textwidth}
%    \vspace{0.2cm}
%        \centering
%        \includegraphics[width=\linewidth]{figures/eval_negative (3).pdf}
%        \captionof{figure}{Significance of negative graph weight $\beta$ on Cora and Texas datasets where we fix the positive graph weight $\alpha=1$.}
%        \label{fig:beta real} 
%    \end{minipage}
%    % \vspace{-0.5cm}
%     % \hspace{-10pt}
% %     \begin{minipage}{.35\textwidth}
% %     \centering
% %     \captionsetup{font=small}
% %     \caption{Accuracy on different splits of train/valid/test dataset. SGC in CSBM and GCN in Cora.}
% %     % SGC test accuracy (\%) comparison results on sbm of \ourst-Label on different splits of train/valid/test dataset. GCN test accuracy (\%) comparison results on Cora of \ourst-Label on different splits of train/valid/test dataset.
% % % The best results are marked in blue on every dataset.
% %     \centering
% %      \resizebox{0.95\linewidth}{!}{
% %     \begin{tabular}{lcc}
% %     \toprule
% %      Splits & CSBM  & Cora   \\
% %     \midrule
% %     % \midrule
% %     % sbm 0/5/5& 57.00 {\footnotesize $\pm$ 13.36}
% %       2/4/4 & 86.25 {\footnotesize $\pm$ 3.01} & 82.80 {\footnotesize $\pm$ 0.81}\\
% %       4/3/3 & 91.50 {\footnotesize $\pm$ 2.52} & 85.39 {\footnotesize $\pm$ 0.18 }\\
% %       6/4/4 & 91.50 {\footnotesize $\pm$ 6.05} & 87.64 {\footnotesize $\pm$ 0.37 }\\
% %       8/1/1 & \cellcolor{best}99.05 {\footnotesize $\pm$ 1.90} & \cellcolor{best} 94.10 {\footnotesize $\pm$ 0.74} \\
% %     \bottomrule
% %     \end{tabular}
% %     \label{tab: ablation}
% %     }
% %     \end{minipage}
%     % \caption{Caption}
%     % \label{tab:my_label}
% % \vspace{-0.12in}
% \end{figure}

% \input{Tables/large_result}




% \subsection{Ablation Study}





% Appedix~\ref{app: ablation}. 



