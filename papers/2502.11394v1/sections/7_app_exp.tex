\begin{center}
	\LARGE \bf {Appendix}
\end{center}


\etocdepthtag.toc{mtappendix}
\etocsettagdepth{mtchapter}{none}
\etocsettagdepth{mtappendix}{subsubsection}
\tableofcontents
\newpage

% \input{sections/2_background}
% \section{Brader Impacts}
% \label{app: impact}
% Considering the high sensitivity of GNNs to the oversmoothing issue and the depth of layers,
% it is important to develop GNNs that can performance well when the layers go deep, especially for realistic scenarios such as the social network where the scale of friends is getting bigger.
% By introducing the concept and theory of \oursfull (\ours), our work can serve as an initiate step towards tacking oversmoothing problem on graphs,
% with the hope to empower GNNs for broader applications and social benefits.
% Besides, this paper does not raise any ethical concerns.
% This study does not involve any human subjects, practices to data set releases, potentially harmful insights, methodologies and applications, potential conflicts of interest and sponsorship, discrimination/bias/fairness concerns, privacy and security issues, legal compliance, and research integrity issues.

% \section{Discussions on limitations of $\ours$ and future directions}
% \label{app_sec: limiatation}
% Although we have conducted comprehensive theoretical analysis and extensive experiments, there are still aspects requiring further investigation. 

% \paragraph{Theoretical limitation.}
% Our theory is based on asymptotic behaviors, it remains to be explored under non-asymptotic conditions.
% We use the classic signed graph theory---structural balance theory, to capture the ideal distribution of the positive and negative edges and further inspire Label-\ours and Feature-\ours, but it still maintains a gap between the theory and the practice as shown that the performance still decreases when the layers go to deepest (e.g., $64$ for GCN~\citep{gcn}, $300$ for SGC~\citep{sgc}).
% % Additionally, we have not fully captured the relationship between the previous methods for alleviating the oversmoothing and the signed graph propagation through the theoretical perspective. 
% Moreover, the label rate for the influence on test performance necessitates further theoretical analysis.

% \paragraph{More sophisticated architectures/parameter tunning.}
% The Label-\ours and Feature-\ours can have multiple implementations.
% We choose the most classic architectures GCN and the linear architecture SGC in our experiments for the purpose of concept verification.
% Moreover, as shown in Appendix \ref{app: exp}, Label-\ours and Feature-\ours still requires certain additional tunning efforts for the objectives.
% Hence we believe it is also a promising future direction to reduce the parameter tunning by leveraging better optimization techniques.

% \paragraph{Better signed graph design.}
% Typical signed graph contains edges with either positive or negative signs~\cite{signedgraph,yan2022two,LRGNN,acmp}.
% Moreover, the value of every edge can be extended from a concrete number, such as $0,1,-1$, to a continuous number.
% Since our implementation of Label-\ours in this work aims to verify the theoretical findings, we do not apply sophisticated edge assignments during the signed graph propagation, simply using the label-enhanced negative subgraph and the feature similarities enhanced negative subgraph while maintaining the positive subgraph as the adjacency matrix.
% Nevertheless, it is promising to leverage better positive and negative subgraphs to improve the performance of the signed graph propagation to alleviate oversmoothing. 
\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/csbm-b-2.pdf}
    \caption{ Figure (a)-(d) shows the effect of negative graph weight $\beta$ by \ours on CSBM. In all cases, $\lambda=0.5$ and $\alpha=1$. The X-axis is the $\beta$ and the Y-axis is the test accuracy. 
    $\phi$ is the hyperparameter to control the level of homophily and $H(G)$ measure the homophily level. 
    SBP1 indicates Label-\ours and SBP2 indicates Feature-\ours. }
    \label{fig:beta csbm}
    % \vspace{-0.1in}
\end{figure*}



\section{Related Work}

\paragraph{Theory of Oversmoothing}
The concept of oversmoothing was initially introduced by \cite{oversmooth_first}: when the number of layers becomes large, the representations of different nodes tend to converge to a common value after excessively exchanging messages with neighboring nodes. 
\cite{Oono2019GraphNN, wu2023demystifying} rigorously show that the convergence of node representations to a common value happens at an exponential rate as the number of layers increases to infinity, for GCNs and attention-based GNNs, respectively. 
% \cite{zhou2020graph} shows that under specific conditions, that the ultimate convergence point solely encodes information about the graph's structure.
\cite{sbm_xinyi}~theoretically proves that oversmoothing can start to happen even in shallow depth under certain random graph settings.
\jq{\cite{zhou2021dirichlet} proposed an appropriate residual connection according to the lower limit of Dirichlet energy and connected to previous methods qualitatively.}

\paragraph{Signed Graph Inspired Methods}
In the heterophilic graphs, various methods are inspired by the signed graph propagation~\citep{H2GNN,orderedgnn, yan2022two,acmp, GRP-GNN}. In particular,
\citet{yan2022two,acmp} utilize the idea that the negative edges denote connections between nodes that are "not similar to each other" to create repulsion between them during message-passing.
\cite{GRP-GNN} extend the coefficients of the output of different layers in the final aggregation to be learnable and find that the odd layer coefficients tends to be negative for heterophilic graphs, suggesting that learning naturally finds signed-graph message-passing. 
However, \cite{signremedy} show that under some specific random graph settings, the oversmoothing will even happen under signed graph propagation. 
% which aligns with the case in our analysis for Theorem~\ref{thm: small nega} when the repulsion among nodes are not sufficient. 
Nevertheless, we extend the theory to generic graphs and prove that in the ideal state---structural balance, signed edges can indeed serve as a remedy to effectively combat oversmoothing. 

\paragraph{Structural Balance}
Structual balance theory has gained significant attention in recent years~\citep{signedgraph,yan2022two,LRGNN,acmp}. 
%
Inspired by the structural balance theory, \cite{signedgraph} characterizes the balanced path intuitively to learn both balanced and unbalanced representations on each layer.
%
\cite{LRGNN} predicts the signed adjacency matrix by an off-the-shelf neural network classifier to generate pseudo labels with the low-rank assumption.
\cite{signed_dynamics_paper_review} introduces the definition of the Laplacian for signed graphs and develops a comprehensive mathematical theory.
%
In this paper, we rigorously show that structural balance is the theoretical solution to alleviate oversmoothing and propose practical methods based on the property without any additional learnable parameters. 


% In addition to the above methods which explicitly make use of the signed graph propagation, in this paper, we also revisit a wide class of previous anti-oversmoothing methods that do not explicitly claim to use signed message-passing. We find that all of them can be attributed to some kind of design of negative edges to the original graph.
% a theoretical analysis that prioritizes the significance of signed graphs over unsigned graphs, by leveraging the principles of structural balance theory.

% Introduce signed graph and related GNN papers using signed graph. Discuss similar works to ours that also analyze from attractive and repulsive forces. Our key innovation: the formulation and the analysis based on SB.\yf{refined as above}

% On the empirical side, the Simplified Graph Convolution Network (SGC)~\cite{sgc} attributes oversmoothing to non-linear operations and suggests removing non-linear activations, resulting in \(H_{(k)} = Z_{(k-1)}W_{(k-1)}\). Other methods address oversmoothing by incorporating normalization operations, such as LayerNorm~\cite{layernorm}, BatchNorm~\cite{batchnorm}, PairNorm~\cite{pairnorm}, and ContraNorm~\cite{contranorm}. Additionally, DropEdge~\cite{dropedge} mitigates oversmoothing by randomly dropping a percentage \(p\) of edges in the graph, while residual connections~\cite{Chen2020SimpleAD, appap} between the initial and current layers' features further alleviate the issue.

% \textbf{Signed Graph Network.}
% Over the past few years, various signed network models have been proposed~\cite{LRGNN,acmp,yan2022two} while each with notable limitations. 
% %
% \cite{LRGNN} attempts to merge features and the adjacency matrix into a low-rank signed graph using a complex optimization algorithm, which is computationally intensive and difficult to scale. 
% %
% \cite{yan2022two} explores the interaction between oversmoothing and heterophily but limits its approach to merely splitting the cosine similarity matrix into positive and negative matrices based on signs, which might oversimplify the complex graph structure interactions. 
% %
% Additionally, \cite{acmp} addresses signed graph behavior under oversmoothing in continuous, finite GNN layers, but their findings are restricted to limited-layer scenarios. 
% %
% In this paper, we propose a unified signed graph framework that revisits previous methods for mitigating oversmoothing and extends these theories to more generalized, infinite situations, thereby overcoming the limitations identified in earlier research.

% Nevertheless, this form of aggregation treats neighboring nodes indiscriminately across various classes, leading to a decline in node classification accuracy with increasing layer depth, known as the phenomenon of \textit{oversmoothing}.

% \textbf{Oversmoothing.}
% We give he definition of oversmoothing inspired by \cite{graph_oversmoothing_survey}.
% \begin{definition}[Oversmoothing]
%     Let $G$ be an undirected, connected graph and node features $X \in \mathbb{R}^{n \times d}$. We call $\mu : \mathbb{R}^{n \times d} \rightarrow \mathbb{R}_{\geq 0}$ a node-similarity measure if it satisfies the following axioms:
%     \begin{enumerate}
%         \item $\exists c \in \mathbb{R}^d$ with $X_u = c$ for all nodes $u \in V \iff \mu(X) = 0$, for $X \in \mathbb{R}^{n \times d}$
%         \item $\mu(X + Y) \leq \mu(X) + \mu(Y)$, for all $X, Y \in \mathbb{R}^{v \times m}$
%     \end{enumerate}
% \end{definition}
% % \todo{introduce similarity and variance measures and figures.}

% \begin{proposition}

% \end{proposition}


\section{More Discussion on GNNs}
\label{app: GNNs}

\subsection{Message-passing Graph Neural Networks (MP-GNNs)}
Let $\mathcal{G}=(A, X)$ denote a graph with $n$ nodes and $m$ edges, where $A \in \{0,1\}^{n\times n}$ is the adjacency matrix, and $X\in \mathbb{R}^{n \times d}$ is the node feature matrix with a node feature dimension of $d$.
Usually, we will transform the concrete adjacency matrix $A$ to the normalized adjacency matrix $\hat{A}$ by the degree matrix.
Define $D=diag(d_1,d_2, \dots, d_n)$ where $d_i$ is the degree of the node $i$.
Then the normalized adjacency matrix $\hat{A}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$.
Moreover, many theoretical works simplified the normalized adjacency matrix to be $D^{-1}A$ or $AD^{-1}$ as the raw-normalized or column-normalized stochastic matrix where the sum of every raw (column) is $1$ and every entry is non-negative. 
In this paper, we use $\hat{A}=D^{-\frac{1}{2}}AD^{-\frac{1}{2}}$.

Different GNNs typically share a common propagation mechanism, where node features are aggregated and transformed along the network's topology to a certain depth.
The $k$-th layer propagation can be formalized as
\begin{equation}
        \label{Ana_eq_propagation_1}
	\begin{aligned}
	H_{(k)} = \textbf{PROPAGATE}(X; \mathcal{G}; k) =\bigg\langle\textbf{\textit{Trans}}\Big(\textbf{\textit{Agg}}\big\{\mathcal{G};H_{(k-1)}\big\}\Big)\bigg\rangle_{k},
	\end{aligned}
\end{equation}
with $H_{(0)} = X$ and $H_{(k)}$ is the output after the $k$-layer propagation. 
The notation \(\langle \rangle_{k}\) generally varies from GNN models and denotes the generalized combination operation following \(k\) convolutions. 
%
\(\textbf{\textit{Agg}}\{\mathcal{G}; H_{(k-1)}\}\) refers to aggregating the $(k-1)$-layer output $\textbf{H}^{(k-1)}$ along graph $\mathcal{G}$. 
%
Meanwhile, \(\textbf{\textit{Trans}}(\cdot)\) is the corresponding layer-wise feature transformation which often includes a non-linear activation function (e.g., ReLU) and a layer-specific learnable weight matrix \(W\) for transformation

\subsection{GCN}
To deal with non-Euclidean graph data, GCNs are proposed for direct convolution operation over graph, and have drawn interests from various domains. GCNisfirstly introduced for a spectral perspective~\cite{gcn}, but soon it becomes popular as a general message-passing algorithm in the spatial domain.
In the feature transformation stage, GCN adopts a non-linear activation function (e.g., ReLU) and a layer-specific learnable weight matrix \(W\) for transformation.
The propagation rule of GCN can formulated as follow:
\begin{equation}
    H_{(k)} = ReLU((\hat{A}H_{(k-1)})W_{(k)})
\end{equation}

\subsection{SGC}
SGC~\cite{sgc} simplifies and separates the two stages of GCNs: feature propagation and (non-linear) feature transformation. 
It finds that utilizing only a simple logistic regression after feature propagation (removing the non-linearities), which makes it a linear GCN, can obtain comparable performance to canonical GCNs. 
The propagation rule of GCN can formulated as follow:
\begin{equation}
    H_{(k)} = \hat{A}H_{(k-1)})W_{(k)}=\hat{A}^{k}H_{(0)})W_{(k)}...W_{(1)}
\end{equation}
Moreover, SGC transforms $W_{(k)}...W_{(1)}$ to a general learnable parameter $W$, so the formula of SGC can be this:
\begin{equation}
    H_{(k)} = \hat{A}^{k}H_{(0)})W
\end{equation}





\section{More Background about Signed Graph}
\label{app: signed graph}

\subsection{Signed Graph Propagation}
Classical GNNs~\citep{gcn,sgc,gat,gin} primarily focused on message-passing on unsigned graphs or graphs composed solely of positive edges.
For example, if there exists a edge $\{i,j\}$ or the sign of edge $\{i,j\}$ is positive, the node $x_i$ updates its value by:
\begin{equation}
\label{app_eq: node attractive}
    \hat{x}_i = x_i + \alpha(x_j-x_i) = (1-\alpha) x_i + \alpha x_j, \alpha \in (0,1). 
\end{equation}
Compared to the unsigned graph, a signed graph extends the edges to either positive or negative.
Notably, if the sign of edge $\{i,j\}$ is negative, the node $x_i$ update its value using the following expression:
% \yf{need to motivate this method. eg we observe that existing techniques can be cast as xx}
\begin{equation}
\label{app_eq: node repel}
    \hat{x}_i = x_i -\beta (x_j-x_i) = (1+\beta) x_i -\beta x_j, \beta \in (0,1).
\end{equation}
In words, the positive interaction~\eqref{app_eq: node attractive} 
indicates the attraction while the negative interaction~\eqref{app_eq: node repel} 
indicates that the nodes will repel their neighbors.

More generally, when considering all of the neighbors of node $x_i$, let $N_i^+$ denote the positive neighbor set while $N_i^-$ denote the negative neighbor set, where $N_i^+ \cup N_i^-= N_i$ and $N_i^+ \cap N_i^-= \emptyset$.
The representation of $x_i$ is thus updated by: 
\begin{equation}
\label{app_eq: sign_node}
    \hat{x}_i = (1-\alpha + \beta) x_i + \frac{\alpha}{|N_i^+|}\sum_{j\in N_i^+}x_j
    -\frac{\beta}{|N_i^-|} \sum_{j\in N_i^-}x_j\,.
\end{equation}
In particular, the two parameters $\alpha$ and $\beta$ mark the strength of positive and negative edges, respectively.
Furthermore, the signed propagation rule~\eqref{app_eq: sign_node} from a single node can be generalized  
over the whole graph $\mathcal{G}$ written in the matrix update form as:
\begin{equation}
\label{app_eq: sign_overall}
    \hat{X} = (1-\alpha + \beta) X + \alpha \pgh{\hat{A}^+ }X - \beta\ngh{\hat{A}^- }X,
\end{equation}
where $\hat{A}^+$ is the raw normalized version of the positive adjacency matrix $A^+ \in \{0,1\}^{n \times n}$ and $\hat{A}^-$ is that of the negative adjacency matrix $A^- \in \{0,1\}^{n \times n}$.


% \section{Details of Signed Graph Framework}

\subsection{Definition of negative graph}
\label{app_sec: negative graph}
For further proofs of the theorems and propositions in the paper, we give a more simple and detailed definition in this section.

Let \(D_{G^+} = \text{diag}(deg_1^+, \ldots, deg_n^+)\) and \(D_{G^-} = \text{diag}(deg_1^-, \ldots, deg_n^-)\) be the degree matrices of the positive subgraph and negative subgraph, respectively. 
%
Let \(A_{G^+}\) be the adjacency matrix of the graph \(G^+\) with \([A_{G^+}]_{ij} = 1\) if \(\{i, j\} \in E^+\) and \([A_{G^+}]_{ij} = 0\) otherwise. 
%
The adjacency matrix \(A_{G^-}\) of the negative subgraph \(G^-\) is defined by \([A_{G^-}]_{ij} = -1\)  for \(\{i, j\} \in E^-\) and \([A_{G^-}]_{ij} = 0\) for \(\{i, j\} \not\in E^-\).

The Laplacian plays a central role in the algebraic representation of structural properties of graphs. 
%
% \jq{ours is $(D+I)^{-\frac{1}{2}}(A+I)(D+I)^{-\frac{1}{2}}$, no form like $D-A$.}
%
In the presence of negative edges, more than one definition of Laplacian is possible; see \cite{signed_dynamics_paper_review}. 
The Laplacian of the positive subgraph \(G^+\) is \(L_{G^+} := D_{G^+} - A_{G^+}\), while for the negative subgraph \(G^-\) the following two variants can be used: \(L_{G^-}^o := D_{G^-} - A_{G^-}\) and \(L_{G^-}^r := -D_{G^-} - A_{G^-}\). 
Consequently, we have the following definitions.

{Definition 1.} Given the signed graph \(G\), its opposing Laplacian is defined as
\begin{equation}
L_{G}^o := L_{G^+} + L_{G^-}^o = D_{G^+} + D_{G^-} - A_{G^+} - A_{G^-},
\end{equation}
and its repelling Laplacian is defined as
\begin{equation}
L_{G}^r = L_{G^+} + L_{G^-}^r = D_{G^+} - D_{G^-} - A_{G^+} - A_{G^-}.
\end{equation}


\subsection{Positive / Negative Interaction}
% \jq{our method doesn't have the same coefficient}

Time is slotted at \(t = 0, 1, \ldots\). 
Each node \(i\) holds a state \(x_i(t) \in \Omega\) at time \(t\) and interacts with its neighbors at each time to revise its state. 
The interaction rule is specified by the sign of the links. 
Let \(\alpha, \beta \geq 0\). 
We first focus on a particular link \(\{i, j\} \in E\) and specify for the moment the dynamics along this link isolating all other interactions.

The DeGroot Rule:
\begin{equation}
    x_s(t + 1) = x_s(t) + \alpha(x_{-s}(t) - x_s(t)) = (1 - \alpha)x_s(t) + \alpha x_{-s}(t),
\end{equation}
where \(-s \in \{i, j\} \setminus \{s\}\) with \(\alpha \in (0, 1)\)

The Opposing Rule:
\begin{equation}
    x_s(t + 1) = x_s(t) + \beta(-x_{-s}(t) - x_s(t)) = (1 - \beta)x_s(t) - \beta x_{-s}(t);
\end{equation}
or
The Repelling Rule:
\begin{equation}
    x_s(t + 1) = x_s(t) - \beta(x_{-s}(t) - x_s(t)) = (1 + \beta)x_s(t) - \beta x_{-s}(t).
\end{equation}



\subsection{Deterministic Networks}
% \jq{our method is more like the repelling negative dynamics intuitively.}

The Repelling Negative Dynamics:
\begin{equation}
\label{eq: repell_neg}
\begin{split}
    x_i(t + 1) &= x_i(t) + \alpha \sum_{j \in N_i^+} (x_j(t) - x_i(t)) - \beta \sum_{j \in N_i^-} (x_j(t) - x_i(t)) \\
    &= (1 - \alpha deg_i^+ + \beta deg_i^-)x_i(t) + \alpha \sum_{j \in N_i^+} x_j(t) - \beta \sum_{j \in N_i^-} x_j(t).
\end{split}
\end{equation}

Denote \(\bold{x}(t) = (x_1(t) \ldots x_n(t))^T\). We can now rewrite \ref{eq: repell_neg} in the compact form

\begin{equation}
\label{eq: over_repell}
\bold{x}(t + 1) = M_{G} \bold{x}(t) = (I - \alpha L_{G_+} - \beta L_{G_-}^r)\bold{x}(t).
\end{equation}
Here,
\begin{equation}
    M_G = I - \alpha L_{G^+} - \beta L_{G^-}^r = I - L_{G}^{rw},
\end{equation}
with \(L_{G}^{rw} = \alpha L_{G^+} + \beta L_{G^-}^r\) being the repelling weighted Laplacian of \(G\). 
From Equation \ref{eq: over_repell}, \(M_G \mathbf{1} = \mathbf{1}\) always holds. 
We present the following result, which by itself is merely a straightforward look into the spectrum of the repelling Laplacian \(L_{G}^{rw}\).

Note that our~\eqref{eq: sign_overall} is consistent with Equation~\eqref{eq: repell_neg}, only need to replace the $\alpha$ and $\beta$ with $\frac{\alpha}{deg_i^+}$ and $\frac{\beta}{deg_i^-}$ respectively.

