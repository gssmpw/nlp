
% \subsection{Asymptotic Behaviors of Message Passing over Signed Graphs}
% \label{sec: asymptotic behaviors}
% To illustrate the benefit of message-passing over signed graphs compared to standard unsigned graphs, in this section, we present the asymptotic behaviors of signed graph propagation established in previous literature~\citep{signed_dynamics_paper_review}. 
% All of the methods discussed in Section \ref{sec: signed pers} can be linked to signed graph propagation.
% For clarity, we summarize the corresponding positive and negative graphs from different techniques in Table \ref{tab: framework}.

% Then, we propose the optimal solution: \oursfull (\ours) and prove that \ours can distinguish different class nodes even when the layers are approximate to infinity. 
% For practice, we propose two straightforward and efficient methods: Feature-\ours and Label-\ours.
% Furthermore, inspired by the structural balance theory, we identify the superior asymptotic property of structurally balanced graphs and the opportunity there to alleviate oversmoothing.  

% We first highlight that through the appearance of the negative edges, signed graph propagation can mitigate the oversmoothing issue by preventing all nodes from converging the same value.
\section{Structural Balance Propagation}
\label{sec: our methods}



% \subsection{Practical Methods}


In this section, we begin by analyzing the asymptotic behavior of signed graph propagation. Our findings suggest that \textit{structural balanced graphs} exhibit controllable long-term asymptotic behavior, making them effective in mitigating oversmoothing with theoretical guarantees. Building on this insight, we propose two methods, Label/Feature-\ours, and introduce a metric $\mathcal{SID}$ to quantify the structural imbalance of different anti-oversmoothing methods approaches. Our theoretical analysis and this metric help identify the underlying reasons for the inefficacy of previous methods in addressing oversmoothing in the deep-layer regime. 

% In this section, we first study the asymptotic behavior of signed graph propagation.
% Then, our analysis suggests that  exhibit controllable asymptotic behaviors in the long-term and thus can be utilized to alleviate oversmoothing with theoretical guarantees. 
% % To enhance graph structural balance further, 
% Built upon this result, we introduce two methods  and present  to compare the structural imbalance degree of different methods.
% \subsection{Signed Graph Propagation with Insufficient Repulsion}
% \paragraph{Signed Graph Propagation with Insufficient Repulsion}
% \jq{almost rewrite}
\subsection{Asymptotic Behavior of Signed Graph Propagation}
\label{subsec: sb theory}
% To explore the asymptotic behavior of signed graph propagation, 
For simplicity, we initially focus on individual nodes and their interactions via positive and negative edges with other nodes~\citep{signed_dynamics_paper_review}. 
This analysis leads us to convert the matrix-based signed graph propagation in~(\ref{eq: sign_overall}) to the equivalent node level as follows:
\begin{equation}
\small
\label{eq: sign_node}
    X_i^{(k+1)} = (1-\alpha + \beta) X_i^{(k)} + \frac{\alpha}{D_i^+}\sum_{j\in N_i^+}X_j^{(k)}
    -\frac{\beta}{D_i^-} \sum_{j\in N_i^-}X_j^{(k)}\,,
\end{equation}
where $N_i^+$ and $N_i^-$ represent the set of positive and negative neighbors for node \(i\), \(D_i^+\) and \(D_i^-\) represents the degree for the positive and negative adjacency matrices, respectively. 
Note that while~(\ref{eq: sign_node}) is defined for a single node \(X_i^{(k+1)}\), it can be naturally extended to the whole matrix \(X^{(k+1)}\) by applying the same iterative process to each node. \footnote{For additional results into the connections between node-level dynamics, the whole graph \(\mathcal{G}_s\), and the oversmoothing, see Appendix~\ref{app: oversmoothing of theorem 4.1 and 4.3}.}
% Here, $N_i^+ \cup N_i^-= N_i$ and $N_i^+ \cap N_i^-= \emptyset$.

We first show that insufficient repulsion among nodes can still result in oversmoothing, even under the signed graph propagation. Conversely, excessive repulsion can also be detrimental, causing node representations to diverge and hindering overall performance.
% why the methods in Sec~\ref{sec: signed pers} can not completely combat the oversmoothing from the signed graph perspective. 
\begin{theorem}% [~\cite{signed_dynamics_paper_review}, Theorem 2]
\label{thm: small nega}
    Suppose that the signed graph \(\mathcal{G}_s\) where $A^+$ represents a connected graph and $X_i^{(k)} $ represents the value of node $i$ after $k$ propagation steps under~(\ref{eq: sign_node}). Then 
    % along~(\ref{eq: sign_node})
    for any \(0 < \alpha < 1/\max_{i \in X} D_i^+\), there exists a critical value \(\beta_* \geq 0\) for \(\beta\) such that if \(\beta < \beta_*\), then we have \(\lim_{k \to \infty} X_i^{(k)} = \sum_{j=1}^n X_j^{(0)}/n\) for all initial values \(X^{(0)}\); if \(\beta > \beta_*\), then \(\lim_{k \to \infty} \|X^{(k)}\| = \infty\) for almost all initial values w.r.t. Lebesgue measure.
    % \begin{enumerate}
    %     \item[(i)] if \(\beta < \beta_*\), then we have \(\lim_{t \to \infty} x_i(t) = \sum_{j=1}^n x_j(0)/n\) for all initial values \(x(0)\);
    % \item[(ii)] if \(\beta > \beta_*\), then \(\lim_{t \to \infty} \|x(t)\| = \infty\) for almost all initial values w.r.t. Lebesgue measure.
    % \end{enumerate}
\end{theorem}
\textbf{Repulsion strength $\beta$ matters.} The parameter $\beta$ represents the strength of repulsion between nodes, acting as a counterforce to the homogenizing trend of oversmoothing and preserving the heterogeneity of node features within the network. Theorem~\ref{thm: small nega} suggests that if the weight $\beta$ assigned to negative edges is small---particularly when $\beta=0$, corresponding to the standard unsigned graph propagation---node features will inevitably converge to a common value, resulting in oversmoothing. 
However, if $\beta$ is too large, the node features will diverge instead of converging, potentially causing numerical issues~\cite{acmp}.


\textbf{Problem of previous anti-oversmoothing methods.} 
Normalization layers generate negative graphs through linear transformations of the original adjacency matrix, augmentation methods randomly mask elements by setting them to zero, and residual connection methods combine different orders of the adjacency matrix linearly. 
However, none of these methods explicitly consider the repulsion strength \(\beta\) or the interaction between positive and negative graphs.
As a result, their induced signed graphs \(\mathcal{G}_s\) effectively remain as variants of the original adjacency matrix, causing the repulsion strength to depend implicitly on the initial graph structure or the positive weight, $\alpha$. 
This implicit dependency limits their effectiveness in mitigating oversmoothing, regardless of whether the graph is homophilic or heterophilic.

% % \begin{theorem}
% % \label{thm: large nega}
% %     Suppose that the positive adjacency matrix $A^+$ represents a connected graph and $x_i(t) $ represents the value of node $i$ after $t$ rounds of propagation in~\eqref{eq: sign_overall}. Then along~\eqref{eq: sign_overall}
% %     for any \(0 < \alpha < 1/\max_{i \in V} d_i^+\), there exists a critical value \(\beta_* \geq 0\) for \(\beta\) such that if \(\beta > \beta_*\), then \(\lim_{t \to \infty} \|x(t)\| = \infty\) for almost all initial values w.r.t. Lebesgue measure.
% %     % \end{enumerate}
% % \end{theorem}
% Moreover, it shows that although negative edges can mitigate the oversmoothing issue from convergence of node representations, unbounded divergence would happen when the repulsion is too strong, which would also harm the model's performance due to issues such as numerical instability~\citep{acmp}. 

% % Instead, we aim for a trade-off between convergence among all the nodes and unbounded divergence, where node features converge within the label classes while becoming significantly different across different label classes, which would be ideal for node classification~\citep{sbm_xinyi}. 
% % This is how the structural balance theory~\citep{cartwright1956structural,repell} comes into play., to systematically and theoretically combat the oversmoothing.


% % \begin{theorem}
% %     Suppose \(G^+\) is connected. Then along Equation \ref{eq: repell_neg} for any \(0 < \alpha < 1/\max_{i \in V} \deg_i^+\), there exists a critical value \(\beta_* \geq 0\) for \(\beta\) such that
% %     \begin{enumerate}
% %         \item[(i)] if \(\beta < \beta_*\), then average consensus is reached in the sense that \(\lim_{t \to \infty} x_i(t) = \sum_{j=1}^n x_j(0)/n\) for all initial values \(x(0)\);
% %         \item[(ii)] if \(\beta > \beta_*\), then \(\lim_{t \to \infty} \|x(t)\| = \infty\) for almost all initial values w.r.t. Lebesgue measure.
% %     \end{enumerate}
% % \end{theorem}

% % \jq{the following is trying to change the repelling format to our method.}

% % Let $\alpha = \frac{\alpha}{deg_i^+}$ and $\beta = \frac{\beta}{deg_i^-}$. 
% % Equation \ref{eq: repell_neg} becomes: 
% % \begin{equation}
% % \label{eq: repell_neg1}
% %     x_i(t + 1) = (1-\alpha + \beta) x_i(t) + \frac{\alpha}{deg_i^+} \sum_{j \in N_i^+} x_j(t) - \frac{\beta}{deg_i^-} \sum_{j \in N_i^-} x_j(t).
% % \end{equation}

% % Let $\alpha = \beta$. Equation \ref{eq: repell_neg1} becomes: 
% % \begin{equation}
% % \label{eq: repell_neg2}
% %     x_i(t + 1) = x_i(t) + \frac{\alpha}{deg_i^+} \sum_{j \in N_i^+} x_j(t) - \frac{\alpha}{deg_i^-} \sum_{j \in N_i^-} x_j(t).
% % \end{equation}
% % \todo{We only need to solve the different coefficient before $x_i(t)$ in Equation \ref{eq: repell_neg2} and Equation \ref{eq: our_method}.}
% % \begin{itemize}
% %     \item maybe add $\gamma$ to $x_i(t)$ is the reason for not converge to $\infty$.
% %     \item during experiments, we found that if it's $1$ rather than $\gamma$, the variance of the node features will converge to $+\infty$.
% % \end{itemize}


% % \subsection{Our Method}

% % Let $A$ be the adjacency matrix, we normalize $A$ by $(D+I)^{-\frac{1}{2}}(A+I)(D+I)^{-\frac{1}{2}}$, where $D$ is the degree matrix of $A$. For convenience, we simplify the normalize to row normalize $(A+I)(D+I)^{-1}$.
% % We take $A$ as the positive graph.

% % Let $M \in \{0,1\}^{n \times n}$ be the label-induced matrix, we define it 
% % by 2 ways: 
% % \begin{itemize}
% %     \item \(M_{ij} = 1\) for \{i, j\} has the same label in train dataset and \(M_{ij} = 0\) for \{i, j\} otherwise \jq{as contranorm}.
% %     \item \(M_{ij} = 0\) for \{i, j\} has the same label in train dataset and \(M_{ij} = 1\) for \{i, j\} otherwise \jq{as repelling negative sign graph}.
% % \end{itemize}
% % The first way is more like ContraNorm \citep{contranorm} which takes the feature similarity as the negative graph.
% % The second way is more like the repelling negative graph. \(M_{ij} = 1\) for \{i, j\} with different labels is like repelling the nodes from different label blocks and only aggregates within the same label block.
% % In this way, only the node with the same label is oversmoothing. 

% % We then apply softmax to $M$, which makes the $M$ row-stochastic which is a non-negative matrix with row sum equal to one.
% % \begin{equation}
% % \begin{split}
% %      \bold{x}(t+1) &= \gamma \bold{x}(t) + (1-\gamma) (A  - M )\bold{x}(t), \\
% %     &=(\gamma I + (1-\gamma) A - (1-\gamma) M) \bold{x}(t).
% % \end{split} 
% % \end{equation}
% % where $\gamma \in (0,1)$. 
% % For a single node, the update function is: 
% % \begin{equation}
% % \label{eq: our_method}
% % \begin{split}
% %     x_i(t + 1) = \gamma x_i(t) + \frac{(1-\gamma)}{deg_i^+} \sum_{j \in N_i^+} x_j(t) - \frac{(1-\gamma)}{deg_i^-} \sum_{j \in N_i^-} x_j(t).
% %     % &= \alpha x_i(t) + (1-\alpha) (\sum_{j \in N_i^+} x_j(t) -\sum_{j \in N_i^-} x_j(t))
% % \end{split}
% % \end{equation}

% % The positive aggregation is consistent with DeGroot's rule of social interactions, which indicates that the opinions of trustful social members are attractive to each other~\cite{degroot}. 
% % Along a negative edge, the opposing rule (introduced in \cite{consensus} indicates that the aggregation will drive a node state to be attracted by the opposite of its neighbor's state; 
% % the repelling rule [42] indicates that the two node states will repel each other instead of being attractive. 
% % The parameter $\alpha$ and $\beta$ mark the strength of positive and negative edges, respectively.



% We give more details in Appendix~\ref{app: residual}.
% \subsection{Discussion}

% \jq{csbm, metric, analysis}
% \input{Tables/sb_sbm}
% One benefit of our signed graph framework is that it can be used to interpret general classes of existing oversmoothing mitigation methods and gain a unified insight into their working mechanisms. 
% In particular, one common type of solutions is to add normalization layers, such as
% % LayerNorm~\cite{layernorm}, 
% BatchNorm~\citep{batchnorm}, PairNorm~\citep{pairnorm}, or ContraNorm~\citep{contranorm}. 
% Other simple yet successful ways to mitigate oversmoothing include randomly dropping edges or nodes~\citep{dropedge} or adding residual connections~\citep{Chen2020SimpleAD, appap}. 

% Surprisingly, many of the seemingly different methods developed for oversmoothing can be unified and written in the form of the signed graph propagation~\eqref{eq: sign_overall} by involving a shared positive graph and method-specific negative graphs.
% but vary in their ability to combat oversmoothing

% Furthermore, it inspires us to propose a novel method with theoretical and practical guarantees. 
% (Theorem~\ref{thm: connected positive graph}).


% Furthermore, we measure the structural imbalance degree $\mathcal{SID}$ for these methods on the synthetic graph $2$-CSBM~\citep{sbm_xinyi} (Table~\ref{tab: sb_sbm}). 
% Compared with GCN and SGC where there are no negative edges with maximum $\mathcal{N}$, we can tell that although BatchNorm, PairNorm and ContraNorm repel nodes from different label classes indicating by a lower $\mathcal{N}$, 
% % they repel nodes from the same label class with an increase of $\mathcal{P}$, too, 
% which \emph{decreases} the overall structural imbalance degree $\mathcal{SID}$.
% As for DropEdge, we can see that they can randomly repel nodes from the same label class with an increase of $\mathcal{P}$. 
% Finally, it is obvious from the definition of structural balance that having only self-loops in the negative graph, residual connections would not satisfy the property.
% Therefore, as measured by $\mathcal{SID}$, existing approaches to address oversmoothing do not effectively enhance structural balance of the signed graph when implicitly injecting the negative graph, which helps explain their limited ability to address oversmoothing in practice.
% % performance  it is worth noting that none of the aforementioned methods will likely satisfy the property.
% We discuss more details in Appendix~\ref{app: SID-csbm}.
% As we have seen, all the mentioned methods for mitigating oversmoothing utilize a shared positive adjacency matrix for aggregating node features along the graph structure, akin to conventional unsigned graph propagation. 
% However, they differ in terms of the negative adjacency matrix. 
% For BatchNorm and PairNorm, the negative adjacency matrix is derived from the normalized adjacency matrix $\hat{A}$, with nodes having more neighbors being heavily repelled. 
% % \jq{more consideration.}
% % \xinyic{$\hat{A}$ is the normalized adjacency; more complicated than just the degrees}
% ContraNorm incorporates feature similarities into the normalized adjacency matrix to repel nodes with similar features and attract those with different features. 
% %
% Other methods like EdgeDrop and residual connection employ random and identity matrices, respectively.

% Surprisingly, all of these methods can be unified and written in the form of~\eqref{eq: sign_overall} by involving a shared positive graph and method-specific negative graphs.
%
% This formulation explains why all these methods can help alleviate oversmoothing to some extent from a signed graph perspective: they introduce repulsion among nodes in message-passing by implicitly injecting a negative graph.
% when the repelling ability dominates, the signed graph propagation has a chance to diverge (Theorem~\ref{thm: connected positive graph}).
% 
% Furthermore, by delving into the asymptotic behaviors of signed graph propagation, we discover that the ideal design of a signed graph should exhibit the \textit{structural balance} property (Theorem \ref{thm: repel_struct}). 
% This means that oversmoothing occurs only within the group of all-positive edges, while each group connected solely by negative edges repels each other. 
% However, it is worth noting that none of the aforementioned methods will likely satisfy the property.
% Specifically, we give a detailed analysis of the normalization as follows:  

% \yf{why not discuss this in the corresponding method? this is not a unified theorem}

% \begin{proposition}
% \label{pro: normalization}
%     Consider the update: $\hat{X} = \pgh{A}X-\ngh{\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}A}X $, where $A\in \{0,1\}^{n \times n}$ is the adjacency matrix. Define the overall signed graph adjacency matrix $A_s=A-\frac{\mathbb{1}_n \mathbb{1}_n^T}{n}A$. Then we have that the signed graph is (weakly) structurally balanced if and only if the original graph can be divided into several isolated complete graphs. 
% \end{proposition}

% Proposition \ref{pro: normalization} shows that in order for the structural balance property to hold for the signed graph of normalization, the graph needs to satisfy an unrealistic condition where the edges strictly cluster the nodes. 
% \xinyic{"where edges can cluster nodes with the same labels" what do you mean by this?}

% On the other hand, edge dropping adopts a random negative adjacency matrix, which means the probability where structural balance property breaks is at least the proportion of within-label-class edges to all edges. 
% Finally, it is clear that having only self-loops in the negative graph, residual connections would not satisfy the property, either.





% \subsection{Signed Graph Propagation}
% \label{sec:framework}

% Classical GNNs~\citep{gcn,sgc,gat,gin} primarily focused on message-passing on unsigned graphs or graphs composed solely of positive edges.
% For example, if there exists a edge $\{i,j\}$ or the sign of edge $\{i,j\}$ is positive, the node $x_i$ updates its value by:
% \begin{equation}
% \label{eq: node attractive}
%     \hat{x}_i = x_i + \alpha(x_j-x_i) = (1-\alpha) x_i + \alpha x_j, \alpha \in (0,1). 
% \end{equation}
% Compared to the unsigned graph, a signed graph extends the edges to either positive or negative.
% Notably, if the sign of edge $\{i,j\}$ is negative, the node $x_i$ update its value using the following expression:
% \yf{need to motivate this method. eg we observe that existing techniques can be cast as xx}
% \begin{equation}
% \label{eq: node repel}
%     \hat{x}_i = x_i -\beta (x_j-x_i) = (1+\beta) x_i -\beta x_j, \beta \in (0,1).
% \end{equation}
% In words, the positive interaction~\eqref{eq: node attractive} 
% indicates the attraction while the negative interaction~\eqref{eq: node repel} 
% indicates that the nodes will repel their neighbors.




% existing techs
% \jq{to connect signed graph with following and previous technics to alleviating oversmoothing.}
% Several techniques have been developed to enhance the performance of deep-layer GNNs by addressing oversmoothing.
% For instance, BatchNorm regularizes the distribution of $\hat{x}_i$, DropEdge randomly removes edges from the graph, and the residual connection combines the $x_i$ and $\hat{x}_i$.
% In this paper, we observe that these techniques can be cast within a unified framework of a signed graph by incorporating various variants of negative edges.  
% Positive edges denote attraction similar to classical GNNs, while negative edges signify repulsion. 


\subsection{Asymptotic Behavior of Propagation over Structurally Balanced Graphs}
\label{subsec: sb-2 theory}


% Builting upon our analysis, we shift our perspective to examine the roles of positive and negative graphs separately.
Given the limitations of previous methods, where attraction implicitly influences repulsion, we shift our focus to separately examining the distinct roles of positive and negative graphs.
% \xw{what do you mean by the previous sentence} 
To this end, we present the \emph{structural balance graph}, which accounts for the distribution of signed edges across different clusters and exhibits controllable asymptotic behavior under signed graph propagation. 
Formally, following~\citet{signed_dynamics_paper_review,structuralbalance}, we define structurally balanced graphs as follows. 
% \jq{\(X_1\) re-name}
\begin{definition}[Structurally Balanced Graph]
    A signed graph $\mathcal{G}_s$ is called \textbf{structurally balanced} if there is a partition of the node set into \( V = \tilde{V}_1 \cup \tilde{V}_2 \) with \( \tilde{V}_1 \) and \( \tilde{V}_2 \) being nonempty and mutually disjoint, where any edge between the two node subsets \( \tilde{V}_1 \) and \( \tilde{V}_2 \) is negative, and any edge within each \( \tilde{V}_i \) is positive.
% \end{definition}
% The notion of structural balance can be weakened in the following definition \ref{def: weak struct}.
% \begin{definition}
    % A signed graph \( G \) is \textbf{weakly structurally balanced} if there is a partition of \( V \) into \( V = V_1 \cup V_2 \cup \ldots \cup V_m \), \( m \geq 2 \) with \( V_1, \ldots, V_m \) being nonempty and mutually disjoint, where any edge between different \( V_i \)'s is negative, and any edge within each \( V_i \) is positive.
    \label{def: struct balance}
\end{definition}

The structural balance property partitions the graph into two disjoint node groups, (\( \tilde{V}_1 \) and \( \tilde{V}_2 \)), where intra-group and inter-group edges are separated based on their signs, as illustrated in Figure~\ref{fig: sb sample}(b).
Moreover, to address the divergence of node representations caused by excessive repulsion, we introduce a bounded function $\mathcal{F}(\cdot)_c$ that constrains node features to a maximum value \(c\), transforming unbounded divergence into a controlled scale.
We characterize the asymptotic behavior of propagation over structurally balanced graphs as follows: 
% Moreover, Theorem~\ref{thm: small nega} shows that although negative edges can mitigate the convergence of node representations, unbounded divergence would happen when the repulsion is too strong, which would also harm the model's performance due to issues such as numerical instability~\citep{acmp}. 
% Then we prove that under a bounded function to constrain the node features from diverging, the convergence only occurs within each node group separately, which is beneficial for node classification tasks.
% Moreover, if the group $V_1$ and $V_2$ are divided by the node label, it raises a change to control the coverage scales of the nodes, restricting the oversmoothing only happens within the label which is even beneficial for node classification tasks. 


\begin{theorem}
% [\cite{signed_dynamics_paper_review}, Theorem 9]
\label{thm: repel_struct} 
Assume that node $i$ is connected to node $j$ and $X_i^{(k)}$ represents the value of node $i$ after $k$ propagation steps in~(\ref{eq: sign_node}). 
$\mathcal{F}(z)_c$ is a bounded function satisfying: if $z < -c\,$, $\mathcal{F}(z)_c=-c\,$; if $z > c\,$, $\mathcal{F}(z)_c=c\,$; if $-c< z < c \,$, $\mathcal{F}(z)_c=z\,$.
Let $\theta=\alpha$ if the edge $\{i,j\}$ is positive and $\theta=-\beta$ if the edge $\{i,j\}$ is negative.
Consider the constrained signed propagation update:
\begin{equation}
\label{eq: constrained repel dyn}
\small
    X_i^{(k+1)} = \mathcal{F}_c((1-\theta) X_i^{(k)}+\theta X_j^{(k)}).
\end{equation}
Let \(\alpha \in (0,1/2)\). 
Assume that \(\mathcal{G}_s\) is a structurally balanced complete graph under the partition \(V = \tilde{V}_1 \cup \tilde{V}_2\). 
When \(\beta\) is sufficiently large, we have that
% for almost all initial values \(x^{(0)}\) w.r.t. Lebesgue measure, there exists a binary random variable \(l(x^{(0)})\) taking values in \(\{-c,c\}\) such that
\begin{equation}
\small
    \mathbb{P}\left(\lim_{k \to \infty} X_i^{(k)} = c, i \in \tilde{V}_1; \lim_{k \to \infty} X_i^{(k)} = -c, i \in \tilde{V}_2 \right) = 1.
\end{equation}
\end{theorem}
Theorem~\ref{thm: repel_struct} shows that if the graph is structurally balanced and the signed graph propagation is constrained with $\mathcal{F}_c$, node features will converge to their respective group-specific values asymptotically under the signed graph propagation defined in~(\ref{eq: sign_overall}).
% \jq{We discuss the relationship of Theorem~\ref{thm: small nega} and Theorem~\ref{thm: repel_struct} to oversmoothing in Appendix~\ref{app: oversmoothing of theorem 4.1 and 4.3}.}
Furthermore, different groups will
repel each other to have distinct values, even asymptotically. 
Based on the insight in~\cite{sbm_xinyi}, this implies that structurally balanced graphs would be an ideal case for provably addressing oversmoothing through signed graph propagation.  
% In this way, the aggregation will only happen along the positive edges, that is, within the group and the replusion will occur between any nodes from different groups under the signed graph propagation (Equation~\ref{eq: sign_overall}).
% We will give the detailed theoretical analysis in Section~\ref{x}.
% If the signed graph is a structural balance ideally, we can control the coverage scales of the nodes, restricting the oversmoothing happens only within the same label which is even beneficial for node classification tasks. 
\begin{remark}
    We can generalize the two distinct groups in the above result to a generic number of distinct groups by introducing a more general notion, weakly structural balance. See a detailed discussion in Appendix \ref{app:weak-balance}.
\end{remark}




% \input{Tables/sb_sbm}

\subsection{Design Structural Balance Propagation for GNNs}
\label{subsec: sb methods}
% If the group $V_1$ and $V_2$ in Definition~\ref{def: struct balance} are divided by the node label, 
% we can restrict the convergence to only happen within the label, beneficial for node classification tasks. 
% However, in practice, building a real structurally balanced graph remains a challenge. 
Previously, we have established the asymptotic behavior of signed graph propagation on structurally balanced graphs, demonstrating its ability to provably alleviate oversmoothing. 
Building on this theoretical insight, we propose \oursfull (\ours) to enhance structural balance in signed graphs. Specifically, we introduce two simple yet effective methods, Label-\ours and Feature-\ours, which leverage label and feature information, respectively, to construct negative graphs. By strategically incorporating these negative edges, our approach improves structural balance, ensuring more stable and expressive node representations under signed graph propagation while effectively combating oversmoothing.
% To achieve a structurally balanced signed graph, we design two methods label-enhanced structural balanced propagation (Label-\ours) and feature-enhanced structural balanced propagation (Feature-\ours) by utilizing the label and feature information to construct the negative graphs respectively.

\textbf{Label-\ours.} 
% \yf{need a more catchy name}
For simplicity, we let positive adjacency matrix to be the original adjacency matrix \(\hat{A}^+=\hat{A}\), aligning with many prior anti-oversmoothing methods summarized in Table~\ref{tab: framework}.
We then construct the negative adjacency matrix \(A_l^-\) based on training labels to achieve an overall structurally balanced graph, thereby mitigating across-label oversmoothing.
% \xinyic{I feel SERF are not the right letters to pick...Maybe SGF?}
Specifically, we repel nodes from different classes by assigning value \(1\), attract nodes from the same class by assigning value \(-1\), and neither repel nor attract nodes by assigning value \(0\) when labels are unknown in the negative adjacency matrix:
% Similar to many of the previous methods, we leave the positive adjacency matrix unchanged as $\hat{A}$ but define the label enhanced negative adjacency matrix $A_{l}$ as follows:
% \begin{wrapfigure}{r}{.4\textwidth}
% \vspace{-2em}
\begin{equation}
% \vspace{-1.5ex}
\label{eq: label_neg}
    (A^-_l)_{ij}=
    \begin{cases}
      1 & \text{if} \ y_i \neq y_j\,, \\
      -1 & \text{if} \ y_i = y_j\,, \\
      0 & \text{otherwise}\,,
    \end{cases}
    % \vspace{-1ex}
\end{equation}
% \vspace{-0.2in}
% \end{wrapfigure}
where \(y_i\) is the ground truth label for node \(i\).
% if nodes $i, j$ have different labels, we assign a positive value $1$ to repel them. Conversely, if they have the same label, we assign $-1$ to attract them. 
% Note that if labels are unknown, we assign $0$ to neither repel nor attract them. 
% As for the positive graph, we leave the adjacency matrix unchanged as $\hat{A}$ to learn the global graph structure.
% For the positive adjacency matrix \(A_l^+\), we employ the initial adjacency matrix $A$ from the graph to learn the global graph structure.
We prove that under certain conditions Label-\ours can construct a structurally balanced graph in Section~\ref{subsec: analysis of sbp}.
% which is a non-negative matrix with row sums equal to one, 
% n the proposed negative matrix depends on labels, which is feasible in fully supervised learning scenarios.  When labels are unknown, this method becomes ineffective. 
% How can we support the label-enhanced negative matrix with alternative approaches under unsupervised learning?
Nonetheless, 
% although Label-\ours can induce the ideal structural balance in the train set, it requires access to labels that might be scarce under a low labeling rate. To deal with this situation, 
we further propose a variant of our method that estimates the negative adjacency matrix based on feature similarities, mitigating the performance degradation of Label-\ours in label-scarce scenarios.

\textbf{Feature-\ours.} 
% If labels are unavailable, 
In addition to the label signal, we can also  leverage the similarity matrix derived from the node features to create the negative matrix $A^-_f$. 
Formally, the negative adjacency matrix $A^-_f$ induced by Feature-\ours is defined as: 
\begin{equation}
    A^-_f = - X^{(0)} X^{(0)\top}.
    % \vspace{-1ex}
\end{equation}
% where $X \in \mathbb{R}^{n \times d}$ is the initial node feature matrix.
While the use of the node feature as the repulsion signal may not be as precise as the Label-\ours,  it can make use of all node features and thus can be adjusted to the test set, improving the overall structural balance property of the graph. 
% If we normalize the node features to have norm one for each node feature, the similarity value will be in the range of $[-1,1]$. 
% Furthermore, we apply the softmax function to simulate the probability of two nodes sharing the same label.
% \xinyic{probability cannot be negative... softmax would make sense}
% To repel nodes with low similarity and attract those with high similarity, we introduce a minus sign in front of the similarity matrix which has the equal effect to \ours-Label.
% The worst case of this negative matrix is $-\frac{\mathbb{1}_n\mathbb{1}_n^T}{n}$ under the assumption that the features are randomly distributed and independent of the labels.
% It can also extend the raw node features $X$ to the internal node features $X^{(k)}$ of the $k$-th layer.
% \paragraph{Signed graph normalization} 

\textbf{Implementation details.} 
% Besides the negative graph construction, to complete our signed graph propagation method, 
% As the features will diverge to infinity without a bound function due to strong repulsion in Theorem~\ref{thm: small nega}, 
We implement the constrained function $\mathcal{F}_c$ in Theorem~\ref{thm: repel_struct} by LayerNorm~\citep{layernorm}, following~\citet{contranorm}.
To avoid numerical instability for repeated message-passing, we ensure that the sum of the coefficients combining the node representations $X^{(k)}$ and the node representations updates by our \ours remains $1$.
% we employ a row-stochastic adjacency matrix $\hat{A}$ as the positive adjacency matrix, which is a non-negative matrix with row sums equal to one. 
Additionally, we apply the softmax function to the negative matrix, resulting in $\hat{A}^- = \text{softmax}(A^-)$. 
% We set the coefficient for the positive graph as $1$ and only modify the coefficient for the negative graph while ensuring the sum of the coefficients of $X^{(k)}$ remains $1$.
% So the signed adjacency matrix $A_s$ for our \ours is \( \pgh{\hat{A} } - \beta \ngh{ \text{softmax}(\hat{A_l})}\). 
As a result, Label/Feature-\ours can be written as:
% \begin{takeaways}
    \begin{equation*}
    % \small
    % \begin{align}
    \label{eq: sbp}
     % \text{(Label/Feature-\ours)} \quad
      % \resizebox{\linewidth}{!}{$
X^{(k+1)} = (1-\lambda)X^{(k)} + \lambda \Big( 
    \alpha \pgh{\hat{A}^+}X^{(k)} 
    - \beta \ngh{\text{softmax}(A^-_l \text{or} A^-_f)} X^{(k)} 
\Big).
% $}
    % \end{align}
    % \vspace{-2ex}
    \end{equation*}
% \end{takeaways}
% We can see that in practice, when the assumptions do not hold, Label-\ours can still effectively decrease the $\mathcal{SID}$ as shown in Table~\ref{tab: sb_sbm}, resulting in a more structurally balanced signed graph. 

% \paragraph{Scalability of \ours}
% It is noted that the direct application of \ours to large-scale graphs can destroy the sparsity of the original adjacency matrix because we try to assign either a plus or minus sign between each pair of nodes.
% So we propose a modified version on the \ours.
% For Label-\ours, we only remove the edge when the pair of nodes are from different classes, which would even increase the sparsity of the graph while maintaining the structural balance property.
% For Feature-\ours, we reorder the matrix multiplication from $HH^T \in \mathbb{R}^{n \times n}$ to $H^TH \in \mathbb{R}^{d \times d}$ to preserve the distinctiveness of node representations across the feature dimension, rather than across the node dimension as through the original node-level repulsion.
% % \xinyic{"the channels of embeddings different from each other" unclear what this means}
% \jq{More implementation and time complexity analysis are provided in the Appendix~\ref{app: time complexity of sbp}}.
\begin{table}[t]
    \centering
    % \captionsetup{font=small}
    \captionof{table}{$\mathcal{SID}$ on CSBM (Contextual Stochastic Block
Model ) with different methods. We set the two class means $u_1=-1$ and $u_2=1$ respectively, the number of nodes $N=100$, intra-class edge probability $p=2\log100/100$ and inter-class edge probability $q=\log100/100$.}
    \label{tab: sid}
    \resizebox{0.5\linewidth}{!}{
    \begin{tabular}{lccc}
\toprule
 Method             & $\mathcal{P}_{\textcolor{orange}{\downarrow}}$           & $\mathcal{N}_{\textcolor{orange}{\downarrow}}$  & $\mathcal{SID}_{\textcolor{orange}{\downarrow}}$\\
\midrule
DropEdge & $92.62$ & $100.00$ & $96.31$  \\
Residual & $90.87$ & $100.00$ & $95.44$ \\
\midrule
GCN/SGC  & $89.87$ & $100.00$ & $94.94$ \\
\midrule
APPNP& $\textcolor{purple}{0.00}$ & $100.00$ & $50.00$\\
JKNET& $\textcolor{purple}{0.00}$ & $100.00$ & $50.00$\\
DAGNN& $\textcolor{purple}{0.00}$ & $100.00$ & $50.00$\\
\midrule
BatchNorm  & $89.87$ & \textcolor{purple}{$4.56$} & $47.22$ \\
PairNorm & $89.87$ & \textcolor{purple}{$4.56$} & $47.22$ \\
ContraNorm & $89.87$ & \textcolor{purple}{$4.56$} & $47.22$ \\

\midrule
Feature-\ourst (ours) & $89.87$ & \textcolor{purple}{$4.56$} & $47.22$ \\
Label-\ourst (ours) & $32.46$ & $36.16$ & \textcolor{purple}{$34.31$} \\

% \ours-Label(ours) & linear &\\
\bottomrule
\end{tabular}
    }
    % \vspace{-0.15in}
\end{table}%

\textbf{Scalability on large-scale graphs.}
To reduce the memory consumption of the potentially dense negative adjacency in large-scale graphs, we introduce a modified version Label-\ours-v2 by only removing edges when pairs of nodes belong to different classes.
This approach allows Label-\ours-v2 to eliminate the computational overhead of the negative graph, preserving the sparsity of large-scale graphs.
For Feature-\ours, as the number of nodes $n$ increases, the complexity of its matrix operation grows quadratically, i.e., $\mathcal{O}(n^2d)$.
To address this, we reorder the matrix multiplication from $-XX^\top \in \mathbb{R}^{n\times n}$ to $-X^\top X \in \mathbb{R}^{d\times d}$. This preserves the distinctiveness of node representations across the feature dimension, rather than across the node dimension as in the original node-level repulsion.
Formally, Feature-\ours-v2 can be expressed as:
\begin{equation*}
% \small
 % (\text{Feature-\ours-v2})\,\,\,\,\, 
 % \resizebox{\linewidth}{!}{$
 X^{(k+1)} =(1-\lambda)X^{(k)}
 +\lambda(\alpha \pgh{\hat{A}^+}X^{(k)} - \beta X^{(k)} \ngh{\text{softmax}(-X^{(0)T}X^{(0)})}).
 % $}
\end{equation*}
This transposed alternative has a linear complexity in the number of samples, i.e., $\mathcal{O}(nd^2)$, significantly reducing the computational burden in cases where $n \gg d$.
\jq{More analysis and time complexity experiments are provided in the Appendix~\ref{app: time complexity of sbp}}.


% \subsection{Theoretical Analysis on \ours}
% \label{subsec: Simulation Result}

% Despite Theorem~\ref{thm: repel_struct} establishes that an ideal case for node classification can be achieved through signed graph propagation, the structurally balanced complete graph condition seems to be hard to satisfy in practice. 

% \subsection{Measure of Structural Balance in Practice}
\vspace{-2ex}
\subsection{Theoretical analysis of \ours}
\label{subsec: analysis of sbp}
% \begin{figure*}[t]
%   \begin{minipage}{0.4\textwidth}
%     \centering
%     \captionof{table}{The $\mathcal{SID}$ on CSBM.}
%     \label{tab: sid}
%     \resizebox{0.90\linewidth}{!}{
%     \begin{tabular}{lccc}
% \toprule
%  Method             & $\mathcal{P}_{\textcolor{orange}{\downarrow}}$           & $\mathcal{N}_{\textcolor{orange}{\downarrow}}$  & $\mathcal{SID}_{\textcolor{orange}{\downarrow}}$\\
% \midrule
% DropEdge & $92.62$ & $100.00$ & $96.31$  \\
% Residual & $90.87$ & $100.00$ & $95.44$ \\
% \midrule
% GCN/SGC  & $89.87$ & $100.00$ & $94.94$ \\
% \midrule
% APPNP& $\textcolor{purple}{0.00}$ & $100.00$ & $50.00$\\
% JKNET& $\textcolor{purple}{0.00}$ & $100.00$ & $50.00$\\
% DAGNN& $\textcolor{purple}{0.00}$ & $100.00$ & $50.00$\\
% \midrule
% BatchNorm  & $89.87$ & \textcolor{purple}{$4.56$} & $47.22$ \\
% PairNorm & $89.87$ & \textcolor{purple}{$4.56$} & $47.22$ \\
% ContraNorm & $89.87$ & \textcolor{purple}{$4.56$} & $47.22$ \\

% \midrule
% Feature-\ourst (ours) & $89.87$ & \textcolor{purple}{$4.56$} & $47.22$ \\
% Label-\ourst (ours) & $32.46$ & $36.16$ & \textcolor{purple}{$34.31$} \\

% % \ours-Label(ours) & linear &\\
% \bottomrule
% \end{tabular}
%     }
%   \end{minipage}%
%   \quad
%   \begin{minipage}{0.5\textwidth}
%     \centering
%     \begin{subfigure}{0.4\textwidth}
%       \includegraphics[width=\linewidth]{figures/sbm_SGC_299.pdf}
%       \caption{SGC, acc=47.5}
%     \end{subfigure}
%     \begin{subfigure}{0.4\textwidth}
%       \includegraphics[width=\linewidth]{figures/sbm_Label_299.pdf}
%       \caption{\ours, acc=97.50}
%     \end{subfigure}
%     \caption{The t-SNE visualization of the node features on CSBM and Layer=300.}
%     \label{fig: visual node}
%   \end{minipage}
%   % \vspace{-0.4cm}
% \end{figure*}
In this section, we show that our method \ours can create a structurally balanced graph under certain conditions and thus provably alleviate oversmoothing as the number of propagation steps increases. 
% Despite the structurally balanced graph can \jq{provably} alleviate oversmoothing, the condition seems hard to satisfy in practice. 
To achieve this, we introduce a metric,  \textit{structural imbalance degree} ($\mathcal{SID}$), to quantify the level of structural balance in arbitrary signed graph.
% Therefore, it is essential 
 Specifically, $\mathcal{SID}$ counts the number of edges that must be changed to achieve the structural balance. 
% Borrowing from the classic literature~\citep{measurementofsb}, we define the  of a graph by :
\begin{definition}[Structural Imbalance Degree]
    For each node $v$ in a signed graph $\mathcal{G}_s$ of $n$ nodes, let $\mathcal{P}(v)$ denote the subset of nodes that has the same label as $v$ but connected to $v$ by a non-positive edge; let $\mathcal{N}(v)$ denote the subset of nodes that has a different label from $v$ but connected to $v$ by a non-negative edge. Then the structural imbalance degree of $\mathcal{G}$ is defined as $\mathcal{SID}(\mathcal{G}_s) = \frac{1}{2n}\sum_{v\in\mathcal{G}_s}(|\mathcal{P}(v)| + |\mathcal{N}(v)|)$.
\end{definition}
$\mathcal{SID}$ exhibits a fundamental characteristic: it increases as more edge signs deviate from the criteria of a structurally balanced graph, suggesting a higher degree of structural imbalance. Specifically, when the signed graph achieves the structural balance, we can assert that $\mathcal{SID}=0$ as follows:
% Based on the $\mathcal{SID}$, we can measure the distance between a real signed graph and the ideal structural balanced signed graph.

\begin{proposition}
\label{pro: sid}
For a structural balanced complete graph $\mathcal{G}_{sb}$, we have $\mathcal{SID}(\mathcal{G}_{sb})=0$.
\end{proposition}
% 
\begin{figure}[t]
% \captionsetup{font=small}
    \centering
    \begin{subfigure}{0.3\textwidth}
      \includegraphics[width=\linewidth]{figures/sbm_SGC_299.pdf}
      \caption{SGC, acc=47.5}
    \end{subfigure}
    \quad
    \begin{subfigure}{0.3\textwidth}
      \includegraphics[width=\linewidth]{figures/sbm_Label_299.pdf}
      \caption{\ours, acc=97.50}
    \end{subfigure}
    \caption{The t-SNE visualization of the node features on the same CSBM setting as Table~\ref{tab: sid} under Layer=300.}
    \label{fig: visual node}
    % \vspace{-0.25in}
\end{figure}

\input{Tables/main_table}
Based on the \(\mathcal{SID}\), we can quantity the degree of structural balance in the equivalent signed graphs induced by anti-oversmoothing methods discussed in the previous section, as shown in Table~\ref{tab: sid}. 
Our results show that previous anti-oversmoothing methods either remain a high $\mathcal{SID}$ or an imbalance $\mathcal{P}$ and $\mathcal{N}$. In contrast, our methods effectively reduce the $\mathcal{SID}$, resulting in a more structurally balanced graph, or at least be on par with previous methods.
Furthermore, we present the visualization of node features learned by Label-\ours in Figure~\ref{fig: visual node}, showing that a lower $\mathcal{SID}$ can indeed lead to higher accuracy even in deeper layers, thus alleviating oversmoothing effectively.
We discuss more interesting behaviors about $\mathcal{SID}$ in Appendix~\ref{app: obs of sid}.
% This shows our carefully designed signed graph under \ours is more structurally balanced and has a lower $\mathcal{SID}$, ultimately achieving higher accuracy even in deeper layers.

Besides the empirical observation, we present the following theoretical result which demonstrates that Label-\ours can be guaranteed to achieve a certain level of structure balance:
\begin{proposition}
% \vspace{-2exs}
\label{pro: ours-label}
% Consider the update:
% \begin{equation}
%     \hat{X} =\pgh{A}X- \ngh{A_l} X.
% \end{equation}
Assuming balanced node label classes with $|Y_1| = |Y_2|$,  a labeled node ratio denoted as $p$, and the signed graph \(\mathcal{G}_s^l\) created by Label-\ours, 
% is defined as $A_s = A - A_l$.
then we have $\mathcal{SID}(\mathcal{G}_s^l) \leq (1-p)n/2$.
\end{proposition}
Proposition~\ref{pro: ours-label} suggests that Label-\ours constrains $\mathcal{SID}$ linearly with the training ratio $p$, indicating that $\mathcal{SID}$ diminishes with an increase in the labeling ratio $p$. In particular, it implies that Label-\ours can strictly establish a structurally balanced graph for any graph under the full supervision condition, making the model easier to distinguish nodes with different labels as the number of layers increases: 
\begin{theorem}
Under full supervision ($p=1$), the signed graph \(\hat{\mathcal{G}}_s^l\) induced by Label-\ours achieves $\mathcal{SID}(\hat{\mathcal{G}}_s^l)=0$.
% , indicating a structurally balanced graph. 
Consequently, under the constrained signed propagation as given by \eqref{eq: constrained repel dyn}, nodes from distinct classes will converge towards unique constants.
\begin{equation}
\small
    \mathbb{P}\left(\lim_{k \to \infty} X_i^{(k)} = c, i \in \tilde{V}_1; \lim_{k \to \infty} X_i^{(k)} = -c, i \in \tilde{V}_2 \right) = 1.
    \vspace{-0.5cm}
\end{equation}
\end{theorem}

% \paragraph{Simulation Results} 
% To verify the theoretical findings, we conduct experiments on the Contextual Stochastic Block Model (CSBM) to show the effectiveness of \ours in reducing \(\mathcal{SID}\). 
% We set the two class means $u_1=-1$ and $u_2=1$ respectively, the number of nodes $N=100$, intra-class edge probability $p=2\log100/100$ and inter-class edge probability $q=\log100/100$.
% Table~\ref{tab: sid} shows that previous anti-oversmoothing methods either remain a high $\mathcal{SID}$ or an imbalance $\mathcal{P}$ and $\mathcal{N}$. In contrast, our methods effectively reduce the $\mathcal{SID}$, resulting in a more structurally balanced graph, or at least be on par with previous methods.
% Figure~\ref{fig: visual node} presents the visualization of node features learned by Label-\ours under the deep layer. This shows our carefully designed signed graph under \ours is more structurally balanced and has a lower $\mathcal{SID}$, ultimately achieving higher accuracy even in deeper layers.
% the simulation results for binary-class cases with $50$ nodes for each class from CSBM.
% We measure the $\mathcal{SID}$ of previous anti-oversmoothing methods which show that previous methods either remain a high $\mathcal{SID}$ or an imbalance $\mathcal{P}$ and $\mathcal{N}$, but our methods can effectively decrease the $\mathcal{SID}$ to achieve a more structural balance graph, or at least be on par with previous methods.
% As the number of layers increases, SGC's node features suffer from oversmoothing (Appendix~\ref{}), causing the two classes to converge and the classification accuracy to drop to $47.50\%$, which is worse than random guessing ($50\%$). 



