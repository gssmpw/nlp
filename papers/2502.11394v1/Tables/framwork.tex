\begin{table*} % {t}{0.4\linewidth}
\centering
% \vspace{-0.25in}
% \captionsetup{font=small}
\caption{The mathematically equivalent raw normalized positive and negative adjacency matrices in signed graph propagation of various anti-oversmoothing methods.}
\label{tab: framework}
\resizebox{0.8\linewidth}{!}{
% \tiny
\begin{tabular}{llcc}

\toprule
 Method       & Characteristic      & Positive $\hat{A}^+$            & Negative  $\hat{A}^-$ \\
\midrule
GCN & $K$-layer graph convolutions &$\hat{A}$ & $0$\\

SGC & $K$-layer linear graph convolutions &$\hat{A}$ & $0$\\
\midrule
BatchNorm &Normalized with column means and variance & $\hat{A}$ & $\mathbb{1}_n\mathbb{1}_n^T/n \hat{A}$\\
% LayerNorm ~\cite{layernorm}& $\hat{A}$ & $\hat{A} \mathbb{1}_d\mathbb{1}_d^T/d $\\
PairNorm & Normalized with the overall means and variance & $\hat{A}$ & $ \mathbb{1}_n\mathbb{1}_n^T/n \hat{A}$ \\
ContraNorm & Uniformed norm derived from contrastive loss &$\hat{A}$ & $(X X^T) \hat{A}$ \\
\midrule
DropEdge & Randomized augmentation &$\hat{A}$ & $\hat{A}_m$\\
\midrule
Residual & Last layer connection & $\hat{A}$ & $I$\\
APPNP & Initial layer connection & $\Sigma_{i=0}^{k+1}\alpha^i\hat{A}^i$&$\alpha \Sigma_{j=0}^{k}\alpha^j\hat{A}^j$\\
JKNET & Jumping to the last layer&$\Sigma_{i=0}^{k}\alpha^i\hat{A}^i+\hat{A}^{k+1}$& $\Sigma_{j=0}^{k}\alpha^i\hat{A}^k$\\
DAGNN & Adaptively incorporating different layer &$\Sigma_{i=0}^{k}\alpha^i\hat{A}^i+\hat{A}^{k+1}$& $\Sigma_{j=0}^{k}\alpha^i\hat{A}^k$\\
\midrule
Feature-\ours (ours) &Label-induced negative graph &$\hat{A}$ & $\text{softmax}(A^-_f)$ \\
Label-\ours (ours) &Feature-induced negative graph&$\hat{A}$ & $\text{softmax}(A^-_{l})$\\
% Label-\ours(ours) & linear &\\
\bottomrule
\end{tabular}
}
% \vspace{-3ex}
\end{table*}