\section{Related Work}
\label{sec:related_work}
Air pollution forecasting has been extensively studied using various statistical and machine learning techniques, see e.g. \cite{bellinger2017, ijerph15040780, Mendez2023, Houdou_2024} for overviews. Traditional approaches, such as autoregressive integrated moving average (ARIMA) models \cite{box1976time}, have been widely used early on, but often struggle with capturing nonlinear dependencies in complex environmental data. More recently, deep learning models, particularly recurrent neural networks (RNNs) and long short-term memory (LSTM) networks \cite{hochreiter1997long, greff2017lstm}, have demonstrated improved performance in time series forecasting tasks and applied to air pollution forecasting, see e.g. \cite{bui2018deeplearningapproachforecasting, su12062570, 8675934, 8784234}. 
%Encoder-decoder architectures have become a common choice for sequence-to-sequence forecasting, enabling effective modeling of temporal dependencies \cite{cho2014learning}. 
However, these architectures may suffer from information bottlenecks when handling long input sequences. To address this, attention mechanisms have been introduced in sequence modeling, allowing neural networks to dynamically weigh different parts of the input sequence \cite{bahdanau2016neuralmachinetranslationjointly, luong-etal-2015-effective, Vaswani2017AttentionIA, Choromanski2020RethinkingAW}. Naturally, such mechanisms have also been tested in air quality prediction tasks, resulting in improved forecasting capabilities \cite{8614140, 9466491, rana2024, pranolo2024}.

%In recent years, the integration of attention mechanisms into neural networks has significantly advanced the modeling of time series data. 
Early on, attention mechanisms have mainly been employed for time series forecasting inside Recurrent Neural Networks as in the seminal paper \cite{bahdanau2016neuralmachinetranslationjointly}. 
Notably, such architectures allow to deal with input and output sequences of variable length, as required by natural language processing where the idea of attention was introduced and is most strongly motivated by heuristic arguments. 
With the advent of transformers \cite{Vaswani2017AttentionIA}, the bottleneck of the recurrent architecture and its inherent problem of poor parallelizability was successfully addressed. Variable length input and output sequences can still be handled by padding (for short sequences) or e.g. by employing copies of the same transformer block connected via a memory mechanism (for sequences longer than the maximal token length). 
An interesting variation of these main lines of thought with a specific focus on time series forecasting can be found in \cite{niu2024attentionrobustrepresentationtime}, see also \cite{kim2024selfattentionseffectivetimeseries} for a review of using attention mechanisms for time series forecasting. 

To finish this section, we briefly discuss the main differences of our model to others in the literature. Since we are using a contemporary attention mechanism containing all the usual features found in modern transformers followed by additional neural network layers, our system is by construction a {\it transformer}. Since it goes beyond simply having a dense layer after the attention mechanism and instead uses a tailored convolutional architecture consisting of several layers, it is a {\it hybrid} of different systems. On top of the typically 72 time steps (hours) of input corresponding to each feature, it uses additional autoregressive features that are given by 72-hour chunks of selected input features at earlier times that are relevant for forecasting during the current horizon. These chunks are selected via the idea of ``social time'', where days are classified not only according to public holidays or similar features, but also via their relation to other holidays and weekends, which influences pollution generation patterns. Hence, the inputs are {\it enhanced} by {\it autoregressive} features selected via expert knowledge, akin to manual implementation of a very long range (up to one year) attention mechanism.  As a consequence of this enhancement, we do not need to consider very long input sequences and the attention mechanism can focus on short range correlations. Since the transformer always receives exactly the maximal token length as input, there is no issue of padding or transferring an internal state via a memory mechanism to another transformer block, leading to superior performance. 

Notable work where related ideas are discussed and implemented includes \cite{gehring2017convolutionalsequencesequencelearning}, where a convolutional architecture is used in conjunction with an attention mechanism for translation tasks.