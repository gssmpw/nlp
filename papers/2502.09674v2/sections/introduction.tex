\section{Introduction}
Large Language Models (LLMs) have demonstrated remarkable capabilities in different domains through extensive pre-training on web-scale text data~\cite{brown2020language, zhao2023survey,qin2024multilingual}. However, toxic content in training data can lead these models to inadvertently generate harmful outputs \cite{su2024mission,gehman2020realtoxicityprompts}. While prior work has aligned LLMs with human preferences~\cite{bai2022training} through safety supervised fine-tuning (SSFT)~\cite{ouyang2022training} and preference optimization like direct preference optimization (DPO)~\cite{rafailov2024direct}, LLMs' safety capabilities can still be bypassed through various attacks, including jailbreak attacks~\cite{zou2023universal,liu2024flipattack,ding2023wolf,yong2023low} and model editing methods \cite{Ball2024UnderstandingJS,arditi2024refusal,carlini2024aligned}. Understanding what models learn during safety fine-tuning is therefore crucial for preventing safety compromises.

\textit{Mechanistic Interpretation}-based methods \cite{bricken2023monosemanticity} have shown promise in explaining safety behaviors of LLMs. These methods study the activation space and identify specific directions that represent meaningful features like toxicity, truthfulness, and refusal \cite{arditi2024refusal, lee2024mechanistic, li2024inference}. However, these directions are typically obtained by training probe vectors on pair-wise datasets (e.g., pairs of safe/unsafe inputs). As a result, the resulting single direction in probe vectors aggregates all contributing signals, potentially conflating different roles of multiple features.

To uncover safety-related directions beyond single-direction probes, we study the activation shift before and after safety fine-tuning, creating a \textit{residual space}. Within this space, we find that safety behavior is controlled by the interplay of multiple safety feature directions. We present a multi-dimensional interpretation of safety mechanisms by explaining each feature direction through its top-contributing training tokens and measuring its effects on other feature directions and safety behaviors. Our contributions are as follows:

\textbf{Introducing Safety Residual Space.} In \autoref{sec:safety_residual}, we define the \emph{safety residual space} as the linear span of representation shifts during safety fine-tuning. We verify that orthogonal directions in this space captures features of alignment goals. In \autoref{sec:linear}, we setup a case study of safety fine-tuning, applying SSFT and DPO on Llama 3 8B for refusing challenging jailbreaks.

\textbf{Discovering Interpretable Directions.} In \autoref{sec:interpretation}, we decompose the space into major directions (i.e., top singular vectors) and extend layer-wise relevance propagation \cite{bach2015pixel} to analyze these directions. We find that a dominant direction governs the model's refusal behavior, while multiple smaller (non-dominant) directions represent distinct and interpretable features such as hypothetical narrative and role-playing. Intervention experiments show that these indirect features regulate different aspects of capabilities learned during safety fine-tuning.

\textbf{Vulnerabilities in Safety Directions.} In \autoref{sec:application}, we examine dynamics in the safety residual space and find the vital role of non-dominant features in promoting dominant direction and refusal. Leverage this insight, we demonstrate that identifying and removing trigger tokens from harmful prompts can reduce refusal even on safety fine-tuned model, thereby circumventing learned safety alignment.

% Overall
% 
% Past work study the direction with blah blah but didn't questions if their direction is a good one.
% Our work find smaller feature that previous method uncapable of, providing a more complete landcape.
% Which can be a new perspective to understand safety vulrbilities