\section{Related Work}


\paragraph{LLM Alignment \& Jailbreak Attacks}

Multiple algorithms have been proposed to align LLMs with human preferences, with the most prevalent approach being reinforcement learning from human feedback, such as DPO \cite{rafailov2024direct}. Recent work has explored tuning-free alignment methods, including pruning \cite{wei2024assessing}, model fusion~\cite{yi2024safety}, weight editing~\cite{uppaal2024detox}, and decoding process modifications~\cite{Xu2024SafeDecodingDA}. In parallel, numerous studies have focused on compromising these safety alignments through jailbreak attacks~\cite{ding2023wolf,yu2023gptfuzzer,zou2023universal,chao2023pair,liu2024flipattack,Jiang2024ArtPromptAA}. Our work primarily investigates jailbreak attacks due to their widespread study and proven effectiveness against even the most recent models.

\paragraph{Mechanistic Interpretation \& Representation Learning}

Another line of research seeks to understand LLM safety alignment by analyzing internal model mechanisms. Through supervised approaches, researchers have localized safety behaviors in various model components: activations \cite{wei2024assessing,Zhou2024HowAA,li2024safety}, attention patterns \cite{Zhou2024OnTR}, and parameters \cite{lee2024mechanistic,arditi2024refusal}. Additionally, unsupervised methods based on superposition and dictionary learning \cite{bricken2023monosemanticity} have identified meaningful safety-related feature directions \cite{Ball2024UnderstandingJS,Balestriero2023CharacterizingLL}. Several works related to ours examine the effects of safety fine-tuning \cite{jain2024makes,lee2024mechanistic, Yang2024BeyondTN} by analyzing representation shifts. Our work advances this understanding by comprehensively characterizing these shifts and attribute clear semantic meaning to the identified directions.
