\section{Preliminaries}
\label{sec:preliminaries}

\paragraph{Linear Representation} 
We build our framework on the Linear Representation Hypothesis from \citet{park2023linear}. A one-dimensional \emph{feature} value $W$ (e.g., "gender", "harmfulness") is defined as a latent variable that instantiates context $w$ through a generator function, denoted as $w := \mathcal{G}(W)$. In safety analysis, $w$ typically represents user queries with varying safety aspects - from benign questions like "What leads to a united society?" to harmful ones like "how to make a handgun". Probability of feature $W$ presenting in output is denoted as $\mathbb{P}(W)$ (e.g., safe or unsafe responses).

Let $\lambda: w \to \mathbb{R}^d$ be a mapping from context $w$ to its representation.
We say that $\mathbf{x} \in \mathbb{R}^d$ is a \emph{feature direction} of feature $W$ if there exists a pair of contexts $w_0, w_1 := \mathcal{G}(W)$ such that $\lambda(w_1) - \lambda(w_0) \in \{\alpha \mathbf{x} : \alpha > 0\}$ satisfying:

\begin{align}
    \frac{\mathbb{P}(W = 1 \mid \lambda(w_1))}{\mathbb{P}(W = 1 \mid \lambda(w_0))} > 1.
    \label{eq:pop}
\end{align}

This inequality ensures that the direction positively contributes to feature $W$.

\paragraph{Safety Directions} 
In LLM safety alignment, researchers have identified distinct feature directions for various safety aspects including bias, toxicity, and refusal behavior. To find such directions, we first construct two sample sets with only difference being $W$ presenting: $\mathcal{G}(W)$ and $\mathcal{G}(\neg W)$. The feature direction $\mathbf{v}_W$ is then obtained by maximizing the distance between these two distributions. To verify causality, we can intervene by suppressing this direction in the activation space:

\begin{align}
    \mathbf{x} := \mathbf{x} - \alpha \mathbf{v}_W.
    \label{eq:intervene}
\end{align}

An example of this apporach is the \emph{refusal direction} found by \citet{arditi2024refusal}, where $\mathcal{G}(W)$ and $\mathcal{G}(\neg W)$ are inputs which LLM gives compliant or refusing responses.

\paragraph{Layer-wise Relevance Propagation} 
Layer-wise Relevance Propagation (LRP)~\cite{bach2015pixel} decomposes a neural network function $f$ into individual contributions from input variables. For each input-output pair $(i,j)$, we compute a relevance score $R_{i \rightarrow j}$ representing how much input $i$ contributes to output $j$:

\begin{align*}
    f_j(\mathbf{x}) \propto R_j = \sum_{i}^N R_{i \rightarrow j}
\end{align*}

A key property of LRP is conservation across layers. In a layered directed acyclic graph, relevance values $R_i^l \propto f_i$ from a later layer are back-propagated to the previous layer $R_i^{l-1}$ while maintaining constant sum: $\sum_i R_i^{l-1} = \sum_i R_i^l$ In this work, to ensure faithful relevance propagation, we adopt implementation from \citet{achtibat2024attnlrp}.


\begin{figure}
    \vskip 0.2in
    \begin{center}
    \centerline{\includegraphics[width=\columnwidth]{figures/illustration.pdf}}
    \caption{Illustration of the Safety Residual Space. The safety residual space is the linear span of representation shifts during safety fine-tuning. In our experiments, the dominant direction predicts safety behavior, while non-dominant directions capture different indirect safety features.}
    \label{fig:illustration}
    \end{center}
    \vskip -0.2in
\end{figure}