\section{Safety Residual Space}
\label{sec:safety_residual}

We first define our framework of safety residual space. Our approach is motivated by recent research on training dynamics of alignment algorithms \cite{jain2024makes,lee2024mechanistic}, which shows that representation shifts during training are both meaningful and interpretable. We focus specifically on the effects of safety fine-tuning by comparing representation dynamics before and after the fine-tuning process, limiting our scope to a single forward pass. Let $\mathbf{x}$ denote vectors in the representation space $\mathcal{X} \subset \mathbb{R}^d$. Then $\mathcal{T}: \mathcal{X} \to \mathcal{X}$ describes the representation shift from unaligned (before fine-tuning) to aligned (after fine-tuning) states. We define the safety residual space as the linear span of representation shifts during safety fine-tuning. Formally:

\begin{definition}[Safety Residual Space]
    Consider $\mathcal{T}$ from training on unaligned samples whose representation is $\mathbf{x} \sim \mathbb{P}(\mathcal{X}_u)$.
    The \emph{safety residual space} $V_\mathcal{T}$ is defined as the optimal affine transformation parameterized as $V_\mathcal{T}(\mathbf{x}) = \mathbf{W} \mathbf{x} + \mathbf{b}$ that minimizes:
    \[
    V_\mathcal{T}
    \;=\;
    \underset{\mathbf{\hat{W}},\,\mathbf{\hat{b}}}{\mathrm{argmin}}
    \;\; 
    \mathbb{E}_{\mathbf{x} \sim \mathcal{X}_u}
    \Bigl\|\mathcal{T}(\mathbf{x}) \;-\; \bigl(\mathbf{\hat{W}}\mathbf{x} + \mathbf{\hat{b}}\bigr)\Bigr\|^2.
    \]
    \label{def:residual}
    \end{definition}
    

Intuitively, this definition captures the linear activation shifts that the model learns from the training. We consider the safety feature directions as linear and ignore non-linear error between $V_\mathcal{T}$ and $\mathcal{T}$. We use activations from transformer block outputs at the position of the first generated token from each layer. We compute activations from the training data as an approximation of the unaligned distribution $\mathcal{X}_u$.

\paragraph{Extracting Principal Components}
 To identify important directions in the residual space, we apply Singular Value Decomposition (SVD) to $\mathbf{W} - \mathbf{I}$ and take the first $k$ right vectors (components) $V^{:k}_\mathcal{T}$. This describes the span of the largest $k$ orthogonal representation shifts from the input space (i.e., the model before training). 

\paragraph{Notation}
We denote different components as \texttt{LN-CK}, where \texttt{LN} is the layer number and \texttt{K} is the \texttt{K}th largest right vector from SVD. Specifically, we refer to the \texttt{LN-C1} as the \emph{dominant component}, while others are \emph{non-dominant components}. We provide an illustration in \autoref{fig:illustration}. We use \textit{component} and \textit{direction} interchangeably in this paper.

\subsection{Component as Feature Direction}

A key question is whether the components in the residual space contain interpretable features, similar to probe vectors. We first establish the connection between feature directions and residual space. We show that residual direction contributes to the training goal under convergence assumption:

\begin{theorem}[Convergence Ensures Utility]
    \label{thm:convergence-utility}
    Suppose training converges to parameters $\mathbf{W}^*$ with
    \[
    \begin{aligned}
    J(\mathbf{W}^*) &\geq \sup\nolimits_{\mathbf{W}\in\mathcal{M}} J(\mathbf{W}) - \epsilon, \\
    J(\mathbf{W}) &:= \mathbb{E}_{(x,y)\sim\mathcal{D}}[\log P_W(y|x)]
    \end{aligned}
    \]
    For any residual direction $v \in V_{\mathcal{T}}$ from alignment shift $\mathcal{T}$, there exist contexts $w_0, w_1 := \mathcal{G}(C)$ (where $C=1$ iff $f_{\mathbf{W}^*}(X)=Y$) such that
    \[
    \exists \alpha>0,\quad \lambda(w_1) - \lambda(w_0) = \alpha v
    \]
    with the difference satisfying \autoref{eq:pop}.
    \end{theorem}
We provide a detailed derivation in the Appendix~\ref{appd:proof}. Intuitively, this theorem shows that when training maximizes the likelihood of expected outputs, all components in the residual space contribute positively to at least the overall training objective. Otherwise, these shift directions would not have been learned. While this does not guarantee human-interpretable features, it suggests a promising approach for automatically discovering feature directions without requiring probing data pairs. In \autoref{sec:interpretation}, we empirically demonstrate that some components have clear semantic roles in generating safety refusals.

\paragraph{Not All Features are in Residuals}

On the other hand, can all features be captured in the residual space? We posit that it primarily reflects features developed during safety training. Features might be absent for two reasons: (1) they are irrelevant to the training objective (e.g., unrelated syntactic patterns), as optimization naturally excludes non-contributing directions; or (2) they already existed in the pre-trained model (e.g., recognizing toxic content), requiring no parameter updates. This implies the residual space spans directions learned during safety fine-tuning.

\begin{corollary}
    The safety residual space is the span of feature directions developed during safety training.
\end{corollary}

\subsection{Experimental Setup}
Now, we describe the experiment setup, focusing on how models learn to recognize and handle unaligned harmful queries through safety fine-tuning.

\paragraph{Dataset} 
We construct a comprehensive preference dataset incorporating various challenging jailbreak methods and alignment blindspots from recent research \cite{ding2023wolf,yu2023gptfuzzer,zou2023universal,chao2023pair,liu2024flipattack}. To generate harmful examples, we apply these jailbreak methods to toxic samples from STRONG REJECT~\cite{souly2024strongreject}. We further incorporate 50\% samples from or-bench~\cite{cui2024or} as harmless samples to balance the dataset. Detailed dataset specifications are provided in the Appendix~\ref{appd:dataset_construction}.

\paragraph{Evaluation Metrics}
Following established practices in jailbreak research~\cite{zou2023universal,souly2024strongreject}, we evaluate model responses along two dimensions: refusal accuracy and response harmfulness. We measure refusal accuracy across both harmless and harmful test samples, while quantifying response harmfulness using STRONG REJECT scores~\cite{souly2024strongreject}.

\paragraph{Safety Fine-tuning}
We perform safety fine-tuning on Llama 3.1 8B Instruct using both SSFT and DPO approaches for one epoch. For SSFT, we follow \citet{inan2023llama} to optimize the model to generate refusal for harmful queries with instruction fine-tuning. For DPO, we additionally create a preference dataset with prefered helpful responses for harmless queries and refusals with disclaimers for harmful queries. We use Llama 3.1 405B Instruct~\cite{dubey2024llama} to generate reference responses for the preference dataset. The effectiveness of our fine-tuning process is demonstrated by two key metrics: the average \strongreject score~\cite{souly2024strongreject} across all jailbreak attempts decreased significantly from 0.65 to 0.05, while the refusal accuracy improved to 90\%. We provide more details and results on different models in the Appendix~\ref{appd:training_procedure}.