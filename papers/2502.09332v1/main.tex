
\documentclass[final,12pt]{alt2025} 




\title[Full Swap Regret and Discretized Calibration]{Full Swap Regret and Discretized Calibration}
\usepackage{times}



















\altauthor{
 \Name{Maxwell Fishelson} \Email{maxfish@mit.edu}\\
 \addr{MIT}
 \AND
 \Name{Robert Kleinberg} \Email{rdk@cs.cornell.edu}\\
 \Name{Princewill Okoroafor} \Email{pco9@cornell.edu}\\
 \addr{Cornell}
 \AND
 \Name{Renato {Paes Leme}} \Email{renatoppl@google.com}\\
 \Name{Jon Schneider} \Email{jschnei@google.com}\\
 \Name{Yifeng Teng} \Email{yifengt@google.com}\\
 \addr{Google Research NYC}
}

\input{commands}

\begin{document}

\maketitle

\begin{abstract}
We study the problem of minimizing swap regret in structured normal-form games. Players have a very large (potentially infinite) number of pure actions, but each action has an embedding into $d$-dimensional space and payoffs are given by bilinear functions of these embeddings. We provide an efficient learning algorithm for this setting that incurs at most $\tilde{O}(T^{(d+1)/(d+3)})$ swap regret after $T$ rounds.

To achieve this, we introduce a new online learning problem we call \emph{full swap regret minimization}. In this problem, a learner repeatedly takes a (randomized) action in a bounded convex $d$-dimensional action set $\mathcal{K}$ and then receives a loss from the adversary, with the goal of minimizing their regret with respect to the \emph{worst-case} swap function mapping $\mathcal{K}$ to $\mathcal{K}$. For varied assumptions about the convexity and smoothness of the loss functions, we design algorithms with full swap regret bounds ranging from $O(T^{d/(d+2)})$ to $O(T^{(d+1)/(d+2)})$.

Finally, we apply these tools to the problem of online forecasting to minimize calibration error, showing that several notions of calibration can be viewed as specific instances of full swap regret. In particular, we design efficient algorithms for online forecasting that guarantee at most $O(T^{1/3})$ $\ell_2$-calibration error and $O(\max(\sqrt{\epsilon T}, T^{1/3}))$ \emph{discretized-calibration} error (when the forecaster is restricted to predicting multiples of $\epsilon$).
\end{abstract}

\begin{keywords}
  Swap Regret, Online Learning, Calibration
\end{keywords}







\section{Introduction}

Regret minimization is a fundamental paradigm in the theory of online learning with many applications to game theory. Perhaps most notably, it has long been known that if two players choose their actions in a repeated zero-sum game by running a learning algorithm that minimizes their \emph{external regret}, they will over time converge to the unique Nash equilibrium (the ``minimax equilibrium'') of the game. This simple observation is at the core of multiple recent successes at developing superhuman-level AIs at games like Go and Poker \citep{silver2017mastering, brown2018superhuman}, as well as being the main technical tool behind innovations such as boosting and GANs \citep{goodfellow2020generative, freund1996game}. 

However, many important games in practice (such as auctions, markets, and bargaining) are not zero-sum but instead \emph{general-sum}; depending on the outcome, both players might be better or worse off, and there is some incentive for the players to cooperate. In such games, the theoretical guarantees of external regret minimization are markedly weaker; in particular, in general-sum games, the play of external regret minimizing algorithms only converges to the class of coarse correlated equilibria (a fairly crude relaxation of Nash equilibria), and these algorithms are also susceptible to manipulation by a strategic agent \citep{braverman2018selling, deng2019strategizing}. 

For this reason, in general-sum games, it is common to consider a more stringent benchmark known as \emph{swap regret}. While external regret measures how much an agent could have improved their utility by switching to a single fixed action, swap regret measures how much an agent could have improved their utility by applying an arbitrary ``swap function'' to their sequence of past actions. Minimizing swap regret guarantees that agents converge to the sharper class of correlated equilibria, and learning algorithms that minimize swap regret are robust to the previously mentioned manipulations \citep{blum2007external, deng2019strategizing}. 

Unfortunately, swap regret is a much harder quantity to minimize than external regret. In general, a player with $n$ actions available to them each round can guarantee a worst-case external regret bound of $O(\sqrt{T\log n})$ after $T$ rounds, but only a worst-case swap regret bound of $\tilde{O}(\sqrt{nT})$. In many real-life games, the number of actions $n$ can be extremely large (e.g. all pure strategies in an extensive-form game such as poker), and so this polynomial dependence on $n$ is troubling. In recent work, \cite{dagan2023external} and \cite{peng2023fast} give alternate low swap regret algorithms with a much better dependence on the number of actions, but at the cost of an exponential dependence on the amount of swap regret: to achieve $\eps T$ swap regret, these algorithms require $\exp(\Omega(1/\eps))$ rounds. Furthermore, both \cite{dagan2023external} and \cite{peng2023fast} prove lower bounds showing that these rates are somewhat necessary: in the absence of any structure on the actions, any no-swap regret algorithm must incur regret that is either polynomial in the number of actions or run for a number of rounds that is exponential in the average regret per round.

\subsection{Structured, low-dimensional games}

In this paper, we consider the problem of minimizing swap regret in games where the strategy sets of the players have a low-dimensional structure, allowing us to sidestep the aforementioned lower bounds. Formally, we consider games between two players, who we will call the \emph{Learner} (who has $n$ actions) and the \emph{Adversary} (who has $n'$ actions). Each of these actions has a corresponding ``embedding'' in a lower-dimensional Euclidean space: in particular, the Learner's $n$ actions correspond to $d$-dimensional vectors $v_1, v_2, \dots, v_n \in \mathbb{R}^d$, and the Adversary's $n'$ actions correspond to the $d$-dimensional vectors $w_1, w_2, \dots, w_{n'} \in \mathbb{R}^{d}$. The payoff the Learner receives\footnote{In general, we consider adversarial online learning environments where our only goal is to minimize the Learner's regret, and thus where we only need to consider the Learner's utility. In results where the payoff of the Adversary is also relevant (e.g. computation of correlated equilibria), we assume the Adversary's payoff is also a bilinear function of (possibly different) embeddings of these actions into $d$-dimensional space.} when they play action $i$ and the Adversary plays action $j$ is given by $\langle v_i, w_j\rangle$. We call such games \emph{$d$-dimensional structured games}.

Structured games encapsulate important classes of games such as Bayesian games, extensive-form games, and convex games, where the number of pure strategies is far larger (often exponentially so) than the underlying dimension of these strategies. Structured games also are a helpful abstraction of normal-form games when the two players have very different numbers of actions; any such game is a $\min(n, n')$-dimensional structured game (see Lemma~\ref{lem:nfg-as-structured} in Appendix~\ref{sec:nfg-as-structured}). Finally, structured games capture the problem of producing \emph{online calibrated predictions}, which will serve as a motivating example throughout this paper (and which we will introduce shortly). 

In our first set of main results, we prove that it is possible to obtain swap regret bounds where the total number of rounds to achieve $\eps$ per-round swap regret scales as $(1/\eps)^{O(d)}$ and \emph{independently} of the number of actions. In addition, the algorithms achieving these bounds are efficient, running in time polynomial in the time-horizon and dimension and again independent of the number of actions (granted access to an efficient convex decomposition oracle, that in $\poly(d)$ time takes an $x \in \conv(\{v_1, v_2, \dots, v_n\})$ in embedding space and returns a mixed action which embeds to $x$). Specifically, we have the following theorem.

\begin{theorem}\label{thm:small-d-intro}
There exists a learning algorithm for the Learner which incurs at most $\tilde{O}(T^{(d+1)/(d+3)})$ swap regret against any Adversary in any $d$-dimensional structured game. Equivalently, the Learner can guarantee $\eps$ per-round swap regret as long as $T = \tilde{\Omega}(1/\eps)^{(d+3)/2}$. This algorithm runs in per-iteration time $\poly(d, T)$ (assuming access to an efficient convex decomposition oracle).
\end{theorem}

One immediate consequence of Theorem~\ref{thm:small-d-intro} is that as long as the embedding space is constant-dimensional, it is possible to design decentralized learning dynamics that converge to an $\eps$-correlated equilibrium in time and number of steps that is polynomial in $1/\eps$.

\begin{corollary}\label{cor:correlated-equilibria}
Let $G$ be any $d$-dimensional structured game between two players.
There exists a pair of learning algorithms for these two players such that if each player selects strategies according to their algorithm, the average distribution of play after $T = \tilde{\Omega}(1/\eps)^{(d + 3)/2}$ rounds is guaranteed to be an $\epsilon$-correlated equilibrium of $G$. In addition, if both players have access to efficient convex decomposition oracles, both algorithms run in per-iteration time $\poly(d, T)$; in particular, it is possible to compute an $\eps$-correlated equilibrium in this game in time $(1/\eps)^{O(d)}$.
\end{corollary}

\subsection{Full swap regret for online learning}

We prove Theorem \ref{thm:small-d-intro} by reducing the relevant problem to another natural problem: \emph{full swap regret minimization}.  In the standard setting of online learning, a learner must, in every round $t$ (for $T$ rounds), play an action $x_t$ belonging to a $d$-dimensional bounded convex set $\cK \subset \R^{d}$ (in fact, we will actually let the learner play a finitely-supported distribution $\x_t \in \Delta(\cK)$ over such actions).  Then, a loss function $\ell_t : \cK \rightarrow \R$ is revealed by the adversary and the learner incurs loss\footnote{Traditionally, this loss function is also assumed to be convex. In the majority of our applications this will be the case, but we will also consider some settings with weaker constraints on the $\ell_t$ (e.g., Lipschitz or concave $\ell_t$).} $\ell_t(\x_t) := \E_{s \sim \x_t}[\ell_t(s)]$. The standard objective in online learning is to minimize the \emph{external regret}, defined as

\begin{align*}
    \Reg &:= \max_{x^* \in\cK} \sum_{t=1}^T \left[\ell_t(\x_t) - \ell_t(x^*)\right].
\end{align*}

We consider a swap regret variant of this objective, where instead of competing against the best fixed action $x^* \in \cK$ in hindsight, we compete against the best arbitrary swap function $\phi: \cK \rightarrow \cK$ in hindsight.

\begin{align*}
    \FSR &:= \max_{\phi:\cK \rightarrow \cK} \sum_{t=1}^T \left[\ell_t(\x_t) - \ell_t(\phi(\x_t))\right].
\end{align*}
Here $\phi(\x_t) \in \Delta(\cK)$ is the distribution of $\phi(s)$ where $s \sim \x_t$. We call this quantity \emph{\textbf{full} swap regret} to emphasize a distinction from other notions of swap regret where the class of transformations $\phi$ is restricted in some way (e.g., linear swap regret, where $\phi$ must also be a linear transformation).  

We provide a family of algorithms to minimize full swap regret over convex action sets. The optimal rates achievable depend on the qualitative properties of the losses faced by the learner (in particular, whether they are strongly convex, smooth, both, or neither).

\begin{theorem}[Informal version of Theorem~\ref{thm:full-main}]\label{thm:informal_summary}
    Let $\cK \subseteq \RR^d$ be a convex set of diameter 1.  Let $\cL \subseteq \set{\ell: \cK \to \RR}$ be a family of $O(1)$-Lipschitz loss functions.  
    We attain the following $\FSR$ guarantees in terms of the following constraints on $\cL$.
    \begin{center}
        \begin{tabular}{l|c}
            For all $\ell \in \cL$ & $\FSR$ rate\\
            \hline 
            $\ell$: no assumption
            & $\tilde{O}\p{T^{\frac{d+1}{d+2}}}$\\
            \hline 
            $\ell$: linear (or more generally, concave)
            & $\tilde{O}\p{T^{\frac{d+1}{d+3}}}$\\
            \hline 
            $\ell$: $O(1)$-smooth
            & $\tilde{O}\p{T^{\frac{d+2}{d+4}}}$\\
            \hline 
            $\ell$: $\Omega(1)$-strongly-convex
            & $\tilde{O}\p{T^{\frac{d}{d+1}}}$\\
            \hline 
            $\ell$: $\Omega(1)$-strongly-convex and $O(1)$-smooth
            & $\tilde{O}\p{T^{\frac{d}{d+2}}}$\\
        \end{tabular}
    \end{center}
\end{theorem}

Theorem \ref{thm:informal_summary} (in particular, the setting with linear losses) can be immediately applied to recover Theorem \ref{thm:small-d-intro} on minimizing swap regret in structured games. Indeed, instead of thinking of the Learner as playing a mixed strategy $\alpha_t \in \Delta_n$ supported on their $n$ pure actions, it suffices to look at the projection $x_t$ of their strategy onto their embedding space $\cK = \conv(\{v_1, v_2, \dots, v_n\}) \subseteq \R^d$. By the structure of the game, they face \emph{linear losses} in this embedding space, and it is therefore possible to upper bound the swap regret the Learner incurs in the original game by the full swap regret they incur in this projected problem, which by Theorem~\ref{thm:small-d-intro} can be guaranteed to be at most $\tilde{O}(T^{(d+1)/(d+3)})$. 


\paragraph{Online calibration and full swap regret}

In cases where the losses in our game have additional structure, it is possible to apply the other guarantees in the statement of Theorem~\ref{thm:informal_summary} to obtain even stronger regret bounds. One important application where this is the case is the problem of producing online calibrated forecasts.

In the problem of online calibration, the Learner is a forecaster who, every round, must make a (randomized) prediction $\x_t \in \Delta([0, 1])$ for the probability of some binary event (e.g., whether it will rain that day or not). Then nature (the Adversary) either realizes this event (sets $b_t = 1$) or does not (sets $b_t = 0$). The quality of the learner's forecasts is evaluated via their calibration error, which essentially asks ``in the rounds where the forecaster predicted 20\% probability of rain, did it rain 20\% of the time?''. Mathematically, we define the \emph{$\ell_2$-calibration error} as:
\begin{equation*}
    \Cal(\x_{1:T}, \mathbf{b}_{1:T}) = \sum_{p \in [0, 1]}\left(\sum_{t} \x_t[p] \right) \cdot \left(p - \frac{\sum_{t} b_t \x_t[p]}{\sum_t \x_t[p]}\right)^2,
\end{equation*}
where $\x_t[p]$ is the probability that the forecaster predicts probability $p \in [0,1]$ in round $t$. It can be shown (see Lemma~\ref{lem:calib_swap_regret} in Appendix~\ref{sec:calib_swap_regret}) that this calibration error $\Cal$ is exactly equal to the Learner's swap regret in the game where they receive utility $u(x, b) = -x^2 + b(2x - 1)$ when the Learner plays the pure strategy $x \in [0, 1]$ and the Adversary plays the pure strategy $b \in \{0, 1\}$. In particular this quantity can be interpreted as the Learner's swap regret in a two-dimensional structured game where the pure strategy $x$ corresponds to the embedding $v_{x} = (2x-1, -x^2)$, and the pure strategy $b$ corresponds to the embedding $w_{b} = (b, 1)$.



Naively applying the bounds in Theorem \ref{thm:small-d-intro} to this game gives us an algorithm with a calibration error of $\tilde{O}(T^{3/5})$. But since the losses in calibration are quadratic functions of the single-dimensional parameter $x$, we can directly apply the strongly convex and smooth case of Theorem \ref{thm:informal_summary} for $d=1$ to instead obtain an online forecaster with $\tilde{O}(T^{1/3})$ $\ell_2$-calibration error.

\begin{theorem}[$\ell_2$-Calibration]\label{thm:calib_bound} There is a forecasting algorithm with $\tilde{O}(T^{1/3})$ $\ell_2$-calibration error.
\end{theorem}

Note that this is also asymptotically better than both the bound on $\ell_2$-calibration inherited from the best bounds for $\ell_1$-calibration \citep{dagan2024breakingt23barriersequential}, and the $O(\sqrt{T})$ bound obtained by naively applying Blum-Mansour to an $\epsilon$-net of predictions. 

\paragraph{Algorithmic techniques} All of the algorithms in Theorem~\ref{thm:informal_summary} can be seen as extensions of the classical swap-regret minimization algorithm of \cite{blum2007external} for the case of discrete actions. In that algorithm, we maintain an instance of an external regret minimization algorithm for each action. In each round, each action's sub-algorithm outputs a probability distribution over the set of all actions, and taken together, these distributions can be viewed as a Markov chain over the actions. The learner then samples an action from the stationary distribution of this Markov chain, which allows us to decompose the swap regret into the sum of external regrets of each of the sub-algorithms. With $n$ discrete actions, this algorithm incurs at most $O(\sqrt{nT})$ swap regret.

The immediate difficulty of applying this idea to convex action sets is that the set of actions is infinite, so the previously mentioned regret bound is vacuous. Our solution to this problem has two main components. The first (rather natural) idea is to discretize the set of actions, apply the Blum-Mansour algorithm over this discretization, and bound the discretization error. If we choose the discretization carefully (by performing an appropriate polytope approximation of the convex set $\cK$), this idea alone is enough to recover the first three rows of guarantees in Theorem \ref{thm:informal_summary}. 

However, this black box application of Blum-Mansour is inherently lossy -- by treating every point in the discretization as an individual action, we lose the information that some pairs of actions correspond to very nearby points whereas others correspond to very dissimilar points. Our second main technical insight is to modify the Blum-Mansour algorithm to take advantage of this information by adjusting the behavior of the exteral regret minimization sub-algorithms.  Indeed, we begin with a discretization $\Ke$ of the action set $\cK$, and we instantiate an external regret algorithm for each point in $\Ke$.  However, instead of running a generic regret minimization algorithm over the set of distributions $\Delta(\Ke)$, we modify each sub-algorithm to be a combination of an online convex optimization algorithm over $\cK$ (e.g. some variant of gradient descent) and an appropriate rounding procedure mapping $\cK$ to $\Delta(\Ke)$. This allows the subroutines to attain improved rates when the losses are strongly-convex (where we can obtain external regret bounds of $O(\log T)$ instead of $O(\sqrt{T})$), and allows us to achieve the remaining guarantees in Theorem \ref{thm:informal_summary}. 

\subsection{Discretized calibration: interpolating between linear and strongly convex losses}

Carefully examining the regret guarantees in Theorem \ref{thm:informal_summary} reveals a mathematically counter-intuitive phenomenon. Let's consider the case $d=1$. If we focus on one extreme of the space of possible loss functions -- the case of linear losses -- it is possible to obtain swap regret bounds that scale as $\tilde O(\sqrt{T})$ (row 2). On the other hand, if we look at another extreme -- strongly convex loss functions -- we again attain $\tilde O(\sqrt{T})$ regret (row 4). But if we interpolate between these two extremes and look at sequences of loss functions that are merely guaranteed to be convex, the best regret bound that applies is simply the general regret bound of $\tilde{O}(T^{2/3})$ (row 1). This is worse than the regret bound at either of the two extremes! This raises the following question:

\begin{question}\label{question:convex-question}
For $d=1$, is there an algorithm which guarantees $\FSR = \tilde{O}\left(\sqrt{T}\right)$ against any sequence of $O(1)$-Lipschitz convex loss functions $\ell_t$?
\end{question}



We do not resolve Question \ref{question:convex-question} in this paper, but we provide some evidence that the answer is positive by examining some natural instances of this question where this phenomenon already occurs. One especially striking example is a natural variant of the online calibration problem that we call \emph{discretized calibration}.

The setting of discretized calibration is almost identical to the online calibration setting described above, with the main difference being that the learner is required to make predictions that are multiples of a fixed $\epsilon$ (e.g., $5\%$)? This is practically motivated by the fact that in many typical forecasting settings, forecasts are often ``binned'' into such discrete multiples; for example, a newspaper may be reluctant to publish a weather forecast of a 13.14159\% probability of rain and instead may prefer to report that there is a 15\% probability of rain instead. Formally, the forecaster makes predictions $\x_t \in \Delta([0,1] \cap \epsilon \Z)$ (note that this action set is a $\lceil 1/\eps \rceil$-dimensional simplex). We modify the calibration loss to $\Cal_\epsilon$ by replacing the $(p - \bar b(p))^2$ term with  $(p - [\bar b(p)]_{\epsilon})^2$, where $[b]_\epsilon \in [0,1] \cap \epsilon \Z$ rounds $b \in [0,1]$ to the nearest multiple of $\eps$. In other words, we only compete against predictors who also are forced to discretize their forecasts (without this constraint, we would be forced to incur $\Omega(\eps T)$ calibration error simply due to this rounding error). 

If the learner is required to make predictions that are multiples of $\epsilon$, there are two natural approaches. The first is to run our algorithm in Theorem \ref{thm:calib_bound} for general (non-discretized) calibration, and round each prediction to the nearest multiple of $\epsilon$. This leads to a discretized calibration error of  $\tilde{O}(T^{1/3} + \epsilon^2 T)$. A second approach is to simply treat this as a regular discrete swap regret problem over an action set with $1/\epsilon$ actions and directly run the algorithm of \cite{blum2007external}. This leads to a discretized calibration error of $O(\sqrt{T / \epsilon})$. The calibration errors of these two approaches are depicted in Figure \ref{fig:discretized_calibration_regret}. The best of those natural approaches cannot guarantee regret below $O(\sqrt{T})$ for $\eps \in (T^{-1/4},1)$. In the following theorem we significantly improve this bound.


 \begin{figure}[h]
\centering
\begin{tikzpicture}[scale=3]

\draw (-.1,0) -- (8/3,0);
\draw (0,-.1) -- (0,1.5);
\node at (0,-.1) {$T^{-1}$};
\node at (2/3,-.1) {$T^{-1/3}$};
\node at (4/3,-.1) {$T^{-1/4}$};
\node at (2,-.1) {$T^{-1/5}$};
\node at (8/3,-.1) {$1$};
\node at (-.2,.2) {$T^{1/3}$};
\node at (-.2,.8) {$T^{1/2}$};
\node at (-.2,1.4) {$T^{3/5}$};
\node at (8/3,.06) {$\epsilon$};
\node at (2,.4) {\color{red} $\sqrt{\epsilon T}$};
\node at (1,1) {\color{black} $T^{1/3} + \epsilon^2 T$};
\node at (2.4,1.3) {\color{blue} $\sqrt{T / \epsilon}$};

\draw[line width=1.5, dashed,  black] (0,.215) -- (2/3,.215) -- (2.13, 1.52); 
\draw[line width=1.5, dashed, blue] (1.86, 1.52) -- (2,1.4) -- (8/3,.8);
\draw[line width=1.5, red] (0,.2) -- (2/3,.2) -- (8/3,.8);
\end{tikzpicture}
\caption{The $x$-axis shows the discretization parameter $\epsilon$ and the $y$-axis the calibration loss $\Cal$ for three different algorithms: dashed blue (regular swap regret on $1/\epsilon$ actions), dashed black (algorithm in Theorem \ref{thm:calib_bound} + rounding) and red (algorithm in Theorem \ref{thm:discrete_calib_bound}).}
\label{fig:discretized_calibration_regret}
 \end{figure}

\begin{theorem}[Discretized $\ell_2$-Calibration]\label{thm:discrete_calib_bound} There is an online algorithm for $\eps$-discretized calibration which incurs calibration error at most $\tilde{O}(\max(T^{1/3}, \sqrt{\epsilon T})).$
\end{theorem}

Our algorithm for discretized calibration is a special case of a $\SR$ algorithm for when the action set is a discretization of a one-dimensional convex set.

\begin{theorem}[Informal version of Theorem~\ref{thm:disc-main}]\label{thm:discretized_swap_regret_informal}
Let $K \subseteq [0, 1]$ be a discrete bounded set such that for each $x \in [0, 1]$ there exists a $y \in K$ such $|x-y| \leq \epsilon$. If the losses are Lipschitz and $\alpha$-strongly convex, then there exists an algorithm that incurs at most $O(\sqrt{\epsilon T} + \epsilon^{-1} \log T)$ swap regret (against the best map $\phi: K \rightarrow K$).
\end{theorem}

The main idea behind the proof of Theorem~\ref{thm:discretized_swap_regret_informal} is to treat the problem as a full swap regret problem by replacing each loss function by its \emph{piecewise linearization}: the continuation of the original loss function $\ell$ from the discrete domain $K$ to the extended domain $[0, 1]$ by taking the lower convex hull of the set of points $\{(x, \ell(x))\}_{x \in K}$. While the original losses are strongly convex, these piecewise linearizations are not, and so we cannot immediately apply the results of Theorem \ref{thm:informal_summary}. However, they are what we call \emph{$(\alpha, \epsilon)$-nearly-strongly-convex}, i.e. they satisfy the strong-convexity property for points that are at least $\epsilon$-apart. We develop a new external regret algorithm for nearly-strongly convex functions (that may be of independent interest); using this algorithm as the sub-routine in Blum-Mansour allows us to recover Theorem~\ref{thm:discretized_swap_regret_informal}.

\subsection{Related work}

\paragraph{Swap regret} There is a large line of recent work on designing low-swap regret algorithms and understanding their game-theoretic guarantees \citep{braverman2018selling, deng2019prior, deng2019strategizing, camara2020mechanisms, mansour2022strategizing, cai2023selling, brown2024learning, haghtalab2024calibrated}. Almost all of these results are constrained to the discrete setting for low dimension. Very recently  \cite{dagan2023external} and \cite{peng2023fast} presented algorithms that achieve full swap regret of $O(T/\log T)$, independent of $d$. This work improves on these results in the regime $d = o(\log(T)/\log\log(T))$. \cite{roth2024forecasting} and \cite{hu2024calibrationerrordecisionmaking} study the problem of designing forecasts such that any downstream agent incurs low swap regret; the problem faced by a single downstream agent (with potentially many actions but where the payoff only depends on a low-dimensional outcome) can be interpreted as a swap-regret minimization problem in a structured game. 

\paragraph{$\phi$-regret} Gordon et al. \citep{GordonGreenwaldMarks2008} introduced a generalization of swap regret called $\phi$-regret, where instead of competing only with standard swaps you compete with all transformation functions $\phi:\cK\rightarrow \cK$ belonging to a set $\Phi$. Our full swap regret corresponds to the extreme case of this where $\Phi$ contains every such function. \cite{mansour2022strategizing}, \cite{farina2024polynomial}, and \cite{daskalakis2024efficient} study a variant of $\phi$-regret called $\LSR$ that competes only against linear maps $\phi$. This is a much weaker notion than $\FSR$ and does not suffice for obtaining bounds on calibration error.  Other variants of $\phi$-regret minimization have found applications in designing learning algorithms for extensive-form games \citep{celli2021decentralized, bai2022efficient, zhang2024efficient}. 

\paragraph{Calibration} There is a well-established connection in the literature between calibration error and swap-regret \citep{cesa2006prediction} (in fact the very first low swap-regret algorithms worked via responding to calibrated loss estimates, \cite{foster1997calibrated}). Designing online forecasters with good calibration guarantees is a problem of major interest \citep{brier1950verification,murphy1972scalar,murphy1973new,foster1998asymptotic, kleinberg2023u, gopalan2022omnipredictors}, with the optimal online bounds for $\ell_1$ calibration a major open problem \citep{qiao2021stronger}. Recent work of \cite{dagan2024breakingt23barriersequential} improve upon the bounds for $\ell_1$ calibration from \cite{qiao2021stronger}. However, our work focuses on $\ell_2$ calibration, as the $\ell_1$ calibration loss cannot be written as the swap regret in some game. 

\paragraph{Structured games} Our definition of $d$-dimensional structured games is similar in many ways to the definition of convex games (also appearing under the names ``polyhedral games'' or ``polytope games'' in the literature), where both players pick actions in a convex set and obtain utility given by a bilinear function of both players' actions \citep{GordonGreenwaldMarks2008, chakrabarti2024efficient, mansour2022strategizing}. We use the term ``structured game'' throughout this paper to emphasize the fact that when computing the swap regret, the swap functions act on the pure strategies in the associated normal-form game, not on the embeddings (i.e., we think of such a game as a large normal-form game with additional structure). 



\section{Preliminaries}

\subsection{Structured Games and Swap Regret Minimization}

A normal-form game with $n$ actions for the first player (the Learner) and $n'$ actions for the second player (the Adversary) is a $d$-dimensional structured game if:

\begin{itemize}
    \item The utility $u_L(i, j)$ the Learner receives when the Learner plays pure action $i \in [n]$ and the Adversary plays pure action $j \in [n']$ can be expressed in the form $\langle v_i, w_j \rangle$ for some  $v_1, v_2, \dots, v_n \in \mathbb{R}^{d}$ and $w_1, w_2, \dots, w_{n'} \in \mathbb{R}^{d}$.

    \item The utility $u_A(i, j)$ the Adversary receives when the Learner plays pure action $i \in [n]$ and the Adversary plays pure action $j \in [n']$ can be expressed in the form $\langle v'_i, w'_j \rangle$ for some (potentially different) $v'_1, v'_2, \dots, v'_n \in \mathbb{R}^{d}$ and $w'_1, w'_2, \dots, w'_{n'} \in \mathbb{R}^{d}$.
\end{itemize}

In general, most of our results pertaining to structured games focus on minimizing the regret of only the Learner, and so only the structure of the Learner's payoff $u_L$ is relevant (the only result where the utility $u_A$ of the Adversary is also relevant is Corollary \ref{cor:correlated-equilibria}, on computation of correlated equilibria). 

We will write $\cK = \conv(\{v_1, v_2, \dots, v_n\})$ be the convex hull of the embeddings of the Learner's actions, and let $\cK' = \conv(\{w_1, w_2, \dots, w_{n'}\})$ be the convex hull of the embeddings of the Adversary's actions. For any mixed strategy $\bp \in \Delta_{n}$, we will let $\pi(\bp) \in \cK$ be the corresponding embedding provided by $\pi(\bp) = \sum_{i=1}^{n} p_iv_i$; note that this is a surjective map from $\Delta_{n}$ to $\cK$. Likewise, for any $\bq \in \Delta_{n'}$ we will let $\pi'(\bq) = \sum_{i=1}^{n'}q_iw_i \in \cK'$. We will further assume that all embeddings have bounded $\ell_2$-norm; that is, $\norm{\x} \leq 1$ for any $\x \in \cK$ and $\norm{\y} \leq 1$ for any $\y \in \cK'$.

To obtain efficient learning algorithms in structured games from efficient full swap regret minimization algorithms, we will also assume that the Learner has access to an efficient \emph{convex decomposition oracle}, which takes any $x \in \cK$ and returns the value of $\pi^{-1}(x) \in \Delta_n$ (for some arbitrary but fixed inverse map $\pi^{-1}(x)$) in time $\poly(d)$. Note that such oracles generally exist for computationally ``nice'' embeddings. For example, in any case where the Learner possesses a membership oracle for the set $\cK$, the Learner can implement such a convex decomposition oracle by following the standard construction in the proof of Caratheodory's theorem (see e.g. Theorem 6.5.11 in \cite{grotschel2012geometric}). Even when an efficient  oracle does not exist, these operations are implementable in time $O(n, n')$, and so in the worst-case this only adds a $\poly(n, n')$ factor to the run-time.

\paragraph{Regret minimization}
We are concerned in designing learning algorithms for the Learner in cases where a structured game is played repeatedly for $T$ rounds. A learning algorithm $\cA$ for the Learner is a collection of functions $\cA_t$ (one for every $t \in [T]$) that map the first $t-1$ mixed actions of the Adversary $\bq_1, \bq_2, \dots, \bq_{t-1} \in \Delta_{n'}$ to the Learner's mixed action $\bp_{t} \in \Delta_{n}$ at round $t$. In repeated structured games, we will assume that after each round, both players receive not only the action ($\bp$ or $\bq$) played by their opponent, but also the corresponding embedding ($\pi(\bp)$ or $\pi'(\bq)$) of this action\footnote{This assumption is made for convenience of expressing time complexity bounds in situations where $n$ or $n'$ is much bigger than $d$. Without this assumption, the player might have to spend $O(nd)$ or $O(n'd)$ time simply reading the other player's action.}.

We evaluate the performance of a learning algorithm $\cA$ in a repeated game via various worst-case counterfactual regret notions. The main regret notion of interest in this paper is the Learner's swap regret. The swap regret of the learner in a given transcript of the game (expressed via sequences $\bp_1, \bp_2, \dots, \bp_{T}$ and $\bq_1, \bq_2, \dots, \bq_T$ of Learner and Adversary actions) is given by:

$$\SwapReg(\bp_{1:T}, \bq_{1:T}) = \max_{\phi: [n] \rightarrow [n]} \sum_{t=1}^{T}\left(u_L(\phi(\bp_t), \bq_t) - u_L(\bp_t, \bq_t)\right),$$

\noindent
where the swap function $\phi:[n]\rightarrow[n]$ is linearly extended to act on $\bp_t \in \Delta_{n}$.

\subsection{Online Convex Optimization}

\paragraph{Convexity} For a convex set $\cK \subseteq \RR^d$, a loss function  $\ell: \cK \to \RR$ is convex if for all $x \in \cK$ there exists a subgradient $\nabla \ell(x)$ such that $\ell(y)-\ell(x)-\nabla \ell(x)^\top(y-x) \geq 0$ for all $y \in \cK$. Whenever we can strengthen it to a quadratic lower bound, we call the function $\alpha$-strongly-convex: $\ell(y)-\ell(x)-\nabla \ell(x)^\top(y-x) \geq \frac{\alpha}{2}\norm{y-x}^2$  for all $x,y \in \cK$. If the inequality holds in the opposite direction we say that the loss function is $\beta$-smooth:  $\ell(y)-\ell(x)-\nabla \ell(x)^\top(y-x) \leq \frac{\beta}{2}\norm{y-x}^2$.

\paragraph{(Scaled) External Regret Minimization} One important primitive in our algorithms for full swap regret minimization is a variant of the standard problem of minimizing external regret in online convex optimization where the regret in different rounds is scaled by an adversarially provided value. In the scaled version of online learning over a convex set $\cK$ for a family of loss functions $\cL$, the learner chooses an action\footnote{Recall that in the earlier definition of online learning, we allowed the learner to choose a distribution over actions $\x_t \in \Delta(\cK)$. This is essential for swap regret but unnecessary for external regret, since playing $\E[\x_t] \in \cK$ always results in at most the same external regret as playing the randomized strategy $\x_t$.} $x_t \in \cK$ and then an adversary reveals a loss $\ell_t \in \cL$ (as usual) and a scale $g_t \in [0,1]$. The external regret is scaled in each step: 
$\Reg := \max_{x^* \in\cK} \sum_{t=1}^T g_t(\ell_t(x_t) - \ell_t(x^*))$ and we expect the regret bound to be given as a function of $G_T = \sum_t g_t$ instead of $T$.

The standard algorithm for this type of problem is Online Gradient Descent (OGD), which starts at an arbitrary point $x_1 \in \cK$ and updates the estimate according to $x_{t+1} \gets \Pi_\cK \p{x_t - \etat g_t \nabla \lt(x_t)}$ where $\Pi_\cK$ is the $\ell_2$-projection into the convex set $\cK$ and $\eta_t$ is a learning rate to be specified in each variation. The following bounds can be obtained from adapting the standard algorithms to the scaled setting (see \cite{hazan2022introduction} for example). We include a full proof for strongly-convex losses in the appendix for completeness:
\begin{itemize}
\item for convex and $L$-Lipschitz losses, OGD has $\Reg \leq  L \sqrt{2G_T}$ 
\item for $\alpha$-strongly-convex and $L$-Lipschitz losses, OGD has  $\Reg \leq \frac{L^2}{2 \alpha} (\log(G_T+1) + 1)$ (Lemma \ref{lemma:GDS})
\end{itemize}

































      











\subsection{Convex Geometry, Nets, and Triangulations}


In order to get low swap regret, it will be important that our learning algorithms play actions supported over a (relatively small) discretization of the full convex set. In this section we review some preliminaries from computational geometry that will be helpful for establishing these algorithms later.

\begin{definition}[$\epsilon$-net]\label{def:ec}
    For a convex set $\cK \subseteq \RR^d$, define an \emph{$\epsilon$-net} $\Ke$ of $\cK$ to be a finite subset of $\cK$ satisfying, for all $x \in \cK$, there exists a point $\Ke(x) \in \Ke$ such $\norm{x-\Ke(x)} \leq \epsilon$.
\end{definition}

For any bounded, constant diameter, $d$-dimensional convex set $K$, we can find an $\epsilon$-net of $K$ with at most $O(\eps^{-d})$ points.

\begin{lemma}[Chapter 4.2 of \cite{vershynin2018high}]\label{lem:eps-net}
For all convex sets $\cK \subseteq \mathbb{R}^d$ contained in the unit ball, there exists an $O(\epsilon)$-net of $\cK$ with $|\Ke| = O(\epsilon^{-d})$. 
\end{lemma}




For some of our algorithms, we will want a discretization of our set with a stronger guarantee than just being an $\eps$-net; we also want the ability to decompose any point in the original set as a convex combination of nearby points. This motivates the following definition of an $\epsilon$-triangulation.

\begin{definition}[$\epsilon$-triangulation]\label{def:et}
    For a convex set $\cK \subseteq \RR^d$, define an \emph{$\epsilon$-triangulation} $\Ke$ of $\cK$ to be a finite subset of $\cK$ satisfying
    \begin{itemize}
        
        \item \sloppy Every point in $\cK$ is within distance $\eps^2$ of some point in $\conv(\Ke)$ (i.e., $\sup_{x \in \cK} \norm{x - \Pi_{\conv(\Ke)}(x)} \leq \epsilon^2$).
        \item Every point in $\conv(\Ke)$ is contained inside a simplex with vertices in $\Ke$ and diameter at most $2\eps$. That is, for all $x \in \conv(\Ke)$, there exists a subset $\Ke(x) \subseteq \Ke$ such that
        \begin{itemize}
            \item $|\Ke(x)| \leq d+1$
            \item $x \in \conv(\Ke(x))$
            \item For all $s \in \Ke(x)$, $\norm{x-s} \leq \epsilon$
        \end{itemize}
    \end{itemize}
    where $\conv(S)$ refers to the convex hull of $S$.
\end{definition}

\begin{lemma}\label{lemma:triangulation}
    For any convex set $\cK \subseteq \RR^d$ contained within the unit ball, there exists an $O(\epsilon)$-triangulation $\Ke$ of $\cK$ with $|\Ke| = O\p{\sqrt{d}\p{\frac{1}{\epsilon}}^d}$.
\end{lemma}

This follows from a result of \cite{bronshteyn1975approximation} with some modifications. We provide a full proof in Appendix \ref{app:triangulation-proof}.

Lastly, we introduce the notation $\text{dist}(x,\cP)$ to refer to the Euclidean, projective distance of a vector $x \in \R^d$ to a convex set $\cP \subseteq \R^d$.


\section{From Structured Games to Full Swap Regret}

In this section, we will  discuss how to formally reduce the problem of minimizing swap regret in a structured game to the problem of minimizing full swap regret in a related online linear optimization instance (thus establishing Theorem \ref{thm:small-d-intro}). We begin by showing that we can bound swap regret in a $d$-dimensional structured game by the full swap regret of a learning algorithm facing linear losses.

\begin{lemma}\label{lem:structure-to-full}
\sloppy{Assume there is an learning algorithm $\cA$ that guarantees $\FSR \leq R(d, T)$ against any sequence of $T$ $d$-dimensional $O(1)$-Lipschitz \emph{linear} losses with domain $\cK$. Then, for any $d$-dimensional structured game $G$, there exists a learning algorithm $\cA'$ for the Learner which incurs at most $R(d, T)$ swap regret after repeatedly playing $G$ for $T$ rounds. Moreover, if the algorithm $\cA$ has a per-round time complexity of $\tau(d, T)$ and the Learner has access to an efficient convex decomposition oracle, then the learning algorithm $\cA'$ has a per-round time complexity of $\poly(d)\tau(d, T)$.}
\end{lemma}
\begin{proof}
Let $v_1, v_2, \dots, v_n \in \RR^d$ be the $d$-dimensional embeddings of the $n$ pure actions of the Learner, and let $\cK = \conv(\{v_1, v_2, \dots, v_n\})$. Recall that every mixed action $\bp \in \Delta_n$ projects to some point $\pi(\bp) \in \cK$ via $\pi(\bp) = \sum_{i=1}^{n} p_iv_i$ and that this projection map is surjective. Fix any specific inverse map $\pi^{-1}(\x)$ (i.e., a specific way of taking an embedding in $\cK$ and constructing a mixed strategy that corresponds to this embedding). 

Consider now an algorithm $\cA$ for online linear optimization over the set $\cK$. This algorithm produces a sequence of randomized actions $\x_t \in \Delta(\cK)$ in response to a sequence of linear losses $\ell_t \in \cL$. We will use it to construct an algorithm $\cA'$ for learning in the repeated structured game $G$ that works as follows:

\begin{enumerate}
    \item Receive a randomized action $\x_t \in \Delta(\cK)$ from $\cA$.
    \item Play the action $\bp_t = \E_{x_t \sim \x_t}[\pi^{-1}(x_t)] \in \Delta_n$ in the repeated game.
    \item Receive the action $\bq_t \in \Delta_{n'}$ played by the adversary in this round, and its embedding $y_t = \pi'(\bq_t) \in \mathbb{R}^{d}$. 
    \item Define the loss $\ell_t(x) : \cK \rightarrow \mathbb{R}$ via $\ell_t(x) = -\langle x, y_t\rangle$. Pass this loss to $\cA$. 
\end{enumerate}

We want to show that the swap regret incurred by $\cA'$ is at most the full swap regret incurred by $\cA$. To see this, note that for any swap function $\phi: [n] \rightarrow [n]$, the utility $u_L(\phi(\bp_t), \bq_t)$ can be written as

$$u_L(\phi(\bp_t), \bq_t) = -\ell_t(\E_{x_t \sim \x_t}[\pi(\phi(\pi^{-1}(x_t)))]) = -\ell_t(\tilde{\phi}(\x_t)),$$

\noindent
for the function $\tilde{\phi}(x): \cK \rightarrow \cK$ given by $\tilde{\phi}(x) = \pi(\phi(\pi^{-1}(x)))$. It follows that
\begin{align*}
\SwapReg(\bp_{1:T}, \bq_{1:T}) &= \max_{\phi: [n] \rightarrow [n]} \sum_{t=1}^{T}\left(u_L(\phi(\bp_t), \bq_t) - u_L(\bp_t, \bq_t)\right)\\
&\leq \max_{\tilde{\phi}:\cK\rightarrow \cK}\sum_{t=1}^{T} \left(\ell_t(\x_t) - \ell_t(\tilde{\phi}(\x_t))\right) = \FSR(\x_{1:T}, \ell_{1:T}).
\end{align*}

Finally, note that with access to a best response oracle, computing $\pi^{-1}(x_t)$ takes at most $\poly(d)$ time, and so computing $\bp_t$ takes at worst $\poly(d)\tau(d, T)$ time (the per-iteration time complexity of $\cA$ cannot be better than the sparsity of $\x$). Overall, $\cA'$ therefore has time complexity at most $\poly(d)\tau(d, T)$.
\end{proof}

Lemma~\ref{lem:structure-to-full} together with our results on full swap regret minimization (to be established in Section \ref{sec:main_full_swap_regret}) directly implies Theorem \ref{thm:small-d-intro} in the introduction (and in turn, Corollary \ref{cor:correlated-equilibria}). 

\begin{proof}[Proof of Theorem~\ref{thm:small-d-intro}]
Note that by the third row of Table~\ref{table:main-results}, there exists a learning algorithm $\cA$ that guarantees $\FSR \leq \tilde{O}(T^{(d+1)/(d+3)})$ running in per-round time $\poly(d, T)$. By Lemma~\ref{lem:structure-to-full}, this implies an algorithm for swap-regret minimization in $d$-dimensional structured games incurring swap-regret at most $\tilde{O}(T^{(d+1)/(d+3)})$ and running in polynomial time, as desired.
\end{proof}


































\section{Algorithmic Template: Blum-Mansour for Convex Action Spaces}

All of our algorithms for full-swap-regret minimization rely on the core algorithmic idea of \cite{blum2007external}.  That idea is to create an instance of some external-regret minimizing algorithm for each action in the decision space, and at each round, output the stationary distribution of a Markov chain induced by the recommendations of these algorithms.  Unmodified, this algorithm cannot attain full-swap-regret over a convex set $\cK$, as it would require an external-regret minimizing instance for each of the infinitely many points in the set.  The key idea behind all of our full-swap-regret minimizing algorithms is to create $\Ke$: a discretization of the set $\cK$.

Instead of creating an external-regret-minimizing instance for every point in $\cK$, we only create instances for each of the finitely many points in $\Ke$.  For non-convex losses, our approach simply entails running Blum Mansour over this discretization (Algorithm \ref{alg:BMNS} in appendix \ref{appendix:bm}).  For strongly convex and nearly-strongly convex losses (the cases of greatest interest for calibration), we obtain significant improvement using a novel technique (Algorithm \ref{alg:BMCS} in appendix \ref{appendix:bm}).  Namely, at each time step, each external-regret-minimizing instance will produce a recommendation in $\cK$, rather than $\Delta(\Ke)$.  We would like the recommendations to be distributions $\Delta(\Ke)$ in order to induce a Markov chain over $\Ke$.  But, rather than having the external regret subroutines produce such recommendations directly, we have them recommend in $\cK$ and then apply a ``rounding procedure'' $\He: \cK \to \Delta(\Ke)$ that converts the recommendations to the desired form.  












This algorithmic template can be described as follows. For each family of loss functions $\cL$ we will specify:
\begin{itemize}
\item a discretization $\Ke$: this can be an $\eps$-net or an $\eps$-triangulation of $\cK$.
\item a rounding procedure $\He: \cK \to \Delta(\Ke)$ which maps each point in the original set to a distribution of points in the discretization
\item a scaled external-regret-minimizing algorithm $\cA$ for online learning problem on the original set $\cK$ and loss functions in $\cL$.
\end{itemize}

With those components, we can run our main algorithm for full-swap-regret-minimization.  We create an instance $\Alg_s$ of the external-regret algorithm $\cA$ for each $s \in \Ke$. At each round $t$, we first query each $\Alg_s$ to obtain a recommended action $q_{s,t} \in \cK$. Now, we use the rounding procedure to convert $q_{s,t} \in \cK$ into a distribution over discretized actions $\z_{s,t} = H(q_{s,t}) \in \Delta(\Ke)$.  We then build a Markov chain with states corresponding to $\Ke$ where the transition from $s$ to $s'$ is given by $\z_{s,t}[s']$. The learner then plays a stationary distribution $\x_t$ of the Markov chain. After the loss $\ell_t$ is observed, we feed each algorithm $\Alg_s$ the loss $\ell_t$ and scale $g_{t,s} = \x_t[s]$.

Following the Blum-Mansour analysis and bounding the discretization error, we obtain the following:

\begin{theorem}\label{thm:BMCS}
    Say we have a rounding procedure $\He: \cK \to \Delta(\Ke)$ such that, for all $\q \in \cK$, for all $\ell \in \cL$, we have
    $\EE_{\s \sim \He(\q)}\ps{\ell(\s)} - \ell(\q) \leq \delta$
    for some $\delta>0$.  Also let $\Reg_s$ be the scaled external regret incurred by $\Alg_s$, then, for Algorithms \ref{alg:BMCS} and \ref{alg:BMNS}: $\FSR \leq \delta T + \sum_{s \in \Ke} \Reg_s$.
\end{theorem}

See Appendix \ref{appendix:bm} for proof and details.




















































































\section{Full Swap Regret}\label{sec:main_full_swap_regret}

We now instantiate the template to obtain the $\FSR$ guarantees. In Table \ref{table:main-results}, we present a summary of our results, including $\FSR$ guarantees for settings where the loss functions are strongly convex, concave, or not having any convexity/concavity assumption.  Our template is designed for the regime where losses are strongly convex, and later nearly-strongly-convex.  These are the settings of interest for establishing algorithms for calibration, and the settings where we stand to gain the most by utilizing the embedded structure of the actions into Euclidean space.  For the concave and no-assumption settings, we are simply using the Blum Mansour framework over a discretization of the action space as a black box.  We include the results for all these settings in one unified table so as to paint a complete baseline picture of what rates can be attained under this new $\FSR$ benchmark.

For external regret algorithms in the strongly convex and nearly-strongly-convex settings, we will use the variants of online gradient descent in Lemma \ref{lemma:GDS} for strongly convex functions and in Theorem \ref{thm:GDK} for nearly strongly convex (we defer discussion of this latter setting to the next section).

For the discretization and rounding in the non-smooth loss case, we will use either $\epsilon$-nets in Lemma \ref{lem:eps-net} coupled with the rounding $\He: \cK \to \Delta(\Ke)$ that (deterministically) maps a point in $\Ke$ to the nearest point in $\Ke$ (i.e. the projection map). The rounding satisfies the following guarantee that trivially follows from Lipschitzness.

\begin{lemma}\label{lemma:NH}
    For all $q \in \cK$, for all $L$-Lipschitz loss functions $\ell: \cK \to \RR$, letting $\He: \cK \to \Delta(\Ke)$ be the projection map on $\eps$-net $\Ke$, we have
    $\EE_{s \sim \He(q)}\ps{\ell(s)} - \ell(q) \leq L \epsilon$.
\end{lemma}
Note that the expectation in the statement is vacuous since the projection is a deterministic map, but we write to highlight that the template allows for randomized maps.

For the discretization and rounding in the $\beta$-smooth loss case, we will use the $\epsilon$-triangulation in Lemma \ref{lemma:triangulation} together with the rounding procedure $\He: \cK \to \Delta(\Ke)$ that takes $x \in \cK$, projects it to $\conv(\Ke)$ obtaining $x_\perp = \Pi_{\conv(\Ke)}(x)$ and then outputs a probability distribution supported in the points $\Ke(x_\perp)$ that equal $x_\perp$ in expectation. (Recall from Lemma \ref{lemma:triangulation} that in a triangulation $\Ke(x)$ is a set of at most $d+1$ points containing $x$ in the convex hull.) See Algorithm \ref{alg:BH} in the appendix for a detailed description in pseudocode together with the proof of the following lemma. With the stronger smoothness property together with the more structured discretization, we can improve the rounding error from $O(\epsilon)$ to $O(\epsilon^2)$:

\begin{lemma}\label{lemma:BH}
    For all $q \in \cK$, for all $L$-Lipschitz, $\beta$-smooth loss functions $\ell: \cK \to \RR$, letting $\He: \cK \to \Delta(\Ke)$ be the map induced by Algorithm \ref{alg:BH} on $\eps$-triangulation $\Ke$, we have
$\EE_{s \sim \He(q)}\ps{\ell(s)} - \ell(q) \leq \p{L + \beta/8}\eps^2$.
\end{lemma}

For the discretization in the concave loss case, rather than taking an $\eps$-net of the entire set $\cK$, it will only be necessary to take an $\eps$-net of the boundary.  Because losses are concave, the loss-minimizing actions will always lie at extreme points in $\cK$.  It will always be preferable to represent any interior point as a convex combination of boundary points.  This discretization is accomplished by taking a polytope approximation of $\cK$.  Theorem \ref{thm:bi} (Appendix \ref{app:triangulation-proof}) guarantees the existence of a polytope $\mathcal{P}$ contained entirely in $\cK$ with at most $(1/\eps)^{d-1}$ vertices such that $\max_{x \in \cK} \text{dist}(x,\mathcal{P}) \leq \eps^2$.  Taking $\Ke$ to be the vertex set of $\mathcal{P}$, we shave a $(1/\eps)$ factor off the magnitude of $\Ke$, which for the $\eps$-net contained $(1/\eps)^d$ many points as it also had to cover the interior of $\cK$.  Additionally, we maintain the  improved rounding error of $O(\epsilon^2)$ from the $\beta$-smooth case as the projection of any $x \in \cK$ to $\conv(\Ke) = \mathcal{P}$ is guaranteed to be at most $\eps^2$ away from $x$.

Our main theorem is obtained from putting these parts together and optimizing the discretization parameter $\epsilon$. We state the full result below:

\begin{table}\label{table:main-results}
\begin{center}\caption{Matrix of regret bounds for Theorem \ref{thm:full-main}.}
    \begin{tabular}{|l|c|c|c|}
        \hline
        \makecell{Extra assumptions \\ on $L$-Lipschitz \\ losses $\ell$}
        & \makecell{$\FSR$\\ Algorithm \\ and $\ER$\\ Subroutine}
        & \makecell{Discretization\\ and rounding}
        & $\FSR$ Rate\\
        \hline
        $\ell$: no assumption
        &\makecell{Algorithm \ref{alg:BMNS} \\ $\cA$: MWU}
        &\makecell{$\Ke$: $\eps$-net,\\ $\eps = T^{-1/(d+2)}$}
        &$O\p{LT^{\frac{d+1}{d+2}}\log T}$\\
        \hline
        $\ell$: $\beta$-smooth
        &\makecell{Algorithm \ref{alg:BMNS} \\ $\cA$: MWU}
        & \makecell{$\Ke$: $\eps$-triangulation,\\ $\eps = T^{-1/(d+4)}$}
        & $O\p{(L+\beta)T^{\frac{d+2}{d+4}} \log T)}$\\
        \hline
        $\ell$: linear or concave
        &\makecell{Algorithm \ref{alg:BMNS} \\ $\cA$: MWU}
        &\makecell{$\Ke$: polytope\\ approximation, \\ $\eps = T^{-1/(d+3)}$}
        & $O\p{LT^{\frac{d+1}{d+3}}\log T}$\\
        \hline
        $\ell$: $\alpha$-strongly-convex
        &\makecell{Algorithm \ref{alg:BMCS} \\ $\cA$: Algorithm \ref{alg:GDS}}
        &\makecell{$\Ke$: $\eps$-net, \\ $\eps = (L/\alpha)^{\frac{1}{d+1}}T^{-\frac{1}{d+1}}$ \\ $\He$: Projection}
        &$O\p{L\p{\frac{L}{\alpha}}^{\frac{1}{d+1}}T^{\frac{d}{d+1}}\log T}$\\
        \hline
        \makecell{$\ell$: $\alpha$-strongly-convex\\ and $\beta$-smooth}
        &\makecell{Algorithm \ref{alg:BMCS}\\ $\cA$: Algorithm \ref{alg:GDS}}
        &\makecell{$\Ke$: $\eps$-triangulation, \\ $\eps = T^{-1/(d+2)}$ \\ $\He$: Algorithm \ref{alg:BH}}
        &$O\p{L\p{\frac{\beta}{L}+\frac{L}{\alpha}}T^{\frac{d}{d+2}}\log T}$\\
        \hline
    \end{tabular}
\end{center}
\end{table}


































\begin{theorem}\label{thm:full-main}
    Let $\cK \subseteq \RR^d$ be a convex set of diameter 1.  Let $\cL \subseteq \set{\ell: \cK \to \RR}$ be a family of $L$-Lipschitz loss functions.  Algorithms \ref{alg:BMCS} and \ref{alg:BMNS} attain the full-swap-regret guarantees described in Table~\ref{table:main-results} in terms of the indicated additional constraints on the loss functions $\ell \in \cL$, using the indicated subroutines $\cA$, discretizations $\Ke$, and rounding algorithms $\He$. All algorithms run in per-iteration time $\poly(|K^{\eps}|) = (1/\eps)^{O(d)} = \poly(d, T)$.
\end{theorem}














































































































































































    





















































\section{Discretized Swap Regret}\label{sec:discretized_swap_regret}

We now consider a setting where a discrete set of points $\Ke \subset \cK$ is given to the learner, who is then constrained to only play randomized actions supported on this discretization $\Ke$.  Accordingly, the learner must only compete with swap functions $\phi: \Ke \rightarrow \Ke$ rather than all transformations $\phi: \Ke \to \cK$. While this is a special case of classic online learning over finitely many actions $k=\mg{\Ke}$, under certain assumptions about the discretization $\Ke$ and the loss functions $\lt$, this additional structure enables an improved swap regret bound.  As a consequence, we obtain improved bounds for discretized calibration (see Figure \ref{fig:discretized_calibration_regret}).

We will again treat this as a problem over the convex hull $\conv(\Ke)$ and we will round points to a distribution of points in $\Ke$. The main tool we will use is to replace the loss function $\ell_t$ with its piecewise-linearization $\del_t$ which we describe below. All the key definitions make sense for any dimension $d$ but we are only able to bound the performance of the rounding procedure for $d=1$. We leave as an open question how to extend the analysis of the rounding procedure to higher dimensions. We keep the presentation general in the hope that some of those tools will be of independent interest.
















\begin{definition}[Piecewise-linearization]\label{def:piece-lin}
    For a set $\Ke \subseteq \RR^d$ with $\mg{\Ke}=k$, and a convex loss function $\ell: \conv(\Ke) \to \RR$, we define the \emph{piecewise-linearized} loss function $\del_{\Ke}$ to be the lower envelope of the convex hull $\conv \set{(\s,\ell(\s))|\s \in \Ke}$.  Equivalently, for all $\x \in \conv(\Ke)$:
    $\del_{\Ke}(\x) = \min_{v \in \Delta(\Ke); \EE[v] = \x} \inp{v,\mu(\Ke)}$
    where $\mu (\Ke) \in \RR^k$ denotes the vector with entries $\ell(\s)$ for each $\s \in \Ke$.  We abbreviate $\del = \del_{\Ke}$ when $\Ke$ is clear from context.\\
    
    For the special case where $\Ke = \set{s_1,\cdots,s_k} \subset \RR$ with $s_i<s_{i+1}$ for all $i$,  we can simplify this expression.  For all $x \in \conv(\Ke) = [s_1,s_k]$, letting $i(x)\in [k]$ satisfy $\x \in [s_i,s_{i+1}]$, $
        \del(x) = \frac{\ell(s_{i(x)+1})-\ell(s_{i(x)})}{s_{i(x)+1}-s_{i(x)}}(x-s_{i(x)})+\ell(s_{i(x)})$.
\end{definition}

For the case where $d=1$, we utilize the simple rounding procedure:
\begin{equation}\label{eq:nearly-rounding}
    \He(x)[s_{i(x)}] = \frac{s_{i(x)+1}-x}{s_{i(x)+1}-s_{i(x)}}, \quad
    \He(x)[s_{i(x)+1}] = \frac{x-s_{i(x)}}{s_{i(x)+1}-s_{i(x)}}, \quad 
    \He(x)[s_{i}] = 0 \text{ o.w.}
\end{equation}

We will use the following fact about this rounding procedure which is special to $d=1$.

\begin{lemma}\label{lemma:KH}
    Let $\Ke \subset \RR$.  For all $t \in [T]$, for recommendations $x_{t} \in \conv(\Ke)$, and convex loss functions $\ell_{t}: \conv(\Ke) \to \RR$.  We have $\del_t(x_t) = \E_{s \sim H(x_t)}[\ell_t(s)]$.
    
    
    
\end{lemma}

\begin{proof}
    This holds since the mixture $\He(x)$ is supported on the interval $[s_{i(x)},s_{i(x)+1}]$ over which $\del$ is linear.  Thus, $ \del_t(x_t) = \E_{s \sim H(x_t)}[\del_t(s)]= \E_{s \sim H(x_t)}[\ell_t(s)]$.
\end{proof}

For higher dimensions, it is not possible to find a `lossless' rounding before seeing the actual loss function.












Following our algorithmic template, we need external-regret-minimization algorithms for loss functions resulting from piecewise-linearization of $\alpha$-strongly convex functions. Unfortunately they are no longer $\alpha$-strongly convex, but they are still what we call $(\alpha,\epsilon)$-nearly-strongly convex.

\begin{definition}\label{def:NSC}
    Consider a convex set $\cK \subseteq \mathbb{R}^d$.  We say that a continuous function $\ell: \cK \to \RR$ is \textbf{$(\alpha,\epsilon)$-nearly strongly convex} if, for all $\x,\y \in \cK$
    \begin{equation}
        \ell(\y)-\ell(\x)-\nabla \ell(\x)^\top(\y-\x) \geq \frac{\alpha}{2}\p{\norm{\y-\x}-\epsilon}_+^2
    \end{equation}
    where $\p{x}_+ = \max(x,0)$ and $\nabla \ell(\x)$ can be any subgradient of $\x$.
\end{definition}

We establish the following lemma demonstrating that piecewise-linearizing strongly-convex loss functions results in nearly-strongly-convex loss functions (proof deferred to \Cref{appendix:discretized_swap_regret}).

\begin{lemma}\label{lemma:linearized_is_nearly_strongly_convex}
    Let $\Ke = \set{s_1,\cdots,s_k} \subset \RR$ be a set of reals with $s_i<s_{i+1}$ for all $i$.  Let $\Ke$ be an $\eps$-triangulation of $\conv(\Ke)$ (i.e. $s_{i+1}-s_i \leq \eps$ for all $i$). Let $\ell: \conv(\Ke) \to \RR$ be $\alpha$-strongly-convex.  Then, $\del$ is $(\alpha,\epsilon)$-nearly-strongly-convex, where $\del$ is the piecewise-linearized $\ell$ according to Definition \ref{def:piece-lin}.
\end{lemma}

\paragraph{External Regret Minimization for Nearly Strongly Convex Functions}
We have reduced the task of external regret minimization of strongly convex loss functions over a discretization $\Ke$ to external regret minimization of nearly-strongly-convex loss functions over the set $\conv(\Ke)$. However, existing algorithms don't work out of the box. One option would be to pretend the nearly-strongly-convex loss functions are actually strongly convex, and apply Algorithm \ref{alg:GDS}. The total regret of such algorithm would be $O(\log T + \epsilon T)$ where $\epsilon T$ is due to the fact that these nearly-strongly-convex loss functions may be linear over intervals of length $\eps$. Similarly we could treat them as standard convex functions, not taking advantage of the of the strong convexity for distant points, and obtain $O(\sqrt{T})$ bound which is again not sufficient.







A key technical contribution of this work is to show that we can apply OGD with a learning rate schedule that initially decays with $1/t$ and later switches to decay $1/\sqrt{t}$ at the right moment. This allows us to capture the effect of strong-convexity for points that are far enough from the optimum, for which the function behaves like a strongly convex function. At the same time, it allows OGD to treat the function as a standard Lipschitz function for points close to the optimum.

Formally, the scaled version of OCO with losses $\ell_t$ and scales $g_t$, the algorithm will apply the standard OGD update: $\x_{t+1} \gets \Pi_\cK \p{\xt - \etat g_t \nabla \lt(\xt)}$ with the learning rate $\eta_t = R'(G_t)$ where $G_t = \sum_{s=1}^{t} g_s$ for the function:
\begin{equation*}
    R'(x)  = \frac{2}{\alpha} \text{ for }x \in [0,1], \quad
R'(x) = \frac{2}{\alpha x} \text{ for }x \in \ps{1,\p{\frac{\sqrt 2 L}{\alpha \eps}}^2}, \quad
R'(x) = \frac{\sqrt2 \eps}{L\sqrt{x}} \text{ for } x \geq \p{\frac{\sqrt 2 L}{\alpha \eps}}^2
\end{equation*}


\begin{theorem}\label{thm:GDK}
    For $(\alpha,\epsilon)$-nearly-strongly-convex loss functions $\ell_t$ and scale parameters $g_t$, an instance of Online Gradient Descent (Algorithm \ref{alg:GDK} in the appendix) achieves the following scaled external regret guarantee:
\begin{align*}
    \ER \leq 2\sqrt{2}\epsilon L\sqrt{G_T} + \frac{L^2}{\alpha}\p{\log(G_T+1)+1}
\end{align*}
\end{theorem}


\subsection{Swap Regret and Discretized $\ell_2$-calibration}

We now can use the algorithm in Theorem \ref{thm:GDK} together with the lossless rounding procedure in Lemma \ref{lemma:KH} in our algorithmic template to obtain the following bound:

\begin{theorem}\label{thm:disc-main}
    Let $\Ke \subset \RR$ be a set of reals such that $\Ke$ is an $\eps$-net of $\conv(\Ke)$ and $\conv(\Ke)$ has diameter 1.  Let $\ell_t$ be $\alpha$-strongly-convex, $L$-Lipschitz loss functions.  Let $\He$ be the rounding procedure described in \eqref{eq:nearly-rounding}. Let $\cA$ be the external-regret-minimizing subroutine Algorithm \ref{alg:GDK2}.

    Then the $\FSR$ achieved with respect to the discretized set $\Ke$ satisfies:
    \begin{equation*}
        \FSR = O\p{L\sqrt{\eps T}+\frac{L^2}{\alpha \eps}\log(T)}
    \end{equation*}
    For the regime $\frac{1}{\eps} = o(\sqrt{T})$, we improve on Theorem \ref{thm:full-main} for non-smooth losses, and for the regime $\frac{1}{\eps} = o(T^{1/3})$, we improve on Theorem \ref{thm:full-main} for $\beta$-smooth losses. 
\end{theorem}

Theorem \ref{thm:disc-main} can be directly applied to our formulation of $\ell_2$-calibration as swap regret to prove our $O(\max(\sqrt{\eps T}, T^{1/3}))$ regret bound for $\eps$-discretized calibration (Theorem \ref{thm:discrete_calib_bound}). This is deferred to Appendix \ref{app:discrete_calib_bound}.






  






\acks{Princewill Okoroafor is supported by the Linkedin-Cornell CIS Fellowship Grant.}

\bibliography{references}

\appendix
\input{appendix}

\crefalias{section}{appendix} 









\end{document}
