\section{Existence of triangulations}\label{app:triangulation-proof}

In this appendix, we provide a self-contained proof that sufficiently good triangulations exist, establishing Lemma \ref{lemma:triangulation}. Our main tool will be the following theorem of \cite{bronshteyn1975approximation} on approximations of convex sets by polytopes.

\begin{theorem}[\cite{bronshteyn1975approximation}]\label{thm:bi}
Let $\cK$ be any $d$-dimensional convex set contained in the unit ball. Then, for any $\delta < 0.01$, there exists a polytope $\cP$ contained in $\cK$ with at most $O(\sqrt{d}\cdot\delta^{-(d-1)/2})$ vertices such that $\max_{x \in \cK} \dist(x, \cP) \leq \delta$. 
\end{theorem}

To form a good triangulation of $\cK$, we will apply Theorem~\ref{thm:bi} to the epigraph of a quadratic function over $\cK$. The epigraph of a convex function $f: \cK \rightarrow \mathbb{R}$ is defined to be the $(d+1)$-dimensional convex set $\graph(f) = \{(x, y) \mid x \in \cK, y \geq f(x)\}$. The following lemma relates the closest distance between a point and the epigraph of $f$ to the vertical distance between the point and the epigraph of $f$.

\begin{lemma}\label{lem:epigraph-distance}
Let $\cK$ be a $d$-dimensional convex set contained in the unit ball and let $f: \cK \rightarrow \mathbb{R}$ be a convex function with bounded subgradient $\norm{\nabla f(x)} \leq G$. Then given any point $p = (x, y) \in \mathbb{R}^{d+1}$ with $y \leq f(x)$, we have that

$$f(x) - y \leq \sqrt{1+G^2}\dist(p, \graph(f)).$$
\end{lemma}
\begin{proof}
Consider any hyperplane $H$ tangent to $f$ at $x$. This hyperplane separates the point $p$ from the graph of the function $f$, so $\dist(p, H) \leq \dist(p, \graph(f))$. On the other hand, the distance from $p$ to $H$ is given by

$$\dist(p, H) = \frac{f(x) - y}{\sqrt{1 + \norm{\nabla f(x)}^2}}.$$

Since $\norm{\nabla f(x)} \leq G$, the result follows.
\end{proof}

We can now prove our main lemma.

\begin{proof}[Proof of Lemma~\ref{lemma:triangulation}]


We will handle both constraints of the definition of an $\eps$-triangulation by separate applications of Theorem~\ref{thm:bi}. We start with the first constraint (that all elements on the boundary of $\cK$ must be within $\eps^2$ of $\conv(\Ke)$). To do this, note that we can directly apply Theorem~\ref{thm:bi} to the convex set $\cK$ (with parameter $\delta = \eps^2$) to obtain a set $K_1$ of at most $O(\sqrt{d}\eps^{-(d-1)})$ points with the property that $\max_{x \in \cK} \dist(x, \conv(K_1)) \leq \eps^2$. 

This construction is only guaranteed to well approximate the boundary of $\cK$. To approximate the interior of $\cK$ and satisfy the second condition, let $f: \conv(K_1) \rightarrow \mathbb{R}$ be the  convex function $f(x) = \norm{x}^2$. Consider the convex set $\cK' = \graph(f) \cap \{y \leq 1\}$ formed by truncating the epigraph of $f$ for all $y \geq 1$ (note that since $\norm{x} \leq 1$ for $x \in \cK$, this does not remove any of the vertices of the form $(x, f(x))$). Since $f(x) \leq 1$ for $(x,y) \in \cK$, the set $\cK'$ is contained within a ball of radius $\sqrt{2}$, so by applying Theorem~\ref{thm:bi} to a scaled version of $\cK'$ (with parameter $\delta = \eps^2$) we obtain a set $K_2$ containing at most $O(\sqrt{d}\eps^{-d})$ points in $\cK'$ with the property that $\max_{x \in \cK'} \dist(x, \conv(K_2)) \leq 2\eps^2$. Now, without loss of generality we will make two modifications to $K_2$:

\begin{itemize}
    \item First, for each vertex $x \in K_1$, we will assume that $(x, 1) \in K_2$. If not, we can just add all these points to $K_2$ (this doesn't affect the asymptotic number of points in $K_2$).
    \item For all other points $(x, y) \in K_2$, we assume that $y = f(x)$ (i.e., this point lies on the boundary of $K_2$). If not, note that replacing it with $(x, f(x))$ strictly increases the size of $\conv(K_2)$ (in particular, $(x, y)$ is a convex combination of $(x, f(x))$ and $(x, 1)$, and the latter point belongs to $\conv(K_2)$ by the previous bullet).
\end{itemize}

We choose $\Ke$ to be the projection of $K_2$ onto the first $d$ coordinates. To see that this satisfies the second constraint, consider the lower envelope of $\conv(K_2)$; that is, the function $g: \conv(K_1) \rightarrow \mathbb{R}$ defined so that $g(x)$ equals the minimum $y$ such that $(x, y) \in \conv(K_2)$ (for this $g$, $\graph(g) \cap \{y \leq 1\} = \conv(K_2)$). Note that the subgradient of $g$ is bounded by the maximum subgradient of $f$ over $\conv(K_1)$. The gradient $\nabla f(x) = 2\cdot ||x|| \leq 2$, so $\norm{\nabla g(x)} \leq 2$.

Now, consider any $x \in \conv(K_1)$ and let $p = (x, f(x))$. By the guarantees of Theorem~\ref{thm:bi}, $\dist(p, \graph(g)) = \dist(p, \conv(K_2)) \leq \eps^2$. By Lemma~\ref{lem:epigraph-distance}, this implies that $g(x) - f(x) \leq \sqrt{5}\eps^2$. But $(x, g(x))$ also lies on some $d$-dimensional face of $K_2$ and can be written as a convex hull of at most $d+1$ points in $K_2$ of the form $(x_i, f(x_i))$. We will choose $\Ke(x)$ to be this set of $x_i$. Write $x = \sum_i \lambda_i x_i$; then $g(x) = \sum_{i} \lambda_i f(x_i) = \sum_{i} \lambda_i \norm{x_i}^2$. We can now express 

\begin{eqnarray*}
g(x) - f(x) &=& \sum_{i} \lambda_i\norm{x_i}^2 - \norm{x}^2 \\
&=& \sum_{i}\lambda_i\norm{x + (x_i - x)}^2 - \norm{x}^2 \\
&=& \sum_{i}\lambda_i \norm{x_i - x}^2
\end{eqnarray*}

In particular, substituting $x = \frac{1}{2}x_i + \frac{1}{2}x_j$ for $i \neq j$ above and using  $g(x) - f(x) \leq \sqrt{5}\eps^2$, we obtain: $\frac{1}{2}\norm{x_i - x_j}^2 \leq \sqrt{5}\eps^2.$
It follows that for any $x_i, x_j \in \Ke(x)$, $\norm{x_i - x_j} \leq O(\eps)$. It follows that $\diam(\Ke(x)) \leq O(\eps)$, as desired. \end{proof}

\section{Analysis of the Algorithmic Template}\label{appendix:bm}

Here, we detail our main algorithmic templates for swap regret minimization and prove \Cref{thm:BMCS}.  We begin with our algorithm for strongly convex and nearly-strongly-convex losses.  We first re-state the algorithm in pseudocode:

\begin{algorithm}
\caption{Blum Mansour for Convex Sets and Convex losses}
\label{alg:BMCS}

\KwIn{Convex set $\cK \subseteq \mathbb{R}^d$, diameter 1}
\KwIn{Loss family $\cL \subseteq \{\ell: \cK \to \RR\}$}
\KwIn{$\Ke$: discretization of $\cK$}
\KwIn{$\He$: rounding procedure $\cK \to \Delta(\Ke)$}
\KwIn{External-regret-minimizing algorithm $\cA$; outputs to $\cK$}
\KwIn{For all $t \in [T]$: $\lt \in \cL$ revealed sequentially}
\KwOut{For all $t \in [T]$: strategies $\xt \in \Delta(\Ke)$}

\For{$s \in \Ke$}{
    Instantiate copy of $\cA$: $\Alg_s$
}

\For{$t = 1:T$}{
    \For{$s \in \Ke$}{
        $q_{s,t} \gets \Alg_s$ recommendation at time $t$
        
        $\Q_t[s] \gets \He(q_{s,t})$
        
        \tcp{Setting row $s$ of a $\Ke \times \Ke$ row-stochastic matrix}
    }
    $\xt \gets \text{stationary distribution}(\Q_t)$
    
    \Return $\xt$
    
    \textbf{observe} $\lt$
    
    \For{$s \in \Ke$}{
        \textbf{update} $\Alg_s$ with $\lt$ and $g_t \gets \xt[s]$
    }
}

\end{algorithm}






























Note that we pass as input to the black-box external-regret-minimizing instance $\Alg_s$ both the loss $\lt$ as well as the weight $g_t = \xt[s]$ placed on the corresponding action that round.  The loss incurred by this instance will be $g_t\lt=\xt[s]\lt$: the loss scaled by this weight.  We represent this as two separate inputs as it will be notationally convenient later when proving that the instance achieves a ``first-order'' external-regret bound in terms of $\sum g_t$.

Now, we provide pseudocode for our algorithmic template in the event that our losses are non-convex.  Here, we just rely on a black-box implementation of Blum Mansour over a discretization of $\cK$:

\begin{algorithm}
\caption{Blum Mansour for Convex Sets and Non-Convex losses}
\label{alg:BMNS}
\KwIn{Convex set $\cK \subseteq \mathbb{R}^d$, diameter 1}
\KwIn{Loss family $\cL \subseteq \{\ell: \cK \to \RR\}$}
\KwIn{$\Ke$: discretization of $\cK$}
\KwIn{External-regret-minimizing algorithm $\cA$; outputs to $\Delta(\Ke)$}
\KwIn{For all $t \in [T]$: $\lt \in \cL$ revealed sequentially}
\KwOut{For all $t \in [T]$: strategies $\xt \in \Delta(\Ke)$}

\For{$s \in \Ke$}{
    Instantiate copy of $\cA$: $\Alg_s$
}

\For{$t = 1$ \KwTo $T$}{
    \For{$s \in \Ke$}{
        $\vect{q}_{s,t} \gets \Alg_s$ recommendation at time $t$
        
        $\Q_t[s] \gets \vect{q}_{s,t}$
        
        \tcp{Setting row $s$ of a $\Ke \times \Ke$ row-stochastic matrix}
    }
    $\xt \gets \text{stationary distribution}(\Q_t)$
    
    \Return $\xt$
    
    \textbf{observe} $\lt$
    
    \For{$s \in \Ke$}{
        \textbf{update} $\Alg_s$ with $\lt$ and $g_t \gets \xt[s]$
    }
}
\end{algorithm}

We are ready to prove our main theorem about the full-swap-regret achieved by Algorithms \ref{alg:BMCS} and \ref{alg:BMNS}, restated below.  We will later use this theorem as a black box to demonstrate that the algorithm also achieves good discretized swap regret for particular settings of $\cL$.
















\paragraph{\Cref{thm:BMCS}} Say we have a rounding procedure $\He: \cK \to \Delta(\Ke)$ such that, for all $\q \in \cK$, for all $\ell \in \cL$, we have
    $\EE_{\s \sim \He(\q)}\ps{\ell(\s)} - \ell(\q) \leq \delta$
    for some $\delta>0$.  Also let $\Reg_s$ be the scaled external regret incurred by $\Alg_s$, then, for Algorithms \ref{alg:BMCS} and \ref{alg:BMNS}: $\FSR \leq \delta T + \sum_{s \in \Ke} \Reg_s$. 



\begin{proof}[Proof of Theorem \ref{thm:BMCS}]
    First, for Algorithm \ref{alg:BMCS}, we have
    \begin{align*}
        &\FSR\p{\x_{1:T},\ell_{1:T}}\\
        &= \sum_{\s \in \cK} \max_{\phi(\s) \in \cK}\p{\sum_{t=1}^T \x[\s](\lt(\s) - \lt(\phi(\s)))}\\
        &= \sum_{\s \in \Ke} \max_{\phi(\s) \in \cK}\p{\sum_{t=1}^T \x[\s](\lt(\s) - \lt(\phi(\s)))}\\
        &=\sum_{\s \in \Ke} \sum_{t=1}^T \p{\sum_{\s' \in \Ke} \x[\s']\Q[\s'][\s]}\lt(\s) - \p{\sum_{\s \in \Ke} \max_{\phi(\s) \in \cK} \sum_{t=1}^T \x[\s]\lt(\phi(\s))} \\
        &=\sum_{\s' \in \Ke} \sum_{t=1}^T \x[\s']\EE_{\s \sim \He(\q_{\s',t})} \ps{\lt(\s)} - \p{\sum_{\s \in \Ke} \max_{\phi(\s) \in \cK} \sum_{t=1}^T \x[\s]\lt(\phi(\s))} \\
        &\leq \sum_{\s' \in \Ke} \sum_{t=1}^T \x[\s']\p{\lt(\q_{\s',t})+\delta} - \p{\sum_{\s \in \Ke} \max_{\phi(\s) \in \cK} \sum_{t=1}^T \x[\s]\lt(\phi(\s))} \\
        &= \delta T + \sum_{\s \in \Ke} \max_{\phi(\s) \in \cK} \sum_{t=1}^T \p{\x[\s]\lt(\q_{\s,t}) - \x[\s]\lt(\phi(\s))} \\
        &= \delta T + \sum_{\s \in \Ke} \ER\p{\q_{\s,1:T},\x[\s]\ell_{1:T}}
    \end{align*}
    as desired.  For Algorithm \ref{alg:BMNS}, we have
    \begin{align*}
        &\FSR\p{\x_{1:T},\ell_{1:T}}\\
        &= \sum_{\s \in \cK} \max_{\phi(\s) \in \cK}\p{\sum_{t=1}^T \x[\s](\lt(\s) - \lt(\phi(\s)))}\\
        &= \sum_{\s \in \Ke} \max_{\phi(\s) \in \cK}\p{\sum_{t=1}^T \x[\s](\lt(\s) - \lt(\phi(\s)))}\\
        &=\sum_{\s \in \Ke} \sum_{t=1}^T \p{\sum_{\s' \in \Ke} \x[\s']\Q[\s'][\s]}\lt(\s) - \p{\sum_{\s \in \Ke} \max_{\phi(\s) \in \cK} \sum_{t=1}^T \x[\s]\lt(\phi(\s))} \\
        &= \sum_{\s' \in \Ke} \sum_{t=1}^T \x[\s']\lt(\q_{\s',t}) - \p{\sum_{\s \in \Ke} \max_{\phi(\s) \in \cK} \sum_{t=1}^T \x[\s]\lt(\phi(\s))} \\
        &= \sum_{\s \in \Ke} \max_{\phi(\s) \in \cK} \sum_{t=1}^T \p{\x[\s]\lt(\q_{\s,t}) - \x[\s]\lt(\phi(\s))} \\
        &= \sum_{\s \in \Ke} \max_{\phi(\s) \in \cK} \sum_{t=1}^T (\x[\s]\lt(\q_{\s,t}) - \x[\s]\EE_{\s^* \sim \He(\phi(\s))} \ps{\lt(\s^*)}\\
        & \qquad \qquad \qquad \qquad \qquad \qquad \quad+ \x[\s]\EE_{\s^* \sim \He(\phi(\s))} \ps{\lt(\s^*)} - \x[\s]\lt(\phi(\s))) \\
        &\leq \delta T + \sum_{\s \in \Ke} \ER\p{\q_{\s,1:T},\x[\s]\ell_{1:T}}
    \end{align*}
    as desired.
\end{proof}

\section{External Regret Minimization Algorithms}\label{appendix:external_regret}


We present the external-regret-minimizing algorithm $\cA$ for use as a subroutine in Algorithm \ref{alg:BMCS} when the loss functions are $\alpha$-strongly-convex.  As stated previously, this algorithm receives two inputs at each time step $t$: loss function $\lt$ and scale parameter $g_t$.  When evaluating the external regret of these algorithms, we do so in terms of the scaled loss functions $g_t\lt$.  They are input separately merely for notational convenience.  We will derive regret bounds in terms of the first-order quantity $G_T = \sum_{t=1}^T g_t$.

































































































When we assume the loss functions in $\cL$ are $\alpha$-strongly-convex and $L$-Lipschitz, we use the following external-regret-minimizing algorithm. Define
\begin{equation*}
    R'_{\alpha}(x) := \begin{cases}
        \frac{1}{\alpha} & \text{ for }x \in [0,1]\\
        \frac{1}{\alpha x} & \text{ for } x \geq 1
    \end{cases}
\end{equation*}
We abbreviate $R' := R'_{\alpha}$ for convenience.

\begin{algorithm}
\caption{Online Gradient Descent for Strongly Convex Loss}
\label{alg:GDS}
\KwIn{Convex set $\cK \subseteq \mathbb{R}^d$}
\KwIn{For all $t \in [T]$: $\alpha$-strongly-convex, $L$-Lipschitz loss functions $\lt: \cK \to \mathbb{R}$}
\KwIn{For all $t \in [T]$: Scale parameters $g_t \in [0,1]$}
\KwIn{Initial $\x_{1} \in \cK$}
\KwOut{For all $t \in [T]$: strategies $\xt \in \cK$}

$G_0 \gets 0$

\For{$t = 1$ \KwTo $T$}{
    \Return $\xt$
    
    \textbf{observe} $\lt, g_t$
    
    $G_t \gets G_{t-1} + g_t$
    
    $\etat \gets R'(G_t)$
    
    $\x_{t+1} \gets \Pi_\cK \left(\xt - \etat g_t \nabla \lt(\xt)\right)$
}

\end{algorithm}






















\begin{lemma}\label{lemma:GDS}
    Let $\cK \subseteq \RR^d$ be a convex set of diameter $\leq 1$.  Let $$\cL = \set{\ell: \cK \to \RR|\text{$\ell$: $\alpha$-strongly-convex and $L$-Lipschitz}}$$  For all $T$, for all $\ell_{1:T} \in \cL^T$, for all $g_{1:T} \in [0,1]^T$, the strategies $\x_{1:T}$ recommended by Algorithm \ref{alg:GDS} satisfy
    \begin{equation*}
        \ER\p{\x_{1:T},g\ell_{1:T}} \leq \frac{L^2}{2\alpha}\p{\log\p{G_T+1}+1}
    \end{equation*}
\end{lemma}

\begin{proof}

Let $\nabla_t = \nabla \lt(\xt)$.  For all $\x^* \in \cK$, we have $\lt(\xt)-\lt(\x^*) \leq \inp{\nabla_t, \xt-\x^*}-\frac{\alpha}{2}\norm{\xt-\x^*}^2$ by $\alpha$-strong-convexity.  Also,
\begin{align*}
    \norm{\x_{t+1}-\x^*}^2 = \norm{\Pi_\cK(\xt-\etat g_t\nabla_t)-\x^*}^2 &\leq \norm{\xt-\etat g_t\nabla_t-\x^*}^2\\
    &=\norm{\xt-\x^*}^2+\etat^2g_t^2\norm{\nabla_t}^2-2\etat g_t \inp{\nabla_t,\xt-\x^*}
\end{align*}
and therefore
\begin{align*}
    g_t\inp{\nabla_t, \xt-\x^*} &\leq \frac{1}{2\eta_t}\p{\norm{\xt-\x^*}^2-\norm{\x_{t+1}-\x^*}^2}+\frac{\eta_t g_t^2}{2} \norm{\nabla_t}^2\\
    &\leq \frac{1}{2\eta_t}\p{\norm{\xt-\x^*}^2-\norm{\x_{t+1}-\x^*}^2}+\frac{\eta_t g_t^2L^2}{2}
\end{align*}
Summing over $t$
\begin{align*}
    2\sum_{t=1}^T g_t(\lt(\xt)-\lt(\x^*))
    &\leq \sum_{t=1}^T \norm{\xt-\x^*}^2 \p{\frac{1}{\eta_t}-\frac{1}{\eta_{t-1}}-\alpha g_t}+ L^2\sum_{t=1}^T \etat g_t^2
\end{align*}
Since $\frac{1}{\etat} = \alpha \max(G_t,1)$, 
\begin{align*}
    \frac{1}{\eta_t}-\frac{1}{\eta_{t-1}}-\alpha g_t \leq \alpha \p{G_t -G_{t-1} - g_t} = 0
\end{align*}
Additionally,
\begin{align}
    \sum_{t=1}^T \etat g_t^2 &\leq \sum_{t=1}^T R'(G_t)g_t \label{eq:OGDSsmoothness1}\\
    &= \sum_{t=1}^T R'(G_t)(G_t-G_{t-1})\nonumber \\
    &\leq \sum_{t=1}^T \p{R(G_t)-R(G_{t-1})} = R(G_T)\label{eq:OGDSsmoothness2}
\end{align}
where \eqref{eq:OGDSsmoothness1} follows from $g_t \leq 1$ and \eqref{eq:OGDSsmoothness2} follows from the fact that $R(x) = \int_0^x R'(x)dx$ is concave since $\frac{d}{dx}R'(x) \leq 0$.  Thus,
\begin{align*}
    \ER\p{\x_{1:T},\ell_{1:T}} = \sum_{t=1}^T (g_t \lt(\xt)-g_t \lt(\x^*)) &\leq \frac{L^2}{2} R(G_T)\\
    &\leq \frac{L^2}{2\alpha}\p{\log\p{G_T+1}+1}
\end{align*}
as desired.\end{proof}


\section{Omitted proofs and results}

\subsection{General normal-form games as structured games}\label{sec:nfg-as-structured}

Here we prove that any normal-form game with $n$ actions for the first player and $n'$ actions for the second player can be expressed as a $d$-dimensional structured game for $d = \min(n, n')$.

\begin{lemma}\label{lem:nfg-as-structured}
Let $G$ be a normal-form game with $n$ actions for the Learner and $n'$ actions for the Adversary. Then $G$ is a $d$-dimensional structured game for $d = \min(n, n')$.
\end{lemma}
\begin{proof}
By symmetry, it suffices to show that the Learner's payoff matrix $u_L(i, j)$ (denoting the Learner's payoff when they play pure action $i$ and the Adversary plays pure action $j$) can be written in the form $\langle v_i, w_j\rangle$ for some $d$-dimensional vectors $v_i$ and $w_j$. If $\min(n, n') = n$, we can accomplish this by letting $v_{i} = e_{i}$ (the $i$th basis vector) and $w_{j} = \sum_{k=1}^{n} u_{L}(k, j)e_{k}$. Similarly, if $\min(n, n') = n'$, we can accomplish this by letting $w_{j} = e_{j}$ and $v_{i} = \sum_{k=1}^{n'} u_{L}(i, k)e_{k}$.
\end{proof}

\subsection{Equivalence of $\ell_2$ calibration and swap regret}\label{sec:calib_swap_regret}

Here we establish that $\ell_2$ calibration can be viewed as the swap regret in a two-dimensional structured game with infinitely many actions, as well as the full swap regret in a one-dimensional domain with strictly convex losses.

\begin{lemma}[$\ell_2$-Calibration as Swap Regret]\label{lem:calib_swap_regret} For any sequences $\x_1, \x_2, \dots, \x_T \in \Delta([0, 1])$ and $b_1, b_2, \dots, b_T \in \{0, 1\}$, the following three quantities agree:

\begin{enumerate}
    \item The $\ell_2$ calibration error $\Cal(\x_{1:T}, \mathbf{b}_{1:T})$.
    \item The full swap regret $\FSR(\x_{1:T}, \ell_{1:T})$ incurred by playing the actions $\x_t \in \Delta([0, 1])$ against the losses $\ell_t: [0, 1] \rightarrow \mathbb{R}$ given by $\ell_t(x) = (x - b_t)^2$.
    \item The swap regret $\SwapReg(\x_{1:T}, \mathbf{b}_{1:T})$ of the Learner in the repeated two-dimensional structured game where the Learner has pure actions $x \in [0, 1]$ with embeddings $v_x = (2x-1, -x^2)$ and the Adversary has pure actions $b \in \{0, 1\}$ with embeddings $w_{b} = (b, 1)$.
\end{enumerate}
\end{lemma}
\begin{proof}
We first show the equivalence of 1 and 2. Note that $\FSR$ with losses $\ell_t$ can be written as 
\begin{eqnarray*}
\FSR(\x_{1:T},\ell_{1:T}) &=& \sup_{\phi: \cK \to \cK} \sum_{t=1}^T \p{\EE_{p \sim \xt}[\lt(p)] - \EE_{p \sim \xt}[\lt(\phi(p))]}\\
&=&\sup_{\phi: \cK \to \cK} \sum_{t=1}^T \sum_{p\in\cK}\xt[p]\big(\lt(p)-\lt(\phi(p))\big)\\
&=&\sum_{p\in\cK}\sup_{p^*\in\cK} \sum_{t=1}^T \xt[p]\big(\lt(p)-\lt(p^*)\big).
\end{eqnarray*}

For any fixed $p$, from the definition of the loss $\lt$ we have
\begin{eqnarray*}
& &\sum_{t=1}^T \xt[p]\big(\lt(p)-\lt(p^*)\big)\\
&=&\sum_{t=1}^T \xt[p]\big((p-b_t)^2-(p^*-b_t)^2\big)\\
&=&\sum_{t=1}^T \xt[p]\big(-(p^*)^2+2b_tp^*+p^2-2b_tp\big)\\
&=&-\left(\sum_{t=1}^T \xt[p]\right)\left(p^*-\frac{\sum_t b_t\xt[p]}{\sum_t\xt[p]}\right)^2+\left(\sum_{t=1}^T \xt[p]\right)\left(p-\frac{\sum_t b_t\xt[p]}{\sum_t\xt[p]}\right)^2
\end{eqnarray*}
is a quadratic function of $p^*$ with maximum value reached at $p^*=\frac{\sum_t b_t\xt[p]}{\sum_t\xt[p]}$. Therefore, 
\begin{align*}
\FSR(\x_{1:T},\ell_{1:T}) &= \sum_{p\in\cK}\sup_{p^*\in\cK} \sum_{t=1}^T \xt[p]\big(\lt(p)-\lt(p^*)\big)\\
&= \sum_{p\in\cK}\left(\sum_{t=1}^T \xt[p]\right)\left(p-\frac{\sum_t b_t\xt[p]}{\sum_t\xt[p]}\right)^2
\end{align*}
which in turn equals $\Cal(\x_{1:T}, \mathbf{b}_{1:T})$.

To show the equivalence of 2 and 3, note that the utility $u_L(x, b)$ obtained by the learner by playing pure action $x \in [0, 1]$ against $b \in \{0, 1\}$ is given by $\langle v_x, w_{b}\rangle = b(2x-1) - x^2 = -(b-x)^2 = -\ell_t(x)$ (the second equality follows since $b = b^2$ for any $b \in \{0, 1\}$). 
\end{proof}





\subsection{Proof of \Cref{thm:calib_bound}}\label{app:calib-bound-proof}



\begin{proof}
We will be using \Cref{alg:BMCS} and its guarantee from \Cref{thm:full-main} to show the above result. We can do this because $\ell_2$-calibration is a full-swap-regret minimization problem as shown in \Cref{lem:calib_swap_regret}.
To use \Cref{alg:BMCS}, we must present the convex set $\cK$, an $\eps$-triangulation of $\cK$, a rounding procedure, the loss family $\cL$ and an external-regret-minimization algorithm $\cA$. For calibration, the convex set $\cK = [0,1]$ has a natural $\eps$-triangulation - $\{0, \eps, 2\eps \ldots, 1 \}$ since every point $p \in [0,1]$ is in $\conv(\{ \lfloor \frac{p}{\eps} \rfloor \eps, \lfloor \frac{p}{\eps} \rfloor \eps + \eps \})$. The losses $\cL = \{ \ell (y) = (1 - y)^2, \ell (y) = y^2\} $ are $1$-lipschitz, $2$-strongly-convex and $2$-smooth. These properties allow us to use \Cref{alg:GDS} as the external-regret minimization algorithm. Plugging all this into Case 3 of \Cref{thm:full-main} gives the desired full-swap-regret bound. 
\end{proof}

\subsection{Proof of \Cref{thm:discrete_calib_bound}}\label{app:discrete_calib_bound}
\begin{proof}
This theorem follows directly from \Cref{thm:disc-main}.
Using the observations made in the proof of \Cref{thm:calib_bound} about the $\ell_2$-calibration problem i.e $\{0, \eps, 2\eps \ldots, 1 \}$ as an $\eps$-triangulation of $\cK = [0,1]$, the $2$-strongly-convex and $1$-Lipschitz properties of the squared loss, we can plug into \Cref{thm:discrete_calib_bound} to obtain a bound of $O(\sqrt{\eps T} + \frac{1}{\eps} \log T)$. For the regime where $\eps = o(T^{1/3})$, this improves on the guaranteee from \Cref{thm:calib_bound}.
\end{proof}

\subsection{Missing Proofs from Section \ref{sec:main_full_swap_regret}}\label{app:full_swap_regret}

\paragraph{$\beta$-smooth loss} When we assume the loss functions in $\cL$ are $\beta$-smooth, we take $\Ke$ to be an $\eps$-triangulation of $\cK$ (Definition~\ref{def:et}).  We define our rounding procedure $\He$ as follows.

\begin{algorithm}[H]
\caption{Rounding Procedure for $\beta$-Smooth Loss}
\label{alg:BH}
\KwIn{Convex set $\cK \subseteq \mathbb{R}^d$, diameter 1}
\KwIn{$\Ke$: $\eps$-triangulation of $\cK$}
\KwIn{$\x \in \cK$}
\KwOut{$\Hex \in \Delta(\Ke)$}

$\x_\perp \gets \Pi_{\conv(\Ke)}(\x)$ \tcp{Project to polytope}
$S = \{\s_1, \cdots, \s_{d+1}\} \gets \Ke(\x_\perp)$ \tcp{In the context of an $\eps$-triangulation, $\Kex \subseteq \Ke$ with $|\Kex| = d+1$, $\x \in \conv(\Kex)$ and $\norm{\x - \s} \leq \eps$ for all $\s \in \Kex$}
$\mat{M} \gets \begin{bmatrix}
    \s_1[1] & \s_2[1] & \cdots & \s_{d+1}[1] \\
    \s_1[2] & \s_2[2] & \cdots & \s_{d+1}[2] \\
    \vdots  & \vdots  & \ddots & \vdots    \\
    \s_1[d] & \s_2[d] & \cdots & \s_{d+1}[d] \\
    1       & 1       & \cdots & 1
\end{bmatrix}$

$\vect{v} \gets \mat{M}^{-1} \begin{bmatrix}
    \x_\perp\\
    1
\end{bmatrix}$ \tcp{Change of basis: express $\x_{\perp}$ as a linear combination of elements of $S$}
\Return $\Hex[\s] \gets \begin{cases}
    \vect{v}[\s] & \text{for } \s \in S\\
    0 & \text{for } \s \not\in S
\end{cases}$
\end{algorithm}
































In the case where $\x_\perp$ belongs to the measure $0$ set for which $\mg{\Ke(\x_\perp)} < d+1$, we take $\mat{M}$ to have only $\mg{\Ke(\x_\perp)}$ columns and solve for $\vect{v}$: $\mat{M}\vect{v} = \begin{bmatrix}
    \x_\perp\\
    1
\end{bmatrix}$ appropriately. 









\begin{proof}[Proof of Lemma \ref{lemma:BH}]
We have $\EE_{\s \sim \He(\q)}\ps{\s} = \q_\perp = \Pi_{\conv(\Ke)}(\q)$ and we decompose
\begin{equation*}
    \EE_{\s \sim \He(\q)}\ps{\ell(\s)} - \ell(\q) = \p{\EE_{\s \sim \He(\q)}\ps{\ell(\s)} - \ell(\q_\perp)} + \p{\ell(\q_\perp) - \ell(\q)}
\end{equation*}

From the $L$-Lipschitzness of $\ell$ and the definition of an $\eps$-triangulation

\begin{equation}
    \ell(\q_\perp) - \ell(\q) \leq L\norm{\q_\perp - \q} \leq L \eps^2
\end{equation}

From $\beta$-smoothness,
\begin{align*}
    \EE_{\s \sim \He(\q)}\ps{\ell(\s)} - \ell(\q_\perp) \leq \inp{\nabla \ell(\q_\perp),\EE_{\s \sim \He(\q)}\ps{\s} - \q_\perp} + \frac{\beta}{2}\Var(\He(\q)) \leq \frac{\beta \eps^2}{8}
\end{align*}
since $\EE_{\s \sim \He(\q)}\ps{\s} = \q_\perp$ and $\Var(\He(\q)) \leq \frac{\eps^2}{4}$ as $\He(\q)$ is supported on $\Ke(\q)$ with $\diam(\Ke(\q)) \leq \eps$.  Combining gives the desired for $\delta = \p{L + \beta/8}\eps^2$. 
\end{proof}

We restate our main theorems on full swap regret minimization.

\paragraph{\Cref{thm:full-main}}
    Let $\cK \subseteq \RR^d$ be a convex set of diameter 1.  Let $\cL \subseteq \set{\ell: \cK \to \RR}$ be a family of $L$-Lipschitz loss functions.  Algorithm \ref{alg:BMNS} (with $\ER$ subroutine $\cA$ as Multiplicative Weights (MWU) over $\Ke$) attains the full-swap-regret guarantees described in table \ref{table:main-results-1} in terms of the indicated additional constraints on the loss functions $\ell \in \cL$, using the indicated discretizations $\Ke$.  Algorithm \ref{alg:BMCS} (with $\ER$ subroutine $\cA$ as Algorithm \ref{alg:GDS}) attains the full-swap-regret guarantees described in table \ref{table:main-results-2} in terms of the indicated additional constraints on the loss functions $\ell \in \cL$, using the indicated discretizations $\Ke$ and rounding procedures $\He$.

\begin{table}[h]\label{table:main-results-1}
\begin{center}\caption{Matrix of regret bounds of Algorithm \ref{alg:BMNS} for Theorem \ref{thm:full-main}.}
    \begin{tabular}{|l|c|c|}
        \hline
        \makecell{Additional assumptions \\ on $L$-Lipschitz losses $\ell$}
        & Discretization
        & $\FSR$ Rate\\
        \hline
        $\ell$: no assumption
        &$\Ke$: $\eps$-net
        &$O\p{LT^{\frac{d+1}{d+2}}\log T}$\\
        \hline
        $\ell$: $\beta$-smooth
        &$\Ke$: $\eps$-triangulation
        & $O\p{(L+\beta)T^{\frac{d+2}{d+4}} \log T)}$\\
        \hline
        $\ell$: concave
        &\makecell{$\Ke$: polytope\\ approximation}
        & $O\p{LT^{\frac{d+1}{d+3}}\log T}$\\
        \hline
    \end{tabular}
\end{center}
\end{table}

\begin{table}[h]\label{table:main-results-2}
\begin{center}\caption{Matrix of regret bounds of Algorithm \ref{alg:BMCS} Theorem \ref{thm:full-main}.}
    \begin{tabular}{|l|c|c|}
        \hline
        \makecell{Additional assumptions \\ on $L$-Lipschitz losses $\ell$}
        & \makecell{Discretization\\ and rounding}
        & $\FSR$ Rate\\
        \hline
        $\ell$: $\alpha$-strongly-convex
        &\makecell{$\Ke$: $\eps$-net \\ $\He$: Projection}
        &$O\p{L\p{\frac{L}{\alpha}}^{\frac{1}{d+1}}T^{\frac{d}{d+1}}\log T}$\\
        \hline
        \makecell{$\ell$: $\alpha$-strongly-convex\\ and $\beta$-smooth}
        &\makecell{$\Ke$: $\eps$-triangulation \\ $\He$: Algorithm \ref{alg:BH}}
        &$O\p{L\p{\frac{\beta}{L}+\frac{L}{\alpha}}T^{\frac{d}{d+2}}\log T}$\\
        \hline
    \end{tabular}
\end{center}
\end{table}

\begin{proof}[Proof of Theorem \ref{thm:full-main}]
    First, we address the 3 cases of table \ref{table:main-results-1}:

    \begin{itemize}
        \item When $\ell_{1:T}$ are $L$-Lipschitz, and $\Ke$ is an $\eps$-net of $\cK$, Lemma~\ref{lemma:NH} guarantees projection satisfies the precondition of Theorem~\ref{thm:BMCS} with $\delta = L\eps$.
        \item When $\ell_{1:T}$ are $L$-Lipschitz and $\beta$-smooth, and $\Ke$ is an $\eps$-triangulation of $\cK$, Lemma~\ref{lemma:BH} guarantees Algorithm \ref{alg:BH} satisfies the precondition of Theorem~\ref{thm:BMCS} with $\delta = (L+\beta/8)\eps^2$.
        \item When $\ell_{1:T}$ are $L$-Lipschitz and concave, and $\Ke$ is the set of vertices of a polytope approximation of $\cK$, Theorem \ref{thm:bi} guarantees that projection of a boundary point to the polytope and then rounding to a vertex via Algorithm \ref{alg:BH} satisfies the precondition of Theorem~\ref{thm:BMCS} with $\delta = L\eps^2$.
    \end{itemize}
    
We now balance $\eps$ for each of the three cases.
    \paragraph{Case 1: $\ell_{1:T}$ are $L$-Lipschitz}

    For $L$-Lipschitz loss functions $\ell_{1:T}$, Theorem~\ref{thm:BMCS} shows that the strategies $\x_{1:T}$ recommended by Algorithm \ref{alg:BMNS} with $\Ke$: $\eps$-net of $\cK$ and $\cA$: MWU over $\Ke$ satisfy
    \begin{equation*}
        \FSR\p{\x_{1:T},\ell_{1:T}} = O\p{L \eps T + L\sqrt{\frac{T}{\eps^d}d\log(1/\eps)}} = O\p{LT^{\frac{d+1}{d+2}}\log(T)}
    \end{equation*}
    for $\eps = T^{-1/(d+2)}$.

    \paragraph{Case 2: $\ell_{1:T}$ are $L$-Lipschitz, $\beta$-smooth}

    For $L$-Lipschitz, $\beta$-smooth loss functions $\ell_{1:T}$, Theorem~\ref{thm:BMCS} shows that the strategies $\x_{1:T}$ recommended by Algorithm \ref{alg:BMNS} with $\Ke$: $\eps$-triangulation of $\cK$ and $\cA$: MWU over $\Ke$ satisfy
    \begin{equation*}
        \FSR\p{\x_{1:T},\ell_{1:T}} = O\p{(L+\beta) \eps^2 T + L\sqrt{\frac{T}{\eps^d}d\log(1/\eps)}} = O\p{(L+\beta)T^{\frac{d+2}{d+4}}\log(T)}
    \end{equation*}
    for $\eps = T^{-1/(d+4)}$.

    \paragraph{Case 3: $\ell_{1:T}$ are $L$-Lipschitz, concave}

    For $L$-Lipschitz, concave loss functions $\ell_{1:T}$, Theorem~\ref{thm:BMCS} shows that the strategies $\x_{1:T}$ recommended by Algorithm \ref{alg:BMNS} $\Ke$: vertices of polytope approximation of $\cK$ and $\cA$: MWU over $\Ke$ satisfy
    \begin{equation*}
        \FSR\p{\x_{1:T},\ell_{1:T}} = O\p{L \eps^2 T + L\sqrt{\frac{T}{\eps^{d-1}}(d-1)\log(1/\eps)}} = O\p{(L+\beta)T^{\frac{d+1}{d+3}}\log(T)}
    \end{equation*}
    for $\eps = T^{-1/(d+3)}$.

    Now, we address the 2 cases of table \ref{table:main-results-2} in turn:

    \begin{itemize}
        \item When $\ell_{1:T}$ are $L$-Lipschitz, and $\Ke$ is an $\eps$-net of $\cK$, Lemma~\ref{lemma:NH} guarantees $\He$ satisfies the precondition of Theorem~\ref{thm:BMCS} with $\delta = L\eps$.
        \item When $\ell_{1:T}$ are $L$-Lipschitz and $\beta$-smooth, and $\Ke$ is an $\eps$-triangulation of $\cK$, Lemma~\ref{lemma:NH} guarantees $\He$ satisfies the precondition of Theorem~\ref{thm:BMCS} with $\delta = (L+\beta/8)\eps^2$.
        \item When $\ell_{1:T}$ are $\alpha$-strongly-convex and $L$-Lipschitz, Lemma~\ref{lemma:GDS} shows that each external-regret-minimizing instance $\Alg_s$: \ref{alg:GDS} satisfies $\ER(\Alg_s, \x[s]\ell_{1:T}) \leq \frac{L^2}{2\alpha}\p{\log\p{1+\sum_{t=1}^T \xt[s]}+1}$.  Thus, Lemma~\ref{lemma:triangulation} gives \begin{align*}
            \sum_{s \in \Ke} \ER(\Alg_s, \x[s]\ell_{1:T}) & \leq \frac{L^2}{2\alpha}\sum_{s \in \Ke} \p{\log\p{1+\sum_{t=1}^T \xt[s]}+1}\\
            &= O\p{\frac{L^2 \log(T)}{\alpha \eps^d}}\\
        \end{align*}
    \end{itemize}
    

    We now balance $\eps$ for each of the two cases.
    
    \paragraph{Case 1: $\ell_{1:T}$ are $L$-Lipschitz, $\alpha$-strongly-convex}

    For $L$-Lipschitz and $\alpha$-strongly-convex loss functions $\ell_{1:T}$, Theorem~\ref{thm:BMCS} shows that the strategies $\x_{1:T}$ recommended by Algorithm \ref{alg:BMCS} with $\Ke$: $\eps$-net of $\cK$, $\cA$: Algorithm \ref{alg:GDS}, and $\He$: projection satisfy
    \begin{equation*}
        \FSR\p{\x_{1:T},\ell_{1:T}} = O\p{L \eps T + \frac{L^2 \log(T)}{\alpha \eps^d}} = O\p{L\p{\frac{L}{\alpha}}^{\frac{1}{d+1}}T^{\frac{d}{d+1}}\log(T)}
    \end{equation*}
    for $\eps = L^{1/(d+1)}\alpha^{-1/(d+1)}T^{-1/(d+2)}$.

    \paragraph{Case 2: $\ell_{1:T}$ are $L$-Lipschitz, $\alpha$-strongly-convex, $\beta$-smooth}

    For $L$-Lipschitz, $\alpha$-strongly-convex, and $\beta$-smooth loss functions $\ell_{1:T}$, Theorem~\ref{thm:BMCS} shows that the strategies $\x_{1:T}$ recommended by Algorithm \ref{alg:BMCS} with $\Ke$: $\eps$-net of $\cK$, $\cA$: Algorithm \ref{alg:GDS}, and $\He$: Algorithm \ref{alg:BH} satisfy
    \begin{equation*}
        \FSR\p{\x_{1:T},\ell_{1:T}} = O\p{(L+\beta) \eps^2 T + \frac{L^2 \log(T)}{\alpha \eps^d}} = O\p{L\p{\frac{\beta}{L}+\frac{L}{\alpha}}T^{\frac{d}{d+2}}\log(T)}
    \end{equation*}
    for $\eps = T^{-1/(d+2)}$.  
\end{proof}


\subsection{Missing Proofs from Section \ref{sec:discretized_swap_regret}}\label{appendix:discretized_swap_regret}

We restate the definition of nearly-strongly-convex (\Cref{def:NSC}).  Consider a convex set $\cK \subseteq \mathbb{R}^d$.  We say that a continuous function $\ell: \cK \to \RR$ is \textbf{$(\alpha,\epsilon)$-nearly strongly convex} if, for all $\x,\y \in \cK$
    \begin{equation}
        \ell(\y)-\ell(\x)-\nabla \ell(\x)^\top(\y-\x) \geq \frac{\alpha}{2}\p{\norm{\y-\x}-\epsilon}_+^2
    \end{equation}
    where $\p{x}_+ = \max(x,0)$ and $\nabla \ell(\x)$ can be any subgradient of $\x$.

We also restate \Cref{def:piece-lin}.    For a set $\Ke \subseteq \RR^d$ with $\mg{\Ke}=k$, and a convex loss function $\ell: \conv(\Ke) \to \RR$, we define the \emph{piecewise-linearized} loss function $\del_{\Ke}$ to be the lower envelope of the convex hull $\conv \set{(\s,\ell(\s))|\s \in \Ke}$.  Equivalently, for all $\x \in \conv(\Ke)$:
    $\del_{\Ke}(\x) = \min_{v \in \Delta(\Ke); \EE[v] = \x} \inp{v,\ell(\Ke)}$
    where $\ell(\Ke) \in (\RR^d)^k$ denotes the vector with entries $\ell(\s)$ for each $\s \in \Ke$.  We abbreviate $\del = \del_{\Ke}$ when $\Ke$ is clear from context.\\
    
    For the special case where $\Ke = \set{s_1,\cdots,s_k} \subset \RR$ with $s_i<s_{i+1}$ for all $i$,  we can simplify this expression.  For all $x \in \conv(\Ke) = [s_1,s_k]$, letting $i(x)\in [k]$ satisfy $\x \in [s_i,s_{i+1}]$, $
        \del(x) = \frac{\ell(s_{i(x)+1})-\ell(s_{i(x)})}{s_{i(x)+1}-s_{i(x)}}(x-s_{i(x)})+\ell(s_{i(x)})$.

Now, we prove

\paragraph{\Cref{lemma:linearized_is_nearly_strongly_convex}} Let $\Ke = \set{s_1,\cdots,s_k} \subset \RR$ be a set of reals with $s_i<s_{i+1}$ for all $i$.  Let $\Ke$ be an $\eps$-triangulation of $\conv(\Ke)$ (i.e. $s_{i+1}-s_i \leq \eps$ for all $i$). Let $\ell: \conv(\Ke) \to \RR$ be $\alpha$-strongly-convex.  Then, $\del$ is $(\alpha,\epsilon)$-nearly-strongly-convex, where $\del$ is the piecewise-linearized $\ell$ according to Definition \ref{def:piece-lin}.

\begin{proof}[Proof of Lemma \ref{lemma:linearized_is_nearly_strongly_convex}]
    For all $x \in \conv(\Ke)$, we have $\del(x) \geq \ell(x)$ since, for all $v \in \Delta(\Ke)$ with $\EE[v] = x$, we have $\EE[\ell(v)] \geq \ell(x)$.

    
    
    
    
    
    
    

    We want to show, for all $x,y \in \conv(\Ke)$,
    \begin{equation*}
        \del(y)-\del(x)-\nabla \del(x)(y-x) \geq \frac{\alpha}{2}\p{\mg{y-x}-\epsilon}_+^2
    \end{equation*}
    We have $\del(y)-\del(x)-\nabla \del(x)(y-x) \geq 0$ for all $x,y$ because $\del$ is convex as it is defined to be the lower envelope of $\conv((s_i,\ell(s_i))_{i \in [k]})$.  Thus, it suffices to show 
    \begin{equation*}
        \del(y)-\del(x)-\nabla \del(x)(y-x) \geq \frac{\alpha}{2}\p{\mg{y-x}-\epsilon}^2
    \end{equation*}
    for all $\mg{y-x} \geq \eps$.  Assume without loss of generality that $y \geq x +\eps$.

    By the $\alpha$-strong-convexity of $\ell$,
    \begin{align*}
        \del(y)\geq \ell(y) &\geq \ell(x+\eps)+\nabla \ell(x+\eps)(y-x-\eps) + \frac{\alpha}{2}\mg{y-x-\eps}^2
    \end{align*}

    It suffices to show $\ell(x+\eps)+\nabla \ell(x+\eps)(y-x-\eps) \geq \del(x)+\nabla \del(x)(y-x)$, and we will show
    \begin{enumerate}
        \item $\ell(x+\eps) \geq \del(x)+\eps \nabla \del(x)$
        \item $\p{\nabla \ell(x+\eps)-\nabla \del(x)}(y-x-\eps)\geq 0$
    \end{enumerate}

    Let $i\in[k]$ be such that $x \in [s_i,s_{i+1}]$.  Since $s_{i+1}-s_i \leq \eps$, we have $x + \eps \geq s_{i+1}$.

    1. holds because, for all $x$ belonging to the interval $[s_i,s_{i+1}]$, we have $\del(x)= \frac{\ell(s_{i+1})-\ell(s_{i})}{s_{i+1}-s_{i}}(x-s_i)+\ell(s_i)$, and $(x+\eps,\ell(x+\eps))$ lies above this line between the points $(s_i,\ell(s_i))$ and $(s_{i+1},\ell(s_{i+1}))$ due to the convexity of $\ell$ and the fact that $x+\eps \geq s_{i+1}$.
    
    2. holds because, by the mean value theorem, there exists $x^* \in [s_i,s_{i+1}]$ with $\nabla \ell(x^*) = \frac{\ell(s_{i+1})-\ell(s_{i})}{s_{i+1}-s_{i}}$.  Since $x+\eps \geq x^*$, we have $\nabla \ell(x+\eps) \geq \nabla \ell(x^*) = \nabla \del(x)$.
    
    
    
    
    
    
    
    
    
    
    
    
\end{proof}


\subsection{External Regret Minimization for Nearly Strongly Convex Functions}

Algorithm \ref{alg:GDK}: an interpolation between standard OGD for convex functions and \ref{alg:GDS} that attains $O(\eps \sqrt{T} + \log T)$ external-regret, bridging the parameter regime gap for intermediate values of $\eps$.  Our algorithm achieves this guarantee without any dimensionality assumption.

\paragraph{The Algorithm} Define
\begin{equation*}
    R'_{\alpha,c}(x) := \begin{cases}
        \frac{2}{\alpha} & \text{ for }x \in [0,1]\\
        \frac{2}{\alpha x} & \text{ for }x \in \ps{1,\p{\frac{2}{\alpha c}}^2}\\
        \frac{c}{\sqrt{x}} & \text{ for } x \geq \p{\frac{2}{\alpha c}}^2
    \end{cases}
\end{equation*}
We abbreviate $R' := R'_{\alpha,c}$ for convenience.

\begin{algorithm}
\caption{Online Gradient Descent for Nearly Strongly Convex Loss}
\label{alg:GDK}
\KwIn{Convex set $\cK \subseteq \mathbb{R}^d$}
\KwIn{Parameters $\alpha, \epsilon, L$}
\KwIn{For all $t \in [T]$: $(\alpha, \epsilon)$-nearly-strongly-convex, $L$-Lipschitz loss functions $\lt: \cK \to \mathbb{R}$}
\KwIn{For all $t \in [T]$: Scale parameters $g_t \in [0,1]$}
\KwIn{Initial $\x_{1} \in \cK$}
\KwOut{For all $t \in [T]$: strategies $\xt \in \cK$}

$c \gets \frac{\sqrt{2} \epsilon}{L}$

$G_0 \gets 0$

\For{$t = 1$ \KwTo $T$}{
    \Return $\xt$
    
    \textbf{observe} $\lt, g_t$
    
    $G_t \gets G_{t-1} + g_t$
    
    $\etat \gets R'_{\alpha,c}(G_t)$
    
    $\x_{t+1} \gets \Pi_\cK \left(\xt - \etat g_t \nabla \lt(\xt)\right)$
}

\end{algorithm}






























\paragraph{\Cref{thm:GDK}}
    For $(\alpha,\epsilon)$-nearly-strongly-convex loss functions $\ell_t$ and scale parameters $g_t$, an instance of Online Gradient Descent (Algorithm \ref{alg:GDK} in the appendix) achieves the following scaled external regret guarantee:
\begin{align*}
    \ER \leq 2\sqrt{2}\epsilon L\sqrt{G_T} + \frac{L^2}{\alpha}\p{\log(G_T+1)+1}
\end{align*}

\begin{proof}[Proof of Theorem \ref{thm:GDK}]
Let $\nabla_t = \nabla \lt(\xt)$
    \begin{align}
        2(\lt(\xt)-\lt(\x^*)) &\leq 2\inp{\nabla_t, \xt-\x^*}-\alpha\p{\norm{\xt-\x^*}-\epsilon}_+^2 \nonumber\\
        &\leq 2\inp{\nabla_t, \xt-\x^*}-\frac{\alpha}{2}\norm{\xt-\x^*}^2 \1\ps{\norm{\xt-\x^*} \geq 2\epsilon} \label{eq:kindcon}
    \end{align}
Also
\begin{align*}
    \norm{\x_{t+1}-\x^*}^2 = \norm{\Pi_\cK(\xt-\etat g_t\nabla_t)-\x^*}^2 &\leq \norm{\xt-\etat g_t\nabla_t-\x^*}^2\\
    &=\norm{\xt-\x^*}^2+\etat^2g_t^2\norm{\nabla_t}^2-2\etat g_t \inp{\nabla_t,\xt-\x^*}
\end{align*}
and therefore
\begin{align}
    g_t\inp{\nabla_t, \xt-\x^*} &\leq \frac{1}{2\eta_t}\p{\norm{\xt-\x^*}^2-\norm{\x_{t+1}-\x^*}^2}+\frac{\eta_t g_t^2}{2} \norm{\nabla_t}^2 \nonumber \\
    &\leq \frac{1}{2\eta_t}\p{\norm{\xt-\x^*}^2-\norm{\x_{t+1}-\x^*}^2}+\frac{\eta_t g_t^2L^2}{2}\label{eq:projgrad}
\end{align}

Combining \eqref{eq:kindcon} and \eqref{eq:projgrad}, and summing over $t$

\begin{align*}
    &2\sum_{t=1}^T g_t(\lt(\xt)-\lt(\x^*))\\
    &\leq \sum_{t=1}^T \norm{\xt-\x^*}^2 \p{\frac{1}{\eta_t}-\frac{1}{\eta_{t-1}} - \frac{\alpha g_t}{2}\mathbf{1}\ps{\norm{\xt-\x^*} \geq 2 \epsilon}} + L^2\sum_{t=1}^T \etat g_t^2
\end{align*}

Since $\frac{d}{dx} \frac{1}{R'(x)} \leq \frac{\alpha}{2}$ for all $x$, we have $\frac{1}{\eta_t}-\frac{1}{\eta_{t-1}} = \frac{1}{R'(G_t)}-\frac{1}{R'(G_{t-1})} \leq \frac{\alpha g_t}{2}$ for all $t$.  Thus,

\begin{align*}
    \sum_{t=1}^T \norm{\xt-\x^*}^2 \p{\frac{1}{\eta_t}-\frac{1}{\eta_{t-1}} - \frac{\alpha g_t}{2}\mathbf{1}\ps{\norm{\xt-\x^*} \geq 2 \epsilon}}
    &\leq 4\epsilon^2 \sum_{t=1}^T \p{\frac{1}{\eta_t}-\frac{1}{\eta_{t-1}}}\\
    &= 4\epsilon^2\p{\frac{1}{\eta_{T}}-\frac{1}{\eta_0}}\\
    &= 4\epsilon^2\p{\frac{1}{R'(G_T)}-\frac{\alpha}{2}}
\end{align*}

Additionally,

\begin{align}
    \sum_{t=1}^T \etat g_t^2 &\leq \sum_{t=1}^T R'(G_t)g_t \label{eq:OGDsmoothness1}\\
    &= \sum_{t=1}^T R'(G_t)(G_t-G_{t-1})\nonumber \\
    &\leq \sum_{t=1}^T \p{R(G_t)-R(G_{t-1})} = R(G_T)\label{eq:OGDsmoothness2}
\end{align}

where \eqref{eq:OGDsmoothness1} follows from $g_t \leq 1$ and \eqref{eq:OGDsmoothness2} follows from the fact that $R(x) = \int_0^x R'(x)dx$ is concave since $\frac{d}{dx}R'(x) \leq 0$.  Thus,

\begin{align*}
    \ER\p{\x_{1:T},\ell_{1:T}} = \sum_{t=1}^T (g_t \lt(\xt)-g_t \lt(\x^*)) \leq 2\epsilon^2\p{\frac{1}{R'(G_T)}-\frac{\alpha}{2}} + \frac{L^2}{2} R(G_T)
\end{align*}

We have

\begin{align*}
    R(x) &= \begin{cases}
        \frac{2}{\alpha}x & \text{ for }x \in [0,1]\\
        \frac{2}{\alpha}\p{1+\log(x)} & \text{ for }x \in \ps{1,\p{\frac{2}{\alpha c}}^2}\\
        \frac{2}{\alpha}\p{1+2\log\p{\frac{2}{\alpha c}}} + 2c\p{\sqrt{x}-\frac{2}{\alpha c}} & \text{ for } x \geq \p{\frac{2}{\alpha c}}^2
    \end{cases}\\
    &= \begin{cases}
        \frac{2}{\alpha}x & \text{ for }x \in [0,1]\\
        \frac{2}{\alpha}\p{1+\log(x)} & \text{ for }x \in \ps{1,\p{\frac{2}{\alpha c}}^2}\\
        2c\sqrt{x} + \frac{2}{\alpha}\p{2\log\p{\frac{2}{\alpha c}}-1}& \text{ for } x \geq \p{\frac{2}{\alpha c}}^2
    \end{cases}
\end{align*}

Thus,

\begin{align*}
    \ER\p{\x_{1:T},\ell_{1:T}} \leq \begin{cases}
        \frac{L^2}{\alpha}G_T & \text{ for }G_T \in [0,1]\\
        \alpha\epsilon^2G_T+\frac{L^2}{\alpha}\p{1+\log(G_T)} & \text{ for }G_T \in \ps{1,\p{\frac{2}{\alpha c}}^2}\\
        \frac{2\epsilon^2\sqrt{G_T}}{c}+ L^2c\sqrt{G_T} + \frac{L^2}{\alpha}\p{2\log\p{\frac{2}{\alpha c}}-1}& \text{ for } G_T \geq \p{\frac{2}{\alpha c}}^2
    \end{cases}
\end{align*}

Setting $c = \frac{\sqrt{2}\epsilon}{L}$,

\begin{align*}
    \ER\p{\x_{1:T},\ell_{1:T}} &\leq \begin{cases}
        \frac{L^2}{\alpha}G_T & \text{ for }G_T \in [0,1]\\
        \alpha\epsilon^2G_T+\frac{L^2}{\alpha}\p{1+\log(G_T)} & \text{ for }G_T \in \ps{1,\frac{2L^2}{\alpha^2 \epsilon^2}}\\
        2\sqrt{2}\epsilon L\sqrt{G_T} + \frac{L^2}{\alpha}\p{2\log\p{\frac{\sqrt{2}L}{\alpha \epsilon}}-1}& \text{ for } G_T \geq \frac{2L^2}{\alpha^2 \epsilon^2}
    \end{cases}\\
    & \leq 2\sqrt{2}\epsilon L\sqrt{G_T} + \frac{L^2}{\alpha}\p{\log(G_T+1)+1}
\end{align*}

since, in the second case, $\alpha\epsilon^2G_T \leq \sqrt{2}\epsilon L\sqrt{G_T}$ for $G_T \leq \frac{2L^2}{\alpha^2 \epsilon^2}$.


\end{proof}



We can wrap Algorithm \ref{alg:GDK} with the piecewise-linearizing subroutine (Definition \ref{def:piece-lin}) and get the following external-regret-minimizing algorithm, for use as a subroutine in Algorithm \ref{alg:BMCS}.

\begin{algorithm}
\caption{Online Gradient Descent for Discretized Regret on Strongly Convex Loss}
\label{alg:GDK2}
\KwIn{$\Ke \subset \mathbb{R}$}
\KwIn{Piecewise-linearize sub-routine $P: \{\conv(\Ke) \to \mathbb{R}\} \to \{\conv(\Ke) \to \mathbb{R}\}$ (Definition \ref{def:piece-lin})}
\KwIn{For all $t \in [T]$: $\alpha$-strongly-convex, $L$-Lipschitz loss functions $\lt: \conv(\Ke) \to \mathbb{R}$}
\KwIn{For all $t \in [T]$: Scale parameters $g_t \in [0,1]$}
\KwOut{For all $t \in [T]$: strategies $\xt \in \conv(\Ke)$}

$\cK \gets \conv(\Ke)$

\textbf{Initialize} $\Alg(\cK)$ (Algorithm \ref{alg:GDK})

\For{$t = 1$ \KwTo $T$}{
    \Return $\xt \gets \mathtt{Alg.recommend}()$
    
    \textbf{observe} $\lt, g_t$
    
    $\del_t \gets P(\ell_t)$
    
    $\mathtt{Alg.update}(\del_t, g_t)$
}

\end{algorithm}





















\paragraph{\Cref{thm:disc-main}}
    Let $\Ke \subset \RR$ be a set of reals such that $\Ke$ is an $\eps$-triangulation of $\conv(\Ke)$ and $\conv(\Ke)$ has diameter 1.  Let $\ell_t$ be $\alpha$-strongly-convex, $L$-Lipschitz loss functions.  Let $\He$ be rounding procedure \eqref{eq:nearly-rounding}. Let $\cA$ be the external-regret-minimizing subroutine Algorithm \ref{alg:GDK2}. Then the $\FSR$ achieved with respect to the discretized set $\Ke$ satisfies:
    \begin{equation*}
        \FSR = O\p{L\sqrt{\eps T}+\frac{L^2}{\alpha \eps}\log(T)}
    \end{equation*}



\begin{proof}[Proof of Theorem \ref{thm:disc-main}]
    Lemma \ref{lemma:KH} guarantees that, for any $\ell_{1:T}$, the rounding procedure $\He$ incurs no rounding error for discretized regret.  That is, $\He$ satisfies the precondition of Theorem~\ref{thm:BMCS} with $\delta = 0$. When $\ell_{1:T}$ are $\alpha$-strongly-convex and $L$-Lipschitz, Lemma \ref{lemma:KH} shows that each external-regret-minimizing instance Algorithm \ref{alg:GDK2} incurs the same external-regret as its corresponding subroutine Algorithm \ref{alg:GDK} on the piecewise-linearized losses.  Theorem~\ref{thm:GDK} shows each Algorithm \ref{alg:GDK} subroutine $\Alg_s$ satisfies $\ER(\Alg_s, \x[s]\del_{1:T}) \leq 2\sqrt{2}\epsilon L\sqrt{G_T} + \frac{2eL^2}{\alpha}\log\p{T}$.  Thus, Lemma~\ref{lemma:triangulation} gives \begin{align*}
        \sum_{s \in \Ke} \ER(\Alg_s, \x[s]\ell_{1:T}) & \leq 2\sqrt{2}L \eps \sum_{s \in \Ke}\sqrt{\sum_{t=1}^T \xt[s]}+ \frac{2eL^2}{\alpha}\sum_{s \in \Ke} \log(T)\\
        &= O\p{L\eps\sqrt{\p{\frac{1}{\eps}}\sum_{s \in \Ke}\sum_{t=1}^T \xt[s]} +\frac{L^2}{\alpha\eps}\log(T)}\\
        &= O\p{L\sqrt{\eps T}+\frac{L^2}{\alpha \eps}\log(T)}
    \end{align*}
    as desired.
\end{proof}

