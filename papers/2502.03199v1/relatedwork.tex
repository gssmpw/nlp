\section{Related Work}
Recently, various methods have been proposed to improve LLM's generating factuality to mitigate hallucinations. These include, but are not limited to, supervised fine-tuning with high-quality data \cite{tian2023fineSFT,zhou2024limaSFT}, reinforcement learning with truthful preference pairs \cite{sun2023aligningRL,yang2023alignmentRL}, retrieval-augmented generation that integrates external knowledge \cite{chern2023factoolRAG},and editing knowledge-related inner representations or parameter-efficient modules \cite{zhang2024truthx,hu2024separateLORA}.


Our research focuses on the field of constraint decoding, which involves applying intervention strategies during model's generation process.
Notably, Inference-Time Intervention (ITI) \cite{li2024inferenceITI} employs probes to locate truthfulness signals within attention heads, while Repe \cite{zou2023representationZOU} locates those within critical layers, then editing on the direction of truthfulness to modify model decoding.
Contrast Decoding (CD) \cite{li2022contrastiveCD} and later Induced-then-Contrast Decoding (ICD) \cite{zhang2023alleviatingICD} contrasts logits from an expert model against those from a weak model, amplifying the knowledge reflected in their differences.
Activation Decoding \cite{chen2024activation} leverages the correlation between context activation sharpness and answer correctness,
incorporating in-context entropy into decoding to improve factuality.


The most relevant work to ours is  DoLA \cite{chuang2023dola}, which selects a single, most distinct layer to contrast with the final layer, amplifying the factual knowledge boosted in higher layers.
However, the change of inner predictions varies by candidate tokens, which means that, at a given generation step, factual tokens may exhibit different growing trends. Therefore, selecting a single caliber layer for all tokens is not accurate and can lead to false negative and false positive problems. 
Unlikely, we propose to process the prediction changes across layers individually for each token. By quantifying their growing trend, we can leverage internal information more accurately to enhance the factuality of generation.


% Decoding method cannot inject extra knowledge into LLMs, it just amplifies model’s inner knowledge to improve next-token predictions and prevent erroneous output. We can make models correctly express what they know while those models don’t know they still don’t know. Therefore, the hallucination cause by information lacking and outdated data cannot be solved in this scope.