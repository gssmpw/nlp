\section{Related Work}
Recently, various methods have been proposed to improve LLM's generating factuality to mitigate hallucinations. These include, but are not limited to, supervised fine-tuning with high-quality data ____, reinforcement learning with truthful preference pairs ____, retrieval-augmented generation that integrates external knowledge ____,and editing knowledge-related inner representations or parameter-efficient modules ____.


Our research focuses on the field of constraint decoding, which involves applying intervention strategies during model's generation process.
Notably, Inference-Time Intervention (ITI) ____ employs probes to locate truthfulness signals within attention heads, while Repe ____ locates those within critical layers, then editing on the direction of truthfulness to modify model decoding.
Contrast Decoding (CD) ____ and later Induced-then-Contrast Decoding (ICD) ____ contrasts logits from an expert model against those from a weak model, amplifying the knowledge reflected in their differences.
Activation Decoding ____ leverages the correlation between context activation sharpness and answer correctness,
incorporating in-context entropy into decoding to improve factuality.


The most relevant work to ours is  DoLA ____, which selects a single, most distinct layer to contrast with the final layer, amplifying the factual knowledge boosted in higher layers.
However, the change of inner predictions varies by candidate tokens, which means that, at a given generation step, factual tokens may exhibit different growing trends. Therefore, selecting a single caliber layer for all tokens is not accurate and can lead to false negative and false positive problems. 
Unlikely, we propose to process the prediction changes across layers individually for each token. By quantifying their growing trend, we can leverage internal information more accurately to enhance the factuality of generation.


% Decoding method cannot inject extra knowledge into LLMs, it just amplifies model’s inner knowledge to improve next-token predictions and prevent erroneous output. We can make models correctly express what they know while those models don’t know they still don’t know. Therefore, the hallucination cause by information lacking and outdated data cannot be solved in this scope.