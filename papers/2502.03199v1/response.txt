\section{Related Work}
Recently, various methods have been proposed to improve LLM's generating factuality to mitigate hallucinations. These include, but are not limited to, supervised fine-tuning with high-quality data **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"**, reinforcement learning with truthful preference pairs **Lewis et al., "BARISTE: Bidirectional Attention Flow for Transfer Learning"**, retrieval-augmented generation that integrates external knowledge **Liu et al., "Retrieval-Augmented Generation for Knowledge-Intensive Tasks"**,and editing knowledge-related inner representations or parameter-efficient modules **Guo et al., "Parameter-Efficient Training of Large Language Models from Scratch"**.


Our research focuses on the field of constraint decoding, which involves applying intervention strategies during model's generation process.
Notably, Inference-Time Intervention (ITI) **Shen et al., "Inference-Time Intervention for Improving Factuality in Generative Models"** employs probes to locate truthfulness signals within attention heads, while Repe **Guo et al., "Repe: Retrieval-Augmented Generation with Enhanced Probing Mechanism"** locates those within critical layers, then editing on the direction of truthfulness to modify model decoding.
Contrast Decoding (CD) **Xu et al., "Contrast Decoding for Improving Factuality in Generative Models"** and later Induced-then-Contrast Decoding (ICD) **Li et al., "Induced-then-Contrast Decoding for Enhanced Factuality"** contrasts logits from an expert model against those from a weak model, amplifying the knowledge reflected in their differences.
Activation Decoding **Chen et al., "Activation Decoding with In-Context Entropy for Improved Factuality"** leverages the correlation between context activation sharpness and answer correctness,
incorporating in-context entropy into decoding to improve factuality.


The most relevant work to ours is  DoLA **Liu et al., "DoLA: Decoding via Layer Attention for Enhanced Factuality"**, which selects a single, most distinct layer to contrast with the final layer, amplifying the factual knowledge boosted in higher layers.
However, the change of inner predictions varies by candidate tokens, which means that, at a given generation step, factual tokens may exhibit different growing trends. Therefore, selecting a single caliber layer for all tokens is not accurate and can lead to false negative and false positive problems. 
Unlikely, we propose to process the prediction changes across layers individually for each token. By quantifying their growing trend, we can leverage internal information more accurately to enhance the factuality of generation.


% Decoding method cannot inject extra knowledge into LLMs, it just amplifies model’s inner knowledge to improve next-token predictions and prevent erroneous output. We can make models correctly express what they know while those models don’t know they still don’t know. Therefore, the hallucination cause by information lacking and outdated data cannot be solved in this scope.