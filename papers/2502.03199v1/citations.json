[
  {
    "index": 0,
    "papers": [
      {
        "key": "tian2023fineSFT",
        "author": "Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea",
        "title": "Fine-tuning language models for factuality"
      },
      {
        "key": "zhou2024limaSFT",
        "author": "Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others",
        "title": "Lima: Less is more for alignment"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "sun2023aligningRL",
        "author": "Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others",
        "title": "Aligning large multimodal models with factually augmented rlhf"
      },
      {
        "key": "yang2023alignmentRL",
        "author": "Yang, Yuqing and Chern, Ethan and Qiu, Xipeng and Neubig, Graham and Liu, Pengfei",
        "title": "Alignment for honesty"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chern2023factoolRAG",
        "author": "Chern, I and Chern, Steffi and Chen, Shiqi and Yuan, Weizhe and Feng, Kehua and Zhou, Chunting and He, Junxian and Neubig, Graham and Liu, Pengfei and others",
        "title": "FacTool: Factuality Detection in Generative AI--A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhang2024truthx",
        "author": "Zhang, Shaolei and Yu, Tian and Feng, Yang",
        "title": "Truthx: Alleviating hallucinations by editing large language models in truthful space"
      },
      {
        "key": "hu2024separateLORA",
        "author": "Hu, Xinshuo and Li, Dongfang and Hu, Baotian and Zheng, Zihao and Liu, Zhenyu and Zhang, Min",
        "title": "Separate the wheat from the chaff: Model deficiency unlearning via parameter-efficient module operation"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li2024inferenceITI",
        "author": "Li, Kenneth and Patel, Oam and Vi{\\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin",
        "title": "Inference-time intervention: Eliciting truthful answers from a language model"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zou2023representationZOU",
        "author": "Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others",
        "title": "Representation engineering: A top-down approach to ai transparency"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "li2022contrastiveCD",
        "author": "Li, Xiang Lisa and Holtzman, Ari and Fried, Daniel and Liang, Percy and Eisner, Jason and Hashimoto, Tatsunori and Zettlemoyer, Luke and Lewis, Mike",
        "title": "Contrastive decoding: Open-ended text generation as optimization"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zhang2023alleviatingICD",
        "author": "Zhang, Yue and Cui, Leyang and Bi, Wei and Shi, Shuming",
        "title": "Alleviating hallucinations of large language models through induced hallucinations"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2024activation",
        "author": "Chen, Shiqi and Xiong, Miao and Liu, Junteng and Wu, Zhengxuan and Xiao, Teng and Gao, Siyang and He, Junxian",
        "title": "In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chuang2023dola",
        "author": "Chuang, Yung-Sung and Xie, Yujia and Luo, Hongyin and Kim, Yoon and Glass, James and He, Pengcheng",
        "title": "Dola: Decoding by contrasting layers improves factuality in large language models"
      }
    ]
  }
]