\begin{figure*}[ht]
\centering
\begin{overpic}
[width=1.0\linewidth]
%[width=\linewidth,grid,tics=10]
{fig/img/pipeline.jpg}
    %\put(12,30.5){\footnotesize (a)}
    %\put(38,30.5){\footnotesize (b)}
    %\put(65,30.5){\footnotesize (c)}
    %\put(87.5,30.5){\footnotesize (d)}
\end{overpic}
\caption{In the simulation, we train a Privileged Observation Encoder $\phi$ to extract the latent representation of privileged information ${z}^t$ and simultaneously train an Adaptation Module $\sigma$ to infer this representation $\tilde{z}^t$ from $H=10$ previous $(o^t, a^{t-1})$ pairs. The latent representation ${z}^t$ is then concatenated with desired grasping pose $p^t$, robot proprioception $q^t$, robot-object distance $\delta^t$, and categorical object parameters to form policy input. In the real world, we rollout trained policy with Adaptation Module $\sigma$ in an end-to-end manner, executing reaching, grasping, and manipulating. We leverage one RGBD image captured at the first frame to extract the desired grasping pose via off-the-shelf vision modules.}


\vspace{-0.5cm}
\label{fig:pipeline}
\end{figure*}