\input{fig/tab_baseline}
\input{fig/fig_exp}
\section{Experiments}
To verify the effectiveness of the proposed method, we conduct extensive evaluations in both
simulation and real-world settings.

\subsection{Data and Task Settings}
In the simulation, following the settings of PartManip~\cite{partmanip}, we conduct our experiments in the IsaacGym simulator and the large-scale PartNet-Mobility dataset \cite{partnet}. We use a fixed-base Franka and a total of 346 articulated 3D objects covering both doors and drawers (modified \textit{StorageFurniture} subset), to carry out the simulation experiments.

In the real-world setting, we perform experiments with a variety of household objects using the Franka Emika robotic arm equipped with an on-hand RealSense D415 camera to capture RGBD images. We leverage Segment Anything (SAM) \cite{kirillov2023segment} for actionable part pointcloud extraction using a first-framed RGBD image and GSNet \cite{gsnet} for grasp prediction.

We evaluate our proposed pipeline with two following tasks: OpenDoor/OpenDoor+ and OpenDrawer/OpenDrawer+. 

%\TODO{1. Describe door opening task, why it's hard?}
\textbf{OpenDoor/OpenDoor+: }A door is initially closed, the agent needs to open the door larger than 15\%/80\% of the maximum door swing. The key requirement for our task setting is that the gripper should firmly grasp the handle while opening the door without cheating by opening from the side or with the robot body.

\textbf{OpenDrawer/OpenDrawer+: }A drawer is initially closed, the agent needs to open the drawer larger than 20\%/80\% of the maximum opening length. Similar to the \textbf{OpenDoor} task, we require the gripper to firmly grasp the handle while opening the drawer.

For simulation and real-world settings, we adopt Success Rate (SR) as the major evaluation metric.
\input{fig/tab_ablations}
\input{fig/fig_kp}
\subsection{Baselines and Ablation Study Design}
We compare our proposed method with articulated-object manipulation pipelines that follow sim-to-real RL paradigm. 

\textbf{PPO}. We directly use the PPO algorithm to learn a state-based policy to handle each task. The detailed PPO parameters and training strategy are similar to our method.

\textbf{Where2Act~\cite{where2act}}. An affordance learning framework predicting the visual actionable affordance using a partial point cloud. We include the part mask as an additional dimension in our task, while keeping other aspects unchanged. 

\textbf{PartManip~\cite{partmanip}}. A vision-based policy learning method that first trains a state-based expert with part-based canonicalization and part-aware rewards, and then distills the knowledge to a vision-based student policy.

\textbf{RGBManip~\cite{rgbmanip}}. An image-only learning method that leverages an eye-on-hand monocular camera to actively perceive the articulated object from multiple perspectives to enhance 6D pose accuracy.

\textbf{GAPartNet~\cite{gapartnet}}. A vision-based method that first does cross-category part segmentation and pose estimation, and then uses the predicted part poses for heuristic-based manipulation

To highlight the contribution and effectiveness of each module within our approach, we conducted four comprehensive ablation studies:

\textbf{Ours w/o Policy Distillation}. We train a policy with observations from only current timestep $o^t$, omitting Adaptation Module and Privileged Observation Encoder.

\textbf{Ours w/o Variable Impedance Control}. We utilize Cartesian Position Control as low-level controller for our policy.

\textbf{Ours w/o Regularization}. We excluded motion-aware rewards from our reward functions.

\textbf{Ours w/o Randomization}. We exclude all forms of randomization in our pipeline, including object pose, desired grasping pose, friction, stiffness, mass, and noisy intrinsic.

\subsection{Results and Findings}

% \textbf{Ours with Pointcloud Distillation}. We distill our policy to a vision-based policy that directly takes scene pointcloud as input. We use Pointnet++\cite{qi2017pointnet++} as our vision backbone.
Results of simulation experiments are shown in Table ~\ref{table:comparison_baselines}, from which we can see that while most baselines perform reasonably well on the training set, their performance tends to decline significantly on the testing set. In contrast, our method maintains consistently strong performance on the evaluation set, without a sharp drop, highlighting the excellent generalization ability of our approach. We also find our controller learns to adapt to different manipulation stages, even without any direct gain rewards (Fig. ~\ref{fig:kp}). Specifically, when the gripper is far from the object, it turns stiffer by setting the controller gain to a higher $k_p$. On the other hand, when the distance is reduced, to minimize the collision penalty, it becomes softer with a lower $k_p$.

Our policy rollout performance in real world can be found in Table ~\ref{table:ablation}. We conduct 50 experiments for our pipeline and each ablated model (500 runs in total) on diverse objects (Fig. ~\ref{fig:obj}). We further investigate our success rate by decoupling the failure cases due to grasp pose estimation in Grasping Stage and due to our pipeline in Opening Stage. For OpenDoor+, we find $6/50$ inferences fail during Grasping Stage while only $4/50$ fail during Opening Stage, suggesting that if a stable grasping pose is initiated, our policy might yield $40/44=0.90\%$ SR. For OpenDrawer+, $7/8$ failure cases are due to unsuccessful grasping.

With the ablation study results demonstrated in Table~\ref{table:ablation}, apart from SR drop in both simulation and the real world, we aim to highlight the non-smooth motions of real-world executions. For \textit{W/o Impedance Control}, we find the main reason for failure cases ($40\%$ drop) is the low flexibility of position control, which requires each predicted action to be executed precisely. This would generate large joint torque to overcome the feedback force of objects, resulting in the robot arm being triggered to stop. In simulation, this behavior does not seem to severely hurt the performance, as evidenced by $>0.8$ success rate. However, in the real world, large torque is substantially dangerous and would trigger an emergency stop, emphasizing the necessity for impedance control. For \textit{W/o Distillation} and \textit{W/o Randomization}, the policy often finishes the task halfway, even when we manually tune a stiffer base value for the impedance controller. We claim that this behavior is due to the physics sim-to-real gap which resulted from non-diverse training settings and short-term observation. For \textit{W/o Regularization}, the reaching and opening motions are jerky, which are highly undesirable and result in grasp failure and contact lost during execution. 

In this work, we hope to introduce a reliable RL policy that can be seamlessly deployed in diverse real-world settings. Our experiments, conducted in both simulation and real-world scenarios, suggest that the manipulation stage should be learned as a smooth and continuous motion in simulation, instead of a discrete waypoint. Together with the tolerance of impedance control, the close-loop real-world transfer could be more efficient, even if the action predictions are slightly suboptimal. %See our webpage for more details.