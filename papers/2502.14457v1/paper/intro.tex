\section{INTRODUCTION}

% \gireesh{We as humans frequently open doors or drawers almost subconsciously, as these are among the most common actions in our daily routines. With articulated objects being so prevalent, it is essential for household robots to learn how to handle them efficiently and safely. These objects, such as doors or drawers, often contain one or more joints (revolute/prismatic) that enable them to change shape or size, facilitating their intended function. However, these joints have limits, within which actions must be executed to avoid damaging the object. For instance, some doors should be opened no more than \ang{70}, while others can be opened up to \ang{90}. Additionally, the weight or stiffness of doors can vary, making friction or feedback force critical components of a safe manipulation algorithm. These principles apply not only to doors but to all types of articulated objects. However, it is challenging to deduce these intrinsic characteristics solely from visual input, which limits the ability of open-loop vision-based policies to achieve truly safe manipulation of articulated objects.}

% A generalist robot represents a big milestone for the robot learning community, with the potential to revolutionize our daily life. With the ubiquity of articulated objects in both household and industry settings, learning how to efficiently manipulate them is one of the main challenges to achieving this goal. Amid the great progress in the embodied AI field in these couple of years\cite{zhang2023gamma, ha2024umionlegs, forcecontrolepfl, agilebutsafe}, generalizable articulated object manipulation remains an open question due to various reasons. One major challenge is that the true articulation characteristics (e.g. pivot center, pivot radius) could only be identified after physical contact is made. Additionally, the weight or stiffness of each instance can vary, making friction or feedback force an indispensable part of generalizable manipulation algorithms. While most state-of-the-art frameworks rely heavily on vision, it is non-trivial how to extract weight or friction values from purely observing the initial state of the object. On the other hand, it is more reasonable to continually learn these values from physical feedback during manipulation. Hence, this vision-dominant paradigm choice inherently hinders open-loop vision-based policies from becoming a truly generalizable long-horizon articulated object manipulation policy.
% task introduction (generalizable, compliance action), fixed-base franka, why we need impedance control? why we need close loop?
% open-loop first frame / close-loop vision: feedback + sim-to-real gap
% propose your method, insight to tackle problems (1)

A generalist robot represents a big milestone for the robot learning community, with the potential to revolutionize our daily life. With the ubiquity of articulated objects in both household and industry settings, learning how to efficiently manipulate them is one of the main challenges to achieving this goal. Amid the great progress in the embodied AI field in these couple of years\cite{zhang2023gamma, ha2024umionlegs, forcecontrolepfl, agilebutsafe}, generalizable articulated object manipulation remains an open question due to various reasons. One major challenge is that the true articulation characteristics (e.g. pivot center, friction, stiffness) could only be identified after physical contact is made. For instance, two objects might appear identical but their physical properties differ significantly. As a result, in order to achieve a generalizable articulated object manipulation pipeline that can seamlessly interact with unseen objects, it necessitates a closed-loop pipeline that can adaptively infer these characteristics during the manipulation stage. Another difficulty lies in the joint constraints of objects which require the applied actions to comply with the actual object joint motion. If the robot actions do not tolerate object joint motion and prioritize completing the given commands, it could result in large forces and damage to both objects and the robot.

% Recent articulated object manipulation works often rely on visual input as the dominant info  at the first frame, either in the form of pointcloud~\cite{coarse,rlafford,gapartnet,eisner2022flowbot3d} or RGB images~\cite{rgbmanip,dooropen,vrb}, as the primary source of information to predict affordances (actionable parts) and corresponding actions. These works have managed to demonstrate their capabilities in real-world scenarios under relatively constrained conditions: 1) Some pipelines \cite{where2act, vatmart,umpnet} output a sequence of actions from a set of primitives even before any physical interaction, which overlooks the intrinsic characteristics of the object and results in non-dexterous unnatural motion; 2) Other works \cite{partmanip, rgbmanip,where2act,umpnet} oversimplify success criteria such as open \ang{30} for doors or 8 cm for drawers. These values do not fully enable the functionality of articulated objects, preventing robots from performing subsequent tasks; 3) Some works \cite{umpnet, where2act, where2explore} use unrealistic embodiments for training such as flying gripper or mobile Franka to relax the condition for IK solver or collision avoidance while these components are crucial for successful sim-to-real pipelines; 4) Some learning-based pipelines \cite{dooropen} does not support zero-shot sim-to-real transfer, requiring object-specific fine-tuning which limits their generalizability to novel scenes.

Recent articulated object manipulation works often rely on visual information as the dominant input for their pipelines. Some prior works leverage vision input in the first frame, either in the form of pointcloud~\cite{coarse,gapartnet,eisner2022flowbot3d} or RGB images~\cite{rgbmanip,dooropen,vrb,where2act, where2explore, gapartnet}, to predict actionable parts followed by a sequence of actions or a waypoint trajectory. This sequence or waypoint is then directly executed in an open-loop manner neglecting all possible physical interaction with objects. This paradigm, despite the natural intuition, overlooks the intrinsic properties of objects and might result in unsafe behaviors. Other works leverage RL backbones \cite{partmanip, umpnet, rlafford, li2024unidoormanip} to output actions in a closed-loop fashion based on vision feedback. However, as this type of pipeline relies heavily on vision feedback at each iteration, it suffers the substantial vision sim-to-real gap inherited from vision modules \cite{dooropen, gapartnet} and can not generalize well. Additionally, during the manipulation stage, this approach might output suboptimal action due to the occlusion of the actionable part. Some \cite{rgbmanip} attempts to leverage impedance control as an off-the-shelf low-level controller to adaptively adjust the predicted waypoint based on some heuristic sample-based rules. However, this approach only affects the local trajectory between two predefined setpoints and results in non-smooth motions.

% Leveraging vision as the dominant input for the manipulation stage is suboptimal due to 1) the substantial vision sim-to-real gap\cite{partmanip, gapartnet}; 2) once grasped, affordance parts are often occluded during execution. In this project, we propose to use observation history to manipulate objects as an alternative for direct vision input. We evidence our intuition by exemplifying how humans can open a door in the dark: given the information about where the door handle is as well as whether the door is left-hinged or right-hinged, one would estimate the circular motion of the door based on the applied actions and its actual consequential motion. One would then gradually adjust the next actions according to this feedback to complete this task even without direct vision input. 
% comply to object motion

In this project, we propose combining closed-loop RL with learnable impedance control for generalizable articulated object manipulation. First, we use observation history to manipulate objects in a closed-loop fashion as an alternative for vision input. We evidence our intuition by exemplifying how humans can open a door in the dark: given the information about where the door handle is as well as whether the door is left-hinged or right-hinged, one would estimate the circular motion of the door based on the applied actions and its actual consequential motion. One would then gradually adjust the next actions according to this feedback to complete this task even without direct vision input. We argue that the benefits of leveraging observation history and diminishing reliance on vision, following this intuition, are twofold: 1) By incorporating vision only as a proxy input we can mitigate the vision sim-to-real gap; 2) By leveraging observation and action history, we can implicitly learn the movement of objects, based on the position error after each execution, thus enable a generalizable closed-loop pipeline.

Second, we address the importance of compliant action for articulated object manipulation by introducing variable impedance control to our pipeline. Impedance control is suitable for tasks that require high tolerance to balance setpoint tracking and object joint movement, which fundamentally differentiates articulated object and rigid object manipulation. While implementing a high-frequency variable impedance controller in simulation, we also learn its parameters jointly with our RL policy. We argue that equipping our well-designed training settings with impedance control allows our policy to generate smooth and continuous motions that comply with object joint movements. We find learning motion instead of a single action or discrete waypoints \cite{vatmart, where2act, where2explore} can yield a higher success rate in the real world.

We summarize our contributions as follows:

\begin{itemize}

\item We propose a novel RL-based pipeline for articulated object manipulation with observation and action history as primary inputs while vision only serves as a proxy. (Section \ref{method:distillation}).

\item We design a training setting where each component is realistic for sim-to-real and a reward function system that enables smooth multi-staged end-to-end manipulation without any heuristic motion planning (Section \ref{method:spaces} \& Section \ref{method:reward}).

\item We introduce a variable impedance controller to RL for higher tolerance to object motion, thus benefiting direct sim-to-real transfer (Section \ref{method:impedance}).

\item Through our extensive experiments with 4 tasks and 500 rollouts in the real world, our method's zero-shot inference reaches 96\% and 84\% success rates in simulation and real-world respectively, as well as demonstrates high generalizability to unseen objects.

\end{itemize}

% Code and extensive experiment results are on our website.