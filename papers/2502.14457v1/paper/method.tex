% \section{Method}
% \label{sec:method}
\section{Problem Statement}
\label{sec:overview}
\input{fig/fig_pipeline}

Given an articulated object \textit{O} and a manipulation task \textit{$\theta$}, we train a policy \textit{$\pi$} to output one dexterous action at a time to finish the task in a closed-loop manner. 

Our task definition is a more challenging and realistic adaptation of VAT-MART\cite{vatmart} and subsequent affordance works  \cite{where2act,where2explore}. Our pulling task (open doors, drawers) requires the policy to reach, grasp actionable parts, and then open untill the object's joint position reaches at least 80\% of the joint limit instead of about half-way\cite{vatmart,rgbmanip}. This criterion, especially when applied to revolute joints, necessitates much dexterous and long-horizon motions since the robot needs to follow the actual $SE(3)$ movements of objects. Moreover, in our settings, we allow only realistic IK configuration of robots (a fixed-base Franka) and do not assume the absolute feasibility of predicted motions as with other waypoint prediction pipelines using a flying gripper or suction cup \cite{where2act,where2explore,umpnet}. 

\section{Proposed method}

\subsection{Action and Observation Space} \label{method:spaces}
We design our framework to facilitate one dexterous action prediction at a time instead of short-horizon primitive actions. Our action for each step $a^t \in \mathbb{R}^{11}$ includes the target delta position $\Delta^t_{xyz} \in \mathbb{R}^3$, target 6D orientation $R^t \in \mathbb{R}^6$, gripper action $G^t \in \mathbb{R}^1$, and impedance control parameter $k_p^t \in \mathbb{R}^1$. Our raw robot action $a^t$ is later converted into robot commands $c^t \in \mathbb{R}^9 $ using an action scaler.

Our observation $o^t$ consists of desired grasping pose $g^t \in \mathbb{R}^7$, robot joint configuration $q^t \in \mathbb{R}^7$, robot-object relative distance $\delta^t \in \mathbb{R}^1$, end-effector pose $ee^t \in \mathbb{R}^9$ with three-dimensional position and 6D rotation, and graspability $ \mathds{1}^t_{grasp} \in \mathbb{R}^1$. Here, desired grasping poses are directly inferred from the handle bounding box in the simulation and from off-the-shelf grasp prediction modules in the real world. Our graspability signal is a distance-based and contact-aware condition, rather than a direct command for open/close gripper. In terms of task-aware observation, for instance, with DoorOpen task, we incorporate noisy pivot center $\tilde{r}^t_{pivot} \in \mathbb{R}^3$, noisy pivot radius $\tilde{r}^t_{radius} \in \mathbb{R}^1$, and right-hinged boolean $\tilde{r}^t_{rh} \in \mathbb{R}^1$. These motion-related arguments serve as high-level guidance for smoother implementation. 
\[ o^t = [g^t, q^t, \delta^t, ee^t, \mathds{1}^t_{grasp}, \tilde{r}^t_{pivot}, \tilde{r}^t_{radius}, \tilde{r}^t_{rh} ] \in \mathbb{R}^{30} \]

Our privileged observation $o^t_{priv}$, including values that are difficult to track in real-world settings, is used only in simulation for better environment understanding. These values are: pivot center ${r}^t_{pivot} \in \mathbb{R}^3$, pivot radius ${r}^t_{radius} \in \mathbb{R}^1$, object stiffness ${r}^t_{stiff} \in \mathbb{R}^1$, object mass ${r}^t_{m} \in \mathbb{R}^1$, object joint position ${q}^t_{obj} \in \mathbb{R}^1$, handle grasped signal $ \mathds{1}^t_{grasped} \in \mathbb{R}^1$.
\[ o^t_{priv} = [{r}^t_{pivot}, {r}^t_{radius},  {r}^t_{m}, {r}^t_{stiff}, {q}^t_{obj}, \mathds{1}^t_{grasped} ] \in \mathbb{R}^{8} \]

\subsection{Online policy distillation with Observation History} \label{method:distillation}
 Articulated object manipulation poses a unique challenge compared to rigid object manipulation because the object itself is a dynamic environment. The fact that object motion can only be observed via physical interactions or that joint ground-truth position is hidden inside the object resembles locomotion tasks where environment parameters (e.g. terrain friction, slope) are difficult to predict. To this end, we adopt the online policy distillation pipeline, which is widely applied for locomotion tasks \cite{deepwholebodycontrol, forcecontrolepfl, agilebutsafe}, and learn two separate modules: Adaptation Module $\sigma$ and Privileged Observation Encoder $\phi$ (Fig. ~\ref{fig:pipeline}).

Privileged Observation Encoder $\phi$ is a shallow MLP, which is utilized during training to learn the latent representation $z^t$ of privileged observations. This 20-dimensional vector is then concatenated with an (observation, action) pair $p^t = (o^t\oplus a^{t-1})$ at the current timestep to form actor inputs. We design the Adaptation Module $\sigma$ to be a temporal architecture to extract latent information about the environment from $H=10$ $p^t$ pairs. We keep only parts of action history as inputs for $\sigma$: position command $\Delta^t_{xyz}$, gripper command $G^t$, and controller gain $k_p^t$.

As the conventional two-staged teacher-student pipeline might result in realizability gap and sim-to-real gap \cite{deepwholebodycontrol}, we simultaneously train Adaptation Module and Privileged Observation Encoder in a single training. Specifically, when jointly train the Adaptation Module with our RL backbone, we also learn to extract similar privileged information $\tilde{z}$ from history buffer by formulating a supervision-regularization loss $\lambda \|z - \text{sg}[\tilde{z}]\|_2 + \|\text{sg}[z] - \tilde{z}\|_2$ on top of PPO objectives ($\text{sg}[.]$ denotes stop gradient operator). We apply a linear schedule for $\lambda$ to prevent our policy from conservative actions in the beginning phase.

\subsection{Reward Design and Domain Randomization} \label{method:reward}
% even when you have a good pipeline, training an end-to-end policy with different objectives for each stage (reaching, followed by grasping, and opening) is not trivial. How to blend multiple objectives? How to have a smooth action sequence?
While the proposed framework is adopted widely for locomotion tasks, it remains non-trivial how to transfer this pipeline for fine-manipulation tasks like articulated object manipulation. To facilitate a single end-to-end policy that can efficiently perform multi-staged motions, we introduce stage-conditioned rewards, including task-aware rewards and motion-aware rewards (see Table \ref{table:reward}). 

Task-aware rewards focus on executing a proper motion sequence, complying A-then-B order, rather than cheating to gain success rewards immediately. For instance, at timestep $t$, state $s_1^t$ with the door opened and the door handle grasped firmly by the gripper is rewarded significantly more than state $s_2^t$ without the grasped handle. 

Motion-aware rewards encourage our policy to generate smooth motions while maintaining a high success rate. These terms are often activated after the policy is trained to complete the main task, thus acting as a fine-tuning incentive for smoother execution. We argue that incorporating these regularization terms is crucial and helps bridge the sim-to-real gap by preventing unnecessary motion or non-achievable target poses.

Recent manipulation works \cite{partmanip,gapartnet,rgbmanip} demonstrate that training a policy with domain randomization may benefit sim-to-real transfer. In our work, we mainly focus on tackling the physics gap by asking our policy to understand object motion by object-robot interaction with noisy intrinsic properties. We randomize object positions and object yaw rotations during training to cover a reasonable workspace for real-world settings. In terms of physical intrinsic, we vary the joint friction, stiffness, and mass for more robust sim-to-real transfer. For desired grasping poses, after we infer a pose from part bounding boxes, we introduce random noise along $y$ and $z$ axes, together with a random rotation target from a pre-defined spherical cone.
\input{fig/tab_reward}

\subsection{Variable Impedance Control} \label{method:impedance}
% During sim2real transfer, due to the physics sim2real gap, we cannot make sure that the same kp=100 in sim and in real will behave identically. Learning to predict kp is similar to active domain randomization, helping the model to be exposed to different joint speed constraints. Claim that this will help better sim2real.

The goal of impedance control is to follow a desired trajectory $x_d$ considering the external force $F_{ext}$ resulted from the interaction between the robot and the environment. The design of impedance control follows a mass-spring-damper system that can dynamically adjust target setpoints based on feedback force as well as the stiffness of the environment. The dynamics model of impedance control is:
\[M(\ddot{x_c} - \ddot{x_d}) + D(\dot{x_c} - \dot{x_d}) + K({x_c} - {x_d}) = F_{ext}\]
where $M$ is the mass-inertia matrix of the robot, $D$ is the damping matrix, $K$ is the stiffness matrix, and $[\ddot{x_c}, \dot{x_c}, {x_c}]$ is impedance trajectory outputs.

In our pipeline, we learn to predict the stiffness factor $k_p$ of our Cartesian impedance controller  and expand it into a six-dimensional
diagonal matrix $K$. Following \cite{admitlearn, compliancetuning}, we assume that $M, K, D$ are positive definite diagonal matrices to ensure system stability. To this end, we scale actor prediction $k_p$ by:
\[c^t_{k_p} = \text{clip}(a^t_{k_p}, -1, 1) * 40 + 100\]
We find this value range generates reasonable motions in both simulation and real-world experiments. From stiffness matrix $K$, we then infer the damping matrix with the critical damping condition $D = 2\sqrt{MK}$.