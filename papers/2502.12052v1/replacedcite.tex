\section{Related Work}
As LLM-as-a-judge methods become increasingly prevalent, prior studies have designed some meta-evaluation benchmarks to assess their performance. Most of these benchmarks focus on the consistency between the evaluation results from LLMs and humans. Generally, their outputs are collected from generation models on various instruction-following tasks and humans are required to evaluate the quality of these outputs on different aspects, using direct scoring ____ or pairwise comparisons ____. A slightly different approach is taken by ____, who conduct the assessment through an objective knowledge-based question-answering task. Moreover, ____ propose a pipeline for automatically constructing benchmarks focused only on factual and logical correctness.

In addition to consistency with human judgments, ____ evaluate the position bias in pairwise comparisons from LLMs. Furthermore, ____ employ the perturbation method to provide a more comprehensive quantification of 12 different biases in LLM-based evaluators. ____, on the other hand, examine the consistency of evaluation results provided by LLM-based evaluators under multiple samplings and different scoring scales. And ____ consider additional expectations for the pairwise evaluation, such as transitivity.

Specifically for NLG tasks, ____ investigate the performance of different LLM-based evaluators across 20 NLP tasks on existing benchmarks, including translation and summarization. ____ design perturbation tests for six NLG tasks to benchmark the evaluation abilities of LLMs. Besides, some studies specifically evaluate the capabilities of LLM-based evaluators on specific NLG tasks, including translation ____, text summarization ____, dialogue generation ____, and story generation ____.