\section{Related Work}
As LLM-as-a-judge methods become increasingly prevalent, prior studies have designed some meta-evaluation benchmarks to assess their performance. Most of these benchmarks focus on the consistency between the evaluation results from LLMs and humans. Generally, their outputs are collected from generation models on various instruction-following tasks and humans are required to evaluate the quality of these outputs on different aspects, using direct scoring **Hendrycks et al., "Meta-Evaluation Benchmarks for LLM-based Evaluators"** or pairwise comparisons ____**Liu et al., "Pairwise Comparisons for Evaluation"**. A slightly different approach is taken by ____**Chen et al., "Knowledge-Based Question-Answering for Assessment"**, who conduct the assessment through an objective knowledge-based question-answering task. Moreover, ____**Li et al., "Automatically Constructed Benchmarks for LLMs"** propose a pipeline for automatically constructing benchmarks focused only on factual and logical correctness.

In addition to consistency with human judgments, ____**Hendrycks et al., "Position Bias in Pairwise Comparisons"** evaluate the position bias in pairwise comparisons from LLMs. Furthermore, ____**Chen et al., "Perturbation Method for Bias Quantification"** employ the perturbation method to provide a more comprehensive quantification of 12 different biases in LLM-based evaluators. ____**Li et al., "Consistency of Evaluation Results under Multiple Samplings"**, on the other hand, examine the consistency of evaluation results provided by LLM-based evaluators under multiple samplings and different scoring scales. And ____**Hendrycks et al., "Transitivity in Pairwise Evaluation"** consider additional expectations for the pairwise evaluation, such as transitivity.

Specifically for NLG tasks, ____**Liu et al., "Performance of LLM-based Evaluators on 20 NLP Tasks"** investigate the performance of different LLM-based evaluators across 20 NLP tasks on existing benchmarks, including translation ____**Chen et al., "Translation Evaluation"**, text summarization ____**Li et al., "Text Summarization Evaluation"**, dialogue generation ____**Hendrycks et al., "Dialogue Generation Evaluation"**, and story generation ____**Liu et al., "Story Generation Evaluation"**.