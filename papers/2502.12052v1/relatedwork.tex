\section{Related Work}
As LLM-as-a-judge methods become increasingly prevalent, prior studies have designed some meta-evaluation benchmarks to assess their performance. Most of these benchmarks focus on the consistency between the evaluation results from LLMs and humans. Generally, their outputs are collected from generation models on various instruction-following tasks and humans are required to evaluate the quality of these outputs on different aspects, using direct scoring \citep{DBLP:conf/iclr/YeKKHKJTKS24,DBLP:journals/corr/abs-2406-05761} or pairwise comparisons \citep{DBLP:conf/nips/ZhengC00WZL0LXZ23,DBLP:conf/iclr/ZengYG0G024,DBLP:journals/corr/abs-2308-01862}. A slightly different approach is taken by \citet{DBLP:journals/corr/abs-2406-12624}, who conduct the assessment through an objective knowledge-based question-answering task. Moreover, \citet{DBLP:journals/corr/abs-2410-12784} propose a pipeline for automatically constructing benchmarks focused only on factual and logical correctness.

In addition to consistency with human judgments, \citet{DBLP:conf/acl/WangLCCZLCKLLS24} evaluate the position bias in pairwise comparisons from LLMs. Furthermore, \citet{DBLP:journals/corr/abs-2410-02736} employ the perturbation method to provide a more comprehensive quantification of 12 different biases in LLM-based evaluators. \citet{DBLP:conf/coling/LeeHT25}, on the other hand, examine the consistency of evaluation results provided by LLM-based evaluators under multiple samplings and different scoring scales. And \citet{zhao-etal-2024-measuring} consider additional expectations for the pairwise evaluation, such as transitivity.

Specifically for NLG tasks, \citet{DBLP:journals/corr/abs-2406-18403} investigate the performance of different LLM-based evaluators across 20 NLP tasks on existing benchmarks, including translation and summarization. \citet{DBLP:journals/corr/abs-2408-13704} design perturbation tests for six NLG tasks to benchmark the evaluation abilities of LLMs. Besides, some studies specifically evaluate the capabilities of LLM-based evaluators on specific NLG tasks, including translation \citep{DBLP:conf/wmt/FreitagMDLAR0BK24}, text summarization \citep{DBLP:conf/naacl/LiuFCZHJLRWC24,DBLP:conf/acl/SiledarNMRNBBPS24}, dialogue generation \citep{DBLP:conf/emnlp/ZhangDTST023,DBLP:conf/emnlp/MendoncaTL24}, and story generation \citep{DBLP:journals/tacl/ChhunSC24}.