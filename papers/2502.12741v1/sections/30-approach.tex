\section{Machine Learning Approach}
\label{sec:approach}

\begin{figure}
\begin{minipage}[b]{0.54\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/approach-narrow.drawio.pdf}
    \caption{Approach overview.}
    \label{fig:approach-overview}
\end{minipage}
\hfill
\begin{minipage}[b]{0.44\linewidth}
\centering
    \includegraphics[width=\linewidth]{figs/processed-data.drawio.pdf}
    \caption{Layout of training dataset.}
    \label{fig:approach-data}
\end{minipage}
\end{figure}

\autoref{fig:approach-overview} shows an overview of our general approach. 
%
First, we conduct simulations using DCSim and preprocess the resulting data (Section~\ref{sec:approach:preprocessing}). %, as detailed in Section~\ref{sec:background}. 
%
We then train our model using the preprocessed simulator data (Section~\ref{sec:approach:training}). The model receives the configuration parameters of the simulation, i.e., the execution platform, characterization of datasets used, and workload configurations, as input and predicts the observables of each job in the workload. 
%
In total, we train three different model architectures (Section~\ref{sec:approach:architectures}). 
%
%After training is finished, our model can be employed to predict the observables of new job sequences. 

\subsection{Data Preprocessing} \label{sec:approach:preprocessing}
To create a sequential dataset that can be used to train our model, we need to preprocess the DCSim output data. 
%
First, we standardize the data to ensure that all features contribute equally to the predictions of the models, preventing features on larger scales from dominating the learning process. % \cite{peshawa_j_muhammad_ali_data_2014}
%
For each feature, we separately subtract the mean and divide the result by the standard deviation, thus adjusting the mean to zero and the standard deviation to one. 
%
%Since the parameter values generated by DCSim follow a normal distribution, we use the \texttt{StandardScaler} for standardization. 
%
We apply an inverse scaling transformation on the models' outputs to retrieve the output data at their original scale. 

Next, since the number of jobs varies in each simulation and some models require fixed-length input sequences, we divide the model input vectors into windows of fixed size. 
%
If the last window of a simulation contains fewer jobs than the fixed size, we pad it with zeros to maintain a consistent window size. 
%
To be able to reconstruct the original data sequence as predicted by the simulations, we add the \texttt{simulation\_ID} (not used by the model) and \texttt{job\_index} (used for model training) as additional features to the entries in each input window.
%
We provide an example illustration of a windowed dataset in \autoref{fig:approach-data}. 


\subsection{Model Training} \label{sec:approach:training}

We split the data into training and evaluation datasets to train the model, using a 70:30 ratio per simulation of the same length. 
%
We employ sequence-to-sequence prediction, predicting exactly one output per input row. 
%
% and shuffle the training data set after each epoch to avoid overfitting, ensuring that the model does not memorize data order and generalizes better to unseen data \cite{nguyen_why_2022}.
%
% The learning rate is set to 0.001 and is managed by a learning rate scheduler that reduces it by a factor of 0.1 when the average loss over the last five epochs changes by less than 0.0001. 
%
% This setup balances convergence speed and stability. 
%
The MSE loss function is used to penalize larger errors more heavily than smaller ones, emphasizing the importance of minimizing significant discrepancies between predicted and actual values.


We adjust hyperparameters, including the hidden size, which defines the dimensionality of each hidden layer, the window size specifying the number of consecutive data points considered in each input window, and the window overlap, determining how much consecutive windows overlap. 
%
Window overlap between windows from the same simulation can be used to capture the continuity and dependencies between jobs within the same simulation, enhancing the model's ability to learn from sequential patterns. 
%
In addition, we tune the batch size, which indicates the number of input rows processed before updating the internal parameters of the model, and the number of hidden layers in the model.

We optimize these hyperparameters manually by evaluating kernel density estimation (KDE) and accuracy plots. 
%
We begin by training ten models with randomly chosen hyperparameter configurations and select the three best-performing models. 
%
For each of these models, we adjust one free hyperparameter at a time, train the model, and evaluate its performance. 
%
Once the optimal value for a specific parameter is identified, we fix it and continue with the following free parameter while retaining the best-performing model from the previous iteration. 
%
This process continues until all hyperparameters have been optimized. 
%
Finally, we compare the three resulting models and select the one that demonstrates the best overall performance. 
%
The tuning order we follow is hidden size, window size, window overlap, number of layers, and batch size.

\subsection{Model Architectures} \label{sec:approach:architectures}

We explore three different model architectures for our surrogate model: BiGRU, BiLSTM, and Transformer models.

The BiGRU architecture consists of a linear input layer, multiple bidirectional Gated Recurrent Unit (BiGRU) layers, and a single linear output layer~\cite{salehinejad_recent_2018, yuanshuai_duan_improved_2023}. 
%
BiGRU processes data in both forward and backward directions, making it effective in scenarios where both past and future contexts help understand the sequence. 
%
Although future jobs cannot influence already completed jobs, the training phase can benefit from data on future jobs by helping the model identify patterns or dependencies that might not be evident when considering only past and current jobs. 
%
Moreover, BiGRU can capture long-term dependencies, which is valuable for predicting simulation results, as it allows the model to account for the influence of events over extended periods. 
%
This capability can lead to more accurate predictions since jobs submitted earlier and still running can influence subsequent jobs.


The BiLSTM architecture has a structure similar to that of BiGRU but uses bidirectional Long Short-Term Memory (BiLSTM) layers instead of BiGRU layers~\cite{khaled_a_althelaya_evaluation_2018, salehinejad_recent_2018}. 
%
As BiGRU, BiLSTM can capture long-term dependencies and processes data in both forward and backward directions. 


The transformer-based architecture we use~\cite{wu_deep_2020} is adapted for numerical predictions. 
Apart from technical scalability and performance benefits, thanks to the self-attention mechanism, transformers can handle long-range dependencies and are suited better for handling local and global contexts simultaneously.
%
In this study, we focus on encoder-only transformers, with the number of attention heads as an additional hyperparameter. 

