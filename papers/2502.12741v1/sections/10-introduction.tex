\section{Introduction}
\label{sec:intro}High energy physics at the LHC relies on the global-scale federated computing infrastructure provided by the Worldwide LHC Computing Grid (WLCG) for processing and storing the sheer amounts of scientific data gathered by the LHC experiments.
To achieve the high-throughput demands with limited financial resources, good performance of the computing system has to be constantly ensured.
With increasing demands expected from future LHC operations and technical and strategic changes in the federation, the best design for the WLCG infrastructure still needs to be identified.

Due to the size and complexity of such infrastructures, it is not feasible to build alternative infrastructures for mere testing.
In addition, due to constant uptime requirements, they cannot be reserved for test and evaluation purposes.
Instead, accurate simulation of workflow executions on such infrastructures and subsequent analysis of the simulated results can be performed without disturbing the operation of the real reference infrastructure, and a variety of alternative designs can be tested with just a fraction of the original investment.

However, there is a general trade-off between accuracy and the computational complexity, and therefore execution time, that comes with designing practical simulation models as simulations with higher accuracy generally implement more fine-granular simulation models with greater detail.
This typically leads to superlinear scaling in the execution time of the respective simulation tools with respect to the size of the simulated infrastructure.
As a result, it quickly becomes unfeasible to simulate workflow executions on large infrastructures.

To circumnavigate the unfavorable scaling of such simulation tools, ML surrogate models, trained on verifiable accurate simulator outputs or data from real-world systems, that predict observables from job execution traces in constant time present a natural solution approach.