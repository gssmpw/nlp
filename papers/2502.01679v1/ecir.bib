
@inproceedings{
behnamghaderLLM2VecLargeLanguage2024,
title={{LLM}2Vec: Large Language Models Are Secretly Powerful Text Encoders},
author={Parishad BehnamGhader and Vaibhav Adlakha and Marius Mosbach and Dzmitry Bahdanau and Nicolas Chapados and Siva Reddy},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=IW1PR7vEBf}
}
@online{bellamyAIFairness3602018,
  title = {{{AI Fairness}} 360: {{An Extensible Toolkit}} for {{Detecting}}, {{Understanding}}, and {{Mitigating Unwanted Algorithmic Bias}}},
  shorttitle = {{{AI Fairness}} 360},
  author = {Bellamy, Rachel K. E. and Dey, Kuntal and Hind, Michael and Hoffman, Samuel C. and Houde, Stephanie and Kannan, Kalapriya and Lohia, Pranay and Martino, Jacquelyn and Mehta, Sameep and Mojsilovic, Aleksandra and Nagar, Seema and Ramamurthy, Karthikeyan Natesan and Richards, John and Saha, Diptikalyan and Sattigeri, Prasanna and Singh, Moninder and Varshney, Kush R. and Zhang, Yunfeng},
  date = {2018-10-03},
  eprint = {1810.01943},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1810.01943},
  urldate = {2024-04-10},
  abstract = {Fairness is an increasingly important concern as machine learning models are used to support decision making in high-stakes applications such as mortgage lending, hiring, and prison sentencing. This paper introduces a new open source Python toolkit for algorithmic fairness, AI Fairness 360 (AIF360), released under an Apache v2.0 license (https://github.com/ibm/aif360). The main objectives of this toolkit are to help facilitate the transition of fairness research algorithms to use in an industrial setting and to provide a common framework for fairness researchers to share and evaluate algorithms.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/ipangbo/Zotero/storage/U77PGXJP/Bellamy et al. - 2018 - AI Fairness 360 An Extensible Toolkit for Detecti.pdf}
}

@online{biGOODDomainGeneralized2024,
  title = {{{GOOD}}: {{Towards Domain Generalized Orientated Object Detection}}},
  shorttitle = {{{GOOD}}},
  author = {Bi, Qi and Zhou, Beichen and Yi, Jingjun and Ji, Wei and Zhan, Haolan and Xia, Gui-Song},
  date = {2024-02-20},
  eprint = {2402.12765},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.12765},
  urldate = {2024-04-12},
  abstract = {Oriented object detection has been rapidly developed in the past few years, but most of these methods assume the training and testing images are under the same statistical distribution, which is far from reality. In this paper, we propose the task of domain generalized oriented object detection, which intends to explore the generalization of oriented object detectors on arbitrary unseen target domains. Learning domain generalized oriented object detectors is particularly challenging, as the cross-domain style variation not only negatively impacts the content representation, but also leads to unreliable orientation predictions. To address these challenges, we propose a generalized oriented object detector (GOOD). After style hallucination by the emerging contrastive language-image pre-training (CLIP), it consists of two key components, namely, rotation-aware content consistency learning (RAC) and style consistency learning (SEC). The proposed RAC allows the oriented object detector to learn stable orientation representation from style-diversified samples. The proposed SEC further stabilizes the generalization ability of content representation from different image styles. Extensive experiments on multiple cross-domain settings show the state-of-the-art performance of GOOD. Source code will be publicly available.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/ipangbo/Zotero/storage/CRGJY6CX/Bi et al. - 2024 - GOOD Towards Domain Generalized Orientated Object.pdf}
}

@inproceedings{blodgettStereotypingNorwegianSalmon2021,
  title = {Stereotyping {{Norwegian Salmon}}: {{An Inventory}} of {{Pitfalls}} in {{Fairness Benchmark Datasets}}},
  shorttitle = {Stereotyping {{Norwegian Salmon}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Blodgett, Su Lin and Lopez, Gilsinia and Olteanu, Alexandra and Sim, Robert and Wallach, Hanna},
  date = {2021},
  pages = {1004--1015},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2021.acl-long.81},
  url = {https://aclanthology.org/2021.acl-long.81},
  urldate = {2024-03-11},
  abstract = {Auditing NLP systems for computational harms like surfacing stereotypes is an elusive goal. Several recent efforts have focused on benchmark datasets consisting of pairs of contrastive sentences, which are often accompanied by metrics that aggregate an NLP system’s behavior on these pairs into measurements of harms. We examine four such benchmarks constructed for two NLP tasks: language modeling and coreference resolution. We apply a measurement modeling lens—originating from the social sciences—to inventory a range of pitfalls that threaten these benchmarks’ validity as measurement models for stereotyping. We find that these benchmarks frequently lack clear articulations of what is being measured, and we highlight a range of ambiguities and unstated assumptions that affect how these benchmarks conceptualize and operationalize stereotyping.},
  eventtitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/9KABKPYJ/Blodgett et al. - 2021 - Stereotyping Norwegian Salmon An Inventory of Pit.pdf}
}

@article{yogarajanTacklingBiasPretrained2023,
  title={Tackling bias in pre-trained language models: Current trends and under-represented societies},
  author={Yogarajan, Vithya and Dobbie, Gillian and Keegan, Te Taka and Neuwirth, Rostam J},
  journal={arXiv preprint arXiv:2312.01509},
  year={2023}
}

@inproceedings{cabelloIndependenceAssociationBias2023,
author = {Cabello, Laura and J\o{}rgensen, Anna Katrine and S\o{}gaard, Anders},
title = {On the Independence of Association Bias and Empirical Fairness in Language Models},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3593013.3594004},
abstract = {The societal impact of pre-trained language models has prompted researchers to probe them for strong associations between protected attributes and value-loaded terms, from slur to prestigious job titles. Such work is said to probe models for bias or fairness—or such probes ‘into representational biases’ are said to be ‘motivated by fairness’—suggesting an intimate connection between bias and fairness. We provide conceptual clarity by distinguishing between association biases [11] and empirical fairness [56] and show the two can be independent. Our main contribution, however, is showing why this should not come as a surprise. To this end, we first provide a thought experiment, showing how association bias and empirical fairness can be completely orthogonal. Next, we provide empirical evidence that there is no correlation between bias metrics and fairness metrics across the most widely used language models. Finally, we survey the sociological and psychological literature and show how this literature provides ample support for expecting these metrics to be uncorrelated.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {370–378},
numpages = {9},
keywords = {Fairness, Natural Language Processing, Representational Bias},
location = {Chicago, IL, USA},
series = {FAccT '23}
}


@article{
caliskanSemanticsDerivedAutomatically2017,
author = {Aylin Caliskan  and Joanna J. Bryson  and Arvind Narayanan },
title = {Semantics derived automatically from language corpora contain human-like biases},
journal = {Science},
volume = {356},
number = {6334},
pages = {183-186},
year = {2017},
doi = {10.1126/science.aal4230},
URL = {https://www.science.org/doi/abs/10.1126/science.aal4230},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aal4230},
abstract = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.}}

@article{catonFairnessMachineLearning,
  title = {Fairness in {{Machine Learning}}: {{A Survey}}},
  author = {Caton, Simon and Haas, Christian},
  journaltitle = {ACM Comput. Surv.},
  abstract = {When Machine Learning technologies are used in contexts that afect citizens, companies as well as researchers need to be conident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is signiicant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the diferent schools of thought and approaches that aim to increase the fairness of Machine Learning. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classiication, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as ive dilemmas for fairness research. ✙ ✙ CCS Concepts: · Computing methodologies Machine learning; · Social and professional topics User ✙ characteristics; · General and reference Surveys and overviews.},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/WQKI8S5E/Caton and Haas - Fairness in Machine Learning A Survey.pdf}
}


@article{chanDynamicDevelopmentSpeaking2015,
author = {Chan, HuiPing and Verspoor, Marjolijn and Vahtrick, Louisa},
title = {Dynamic Development in Speaking Versus Writing in Identical Twins},
journal = {Language Learning},
volume = {65},
number = {2},
pages = {298-325},
keywords = {writing, speaking, syntactic complexity, developmental patterns, HMM, usage-based, self-organization},
doi = {https://doi.org/10.1111/lang.12107},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12107},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/lang.12107},
abstract = {Taking a dynamic usage-based perspective, this longitudinal case study compares the development of sentence complexity in speaking versus writing in two beginner Taiwanese learners of English (identical twins) in an extensive corpus consisting of 100 oral and 100 written texts of approximately 200 words produced by each twin over 8 months. Three syntactic complexity measures were calculated: mean length of T-unit, dependent clauses per T-unit, and coordinate phrases per T-unit. The working hypothesis was that (a) the learners’ oral texts would become more complex sooner than their written texts and that (b) the two learners would show similar developmental patterns. We found that these two learners initially demonstrated syntactic complexity in their oral language rather than in their written language, yet over time they were found to exhibit inverse trends of development. This observation was confirmed with dynamic modeling by means of a hidden Markov model, which allowed us to detect moments of self-organization in the learners’ spoken and written output (i.e., moments where the interaction among various measures changes and takes on a new configuration).},
year = {2015}
}



@inproceedings{
changBOOOOKSCORESYSTEMATICEXPLORATION2024,
title={BooookScore: A systematic exploration of book-length summarization in the era of {LLM}s},
author={Yapei Chang and Kyle Lo and Tanya Goyal and Mohit Iyyer},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=7Ttk3RzDeu}
}


@article{changSurveyEvaluationLarge2024,
author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
title = {A Survey on Evaluation of Large Language Models},
year = {2024},
issue_date = {June 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {3},
issn = {2157-6904},

doi = {10.1145/3641289},
abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the ‘where’ and ‘how’ questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:},
journal = {Association for Computing Machinery Transactions on Intelligent Systems and Technology},
month = {March},
articleno = {39},
numpages = {45},
keywords = {Large language models, evaluation, model assessment, benchmark}
}


@article{devlinBERTPretrainingDeep2019,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{duUnderstandingGenderBias2022,
  title = {Understanding {{Gender Bias}} in {{Knowledge Base Embeddings}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Du, Yupei and Zheng, Qi and Wu, Yuanbin and Lan, Man and Yang, Yan and Ma, Meirong},
  date = {2022},
  pages = {1381--1395},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.98},
  url = {https://aclanthology.org/2022.acl-long.98},
  urldate = {2024-07-02},
  abstract = {Knowledge base (KB) embeddings have been shown to contain gender biases (Fisher et al., 2020b). In this paper, we study two questions regarding these biases: how to quantify them, and how to trace their origins in KB? Specifically, first, we develop two novel bias measures respectively for a group of person entities and an individual person entity. Evidence of their validity is observed by comparison with real-world census data. Second, we use influence function to inspect the contribution of each triple in KB to the overall group bias. To exemplify the potential applications of our study, we also present two strategies (by adding and removing KB triples) to mitigate gender biases in KB embeddings.},
  eventtitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/K962UXGG/Du et al. - 2022 - Understanding Gender Bias in Knowledge Base Embedd.pdf}
}

@inproceedings{elsafouryDarknessCanNot2022,
  title = {Darkness Can Not Drive out Darkness: {{Investigating Bias}} in {{Hate SpeechDetection Models}}},
  shorttitle = {Darkness Can Not Drive out Darkness},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Student Research Workshop}}},
  author = {Elsafoury, Fatma},
  date = {2022},
  pages = {31--43},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-srw.4},
  url = {https://aclanthology.org/2022.acl-srw.4},
  urldate = {2024-07-02},
  abstract = {It has become crucial to develop tools for automated hate speech and abuse detection. These tools would help to stop the bullies and the haters and provide a safer environment for individuals especially from marginalized groups to freely express themselves. However, recent research shows that machine learning models are biased and they might make the right decisions for the wrong reasons. In this thesis, I set out to understand the performance of hate speech and abuse detection models and the different biases that could influence them. I show that hate speech and abuse detection models are not only subject to social bias but also to other types of bias that have not been explored before. Finally, I investigate the causal effect of the social and intersectional bias on the performance and unfairness of hate speech detection models.},
  eventtitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Student Research Workshop}}},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/WB8RAPJ5/Elsafoury - 2022 - Darkness can not drive out darkness Investigating.pdf}
}

@article{fanGeneratingFullLength,
  title = {Generating {{Full Length Wikipedia Biographies The Impact}} of {{Gender Bias}} on the {{Retrieval-Based Generation}} of {{Women Biographies}}},
  author = {Fan, Angela and Gardent, Claire},
  abstract = {Generating factual, long-form text such as Wikipedia articles raises three key challenges: how to gather relevant evidence, how to structure information into well-formed text, and how to ensure that the generated text is factually correct. We address these by developing a model for English text that uses a retrieval mechanism to identify relevant supporting information on the web and a cache-based pre-trained encoderdecoder to generate long-form biographies section by section, including citation information. To assess the impact of available web evidence on the output text, we compare the performance of our approach when generating biographies about women (for which less information is available on the web) vs. biographies generally. To this end, we curate a dataset of 1,500 biographies about women. We analyze our generated text to understand how differences in available web evidence data affect generation. We evaluate the factuality, fluency, and quality of the generated texts using automatic metrics and human evaluation. We hope that these techniques can be used as a starting point for human writers, to aid in reducing the complexity inherent in the creation of long-form, factual text.},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/2LZQ7F88/Fan and Gardent - Generating Full Length Wikipedia Biographies The I.pdf}
}


@article{gallegosBiasFairnessLarge2024,
    author = {Gallegos, Isabel O. and Rossi, Ryan A. and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K.},
    title = "{Bias and Fairness in Large Language Models: A Survey}",
    journal = {Computational Linguistics},
    pages = {1-83},
    year = {2024},
    month = {08},
    abstract = "{Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this article, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely, metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.}",
    issn = {0891-2017},
    doi = {10.1162/coli_a_00524},
    eprint = {https://direct.mit.edu/coli/article-pdf/doi/10.1162/coli\_a\_00524/2465627/coli\_a\_00524.pdf},
}





@online{grootendorstBERTopicNeuralTopic2022,
  title = {{{BERTopic}}: {{Neural}} Topic Modeling with a Class-Based {{TF-IDF}} Procedure},
  shorttitle = {{{BERTopic}}},
  author = {Grootendorst, Maarten},
  date = {2022-03-11},
  eprint = {2203.05794},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.05794},
  urldate = {2024-05-01},
  abstract = {Topic models can be useful tools to discover latent topics in collections of documents. Recent studies have shown the feasibility of approach topic modeling as a clustering task. We present BERTopic, a topic model that extends this process by extracting coherent topic representation through the development of a class-based variation of TF-IDF. More specifically, BERTopic generates document embedding with pre-trained transformer-based language models, clusters these embeddings, and finally, generates topic representations with the class-based TF-IDF procedure. BERTopic generates coherent topics and remains competitive across a variety of benchmarks involving classical models and those that follow the more recent clustering approach of topic modeling.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ipangbo/Zotero/storage/I89C4CTE/Grootendorst - 2022 - BERTopic Neural topic modeling with a class-based.pdf}
}

@inproceedings{guptaMitigatingGenderBias2022,
  title = {Mitigating {{Gender Bias}} in {{Distilled Language Models}} via {{Counterfactual Role Reversal}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Gupta, Umang and Dhamala, Jwala and Kumar, Varun and Verma, Apurv and Pruksachatkun, Yada and Krishna, Satyapriya and Gupta, Rahul and Chang, Kai-Wei and Ver Steeg, Greg and Galstyan, Aram},
  date = {2022},
  pages = {658--678},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.55},
  url = {https://aclanthology.org/2022.findings-acl.55},
  urldate = {2024-07-02},
  abstract = {Language models excel at generating coherent text, and model compression techniques such as knowledge distillation have enabled their use in resource-constrained settings. However, these models can be biased in multiple ways, including the unfounded association of male and female genders with gender-neutral professions. Therefore, knowledge distillation without any fairness constraints may preserve or exaggerate the teacher model’s biases onto the distilled model. To this end, we present a novel approach to mitigate gender disparity in text generation by learning a fair model during knowledge distillation. We propose two modifications to the base knowledge distillation based on counterfactual role reversal—modifying teacher probabilities and augmenting the training set. We evaluate gender polarity across professions in open-ended text generated from the resulting distilled and finetuned GPT–2 models and demonstrate a substantial reduction in gender disparity with only a minor compromise in utility. Finally, we observe that language models that reduce gender polarity in language generation do not improve embedding fairness or downstream classification fairness.},
  eventtitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/ELZP2NQ9/Gupta et al. - 2022 - Mitigating Gender Bias in Distilled Language Model.pdf}
}

@inproceedings{gustafsonFACETFairnessComputer2023,
  title = {{{FACET}}: {{Fairness}} in {{Computer Vision Evaluation Benchmark}}},
  shorttitle = {{{FACET}}},
  booktitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Gustafson, Laura and Rolland, Chloe and Ravi, Nikhila and Duval, Quentin and Adcock, Aaron and Fu, Cheng-Yang and Hall, Melissa and Ross, Candace},
  date = {2023-10-01},
  pages = {20313--20325},
  publisher = {IEEE},
  location = {Paris, France},
  doi = {10.1109/ICCV51070.2023.01863},
  url = {https://ieeexplore.ieee.org/document/10377223/},
  urldate = {2024-03-07},
  abstract = {Computer vision models have known performance disparities across attributes such as gender and skin tone. This means during tasks such as classification and detection, model performance differs for certain classes based on the demographics of the people in the image. These disparities have been shown to exist, but until now there has not been a unified approach to measure these differences for common use-cases of computer vision models. We present a new benchmark named FACET (FAirness in Computer Vision EvaluaTion), a large, publicly available evaluation set of 32k images for some of the most common vision tasks - image classification, object detection and segmentation. For every image in FACET, we hired expert reviewers to manually annotate person-related attributes such as perceived skin tone and hair type, manually draw bounding boxes and label fine-grained person-related classes such as disk jockey or guitarist. In addition, we use FACET to benchmark state-of-the-art vision models and present a deeper understanding of potential performance disparities and challenges across sensitive demographic attributes. With the exhaustive annotations collected, we probe models using single demographics attributes as well as multiple attributes using an intersectional approach (e.g. hair color and perceived skin tone). Our results show that classification, detection, segmentation, and visual grounding models exhibit performance disparities across demographic attributes and intersections of attributes. These harms suggest that not all people represented in datasets receive fair and equitable treatment in these vision tasks. We hope current and future results using our benchmark will contribute to fairer, more robust vision models. FACET is available publicly at https://facet.metademolab.com.},
  eventtitle = {2023 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {9798350307184},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/X5UYUZFU/Gustafson et al. - 2023 - FACET Fairness in Computer Vision Evaluation Benc.pdf}
}

@inproceedings{heroldApplyingStereotypeContent2022,
  title = {Applying the {{Stereotype Content Model}} to Assess Disability Bias in Popular Pre-Trained {{NLP}} Models Underlying {{AI-based}} Assistive Technologies},
  booktitle = {Ninth {{Workshop}} on {{Speech}} and {{Language Processing}} for {{Assistive Technologies}} ({{SLPAT-2022}})},
  author = {Herold, Brienna and Waller, James and Kushalnagar, Raja},
  date = {2022},
  pages = {58--65},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.slpat-1.8},
  url = {https://aclanthology.org/2022.slpat-1.8},
  urldate = {2024-07-02},
  abstract = {Stereotypes are a positive or negative, generalized, and often widely shared belief about the attributes of certain groups of people, such as people with sensory disabilities. If stereotypes manifest in assistive technologies used by deaf or blind people, they can harm the user in a number of ways—especially considering the vulnerable nature of the target population. AI models underlying assistive technologies have been shown to contain biased stereotypes, including racial, gender, and disability biases. We build on this work to present a psychologybased stereotype assessment of the representation of disability, deafness, and blindness in BERT using the Stereotype Content Model. We show that BERT contains disability bias, and that this bias differs along established stereotype dimensions.},
  eventtitle = {Ninth {{Workshop}} on {{Speech}} and {{Language Processing}} for {{Assistive Technologies}} ({{SLPAT-2022}})},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/D6TJTWJK/Herold et al. - 2022 - Applying the Stereotype Content Model to assess di.pdf}
}

@online{kanekoDebiasingPretrainedContextualised2021,
  title = {Debiasing {{Pre-trained Contextualised Embeddings}}},
  author = {Kaneko, Masahiro and Bollegala, Danushka},
  date = {2021-01-23},
  eprint = {2101.09523},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2101.09523},
  urldate = {2024-03-01},
  abstract = {In comparison to the numerous debiasing methods proposed for the static noncontextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pretrained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Context-Debias},
  file = {/Users/ipangbo/Zotero/storage/3RNN63AN/Kaneko and Bollegala - 2021 - Debiasing Pre-trained Contextualised Embeddings.pdf}
}


@inproceedings{
lanALBERTLiteBERT2020,
title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1eA7AEtvS}
}

@article{leeObjectAwareDomainGeneralization2024,
  title = {Object-{{Aware Domain Generalization}} for {{Object Detection}}},
  author = {Lee, Wooju and Hong, Dasol and Lim, Hyungtae and Myung, Hyun},
  date = {2024-03-24},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {38},
  number = {4},
  pages = {2947--2955},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i4.28076},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/28076},
  urldate = {2024-04-12},
  abstract = {Single-domain generalization (S-DG) aims to generalize a model to unseen environments with a single-source domain. However, most S-DG approaches have been conducted in the field of classification. When these approaches are applied to object detection, the semantic features of some objects can be damaged, which can lead to imprecise object localization and misclassification. To address these problems, we propose an object-aware domain generalization (OA-DG) method for single-domain generalization in object detection. Our method consists of data augmentation and training strategy, which are called OA-Mix and OA-Loss, respectively. OA-Mix generates multi-domain data with multi-level transformation and object-aware mixing strategy. OA-Loss enables models to learn domain-invariant representations for objects and backgrounds from the original and OA-Mixed images. Our proposed method outperforms state-of-the-art works on standard benchmarks. Our code is available at https://github.com/WoojuLee24/OA-DG.},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/H8JTCQ9Z/Lee et al. - 2024 - Object-Aware Domain Generalization for Object Dete.pdf}
}

@inproceedings{liangDebiasingSentenceRepresentations2020,
  title = {Towards {{Debiasing Sentence Representations}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Liang, Paul Pu and Li, Irene Mengze and Zheng, Emily and Lim, Yao Chong and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  date = {2020},
  pages = {5502--5515},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.488},
  url = {https://www.aclweb.org/anthology/2020.acl-main.488},
  urldate = {2024-03-01},
  abstract = {As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, SENTDEBIAS, to reduce these biases. We show that SENT-DEBIAS is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {Sentence-Debias},
  file = {/Users/ipangbo/Zotero/storage/XMTNVGN3/Liang et al. - 2020 - Towards Debiasing Sentence Representations.pdf}
}

@inproceedings{linDomainInvariantDisentangledNetwork2021,
  title = {Domain-{{Invariant Disentangled Network}} for {{Generalizable Object Detection}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Lin, Chuang and Yuan, Zehuan and Zhao, Sicheng and Sun, Peize and Wang, Changhu and Cai, Jianfei},
  date = {2021-10},
  pages = {8751--8760},
  publisher = {IEEE},
  location = {Montreal, QC, Canada},
  doi = {10.1109/ICCV48922.2021.00865},
  url = {https://ieeexplore.ieee.org/document/9710354/},
  urldate = {2024-04-10},
  abstract = {We address the problem of domain generalizable object detection, which aims to learn a domain-invariant detector from multiple “seen” domains so that it can generalize well to other “unseen” domains. The generalization ability is crucial in practical scenarios especially when it is difficult to collect data. Compared to image classification, domain generalization in object detection has seldom been explored with more challenges brought by domain gaps on both image and instance levels. In this paper, we propose a novel generalizable object detection model, termed Domain-Invariant Disentangled Network (DIDN). In contrast to directly aligning multiple sources, we integrate a disentangled network into Faster R-CNN. By disentangling representations on both image and instance levels, DIDN is able to learn domain-invariant representations that are suitable for generalized object detection. Furthermore, we design a cross-level representation reconstruction to complement this two-level disentanglement so that informative object representations could be preserved. Extensive experiments are conducted on five benchmark datasets and the results demonstrate that our model achieves state-of-theart performances on domain generalization for object detection.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/E8S3E94C/Lin et al. - 2021 - Domain-Invariant Disentangled Network for Generali.pdf}
}


@article{liSurveyFairnessLarge2024,
  title={A survey on fairness in large language models},
  author={Li, Yingji and Du, Mengnan and Song, Rui and Wang, Xin and Wang, Ying},
  journal={arXiv preprint arXiv:2308.10149},
  year={2023}
}

@online{liuOutOfDistributionGeneralizationSurvey2023,
  title = {Towards {{Out-Of-Distribution Generalization}}: {{A Survey}}},
  shorttitle = {Towards {{Out-Of-Distribution Generalization}}},
  author = {Liu, Jiashuo and Shen, Zheyan and He, Yue and Zhang, Xingxuan and Xu, Renzhe and Yu, Han and Cui, Peng},
  date = {2023-07-27},
  eprint = {2108.13624},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2108.13624},
  urldate = {2024-04-10},
  abstract = {Traditional machine learning paradigms are based on the assumption that both training and test data follow the same statistical pattern, which is mathematically referred to as Independent and Identically Distributed (i.i.d.). However, in real-world applications, this i.i.d. assumption often fails to hold due to unforeseen distributional shifts, leading to considerable degradation in model performance upon deployment. This observed discrepancy indicates the significance of investigating the Out-of-Distribution (OOD) generalization problem. OOD generalization is an emerging topic of machine learning research that focuses on complex scenarios wherein the distributions of the test data differ from those of the training data. This paper represents the first comprehensive, systematic review of OOD generalization, encompassing a spectrum of aspects from problem definition, methodological development, and evaluation procedures, to the implications and future directions of the field. Our discussion begins with a precise, formal characterization of the OOD generalization problem. Following that, we categorize existing methodologies into three segments: unsupervised representation learning, supervised model learning, and optimization, according to their positions within the overarching learning process. We provide an in-depth discussion on representative methodologies for each category, further elucidating the theoretical links between them. Subsequently, we outline the prevailing benchmark datasets employed in OOD generalization studies. To conclude, we overview the existing body of work in this domain and suggest potential avenues for future research on OOD generalization. A summary of the OOD generalization methodologies surveyed in this paper can be accessed at http://out-of-distribution-generalization.com.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/ipangbo/Zotero/storage/XDEKS32K/Liu et al. - 2023 - Towards Out-Of-Distribution Generalization A Surv.pdf}
}



@misc{
  liuRoBERTaRobustlyOptimized2019,
  title={Ro{{B}{E}{R}{T}}a: A Robustly Optimized {{B}{E}{R}{T}} Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  year={2020},
  url={https://openreview.net/forum?id=SyxS0T4tvS}
}



@inproceedings{liYouHearPeople2023,
    title = "Do You Hear The People Sing? Key Point Analysis via Iterative Clustering and Abstractive Summarisation",
    author = "Li, Hao  and
      Schlegel, Viktor  and
      Batista-Navarro, Riza  and
      Nenadic, Goran",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = July,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.786",
    doi = "10.18653/v1/2023.acl-long.786",
    pages = "14064--14080",
    abstract = "Argument summarisation is a promising but currently under-explored field. Recent work has aimed to provide textual summaries in the form of concise and salient short texts, i.e., key points (KPs), in a task known as Key Point Analysis (KPA). One of the main challenges in KPA is finding high-quality key point candidates from dozens of arguments even in a small corpus. Furthermore, evaluating key points is crucial in ensuring that the automatically generated summaries are useful. Although automatic methods for evaluating summarisation have considerably advanced over the years, they mainly focus on sentence-level comparison, making it difficult to measure the quality of a summary (a set of KPs) as a whole. Aggravating this problem is the fact that human evaluation is costly and unreproducible. To address the above issues, we propose a two-step abstractive summarisation framework based on neural topic modelling with an iterative clustering procedure, to generate key points which are aligned with how humans identify key points. Our experiments show that our framework advances the state of the art in KPA, with performance improvement of up to 14 (absolute) percentage points, in terms of both ROUGE and our own proposed evaluation metrics. Furthermore, we evaluate the generated summaries using a novel set-based evaluation toolkit. Our quantitative analysis demonstrates the effectiveness of our proposed evaluation metrics in assessing the quality of generated KPs. Human evaluation further demonstrates the advantages of our approach and validates that our proposed evaluation metric is more consistent with human judgment than ROUGE scores.",
}


@article{macdonaldRacismSilencingMedia2021,
  title = {Racism and Silencing in the Media in {{Aotearoa New Zealand}}},
  author = {MacDonald, Liana and Ormond, Adreanne},
  date = {2021-06},
  journaltitle = {AlterNative: An International Journal of Indigenous Peoples},
  shortjournal = {AlterNative: An International Journal of Indigenous Peoples},
  volume = {17},
  number = {2},
  pages = {156--164},
  issn = {1177-1801, 1174-1740},
  doi = {10.1177/11771801211015436},
  url = {http://journals.sagepub.com/doi/10.1177/11771801211015436},
  urldate = {2024-04-09},
  abstract = {Racism in the Aotearoa New Zealand media is the subject of scholarly debate that examines how Māori (Indigenous Peoples of New Zealand) are broadcast in a negative and demeaning light. Literature demonstrates evolving understandings of how the industry places Pākehā (New Zealanders primarily of European descent) interests at the heart of broadcasting. We offer new insights by arguing that the media industry propagates a racial discourse of silencing that sustains widespread ignorance of the ways that Pākehā sensibilities mediate society. We draw attention to a silencing discourse through one televised story in 2018. On-screen interactions reproduce and safeguard a harmonious narrative of settler–Indigenous relations that support ignorance and denial of the structuring force of colonisation, and the Television Code of Broadcasting Practice upholds colour-blind perceptions of discrimination and injustice through liberal rhetoric. These processes ensure that the media industry is complicit in racism and the ongoing oppression of Indigenous peoples.},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/CATD3455/MacDonald and Ormond - 2021 - Racism and silencing in the media in Aotearoa New .pdf}
}


@inproceedings{mayMeasuringSocialBiases2019,
    title = "On Measuring Social Biases in Sentence Encoders",
    author = "May, Chandler  and
      Wang, Alex  and
      Bordia, Shikha  and
      Bowman, Samuel R.  and
      Rudinger, Rachel",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1063",
    doi = "10.18653/v1/N19-1063",
    pages = "622--628",
    abstract = "The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test{'}s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.",
}


@article{mcinnesHdbscanHierarchicalDensity2017, doi = {10.21105/joss.00205}, url = {https://doi.org/10.21105/joss.00205}, year = {2017}, publisher = {The Open Journal}, volume = {2}, number = {11}, pages = {205}, author = {Leland McInnes and John Healy and Steve Astels}, title = {{HDBSCAN}: Hierarchical density based clustering}, journal = {Journal of Open Source Software} }


@article{mcinnesUMAPUniformManifold2018,
  title={{UMAP}: Uniform Manifold Approximation and Projection},
  author={McInnes, Leland and Healy, John and Saul, Nathaniel and Grossberger, Lukas},
  journal={The Journal of Open Source Software},
  volume={3},
  number={29},
  pages={861},
  year={2018}
}


@inproceedings{meadeEmpiricalSurveyEffectiveness2022,
    title = "An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models",
    author = "Meade, Nicholas  and
      Poole-Dayan, Elinor  and
      Reddy, Siva",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.132",
    doi = "10.18653/v1/2022.acl-long.132",
    pages = "1878--1898",
    abstract = "Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model{'}s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",
}

@article{meiLargeScaleDocument2017,
  title = {Large {{Scale Document Categorization With Fuzzy Clustering}}},
  author = {Mei, Jian-Ping and Wang, Yangtao and Chen, Lihui and Miao, Chunyan},
  date = {2017-10},
  journaltitle = {IEEE Transactions on Fuzzy Systems},
  shortjournal = {IEEE Trans. Fuzzy Syst.},
  volume = {25},
  number = {5},
  pages = {1239--1251},
  issn = {1063-6706, 1941-0034},
  doi = {10.1109/TFUZZ.2016.2604009},
  url = {http://ieeexplore.ieee.org/document/7555373/},
  urldate = {2024-04-27},
  abstract = {Clustering documents into coherent categories is a very useful and important step for document processing and understanding. The introducing of fuzzy set theory into clustering provides a favorable mechanism to capture overlapping among document clusters. Document dataset is commonly represented as a collection of high-dimensional vectors, which may not be able to fit into memory entirely, when the dataset is large and with a very high dimensionality. However, most of the existing fuzzy clustering approaches deal with small and static datasets. Some of them may have a good scalability but they are only effective for low dimensional data. The study presented in this paper is about new efforts on fuzzy clustering of large-scale and high-dimensional data—especially suitable for document categorization. To consider both large scale and high dimensionality into the problem formulation, our key idea is to incorporate document-tailored fuzzy clustering into a scheme, which is effective for dealing with a large-scale problem. We first identified three representative schemes in fuzzy clustering for handling large-scale data, namely sampling extension, single pass, and divide ensemble. The limitation of fuzzy C-means (FCM)-based approaches for a large document clustering are then investigated. Based on the study, we propose new approaches by incorporating each of hyperspherical FCM and fuzzy coclustering with the three scale-up schemes, respectively. This enables our new approaches to maintain effectiveness for high-dimensional data with an extended scalability. Extensive experimental studies with real-world large document datasets have been conducted and the results demonstrate that the proposed approaches perform consistently better over existing ones in document categorization.},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/5LB8KZH9/Mei et al. - 2017 - Large Scale Document Categorization With Fuzzy Clu.pdf}
}

@online{muennighoffMTEBMassiveText2023,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loïc and Reimers, Nils},
  date = {2023-03-19},
  eprint = {2210.07316},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.07316},
  urldate = {2024-05-01},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https: //github.com/embeddings-benchm ark/mteb.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/ipangbo/Zotero/storage/QQS2MRBE/Muennighoff et al. - 2023 - MTEB Massive Text Embedding Benchmark.pdf}
}


@inproceedings{nadeemStereoSetMeasuringStereotypical2020,
    title = "{S}tereo{S}et: Measuring stereotypical bias in pretrained language models",
    author = "Nadeem, Moin  and
      Bethke, Anna  and
      Reddy, Siva",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = "August",
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.416",
    doi = "10.18653/v1/2021.acl-long.416",
    pages = "5356--5371",
    abstract = "A stereotype is an over-generalized belief about a particular group of people, e.g., Asians are good at math or African Americans are athletic. Such beliefs (biases) are known to hurt target groups. Since pretrained language models are trained on large real-world data, they are known to capture stereotypical biases. It is important to quantify to what extent these biases are present in them. Although this is a rapidly growing area of research, existing literature lacks in two important aspects: 1) they mainly evaluate bias of pretrained language models on a small set of artificial sentences, even though these models are trained on natural data 2) current evaluations focus on measuring bias without considering the language modeling ability of a model, which could lead to misleading trust on a model even if it is a poor language model. We address both these problems. We present StereoSet, a large-scale natural English dataset to measure stereotypical biases in four domains: gender, profession, race, and religion. We contrast both stereotypical bias and language modeling ability of popular models like BERT, GPT-2, RoBERTa, and XLnet. We show that these models exhibit strong stereotypical biases. Our data and code are available at \url{https://stereoset.mit.edu}.",
}


@inproceedings{nangiaCrowSPairsChallengeDataset2020,
    title = "{CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models}",
    author = "Nangia, Nikita  and
      Vania, Clara  and
      Bhalerao, Rasika  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics"
}


@article{navigliBiasesLargeLanguage2023,
author = {Navigli, Roberto and Conia, Simone and Ross, Bj\"{o}rn},
title = {Biases in Large Language Models: Origins, Inventory, and Discussion},
year = {2023},
issue_date = {June 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1936-1955},
doi = {10.1145/3597307},
abstract = {In this article, we introduce and discuss the pervasive issue of bias in the large language models that are currently at the core of mainstream approaches to Natural Language Processing (NLP). We first introduce data selection bias, that is, the bias caused by the choice of texts that make up a training corpus. Then, we survey the different types of social bias evidenced in the text generated by language models trained on such corpora, ranging from gender to age, from sexual orientation to ethnicity, and from religion to culture. We conclude with directions focused on measuring, reducing, and tackling the aforementioned types of bias.},
journal = {J. Data and Information Quality},
month = {June},
articleno = {10},
numpages = {21},
keywords = {Bias in NLP, language models}
}

@misc{OurVoicesProjects,
  title = {Our {V}oices {H}ome},
  author = {OurVoices},
  url = {https://ourvoices.auckland.ac.nz/},
  urldate = {2024-09-23}
}


@inproceedings{parrishBBQHandbuiltBias2022,
    title = "{BBQ}: A hand-built bias benchmark for question answering",
    author = "Parrish, Alicia  and
      Chen, Angelica  and
      Nangia, Nikita  and
      Padmakumar, Vishakh  and
      Phang, Jason  and
      Thompson, Jana  and
      Htut, Phu Mon  and
      Bowman, Samuel",
    editor = "Muresan, Smaranda  and
      Nakov, Preslav  and
      Villavicencio, Aline",
    booktitle = "Findings of the Association for Computational Linguistics: 60th Annual Meeting of the Association for Computational Linguistics",
    month = "May",
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.165",
    doi = "10.18653/v1/2022.findings-acl.165",
    pages = "2086--2105",
    abstract = "It is well documented that NLP models learn social biases, but little work has been done on how these biases manifest in model outputs for applied tasks like question answering (QA). We introduce the Bias Benchmark for QA (BBQ), a dataset of question-sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts. Our task evaluate model responses at two levels: (i) given an under-informative context, we test how strongly responses reflect social biases, and (ii) given an adequately informative context, we test whether the model{'}s biases override a correct answer choice. We find that models often rely on stereotypes when the context is under-informative, meaning the model{'}s outputs consistently reproduce harmful biases in this setting. Though models are more accurate when the context provides an informative answer, they still rely on stereotypes and average up to 3.4 percentage points higher accuracy when the correct answer aligns with a social bias than when it conflicts, with this difference widening to over 5 points on examples targeting gender for most models tested.",
}


@inproceedings{
pozzobonChallengesUsingBlackBox2023,
title={On the Challenges of Using Black-Box {API}s for Toxicity Evaluation in Research},
author={Luiza Amador Pozzobon and Beyza Ermis and Patrick Lewis and Sara Hooker},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
url={https://openreview.net/forum?id=Y6w2prqvjM}
}

@inproceedings{chanthranMalaysianEnglishNews,
    title = "{M}alaysian {E}nglish News Decoded: A Linguistic Resource for Named Entity and Relation Extraction",
    author = "Chanthran, MohanRaj  and
      Soon, Lay-Ki  and
      Ong, Huey Fang  and
      Selvaretnam, Bhawani",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.959",
    pages = "10999--11022",
    abstract = "Standard English and Malaysian English exhibit notable differences, posing challenges for natural language processing (NLP) tasks on Malaysian English. An experiment using state-of-the-art Named Entity Recognition (NER) solutions in Malaysian English news articles highlights that they cannot handle morphosyntactic variations in Malaysian English. Unfortunately, most of the existing datasets are mainly based on Standard English, which is not sufficient to enhance NLP tasks in Malaysian English. To the best of our knowledge, there is no annotated dataset that can be used to improve the model. To address this issue, we have constructed a Malaysian English News (MEN) dataset, which contains 200 news articles that are manually annotated with entities and relations. We then fine-tuned the spaCy NER tool and validated that having a dataset tailor-made for Malaysian English could significantly improve the performance of NER in Malaysian English. This paper presents our efforts to acquire data, the annotation methodology, and a detailed analysis of the annotated dataset. To ensure the quality of the annotation, we have measured the Inter-Annotator Agreement (IAA), and any disagreements were resolved by a subject matter expert through adjudication. After a rigorous quality check, we have developed a dataset with 6,061 entities and 3,268 relation instances. Finally, we discuss spaCy fine-tuning setup and analysis of NER performance. This unique dataset will contribute significantly to the advancement of NLP research in Malaysian English, allowing researchers to accelerate their progress, particularly in NER and relation extraction.",
}


@inproceedings{qianPerturbationAugmentationFairer2022,
    title = "Perturbation Augmentation for Fairer {NLP}",
    author = "Qian, Rebecca  and
      Ross, Candace  and
      Fernandes, Jude  and
      Smith, Eric Michael  and
      Kiela, Douwe  and
      Williams, Adina",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.646",
    doi = "10.18653/v1/2022.emnlp-main.646",
    pages = "9496--9521",
    abstract = "Unwanted and often harmful social biases are becoming ever more salient in NLP research, affecting both models and datasets. In this work, we ask whether training on demographically perturbed data leads to fairer language models. We collect a large dataset of human annotated text perturbations and train a neural perturbation model, which we show outperforms heuristic alternatives. We find that (i) language models (LMs) pre-trained on demographically perturbed corpora are typically more fair, and (ii) LMs finetuned on perturbed GLUE datasets exhibit less demographic bias on downstream tasks, and (iii) fairness improvements do not come at the expense of performance on downstream tasks. Lastly, we discuss outstanding questions about how best to evaluate the (un)fairness of large language models. We hope that this exploration of neural demographic perturbation will help drive more improvement towards fairer NLP.",
}



@article{radfordLanguageModelsAre,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@inproceedings{ravfogelNullItOut2020,
  title = {Null {{It Out}}: {{Guarding Protected Attributes}} by {{Iterative Nullspace Projection}}},
  shorttitle = {Null {{It Out}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Ravfogel, Shauli and Elazar, Yanai and Gonen, Hila and Twiton, Michael and Goldberg, Yoav},
  date = {2020},
  pages = {7237--7256},
  publisher = {Association for Computational Linguistics},
  location = {Online},
  doi = {10.18653/v1/2020.acl-main.647},
  url = {https://www.aclweb.org/anthology/2020.acl-main.647},
  urldate = {2024-03-01},
  abstract = {The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.},
  eventtitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {INLP Iterative Nullspace Projection},
  file = {/Users/ipangbo/Zotero/storage/F7UAUGWW/Ravfogel et al. - 2020 - Null It Out Guarding Protected Attributes by Iter.pdf}
}


@article{renInvestigatingFactualKnowledge2023,
  title={Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation},
  author={Ren, Ruiyang and Wang, Yuhao and Qu, Yingqi and Zhao, Wayne Xin and Liu, Jing and Tian, Hao and Wu, Hua and Wen, Ji-Rong and Wang, Haifeng},
  journal={arXiv preprint arXiv:2307.11019},
  year={2023}
}

@online{santusRankBasedSimilarityMetric2018,
  title = {A {{Rank-Based Similarity Metric}} for {{Word Embeddings}}},
  author = {Santus, Enrico and Wang, Hongmin and Chersoni, Emmanuele and Zhang, Yue},
  date = {2018-05-04},
  eprint = {1805.01923},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1805.01923},
  urldate = {2024-05-30},
  abstract = {Word Embeddings have recently imposed themselves as a standard for representing word meaning in NLP. Semantic similarity between word pairs has become the most common evaluation benchmark for these representations, with vector cosine being typically used as the only similarity metric. In this paper, we report experiments with a rank-based metric for WE, which performs comparably to vector cosine in similarity estimation and outperforms it in the recently-introduced and challenging task of outlier detection, thus suggesting that rank-based measures can improve clustering quality.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ipangbo/Zotero/storage/4XZIDSHB/Santus et al. - 2018 - A Rank-Based Similarity Metric for Word Embeddings.pdf}
}



@article{schickSelfDiagnosisSelfDebiasingProposal2021,
    author = {Schick, Timo and Udupa, Sahana and Schütze, Hinrich},
    title = "{Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1408-1424},
    year = {2021},
    month = {12},
    abstract = "{⚠ This paper contains prompts and model outputs that are offensive in nature.When trained on large, unfiltered crawls from the Internet, language models pick up and reproduce all kinds of undesirable biases that can be found in the data: They often generate racist, sexist, violent, or otherwise toxic language. As large models require millions of training examples to achieve good performance, it is difficult to completely prevent them from being exposed to such content. In this paper, we first demonstrate a surprising finding: Pretrained language models recognize, to a considerable degree, their undesirable biases and the toxicity of the content they produce. We refer to this capability as self-diagnosis. Based on this finding, we then propose a decoding algorithm that, given only a textual description of the undesired behavior, reduces the probability of a language model producing problematic text. We refer to this approach as self-debiasing. Self-debiasing does not rely on manually curated word lists, nor does it require any training data or changes to the model’s parameters. While we by no means eliminate the issue of language models generating biased text, we believe our approach to be an important step in this direction.1}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00434},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00434/1979270/tacl\_a\_00434.pdf},
}






@online{shaoCharacterLLMTrainableAgent2023,
  title = {Character-{{LLM}}: {{A Trainable Agent}} for {{Role-Playing}}},
  shorttitle = {Character-{{LLM}}},
  author = {Shao, Yunfan and Li, Linyang and Dai, Junqi and Qiu, Xipeng},
  date = {2023-12-14},
  eprint = {2310.10158},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.10158},
  urldate = {2024-04-08},
  abstract = {Large language models (LLMs) can be used to serve as agents to simulate human behaviors, given the powerful ability to understand human instructions and provide high-quality generated texts. Such ability stimulates us to wonder whether LLMs can simulate a person in a higher form than simple human behaviors. Therefore, we aim to train an agent with the profile, experience, and emotional states of a specific person instead of using limited prompts to instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar, etc. Our method focuses on editing profiles as experiences of a certain character and training models to be personal simulacra with these experiences. To assess the effectiveness of our approach, we build a test playground that interviews trained agents and evaluates whether the agents \textbackslash textit\{memorize\} their characters and experiences. Experimental results show interesting observations that help build future simulacra of humankind.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ipangbo/Zotero/storage/ZWHRL4GV/Shao et al. - 2023 - Character-LLM A Trainable Agent for Role-Playing.pdf}
}


@article{sibleyEthnicGroupStereotypes2011,
          volume = {40},
         journal = {New Zealand journal of psychology},
           title = {Ethnic Group Stereotypes in New Zealand.},
           month = {October},
           pages = {25--36},
          author = {Chris G Sibley and Kate Stewart and Carla Houkamau and Sam Manuela and Ryan Perry and Liz W Wootton and Jessica F Harding and Yang Zhang and Nikhil Sengupta and Andrew Robertson},
            year = {2011},
          number = {2},
             url = {https://kar.kent.ac.uk/84622/}
}


@online{smithSorryHearThat2022,
  title = {"{{I}}'m Sorry to Hear That": {{Finding New Biases}} in {{Language Models}} with a {{Holistic Descriptor Dataset}}},
  shorttitle = {"{{I}}'m Sorry to Hear That"},
  author = {Smith, Eric Michael and Hall, Melissa and Kambadur, Melanie and Presani, Eleonora and Williams, Adina},
  date = {2022-10-27},
  eprint = {2205.09209},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2205.09209},
  urldate = {2024-03-01},
  abstract = {As language models grow in popularity, it becomes increasingly important to clearly measure all possible markers of demographic identity in order to avoid perpetuating existing societal harms. Many datasets for measuring bias currently exist, but they are restricted in their coverage of demographic axes and are commonly used with preset bias tests that presuppose which types of biases models can exhibit. In this work, we present a new, more inclusive bias measurement dataset, HOLISTICBIAS, which includes nearly 600 descriptor terms across 13 different demographic axes. HOLISTICBIAS was assembled in a participatory process including experts and community members with lived experience of these terms. These descriptors combine with a set of bias measurement templates to produce over 450,000 unique sentence prompts, which we use to explore, identify, and reduce novel forms of bias in several generative models. We demonstrate that HOLISTICBIAS is effective at measuring previously undetectable biases in token likelihoods from language models, as well as in an offensiveness classifier. We will invite additions and amendments to the dataset, which we hope will serve as a basis for more easy-to-use and standardized methods for evaluating bias in NLP models.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,HolisticBias},
  file = {/Users/ipangbo/Zotero/storage/9V4V538R/Smith et al. - 2022 - I'm sorry to hear that Finding New Biases in La.pdf}
}
@inproceedings{borgeltImplementationFPgrowthAlgorithm2005,
author = {Borgelt, Christian},
title = {An implementation of the FP-growth algorithm},
year = {2005},
isbn = {1595932100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1133905.1133907},
abstract = {The FP-growth algorithm is currently one of the fastest approaches to frequent item set mining. In this paper I describe a C implementation of this algorithm, which contains two variants of the core operation of computing a projection of an FP-tree (the fundamental data structure of the FP-growth algorithm). In addition, projected FP-trees are (optionally) pruned by removing items that have become infrequent due to the projection (an approach that has been called FP-Bonsai). I report experimental results comparing this implementation of the FP-growth algorithm with three other frequent item set mining algorithms I implemented (Apriori, Eclat, and Relim).},
booktitle = {Proceedings of the 1st International Workshop on Open Source Data Mining: Frequent Pattern Mining Implementations},
pages = {1–5},
numpages = {5},
location = {Chicago, Illinois},
series = {OSDM '05}
}
@online{tanLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Data Annotation}}: {{A Survey}}},
  shorttitle = {Large {{Language Models}} for {{Data Annotation}}},
  author = {Tan, Zhen and Beigi, Alimohammad and Wang, Song and Guo, Ruocheng and Bhattacharjee, Amrita and Jiang, Bohan and Karami, Mansooreh and Li, Jundong and Cheng, Lu and Liu, Huan},
  date = {2024-02-20},
  eprint = {2402.13446},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2402.13446},
  urldate = {2024-04-05},
  abstract = {Data annotation is the labeling or tagging of raw data with relevant information, essential for improving the efficacy of machine learning models. The process, however, is laborintensive and expensive. The emergence of advanced Large Language Models (LLMs), exemplified by GPT-4, presents an unprecedented opportunity to revolutionize and automate the intricate process of data annotation. While existing surveys have extensively covered LLM architecture, training, and general applications, this paper uniquely focuses on their specific utility for data annotation. This survey contributes to three core aspects: LLM-Based Data Annotation, Assessing LLM-generated Annotations, and Learning with LLM-generated annotations. Furthermore, the paper includes an in-depth taxonomy of methodologies employing LLMs for data annotation, a comprehensive review of learning strategies for models incorporating LLM-generated annotations, and a detailed discussion on primary challenges and limitations associated with using LLMs for data annotation. As a key guide, this survey aims to direct researchers and practitioners in exploring the potential of the latest LLMs for data annotation, fostering future advancements in this critical domain. We provide a comprehensive papers list at https://github.com/ Zhen-Tan-dmml/LLM4Annotation.git.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ipangbo/Zotero/storage/8TSLQLWK/Tan et al. - 2024 - Large Language Models for Data Annotation A Surve.pdf}
}


@misc{touvronLlamaOpenFoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}
@article{wolf2019huggingface,
  title={HuggingFace's Transformers: State-of-the-art natural language processing},
  author={Wolf, T},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}
@INPROCEEDINGS{touvronLlamaOpenFoundation2023,
  author={Vidit, Vidit and Engilberge, Martin and Salzmann, Mathieu},
  booktitle={2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={CLIP the Gap: A Single Domain Generalization Approach for Object Detection}, 
  year={2023},
  volume={},
  number={},
  pages={3219-3229},
  keywords={Training;Location awareness;Computer vision;Semantics;Object detection;Detectors;Benchmark testing;Transfer;meta;low-shot;continual;or long-tail learning},
  doi={10.1109/CVPR52729.2023.00314}}

@ARTICLE{liSurveyDeepLearning2022,
  author={Li, Jing and Sun, Aixin and Han, Jianglei and Li, Chenliang},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Deep Learning for Named Entity Recognition}, 
  year={2022},
  volume={34},
  number={1},
  pages={50-70},
  keywords={Deep learning;Task analysis;Tools;Text recognition;Annotations;Encyclopedias;Natural language processing;named entity recognition;deep learning;survey},
  doi={10.1109/TKDE.2020.2981314}}

@inproceedings{viditCLIPGapSingle2023a,
  title = {{{CLIP}} the {{Gap}}: {{A Single Domain Generalization Approach}} for {{Object Detection}}},
  shorttitle = {{{CLIP}} the {{Gap}}},
  booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Vidit, Vidit and Engilberge, Martin and Salzmann, Mathieu},
  date = {2023-06},
  pages = {3219--3229},
  publisher = {IEEE},
  location = {Vancouver, BC, Canada},
  doi = {10.1109/CVPR52729.2023.00314},
  url = {https://ieeexplore.ieee.org/document/10203104/},
  urldate = {2024-04-12},
  abstract = {Single Domain Generalization (SDG) tackles the problem of training a model on a single source domain so that it generalizes to any unseen target domain. While this has been well studied for image classification, the literature on SDG object detection remains almost non-existent. To address the challenges of simultaneously learning robust object localization and representation, we propose to leverage a pre-trained vision-language model to introduce semantic domain concepts via textual prompts. We achieve this via a semantic augmentation strategy acting on the features extracted by the detector backbone, as well as a text-based classification loss. Our experiments evidence the benefits of our approach, outperforming by 10\% the only existing SDG object detection method, Single-DGOD [52], on their own diverse weather-driving benchmark.},
  eventtitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {9798350301298},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/SWDK99U4/Vidit et al. - 2023 - CLIP the Gap A Single Domain Generalization Appro.pdf}
}

@online{wangGeneralizingUnseenDomains2022,
  title = {Generalizing to {{Unseen Domains}}: {{A Survey}} on {{Domain Generalization}}},
  shorttitle = {Generalizing to {{Unseen Domains}}},
  author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip S.},
  date = {2022-05-23},
  eprint = {2103.03097},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.03097},
  urldate = {2024-04-12},
  abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/ipangbo/Zotero/storage/AZQIEE2I/Wang et al. - 2022 - Generalizing to Unseen Domains A Survey on Domain.pdf}
}

@online{wangRoleLLMBenchmarkingEliciting2023,
  title = {{{RoleLLM}}: {{Benchmarking}}, {{Eliciting}}, and {{Enhancing Role-Playing Abilities}} of {{Large Language Models}}},
  shorttitle = {{{RoleLLM}}},
  author = {Wang, Zekun Moore and Peng, Zhongyuan and Que, Haoran and Liu, Jiaheng and Zhou, Wangchunshu and Wu, Yuhan and Guo, Hongcheng and Gan, Ruitong and Ni, Zehao and Zhang, Man and Zhang, Zhaoxiang and Ouyang, Wanli and Xu, Ke and Chen, Wenhu and Fu, Jie and Peng, Junran},
  date = {2023-10-01},
  eprint = {2310.00746},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.00746},
  urldate = {2024-04-08},
  abstract = {The advent of Large Language Models (LLMs) has paved the way for complex tasks such as role-playing, which enhances user interactions by enabling models to imitate various characters. However, the closed-source nature of state-of-the-art LLMs and their general-purpose training limit role-playing optimization. In this paper, we introduce RoleLLM, a framework to benchmark, elicit, and enhance role-playing abilities in LLMs. RoleLLM comprises four stages: (1) Role Profile Construction for 100 roles; (2) Context-Based Instruction Generation (Context-Instruct) for rolespecific knowledge extraction; (3) Role Prompting using GPT (RoleGPT) for speaking style imitation; and (4) Role-Conditioned Instruction Tuning (RoCIT) for fine-tuning open-source models along with role customization. By Context-Instruct and RoleGPT, we create RoleBench, the first systematic and fine-grained character-level benchmark dataset for role-playing with 168,093 samples. Moreover, RoCIT on RoleBench yields RoleLLaMA (English) and RoleGLM (Chinese), significantly enhancing role-playing abilities and even achieving comparable results with RoleGPT (using GPT-4)1.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ipangbo/Zotero/storage/ZPVB5FTR/Wang et al. - 2023 - RoleLLM Benchmarking, Eliciting, and Enhancing Ro.pdf}
}

@online{wangZeroLabelLanguageLearning2021,
  title = {Towards {{Zero-Label Language Learning}}},
  author = {Wang, Zirui and Yu, Adams Wei and Firat, Orhan and Cao, Yuan},
  date = {2021-09-19},
  eprint = {2109.09193},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2109.09193},
  urldate = {2024-04-08},
  abstract = {This paper explores zero-label learning in Natural Language Processing (NLP), whereby no human-annotated data is used anywhere during training and models are trained purely on synthetic data. At the core of our framework is a novel approach for better leveraging the powerful pretrained language models. Specifically, inspired by the recent success of fewshot inference on GPT-3, we present a training data creation procedure named Unsupervised Data Generation (UDG), which leverages fewshot prompts to synthesize high-quality training data without real human annotations. Our method enables zero-label learning as we train task-specific models solely on the synthetic data, yet we achieve better or comparable results from strong baseline models trained on human-labeled data. Furthermore, when mixed with labeled data, our approach serves as a highly effective data augmentation procedure, achieving new state-of-the-art results on the SuperGLUE benchmark1.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/ipangbo/Zotero/storage/JLZHLFSW/Wang et al. - 2021 - Towards Zero-Label Language Learning.pdf}
}

@article{websterMeasuringReducingGendered,
  title = {Measuring and {{Reducing Gendered Correlations}} in {{Pre-trained Models}}},
  author = {Webster, Kellie and Wang, Xuezhi and Tenney, Ian and Beutel, Alex and Pitler, Emily and Pavlick, Ellie and Chen, Jilin and Chi, Ed and Petrov, Slav},
  abstract = {Pre-trained models have revolutionized natural language understanding. However, researchers have found they can encode artifacts undesired in many applications, such as professions correlating with one gender more than another. We explore such gendered correlations as a case study for how to address unintended correlations in pre-trained models. We define metrics and reveal that it is possible for models with similar accuracy to encode correlations at very different rates. We show how measured correlations can be reduced with general-purpose techniques, and highlight the trade offs different strategies have. With these results, we make recommendations for training robust models: (1) carefully evaluate unintended correlations, (2) be mindful of seemingly innocuous configuration differences, and (3) focus on general mitigations.},
  langid = {english},
  keywords = {Dropout},
  file = {/Users/ipangbo/Zotero/storage/AG479RIL/Webster et al. - Measuring and Reducing Gendered Correlations in Pr.pdf}
}

@article{wiedemerPROVABLECOMPOSITIONALGENERALIZATION,
  title = {{{PROVABLE COMPOSITIONAL GENERALIZATION FOR OBJECT-CENTRIC LEARNING}}},
  author = {Wiedemer, Thaddaus and Brady, Jack and Panfilov, Alexander and Juhos, Attila and Bethge, Matthias and Brendel, Wieland},
  abstract = {Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/JAPVWP2M/Wiedemer et al. - PROVABLE COMPOSITIONAL GENERALIZATION FOR OBJECT-C.pdf}
}

@inproceedings{wuSingleDomainGeneralizedObject2022,
  title = {Single-{{Domain Generalized Object Detection}} in {{Urban Scene}} via {{Cyclic-Disentangled Self-Distillation}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wu, Aming and Deng, Cheng},
  date = {2022-06},
  pages = {837--846},
  publisher = {IEEE},
  location = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.00092},
  url = {https://ieeexplore.ieee.org/document/9878404/},
  urldate = {2024-04-15},
  abstract = {In this paper, we are concerned with enhancing the generalization capability of object detectors. And we consider a realistic yet challenging scenario, namely Single-Domain Generalized Object Detection (Single-DGOD), which aims to learn an object detector that performs well on many unseen target domains with only one source domain for training. Towards Single-DGOD, it is important to extract domain-invariant representations (DIR) containing intrinsical object characteristics, which is beneficial for improving the robustness for unseen domains. Thus, we present a method, i.e., cyclic-disentangled self-distillation, to disentangle DIR from domain-specific representations without the supervision of domain-related annotations (e.g., domain labels). Concretely, a cyclic-disentangled module is first proposed to cyclically extract DIR from the input visual features. Through the cyclic operation, the disentangled ability can be promoted without the reliance on domain-related annotations. Then, taking the DIR as the teacher, we design a self-distillation module to further enhance the generalization ability. In the experiments, our method is evaluated in urban-scene object detection. Experimental results of five weather conditions show that our method obtains a significant performance gain over baseline methods. Particularly, for the night-sunny scene, our method outperforms baselines by 3\%, which indicates that our method is instrumental in enhancing generalization ability. Data and code are available at https://github.com/AmingWu/Single-DGOD.},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66546-946-3},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/C33KN3C7/Wu and Deng - 2022 - Single-Domain Generalized Object Detection in Urba.pdf}
}

@inproceedings{xuResearchClusteringAlgorithms2022,
  title = {Research on Clustering Algorithms in Data Mining},
  booktitle = {2022 3rd {{International Conference}} on {{Big Data}}, {{Artificial Intelligence}} and {{Internet}} of {{Things Engineering}} ({{ICBAIE}})},
  author = {Xu, Haoxiang},
  date = {2022-07-15},
  pages = {652--655},
  publisher = {IEEE},
  location = {Xi’an, China},
  doi = {10.1109/ICBAIE56435.2022.9985831},
  url = {https://ieeexplore.ieee.org/document/9985831/},
  urldate = {2024-04-23},
  abstract = {Clustering is a necessary data pre-processing method in data mining research, which aims to obtain the intrinsic distribution structure of valuable datasets from unlabeled datasets and thus simplify the description of the datasets. Data mining technology can discover potential and valuable knowledge from a large amount of data, giving a new meaning to the massive amount of data accumulated by people in the information age—new meaning. With the rapid development of data mining technology, as an essential part of it, grid clustering technology has been widely used in data analysis images. As a mainstream data mining method, cluster analysis facilitates data processing in machine learning by implementing clustering algorithms to analyze data and the proper operation of machines. Through the continuous experiments and proofs of previous generations, the algorithms on cluster analysis are becoming more and more mature. The text lists an overview of the evolution of high-quality clustering algorithms to understand the applications better and explore multiple clustering approaches. This study reviews papers that refer to 16 more wellknown and research-discussed developmental academic articles to understand their applications on clustering and explore the development of clustering in various fields. This systematic review aims to summarize the cluster analysis and grouping techniques used to date and make recommendations for future developments.},
  eventtitle = {2022 3rd {{International Conference}} on {{Big Data}}, {{Artificial Intelligence}} and {{Internet}} of {{Things Engineering}} ({{ICBAIE}})},
  isbn = {978-1-66545-160-4},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/Y7HCZHMI/Xu - 2022 - Research on clustering algorithms in data mining.pdf}
}

@online{yeZeroGenEfficientZeroshot2022,
  title = {{{ZeroGen}}: {{Efficient Zero-shot Learning}} via {{Dataset Generation}}},
  shorttitle = {{{ZeroGen}}},
  author = {Ye, Jiacheng and Gao, Jiahui and Li, Qintong and Xu, Hang and Feng, Jiangtao and Wu, Zhiyong and Yu, Tao and Kong, Lingpeng},
  date = {2022-10-21},
  eprint = {2202.07922},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2202.07922},
  urldate = {2024-04-07},
  abstract = {There is a growing interest in dataset generation recently due to the superior generative capacity of large pre-trained language models (PLMs). In this paper, we study a flexible and efficient zero-short learning method, ZEROGEN. Given a zero-shot task, we first generate a dataset from scratch using PLMs in an unsupervised manner. Then, we train a tiny task model (e.g., LSTM) under the supervision of the synthesized dataset. This approach allows highly efficient inference as the final task model only has orders of magnitude fewer parameters comparing to PLMs (e.g., GPT2-XL). Apart from being annotationfree and efficient, we argue that ZEROGEN can also provide useful insights from the perspective of data-free model-agnostic knowledge distillation, and unreferenced text generation evaluation. Experiments and analysis on different NLP tasks, namely, text classification, question answering, and natural language inference, show the effectiveness of ZEROGEN.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/ipangbo/Zotero/storage/ZG3QWCY7/Ye et al. - 2022 - ZeroGen Efficient Zero-shot Learning via Dataset .pdf}
}

@online{zhangClusterLLMLargeLanguage2023,
  title = {{{ClusterLLM}}: {{Large Language Models}} as a {{Guide}} for {{Text Clustering}}},
  shorttitle = {{{ClusterLLM}}},
  author = {Zhang, Yuwei and Wang, Zihan and Shang, Jingbo},
  date = {2023-11-03},
  eprint = {2305.14871},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.14871},
  urldate = {2024-04-23},
  abstract = {We introduce CLUSTERLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon “small” embedders, CLUSTERLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user’s preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions {$<$}does A better correspond to B than C{$>$}, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions {$<$}do A and B belong to the same category{$>$}, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that CLUSTERLLM consistently improves clustering quality, at an average cost of ∼\$0.61 per dataset. The code will be available at https: //github.com/zhang-yu-wei/ClusterLLM.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/ipangbo/Zotero/storage/NGXQ67EH/Zhang et al. - 2023 - ClusterLLM Large Language Models as a Guide for T.pdf}
}

@online{zhangDomainGeneralizationObject2022,
  title = {Towards {{Domain Generalization}} in {{Object Detection}}},
  author = {Zhang, Xingxuan and Xu, Zekai and Xu, Renzhe and Liu, Jiashuo and Cui, Peng and Wan, Weitao and Sun, Chong and Li, Chen},
  date = {2022-03-27},
  eprint = {2203.14387},
  eprinttype = {arXiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2203.14387},
  urldate = {2024-04-10},
  abstract = {Despite the striking performance achieved by modern detectors when training and test data are sampled from the same or similar distribution, the generalization ability of detectors under unknown distribution shifts remains hardly studied. Recently several works discussed the detectors’ adaptation ability to a specific target domain, which are not readily applicable in real-world applications since detectors may encounter various environments or situations while pre-collecting all of them before training is inconceivable. In this paper, we study the critical problem, domain generalization in object detection (DGOD), where detectors are trained with source domains and evaluated on unknown target domains. To thoroughly evaluate detectors under unknown distribution shifts, we formulate the DGOD problem and propose a comprehensive evaluation benchmark to fill the vacancy. Moreover, we propose a novel method named Region Aware Proposal reweighTing (RAPT) to eliminate dependence within RoI features. Extensive experiments demonstrate that current DG methods fail to address the DGOD problem and our method outperforms other state-of-the-art counterparts.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/ipangbo/Zotero/storage/CSHAZS8A/Zhang et al. - 2022 - Towards Domain Generalization in Object Detection.pdf}
}


@inproceedings{
zhengLARGELANGUAGEMODELS2024,
title={Large Language Models Are Not Robust Multiple Choice Selectors},
author={Chujie Zheng and Hao Zhou and Fandong Meng and Jie Zhou and Minlie Huang},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=shr9PXz7T0}
}

@inproceedings{zhouSenseEmbeddingsAre2022,
  title = {Sense {{Embeddings}} Are Also {{Biased}} – {{Evaluating Social Biases}} in {{Static}} and {{Contextualised Sense Embeddings}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zhou, Yi and Kaneko, Masahiro and Bollegala, Danushka},
  date = {2022},
  pages = {1924--1935},
  publisher = {Association for Computational Linguistics},
  location = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.135},
  url = {https://aclanthology.org/2022.acl-long.135},
  urldate = {2024-07-02},
  eventtitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/NH36B36U/Zhou et al. - 2022 - Sense Embeddings are also Biased – Evaluating Soci.pdf}
}

@inproceedings{zmigrodCounterfactualDataAugmentation2019,
  title = {Counterfactual {{Data Augmentation}} for {{Mitigating Gender Stereotypes}} in {{Languages}} with {{Rich Morphology}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Zmigrod, Ran and Mielke, Sebastian J. and Wallach, Hanna and Cotterell, Ryan},
  date = {2019},
  pages = {1651--1661},
  publisher = {Association for Computational Linguistics},
  location = {Florence, Italy},
  doi = {10.18653/v1/P19-1161},
  url = {https://www.aclweb.org/anthology/P19-1161},
  urldate = {2024-03-01},
  eventtitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  langid = {english},
  keywords = {Counterfactual Data Augmentation (CDA)},
  file = {/Users/ipangbo/Zotero/storage/JBE9RHP4/Zmigrod et al. - 2019 - Counterfactual Data Augmentation for Mitigating Ge.pdf}
}

@book{zobelWritingComputerScience2014,
  title = {Writing for {{Computer Science}}},
  author = {Zobel, Justin},
  date = {2014},
  publisher = {Springer London},
  location = {London},
  doi = {10.1007/978-1-4471-6639-9},
  url = {https://link.springer.com/10.1007/978-1-4471-6639-9},
  urldate = {2024-05-22},
  isbn = {978-1-4471-6638-2 978-1-4471-6639-9},
  langid = {english},
  file = {/Users/ipangbo/Zotero/storage/89JUMSB4/Zobel - 2014 - Writing for Computer Science.pdf}
}
