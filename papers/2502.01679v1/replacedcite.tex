\section{Related Work}
\subsubsection{{Bias in Large Language Model.}}
Bias occurs when a model assumes a person possesses characteristics stereotypical of their group ____. For instance, an LLM might use ``her'' when processing sentences that include ``nurse'', reflecting a gender bias. Such biases can lead to social injustice; for example, if a biased LLM is used for nurse CV screening, it may preferentially select females over males due to the stereotype of associating nursing with women.

\subsubsection{{Measuring Bias of Large Language Model.}}
Methods for testing LLM bias include embedding-based, generated text-based, and probability-based methods. Embedding-based methods such as WEAT ____ and SEAT ____ assess similarities between vectors for target social groups and stereotype-associated vectors in text encoders' embedding. While the approach is simple, it measures upstream of LLMs and is not representative enough of downstream tasks ____. The generated text-based approach directly tests LLMs using datasets to generate potentially stereotyped results. Applicable to all LLMs, it requires only model outputs without access to text encoders or logits. However, using classifiers to analyze these outputs can introduce their own biases ____. Probability-based methods, exemplified by StereoSet ____ and CrowS-Pairs ____, use pseudo-log likelihoods to assess the probability of word generation, ideally suited for open-source LLMs or APIs providing logits data. The pseudo-log likelihoods used by StereoSet ____ are represented as follows: $L(S)=\frac{1}{|M|}\sum_{t \in M}\log P(t|U;\theta)$\label{mlmfs}, where $S$ is a sentence, $M$ and $U$ are masked and other words in $S$, and $\theta$ is other parameters in the model. The Stereotype Score (\textit{ss}) measures the proportion of biased sentences preferred by the model. In contrast, the Language Model Score (\textit{lms}) reflects the selection percentage of non-irrelevant terms, showing the model's language ability. StereoSet ____ integrates these into the Idealized CAT Score (\textit{iCAT}): $iCAT = lms \cdot \frac{min((100 - ss), ss)}{50}$\label{icat}. Our proposed metric improves bias measurement by analyzing the distributional distance between logits for stereotypical and anti-stereotypical responses, unlike traditional metrics focusing solely on the highest probability choice. Our approach detects model preferences certainty, providing a more comprehensive understanding of model bias across various behaviours.

Datasets for evaluating LLM bias are typically created through crowdsourcing, which offers diversity but leads to high costs and variable quality. Alternative methods like filling sentence templates with varying words produce monotonous content and lack syntactic diversity ____. Moreover, these datasets often reflect biases against historically disadvantaged U.S. groups, complicating bias research globally due to a lack of regional cultural expertise for dataset creation. Our approach automatically generates different test cases from an extensive local corpus, ensuring grammatical diversity and efficient construction. Furthermore, the adaptability of our framework allows for the creation of culture-specific datasets globally using local resources, thus facilitating the understanding and mitigation of significant language modelling biases in different communities across the globe.