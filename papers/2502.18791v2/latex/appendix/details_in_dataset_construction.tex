\section{Details and Trials During Extraction}
\label{appendix:data_extraction}

We provide the details and trials during the data extraction process.

\paragraph{Hyperparameters and Prompt Tuning}

We did not extensively tune any LLM hyperparameters, instead, we used OpenAI’s default generation setup with greedy decoding.
For prompting, we initially built upon previous work~\citep{bai2023schema} as a foundation. 
Still, we adapted the prompts to better suit our project’s objectives, such as (1) determining whether the table includes experimental results for our target model or (2) enhancing records using contextual information from the paper. 


\paragraph{Preprocessing} To extract the experimental results of target models and attributes, we utilize arXiv sources\footnote{\url{https://info.arxiv.org/help/bulk_data_s3.html}} published between January 2023 and December 2024, as this timeframe aligns with the release of proprietary models, ensuring relevance to the latest advancements in the field. 
We specifically targeted papers in the machine learning (ML) domain (cs.CV, cs.AI, cs.CL, cs.LG) and downloaded their LaTeX source files for detailed analysis. 
To extract structured data, such as tables, we utilized a regex-based rule extraction method to retrieve the LaTeX source of tables along with their indices~\citep{bai2023schema}.


\paragraph{Filtering}

Before the extraction stage, we filter tables to reduce extraction time and API costs. 
To cut computational expenses, we pre-filter tables for those containing results of target models. 
Using simple heuristics, we filter tables based on keywords related to the models (e.g., "gemini," "gpt"), which significantly reduces irrelevant data and minimizes LLM API usage.
Moreover, we focus on leaderboard tables, which present the main results on specific benchmarks~\citep{kardas2020axcell}, to assess LLM performance. 
We employ an LLAMA3.1-70B-Instruct~\citep{dubey2024llama} to filter out non-leaderboard tables.


\paragraph{Extraction and Augmentation}

We employ schema-driven extraction to obtain records matching our target attributes from tables (detailed in \Cref{subsection:attributes}). 
The LLM first determines if a table contains target model records and only proceeds with extraction when relevant models are present, skipping tables without target model data.
While our approach resembles \citet{bai2023schema}, it achieves significant cost savings while maintaining accuracy (described in \cref{subsec:quality_assessment}) by focusing solely on target model records rather than processing entire tables. 
This selective approach reduces API calls by up to 1/200 per table.

After extracting information from the table, we further augment the extracted records by incorporating additional context from the entire paper. 
This is important because valuable details, such as experimental specifics, are often found in various sections of the paper. 
We heuristically filter out irrelevant sections from the full latex source and provide the LLM with the extracted information to enhance the attribute details using the context. 
During this process, we also gathered BibTeX references for the datasets associated with each record, allowing us to link to and retrieve the original papers describing these datasets.



\paragraph{Dataset Description Generation}

Initially, we asked the LLM to generate descriptions using only the dataset name and subset information from its internal knowledge. 
This approach was cost-effective as it didn't require processing additional context. 
However, when the LLM was uncertain—particularly with lesser-known datasets or those beyond its knowledge cutoff—it acknowledged its uncertainty, prompting the second stage of extraction from source papers.

When LLM knowledge was insufficient, we directly extracted dataset descriptions from source papers. 
These source papers could be either the paper containing the table being processed or the original dataset paper cited by it. 
We used citation tags from \cref{subsection:extraction} and rule-based heuristics to link to external arXiv papers associated with the original dataset.
We then used the full content of these source papers to prompt the LLM to extract the dataset descriptions again, using the paper’s contents as a reference.

We combined the dataset name and subset name to facilitate information generation or extraction. 
The schema for generation includes \textit{Dataset Summary}, \textit{Task Explanation}, and \textit{Subset Description}. 
Initially, we attempted to generate a \textit{Collection Process} section, detailing how the dataset was sourced and curated, but this led to excessive inaccuracies, so we decided to omit this part.

\paragraph{Models} We utilize GPT-4o \citep{hurst2024gpt} as the LLM for the pipeline and employ Llama-3.1 70B \citep{dubey2024llama} for filtering tables.

\paragraph{Valid Metrics} 

To ensure consistent analysis, we established a standardized set of metrics and excluded records using metrics outside this set. 
We also normalized all metric values to maintain uniformity across the analysis.
The approved metrics are: Accuracy, Exact Match, F1, BLEU, Rouge, MRR (Mean Reciprocal Rank), Precision, Recall, Pearson Correlation Coefficient, MAE (Mean Absolute Error), and MSE (Mean Square Error). 
For all metrics except Pearson correlation coefficient, MAE, and MSE, we standardized the values to range from a minimum of 0 to a maximum of 100 (e.g., 0.63\% was scaled to 63).

\paragraph{Missing Values} If certain target attributes were unavailable from the paper or the LLM couldn't locate specific information, we instructed it to mark these missing values with "xx" as a placeholder.

\paragraph{Canonicalization} 

Using rule-based heuristics, we canonicalized the \textit{Dataset name} by grouping identical names or methods under different labels. 
We took care to avoid merging similar but distinct names representing different versions, variations, or separate entities by applying strict character and abbreviation matching rules.
We initially explored linking datasets to the PaperswithCode ontology\footnote{\url{https://paperswithcode.com/datasets}}. 
However, the outdated and incomplete nature of its dataset list made this approach infeasible. 
We then experimented with clustering algorithms and large language models (LLMs) for grouping, but these methods resulted in an excessive number of false positives. 
Ultimately, we adopted a rule-based grouping algorithm as our approach.

\paragraph{Postprocessing} 

To ensure dataset quality, we removed duplicate records with identical values for \textit{Dataset Name}, \textit{Subset}, \textit{Number of Few\-Shots}, \textit{Prompting Method}, \textit{Metric Name} but differing \textit{Performance}. 
Additionally, we filtered out records with invalid dataset descriptions.


% \subsection{Details in Implementation}
% \label{subsection:details}

