\section{Details in Human Evaluation}
\label{appendix:human_evaluation}

We detail the human evaluation process.

\paragraph{Extraction Evaluation}
The objective is to assess whether the fields within records have been accurately extracted. 
This evaluation involves checking each field in a record against the original source paper to confirm its correctness. 
Fields are marked with 'o' for success or 'x' for failure, depending on the accuracy of the extraction. 
Annotators were instructed to use the \textit{original\_extracted\_dictionary} for verification, as metric names have been standardized and canonicalized. 
A table index was provided to help locate records and tables more quickly for annotation. 
Additionally, it is permissible for prompting methods to include few-shot examples if relevant information is not found in the paper.  


\paragraph{Description Evaluation}
This step evaluates whether the description aligns appropriately with the dataset-subset pair. 
The evaluation protocol (5-Point Likert Scale) involves assessing the quality of the dataset description based on the following rubric:  

\begin{itemize}
\item Score 1: The description is completely unrelated to the dataset-subset pair.  
\item Score 2: The description has minimal relevance but lacks alignment or context. 
\item Score 3: The description is moderately relevant, capturing the essence of the dataset-subset pair but includes noticeable inaccuracies.  
\item Score 4: The description is highly relevant with only minor inaccuracies.  
\item Score 5: The description is fully relevant and entirely accurate, with no errors.  
\end{itemize}

When scoring, annotators used references from the web or literature searches to ensure the evaluation was well-informed.
Our scoring rubric is designed to approximate an interval scale, allowing us to compute average scores. 
This approach aligns with standard practices in the machine learning field for evaluating response quality, whether through model-based or human-based assessments~\citep{liu2023g, kim2023prometheus}.