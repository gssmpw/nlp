\section{Dataset Examples}
\label{appendix:dataset_examples}

We provide a curated selection of examples from \datasetname~in Table~\ref{tab:dataset-instances} and Table~\ref{tab:dataset-instances-non-gpt}.

\begin{table*}[ht]  
    \centering  
    \resizebox{\textwidth}{!}{
    \small % Further reduce font size  
    \setlength{\tabcolsep}{2pt} % Reduce horizontal padding  
    \renewcommand{\arraystretch}{0.9} % Reduce vertical padding  
    \begin{tabular}{c|c|c|c|p{8.5cm}|c|c|c|c|c}  
    
    \toprule
    
    \textbf{ArXiv ID} & \textbf{Table} & \textbf{Dataset} & \textbf{Subset} & \textbf{\quad\quad\quad\quad\quad\quad\quad\quad\quad
    Dataset Description} & \textbf{Model} & \textbf{Shots} & \textbf{Prompt} & \textbf{Metric} & \textbf{Value} \\ \midrule  
    2301.08721 & 2 & CommonsenseQA & xx &   

\textbf{Dataset Summary}: This dataset is a multiple-choice question answering dataset focused on commonsense knowledge. It is designed to evaluate a model's ability to understand and apply commonsense reasoning to answer questions correctly. The dataset is widely used in the natural language processing domain to benchmark the performance of models on tasks requiring commonsense understanding.

\textbf{Task Explanation}: The task involves providing a question along with multiple answer choices, where the input is the question and the list of possible answers. The output is the correct answer choice. The task is evaluated based on the accuracy of the model's predictions compared to the correct answers provided in the dataset.
& GPT-4 & 12 & Batch CoT & Acc & 86 \\ \midrule  

    2301.08745 & 8 & Flores-101 & De $\rightarrow$ En &   
    
\textbf{Dataset Summary}: This dataset is a multilingual translation benchmark designed to evaluate machine translation systems across a wide range of languages. It covers 101 languages, providing a comprehensive resource for assessing translation quality and performance. The dataset is intended for tasks involving language translation, with a focus on both high-resource and low-resource languages. Key characteristics include its diverse language pairings and standardized evaluation metrics, which aim to facilitate consistent and fair comparisons of translation models.

\textbf{Task Explanation}: The primary task associated with this dataset is machine translation. The input consists of text in one language, and the output is the translated text in another language. For this specific subset, the task involves translating text from German (De) to English (En). Evaluation of the task is typically conducted using metrics such as BLEU, which measures the accuracy and fluency of the translated output compared to reference translations.

\textbf{Subset Description}: The De$\rightarrow$En subset specifically focuses on translations from German to English. This subset is used to evaluate the performance of translation models in converting German text into English, providing insights into the model's ability to handle this particular language pair.
     & GPT-4 & 0 & Direct & BLEU & 46 \\ \midrule

       2303.07992 & 3 & KQApro & xx &   
    
\textbf{Dataset Summary}: This dataset is a large-scale knowledge-based question answering dataset designed to evaluate the ability of models to understand and reason over complex questions. It is primarily used in the domain of natural language processing and artificial intelligence, focusing on tasks that require comprehension of structured knowledge bases. The dataset features a diverse set of questions that test various reasoning skills, such as logical reasoning, comparison, and temporal reasoning. The evaluation goals are to assess the accuracy and reasoning capabilities of models in answering these questions correctly.

\textbf{Task Explanation} The primary task associated with this dataset is knowledge-based question answering. The input consists of a natural language question, and the output is the correct answer derived from a structured knowledge base. Models are expected to interpret the question, retrieve relevant information from the knowledge base, and perform the necessary reasoning to arrive at the correct answer. The task evaluation method typically involves measuring the accuracy of the model's answers compared to a set of ground truth answers. & GPT-4 & 0 & xx & Accuracy & 57.2 \\ \midrule

       2304.08244 & 3 & API-Bank & Call &   

\textbf{Dataset Summary}: This dataset is a comprehensive benchmark designed to evaluate and enhance the capabilities of Large Language Models (LLMs) in utilizing external API tools. It focuses on tool-augmented LLMs, assessing their abilities in planning, retrieving, and calling APIs. The dataset includes a wide range of domains and APIs, aiming to provide a realistic and diverse evaluation environment. The primary goal is to understand the effectiveness of current LLMs in tool usage, improve their capabilities, and identify challenges in leveraging tools.

\textbf{Task Explanation}: The task involves evaluating LLMs on their ability to interact with APIs based on user queries. The input consists of user queries and API documentation, while the output is the correct API call and response. The task is evaluated based on the correctness of API calls and the quality of responses, using metrics like accuracy and ROUGE-L.

\textbf{Subset Description}: The ""Call"" subset specifically focuses on evaluating the ability of LLMs to make API calls based on given queries when the APIs are known. This subset tests the basic capability of LLMs to interact with APIs directly without the need for retrieval or planning. & GPT-4 & 0 & Zero-shot & Rouge & 36.91 \\ \midrule 


       2310.05915 & 1 & HotpotQA & xx &   

\textbf{Dataset Summary}: This dataset is a large-scale, high-quality question answering dataset designed to facilitate research in the domain of natural language processing, specifically in multi-hop question answering tasks. It contains questions that require reasoning over multiple documents to arrive at the correct answer. The dataset is intended to evaluate the ability of models to perform complex reasoning and synthesis of information across different contexts.

\textbf{Task Explanation}: The primary task involves providing a natural language question as input and expecting a concise answer as output. The questions are designed to require multi-hop reasoning, meaning that the answer cannot be found in a single document but requires synthesizing information from multiple sources. The task is evaluated based on the accuracy of the answers provided by the model, often using metrics such as exact match and F1 score to assess performance. & GPT-4 & xx & IO & Exact Match & 37.2 \\ 


\bottomrule  
    \end{tabular}}  
    \caption{Curated list of examples from \datasetname~(Target Model: GPT-4~\citep{achiam2023gpt}).}
    \label{tab:dataset-instances}  
\end{table*}  
     






\begin{table*}[ht]  
    \centering  
    \resizebox{\textwidth}{!}{
    \small % Further reduce font size  
    \setlength{\tabcolsep}{2pt} % Reduce horizontal padding  
    \renewcommand{\arraystretch}{0.9} % Reduce vertical padding  
    \begin{tabular}{c|c|c|c|p{9cm}|c|c|c|c|c}  
    
    \toprule
    
    \textbf{ArXiv ID} & \textbf{Table} & \textbf{Dataset} & \textbf{Subset} & \textbf{\quad\quad\quad\quad\quad\quad\quad\quad\quad Dataset Description} & \textbf{Model} & \textbf{Shots} & \textbf{Prompt} & \textbf{Metric} & \textbf{Value} \\ \midrule  
    
    2407.19619 & 1 & HPC & Fortran2C++ &   

\textbf{Dataset Summary}: This dataset is designed for the domain of high-performance computing (HPC) code translation, specifically focusing on translating between OpenMP Fortran and C++ code. It aims to train and evaluate large language models (LLMs) to enhance their translation capabilities between these two languages. The dataset is characterized by its diverse and representative collection of open-source OpenMP benchmarks, refined through code similarity tests. The evaluation goals include improving translation accuracy, as measured by CodeBLEU scores, and achieving human-level translation proficiency.

\textbf{Task Explanation}: The primary task is to translate code from OpenMP Fortran to C++. The input is a Fortran code snippet, and the output is its equivalent C++ translation. The task is evaluated using CodeBLEU, a metric that assesses the quality of code translation by considering both syntactic and semantic elements. Human evaluation is also employed to ensure the translations are correct, readable, and maintain the original code's functionality.

\textbf{Subset Description}: The Fortran to C++ translation subset focuses specifically on translating code from Fortran to C++. It serves the purpose of training models to understand and convert Fortran code into its C++ equivalent, addressing the challenges of translating between these two prominent HPC languages.
& GPT-4o & 0 & Zero-Shot & BLEU & 37.1 \\ \midrule  

    2408.02718 & 3 & MMIU & Overall &   
    
\textbf{Dataset Summary}: This dataset is designed for the domain of multimodal understanding, specifically focusing on tasks that require the integration of information from multiple images. It aims to facilitate research in areas such as image comparison, visual storytelling, and cross-image reasoning. The key characteristics of the dataset include a diverse set of image pairs or groups, each accompanied by annotations or questions that require understanding the relationships or narratives across the images. The evaluation goals are to assess the ability of models to comprehend and reason about multiple images simultaneously.

\textbf{Task Explanation}: The primary task involves taking multiple images as input and producing an output that demonstrates understanding of the relationships or narratives between them. This could involve answering questions, generating descriptive text, or identifying similarities and differences. The task evaluation method typically involves comparing the model's output to human-generated annotations or answers, using metrics such as accuracy, BLEU score, or human judgment for qualitative assessments.

\textbf{Subset Description}: The Overall subset encompasses the entire dataset, providing a comprehensive collection of all image pairs or groups and their associated annotations. This subset is intended for general evaluation and benchmarking of models on the full range of tasks supported by the dataset.
     & GPT-4o & xx & xx & Accuracy & 55.7 \\ \midrule

       2405.02861 & 3 & LexBench & IED &   
 \textbf{Dataset Summary}: This dataset is part of a comprehensive evaluation suite designed to test language models on various semantic phrase processing tasks. It focuses on idiomatic expression detection, which is one of the ten tasks included in the suite. The dataset aims to evaluate the ability of language models to understand and process idiomatic expressions, which are non-compositional phrases whose meanings cannot be deduced from their individual components. The evaluation goals include assessing model performance in classification, extraction, and interpretation tasks related to idiomatic expressions.

\textbf{Task Explanation}: The task involves detecting idiomatic expressions within a given context. The input consists of a sentence containing an idiomatic expression, and the output is a choice from multiple options that best describes the idiomatic expression's meaning. The task is evaluated using exact match accuracy, where the model's prediction is compared against the correct option.

\textbf{Subset Description}: The Idiomatic Expression Detection (IED) subset specifically focuses on identifying idiomatic expressions within sentences and selecting the correct interpretation from multiple options. This subset is designed to challenge language models in understanding the non-compositional nature of idiomatic expressions.
& Claude3 & 0 & zero-shot & Accuracy & 66.3 \\ \midrule

       2409.19667 & 5 & ProGraph & BGT &   

\textbf{Dataset Summary}: This dataset is a benchmark designed to evaluate the capability of large language models (LLMs) in graph analysis tasks. It focuses on enabling LLMs to process and analyze graphs using external APIs, similar to how human experts would approach such tasks. The dataset is crafted to assess the models' ability to generate code solutions for graph-related problems, rather than relying solely on reasoning over raw inputs. The benchmark includes 512 problems across three categories: basic graph theory, graph statistical learning, and graph embedding. The evaluation aims to measure the models' performance in leveraging programming libraries to solve graph tasks.

\textbf{Task Explanation}: The primary task involves generating Python code to solve graph-related problems using specified APIs. The input consists of a problem statement describing a graph task, and the output is the corresponding Python code that utilizes relevant APIs to solve the task. The evaluation method involves assessing the pass rate (the ratio of executable code) and accuracy (the ratio of correct answers from the executable code) of the generated solutions.

\textbf{Subset Description}: The Basic Graph Theory (BGT) subset focuses on fundamental concepts of graph theory, including types, properties, classical algorithms, and basic operations. Tasks in this subset may involve checking graph acyclicity, computing node centrality, or finding maximum cardinality matching.
& Gemini1.0 & xx & xx & Accuracy & 27.7 \\ 

\bottomrule  
    \end{tabular}}  
    \caption{Curated list of examples from \datasetname~(Target Models: GPT-4o~\citep{hurst2024gpt}, Claude3 Opus~\citep{anthropic@claude}, and Gemini1.0 Pro~\citep{team2023gemini}).}  
    \label{tab:dataset-instances-non-gpt}  
\end{table*}  