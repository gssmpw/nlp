\section{Related Work}
% \gabis{I prefer to get to our new stuff ASAP, so if these aren't crucial for understanding our work, I vote to move this section to the end of the paper.}

\paragraph{Information Extraction} Previous works have focused on extracting basic result tuples (e.g., task, dataset, metric) from scientific literature~\citep{singh2019automated, hou2019identification, kardas2020axcell, yang2022telin, bai2023schema, singh2024legobench, csahinucc2024efficient, kabongo2024orkg}. 
Our extraction pipeline improves upon this approach in two significant ways: it extracts enriched tuples that include prompting-related attributes and generates detailed dataset descriptions by leveraging LLM and automatically linked source papers.
Hence, unlike previous works that primarily compiled leaderboard tables, our enhanced extraction pipeline enables deeper review analysis, contributing to the broader goal of AI-driven scientific discovery~\citep{xu2021artificial, majumder2024discoverybench, m2024augmenting}.

\paragraph{LLM \& Prompting} Our study focuses on extracting experimental results of frontier proprietary LLMs~\citep{achiam2023gpt, anthropic@claude, team2023gemini}, with a specific emphasis on target attributes that incorporate information about prompting methods~\citep{brown2020language, wei2022chain, min2022rethinking}. 
In the context of prompting, prior studies have systematically analyzed the mechanisms behind prompting methods, focusing either on the use of in-context examples~\citep{min2022rethinking,lampinen2022can,weber2023mind, zhang2022robustness} or techniques that elicit reasoning, such as CoT prompting~\citep{wei2022chain, shaikh2023second, wang2023towards, turpin2024language, sprague2024cot, liu2024mind}.  
Conversely, we examine the model's behavior by conducting a literature analysis, which compiles data from scientific sources to reveal insights.


\paragraph{Literature Analysis}

Literature analysis systematically aggregates and examines data from multiple independent studies on a given topic to derive more precise and reliable conclusions. 
It has been widely applied in the biomedical domain for identifying target materials or clinical records~\citep{bao2019using, yun2024automatically}. 
In the NLP domain, review analysis has been used for metric standardization~\citep{reiter2018structured}, literature review~\citep{santu2024prompting, du2024llms}, and assessing evaluation criteria for specific domains~\citep{ostheimer2023call}. 
In contrast, our work employs a review analysis approach to evaluate the behavior of LLMs.
In the context of LLMs, \citet{asai2024openscholar} utilizes retrieval-augmented language models to synthesize scientific literature. 
However, this approach can only process a limited number of documents during retrieval to synthesize.
The work by~\citet{sprague2024cot} is perhaps the most closely related to ours. 
They conducted a review analysis through a literature survey to examine the effectiveness of CoT prompting for LLMs. 
However, their study is focused on CoT prompting, conducted on a limited scale, and relies on manual extraction methods.






% \paragraph{Regression} In our study, we perform a regression task to forecast the performance of an unseen dataset on a specific model, given a particular prompting setup. 
% Similarly, \citet{xia2020predicting} employed traditional regression models~\citep{chen2016xgboost} to predict the performance of translation tasks, treating it more like a table-in-filling task.
% More recently, studies have investigated regression tasks with LLMs, mainly focusing on AutoML or predefined numeric functions~\citep{song2024omnipred, vacareanu2024words, nguyen2024predicting, requeima2024llm, tang2024understanding}. 
% However, our study is notably different as it uses textual descriptions of datasets and prompting setups as input rather than numeric features.