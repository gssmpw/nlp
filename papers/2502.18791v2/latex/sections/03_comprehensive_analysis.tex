
\section{Comprehensive Analysis of the Dataset}
\label{sec:statistics_and_quality}

% \begin{figure}[t!]
%     \centering
%     \includegraphics[width=\columnwidth]{figures/metric_statistics.png}
%     \caption{The frequency distribution of experimental results per metric.}
%     \label{fig:metric_statistics}
% \end{figure}


\begin{table}[t!]
\centering
\resizebox{0.89\columnwidth}{!}{
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\toprule
\multicolumn{2}{l}{\textbf{General Statistics}} \\
Total Records & 18,127 \\
Number of Unique Datasets & 2,984 \\
% Number of arXiv Papers (2023-2024) & 379,639 \\
Number of Table Source Papers & 1,737 \\
Number of Unique Tables & 2,694 \\
% \hline
% \multicolumn{2}{l}{\textbf{Extraction Processing Time}} \\
% A Single Paper using Our Pipeline &  30s \\
% A Single Paper by Human Expert &  10 minutes \\
\hline
\multicolumn{2}{l}{\textbf{Records per Model}} \\
GPT-4~\citep{achiam2023gpt} & 12,475 \\
GPT-4o~\citep{hurst2024gpt} & 4,589 \\
Claude3-Opus~\citep{anthropic@claude} & 661 \\
Gemini1.0-Pro~\citep{team2023gemini} & 402 \\
\hline
\multicolumn{2}{l}{\textbf{Missing Values ('xx')}} \\
Records missing \textit{Subset} & 2,892 \\
Records missing \textit{Prompting Method} & 5,489 \\
Records missing \textit{Number of Few-Shots} & 9,081 \\
\hline
\multicolumn{2}{l}{\textbf{Dataset Description Sources}} \\
Source Paper of the Table & 9,334 \\
GPT-4o~\citep{hurst2024gpt} & 7,199 \\
Source Paper of the Linked Dataset & 1,594 \\
\bottomrule
\end{tabular}}
\caption{Dataset Statistics Overview. 
Dataset description sources specify whether descriptions come from LLM internal knowledge or source papers.}
\label{tab:dataset_stats}
\end{table}


%\subsection{Statistics}
% \label{subsection:statistics}

Table \ref{tab:main-dataset-instances} presents sampled instances of \datasetname, while the dataset's statistics are summarized in Table \ref{tab:dataset_stats}.
A total of 18,127 experimental records of four target models were extracted from scanning over 300k arXiv source papers.
GPT-4~\citep{achiam2023gpt} and GPT-4o~\citep{achiam2023gpt} dominate the results, while Claude 3 Opus~\citep{anthropic@claude} and Gemini 1.0 Pro~\citep{team2023gemini} have significantly fewer entries, highlighting a bias favoring certain proprietary models. 
Many prompting configurations have missing values, confirmed by human verification~(\cref{subsec:quality_assessment}) as unreported by the original authors, likely indicating the default setup.
The majority of dataset descriptions are generated referencing the source paper of the table, as these papers often describe datasets when presenting benchmarks or detailing experimental sections.


\subsection{Quality and Efficiency Assessment}
\label{subsec:quality_assessment}

Our human evaluation assessed extraction accuracy and description quality across 40 records of \datasetname~(280 total attributes) from different papers. 
Two NLP Ph.D. students verified field extraction correctness~\citep{bai2023schema} and rated descriptions on a 5-point scale from (1) unrelated to (5) fully relevant and accurate~\citep{amidei2019use, liu2023geval}. 
Missing values were marked correct if the information was genuinely unavailable in the source papers.
More details are in \cref{appendix:human_evaluation}.

Table~\ref{tab:dataset_human} shows strong extraction accuracy and description quality of \datasetname. 
GPT-4o \citep{hurst2024gpt}, which we used during the data extraction, demonstrated effective long-context information extraction, showcasing its potential for scientific literature synthesis.
Moreover, high validation scores suggest missing values stem from unreported setups. 
Cohen's Kappa scores of 0.68 (extraction) and 0.57 (description) indicate substantial inter-annotator agreement~\citep{mchugh2012interrater}.

During the study, we recorded the time experts spent annotating target attribute information (excluding descriptions) from additional samples based on a list of papers and tables indexing experimental results of target models. 
On average, it took 7 minutes and 50 seconds per table, totaling approximately about 350 hours for \datasetname~of 2,694 tables. 
This does not include the initial effort of surveying and identifying papers and tables. 
In contrast, our pipeline identifies and extracts data from arXiv sources from 2023 to 2024 in a single day for under \$500 using a batching API.


\begin{table}[t!]
\centering
\resizebox{0.97\columnwidth}{!}{
\begin{tabular}{lc}
\toprule
\textbf{Attribute} & \textbf{Accuracy (Score)} \\
\toprule
\multicolumn{2}{l}{\textbf{Extraction}} \\
Dataset Name & 95 \% \\
Model Name & 100 \% \\
Prompting Method & 86.3 \% \\
Number of Few-Shot Examples & 95 \% \\
Metric & 100 \%  \\
Metric Value & 98.8 \% \\
\hline
\multicolumn{2}{l}{\textbf{Description}} \\
Dataset Description & 4.55 \\
\bottomrule
\end{tabular}}
\caption{Results of Human Evaluation.}
\label{tab:dataset_human}
\end{table}


\subsection{Categorization by Required Skills}
\label{subsection:core_skills}

We categorize experimental records from \datasetname~by required skills to enable comprehensive literature analyses and offer researchers a searchable resource. 
Expanding on Tulu3's core skills~\citep{lambert2024t}, we defined 10 categories: \textit{Knowledge}, \textit{Reasoning}, \textit{Math}, \textit{Coding}, \textit{Multimodality}, \textit{Instruction Following}, \textit{Safety}, \textit{Multilinguality}, \textit{Tool Use (Agent Framework)}, and \textit{Other}.
Records were classified using an LLM API~\citep{hurst2024gpt} based on dataset names, subsets, and descriptions. 
Since datasets can fit multiple categories, we applied multi-label classification. 
This categorized information is used throughout the analyses in \cref{section:literature_analysis_prompting_behavior}.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.85\columnwidth]{figures/figure_trend_graph}
    \caption{Log-scale frequency trends show a rapid increase of evaluation studies over successive quarters (Q). \emph{Reasoning} remains the most popular evaluation category, while \emph{Multimodality} exhibits recent rapid growth.}
    \label{fig:trend}
\end{figure}

Figure~\ref{fig:trend} shows the log-scale frequency of skill categories evaluated every three months based on arXiv publication dates. 
Over time, experimental results for all skills have increased due to rising interest in LLMs. 
\textit{Reasoning} tasks are the most popular and continue to grow, while \textit{Multimodality} has shown recent growth. \textit{Knowledge}, \textit{Multilinguality}, and \textit{Math} have steadily increased in evaluation rates. 
Frequency is based on the unique count of dataset-subsets aggregated by paper. 
The most frequently used datasets for each category are listed in \cref{appendix:frequent_datasets}.