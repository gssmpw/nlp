\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\textwidth]{figures/figure_data_extraction}
\caption{Data extraction pipeline overview with an extracted example from arXiv paper~\citep{cheng2023batch}. Target attributes are identified and extracted from the table, augmented with paper content.}
\label{fig:data_extraction}
\end{figure*}


\begin{figure*}[t!]
\centering
\includegraphics[width=0.9\textwidth]{figures/figure_data_description}
\caption{Dataset description generation pipeline overview using the example of "SVAMP" dataset~\citep{patel2021nlp}. Since LLM lacked knowledge of the given dataset, the model references the original dataset's arXiv source retrieved through extracted BibTex. If confident, however, the LLM's generated descriptions will be used.}
\label{fig:data_description}
\end{figure*}


\section{Data Extraction}
\label{section:approach}

Automated data extraction is essential for scaling and improving literature analysis efficiency. 
This section outlines our pipeline for extracting experimental results and model attributes from arXiv sources, which are used to construct \datasetname.
We define target models and attributes (\cref{subsection:attributes}) and introduce our LLM-powered extraction process, which includes three stages: preprocessing \& filtering (\cref{subsection:preprocessing}), extraction \& augmentation (\cref{subsection:extraction}), and description generation (\cref{subsection:generation}).
Unless stated otherwise, we utilize GPT-4o \citep{hurst2024gpt} throughout the pipeline.

\subsection{Defining Target Models and Attributes}
\label{subsection:attributes}

\paragraph{Target Models} 

We analyze four leading proprietary models in the NLP/AI field: GPT-4~\citep{achiam2023gpt}, GPT4-o~\citep{hurst2024gpt}, Claude3 Opus~\citep{anthropic@claude}, and Gemini 1.0 Pro~\citep{team2023gemini}. 
Our analysis focuses on proprietary LLMs accessible via APIs that are not possible to fine-tune.\footnote{GPT-4o and Gemini 1.0 Pro became available for fine-tuning during our study period, but we used rule-based heuristics to filter out limited fine-tuned results.} We chose to exclude fine-tuned models from our study because comparing diverse fine-tuning methods would make controlled cross-study comparisons much more challenging. 
We also exclude recently released advanced reasoning models (GPT4-o1~\citep{GPT4o1} and Deepseek-R1~\citep{guo2025deepseek}) due to very limited published results to date. 
In the future, our pipeline can readily extract experimental data for these models along with those of other target models, as more studies are published. 
For further discussion, see \cref{sec:limitation}.


\paragraph{Target Attributes} To enable thorough analysis across different studies, we extract various performance-related fields from proprietary models.
The targeted attributes include: \textit{Dataset Name}, \textit{Subset}, \textit{Model Name}, \textit{Prompting Method}, \textit{Number of Demonstrations}, \textit{Metric Name}, and \textit{Performance}. 
Note that the subset refers to any division of the dataset, such as a subtask, domain, split, or language pair.
For instance, as shown in Fig.\ref{fig:data_extraction}, the arXiv paper with ID \textit{2301.08721}~\citep{cheng2023batch} presents experimental results for \textit{GPT-4}~\citep{achiam2023gpt} on the \textit{SVAMP} dataset~\citep{patel2021nlp}. 
The evaluation used \textit{Batch Prompting with CoT}~\citep{wei2022chain, cheng2023batch} and \textit{12 in-context samples}~\citep{brown2020language}, achieving an \textit{accuracy} score of \textit{95.0}.

\subsection{Preprocessing \& Filtering}
\label{subsection:preprocessing}

To extract the experimental results of target models and attributes, we used arXiv sources from January 2023 to December 2024, focusing on machine learning papers (cs.CV, cs.AI, cs.CL, cs.LG). 
arXiv LaTeX sources preserve structure better than parsed PDFs, enabling easier dataset extraction—especially from tables—using simple regex, while PDFs require complex tools. 
Hence, following prior work~\citep{kardas2020axcell, bai2023schema, wang2024charxiv}, we adopt arXiv LaTeX sources for data extraction, as over 90\% of arXiv papers are submitted in LaTeX format~\citep{frankston2024html}.

We downloaded LaTeX source files and applied regex-based methods to extract structured data like tables.
Before extraction, we filtered tables to reduce time and API costs by selecting those with target model results.
We also concentrated on leaderboard tables presenting benchmark results following~\citet{kardas2020axcell} and used Llama3.1-70B-Instruct \citep{dubey2024llama} for the filtering.


\subsection{Extraction \& Augmentation}
\label{subsection:extraction}


Based on the selected tables, we extract records from tables and paper contents. 
Using schema-driven extraction, we obtain records matching target attributes from tables, focusing only on relevant model data to save costs while maintaining accuracy. 
This selective approach significantly reduces API costs compared to extracting full results from each table~\citep{bai2023schema}. 
After table extraction, we augment records with context from the entire paper, adding experimental specifics. 
During this stage, we also gather BibTeX references for datasets to link and retrieve the original papers describing these datasets from arXiv.

\begin{table*}[ht]  
    \centering  
    \resizebox{\textwidth}{!}{
    \small % Further reduce font size  
    \setlength{\tabcolsep}{2pt} % Reduce horizontal padding  
    \renewcommand{\arraystretch}{0.9} % Reduce vertical padding  
    \begin{tabular}{c|c|c|c|p{4cm}|c|c|c|c|c}  
    
    \toprule
    
    \textbf{ArXiv ID} & \textbf{Table} & \textbf{Dataset} & \textbf{Subset} & \textbf{\quad\quad
    Dataset Description} & \textbf{Model} & \textbf{Shots} & \textbf{Prompt} & \textbf{Metric} & \textbf{Value} \\ \midrule  
    2301.08721 & 2 & CommonsenseQA & xx &   

\textbf{Dataset Summary} This dataset is a multiple-choice question answering dataset focused on commonsense knowledge...
& GPT-4 & 12 & Batch CoT & Acc & 86 \\ \midrule

2408.02718 & 3 & MMIU & Overall &   
    
\textbf{Dataset Summary} This dataset is designed for the domain of multimodal understanding, specifically focusing on tasks that require the integration of information from multiple images...
& GPT-4o & xx & xx & Accuracy & 55.7 \\ \midrule

       2405.02861 & 3 & LexBench & IED &   
 \textbf{Dataset Summary} This dataset is part of a comprehensive evaluation suite designed to test language models on various semantic phrase processing tasks...
& Claude3 & 0 & zero-shot & Accuracy & 66.3 \\ \midrule

       2409.19667 & 5 & ProGraph & BGT &   

\textbf{Dataset Summary} This dataset is a benchmark designed to evaluate the capability of large language models (LLMs) in graph analysis tasks...
& Gemini1.0 & xx & xx & Accuracy & 27.7 \\ 


\bottomrule  
    \end{tabular}}  
    \caption{Sampled dataset instances. 
    ‘xx’ indicates missing values from the paper.
    Dataset descriptions, including summaries, tasks, and subsets, are abbreviated due to space limits. Full versions are in \cref{appendix:dataset_examples}.
    }
    \label{tab:main-dataset-instances}  
\end{table*}  


\subsection{Dataset Description Generation}
\label{subsection:generation}

In addition to extracting structured representations of experiments, we also generate summary descriptions of the relevant tasks and datasets (Fig. \ref{fig:data_description}) that can aid in-depth literature analysis, such as classifying records by research subarea. 
We use a two-stage approach to create these descriptions. 
Initially, the LLM generates descriptions using its internal knowledge based on the dataset name and subset, which is cost-effective.
When the LLM is uncertain, especially with lesser-known datasets or due to a knowledge cut-off, we prompt it to extract descriptions using the full content of the source papers as a reference.
The source papers can be either the paper containing the table or the original dataset paper cited within it, which is retrieved using the BibTex references obtained from \cref{subsection:extraction}.
     