\section{Introduction}

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.95\columnwidth]
    {figures/figure_intro}
    \caption{The diagram of our semi-automated literature analysis process and a key finding derived from data automatically extracted from the arXiv database.}
    \label{fig:teaser}
\end{figure}

The rapid advancement of Large Language Models (LLMs)~\citep{brown2020language, chowdhery2022palm, achiam2023gpt, touvron2023llama, team2023gemini, anthropic@claude} has led to a proliferation of empirical studies. 
Recent \textit{``surveys of surveys''}~\citep{ABigSurvey, ABigSurveyOfLLMs} highlight the overwhelming growth of LLM publications, surpassing what individual researchers can manually review.
The growing number of evaluations—each using different models, datasets, and prompting configurations—makes it challenging to analyze and synthesize findings across studies, underscoring the urgent need for automated analysis.

A manual literature analysis
%\footnote{We use 'literature-analysis' instead of 'meta-analysis' because our approach does not meet the strict criteria involving statistical aggregation of results.}, 
that examines data from multiple independent studies to identify overall trends, offers a solution~\citep{reiter2018structured, hupkes2023taxonomy}.  
However, conducting such an analysis is time-consuming~\citep{yun2023appraising, yun2024automatically}.
For instance, as shown in Fig.~\ref{fig:teaser}, understanding the effects of combining Chain-of-Thought (CoT) with in-context examples requires identifying relevant papers, extracting specific experimental results, and aggregating the data.
Furthermore, analyses based on existing studies are likely to become quickly outdated in a fast-moving field, motivating the need for automated analyses that can be continuously updated as new results are shared on preprint servers such as arXiv.org.


In this study, we conduct the most extensive quantitative literature analysis of frontier LLMs to date. 
Our dataset, \datasetname, comprises 18,127 experimental records extracted from 1,737 papers, along with 359 referenced papers describing datasets used in these experiments. 
\datasetname~can also be dynamically updated with minimal human effort to incorporate results from new models or studies as they are published.
This comprehensive dataset enables us to present new insights into LLM performance, a first in the field where previous studies on extracting leaderboards from machine learning papers have primarily focused on improving data extraction accuracy \citep{kardas2020axcell, yun2024automatically}.


To achieve this, we experiment with an automated approach to data extraction that efficiently processes research literature. 
This approach uses an LLM to scan the arXiv database, identify relevant papers with experimental results on target models, and extract experimental results along with pertinent attributes. 
This reduces the manual effort needed for surveying and extraction by more than 93\% compared to expert manual data extraction.
The approach employs schema-driven extraction~\citep{bai2023schema} and context augmentation from paper contents to capture essential attributes for a comprehensive understanding of evaluation results. 
Additionally, the process creates dataset descriptions that summarize key characteristics of the dataset associated with the evaluation record, enhancing the dataset's potential utility in literature analysis. 

Our analysis shows that \datasetname~can efficiently replicate a previous manual analysis~\citep{sprague2024cot}, which found that CoT reasoning primarily benefits mathematical and symbolic reasoning tasks. 
We also identify three novel insights from the dataset on prompting configurations. 
First, in-context learning (ICL) enhances coding \& multimodal tasks but offers limited benefits for mathematical tasks compared to a zero-shot setup. 
Second, our qualitative analysis reveals specific characteristics of datasets that tend to reduce the effectiveness of CoT prompting and ICL.
Third, we find that CoT prompting with demonstrations—commonly implemented through ICL—typically yields better performance than CoT without demonstrations.
However, the relative improvement of CoT over standard direct prompting remains stable regardless of the number of demonstrations used (i.e., it holds true in both zero-shot and few-shot settings).
We release our dataset and source code to support the ongoing automated integration of new findings in the literature.
\footnote{The code is available at \url{https://github.com/JJumSSu/meta-analysis-frontier-LLMs}, and the dataset can be accessed at \url{https://huggingface.co/datasets/jungsoopark/LLMEvalDB}.}


% Second, our qualitative analysis identifies certain dataset characteristics linked to negative impacts when using CoT or in-context learning (ICL). 
% Lastly, CoT with demonstrations (i.e., ICL) generally outperforms CoT without them. 
% However, CoT's improvement over direct (i.e., standard) prompting stays consistent with the same number of demonstrations, whether zero-shot or few-shot.
% We hope our open-sourced dataset and code will help streamline literature analysis through automated and continuous study integration.


% \footnote{The code is available at \url{https://github.com/JJumSSu/meta-analysis-frontier-LLMs} and the dataset can be accessed at \url{https://huggingface.co/datasets/jungsoopark/LLMs-Performance-Data}.}
