\section{Limitations}
\label{sec:limitation}

\paragraph{Target Model Scope}
Our study focuses on four leading proprietary LLMs selected for their widespread adoption and extensive documentation in existing literature. 
We excluded newer models like GPT4-o1 and Deepseek-R1 due to limited published results, though our analysis pipeline can easily accommodate them as more experimental data becomes available. 
While most analyzed models are not fine-tunable, this limitation presents an opportunity for future research combining fine-tuning and prompting methods analysis.

\paragraph{Further Validation}

Our literature analysis aims to identify trends by aggregating information across multiple studies, which generates potential hypotheses but requires systematic validation. 
While we report findings and patterns observed in the aggregated scientific literature, we did not independently validate each claim or finding. 
This limitation suggests the need for future work to rigorously test the hypotheses emerging from our analysis.

\paragraph{Attributes' Descriptiveness}
We frequently observed that the extracted attributes were not descriptive enough, which can hinder the dataset's utility for further analysis. 
Techniques like \textit{Batch COT}, for example, would benefit from more detailed descriptions. 
Additionally, the dataset descriptions could be enhanced to better differentiate between various dataset characteristics. 
Our attempts to further generate the ``collection process'' of the dataset resulted in numerous inaccuracies. Moreover, efforts to automatically link descriptions to actual dataset instances also encountered technical challenges, necessitating extensive manual intervention. 
Future work should aim to develop more effective methods for comprehensive dataset characterization.

\paragraph{Dataset Canonicalization}
Cross-study analysis requires standardizing dataset names and formats. 
While we implemented strict rules for dataset canonicalization, our approach likely missed potential matches. 
Alternative matching techniques we explored using LLM produced too many false negatives, whereas linking to the PaperswithCode dataset ontology\footnote{https://paperswithcode.com/datasets} was limited by its incomplete coverage of datasets.
