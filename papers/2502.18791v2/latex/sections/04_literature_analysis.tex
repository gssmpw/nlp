\section{Prompting Behavior in Frontier LLMs: A Literature Review}
\label{section:literature_analysis_prompting_behavior}


\datasetname~enables partially automated literature analyses of LLM-prompting behaviors through its structured attributes.
We validate our semi-automated approach to literature analysis by replicating \citet{sprague2024cot}'s manual analysis, confirming CoT's advantages over direct prompting in mathematical and symbolic reasoning tasks (\cref{subsection:effect_cot}). 
We then show that \datasetname~enables a fine-grained analysis, showing that in-context examples boost performance in coding \& multimodal tasks but not in mathematical ones (\cref{subsection:effect_icl}). 
We qualitatively analyze dataset characteristics, such as required expert knowledge, that may negatively affect the performance of CoT and ICL (\cref{subsection:negative_analysis_dataset_characteristics}).
Moreover, we analyze CoT and ICL interactions, finding that while demonstrations enhance CoT performance, they do not affect CoT's relative improvement over standard direct prompting (\cref{subsection:joint_behavior}).
Finally, we analyze a subset of peer-reviewed papers published in major journals or conferences, revealing consistent trends that further support the robustness and generalizability of our findings (\cref{subsection:subset_analysis}).



\subsection{Which Tasks Benefit from CoT?}
\label{subsection:effect_cot}


\begin{figure}[t!]
    \centering  
    \includegraphics[width=0.9\columnwidth]{figures/figure_literature_analysis_cot.pdf}
    \caption{CoT shows significant performance improvement over direct prompting in mathematical tasks, whereas its impact on reasoning tasks is less distinct due to their complexity and diversity.
    Grey dots indicate individual deltas (improvements), blue dots represent the mean delta per paper, and a purple star marks the mean delta for each category.}
    \label{fig:cot_analysis_our_category}
\end{figure}


\paragraph{Motivation and Setup} 
CoT~\citep{wei2022chain}, a prompting technique for eliciting reasoning, has attracted considerable attention, with extensive literature on the subject \citep{wang2023towards, yao2024tree}
Recently, \citet{sprague2024cot} conducted a manual analysis, concluding that \textit{CoT improves over standard direct prompting primarily in mathematical and symbolic reasoning tasks}.  
We extract instances from \datasetname~that meet the criteria for replicating their investigation. 

This identification is achieved through filtering using the dataset's attributes. 
We focus on instances from the same source paper and table, identifying experiments that feature both CoT and standard direct prompts under the same conditions (model, dataset, subset, metric, and few-shot setup).
Human input is used only to identify whether a prompt is a CoT prompt or a standard direct prompt based on the \textit{prompting method} attribute; this is the only manual effort required for the analysis aside from implementing the analysis itself. 
We exclude CoT variations like "xx of Thoughts" \citep{yao2024tree} and CoT-SC \citep{wang2022self}. 
We then calculate CoT's performance improvement over standard direct prompting, namely \emph{delta}, where a positive delta indicates CoT outperforming direct prompting, and a negative delta indicates the reverse.

% For the category information of each delta, we utilize the category details provided in~\cref{subsection:core_skills}.

\begin{figure}[t!]
    \centering  
    \includegraphics[width=0.9\columnwidth]{figures/figure_literature_analysis_cot_replica_truncated}
    \caption{In the reasoning category, CoT shows notable improvement over direct prompting in \emph{Symbolic Reasoning} tasks compared to other reasoning tasks. The figure displays an abbreviated version of our study, using categories defined by \citet{sprague2024cot}.
    }
    \label{fig:cot_improvement_verification}
\end{figure}



\paragraph{Analysis} 
Our analysis, shown in Fig.\ref{fig:cot_analysis_our_category}, reveals that CoT significantly enhances performance in mathematical tasks, with both median and mean improvements surpassing the overall median. However, we do not see clear improvements for reasoning tasks, likely due to their diversity, such as commonsense and logical reasoning. 

To investigate further, we apply more detailed classification categories (but less general) from \citet{sprague2024cot} and redo our study using LLM classification. 
The truncated results, presented in Fig.~\ref{fig:cot_improvement_verification}, clearly show that symbolic reasoning and mathematical tasks exhibit notable improvement over other reasoning and non-reasoning tasks. 
This confirms the reliability and efficiency of \datasetname~for literature analyses, as it provides consistent results with those obtained through manual curation in previous studies. 
Statistical tests, as shown in \textit{Total Results} from Table. \ref{tab:statistical-tests}, confirm these patterns.


\subsection{Which Tasks Benefit from ICL?}
\label{subsection:effect_icl}


\paragraph{Motivation and Setup}

Previous research has systematically explored ICL behavior~\citep{min2022rethinking, agarwal2024many, wei2023larger, bertsch2024context}. 
In line with our study in~\cref{subsection:effect_cot}, we analyze the improvement of using in-context examples compared to not using any. 
To achieve this, we extract instances from \datasetname~that come from the same papers and tables comparing few-shot and zero-shot setups under identical conditions (i.e., model, dataset, subset, metric, and prompting method).
No manual labor was needed beyond implementation, as the \textit{number of in-context examples} is an integer value that can be easily computed for filtering. 
We measure performance deltas between few-shot and zero-shot setups, where positive values indicate few-shot advantages and negative values indicate the reverse.


\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\columnwidth]{figures/figure_literature_analysis_few_shot_vs_zero_shot.pdf}
    \caption{
    Few-shot learning significantly improves performance over zero-shot learning in \emph{coding} and \emph{multimodal} tasks, whereas its impact on math tasks is less pronounced compared to CoT, as shown in Fig.~\ref{fig:cot_analysis_our_category}.
     Grey dots represent individual deltas (improvements), sky pink dots show the mean delta per paper, and a pink star marks the mean delta for each category.}
    \label{fig:icl_improvement}
\end{figure}

\paragraph{Analysis}
Figure \ref{fig:icl_improvement} presents the results. 
In contrast to CoT's strong performance in mathematical reasoning, ICL shows only modest benefits for math tasks.
However, ICL demonstrates more substantial improvements in coding and multimodal applications despite considerable performance variability across different cases.
This variance may be due to the broad scope of multimodal tasks, including speech and image processing. 
ICL shows more uniform improvements over zero-shot performance across different categories compared to the study in Fig.~\ref{fig:cot_analysis_our_category} (std: 8.3).
Note that we did not account for the number of in-context examples used; we only distinguished between the use of in-context examples and zero-shot examples. 
We also plot the performance improvement distribution of more versus fewer in-context examples in \cref{appendix:details_in_meta_analysis} and find a similar overall median. 
This suggests that the presence of demonstrations is more important than their quantity.


\subsection{Which Dataset Characteristics Negatively Affect CoT and ICL?}
\label{subsection:negative_analysis_dataset_characteristics}

\begin{table}[t!]
\centering
\resizebox{0.9\columnwidth}{!}{
\begin{tabular}{l c c}
\toprule
\textbf{Key Characteristic} & \textbf{CoT Ratio} & \textbf{ICL Ratio} \\
\toprule
Expert Knowledge & 31.6\% & 31.0\% \\
Faithfulness & 20.9\% & 4.6\% \\
Complex Reasoning & 17.9\% & 13.8\% \\
Information Synthesis & 9.7\% & 10.3\% \\
Cognitive Tasks & 8.7\% & - \\
Affective Analysis & 7.1\% & 13.8\% \\
Structured Prediction & - & 12.6\% \\
Other & 2.5\% & 13.8\% \\
\bottomrule
\end{tabular}}
\caption{A qualitative analysis of dataset characteristics that resulted in decreased performance with CoT or ICL, compared to direct prompting or zero-shot set-up.
Key characteristics refer to the representative traits identified through qualitative analysis. The CoT and ICL ratios indicate the proportion of these traits among all negative cases for each method.}
\label{tab:dataset_negative_analysis}
\end{table}


\paragraph{Motivation and Setup} 
To comprehend the patterns of performance degradation, we analyze cases where performance declined: specifically, where CoT resulted in worse outcomes than direct prompting and where few-shot learning performed worse than zero-shot learning (i.e. deltas below zero). 
Table~\ref{tab:dataset_negative_analysis} summarizes the key dataset characteristics associated with performance declines under both approaches based on analyzing the descriptions.


\paragraph{Analysis} 
Out of the cases of performance decline, tasks requiring expert-level knowledge showed the highest ratio for both cases, approximately 31\%, indicating that knowledge-intensive tasks do not benefit much from different prompting configurations alone. 
\citet{sprague2024cot} notes a similar point that apparent performance gains in knowledge tasks mainly stem from "reasoning" or "math" components in datasets like MMLU~\citep{hendrycksmeasuring}.  
This also aligns partly with \citet{liu2024mind}, who found that CoT can hinder LLM performance on tasks requiring minimal deliberative thinking.  

The two approaches show different patterns in other aspects. 
CoT significantly degraded performance in faithfulness and factual verification tasks, possibly due to introduced reasoning biases \citep{shaikh2023second, chua2024bias}. 
In contrast, demonstrations led to notable declines in affective analysis and structured prediction tasks, where examples may bias the model toward imitating specific patterns rather than genuine reasoning. 
This also partially aligns with the finding from \citet{zhang2022robustness} that demonstrations increase the confidence of model predictions on captured superficial patterns, possibly incurring biases for structure prediction tasks such as NER.

\begin{table}[t]  
\centering  
\resizebox{0.87\columnwidth}{!}{  
\begin{tabular}{lccc}  
\toprule  
\textbf{CoT Reasoning} & \textbf{Median} & \textbf{Q1} & \textbf{Q3}  \\  
\toprule  
$\Delta$ (Few-shot, Zero-shot) & 3.0  & 0.4 & 9.2 \\  
$\Delta$ (More-shot, Less-shot) & 3.1  & 0.4 & 9.0  \\  
\bottomrule  
\end{tabular}  
}  
\caption{Statistical distribution of performance improvements when using varying levels of demonstrations in CoT prompting. $\Delta$(A, B) refers to the performance improvement of A over B, Q1 and Q3 represent the first and third quartiles, respectively.}  
\label{tab:cot-demo-improvements}  
\end{table}


\begin{table}[t]  
\centering  
\resizebox{0.95\columnwidth}{!}{  
\begin{tabular}{lccc}  
\toprule  
\textbf{$\Delta$ (CoT, Direct Prompting)} & \textbf{Median} & \textbf{Q1} & \textbf{Q3} \\  
\toprule  
% \multicolumn{4}{l}{\textbf{CoT Improvement over Standard Prompting}} \\  
Few-shot & 0.9 & -1.2 & 3.7 \\  
Zero-shot & 1.3 & -0.4 & 4.7 \\  
\bottomrule  
\end{tabular}  
}
\caption{Statistical distribution of performance improvements for CoT compared to direct prompting given different levels of demonstrations.}  
\label{tab:cot-vs-standard}  
\end{table}



\begin{table*}[ht]  
\centering  
\small  
\renewcommand{\arraystretch}{1.3}  
\begin{tabular}{l|c|cc|c|cc}  
\toprule  
\multirow{2}{*}{\textbf{Category}} & \multicolumn{3}{c|}{\textbf{Total Results}} & \multicolumn{3}{c}{\textbf{Filtered Results}} \\  
& Mean $\Delta$ & p-value & Significant & Mean $\Delta$ & p-value & Significant \\  
\midrule  
Math & 14.61 & 0.0000 & Yes & 13.53 & 0.0000 & Yes \\  
Symbolic and algorithmic & 8.85 & 0.0002 & Yes & 9.13 & 0.0000 & Yes \\  
Spatial and temporal reasoning & 3.03 & 0.0166 & No & 2.07 & 0.0056 & No \\  
Logical reasoning & 2.39 & 0.0084 & No & 1.18 & 0.3776 & No \\  
Commonsense reasoning & 5.41 & 0.0450 & No & 5.61 & 0.0748 & No \\  
Multi-hop QA & 2.05 & 0.0000 & Yes & 1.21 & 0.0064 & No \\  
Context-aware QA & 2.45 & 0.0014 & Yes & 0.28 & 0.7060 & No \\  
Encyclopedic knowledge & 2.18 & 0.0076 & No & 3.31 & 0.0138 & No \\  
Generation & 4.24 & 0.0280 & No & 1.92 & 0.3920 & No \\  
Text classification & 1.01 & 0.4464 & No & 0.27 & 0.8644 & No \\  
Entailment & 0.81 & 0.2070 & No & 0.93 & 0.1666 & No \\  
\bottomrule  
\end{tabular}  
\caption{Statistical test results across different categories for replicating the study from \citet{sprague2024cot}. \textbf{Total Results} refers to the results without filtering any papers. \textbf{Filtered Results} refer to the results from the filtered papers that are published in peer-reviewed journals \& conferences. Mean $\Delta$ shows the average improvement when using CoT over standard prompting, and significance is determined at $p=0.00227$ after applying a Bonferroni correction.}  
\label{tab:statistical-tests}
\end{table*}  


\subsection{How do CoT and ICL Interact?}
\label{subsection:joint_behavior}


\paragraph{Motivation and Setup}
Building on previous studies \cref{subsection:effect_cot} and \cref{subsection:effect_icl}, our analysis uncovers how CoT reasoning interacts with demonstrations in prompting, highlighting their complementarity. 
We extract instances from \datasetname~where CoT is used with varying levels of demonstrations, as well as instances that compare CoT and direct prompting under different demonstration levels (zero-shot vs. few-shot), all while keeping other attributes constant.

\paragraph{Analysis} 
From Table~\ref{tab:cot-demo-improvements}, demonstrations consistently enhance CoT performance: few-shot CoT outperforms zero-shot CoT by 3.0 points, and increasing the number of demonstrations yields a similar 3.1-point improvement. 
These comparable gains suggest that the mere presence of demonstrations, rather than their quantity, drives the improvement.
However, from Table~\ref{tab:cot-vs-standard}, we can see that when comparing CoT against standard prompting, the benefits remain relatively constant regardless of demonstration use. 
CoT's improvement over standard prompting is similar in both few-shot and zero-shot settings. 
This indicates that while demonstrations enhance both approaches, they do not amplify CoT's relative advantage over standard prompting.


\subsection{How Do Trends in Peer-Reviewed Papers Compare to Those in arXiv Publications?}
\label{subsection:subset_analysis}


\paragraph{Motivation and Setup}

\datasetname~includes information from papers published on arXiv. To examine whether the trends observed in our general analysis align with those in more selective venues, we conducted a focused analysis on a subset of peer-reviewed papers accepted at journals or conferences, using metadata from DBLP.\footnote{\url{https://dblp.org/}}
We report statistical significant test results from \ref{subsection:effect_cot} using a subset of peer-reviewed papers. Following \citet{sprague2024cot}, we perform a one-sided bootstrap test to assess whether each category shows performance gains (mean improvement > 0). 


\paragraph{Analysis} 

Table~\ref{tab:statistical-tests} represents the results.
The statistically significant test results from our \textbf{Total Results} and \textbf{Filtered Results} indicate that math and symbolic reasoning tasks derive substantial benefit from using Chain-of-Thought (CoT) prompting, which aligns with both manual analysis~\citep{sprague2024cot} and our examination in \cref{subsection:effect_cot}.
We can see that the core finding remains consistent even if we used a subset of peer-reviewed papers.
This invariance in analysis patterns extends to other examinations like those in \cref{subsection:joint_behavior}, with additional details provided in Appendix~\ref{appendix:details_in_meta_analysis}.
These consistent results across both the complete dataset and the filtered selection of high-quality published papers serve to corroborate our findings and implicitly confirm our hypothesis.


