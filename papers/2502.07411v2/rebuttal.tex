\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage[rebuttal]{cvpr}

% Include other packages here, before hyperref.
\usepackage{bm}
\usepackage{graphicx}
\usepackage{pifont}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{threeparttable}
\usepackage{makecell}
\usepackage{cancel}
\usepackage{color}
\usepackage{multirow}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{cases}
\renewcommand\thetable{R\arabic{table}}
\renewcommand{\arraystretch}{0.72}
\usepackage{caption}
\captionsetup[table]{font={stretch=0.5}}

\newcommand{\RA}{{\color{cyan}4WdF}}
\newcommand{\RB}{{\color{orange}x9hY}}
\newcommand{\RC}{{\color{green}VC77}}

\definecolor{lightred}{RGB}{255, 230, 230}
\definecolor{lightgreen}{RGB}{230, 255, 230}
\definecolor{lightyellow}{RGB}{255, 255, 204}
\definecolor{lightorange}{RGB}{255, 229, 204}
\definecolor{lightblue}{RGB}{204, 229, 255}
\definecolor{limegreen}{RGB}{76, 175, 80} 
\definecolor{verylightgray}{gray}{0.95} 

\newcommand{\cmark}{\textcolor{green}{\ding{51}}} % 深绿色的勾
\newcommand{\xmark}{\textcolor{red}{\ding{55}}} % 红色的叉
\newcommand{\dataset}{{EgoTextVQA}}
\newcommand{\datasetout}{{EgoTextVQA-Outdoor}}
\newcommand{\datasetin}{{EgoTextVQA-Indoor}}


\newcommand{\JB}[1]{{\color{red} {#1}}} 
\newcommand{\ZS}[1]{{\color{blue} {#1}}} 

% Import additional packages in the preamble file, before hyperref
\input{preamble}

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

% If you wish to avoid re-using figure, table, and equation numbers from
% the main paper, please uncomment the following and change the numbers
% appropriately.
%\setcounter{figure}{2}
%\setcounter{table}{1}
%\setcounter{equation}{2}

% If you wish to avoid re-using reference numbers from the main paper,
% please uncomment the following and change the counter value to the
% number of references you have in the main paper (here, 100).
%\makeatletter
%\apptocmd{\thebibliography}{\global\c@NAT@ctr 100\relax}{}{}
%\makeatother

%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\paperID{1230} % *** Enter the Paper ID here
\def\confName{CVPR}
\def\confYear{2025}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{}  % **** Enter the paper title here
% EgoTextVQA: Towards Egocentric Scene-Text Aware Video Question Answering

\maketitle
\thispagestyle{empty}
\appendix

%We thank the reviewers for finding our proposed task promotive (R-\RA), our datasets novel (R-\RA), admired (R-\RB), and valuable (R-\RC), our model effective (R-\RB), and our experiments extensive (R-\RB) and comprehensive (R-\RC). Below we list our detailed responses.

%%%%%%%%% BODY TEXT - ENTER YOUR RESPONSE BELOW
%We thank all reviewers for their valuable time and comments. We are grateful that the reviewers think our proposed task challenging (\RA),  and the dataset is meaningful to advance research in egocentric scene-text QA assistance (\RB, \RC). We address the concerns below.
We thank all reviewers for their valuable time and feedback. We appreciate that they find our proposed task challenging (\RA) and recognize the dataset's significance in advancing research on egocentric scene-text QA assistance (\RB, \RC). We address the concerns below.

\noindent{\underline{\textbf{Reviewer~\#~\RA}}} \noindent\textbf{\textcolor{cyan}{- Q1:}}~\textbf{{Cross-validation?}}~The manual check has 5 stages (Section 3.1). The participants at each stage (except the 4th) are asked to first check the remaining QAs from the prior stage. Thus, the final QA set is cross-validated by 4 different people. 
% Moreover, the first two authors participant into all stages for checking.
% To clarify, our QA pairs go through a rigorous five-round cross-checking process, with each QA checked at least \underline{three times} by different trained annotators: (1) Five annotators perform quality checks, reducing the QA set to $\sim$50\% of the original. (2) and (3) Two annotators in each round review and verify the results of the previous round, further reducing the acceptable QA to $\sim$30\%. By this stage, each QA has been reviewed by at least three different annotators. (4) Five annotators refine the data to improve its colloquial quality. (5) Finally, another four annotators conduct a comprehensive dataset review.

\noindent\textbf{\textcolor{cyan}{- Q2:}}~\textbf{{Unreadable text.}}~To limit this, we use a robust OCR system to filter out frames without (or w/ unreadable) text (Section 3.1) before QA generation. Also, we prompt GPT-4o to generate \emph{scene-text aware QA pairs}, and manually check to ensure that QAs are from readable scene text. 


% after rigorous manual review (\textbf{Q1A1}), the retained QA pairs ensure readable scene text. The lower human performance in Table 4 compared to closed-source models is due to low resolution, motion blur, and text localization challenges in long videos, making scene text \emph{difficult but not impossible} to read. 
% Experiments show that key frame input (Table 5) and recognized scene text input (Table 7) help models locate and recognize text. Enhancing scene text localization with OCR detection and super-resolution techniques can address these challenges and advance scene-text-aware MLLM research.

%It should be noted that after rigorous manual review (\textbf{Q1A1}), the retained QA pairs ensure that the scene text involved is readable. One of the reasons for the lower human performance compared to closed-source models in Table 4 is that scene text recognition is \emph{difficult rather than essentially unrecognizable}. This challenge is mainly due to low resolution, motion-induced text blur, and difficulty in localizing text in long videos. The experiments show that key frame input (Table 5) and recognized scene text input (Table 7) can help the model locate and recognize scene text to answer questions. Thus, focusing on scene text localization and incorporating OCR detection and super-resolution techniques can effectively alleviate the difficulties of scene text recognition, and is also a valuable direction for advancing the current research of scene-text aware MLLM.

\noindent\textbf{\textcolor{cyan}{- Q3:}}~\textbf{{Clarity on external knowledge.}}~Our analyses reveal that \datasetout~involves knowledge about store or restaurant chains (\eg, ``Home Depot" sells building materials), and \datasetin~focuses on items like kitchen utensils and board games (\eg, a ``mandoline" slices potatoes). More details will be included in the revision.

% To clarify the external knowledge involved in answering questions in our dataset—particularly knowledge that models might possess but human annotators might lack—we analyzed cases where the best-performing model, Gemini-1.5 Pro, answered correctly while humans did not. In the EgoTextVQA-Outdoor subset, external knowledge often includes understanding what specific stores or restaurants offer, such as Home Depot specializing in building materials (“Q: Which store should I go to buy building materials? A: Home Depot”) or associating brands with product categories. In the EgoTextVQA-Indoor subset, it involves knowledge of repair tools, kitchen utensils, board games, cooking, and specialized books. For example, “Q: Where can I find instructions about quantum field theory? A: Quantum field theory book” requires recognizing specialized literature, and “Q: What should be used to slice onions? A: Mandoline slicer” relies on understanding kitchen tools.

\noindent\textbf{\textcolor{cyan}{- Q4:}}~\textbf{{GPT-4o~mini evaluation.}}~We conduct human evaluation on 100 random samples from \datasetout. Tab.~\ref{tab:re_tab1} shows that humans and GPT-4o~mini give similar Acc and Score, with \emph{Pearson correlation coefficients} ({\bf0.80/0.87} for Acc/Score) indicating high consistency.
Future evals can ensure consistency by using the same model (GPT-4o-mini-2024-07-18) and prompt (Tab. 6, appendix).
% We conduct human evaluation on 100 random samples from \datasetout. Table~\ref{tab:re_tab1} shows that the Acc and Score are close between human and GPT-4o mini. Besides, the {\emph{Pearson correlation coefficients between human and GPT are {\bf0.80/0.87} for Acc/Score}}, suggesting high consistency. Future evaluation can use the same GPT (GPT-4o-mini-2024-07-18) and prompt (Table 6 in appendix) for evaluation. 
% The Cohen's Kappa coefficients (-1$\sim$1) among three volunteers are 0.77 on Acc, indicating a high human agreement.
%and 0.47 on Score, (Because the 0-5 scoring is more detailed and may produce more small differences, Cohen's Kappa will be lower.) 

\vspace{-0.3cm}
\begin{table}[h]
\caption{\small Judgments of human and GPT-4o mini.}
\vspace{-0.3cm}
\label{tab:re_tab1}
\setlength{\tabcolsep}{.6em}
\centering
\fontsize{6}{8}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ll|ll}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{GPT-4o~[27]}} & \multicolumn{2}{c}{\textbf{Gemini-1.5 Pro~[29]}}  \\
\cline{2-5}
& Accuracy & Score & Accuracy & Score  \\
\hline
Human  & 36.0 & 1.9 & 47.3 & 2.5 \\
GPT-4o mini  & 34.0 & 1.8 & 42.0 & 2.3 \\
\hline
\end{tabular}
\end{threeparttable}}
\vspace{-0.35cm}
\end{table}
% qwen2vl & 25.0 & 2.25
% GPT-4o  & 34.0 & 1.78


\noindent\textbf{\textcolor{cyan}{- Q5:}}~\textbf{{Humans allowed playback / zoom?}}~Yes, they are allowed; the only constraint is that human's answers must be based on video content \emph{before the question timestamp}.
% , replay videos, and zoom in and out to identify scene text, ensuring a fair evaluation of performance. 


\noindent{\underline{\textbf{Reviewer~\#~\RB}}}
\noindent\textbf{\textcolor{orange}{- Q1:}}~\textbf{{Dataset novelty?}}~EgoTextVQA focuses on egocentric \emph{scene-text QA} and reflects \emph{real-user needs}. It has timestamps to facilitate \emph{live QA}. Additionally, we include driving -- an important application scenario for Ego-VQA assistance. These aspects are not addressed in the suggested datasets, making EgoTextVQA a valuable contribution. More comparisons will be added in the revision.

% \vspace{-0.35cm}
% \begin{table}[h]
% \centering
% \caption{\small Comparison with egocentric VideoQA datasets. EgoQ: QAs with human needs. 
% OE: Open-ended. MC: Multi-choice.}
% \label{tab:re_tab2}
% \setlength{\tabcolsep}{.4em}
% \vspace{-0.3cm}
% \centering
% \fontsize{7}{9}\selectfont
% \resizebox{\columnwidth}{!}{
% \begin{threeparttable}
% \begin{tabular}{lccccccc}
% \hline
% \textbf{Benchmark} & \textbf{STQ} & \textbf{STA} & \textbf{EgoQ} & \textbf{IntQ}  & \textbf{QC}  &  \textbf{TS} &  \textbf{Set} \\ 
% \hline 
% QAEgo4D~[6] & \xmark & \xmark & \xmark &  \xmark & \xmark & \xmark & OE \\ 
% EgoSchema~[23] & \xmark & \xmark & \xmark & \xmark & \xmark & \xmark & MC \\ 
% EgoMemoria~[{\color{blue}1}] & \xmark & \xmark & \xmark & \cmark & \cmark & \cmark & MC \\
% \rowcolor{gray!20} \dataset~(Ours) & \cmark & \cmark & \cmark & \cmark & \cmark & \cmark & OE \\ 
% \hline
% \end{tabular}
% \end{threeparttable}}
% \vspace{-0.5cm}
% \end{table}

\noindent\textbf{\textcolor{orange}{- Q2:}}~\textbf{{Effects of model designs.}}~Tables~3 and 4 present results based on the models' optimal settings. We analyze key factors contributing to the superior performance of strong models (Qwen2-VL, LLaVA-NeXT-Video, and InternVL2): 
1) large-scale OCR training data, 
2) more powerful LLM backbones, and 
3) better visual encoder that accept high-resolution and long-video inputs.
We provide more results in Table~\ref{tab:re_tab3} for reference. When the number of video frames increases, Qwen2-VL's performance improves, while InternVL2's declines, highlighting Qwen2-VL's superior video embedding design. The revision will include more architectural analysis for different models.
% It shows that more frames benefit Qwen2-VL while larger LLM helps InternVL2.
% emphasizes the importance of the language model. Notably, Qwen2-VL outperforms InternVL2 in handling multiple frames, underscoring the significance of visual encoder and video position embedding design. %This analysis will be added in the revision.

% \vspace{-0.3cm}
% \begin{table}[h]
% \caption{\small Study of MLLM design on \datasetout. Res.=$448^{2}$.  Alignment module is MLP layer. VE: Visual Encoder.
% }
% \label{tab:re_tab3}
% \setlength{\tabcolsep}{.3em}
% \vspace{-0.3cm}
% \centering
% \fontsize{8}{10}\selectfont
% \resizebox{\columnwidth}{!}{
% \begin{threeparttable}
% \begin{tabular}{l|l|l|l|ll}
% \hline 
% \textbf{Method} & \textbf{LLM} & \textbf{VE} & \textbf{\#F} & Accuracy & Score  \\
% \hline
% \multirow{2}{*}{\makecell[c]{Qwen2-VL~[32] }}
% & Qwen2-7B & ViT-675M & 16 & 22.4 & 1.6 \\
% & Qwen2-7B & ViT-675M & 32 &  23.6{\color{limegreen} $\uparrow$ \bf1.2} & 1.7\\
% \hline 
% \multirow{2}{*}{InternVL2~[4]}  & InternLM2.5-7B & InternViT-300M & 16 &  16.5 & 1.3 \\
% & InternLM2.5-7B & InternViT-300M & 32 & 16.4 {\color{red} $\downarrow$ \bf0.1} & 1.3 \\
% \hline 
% \end{tabular}
% \end{threeparttable}}
% \vspace{-0.3cm}
% \end{table}

\vspace{-0.3cm}
\begin{table}[h]
\caption{\small Study of MLLM design on \datasetin. Res.=$448^{2}$.  Alignment module is MLP layer. VE: Visual Encoder.
}
\label{tab:re_tab3}
\setlength{\tabcolsep}{.3em}
\vspace{-0.3cm}
\centering
\fontsize{8}{10}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|l|l|l|ll}
\hline 
\textbf{Method} & \textbf{LLM} & \textbf{VE} & \textbf{\#F} & Accuracy & Score  \\
\hline
\multirow{2}{*}{\makecell[c]{Qwen2-VL~[32] }}
& Qwen2-7B & ViT-675M & 16 & 19.8 & 1.6 \\
& Qwen2-7B & ViT-675M & 32 & 21.9 {\color{limegreen} $\uparrow$ \bf2.1} & 1.7 \\
% & Qwen2-7B & ViT-675M & 48 &  23.3  & 1.8 \\
\hline 
\multirow{2}{*}{InternVL2~[4]}  & InternLM2.5-7B & InternViT-300M & 16 & 13.8 & 1.3 \\
& InternLM2.5-7B & InternViT-300M & 32 & 13.6 {\color{red} $\downarrow$ \bf0.2 } & 1.2 \\
\hline 
\end{tabular}
\end{threeparttable}}
\vspace{-0.3cm}
\end{table}

\noindent\textbf{\textcolor{orange}{- Q3:}}~\textbf{{Human results in Table 4.}}~The human results in Table~4 are based on two rounds of standard human studies. To further address this concern, we sample 100 additional questions for humans to answer by providing the corresponding question frames. 
Table~\ref{tab:re_tab4} shows that humans perform better without the challenge of temporal grounding but still lag behind the best closed-source model (GPT-4o). This suggests advanced models may surpass most humans in scene-text recognition or external knowledge, highlighting the importance of research on scene-text QA assistance.
% the best model still outperforms humans due to its superior text recognition and ability to provide more detailed answers. This reaffirms our dataset's goal of advancing human-assisted AI research.
% (\textbf{{\color{red}Merge with R1Q2}})

\vspace{-0.2cm}
\begin{table}[h]
\caption{\small Results of using QA frames (three frames for QA generation) vs video on \datasetin.}
\label{tab:re_tab4}
\vspace{-0.3cm}
\setlength{\tabcolsep}{.6em}
\centering
\fontsize{8}{10}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ll|ll}
\hline 
\multirow{2}{*}{\textbf{Method}}  & \multicolumn{2}{c|}{\textbf{Video}} & \multicolumn{2}{c}{\textbf{QA Frames}}  \\
\cline{2-5}
& Accuracy & Score & Accuracy & Score  \\
\hline
Human  &  26.0  & 1.9  & 36.0 & 2.3   \\ 
GPT-4o~[27]   & 25.0  & 1.6  &  \bf39.0 &  \bf2.4  \\
Gemini-1.5~Pro~[29]   & \bf33.0  & \bf2.0 & 35.0 &  2.2  \\ 
\hline 
\end{tabular}
\end{threeparttable}}
\vspace{-0.3cm}
\end{table}


\noindent\textbf{\textcolor{orange}{- Q4:}}~\textbf{{Annotation agreement \& revised question ratios?
}}~After cross-validation ({Reviewer~\RA-\textcolor{cyan}{Q1}}), about 70\% of the generated QAs were deleted and 30\% of the remaining QAs were revised. This means that all QAs in the final dataset were manually agreed.


%About 70\% of the generated QAs are removed after the initial two rounds of human checking. The remaining 30\% have two additional round of checks and are corrected as needed. This means that all QAs in the final dataset are agreed and modified by humans.



 % Thus, 4 people agreed on each QA in the final dataset. % The manual check removed 70\% QAs and modified the left 30\%.
% After manual checking and correction, over 70\% of the QAs are deleted and the remaining 30\% are modified.
% standardized for colloquial expression.

% (\textbf{{\color{red}Merge with R1Q1, R1Q4}})


\noindent\textbf{\textcolor{orange}{- Q5:}}~\textbf{{More closed-source model results.}}~Thanks. We will add more closed-source models in the revision.


\noindent{\underline{\textbf{Reviewer~\#~\RC}}}
\noindent\textbf{\textcolor{green}{- Q1:}}~\textbf{{Unsuitable track?}}~Aside from the dataset, we introduce several heuristic solutions to improve scene-text VideoQA, hence we select the ``vision, language, reasoning" track. Regardless, we don't think this should be considered a major weakness of our work.

% Our work proposes a dataset and many heuristic solutions, hoping to advance MLLM research in vision and language fields.

\noindent\textbf{\textcolor{green}{- Q2:}}~\textbf{{Limited dataset scenarios.}}~While there is room for greater diversity, the current version already presents key challenges for scene-text QA assistance research, as evidenced by the poor performance of existing models. 
% Yet, thanks, we will consider extending the data scope in the future.

\noindent\textbf{\textcolor{green}{- Q3:}}~\textbf{{Open-ended vs multi-choice QA.}}~Multi-choice QA ensures deterministic evaluation but answer bias often hinders visual reasoning [37]. Open-ended QA better aligns with real-world applications. Language diversity can be handled with LLM evaluators, while answer subjectivity can be mitigated by providing {multiple reference answers as ground truth -- an issue we aim to address soon.}

% This also aligns with existing TextVQA and some egocentric VideoQA datasets that use open-ended setting (\eg, ST-VQA and QAEgo4D).
% We will provide multiple correct answers to alliviate answer diverity.
% This setting presents greater challenges and practical value. While answer diversity poses evaluation challenges, providing multiple correct answers, as suggested in our paper, offers a viable solution.

\noindent\textbf{\textcolor{green}{- Q4:}}~\textbf{{Missing citations.}}~Thanks for the suggestions. We are happy to include them. However, our focus is zero-shot \emph{scene-text QA}, which the suggested works do not support. %capabilities of MLLMs. All suggested works cannot do this. %Yet,  we will add a discussion on these works.

% The suggested methods is considered  
% Our main goal is to advance TextVideoQA. Unlike the cited works and Table~\ref{tab:re_tab2}, our key innovation is to propose scene text QA that reflects real-world human needs. We will include these works and elaborate on our contributions in the revision. 

% (\textbf{{\color{red}Merge with R2Q1}})

\noindent\textbf{\textcolor{green}{- Q5:}}~\textbf{{Effects of resolution.
}}~No, it does not. The results in Tabs.~3, 4 correspond to the models' optimal setting. Additionally, some models only accept inputs of fixed frame resolution (\eg, LLaVA-NeXT-Video).
% The two datasets are independent with distinct characteristics, and resolution differences reflect their unique properties and do not affect evaluation validity.


%%%%%%%%% REFERENCES
% {
%     \small
%     \bibliographystyle{ieeenat_fullname}
%     \bibliography{main}
% }

% \noindent[1] Grounded Question-Answering in Long Egocentric Videos. In \emph{CVPR}, 2024.

% \noindent[2] Egocentric video-language pertaining. In \emph{NIPS}, 2022.

% \noindent[3] Egovlpv2: Egocentric video-language pre-training with fusion in the backbone. In \emph{ICCV}, 2023.

% \noindent[4] Multi-Factor Adaptive Vision Selection for Egocentric Video Question Answering. In \emph{ICML}, 2024.

% \noindent[1] MM-Ego: Towards Building Egocentric Multimodal LLMs. \emph{arXiv preprint arXiv:2410.07177}, 2024.



\end{document}
