\section{\dataset~Dataset}

%--------------------------------------
\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figure/qa_examples_new.pdf}
\caption{Examples of \dataset. Scene text plays pivotal role in understanding and answering the questions which reflect real user needs. Yet, the videos are without the visual focus on scene text.} 
\label{fig:exp}
\vspace{-0.5cm}
\end{figure*}


\subsection{Dataset Creation} 
We leverage existing text-rich video sequences and the multimodal understanding and generation capabilities of MLLMs to create our dataset. The generated QA candidates are carefully reviewed and refined by human annotators as necessary to ensure the dataset's high diversity and quality.

\begin{table}
\centering
\caption{Data distribution and related examples of \dataset. The {\color{limegreen}green} words denote scene texts that appear in the video. }
\vspace{-0.2cm}
\label{tab:data_dis}
\setlength{\tabcolsep}{.8em}
\fontsize{6.4}{8}\selectfont
\begin{tabular}{>{\raggedright\arraybackslash}p{0.95cm} | >{\raggedright\arraybackslash}p{0.75cm} | >{\raggedright\arraybackslash}p{4.8cm}}  
\Xhline{1pt}
\textbf{Category} & \makecell[l]{\textbf{\#Q / \#V}} & \makecell[l]{\textbf{QA Examples}}   \\ 
\Xhline{1pt}
\rowcolor{lightorange}\multicolumn{3}{c}{{\datasetout}} \\
\hline
\textbf{Location} & \makecell[l]{1,505/553} & \makecell[l]{\emph{Q: Where should I go if I need a dentist?} \\ A: {\color{limegreen}Vijaya Dental Hospital}.} \\ 
\hline
{\textbf{Description}} & 1,387/601 & \makecell[l]{\emph{Q: What is the license plate of the car in front}  \emph{of me?} \\
A: {\color{limegreen}Ts07ge5554}.} \\ 
\hline
\textbf{Direction} & 952/468 & \makecell[l]{\emph{Q: Which direction do I go to get to {\color{limegreen}Vapor Inn}?} \\
A: Continue straight and turn right at the next road.} \\ 
\hline
\makecell[l]{\textbf{Intention} \\ \textbf{Reasoning}} & 813/443& \makecell[l]{\emph{Q: What should I be cautious of in this area?} \\ A: Watch for pedestrians crossing at the crosswalk.}  \\
\hline
\textbf{Others} & 191/152& \makecell[l]{\emph{Q: What is the main theme of the event advertised on } \\ \emph{the bus and when and where is it happening?} \\ A: The main theme is ``{\color{limegreen}Race Against HIV}", and it is  \\ happening on  {\color{limegreen}Dec 1, 2018}, at {\color{limegreen}People's Plaza}.} \\ 
\hline
\rowcolor{lightblue}\multicolumn{3}{c}{{\datasetin}} \\
\hline
\textbf{Hands-on} & 698/284 &  \makecell[l]{\emph{Q: How should I properly handle the {\color{limegreen}AVT} equipment?} \\ A: Maintain a firm grip and use proper posture. }   \\ 
\cline{1-3}
\textbf{Shopping} & 358/78 &  \makecell[l]{\emph{Q: Where can I find the ``{\color{limegreen}Milano}" cookies?}  \\ A: Top shelf on the right. }   \\ 
\cline{1-3}
\textbf{Kitchen} & 335/149 & \makecell[l]{\emph{Q: What should I use to clean up countertop spills? } \\ A: {\color{limegreen}Ecover} dish soap }   \\ 
\cline{1-3}
\textbf{Book-related} & 323/103 & \makecell[l]{\emph{Q: Where can I find information on learning the guitar? } \\ A: {\color{limegreen}Guitar for beginners}. }  \\ 
\cline{1-3}
\textbf{Gameplay} & 301/124 & \makecell[l]{\emph{Q: What card did my opponent play before I placed the} \\ \emph{red {\color{limegreen}9}?}  A: Red {\color{limegreen}8}. }   \\ 
\cline{1-3}
\textbf{Others} & 201/88 & \makecell[l]{\emph{Q: Where might I find the office rules? } \\ A: On the notices on the wall near the entrance. } \\
\Xhline{1pt}
\end{tabular}
\vspace{-0.5cm}
\end{table}


\noindent\textbf{Raw Video Filtering} 
Our videos are drawn from two public ego-view video datasets, RoadTextVQA~\cite{tom2023reading} and EgoSchema~\cite{mangalam2023egoschema}. They cover outdoor and indoor scenarios, centered around driving and housekeeping activities. 
RoadTextVQA has 3,222 ten-second videos from dashcam footage of people driving. 
EgoSchema features 5,063 three-minute videos sourced from Ego4D~\cite{grauman2022ego4d}; its videos are mostly about indoor activities such as cooking, game playing, manufacturing, \etc.
To find videos rich in scene text, we first apply a state-of-the-art scene-text detection system~\cite{he2024gomatching} to the raw videos and threshold for videos with a significant percentage of frames with scene text: exceeding 5\% for Ego4D and 15\% for RoadTextVQA (since it features more scene text). 
After automatic filtering, we obtain 700 videos from RoadTextVQA~\cite{tom2023reading} and 1,800 from EgoSchema~\cite{grauman2022ego4d}. Since Ego4D videos are not collected for scene text study, we further filter the unqualified videos, mainly removing the tedious videos and videos that contain watermark or text present only on the clothes of the camera wearers. Here, we obtain 933/700 videos from Ego4D/RoadTextVQA.


\noindent\textbf{QA Generation}
Manually collecting QA pairs is labor-intensive; the QAs may also lack diversity. We use advanced MLLMs (\ie, GPT-4o) and generate the initial QAs with the following protocol: First, a video is decoded into 6 frames per second (fps) and evenly divided into 5 segments, wherein the frames without scene text are removed. Then, we uniformly sample 3 frames from each segment, and feed them to GPT-4o along with the prompts (in Appendix~{\color{red}D})~%~\ref{sup:prompt}
to generate 3 QA pairs. This leads to a maximum of 15 QA pairs per video. The prompts are carefully designed to elicit questions that are:  
\textbf{(1)} goal-oriented, capturing real user needs by engaging with the visual scene from a first-person perspective;  
\textbf{(2)} aware of scene text, though not necessarily requiring the exact text transcription;  
\textbf{(3)} naturally expressed in a colloquial, first-person manner with referring expressions; and  
\textbf{(4)} challenging, as they demand an understanding of the video beyond a single image.


\begin{figure*}[t!]
\centering
\includegraphics[width=\textwidth]{figure/dataset_analysis_dis.pdf}
\vspace{-0.5cm}
\caption{Distribution of QAs and OCR numbers.} 
\label{fig:data_dis}
\vspace{-0.2cm}
\end{figure*}



\begin{table*}[t!]
\centering
\caption{Comparison of related egocentric VQA and scene-text VQA test benchmarks. ST: Scene Text. STQ/STA: Questions/Answers contain scene text. EgoV: Egocentric Video. IntQ: Intentional Questions. QC: Question Category. OE/MC: Open-ended/Multi-choice.
}
\vspace{-0.2cm}
\label{tab1:dataset_compare}
\setlength{\tabcolsep}{.6em}
\centering
\fontsize{8}{10}\selectfont
\small
\resizebox{\linewidth}{!}{
\begin{threeparttable}
\begin{tabular}{lccccccccccccc}
\Xhline{1pt}
\textbf{Benchmark}  & \textbf{\#Que.} & \textbf{\#Video} & \textbf{Ave. Len. (s)} & \textbf{ST} & \textbf{STQ (\%)} & \textbf{STA (\%)} & \textbf{EgoV} & {\bf Indoor} & {\bf Outdoor} &  \textbf{Real Time} & \textbf{IntQ}  & \textbf{QC}  & \bf Task \\ 
\Xhline{1pt}
\multicolumn{3}{l}{\emph{Egocentric VQA Benchmarks}} \\
\hline
EgoThink~\cite{cheng2024egothink} & 700  & -  & - & \xmark  & - & - & \cmark &\cmark & \cmark  & \xmark & \xmark & \cmark & OE  \\
QAEgo4D~\cite{Baermann_2022_CVPR}  & 1,854 & 166  & 495.1 & \xmark & - & - & \cmark & \cmark & \cmark &  \xmark  & \xmark  & \xmark  & OE \\ 
AssistQ~\cite{wong2022assistq}  &  531 & 100 & 115.0 & \xmark & -  &  - & \cmark & \cmark & \xmark & \xmark &  \cmark & \xmark   & MC \\ 
EgoSchema~\cite{mangalam2023egoschema} & 5,063  & 5,063  & 180.0 & \xmark  &  - & -  & \cmark & \cmark & \cmark  & \xmark  &  \xmark  &  \xmark & MC \\  
EgoMemoria~\cite{ye2024mm} & 7,026 & 629 & 858.5 & \xmark &  - & -  & \cmark  & \cmark & \cmark & \xmark & \xmark & \cmark  & MC \\ 
\hline
\multicolumn{4}{l}{\emph{Scene-Text VQA Benchmarks}} \\
\hline
TextVQA~\cite{singh2019towards} & 5,734 & -  & - & \cmark & 16 & 85 & \xmark & \cmark & \cmark & \xmark & \xmark & \xmark  & OE \\ 
ST-VQA~\cite{biten2019scene}   & 4,163 & -  & - & \cmark & 18 & 92 & \xmark & \cmark &\cmark & \xmark & \xmark & \xmark  & OE \\ 
ESTVQA~\cite{wang2020general} & 5,014 & -  & - & \cmark  & 25 & 97 & \xmark &\cmark & \cmark & \xmark & \xmark & \xmark  & OE \\
M4ViteVQA~\cite{zhao2022towards}  & 2,103 & 680 & 5.7 & \cmark &  30 & 96 & \xmark & \cmark & \cmark & \xmark & \xmark & \xmark  & OE \\ 
RoadTextVQA~\cite{tom2023reading}  & 1,052 & 329 & 10.0  & \cmark & 60 & 65 & \cmark & \xmark & \cmark & \xmark & \xmark & \xmark  & OE \\ 
\rowcolor{gray!20} \dataset~(Ours)  & 7,064 & 1,507 &  101.7 & \cmark & 52 & 45 & \cmark  & \cmark & \cmark & \cmark & \cmark & \cmark & OE \\ 
\Xhline{1pt}
\end{tabular}
\vspace{-0.5cm}
\end{threeparttable}}
\end{table*}


\noindent\textbf{Manual Participation} 
\label{dataset:mp}
Despite using well-designed prompts, the generated QA pairs may fail to meet our strict requirements. Some generated questions are hallucinated or irrelevant to the visual content, have wrong formats, or are gibberish, \eg, involving special typeset characters not part of spoken language. We first employ GPT-4o to answer the questions without videos and remove the questions that can be answered blindly ($\sim$10\%). 
We then conduct manual filtering and correction for the remaining QAs. We provide some representative generated QA errors in Appendix~{\color{red}A}.%~\ref{sup:data}.

Specifically, we invite 9 well-trained university students for annotation. The manual check has 5 stages: (1) five annotators review for problematic QAs which may be redundant, not reflect real user needs, and irrelevant to scene text. The questions are amended where possible, or removed, leaving half of the original QAs; (2) and (3) two additional annotators re-examine and correct any mechanical issues, further reducing the QAs to 30\%; (4) five annotators refine the data to improve its colloquial quality; (5) four annotators strictly refine the remaining QAs according to the criteria specified in QA generation. The participants at each stage are asked to first check the remaining QAs from the prior stage. Additionally, we manually enrich $\sim$10\% time-sensitive questions to simulate real-time live QA by copying each suitable question to a different timestamp and invite five additional annotators to provide concise answers based on the visual content at the new timestamp.
Ultimately, we obtain 7,064 QA pairs of 1,507 videos to form the \dataset~dataset. Detailed question type and video scenario labels are provided. Figure~\ref{fig:exp} shows some representative examples; more are presented in Appendix~{\color{red}B.5}.%~\ref{sup:qa_case}. 


\subsection{Dataset Analysis}
\noindent\textbf{Dataset Statistics}
\dataset ~characterizes assistance-seeking questions related to scene text in diverse real-life scenarios. It consists of two parts: \textbf{(1) \datasetout}~focuses on the outdoor scenarios, with 694 videos and 4,848 QA pairs that may arise when driving; \textbf{(2) \datasetin~} emphasizes indoor scenarios, with 813 videos and 2,216 QA pairs that users may encounter in house-keeping activities. Distributions of videos and questions are listed in Table~\ref{tab:data_dis}. We also analyze the most frequent question and answer patterns in Figure~\cref{fig:data_dis} \textcircled{1}, \textcircled{2}. The statistics reveal a clear discrepancy between QAs; the driving questions are mostly related to navigation which pure scene text cannot answer, while the house-keeping questions are information seeking, where answers can be extracted from scene text. Figure~\ref{fig:data_dis} \textcircled{3} shows that the 
\textbf{\datasetin}~videos have a long-tailed distribution in the {number of OCR tokens}, with the majority having less than 200.  In contrast, \textbf{\datasetout}~videos are more evenly distributed with around 500. 


\noindent\textbf{Dataset Comparison}
We compare \dataset~with several related benchmarks in Table~\ref{tab1:dataset_compare}. \dataset~has several unique features. Specifically, \dataset~is the first VideoQA testbed designed for \emph{egocentric scene-text aware QA assistance} in real-world scenarios, containing 7K QA pairs across 1.5K egocentric visual scenarios, covering both indoor and outdoor activities. The QA pairs focus on scene text comprehension, with about 45\% of answers and 52\% of questions referencing the exact scene text. Notably, \dataset~introduces \emph{real-time QA}, providing detailed timestamps for each question. While EgoMemoria~\cite{ye2024mm} also includes timestamps, its answers are based on the entire video, whereas \dataset~answers are derived from video content before the question timestamp. Additionally, the questions in \dataset~focus on inferring user intentions rather than pure visual understanding, with question categories provided for model analysis.














