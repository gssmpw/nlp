
\section{Introduction}
\label{sec:intro}
The advances in wearable cameras and egocentric
vision research~\cite{fan2019egovqa, grauman2022ego4d, lin2022egocentric, pramanick2023egovlpv2} have led to a surge of interest in developing assistance applications. A promising direction is egocentric video question answering (VideoQA) \cite{xiao2024videoqa, li2023transformer, yang2024robust, guo2021context, di2024grounded, zhangmulti}, where AI agents offer live help by answering user questions. Despite the potential, related dataset research~\cite{ye2024mm, fan2019egovqa, jia2022egotaskqa, mangalam2023egoschema, cheng2024videgothink} has been designed to assess VideoQA models in their visual understanding capabilities. The questions are not curated to reflect real user needs in assistive applications (see EgoTaskQA in~Figure~\ref{fig:intro}). 
QAEgo4D \cite{Baermann_2022_CVPR} and AssistQ \cite{wong2022assistq} are closer in aim, but these datasets are designed to assist with episodic memory and ``how to'' demonstration; they emphasize pure visual understanding while neglecting scene text.  

Research in assistive techniques \cite{gurari2018vizwiz, zhou2023exploring, singh2019towards, li2022invariant} have shown that scene text, including signs, labels, and other text elements that are ubiquitous in our daily lives, is frequently involved in user questions when seeking assistance. Existing efforts~\cite{singh2019towards, zhou2024scene, biten2019scene} for QA on scene-text primarily aim at Optical Character Recognition (OCR) from ideal imaging conditions.  
They assume that people take good pictures and ask questions directly pinpointing the scene-text regions (see TextVQA~\cite{singh2019towards} in~Figure~\ref{fig:intro}). Despite the progress (\eg, leading results have accuracies of 85\% on ST-VQA \cite{biten2019scene,wang2024qwen2}), such a simplified setting has limited application value. Consider, for example, those with a visual impairment~\cite{gurari2018vizwiz}, they will likely struggle to take clean and in-focus images; or can they point to scene-text regions. More recent works \cite{zhao2022towards,tom2023reading} begin to study text-based VideoQA which allows QA on multiple images to reduce the proportion of unanswerable cases. Nonetheless, they still assume people know well the locations of scene text (see RoadTextVQA~\cite{tom2023reading} in~Figure~\ref{fig:intro}). Corresponding VQA models, while achieving good results on such benchmarks, cannot reason over user intentions other than an extraction of the OCR results \cite{jahagirdar2023understanding}.

In light of this, we introduce \dataset, a novel and rigorously constructed \underline{Ego}centric Scene-\underline{Text} Aware \underline{V}ideo \underline{Q}uestion \underline{A}nswering benchmark. \dataset ~is designed to advance research on egocentric QA assistance in real-life scenarios. We collect 1.5K ego-view videos and 7K scene-text aware questions from outdoor driving (\datasetout) and indoor house-keeping (\datasetin) activities. By emphasizing real user needs, we allow questions and answers that do not explicitly pinpoint the scene text. However, comprehending scene text is crucial to answering the questions (see examples in Figure \ref{fig:intro}). Moreover, to simulate real-time and streaming video QA, we set a timestamp for each question and allow the models to access \emph{only} visual contents before the question's timestamp to make related responses. Detailed visual scenarios and question types are also provided for better analysis of models.

For its realistic setting, \dataset~retains all the challenges in existing scene text VQA datasets and incorporates new major difficulties: First and foremost, it is crucial to \emph{reason about the user's intentions} with respect to the visual scene and scene text to both understand and to answer the questions. 
Second, the model may need to \emph{reason across multiple frames} to either infer the user's behavior or to locate the required scene text for answering. 
Third, the model may need to \emph{infer the user's current state} when posing the questions and provide actionable answers for meaningful assistance. Finally, the egocentric dynamic situation poses an additional challenge to \emph{scene text recognition} as opposed to the well-captured images and focused scene text.

With \dataset, we comprehensively benchmark 10 prominent models that perform well on existing scene-text VQA datasets, and find they all struggle, especially the open-source ones like MiniCPM-V 2.6 \cite{yao2024minicpm}, ShareGPT4Video \cite{chen2024sharegpt4video} and CogVLM2-Video \cite{hong2024cogvlm2}. The best-performing model is Gemini 1.5 Pro~\cite{reid2024gemini}, yet its accuracy is still lower at around 33\%. Comprehensive analyses and additional heuristic explorations reveal many insights on model behaviors and possible directions for improvements: 
\begin{enumerate}
    \item Temporally localizing the key frames and jointly reasoning over multiple key frames (\vs~a single key frame) are crucial for improvements, especially for long video QA scenarios on \datasetin.
    \item High-resolution image and scene text input can significantly boost models' performances. Yet, one needs to mind the compute efficiency. Also, models that inherently take fixed higher-resolution inputs are not necessarily better than those using lower-resolution inputs.
    \item Scene text from additional OCR techniques, despite being auxiliary, are extremely helpful for all models. 
\end{enumerate}

Additionally, we find that even humans cannot perform well on this task (43\% on \datasetout~and 27\% on \datasetin), largely
due to the difficulty of scene text recognition and the limited knowledge of humans, highlighting the significance of this research.
To summarize our contributions: 1) We propose to study scene-text aware VideoQA towards egocentric assistance and construct the \dataset ~dataset containing both \datasetin ~and \datasetout. 2) We benchmark 10 contemporary powerful models covering both open-source and closed-source ones and comprehensively analyze their limitations. 3) We explore a series of heuristic methods for improvements and share insightful findings for future work.


