\section{Experiments}
\label{sec:expr}
\subsection{Experimental Setups}
\begin{table*}[t]
\caption{Performance of SoTA MLLMs on \datasetout. \textbf{Res.} denotes image resolution. We uniformly sample \textbf{\#F} frames that the models can accept from only video content before the timestamp. In this table, we test the performance of the model on low-resolution videos (640$\times$360 and 960$\times$540) of the \datasetout~ dataset. The human study is conducted on a random 30\% of all questions, with the only constraint that answers rely on video content before the question timestamp. The \textbf{best} and \underline{second-best} results are highlighted.}
\label{tab:expr_tab1}
\vspace{-0.2cm}
\setlength{\tabcolsep}{.6em}
\centering
\fontsize{8}{9}\selectfont
\resizebox{\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|c|c|c|ccccc|c}
\Xhline{1pt}  
{\textbf{Method}} &{\textbf{LLM}} & {\textbf{Res.}} & {\textbf{\#F}} & {\textbf{Location}} & {\textbf{Direction}}  & {\textbf{Description}} & {\textbf{Int. Reasoning}} & {\textbf{Others}} & {\textbf{Average}} \\ 
\Xhline{1pt}
\rowcolor{gray!20}
Human & - & - &  - & 38.7 / 2.3 & 32.4 / 2.2 & 54.9 / 3.0 & 45.3 / 2.6 & 36.9 / 2.5 & 43.1 / 2.6 \\ 
\hline
\multicolumn{10}{l}{\emph{\textcolor{lightgray}{Open-source Models}}} \\
ShareGPT4Video~\cite{chen2024sharegpt4video} & LLaMA3-8B & - & 60  & 11.7 / 1.5  & 20.8 / 1.8 & 5.5 / 0.7 & 10.7 / 1.3 & 10.0 / 1.2 & 11.5 / 1.3  \\
CogVLM2-Video~\cite{hong2024cogvlm2} & LLaMA3-8B & - & 60 & 10.6 / 1.5 & 15.7 / 1.7  & 12.4 / 1.0 & 12.4 / 1.3 & 10.0 / 1.3 & 12.4 / 1.4 \\ 
MiniCPM-V 2.6~\cite{yao2024minicpm} & Qwen2-7B & $448^{2}$ & 60 & 7.7 / 0.5 & 14.4 / 0.9 & 15.1 / 0.9 & 8.7 / 0.6 & 10.0 / 0.8 &  11.4 / 0.7 \\
VILA1.5~\cite{lin2024vila} & LLaMA3-8B & $384^{2}$ & 32 & 14.6 / 1.5  & 20.2 / 1.6 & 18.2 / {1.3} & 18.3 / {1.5} & 14.7 / {1.5} & 17.4 / {1.5}  \\ 
LLaVA-NeXT-Video~\cite{zhang2024llavanextvideo} & Qwen2-7B &  $384^{2}$ & 60 & {18.1} / 1.3 & {22.5} / 1.5 & {19.6} / 1.3 & {19.3} / 1.4 & {15.7} / 1.4  & {19.5} / 1.4 \\ 
% Qwen2-VL~\cite{wang2024qwen2} & Qwen2-7B & $448^{2}$ & 32 & {19.8} / {1.5} &  {31.7} / {2.2} & {23.1} / {1.5} & {22.3} / {1.7}  & {22.0}  / {1.8} & {23.6} / {1.7}  \\
InternVL2-8B~\cite{chen2024far} & InternLM2.5-7B & $448^{2}$  & 32  & 15.8 / 1.4  &  21.9 / 1.7 & 14.8 / 1.0 & 14.5 / 1.2  & 13.6 / 1.3  & 16.4 / 1.3 \\
InternVL2-26B~\cite{chen2024far} & InternLM2-20B & $448^{2}$  & 16  & 21.5 / 1.7 & 27.5 / \underline{2.0}  & 23.5 / 1.5  & 22.9 / 1.7  & 22.5 / 1.7 & 23.5 / 1.7   \\ 
% Qwen2-VL~\cite{wang2024qwen2} & Qwen2-7B & $448^{2}$ & 16  &   18.4 / 1.5 & 30.6 / \underline{2.2} & 22.7 / 1.5 & 21.2 / 1.7 & 15.7 / 1.6 &  22.4 / 1.6  \\ 
Qwen2-VL~\cite{wang2024qwen2} & Qwen2-7B & -  & 16  & 23.4 / \underline{1.8} & \textbf{35.2} / \textbf{2.4}  & 30.8 / \underline{1.9} & 25.6 / \underline{1.9} & 22.5 / 1.8  & 28.2 / \textbf{2.0} \\ 
% \hline
% \multicolumn{10}{l}{{\emph{\textcolor{lightgray}{Open-source Models $\textgreater$ 8B}}}} \\
\hline 
\multicolumn{10}{l}{{\emph{\textcolor{lightgray}{Closed-source Models}}}} \\
GPT-4o~\cite{achiam2023gpt}  & - & $768^{2}$ & 32  & \underline{28.3} / {1.6}  & 24.9 / 1.4  & 35.0 / \underline{1.9}  & \underline{31.6} / 1.8 & \underline{29.8} / 1.8 & \underline{30.3} / 1.7   \\ 
Gemini 1.5 Flash~\cite{reid2024gemini} & - & $768^{2}$ & 32 & 25.4 / 1.6 & \underline{28.3} / {1.8} &  \underline{38.1} / \textbf{2.2} &  28.1 / {1.8} & 26.6 / \underline{2.0} &  30.1 / \underline{1.9} \\ 
Gemini 1.5 Pro~\cite{reid2024gemini}  & - & $768^{2}$ & 32  & \textbf{33.2} / \textbf{2.1} &  \underline{28.3} / {1.8} &  \textbf{38.8} / \textbf{2.2} & \textbf{32.7} / \textbf{2.0}  & \textbf{34.6} / \textbf{2.2}  &  \textbf{33.4} / \textbf{2.0}  \\ 
\Xhline{1pt}
\end{tabular}
\vspace{-0.5cm}
\end{threeparttable}}
\end{table*}





\noindent\textbf{Evaluation Metric}
We use GPT-4o mini \cite{gpt4omini2024} to evaluate the semantic similarity between the generated and the ground-truth (GT) answers, thus aligning closely with human scoring. Following \cite{maaz2023video}, we assess the generated answers with two metrics: Accuracy (0-100\%, the percentage of ``yes'' answers from the evaluator) and Score (0-5, with 5 being the highest match). Concretely, we format the question, predicted answer, and GT answer along with our customized evaluation prompts (in Appendix~{\color{red}D})~%~\ref{sup:prompt} 
for GPT-4o mini to determine the ``yes/no'' and score.




\noindent\textbf{Model Evaluation}
We evaluate three closed-source API-based models (GPT-4o~\cite{achiam2023gpt}, Gemini 1.5 Flash~\cite{reid2024gemini}, Gemini 1.5 Pro~\cite{reid2024gemini}) and seven advanced open-source MLLMs (MiniCPM-V 2.6~\cite{yao2024minicpm}, ShareGPT4Video~\cite{chen2024sharegpt4video}, InternVL2~\cite{chen2024far}, VILA1.5~\cite{lin2024vila}, LLaVA-NeXT-Video~\cite{zhang2024llavanextvideo}, CogVLM2-Video~\cite{hong2024cogvlm2}, Qwen2-VL~\cite{wang2024qwen2}) for evaluation. The models have reported the state-of-the-art (SOTA) on existing scene-text VQA benchmarks \cite{biten2019scene,singh2019towards,gurari2018vizwiz}, \eg, Qwen2-VL achieves QA accuracy $\sim$85\% on ST-VQA \cite{biten2019scene}. More details about these MLLMs and their specific prompts are presented in Appendix~{\color{red}B.1} and ~{\color{red}D}.
%~\ref{sup:models} and~\ref{sup:prompt}.

\subsection{Result Analysis}
\label{expr:ra}
Tables~\ref{tab:expr_tab1} and \ref{tab:expr_tab2} show that all models, especially the open-source ones, struggle to perform well on \dataset. The best results, achieved by Gemini~Pro~1.5~\cite{reid2024gemini}, are around 33$\sim$34\% on both \datasetout~and \datasetin, exceeding the best-performing open-source models by approximately $\sim$5\% and $\sim$9\% on \datasetout~and \datasetin, respectively. The performances vary significantly among open-source models, with Qwen2-VL~\cite{wang2024qwen2} and LLaVA-NeXT-Video~\cite{zhang2024llavanextvideo} performing the best, while MiniCPM-V 2.6~\cite{yao2024minicpm} and ShareGPT4Video~\cite{chen2024sharegpt4video} perform the worst.



\begin{table*}[t]
\caption{Performance of SoTA MLLMs on \datasetin. \textbf{Res.} denotes image resolution. We uniformly sample the \textbf{\#F} frames that the model can accept from all video frames before the timestamp as input. In this table, we test the performance of the model on the videos (the resolution mainly includes 480$\times$360 and 640$\times$360) of the \datasetin~ dataset. The human study is conducted on a randomly sampled 30\% of all questions. The best and second-best Accuracy / Score results are \textbf{bolded} and \underline{underlined} respectively.  }
\label{tab:expr_tab2}
\vspace{-0.2cm}
\setlength{\tabcolsep}{.3em}
\centering
\fontsize{7}{8}\selectfont
\resizebox{\textwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|c|c|c|cccccc|c}
\Xhline{1pt}  
{\textbf{Method}} &{\textbf{LLM}} & {\textbf{Res.}} & {\textbf{\#F}} & {\textbf{Hands-on}} & {\textbf{Kitchen}}  & {\textbf{Shopping}} & {\textbf{Gameplay}} & {\textbf{Book-Related}} & {\textbf{Others}} & {\textbf{Average}} \\ 
\Xhline{1pt}
\rowcolor{gray!20}
Human & - & - &  - & 26.6 / 1.9 & 31.3 / 2.1 & 18.2 / 1.4 & 34.8 / 2.2 & 27.2 / 1.8 & 32.8 / 2.1 & 27.7 / 1.9 \\ 
\hline
\multicolumn{11}{l}{\emph{\textcolor{lightgray}{Open-source Models}}} \\
ShareGPT4Video~\cite{chen2024sharegpt4video} & LLaMA3-8B & - & 128 & 7.0 / 1.0 & 5.1 / 1.0 & 5.9 / 1.3 & 12.3 / 1.2  & 7.1 / 1.1 & 4.0 / 0.9 & 7.0 / 1.1  \\
% ShareGPT4Video~\cite{chen2024sharegpt4video} & LLaMA3-8B & - & 1080 & 6.02 / 0.96 & 4.78 / 1.01 & 6.15 / 1.24  & 10.96 / 1.21  & 5.57 / 1.02  & 5.47 / 0.98 & 6.41 / 1.06   \\
CogVLM2-Video~\cite{hong2024cogvlm2} & LLaMA3-8B & - & 128 & 12.3 / 1.4 & 11.6 / 1.4 & 10.9 / 1.4  & 23.6 / 1.8  & 11.2 / 1.2 & 13.4 / 1.4 & 13.5 / 1.4   \\ 
MiniCPM-V 2.6~\cite{yao2024minicpm} & Qwen2-7B & $448^{2}$ & 120 & 14.2 / 0.8 & 8.1 / 0.5  & 10.3 / 0.6 & 17.6 / 1.1  & 16.4 / 1.0 & 12.9 / 0.8 & 13.3 / 0.8   \\
VILA1.5~\cite{lin2024vila} & LLaMA3-8B & $384^{2}$ & 32 & 16.5 / 1.5 & 18.8 / 1.5 & 15.1 / 1.5  &  24.9 / 1.9 & 18.9 / 1.6 & 17.4 / 1.5 &  18.2 / 1.6  \\ 
% VILA1.5~\cite{lin2024vila} & LLaMA3-8B & $384^{2}$ & 128 & 1.15 / 0.15 & 0.30 / 1.12 & 0.56 / 0.17  & 1.67 / 0.24 & 0.92 / 0.16  & 0 / 0.05  & 0.86 / 0.15    \\ 
% Qwen2-VL~\cite{wang2024qwen2} & Qwen2-7B & $448^{2}$ & 32 & {21.06} / {1.73} & 20.60 / 1.65  &  20.39 / 1.67 & 27.91 / {2.04} & 21.67 / 1.78 & 20.90 / 1.63  & 21.89 / 1.75   \\ 
InternVL2-8B~\cite{chen2024far} & InternLM2.5-7B & $448^{2}$  & 32  & 13.5 / 1.2 & 12.5 / 1.1 & 14.5 / 1.3 & 13.3 / 1.2  & 14.9 / 1.3  & 12.4 / 1.1 & 13.6 / 1.2 \\
InternVL2-26B~\cite{chen2024far} & InternLM2-20B & $448^{2}$  & 16  &  20.3 / 1.6 & 22.4 / 1.8  & 18.2 / 1.6  & 25.6 / 1.8 & 19.2 / 1.6 & 21.9 / 1.7 & 21.0 / 1.7    \\ 
Qwen2-VL~\cite{wang2024qwen2} & Qwen2-7B & $448^{2}$ & 48 & {21.5} / {1.8}  & {21.5} / {1.7}  & {22.5} / {1.8} & {29.9} / {2.1}  & {22.0} / {1.8}  & {26.4} / {1.8} & 23.3 / {1.8}    \\
% Qwen2-VL~\cite{wang2024qwen2} & Qwen2-7B & -  & 32  & 21.92 / 1.77 & 19.10 / 1.63 & 21.51 / 1.76  & 30.23 / 2.16 & 21.05 / 1.69  & 24.38 / 1.80 & 22.65 / 1.79       \\ 
% LLaVA-NeXT-Video~\cite{zhang2024llavanextvideo} & Qwen2-7B &  $384^{2}$ & 32 & {23.07} / 1.65 & 20.30 / 1.58  & 20.95 / 1.59  & 29.24 / 1.88 & 20.43 / 1.53 & 23.38 / 1.61 & 22.79 / 1.64  \\
LLaVA-NeXT-Video~\cite{zhang2024llavanextvideo} & Qwen2-7B &  $384^{2}$ & 128 & {24.4} / {1.7} & {23.6} / {1.7} & {21.8} / {1.7} & {29.9} / {1.9}  & {27.9} / {1.9}  & {27.8} / {1.8} & {25.4} / {1.8}  \\ 
% \hline
% \multicolumn{11}{l}{\emph{\textcolor{lightgray}{Open-source Models $\textgreater$ 8B }}} \\
% VILA1.5~\cite{lin2024vila} & LLaMA3-13B & $384^{2}$ & 32 &    &    &   &   &    &       \\
%VILA1.5~\cite{lin2024vila} & LLaMA3-13B & $384^{2}$ & 128 &    &    &   &   &    &       \\
\hline
\multicolumn{11}{l}{\emph{\textcolor{lightgray}{Closed-source Models}}} \\ 
GPT-4o~\cite{achiam2023gpt} & - & $768^{2}$ & 60 & 25.4 / 1.6 & 30.2 / 1.9  & 29.6 / 1.9  & 32.9 / 1.9  & 29.7 / 1.9 & 23.9 / 1.5 & 28.3 / 1.8   \\ 
Gemini 1.5 Flash~\cite{reid2024gemini} & - & $768^{2}$ & 60 & \underline{29.7} / \underline{2.0} & \underline{33.7} / \underline{2.1} & \underline{32.3} / \underline{2.1}  & \underline{34.5} / \underline{2.3}  & \underline{33.6} / \textbf{2.2}  & \underline{30.1} / \underline{2.0}  & \underline{32.0} / \underline{2.1}  \\ 
Gemini 1.5 Pro~\cite{reid2024gemini} & - & $768^{2}$ & 60  & \textbf{33.2} / \textbf{2.1} & \textbf{35.2} / \textbf{2.1}  & \textbf{33.8} / \textbf{2.1}  & \textbf{39.5} / \textbf{2.4}  & \textbf{34.3} / \underline{2.1} & \textbf{30.9} / \textbf{2.0} & \textbf{34.4} / \textbf{2.1}   \\ 
\Xhline{1pt}
\end{tabular}
\end{threeparttable}}
\vspace{-0.4cm}
\end{table*}

% \ZS{For some real-time questions, the model can respond with ``I didn't find it" if the required information %to fulfill the user's need 
% has not yet appeared in the scene. This approach informs the user that their needs cannot be addressed currently. It is observed that with the same response prompts, MiniCPM-V 2.6 performs better than other models in this aspect. For example, Qwen2-VL~\cite{wang2024qwen2} often hallucinates inaccurate answers, even though the scene text necessary to meet the user's needs has not yet appeared.}

\begin{figure}[t!]
\centering
\includegraphics[width=1.0\columnwidth]{figure/visualization.pdf}
\vspace{-0.2in}
\caption{Result Visualization.} 
\vspace{-0.6cm}
\label{fig:vis}
\end{figure}


\noindent\textbf{\datasetout} 
Table~\ref{tab:expr_tab1} shows that MLLMs struggle across different question categories. The open-source models perform particularly poorly on “Location” and “Intention Reasoning” questions, while closed-source models, GPT-4o and Gemini 1.5 Pro/Flash, often fail on “Direction” questions. “Location” questions require understanding spatial relationships, while “Intention Reasoning” involves inferring user intent from egocentric visual context. These challenges highlight the limitations of open-source models in fine-grained visual reasoning. For “Direction” questions, closed-source models frequently respond with phrases like “unknown scene text” suggesting difficulties in recognizing or locating specific scene text (see Figure~\ref{fig:vis} Top). Overall, Gemini 1.5 Pro~\cite{reid2024gemini} performs best, achieving 33.4\% accuracy and a score of 2.0, making it the SOTA model on \datasetout. However, it still trails human performance by $\sim$10\%, indicating that even advanced MLLMs struggle with scene text perception in complex, dynamic, egocentric environments.

%Table~\ref{tab:expr_tab1} shows that MLLMs consistently perform poorly across different question categories. The open-source models struggle with answering “Location” and “Intention Reasoning” questions, while the closed-source models, GPT-4o and Gemini 1.5 Pro/Flash, tend to fail on “Direction” questions. “Location” questions require a nuanced understanding of the spatial positions of targets within visual scenes, while “Intention Reasoning” questions demand strong inference capabilities to determine user intentions based on the egocentric visual context. The poor performance on these questions highlights the severe limitations of existing open-source models in fine-grained visual understanding and reasoning in egocentric scenarios. Regarding the weak performance of closed-source models on “Direction” questions, a careful analysis of incorrect predictions reveals that these models often respond with phrases like “unknown scene text” due to their inability to locate or recognize specific scene text (see Figure~\ref{fig:vis}, top). Overall, Gemini 1.5 Pro~\cite{reid2024gemini} outperforms other models across most categories, achieving an overall accuracy of 33.4\% and a score of 2.0, establishing it as the SOTA model on \datasetout. However, its performance still lags 10\% behind human accuracy. These results indicate that even for powerful MLLMs, perceiving scene text in complex, dynamic, egocentric environments remains a challenging and open research problem.

In Figure~\ref{fig:expr_2_hist}, we further analyze model behavior on the real-time QA subset of \datasetout. The results show that all models perform significantly worse on this subset, with the highest accuracy reaching only 20.2\% (\vs. 33.4\% on the full set), underscoring the substantial challenge of egocentric live QA. Interestingly, MiniCPM-V~2.6~\cite{yao2024minicpm} ranks among the top two open-source models for real-time QA, despite being the worst on the full set. However, a closer examination of its predictions reveals that MiniCPM-V~2.6 primarily excels at answering unanswerable questions with responses like ``I did not find it” whereas other models tend to hallucinate answers. This finding suggests that current models remain fundamentally weak in generating meaningful answers that satisfy user needs.


\begin{figure}[t!]
\centering
\includegraphics[width=0.8\columnwidth]{figure/expr_2_hist.pdf}
\vspace{-0.2cm}
\caption{Performance of MLLMs on the real-time QA subset of \datasetout~($\sim$623 QA pairs).} 
\vspace{-0.4cm}
\label{fig:expr_2_hist}
\end{figure}


\noindent\textbf{\datasetin} 
Table~\ref{tab:expr_tab2} shows that MLLMs struggle with the questions across indoor scenarios, especially in “Kitchen” and “Shopping” activities. Our qualitative analysis (see Figure~\ref{fig:vis} Bottom) shows that people often ask about the location of ingredients and utensils in the kitchen, where the models tend to provide vague positional information (\eg, ``\texttt{on the left}" rather than ``\texttt{on the top shelf on the left}"). In shopping, it is challenging for the models to locate a user-specified item among large and densely packed products. ShareGPT4Video~\cite{chen2024sharegpt4video} performs the worst despite training on Ego4D videos, suggesting limited scene-text awareness. Meanwhile, closed-source models perform relatively better across scenarios.

%Intriguingly, humans perform worse than closed-source models. Feedback from annotators and answer analysis suggest three key reasons: (1) scene text recognition in Ego4D videos is difficult due to low resolution, motion blur, occlusion, and video length; (2) many questions require external knowledge beyond text recognition, such as store chains (\eg, \texttt{Home Depot} sells building materials”) in \datasetout~or specialized items (\eg, a \texttt{mandoline}" slices potatoes) in \datasetin; (3) answer diversity lowers human performance. The first two points highlight the challenges of egocentric scene-text QA, while the third reveals a dataset limitation we aim to address by enriching ground-truth answers. More details on human study analysis are in Appendix~\ref{sup:human} and~\ref{sup:agreement}.

Intriguingly, we find that humans perform even worse than the closed-source models. Feedback from our human annotators and a careful analysis of the answers suggest that the primary reasons are as follows: (1) scene text recognition is challenging for humans in Ego4D videos due to lower resolution, motion blur, occlusion, and long video; (2) answering the questions needs external knowledge beyond simple scene text recognition. \datasetout~mainly involves knowledge about store or restaurant chains (\eg, ``\texttt{Home Depo}" {sells building materials), and \datasetin~focuses on items like kitchen utensils, board games, and specialized books (\eg, identifying {a} ``\texttt{mandoline}" {slices potatoes} or {a} ``\texttt{quantum}" {{field theory book})};
(3) answer diversity also results in relatively lower human performance. 
The first two points underscore the challenges and significance of research on egocentric scene-text QA assistance. The third point unveils a limitation of our dataset, which we wish to solve in the future by enriching the GT answers for each question. We provide more human study analysis in Appendix~{\color{red}B.4} and ~{\color{red}C}.
%~\ref{sup:human} and~\ref{sup:agreement}.


\begin{table}[t!]
\caption{Effects of taking as input an image (``TS Frame") and three images (``QA Frames" analogous to QA generation) at the question timestamp.}
\label{tab:expr_tab3}
\setlength{\tabcolsep}{.1em}
\centering
\vspace{-0.2cm}
\fontsize{10}{12}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|c|ll|ll}
\Xhline{1pt}  
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Input}} & \multicolumn{2}{c|}{\textbf{\datasetout}} & \multicolumn{2}{c}{\textbf{\datasetin}}  \\
\cline{3-6}
& & Accuracy & Score & Accuracy & Score  \\
\Xhline{1pt}
\multirow{3}{*}{InternVL2-8B~\cite{chen2024far}} & Video & 16.4 & 1.3 & 13.6 & 1.2 \\ 
& TS Frame &  15.8 {\color{red} $\downarrow$ 0.6} &  1.2 {\color{red} $\downarrow$ 0.1} &  6.5 {\color{red} $\downarrow$ 7.1} &  0.7 {\color{red} $\downarrow$ 0.5} \\ 
& QA Frames &  18.5 {\color{limegreen} $\uparrow$ 2.1} &  1.4 {\color{limegreen} $\uparrow$ 0.1} &  17.7 {\color{limegreen} $\uparrow$ 4.1} &  1.4 {\color{limegreen} $\uparrow$ 0.2} \\ 
\hline
\multirow{3}{*}{MiniCPM-V 2.6~\cite{yao2024minicpm}} & Video & 11.4 & 0.7 & 13.3 & 0.8 \\ 
& TS Frame &  7.9 {\color{red} $\downarrow$ 3.5} &  0.2 {\color{red} $\downarrow$ 0.5} &  3.8 {\color{red} $\downarrow$ 9.5} &  0.3 {\color{red} $\downarrow$ 0.4} \\ 
& QA Frames &  {9.3} {\color{red} $\downarrow$ 2.1}  &  {0.6} {\color{red} $\downarrow$ 0.1} &  13.7 {\color{limegreen} $\uparrow$ 0.4} &  0.9 {\color{limegreen} $\uparrow$ 0.1} \\ 
\hline
\multirow{3}{*}{Qwen2-VL~\cite{wang2024qwen2}} & Video & {28.2} & {2.0} & {23.3} & {1.8} \\
& TS Frame & {30.9} {\color{limegreen} $\uparrow$ 2.7} & {2.1} {\color{limegreen} $\uparrow$ 0.1} &  15.7 {\color{red} $\downarrow$ 7.6} &  1.4 {\color{red} $\downarrow$ 0.4} \\
& QA Frames &  31.9 {\color{limegreen} $\uparrow$ 3.7} &  \textbf{2.2}  {\color{limegreen} $\uparrow$ 0.2} &  30.7 {\color{limegreen} $\uparrow$ 7.4} &  2.2 {\color{limegreen} $\uparrow$ 0.4} \\
\hline
\multirow{3}{*}{GPT-4o~\cite{achiam2023gpt}} & Video &  30.3 &  1.7 & 28.3  & 1.8   \\ 
& TS Frame &  27.7 {\color{red} $\downarrow$ 2.6} &   1.6 {\color{red} $\downarrow$ 0.1} &  14.8 {\color{red} $\downarrow$ 13.5} &  0.9 {\color{red} $\downarrow$ 0.9} \\ 
& QA Frames &  30.4 {\color{limegreen} $\uparrow$ 0.1}  &  1.8 {\color{limegreen} $\uparrow$ 0.1} &    \textbf{41.6} {\color{limegreen} $\uparrow$ 13.3} &   \textbf{2.4} {\color{limegreen} $\uparrow$ 0.6} \\ 
\hline
\multirow{3}{*}{Gemini 1.5 Pro~\cite{reid2024gemini}} & Video &  {\bf33.4} & {2.0}  &  34.4 & 2.1 \\ 
& TS Frame &  30.4 {\color{red} $\downarrow$ 3.0} & 1.8 {\color{red} $\downarrow$ 0.2} & 15.8 {\color{red} $\downarrow$ 18.6} & 1.1 {\color{red} $\downarrow$ 1.0} \\ 
& QA Frames & 26.3 {\color{red} $\downarrow$ 7.1} & 1.6 {\color{red} $\downarrow$ 0.4} & {38.2} {\color{limegreen} $\uparrow$ 3.8} & {2.4} {\color{limegreen} $\uparrow$ 0.3} \\ 
\Xhline{1pt}
\end{tabular}
\vspace{-0.6cm}
\end{threeparttable}}
\end{table}


\noindent\textbf{Outdoor Driving \vs Indoor House-Keeping} 
While humans perform worse on \datasetin, the MLLMs are basically at the same performance level on both datasets. In particular, we find Qwen2-VL~\cite{wang2024qwen2} stands out as the best open-source model on \datasetout, whereas LLaVA-NeXT-Video~\cite{zhang2024llavanextvideo} ranks the top on \datasetin. We speculate that LLaVA-NeXT-Video is better at coping with long videos. More model design and case analysis are provided in Appendix~{\color{red}B.2} and~{\color{red}B.5}.
%~\ref{sup:model_design} and \ref{sup:qa_case}.

% Major observations on different model behavior in \datasetin~and \datasetout

% \noindent\textbf{Qualitative Analysis.} 
% Why do the models fail? Should qualitatively analyze some common failure pattern. (better link to the investigations in the next section.)



\vspace{-0.2cm}
\section{Heuristic Solution Investigations}
To facilitate future study, we conduct extensive experiments to thoroughly analyze the challenges presented by \dataset~in Tables~\ref{tab:expr_tab3} to \ref{tab:expr_modal} and Figures \ref{fig:expr_2_hist} to \ref{fig:expr_1_leida}.  Our analysis centers on the following four questions: %1) Can image-level understanding solve our task? 2) How important is temporal grounding? 3) Is high-resolution scene text critical? 4) How does different modality affect performance?



\noindent\textbf{Q1: Can image-level understanding solve our task?  Sometimes.}
We test the models by inputting a single frame at the question timestamp (``TS Frame"). Compared with using uniformly sampled video frames, Table~\ref{tab:expr_tab3} shows that almost all the models have a significant performance drop ranging from 0.6$\sim$3.5\% on \datasetout ~and as much as 7.1$\sim$18.6\% on \datasetin. This signifies the importance of video-level reasoning, especially for \datasetin, where questions feature longer video action understanding. Notably, Qwen2-VL \emph{improves} its performance when fed with a single frame on \datasetout. We speculate that 
Qwen2-VL excels at scene text recognition at static frames but shows insufficient capability in handling information redundancy across multiple frames.
\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figure/expr_3_zhexian.pdf}
\vspace{-0.2in}
\caption{Effects with different numbers of frame inputs sampled at 1fps backward from the question timestamp.} 
\label{fig:expr_3_zhexian}
\vspace{-0.6cm}
\end{figure}

\noindent\textbf{Q2: Is temporal grounding important? Sometimes.}
We use the three frames from question generation as model inputs (``QA Frames"), which consistently improve performance on \datasetin~but have inconsistent effects on \datasetout in Table~\ref{tab:expr_tab3}.
%We take the three frames used in question generation as model inputs (``QA Frames"). Table~\ref{tab:expr_tab3} shows that this effectively boosts all model performances on \datasetin. However, the improvements on \datasetout~are not stable among models. Specifically,
Qwen2-VL~\cite{wang2024qwen2} and InternVL2-8B~\cite{chen2024far} increase their accuracy by 3.7\% and 2.1\% respectively but MiniCPM-V 2.6~\cite{yao2024minicpm} and Gemini 1.5 Pro decrease by 2.1\% and 7.1\%, respectively. Overall, the improvement suggests there is space for models to optimize their temporal grounding, \ie, to localize the key frames for answering.
In contrast, the decrease in performance indicates that the models may be vulnerable to seriously reduced information input. It may also reflect the great efforts made by annotators for correcting mistakes from the automatic QA generation, as also exemplified by the poor results of GPT-4o (QA generator).


We investigate a question-timestamp aware sampling strategy: starting from the question timestamp and sampling backwards at 1 fps. This strategy is inspired by intuition that people often pose questions based on their most recent visual settings. Figure \ref{fig:expr_3_zhexian} shows that incorporating such a sampling strategy significantly improves Qwen2-VL's performance on \datasetout, even with fewer frames. However, for Gemini 1.5 Pro, the opposite is true and more frames are key for better performance. On \datasetin, we find that both Qwen2-VL and Gemini 1.5 Pro shrink their performances compared with standard uniform sampling across the whole video, possibly due to a lack of necessary information for long video reasoning.




\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{figure/expr_1_leida.pdf}
\caption{Higher resolution (1920$\times$1080, 1280$\times$720) QA Frames generally improve performance on \datasetout. } 
\label{fig:expr_1_leida}
\vspace{-0.3cm}
\end{figure}


\noindent\textbf{Q3: Are high-resolution frames for scene text critical? Yes.} Most existing MLLMs are limited to accepting fixed, low-resolution images as input. When higher resolutions are used, the number of input frames is limited. We compare the performance of the model when using the “low-resolution video” and the three key “low-resolution frames” and “high-resolution frames” specified for question generation as input. Here we choose GPT-4o~\cite{achiam2023gpt} and three open-source models that can accept frames with resolutions of up to 1920$\times$1080 and 1280$\times$720. Figure~\ref{fig:expr_1_leida} shows that answering with three key frames (``QA Frames'') at low resolution is comparable to uniform video sampling. Yet, with the increase of resolution, GPT-4o~\cite{achiam2023gpt}, Qwen2-VL~\cite{wang2024qwen2}, InternVL2-8B~\cite{chen2024far}, and MiniCPM-V 2.6~\cite{yao2024minicpm} all show profound improvements, up to 18.6$\sim$31.0\%. 

Aside from increasing the \emph{global frame} resolution, we focus on increasing the \emph{local scene-text} resolution while keeping the global frame resolution unchanged. We use Microsoft Azure OCR\footnotetext{\href{https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/}{https://azure.microsoft.com/en-us/services/cognitive-services}} for scene text detection and apply the SOTA scene text super-resolution model DiffTSR~\cite{zhang2024diffusion} to increase the resolution of the detected regions to 380 $\times$ 128. To project the super-resolution scene text back to the original low-resolution video frames, we consider three options: resize to 1.00 $\times$, 1.25$\times$, and 1.50$\times$ of the original scene text size.
Table \ref{tab:str} shows that higher-resolution scene text improves performance on \datasetout~but yields inconsistent results on \datasetin. We speculate that scene text detection in Ego4D videos is more challenging than in RoadTextVQA due to lower resolution and complex perspectives. This is further supported by the smaller performance gains on \datasetin~(\vs.~Outdoor) with additional OCR results in Table \ref{tab:expr_modal}.


\begin{table}
\caption{Effects of scene-text resolution and area ratio. \emph{Scale} parameter is to control the magnification ratio of the scene text regions. SR: Super Resolution Scene Text. We experiment with 30\% of the data for efficiency.}
\label{tab:str}
\vspace{-0.2cm}
\setlength{\tabcolsep}{.4em}
\centering
\label{tab:expr_tab6}
\fontsize{10}{12}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|c|cc|cc}
\Xhline{1pt}  
\multirow{2}{*}{\textbf{Method}} & \multirow{2}{*}{\textbf{Scale}} & \multicolumn{2}{c|}{\textbf{\datasetout}} & \multicolumn{2}{c}{\textbf{\datasetin}}  \\
\cline{3-6}
& & Accuracy & Score & Accuracy & Score  \\
\hline
{Qwen2-VL~\cite{wang2024qwen2}} & - & {28.2} & {2.0} & {23.3} & {1.8} \\
\hline
\multirow{3}{*}{Qwen2-VL w/ SR} 
& 1.00 $\times$ & 31.9 & 2.1 &  24.2 & 1.8 \\
&  1.25 $\times$  &  33.1  &  2.2  &  23.5  & 1.8  \\
&  1.50 $\times$  &  34.1 & 2.2 &  22.3  & 1.7 \\
\hline
Gemini 1.5 Pro~\cite{reid2024gemini} & - & 33.4  & 2.0 & 34.4  & 2.1 \\
\hline
\multirow{3}{*}{Gemini 1.5 Pro w/ SR} 
& 1.00 $\times$  & 36.9  & 2.2 &  31.7 & 2.1 \\
& 1.25 $\times$ &  38.1  &  2.2  &  30.1  & 2.0  \\
& 1.50 $\times$ &  38.1 & 2.2 & 30.2 & 2.0   \\
\Xhline{1pt}
\end{tabular}
\end{threeparttable}}
\vspace{-0.3cm}
\end{table}

\begin{table}[t!]
\caption{Effects of different modality inputs. V: Video. Q: Question. ST: Scene Text.}
\vspace{-0.2cm}
\label{tab:expr_modal}
\setlength{\tabcolsep}{.1em}
\centering
\fontsize{13}{15}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ccc|ll|ll}
\Xhline{1pt}  
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Input}} & \multicolumn{2}{c|}{\textbf{EgoTextVQA-Outdoor}} & \multicolumn{2}{c}{\textbf{EgoTextVQA-Indoor}} \\
\cline{2-8}
& V  & Q  & ST  & Accuracy  & Score & Accuracy  & Score  \\
\Xhline{1pt}
\multirow{4}{*}{Qwen2-VL~\cite{wang2024qwen2}} & - & \checkmark & - & 2.1  &  0.5 &  2.5  &  0.5  \\
& - & \checkmark & \checkmark & 15.4  & 1.2   &  13.3 & 1.1   \\
\cline{2-8}
& \checkmark & \checkmark & - &   28.2 & 2.0  &  23.3 & 1.8   \\
& \checkmark & \checkmark & \checkmark  &  39.9 {\color{limegreen} $\uparrow$ 11.7}  &  2.5 {\color{limegreen} $\uparrow$ 0.5}  &  23.6 {\color{limegreen} $\uparrow$ 0.3} &  1.8 {\color{limegreen} $\uparrow$ 0.0} \\
\hline
\multirow{4}{*}{LLaVA-Next-Video~\cite{zhang2024llavanextvideo}} & - & \checkmark & - &  8.5 & 1.1& 6.0 & 1.0  \\
& - & \checkmark & \checkmark &  20.6 & 1.4  &  15.6  & 1.2   \\
\cline{2-8}
& \checkmark & \checkmark & - & 19.5 & 1.4 & 22.7 & 1.8   \\
& \checkmark & \checkmark & \checkmark  & 37.4 {\color{limegreen} $\uparrow$ 17.9 } & 2.3 {\color{limegreen} $\uparrow$ 0.9 } & 30.8 {\color{limegreen} $\uparrow$ 8.1} & 2.1 {\color{limegreen} $\uparrow$ 0.3} \\
\hline
% \multirow{4}{*}{VILA1.5-8B~\cite{lin2024vila}}& - & \checkmark & - & 3.4 & 0.7 & 3.8 & 0.7 \\
% & - & \checkmark & \checkmark & 15.4 & 1.1 &  11.2  & 1.0   \\
% \cline{2-8}
% & \checkmark & \checkmark & - & 17.3 & 1.5 & 18.2 & 1.6 \\
% & \checkmark & \checkmark & \checkmark  & 22.7 {\color{limegreen} $\uparrow$ 5.4 } & 1.6 {\color{limegreen} $\uparrow$ 0.1 } &  21.8 {\color{limegreen} $\uparrow$ 3.7} &  1.8 {\color{limegreen} $\uparrow$ 0.2} \\
% \hline
\multirow{4}{*}{GPT-4o~\cite{achiam2023gpt}}& - & \checkmark & - & 5.26 & 0.84 & 5.7 & 0.9  \\
& - & \checkmark & \checkmark & 24.1  & 1.71  & 17.6 & 1.4  \\
\cline{2-8}
& \checkmark & \checkmark & - & 30.3  & 1.7 & 28.3  & 1.8  \\
& \checkmark & \checkmark & \checkmark  & {\bf52.9} {\color{limegreen} $\uparrow$ 22.6} & {\bf3.0} {\color{limegreen} $\uparrow$ 1.3} & 37.9 {\color{limegreen} $\uparrow$ 9.6 } & 2.3 {\color{limegreen} $\uparrow$ 0.5 } \\
\hline
\multirow{4}{*}{Gemini 1.5 Pro~\cite{reid2024gemini}}& - & \checkmark & - & 4.3  &  0.8 & 4.6  & 0.8 \\
& - & \checkmark & \checkmark & 18.2 & 1.3  & 15.2  & 1.0   \\
\cline{2-8}
& \checkmark & \checkmark & - & 33.4 & 2.0 & 34.4 & 2.1  \\
& \checkmark & \checkmark & \checkmark  & 49.5 {\color{limegreen} $\uparrow$ 16.1 } & 2.9 {\color{limegreen} $\uparrow$ 0.9 } & {\bf39.5} {\color{limegreen} $\uparrow$ 5.1 } & {\bf2.4} {\color{limegreen} $\uparrow$ 0.3 } \\
\Xhline{1pt}
\end{tabular}
\end{threeparttable}}
\vspace{-0.5cm}
\end{table}




\noindent\textbf{Q4: Is text alone enough to answer questions? No.}
We feed the best-performing open-source and closed-source models with only the scene text, but as Table~\ref{tab:expr_modal} shows, none of these models can answer the questions reasonably. Pure scene text inputs are sufficient for answering some questions, but video input is more important for almost all the models. Furthermore, adding scene text together with video input further boosts the performances significantly, leading to the highest results in all our explorations. 
For instance, GPT-4o achieves 52.9\% on \datasetout ~and Gemini's accuracy on \datasetin~reaches to 39.5\%. The findings demonstrate the importance of both video and scene text inputs for better QA assistance.
More heuristic solution investigations can be found in Appendix~{\color{red}B.3}.
%~\ref{sup:heuristic}.


%------------------------------------------------------
% \begin{table}
% \caption{Effects of different numbers of video frames sampled at 1fps right before the question timestamp.}
% \label{tab:expr_tab3}
% \setlength{\tabcolsep}{.5em}
% \centering
% \fontsize{8}{10}\selectfont
% \resizebox{\columnwidth}{!}{
% \begin{threeparttable}
% \begin{tabular}{l|c|ll|c|ll}
% \Xhline{1pt}  
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{\datasetout}} & \multicolumn{3}{c}{\textbf{\datasetin}}  \\
% \cline{2-7}
% & \#F & Accuracy & Score & \#F & Accuracy & Score  \\
% \Xhline{1pt}
% \multirow{4}{*}{Qwen2-VL~\cite{wang2024qwen2}} 
% & 1 &    &   & 1  &    & 1.5   \\ 
% & 4 &    &   & 4  &   & 1.7  \\ 
% & 8 &    &   & 8  &     & 1.7    \\ 
% & 16 &   &   & 16  &     & 1.8   \\ 
% \hline
% \multirow{4}{*}{Gemini 1.5 Pro~\cite{reid2024gemini}}
% & 1 &    &  & 10  & 28.1  & 1.8  \\ 
% & 4 &    &   & 20  & 29.5  &  1.9 \\ 
% & 8 &    &   & 30  &  30.7 & 2.0  \\ 
% & 16 &   &   & 40  & 31.9  & 2.0  \\ 
% \Xhline{1pt}
% \end{tabular}
% \end{threeparttable}}
% \end{table}


%------------------------------------------------------


% This illustrates the importance of high resolution for scene text understanding, and emphasizes that currently video mllm does not support video-level understanding when high resolution is used as input.
% \begin{table}
% \caption{Performance of MLLMs {when taking three video frames with high resolution (1920$\times$1080 and 1280$\times$720) for each QA generation as input on \datasetout}. Here we choose the MLLMs that can accept dynamic image resolutions for testing.} % (resolution is 640$\times$360) and \datasetin. 
% \label{tab:tab4}
% \setlength{\tabcolsep}{.2em}
% \centering
% \fontsize{10}{12}\selectfont
% \resizebox{\columnwidth}{!}{
% \begin{threeparttable}
% \begin{tabular}{l|cccccc}
% \Xhline{1pt}  
% \textbf{Method} & \textbf{Loc.} & \textbf{Dir.} & \textbf{Des.} & \textbf{Nec.} & \textbf{Oth.} & \textbf{Ave.} \\
% \Xhline{1pt}
% GPT-4o~\cite{achiam2023gpt} w/ low res. video & {28.31} / {1.64}  & 24.89 / 1.44  & 35.33 / {1.93}  & 31.61 / 1.81 & {29.84} / 1.81 & {30.26} / 1.72   \\ 
% GPT-4o~\cite{achiam2023gpt} w/ low res. frames &  25.78 / 1.54 & 29.83 / 1.75  & 35.54 / 1.98 & 29.89 / 1.78 & 31.94 / 1.97 & 30.30 / 1.76 \\ 
% GPT-4o~\cite{achiam2023gpt} w/ high res. frames &  49.44 / 2.71  & 43.17 / 2.42  &  65.68 / 3.46 & 58.67 / 3.15  & 56.02 / 3.16  & 54.66 / 2.96   \\ 
% \hline
% Qwen2-VL~\cite{wang2024qwen2} w/ low res. video & {23.39} / {1.81} & {35.19} / {2.37} & {30.79} / {1.94} & {25.58} / {1.90}  & {22.51}  / {1.84} & {28.16} / {1.97} \\
% Qwen2-VL~\cite{wang2024qwen2} w/ low res. frames &  31.03 / 2.12 & 35.71 / 2.49  & 33.17 / 2.04  & 28.41 / 2.01 & 25.13 / 2.03  &  31.89 / 2.15 \\
% Qwen2-VL~\cite{wang2024qwen2}  w/ high res. frames & 54.35 / 3.11 & 46.43 / 2.90 & 58.40 / 3.26 & 52.77 / 3.08 & 50.26 / 3.11 & 53.53 / 3.11 \\
% \hline
% InternVL2~\cite{chen2024far} w/ low res. video & 15.81 / 1.36  &  21.85 / 1.74 & 14.78 / 1.01 & 14.51 / 1.22  & 13.61 / 1.29  & 16.40 / 1.31 \\
% InternVL2~\cite{chen2024far} w/ low res. frames &  18.74 / 1.41 & 23.74 / 1.78  & 15.86 / 1.10  & 17.22 / 1.32  & 15.71 / 1.34  & 18.52 / 1.37 \\
% InternVL2~\cite{chen2024far} w/ high res. frames & 36.48 / 2.34 & 30.88/ 2.15 & 35.90 / 2.13 & 35.55 / 2.24 & 35.08 / 2.44 & 35.00 / 2.23 \\ 
% \hline
% MiniCPM-V 2.6~\cite{yao2024minicpm} w/ low res. video & 7.71 / 0.5 & 14.4 / 0.9 & 15.1 / 0.9 & 8.7 / 0.6 & 10.0 / 0.8 &  11.4 / 0.7 \\
% MiniCPM-V 2.6~\cite{yao2024minicpm} w/ low res. frames & 5.9 / 0.4  & 13.0 / 0.8  & 11.7 / 0.7  & 7.5 / 0.5 & 9.4 / 0.7  & 9.3 / 0.6  \\
% MiniCPM-V 2.6~\cite{yao2024minicpm} w/ high res. frames & 22.1 / 1.3 & 21.7 / 1.3 & 46.5 / 2.5 & 31.2 / 1.8 & 33.5 / 2.1 & 30.1 / 1.8  \\ 
% \Xhline{1pt}
% \end{tabular}
% \end{threeparttable}}
% \end{table}




% \begin{table}
% \caption{Effects of combining different heuristic strategies. T: Timestamp-Aware Sampling. ST: Additional Scene Text Input. HR: High-Resolution Scene Text (Scale = 1.25$\times$). We experiment with 30\% of the data for efficiency.}
% \label{tab:expr_comb}
% \setlength{\tabcolsep}{.1em}
% \centering
% \fontsize{13}{15}\selectfont
% \resizebox{\columnwidth}{!}{
% \begin{threeparttable}
% \begin{tabular}{l|ccc|cc|cc}
% \Xhline{1pt}  
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Input}} & \multicolumn{2}{c|}{\textbf{EgoTextVQA-Outdoor}} & \multicolumn{2}{c}{\textbf{EgoTextVQA-Indoor}} \\
% \cline{2-8}
% & T  & ST  & HR  & Accuracy  & Score & Accuracy  & Score  \\
% \Xhline{1pt}
% \multirow{3}{*}{Qwen2-VL~\cite{wang2024qwen2}} & \checkmark &  -  &  - &   & &   & \\
% & \checkmark & \checkmark & - &    &   &   &   \\
% & \checkmark & \checkmark & \checkmark  &    &   &   & \\
% \hline
% \multirow{3}{*}{Gemini 1.5 Pro~\cite{reid2024gemini}}& \checkmark & - & - &    &   &   &   \\
% & \checkmark & \checkmark & - &    &   &   &     \\
% & \checkmark & \checkmark & \checkmark  & 51.1 & 3.0  & 36.5 & 2.2 \\
% \Xhline{1pt}
% \end{tabular}
% \end{threeparttable}}
% \end{table}


%------------------------------------------------------


% \begin{table}
% \caption{Performance of MLLMs on the time-sensitive QA subset of \datasetout~including 623 QA pairs. }
% \label{tab:tab4}
% \setlength{\tabcolsep}{.5em}
% \centering
% \fontsize{9}{11}\selectfont
% \resizebox{\columnwidth}{!}{
% \begin{threeparttable}
% \begin{tabular}{l|cc|cc}
% \Xhline{1pt}  
% \multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{\datasetout}} & \multicolumn{2}{c}{\textbf{ }}  \\
% \cline{2-5}
% & Accuracy & Score & Accuracy & Score  \\
% \Xhline{1pt}
% GPT-4o~\cite{achiam2023gpt} & 19.26 & 1.20 & &\\ 
% Gemini 1.5 Flash~\cite{reid2024gemini} & 18.99 & 1.29 & &\\ 
% Gemini 1.5 Pro~\cite{reid2024gemini} & 20.22 & 1.37 & &\\ 
% \hline
% CogVLM2-Video~\cite{hong2024cogvlm2} & 7.22 & 0.98 & & \\ 
% ShareGPT4Video~\cite{chen2024sharegpt4video} & 8.03 & 1.01 & &\\ 
% MiniCPM-V 2.6~\cite{yao2024minicpm} & 15.09 & 0.87 & & \\
% LLaVA-Next-Video~\cite{zhang2024llavanextvideo} & 15.25 & 1.03 & &\\ 
% VILA1.5-8B~\cite{lin2024vila} & 10.59 & 1.05 \\ 
% InternVL2-8B~\cite{chen2024far}  & 12.04 & 1.10 \\ 
% Qwen2-VL~\cite{wang2024qwen2} & 14.13 & 1.34 \\
% \Xhline{1pt}
% \end{tabular}
% \end{threeparttable}}
% \end{table}









