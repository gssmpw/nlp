\section{Related Work}
\subsection{Scene-Text VQA Benchmark}
In the scene-text VQA field, various image- and video-based datasets~\cite{singh2019towards, wang2020general, biten2019scene, zhao2022towards, tom2023reading, mishra2019ocr, liu2024ocrbenchhiddenmysteryocr} have been proposed.  
The datasets like TextVQA~\cite{singh2019towards}, ST-VQA~\cite{biten2019scene}, and ESTVQA~\cite{wang2020general} provide high-quality images and questions that clearly point to scene text, but such simple settings limit practical applications. 
To expand the application scope, the datasets like M4viteVQA~\cite{zhao2022towards} and RoadTextVQA~\cite{tom2023reading} offer text-rich videos, but they design simple questions which refer to well-focused scene text. Such simplified settings make QA less practical and they challenge no more than an identification of the scene text. In addition, RoadTextVQA focuses merely on road driving.
In this work, we introduce \dataset~to advance research in real-life QA assistance from an egocentric perspective. Our videos cover diverse daily scenarios, with the questions reflecting real user needs yet without the visual attention on scene text.


\subsection{MLLMs for Scene-Text VQA}
Recent advancements in Multimodal Large Language Models (MLLMs)~\cite{zhang2024beyond, wang2023cogvlm, zhang2024llava} have shown significant potential in addressing scene-text VQA tasks~\cite{singh2019towards, biten2019scene, liu2024ocrbenchhiddenmysteryocr}.  Current MLLMs~\cite{luo2024feast, zhang2024beyond, li2025flexattention} achieve substantial performance gains in image scene-text QA by enhancing their ability to process higher-resolution images, leading to significant improvements and strong performance. However, these models are limited to image-level input, leaving their video comprehension capabilities largely unexplored. To address this limitation, recent models~\cite{yao2024minicpm, zhang2024llavanextvideo, wang2024qwen2, lin2024vila, chen2024far, yang2022video,yang2021deconfounded,hong2024cogvlm2,chen2024sharegpt4video} are developed to support video-level or multi-frame inputs while retain their performance on image-based scene-text QA datasets. Nonetheless, these models have been evaluated separately on image-based OCR-rich benchmarks~\cite{singh2019towards, biten2019scene, liu2024ocrbenchhiddenmysteryocr, mishra2019ocr, mathew2021docvqa, mathew2022infographicvqa} and video understanding benchmarks~\cite{mangalam2023egoschema, xiao2021next, fu2024video, li2024mvbench,shang2019annotating}, lacking suitable assessment for scene-text comprehension in more realistic and dynamic environments. In this work, we will comprehensively analyze the model behaviors in egocentric QA assistance in real-life dynamic environments.



