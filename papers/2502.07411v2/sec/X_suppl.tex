\clearpage
\setcounter{page}{12}
\maketitlesupplementary

\setcounter{section}{0} 
\renewcommand{\thesection}{\Alph{section}}

\section{\dataset~Dataset} 
\label{sup:data}
\subsection{Manual Participation}
We present examples of QA pairs generated by GPT-4o in Figure~\ref{fig:supp_human} to highlight the issues of automatic generation and underscore the value of manual correction. The primary problems observed are as follows: 
\textbf{(a) Hallucinated Answers:} The generated answers are unseen from the visual environment and cannot be confirmed by the annotators.
% These questions are arised from the model’s hallucination, where the answers cannot be derived from the video content and typically require manual deletion. 
\textbf{(b) Scene Text Irrelevance:} The questions have vague references and fail to incorporate scene text understanding for answers. 
\textbf{(c) Scene Text Errors:} The questions or answers contain incorrect scene text. 
\textbf{(d) and (f) Non-colloquial Questions:} The questions are mechanical; they are phrased unnaturally and do not align well with daily spoken language. 
\textbf{(e) Not Reflect User Needs:} The question does not reflect real user needs and hardly occurs in human daily life.
After manual participation, about 70\% of the generated QAs are deleted and 30\% of the remaining QAs are revised. 


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figure/supp_human.pdf}
\caption{Manual participation on \dataset~creation.}
\label{fig:supp_human}
\end{figure*}


\section{Experiment}
\subsection{Model Details}
\label{sup:models}
We provide a concise introduction to the MLLMs evaluated in Section~{\color{red}4}%~\ref{sec:expr}
, as outlined below:
\begin{itemize}
    \item \textbf{GPT-4o}~\cite{achiam2023gpt} advances the GPT-4 family towards more natural human-computer interactions. 
    \item \textbf{Gemini 1.5 Pro}~\cite{reid2024gemini} builds on Gemini 1.0’s~\cite{team2023gemini} research advances and multimodal capabilities and it is optimized for a wide-range of reasoning tasks. 
    \item \textbf{Gemini 1.5 Flash}~\cite{reid2024gemini} is a model from Gemini 1.5 family offering low latency and enhanced performance. 
    \item \textbf{Qwen2-VL}~\cite{wang2024qwen2} employs a ViT-675M~\cite{radford2021learning} as the visual encoder, Qwen2-7B as the language model, and an MLP projector. It improves upon Qwen-VL~\cite{bai2023qwen} with (1) naive dynamic resolution, allowing ViT to handle images of varying resolutions, and (2) multimodal rotary position embedding, which decomposes positional encoding into temporal, height, and width components. Qwen2-VL is pre-trained on diverse datasets, including image-text pairs, OCR data, interleaved articles, VQA datasets, video dialogues, and image knowledge sources, enabling a stronger multimodal understanding.
    \item \textbf{LLaVA-NeXT-Video}~\cite{zhang2024llavanextvideo} choose SigLIP-SO400M~\cite{zhai2023sigmoid} as the visual encoder, Qwen2~\cite{wang2024qwen2} as the language model, and a two-layer MLP as the projector. It utilizes the AnyRes~\cite{liu2024llavanext} technique to segment high-resolution images for the visual encoder and extends this approach to video processing. LLaVA-NeXT-Video has excellent reasoning, OCR, and world knowledge capabilities, achieving strong performance in video-based multimodal tasks.
    \item \textbf{VILA1.5}~\cite{lin2024vila} integrates CLIP-L~\cite{radford2021learning} as the visual encoder, LLaMA-2~\cite{touvron2023llama} as the language model, and a linear projector. It fine-tunes on a mix of internal data, including OCR-VQA~\cite{mishra2019ocr} and ST-VQA~\cite{biten2019scene}, and improves contextual learning by unfreezing the LLM during interleaved image-text pre-training. VILA1.5 excels in video reasoning, in-context learning, visual chain-of-thought reasoning, and world knowledge.
    \item \textbf{InternVL2-8B}~\cite{chen2024far} integrates InternViT-300M~\cite{chen2024internvl} with InternLM2.5-7B~\cite{cai2024internlm2} via a randomly initialized MLP projector. It is trained on OCR datasets generated by PaddleOCR~\cite{li2022pp}, utilizing Chinese images from Wukong and English images from LaionCOCO~\cite{schuhmann2021laion}. Building on the strong visual representations and high-resolution image processing capabilities of InternVL1.5~\cite{chen2024far}, InternVL2 incorporates instruction tuning, enabling competitive performance in document and chart comprehension, infographics QA, scene text understanding, OCR, and multimodal reasoning tasks.
    \item \textbf{CogVLM2-Video}~\cite{hong2024cogvlm2} utilizes the EVA-CLIP~\cite{sun2023eva} as the visual encoder, LLaMA3-8B as the language model, and a 2$\times$2 convolutional layer followed by a SwiGLU~\cite{shazeer2020glu} as the adapter. Unlike CogVLM~\cite{wang2023cogvlm}, CogVLM2 improves pre- and post-training data diversity and quality. The Synthetic OCR Dataset, a key pre-training resource, includes four OCR scenarios: (1) synthetic OCR images with text generated in Python, (2) real-world images with PaddleOCR~\cite{li2022pp}, (3) academic papers with extracted LaTeX via Nougat~\cite{blecher2023nougat}, and (4) HTML/LaTeX-rendered tables and formulae. CogVLM2-Video adapts CogVLM2 for videos, enhancing open-domain QA with temporal localization and timestamp-aware QA.
    \item \textbf{MiniCPM-V 2.6}~\cite{yao2024minicpm} employs SigLIP-SO400M~\cite{zhai2023sigmoid} as the visual encoder, Qwen2~\cite{wang2024qwen2} as the language model, and a compression module with one-layer cross-attention and a moderate number of queries as the projector. Its training includes pre-training on English and Chinese image captioning and OCR data, followed by fine-tuning on datasets like TextVQA~\cite{singh2019towards}, OCR-VQA~\cite{mishra2019ocr}, and ST-VQA~\cite{biten2019scene}. MiniCPM-V 2.6 excels in conversational and reasoning tasks across multiple images and videos, with high-resolution perception enabling features like table-to-markdown conversion and OCR transcription.
    \item \textbf{ShareGPT4Video}~\cite{chen2024sharegpt4video} builts on LLaVA-Next-8B~\cite{li2024llava}. Based on the proposed ShareGPT4Video~\cite{chen2024sharegpt4video} dataset, the proposed captioning model ShareCaptioner-Video generates high-quality captions with detailed temporal descriptions for various videos. ShareCaptioner-Video is fine-tuned with the collected video caption data. For video understanding, ShareGPT4Video's training dataset combines VQA samples from various instructional video-to-text datasets with video-caption pairs.
\end{itemize}



\subsection{Study of MLLM Design}
\label{sup:model_design}
%As the results in Tables~\ref{tab:expr_tab1} and \ref{tab:expr_tab2}, 
We analyze key factors contributing to the superior performance of strong models (Qwen2-VL~\cite{wang2024qwen2}, LLaVA-NeXT-Video~\cite{zhang2024llavanextvideo}, and InternVL2~\cite{chen2024internvl}): 
(1) \textbf{enhanced visual encoder} capable of handling high-resolution and long-video inputs. In Table~\ref{tab:re_tab3}, increasing the number of video frames and resolution improves Qwen2-VL's performance by 1.2\% and 5.8\%;
(2) \textbf{more powerful LLM backbones}. Compared with InternVL2-8B, InternVL2-26B  performance has a 7\% increases in Table~\ref{tab:re_tab3}; and 
(3) \textbf{large-scale OCR training data}. Beyond the commonly used TextVQA datasets, InternVL2 leverages PaddleOCR to generate OCR samples for training. 
Additionally, we observe that as the number of video frames increases, Qwen2-VL's performance improves, whereas InternVL2's declines, underscoring the effectiveness of Qwen2-VL's video embedding design. 


\subsection{Heuristic Solution Investigations}
\label{sup:heuristic}
\noindent\textbf{Effect of Timestamp-Aware Sampling}
We further investigate an alternative question-timestamp aware sampling strategy: starting from the question timestamp and uniformly sampling within fixed durations of 4 seconds and 32 seconds. As shown in Table~\ref{tab:supp_tab1}, on \datasetout, when sampling the same number of frames (\#F=16), this fixed duration sampling strategy achieves comparable or even superior performance to standard uniform sampling across the whole video for both Qwen2-VL~\cite{wang2024qwen2} and Gemini 1.5 Pro~\cite{reid2024gemini}. However, on \datasetin, when sampling \#F=48, we observe that while Qwen2-VL~\cite{wang2024qwen2} maintains comparable performance to standard uniform sampling, the performance of Gemini 1.5 Pro~\cite{reid2024gemini} drops by by about 7\%. This decline may stem from Gemini 1.5 Pro's stronger performance on questions requiring long-term video comprehension, which is less effectively captured by this fixed-duration sampling approach.


\noindent\textbf{Combination of Heuristic Strategies}
In the main text, we have explored different heuristic strategies separately. Here, we additionally study the combinations of heuristic strategies. First, for timestamp-aware video sampling, we adopt the strategy of ``\emph{fixed-duration sampling}'' to \datasetout~and ``\emph{1fps-backward sampling}'' to \datasetin, inspired by the results of the two different video sampling strategies on these two datasets.  
The results in Table~\ref{tab:supp_tab2} show that the models achieve cumulative performance improvements as heuristic strategies are progressively applied on \datasetout. Yet, the improvements are not stable on \datasetin, suggesting the significant challenge of egocentric scene-text aware QA assistance in daily house-keeping. 

\begin{table}
\caption{\small Study of MLLM design on \datasetout. The alignment module is MLP layer. VE: Visual Encoder.
}
\label{tab:re_tab3}
\setlength{\tabcolsep}{.4em}
\centering
\fontsize{8}{10}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|l|c|l|ll}
\hline 
\textbf{Method} & \textbf{VE} & \textbf{Res.} & \textbf{\#F} & Accuracy & Score  \\
\hline
\multirow{3}{*}{\makecell[c]{Qwen2-VL~\cite{wang2024qwen2} }}
& ViT-675M  & $448^{2}$ & 16 & 22.4 & 1.6 \\
& ViT-675M  & $448^{2}$ & 32 & 23.6 & 1.7 \\
& ViT-675M & - & 16 & 28.2 & 2.0 \\
\hline 
\multirow{2}{*}{InternVL2-8B~\cite{chen2024internvl}}  & InternViT-300M & $448^{2}$  & 16 & 16.5 & 1.3 \\
& InternViT-300M & $448^{2}$ & 32 & 16.4 & 1.3 \\
\hline 
InternVL2-26B~\cite{chen2024internvl}
& InternViT-6B & $448^{2}$ & 16 & 23.5 & 1.7 \\
\hline 
\end{tabular}
\end{threeparttable}}
\end{table}


%------------------------------------------------------
\begin{table}
\caption{Effects of different numbers of video frames uniformly sampled within fix-duration before the question timestamp. We set a fixed duration of 4 seconds in \datasetout~and 32 seconds in \datasetin. S: Standard Uniformly Sampling. F: Fix-Duration Sampling. We experiment with 30\% of the data for efficiency.}
\label{tab:supp_tab1}
\setlength{\tabcolsep}{.4em}
\centering
\fontsize{8}{10}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|c|ll|c|ll}
\Xhline{1pt}  
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{\datasetout}} & \multicolumn{3}{c}{\textbf{\datasetin}}  \\
\cline{2-7}
& \#F & Accuracy & Score & \#F & Accuracy & Score  \\
\Xhline{1pt}
Qwen2-VL~\cite{wang2024qwen2} w/ S & 16 & 28.2 & 2.0  & 48  & 23.3 & 1.8  \\
\hline
\multirow{4}{*}{Qwen2-VL~\cite{wang2024qwen2} w/ F} 
& 4 & 26.1 & 1.8  & 12  & 16.2 & 1.4  \\ 
& 8 & 29.3 & 2.0  & 24  & 18.3 & 1.5   \\ 
& 12 & 29.9 & 2.0 & 36 & 22.1 & 1.7  \\ 
& 16 & 30.9 & 2.1 & 48 & 22.7 & 1.7  \\ 
\hline
Gemini 1.5 Pro~\cite{reid2024gemini} w/ S & 32 & 33.4 & 2.0  & 60  & 34.4 & 2.1  \\
\hline
\multirow{4}{*}{Gemini 1.5 Pro~\cite{reid2024gemini} w/ F}
& 4 & 27.9 & 1.7  & 12  & 22.8 & 1.6 \\ 
& 8 & 32.9 & 1.9  & 24  & 27.1 & 1.7 \\ 
& 12 & 33.5 & 2.0  & 36 & 29.0 & 1.8 \\ 
& 16 & 33.4 & 2.0  & 48 & 27.3 & 1.8  \\ 
\Xhline{1pt}
\end{tabular}
\vspace{-0.3cm}
\end{threeparttable}}
\end{table}
% ego4d,qwen2vl在8s内均匀采样48帧, acc=19.7, score=1.6 
% ego4d,qwen2vl在16s内均匀采样48帧, acc=21.1, score=1.7
% ego4d,qwen2vl在32s内均匀采样48帧, acc=22.7, score=1.7



\subsection{Human Study}
\label{sup:human}
In Section~{\color{red}4}, the human results %in Tables~\ref{tab:expr_tab1} and \ref{tab:expr_tab2}
are based on two rounds of standard human studies. Based on the reason analysis for the poor human performance in Section~{\color{red}4.2},
%~\ref{expr:ra}
we further validate the human performance by reducing the scene text recognition challenge. We sample 100 additional questions for humans to answer by providing the corresponding question frames. 
Table~\ref{tab:re_tab4} shows that humans perform better without the challenge of temporal grounding but still lag behind the best closed-source model (GPT-4o~\cite{achiam2023gpt}). This suggests advanced models may surpass humans in scene-text recognition or external knowledge, highlighting the importance of research on scene-text QA assistance.


\begin{table}
\caption{Effects of combining different heuristic strategies. T: Timestamp-Aware Sampling. ST: Additional Scene Text Input. HR: High-Resolution Scene Text (Scale = 1.25$\times$). We experiment with 30\% of the data for efficiency.}
\label{tab:supp_tab2}
\setlength{\tabcolsep}{.1em}
\centering
\fontsize{13}{15}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ccc|cc|cc}
\Xhline{1pt}  
\multirow{2}{*}{\textbf{Method}} & \multicolumn{3}{c|}{\textbf{Input}} & \multicolumn{2}{c|}{\textbf{EgoTextVQA-Outdoor}} & \multicolumn{2}{c}{\textbf{EgoTextVQA-Indoor}} \\
\cline{2-8}
& T  & ST  & HR  & Accuracy  & Score & Accuracy  & Score  \\
\Xhline{1pt}
\multirow{4}{*}{Qwen2-VL~\cite{wang2024qwen2}} & - & - & - &  28.2 & 2.0 & 23.3  & 1.8 \\
& \checkmark &  -  &  - & 30.9 & 2.1 & 22.6 & 1.7 \\
& \checkmark & \checkmark & - & 42.4 & 2.7  & 25.3 & 1.8  \\
& \checkmark & \checkmark & \checkmark  & 42.6 & 2.7 & 27.1 & 1.9 \\
\hline
\multirow{4}{*}{Gemini 1.5 Pro~\cite{reid2024gemini}}& - & - & - & 33.4 & 2.0 & 34.4 & 2.1 \\
& \checkmark & - & - &  34.7 & 2.0  & 31.1 & 2.0  \\
& \checkmark & \checkmark & - &  49.5 & 2.9 & \bf38.0 & \bf2.3    \\
& \checkmark & \checkmark & \checkmark  & \bf51.1 & \bf3.0  & 36.5 & 2.2 \\
\Xhline{1pt}
\end{tabular}
\end{threeparttable}}
\end{table}

\begin{table}
\caption{Results of using video \emph{vs.} QA frames (three frames for QA generation) on \datasetin.}
\label{tab:re_tab4}
\setlength{\tabcolsep}{.6em}
\centering
\fontsize{6}{8}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ll|ll}
\hline 
\multirow{2}{*}{\textbf{Method}}  & \multicolumn{2}{c|}{\textbf{Video}} & \multicolumn{2}{c}{\textbf{QA Frames}}  \\
\cline{2-5}
& Accuracy & Score & Accuracy & Score  \\
\hline
 Human  &  26.0  & 1.9  & 36.0 & 2.3   \\
\hline
GPT-4o~[27]   & 25.0  & 1.6  &  \bf39.0 &  \bf2.4  \\
Gemini-1.5~Pro~[29]   & \bf33.0  & \bf2.0 & 35.0 &  2.2  \\ 
\hline 
\end{tabular}
\end{threeparttable}}
\end{table}


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{figure/supp_results.pdf}
\caption{Result visualization on \dataset.}
\label{fig:supp_results}
\end{figure*}


\subsection{Case Analysis}
\label{sup:qa_case}
As shown in Figure~\ref{fig:supp_results}, we qualitatively analyze the performance of MLLMs on \dataset. For \textbf{\datasetout}, the ``Description" example shows that all models struggle to accurately identify the target referred to by the question at the queried timestamp. The ``Location" example shows that only Gemini 1.5 Pro~\cite{reid2024gemini} correctly inferred the intention of the question and provided the precise answer. For the real-time ``Direction" examples, where the same question is posed at different timestamps, which corresponds to different answers, the left example shows that the questioned building is located right in front of the user at the question timestamp (4.3s), but all models fail to provide the correct answer, likely due to poor 3D spatial relation reasoning. 
Also, if the user poses the same question at 7.2s when he has moved to the left of the building, all models are unresponsive to such visual changes and tend to keep their original answers. This indicates that the models struggle to provide reasonable answers based on the real-time visual context in dynamic environment. 

For \textbf{\datasetin}, the ``Shopping" example shows that all models fail to effectively answer the total expense of 46.85 after the checkout process, indicating their
limitations in infer the total number after observing the price changes on the cashier's display during the checkout process. 
In the ``Book" example, the models need to identify the book from a large collection of books that matches the user’s needs (\eg, related to topics on plants or nature). The wrong answers indicate that the corresponding models are either weak at scene-text recognition and knowledge reasoning or tend to hallucinate their responses to match some question key words. Similar issues are also observed in the ``Hands-on" example.
% the model is tasked with observing real-time changes in the numerical values displayed on the screen and responding accurately; only Gemini 1.5 Pro provided the correct answer. 
Finally, the failures in the ``Gameplay'' example suggest that most models are weak in reasoning the real-time object state and people's real-time actions from an ego point of view. For example, while the ``rule sheet'' is on the table most of the time, it is on the other game player's hand at the time of user questioning.
% For the ``Gameplay" example, the challenge lies in focusing on the user's actions at the question timestamp. The model must base its answer on the events at 61.2s of the video rather than earlier content. In this case, Gemini 1.5 Pro answers correctly, while other models fail.



\section{Agreement between Human and Evaluator}
\label{sup:agreement}
In this section, we evaluate the performance of models on \datasetout~using GPT-4o mini~\cite{gpt4omini2024} and human annotators. Following \cite{cheng2024egothink}, we invite three annotators to assess GPT-4o~\cite{achiam2023gpt} and Gemini 1.5 Pro~\cite{reid2024gemini}, the overall best-performing model. Human annotators maintain the same scoring principle as the model, as shown in Table~\ref{tab:supp_pro3}. We randomly sample 100 QA pairs for evaluation. As shown in Table~\ref{tab:re_tab1}, GPT-4o mini and human annotators achieve similar Accuracy and Score, with Pearson correlation coefficients of 0.80 and 0.87, respectively, indicating strong consistency. The Cohen's Kappa coefficients among three volunteers are 0.77 on Accuracy, indicating a high human agreement. 
To ensure reproducibility, future evaluations should use the same model version (GPT-4o-mini-2024-07-18) and the prompt in Table~\ref{tab:supp_pro3}.
%The Cohen's Kappa coefficients (-1$\sim$1) among three volunteers are 0.77 on Acc, indicating a high human agreement.


\begin{table}[h]
\caption{\small Judgments of human and GPT-4o mini.}
\label{tab:re_tab1}
\setlength{\tabcolsep}{.4em}
\centering
\fontsize{6}{8}\selectfont
\resizebox{\columnwidth}{!}{
\begin{threeparttable}
\begin{tabular}{l|ll|ll}
\hline
\multirow{2}{*}{\textbf{Method}} & \multicolumn{2}{c|}{\textbf{GPT-4o~\cite{achiam2023gpt}}} & \multicolumn{2}{c}{\textbf{Gemini 1.5 Pro~\cite{reid2024gemini}}}  \\
\cline{2-5}
& Accuracy & Score & Accuracy & Score  \\
\hline
Human  & 36.0 & 1.9 & 47.3 & 2.5 \\
GPT-4o mini~\cite{gpt4omini2024}  & 34.0 & 1.8 & 42.0 & 2.3 \\
\hline
\end{tabular}
\end{threeparttable}}
\end{table}

\section{Model Prompts}
\label{sup:prompt}
Table~\ref{tab:supp_pro1} provides the prompts used by GPT-4o for question generation and filtering. Table~\ref{tab:supp_pro2} lists the prompts employed by GPT-4o for automatic question label annotation. Table~\ref{tab:supp_pro5} details the specific prompts applied for model inference. Table~\ref{tab:supp_pro3} shows the prompts used by GPT-4o-mini for model evaluation.  Table~\ref{tab:supp_pro4} includes the prompts designed for heuristic solutions with different modality inputs. 




\begin{table*}
\caption{Prompts for question-answer generation and filtering on \dataset}
\centering
\label{tab:supp_pro1}
\fontsize{9}{10}\selectfont 
\begin{tabular}{p{17cm}}
\hline
\makecell[c]{\textbf{Question-Answer Generation Prompts}}  \\
\hline
\#\# Question Prompt: \\
Give you a first-person perspective video, which records the scene you see from the first-person perspective. Please judge from your perspective whether there are scene texts in the video. If so, please tell me what these scene texts are. Then, you have some questions about the scene texts you see, and ask three questions related to the activities you are going to carry out.
These scene texts can serve as clues to help you answer your questions.
Please generate three highly diverse questions based on the scene texts related to your activities in the first-person perspective video. 
If there is no scene text in the video, it is not necessary. Your questions should meet the following requirements:\\
Requirement 1: The questions should involve scene text understanding in the video.\\
Requirement 2: The questions should be goal-oriented and relevant to human daily life.\\
Requirement 3: The questions should require understanding multiple video frames, not just a single frame.\\
Requirement 4: The questions should be asked from a first-person perspective, expressed as colloquially as possible, and the first-person pronoun ``I" should be used appropriately.\\
Requirement 5: The questions should be of moderate length. \\
When announcing the question please label each question as ``Question 1, 2, 3: \{\emph{question}\}".\\
Please start your questions with the question word ``what", ``where", ``which", etc. You don't need to explain too much about what you are doing or indicate the location of the scene text in the video. Avoid the words ``video" and ``frame" in the questions. Remember to make sure that the correct answer to your question can be taken directly from the video and is concise enough.\\
Examples of good questions:\\
``Question 1: Which way is the exit?" \\
``Question 2: Could you tell me how much this item costs?" \\
``Question 3: What is the speed limit on this road?" \\
Image:\{\emph{image1}\}  Image:\{\emph{image2}\}  Image:\{\emph{image3}\} \\
\hline
\#\# Answer Prompt: \\
I provide three questions as follows: \{\emph{question}\} \\
You need to create an exam that tests above student abilities based on the three questions I just provided. Each question should have open-ended but short correct answers. Your answers have the following requirements:\\
Requirement 1: Your answers should be short and be closely related to the scene text in the video.\\
Requirement 2: Your answers should not mention any particular video frame number.\\
Requirement 3: Do not use letters for the answer choices.\\
You must print one correct answer and four wrong answers on separate lines in the following format: \"\ \\
Correct Answer :\{\emph{answer}\}\\
\hline
\makecell[c]{\textbf{Automatic Filtering Prompt}}  \\
\hline
You are a helpful assistant. You can answer the following questions based on your general knowledge. \\
Question: \{\emph{question}\} 
Answer briefly with a single word, a phrase, or a short sentence. \\
\hline
\end{tabular}
\end{table*}



\begin{table*}
\caption{Prompts for GPT-4o to annotate question categories on \datasetout.}
\centering
\label{tab:supp_pro2}
\fontsize{9}{11}\selectfont % 将字体大小设置为 10pt，行间距设置为 12pt
\begin{tabular}{p{17cm}}
\hline
\makecell[c]{\textbf{Question Classification Prompts}}  \\
\hline
Question: \{\emph{question}\} \\
Which of the following five categories does this question belong to? Please only answer the category name, such as Direction. \\
1. \textbf{Location}: Questions about a place or location. For example:  \\ 1) Where is the gas station? \\ 2) Which stores can I find on the right side of the road at this intersection? \\
2. \textbf{Direction}: Questions related to navigation, driving direction, and turns. For example:  \\ 1) Is the next road a left or right turn?  \\ 2) If I want to go to Cava, on which side of the street should I look for it?  \\ 3) Where should trucks go according to the signs? \\
3. \textbf{Description}: Questions that focus on scene text such as road signs, price labels, and billboards.  For example:  \\ 1) What does the sign on the side of the road say? \\ 2) What is the name of the center on the left side of the road? \\ 3) What is the name of the street to my right? \\
%4. \textbf{Distance}: questions about distance, mileage, or arrival time. For example: How many kilometers are there to the destination?  How far is it to the American Oncology Institute from this point? \\
4. \textbf{Intention Reasoning}: Questions about behavioral activities involving drivers or passengers to solve personal needs.  For example: \\
1) Where do I need to go to solve my financial problems? \\ 2) Is there a place nearby where I can shop for appliances and electronics?  \\
5. \textbf{Others}: Composite questions that involve multiple different or the same types of the above, such as asking about both description and location.  For example: \\ 1) What event is being advertised on the bus, and where is it taking place? \\ 2) What is the contact number for the leadspace building, and what service might they provide? \\
\hline
\end{tabular}
\end{table*}

\begin{table*}
\caption{Prompts for MLLM inference on \dataset.}
\centering
\label{tab:supp_pro5}
\fontsize{9}{12.5}\selectfont 
\begin{tabular}{l|p{13.5cm}}
\hline
\textbf{Model} & \makecell[c]{\textbf{General Prompts}} \\
\hline
GPT-4o & Based on the following images from a video, please briefly answer the following question with a single word, a phrase, or a short sentence. Question: \{\emph{question}\}.  Output the answer to the question in the following format: Answer: \{\emph{answer}\}. If you cannot answer the question, please answer ``Unanswerable'' and briefly explain why you cannot answer. \\
\hline
Gemini 1.5 Flash & Based on the following images from a video, please briefly answer the following question with a single word, a phrase, or a short sentence. Question: \{\emph{question}\}.  Output the answer to the question in the following format: Answer: \{\emph{answer}\}. If you cannot answer the question, please answer ``Unanswerable'' and briefly explain why you cannot answer. \\
\hline
Gemini 1.5 Pro & Based on the following images from a video, please briefly answer the following question with a single word, a phrase, or a short sentence. Question: \{\emph{question}\}.  Output the answer to the question in the following format: Answer: \{\emph{answer}\}. If you cannot answer the question, please answer ``Unanswerable'' and briefly explain why you cannot answer. \\
\hline
LLaVA-Next-Video & Please answer the following questions related to this video.  If you cannot answer the question, please answer ``Unanswerable'' and briefly explain why you cannot answer. Keep your answer as short as possible. Keep your answer as short as possible. Keep your answer as short as possible. Question: \{\emph{question}\} \\
\hline
CogVLM2-Video & You are a person in the situation shown in the following consecutive images from a video. You can answer questions that humans ask to help them make decisions. Now you are observing your surroundings and answering questions based on the current situation. Understanding the scene text around you is important for answering questions.  Answer the questions in the first-person perspective. If you cannot answer the question, please answer ``Unanswerable'' and briefly explain why you cannot answer.  Question: \{\emph{question}\} \\ 
% Understanding the scene text around you is important for answering questions. 对CogVLM2-Video有正面影响，但重复多次Keep your answer as short as possible.对答案长短没有影响。
\hline
InternVL2 & You are a person in the situation shown in the following consecutive images from a video.  You can answer questions that humans ask to help them make decisions. Now you are observing your surroundings and answering questions based on the current situation. Understanding the scene text around you is important for answering questions. Answer the questions in the first-person perspective. If you cannot answer the question, please answer 'Unanswerable' and briefly explain why you cannot answer. Keep your answer as short as possible! Keep your answer as short as possible! Keep your answer as short as possible! Question: \{\emph{question}\}\\
% Internvl2: 蓝色字段对结果有影响，但在answer准确程度上，肉眼看带来的影响有好有坏，我觉得两个prompt需要定量的检测一下准确度；
\hline
Qwen2-VL &  You are a person in the situation shown in the following consecutive images from a video.  You can answer questions that humans ask to help them make decisions. Now you are observing your surroundings and answering questions based on the current situation. Understanding the scene text around you is important for answering questions. Answer the questions in the first-person perspective. If you cannot answer the question, please answer 'Unanswerable' and briefly explain why you cannot answer. Question: \{\emph{question}\} \\ 
% Qwen2模型的结果较好1.加上蓝色字段某些问题的答案会更准确，可以进一步量化准确度来验证；2.变动prompt对结果的篇幅大小几乎没有影响
\hline
VILA1.5 & You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language. Question: \{\emph{question}\} \\
\hline
ShareGPT4Video & You are a person in the situation shown in the following consecutive images from a video. You can answer questions that humans ask to help them make decisions. Now you are observing your surroundings and answering questions based on the current situation. Understanding the scene text around you is important for answering questions. Answer the questions in the first-person perspective. If you cannot answer the question. please answer ``Unanswerable" and briefly explain why you cannot answer. Question: \{\emph{question}\} \\
\hline
MiniCPM-V 2.6  & You are a person in the situation shown in the following consecutive images from a video.  You can answer questions that humans ask to help them make decisions. Now you are observing your surroundings and answering questions based on the current situation. Understanding the scene text around you is important for answering questions. Answer the questions in the first-person perspective. If you cannot answer the question, please answer 'Unanswerable' and briefly explain why you cannot answer. Keep your answer as short as possible! Keep your answer as short as possible! Keep your answer as short as possible! Question: \{\emph{question}\} \\
% miniCPM模型整体结果很差，“unanswered”结果占比极高，调整prompt对结果影响不大，依然比较差，上面给出的两个promp只有有无“Keep your answer as short as possible!”字段的区别
\hline
\end{tabular}
\end{table*}




% 加入gp4o-mini用于evaluation的prompt
\begin{table*}
\caption{Prompts for GPT-4o-mini to evaluate MLLMs on \dataset.}
\centering
\label{tab:supp_pro3}
\fontsize{9}{12}\selectfont
\begin{tabular}{p{17cm}}
\hline
\makecell[c]{\textbf{Evaluation Prompts}}  \\
\hline
You are an intelligent chatbot designed for evaluating the correctness of generative outputs for question-answer pairs.
Your task is to compare the predicted answer with the correct answer and determine if they match meaningfully. Here's how you can accomplish the task: \\
\\
\#\#INSTRUCTIONS: \\
- Focus on the meaningful match between the predicted answer and the correct answer. 
Please note that not only matches of noun phrases between answers, but also matches of prepositional phrases.  \\
For example, ``at the car wash on your right" does not exactly match ``car wash".  ``at the gas station beside the sign 'gas sale'" does not exactly match ``gas station"" \\
- Consider synonyms or paraphrases as valid matches. 
Note that the predicted answer must be consistent with the string type of the correct answer, which may include phone numbers, email addresses, numbers, dates, etc.  \\
For example, the string types ``www.usps.com" and ``visit their website" are inconsistent,  the string types ``9849041316" and ``advertiser's contact number" are inconsistent." \\
- Evaluate the correctness of the prediction compared to the answer." \\

\\
Please evaluate the following video-based question-answer pair: \\
Question: \{\emph{question}\} Correct Answer: \{\emph{GT answer}\}  Predicted Answer: \{\emph{predicted answer}\} \\
Provide your $eval_{code}$ only as a yes/no and score where the score is an integer value between 0 and 5, with 5 indicating the highest meaningful match. \\
Please generate the response in the form of a Python dictionary string with keys 'pred' and 'score', where the value of 'pred' is a string of 'yes' or 'no' and the value of 'score' is in INTEGER, not STRING.
DO NOT PROVIDE ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string. 
For example, your response should look like this: \{'pred': 'yes', 'score': 5\}, \{'pred': 'no', 'score': 1\}.\\
\hline
\end{tabular}
\end{table*}



\begin{table*}
\caption{Prompts for heuristic solution study of different modality inputs on \dataset.}
\centering
\label{tab:supp_pro4}
\fontsize{9}{12}\selectfont 
\begin{tabular}{l|p{14.5cm}}
\hline
\textbf{Model Input} & \makecell[c]{\textbf{Prompts}} \\
\hline
w/ Q & You are a helpful assistant. You can answer the following questions based on your general knowledge. Question: \{\emph{question}\}\\
\hline
w/ Q \& ST & You are a helpful assistant. You are provided with some important scene text information. You can answer the following questions based on your common sense or the scene text information I provide.  Please answer as briefly as possible. Please note that this scenario text information is very important. You can find the scene text related to the question as the answer. Scene Text: \{\emph{OCR results}\} Question: \{\emph{question}\} \\
\hline
w/ V \& Q \& ST & You are a person in the situation shown in the following consecutive images from a video. You can answer questions that humans ask to help them make decisions. Now you are observing your surroundings and answering questions based on the current situation. I will provide you with the following scene text that may be included in each image. Understanding the scene text is important for answering questions. Answer the questions in the first-person perspective. If you cannot answer the question, please answer 'Unanswerable' and briefly explain why you cannot answer. The scene texts in Frame {0} include: \{\emph{OCR results}\}. The scene texts in Frame 1 include: \{\emph{OCR results}\}. The scene texts in Frame {2} include: \{\emph{OCR results}\}. The scene texts in Frame \{\emph{frame id}\} include: \{\emph{OCR results}\}. Question: \{\emph{question}\}  \\
\hline
\end{tabular}
\end{table*}







% \begin{figure*}
% \centering
% \includegraphics[width=\textwidth]{figure/qa_examples.pdf}
% \caption{Examples of \dataset.}
% \label{appfig:exp}
% \end{figure*}