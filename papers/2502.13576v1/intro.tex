\section{Introduction}


% 语言模型在size方面的scaling up带来了强大的能力涌现，同时也对高效地评测它们带来了障碍。
% For instance, evaluating a model with about 10B parameters on the HELM leaderboard can cost over \$1.7K \$ or more than 1.2K GPU hours. This inefficiency is particularly problematic when在开发阶段选择最优训练设置与在应用阶段选择最优模型、推理设置。
Scaling up models in multiple dimensions has led to remarkable advancements in their capabilities \citep{llama2,insgpt}, which also presents significant challenges for efficiently assessing them. For instance, \citet{liang2022holistic} reports that evaluating a model with approximately 10 billion parameters on the HELM leaderboard costs over \$1,700 via APIs or more than 1,200 GPU hours. Moreover, these costs scale by a factor of $X$ when exploring and comparing $X$ different training or inference configurations during the development or deployment phase.

% Moreover, such overhead will grow $X$ times when ablating the training configurations during the development phase or comparing the model and inference settings during the deployment phase among $X$ candidates. 
% when determining optimal training configurations during the development phase or selecting the best model and inference settings during the deployment phase among $X$ candidates. 

% 为了缩减评测开销，一些研究尝试从benchmark中选择具有代表性的子集，并测试待评测的目标模型在该子集上的表现，来近似其在整个benchmark上的结果。
% 为了缩减评测开销，一些研究探索了以下评测范式：依据一些模型（denoted as 原模型）在benchmark上的预测（which are freely available for many popular benchmarks）构建每个数据点的representation、对数据点进行聚类并选择若干聚类中心点构成core set、依据目标模型在core set上的predictions来近似其在整个benchmark上的表现。
% 该范式is built upon a simple insight: for many pairs of points from a given dataset, the predicted probability of the correct class strongly correlates across models. 

To achieve efficient evaluation, some studies \citep{AP,tiny} have explored the following paradigm: \textit{step 1.} constructing example embeddings according to the predictions from a set of \textbf{\textit{source models}} (which are freely available for popular leaderboards\footnote{\url{https://huggingface.co/open-llm-leaderboard}}\textsuperscript{,}\footnote{\url{https://rank.opencompass.org.cn}}\textsuperscript{,}\footnote{\url{https://crfm.stanford.edu/helm}}); \textit{step 2.} clustering the benchmark and selecting the cluster centroids to form a coreset (typically less than 100 examples); \textit{step 3.} approximating the performance of \textbf{\textit{target models}} under evaluation based on their predictions on the coreset. (See detailed related works in Appendix~\ref{apd:related_works}.) Underlying this approach is the assumption that performance patterns generalize: if source models respond similarly to two examples $a$ and $b$, then a target model’s performance on $a$ can be used to estimate its performance on $b$.
% Thus, if the source models' predictions on example $a$ and $b$ exhibit the same trend, then testing the target model on example $a$ should be sufficient to predict its result on example $b$ --- an embodiment of approximating the results on the benchmark according to the results on the coreset.
% This paradigm is based on the assumption that, for many examples in a benchmark, multiple models tend to exhibit similar performance. Consequently, if the source models' predictions for examples $a$ and $b$ follow a similar pattern, this similarity is expected to generalize to the target model --- testing the target model on example $a$ can thus predict its result on $b$.
% 然而，我们发现上述假设并不严格成立。Following AP, we construct an $M$-dimensional vector based on the correctness of $M$ source models for each example and visualize them through Principal Component Analysis. 用红色点表示的多个聚类中心，是AP通过聚类算法选出的coreset。与预想不同的是，在待评测的目标模型的表征空间下，coreset中的example显然不再能均匀地代表整个benchmark. 这说明
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.95\textwidth}
        \includegraphics[width=\textwidth]{figs/DistributionShift/DistributionShift_a.pdf}
        \caption{Hellaswag benchmark represented by source-model embeddings $\mathcal{D}^\mathcal{S}$.}
        \label{FigDistShift_a}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.95\textwidth}
        \includegraphics[width=\textwidth]{figs/DistributionShift/DistributionShift_b.pdf}
        \caption{Hellaswag benchmark represented by target-model embeddings $\mathcal{D}^\mathcal{T}$.}
        \label{FigDistShift_b}
    \end{subfigure}
    \vspace{-0.15cm}
    \caption{The t-SNE visualization of the Hellaswag benchmark using embeddings derived from source (above) and target (below) models' predictions. The increased average distance between examples and their cluster centroids in the target-based embedding indicates that the coreset (centroids) obtained from source-based embeddings no longer effectively represents the entire benchmark for target models.}
    \label{FigDistShift}
    \vspace{-0.5cm}
\end{figure*}
% However, we find that the above assumption does not necessarily hold. 

Nevertheless, we find that such generalizability between source and target models does not necessarily hold.
Following \textsc{AnchorPoint} \citep{AP}, we construct an embedding based on the correctness (e.g., the probability of the correct option) of all source models for each example and visualize them using t-SNE algorithm \citep{tsne}.
In these embeddings (Figure~\ref{FigDistShift_a}), nearby examples elicit similar predictions from the source models, allowing cluster centroids (marked by stars) to serve as representative points. Yet, when we adopt embeddings derived from the correctness of target models instead (Figure~\ref{FigDistShift_b}), the average distance between the example and its centroid increases from 10.09 to 12.48, indicating that the previously chosen centroids fail to represent their respective clusters effectively.
% The closer the two examples are, the more similar the source models perform on them (Figure~\ref{FigDistShift_a}). Therefore, the cluster centroids (marked as stars) are selected to represent each cluster and form the coreset.
% Prediction consistency引入
% However, when representing the examples using the correctness of target models (Figure~\ref{FigDistShift_b}), the examples in the original coreset are no longer able to represent the corresponding clusters perfectly, indicating that the prediction consistency among source and target models is not as high as assumed. This lack of generalizability results in the performance of target models on the coreset failing to approximate their true performance accurately.
% However, when using the correctness of target models (Figure~\ref{FigDistShift_b}) to represent examples, we observe that centroids in the original coreset fail to represent their corresponding clusters effectively. 
% This suggests a discrepancy in prediction behaviors between source and target models, which we refer to as \textit{prediction consistency}——the degree to which models exhibit similar prediction trends on the same examples. When prediction consistency is low, the generalizability of source model-based coreset to target models is compromised, leading to inaccurate performance approximations for target models.
This reveals a discrepancy in prediction behaviors between source and target models, which we term \textit{prediction consistency}—the extent to which their predictions align on the same examples. When prediction consistency is low, source-model-derived coresets fail to generalize, resulting in inaccurate performance estimates for target models.

% \begin{figure}[h]
%     \centering % [trim=left bot right top]
%     \includegraphics[scale=0.37]{figs/DistributionShift.pdf}
%     \caption{Phenomenon of Distribution Shift}
%     \label{FigDistShift}
% \end{figure}
% 为了解决原有范式的弊端，我们提出TailoredBench方法来通过Hybrid Priors-Posteriors构造目标模型自适应的coreset来准确衡量它们的性能。
% 具体而言，我们首先依据全体源模型与目标模型有着一定的一致性的先验构建一个静态的G-set(global-coreset)。目标模型在G-set上的预测结果作为探针被用来挑选与目标模型具有较强预测一致性的自适应源模型集合。依据该后验信息，我们设计了可扩展聚类技术来依据benchmark在自适应源模型集合下的表征将G-set扩展为动态的N-set(native-coreset). 最终，我们通过校准还原策略依据目标模型在N-set上的预测结果来近似其在整个benchmark上的表现。
To address the aforementioned issue, we propose the \textsc{TailoredBench} method, which adaptively constructs model-specific evaluation coreset in a global to native manner for accurate and efficient evaluation.
Specifically, we first construct a static G-set (Global-coreset) based on the prediction results of all the source models.
By applying an adaptive source model selection strategy, the predictions of target models on the G-set are used as a probe to select a native source model set for each target model that has stronger prediction consistency with them. 
Based on this posterior, we design a scalable K-Medoids clustering technique to expand the G-set into an N-set (Native-coreset) for each target model, according to the benchmark embeddings under the metric of corresponding native source models. Finally, we approximate the overall performance of target models by employing a calibrated estimation strategy based on their predictions on the N-set. 

We conduct extensive experiments on five benchmarks across more than 300 models, involving tasks in the fields of natural language and multi-modality. Compared to non-customized efficient evaluation baselines, \textsc{TailoredBench} can more accurately estimate the performance of models (attaining an average of 31.4\% MAE degradation improvement on accuracy) under the same small-size inference budgets (generally 20\textasciitilde 40 examples). Our contributions are summarized as follows:
% \begin{enumerate}[itemsep=0.5pt]
\begin{itemize}[leftmargin=20pt]
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\setlength{\parskip}{0pt}
\item We analyze that the existing efficient evaluation methods overestimate the prediction consistency across models, thus the source-model-based static coreset may fail to assess the target models accurately.
\item We propose the \textsc{TailoredBench} method to conduct tailored evaluation on adaptively constructed N-set for each target model to attain more accurate evaluation results.
\item We conduct comprehensive experiments and analyses on multiple settings to validate the excellent effectiveness and strong generalizability of \textsc{TailoredBench}.
% \end{enumerate}
\end{itemize}

% The rapid development of large language models (LLMs) have driven a significant demand for comprehensive evaluation of their capabilities. Traditional benchmarks, which often consist of thousands or even tens of thousands of examples (citation: OPEN-llm-leaderboard, AlpacaEval2.0, MMLU pro), provide a testbed for thorough assessment of model performance. However, assessing models on the whole benchmark can be computationally intensive and time-consuming. For instance, evaluating a model with about 10B parameters on the HELM leaderboard(citation: HELM-leaderboard), which includes large datasets like Hellaswag and GSM8K, can cost over \$1.7K or require more than 1.2K GPU hours. This inefficiency is particularly problematic when ranking numerous models, optimizing prompts, or during the model development phase, where rapid iteration and feedback are crucial for refining model capabilities.


% Fortunately, previous studies have demonstrated that evaluation benchmarks  generally contain a significant amount of redundant samples(citation: Anchor Points), where models exhibit similar performance across these samples, implying that a single sample can often represent a larger set of homogenized samples. Based on this insight, some works consider selecting representative samples to reduce the evaluation costs. (citation: Anchor Points) proposed clustering the entire benchmark directly, (citation: tinyBenchmark) trained IRT models to model the relationship between benchmark samples and target models. However, these methods often struggle with substantial distribution shifts between the source and target models, where source models are those used to construct the compressed benchmark, and target models are those being evaluated. For a set of source models , we construct an M-dimensional vector based on their answer results (e.g., correctness) on a given sample, thereby creating a benchmark representation space according to . In this space, the proximity of two samples indicates the correlation between them under the  metric. In Figure X, we illustrate the benchmark representation space measured by a set of source models (100 models) alongside the compressed set selected through clustering (using the Anchor Point method), and how this set is distributed within the representation spaces of the Llama-series and GPT-series models. This comparison highlights that a compressed set, while being the most representative under the full source model set, may not necessarily retain its representativeness within the representation spaces of other model series. Consequently, relying on the globally representative set for performance estimation across different model series can lead to suboptimal results.
