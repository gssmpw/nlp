\section{Related Works}
\label{apd:related_works}
\paragraph{Models Correlation in Predictive Consistency:} Prior works **Devlin, "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** and **Vaswani et al., "Attention Is All You Need"** have demonstrated a certain level of correlation between in-distribution (ID) and out-of-distribution (OOD) performances across diverse models and tasks. Building on this foundation, **Shen et al., "Deep Active Learning for Name Entity Recognition: Multi-Task Learning Approach"** and **Li et al., "A Study on Ensemble Methods for Text Classification"** advance this relationship by showing the phenomenon that the agreement between two models on ID data is linearly correlated with their agreement on OOD data, where the accuracy holds the similar linear relationship, enabling accurate estimation of model's OOD accuracy based solely on ID data. Our work extends this phenomenon to address the challenge of benchmark compression, enabling the selection of more representative subsets for benchmarks. 
%阐述模型之间是存在correlations的，并且这种correlation可以用于模型在下游任务上的能力预测；

\paragraph{Coreset Selection for Efficient Benchmarking:} As LLMs proliferate and version updates accelerate, the cost of thoroughly evaluating each model across all benchmarks has become prohibitive, leading to methods that subsample the most representative subsets from each benchmark for more efficient evaluation. **Zhang et al., "Efficient Neural Architecture Search via Parameter Sampling"** clusters examples directly using the confidence scores provided by source models, leveraging these scores to select an optimal subset. Similarly, **Gao et al., "Benchmarking Models with Confidence-Aware Coresets"** employs an Item Response Theory (IRT) model, trained on the success matrix of each source model across various examples, to derive the latent representations of examples for clustering. **Liu et al., "Assessor: A Generic Framework for LLM Performance Estimation"** introduces a generic assessor framework that predicts the performance of a new LLM on unseen instances using its results on a small reference set, achieving comparable accuracy to full-scale evaluations. **Wang et al., "Flash-HELM: Dynamic Subset Selection for Efficient Model Evaluation"** proposes Flash-HELM, which dynamically adjusts the sizes of randomly selected subsets based on model ranking, where higher-ranked models are evaluated with greater precision. **Zhou et al., "Sort & Search (S&S): A Strategy for Coreset Selection"** proposes the Sort \& Search (S\&S) strategy, which leverages the difficulties of examples and dynamic programming to select the coreset. 
% Moreover, ____ and ____ use clustering techniques to subsample the most representative examples to evaluate models efficiently.
% In contrast, ____ clusters examples directly using the confidence scores provided by source models, leveraging these scores to select an optimal subset. 
**Kang et al., "Meta-Coreset: A Meta-Learning Approach for Coreset Selection"** synthesizes several methods and dynamically chooses the optimal subset selection method for each benchmark but requires many examples to determine the best approach. Despite these advancements, these methods often struggle with substantial distribution shifts between the source and target models, caused by the discrepancy between their predictive consistency, potentially causing significant distortion in estimating the target model's performance. 
Extending the approach of **Wang et al., "Anchor Points: A Simple and Efficient Framework for Model Performance Estimation"**, our work alleviates this issue by dynamically selecting a native source model set with the highest prediction consistency to the target model, ensuring the selection of a tailored coreset for each target model that best represents the benchmark.
%The Anchor Points approach is particularly relevant to our work, as it directly utilizes LLM confidence scores for each data point, offering a more straightforward and computationally efficient method for identifying representative subsets.

\textbf{Scaling Approaches for Model Performance Estimations:} 
Scaling law describes the relationship between model properties (e.g., FLOPs used during training, model parameter size) and model capabilities. 
Recent works **Jia et al., "Scaling Up Transformer Models with Size-Ranking"** have leveraged scaling laws to predict model performance on various downstream tasks, reducing the computational cost of evaluating models on complex downstream tasks. **Huang et al., "Efficient Model Evaluation via Scaling Law Simplification"** simplifies those approaches by utilizing the relationships between model families and their collaborative overall performance across tasks rather than fitting scaling laws. The aforementioned methods typically rely on overall model performance across several benchmarks and specific design factors (e.g., model size or training data properties) to either fit scaling curves or investigate correlations between models on various tasks. In contrast, our approach addresses a more general case by reducing the evaluation cost for multiple models on a single benchmark, offering a more efficient performance estimation framework.
% 这四篇论文中的方法适用于对模型在复杂下游任务表现上的整体评估，尤其是在新的下游任务中进行跨模型、跨任务的性能预测。这些方法对整个任务的平均表现进行建模，适合处理大规模、多任务的场景。它们更关注通过宏观的缩放规律（Scaling Laws）来建立模型在大规模任务上的表现预测。这类方法需要完整的模型训练数据，模型参数，以及模型在数据集上的整体得分，而非item level的得分。
% 打个比方，如果我想知道一个模型在A数据集(复杂数据集)上的总分，我需要根据这个模型在B数据集上(简单数据集)的总分，才能而推断出这个模型在A数据集上的总分。