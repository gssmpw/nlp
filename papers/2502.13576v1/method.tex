\begin{figure*}[ht]
    \centering
    \includegraphics[scale=0.36]{figs/MethodFig.pdf}
    \vspace{-0.1cm}
    \caption{Overview of \textsc{TailoredBench}.}
    \label{FigMethod}
    \vspace{-0.4cm}
\end{figure*}
\section{TailoredBench Approach}

The \textsc{TailoredBench} approach centers on dynamically selecting prediction-consistent source models and crafting an N-set that faithfully represents the entire benchmark for each target model.
Its formulation proceeds through four tightly integrated steps: constructing a globally representative G-set (\S\ref{sec:G-set}), identifying native source models (\S\ref{sec:AdpSM}) and developing N-set for each target model (\S\ref{sec:N-set}), and finally estimating the target models' overall performance (\S\ref{sec:EsmPerform}).

\subsection{Task Set-Up}
Let $\mathcal{D} = \left\{(x_{k}, y_{k})\right\}_{k=1}^{|\mathcal{D}|}$ denotes a benchmark, where $x_{k}$ is the input and $y_{k}$ is the corresponding ground truth output. We define the set of target models under evaluation as $\mathcal{T} = \{ t_{m} \}_{m=1}^{|\mathcal{T}|}$. Additionally, we denote the source model set as $\mathcal{S} = \{ s_{n} \}_{n=1}^{|\mathcal{S}|}$, for which we have access to their predictions across all examples in $\mathcal{D}$. Following previous works, we ensure that $\mathcal{T} \cap \mathcal{S} = \varnothing$. Our objective is to accurately estimate the performance $P_{t_m}$ of each target model $t_m \in \mathcal{T}$ and determine the ranking relationships within $\mathcal{T}$, while minimizing the model inference cost.

\subsection{Constructing G-set}
\label{sec:G-set}
We first construct the G-set $\mathcal{G}$, which is designed as a probe for each target model to identify a set of source models with the highest prediction consistency. 
Consequently, it is intended to be a small yet relatively representative subset of the benchmark, ensuring its generalizability across target models. 

Following prior works \citep{AP}, we employ clustering based on the correctness of source models to construct the G-set. Here, correctness can be either the predictive probability of the correct option (continuous value [0, 1]) or whether the model answers the example correctly (discrete binary value \{0, 1\}).

For each example $x_k$ in the benchmark $\mathcal{D}$, we compute an embedding using the correctness scores $c_{s_n,x_k}$ from each source model $s_n$:
\begin{equation}
    \dot{x}_{k}^{\mathcal{S}} = \begin{pmatrix} c_{s_1, x_k} \\ c_{s_2, x_k} \\ \vdots \\ c_{s_{|\mathcal{S}|}, x_k} \end{pmatrix}
\end{equation}
The superscript $\mathcal{S}$ indicates that the embedding is derived from source models' correctness, and each embedding is $|\mathcal{S}|$-dimensional. The collection of these embeddings constitutes the benchmark's representation $\mathcal{D}^{\mathcal{S}} = \{\dot{x}_k^\mathcal{S}\}_{k=1}^{|\mathcal{D}|}$.

% Leveraging the correctness scores $c_{s_n, x_k}$ of each source model $s_n$ on the example $x_k$, we embed the benchmark $\mathcal{D}$ into $\mathcal{D}^{\mathcal{S}} = \{\dot{x}_{k}^{\mathcal{S}}\}_{k=1}^{|\mathcal{D}|}$ as follows:
% \begin{equation}
%     \dot{x}_{k}^{\mathcal{S}} = \begin{pmatrix} c_{s_1, x_k} \\ c_{s_2, x_k} \\ \vdots \\ c_{s_{|\mathcal{S}|}, x_k} \end{pmatrix}
% \end{equation}
% where $\dot{x}_{k}^{\mathcal{S}}$ denotes the embedding of $x_k$, derived from source models' correctness (as indicated by the superscript $\mathcal{S}$) with dimensionality equal to $|\mathcal{S}|$.

Based on $\mathcal{D}^{\mathcal{S}}$, we apply K-Medoids clustering \citep{pam} to select the G-set with the objective function below:
\begin{equation}
    \label{eq1}
    \min_{\{\mathcal{G}, \mathcal{C}_{g}\}} \sum_{x_{g} \in \mathcal{G}} \sum_{x_{k} \in \mathcal{C}_{g} \setminus \{x_{g}\}} \texttt{Dis} (\dot{x}_{g}^{\mathcal{S}}, \dot{x}_{k}^{\mathcal{S}})
\end{equation}
where $x_{g}$ is an example in the G-set $\mathcal{G} = \{x_{g}\}_{g=1}^{|\mathcal{G}|}$, and $\mathcal{C}_{g}$ is the cluster for which $x_{g}$ is the centroid. 
$\texttt{Dis}$ denotes the distance metric in clustering.

To maximize the generalization capability of our method, the choice of distance metric is critical. Previous approaches \citep{AP, miller2021accuracy, baek2022agreement, mehra2024predicting} using correlation distance \citep{ref_pearsonr} to measure example consistency often assume linear relationships in scoring patterns among models or examples. However, this assumption may not hold for discrete numerical embeddings, leading to significant performance degradation. In contrast, element-wise distance (e.g., Manhattan distance) can effectively capture individual discrepancies in correctness vectors, thereby accommodating various correctness formats. 
By default, we adopt manhattan distance as $\texttt{Dis}$ for our \textsc{TailoredBench} method. 

\subsection{Adaptive Native Source Model Selection}
\label{sec:AdpSM}
After constructing G-set $\mathcal{G}$, we attain the prediction results of target models $\mathcal{T}$ on it, which we use as a probe to construct a Native Source Model Set $\mathcal{S}_{t_m}$ that exhibits the highest prediction consistency for each $t_m \in \mathcal{T}$.

Specifically, we first embed all the source models $s_n \in \mathcal{S}$ and target models $t_m \in \mathcal{T}$ based on their prediction correctness on $\mathcal{G}$ as follows:
\begin{equation}
    \label{eq2}
    \begin{gathered}
    \dot{s}_n^{\mathcal{G}} = \begin{pmatrix} c_{s_n, x_1} \\ c_{s_n, x_2} \\ \vdots \\ c_{s_n, x_{|\mathcal{G}|}} \end{pmatrix}, \quad
    \dot{t}_m^{\mathcal{G}} = \begin{pmatrix} c_{t_m, x_1} \\ c_{t_m, x_2} \\ \vdots \\ c_{t_m, x_{|\mathcal{G}|}} \end{pmatrix}
    \end{gathered}
\end{equation}
Here, the superscript $\mathcal{G}$ denotes that each embedding dimension is derived from the model's prediction correctness on the G-set. Leveraging these embeddings, we compute the average prediction consistency $\bar{d}$ among all the models (both source and target) on the G-set as follows: 
\begin{equation}
    \begin{gathered}
    \bar{d} = \frac{2}{(|\mathcal{S}| + |\mathcal{T}|)(|\mathcal{S}| + |\mathcal{T}| - 1)} \sum_{i<j} d_{ij}, \\
    \text{where } d_{ij} = \texttt{Dis}\left(\dot{\phi}_i^{\mathcal{G}}, \dot{\phi}_j^{\mathcal{G}}\right)
    \end{gathered}
\end{equation}
In this context, $i, j \in [1, |\mathcal{S}| + |\mathcal{T}|]$ and $\phi$ represents any model from $\mathcal{S} \cup \mathcal{T}$. By computing $\bar{d}$ across all models, we establish a robust threshold that reflects the model set's similarity landscape, enabling a consistent and effective selection of native source models for each target model.

On this basis, we determine $\bar{n}$, the size of the native source model set for target models, by calculating the average number of source models whose prediction consistency with each target model exceeds the threshold $\bar{d}$ as follows:
\begin{equation}
    \begin{gathered}
    \bar{n} = \left\lfloor \frac{1}{|\mathcal{T}|} \sum_{m=1}^{|\mathcal{T}|} |\mathcal{S}_{t_m}| \right\rfloor \label{eq:bar_n}, \\
    \text{where } \mathcal{S}_{t_m} = \left\{ s_n \in \mathcal{S} \ \bigg| \ \texttt{Dis}\left(\dot{s}_n^{\mathcal{G}}, \dot{t}_m^{\mathcal{G}}\right) < \bar{d} \right\}   
    \end{gathered}
\end{equation}
For a target model $t_m$, the top $\bar{n}$ source models exhibiting the highest prediction consistency are selected to form its dynamic source model set $\mathcal{S}_{t_m}$. By standardizing the number of native source models across all target models, we ensure that each target model's feature representation maintains consistent dimensionality and informational richness during subsequent clustering.

\subsection{Developing N-set}
\label{sec:N-set}

Leveraging the selected native source models $\mathcal{S}_{t_m}$, we construct the most representative N-set $\mathcal{N}_{t_m}$ for each target model $t_m$. To maximize the utilization of the observed prediction results of target models on $\mathcal{G}$, we propose a \textsc{Scalable K-Medoids Clustering} algorithm to extend $\mathcal{G}$ into the N-set. 

Initially, each example $x_k \in \mathcal{D}$ is represented by a feature vector $\dot{x}_k^{\mathcal{S}_{t_m}}$, which is based on the correctness of its native source models $\mathcal{S}_{t_m}$. Then, our \textsc{Scalable K-Medoids Clustering} algorithm operates as follows:

\paragraph{Anchored Medoid Initialization:} We fix the examples in G-set $|\mathcal{G}|$ as initial medoids. To reach an N-set size of $|\mathcal{N}_{t_m}|$, we randomly add $|\mathcal{N}_{t_m}| - |\mathcal{G}|$ additional examples from $\mathcal{D} \setminus \mathcal{G}$ to form the initial medoid set.
% We fix the examples in G-set as initial medoids. If the desired N-set size is $|\mathcal{N}_{t_m}|$ and the G-set contains $|\mathcal{G}|$ examples, we randomly select $|\mathcal{N}_{t_m}| - |\mathcal{G}|$ additional examples from $\mathcal{D} \setminus \mathcal{G}$ to form the initial medoid set.

\paragraph{Cluster Assignment:} Assign each example $x_k \in \mathcal{D}$ to the nearest medoid $x_{\mu}$ to form the cluster $\mathcal{C}_{\mu}$:
\begin{equation}
    \begin{gathered}
    x_k \in \mathcal{C}_{\mu},\\ \text{ where } \mu=\arg\min_{\mu} \texttt{Dis} \left( x_{\mu}^{S_{t_m}}, x_k^{S_{t_m}}\right)
    \end{gathered}
\end{equation}
\paragraph{Dynamic Medoid Refinement:} For each cluster $\mathcal{C}_{\mu}$ with a non-G-set medoid, update the medoid $x_{\mu}$ by selecting the example within $\mathcal{C}_{\mu}$ that minimizes the total distance to all other examples in $\mathcal{C}_{\mu}$:
\begin{equation}x_{\mu}=\arg\min_{x_i\in\mathcal{C}_{\mu}}\sum_{x_j\in{\mathcal{C}_{\mu}}\setminus{\{x_i\}}}\texttt{Dis}\left(x_i^{\mathcal{S}_{t_m}}, x_j^{\mathcal{S}_{t_m}}\right)
\end{equation}
Medoids corresponding to G-set remain fixed during this process.

\paragraph{Convergence Verification:} Repeat the Cluster Assignment and Dynamic Medoid Refinement steps until convergence is achieved, i.e., when medoids no longer change or a maximum number of iterations is reached.

By incorporating the G-set examples as fixed medoids, the clustering process ensures that these pivotal examples guide the formation of clusters and the selection of additional N-set examples.

\subsection{Calibrated Performance Estimation}
\label{sec:EsmPerform}
After establishing the N-set $\mathcal{N}_{t_m}$ for a target model $t_{m}$, for previous methods \cite{AP,tiny}, they may estimate the model’s overall performance by first evaluating it on these centroid examples and then weighting the results according to each centroid’s coverage of the benchmark. However, simply relying on medoids overlooks subtle variations in how individual examples within each cluster are predicted, potentially leading to less accurate global estimates.

To address this, we leverage the prediction consistency between the target model $t_m$ and its native source models $\mathcal{S}_{t_m}$ to obtain the calibrated correctness estimates for the target model. For a given cluster with medoid $x$, consider any non-medoid example $x^{\prime}$ in the same cluster. We compute a scaling factor based on the native source models' average correctness, which reflects how the prediction patterns at $x^{\prime}$ relate to those at $x$:
\begin{equation}
    \mathrm{Scale}(x^{\prime})=\frac{\bar{c}_{\mathcal{S}_{t_m},x^{\prime}}+0.5}{\bar{c}_{\mathcal{S}_{t_m},x}+0.5}
\end{equation}
Here, $\bar{c}_{S_{t_m},x}$ and $\bar{c}_{S_{t_m},x^{\prime}}$ denote the average correctness of $\mathcal{S}_{t_m}$ on the medoid $x$ and the non-medoid $x^{\prime}$, respectively. The addition of 0.5 ensures numerical stability by preventing the denominator from becoming zero. Given that $t_m$ and $\mathcal{S}_{t_m}$ exhibit similar prediction consistencies, we assume this scaling factor can be applied to estimate the target model's correctness on $x^{\prime}{:}$
\begin{equation}
    c_{t_m,x^{\prime}}=(c_{t_m,x}+0.5)\cdot\mathrm{Scale}(x^{\prime})-0.5
\end{equation}
By integrating these inferred correctness values across all examples in the benchmark $\mathcal{D}$, we obtain a more faithful global performance estimation without re-evaluating the entire dataset:
\begin{equation}
    P_{t_m}=\frac1{|\mathcal{D}|}\sum_{x^{\prime}\in\mathcal{D}}c_{t_m,x^{\prime}}
\end{equation}
% The experiments in Appendix \ref{apd:ablation_calibration} demonstrate that by calibrating intra-cluster prediction variations using the consistent prediction patterns of native source models, the calibration process significantly enhances the accuracy of the target models' global performance estimation.

% This calibration process improves the accuracy of the target model's overall performance estimation by accounting for differences in predictions within each cluster, utilizing the consistent prediction behaviors of Native Source Models.



% xxxxxxxxxxxxxxxxxxxxxxx
% With the N-set $\mathcal{N}_{t_m}$ established for each target model $t_m$, we estimate its global benchmark performance by referencing the correctness patterns of its Native Source Models $\mathcal{S}_{t_m}$. For each example $x \in \mathcal{N}_{t_m}$, we begin by computing the Native Source Models’ average correctness:
% \begin{equation}
% \bar{c}_{\mathcal{S}_{t_m}, x} = \frac{1}{|\mathcal{S}_{t_m}|} \sum_{s_n \in \mathcal{S}_{t_m}} c_{s_n, x}
% \end{equation}
% To meaningfully incorporate the entire correctness distribution of each cluster rather than relying solely on its medoid, we introduce a scale factor that adjusts the Native Source Models’ average scores relative to the target model:
% \begin{equation}
% \text{Scale}(x) = \frac{c_{t_m, x} + 0.5}{\bar{c}_{\mathcal{S}_{t_m}, x} + 0.5}
% \end{equation}
% the offset here serves as a midpoint in the [0,1] range, providing a neutral baseline that stabilizes ratio calculations. This factor, computes for each cluster medoid $x$, is then applied to every example $x'$ within the corresponding cluster to produce a modified score:
% \begin{equation}
% \text{ModScore}(x') = \bigl(\bar{c}_{\mathcal{S}_{t_m}, x'} + 0.5\bigr)\cdot \text{Scale}(x) - 0.5
% \end{equation}
% Finally, we average the modified scores over the entire benchmark to obtain a resource-efficient yet reliable approximation of the target model’s overall performance:
% \begin{equation}
% P_{t_m} = \frac{1}{|\mathcal{D}|}\sum_{x' \in \mathcal{D}} \text{ModScore}(x'),
% \end{equation}
% This approach effectively translates localized corrections informed by the N-set into a faithful global estimate, significantly reducing computational overhead while maintaining high fidelity in performance estimation.
% xxxxxxxxxxxxxxxxxxxxxxx

% With the N-set $\mathcal{N}_{t_m}$ established for each target model $t_m$, we estimate its global benchmark performance by referencing the correctness patterns of its Native Source Models $\mathcal{S}_{t_m}$. For each example $x \in \mathcal{N}_{t_m}$, we begin by computing the Native Source Models’ average correctness:
% \begin{equation}
% \bar{c}_{\mathcal{S}_{t_m}, x} = \frac{1}{|\mathcal{S}_{t_m}|} \sum_{s_n \in \mathcal{S}_{t_m}} c_{s_n, x}.
% \end{equation}
% To meaningfully incorporate the entire correctness distribution of each cluster rather than relying solely on its medoid, we introduce a scale factor that adjusts the Native Source Models’ average scores relative to the target model:
% \begin{equation}
% \text{Scale}(x) = \frac{c_{t_m, x} + 0.5}{\bar{c}_{\mathcal{S}_{t_m}, x} + 0.5}.
% \end{equation}
% This factor, computed for each cluster medoid $x$, is then applied to every example $x'$ within the corresponding cluster to produce an adjusted score:
% \begin{equation}
% \text{AdjustedScore}(x') = \bigl(\bar{c}_{\mathcal{S}_{t_m}, x'} + 0.5\bigr)\cdot \text{Scale}(x) - 0.5.
% \end{equation}
% Finally, by averaging these adjusted scores over the entire benchmark,
% \begin{equation}
% P_{t_m} = \frac{1}{|\mathcal{D}|}\sum_{x' \in \mathcal{D}} \text{AdjustedScore}(x'),
% \end{equation}
% we obtain a resource-efficient yet reliable approximation of the target model’s overall performance. This approach effectively translates localized corrections informed by the N-set into a faithful global estimate, significantly reducing computational overhead while maintaining high fidelity in performance estimation.

% xxxxxxxxxxxxxxxx

% With the N-set $\mathcal{N}_{t_m}$ constructed for each target model $t_m$, we estimate its global benchmark performance by leveraging the correctness from the Native Source Models $\mathcal{S}_{t_m}$ as references. For each example $x \in \mathcal{N}_{t_m}$, we first compute the Native Source Models’ average correctness,
% \begin{equation}
% \bar{c}_{\mathcal{S}_{t_m}, x} = \frac{1}{|\mathcal{S}_{t_m}|} \sum_{s_n \in \mathcal{S}_{t_m}} c_{s_n, x},
% \end{equation}
% To incorporate the entire cluster’s correctness distribution, we define a scale factor to rectify the correctness from the Native Source Models for prediction:
% \begin{equation}
% \text{Scale}(x) = \frac{c_{t_m, x} + 0.5}{\bar{c}_{\mathcal{S}_{t_m}, x} + 0.5}.
% \end{equation}
% This scale factor is computed for each cluster medoid $x$ in the N-set. We then propagate it to all examples $y$ belonging to the cluster represented by $x$, adjusting their scores by:
% \begin{equation}
% \text{AdjustedScore}(y) = \bigl(\bar{c}_{\mathcal{S}_{t_m}, y} + 0.5\bigr)\cdot \text{Scale}(x) - 0.5.
% \end{equation}
% Finally, aggregating these adjusted scores across the entire benchmark,
% \begin{equation}
% P_{t_m} = \frac{1}{|\mathcal{D}|}\sum_{y \in \mathcal{D}} \text{AdjustedScore}(y),
% \end{equation}
% The method yields a resource-efficient yet reliable approximation of $t_m$’s overall performance on the full dataset. This approach seamlessly transforms local corrections informed by the N-set into a faithful global performance estimate, all while significantly reducing computational overhead.

% xxxxxxxxxxxxxxxxxx

% %------------------------------------
% After constructing the N-set $\mathcal{N}_{t_m}$ for a target model $t_m$, we evaluate $t_m$ on the examples in $\mathcal{N}_{t_m} \setminus \mathcal{G}$, as $t_m$ has already been assessed on the G-set $\mathcal{G}$. A straightforward method to estimate the overall performance of $t_m$ is to compute its accuracy on the N-set. However, this approach may overlook the varying sizes of the clusters that each N-set example represents.

% To address this limitation, we follow previous methodologies and estimate the performance $\hat{P}_{t_m}$ of $t_m$ on the entire benchmark $\mathcal{D}$ by weighting the accuracy on the N-set according to the proportion of examples each cluster encompasses:

% \begin{equation}
%     \hat{P}_{t_m}=\sum_{{\mu}=1}^{|\mathcal{N}|}\left(\frac{|\mathcal{C}_\mu|}{|\mathcal{D}|}\cdot c_{t_m , x_{\mu}}\right)
% \end{equation}

% In this manner, the cluster size bias is effectively calibrated, and $\hat{P}_{t_m}$ serves as an unbiased estimate of the target model $t_m$'s performance across the entire benchmark.


% where $c_{t_m , x_{\mu}}$ is the correctness of $t_m$ on the medoid $x_{\mu}$.
% \begin{equation}
%     \min_{\mathcal{M}} \sum_{k=1}^{|\mathcal{N}|} \sum_{x_i \in \mathcal{C}_k} \texttt{EleDis}\left(x_i^{\mathcal{S}_{t_m}}, x_{\mu_k}^{\mathcal{S}_{t_m}}\right) + \sum_{g_j \in \mathcal{G}} \texttt{EleDis}\left(g_j^{\mathcal{S}_{t_m}}, x_{\mu_k}^{\mathcal{S}_{t_m}\right),
% \end{equation}
% \subsection{The Element-Wise Distance} 
% \label{sec}
% Previous work employs correlation distance to estimate the consistency between examples, operating under the assumption that scoring patterns between models or examples exhibit linear trends. However, this approach can encounter limitations, particularly when attempting to establish linear relationships between discrete numerical embeddings across different models, often resulting in a significant decline in performance. In contrast, the \textsc{TailoredBench} utilizes element-wise distance metrics, such as Manhattan distance and cosine similarity, to effectively capture individual discrepancies in correctness vectors without relying on a strict linear relationship. Our experiments in Table \ref{tab:distance} demonstrate that element-wise metrics outperform correlation distance in constructing representative G-sets and N-sets, as well as in selecting Native Source Models. Consequently, we use Manhattan distance as the primary $\texttt{EleDis}$ metric in the \textsc{TailoredBench} method.

% In the TailoredBench framework, we employ element-wise distance metrics, such as the Manhattan distance and cosine similarity, effectively capture individual discrepancies in correctness vectors without assuming a strict linear relationship. This challenge especially arises from the difficulty in establishing linear relationships between two items embedding in discrete numbers across different models, which can lead to a significant drop in performance. Our experiments (see Table \ref{tab:distance}) indicate that the Element-wise outperform the correlation distance in constructing representative G-sets and N-sets, as well as in selecting Native Source Models. Therefore, we adopt the Manhattan distance as our primary $\texttt{EleDis}$ in the TailoredBench method.
