% \section{Limitations}
% % limitations一定是futurework吗？
% A primary limitation of mainstream approaches in benchmark compression, including \cite{AP}, \cite{tiny}, and our method, is their dependence on comprehensive evaluation results from existing models across all examples within a benchmark. As described above, these results are typically readily accessible through public leaderboards. However, for new benchmarks or certain private benchmarks, obtaining initial model performance results is necessary, which introduces additional inference overhead. Nonetheless, we maintain that this initial cost is justified, as it is offset by the significant resource savings achieved through numerous subsequent rapid evaluations facilitated by our method.

% One of the primary limitations of our approach is its reliance on existing evaluation data from pre-trained models. Specifically, when applied to entirely new benchmarks or proprietary benchmarks, our method requires initial acquisition of model performance results on these benchmarks, which introduces additional inference overhead. However, we maintain that this initial cost is justified, as it is offset by the significant resource savings enabled by numerous subsequent rapid evaluations facilitated by our method.

\section{Conclusions}
In this paper, we propose the \textsc{TailoredBench} method, which mainly includes an adaptive source model set selection strategy, a scalable K-Medoids clustering algorithm and a calibrated performance estimation strategy. 
%摒弃ONE-SIZE-FITS-ALL的思想，我们在构建的native coreset上为每个模型进行了定制化的evaluation
Abandoning the one-size-fits-all approach, we have customized the evaluation on the constructed native coreset for each target model.
% By leveraging the prediction results of existing models on benchmarks, we construct a Global-set as a probe that adaptively selects a Native-set for each target model.
This approach enables a more accurate reconstruction and ranking of the model's performance on the entire benchmark with a small-size inference budget. 
% \textsc{TAILOREDBENCH} significantly reduces the resources required for evaluation.
Comprehensive experiments show that \textsc{TailoredBench} can achieve more accurate model evaluation (an average of 31.4\% estimation MAE loss degradation) with few inference costs.

\section*{Limitations} A primary limitation of mainstream approaches in benchmark compression, including \citep{AP, tiny}, and our method, is their dependence on comprehensive evaluation results from existing models across all examples within a benchmark. As described above, these results are typically readily accessible through public leaderboards. 
However, obtaining initial model performance results is necessary for new or certain private benchmarks, which introduces additional inference overhead. Nonetheless, we maintain that this initial cost is justified, as it is offset by the significant resource savings achieved through numerous subsequent rapid evaluations facilitated by our method.

\section*{Ethics Statement}
All of the datasets used in this study were publicly available, and no annotators were employed for our data collection. We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We have cited the datasets and relevant works used in this study.
% \paragraph{Future Work:} An important direction for future work involves the dynamic determination of the optimal sizes for the G-set and N-set. While we have demonstrated that our method performs robustly across a variety of G-set and N-set configurations, the ability to ascertain the most suitable sizes for a specific benchmark and its corresponding source models would be highly beneficial. This advancement would enable users to better plan their computational resources in advance, optimizing the balance between inference cost and performance.
