\begin{abstract}
%在一个大benchmark上评测模型通常很resource-intensive，特别是身处一个模型快速迭代的时期。
%评测模型所需的开销
Evaluating models on large benchmarks is very resource-intensive, especially during the  period of rapid model evolution.
Existing efficient evaluation methods estimate the performance of target models by testing them only on a small and static coreset of the benchmark, which is derived from the publicly available evaluation results of source models.
These methods rely on the assumption that target models have high prediction consistency with source models. However, we demonstrate that it doesn’t generalize well in practice.
To alleviate the inconsistency issue, we present \textsc{TailoredBench}, a method that conducts customized evaluation tailored to each target model. 
Specifically, a Global-coreset is first constructed as a probe to identify the most consistent source models for each target model with an adaptive source model selection strategy. 
Afterwards, a scalable K-Medoids clustering algorithm is proposed to extend the Global-coreset to a tailored Native-coreset for each target model. 
According to the predictions on Native-coresets, we obtain the performance of target models on the whole benchmark with a calibrated estimation strategy. 
Comprehensive experiments on 5 benchmarks across over 300 models demonstrate that compared to best performing baselines, \textsc{TailoredBench} achieves an average reduction of 31.4\% in MAE of accuracy estimates under the same inference budgets, showcasing strong effectiveness and generalizability. 
% In practice, \textsc{TailoredBench} can accurately replicate the performance of the target model on 6000 data points of Hellaswag using only 20 data points, largely.
\end{abstract}