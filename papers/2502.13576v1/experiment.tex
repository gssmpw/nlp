% The initial cost of evaluating source models is justified by the significant reduction in development-time evaluations, which can instead be performed on the representative N-set we extract.
\section{Experiments}
\subsection{Experimental Setup}
\label{sec:exp-3.1}
\begin{table*}[ht]
\renewcommand\arraystretch{1.2}
\centering
\small
\setlength{\tabcolsep}{0.49em} 
\begin{tabular}{cl*{5}{cc}}
\toprule
\multirow{2}{*}{\textbf{Benchmarks}} & \multirow{2}{*}{\textbf{Inference counts}} & \multicolumn{2}{c}{20} & \multicolumn{2}{c}{25} & \multicolumn{2}{c}{30} & \multicolumn{2}{c}{35} & \multicolumn{2}{c}{40} \\
&  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\
\midrule
\multirow{2}{*}{ARC Challenge}&\textsc{Best Baseline} & 0.662 & 0.046 & 0.663 & 0.046 & 0.676 & 0.036 & 0.713 & 0.036 & 0.714 & 0.029 \\
                           & \textsc{TailoredBench} & \textbf{0.711} & \textbf{0.031} & \textbf{0.737} & \textbf{0.029} & \textbf{0.756} & \textbf{0.028} & \textbf{0.766} & \textbf{0.027} & \textbf{0.773} & \textbf{0.027} \\
\hdashline
\multirow{2}{*}{Hellaswag}&\textsc{Best Baseline} & 0.860 & 0.060 & 0.880 & 0.053 & 0.877 & 0.043 & 0.897 & 0.038 & 0.898 & 0.032 \\
                           & \textsc{TailoredBench} & \textbf{0.900} & \textbf{0.020} & \textbf{0.909} & \textbf{0.018} & \textbf{0.913} & \textbf{0.018} & \textbf{0.914} & \textbf{0.017} & \textbf{0.918} & \textbf{0.017} \\
\hdashline
\multirow{2}{*}{GSM8K}&\textsc{Best Baseline} & 0.811 & 0.055 & 0.828 & 0.047 & 0.839 & 0.041 & 0.847 & 0.038 & 0.858 & 0.034 \\
                           &\textsc{TailoredBench} & \textbf{0.852} & \textbf{0.035} & \textbf{0.858} & \textbf{0.034} & \textbf{0.863} & \textbf{0.033} & \textbf{0.869} & \textbf{0.031} & \textbf{0.878} & \textbf{0.029} \\
\hdashline
\multirow{2}{*}{Winogrande}&\textsc{Best Baseline} & 0.472 & 0.041 & 0.487 & 0.038 & 0.514 & 0.038 & 0.521 & 0.036 & 0.518 & 0.034 \\
                           & \textsc{TailoredBench} & \textbf{0.565} & \textbf{0.028} & \textbf{0.568} & \textbf{0.026} & \textbf{0.604} & \textbf{0.024} & \textbf{0.608} & \textbf{0.023} & \textbf{0.618} & \textbf{0.022} \\
\hdashline
\multirow{2}{*}{POPE}&\textsc{Best Baseline} & 0.488 & 0.038 & 0.510 & 0.037 & 0.518 & 0.034 & 0.547 & 0.033 & 0.556 & \textbf{0.031} \\
                           &\textsc{TailoredBench} & \textbf{0.521} & \textbf{0.036} & \textbf{0.547} & \textbf{0.035} & \textbf{0.562} & \textbf{0.031} & \textbf{0.563} & \textbf{0.031} & \textbf{0.574} & 0.032 \\
\bottomrule
\end{tabular}
\caption{Results on all benchmarks. For each setting, we take the best result from multiple baselines to compare with \textsc{TailoredBench}. The detailed performance of each baseline under each setting is presented in Table~\ref{tab:apdallresult}. Values in bold represent the best results.}
\label{tab:allresult}
\vspace{-0.4cm}
\end{table*}


% \begin{figure*}[ht]
%     \centering
%     \begin{subfigure}[b]{0.49\textwidth} % 修改这里从 \columnwidth 到 0.49\textwidth
%         \centering % 添加 centering 以确保图片居中
%         \includegraphics[width=\linewidth]{figs/modelratio/GSM8K_modelratio_kendall.pdf}
%         \caption{The impact of the quantity of Native Source Models (with prediction consistency kept the same).}
%         % \caption{Independent Analysis of Native Source Model Quantity on GSM8K Benchmark.}
%         \label{fig:GSM8K_modelratio_kendall}
%     \end{subfigure}
%     \hfill % 这是为了在两个子图之间增加空白
%     \begin{subfigure}[b]{0.49\textwidth} % 同样修改宽度
%         \centering % 同理添加 centering
%         \includegraphics[width=\linewidth]{figs/modelsimilarity/GSM8K_similarity_kendall.pdf}
%         % \caption{Independent Analysis of Native Source Model–Target Model Prediction Consistency on GSM8K Benchmark.}
%         \caption{The impact of prediction consistency between the Native Source Model and Target Model (with quantity kept the same).}
%         \label{fig:GSM8K_similarity_kendall}
%     \end{subfigure}
%     \caption{Effects of Native Source Model Quantity and Prediction Consistency on GSM8K Performance.}
%     \label{fig:GSM8K_combined}
%     \vspace{-0.5cm}
% \end{figure*}


% To evaluate the effectiveness and generalizability of \textsc{TailoredBench}, we conduct experiments on five diverse benchmarks, involving tasks in the fields of natural language and multi-modality:
% \begin{itemize}
% \item \textbf{ARC Challenge \citep{clark2018arc}: }Contains 1,172 grade-school science questions requiring scientific reasoning. We collect the outputs of 153 models on this benchmark. 
% \item \textbf{Hellaswag \citep{zellers2019hellaswag}: }Contains over 10,000 validation examples for commonsense inference. For computational efficiency, we use the first 6,000 examples and gather the outputs of 139 models on this subset. 
% \item \textbf{GSM8K \citep{cobbe2021GSM8K}: }Contains 1,319 grade-school math problems designed to test mathematical reasoning. We summarize the evaluation results of 150 models on this benchmark. 
% \item \textbf{Winogrande \citep{sakaguchi2021winogrande}: }Contains 1,267 examples presenting challenging pronoun resolution tasks in commonsense reasoning. We summarize the evaluation results of 150 models on this dataset. 
% \item \textbf{POPE \citep{li2023pope}: }Contains 5,127 examples to evaluate hallucination in multimodal models. We summarize the evaluation results of 99 models on this benchmark.
% \end{itemize}

% Specifically, For \textit{ARC Challenge} and \textit{Hellaswag} benchmark, we use continuous values to represent the probability of the model correctly predicting the answer. For \textit{GSM8K}, \textit{Winogrande}, and \textit{POPE} benchmark, we use discrete values in \{0, 1\} as the correctness, where $1$ indicates a correct prediction and $0$ indicates an incorrect prediction.

% The model prediction results for ARC Challenge, Hellaswag, GSM8K, and Winogrande are sourced from the Open LLM Leaderboard \citep{open-llm-leaderboard}, while those for POPE come from the OpenCompass Leaderboard \citep{2023opencompass}. For each benchmark, we randomly partition the models into source and target model sets, ensuring that their intersection is empty. The models used in each benchmark are listed in the appendix.
\paragraph{Benchmarks and Models} 
We validate \textsc{TailoredBench} on five diverse benchmarks spanning natural language and multimodal tasks. ARC Challenge \citep{clark2018arc} consists of 1,172 scientific reasoning questions, with predictions from 153 models. Hellaswag \citep{zellers2019hellaswag} provides 6,000 commonsense inference examples (a subset of its validation set) and outputs from 139 models. GSM8K \citep{cobbe2021GSM8K} includes 1,319 math reasoning problems tested on 150 models. Winogrande \citep{sakaguchi2021winogrande} has 1,267 pronoun resolution examples with 150 models evaluated. POPE \citep{li2023pope} features 5,127 instances for assessing multimodal hallucination, accompanied by results from 99 models. A complete list of models used for each benchmark is provided in Appendix \ref{apd:modellist}. We randomly split models into source and target sets for each benchmark, ensuring that their intersection is empty. 

For ARC Challenge and Hellaswag, model correctness is represented by continuous probabilities, while GSM8K, Winogrande, and POPE use binary correctness \{0, 1\}. Predictions for ARC Challenge, Hellaswag, GSM8K, and Winogrande come from the Open LLM Leaderboard \citep{open-llm-leaderboard}, and those for POPE are from the OpenCompass Leaderboard \citep{2023opencompass}. 


\paragraph{Baseline and Evaluation Metrics} We compare \textsc{TailoredBench} against three baselines: a \textit{Random Sampling} strategy that randomly selects a subset of examples from the benchmark to estimate model performance, serving as a basic reference point; the \textit{Anchor Points} method \citep{AP}, which uses K-Medoids clustering on source-model predictions to identify a fixed representative coreset; and \textit{gp-IRT} \citep{tiny}, which employs an Item Response Theory model trained on the predictions of the source models to estimate target models' performance on the full benchmark. In all cases, we use the same source models and target models to ensure a fair comparison.

% The \textit{Random Sampling} randomly selects a subset of examples from the benchmark to estimate model performance, serving as a basic reference point. \textit{Anchor Points} uses K-Medoids to select a fixed set of representative examples based on the source models to estimate the performance of target models. \textit{gp-IRT} method employs an Item Response Theory model trained on the predictions of the source models to estimate target models' performance on the full benchmark. Similar to our approach, we train the gp-IRT model using the predictions of the same set of source models and estimate the performance of the same set of target models. 

We employ two metrics to assess these methods. \textit{Kendall's $\tau$ Correlation Coefficient} evaluates the ordinal agreement between estimated and true model rankings, indicating how well the relative performance order is preserved. \textit{Mean Absolute Error (MAE)} measures the average absolute deviation between estimated and true performance scores, thereby capturing the precision of performance estimation for individual target models.

\begin{figure*}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth} % 修改这里从 \columnwidth 到 0.49\textwidth
        \centering % 添加 centering 以确保图片居中
        \includegraphics[width=\linewidth]{figs/modelratio/GSM8K_modelratio_kendall.pdf}
        \caption{The impact of the quantity of Native Source Models (with prediction consistency kept the same).}
        % \caption{Independent Analysis of Native Source Model Quantity on GSM8K Benchmark.}
        \label{fig:GSM8K_modelratio_kendall}
    \end{subfigure}
    \hfill % 这是为了在两个子图之间增加空白
    \begin{subfigure}[b]{0.49\textwidth} % 同样修改宽度
        \centering % 同理添加 centering
        \includegraphics[width=\linewidth]{figs/modelsimilarity/GSM8K_similarity_kendall.pdf}
        % \caption{Independent Analysis of Native Source Model–Target Model Prediction Consistency on GSM8K Benchmark.}
        \caption{The impact of prediction consistency between the Native Source Model and Target Model (with quantity kept the same).}
        \label{fig:GSM8K_similarity_kendall}
    \end{subfigure}
    \caption{Investigating the impact of native source model quantity and prediction consistency with target model on GSM8K using the controlled variable method.}
    \label{fig:GSM8K_combined}
    \vspace{-0.4cm}
\end{figure*}

% \paragraph{Baseline and Evaluation Metrics} We compare our \textsc{TailoredBench} method against three baselines: 
% \begin{itemize}
% \item \textbf{Random Sampling: }The Random Sampling strategy selects a subset of examples from the benchmark to estimate model performance, serving as a basic reference.

% \item \textbf{Anchor Points \citep{AP}: }Anchor Points method uses K-Medoids to select a fixed set of representative examples based on the source models to estimate the performance of target models.

% \item \textbf{gp-IRT \citep{tiny}: }gp-IRT method employs an Item Response Theory model trained on the predictions of the source models to estimate target models' performance on the full benchmark. We train the gp-IRT model using the predictions from the same source models and estimate the performance of the same target models.
% \end{itemize}

% We evaluate the performance of these methods with two metrics: 
% \begin{itemize}
% \item \textbf{Kendall's $\tau$ Correlation Coefficient: }Kendall's $\tau$ measures the ordinal association between the estimated and true model rankings, effectively capturing how well our method preserves the relative performance order of the target models.

% \item \textbf{Mean Absolute Error (MAE): }MAE quantifies the average absolute difference between the estimated and true performance scores, assessing the precision of our method in estimating individual model performances. 
% \end{itemize}



\subsection{Main Results}
\paragraph{TailoredBench: Effective Ranking and Estimation of Model Performances} Tables~\ref{tab:allresult} present a comprehensive comparison between our \textsc{TailoredBench} method and the best baseline approaches for each metric across all benchmarks. Full results are available in Appendix \ref{apd:all_result}. In our experiments, we allocated 10 examples to the G-set and averaged the outcomes over 100 randomized trials to ensure statistical reliability. The inference count—defined as the number of examples in the N-set for our method—varied from 20 to 40.

% \paragraph{TailoredBench: Effective Ranking and Estimation of Model Performances.} Tables~\ref{tab:allresult} present a comprehensive comparison between our \textsc{TailoredBench} method and best baseline approaches on each metric across all benchmarks. (Full results are listed in Appendix \ref{apd:all_result}.) In our experiments, we allocated 10 examples to the G-set and averaged the outcomes over 100 randomized trials to ensure statistical reliability. The inference count—defined as the number of examples in the N-set for our method—is varied from 20 to 40. 

% 如表中展示的，对所有的inference count，我们的方法在Kendall's $\tau$和MAE metric上都持续地优于baseline methods。并且随着infercount逐渐增加，我们的方法的效果也越来越好，具体表现为Kendall's $\tau$持续上升，且MAE指标持续下降。特别地，与静态的AnchorPoints方法相比，我们的MAE指标几乎下降了一半。这些结果说明了我们的方法能够很好地估计目标模型之间的能力相对关系，并更为精准地估计它们在benchmark全集上的表现。

As demonstrated in the tables, our method consistently outperforms baseline approaches in both Kendall's $\tau$ and MAE metrics across all inference counts and benchmarks featuring different correctness types. When the inference count increases, the performance of our method continues to improve, evidenced by a steady increase in Kendall's $\tau$ and a continuous decrease in MAE. Notably, compared to best performing baselines, our approach achieves nearly a 31.4\% reduction in MAE. These results indicate that our method effectively estimates the relative performance among target models and provides more accurate estimations of their performance on the entire benchmark. Furthermore, compared to the static AnchorPoints method, our approach significantly improves both Kendall's $\tau$ and MAE metrics, highlighting its effectiveness in adaptively selecting a more representative N-set for each target model and thereby improving estimation accuracy. We also calculate the accuracy of our method in ranking the performance between every pair of target models. The results show that the accuracy reached 96.0\% on the Hellaswag benchmark and 93.6\% on the GSM8K benchmark. In terms of robustness, Appendix \ref{apd:variance} demonstrates that our method exhibits significantly lower variance compared to the baselines. 

Moreover, across all benchmarks and inference counts, we conduct a one-sided Z-test over 100 repeated experiments. Whenever our method outperformed the baselines, the p-values remained below 0.05, confirming a statistical advantage.
% Moreover, we conducted a one-sided Z-test across all benchmarks based on the results of 100 repeated experiments comparing our method to the baselines. Specifically, when our method outperformed the others, the p-values consistently fell below 0.05, indicating its statistically significant advantage.

% \begin{figure}[ht]
%     \centering
%     \begin{subfigure}[b]{\columnwidth}
%         \includegraphics[width=0.95\textwidth]{figs/modelnum/GSM8K_modelnum.pdf}
%         \caption{Analyses of Native Source Model Quantity on GSM8K Benchmark.}
%         \label{fig:GSM8K_modelnum}
%     \end{subfigure}
    
%     \begin{subfigure}[b]{\columnwidth}
%         \includegraphics[width=0.95\textwidth]{figs/modelsimilarity/GSM8K_similarity_kendall.pdf}
%         \caption{Analyses of Native Source Model Prediction Consistency on GSM8K Benchmark.}
%         \label{fig:GSM8K_similarity_kendall}
%     \end{subfigure}
%     \caption{Analyses of Native Source Model Selection on Method Performance.}
%     \label{fig:GSM8K_combined}
%     \vspace{-0.3cm}
% \end{figure}
% Moreover, results marked with an ${*}$ in our tables denote instances where a one-sided Z-test is conducted on the outcomes of 100 repeated experiments using our method versus 100 experiments with the baseline method. In these cases, the p-value for the hypothesis that our method's results are greater than those of the baseline exceeds 0.05. This indicates that, in these instances, we cannot statistically confirm that our method outperforms the baseline.



\subsection{Ablation Studies}
\paragraph{Element-Wise Distance Effectively Facilitates Handling Various Data Forms} 
Our method uses element-wise Distance (specifically Manhattan distance) to effectively handle both continuous and discrete values. As shown in Table \ref{tab:distance}, with 30 inference counts, element-wise Distances outperform the correlation distance used by AnchorPoints. This confirms its effectiveness in improving our method’s performance. Detailed per-dataset results are provided in Appendix \ref{apd:distance}.
\begin{table}[htbp]
\renewcommand\arraystretch{1}
\centering
% \small
\setlength{\tabcolsep}{1.25em} 
\begin{tabular}{l cc}
\toprule
\textbf{Distance}& \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
\midrule
\textsc{Correlation}    & 0.720 & 0.032 \\
\textsc{Cosine}         & \uline{0.736} & \uline{0.028}  \\
\textsc{Manhattan}      & \textbf{0.740} & \textbf{0.027} \\
\bottomrule
\end{tabular}
\caption{Average performance with different types of distance across benchmarks.}
\label{tab:distance}
\vspace{-0.5cm}
\end{table}
% Our method employs Element-Wise Distance (specifically Manhattan distance), enabling effective handling of both continuous and discrete values. In contrast, the AnchorPoints method, which uses correlation distance, performs significantly worse on the all benchmarks, as shown in Table \ref{tab:allresult}. Further experiments in Table \ref{tab:distance} under 30 inference counts demonstrate that Element-Wise Distance, including both cosine similarity and Manhattan distance, consistently outperforms correlation distance used by AnchorPoints method across all benchmarks. This confirms the effectiveness of Element-Wise Distance in improving our method's performance. 


% \begin{table*}
% \renewcommand\arraystretch{1}
% \centering
% % \small
% \setlength{\tabcolsep}{0.3em} 
% \begin{tabular}{l *{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{Distance}} & \multicolumn{2}{c}{ARC Challenge} & \multicolumn{2}{c}{Hellaswag} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Winogrande} & \multicolumn{2}{c}{POPE} \\ 
% \noalign{\vskip -0.17em}
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
% \midrule
% \textsc{Correlation}    & \textbf{0.766} & 0.033 & 0.903 & \uline{0.019} & \uline{0.828} & 0.041 & 0.557 & 0.029 & 0.547 & 0.038 \\
% \textsc{Cosine}         & 0.746 & \uline{0.031} & \textbf{0.914} & \uline{0.019} & 0.827 & \uline{0.040} & \textbf{0.616} & \textbf{0.024} & \textbf{0.577} & \textbf{0.024} \\
% \textsc{Manhattan}      & \uline{0.756} & \textbf{0.028} & \uline{0.913} & \textbf{0.018} & \textbf{0.863} & \textbf{0.033} & \uline{0.604} & \uline{0.024} & \uline{0.562} & \uline{0.031} \\
% \bottomrule
% \end{tabular}
% \caption{Performance Evaluation Across Different Distance Measures.}
% \label{tab:distance}
% % \vspace{-0.5cm}
% \end{table*}

\paragraph{Calibrated Estimation Strategy Improves Performance Estimation} 
% Here we present the averaged results of our ablation study comparing \textsc{TailoredBench} with and without calibration. As shown in Table \ref{tab:calibrate}, the calibrated version achieves higher Kendall’s $\tau$ and lower MAE, indicating that the calibrated performance estimation process enhances the accuracy of performance estimation. Detailed per-dataset results are provided in Appendix \ref{apd:ablation_calibration}.
We compare \textsc{TailoredBench} with and without calibration. As shown in Table \ref{tab:calibrate}, with 30 inference counts, the calibrated variant achieves higher Kendall’s $\tau$ and lower MAE, confirming that calibration enhances the accuracy of performance estimation. Detailed per-dataset results are provided in Appendix \ref{apd:ablation_calibration}.

\paragraph{More Ablation Studies} 
We conduct additional ablation studies on our method in Appendices \ref{apd:dynamic-native-source-models} and \ref{apd:not-fix-gset}. The results show that (1) \textbf{using a fixed number of native source models for each target model stabilizes performance}; (2) \textbf{fixing the G-set as part of the N-set strikes a balance between effectiveness and the inference budget}.

\subsection{Analyses}
\paragraph{Impact of Native Source Model Selection on Method Performance} Here, we isolate the effects of both the number of native source models and their prediction consistency with the target model by independently varying these factors.
% , without applying our adaptive native source model selection mechanism.

% As shown in Figure \ref{fig:GSM8K_modelratio_kendall}
\textit{When native source models share a fixed level of prediction consistency with the target model, increasing their number enhances performance.} To investigate this, we randomly select models designated as native source models, from 20\% to 100\%. As shown in Figure \ref{fig:GSM8K_modelratio_kendall}, performance improves as more native source models are included, since a larger set of models offers a greater chance of obtaining a more robust embedding. See Appendix \ref{apd:quantity} for results on more benchmarks.

% We maintain consistent overall prediction accuracy between the native source models and the target model, while varying the proportion of randomly select models designated as native source models, from 20\% to 100\%. The results clearly demonstrate that the performance of our method improves as the number of native source models increases. This is because a larger set of models offers a greater chance of obtaining a more robust embedding. See Appendix \ref{apd:quantity} for results on more benchmarks.
\textit{When the number of native source models is fixed, higher prediction consistency with the target model enhances performance.} To examine this, we select a fixed number of native source models at various consistency levels relative to the target model (top 20\%, 20\textasciitilde 40\%, up to 80\textasciitilde 100\%). 
\begin{table}[htbp]
\renewcommand\arraystretch{1}
\centering
\setlength{\tabcolsep}{1em} 
\begin{tabular}{l cc}
\toprule
\textbf{Method Variants}& \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
\midrule
\textsc{Non-Calibrated}    & 0.724 & 0.030 \\
\textsc{Calibrated}         & \textbf{0.740} & \textbf{0.027}  \\
\bottomrule
\end{tabular}
\caption{Average performance with and without calibration across benchmarks.}
\label{tab:calibrate}
\vspace{-0.5cm}
\end{table}
As shown in Figure \ref{fig:GSM8K_similarity_kendall} (with the horizontal axis representing the Consistency Percentile Range for these intervals), Kendall’s $\tau$ decreases sharply as the consistency percentile range expands. See Appendix \ref{apd:similarity} for additional benchmark results.

% Figure \ref{fig:GSM8K_similarity_kendall} illustrates this by showing our method's performance with a fixed number of native source models select based on their prediction consistency rankings (top 20\%, 20\textasciitilde 40\%, up to 80\textasciitilde 100\%) relative to the target model. The horizontal axis represents the Consistency Percentile Range, corresponding to these intervals. As shown, Kendall's $\tau$ for our method drops sharply as the Consistency Percentile Range increases. See Appendix \ref{apd:similarity} for additional benchmark results. 

% Detailed analysis of the impact of native source model quantity is provided in Appendix \ref{apd:quantity}.
% Further analysis on the impact of prediction consistency is discussed in Appendix \ref{apd:similarity}.

% In the left panel of Figure \ref{fig:GSM8K_combined}, we maintain the overall prediction consistency between the Native source models and the target models constant, while varying the proportion of the source models designated as Native Source Models from 20\% to 100\% for the target models. It is evident that when the inference count ranges from 20 to 40, the performance of our method progressively improves with the increase in the number of Native Source Models. This is because, as the feature length increases, there is a greater possibility of reducing the noise impact caused by the prediction inconsistency between the source and target models.

% Dually, in the right panel of Figure \ref{fig:GSM8K_combined}, we demonstrate the performance of our method with a fixed number of Native Source Models, selecting those whose prediction consistency is ranked in the top 20\%, 20\%\textasciitilde 40\%, up to 80\%\textasciitilde 100\% to the target model. The horizontal axis in this figure represents the Top Consistency Percentile, corresponding to these prediction consistency ranking ranges. It can be observed that across all inference counts, the performance decreases as the prediction consistency between the Native Source Models and the target model diminishes. Additional results on more benchmarks are provided in Appendix \ref{apd:quantity} and \ref{apd:similarity}.

% In summary, both the number of native source models and their prediction consistency with the target model significantly impact the performance of the method. This underscores the necessity of selecting source models that exhibit the highest prediction consistency with the target model while balancing the effects of the number of select source models. Subsequently, we will demonstrate that \textsc{TailoredBench} can adaptively select an optimal set of Native Source Models for each target model, achieving near-optimal results.



% From the experimental results, we also observe that the gp-IRT method does not exhibit outstanding performance. This may be attributed to our more general experimental setting, which involves compressing a single benchmark rather than utilizing an entire leaderboard's data as done in their paper. This likely results in insufficient data to effectively train the gp-IRT model, thereby diminishing its effectiveness.

% In these experiments, we set the number of examples in the G-set as 10 and average the result over 100 randomized runs. Our objective is to observe and compare the performance of our method with baseline methods as the inference count, defined as the number of examples in the N-set for our method, ranged from 20 to 40. Observing the tables, when the inference count ranges from 20 to 40, our method significantly outperforms the baselines in terms of Kendall's $\tau$ metric. This indicates that our approach more effectively reconstructs the capability rankings among target models using only a small amount of evaluation data. A comparison between our method and the baselines in terms of variance is provided in Appendix \ref{apd:variance}, showing that our method exhibits significantly lower variance, indicating a higher level of robustness compared to the baseline approaches. Moreover, The results marked with an asterisk in our table mean that, when a one-sided Z-test is performed on the results of 100 repeated experiments with our method compared to 100 experiments with the baseline method, the p-value indicating our method's results are greater than those of the baseline is greater than 0.05.
% Notably, on the ARC Challenge benchmark, our method achieves up to a 7\% improvement in Kendall's $\tau$ overall baseline methods.
% Furthermore, our method consistently outperforms the baselines on the MAE metric across all benchmarks, achieving an average reduction of 31.4\% in estimation MAE loss, indicating a more accurate estimation of the target models' actual performance. Especially, Compared to the static AnchorPoints method, our MAE is typically only half as large, highlighting our method’s ability to identify a more representative N-set for each target model. From the experimental results, we also observe that the gp-IRT method does not exhibit outstanding performance. This may be due to our more general experimental setting, which is compressing a single benchmark rather than utilizing an entire leaderboard's data as done in their paper. This likely results in insufficient data to effectively train the gp-IRT model, thereby diminishing its effectiveness.

% \begin{figure}[ht]
%     % \vspace{-0.3cm}
%     \centering
%     \includegraphics[scale=0.42]{figs/main_analysis_GSM8K.pdf}
%     \caption{Effect of the Number of Native Source Models in Our Method on GSM8K Benchmark.}
%     \label{FigMainAnalysis}
%     % \vspace{-0.2cm}
% \end{figure}

\begin{figure*}[t]
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/GSM8K_modelnum_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/GSM8K_modelnum_MAE.pdf}
  \caption {Performance of \textsc{TailoredBench} with varying numbers of Native Source Models on GSM8K benchmark. The shaded area indicates the adaptively selected number of native source models and the corresponding performance of our method.}
\vspace{-0.4cm}
\label{FigMainAnalysis}
\end{figure*}



\paragraph{TailoredBench Method Adaptively Selects Optimal Native Source Model Sets} Here, we analyze the ability of our method to select the optimal native source model sets. Figure \ref{FigMainAnalysis} shows the performance of our method on the GSM8K benchmark, where source models with the top-k prediction consistency to the target model are selected as Native source models. The results reveal that Kendall's $\tau$ coefficient initially increases and then decreases as the number of native source models grows, while the MAE first decreases and then increases. This trend aligns with our observations in Figure \ref{fig:GSM8K_combined}. Specifically, when only a few native source models are selected, their high consistency with the target model is offset by the noise introduced due to the small sample size, which reduces clustering performance. Increasing the number of native source models helps mitigate this issue and improves performance until an optimal point is reached. However, selecting too many native source models incorporates models with lower prediction consistency to the target model, which diminishes effectiveness. Our method addresses this by adaptively selecting the near-optimal number of native source models across all benchmarks. For example, as shown in Figure \ref{FigMainAnalysis}, our approach selects 40 native source models for each target model on the GSM8K benchmark, achieving near-optimal performance. Further experiments pertaining to this section are detailed in Appendix \ref{apd:main_analysis}.

Additionally, we observe that target models preferentially select native source models from their own family, which can better capture the nuances and prediction patterns distinctive to their respective model lineages and contribute to more accurate performance estimations. This intra-family selection bias and the performance of our method when target models significantly differ from source models is further explored in detail in Appendix \ref{apd:intra-family-models}.

% This intra-family affinity may facilitate more accurate performance estimation, as the selected Native Source Models can better capture the nuances and prediction patterns distinctive to their respective model lineages.

% \begin{table*}
% \renewcommand\arraystretch{1}
% \centering
% \setlength{\tabcolsep}{0.5em} 
% \begin{tabular}{c *{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{ $\lvert$G-set$\rvert$ }} & \multicolumn{2}{c}{ARC Challenge} & \multicolumn{2}{c}{Hellaswag} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Winogrande} & \multicolumn{2}{c}{POPE} \\ 
% \noalign{\vskip -0.17em}
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
% \midrule
% \textsc{5}  & 0.719 & 0.031 & \uline{0.912} & \uline{0.019} & \textbf{0.865} & 0.035 & \uline{0.624} & 0.026 & 0.549 & 0.037 \\
% \textsc{10} & \textbf{0.756} & \textbf{0.028} & \textbf{0.913} & \textbf{0.018} & \uline{0.863} & \textbf{0.033} & 0.604 & \textbf{0.024} & \textbf{0.562} & \textbf{0.031} \\
% \textsc{15} & \uline{0.751} & \uline{0.029} & 0.911 & \textbf{0.018} & 0.854 & \uline{0.034} & 0.608 & \uline{0.025} & \uline{0.556} & \uline{0.033} \\
% \textsc{20} & 0.740 & \uline{0.029} & 0.910 & \textbf{0.018} & 0.862 & \uline{0.034} & 0.621 & 0.026 & 0.541 & 0.034 \\
% \textsc{25} & 0.725 & 0.030 & 0.909 & \uline{0.019} & 0.851 & 0.036 & \textbf{0.638} & 0.026 & 0.533 & 0.036 \\
% \bottomrule
% \end{tabular}
% \caption{Impact of G-set Size on Performance Across Benchmarks.}
% \label{tab:gset}
% \end{table*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% G-set消融
% \vspace{-0.5cm}
% \paragraph{10 Examples are Sufficient for the Probe.} We conducted an ablation study to assess the impact of the G-set size on our method’s performance, fixing the N-set at 30 examples and varying the G-set from 5 to 25 examples across all benchmarks. The results in Table \ref{tab:gset} show that Kendall's $\tau$ initially increases and then decreases, while MAE first decreases and then increases as the G-set size grows. This trend arises because a small G-set fails to capture the full prediction consistency between the source and target models, limiting N-set selection. Conversely, an excessively large G-set reduces the representativeness of the N-set, as it becomes dominated by points from the G-set itself, leading to diminished performance. Specifically, a G-set size of 10 examples yielded the best performance, achieving the highest Kendall's $\tau$ and the lowest MAE. Detailed results for each dataset are provided in Appendix \ref{apd:gset}.

\paragraph{10 Examples are Sufficient for the Probe} 
We investigate how G-set size affects our method's performance by fixing the N-set at 30 examples and varying the G-set from 5 to 25 examples across all benchmarks. As shown in Table \ref{tab:gset}, Kendall's $\tau$ peaks and MAE reaches a minimum at a G-set size of 10. Smaller G-set fail to capture the prediction consistency between source and target models, limiting effective N-set selection. Conversely, a larger G-set reduces N-set representativeness by being dominated by G-set points, leading to diminished performance. Detailed per-dataset results are provided in Appendix \ref{apd:gset}.

\begin{table}[htbp]
\renewcommand\arraystretch{1}
\centering
% \small
\setlength{\tabcolsep}{1.25em} 
\begin{tabular}{ccc}
\toprule
\textbf{$\lvert$G-set$\rvert$} & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
\midrule
\textsc{5}    & 0.734 & 0.030 \\
\textsc{10}   & \textbf{0.740} & \textbf{0.027} \\
\textsc{15}   & \uline{0.736} & \uline{0.028} \\
\textsc{20}   & 0.735 & \uline{0.028} \\
\textsc{25}   & 0.731 & 0.029 \\
\bottomrule
\end{tabular}
\caption{Average performance with different G-set size across benchmarks.}
\label{tab:gset}
\vspace{-0.5cm}
\end{table}
% Notably, on the Winogrande benchmark, Kendall's $\tau$ metric shows a trend of first decreasing and then increasing. This behavior is related to the characteristics of this benchmark and the models we used. A more detailed analysis is provided in Appendix \ref{apd:winogrande_analysis}. Despite the G-set selection under this setting not being optimal for Winogrande, as shown in Table \ref{tab:winogrande}, the TailoredBench method still significantly outperforms the baseline methods, demonstrating superior performance. In conclusion, considering all results and balancing the models' inference cost with actual performance, we conclude that 10 examples are sufficient to serve as the G-set for all benchmarks.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 距离消融

\paragraph{Performance with Larger Inference Count On the Hellaswag Benchmark.} 
We further evaluate our method with larger inference counts on the Hellaswag benchmark. 
Table \ref{tab:largeNset} shows that as inference counts increase from 50 to 150, \textsc{TailoredBench} consistently improves model performance prediction and ranking, maintaining a clear advantage over baseline methods, demonstrating its effectiveness with larger inference budgets.
% As shown in Table \ref{tab:largeNset}, as the inference counts increase from 50 to 150, \textsc{TailoredBench} consistently achieves improvements in model performance prediction and ranking, maintaining a clear advantage over various baseline methods. This demonstrates that \textsc{TailoredBench} generalizes effectively to larger inference budgets.

\begin{table}[htbp]
\renewcommand\arraystretch{1.41}
\centering
\small
\setlength{\tabcolsep}{0.39em} % 可根据需要调整列间距
\begin{tabular}{c *{3}{cc}}
\toprule
\multirow{2}{*}{\makecell{\textbf{Inference} \\ \textbf{counts}}} & \multicolumn{2}{c}{50} & \multicolumn{2}{c}{100} & \multicolumn{2}{c}{150} \\  
\noalign{\vskip -0.17em}
 & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$} \\ 
\midrule 
\textsc{Random}         & 0.887 & 0.053 & 0.920 & 0.038 & 0.935 & \uline{0.030} \\
\hdashline
\makecell{\textsc{Anchor} \\ \textsc{Points}}   & \uline{0.915} & 0.046 & \uline{0.931} & 0.040 & \uline{0.940} & 0.040 \\
\hdashline
\textsc{gp-IRT}         & 0.869 & \uline{0.026} & 0.915 & \uline{0.015} & 0.936 & \textbf{0.012} \\
\hdashline
\makecell{\textsc{Tailored} \\ \textsc{Bench}}  & \textbf{0.923} & \textbf{0.016} & \textbf{0.934} & \textbf{0.014} & \textbf{0.943} & \textbf{0.012} \\
\bottomrule
\end{tabular}
\caption{Performance of compared methods on the Hellaswag benchmark with larger inference counts.}
\label{tab:largeNset}
\vspace{-0.6cm}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 更大的N-set
% \begin{wraptable}[10]{r}{0.65\textwidth}%靠文字内容的右侧
% \vspace{-10pt}
% \centering
% \renewcommand{\arraystretch}{1.1}
% \begin{tabular}{l *{3}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{ $\lvert$N-set$\rvert$ }} & \multicolumn{2}{c}{50} & \multicolumn{2}{c}{100} & \multicolumn{2}{c}{150} \\ 
% \noalign{\vskip -0.17em}
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$} \\ 
% \midrule 
% \textsc{Random}         & 0.887 & 0.053 & 0.920 & 0.038 & 0.935 & 0.030 \\
% \textsc{AnchorPoints}   & \uline{0.915} & 0.046 & \uline{0.931} & 0.040 & \uline{0.940} & 0.040 \\
% \textsc{gp-IRT}         & 0.869 & \uline{0.026} & 0.915 & \uline{0.015} & 0.936 & \uline{0.012} \\
% \textsc{TailoredBench}  & \textbf{0.919} & \textbf{0.016} & \textbf{0.934} & \textbf{0.013} & \textbf{0.943} & \textbf{0.011} \\

% \bottomrule
% \end{tabular}
% \caption{Performance Improvement with Increasing N-set Size on Hellaswag Benchmark}
% \label{tab:largeNset}
% \end{wraptable}
