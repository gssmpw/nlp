
\appendix
%\newpage
\section{Related Works}
\label{apd:related_works}
\paragraph{Models Correlation in Predictive Consistency:} Prior works \citep{taori2020measuring, miller2021accuracy, awadalla2022exploring} have demonstrated a certain level of correlation between in-distribution (ID) and out-of-distribution (OOD) performances across diverse models and tasks. Building on this foundation, \cite{baek2022agreement} and \cite{mehra2024predicting} advance this relationship by showing the phenomenon that the agreement between two models on ID data is linearly correlated with their agreement on OOD data, where the accuracy holds the similar linear relationship, enabling accurate estimation of model's OOD accuracy based solely on ID data. Our work extends this phenomenon to address the challenge of benchmark compression, enabling the selection of more representative subsets for benchmarks. 
%阐述模型之间是存在correlations的，并且这种correlation可以用于模型在下游任务上的能力预测；

\paragraph{Coreset Selection for Efficient Benchmarking:} As LLMs proliferate and version updates accelerate, the cost of thoroughly evaluating each model across all benchmarks has become prohibitive, leading to methods that subsample the most representative subsets from each benchmark for more efficient evaluation. \cite{AP} clusters examples directly using the confidence scores provided by source models, leveraging these scores to select an optimal subset. Similarly, \cite{tiny} employs an Item Response Theory (IRT) model, trained on the success matrix of each source model across various examples, to derive the latent representations of examples for clustering. \cite{100instances} introduces a generic assessor framework that predicts the performance of a new LLM on unseen instances using its results on a small reference set, achieving comparable accuracy to full-scale evaluations. \cite{perlitz2023efficient} proposes Flash-HELM, which dynamically adjusts the sizes of randomly selected subsets based on model ranking, where higher-ranked models are evaluated with greater precision. \cite{prabhu2024lifelong} proposes the Sort \& Search (S\&S) strategy, which leverages the difficulties of examples and dynamic programming to select the coreset. 
% Moreover, \cite{tiny} and \cite{AP} use clustering techniques to subsample the most representative examples to evaluate models efficiently.
% In contrast, \cite{AP} clusters examples directly using the confidence scores provided by source models, leveraging these scores to select an optimal subset. 
\cite{xu2024data} synthesizes several methods and dynamically chooses the optimal subset selection method for each benchmark but requires many examples to determine the best approach. Despite these advancements, these methods often struggle with substantial distribution shifts between the source and target models, caused by the discrepancy between their predictive consistency, potentially causing significant distortion in estimating the target model's performance. 
Extending the approach of \cite{AP}, our work alleviates this issue by dynamically selecting a native source model set with the highest prediction consistency to the target model, ensuring the selection of a tailored coreset for each target model that best represents the benchmark.
%The Anchor Points approach is particularly relevant to our work, as it directly utilizes LLM confidence scores for each data point, offering a more straightforward and computationally efficient method for identifying representative subsets.

\textbf{Scaling Approaches for Model Performance Estimations:} 
Scaling law describes the relationship between model properties (e.g., FLOPs used during training, model parameter size) and model capabilities. 
Recent works \citep{hu2023predicting, ruan2024observational, isik2024scaling} have leveraged scaling laws to predict model performance on various downstream tasks, reducing the computational cost of evaluating models on complex downstream tasks. \cite{zhang2024collaborative} simplifies those approaches by utilizing the relationships between model families and their collaborative overall performance across tasks rather than fitting scaling laws. The aforementioned methods typically rely on overall model performance across several benchmarks and specific design factors (e.g., model size or training data properties) to either fit scaling curves or investigate correlations between models on various tasks. In contrast, our approach addresses a more general case by reducing the evaluation cost for multiple models on a single benchmark, offering a more efficient performance estimation framework.
% 这四篇论文中的方法适用于对模型在复杂下游任务表现上的整体评估，尤其是在新的下游任务中进行跨模型、跨任务的性能预测。这些方法对整个任务的平均表现进行建模，适合处理大规模、多任务的场景。它们更关注通过宏观的缩放规律（Scaling Laws）来建立模型在大规模任务上的表现预测。这类方法需要完整的模型训练数据，模型参数，以及模型在数据集上的整体得分，而非item level的得分。
% 打个比方，如果我想知道一个模型在A数据集(复杂数据集)上的总分，我需要根据这个模型在B数据集上(简单数据集)的总分，才能而推断出这个模型在A数据集上的总分。
%\clearpage

%\newpage
\section{More Experimental Results}

\subsection{Comprehensive Experimental Results Across All Datasets}
\label{apd:all_result}
In Table \ref{tab:apdallresult}, we present a comprehensive comparison of our approach against all baseline methods across the full range of benchmark datasets. The results indicate that our method consistently outperforms every baseline under all considered inference counts, thereby demonstrating the overall effectiveness of our proposed approach.
\begin{table*}[ht]
\renewcommand\arraystretch{1.2}
\centering
\small
\setlength{\tabcolsep}{0.49em} 
\begin{tabular}{cl*{5}{cc}}
\toprule
\multirow{2}{*}{\textbf{Benchmarks}} & \multirow{2}{*}{\textbf{Inference counts}} & \multicolumn{2}{c}{20} & \multicolumn{2}{c}{25} & \multicolumn{2}{c}{30} & \multicolumn{2}{c}{35} & \multicolumn{2}{c}{40} \\
&  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\
\midrule
\multirow{4}{*}{ARC Challenge}&\textsc{Random}        & 0.626 & 0.078 & 0.659 & 0.065 & 0.676 & 0.067 & 0.694 & 0.062 & 0.712 & 0.057 \\
&\textsc{AnchorPoints} & \uline{0.662} & 0.064 & \uline{0.663} & 0.058 & \uline{0.676} & 0.053 & \uline{0.713} & 0.048 & \uline{0.714} & 0.043 \\
&\textsc{gp-IRT}        & 0.589 & \uline{0.046} & 0.620 & \uline{0.046} & 0.662 & \uline{0.036} & 0.681 & \uline{0.036} & 0.695 & \uline{0.029} \\ 
                           & \textsc{TailoredBench} & \textbf{0.711} & \textbf{0.031} & \textbf{0.737} & \textbf{0.029} & \textbf{0.756} & \textbf{0.028} & \textbf{0.766} & \textbf{0.027} & \textbf{0.773} & \textbf{0.027} \\
\hdashline
\multirow{4}{*}{Hellaswag}&\textsc{Random}         & 0.811 & 0.083 & 0.836 & 0.077 & 0.850 & 0.066 & 0.863 & 0.060 & 0.871 & 0.058  \\
&\textsc{AnchorPoints}  & \uline{0.860} & \uline{0.060} & \uline{0.880} & 0.061 & \uline{0.877} & 0.067 & \uline{0.897} & 0.059 & \uline{0.898} & 0.057 \\
&\textsc{gp-IRT}         & 0.724 & 0.062 & 0.776 & \uline{0.053} & 0.810 & \uline{0.043} & 0.827 & \uline{0.038} & 0.849 & \uline{0.032} \\ 
                           & \textsc{TailoredBench} & \textbf{0.900} & \textbf{0.020} & \textbf{0.909} & \textbf{0.018} & \textbf{0.913} & \textbf{0.018} & \textbf{0.914} & \textbf{0.017} & \textbf{0.918} & \textbf{0.017} \\
\hdashline
\multirow{4}{*}{GSM8K}&\textsc{Random}        & \uline{0.811} & 0.062 & \uline{0.828} & 0.055 & \uline{0.839} & 0.052 & \uline{0.847} & 0.049 & \uline{0.858} & 0.044 \\
&\textsc{AnchorPoints} & 0.786 & 0.087 & 0.791 & 0.079 & 0.796 & 0.073 & 0.800 & 0.071 & 0.799 & 0.071 \\
&\textsc{gp-IRT}        & 0.787 & \uline{0.055} & 0.807 & \uline{0.047} & 0.829 & \uline{0.041} & 0.842 & \uline{0.038} & 0.858 & \uline{0.034} \\ 
                           &\textsc{TailoredBench} & \textbf{0.852} & \textbf{0.035} & \textbf{0.858} & \textbf{0.034} & \textbf{0.863} & \textbf{0.033} & \textbf{0.869} & \textbf{0.031} & \textbf{0.878} & \textbf{0.029} \\
\hdashline
\multirow{4}{*}{Winogrande}&\textsc{Random}        & 0.373 & 0.078 & 0.408 & 0.067 & 0.446 & 0.062 & 0.470 & 0.055 & 0.492 & 0.052 \\
&\textsc{AnchorPoints} & \uline{0.472} & 0.086 & \uline{0.487} & 0.085 & \uline{0.514} & 0.075 & \uline{0.521} & 0.087 & \uline{0.518} & 0.073 \\
&\textsc{gp-IRT}        & 0.263 & \uline{0.041} & 0.313 & \uline{0.038} & 0.353 & \uline{0.038} & 0.392 & \uline{0.036} & 0.419 & \uline{0.034} \\ 
                           & \textsc{TailoredBench} & \textbf{0.565} & \textbf{0.028} & \textbf{0.568} & \textbf{0.026} & \textbf{0.604} & \textbf{0.024} & \textbf{0.608} & \textbf{0.023} & \textbf{0.618} & \textbf{0.022} \\
\hdashline
\multirow{4}{*}{POPE}&\textsc{Random}        & \uline{0.488} & 0.058 & \uline{0.510} & 0.054 & 0.507 & 0.048 & 0.515 & 0.044 & 0.547 & 0.040 \\
&\textsc{AnchorPoints} & 0.474 & 0.040 & 0.483 & 0.038 & \uline{0.518} & \uline{0.034} & \uline{0.547} & \uline{0.033} & \uline{0.556} & \textbf{0.031} \\
&\textsc{gp-IRT}        & 0.481 & \uline{0.038} & 0.470 & \uline{0.037} & 0.462 & 0.036 & 0.482 & 0.034 & 0.477 & 0.033 \\ 
                           &\textsc{TailoredBench} & \textbf{0.521} & \textbf{0.036} & \textbf{0.547} & \textbf{0.035} & \textbf{0.562} & \textbf{0.031} & \textbf{0.563} & \textbf{0.031} & \textbf{0.574} & \uline{0.032} \\
\bottomrule
\end{tabular}
\caption{Results on all benchmarks. Values in bold represent the best results, while values that are underlined represent the second-best results.}
\label{tab:apdallresult}
\end{table*}
%\clearpage
% \begin{table*}
% \renewcommand\arraystretch{1}
% \centering
% %\small
% \setlength{\tabcolsep}{0.5em} 
% \begin{tabular}{l*{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{inference count}} & \multicolumn{2}{c}{20} & \multicolumn{2}{c}{25} & \multicolumn{2}{c}{30} & \multicolumn{2}{c}{35} & \multicolumn{2}{c}{40} \\ 
% \noalign{\vskip -0.17em}
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
% \midrule
% \textsc{Random}        & 0.626 & 0.078 & 0.659 & 0.065 & 0.676 & 0.067 & 0.694 & 0.062 & 0.712 & 0.057 \\
% \textsc{AnchorPoints} & \uline{0.662} & 0.064 & \uline{0.663} & 0.058 & \uline{0.676} & 0.053 & \uline{0.713} & 0.048 & \uline{0.714} & 0.043 \\
% \textsc{gp-IRT}        & 0.589 & \uline{0.046} & 0.620 & \uline{0.046} & 0.662 & \uline{0.036} & 0.681 & \uline{0.036} & 0.695 & \uline{0.029} \\ 
% \textsc{\textsc{TailoredBench}} & \textbf{0.711} & \textbf{0.031} & \textbf{0.737} & \textbf{0.029} & \textbf{0.756} & \textbf{0.028} & \textbf{0.766} & \textbf{0.027} & \textbf{0.773} & \textbf{0.027} \\
% \bottomrule
% \end{tabular}
% \caption{Results on ARC Challenge benchmark.}
% \vspace{-0.1cm}
% \label{tab:arc}
% \end{table*}

% \begin{table*}
% \renewcommand\arraystretch{1}
% \centering
% %\small
% \setlength{\tabcolsep}{0.5em} 
% \begin{tabular}{l*{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{inference count}} & \multicolumn{2}{c}{20} & \multicolumn{2}{c}{25} & \multicolumn{2}{c}{30} & \multicolumn{2}{c}{35} & \multicolumn{2}{c}{40} \\ 
% \noalign{\vskip -0.17em}
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
% \midrule
% \textsc{Random}         & 0.811 & 0.083 & 0.836 & 0.077 & 0.850 & 0.066 & 0.863 & 0.060 & 0.871 & 0.058  \\
% \textsc{AnchorPoints}  & \uline{0.860} & \uline{0.060} & \uline{0.880} & 0.061 & \uline{0.877} & 0.067 & \uline{0.897} & 0.059 & \uline{0.898} & 0.057 \\
% \textsc{gp-IRT}         & 0.724 & 0.062 & 0.776 & \uline{0.053} & 0.810 & \uline{0.043} & 0.827 & \uline{0.038} & 0.849 & \uline{0.032} \\ 
% \textsc{\textsc{TailoredBench}} & \textbf{0.900} & \textbf{0.020} & \textbf{0.909} & \textbf{0.018} & \textbf{0.913} & \textbf{0.018} & \textbf{0.914} & \textbf{0.017} & \textbf{0.918} & \textbf{0.017} \\
% \bottomrule
% \end{tabular}
% \caption{Results on Hellaswag benchmark.}
% \vspace{-0.1cm}
% \label{tab:hellaswag}
% \end{table*}

% \begin{table*}
% \renewcommand\arraystretch{1}
% \centering
% %\small
% \setlength{\tabcolsep}{0.5em} 
% \begin{tabular}{l*{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{inference count}} & \multicolumn{2}{c}{20} & \multicolumn{2}{c}{25} & \multicolumn{2}{c}{30} & \multicolumn{2}{c}{35} & \multicolumn{2}{c}{40} \\ 
% \noalign{\vskip -0.17em}
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
% \midrule
% \textsc{Random}        & \uline{0.811} & 0.062 & \uline{0.828} & 0.055 & \uline{0.839} & 0.052 & \uline{0.847} & 0.049 & \uline{0.858} & 0.044 \\
% \textsc{AnchorPoints} & 0.786 & 0.087 & 0.791 & 0.079 & 0.796 & 0.073 & 0.800 & 0.071 & 0.799 & 0.071 \\
% \textsc{gp-IRT}        & 0.787 & \uline{0.055} & 0.807 & \uline{0.047} & 0.829 & \uline{0.041} & 0.842 & \uline{0.038} & 0.858 & \uline{0.034} \\ 
% \textsc{\textsc{TailoredBench}} & \textbf{0.852} & \textbf{0.035} & \textbf{0.858} & \textbf{0.034} & \textbf{0.863} & \textbf{0.033} & \textbf{0.869} & \textbf{0.031} & \textbf{0.878} & \textbf{0.029} \\
% \bottomrule
% \end{tabular}
% \caption{Results on GSM8K benchmark.}
% \vspace{-0.1cm}
% \label{tab:GSM8K}
% \end{table*}

% \begin{table*}
% \renewcommand\arraystretch{1}
% \centering
% %\small
% \setlength{\tabcolsep}{0.5em} 
% \begin{tabular}{l*{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{inference count}} & \multicolumn{2}{c}{20} & \multicolumn{2}{c}{25} & \multicolumn{2}{c}{30} & \multicolumn{2}{c}{35} & \multicolumn{2}{c}{40} \\ 
% \noalign{\vskip -0.17em}
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
% \midrule
% \textsc{Random}        & 0.373 & 0.078 & 0.408 & 0.067 & 0.446 & 0.062 & 0.470 & 0.055 & 0.492 & 0.052 \\
% \textsc{AnchorPoints} & \uline{0.472} & 0.086 & \uline{0.487} & 0.085 & \uline{0.514} & 0.075 & \uline{0.521} & 0.087 & \uline{0.518} & 0.073 \\
% \textsc{gp-IRT}        & 0.263 & \uline{0.041} & 0.313 & \uline{0.038} & 0.353 & \uline{0.038} & 0.392 & \uline{0.036} & 0.419 & \uline{0.034} \\ 
% \textsc{\textsc{TailoredBench}} & \textbf{0.565} & \textbf{0.028} & \textbf{0.568} & \textbf{0.026} & \textbf{0.604} & \textbf{0.024} & \textbf{0.608} & \textbf{0.023} & \textbf{0.618} & \textbf{0.022} \\
% \bottomrule
% \end{tabular}
% \caption{Results on Winogrande benchmark.}
% \vspace{-0.1cm}
% \label{tab:winogrande}
% \end{table*}

% \begin{table*}
% \renewcommand\arraystretch{1}
% \centering
% %\small
% \setlength{\tabcolsep}{0.5em} 
% \begin{tabular}{l*{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{inference count}} & \multicolumn{2}{c}{20} & \multicolumn{2}{c}{25} & \multicolumn{2}{c}{30} & \multicolumn{2}{c}{35} & \multicolumn{2}{c}{40} \\ 
% \noalign{\vskip -0.17em}
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
% \midrule
% \textsc{Random}        & \uline{0.488} & 0.058 & \uline{0.510} & 0.054 & 0.507 & 0.048 & 0.515 & 0.044 & 0.547 & 0.040 \\
% \textsc{AnchorPoints} & 0.474 & 0.040 & 0.483 & 0.038 & \uline{0.518} & \uline{0.034} & \uline{0.547} & \uline{0.033} & \uline{0.556} & $\textbf{0.031}^{*}$ \\
% \textsc{gp-IRT}        & 0.481 & \uline{0.038} & 0.470 & \uline{0.037} & 0.462 & 0.036 & 0.482 & 0.034 & 0.477 & 0.033 \\ 
% \textsc{\textsc{TailoredBench}} & \textbf{0.521} & \textbf{0.036} & \textbf{0.547} & \textbf{0.035} & \textbf{0.562} & \textbf{0.031} & \textbf{0.563} & $\textbf{0.031}^{*}$ & \textbf{0.574} & \uline{0.032} \\
% \bottomrule
% \end{tabular}
% \caption{Results on POPE benchmark.}
% \label{tab:pope}
% \end{table*}
% \FloatBarrier
% %\clearpage
%\newpage
\subsection{Comprehensive Distance Measures Ablation Across Benchmarks}
\label{apd:distance}
Here, we provide comprehensive results of our ablation study evaluating the impact of different distance measures on our method's performance with 30 inference counts across various benchmarks. Table \ref{apdtab:distance} presents detailed Kendall's $\tau$ and MAE metrics for cosine similarity, Manhattan distance, and correlation distance across all datasets. These results offer deeper insights into the effectiveness of Element-Wise Distance measures in enhancing benchmark compression.

\begin{table*}
\renewcommand\arraystretch{1}
\centering
% \small
\setlength{\tabcolsep}{0.3em} 
\begin{tabular}{l *{5}{cc}}
\toprule
\multirow{2}{*}{\textbf{Distances}} & \multicolumn{2}{c}{ARC Challenge} & \multicolumn{2}{c}{Hellaswag} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Winogrande} & \multicolumn{2}{c}{POPE} \\ 
\noalign{\vskip -0.17em}
 & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
\midrule
\textsc{Correlation}    & \textbf{0.766} & 0.033 & 0.903 & \uline{0.019} & \uline{0.828} & 0.041 & 0.557 & 0.029 & 0.547 & 0.038 \\
\textsc{Cosine}         & 0.746 & \uline{0.031} & \textbf{0.914} & \uline{0.019} & 0.827 & \uline{0.040} & \textbf{0.616} & \textbf{0.024} & \textbf{0.577} & \textbf{0.024} \\
\textsc{Manhattan}      & \uline{0.756} & \textbf{0.028} & \uline{0.913} & \textbf{0.018} & \textbf{0.863} & \textbf{0.033} & \uline{0.604} & \uline{0.024} & \uline{0.562} & \uline{0.031} \\
\bottomrule
\end{tabular}
\caption{Detailed ablation results for distance selection across all benchmarks.}
\label{apdtab:distance}
% \vspace{-0.5cm}
\end{table*}

\subsection{Detailed Calibration Ablation Results}
\label{apd:ablation_calibration}
Table \ref{tab:ablation_calibration} presents the results of our ablation study, comparing our \textsc{TailoredBench} method with and without the calibrated performance estimation process under 30 inference counts. The calibrated version of our method generally achieves higher Kendall’s $\tau$ scores and lower mean absolute errors (MAE) across various benchmarks and inference counts, demonstrating that the calibrated performance estimation process effectively enhances the performance estimation ability of our method.

% \begin{table*}[ht]
% \renewcommand\arraystretch{1.2}
% \centering
% \small
% \setlength{\tabcolsep}{0.48em} 
% \begin{tabular}{cl*{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{Benchmarks}} & \multirow{2}{*}{\textbf{Inference counts}} & \multicolumn{2}{c}{20} & \multicolumn{2}{c}{25} & \multicolumn{2}{c}{30} & \multicolumn{2}{c}{35} & \multicolumn{2}{c}{40} \\
% &  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\
% \midrule
% \multirow{2}{*}{ARC Challenge}&\textsc{Non-Calibrated} & 0.705 & \textbf{0.029} & 0.731 & \textbf{0.027} & 0.748 & \textbf{0.026} & 0.756 & \textbf{0.025} & 0.767 & \textbf{0.024} \\
%                            & \textsc{Calibrated} & \textbf{0.711} & 0.031 & \textbf{0.737} & 0.029 & \textbf{0.756} & 0.028 & \textbf{0.766} & 0.027 & \textbf{0.773} & 0.027 \\
% \hdashline
% \multirow{2}{*}{Hellaswag}&\textsc{Non-Calibrated} & 0.898 & 0.020 & 0.906 & 0.018 & 0.910 & 0.017 & 0.911 & 0.017 & 0.916 & 0.016 \\
%                            & \textsc{Calibrated} & \textbf{0.900} & \textbf{0.020} & \textbf{0.909} & \textbf{0.018} & \textbf{0.913} & \textbf{0.018} & \textbf{0.914} & \textbf{0.017} & \textbf{0.918} & \textbf{0.017} \\
% \hdashline
% \multirow{2}{*}{GSM8K}&\textsc{Non-Calibrated} & 0.851 & 0.041 & 0.853 & 0.039 & 0.862 & 0.036 & 0.866 & 0.035 & \textbf{0.879} & 0.032 \\
%                            &\textsc{Calibrated} & \textbf{0.852} & \textbf{0.035} & \textbf{0.858} & \textbf{0.034} & \textbf{0.863} & \textbf{0.033} & \textbf{0.869} & \textbf{0.031} & \textbf{0.879} & \textbf{0.029} \\
% \hdashline
% \multirow{2}{*}{Winogrande}&\textsc{Non-Calibrated} & 0.543 & 0.032 & 0.561 & 0.031 & 0.588 & 0.028 & 0.588 & 0.027 & 0.605 & 0.025 \\
%                            & \textsc{Calibrated} & \textbf{0.565} & \textbf{0.028} & \textbf{0.568} & \textbf{0.026} & \textbf{0.604} & \textbf{0.024} & \textbf{0.608} & \textbf{0.023} & \textbf{0.618} & \textbf{0.022} \\
% \hdashline
% \multirow{2}{*}{\textsc{POPE}}&\textsc{Non-Calibrated} & 0.509 & 0.043 & 0.526 & 0.041 & 0.531 & 0.043 & 0.547 & 0.038 & 0.551 & 0.039 \\
%                            &\textsc{Calibrated} & \textbf{0.521} & \textbf{0.036} & \textbf{0.547} & \textbf{0.035} & \textbf{0.562} & \textbf{0.031} & \textbf{0.563} & \textbf{0.031} & \textbf{0.574} & \textbf{0.032} \\
% \bottomrule
% \end{tabular}
% \caption{Detailed Ablation Results for Calibrated Performance Estimation Process Across All Benchmarks.}
% \label{tab:ablation_calibration}
% \end{table*}
%\clearpage

\begin{table*}
\renewcommand\arraystretch{1}
\centering
% \small
\setlength{\tabcolsep}{0.3em} 
\begin{tabular}{l *{5}{cc}}
\toprule
\multirow{2}{*}{\textbf{Method Variants}} & \multicolumn{2}{c}{ARC Challenge} & \multicolumn{2}{c}{Hellaswag} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Winogrande} & \multicolumn{2}{c}{POPE} \\ 
\noalign{\vskip -0.17em}
 & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
\midrule
\textsc{Non-Calibrated}    & 0.748 & \textbf{0.026} & 0.910 & \textbf{0.017} & 0.862 & 0.036 & 0.588 & 0.028 & 0.531 & 0.043 \\
\textsc{Calibrated}      & \textbf{0.756} & 0.028 & \textbf{0.913} & 0.018 & \textbf{0.863} & \textbf{0.033} & \textbf{0.604} & \textbf{0.024} & \textbf{0.562} & \textbf{0.031} \\
\bottomrule
\end{tabular}
\caption{Detailed ablation results for calibrated performance estimation process across all benchmarks.}
% Average performance with and without calibration across benchmarks.
\label{tab:ablation_calibration}
\vspace{-0.3cm}
\end{table*}

\subsection{Impact of Dynamic Native Source Model Quantity on Performance}
\label{apd:dynamic-native-source-models}
In this ablation study, we investigate the effect of dynamically selecting varying numbers of native source models for each target model, as opposed to using a standardized quantity across all target models. Specifically, instead of treating \( \bar{n} \) (as computed in Eq. \ref{eq:bar_n}) as the fixed number of native source models, we now interpret it as a lower bound—thereby including all source models whose prediction consistency exceeds the threshold.

Table \ref{tab:dynamic-native-source-models} summarizes the results for all benchmarks under an inference count of 30. Notably, we observe improvements on the GSM8K and POPE datasets, while a slight decrease in performance is seen on other datasets.

These findings underscore that the core strength of our method lies in maximizing the consistency between the source and target models. When exactly \( \bar{n} \) native source models are selected for each target model, performance appears to be near optimal. In contrast, adding additional native source models for certain target models may introduce performance variability. Consequently, we adopt a standardized number of native source models to ensure stability.
\begin{table*}
\renewcommand\arraystretch{1}
\centering
\setlength{\tabcolsep}{0.3em} 
\begin{tabular}{l *{5}{cc}}
\toprule
\multirow{2}{*}{\textbf{Method Variants}} & \multicolumn{2}{c}{ARC Challenge} & \multicolumn{2}{c}{Hellaswag} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Winogrande} & \multicolumn{2}{c}{POPE} \\ 
\noalign{\vskip -0.17em}
 & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
\midrule
\makecell{\textsc{Dynamic native} \\ \textsc{source models number}} & 0.741 & 0.029 & 0.909 & \textbf{0.018} & \textbf{0.875} & \textbf{0.031} & 0.592 & 0.025 & \textbf{0.604} & \textbf{0.031} \\
\makecell{\textsc{Standardized native} \\ \textsc{source models number}} & \textbf{0.756} & \textbf{0.028} & \textbf{0.913} & \textbf{0.018} & 0.863 & 0.033 & \textbf{0.604} & \textbf{0.024} & 0.562 & \textbf{0.031} \\ 
\bottomrule
\end{tabular}
\caption{Ablation Study on Dynamic vs. Standardized Native Source Model Selection.}
\label{tab:dynamic-native-source-models}
% \vspace{-0.5cm}
\end{table*}

\subsection{Impact of Fixed Medoids in N-set Construction on Performance}
\label{apd:not-fix-gset}
In the Developing N-set module, our Scalable K-Medoids Clustering algorithm employs the G-set examples as fixed (anchored) medoids. To assess the effectiveness of this design choice, we conducted an ablation study comparing our standard approach (with fixed G-set medoids) against a variant where the G-set points are allowed to change during medoid refinement. The results are summarized in Table \ref{tab:not-fix-gset}.

When the N-set size is fixed at 30, our method with fixed medoids shows slightly inferior performance compared to the variant without fixed medoids (see rows 1 and 3 in Table \ref{tab:not-fix-gset}). When the inference budget is fixed at 30, our method outperforms the variant without fixed medoids (see rows 1 and 2 in Table \ref{tab:not-fix-gset}). These findings suggest that anchoring the G-set as fixed medoids helps achieve a balanced trade-off between the size of the N-set and the available inference budget.

\begin{table*}[ht]
\renewcommand\arraystretch{1}
\centering
\begin{tabular}{lccccc}
\toprule
\multirow{2}{*}{\textbf{Method Variants}} & \multirow{2}{*}{$|\text{G-set}|$} & \multirow{2}{*}{$|\text{N-set}|$} & \multirow{2}{*}{Inference counts} & \multicolumn{2}{c}{Average} \\
&  &  &  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$} \\
\midrule
\textsc{Fixed G-set}           & 10  & 30  & 30  &  0.740  & 0.027 \\
\textsc{not-Fixed G-set}      & 10  & 20  & 30  &  0.719  & 0.031 \\
\textsc{not-Fixed G-set}       & 10  & 30  & 40  &  \textbf{0.745} & \textbf{0.027} \\
\bottomrule
\end{tabular}
\caption{Performance Comparison of N-set Construction Methods with and without Fixed G-set Medoids.}
\label{tab:not-fix-gset}
\end{table*}


\subsection{Intra-Family Affinity and Its Impact on Performance under Different Source-Target Model Similarity}
\label{apd:intra-family-models}
We conducted an additional analysis on the GSM8K dataset to investigate whether models within the same family (e.g., Llama, Mistral) tend to select their own family members as native source models. As shown in Table \ref{tab:native_model_selection}, with a similar number of models from each family within the source and target model set, the results indicate a significant intra-family preference. On average, each llama-series model selected approximately 5.5 Mistral models and 6.4 Llama models as their native source models. Similarly, each Mistral-series model chose about 7.6 Mistral models and 3.0 Llama models on average. These findings suggest that models exhibit a bias toward source models with similar architectures, potentially due to shared representation spaces or analogous decision boundaries. This intra-family affinity may facilitate more accurate performance estimation, as the selected native source models can better capture the nuances and prediction patterns distinctive to their respective model lineages.

Furthermore, we conduct experiments on the GSM8K dataset to evaluate the performance of our method when the target models differ significantly from the source models. Specifically, by selecting only Llama series models as the target and using an inference count of 30, we compare the performance of all methods across two sets of source models: one that includes Llama series models and one that does not, with each set comprising an equal number of models. As shown in Table \ref{tab:Source_Models_Type}, the performance of all methods is closely correlated with the similarity between the target and source models; when these models differ, performance declines across all methods. Nonetheless, our method consistently outperforms the baselines regardless of the target–source model similarity, underscoring the generalizability of our approach.

\begin{table}[htbp]
    \renewcommand\arraystretch{1}
    \centering
    % \small
    \setlength{\tabcolsep}{0.65em} 
    \begin{tabular}{ccc}
    \toprule
    \makecell{\textbf{Model} \\ \textbf{Family}} & 
    \makecell{{Avg. Selected} \\ {Mistral Models}} & 
    \makecell{{Avg. Selected} \\ {Llama Models}} \\
    \midrule
    \textsc{Llama}    & 5.5 & 6.4 \\
    \textsc{Mistral}  & 7.6 & 3.0 \\
    \bottomrule
    \end{tabular}
    \caption{Statistics of native source model selection within model families on GSM8K benchmark.}
    \label{tab:native_model_selection}
    \vspace{-0.1cm}
\end{table}

\begin{table}
    \renewcommand\arraystretch{1}
    \centering
    % \small
    \setlength{\tabcolsep}{0.3em} 
    \begin{tabular}{l *{2}{cc}}
    \toprule
    \multirow{2}{*}{\makecell{\textbf{Source Models} \\ \textbf{Composition}}} & \multicolumn{2}{c}{w/o Llama} & \multicolumn{2}{c}{with Llama}\\ 
    \noalign{\vskip -0.17em}
     & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$} \\ 
    \midrule
    \textsc{AnchorPoints}   & 0.388 & 0.048 & 0.525 & 0.050\\
    \textsc{GP-IRT}         & 0.505 & 0.064 & 0.526 & 0.038\\
    \textsc{TailoredBench}  & \textbf{0.634} & \textbf{0.031} & \textbf{0.704} & \textbf{0.022}\\
    \bottomrule
    \end{tabular}
    \caption{Methods' Performance under Different Source-Target Model Similarity.}
    \label{tab:Source_Models_Type}
\end{table}

% \begin{table}[htbp]
%     \renewcommand\arraystretch{1}
%     \centering
%     % \small
%     \setlength{\tabcolsep}{0.65em} 
%     \begin{tabular}{ccc}
%     \toprule
%     \makecell{\textbf{Source} \\ \textbf{Models}} & 
%     \makecell{\textbf{w/o} \\ \textbf{Llama}} & 
%     \makecell{\textbf{with} \\ \textbf{Llama}} \\
%     \midrule
%     Llama    & 5.5 & 6.4 \\
%     Mistral  & 7.6 & 3.0 \\
%     Mistral  & 7.6 & 3.0 \\
%     \bottomrule
%     \end{tabular}
%     \caption{Statistics of native source model selection within model families on GSM8K benchmark.}
%     \label{tab:native_model_selection}
%     \vspace{-0.3cm}
% \end{table}

\subsection{Comprehensive G-set Size Evaluation Across Benchmarks}
\label{apd:gset}
In this section, we present a comprehensive evaluation of how varying G-set sizes affect our method’s performance across multiple benchmarks. Table \ref{apdtab:gset} reports Kendall's $\tau$ and MAE metrics for G-set sizes ranging from 5 to 25 for each benchmark while fixing the N-set size as 30. These results provide deeper insights into selecting the optimal G-set size and support the conclusions drawn in the main text.
% In this section, we present the comprehensive results of our experiment that evaluates the effect of varying G-set sizes on the performance of our method across multiple benchmarks. Table \ref{apdtab:gset} provides detailed Kendall's $\tau$ and MAE metrics for G-set sizes ranging from 5 to 25 across all datasets. These results offer deeper insights into the optimal selection of G-set size, supporting the conclusions drawn in the main text.

\begin{table*}
\renewcommand\arraystretch{1}
\centering
\setlength{\tabcolsep}{0.5em} 
\begin{tabular}{c *{5}{cc}}
\toprule
\multirow{2}{*}{\textbf{ $\lvert$G-set$\rvert$ }} & \multicolumn{2}{c}{ARC Challenge} & \multicolumn{2}{c}{Hellaswag} & \multicolumn{2}{c}{GSM8K} & \multicolumn{2}{c}{Winogrande} & \multicolumn{2}{c}{POPE} \\ 
\noalign{\vskip -0.17em}
 & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
\midrule
\textsc{5}  & 0.719 & 0.031 & \uline{0.912} & \uline{0.019} & \textbf{0.865} & 0.035 & \uline{0.624} & 0.026 & 0.549 & 0.037 \\
\textsc{10} & \textbf{0.756} & \textbf{0.028} & \textbf{0.913} & \textbf{0.018} & \uline{0.863} & \textbf{0.033} & 0.604 & \textbf{0.024} & \textbf{0.562} & \textbf{0.031} \\
\textsc{15} & \uline{0.751} & \uline{0.029} & 0.911 & \textbf{0.018} & 0.854 & \uline{0.034} & 0.608 & \uline{0.025} & \uline{0.556} & \uline{0.033} \\
\textsc{20} & 0.740 & \uline{0.029} & 0.910 & \textbf{0.018} & 0.862 & \uline{0.034} & 0.621 & 0.026 & 0.541 & 0.034 \\
\textsc{25} & 0.725 & 0.030 & 0.909 & \uline{0.019} & 0.851 & 0.036 & \textbf{0.638} & 0.026 & 0.533 & 0.036 \\
\bottomrule
\end{tabular}
\caption{Detailed results for G-set size across all benchmarks.}
\label{apdtab:gset}
\end{table*}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{table*}[ht]
% \begin{tabular}{l *{5}{cc}}
% \toprule
% \multirow{2}{*}{\textbf{Distance}} & \multicolumn{2}{c}{Average} \\ 
%  & \textbf{$\tau \uparrow$} & {\scriptsize \textbf{MAE} $\downarrow$}  \\ 
% \midrule
% {10(fix)+20
% |N-set|=30
% Inference counts=30}    & 0.740 & 0.027 \\
% {10(not-fix)+20
% |N-set|=20
% Inference counts=30}    & 0.719 & 0.031 \\
% {10(not-fix)+30
% |N-set|=30
% Inference counts=40}    & 0.745 & 0.027 \\
% \bottomrule
% \end{tabular}
% \end{table*}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%\newpage
\subsection{Demonstration of Method Effectiveness with Variance}
\label{apd:variance}
In this section, we present visual comparisons of our method and other approaches, including their respective variances, as illustrated in Figures \ref{Variance_arc} to \ref{Variance_pope}. The results demonstrate that our method outperforms the baseline methods on all datasets and exhibits greater robustness (with smaller variance).



%\clearpage
%\newpage
\subsection{More Analyses On the Impact of Native Source Model Quantity on Our Method}
\label{apd:quantity}
% 在这一部分，我们展示了我们的方法
In this section, we maintain the overall prediction consistency between the native source models and the target models constant, while varying the proportion of the source models designated as native source models from 20\% to 100\% for the target models across various benchmarks. The results are illustrated in Figures \ref{Quantity_arc} to \ref{Quantity_pope}, indicating that, under the condition of maintaining the prediction consistency between the native source models and the target model, the number of native source models significantly influences the method's performance.



%\clearpage
%\newpage
\subsection{More Analyses On the Impact of Native Source Models' Prediction Consistency on Our Method}
\label{apd:similarity}
We conduct ablation studies by selecting native source models based on their prediction consistency with the target model across various benchmarks, ranging from the top 20\% to the 80\%\textasciitilde100\% range. The results, presented in Figures \ref{Similarity_arc} to \ref{Similarity_pope}, indicate that the performance of the method significantly declines as the prediction consistency between the native source models and the target model decreases, under the condition of keeping the number of native source models constant.


%\clearpage
%\newpage
\subsection{Extended Results on Optimal Native Source Model Selection}
This section presents the results of our method as the number of native source models is incrementally increased based on their prediction consistency with the target model. The results in Figures \ref{mainanalysis_arc} to \ref{mainanalysis_pope} show that, overall, Kendall's $\tau$ initially increases and then decreases as the number of native source models increases, while the MAE initially decreases and then increases with the increase in the number of native source models. 

Moreover, Our method adaptively selects 45 native source models for the ARC Challenge benchmark, 40 for the Hellaswag benchmark, 33 for the Winogrande benchmark, and 35 for the POPE benchmark. These selections represent near-optimal numbers of native source models, as demonstrated in Figures \ref{mainanalysis_arc} to \ref{mainanalysis_pope}.
\label{apd:main_analysis}


%\clearpage

% \subsection{Why Our Method Behaviors different on Winogrande Benchmark?}
% \label{apd:winogrande_analysis}
% In Table \ref{tab:gset}, Our method seems behavior different with other benchmarks


\section{Models Used in Our Experiments}
\label{apd:modellist}
Tables \ref{apdtab:arc-model-list}, \ref{apdtab:hella-model-list}, \ref{apdtab:winoandGSM8K-model-list}, \ref{apdtab:pope-model-list} provide comprehensive lists of models corresponding to each benchmark.
\begin{table*}[htbp]
    \renewcommand{\arraystretch}{1.2}
    \centering
    \small
    \setlength{\tabcolsep}{0.5em} % Adjust column spacing
    \begin{tabular}{p{2cm}p{12cm}}
        \toprule
        \textbf{Benchmark} & \textbf{Model Names} \\
        \midrule
        ARC Challenge & Qwen2-72B-Instruct, Meta-Llama-3-70B-Instruct, Qwen2-72B, zephyr-orpo-141b-A35b-v0.1, Phi-3-medium-4k-instruct, Yi-1.5-34B-Chat, c4ai-command-r-plus, Qwen1.5-110B, Smaug-72B-v0.1, Qwen1.5-110B-Chat, Yi-1.5-9B-Chat, Qwen1.5-32B-Chat, Nous-Hermes-2-Mixtral-8x7B-DPO, deepseek-llm-67b-chat, Qwen1.5-32B, Yi-1.5-34B-32K, Meta-Llama-3-70B, Phi-3-mini-4k-instruct, mixtral-8x22B-v0.3, Mixtral-8x22B-v0.1, Phi-3-mini-128k-instruct, Yi-1.5-34B, c4ai-command-r-v01, Qwen2-7B-Instruct, Hermes-2-Theta-Llama-3-8B, aya-23-35B, Mixtral-8x7B-Instruct-v0.1, notux-8x7b-v1, Meta-Llama-3-8B-Instruct, Yi-34B-Chat, Smaug-34B-v0.1, Qwen2-7B, Nous-Hermes-2-SOLAR-10.7B, K2-Chat, Yi-1.5-9B-Chat-16K, Llama-3-Refueled, WizardLM-70B-V1.0, Yi-34B, Yi-1.5-6B-Chat, NeuralDaredevil-8B-abliterated, Yi-1.5-9B, Nous-Hermes-2-Mixtral-8x7B-SFT, Hermes-2-Pro-Mistral-7B, Hermes-2-Pro-Llama-3-8B, openchat\_3.5, neural-chat-7b-v3-2, OpenHermes-2-Mistral-7B, OpenHermes-2.5-Mistral-7B, Qwen1.5-14B-Chat, Nous-Hermes-2-Mistral-7B-DPO, neural-chat-7b-v3-1, Starling-LM-7B-alpha, Qwen1.5-14B, neural-chat-7b-v3-3, Yi-34B-200K, SOLAR-10.7B-Instruct-v1.0, Yi-1.5-9B-32K, Mixtral-8x7B-v0.1, Mistral-7B-Instruct-v0.3, zephyr-7b-alpha, Mistral-7B-Instruct-v0.2, dolphin-2.9-llama3-8b, Llama-2-70b-hf, Orca-2-13b, Llama-3-8B-Instruct-Gradient-1048k, neural-chat-7b-v3, zephyr-7b-beta, Mistral-7B-OpenOrca, Yi-9B, Yi-9B-200K, DeciLM-7B-instruct, gemma-1.1-7b-it, SOLAR-10.7B-v1.0, merlinite-7b, Qwen1.5-7B-Chat, 14B, Yi-1.5-6B, stablelm-2-12b-chat, aya-23-8B, zephyr-7b-gemma-v0.1, Yarn-Solar-10b-32k, phi-2, phixtral-2x2\_8, gemma-7b, Qwen1.5-7B, WizardLM-13B-V1.2, LLaMA-Pro-8B-Instruct, Yarn-Solar-10b-64k, DeciLM-7B, OrpoLlama-3-8B, Qwen1.5-MoE-A2.7B-Chat, deepseek-llm-7b-chat, Mistral-7B-v0.1, CollectiveCognition-v1.1-Mistral-7B, Mistral\_Pro\_8B\_v0.1, Mistral-7B-v0.3, Orca-2-7b, Mistral-7B-v0.2, Yi-6B-Chat, Qwen2-1.5B-Instruct, stablelm-2-12b, openchat\_v3.2, falcon-11B, Yi-6B, Mistral-7B-Instruct-v0.1, Yarn-Mistral-7b-64k, Meta-Llama-3-8B, Yarn-Mistral-7b-128k, gemma-7b-it, openchat\_v3.2\_super, Llama-2-70b-chat-hf, Qwen1.5-MoE-A2.7B, stablelm-zephyr-3b, Qwen1.5-4B-Chat, starcoder2-15b, OpenHermes-13B, MetaMath-Mistral-Pro, Yi-6B-200K, falcon-40b, Qwen1.5-4B, Llama-2-13b-chat-hf, Llama-2-13b-hf, vicuna-7b-v1.5, OLMo-7B-Instruct-hf, internlm2-chat-1\_8b, falcon-40b-instruct, Qwen2-1.5B, deepseek-moe-16b-chat, OpenHermes-7B, Llama-2-7b-chat-hf, Nous-Hermes-llama-2-7b, stablelm-2-zephyr-1\_6b, Qwen1.5-1.8B, Qwen1.5-1.8B-Chat, LLaMA-Pro-8B, Llama-2-7b-hf, stablelm-2-1\_6b-chat, internlm2-1\_8b, Yarn-Llama-2-13b-128k, NexusRaven-V2-13B, starcoder2-7b, Llama-2-7B-32K-Instruct, deepseek-llm-7b-base, recurrentgemma-2b-it, gemma-1.1-2b-it, granite-7b-base, deepseek-moe-16b-base, gemma-2b, stablelm-3b-4e1t, gemma-2b-it, Yarn-Llama-2-7b-64k, Qwen2-0.5B, phi-1\_5 \\
        \bottomrule
    \end{tabular}
    \caption{Models used for ARC Challenge benchmark.}
    \label{apdtab:arc-model-list}
\end{table*}


\begin{table*}[htbp]
    \renewcommand{\arraystretch}{1.2}
    \centering
    \small
    \setlength{\tabcolsep}{0.5em} % Adjust column spacing
    \begin{tabular}{p{2cm}p{12cm}}
        \toprule
        \textbf{Benchmark} & \textbf{Model Names} \\
        \midrule
        HellaSwag & LLaMAntino-3-ANITA-8B-Inst-DPO-ITA, luxia-21.4b-alignment-v1.0, UNA-ThePitbull-21.4-v1, T3Q-ko-solar-dpo-v6.0, MultiVerse\_70B, RoleBeagle-11B, Capricorn-7B-DPO, Tess-2.0-Llama-3-70B, Truthful\_DPO\_MOE\_19B, multimaster-7b-v5, guanaco-65B-HF, FusionNet\_34Bx2\_MoE\_v0.1, Mixtral-8x7B-v0.1, Evangelion-7B, Lumina-5.5-Instruct, Mistral-Hermes-2x7b, Bagel-Hermes-2x34B, shqiponja-15b-v1, CollectiveCognition-v1.1-Mistral-7B-dare-0.85, etri-ones-solar, mpt-30b-instruct, openbuddy-mixtral-7bx8-v18.1-32k, bagel-dpo-7b-v0.4, OpenHermes-2.5-Mistral-7B, NeuralHermes-2.5-Mistral-7B, dolphin-2.1-mistral-7b-snr-math-laser, NeuralHermes-2.5-Mistral-7B, openbuddy-qwen1.5-32b-v21.1-32k, internlm2-20b-llama, Matter-0.2-7B-DPO, airoboros-13b-gpt4-1.2, L3-SnowStorm-v1.15-4x8B-B, Pallas-0.5-LASER-0.6, BgGPT-7B-Instruct-v0.1, Seagull-llama-3-8B-orpo-v0.5, vigogne-7b-instruct, Llama-2-7b-chat-hf-activity-fine-tuned-v4, Llama-2-7b-chat-hf-activity-fine-tuned-v3, vicuna-class-tutor-7b-ep3, Llama-2-7b-chat-hf-afr-200step-flan-v2, llama3-8b-instruct-align-test1-kto, MFANN3bv0.7, openbuddy-yi1.5-9b-v21.1-32k, openbuddy-mixtral-7bx8-v17.1-32k, odia\_llama2\_7B\_base, MT7Bi-alpha-dpo-v0.2, llama-shishya-7b-ep3-v2, Instruct\_Yi-6B\_Dolly15K, Gaja-v2.00-dpo, phi-2-OpenHermes-2.5, lion-gemma-7b-cn-v2, ToRoLaMa-7b-v1.0, gogpt-7b, Amber, open\_llama\_3b\_v2, openllama\_3b\_EvolInstruct\_lora\_merged, gemma-7B-it-firefly, Qwen1.5-4B, google-gemma-7b-it-dpo-v1, openhermes-2b-gemma-sft-qlora, RedPajama-INCITE-Chat-3B-v1, mistral\_v1, gpt-j-6b, GPT-J-Pyg\_PPO-6B, ScarletPajama-3B-HF, LLama2-7B-Structural-Prune-1.5x, illuni-llama-2-ko-7b-test, RedPajama-INCITE-Chat-3B-ShareGPT-11K, RedPajama-INCITE-Base-3B-v1, Guanaco-3B-Uncensored-v2-GPTQ, glaive-coder-7b, xglm-7.5B, gpt-sw3-6.7b, cisco-iNAM-1.1B, pythia-2.7b, qd-phi-1\_5, pythia-2.8b-deduped, LLmRa-2.7B, Tinyllama-1.3B-Cinder-Reason-Test-2, TinyPoliticaLlama-1.1B, Galpaca-30b-MiniOrca, finetune\_test\_qwen15-1-8b-sft-lora, TinyLlama-1.1B-Chat-v0.3, TinyLlama-1.1B-Chat-v0.1, CroissantLLMBase, pygmalion-2.7b, blossom-v2-3b, falcon\_1b\_stage3, MiniMerlin-3b-v0.1, DPO-miniguanaco-1.5T, CodeQwen1.5-7B-Chat, yayi2-30b-llama, rho-math-1b-v0.1, LLmRa-1.3B\_V2, TinyLlama-1.1B-intermediate-step-480k-1T, gemma-2b-ko-dev-pbmt192, gpt2-chatbot, CodeLlama-7b-Python-hf, Deita-500m, TinyWand-SFT, tinyllama-coder-py-v13, d-Qwen1.5-1.8B, TinyLlama-1.1B-intermediate-step-240k-503b, dlite-v1-1\_5b, pythia-1b-deduped, gpt2-large, WizardCoder-Guanaco-15B-V1.0, Qwen1.5-0.5B-vortex-v2, Sailor-0.5B-Chat, WizardCoder-Guanaco-15B-V1.1, Alpaca\_refine\_gpt2\_e1\_se0, deepseek-coder-1.3b-chat, speechless-coder-ds-1.3b, Instruct\_GPT, deepseek-coder-1.3b-chat-and-function-calling, megatron-gpt2-345m, starcoderbase-3b, dlite-v1-355m, gov-qna-ko-merged, SSH\_355M, CodeLlama-34b-Instruct-hf, CodeLlama-34B-Instruct-fp16, mptk-1b, KoAlpaca-Polyglot-5.8B, Llama-160M-Chat-v1, llama-160m, CodeLlama-34b-hf, KoAlpaca-KoRWKV-6B, Quokka\_590m, pruned-yi-3b-prerelease-ckpt01, gpt2\_test, finetuned-gpt2-tiny, Kaori-34b-v2, kaori-34b-v4, tiny\_starcoder\_py, GPT-2-Large-51k-steps, DialoGPT-small, test\_mistral2, pythia-31m-KI\_v1-2048-scratch \\
        \bottomrule
    \end{tabular}
    \caption{Models used for Hellaswag benchmark.}
    \label{apdtab:hella-model-list}
\end{table*}


\begin{table*}[htbp]
    \renewcommand{\arraystretch}{1.2}
    \centering
    \small
    \setlength{\tabcolsep}{0.5em} % Adjust column spacing
    \begin{tabular}{p{2cm}p{12cm}}
        \toprule
        \textbf{Benchmark} & \textbf{Model Names} \\
        \midrule
        GSM8K \& Winogrande & ExtremeDolphin-MoE, Mistral-7B-Instruct-v0.2-sparsity-20, PiVoT-SUS-RP, polyglot-math-4x7b, SOLAR-10B-Nector-DPO-Jawade, Starling-LM-11B-alpha, NeuralPipe-7B-slerp, oswald-7b, MistralTrixTest, Sensualize-Mixtral-bf16, Sensualize-Solar-10.7B, FusionNet\_passthrough, finance-chat, Kunoichi-7B, dolphin-2.2.1-mistral-7b, CarbonVillain-en-10.7B-v3, xDAN-SlimOrca, Mistral-11B-v0.1, dm7b\_sft\_gpt88w\_merge, Loyal-Macaroni-Maid-7B, Yi-34B-200K-DARE-merge-v5, WinterGoddess-1.4x-70B-L2, vicuna-class-shishya-ac-hal-13b-ep3, Kaori-34B-v1, mistral-megamerge-dare-7b, Chupacabra-8x7B-MoE, bagel-7b-v0.1, Mixtral-8x7B-v0.1, openbuddy-deepseek-67b-v15-base, Falkor-7b, synapsellm-7b-mistral-v0.4-preview3, llama2-13b-ft-openllm-leaderboard-v1, synapsellm-7b-mistral-v0.3-preview, Tess-M-v1.3, monika-ddlc-7b-v1, speechless-mistral-7b-dare-0.85, mistral-7b-v0.1-layla-v1, Mistral-v0.1-PeanutButter-v0.0.2-7B, chronos-70b-v2, L2-7b-Beluga-WVG-Test, llama-2-13b-FINETUNE3\_3.3w-r8-gate\_up\_down, airoboros-c34b-2.2.1, llama-2-13b-FINETUNE4\_3.8w-r8-q\_k\_v\_o, llama-2-13b-FINETUNE3\_3.3w-r16-gate\_up\_down, MLewd-Chat-v2-13B, Mistral-7B-v0.1-Open-Platypus, llama-2-13b-FINETUNE1\_17w-r4, EverythingLM-13b-V3-peft, Llama2-7B-guanaco-1k, llama-2-13b-FINETUNE4\_3.8w-r8-q\_k\_v\_o\_gate\_up\_down, Koss-7B-chat, ReMM-v2.2-L2-13B, WizardLM-1.0-Uncensored-CodeLlama-34b, airoboros-13b, airoboros-7b-gpt4-1.4.1-qlora, Wizard-Vicuna-7B-Uncensored-HF, Luban-Platypus2-13B-QLora-0.80-epoch, CodeLlama-34b-hf, airoboros-33b-gpt4-m2.0, llama2-22b-blocktriangular, GPT-JT-6B-v0, llama2-70b-oasst-sft-v10, vigogne-7b-instruct, based-30b, mpt-30b-chat, qCammel-70x, GiftedConvo13bLoraNoEconsE4, llama-2-13b-platypus-vicuna-wizard, GOAT-7B-Community, genz-13b-v2, chronolima-airo-grad-l2-13B, Vicuna-13B-CoT, Llama-2-7b-ft-instruct-es, OpenOrca-Preview1-13B, Tulpar-7b-v0, zephyr-7b-sft-full, Mixtral-Orca-v0.1, Marcoroni-7b-DPO-Merge, Aquila2-34B, SOLAR-10.7B-Instruct-v1.0-128k, dolphin-2.6-mistral-7b-dpo-orca-v3, flux-7b-v0.1, Turdus, A0110, yayi2-30b-llama, NeuralMarcoro14-7B, Deacon-34b-Adapter, test0, Pallas-0.5-LASER-0.4, Marcoro14-7B-ties, Antares-11b-v1, CodegebraGPT-10b, Mistral-Syndicate-7B, Nous-Hermes-2-Yi-34B, Half-NSFW\_Noromaid-7b, neural-chat-7b-v3-3-wizardmath-dare-me, apricot-wildflower-20, SauerkrautLM-UNA-SOLAR-Instruct, kalomaze-stuff, Walter-Mistral-7B, Starling-LM-alpha-8x7B-MoE, una-neural-chat-v3-3-P2-OMA, Dans-07YahooAnswers-7b, Chupacabra-7B-v2.03, PlatYi-34B-200K-Q, chinese-alpaca-2-13b-16k, ALMA-7B-Ja-V2, speechless-code-mistral-7b-v2.0, Mistral7B\_adaptor\_v1, notus-7b-v1, Chupacabra-7B-v2, SciPhi-Self-RAG-Mistral-7B-32k, Ferret-7B, llama-2-13B-instructed, glaive-coder-7b, Mistralic-7B-1, kuchiki-l2-7b, llama\_7b\_lora, Slerpeno, Llama2-7b-openorca-mc-v2-dpo, CAMEL-13B-Role-Playing-Data, starchat-beta, testmodel2, Huginn-13b-v1.2, Dans-AdventurousWinds-7b, Wizard-Vicuna-13B-Uncensored-HF, Llama-2-13b-hf-ds\_wiki\_1024\_full\_r\_64\_alpha\_16\_merged, Emerald-13B, koala-13B-HF, tulu-7B-fp16, airoboros-c34b-2.1, airoboros-7b-gpt4-1.1, 13B-Chimera, Nous-Hermes-Platypus2-13B-QLoRA-0.80-epoch, airoboros-l2-7b-gpt4-m2.0, llama-7b, llama-65b-instruct, Flash-Llama-7B, StableBeluga-13B, huginnv1.2, llama\_13b\_sharegpt94k\_fastchat, CAMEL-13B-Combined-Data, MelangeC-70b, chronos-13b-v2, stack-llama-2, CodeLlama-34b-Python-hf, UltraLM-65b, Platypus-30B, bimoGPT-llama2-13b, test-llama2-7b \\
        \bottomrule
    \end{tabular}
    \caption{Models used for GSM8K and Winogrande benchmark.}
    \label{apdtab:winoandGSM8K-model-list}
\end{table*}


\begin{table*}[htbp]
    \renewcommand{\arraystretch}{1.2}
    \centering
    \small
    \setlength{\tabcolsep}{0.5em} % Adjust column spacing
    \begin{tabular}{p{2cm}p{12cm}}
        \toprule
        \textbf{Benchmark} & \textbf{Model Names} \\
        \midrule
        POPE & InternVL2-76B, paligemma-3b-mix-448, InternVL-Chat-V1-5, cambrian\_13b, cogvlm-chat, CloudWalk, Ovis1.5-Gemma2-9B, cambrian\_8b, InternVL2-26B, Ovis1.5-Llama3-8B, llava\_next\_vicuna\_13b, glm-4v-9b, emu2\_chat, llava\_next\_mistral\_7b, llava\_next\_vicuna\_7b, WeMM, cambrian\_34b, llava\_next\_llama3, 360VL-70B, Bunny-llama3-8B, GLM4V, MiniCPM-V-2, llava\_next\_qwen\_32b, Yi-Vision, InternVL2-2B, GeminiPro1-5, InternVL2-8B, llava\_next\_interleave\_7b\_dpo, XComposer2d5, MiniCPM-V-2\_6, Mini-InternVL-Chat-2B-V1-5, cogvlm2-llama3-chat-19B, llava\_next\_yi\_34b, Step1V, InternVL2-1B, InternVL2-4B, Phi-3-Vision, llava\_next\_interleave\_7b, monkey-chat, OmniLMM\_12B, InternVL2-40B, idefics2\_8b, deepseek\_vl\_7b, GPT4o\_20240806, sharecaptioner, monkey, llava-v1.5-7b-xtuner, GPT4o\_HIGH, RekaEdge, GPT4o, Mantis-8B-Idefics2, MiniCPM-Llama3-V-2\_5, llava-llama-3-8b, sharegpt4v\_7b, Mini-InternVL-Chat-4B-V1-5, llava-internlm-7b, llava-v1.5-13b-xtuner, sharegpt4v\_13b, llava\_v1.5\_7b, GPT4o\_MINI, deepseek\_vl\_1.3b, RekaFlash, llava\_v1.5\_13b, Mantis-8B-siglip-llama3, MiniCPM-V, QwenVLPlus, Mantis-8B-clip-llama3, Yi\_VL\_6B, llava-internlm2-20b, XComposer2\_1.8b, mPLUG-Owl2, GPT4V, Yi\_VL\_34B, llava-internlm2-7b, Claude3-5V\_Sonnet, MMAlaya, instructblip\_7b, XComposer2, XComposer2\_POPE\_TEST, TransCore\_M, Claude3V\_Haiku, Claude3V\_Sonnet, Claude3V\_Opus, idefics\_9b\_instruct, chameleon\_30b, QwenVLMax, qwen\_chat, llava\_v1\_7b, PandaGPT\_13B, qwen\_base, XComposer, MiniGPT-4-v1-7B, VisualGLM\_6b, flamingov2, MiniGPT-4-v2, VXVERSE, idefics\_80b\_instruct, chameleon\_7b, XComposer2\_4KHD \\
        \bottomrule
    \end{tabular}
    \caption{Models used for POPE benchmark.}
    \label{apdtab:pope-model-list}
\end{table*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/arc_modelvariance_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/arc_modelvariance_MAE.pdf}
  \caption {Demonstration of method effectiveness with variance on ARC Challenge benchmark.}
\vspace{-0.3cm}
\label{Variance_arc}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/hella_modelvariance_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/hella_modelvariance_MAE.pdf}
  \caption {Demonstration of method effectiveness with variance on Hellaswag benchmark.}
\vspace{-0.3cm}
\label{Variance_hella}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/GSM8K_modelvariance_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/GSM8K_modelvariance_MAE.pdf}
  \caption {Demonstration of method effectiveness with variance on GSM8K benchmark.}
\vspace{-0.3cm}
\label{Variance_GSM8K}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/winogrande_modelvariance_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/winogrande_modelvariance_MAE.pdf}
  \caption {Demonstration of method effectiveness with variance on winogrande benchmark.}
\vspace{-0.3cm}
\label{Variance_winogrande}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/POPE_modelvariance_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelvariance/POPE_modelvariance_MAE.pdf}
  \caption {Demonstration of method effectiveness with variance on POPE benchmark.}
\vspace{-0.3cm}
\label{Variance_pope}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelratio/arc_modelratio_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelratio/arc_modelratio_MAE.pdf}
  \caption {The impact of the quantity of Native Source Models (with
prediction consistency kept the same) on ARC Challenge benchmark.}
\vspace{-0.3cm}
\label{Quantity_arc}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelratio/hella_modelratio_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelratio/hella_modelratio_MAE.pdf}
  \caption {The impact of the quantity of Native Source Models (with prediction consistency kept the same) on Hellaswag benchmark.}
\vspace{-0.3cm}
\label{Quantity_hellaswag}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelratio/GSM8K_modelratio_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelratio/GSM8K_modelratio_MAE.pdf}
  \caption {The impact of the quantity of Native Source Models (with prediction consistency kept the same) on GSM8K benchmark.}
\vspace{-0.3cm}
\label{Quantity_GSM8K}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelratio/winogrande_modelratio_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelratio/winogrande_modelratio_MAE.pdf}
  \caption {The impact of the quantity of Native Source Models (with prediction consistency kept the same) on Winogrande benchmark.}
\vspace{-0.3cm}
\label{Quantity_winogrande}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelratio/POPE_modelratio_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelratio/POPE_modelratio_MAE.pdf}
  \caption {The impact of the quantity of Native Source Models (with prediction consistency kept the same) on POPE benchmark.}
\vspace{-0.3cm}
\label{Quantity_pope}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/arc_similarity_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/arc_similarity_MAE.pdf}
  \caption {The impact of prediction consistency between the Native Source Model and Target Model (with quantity kept the same) on ARC Challenge benchmark.}
\vspace{-0.3cm}
\label{Similarity_arc}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/hella_similarity_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/hella_similarity_MAE.pdf}
  \caption {The impact of prediction consistency between the Native Source Model and Target Model (with quantity kept the same) on Hellaswag benchmark.}
\vspace{-0.3cm}
\label{Similarity_hella}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/GSM8K_similarity_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/GSM8K_similarity_MAE.pdf}
  \caption {The impact of prediction consistency between the Native Source Model and Target Model (with quantity kept the same) on GSM8K benchmark.}
\vspace{-0.3cm}
\label{Similarity_GSM8K}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/winogrande_similarity_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/winogrande_similarity_MAE.pdf}
  \caption {The impact of prediction consistency between the Native Source Model and Target Model (with quantity kept the same) on Winogrande benchmark.}
\vspace{-0.3cm}
\label{Similarity_winogrande}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/POPE_similarity_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/modelsimilarity/POPE_similarity_MAE.pdf}
  \caption {The impact of prediction consistency between the Native Source Model and Target Model (with quantity kept the same) on POPE benchmark.}
\vspace{-0.3cm}
\label{Similarity_pope}
\end{figure*}

\begin{figure*}[htbp]
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/arc_modelnum_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/arc_modelnum_MAE.pdf}
  \caption {Performance of \textsc{TailoredBench} with varying numbers of Native Source Models on ARC Challenge benchmark.}
\vspace{-0.3cm}
\label{mainanalysis_arc}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/hella_modelnum_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/hella_modelnum_MAE.pdf}
  \caption {Performance of \textsc{TailoredBench} with varying numbers of Native Source Models on Hellaswag benchmark.}
\vspace{-0.3cm}
\label{mainanalysis_hella}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/winogrande_modelnum_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/winogrande_modelnum_MAE.pdf}
  \caption {Performance of \textsc{TailoredBench} with varying numbers of Native Source Models on Winogrande benchmark.}
\vspace{-0.3cm}
\label{mainanalysis_winogrande}
\end{figure*}

\begin{figure*}[ht]
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/POPE_modelnum_kendall.pdf} \hfill
  \includegraphics[width=0.49\linewidth]{figs/main_analysis/POPE_modelnum_MAE.pdf}
  \caption {Performance of \textsc{TailoredBench} with varying numbers of Native Source Models on POPE benchmark.}
\vspace{-0.3cm}
\label{mainanalysis_pope}
\end{figure*}