We first show the result suggesting it is possible to identify a distribution through its noisy version obtained by corrupting its samples by injecting independent Gaussian noises. 
\PROPCONVINDENTIFY*

\begin{lemma}
\label{appx:lem:kl_bd_char}
	Given two distributions $p$ and $q$ on $\Rb^d$. Let $\Phi_p(\uv)$ and $\Phi_q(\uv)$ be their characteristic functions. Then for all $\uv \in \Rb^d$, we have
	\begin{align}
			\bigl|\Phi_p(\uv) - \Phi_q(\uv)\bigr|
			\;\le\;
			\sqrt{2 \,D_{\mathrm{KL}}(p\;\|\;q)}.
	\end{align}
\end{lemma}


\begin{proof}
We note that 
\[
\Phi_p(\uv) \;=\; \mathbb{E}_p[\exp(i\uv^\top \xv)],
\quad
\Phi_q(\uv) \;=\; \mathbb{E}_q[\exp(i\uv^\top \xv)].
\]
Then for any $\uv \in \Rb^d$, we have
\begin{align*}
\bigl|\Phi_p(\uv) - \Phi_q(\uv)\bigr|
& \le
\left|\int_{\Rb^d} \exp(i\uv^\top\xv) p(\xv) \diff \xv - \int_{\Rb^d} \exp(i\uv^\top\xv) q(\xv) \diff \xv \right| \\
& = \left|\int_{\Rb^d} \exp(i\uv^\top\xv) \Big(p(\xv) -  q(\xv) \Big) \diff \xv \right|  \leq \int_{\Rb^d} \underbrace{\left|\exp(i\uv^\top\xv)\right|}_{=1}  \left| p(\xv)  -  q(\xv) \right| \diff \xv \\
& = \int_{\Rb^d}  \left| p(\xv)  -  q(\xv) \right| \diff \xv \\
& = 2 \; \|p - q\|_{\mathrm{TV}},
\end{align*}
where the last equality is due to Scheffe's theorem \citep[Lemma 2.1, p. 84]{Tsybakov09}). 


Then, by Pinsker's inequality \citep[Lemma 2.5, p. 88]{Tsybakov09}, we have 
\[
\bigl|\Phi_p(\uv) - \Phi_q(\uv)\bigr|
\;\le\;
2 \; \|p - q\|_{\mathrm{TV}}
\;\le\;
\sqrt{2\,D_{\mathrm{KL}}(P\;\|\;Q)}.
\]
which completes the proof. 
\end{proof}
\begin{proof}[Proof of \cref{prop:conv_identify}]
	Note that, by the convolution theorem \citep[A.4]{Meister2009},  for all $\uv \in \Rb^d$, we have 
	\begin{align*}
		\Phi_{p * h}(\uv) = \Phi_{p}(\uv) \; \Phi_{h}(\uv) = \Phi_{p}(\uv) \; \exp \big(-\frac{\sigma_\zeta^2}{2} \|\uv\|^2 \big),
	\end{align*}
	as $h\sim \Nc(\mathbf{0}, \sigma_\zeta^2\mathbf{I})$ having $\Phi_h(\uv) = \exp \big(-\frac{\sigma_\zeta^2}{2} \|\uv\|^2 \big)$. Applying \cref{appx:lem:kl_bd_char}, we have
	\begin{align}
		\exp \big(-\frac{\sigma_\zeta^2}{2}\|\uv\|^2\big) \; \Big|\Phi_{p}(\uv) - \Phi_{q}(\uv)\Big|  = \big|\Phi_{p * h}(\uv) - \Phi_{q * h}(\uv) \big| \leq \sqrt{2\,D_{\mathrm{KL}}(p \conv h\|q\conv h)}. 
	\end{align}	
	Rearranging the inequality completes the proof. 
\end{proof}


We then derive the proofs regarding the sample complexity of the deconvolution problem. 
\MISEUpperBdound*
\begin{proof}
    The result is constructed based on the work by \citet{StefanskiC1990}. In particular, assuming that $p_\text{data}$ is continuous, bounded and has two bounded integrable derivatives such that 
    \begin{align}
        \int p''_\text{data} (x) \diff x < \infty,
    \end{align}
    we can construct a kernel based estimator of $p_\text{data}$ of rate
    \begin{align}
        \frac{\lambda^4}{4} \mu^2_{K, 2} \int p''_\text{data} (x) \diff x,
    \end{align}
    where $\mu^2_{\kappa, 2}$ is a constant determined by the selected kernel $\kappa$ and $\lambda$ is a function of number of samples $n$ gradually decreasing to zero as $n \rightarrow \infty$. It is required that $\lambda$ satisfies
    \begin{align}
        \frac{1}{2 \pi n \lambda} \exp( \frac{B^2 \sigma_\zeta^2}{\lambda^2}) \rightarrow 0 \label{appx:eq:mise_upper_bound:1}
    \end{align}
    as $n \rightarrow \infty$, where $B > 0$ is a constant depending on the picked kernel $\kappa$. Here, we assume we picked a kernel with $B < 1$.    
    
    To satisfy the constraint, we choose $\lambda (n) = \frac{\sigma_\zeta}{\sqrt{\log n}}$. Plugging it into \cref{appx:eq:mise_upper_bound:1}, we have 
    \begin{align}
        \lim_{n \rightarrow \infty} \frac{1}{n \lambda} \exp( \frac{B^2 \sigma_\zeta^2}{\lambda^2}) = \lim_{n \rightarrow \infty} \frac{\sqrt{\log n}}{n {\sigma_\zeta }} \exp{(B^2 \log n)} = \lim_{n \rightarrow \infty} \frac{\sqrt{\log n}}{n^{1-B^2} {\sigma_\zeta }}. 
    \end{align}
    To show $\lim_{n \rightarrow \infty} \frac{\sqrt{\log n}}{n^{1-B^2} {\sigma_\zeta }} = 0$, it suffices to show 
    $
        \lim_{n \rightarrow \infty} \frac{\log n}{n^{2 - 2 B^2} {\sigma_\zeta^2 }} = 0
    $. By L'Hopital's rule, we have
    \begin{align}
        \lim_{n \rightarrow \infty} \frac{\log n}{n^{2 - 2 B^2} {\sigma_\zeta^2 }} =  \lim_{n \rightarrow \infty} \frac{1}{(2 - 2 B^2) n^{2 - 2 B^2} {\sigma_\zeta^2 }} = 0
    \end{align}
    As a result, $\lambda (n) = \frac{\sigma_\zeta}{\sqrt{\log n}}$ is a valid choice, which gives the convergence rate
    $\frac{\sigma_\zeta^4}{(\log n)^2}$.
\end{proof}

\MISELowerBound*
\begin{proof}
    This result is a special case of Theorem 2.14 (b) in \citep{Meister2009}. When the error density is Gaussian, we have $\gamma = 2$. In addition, in the proof of \cref{thm:MISE_upper_bound}, we assumed that $p_\text{data}$ has two bounded integrable derivatives, which equivalently assumes $p_\text{data}$ satisfies the Soblev condition with smoothness degree $\beta = 2$ (see Eq. A.8,  \citealt{Meister2009}). Then the theorem shows $\mathrm{MISE}(\hat{p}, p_\textrm{data}) \geq \textrm{const} \cdot (\log n)^{-2\beta / \gamma} = \textrm{const} \cdot (\log n)^{-2}$. 
\end{proof}





