The rise of large diffusion models trained on massive datasets has sparked growing concerns about copyright infringement and memorization of training data \citep{CarliniHNJSTBIDW2023, SomepalliSGGG2023}. While differential privacy (DP) has been explored as a mitigation strategy \citep{AbadiCGMMTZ2016, XieLWWR2018, DockhornCVK2023}, it often presents practical challenges. Notably, DP can require users to share their original data with a central server for training unless local devices have sufficient computational power for backpropagation.

% In contrast, training with corrupted data offers a compelling alternative. By sharing only non-invertible, corrupted versions of their data, users can contribute to model training without exposing their original data. This approach eliminates the need for data transmission of original data and ensures that sensitive information remains entirely on users' devices.

In contrast, training on corrupted data provides a compelling alternative, allowing users to contribute without exposing their original data. By sharing only non-invertible, corrupted versions, sensitive information remains on usersâ€™ devices, eliminating the need to transmit original data.

Learning generative models from corrupted data poses a significant challenge, as the model must reconstruct the underlying data distribution from incomplete or noisy information. In their work on AmbientGAN, \citet{BoraPD2018} showed that it is empirically feasible to train GANs using corrupted images. They also provided a theoretical guarantee that, with a sufficient number of corrupted samples generated by randomly blacking out pixels, the learned distribution converges to the true data distribution. Building on this, \citet{WangZHCZ2023} demonstrated a closely related result: under certain weak assumptions, if the model-generated fake samples and the corrupted true samples share the same distribution after undergoing identical corruption, then the fake data distribution aligns perfectly with the true data distribution. Their analysis applies to scenarios where corruption is implemented via a forward diffusion process but does not address cases where the two corrupted distributions are similar but not identical -- a case we explore in \cref{prop:conv_identify} below.

% Inspired by the success of training GANs using corrupted data, \citet{DarasSDGDK2023, AaliAKT2023, DarasA2023, BaiWCS2024, DarasDD2024} demonstrated the feasibility of training diffusion models with corrupted data. Importantly, when corruption is implemented through a forward diffusion, \citet{DarasDD2024} showed that when the consistency constraints are satisfied, the diffusion models must recover the distribution of the samples below the observed corrupted data noise; therefore, the model has to be the same as the one trained on the clean data as long as the model perfectly learns the distributions above the corruption noise level. As a result, they propose a consistency loss to guide the model to satisfy the consistency constraints better, though they only show its effectiveness in fine-tuning latent diffusion models. 

Inspired by the success of training GANs using corrupted data, \citet{DarasSDGDK2023, AaliAKT2023, DarasA2023, BaiWCS2024, DarasDD2024} demonstrated the feasibility of training diffusion models with corrupted data. Notably, \citet{DarasDD2024} showed that when corruption is performed by a forward diffusion process, the marginal distribution at one time step determines the distributions at other time steps, all of which must satisfy certain consistency constraints. Building on this, they showed that if a model learns distributions above the corruption noise level, it can infer those below the noise level by adhering to these constraints. To enforce this, they introduced a consistency loss to improve compliance with the constraints, though its effectiveness was demonstrated only in fine-tuning latent diffusion models.


Outside the field of machine learning, the problem of estimating the original distribution from noisy samples has traditionally been addressed through density deconvolution~\citep{Meister2009}. This research area aims to recover the distribution of error-free data from noise-contaminated observations. Most existing deconvolution methods are limited to the univariate setting \citep{CarrollHall88, Zhang1990, Fan91, CordyT1997, DelaigleH2008, MeisterN2010, LouniciN2011, Guan2021}, with only a few approaches extending to the multivariate case. These multivariate techniques typically rely on normal mixture models \citep{BovyHR2011, SarkarPCMC2018} or kernel smoothing methods \citep{Masry1993, LepskiW2019}. Integrating these theoretical insights into modern generative model frameworks remains a significant challenge. However, by reinterpreting generative models trained on noisy data through the lens of deconvolution theory, we can gain a deeper understanding of their fundamental limitations and capabilities, as they inherently address the deconvolution problem.
