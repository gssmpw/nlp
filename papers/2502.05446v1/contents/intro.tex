Diffusion-based generative models \citep{DicksteinWMG2015,HoJA2020, SongME2021, SongDCKKEP2021, SongDCS2023} have gained increasing attention. Nowadays, it is considered one of the most powerful frameworks for learning high-dimensional distributions and we have witnessed many impressive breakthroughs  \citep{CroitoruHIS2023} in generating images \citep{HoJA2020, SongME2021, SongDCKKEP2021, RombachBLEO2022, SongDCS2023}, audios \citep{kong2021diffwave, YangYWWWZY2023} and videos \citep{HoSGCNF2022}. 

Due to some inherent properties, diffusion models are relatively easier to train. This unlocks the possibility of training very large models on web-scale data, which has been shown to be critical to train powerful models. This paradigm has recently led to impressive advances in image generation, as demonstrated by cutting-edge models like Stable Diffusion (-XL) \citep{RombachBLEO2022, PodellELBDMPR2023} and DALL-E (2, 3) \citep{BGJBWLOZLG2023}. However, despite their success, the reliance on extensive web-scale data introduces challenges. The complexities of the datasets at such a scale often result in the inclusion of copyrighted content. Furthermore, diffusion models exhibit a greater tendency than earlier generative approaches, such as Generative Adversarial Networks (GANs) \citep{GoodfellowPMXWOCB2014, GoodfellowPMXWOCB2020}, to memorize training examples. This can lead to the replication of parts or even entire images from their training sets \citep{CarliniHNJSTBIDW2023, SomepalliSGGG2023}.

A recently proposed approach to address memorization and copyright concerns involves training (or fine-tuning) diffusion models using corrupted samples~\cite{DarasSDGDK2023, SomepalliSGGG2023, DarasA2023, DarasDD2024}. In this framework, the model is never exposed to the original samples during training. Instead, these samples undergo a known non-invertible corruption process, such as adding independent Gaussian noise to each pixel in image datasets. This ensures that the model cannot memorize or reproduce the original content, as the corruption process is irreversible for individual samples. 
 
Interestingly, under mild assumptions, certain non-invertible corruption processes, such as Gaussian noise injection, create a mathematical bijection between the noisy and original distributions. Thus, in theory, a generative model can learn the original distribution using only noisy samples \citep{BoraPD2018}. Building on this concept, 
\citet{DarasDD2024} demonstrated that when an image is corrupted via a forward diffusion up to a specific noise level $\sigma$, diffusion models can recover distributions at noise levels below $\sigma$ by enforcing consistency constraints \citep{DarasDDD2023}.

While \citet{DarasDD2024} empirically showed that their approach could be used to fine-tune Stable Diffusion XL~\citep{PodellELBDMPR2023} using noisy images with a heuristic consistency loss, they did not explore whether a diffusion model can be successfully trained solely with noisy images. Moreover, the effectiveness of the consistency loss in such scenarios remains an open question. 

In this paper, we address these questions by connecting the task of estimating the original distribution from noisy samples to the well-studied density deconvolution problem \citep{Meister2009}. Through the lens of deconvolution theory, we establish that the optimal convergence rate for estimating the data density is $\Oc(\log n)^{-2}$ when $n$ noisy samples are generated via a forward diffusion process. This pessimistic rate suggests that while it is theoretically feasible to learn the data distribution from noisy samples, the practical challenge of collecting sufficient samples makes successful learning nearly unattainable. Our empirical studies further validate this theoretical insight and suggest the inefficiency of the current consistency loss outside the regime of fine-tuning latent diffusion models.


To address the poor convergence rate in training diffusion models with noisy data, we propose pretraining models on a small subset of copyright-free clean data as an effective solution. Since the current consistency loss remains ineffective even with pretraining, we propose a new deconvolution method, Stochastic Forwardâ€“Backward Deconvolution (SFBD, pronounced \texttt{sofabed}), that is fully compatible with the existing diffusion training framework. Experimentally, we achieve an FID of 6.31 with just 4\% clean images on CIFAR-10 and 3.58 with 10\% clean images. Our theoretical results ensure that the learnt distribution converges to true data distribution and justifies the necessity of pretraining. Furthermore, our results suggest that models can be pretrained using datasets with similar features when clean, copyright-free data are unavailable. Ablation studies provide additional evidence supporting our claims.
