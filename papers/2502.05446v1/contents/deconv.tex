In this section, we evaluate the complexity of a deconvolution problem when the data corruption process is modelled using a forward diffusion process. Through the framework of deconvolution theory, we demonstrate that while \citet{DarasDD2024} showed that diffusion models can be trained using noisy samples, obtaining a sufficient number of samples to train high-quality models is practically infeasible.

The following two theorems establish that the optimal convergence rate for estimating the data density is $\Oc(\log n)^{-2}$. These results, derived using standard deconvolution theory~\citep{Meister2009} under a Gaussian noise assumption, highlight the inherent difficulty of the problem. We present the result for $d = 1$, which suffices to illustrate the challenge.  
\begin{restatable}{theorem}{MISEUpperBdound}
\label{thm:MISE_upper_bound}
Assume $ \Yc $ is generated according to \eqref{eq:gen_conv_samples} with $ \epsilon \sim \Nc(0, \sigma_\zeta^2) $ and $ p_\text{data} $  is a univariate distribution.  Under some weak assumptions on $ p_\text{data}$, for a sufficiently large sample size $ n $, there exists an estimator $ \hat{p}(\cdot; \mathcal{Y}) $ such that
    \begin{align}
        \mathrm{MISE}(\hat{p}, p_\textrm{data}) \leq C \cdot \frac{ \sigma_\zeta^4}{(\log n)^2},\label{eq:MISE_upper_bound}
    \end{align}
    where $ C $ is determined by $ p_\textrm{data} $.
\end{restatable}
\begin{restatable}{theorem}{MISELowerBound}
\label{thm:MISE_lower_bound}
In the same setting as \cref{thm:MISE_upper_bound}, for an arbitrary estimator $ \hat{p}(\cdot; \mathcal{Y}) $ of $p_\textrm{data}$ based on $\Yc$, 
\begin{align}
	\mathrm{MISE}(\hat{p}, p_\textrm{data}) \geq K \cdot (\log n)^{-2},
\end{align}
where $K>0$ is determined by $p_\textrm{data}$ and error distribution~$h$. 
\end{restatable}
The optimal convergence rate $\Oc(\log n)^{-2}$ indicates that reducing the MISE to one-fourth of its current value requires an additional $ n^2 - n $ samples. In contrast, under the error-free scenario, the optimal convergence rate is known to be $ \Oc(n^{-4/5})$ \citep{Wand1998}, where reducing the MISE to one-fourth of its current value would only necessitate approximately $ 4.657n $ additional samples.

The pessimistic rate indicates that effectively training a generative model using only corrupted samples with Gaussian noise is nearly impossible. Consequently, this implies that training from scratch,  using only noisy images, with the consistency loss discussed in \cref{prel:deconv_through_consist_const}, is infeasible. Notably, as indicated by \cref{eq:MISE_upper_bound}, this difficulty becomes significantly more severe with larger $\sigma_{\zeta}^2$, while a large $\sigma_{\zeta}^2$ is typically required to alter the original samples significantly to address copyright and privacy concerns.

To address the pessimistic statistical rate, we propose pretraining diffusion models on a small set of copyright-free samples. While this limited dataset can only capture a subset of the features and variations of the full true data distribution, we argue that it provides valuable prior information, enabling the model to start from a point much closer to the ground distribution compared to random weight initialization. For example, for image generation, pretraining allows the model to learn common features and structures shared among samples, such as continuity, smoothness, edges, and general appearance of typical object types.

Unfortunately, our empirical study in \cref{sec:emp} will show that the consistency loss-based method discussed in \cref{prel:deconv_through_consist_const} cannot deliver promising results even after pretraining. We suspect that this is caused by the gap between their theoretical framework and the practical implementation. As a result, we propose SFBD in \cref{sec:SFBD} to bridge such a gap. 

