

\documentclass[journal]{IEEEtran}
%\usepackage[options]{package}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{cite}
\usepackage{hhline}
\usepackage[cmex10]{amsmath}
\usepackage{graphicx}
    \graphicspath{{./figs/}}
    \DeclareGraphicsExtensions{.pdf}
%\usepackage[keeplastbox]{flushend}
\usepackage{balance}
\usepackage{algorithmic}
\usepackage{dsfont}
\usepackage{hhline}
\usepackage[usenames]{color}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{array}
\usepackage{url}
\usepackage{xspace}
\usepackage{mathtools}
	\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
	\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\usepackage[table,xcdraw]{xcolor}
\usepackage{blindtext}
\usepackage{diagbox}
\usepackage{cleveref}
\usepackage{tablefootnote}
\newcommand{\eg}{\textit{e.g.},\xspace}
\newcommand{\ie}{\textit{i.e.},\xspace}
\newcommand{\etal}{\textit{et al}.\xspace}
\newcommand{\covid}{COVID-19\xspace}


\newcommand{\rc}[1]{{\color{orange} {#1}}}
\newcommand{\ac}[1]{{\color{magenta}{#1}}}
\newcommand{\jg}[1]{{\color{red}{#1}}}

\usepackage{multirow}
\usepackage{booktabs}
\usepackage{amsthm}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.

%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%


\title{Random Projections and Natural Sparsity in Time-Series Classification: A Theoretical Analysis}

\author{\IEEEauthorblockN{
Jorge Marco-Blanco\IEEEauthorrefmark{1},
Rubén Cuevas\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
}

\IEEEauthorblockA{\IEEEauthorrefmark{1} Universidad Carlos III de Madrid, Spain}\\
\IEEEauthorblockA{\IEEEauthorrefmark{2}UC3M-Santander Big Data Institute, Spain}\\


}
\maketitle



% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
The classification of time-series data plays a crucial role in diverse fields, from healthcare diagnostics and industrial systems monitoring to financial prediction and activity detection in humans. Within this domain, the Rocket algorithm stands out as an elegantly straightforward yet robust approach that has demonstrated superior performance compared to existing methods. At its core, the algorithm transforms time-series data through randomly initialized convolutional kernels, followed by a non-linear transformation step. This structure mirrors a simplified convolutional neural network with a single hidden layer, but uniquely eliminates the computational burden of parameter optimization.
While Rocket's practical effectiveness is well-documented, its theoretical underpinnings have remained largely unexplored. Our research addresses this gap by establishing a theoretical framework that connects Rocket's random convolution operations to compressed sensing principles, demonstrating how random projections maintain the distinctive patterns within time-series data. This theoretical analysis illuminates the connections between kernel configuration and signal properties, offering a more systematic approach to algorithm tuning. Furthermore, we demonstrate that its non-linear transformation component, which calculates the ratio of positive values post-convolution, effectively captures the inherent sparsity patterns in time-series data. Our mathematical investigation additionally proves that Rocket exhibits two essential properties for time-series classification: invariance to temporal shifts and resilience against noise. These insights not only enhance the algorithm's transparency - particularly valuable in regulated industries - but also provide practical guidelines for parameter selection in challenging scenarios, thus contributing to both the theoretical foundations and practical applications of time-series classification methods.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
%\begin{IEEEkeywords}
%Communications Society, IEEE, IEEEtran, journal, \LaTeX, paper, template.
%\end{IEEEkeywords}






% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle




\section{Introduction.}
%rocket intro:
% Most methods for time series classification that attain state-of-the-art accuracy have high computational complexity, requiring significant training time even for smaller datasets, and simply do not scale to large datasets. This has motivated the development of more scalable methods such as Proximity Forest (Lucas et al. 2019), TS-CHIEF (Shifaz et al. 2019), and InceptionTime (Ismail Fawaz et al. 2019c).
% We show that state-of-the-art classification accuracy can be achieved using a fraction of the time required by even these recent, more scalable methods, by trans- forming time series using random convolutional kernels, and using the transformed features to train a linear classifier. We call this method Rocket (for RandOm Convolutional KErnel Transform).


Time-series classification is a fundamental challenge in data mining and knowledge discovery, with applications spanning finance, healthcare, industrial monitoring, and many other domains \cite{bagnall2017great, ismail2019deep, keogh2002need, rakthanmanon2013addressing}. Its importance stems not only from the vast amount of temporal data generated in these fields but also from the inherent complexities—such as high dimensionality, noise, and temporal shifts—that make accurate analysis both essential and challenging.

When developing algorithms for this domain, researchers must address several key challenges. First, time-series data often exhibits high dimensionality, which can make traditional classification approaches computationally intensive and less effective. To address this, many researchers employ feature-based algorithms that transform the raw input data into more manageable representations before applying classification methods.
A second major challenge lies in the inherent variability between instances of the same class. This variability can manifest in multiple ways: temporal shifts where similar patterns occur at different time points, amplitude variations where the magnitude of patterns differs while maintaining the same shape, and noise from various sources such as measurement imprecision or environmental factors. These variations make it particularly challenging to develop robust classification algorithms that can reliably recognize patterns despite such differences.

Among feature-based methods, Rocket (for RandOm Convolutional KErnel Transform) algorithm \cite{dempster2019rocket} stands out, offering a computationally efficient framework with exceptional results. The algorithm applies thousands of convolutional kernels, whose parameters are chosen at random, to the input time series, producing an equivalent number of secondary time series. These transformed signals then undergo two nonlinear operations: one computes the proportion of positive values (PPV) and the other takes the maximum value. Finally, a linear classifier processes these features for the final classification. While Rocket's architecture is equivalent to that of a one-hidden-layer convolutional neural network, combining convolution operations with non-linearities, it operates without any parameter training. This is a key distinction that leads to remarkable computational efficiency - a characteristic that has proven crucial to its success. Indeed, Rocket and its variants have achieved state-of-the-art time series classification performance while maintaining this exceptional computational efficiency \cite{middlehurst2024bake}. The framework has evolved through both supervised \cite{dempster2021minirocket,dempster2023hydra,tan2022multirocket} and unsupervised \cite{jorge2024time} learning paradigms. Notably, these variants have consistently moved away from the MAX nonlinearity in favor of PPV based on empirical results.

Despite its remarkable empirical success, several fundamental questions about Rocket's theoretical foundations remain unexplored. Why does random parameter selection prove more effective than carefully designed feature extraction? How does the PPV non-linearity consistently outperform maximum value operations? The original paper empirically selects hyperparameters independently of signal characteristics - yet puzzlingly, these same parameters work effectively across time series of vastly different lengths, from 24 to thousands of elements. This paper develops a theoretical framework to address these fundamental questions.
Our key contributions are:

\begin{itemize}
    \item We establish a theoretical foundation for Rocket's use of randomness through the lens of compressed sensing theory.

    \item We demonstrate that the PPV non-linearity functions as a measure of sparsity, revealing its fundamental role in capturing key time-series properties.

    \item We provide theoretical justification for the experimental hyperparameter values identified in the original study.

\end{itemize}






%By answering these questions we reach the conclusion that the transform represent the sparsity of the input signal across a diverse set of basis 




% This paper faces this challenge by providing four key contributions. First, we show that the algorithm satisfies two strong necessary condition for classification: noise robustness and translation invariance. Second, we justify that the algorithm keeps the information in its phases. First by randomness, then trough sparsity. Finally, we establish relationships between algorithm hyperparameters and signal properties, particularly between kernel length and signal length and sparsity, enabling practical application of our theoretical insights.



% This paper faces this challenge by providing four key contributions. First, we show that the algorithm satisfies two strong necessary condition for classification: noise robustness and translation invariance. Second, we explain why choosing the parameters at random is crucial, showing how they maximize the diversity of signal representations. Third, we show that the key nonlinearity—proportion of positive values (PPV)—through is a measure of sparsity, a fundamental signal characteristic. Finally, we establish relationships between algorithm hyperparameters and signal properties, particularly between kernel length and signal length and sparsity, enabling practical application of our theoretical insights.

These contributions advance the field in several significant ways. Our theoretical framework represents a relevant step forward for time series classification. While other prominent methods like dynamic time warping \cite{keogh2005exact, bagnall2014experimental} and shapelet-based approaches \cite{ye2009time} rest on solid theoretical foundations, Rocket - despite its state-of-the-art performance - has lacked such grounding until now. Beyond theory, our work enhances Rocket's practical applicability. By improving interpretability, we enable its use in highly regulated domains where explainability is mandated, such as applications governed by the EU's AI Act. This combination of computational efficiency and interpretability is paramount for real-world deployment. Furthermore, our insights into parameter optimization establish clear relationships between hyperparameters and time-series characteristics like length and sparsity. This understanding allows for more effective parameter tuning in extreme cases, such as very short or long time series, or those with unusual sparsity patterns - potentially pushing the boundaries of classification performance even further.


\section{Algorithm structure and notation.}

To establish our theoretical framework, we begin by introducing the notation used throughout this paper and break down the algorithm into three key steps. Kernels are generated with random lengths, weights, biases, dilation factors, and padding configurations. This randomness ensures diversity in feature extraction while maintaining computational efficiency. Below, we formalize the three-step process:

\begin{itemize}
    \item \textbf{Random Convolution (First Transform) $\Phi_i(\mathbf{x})$.} 
    For each kernel index $i \in \{1,\dots,L\}$, the algorithm builds a random kernel $\mathbf{h}_i \in \mathbb{R}^K$ by sampling its weights from a Gaussian distribution
 with zero mean and variance $1/K$, and a bias from an uniform $U(-1,1)$ distribution. Then the input time series $\mathbf{x} \in \mathbb{R}^N$ is convolved with each kernel:
    \[
        \mathbf{y^{(i)} } \;=\; \Phi^{(i)}(\mathbf{x}) \;=\; \mathbf{x} \,\ast\, \mathbf{h}^{(i)} + bias,
    \]
    thereby producing $L$ convolved time series, each of length $N$. Other relevant kernel components include dilation, also known as “atrous convolution,” which spaces apart the kernel elements by inserting gaps that effectively enlarge the receptive field without increasing the kernel size. Padding complements this by adding extra values at the boundaries of the input time series, ensuring that the convolution maintains the original length of the series.

    \item \textbf{PPV (Second Transform) $PPV(\mathbf{y^{(i)}})$.}
    Next, for each convolved series $\mathbf{y^{(i)}}$, we compute the fraction of values that exceed zero:
    \[
        PPV(\mathbf{y^{(i)}}) \;=\;
        \frac{1}{N} \sum_{n=0}^{N-1} \mathbb{I}\bigl(\mathbf{y^{(i)}}[n] > 0\bigr),
    \]
    where $\mathbb{I}(\cdot)$ is the indicator function and $\mathbf{y_i}[n]$ denotes the $n$-th element of the $i$-th convolved series. This step yields $L$ scalar values.
    
    \item \textbf{Classification.} Collect all $L$ PPV values into a single feature vector,
    \[
        \mathbf{z} = \bigl[\,PPV(\Phi^{(1)}(\mathbf{x})), \ldots, PPV(\Phi^{(L)}(\mathbf{x}))\bigr],
    \]
    and input this vector to a ridge classifier to assign class labels.
\end{itemize}


\medskip

Having established the notation and algorithm structure, we provide a theoretical analysis of each algorithmic component: Section \ref{related} reviews related work in random projections, convolutions, and signal processing. Section \ref{invariances} demonstrates that the complete Rocket transform \(PPV(\Phi(\mathbf{x}))\) satisfies two necessary conditions for effective classification: shift invariance and noise robustness. While these properties are important, they alone are insufficient (even a trivial constant transformation would satisfy them). To understand why the algorithm works, we need to show that the information is preserved under the successive transformations. In Section \ref{random_parameters}, we build on advances in the field of compressed sensing that demonstrate how each of the signals produced in the first step of the algorithm \(\Phi_i(\mathbf{x})\) is recoverable, thus keeping all the information of the original signal. Then, in Section \ref{diversity_sparsity}, we establish that the PPV operation effectively measures the signal sparsity of each of the time series. This multi-basis sparsity measurement is particularly powerful for classification, as it captures complementary signal characteristics—analogous to how a sinusoid's frequency-domain sparsity reveals its fundamental structure. The effectiveness of this approach draws parallels with fundamental principles in signal processing, such as the uncertainty principle, where different bases provide complementary views of the underlying signal.



\section{Related work.}
\label{related}

Our theoretical analysis of Rocket draws from and connects to several fundamental areas in signal processing and machine learning. We first examine compressed sensing theory, which provides insights into signal recoverability and the role of sparsity. We then explore work on invariance and stability in signal representations, particularly through wavelet transforms. Finally, we review random projection methods and neural networks with random weights, which offer complementary perspectives on why randomly initialized features can be effective for classification.


\subsection{Recoverability and compressed sensing.}

Compressed sensing (CS) \cite{donoho2006compressed,candes2008introduction} explores techniques for projecting signals into lower-dimensional spaces while ensuring that the original signal can be accurately reconstructed. A notable subfield focuses on sensing with random features \cite{baraniuk2008simple,tropp2006random,candes2006near,haupt2010toeplitz,bajwa2007toeplitz}, where randomness is leveraged to capture the essential characteristics of a signal. Additional studies examine the properties of sensing bases that are critical for recoverability \cite{cai2011orthogonal,donoho2001uncertainty}.

While the primary goal in compressed sensing is to reduce dimensionality by eliminating redundancy, such redundancy can be advantageous for classification tasks. In our work, we draw on the insights from compressed sensing to interpret key components of our algorithm. However, rather than aggressively compressing the data, we preserve a level of redundancy that helps maintain the informative content of the signal. Moreover, whereas sparsity in compressed sensing is used to assess the probability of recovery: signals must be compressible in some domain (e.g., Fourier, wavelet), in our approach, sparsity serves as a crucial feature that underpins effective classification.

Although compressed sensing aims to reduce dimensionality by eliminating redundancy, we will explain how random kernels leverages such redundancy for classification. Additionally, while compressed sensing uses sparsity to evaluate signal recovery probability, we will show that sparsity itself as a key discriminative feature for classification.


\subsection{Invariance and stability.}


The stability of classification algorithms in the presence of additive noise is a fundamental concern in signal processing and machine learning. Stéphane Mallat's pioneering work on wavelet transforms \cite{mallat2012wavelet} introduced a framework that achieves both invariance and stability—qualities essential for robust classification. Building on these ideas, Mallat \cite{mallat2012group} and Bruna and Mallat \cite{bruna2013invariant} developed scattering transforms that offer translation invariance and robustness against deformations and noise. Although scattering transforms and random kernels both rely on convolutional architectures, scattering transforms employ more  wavelet convolutions and modulus nonlinearities instead of the PPV operation. However, despite their strong theoretical foundations, wavelet-based approaches have not consistently demonstrated superior performance.

These stability considerations remain central to modern machine learning, particularly given the vulnerability of deep neural networks to adversarial noise, as demonstrated by Goodfellow et al. \cite{goodfellow2014explaining}.


\subsection{Random-weights neural networks and random projections.}

Random-weight neural network kernels have been shown to exhibit strong universal approximation capabilities even when their parameters are initialized randomly. Theoretical explanations for using random parameters are partially provided by the framework of random feature maps, as introduced by Rahimi and Recht \cite{rahimi2007random}. Their work demonstrates that a sufficiently large set of random features can approximate a wide class of functions with bounded error, offering some justification for the success of such networks. Additionally, studies on extreme learning machines by Huang et al. \cite{huang2006universal} and subsequent work by Cao et al. \cite{cao2018review} have empirically validated that randomly initialized layers can effectively capture complex patterns with minimal training. Contributions from random projection theory, notably by Bingham and Mannila \cite{bingham2001random} and Blum \cite{blum2005random}, further support this approach by showing that random matrices can preserve the geometric structure of high-dimensional data via the Johnson-Lindenstrauss lemma. However, while these insights offer valuable perspectives, a comprehensive theoretical understanding of why random parameters work so well remains incomplete. The existing theories typically rely on high-dimensional assumptions and the law of large numbers, leaving several open questions regarding optimal design and performance guarantees. Furthermore, the reliance on dense random matrices incurs significant computational overhead.

While these areas provide valuable theoretical context, they do not much explain the empirical success of random kernels classification. This motivates our theoretical analysis, beginning with two fundamental properties required for robust classification.

\section{Translation invariance and noise robustness.}
\label{invariances}
In this section, we demonstrate that the Rocket transform satisfies two key necessary conditions for effective time series classification: robustness to additive noise and invariance to signal translations. 

\subsection{Noise robustness.}
Additive noise is commonly characterized as white Gaussian noise \cite[p.~69]{oppenheim1999discrete}, \cite[p.~590]{proakis1996digital}, \cite[p.~7]{kay1993fundamentals}. Accordingly, we represent the noisy signal \( g[n] \) as

    \begin{equation}
        g[n] = f[n] + \varepsilon[n]
    \end{equation}
where $ \varepsilon[n] \sim \mathcal{N}(0,1) $ are independent random variables with the same distribution.

To ensure stability to additive noise, a transformation must satisfy the Lipschitz continuity condition \cite{bruna2013invariant}. Specifically, there must exist a constant $ C > 0 $ such that:

    \begin{equation}
        \| \Phi g - \Phi f \| \leq C \| g - f \|
    \end{equation}
where $ \| \cdot \| $ represents the Euclidean norm. This property ensures that small perturbations in the input signal produce only bounded changes in the transformed signal, thereby conferring noise robustness. A counter-example of non-robust transformation is the Fourier transform, that do not remain stable under small deformations—particularly at high frequencies \cite{mallat2012group}.


With high probability (with respect to the random Gaussian noise), the difference between the Rocket features derived from a noisy signal and those from the corresponding clean signal remains bounded by a constant multiple of the difference between the two signals. This constant depends on the total number of random kernels in Rocket as well as the chosen statistical confidence level. Concretely, Rocket is “Lipschitz continuous” with high probability, thus ensuring robustness against moderate levels of additive Gaussian noise. A full proof is provided in Appendix \ref{lipschitz}.



\subsection{Translation invariance.}

In time series classification, translation invariance refers to the ability of a classifier to produce consistent outcomes despite temporal shifts in the input signal. It is a fundamental property since discriminative patterns in time-series can appear at arbitrary temporal positions. Mallat \cite{mallat2012wavelet} extensively analyzed this property in the context of wavelet convolutions. We extend this analysis to show how the combination of random convolutions and nonlinear transformations achieves translation invariance. Formally, we consider that a representation (transform) $\Phi$ is invariant to translations if it meets the following condition:

\begin{align*}
    f_c[n] = f[n-c],  \qquad n,c \in \mathbb{N} \qquad \text{if} \qquad  \Phi f_c = \Phi f
\end{align*}

Intuitively, translation invariance arises from the aggregation over time: The Rocket transform computes features by counting how often the convolved signal exceeds a threshold (bias) across the entire time axis. Since this summation spans all positions equally, shifting the signal redistributes contributions but preserves the total count. A formal proof of this invariance is provided in Appendix \ref{translation_invariance}

While these properties are necessary conditions for classification for good classification performance,  even a constant function would satisfy them. Therefore in the next sections we explain the transformations also keep discriminative information of the input signal.



\section{The logic behind random parameters.}
\label{random_parameters}

In this section we will analyze how the field of compressed sensing allows to gain a deeper understanding of the Rocket algorithm and its random features. First we will introduce two key concepts of this field that will help us in this section. We explore how these concepts help explain why certain kernel configurations perform better than others and provide theoretical justification for the selection of key hyperparameters, such as kernel length and number of kernels.

In the field of compressed sensing, two fundamental concepts: \textit{coherence} and the \textit{Restricted Isometry Property} (RIP) play a central role in analyzing the quality of sensing matrices. These concepts govern how well a sensing matrix $\Phi$ preserves information about sparse signals.


\subsection{Coherence.}
Coherence measures the degree of similarity or correlation between the columns (or rows) of a matrix. For a sensing matrix $\Phi$ with normalized columns $\phi_i$, coherence is defined as:

\begin{equation}
\mu(\Phi) = \max_{i \neq j} |\langle \phi_i, \phi_j \rangle|,
\end{equation}

where $\langle \phi_i, \phi_j \rangle$ denotes the inner product between two columns $\phi_i$ and $\phi_j$. Lower coherence implies that the columns are nearly orthogonal, which is desirable in sensing applications because it reduces redundancy and improves the ability to distinguish between different signals. Coherence plays a critical role in determining the sparsity levels that a sensing matrix can reliably support. 

\subsection{Restricted isometry property (RIP).}
The RIP provides a more general framework for analyzing the performance of sensing matrices. A matrix $\Phi$ satisfies the RIP of order $s$ with constant $\delta_s \in (0,1)$ if, for all $s$-sparse signals $\mathbf{x}$, the following relationship holds:

\begin{align}
(1-\delta_s) \|\mathbf{x}\|_2^2 \leq \|\Phi \mathbf{x}\|_2^2 \leq (1+\delta_s) \|\mathbf{x}\|_2^2,
\end{align}

where $\|\cdot\|_2$ denotes the Euclidean norm. This condition ensures that $\Phi$ approximately preserves the geometry of $s$-sparse signals during the transformation, enabling stable and accurate recovery.

Random matrices with entries drawn from certain distributions, such as Gaussian or sub-Gaussian, are known to satisfy RIP with high probability \cite{vershynin2018high}, provided the matrix dimensions are sufficiently large. Structured random matrices (e.g., Toeplitz) require careful analysis, as their parameters introduce tradeoffs between measurement efficiency and recovery guarantees \cite{haupt2010toeplitz}. These tradeoffs become critical when designing convolutional sensing schemes, as we explore next.

\subsection{Matrix representation of Rocket's first transform.}
\label{Rocket_Toeplitz}

The first transform of Rocket ($\Phi)$ is a discrete convolution between a signal $\mathbf{x}$ of length $N$ and a kernel $h[k]$ of length $K$ is defined as ($\mathbf{y^{(i)}} = \Phi^{(i)}(\mathbf{x})$) where each component of $\mathbf{y^{(i)}}$ can be computed as:

\begin{equation}
    y^{(i)}[n] = (\mathbf{x} \,\ast\, \mathbf{h}^{(i)} + bias^{(i)})[n] = \sum_{k=0}^{K-1} h^{(i)}[k]x[n-k] + bias^{(i)}
\end{equation}


This operation can be expressed as a matrix multiplication $\mathbf{y}^{(i)} = H^{(i)}\mathbf{x}$, where $H^{(i)}$ is a Toeplitz matrix. For a kernel $h = [h_0, h_1, ..., h_{K-1}]$ and signal $x = [x_0, x_1, ..., x_{N-1}]^T$, the transform for a given kernel can be written (omitting the index $i$ for clarity) as:




\begin{equation*}
    \resizebox{0.995\columnwidth}{!}{$
    \begin{bmatrix} 
        y_0 \\
        y_1 \\
        %y_2 \\
        \vdots \\
        y_{N-K}
    \end{bmatrix} = 
    \begin{bmatrix}
        h_0 & h_1 & \cdots & h_{K-1} & 0 & 0 & \cdots & 0 \\
        0 & h_0 & h_1 & \cdots & h_{K-1} & 0 & \cdots & 0 \\
        %0 & 0 & h_0 & h_1 & \cdots & h_{K-1} & \cdots & 0 \\
        \vdots & \vdots & \ddots & \ddots & \ddots & \ddots & \ddots & \vdots \\
        0 & 0 & \cdots & 0 & h_0 & h_1 & \cdots & h_{K-1}
    \end{bmatrix}
    \begin{bmatrix}
        x_0 \\
        x_1 \\
        %x_2 \\
        \vdots \\
        x_{N-1}
    \end{bmatrix}
    + b
    \begin{bmatrix}
        1 \\
        1 \\
        %1 \\
        \vdots \\
        1
    \end{bmatrix}
    $}
\end{equation*}


\medskip
\noindent
Where, we recall, each $h_k$ is drawn from a standard Gaussian distribution and b stand for bias. The resulting Toeplitz matrix preserves both the randomness of the filter and the structure of the convolution operation. Toeplitz matrices with entries drawn from appropriate random distributions, such as Gaussians, are known to satisfy RIP of order $s$ for $s$-sparse signals with high probability under the following condition\cite{haupt2010toeplitz}:  

\begin{align}
\label{K_relationship}
   K > \frac{s^2}{c_s} \log(N),
\end{align}

where $K$ is the length of the kernel, $s$ is the sparsity of the input signal, $N$ is the length of the time series, and $c_s$ is a constant. This relationship implies that for a signal characterized by $N$ and $s$, choosing $K$ large enough ensures recoverability. Moreover, larger $K$ generally improves recoverability. However, this theoretical insight partially contradicts empirical findings: increasing $K$ beyond a certain point degrades the classification accuracy. Empirically, the optimal $K$ for the UCR datasets is often smaller (e.g., around 9), suggesting an additional factor influencing performance.

To explain this behaviour, we invoke the concept of mutual incoherence introduced before. A known result in compressed sensing \cite[Chapter~5]{foucart2013invitation}, \cite{cai2011orthogonal} states that a sparse signal can be recovered if the mutual coherence satisfies:

\begin{align}
\label{coherence}
    \mu(\Phi) < \frac{1}{2s-1}.
\end{align}

In our context, the rows of the Toeplitz matrix act as sensing vectors. We find that increasing $K$ raises the likelihood of higher coherence among these rows, reducing the quality of the sensing matrix:


\subsubsection{Coherence of a Toeplitz Matrix: Summary of Demonstration}
\label{toeplitz_matrix_coherence}

We analyze the coherence of a Toeplitz matrix generated by a convolution kernel of length \(K\) and overall signal size \(N\). Our goal is to bound the probability that the maximum off-diagonal entry, which represents the mutual interference between different shifted versions of the kernel, exceeds a prescribed threshold \(\alpha\).

Let $T_{ij} = |\langle \phi_i, \phi_j \rangle|$ denote the inner product magnitude between bases $\phi_i$ and $\phi_j$. Starting with the definition of coherence, we apply the union bound (Boole's inequality) to express the probability that any off-diagonal entry exceeds \(\alpha\) as:
\[
  \mathbb{P}\Bigl(\max_{i<j}\lvert T_{ij}\rvert > \alpha\Bigr)
  \leq \sum_{i<j} \mathbb{P}\bigl(\lvert T_{ij}\rvert > \alpha\bigr).
\]
Using Chebyshev’s inequality for each off-diagonal element, we bound these probabilities in terms of the variance of \(T_{ij}\). Assuming that the kernel entries \(h_i\) are i.i.d. with zero mean and variance \(\frac{1}{K}\), the variance of an entry like
\[
  T_{01} = h_0h_1 + h_1h_2 + \cdots + h_{K-2}h_{K-1}
\]
is found to be
\[
  \mathrm{Var}[T_{01}] = \frac{K-1}{K^2}.
\]

We then introduce \(\theta_{ij}\), the number of overlapping terms between columns \(i\) and \(j\) in the Toeplitz matrix, defined as:
\[
\theta_{ij} =
\begin{cases}
K - (j - i), & \text{if } j - i < K, \\
0, & \text{otherwise}.
\end{cases}
\]
Summing over all pairs \((i,j)\) and substituting the variance bound yields the final coherence bound:
\[
\mathbb{P}\Bigl(\max_{i<j}\lvert T_{ij}\rvert > \alpha\Bigr)
\le N\;\cdot\;\frac{(K-1)}{2\,K\,\alpha^2}.
\]

This result indicates that as either \(K\) or \(N\) increases, the probability of encountering a high off-diagonal entry (i.e., higher coherence) grows. The detailed derivation of these bounds is provided in the Appendix. These findings are critical for understanding how the structure of convolutional transforms impacts the preservation of signal information and the discriminative power of the resulting features. (see Appendix \ref{toeplitz_coherence}) for a more detailed demonstration.








\medskip


The result of the coherence of a Toeplitz matrix and the RIP condition together create a concave relationship:

\begin{itemize}
    \item Small \( K \): Coherence is low, but RIP compliance fails (\( K \ll \frac{s^2}{c}\log(N) \)), limiting discriminative power.

    \item Large \( K \):RIP is satisfied, but high coherence corrupts feature separability.  
    
\end{itemize}

Empirically, the peak accuracy at \( K=9 \) (UCR datasets) aligns with the theoretical equilibrium where \( K \sim \mathcal{O}(s^2 \log N) \) and \( \mu(\Phi) < \frac{1}{2s-1} \) hold simultaneously. 


Our analysis reveals that Rocket's first transform—convolution with random Gaussian kernels using a Toeplitz structure—preserves signal recoverability when the kernel length ($K$) is appropriately chosen based on the input signal's sparsity ($s$) and length ($N$). While this preservation of information ensures the retention of discriminative features, the transformed time series remains sensitive to the input signal's phase, lacking the desired translation invariance. The second transform, computing the proportion of positive values (PPV), resolves this limitation by providing phase-invariant features. In the following section, we reveal the fundamental connection of PPV to sparsity and justify the conservation of discriminative power of this operation.






\section{Sparsity and diversity.}
\label{diversity_sparsity}

In the previous sections, we showed how the proportion of positive values (PPV) achieves translation invariance. A natural follow-up question is whether PPV still retains sufficient information from the original time series to preserve discriminative power. In this section, we address this concern by demonstrating that PPV effectively serves as a \emph{sparsity measure}, thus capturing the essential characteristics needed for classification. This connection to sparsity is especially relevant because different classes of signals often exhibit unique sparsity patterns—an attribute successfully leveraged in numerous applications \cite{Wright09,Elad06}.

\subsection{PPV + bias as sparsity measure.}

In the absence of noise, PPV provides an estimate of a signal's theoretical sparsity. Consider a standardized signal (zero mean, unit variance) of length $N$ with $s$ nonzero elements, where $s^+$ are positive. Given the standardization, we expect an equal number of positive and negative elements, thus $s^+ = s/2$. Measuring the PPV yields:
\[
    \text{PPV} = \frac{s^+}{N} = \frac{s}{2N},
\]
providing a direct relationship to sparsity. Note that a signal is considered sparse when it concentrates its energy in a small number of components $s$. Therefore, while PPV increases with $s$, it is actually inversely related to sparsity: higher PPV values indicate less sparse signals.


In real scenarios, however, noise complicates matters. One practical approach is to select a threshold above the average noise level and consider only the components of the signal that rise above this threshold as “significant.” Figure \ref{fig:sparsity} illustrates a practical example. It shows a noisy signal with a nominal sparsity of \(s=3\). When a threshold (shown as the dashed line) is applied, only three peaks exceed this level, revealing the underlying sparse components. The key observation is that incorporating an appropriate bias term into the PPV calculation isolates these true signal features from noise. This approach enables effective class differentiation: for example, if another signal class were to contain only one significant component (\(s=1\)), the PPV measure would readily distinguish between the two classes. In contrast, a simple \(\max\) function might struggle to differentiate them when peak amplitudes are similar.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{Images/3_threshold.png}
    \caption{Thresholding a noisy signal (blue) to estimate sparsity. Components above the threshold (dashed line) are retained as true signal features, indicating a sparsity of $s$=3}
    \label{fig:sparsity}
\end{figure}

This concept reveals a broader insight: only kernels or transformations that include a bias sufficient to push their outputs above the noise threshold can accurately capture sparsity. In our framework, sparsity is a characteristic shared by elements within the same class. Therefore, kernels that yield outputs above the threshold are adept at capturing this class-specific variability. On the other hand, transformations that remain below the threshold generate PPV values with minimal variability across instances of different classes.

Building on this, if one applies a variable selection method based on output variability, an unsupervised learning transformation emerges. For instance, Marco et al. \cite{jorge2024time} extract features using random kernels combined with PPV, then apply PCA to select features according to their variability, and finally use K-means clustering. This configuration achieves state-of-the-art clustering results. In contrast, for supervised classification tasks, methods like Rocket and its derivatives employ ridge regression which includes L2 regularization to penalize features that exhibit negligible variability. Consequently, the ridge classifier automatically shrinks the coefficients of these invariant PPV measurements toward zero, effectively downweighting them. This process ensures that only features with meaningful, class-specific variability are emphasized during classification, thereby enhancing overall performance.

To further validate the relationship between PPV and sparsity, and noting that PPV effectively measures the inverse of sparsity, we evaluate 1/PPV as a sparsity measure within the axiomatic framework established in \cite{hurley2009comparing}. In this work, Hurley et al. establish desired properties for quantifying sparsity. As shown in Appendix \ref{ppv_sparsity}, 1/PPV satisfies three critical properties from Hurley’s framework: scaling invariance, cloning invariance, and sensitivity to added zeros. This performance aligns it with the top 30\% of measures in Hurley’s comparative study, where only 3 of 16 widely used sparsity metrics satisfy three properties, and only 2 more metrics exceed this threshold. Therefore, we can establish $\frac{1}{\mathrm{PPV}}$ as a valid sparsity measure, and consequently, PPV as an indicator of non-sparsity.

Once we recognize PPV as a measure of sparsity, we can better understand its application to the thousands of signals generated by random convolutions. Each random kernel provides a different``view'' of the original data, with the large number of kernels serving two purposes. First, it enables natural elimination of noisy transformations through random sampling and thresholding. Second, it creates a rich, diverse set of signal representations.

Unlike methods such as wavelets or Fourier transforms, which aim to find sparse representations by aligning the kernel patterns with those of the input signal, random kernels focus on generating a broad set of representational domains. The goal is not to create sparse representations per se, but rather to capture sparsity across a diverse set domains that collectively capture the essence of the original input.
This concept of diversity is closely tied to the concept of \emph{coherence} among the multiple bases induced by the random kernels, which we explore next.


\subsection{Coherence Across Multiple Bases.}
\label{sec:coherence_bases}

In this section we formally study the diversity of the random kernels through the concept of coherence among different bases (different from the previous concept of coherence in a given basis introduced before) . Recall from Section \ref{Rocket_Toeplitz} that each random kernel generates its own basis, denoted by \(\Phi\). To quantify the similarity between any two such bases, \(\Phi^{(i)}\) and \(\Phi^{(j)}\), we adopt the coherence measure defined in \cite{donoho2001uncertainty}:

\[
\mu(\Phi^{(i)}, \Phi^{(j)}) = \max_{r,s} \frac{\bigl| \langle \phi^{(i)}_r, \phi^{(j)}_s \rangle \bigr|}{\|\phi^{(i)}_r\| \cdot \|\phi^{(j)}_s\|},
\]
where \(i, j \in \{1, \dots, L\}\) and \(r, s \in \{0, \dots, N-1\}\).

In the context of random kernels, \(\phi^{(i)}_r\) corresponds to the \(r\)th row of the Toeplitz matrix induced by kernel \(i\). This measure generalizes the concept of orthogonality and captures the general notion of complementarity observed between the time and frequency domains. For example, the coherence between the discrete Fourier transform (DFT) basis and the canonical (time) basis (i.e., the identity matrix) reaches its minimum possible value of \(1/\sqrt{N}\) in an \(N\)-dimensional space, indicating maximal incoherence between the two representations.

In the case of random kernels, one key question is how to further reduce coherence, thus increasing the diversity of the feature representations of the input signal. One effective approach is to use random weights, since the expected value of the scalar product between different bases is zero, naturally promoting orthogonality. Furthermore, Introducing dilations into random kernels affects the resulting scalar product matrices by increasing the number of zero entries. When a kernel is dilated, its active coefficients are spread further apart, resulting in fewer overlapping nonzero elements when computing inner products between different bases. This altered structure leads to lower overall variability in the inner products and specifically reduces the maximum normalized inner product, which is the formal definition of coherence.

In summary, random weights ensure that the average overlap between different bases remains low, while dilations reduce the number of substantial overlaps. This not only reinforces the rationale for using random weights but also offers a fresh perspective on the use of dilations in random kernels.



\section{Empirical validation: analysis of the feature space}
\label{sec:results}

Thus far, our theoretical analysis has detailed the key architecture and parameters underlying random kernels for classification. In particular, we posited that the combination of PPV and bias serves not only as an effective discriminator but also as a measure of the intrinsic compactness of the signal representation. Given the importance of this conclusion, we designed experiments to validate these claims further.

Our experimental framework is based on the original Rocket configuration, using 10,000 random kernels (i.e., \(L = 10,000\)) to generate high-dimensional feature spaces. We conducted our experiments on the same 117 datasets from the UCR archive \cite{dau2019ucr} as used in the original Rocket paper. These datasets span a wide range of domains and complexities, providing a robust test bed for our approach.

For each dataset, the Rocket transformation embeds the time-series data into a 10,000-dimensional space. To assess the effective dimensionality of these representations, we applied principal component analysis (PCA) and computed the number of principal components required to capture 90\% and 95\% of the total variance. Table \ref{tab:stats} summarizes key statistics (minimum, 25th percentile, median, 75th percentile, and maximum) for the number of components needed across the datasets.

\begin{table}[ht]
\centering
\caption{PCA Component Statistics: Number of Variables Required to Explain Data Variability in the Rocket Feature Space across 117 UCR Datasets}
\label{tab:stats}
\begin{tabular}{lcc}
    \toprule
    \textbf{Statistic} & \textbf{90\% Variability} & \textbf{95\% Variability} \\
    \midrule
    Minimum & 1 & 3\\ 
    25th Percentile (Q1) & 8 & 15 \\
    Median (Q2) & 15 & 39 \\
    75th Percentile (Q3) & 26 & 76 \\
    Maximum & 820 & 1474 \\
    \bottomrule
\end{tabular}
\end{table}

These results indicate that, in many cases, a small number of principal components can capture the majority of the variability in the Rocket feature space. For example, the 75th percentile for 95\% variability is only 76 components—demonstrating that, despite the initial 10,000 dimensions, the critical information is effectively condensed into a much lower-dimensional subspace. This empirical finding corroborates our theoretical framework: the PPV+bias mechanism not only isolates discriminative features but also encapsulates the essential structure of the input signal. 

Moreover, the variability observed across datasets—ranging from as few as one component to several hundred—reflects the adaptability of the Rocket transformation to different types of time-series data. This comprehensive experimental design reinforces the claim that the combined use of PPV and bias is pivotal in constructing a feature space that is both compact and highly informative.

\section{Conclusion and Future Work}
\label{sec:conclusion}

In this work, we have provided a theoretical interpretation of random kernels for time-series classification, with a specific focus on the Rocket algorithm. Our analysis frames the two-stage transformation within the paradigms of compressed sensing and signal sparsity. By modeling the transformation as a collection of Toeplitz matrices that satisfy the Restricted Isometry Property (RIP) under appropriate hyperparameter choices—particularly the kernel length \(K\)—we demonstrated that the convolved signals preserve crucial discriminative information while effectively capturing how the original signal projects onto distinct random bases.

A key insight of our study is the dual role of the \emph{proportion of positive values} (PPV) combined with bias. This mechanism not only serves as a discriminator but also functions as a measure of sparsity in the transformed signal representation. Consequently, the final feature vectors encode the inherent structure of the input across multiple bases, mapping time series with similar patterns to comparable coordinates in a low-dimensional subspace. Empirical validation on 117 UCR datasets confirms that, despite the initial 10,000-dimensional embedding generated by Rocket, the essential information is concentrated in a much smaller number of dimensions. This finding is central to understanding why Rocket performs well in practice and underscores the value of PPV+bias as an interpretative tool.

The interpretability of our approach is of paramount importance. Grounding the Rocket transformation in established theoretical principles such as the RIP and sparsity not only demystifies its effectiveness but also provides clear avenues for further optimization. This clarity fosters greater trust in the algorithm and facilitates its adaptation to other domains beyond time series, such as image and audio processing.

Looking ahead, several promising directions for future research emerge:
\begin{itemize}
    \item \textbf{Dynamic Hyperparameter Optimization:} Investigate methods to dynamically adjust kernel hyperparameters, such as kernel length \(K\) and bias values, to further enhance the extraction of discriminative features.
    \item \textbf{Broader Applications:} Extend the use of PPV+bias as a sparsity measure to unsupervised learning tasks and to other data modalities, including images and audio, to evaluate its generality and effectiveness.
    \item \textbf{Interpretability and Explainability:}  Enhancing transparency in this manner is crucial not only for building user trust but also for meeting regulatory requirements in regions where algorithmic explainability is mandated for real-world applications. By ensuring that stakeholders have clear, accessible explanations of model decisions, we can facilitate the broader adoption and responsible deployment of these algorithms in practice.
    \item \textbf{Integration with Deep Learning:} Explore the integration of random kernel methods with deep neural network architectures to combine the benefits of interpretability and the representational power of deep learning.
\end{itemize}

In summary, our work not only provides a solid theoretical foundation for the success of Rocket but also highlights the critical importance of interpretability in algorithm design. The discovery that the PPV+bias mechanism can effectively measure sparsity in the transformed signal opens up new perspectives for both supervised and unsupervised classification tasks, offering a pathway toward more transparent and robust machine learning systems.














% \section*{Declarations}
% \subsection{Funding}

% The research conducted to obtain the results presented in this paper, as well as the elaboration of the paper itself, have received funding from the following grants: Ministerio de Asuntos Económicos y Transformación Digital and the European Union-NextGenerationEU through the project PRITIA-CLOUD;  Ministerio de Ciencia e Innovación/AEI and the and the European Union-NextGenerationEU through the project Towards an Auditable Internet (AUDINT) under the grant number TED2021-132076B-I00.

% \subsection{Conflict of interest}

% The authors declare that they maintain no conflicts of interest and have no associations with or participation in any organization or entity that possesses any financial stake or non-financial interest in the topics discussed within this manuscript.

\bibliographystyle{IEEEtran}
\bibliography{bibliography}

%\end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}







\newpage
%\appendix
%\appendix


\begin{appendices}
\renewcommand{\thesection}{A\roman{section}}
\section{Lipschitz Continuity Criterion}
\label{lipschitz}
A transformation \(\Phi\) is said to be Lipschitz continuous if there exists a constant \(C\) such that
\[
    \|\Phi(g) - \Phi(f)\| \;\le\; C\,\|g - f\|
    \quad
    \text{for all } f,\,g.
\]
Our goal is to show that, with high probability (over Gaussian noise), the Rocket transform approximately satisfies such a bound.

\subsection{Step 1: Bounding the Output Difference}
By definition,
\[
  (\Phi g - \Phi f)[l]
  \;=\;
  \mathrm{PPV}(g * h_l)
  \;-\;
  \mathrm{PPV}(f * h_l).
\]
Denote
\[
  \phi[l] \;=\; (\Phi g - \Phi f)[l].
\]
Since \(\mathrm{PPV}(x)\in [0,1]\) for any real vector \(x\), it follows that each difference \(\phi[l]\in [-1,1]\).  Hence
\[
  \phi[l]^2 \;\le\; 1
  \quad\Longrightarrow\quad
  \sum_{l=0}^{L-1} \phi[l]^2 \;\le\; L.
\]
Therefore,
\[
  \|\Phi g - \Phi f\|^2
  \;=\;
  \sum_{l=0}^{L-1} \bigl(\phi[l]\bigr)^2
  \;\le\;
  L
  \quad\Longrightarrow\quad
  \|\Phi g - \Phi f\| \;\le\; \sqrt{L}.
\]

\subsection{Step 2: Distribution of the Noise Norm}
Let \(g = f + \varepsilon\).  If \(\varepsilon[n] \sim \mathcal{N}(0,1)\) i.i.d., then
\[
   \|\varepsilon\|^2
   \;=\;
   \sum_{n=0}^{N-1} \varepsilon[n]^2
   \;\sim\; \chi^2_{N}.
\]
For a chosen confidence level \(1-\alpha\), let \(x_{N,\alpha}\) be the \((1-\alpha)\)-quantile of \(\chi^2_{N}\), i.e.,
\[
   P\bigl(\,\|\varepsilon\|^2 \;\ge\; x_{N,\alpha}^2\bigr)
   \;=\;
   1-\alpha.
\]

\subsection{Step 3: High-Probability Lipschitz Bound}
Combining:
\[
   \|\Phi g - \Phi f\|^2 \;\le\; L
   \quad
   \text{and}
   \quad
   \|\varepsilon\|^2 \;\ge\; x_{N,\alpha}^2
   \text{ with probability } 1-\alpha,
\]
we get
\[
   \frac{\|\Phi g - \Phi f\|^2}{\|\varepsilon\|^2}
   \;\;\le\;\;
   \frac{L}{\|\varepsilon\|^2}
   \;\;\le\;\;
   \frac{L}{x_{N,\alpha}^2}
   \quad
   \text{with probability } 1-\alpha.
\]
Hence
\[
   P\!\Bigl(\,
      \|\Phi g - \Phi f\|^2
      \;\le\;
      \frac{L}{x_{N,\alpha}^2}\,
      \|\varepsilon\|^2
   \Bigr)
   \;\;\ge\;\;
   1-\alpha.
\]
Taking square roots yields
\[
   P\!\Bigl(\,
      \|\Phi g - \Phi f\|
      \;\le\;
      \sqrt{\frac{L}{x_{N,\alpha}^2}}
      \,\|\varepsilon\|
   \Bigr)
   \;\;\ge\;\; 1-\alpha.
\]
Thus, with probability at least \(1-\alpha\),
\[
   \|\Phi g - \Phi f\|
   \;\le\;
   \sqrt{\frac{L}{x_{N,\alpha}^2}}\,\|g - f\|.
\]
In other words, Rocket is Lipschitz with a constant
\(
   C = \sqrt{\tfrac{L}{x_{N,\alpha}^2}}
\)
(with high probability).

To concretize the stability analysis, consider a time series of length $ N = 80 $ and a ROCKET transform configured with $ L = 10{,}000 $ kernels. Utilizing the properties of the chi-squared distribution with $ N = 80 $ degrees of freedom, we determine the critical value $ x^2_{80,0.005} $ corresponding to a confidence level of $ 1 - \alpha = 0.995 $. 

    \begin{equation}
        P(\chi^2_{80} > x^2_{80,0.005}) = 0.995
    \end{equation}

From chi-squared distribution tables or statistical software, this critical value is approximately $ x^2_{80,0.005} = 51.2 $. Substituting these values into the probabilistic bound derived earlier, we obtain:

    \begin{align}
        \frac{ \| \Phi g - \Phi f \|^2 }{ \| g - f \|^2 } &\leq \frac{L}{x^2_{N,\alpha}} \\
        \frac{ \| \Phi g - \Phi f \|^2 }{ \| g - f \|^2 } &\leq \frac{10{,}000}{51.2} \\
        \frac{ \| \Phi g - \Phi f \|^2 }{ \| g - f \|^2 } &\approx 195.3
    \end{align}

Therefore, with a probability of $ 0.995 $, the ratio of the squared norms satisfies:

    \begin{align}
        P\left( \frac{ \| \Phi g - \Phi f \|^2 }{ \| g - f \|^2 } \leq 195.3 \right) \geq 0.995
    \end{align}
    
This implies that the Lipschitz continuity condition is fulfilled with a Lipschitz constant $ C = 195.3 $ at a probability of $ 99.5\% $. Consequently, the ROCKET transform ensures that the feature representation remains stable under additive Gaussian noise, preserving the classification integrity with high probability.



Considering a more general additive noise model, where the noise consists of independent and identically distributed random variables, the Central Limit Theorem (CLT) can be employed to derive similar stability results. According to the CLT, as the number of noise components increases, the distribution of the sum of these components approaches a Gaussian distribution, regardless of the original distribution of the noise. This allows for the approximation of the norm $ \| g - f \| $ and the derivation of bounds on the Lipschitz constant $ C $ under broader noise conditions.






\section{Translation invariance}
\label{translation_invariance}


We demonstrate that the Rocket transform , satisfies translation invariance under the assumption of periodic padding. Specifically, we show that for any circular shift of the input signal, the Rocket features remain unchanged.

Consider a single-channel time series signal $f$ of length $N$. The Rocket transform involves convolving $f$ with a set of random convolutional kernels $h_l$, followed by a feature extraction process. For a given kernel $h_l$, the Rocket transform at a specific feature $\Phi f_c[l]$ is defined as:

    \begin{equation}
        (\Phi f_c)[l] = \text{PPV} (f_c * h_l) = \frac{1}{N} \sum_{n=0}^{N-1} \mathbb{I}\left\{ (f_c * h_l)[n] > \text{bias} \right\}
    \end{equation}

Expanding the convolution:

    \begin{equation}
        (\Phi f_c)[l] = \frac{1}{N} \sum_{n=0}^{N-1} \mathbb{I}\left\{ \sum_{m=-M}^{M} f_c[n - m] h_l[m] > \text{bias} \right\}
    \end{equation}

Here, $M$ is the half-width of the convolutional kernel $h_l$, and $\mathbb{I}$ is the indicator function that outputs 1 if the condition inside is true and 0 otherwise.

To analyze the translation invariance, perform a change of variable by letting $n = p + c$, where $c$ is an arbitrary shift. Substituting into the equation:

    \begin{align*}
        (\Phi f_c)[l] &= \frac{1}{N} \sum_{p=-c}^{N-1 - c} \mathbb{I}\left\{ \sum_{m=-M}^{M} f_c[p + c - m] h_l[m] > \text{bias} \right\} \nonumber \\
        &= \frac{1}{N} \sum_{p=-c}^{N-1 - c} \mathbb{I}\left\{ \sum_{m=-M}^{M} f[p - m] h_l[m] > \text{bias} \right\}
    \end{align*}

Assuming periodic padding (i.e., the signal is extended by wrapping around), the above expression simplifies. Specifically, because of periodicity, shifting the index does not affect the overall sum:

    \begin{equation}
        f_c[p + c - m] = f[p - m]
    \end{equation}

Thus, the transformed feature becomes:

\begin{equation}
    (\Phi f_c)[l] = \frac{1}{N} \sum_{p=-c}^{N-1 - c} \mathbb{I}\left\{ \sum_{m=-M}^{M} f[p - m] h_l[m] > \text{bias} \right\}
\end{equation}


To address the summation limits, we split the sum into two parts:

    \begin{align*}
        (\Phi f_c)[l]
        &= \frac{1}{N} \bigg( \sum_{p=-c}^{-1} \mathbb{I} \left\{ \sum_{m=-M}^{M} f[p - m] h_l[m] > \text{bias} \right\}  \\
        &+ \sum_{p=0}^{N-1 - c} \mathbb{I}\left\{ \sum_{m=-M}^{M} f[p - m] h_l[m] > \text{bias} \right\} \bigg) \nonumber \\
        &= \frac{1}{N} \sum_{p=0}^{N-1} \mathbb{I}\left\{ \sum_{m=-M}^{M} f[p - m] h_l[m] > \text{bias} \right\} \nonumber \\
        &= (\Phi f)[l]
    \end{align*}

The first summation accounts for the wrap-around due to periodic padding, ensuring that all indices $p - m$ are valid within the signal's range.


By equating $(\Phi f_c)[l]$ to $(\Phi f)[l]$, we establish that the Rocket transform operator $\Phi$ is \textbf{translation invariant}. This means that shifting the input signal $f$ by any integer $c$ does not alter the extracted Rocket features, provided that periodic padding is used.

By maintaining consistent feature representations regardless of shifts, Rocket enhances classification accuracy and generalization across varied datasets.




% \subsubsection*{Translation Invariance of the Rocket Transform}

% Under periodic padding, the Rocket transform is translation invariant. Let $f$ be a time series of length $N$, and $f_c$ its circular shift by $c$, i.e., $f_c[n] = f[(n - c) \mod N]$. The Rocket feature for kernel $h_l$ is:
% \[
% (\Phi f)[l] = \frac{1}{N} \sum_{n=0}^{N-1} \mathbb{I}\left( \sum_{m=-M}^{M} f[n - m] h_l[m] > \mathrm{bias} \right),
% \]
% where $h_l$ is a kernel of half-width $M$, and $\mathbb{I}$ is the indicator function.

% \subsection*{Proof of Invariance}
% For the shifted signal $f_c$, the feature becomes:
% \[
% (\Phi f_c)[l] = \frac{1}{N} \sum_{n=0}^{N-1} \mathbb{I}\left( \sum_{m=-M}^{M} f_c[n - m] h_l[m] > \mathrm{bias} \right).
% \]
% Substitute $f_c[n - m] = f[(n - m - c) \mod N]$ and let $p = (n - c) \mod N$. Due to periodicity, the sum over $n$ becomes a sum over $p$:
% \[
% (\Phi f_c)[l] = \frac{1}{N} \sum_{p=0}^{N-1} \mathbb{I}\left( \sum_{m=-M}^{M} f[p - m] h_l[m] > \mathrm{bias} \right) = (\Phi f)[l].
% \]
% Thus, $\Phi$ is translation invariant. \qed








% [3] F. Krahmer et al., "Beyond incoherence: Stable and robust sampling strategies for compressive imaging," IEEE TIT, 2021.
% [4] J. Romberg et al., "Compressed sensing by random convolution," SIAM J. Imaging Sciences, 2009.
% [5] S. Mendelson et al., "Compressed sensing with coherent and redundant dictionaries," Applied and Computational Harmonic Analysis, 2020.



































\section{Coherence of a Toeplitz Matrix}
\label{toeplitz_coherence}

We analyze the coherence of a Toeplitz matrix of size \(N\) with kernel length \(K\). The coherence is bounded by studying the probability that the maximum off-diagonal entry exceeds a threshold \(\alpha\):

\[
  \mathbb{P}\left( \max_{i<j} \lvert T_{ij} \rvert > \alpha \right) 
  = \mathbb{P}\left( \bigcup_{i<j} \{\lvert T_{ij} \rvert > \alpha\} \right) 
  \leq \sum_{i<j} \mathbb{P}\left( \lvert T_{ij} \rvert > \alpha \right).
\]

The event \(\{\max_{i<j} \lvert T_{ij} \rvert > \alpha\}\) corresponds to at least one pair \((i,j)\) having \(\lvert T_{ij} \rvert > \alpha\). Applying the union bound (Boole's inequality) and then Chebyshev's inequality to each term in the sum gives:

\[
  \sum_{i<j} \mathbb{P}\left( \lvert T_{ij} \rvert > \alpha \right) 
  \leq \sum_{i<j} \frac{\mathrm{Var}[T_{ij}]}{\alpha^2}.
\]

\textbf{Mean and Variance Analysis }
Assume the entries \(h_i\) are i.i.d. with \(\mathbb{E}[h_i] = 0\) and \(\mathrm{Var}[h_i] = \frac{1}{K}\). For \(T_{01}\), which is a sum of products of adjacent \(h_i\):

\[
  T_{01} = h_0h_1 + h_1h_2 + \cdots + h_{K-2}h_{K-1},
\]

the expectation vanishes due to independence and zero-mean properties:

\[
  \mathbb{E}[T_{01}] = \sum_{k=0}^{K-2} \mathbb{E}[h_kh_{k+1}] = \sum_{k=0}^{K-2} \mathbb{E}[h_k]\mathbb{E}[h_{k+1}] = 0.
\]

For the variance, we exploit the independence of cross-terms. Covariances vanish because non-overlapping products are uncorrelated:

\[
  \mathrm{Var}[T_{01}] = \sum_{k=0}^{K-2} \mathrm{Var}[h_kh_{k+1}] = \sum_{k=0}^{K-2} \mathrm{Var}[h_k]\mathrm{Var}[h_{k+1}].
\]

Since \(\mathrm{Var}[h_k] = \frac{1}{K}\), this simplifies to:

\[
  \mathrm{Var}[T_{01}] = (K-1) \left(\frac{1}{K}\right)^2 = \frac{K-1}{K^2}.
\]

\vspace{1mm}
\textbf{Generalization.}
For any \(T_{ij}\), the number of overlapping terms \(\theta_{ij}\) corresponds to the kernel length minus the shift between indices \(i\) and \(j\). In the case of adjacent indices (e.g., \(T_{01}\)), \(\theta_{01} = K-1\). Substituting into the variance bound:

\[
  \sum_{i<j} \frac{\mathrm{Var}[T_{ij}]}{\alpha^2} 
  = \sum_{i<j} \frac{\theta_{ij}}{K^2 \alpha^2}.
\]

The Toeplitz structure ensures \(\theta_{ij}\) depends only on the relative position of \(i\) and \(j\), simplifying the final coherence bound.



\textbf{Calculation of \(\theta_{ij}\). } 
Define \(\theta_{ij}\) as the number of overlapping terms between columns \(i\) and \(j\) in the Toeplitz matrix:
\[
\theta_{ij} \;=\;
\begin{cases}
K - (j - i), & \text{if } j - i < K, \\
0, & \text{otherwise}.
\end{cases}
\]
To compute \(\sum_{i<j} \theta_{ij}\), observe that \(j - i < K\) partitions the summation into overlapping windows. For each row \(i\), the valid column indices \(j\) satisfy \(i+1 \leq j \leq i+K-1\). Substituting \(r = j - i\), the inner sum becomes:
\[
\sum_{j=i+1}^{i+K-1} \bigl[K - (j - i)\bigr] 
= \sum_{r=1}^{K-1} (K - r) 
= \sum_{s=1}^{K-1} s 
= \frac{K(K-1)}{2},
\]
where \(s = K - r\) reindexes the arithmetic series.  

Since the Toeplitz matrix has \(N\) rows and each row contributes \(\frac{K(K-1)}{2}\) overlapping terms, the total sum is:
\[
\sum_{i<j} \theta_{ij} 
= N \cdot \frac{K(K-1)}{2}.
\]


Summarizing, we obtain
\[
\begin{aligned}
\mathbb{P}\Bigl(\max_{i<j}\lvert T_{ij}\rvert \;>\;\alpha\Bigr)
&\;\;\le\;\;
\sum_{i<j}\mathbb{P}\bigl(\lvert T_{ij}\rvert>\alpha\bigr)
\;\;\le\;\;
\sum_{i<j}\frac{\mathrm{Var}[T_{ij}]}{\alpha^2}
\\[4pt]
&=\;
\sum_{i<j}\frac{\theta_{ij}}{K^2\,\alpha^2}
\;=\;
N\;\cdot\;\frac{(K-1)}{2\,K\,\alpha^2}.
\end{aligned}
\]
Hence, as \(K\) or \(N\)  increases, the probability of a higher coherence of the Toeplitz matrix grows as well.

\section{PPV as sparsity}
\label{ppv_sparsity}
\newcommand{\I}{\mathbb{I}} 


%\section*{Analysis of \( S(\mathbf{c}) = \frac{1}{\mathrm{PPV}(\mathbf{c}) \)}

\subsection*{Property D1 (Robin Hood)}
\textbf{Requirement}: For \( c_i > c_j \) and \( 0 < \alpha < \frac{c_i - c_j}{2} \), transferring \( \alpha \) from \( c_i \) to \( c_j \) must strictly decrease sparsity.

\begin{proof}[Counterexample]
Let \( c_i > 0 \) and \( c_j > 0 \). After transfer:
\begin{align*}
\mathrm{PPV}(\mathbf{c}') = \mathrm{PPV}(\mathbf{c}) 
\implies S(\mathbf{c}') = S(\mathbf{c})
\end{align*}
Violates strict inequality requirement. Thus, \( S(\mathbf{c}) \) fails D1.
\end{proof}

\subsection*{Property D2 (Scaling Invariance)}
\textbf{Requirement}: For \( \alpha > 0 \), \( S(\alpha \mathbf{c}) = S(\mathbf{c}) \).

\begin{align*}
    & \mathrm{PPV}(\alpha \mathbf{c}) = \frac{1}{n} \sum_{k=1}^n \I\{\alpha c_k > 0\}
    = \frac{1}{n} \sum_{k=1}^n \I\{c_k > 0\}
    = \mathrm{PPV}(\mathbf{c}) \\[4pt]
    & \implies S(\alpha \mathbf{c}) = S(\mathbf{c})
\end{align*}




\( S(\mathbf{c}) \) satisfies D2.

\subsection*{Property D3 (Rising Tide)}
\textbf{Requirement}: Adding \( \alpha > 0 \) to all components must decrease sparsity.

\begin{proof}[Failure Case]
For small \( \alpha \) that doesn't flip any signs:
\begin{align*}
\mathrm{PPV}(\alpha + \mathbf{c}) &= \mathrm{PPV}(\mathbf{c}) \\[4pt]
\implies S(\alpha + \mathbf{c}) &= S(\mathbf{c})
\end{align*}
Violates strict decrease requirement. Thus, \( S(\mathbf{c}) \) fails D3.
\end{proof}

\subsection*{Property D4 (Cloning Invariance)}
\textbf{Requirement}: \( S(\mathbf{c} \,\|\, \mathbf{c}) = S(\mathbf{c}) \).

\begin{align*}
&\mathrm{PPV}(\mathbf{c} \,\|\, \mathbf{c}) = \frac{2\sum_{k=1}^n \I\{c_k > 0\}}{2n} 
= \mathrm{PPV}(\mathbf{c}) \\[4pt]
&\implies S(\mathbf{c} \,\|\, \mathbf{c}) = S(\mathbf{c})
\end{align*}
\( S(\mathbf{c}) \) satisfies D4.

\subsection*{Property P1 (Bill Gates)}
\textbf{Requirement}: Exists \( \beta_i > 0 \) such that \( \forall \alpha > 0 \):
\[ S(\dots,c_i+\beta_i+\alpha,\dots) > S(\dots,c_i+\beta_i,\dots) \]

\begin{proof}[Failure]
If \( c_i+\beta_i > 0 \), adding \( \alpha \) doesn't change positivity count. If \( c_i+\beta_i \leq 0 \), large \( \alpha \) might decrease \( S(\mathbf{c}) \). No \( \beta_i \) satisfies requirement. Thus, \( S(\mathbf{c}) \) fails P1.
\end{proof}

\subsection*{Property P2 (Babies)}
\textbf{Requirement}: \( S(\mathbf{c}\,\|\,0) > S(\mathbf{c}) \).

\begin{align*}
    &\mathrm{PPV}(\mathbf{c}\,\|\,0) = \frac{\sum \I\{c_k > 0\}}{n+1} 
    < \frac{\sum \I\{c_k > 0\}}{n} 
    = \mathrm{PPV}(\mathbf{c}) \\[4pt]
    &\implies S(\mathbf{c}\,\|\,0) = \frac{n+1}{\sum \I\{c_k > 0\}} > S(\mathbf{c})
\end{align*}

\( S(\mathbf{c}) \) satisfies P2.


\section*{Summary Table}
\begin{center}
\begin{tabular}{|l|c|}
\hline
\textbf{Property} & \textbf{Satisfied?} \\ 
\hline
D1 (Robin Hood) & \text{No} \\
D2 (Scaling) & \text{Yes} \\
D3 (Rising Tide) & \text{No} \\
D4 (Cloning) & \text{Yes} \\
P1 (Bill Gates) & \text{No} \\
P2 (Babies) & \text{Yes} \\
\hline
\end{tabular}
\end{center}

\section*{Conclusion}
The reciprocal PPV measure \( S(\mathbf{c}) = \frac{1}{\mathrm{PPV}(\mathbf{c})} \) satisfies scaling invariance (D2), cloning invariance (D4), and the Babies property (P2), but fails to satisfy the Robin Hood (D1), Rising Tide (D3), and Bill Gates (P1) properties. This demonstrates its utility as a simple count-based sparsity measure with particular sensitivity to zero components, while lacking sensitivity to magnitude redistribution.


\end{appendices}





















% that's all folks
\end{document}
