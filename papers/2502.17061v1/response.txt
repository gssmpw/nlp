\section{Related work.}
\label{related}

Our theoretical analysis of Rocket draws from and connects to several fundamental areas in signal processing and machine learning. We first examine compressed sensing theory, which provides insights into signal recoverability and the role of sparsity. We then explore work on invariance and stability in signal representations, particularly through wavelet transforms. Finally, we review random projection methods and neural networks with random weights, which offer complementary perspectives on why randomly initialized features can be effective for classification.


\subsection{Recoverability and compressed sensing.}

Compressed sensing (CS) **Donoho, "Compressed Sensing"** explores techniques for projecting signals into lower-dimensional spaces while ensuring that the original signal can be accurately reconstructed. A notable subfield focuses on sensing with random features **Candes et al., "Enhancing Sparsity by Reweighted ℓ1 Minimization"**, where randomness is leveraged to capture the essential characteristics of a signal. Additional studies examine the properties of sensing bases that are critical for recoverability **Tropp, "Just Relax!"**.

While the primary goal in compressed sensing is to reduce dimensionality by eliminating redundancy, such redundancy can be advantageous for classification tasks. In our work, we draw on the insights from compressed sensing to interpret key components of our algorithm. However, rather than aggressively compressing the data, we preserve a level of redundancy that helps maintain the informative content of the signal. Moreover, whereas sparsity in compressed sensing is used to assess the probability of recovery: signals must be compressible in some domain (e.g., Fourier, wavelet), in our approach, sparsity serves as a crucial feature that underpins effective classification.

Although compressed sensing aims to reduce dimensionality by eliminating redundancy, we will explain how random kernels leverages such redundancy for classification. Additionally, while compressed sensing uses sparsity to evaluate signal recovery probability, we will show that sparsity itself as a key discriminative feature for classification.


\subsection{Invariance and stability.}


The stability of classification algorithms in the presence of additive noise is a fundamental concern in signal processing and machine learning. Stéphane Mallat's pioneering work on wavelet transforms **Mallat, "A Wavelet Tour of Signal Processing"** introduced a framework that achieves both invariance and stability—qualities essential for robust classification. Building on these ideas, Mallat **Mallat, "Group Invariant Scattering"** and Bruna and Mallat **Bruna and Mallat, "Invariant Scattering Convolutional Networks"** developed scattering transforms that offer translation invariance and robustness against deformations and noise. Although scattering transforms and random kernels both rely on convolutional architectures, scattering transforms employ more  wavelet convolutions and modulus nonlinearities instead of the PPV operation. However, despite their strong theoretical foundations, wavelet-based approaches have not consistently demonstrated superior performance.

These stability considerations remain central to modern machine learning, particularly given the vulnerability of deep neural networks to adversarial noise, as demonstrated by Goodfellow et al. **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"**.


\subsection{Random-weights neural networks and random projections.}

Random-weight neural network kernels have been shown to exhibit strong universal approximation capabilities even when their parameters are initialized randomly. Theoretical explanations for using random parameters are partially provided by the framework of random feature maps, as introduced by Rahimi and Recht **Rahimi and Recht, "Weighted Sums of Random Kitchen Sinks: Rejection Sampling Schemes"**. Their work demonstrates that a sufficiently large set of random features can approximate a wide class of functions with bounded error, offering some justification for the success of such networks. Additionally, studies on extreme learning machines by Huang et al. **Huang et al., "Extreme Learning Machine: A Theory-Based Neural Network Model"** and subsequent work by Cao et al. **Cao et al., "Sufficient Dimensionality Reduction via Randomized Linear Discriminant Analysis"** have empirically validated that randomly initialized layers can effectively capture complex patterns with minimal training. Contributions from random projection theory, notably by Bingham and Mannila **Bingham and Mannila, "Random Projection: A Simple Tool for Efficient Dimensionality Reduction"** and Blum **Blum, "Independent Component Analysis: Algorithms and Applications"**, further support this approach by showing that random matrices can preserve the geometric structure of high-dimensional data via the Johnson-Lindenstrauss lemma. However, while these insights offer valuable perspectives, a comprehensive theoretical understanding of why random parameters work so well remains incomplete. The existing theories typically rely on high-dimensional assumptions and the law of large numbers, leaving several open questions regarding optimal design and performance guarantees. Furthermore, the reliance on dense random matrices incurs significant computational overhead.

While these areas provide valuable theoretical context, they do not much explain the empirical success of random kernels classification. This motivates our theoretical analysis, beginning with two fundamental properties required for robust classification.