\section{Related work}
%
\label{sec:related_work}
Our work is part of the broad field of geometric deep learning____.
In this section, we aim to discuss the most relevant work to the present paper, but we reference many other relevant works throughout.

\subsection{Equivariant networks for image input}\label{sec:equi_image}
Most work on equivariant networks on image input concerns convolutional networks (ConvNets)____.
ConvNets are themselves equivariant to (cyclic) image translations.
The ConvNets in this paper are built on ConvNeXt____, which is a modern variant with state-of-the-art classification performance.

Making ConvNets equivariant to rotations and reflections was done by ____ and ____, with many follow-up works.
The unifying framework of steerable $E(2)$-equivariant ConvNets____, subsumes much of the prior and subsequent work and proposes decomposing the feature spaces into irreps (irreducible representations, see Section~\ref{sec:groups}) of the symmetry group.
Our ConvNeXt-variants are special cases of this general framework too.

Apart from ConvNets, there are works proposing attention- and transformer-based equivariant vision architectures____.
Most similar to our ViT-based networks are the $\mathrm{SO}(2)$-steerable transformers by ____.
The main differences are the group considered, the fact that they use complex-valued features, the nonlinearities used and the type of positional embeddings.
However, the main idea of parametrizing the features in terms of irreps (following steerable ConvNets) and modifying the ViT architecture accordingly is the same in our ViTs and the one by ____.


\subsection{Equivariant networks for other input than images}

____ found that the equivariant transformer GATr____ outperformed non-equivariant transformers on a task of 3D rigid-body interactions, both in terms of parameter-efficiency and learning-efficiency, i.e., the number of FLOPs required during training.
Unlike our models, the number of FLOPs-per-parameter remained higher than that of the non-equivariant transformer.


____ proposed a fast equivariant group convolutional network on point cloud input by using separable group convolutions on position orientation space $\mathbb{R}^3 \times S^2$ and parallelizing the message passing step. 
Their networks use ConvNeXt-style blocks, as do some of our networks.

In contrast to our networks, none of the mentioned works develop networks that are directly comparable with non-equivariant ones, with a similar FLOPs-per-parameter ratio.