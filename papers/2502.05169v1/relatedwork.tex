\section{Related work}
%
\label{sec:related_work}
Our work is part of the broad field of geometric deep learning~\cite{bronsteinGeometricDeepLearning2021}.
In this section, we aim to discuss the most relevant work to the present paper, but we reference many other relevant works throughout.

\subsection{Equivariant networks for image input}\label{sec:equi_image}
Most work on equivariant networks on image input concerns convolutional networks (ConvNets)~\cite{fukushima1975cognitron, lecun-1989}.
ConvNets are themselves equivariant to (cyclic) image translations.
The ConvNets in this paper are built on ConvNeXt~\cite{liu2022convnext}, which is a modern variant with state-of-the-art classification performance.

Making ConvNets equivariant to rotations and reflections was done by \citet{cohen2016group} and \citet{dieleman2016exploiting}, with many follow-up works.
The unifying framework of steerable $E(2)$-equivariant ConvNets~\cite{cohen-steerable, weiler_cesa_2019}, subsumes much of the prior and subsequent work and proposes decomposing the feature spaces into irreps (irreducible representations, see Section~\ref{sec:groups}) of the symmetry group.
Our ConvNeXt-variants are special cases of this general framework too.

Apart from ConvNets, there are works proposing attention- and transformer-based equivariant vision architectures~\cite{romero2020attentive, xu20232}.
Most similar to our ViT-based networks are the $\mathrm{SO}(2)$-steerable transformers by \citet{steerabletransformers:2024}.
The main differences are the group considered, the fact that they use complex-valued features, the nonlinearities used and the type of positional embeddings.
However, the main idea of parametrizing the features in terms of irreps (following steerable ConvNets) and modifying the ViT architecture accordingly is the same in our ViTs and the one by \citet{steerabletransformers:2024}.


\subsection{Equivariant networks for other input than images}

\citet{brehmer2024does} found that the equivariant transformer GATr~\cite{brehmer2024geometric} outperformed non-equivariant transformers on a task of 3D rigid-body interactions, both in terms of parameter-efficiency and learning-efficiency, i.e., the number of FLOPs required during training.
Unlike our models, the number of FLOPs-per-parameter remained higher than that of the non-equivariant transformer.


\citet{bekkers2024fast} proposed a fast equivariant group convolutional network on point cloud input by using separable group convolutions on position orientation space $\mathbb{R}^3 \times S^2$ and parallelizing the message passing step. 
Their networks use ConvNeXt-style blocks, as do some of our networks.

In contrast to our networks, none of the mentioned works develop networks that are directly comparable with non-equivariant ones, with a similar FLOPs-per-parameter ratio.