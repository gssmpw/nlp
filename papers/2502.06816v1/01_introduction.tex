\section{Introduction} \label{Sec:Intro}
%Training representation learning models and extending them across various tasks have become the de-facto pipeline in various fields like computer vision (CV) and natural language processing (NLP). Inspired by these methodologies, the Electronic Design Automation (EDA) field has shown increasing interest in training circuit representation learning models~\cite{chen2024large}. Previous researches, e.g. DeepGate Family~\cite{li2022deepgate, shi2023deepgate2, shi2024deepgate3}, FGNN~\cite{wang2022functionality}, Gamora~\cite{wu2023gamora} and HOGA~\cite{deng2024less} introduce the circuit learning models, trained to capture rich embeddings related to circuit functionality and structure, can be finetuned and deployed efficiently for various downstream tasks. Notably, these models have shown substantial improvements in tasks like testability analysis~\cite{shi2022deeptpi}, circuit identification~\cite{wu2023gamora} and logic verification~\cite{shi2024eda}. 

Representation learning has emerged as a powerful paradigm across domains such as computer vision (CV) and natural language processing (NLP), where pretrained models finetuned for specific tasks achieve state-of-the-art results. Inspired by these advancements, the field of Electronic Design Automation (EDA) has explored similar methodologies for circuit representation learning. Existing approaches, such as DeepGate Family~\cite{li2022deepgate, shi2023deepgate2}, FGNN~\cite{wang2022functionality}, Gamora~\cite{wu2023gamora} and HOGA~\cite{deng2024less}, demonstrate significant improvements in tasks like testability analysis, circuit identification, and design verification by focusing on And-Inverter Graphs (AIGs). However, this narrow reliance on AIGs limits their generalization to more complex and practical circuit abstractions.

\iffalse
Despite these advancements, existing representation learning models primarily focus on And-Inverter Graphs (AIGs), an intermediate representation in logic synthesis. This narrow focus on a specific circuit format limits the generalization ability of circuit representation learning models. For instance, existing models are unable to handle practical tasks on netlists composed of arbitrary gates beyond AND and NOT, such as Design-for-Test (DFT), Engineering Change Order (ECO), and technology mapping. How to extend these models to handle post-mapping (PM) netlists composed of standard cells or arbitrary logic units (e.g. NAND, NOR, XOR or MAJ) remains an open question. 

Unfortunately, training a representation model for PM netlists is not as straightforward as deploying an AIG-based model on a graph with diverse gate types. The primary challenge lies in designing a pretraining task that can effectively capture the rich and complex representations of individual standard cells, accommodating the intricate characteristics and heterogeneous nature of these PM netlists. Existing pretraining approaches have limitation in adapting to PM netlist learning. On the one hand, supervised methods, like DeepGate2~\cite{shi2023deepgate2}, require simulations to identify functional equivalence as labels, while few cells are equivalent in the netlists after logic optimization. Additionally, recognition of XOR and MAJ functions~\cite{wu2023gamora} is incompatible with circuits already mapped into standard cells. On the other hand, FGNN~\cite{wang2022functionality} proposes to differentiate equivalent and non-equivalent circuits in unsupervised manner. However, such contrastive learning approach are inefficient since it requires constructing vast numbers of sample pairs to achieve reliable learning outcomes. 
\fi 

Post-mapping (PM) netlists, composed of diverse standard cells, represent a key stage in the design flow but remain underexplored in circuit representation learning. These netlists introduce challenges due to their structural and functional heterogeneity, which is difficult to be effectively captured by existing AIG-based methods. Current solutions, such as simulation-based supervised training~\cite{shi2023deepgate2} or contrastive learning~\cite{wang2022functionality}, struggle with scalability and efficiency in handling PM netlists. This gap hinders progress in critical post-mapping tasks like technology mapping and functional Engineering Change Orders (ECO).

%To overcome the above limitation, we propose DeepCell, a novel circuit representation learning framework designed to represent PM netlists using a multiview learning mechanism. Instead of relying on complicated pretraining tasks, our approach incorporates rich information from the AIG view, enabling a more efficient and effective representation of PM netlists. Specifically, our framework comprises a trainable Graph Neural Network (GNN)-based PM encoder and a pretrained AIG encoder. Inspired by the Masked Language Modeling (MLM) technique~\cite{devlin2018bert} from natural language processing (NLP), where models are trained to predict masked tokens based on surrounding context, DeepCell introduces Mask Circuit Modeling (MCM) to mask and reconstruct embeddings by leveraging the remaining cells in PM netlists and gate representations from AIGs. Consequently, DeepCell effectively captures general functional information of cells in PM netlists in self-supervised manner and supports flexible deployment for a variety of practical tasks. 

To address these challenges, we propose \textbf{DeepCell}, a novel multiview representation learning framework for PM netlists. DeepCell integrates information from both PM netlists and AIGs using a Graph Neural Network (GNN)-based PM encoder and a pretrained AIG encoder. At its core, DeepCell employs \emph{Mask Circuit Modeling (MCM)}, a self-supervised mechanism inspired by Masked Language Modeling (MLM)~\cite{devlin2018bert}, which leverages AIG embeddings to refine PM netlist representations. By bridging the structural-functional gap inherent in PM netlists, DeepCell achieves rich and generalizable embeddings. We then validate DeepCell through its application to the functional ECO task, a critical post-mapping challenge involving design modification after tape-out~\cite{huang2013match}. Integrated as a plug-in to an existing ECO tool, DeepCell significantly reduces patch generation costs and runtime. 

%Our experimental results highlight the effectiveness of the proposed multiview training strategy, which significantly reduces training loss compared to models trained solely on PM netlists. Additionally, DeepCell outperforms general GNNs such as GCN~\cite{kipf2016semi} and GAT~\cite{velivckovic2017graph}, as well as the domain-specific model DeepGate2~\cite{shi2023deepgate2}. Furthermore, we demonstrate the utility of DeepCell in the functional ECO task. DeepCell can be seamlessly integrated as a plug-in into existing ECO tools with minimal engineering effort, leading to substantial reductions in both cost and runtime for the ECO task. 



% which involves generating a patch to modify the functionality of a PM netlist. Existing ECO tools face performance bottlenecks due to the vast space of candidate signals, which significantly hinders their efficiency. By finetuning DeepCell, we enable it to predict potential driven signals, thereby substantially reducing the search space. Moreover, DeepCell can be seamlessly incorporated as a plug-in to existing ECO tools with minimal engineering effort. This integration results in significant reductions in both the cost and runtime of the ECO task. 


Our contributions are summarized as follows:
\begin{itemize}
\iffalse
    \item We propose a circuit representation learning framework, DeepCell, designed to learn a general representation of post-mapping netlists. Leveraging a novel netlist feature extraction mechanism and model architecture, DeepCell generates cell-level embeddings that effectively capture functionality across arbitrary technology libraries. 
    \item To the best of our knowledge, DeepCell is the first work to incorporate multiview learning in the field of circuit representation learning. We introduce Mask Circuit Modeling (MCM), which masks cells in the PM netlist and reconstructs their embeddings using the view derived from the AIG. 
    \item We employ DeepCell in a practical EDA application, Functional ECO. By incorporating DeepCell, the ECO solution can reduces low-quality candidate signals, reduces runtime by 15\%, reduce the cost and gates on the generated patches by an average of 6\% and 15\%, respectively.
\fi 
    \item We propose DeepCell, the first multiview and multimodal representation learning framework tailored for PM netlists, integrating structural and functional insights from diverse standard cells.
    \item We introduce Mask Circuit Modeling (MCM), a self-supervised mechanism for refining PM netlist embeddings using AIG-based representations.
    \item We demonstrate the utility of DeepCell in functional ECO, achieving reductions in patch generation costs, gate count, and runtime while maintaining high-quality results.

\end{itemize}

The remainder of this paper is organized as follows: Section \ref{Sec:Related} reviews related work. Section \ref{Sec:Method} describes the proposed DeepCell framework, including architecture and Mask Circuit Modeling mechanism. Section \ref{Sec:Experiment} presents the pretraining results and investigate the effect of proposed training strategy. Next, we apply DeepCell in functional ECO tasks. Finally Section \ref{Sec:Conclusion} concludes this paper. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/overview.pdf}
\vspace{-5pt}
    
    \caption{Overview of DeepCell}
    \label{fig:overview}
\vspace{-15pt}
    
\end{figure*}
