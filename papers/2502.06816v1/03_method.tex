\section{Methodology} \label{Sec:Method}
\subsection{Overview}
Fig.~\ref{fig:overview} presents the overview of DeepCell framework, which consists of an AIG encoder, a PM encoder and a Transformer model. The framework operates in two stages to capture the general representation of post-mapping (PM) netlists. In Stage 1, the PM encoder is trained to learn cell embeddings (\textit{Cell Emb.}) from the PM netlist. In Stage 2, the AIG encoder, which has been pretained and frozen, is utilized to extract gate embeddings (\textit{Gate Emb.}) from the corresponding AIG netlist. Then, we mask a random subset of the cell embeddings and reconstruct these masked embeddings using the gate embeddings. 

\subsection{PM Encoder}
Given a PM netlist, we convert it into graph format $\mathcal{G}^P=(\mathcal{V}^P, \mathcal{E}^P)$, where each standard cell is represented as node and each wire is treated as edge on the graph. 

\subsubsection{Node Features}
PM netlists consist of a wide variety of standard cells, making it impractical to represent them using one-hot encoding for each type of cell. Therefore, To address this, we embed the truth table of each standard cell into its corresponding node feature $x_{i}$. Formally, the node feature encoding is defined in in Eq.~\eqref{eq:feature}, where $D$ is the dimension of node feature vector and $tt_{i}$ represents the 0/1 truth table vector of standard cell $i$. The truth table $tt_i$ is repeated until the node feature vector reaches the specified dimension $D$. In the default setting, we assign $D=64$, ensuring that this encoding mechanism is adaptable to PM netlists across various technology libraries and supports arbitrary logic units with up to 6 inputs. 
\begin{equation} \label{eq:feature}
    x_i = repeat(tt_i, D)
\end{equation}

For example, the cell \texttt{xor2\_1} defines XOR functionality with 2 inputs and 1 output. Its truth table, \texttt{0110}, is extracted from the technology library and expanded into a 64-dimensional feature vector by repeating the pattern. Thus, the node feature of the \texttt{xor2\_1} cell becomes \texttt{0110 0110 ... 0110}.

\subsubsection{Aggregator}
We introduce a DAG-based GNN to encode circuit graph into embedding vectors $\mathbf{H}^P$. For each cell $i\in \mathcal{V}^P$, its representation vector is denoted as $H_i^P = \{hs_i^P, hf_i^P\}$, where $hs_i^P$ and $hf_i^P$ are the structural and functional embeddings, $H_i^P \in \mathbf{H}^P$. 
To compute these embeddings, we propose two aggregators: $aggr^{s}$ for structural message aggregation and $aggr^{f}$ for functional message aggregation.

For structural embedding aggregation, $aggr^{s}$ is implemented using the GCN aggregator~\cite{kipf2016semi}, which aggregates messages from the predecessors of $i$. Here, $\mathcal{P}(i)$ denotes the set of fanin cells of $i$: 
\begin{equation} \label{eq:aggrhs}
    hs_i = aggr^s(\{hs_j | j\in \mathcal{P}(i)\})
\end{equation}

For functional embedding aggregation, $aggr^{f}$ is implemented using a self-attention aggregator~\cite{velivckovic2017graph} to distinguish the functionality of the predecessors. Unlike AIG netlists, which consist solely of AND gates and inverters, PM netlists contain diverse standard cells. To account for this diversity, we differentiate cells using their node features $x_i$ and introduce an update function, $update$. Formally, the functional aggregation process in DeepCell is defined as Eq.~\eqref{eq:aggrhf}, with the $update$ function implemented as a multi-layer perceptron (MLP).  

\begin{equation} \label{eq:aggrhf}
    \begin{split}
        msg_i & = aggr^f(\{cat(hs_j, hf_j) | j \in \mathcal{P}(i)\}) \\ 
        hf_i & = update(msg_i, x_i) 
    \end{split}
\end{equation}

Finally, the embeddings of PM netlist are denoted as Eq.~\eqref{eq:pm}, where $encoder$ is the above GNN-based PM encoder. 
\begin{equation} \label{eq:pm}
    \begin{split}
        \mathbf{H}^P & = encoder(\mathcal{G}^P) \\ 
        & H_i^P \in \mathbf{H}^P, i \in \mathcal{V}^P 
    \end{split}
\end{equation}

\subsection{AIG Encoder}
Our DeepCell framework incorporates a multiview representation learning mechanism, enabling it to learn cell embeddings in PM netlists from an additional perspective provided by AIGs. Specifically, given a PM netlist $\mathcal{G}^P=(\mathcal{V}^P, \mathcal{E}^P)$, we convert it into an AIG netlist $\mathcal{G}^A=(\mathcal{V}^A, \mathcal{E}^A)$. We then employ DeepGate2~\cite{shi2023deepgate2} as the AIG encoder to derive the gate-level embeddings $H_j^A\in\mathbf{H}^A$ as formulated in Eq.~\eqref{eq:aig}. 

\begin{equation} \label{eq:aig}
    \begin{split}
        \mathbf{H}^A & = DeepGate2(\mathcal{G}^A) \\
        & H_j^A \in \mathbf{H}^A, j \in \mathcal{V}^A 
    \end{split}
\end{equation}

\subsection{Mask Circuit Modeling}
\begin{figure} [!t]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/mcm.pdf}
    \vspace{-5pt}
    \caption{Mask Circuit Modeling}
    \label{fig:mcm}
    \vspace{-15pt}
\end{figure}

We refine the cell embeddings $\mathbf{H}^P$ using information from the AIG view by mask circuit modeling (MCM). As shown in Fig.~\ref{fig:mcm}, given a PM netlist $\mathcal{G}^P$, we convert $\mathcal{G}^P$ to AIG Netlist $\mathcal{G}^A$. Both $\mathcal{G}^P$ and $\mathcal{G}^A$ are encoded by PM encoder and AIG encoder in Eq.~\eqref{eq:pm} and Eq.~\eqref{eq:aig}, respectively. 

To apply MCM, we randomly select node $p \in \mathcal{G}^P$ and extract a $k$-hop predecessors around node $p$, denoted as the masked hop $\mathcal{M}(p)$. Within the masked hop $\mathcal{M}(p)$, we replace the functional embeddings $hf_i$ of the nodes with a learnable masked token $hm$, while preserving their structural embeddings $hs_i$. 

Therefore, we totally have $M+N$ tokens (see Eq.~\eqref{eq:token}), where $M = |\mathcal{V}^P|$ and $N = |\mathcal{V}^A|$. All the selected masked nodes $p$ is in set $\mathcal{S}$. 

\begin{equation} \label{eq:token}
    \begin{split}
        {H_i^P} & = \{hs_i^P, hf_i^P\}, ~i\in \mathcal{V}^P, i \notin \mathcal{M}(p) \\ 
        {H_i^P} & = \{hs_i^P, hm\}, ~i\in \mathcal{V}^P, i \in \mathcal{M}(p) \\ 
        {H_j^A} & = \{hs_j^A, hf_j^A\}, ~j\in \mathcal{V}^A \\ 
    \end{split}
\end{equation}

We process these $M+N$ tokens using a Transformer model $\mathcal{T}$. Formally, we define the input and output of the Transformer model as below. The training objective is to reconstruct the masked area using the remaining cell embeddings and the learned gate embeddings from AIG view. 
\begin{equation} \label{eq:transformer}
    \{\mathbf{H}^{A'}, \mathbf{H}^{P'}\} = \mathcal{T}(\{\mathbf{H}^A, \mathbf{H}^P\})
\end{equation}

\subsection{Model pretaining}
To train DeepCell, we employ a two-stage training strategy. As illustrated in Fig.~\ref{fig:overview}, in the first stage, we perform random simulation with $15,000$ patterns on PM netlist and record the logic-1 probability $prob_i, i \in \mathcal{V}^P$. Next, we use a simple 3-layer MLP to readout the functional embeddings as the predicted probability. The training loss for this stage is defined in Eq.~\eqref{eq:stage1}. 
\begin{equation} \label{eq:stage1}
    \begin{split}
        % prob_{i}^{'} & = MLP(hf_i),~i \in \mathcal{V}^P\\ 
        \hat{prob}_{i} & = MLP(hf_i),~i \in \mathcal{V}^P\\ 
        loss_{prob} & = L1Loss(prob_i, prob_{i}^{'}) \\ 
    \end{split}
\end{equation}

In the second stage, we refine the cell embeddings by incorporating representations from the AIG view. We utilize a pretained and frozen AIG encoder to provide rich contextual information from the AIG. The training objective in this stage is to reconstruct the functionality of masked cells using information from their neighboring cells and the global AIG perspective. Accordingly, we define the MCM training loss in Eq.~\eqref{eq:stage2}, where the model is trained to recover the functional embeddings of the masked cells. 
\begin{equation} \label{eq:stage2}
    loss_{mcm} = L1Loss(H_i^{P}, H_i^{P'})
\end{equation}

Consequently, the overall loss functions for both stages are defined as follows, where $w_{prob}$ and $w_{mcm}$ present the weights of these two training tasks. We assign $w_{prob}=1$ and $w_{mcm}=1$ in default setting. 
\begin{equation}
    \begin{split}
        L_{stage1} & = loss_{prob} \\
        L_{stage2} & = w_{prob} \cdot loss_{prob} + w_{mcm} \cdot loss_{mcm} \\
    \end{split}
\end{equation}