\section{Experiments} \label{Sec:Experiment}
In this section, we train our DeepCell to learn the general representations of standard cells in the post-mapping netlists. 

\subsection{Experiment Settings}
\subsubsection{Data Preparation} 
% We collect the open-source designs from GitHub and the open-source circuit benchmarks~\cite{ISCAS89, ITC99, EPFLBenchmarks}. All circuits are designed with Skywater 130nm as the target process library~\cite{edwards2021introduction}, using commercial logic synthesis tool Design Compiler command \textit{compile\_ultra} to obtain the corresponding PM netlist. In the AIG part, we read the RTL file through yosys, use ABC to obtain the corresponding AIG file, and run the \textit{resyn2} command once, which proved to be beneficial to the training of the model~\cite{li2022deepsat}. In practice, some RTLs are limited by yosys performance and cannot be read. We will use Design Compiler to read the unreadable file and use \textit{compile} to get the corresponding PM netlist, and then ABC reads the PM netlist and uses the command \textit{strash} and \textit{resyn2} to get AIG. According to statistics, the PM netlists we obtained contains a total of 32,073,967 cells. We split the dataset into 80\% for training and 20\% for testing.

We collect the open-source verilog designs from GitHub and the open-source circuit benchmarks~\cite{ISCAS89, ITC99, EPFLBenchmarks}. All circuits are designed with Skywater 130nm as the target process library~\cite{edwards2021introduction}, using commercial logic synthesis tool Design Compiler command \texttt{compile\_ultra} to obtain the corresponding PM netlist. To accelerate model pretraining, we randomly extract the sub-circuits within 5,000 cells. Then, we use ABC \texttt{strash} command to obtain the corresponding AIGs. Consequently, there are totally 83,155 PM netlists and corresponding AIGs. We split the dataset into 80\% for training and 20\% for testing. 

\subsubsection{Evaluation Metrics} In the following experiments, we evaluate model performance in predicting the logic-1 probability under random simulation, a widely used metric for assessing circuit representation ability~\cite{li2022deepgate, liu2024polargate}. We calculate the average prediction error (PE) as Eq.~\eqref{eq:pe}. 
\begin{equation} \label{eq:pe}
    PE = \frac{1}{|\mathcal{V}^P|}\sum_{i\in\mathcal{V}^P}|prob_i - \hat{prob}_{i}|
\end{equation}

Additionally, we assess model performance on mask circuit modeling tasks, where the average reconstruction error (RE) is defined as Eq.~\eqref{eq:re}, where all the select nodes $p$ are in set $\mathcal{S}$ and the masked hop is $\mathcal{M}(p)$. 
\begin{equation} \label{eq:re}
    RE = \frac{1}{\sum|\mathcal{M}(p)|}\sum_{i\in \mathcal{M}(p)}|hf_i^P-hf_i^{P'}|, p\in\mathcal{S} 
\end{equation}

\subsubsection{Model Implementation and Training}
% We deploy the pretrained DeepGate2 \cite{shi2023deepgate2} as the AIG encoder. 
The Transformer model used to refine cell embeddings consists of 4 Transformer blocks, each with 8 attention heads. After encoding, each cell is represented by a 128-dimensional structural embedding and a 128-dimensional functional embedding. DeepCell is pretrained for 60 epochs in Stage 1 and an additional 60 epochs in Stage 2 to ensure convergence. The pretraining process is conducted with a batch size of 128 using 8 Nvidia A800 GPUs. We employ the Adam optimizer with a learning rate of $10^{-4}$. 

\subsection{Experimental Results}
\subsubsection{Effect of Mask Modeling}
We investigate the optimal settings for Mask Circuit Modeling (MCM) by exploring its two key hyperparameters: the number of selected nodes to be masked, $|\mathcal{S}|$, and the size of the masked hop, $k$. To evaluate performance, we pretrain the model using various hyperparameter combinations, where $\mathcal{|S|}=\theta \cdot |\mathcal{V}^P|$ and $\theta = 1\%, 5\%, 10\%, 20\%$ of total nodes in PM netlist, with hop sizes of $k=4$ or $k=6$. 

\begin{table}[!t]
\caption{Effect of Mask Modeling} \label{TAB:mask}
\vspace{-5pt}
\centering
\begin{tabular}{@{}l|ll|ll@{}}
\toprule
\multicolumn{1}{c|}{\multirow{2}{*}{$\theta$}} & \multicolumn{2}{c|}{$k=4$}            & \multicolumn{2}{c}{$k=6$} \\
\multicolumn{1}{c|}{}                                                          & PE              & RE              & PE        & RE        \\ \midrule
0.01                                                                           & \textbf{0.0322} & 0.0099          & 0.0380    & 0.0211    \\
0.05                                                                           & 0.0323          & \textbf{0.0097} & 0.0418    & 0.0257    \\
0.10                                                                           & 0.0334          & 0.0110          & 0.0552    & 0.0794    \\
0.20                                                                           & 0.0446          & 0.0398          & 0.0682    & 0.1035    \\ \bottomrule
\end{tabular}
\end{table}

Table~\ref{TAB:mask} presents the results for different values of $\theta$ and $k$. First, the reconstruction error (RE) increases with a larger masking hop size. For example, when $\theta =0.05$, the RE for $k=6$ is 0.0257, which is 164.95\% higher than that for $k=4$ (RE=0.0097). Second, masking a smaller number of nodes consistently reduces both the RE and PE. However, using a smaller $\theta$ makes the task less challenging and diminishes its effectiveness as a pretraining objective. Based on these observation, we select $\theta=0.05$ and $k=4$ as a trade-off between task complexity and model performance in the following experiments. 

\subsubsection{Effect of Multiview Learning}
We investigate the impact of incorporating the AIG view on training our PM netlist representation model. To evaluate the representation capability of the DeepCell framework, we use the PE as the primary metric. Specifically, we compare the full multiview MCM training strategy (w/ multiview) with a baseline that uses only the PM encoder without refining embeddings through multiview training (w/o multiview). The RE metric is not available in the w/o multiview setting. It is important to note that the DeepCell w/o multiview setting reflects the representation capability of DeepGate2~\cite{shi2023deepgate2} on PM netlists, since our DeepCell utilizes a similar aggregator in the PM encoder. 

To the best of our knowledge, no prior work has focused on learning a general representation of PM netlists composed of standard cells. In addition to evaluating DeepCell, we also employ general Graph Neural Network (GNN) models, such as GCN~\cite{kipf2016semi} and GAT~\cite{velivckovic2017graph}, as PM encoders to investigate the impact of MCM training. 

\begin{table}[!t]
\caption{Effect of Multiview Learning} \label{TAB:multiview}
\vspace{-5pt}
\centering
\begin{tabular}{@{}llll@{}}
\toprule
         &               & PE              & RE              \\ \midrule
GCN~\cite{kipf2016semi}      & w/o multiview & 0.0956          & -               \\
         & w/ multiview  & 0.0529          & 0.0571          \\ \midrule
GAT~\cite{velivckovic2017graph}      & w/o multiview & 0.1466          & -               \\
         & w/ multiview  & 0.0682          & 0.0869          \\ \midrule
DeepCell & w/o multiview & 0.0564          & -               \\
         & w/ multiview  & \textbf{0.0323} & \textbf{0.0097} \\ \bottomrule
\end{tabular}
\vspace{-15pt}
\end{table}

Table~\ref{TAB:multiview} compares the performance of the models with and without multiview training. First, compared to other PM encoders, whether using multiview training or not, DeepCell outperforms alternatives such as GCN~\cite{kipf2016semi} and GAT~\cite{velivckovic2017graph}, achieving the lowest PE of 0.0323. Second, DeepCell effectively reconstructs functional embeddings by leveraging surrounding cell embeddings and additional gate embeddings from the AIG, achieving an average RE of 0.0097. Third, all models with multiview training outperform their counterparts without multiview training in terms of PE. For example, DeepCell with multiview training reduces PE by 42.7\%, from 0.0564 to 0.0323. These results demonstrate the effectiveness of the proposed multiview representation learning mechanism for PM netlists.

\section{Downstream Task: Functional ECO} \label{Sec:downstream task}
In this section, we combine our DeepCell with the open-source EDA tools and apply our model to practical EDA tasks:
functional ECO. Our DeepCell provides probability for nodes in the original circuit and provides guidance for ECO to find candidate patch signals. This method effectively leaves higher quality and more likely to be a patch signal, making the ECO solution more rapid and higher quality.

\subsection{Preliminary}
%ABC..
In order to clarify the role of the model in the algorithm, we first briefly describe the method in~\cite{dao2018efficient}. In the original SAT-based solution, for the POs reachable from the target, the method simultaneously identifies the PIs contained in these POs in the original netlist and the target netlist. Then find all signals in the original circuit, which are not in 
the transitive fanout cone of the targets and whose support is contained in the calculated set of PIs. These candidate signals are sorted by cost in ascending order and a fixed number of nodes (default is the top 5,000) are selected. Then used the SAT-based solution for the single-target ECO problem proposed in~\cite{wu2010robust} to prove whether there is a solution.


%lack
After adding a large number of candidate signals, it is very expensive to use the positive and negative remainders of target to construct the miter circuit and solve it, which is also the reason why the number of candidate signals cannot be selected in large quantities. It is necessary to reduce the number of candidate signals at the outset.

\vspace{-5pt}
\subsection{Model Finetuning}
\begin{figure}
    \centering
    \includegraphics[width=0.90\linewidth]{fig/eco.pdf}
    \vspace{-5pt}
    \caption{Finetuning for ECO Application}
    \label{fig:eco}
  \vspace{-15pt}
\end{figure}

% \ST{@Stone: Explain how DeepCell gets the probability of each node becoming a candidate node}
% Intuitively, DeepCell is pretrained to capture the correlation between masked PM netlist and complete AIG, thus we consider the original circuit to be inserted patch as the PM netlist and the golden circuit as complete AIG, then finetune the model to formulate their correlation. 

DeepCell is pretrained to capture the correlation between masked PM netlist and complete AIG. Intuitively, the original circuit that requires patch insertion in ECO task is treated as PM netlist with masking, while the golden circuit provide the complete view. We then finetune DeepCell to effectively learn and represent the relationship between these two circuits. 

\begin{figure} [!t]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/eco_intro.pdf}
    \vspace{-10pt}
    \caption{Select candidate signal with DeepCell}
    \label{fig:eco_intro}
    \vspace{-15pt}   
\end{figure}

Therefore, we finetune DeepCell to reduce the space of candidate signals in the ECO process. Specifically, given a PM netlist $\mathcal{G}^P$, we randomly select a cell $p$ within the netlist and remove its entire driven cone from this node to PI. The removed area, $\mathcal{M}(p)$, is treated as the ground-truth path in ECO. Consequently, the patch-removed netlist is denoted as $\widetilde{\mathcal{G}}^P$, while the corresponding AIG netlist for the original PM netlist is denoted as $\mathcal{G}^A$. Next, we use the PM encoder to encode both patch-removed netlist $\widetilde{\mathcal{G}}^P$ and the original netlist $\mathcal{G}^P$, as denoted in Eq.~\eqref{eq:eco}. The AIG encode is employed to encode the AIG netlist $\mathcal{G}^A$, which serves as the reference ground-truth implementation. The embeddings of the patch-removed netlist are then refined using the AIG embeddings, as described in Eq.\eqref{eq:transformer}, to produce the refined embeddings $\widetilde{\mathbf{H}}^{P}$. 
\begin{equation} \label{eq:eco}
    \begin{split}
        \mathbf{H}^A & = DeepGate2(\mathcal{G}^A) \\
        \mathbf{H}^P & = encoder(\mathcal{G}^P) \\
        \widetilde{\mathbf{H}}^{P} & = encoder(\widetilde{\mathcal{G}}^P) \\
        \{\mathbf{H}^{A'}, \widetilde{\mathbf{H}}^{P'}\} & = \mathcal{T}(\{\mathbf{H}^{A}, \widetilde{\mathbf{H}}^{P}\})
    \end{split}
\end{equation}


\begin{table*}[!t]
  \centering
  \caption{Performance Comparison between w/ DeepCell and w/o DeepCell on ICCAD’17 CAD Contest Benchmarks}
  \vspace{-5pt}
    \tabcolsep=0.0185\linewidth
    % \renewcommand\tabcolsep{3.0pt}
    \begin{tabular}{cccccc|ccc|ccc}
    \toprule
    \multirow{2}[2]{*}{Circuit name} & \multicolumn{5}{c}{Circuit information} & \multicolumn{3}{c}{\textit{w/o DeepCell}} & \multicolumn{3}{c}{\textit{w/ DeepCell}} \\
          & \multicolumn{1}{c}{PI} & \multicolumn{1}{c}{PO} & \multicolumn{1}{c}{gate(F)} & \multicolumn{1}{c}{gate(G)} & tatget & cost  & gate  & time(s) & cost  & gate  & time(s) \\
    \midrule
    unit 1 & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{6} & 1 & 4     & 1     & 0.37  & 4     & 1     & 0.37 \\
    unit 2 & \multicolumn{1}{c}{157} & \multicolumn{1}{c}{64} & \multicolumn{1}{c}{1120} & \multicolumn{1}{c}{1219} & 1 & 17    & 4     & 0.92  & 17    & 4     & 1.01 \\
    unit 3 & \multicolumn{1}{c}{411} & \multicolumn{1}{c}{128} & \multicolumn{1}{c}{2074} & \multicolumn{1}{c}{1929} & 1 & 80    & 3     & 0.46  & 80    & 3     & 0.4 \\
    unit 4 & \multicolumn{1}{c}{11} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{75} & \multicolumn{1}{c}{77} & 1 & 32    & 5     & 0.49  & 32    & 2     & 0.55 \\
    unit 5 & \multicolumn{1}{c}{450} & \multicolumn{1}{c}{282} & \multicolumn{1}{c}{24357} & \multicolumn{1}{c}{21056} & 2 & 47    & 30    & 46.3  & 47    & 29    & 15.39 \\
    unit 6 & \multicolumn{1}{c}{99} & \multicolumn{1}{c}{128} & \multicolumn{1}{c}{13828} & \multicolumn{1}{c}{11812} & 2 & -     & -     & -     & -     & -     & - \\
    unit 7 & \multicolumn{1}{c}{207} & \multicolumn{1}{c}{24} & \multicolumn{1}{c}{2944} & \multicolumn{1}{c}{1721} & 1 & 284   & 2     & 8.5   & 284   & 2     & 6.88 \\
    unit 8 & \multicolumn{1}{c}{179} & \multicolumn{1}{c}{64} & \multicolumn{1}{c}{2513} & \multicolumn{1}{c}{3337} & 1 & 78    & 4     & 4.1   & 78    & 3     & 3.17 \\
    unit 9 & \multicolumn{1}{c}{256} & \multicolumn{1}{c}{245} & \multicolumn{1}{c}{5849} & \multicolumn{1}{c}{4657} & 4 & 50    & 35    & 1.33  & 50    & 26    & 1.7 \\
    unit 10 & \multicolumn{1}{c}{32} & \multicolumn{1}{c}{129} & \multicolumn{1}{c}{1581} & \multicolumn{1}{c}{1956} & 2 & -     & -     & -     & -     & -     & - \\
    unit 11 & \multicolumn{1}{c}{48} & \multicolumn{1}{c}{50} & \multicolumn{1}{c}{2057} & \multicolumn{1}{c}{2160} & 8 & 4142  & 1063  & 7200  & 2312  & 746   & 6975.3 \\
    unit 12 & \multicolumn{1}{c}{46} & \multicolumn{1}{c}{27} & \multicolumn{1}{c}{13804} & \multicolumn{1}{c}{821} & 1 & 104   & 1     & 1.02  & 104   & 1     & 0.71 \\
    unit 13 & \multicolumn{1}{c}{25} & \multicolumn{1}{c}{39} & \multicolumn{1}{c}{369} & \multicolumn{1}{c}{426} & 1 & 3467  & 9     & 1.5   & 3467  & 9     & 1.33 \\
    unit 14 & \multicolumn{1}{c}{17} & \multicolumn{1}{c}{15} & \multicolumn{1}{c}{1981} & \multicolumn{1}{c}{1006} & 12 & 95    & 41    & 5.43  & 95    & 41    & 6.72 \\
    unit 15 & \multicolumn{1}{c}{198} & \multicolumn{1}{c}{14} & \multicolumn{1}{c}{1886} & \multicolumn{1}{c}{2262} & 1 & 191   & 11    & 1.93  & 191   & 11    & 0.73 \\
    unit 16 & \multicolumn{1}{c}{417} & \multicolumn{1}{c}{214} & \multicolumn{1}{c}{2371} & \multicolumn{1}{c}{9324} & 2 & 278   & 15    & 16.58 & 204   & 9     & 14.67 \\
    unit 17 & \multicolumn{1}{c}{136} & \multicolumn{1}{c}{31} & \multicolumn{1}{c}{2910} & \multicolumn{1}{c}{2052} & 8 & 434   & 79    & 5.04  & 434   & 63    & 6.01 \\
    unit 18 & \multicolumn{1}{c}{245} & \multicolumn{1}{c}{100} & \multicolumn{1}{c}{4860} & \multicolumn{1}{c}{3881} & 1 & 18    & 1     & 5.14  & 18    & 1     & 2.44 \\
    unit 19 & \multicolumn{1}{c}{99} & \multicolumn{1}{c}{128} & \multicolumn{1}{c}{13349} & \multicolumn{1}{c}{10787} & 4 & -     & -     & -     & -     & -     & - \\
    unit 20 & \multicolumn{1}{c}{1874} & \multicolumn{1}{c}{7105} & \multicolumn{1}{c}{30876} & \multicolumn{1}{c}{34002} & 4 & 136   & 6     & 0.6   & 120   & 5     & 1.12 \\
    \midrule
    Geomean &       &       &       &       &       & 112.36  & 8.96  & 3.72  & \textbf{105.83}  & \textbf{7.60}  & \textbf{3.19}  \\
    Imp. &       &       &       &       &       & 1     & 1     & 1x    & \textbf{0.06}  & \textbf{0.15}  & \textbf{0.85x} \\
    \bottomrule
    \end{tabular}%
  \label{compare on ICCAD 2017 contest}%
  \vspace{-15pt}
\end{table*}%


The finetuning tasks are defined as patch reconstruction and driven signal identification. First, in the ideal scenario, the finetuned model should recover the embeddings of the patch with the correct functionality, as informed by the AIG view. We treat the embeddings of the original netlist (prior to patch removal) as the ground truth and define the patch reconstruction loss, $loss_{pr}$ as specified in Eq.~\eqref{eq:pr} (see Fig.~\ref{fig:eco}). We finetune DeepCell using only 8,000 circuits over 10 epochs with loss function $L=loss_{pr} + loss_{drv}$. 
\begin{equation} \label{eq:pr}
    loss_{pr} = L1Loss(H_i^{P'}, \widetilde{H}_i^{P'}),~i\in\mathcal{M}(p)
\end{equation}

Second, in the driven signal identification task, we utilize the refined embeddings to predict whether a given cell is driven by the selected cell $p$. 
\begin{equation} \label{eq:driven}
    loss_{drv} = BCELoss(MLP(\{\widetilde{H}_i^{P'}, \widetilde{H}_p^{P'}\}), 0/1), i\in\mathcal{V}^P
\end{equation}

As shown in Fig.~\ref{fig:eco_intro}, with DeepCell intervention, according to the obtained probability, we remove signals that are almost impossible to drive as targets, so that the number of candidate signals is greatly reduced, so that the situation that needs to be considered in the subsequent steps is controlled in a small range. This operation greatly reduces the running time.

\subsection{Experiment Settings}
Our model is equipped into ABC as a plug-in and integrated into the command \textit{`runeco'}, which is an efficient SAT-based solution that won the first place in the 2017 ICCAD contest problem A and added the SAT-based exact pruning method~\cite{dao2018efficient}. Usually, the number of signals used in the patches generated by the ECO problem is not so much. In the subsequent experiments, the maximum number of candidate signals we selected is 1000. In~\cite{dao2018efficient}, the quantified Boolean formula (QBF) method that can be used when the SAT-based feasibility calculation time exceeds the range is introduced. DeepCell, as a generic plug-in, can also be added to these processes if the method code is open source.

\subsection{Main Results}
We verify the effectiveness of our model in the 2017 ICCAD contest problem A~\cite{huang20172017}. These benchmarks are using ISCAS-85/89~\cite{brglez1989combinational}, ITC-99~\cite{davidson1999characteristics}, IWLS-2005~\cite{albrecht2005iwls}, OpenCore and LGSynth-93 benchmarks and real-world ECO problems from industrial design. As shown in Table \ref{compare on ICCAD 2017 contest}, the sections labeled \text{`gate(F)'} and \text{`gate(G)'} correspond to the number of gates in the original circuit and the golden circuit, respectively. The \textit{`w/o DeepCell'} and \textit{`w/ DeepCell'} sections lists the effects of \textit{`runeco'} before and after inserting DeepCell. We limit the maximum running time to two hours. These timeout units are labeled as `-'. It should be denoted that the time reported in the table includes both model inference time and ECO tool runtime. All experiments are performed on a single core of 2.10 GHz Intel(R) Core(TM) i7-14700F CPU with 32GB memory. 

From Table \ref{compare on ICCAD 2017 contest}, we can observe that that integrating our DeepCell into the ECO tool achieves significant improvements. Specifically, it reduces the average cost of patches by 6\% and the average number of gates by 15\%. Additionally, the required runtime decreases by 15\%, with no units experiencing increased costs or gate counts. Notably, in the top-performing unit 16, our model achieves a remarkable 27\% reduction in cost and a 40\% reduction in gate count. These results are attributed to the ability of effectively capturing node relationships, enabling it to calculate the probability of each node for every target in multi-target units. % thereby ensuring near-universal optimization across these units.

For small and easy cases (unit 1-4), incorporating DeepCell may slightly increase the runtime due to the high proportion of model inference time. However, as the circuit size increases, the number of potential candidate nodes can be significantly reduced by utilizing the probabilities provided by DeepCell, which in turn accelerates the process. As a result, out of 20 units, the running time increased in 10 instances. The most efficient application (unit 5) achieved a 70\% reduction in runtime, while the least efficient case only saw an increase of 0.52 seconds (unit 20).
% \vspace{-10pt}

\begin{table}[!t]
  \centering
  \caption{Performance Comparison between w/ DeepCell and w/o DeepCell on Find Feasible ECO Solution}
\vspace{-5pt}
    \tabcolsep=0.014\linewidth
    \begin{tabular*}{\linewidth}{ccccccc}
    \toprule
    \multirow{2}[2]{*}{Circuit name} & \multicolumn{3}{c}{\textit{w/o DeepCell}} & \multicolumn{3}{c}{\textit{w/ DeepCell}} \\
          & cost  & support size & time(s) & cost  & support size & time(s) \\
    \midrule
    unit 6 & -     & -     & $>$12600 & 2500  & 42    & 538.06 \\
    unit 10 & 63    & 16    & 48.64 & 63    & 16    & 29.2 \\
    unit 11 & 54    & 3     & 3977.58 & 36    & 2     & 2379.78 \\
    unit 19 & -     & -     & $>$12600 & -     & -     & $>$12600 \\
    \bottomrule
    \end{tabular*}%
\vspace{-15pt}
  \label{compare in large unit}%
\end{table}%

% In the original process, when the patch support is determined, the function of the patch needs to be calculated and minimized. Although DeepCell can effectively accelerate the first part of the algorithm, the time-consuming of this step will become unacceptable when the number of support is too large. 

For four units that encountered timeout and large runtime (unit 6, 10, 11, 19), we conduct further analysis in Table \ref{compare in large unit}. Data regarding the cost, support size, and runtime for the patches created for the first target of these units are collected. We increase the limit time of this part of the test to three and a half hours. Except for unit 19, the other three units are able to provide a viable solution more quickly, and the quality of these solutions are superior.


