\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amssymb,amsmath,amsthm}
\newtheorem{theorem}{Theorem}
\usepackage{comment}
\usepackage{orcidlink}
\usepackage{pifont}%%for \ding{55}, namely crossmark
\usepackage{makecell} %%for multiple lines in a cell ybder tabular environ.
\newtheorem{definition}{Definition}
% Define the problem environment
\newtheorem{problem}{Problem}

\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\begin{document}
\title{Diffusion Models for Tabular Data: \\ Challenges, Current Progress, and Future Directions}

%\author{Anonymous authors}


%\begin{comment}
\author{Zhong Li \orcidlink{0000-0003-1124-5778}, Qi Huang* \orcidlink{0009-0007-4989-135X}, Lincen Yang* \orcidlink{0000-0003-1936-2784} \thanks{*Qi and Lincen contributed equally.}, Jiayang Shi 
\orcidlink{0000-0002-7014-0805}, Zhao Yang 
\orcidlink{0000-0002-7011-4340},\\ Niki van Stein \orcidlink{0000-0002-0013-7969} (IEEE Member), Thomas Bäck \orcidlink{0000-0001-6768-1478} (IEEE Fellow), Matthijs van Leeuwen \orcidlink{0000-0002-0510-3549}
\thanks{\#Zhao Yang is with VU Amsterdam. All other authors are with LIACS, Leiden University, the Netherlands. Corresponding Email: z.li@liacs.leidenuniv.nl (Zhong Li)}}
%\end{comment}

\markboth{Manuscript submitted to IEEE for possible publication}%
{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
In recent years, generative models have achieved remarkable performance across diverse applications, including image generation, text synthesis, audio creation, video generation, and data augmentation. Diffusion models have emerged as superior alternatives to Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) by addressing their limitations, such as training instability, mode collapse, and poor representation of multimodal distributions. This success has spurred widespread research interest. In the domain of tabular data, diffusion models have begun to showcase similar advantages over GANs and VAEs, achieving significant performance breakthroughs and demonstrating their potential for addressing unique challenges in tabular data modeling. However, while domains like images and time series have numerous surveys summarizing advancements in diffusion models, there remains a notable gap in the literature for tabular data. Despite the increasing interest in diffusion models for tabular data, there has been little effort to systematically review and summarize these developments. This lack of a dedicated survey limits a clear understanding of the challenges, progress, and future directions in this critical area. This survey addresses this gap by providing a comprehensive review of diffusion models for tabular data. Covering works from June 2015, when diffusion models emerged, to December 2024, we analyze nearly all relevant studies, with updates maintained in a \href{https://github.com/Diffusion-Model-Leiden/awesome-diffusion-models-for-tabular-data}{GitHub repository}. Assuming readers possess foundational knowledge of statistics and diffusion models, we employ mathematical formulations to deliver a rigorous and detailed review, aiming to promote developments in this emerging and exciting area.

%We can take \cite{croitoru2023diffusion} as an example.

\end{abstract}

\begin{IEEEkeywords}
Diffusion Models, Tabular Data, Generative Models
\end{IEEEkeywords}

 \section{Introduction}
 \label{sec:introdcution}

%1.Tabular Data and Generative AI: 1.1)What is tabular data? 1.2)Why tabular data is important? and 1.3)Why generative models for tabular data are important? (Todo: Write more on tabular data generation) 

Tabular data is a data modality in which information is organized into rows, representing individual records, and columns, representing features or attributes. It is  ubiquitous in real-world domains, including but not limited to healthcare~\cite{yoo2012data}, finance~\cite{dixon2020machine}, education~\cite{algarni2016data}, transportation~\cite{anand2018extensive}, psychology~\cite{king2014data}, etc. The demand for high-quality generative models in these domains is acute due to data privacy regulations such as GDPR~\cite{gdpr2016general} and CCPA~\cite{california2018consumer}. Consequently, real user data is often restricted from public release, whereas synthetic data generated by generative models can preserve machine learning utility while being legally shareable~\cite{kotelnikov2023tabddpm}. Beyond privacy concerns, real-world tabular datasets often contain missing values, which can arise due to human errors or technical malfunctions like sensor failures. To address this, generative models have been employed for missing value imputation, demonstrating promising performance~\cite{mattei2019miwae}. Furthermore, tabular data often presents challenges related to imbalanced class distributions~\cite{kaur2019systematic}, where certain categories dominate and result in biased models. Generative models can help mitigate this issue by generating synthetic samples for underrepresented classes, improving model performance on minority categories~\cite{fajardo2021oversampling}. Overall, these multifaceted applications underscore the growing importance of generative models for tabular data, ranging from privacy protections~\cite{assefa2020generating,hernandez2022synthetic}, missing value imputation~\cite{you2020handling, yoon2018gain}, to training data augmentation~\cite{fonseca2023tabular}. 


%2. Generative Models: An overview of existing generative models? Why Diffusion Models are preferred? (What are their their pros and cons?) 
Deep generative models mainly include Energy-based Models (EBMs) \cite{lecun2006tutorial}, Variational Autoencoders (VAEs) \cite{ kingma2013auto}, Generative Adversarial Networks (GANs) \cite{goodfellow2014generative}, Autoregressive Models \cite{vaswani2023attentionneed}, Normalized Flows \cite{kobyzev2020normalizing}, and Diffusion Models \cite{sohl2015deep}. Diffusion Models offer several advantages that make them a preferred choice for many generative tasks. Unlike GANs, which often suffer from mode collapse and unstable training due to adversarial loss dynamics \cite{thanh2020catastrophic}, Diffusion Models are inherently stable and effectively capture the full data distribution. Moreover, they produce sharper and more realistic outputs than VAEs, avoiding the blurry reconstructions in images caused by Gaussian latent space assumptions \cite{dai2019diagnosing}. Compared to EBMs, Diffusion Models do not rely on computationally expensive sampling methods like Markov chain Monte Carlo (MCMC) and are easier to train \cite{carbone2024hitchhiker}. They also overcome the limitations of Normalizing Flows by avoiding bijective transformation constraints and Jacobian calculations \cite{kobyzev2020normalizing, carbone2024hitchhiker}, enabling more powerful expressivity. Finally, unlike Autoregressive Models, Diffusion Models are not constrained by sequential generation \cite{lin2021limitations}, allowing them to utilize flexible architectures to learn arbitrary data distributions for diverse data types. These strengths together make Diffusion Models a highly flexible and robust choice for generative modeling tasks.


%3. Diffusion Models: What are diffusion models?

More concretely, diffusion models \cite{sohl2015deep, ho2020denoising, songscore} are likelihood-based generative models designed to learn the underlying distribution of training data and generate samples that closely resemble it. Typically, a diffusion model comprises a forward diffusion Markov process, which gradually transforms training data into pure noise, and a reverse denoising Markov process, which reconstructs realistic synthetic data from the noise.  Given the remarkable performance of diffusion models in generating high-quality and diverse synthetic samples across various domains—such as images \cite{ho2020denoising, songscore}, audio \cite{chen2020wavegrad, kong2020diffwave}, text \cite{hoogeboom2021argmax, austin2021structured}, video \cite{xing2024survey}, and graphs \cite{liu2023generative}—recent studies \cite{kimstasy, kotelnikov2023tabddpm, suh2023autodiff, lee2023codi, zhangmixed, jolicoeur2024generating} have explored their application to tabular data. These studies demonstrate that diffusion models outperform GAN- and VAE-based approaches across a wide range of benchmarks in tabular data modeling.

%4. Challenges in Tabular Diffusion Models/Generative Models: What are the challenges of diffusion models for tabular data?
However, the exploration of diffusion models in tabular data modeling is arguably still in its infancy due to the inherent challenges posed by the unique characteristics of tabular data. Unlike images or audio, tabular data presents distinct challenges: \textit{missing values}, where some values are absent, requiring models to handle incomplete data; \textit{non-Gaussian distributions}, where numerical features typically do not follow a Gaussian distribution; \textit{multimodal distributions}, where a column's distribution can exhibit multiple modes; \textit{highly imbalanced categorical columns}, where major categories often dominate minor categories in terms of frequency; \textit{heterogeneous features}, where different columns can have varying data types such as numerical or categorical; \textit{mixed-type single features}, where a single column may contain values of different data types; \textit{feature dependencies}, where correlations among features must be accounted for; and \textit{small datasets}, where the training data is often limited compared to datasets for images or text. As a result, directly adapting diffusion models designed for other modalities, particularly images or text, is insufficient for tabular data. Their unique characteristics necessitate the development of tailored diffusion models, and significant research efforts are required to invent or refine diffusion models that effectively address the complexities of tabular data modeling.


%5.Research Gaps: What are the research gaps?  1) many new works developed to solve these challenges; 2) lack of dedicated survey on this topic although there exist many surveys on other data modalities.
While numerous surveys summarize advancements in diffusion models across various domains \cite{yang2023diffusion,cao2024survey} or on specific data modalities such as images \cite{croitoru2023diffusion}, text \cite{zhu2023diffusion}, videos \cite{xing2024survey}, time series \cite{lin2024diffusion}, and graphs \cite{liu2023generative}, surveys specifically focused on generative models for tabular data \cite{wang2024challenges, kim2024generative} primarily cover traditional methods, VAEs and GANs, with little to no attention given to diffusion models. In other words, despite the increasing interest in diffusion models for tabular data and their demonstrated breakthroughs in performance, there has been little effort \cite{koo2023comprehensive} to systematically review and summarize these developments. This lack of a dedicated survey limits a clear understanding of the challenges, progress, and future directions in this critical area.

%6. Aims and Contributions: What are our aims of writing this survey? And  what are the contributions of this survey?

To address this gap in the literature, we present the first comprehensive survey dedicated to diffusion models for tabular data. Our main contributions are as follows: 1) We provide a overview of the historical development of generative models for tabular data, and on this foundation, we identify the key challenges in developing generative models for tabular data; 2) We recap the fundamental concepts of diffusion models, describe the most widely used diffusion model frameworks for tabular data modeling, and highlight typical applications of diffusion models in the tabular domain; and 3) We offer a detailed and extensive review of diffusion models for tabular data. Covering works from June 2015, marking the inception of diffusion models, to December 2024, we analyze nearly all relevant studies and maintain a \href{https://github.com/Diffusion-Model-Leiden/awesome-diffusion-models-for-tabular-data}{GitHub repository} for continuous updates. We also review diffusion models in discrete spaces that can be useful for generating categorical features in tabular data, followed by summarizing evaluation metrics for assessing the performance of tabular generative models.

%Organization of Paper: How is this survey organized?

This survey is organized as follows. Section~\ref{sec:ImpHisCha_TabDataGen} discusses the  historical evolution of tabular data generation, the unique challenges associated with modeling tabular data, and the taxonomy of diffusion models for tabular data. Section~\ref{sec:preliminaries} introduces the preliminaries of diffusion models and their applications in tabular data. Sections~\ref{sec:DataAug}, \ref{sec:DataImp}, \ref{sec:TrustSynth}, and \ref{sec:AnoDec}, provide an in-depth review of diffusion models specifically designed for tabular data. In Section~\ref{sec:other_related_work}, we further explore discrete diffusion models and evaluation metrics relevant to tabular data modeling. Finally, Section~\ref{sec:conclusions} offer a discussion on future directions and concludes the survey.

    
\section{Generative Models for Tabular Data}
\label{sec:ImpHisCha_TabDataGen}
In this section, we start by reviewing the development history of generative models for tabular data. We then discuss the unique characteristics of tabular data and the challenges these pose for developing generative models. Finally, we outline the taxonomy of diffusion models for tabular data adopted in this survey, categorized based on their applications.

\subsection{History of Generative Models for Tabular Data} 

\begin{figure*}
    \centering
\includegraphics[width=1\linewidth]{Image/TabGenAI2.pdf}
    \caption{Timeline of Generative Models for Tabular Data: Below the timeline, key advancements in traditional machine learning models and deep generative models are shown, while above the timeline, their extensions in tabular data, such as data synthesis and imputation, are highlighted.}
    \label{fig:TabGenTimeline}
\end{figure*}

As shown in Figure~\ref{fig:TabGenTimeline}, prior to the advent of models explicitly designed for data generation (e.g., VAEs \cite{kingma2019introduction}, GANs \cite{goodfellow2014generative}, and Diffusion Models \cite{sohl2015deep}), probabilistic models like Copula \cite{sklar1973random}, Gaussian Mixture Models \cite{reynolds2009gaussian} and Bayesian Networks \cite{rabaey2024clinical} were commonly employed for data synthesis. Later, specialized methods, including distance-based approaches like SMOTE \cite{chawla2002smote} and its variants (e.g., Borderline-SMOTE \cite{han2005borderline}, SMOTETomek \cite{wang2023synthetic}, and SMOTE-ENC \cite{mukherjee2021smote}), as well as ADASYN \cite{he2008adasyn}, and probabilistic models like Synthpop \cite{nowok2016synthpop}, were introduced for data synthesis and imputation. However, distance-based methods, including SMOTE and its variants, encounter challenges when dealing with large datasets and complex data distributions. Additionally, probabilistic methods, such as Copulas and Synthpop, often struggle with heterogeneous data, impose predefined distributions, and are prone to assumption biases \cite{wang2024challenges}. In contrast, deep generative models explicitly designed for data generation have gained increasing prominence in the tabular domain, offering significant advancements and more successful applications compared to traditional tabular data generation techniques. For example,
\begin{itemize}
    \item VAE \cite{kingma2019introduction} based methods such as TVAE \cite{xu2019modeling} and GOGGLE \cite{liu2023goggle} are shown to achieve superior performance. Importantly, GOGGLE is the first to explicitly model the correlations among features, by using a VAE-based model with GNN as the encoder and decoder models. However, VAEs-based methods are prone to certain limitations, including blurry outputs in the generated data due to the inherent randomness introduced by the latent space and potential difficulty in balancing reconstruction loss and regularization during training, which can affect the quality of the synthetic data. Additionally, VAEs may struggle with accurately capturing multimodal distributions, a common characteristic in real-world tabular datasets.
    \item GAN \cite{goodfellow2014generative} based methods have demonstrated promising results in tabular data synthesis, with key examples including CTGAN \cite{xu2019modeling} and its variants such as CTABGAN \cite{zhao2021ctab} and CTABGAN+ \cite{zhao2024ctab2}. These models typically use Gaussian Mixture Models to model continuous features, which may be suboptimal for certain real-world data. Additionally, they handle categorical features using one-hot encoding, which can substantially increase data dimensionality. Moreover, GANs inherently suffer from well-known limitations such as mode collapse, where the generator fails to capture the full data distribution, and training instability that makes the optimization process difficult and often requires careful hyperparameter tuning and regularization.
    \item LLMs \cite{minaee2024large} have also been explored for tabular data synthesis. For example, GReaT \cite{borisov2023language} employs large language models by converting each row into a natural language sentence and learning sentence-level distributions using GPT \cite{achiam2023gpt}. This approach suffers from some limitations, including potential information loss during data-to-text conversion, increased dimensionality leading to higher computational costs, and context length limitations, which can affect scalability when handling large datasets.
    \item Diffusion Models \cite{sohl2015deep} have demonstrated superior performance compared to VAEs and GANs in image synthesis tasks \cite{dhariwal2021diffusion}. Recent studies, including but not limited to SOS \cite{kim2022sos}, STaSy \cite{kimstasy}, TabDDPM \cite{kotelnikov2023tabddpm}, CoDi \cite{lee2023codi}, and TabSyn \cite{zhangmixed}, indicate that these advantages extend to tabular data synthesis as well.
\end{itemize}
Since this survey focuses specifically on diffusion models for tabular data, we recommend readers refer to dedicated survey papers \cite{wang2024challenges, kim2024generative,fang2024large} for a comprehensive review of other generative models for tabular data.

%[@Zhong: I will add more descriptions for each representative method after reading some related work.] 
 
\subsection{Challenges with Generative Models for Tabular Data}
\label{Sec:ChallengeswithTabularData}
Training generative models in tabular data can be inherently more challenging than in image or text data due to the following challenges.

\subsubsection{Missing Values} This phenomenon often happens in real-world tabular datasets for several reasons \cite{zheng2022diffusion}, e.g., privacy concerns (people's refusal to answer
questions about their employment or income information in census data \cite{lillard1986we}), difficulty of data collection (drop-out in studies and merging unrelated data in healthcare data \cite{wells2013strategies}), human operation errors when processing data \cite{emmanuel2021survey}, or machine error due to malfunctioning of equipment \cite{emmanuel2021survey}. Furthermore, the missing value problem can be categorized into \cite{rubin1976inference}: Missing At Random (MAR), Missing Completely At Random (MCAR), and Missing Not At Random (MNAR). Most generative models for tabular data cannot be directly trained on incomplete data \cite{jolicoeur2024generating}. 

\subsubsection{Intricate Individual Feature Distribution} It is pointed out \cite{xu2019modeling} that a feature (namely a column in a table) in tabular data may have complicated distributions in the sense that: 2.1) for numerical feature, the distribution can be non-Gaussian and/or it can have multiple distribution modes; 2.2) for a categorical feature, the distribution of categories can be highly imbalanced. For example, instances of the major category can take more 95\% while those of minor categories take only 5\% in total; 2.3) within the same feature type (namely numerical or categorical), different features usually have different statistical properties (e.g., feature-wise marginal distribution) due to the fact that the meanings of different features can be various. In contrast, pixel values of each image in an image dataset are usually assumed to follow the same distribution.

\subsubsection{Heterogeneous Features} In the context of tabular data, heterogeneous features refer to columns that contain different types of data, which is considered the most challenging issue by the community \cite{suh2023autodiff}. These types may include numerical, categorical, ordinal, boolean, text, or datetime data. This diversity makes processing and modeling such data more complex than homogeneous datasets, which consist of only one type of feature. This is because  most generative models only focus on learning distributions either on numerical or discrete domains. It is unclear whether it is effective by simply combining  different types of models that are separately designed for different types of features.
\subsubsection{Feature Dependencies (Correlations between Features)} Tabular data synthesis should learn the joint probability of multiple columns to generate a sample, and these columns are usually not independent to each other (namely their joint probability cannot be decomposed into the production of their marginal probabilities). Unlike image data which contains only continuous pixel values with local spatial correlations or text data which comprises tokens sharing the same dictionary space \cite{shi2024tabdiff}, capturing the correlations among features has been a long-standing challenge in tabular data synthesis \cite{suh2023autodiff}. However, the heterogeneous nature of tabular data makes it even more challenging than capturing the correlations among purely categorical or numerical features.

\subsubsection{Mixed-Type Feature} A mixed-type feature refers to a single feature (column in a dataset) that contains more than one type of data. Unlike heterogeneous features, which describe the diversity of feature types across columns, mixed-type features involve a single column containing values of different data types, such as numerical and categorical data mixed together.

\subsubsection{Small Data Size} Compared to image or text datasets, which typically have massive amounts of available data, tabular datasets are usually smaller in volume. Specifically, the number of instances or samples is often limited, making it challenging to train data-hungry deep models effectively.

\subsubsection{Domain Specific Constraints} Real-world tabular datasets span various domains, such as healthcare, finance, and engineering, each with domain-specific features and constraints. For example, healthcare data must maintain realistic ranges for age or blood pressure, while finance data requires valid transaction amounts and balances. Unlike images or text, these constraints are often difficult to verify without domain expertise, making the generation of high-quality synthetic tabular data particularly challenging.

\subsection{Taxonomy of Diffusion Models for Tabular Data }
 \label{sec:AppOfDMTabular}

 \begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Image/TaxonomyTabDM3.pdf}
    \caption{Taxonomy of Diffusion Models for Tabular Data.}
    \label{fig:taxonomy}
\end{figure}

Generative models have achieved notable success across a wide range of real-world applications for tabular data, with diffusion models emerging as a powerful approach alongside other generative frameworks such as GANs and VAEs. These applications address critical challenges in domains such as healthcare and finance, where tabular data is ubiquitous. Broadly, generative models for tabular data can be divided into four key categories: \textit{data augmentation}, \textit{data imputation}, \textit{trustworthy data synthesis}, and \textit{anomaly detection}. 

While this taxonomy applies to tabular generative models in general, the focus of this survey is on diffusion models. Therefore, we will systematically examine diffusion-based approaches within each category, discussing how they leverage the diffusion process to enhance tabular data modeling.  This application-driven taxonomy was chosen instead of a classification based on the underlying diffusion model types (discussed in Section~\ref{sec:preliminaries}) because it provides a more intuitive and practical perspective on how these models are applied to real-world problems. Practitioners and researchers are often more concerned with their end-use cases rather than the underlying mathematical formulations. This taxonomy allows us to systematically categorize and evaluate the growing body of literature on diffusion models for tabular data, highlighting key contributions, challenges, and future directions in each category. 

The relevant methods are explored in detail in Sections~\ref{sec:DataAug}–\ref{sec:AnoDec}. For \textit{data augmentation} (see Section~\ref{sec:DataAug}), we will review methods that generate synthetic data. The section \textit{data imputation} (see Section~\ref{sec:DataImp}) will discuss approaches focused on handling missing values in tabular datasets. Next, we will explore \textit{trustworthy data synthesis} (see Section~\ref{sec:TrustSynth}), which includes privacy-preserving and fairness-preserving data generation techniques. Finally, in the \textit{anomaly detection} section (see Section~\ref{sec:AnoDec}), we will review methods that leverage diffusion models to identify anomalies by learning the normal data distribution.

For clarity, each section (Sections~\ref{sec:DataAug}, \ref{sec:DataImp}, \ref{sec:TrustSynth}, and \ref{sec:AnoDec}) begins with a brief introduction to the background, problem definitions and notations used, accompanied by a summary table of the relevant works. This is followed by an in-depth chronological review of the papers, emphasizing how they address key challenges, such as \textit{handling mixed feature types}, \textit{preserving feature dependencies}, and \textit{managing missing data}, along with their performance evaluations and limitations. To ensure completeness, before reviewing diffusion models for tabular data, we first introduce the core mechanism of diffusion models and the prominent frameworks used for tabular data modeling in Section~\ref{sec:preliminaries}. Readers already familiar with these concepts may skip this section.

\section{Preliminaries of Diffusion Models}
\label{sec:preliminaries}

In this section, we first introduce the core mechanism of diffusion models, covering both the forward diffusion process (gradual addition of noise) and the reverse process (learning to denoise). We then present prominent diffusion model frameworks, including DDPMs (Gaussian Diffusion Models), Multinomial Diffusion Models, Score-based Generative Models (SGMs), Score-based Generative Models through Stochastic Differential Equations (SDEs), and conditional diffusion models. For clarity, the notation used throughout this survey is summarized in Table~\ref{tab:notations}. 

 % Table of Notations
\begin{table}[]
\centering
\caption{Summary of notation used in this survey}
\begin{tabular}{@{}lp{0.7\columnwidth}@{}}
\toprule
\textbf{Notation} & \textbf{Description} \\ 
\midrule
$R$      & Table, where each row represents a sample and each column a feature or the prediction target  \\ 
$\mathbf{X}$      & Tabular data matrix \\ 
$\mathbf{x} :=(\mathbf{x}^{\text{num}}, \mathbf{x}^{\text{cat}})$      & An instance or sample, namely a row in the table, consisting of numerical and/or categorical feature values \\ 
$\mathbf{x}^{\text{num}}$      & The numerical part of the instance  \\ 
$\mathbf{x}^{\text{cat}}$      & The categorical part of the instance  \\ 
$\mathbf{x}$      & An instance or sample, namely a row in the table  \\ 
 $\prescript{j}{i}{\mathbf{x}_{t}}$ & The $j$-th feature value of the $i$-th sample at time $t$ \\
 $\prescript{j}{i}{\mathbf{x}_{t}^{\text{num}}}$ & The $j$-th numerical feature value of the $i$-th sample at time $t$ \\
  $\prescript{j}{i}{\mathbf{x}_{t}^{\text{cat}}}$ & The $j$-th categorical feature value of the $i$-th sample at time $t$ \\
 $\prescript{j}{i}{\mathbf{x}^{\text{num}}}$ & The $j$-th numerical feature value of the $i$-th sample \\
  $\prescript{j}{i}{\mathbf{x}^{\text{cat}}}$ & The $j$-th categorical feature value of the $i$-th sample \\
$\prescript{j}{i}{\mathbf{x}}$  & The $j$-th feature of the $i$-th sample \\
$\prescript{j}{}{\mathbf{x}}$ & The $j$-th feature of the given sample \\
$\prescript{}{i}{\mathbf{x}}$ & the $i$-th  sample \\ 
$\prescript{j}{}{\mathbf{x}_{t}}$ & the $j$-th feature of the given sample at time point $t$ \\ $\prescript{}{i}{\mathbf{x}_{t}}$ & the $i$-th sample at time $t$ \\ 
$\mathbf{x}_{t}$ & the sample at time $t$\\
$M_{\text{num}}$      & The number of numerical features in the table  \\ 
$M_{\text{cat}}$      & The number of categorical features in the table  \\ 
$\mathbf{z}$      & Latent embedding of $\mathbf{x}$ in the diffusion model \\ 
$\boldsymbol{\epsilon}$        & Gaussian noise added during the diffusion process \\ 
$T$               & Total number of timesteps in the diffusion process \\ 
$p_{\text{data}}(\cdot)$, $p_{\text{noise}}(\cdot)$         & Probability distribution of the data or noise \\ 
$\mathcal{L}$     & Loss function used for training the model \\ 
$\boldsymbol{\theta}$          & Parameters of the model \\
$S_{\boldsymbol{\theta}}(\cdot)$ & The score network function used to predict the score\\
$\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\cdot)$ & The noise function used to predict the added noise\\
\bottomrule
\end{tabular}
\label{tab:notations}
\end{table}

 \subsection{Mathematics of Diffusion Models}
 \label{sec:Background_Math_DM} 

\textit{Diffusion probabilistic models}, or more commonly known as \textit{diffusion models} \cite{sohl2015deep}, are deep generative models defined from a forward diffusion process and a reverse denoising process. Specifically, the \textit{diffusion process} aims to gradually corrupt a sample $\mathbf{x}_{0}$ (drawn from the training data distribution $q_{\text{data}}(\cdot)$) to a noisy instance $\mathbf{x}_{T}$ (defined by a prior distribution $q_{\text{noise}}(\cdot)$) by using the following process:
\begin{equation}
    q(\mathbf{x}_{1:T}\vert\mathbf{x}_{0}) := \prod_{t=1}^{T}q(\mathbf{x}_{t}\vert \mathbf{x}_{t-1}),\label{equ:DDPMForward}
\end{equation}
with $q(\mathbf{x}_{t}\vert \mathbf{x}_{t-1})$ the forward transition probability. Meanwhile, the \textit{denoising process} attempts to remove noises and generate a synthetic but realistic sample $\hat{\mathbf{x}}_{0}$ from $\mathbf{x}_{T}$ as follows:
\begin{equation}
    p_{\boldsymbol{\theta}}(\mathbf{x}_{0:T}):= p(\mathbf{x}_{T})\prod_{t=1}^{T}p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t}),\label{equ:DDPMReverse}
\end{equation}
with $p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t})$ an approximation of the reverse of the forward transition probability, which is learned by a neural network with parameters $\boldsymbol{\theta}$. Particularly, they learn $\boldsymbol{\theta}$ by minimizing the following variational upper bound ($L_{\text{VUB}}$) on the negative log-likelihood:

\begin{align}
   - \log \,\, & p(\mathbf{x}) \leq \mathbb{E}_{q(\mathbf{x}_{1}|\mathbf{x}_{0})}[   \underbrace{-\log p_{\boldsymbol{\theta}}(\mathbf{x}_{0}\vert \mathbf{x}_{1})}_{L_0(L_{\text{recons}})} \nonumber ] \\ 
   & + \underbrace{D_{KL}[q(\mathbf{x}_{T}\vert\mathbf{x}_{0})\vert\vert p(\mathbf{x}_{T})]}_{L_T(L_{\text{prior}})} \nonumber \\
    & + \sum_{t=2}^{T} \mathbb{E}_{q(\mathbf{x}_{t}|\mathbf{x}_{0})} \underbrace{D_{KL}[q(\mathbf{x}_{t-1}\vert\mathbf{x}_{t},\mathbf{x}_{0})\vert \vert p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t})]}_{L_{t}(L_{\text{diffusion}})} .
    \label{equ:DDPMLoss}
\end{align}

Here, $L_0$ can be interpreted as a reconstruction term that predicts the log probability of original sample $\mathbf{x}_{0}$ given the noised latent $\mathbf{x}_{1}$, while $L_T$ measures how the final corrupted sample $\mathbf{x}_{T}$ resembles the noise prior distribution; Meanwhile, $L_{t}$ measures how close is the estimated transition probability $p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t})$ to the ground-truth posterior transition probability $q(\mathbf{x}_{t-1}\vert\mathbf{x}_{t},\mathbf{x}_{0})$.

Note that the formulations provided above define only the generic structure of diffusion models. Depending on the specific data types (namely continuous or discrete), the definitions of prior noise distribution $p(\mathbf{x}_{T})$, forward transition probability $q(\mathbf{x}_{t} \vert \mathbf{x}_{t-1})$, the reverse of forward transition probability $p_{\theta}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t})$, and their training objectives can be different. In the following, we concisely review the key frameworks of diffusion models most commonly used in tabular data modeling: 1) Gaussian diffusion model, 2) multinomial diffusion model, 3) score-based generative model, 4) score-based generative model via SDEs, and 5) conditional diffusion models.

\subsubsection{Gaussian Diffusion Models} \label{sec:Background_Math_DDPM} 
Gaussian diffusion models, often known as Denoising Diffusion Probabilistic Models (DDPMs) \cite{ho2020denoising}, is a family of diffusion probabilistic models 
that operate in continuous spaces. They
define the diffusion and reverse processes  with the following instantiations:
\begin{subequations} \label{equ:DDPMEquationsCon}
\begin{align}
    p(\mathbf{x}_{T}) & := \mathcal{N}(\mathbf{x}_{T};\mathbf{0},\mathbf{I}), 
    \label{equ:DDPMContinuousPrior} \\
    q(\mathbf{x}_{t}\vert \mathbf{x}_{t-1}) & := \mathcal{N}(\mathbf{x}_{t};\sqrt{1-\beta_{t}}\mathbf{x}_{t-1},\beta_{t}\mathbf{I}),
    \label{equ:DDPMContinuousForward} \\
    p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t}) & := \mathcal{N}(\mathbf{x}_{t-1};\boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t),\boldsymbol{\Sigma}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)),
    \label{equ:DDPMContinuousBackward}
\end{align}
\end{subequations}
where Gaussian noises are gradually injected into the sample based on a time-dependent variance schedule $\{\beta_{t}\}_{t=1}^{T}$, with $\beta_{t}\in(0,1)$ determining the amount of noise added at time step $t$. To approximate $p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t})$, \cite{ho2020denoising} propose to define:
\begin{equation}
\begin{aligned}
\label{eq:DDPM_mu_Sigma}
    \boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t) &= \frac{1}{\sqrt{\alpha_{t}}}\Big(\mathbf{x}_{t} - \frac{\beta_{t}}{\sqrt{1-\bar{\alpha}_{t}}}\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)\Big), \\
    \boldsymbol{\Sigma}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t) &= \sigma_{t}\mathbf{I}, 
\end{aligned}
\end{equation}
where $\sigma_{t}$ controls the noise level added at time step $t$, $\alpha_{t} := 1- \beta_{t},\bar{\alpha}_{t}:=\prod_{i=1}^{t}\alpha_{i}$, and $\boldsymbol{\epsilon}_{\boldsymbol{\theta}}$ is a neural network to predict ground truth noise $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})$ that has been added to noise sample $\mathbf{x}_{t}$. As a result, they propose to optimize the following simplified objective function (rather than Eq.~\ref{equ:DDPMLoss}):
\begin{equation}
    L_{\text{simple}}^{\text{Gauss}}(\boldsymbol{\theta}) := \mathbb{E}_{t}\mathbb{E}_{\mathbf{x}_{0} \sim q(\mathbf{x}_{0})}\mathbb{E}_{\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0},\mathbf{I})}\big[\lambda(t)\Vert\boldsymbol{\epsilon}-\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)\Vert_{2}^{2}\big],\label{equ:DDPMContinuousLossSimple}
\end{equation}
where $\lambda(t)$ is a weighting function to adjust the noise scales, and $\Vert \cdot\Vert_2$ denotes the Euclidean norm. %Further note that $\mathbf{x}_t$ is a function of $\mathbf{x}_0$ and hence we do not need to explicitly take the expectation with respect to $\mathbf{x}_t$.

\subsubsection{Multinomial Diffusion Models} \label{sec:Background_Math_Multinomial} 
Multinomial diffusion models \cite{hoogeboom2021argmax} \cite{austin2021structured} is a representative (also the first) diffusion probabilistic model in discrete spaces. Their diffusion and denoising processes operate in discrete spaces, designed to generate categorical data $\mathbf{x}_{t} \in \{0,1\}^{K}$ (i.e., one-hot encoding with $K$ distinct values). They are defined as follows:
\begin{subequations} \label{equ:DDPMEquationsDis}
\begin{align}
    p(\mathbf{x}_{T}) & := \mathcal{\text{Cat}}(\mathbf{x}_{T};1/K), 
    \label{equ:DDPMDiscretePrior} \\
    q(\mathbf{x}_{t}\vert \mathbf{x}_{t-1}) & := \mathcal{\text{Cat}}(\mathbf{x}_{t};(1-\beta_{t})\mathbf{x}_{t-1}+\beta_{t}/K),
    \label{equ:DDPMDiscreteForward} \\
    p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t}) & := \sum_{\hat{\mathbf{x}}_{0}=1}^{K}q(\mathbf{x}_{t-1}\vert \mathbf{x}_{t},\hat{\mathbf{x}}_{0})p_{\boldsymbol{\theta}}(\hat{\mathbf{x}}_{0}\vert\mathbf{x}_{t}),
    \label{equ:DDPMDiscreteBackward}
\end{align}
\end{subequations}
with $\mathcal{\text{Cat}}(\cdot)$ categorical distribution and $K$ the number of categories (the computation between scalars and vectors are done in an element-wise way). Importantly, \textit{uniform noise} (rather than Gaussian noise) is added to the sample according to the noise schedule $\beta_{t}$. As we can see,  $p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t})$ is parameterized as $q(\mathbf{x}_{t-1}\vert \mathbf{x}_{t},\hat{\mathbf{x}}_{0}(\mathbf{x}_{t},t))$, with $\hat{\mathbf{x}}_{0}(\mathbf{x}_{t},t)$ predicted by a neural network, which can be trained via the multinomial diffusion loss  defined using Eq.~\ref{equ:DDPMLoss}.
% As a result, the multinomial diffusion loss $L^{\text{Mult}}$ is still defined using Eq.~\ref{equ:DDPMLoss}.

It is important to note that the multinomial diffusion model can handle only one categorical feature at a time. In other words, for a table with \( C \) categorical features, \( C \) separate multinomial diffusion models would need to be built. %(DDIM \cite{song2021denoising} may offer a more efficient solution.) 
More importantly, several novel diffusion models for discrete data have been proposed recently; these will be reviewed in Section~\ref{subsec:DiffModelInDiscreteAll} for better readability.

\subsubsection{Score-based Generative Models (SGMs)}
\label{sec:Background_Math_Score} For these models, the forward diffusion process follows the same structure as the Gaussian diffusion model, whereas the reverse process is defined differently, as shown below. Given an instance $\mathbf{x}$ and its distribution $p(\mathbf{x})$, its score function is defined as $\nabla_{\mathbf{x}}\log p(\mathbf{x})$. To estimate the score function, one can train a neural network $S_{\boldsymbol{\theta}}(\cdot)$ with the following objective:
\begin{equation}
    \mathbb{E}_{\mathbf{x}\sim p(\mathbf{x})}\Vert S_{\boldsymbol{\theta}}(\mathbf{x}) - \nabla_{\mathbf{x}}\log p(\mathbf{x}) \Vert^{2}_{2}.
\end{equation}
However, Song \& Ermon \cite{song2019generative} point out that the estimated score functions are inevitably imprecise in low density regions when the low-dimensional manifolds are embedded into a high-dimensional space. To mitigate this, in the diffusion process they propose to perturb the original data $\mathbf{x}$ with a sequence of random
Gaussian noises with intensifying scales $0<\sigma_{1}<\cdots <\sigma_{T}$. In other words, $p_{\sigma_{1}}\approx p(\mathbf{x}_{0})$, $p_{\sigma_{T}}\approx \mathcal{N}(\mathbf{0},\mathbf{I})$, and $p_{\sigma_{t}}\approx \mathcal{N}(\mathbf{x}_{t};\mathbf{x}_{0},\sigma_{t}^{2}\mathbf{I})$. In the reverse process, they utilize a noise-conditioned score network $S_{\boldsymbol{\theta}}(\cdot)$ to approximate $\nabla_{\mathbf{x}}\log p_{\sigma_{t}}(\mathbf{x})$, which analytically equals to $(\mathbf{x}_{0}-\mathbf{x}_{t})/\sigma_{t}$. As a result, the training objective is as follows:
\begin{equation}
    \frac{1}T{}\sum_{t=1}^{T}\lambda(\sigma_{t})\mathbb{E}_{p(\mathbf{x}_{0})}\mathbb{E}_{\mathbf{x}_{t}\sim p_{\sigma_{t}}(\mathbf{x}_{t}\vert\mathbf{x})}\left\Vert S_{\boldsymbol{\theta}}(\mathbf{x}_{t},\sigma_{t})+(\frac{\mathbf{x}_{t}-\mathbf{x}_{0}}{\sigma_{t}})\right\Vert^{2}_{2}.
\end{equation}
After training $ S_{\boldsymbol{\theta}}(\cdot)$, new samples are generated with the annealed Langevin dynamics (see \cite{song2019generative} for details). Note that SGMs are defined in discrete time space, a special case corresponding to the variance exploding form in the generalized version presented in the sequel.

\subsubsection{Score-based Generative Models through Stochastic Differential Equations (SDEs) \cite{songscore}} \label{sec:Background_Math_ScoreSDEs} This is a continuous-time generalization of the denoising diffusion models (DDPMs that correspond to variance preserving form) and score based generative models (SGMs that correspond to variance exploding form). Particularly, the \textit{diffusion process} is defined with the following itô stochastic differential equation \cite{kloeden1992stochastic}:
\begin{equation}
    d\mathbf{x} = \mathbf{f}(\mathbf{x},t)dt +g(t)d\mathbf{w}, \label{equ:ScoreSDEForward}
\end{equation}
where $\mathbf{f}(\mathbf{x},t)=f(t)\mathbf{x}$, and $f(\cdot)$, $g(\cdot)$ are referred to as the drift and diffusion coefficients of $\mathbf{x}_{t}$, respectively. 
%where $\mathbf{f}(\mathbf{x},t)=f(t)\mathbf{x}$, and $f(\cdot)$, $g(\cdot)$ are referred to as the drift and diffusion coefficients of $\mathbf{x}_{t}$, respectively. Concretely, $\mathbf{x}_{t}$ is the value of sample $\mathbf{x}$ at time point $t$, where $\mathbf{x}_{0}$ is a real sample and $\mathbf{x}_{T}$ is a noised sample. 
Moreover, $\mathbf{w}$ is the standard Wiener process. The most widely studied and commonly used diffusion models can be broadly categorized into three main types: 1) Variance Exploding (VE), 2) Variance Preserving (VP), and 3) sub-Variance Preserving (sub-VP) based on the types of functions  $f(\cdot)$ and $g(\cdot)$ as follows:
\begin{align}
    f(\mathbf{x},t)&=\begin{cases}0,  &\textrm{ if VE,}\\
    -\frac{1}{2}\gamma_{t}\mathbf{x},&\textrm{ if VP,}\\
    -\frac{1}{2}\gamma_{t}\mathbf{x},&\textrm{ if sub-VP,}\\
    \end{cases}\label{equ:ScoreSDEdrift}\\
    g(t)&=\begin{cases}\sqrt{\frac{\texttt{d}[\sigma^{2}_{t}]}{\texttt{d}t}},&\textrm{ if VE,}\\
    \sqrt{\gamma_{t}},&\textrm{ if VP,}\\
    \sqrt{\gamma_{t}(1-e^{-2\int_0^t \gamma_{s}\, \texttt{d}s})},&\textrm{ if sub-VP,}\\
    \end{cases}\label{equ:dScoreSDEiffusion}
\end{align}
where $\gamma_{t}$ and $\sigma_{t}$ are noise functions w.r.t. the time variable $t$. Meanwhile, the \textit{denoising process} is defined as the reverse of the diffusion process:
\begin{equation}
    d\mathbf{x}=[\mathbf{f}(\mathbf{x},t)-g(t)^{2}\nabla_{\mathbf{x}}\log p_{t}(\mathbf{\mathbf{x}})]dt+g(t)d\bar{\mathbf{w}},\label{equ:ScoreSDEReverse}
\end{equation}
where $\bar{\mathbf{w}}$ is a Wiener process running backward in time, and the score function $\nabla_{\mathbf{x}}\log p_{t}(\mathbf{\mathbf{x}})$ is approximated by a learnable neural network $S_{\boldsymbol{\theta}}(\mathbf{x},t)$.  However, directly approximating the score function is computationally intractable and thus they propose to  train $S_{\boldsymbol{\theta}}(\cdot)$ by estimating the transition probability $\nabla_{\mathbf{x}_{t}}\log p(\mathbf{\mathbf{x}}_{t}\vert\mathbf{x}_{0})$ as follows \cite{songscore}:
\begin{align}
    \underset{\theta}{\arg\min} & \, \mathbb{E}_{t}\Big\{\lambda(t) \mathbb{E}_{\mathbf{x}_{0}} \Big[\mathbb{E}_{\mathbf{x}_{t}\vert\mathbf{x}_{0}} \Big[ \nonumber \\
    & \lVert S_{\boldsymbol{\theta}}(\mathbf{x}_{t},t) - \nabla_{\mathbf{x}_t} \log p(\mathbf{x}_t \vert \mathbf{x}_0) \rVert^{2}_{2} \Big]\Big]\Big\},
    \label{equ:ScoreSDELoss}
\end{align}
where $\nabla_{\mathbf{x}_{t}}\log p(\mathbf{\mathbf{x}}_{t}\vert\mathbf{x}_{0})$ follows the Gaussian distribution and can be collected during the diffusion process. Moreover, $\lambda(t)$ is used to trade-off between sample quality and likelihood. 

After training $S_{\boldsymbol{\theta}}(\cdot)$, we can generate new samples with the following two methods: 1) the predictor-corrector framework, or 2) the \textit{probability flow} framework. In general,  the probability flow framework is preferred due to its fast sampling and exact log-probability computation compatibilities (see \cite{songscore} for more details). In short, the \textit{probability flow} employs the following neural ordinary differential equation (NODE) based model \cite{chen2018neural}:
\begin{equation}
    d\mathbf{x} = \left(\mathbf{f}(\mathbf{x},t)-\frac{1}{2}g(t)^{2}\nabla_{\mathbf{x}}\log p_{t}(\mathbf{x}) \right)dt, \label{equ:ScoreSDE_NODESolver}
\end{equation}
which describes a deterministic process whose marginal probability is equivalent to that of the original reverse SDE (namely Eq.~\ref{equ:ScoreSDEReverse}) \cite{songscore}.

\subsubsection{Conditional Diffusion Models}\label{sec:Background_Math_ConDiffusion}
The diffusion models introduced in Sections~\ref{sec:Background_Math_DDPM}, \ref{sec:Background_Math_Multinomial}, \ref{sec:Background_Math_Score}, and \ref{sec:Background_Math_ScoreSDEs} are all unconditional, meaning the posterior estimator function \( p_{\boldsymbol{\theta}}(\cdot) \) does not know the label of the data it is modeling. Given a label vector $\mathbf{y}$, \cite{sohl2015deep} and \cite{dhariwal2021diffusion} suggest that this can be achieved through a so-called \textit{conditional reverse process} in DDPM, defined as:
\begin{equation}
\label{Equ:ClassifierGuidanceDDPM}
    p_{\boldsymbol{\theta},\boldsymbol{\phi}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t},\mathbf{y}) \propto p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t})p_{\boldsymbol{\phi}}(\mathbf{y}\vert \mathbf{x}_{t-1}),
\end{equation}
which requires to train a classifier $p_{\boldsymbol{\phi}}(\cdot)$ and thus is known as \textit{classifier-guided} DDPM. Dhariwa and Nichol \cite{dhariwal2021diffusion} further approximate the logarithm of it with a perturbed Gaussian transition as follows:
\begin{equation}
\label{Equ:ClassifierGuidanceApprDDPM}
    \log( p_{\boldsymbol{\theta},\boldsymbol{\phi}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t},\mathbf{y})) \approx \log(p(\mathbf{z}))+C,
\end{equation}
where $C$ is a constant, $\mathbf{z} \sim \mathcal{N}(\boldsymbol{\mu} + \boldsymbol{\Sigma}\mathbf{g},\boldsymbol{\Sigma})$, and $\mathbf{g} = \nabla_{\mathbf{x}_{t-1}} \log (p_{\boldsymbol{\phi}}(\mathbf{y}\vert \mathbf{x}_{t-1}))\lvert_{\mathbf{x}_{t-1} =\boldsymbol{\mu}}$ is computed from the classifier $p_{\boldsymbol{\phi}}(\cdot)$. To avoid training a separate classifier, Ho \& Salimans \cite{ho2021classifier} propose a \textit{classifier-free guided} DDPM as follows:
\begin{equation}
\label{Equ:ClassifierFreeGuidanceDDPM}
    \bar{\boldsymbol{\epsilon}}(\mathbf{x}_{t},\mathbf{y},t) = \hat{\boldsymbol{\epsilon}}(\mathbf{x}_{t},t)+\omega_{g}[\hat{\boldsymbol{\epsilon}}(\mathbf{x}_{t},\mathbf{y},t)-\hat{\boldsymbol{\epsilon}}(\mathbf{x}_{t},t)],
\end{equation}
where $\hat{\boldsymbol{\epsilon}}(\cdot)$ is the noise estimator $\boldsymbol{\epsilon}_{\theta}(\cdot)$ defined in DDPM. Moreover, $[\hat{\boldsymbol{\epsilon}}(\mathbf{x}_{t},\mathbf{y},t)-\hat{\boldsymbol{\epsilon}}(\mathbf{x}_{t},t)]$ is guidance of $\mathbf{y}$ and $\omega_{g}$ the guidance weight.
 
\section{Diffusion Models for Data Augmentation}
\label{sec:DataAug}

\textit{Data augmentation} is a long-standing research problem in tabular data \cite{cui2024tabular}. In general, it can be divided into two different tasks: 1) \textit{data synthesis}, which is the process of generating synthetic data that mimics the characteristics of real-world data. Depending on whether we generate a single table or a set of connected tables, it can be further divided into \textit{single table synthesis} or \textit{multi-relational dataset synthesis}; and 2) \textit{over-sampling}, which balances an imbalanced table by increasing the number of samples in the minority class(es). Particularly, {over-sampling} can be considered as a special case of \textit{single table synthesis} where we only generate a part of the table. Accordingly, in this survey, we categorize relevant works into \textit{diffusion models for single-table synthesis} (including over-sampling) and \textit{diffusion models for multi-relational dataset synthesis}, providing their formal definitions and reviewing related studies in Section~\ref{subsec:DataAug_Single} and ~\ref{subsec:DataAug_Multi}, respectively.


\subsection{Diffusion Models for Single Table Synthesis} 
\label{subsec:DataAug_Single}

This subsection reviews works on \textit{diffusion models for single table synthesis} (including over-sampling). For clarity, we further subdivide related works into three sub-categories: 1) generic diffusion models for tabular data, 2) diffusion models for tabular data in healthcare domain, and 3) diffusion models for tabular data in finance  domain. To ensure readers can follow along, we present the notation and problem definitions below and provide a summary of the models in Table~\ref{SummarySingleTableSynthesis}. 

Given a table $R$, we utilize $\prescript{j}{i}{\mathbf{x}_{t}}$ to denote the $j$-th feature value of the $i$-th sample at time point $t$.  On this basis, let the number of numerical features be  $M_{\text{num}}$ and the number of categorical features be $M_{\text{cat}}$. By reorganizing the order of the features, a sample (i.e., a row) can be represented as $ \mathbf{x} = (\mathbf{x}^{\text{num}}, \mathbf{x}^{\text{cat}}) $, where $ \mathbf{x}^{\text{num}} \in \mathbb{R}^{M_{\text{num}}} $ and $ \mathbf{x}^{\text{cat}} \in \mathbb{Z}^{M_{\text{cat}}} $. The $j$-th categorical feature can have $C_{j}$ distinct feature values $\{1,\cdots,C_{j}\}$, namely $\prescript{j}{}{\mathbf{x}^{\text{cat}}} \in \{1,\cdots,C_{j}\}$. With these notations, the \textit{single table synthesis problem} can be defined as follows:
\begin{problem}[Single Table Synthesis]
 Given a table $R=\{\mathbf{x}\}$, we aim to learn a generative model $p_{\boldsymbol{\theta}}(R)$ to generate a synthetic table $\hat{R}=\{\hat{\textbf{x}}\}$ with high-quality and diversity.
\end{problem}

\begin{table}[htbp]
\centering
\caption{Overview of Diffusion Models for Single Table Synthesis. `*' refers to a name created by us for simplicity.}
\label{SummarySingleTableSynthesis}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Model Name} & \textbf{Year} & \textbf{Venue} & \textbf{Feature Type}  & \textbf{Domain}  \\ 
\midrule
\textbf{SOS} \cite{kim2022sos}  & 2022         & KDD            & Num by Default & Generic\\
\textbf{STaSy} \cite{kimstasy}              & 2023         & ICLR           & Num+Cat & Generic      \\
\textbf{TabDDPM} \cite{kotelnikov2023tabddpm}             & 2023         & ICML           & Num+Cat    & Generic   \\
 \textbf{CoDi} \cite{lee2023codi}               & 2023         & ICML           & Num+Cat      & Generic \\
\textbf{MissDiff} \cite{ouyang2023missdiff}               & 2023         & ICMLW           & Num+Cat  & Generic     \\
\textbf{AutoDiff} \cite{suh2023autodiff}           & 2023         & NeurIPSW          & Num+Cat+Mixed & Generic \\
\textbf{DPM-EHR}* \cite{nicholas2023synthetic}           & 2023         & NeurIPSW          & Num+Cat & Healthcare \\
\textbf{FinDiff} \cite{sattarov2023findiff}          & 2023         & ICAIF          & Num+Cat & Finance\\
\textbf{MedDiff} \cite{he2023meddiff}  & 2023 & ArXiv & Num by default  & Healthcare\\
\textbf{EHR-TabDDPM} \cite{ceritli2023synthesizing}  & 2023 & ArXiv & Num+Cat  & Healthcare\\
\textbf{TabSyn} \cite{zhangmixed}              & 2024         & ICLR           & Num+Cat        & Generic \\
\textbf{FlexGen-EHR} \cite{he2024flexible} & 2024         & ICLR           & Cat+TS       & Healthcare \\
\textbf{EHRDiff} \cite{yuan2024ehrdiff} & 2024         & TMLR           & Cat+Num+TS    & Healthcare  \\
\textbf{Forest-Diffusion} \cite{jolicoeur2024generating} & 2024 & AISTATS       & Num+Cat     & Generic   \\
\textbf{TabDiff} \cite{shi2024tabdiff}            & 2024         & NeurIPSW   & Num+Cat & Generic       \\
\textbf{EntTabDiff} \cite{liu2024entity}            & 2024         & ICAIF   & Num+Cat  & Finance      \\
\textbf{Imb-FinDiff} \cite{schreyer2024imb}            & 2024         & ICAIF   & Num+Cat  & Finance      \\
%\textbf{MTabGen} \cite{villaizan2024diffusion}            & 2024         & ArXiv   & Num+Cat      & Generic \\
\textbf{EHR-D3PM} \cite{han2024guided}            & 2024         & ArXiv   & Cat   & Healthcare    \\
\textbf{TabUnite} \cite{si2024tabunite}            & 2024         & OpenReview   & Num+Cat   & Generic    \\
\textbf{CDTD} \cite{mueller2024continuous}            & 2024         & OpenReview   & Num+Cat   & Generic    \\
\bottomrule
\end{tabular}%
}
\label{tab:tabular_data_synthesis}
\end{table}


\subsubsection{Generic Diffusion Models for Single Table Synthesis} In this part, we review generic diffusion models for single table synthesis, which represent the most significant advancements in tabular data generation. Unlike domain-specific models tailored for particular fields, such as healthcare or finance, generic models are designed to handle diverse types of tabular data, including mixed-type features (numerical, categorical) and datasets with varying scales, sparsity, and correlations. These models aim to be universally applicable across different domains, making them highly valuable for a wide range of machine learning tasks.


\textbf{SOS} \cite{kim2022sos} is the first to apply score-based generative models (SGMs via SDEs \cite{songscore}) for tabular data oversampling. Specifically, given a training dataset $\mathbf{X}_{\text{train}}$, which can be divided into $M$ distinct subsets $\mathbf{X}_{1}, \ldots, \mathbf{X}_{M}$ based on the labels of interest, they train a separate score network $S_{\theta_{m}}$ for each subset corresponding to a specific class $\mathbf{X}_{m}$ without altering the diffusion or denoising processes. SOS introduces two methods for generating new instances in the target minority class: 1) \textit{Style-transfer generation}: A sample $\mathbf{x}_{0}$ is first drawn from the major class, converted into a noised instance $\mathbf{x}_{T}$ using the shared diffusion process, and then denoised into a sample in the target class ($\hat{\mathbf{x}}_{0}$) using the specific denoising process of the target class $S_{\boldsymbol{\theta}_{\text{target}}}$; and 2) \textit{Plan generation}: A noise sample $\mathbf{x}_{T}$ is drawn from a Gaussian distribution and denoised into a sample in the target class ($\hat{\mathbf{x}}_{0}$) using the denoising process of the target class $S_{\boldsymbol{\theta}_{\text{target}}}$.

Besides, they propose a fine-tuning method to prevent the reverse SDE process from moving too far into the ``gray region," where instances from different classes overlap. However, their method primarily focuses on numerical features and does not explicitly handle categorical features. The model is evaluated with respect to machine learning utility.


Similar to SOS, \textbf{Naïve-STaSy} \cite{kimstasy} attempts to directly extend SGMs via SDE for tabular data but encounters  training difficulties. To address these challenges and enhance training stability, \textbf{STaSy} \cite{kimstasy} incorporates self-paced learning, which gradually trains the model from easier to more complex samples, and fine-tuning techniques that adjust model parameters incrementally. Specifically, for self-paced learning, they define the denoising score matching loss for the $i$-th sample $\prescript{}{i}{\mathbf{x}}$ as follows (based on Eq.~\ref{equ:ScoreSDELoss}):
\begin{equation}
l_{i} = \mathbb{E}_{t}\mathbb{E}_{\prescript{}{i}{\mathbf{x}}_{t}}\left[\lambda(t)\lVert S_{\boldsymbol{\theta}}(\prescript{}{i}{\mathbf{x}}_{t},t)-\nabla_{\prescript{}{i}{\mathbf{x}}_{t}}\log p(\prescript{}{i}{\mathbf{x}}_{t}\vert\prescript{}{i}{\mathbf{x}}_{0})\rVert^{2}_{2}\right], \label{equ:STaSyLoss1}
\end{equation}
and define the objective (based on self-paced learning) as:
\begin{equation}
    \underset{\mathbf{\boldsymbol{\theta}},\mathbf{v}}{\min}\sum_{i=1}^{N}v_{i}l_{i}+r(\mathbf{v};\alpha,\beta),
\end{equation}
where $r(\cdot)$ is a self-paced regularizer and $\alpha$ and $\beta$ are its hyperparameters. To solve the reverse SDE process, they utilize the \textit{probability flow} framework. Next, as the \textit{probability flow} framework enables the computation of exact log-probability, they further fine-tune $\boldsymbol{\theta}$ with Eq.~\ref{equ:STaSyLoss1} based on the exact log-probability.

To handle mixed feature types, they apply one-hot encoding for preprocessing categorical columns and use a softmax function followed by rounding for post-processing. For numerical columns, they employ a min-max scaler for preprocessing and its inverse transformation for post-processing. On this basis, they employ a single diffusion model (namely SDEs) for all features. However, their method has certain limitations: 1) it is slower compared to GAN-based methods; 2) SGMs are known to be unstable for high-dimensional data (e.g., high-resolution images), which may pose challenges for high-dimensional tabular datasets. They evaluate their approach using metrics such as machine learning utility, log probability, and diversity.

Unlike STaSy, which uses a single diffusion model for all feature types, \textbf{TabDDPM} \cite{kotelnikov2023tabddpm} employs Gaussian diffusion (DDPM) for numerical features after applying quantile transformations and multinomial diffusion with one-hot encoding for categorical features. These two processes are modeled independently, treating numerical and categorical features separately. Moreover, each categorical feature is considered independent, and a distinct forward diffusion process is applied to each one. The model is trained by minimizing the following loss function:
\begin{equation}  
    L_{\text{TabDDPM}} = L_{\text{simple}}^{\text{Gauss}} + \frac{\sum_{i}^{C}L_{i}^{\text{Mult}}}{C}  
\end{equation}  
where \( L_{\text{simple}}^{\text{Gauss}} \) represents the simplified loss function for Gaussian diffusion (refer to Eq.~\ref{equ:DDPMContinuousLossSimple}), and \( L_{i}^{\text{Mult}} \) denotes the loss function for the multinomial diffusion model of the \( i \)-th categorical feature (refer to Eq.~\ref{equ:DDPMLoss} for \( L_{\text{VUM}} \)), with \( C \) being the total number of categorical features. For conditional synthesis, they adopt two approaches:  1) for classification datasets, they employ a class-conditional model to learn \( p_{\boldsymbol{\theta}}(\mathbf{x}_{t-1}\vert \mathbf{x}_{t}, y) \), where \( y \) is the label scalar; and 2) for regression datasets, they treat the target column as an additional numerical feature and learn the joint distribution among all numerical features. 

As a result, TabDDPM may struggle to capture feature correlations effectively, including: 1) correlations between numerical and categorical features, 2) correlations among categorical features, and 3) correlations between categorical features and the target column in regression datasets. The model's performance is evaluated based on machine learning utility, fidelity, and privacy.


While STaSy \cite{kimstasy} handles categorical features through one-hot encoding and perform sampling in the continuous space, this approach can lead to suboptimal performance due to potential sampling errors. Additionally, treating categorical features as continuous variables may hinder the accurate modeling of correlations between numerical and categorical features. To address these limitations, \textbf{CoDi} \cite{lee2023codi} proposes to handle numerical and categorical features separately using two distinct diffusion models, similar to TabDDPM \cite{kotelnikov2023tabddpm}. Unlike TabDDPM, however, CoDi conditions the two diffusion models on each other, enabling them to co-evolve during training to better capture cross-feature correlations. To further strengthen the connection between the models, CoDi employs a contrastive learning approach with negative sampling.

Similar to STaSy, they use a min-max scaler to preprocess numerical features and apply one-hot encoding to categorical features before utilizing the diffusion models. However, CoDi is specifically tailored for datasets containing both numerical and categorical features, making it unsuitable for datasets with only numerical or only categorical features. The model is evaluated based on machine learning utility and diversity.

\textbf{AutoDiff} \cite{suh2023autodiff} addresses the challenge of \textit{heterogeneous features} by integrating an autoencoder with a score-based diffusion model. Specifically, the autoencoder is employed to map the original heterogeneous features into a continuous latent space, where the resulting embeddings serve as input for the forward diffusion process. After the reverse diffusion process is completed, the output of the diffusion model is passed through the autoencoder’s decoder to reconstruct samples in the original heterogeneous feature space. To handle the issue of \textit{mixed-type features}, they introduce a dummy variable that encodes the frequency of repeated values within such features. Additionally, they mitigate the challenge of \textit{feature correlations} by learning the joint distribution of feature embeddings in the continuous latent space. Building on this approach, they propose two models: STaSy-AutoDiff, which adapts the framework of STaSy, and TabDDPM-AutoDiff, which builds on TabDDPM, each utilizing AutoDiff as the underlying diffusion model.

They evaluate model performance across three key criteria: fidelity, machine learning utility, and privacy. In particular, their models exhibit weaker privacy guarantees compared to other approaches. They attribute this to two factors: the tendency of a highly sophisticated autoencoder to overfit the input data, and the inherent memorization behavior of diffusion models \cite{carlini2023extracting}. Additionally, they note that while TabDDPM demonstrates high fidelity for individual features, it fails to effectively capture the correlation structures among features.

The vanilla diffusion models typically require complete (fully observed) training data, while tabular data often suffers from missing values. Ouyang et al. \cite{ouyang2023missdiff} observe that the ``impute-then generate" pipeline may lead to learning bias due to the fact that single imputation cannot capture the data variability.
To train a diffusion model on data with missing values, they propose \textbf{MissDiff}. This is the first tabular generative model that can handle mixed-type features with missing values, where the missing values are directly used in the training without prior imputations. Specifically, MissDiff attempts to directly incorporate the uncertainty of missing values into the learning process as follows: it masks the regression loss of Denoising Score Matching, and finds the optimal parameters $\boldsymbol{\theta}$ by optimizing the following objective:
\begin{equation}
    \frac{T}{2}\mathbb{E}_{t}\{\lambda(t)\mathbb{E}_{p(\mathbf{x}^{\text{obs}}_{0},\mathbf{m})}\mathbb{E}_{p(\mathbf{x}^{\text{obs}}_{t}\vert \mathbf{x}^{\text{obs}}_{0})}L_{\text{miss}
    }\},
\end{equation}
with 
$
    L_{\text{miss}} := \lVert S_{\boldsymbol{\theta}}(\mathbf{x}_{t}^{\text{obs}},t) - \nabla_{\mathbf{x}_{t}^{\text{obs}}} \log p(\mathbf{x}_t^{\text{obs}} \vert \mathbf{x}_{0}^{\text{obs}}) \odot\mathbf{m} \rVert^{2}_{2},
$
and $\mathbf{m} = \mathbb{I}\{\mathbf{x}^{\text{obs}}_{0} = \text{``na"}\}$ denotes the missing entries, while the meanings of the other symbols remain consistent with those defined in Section~\ref{sec:Background_Math_ScoreSDEs}. Specifically, to ensure that the function $p(\cdot)$ and its corresponding score function are well-defined, they replace ``na" in numerical features with $0$ and introduce a separate category for ``na" in categorical features. To handle mixed-type features, they adopt the same techniques as STaSy \cite{kimstasy}  and CoDi \cite{kimstasy}, applying min-max scaling and its inverse during generation for numerical features, and using one-hot encoding for categorical features. For categorical feature generation, they apply a softmax function followed by a rounding operation. The model’s quality is evaluated based on fidelity and machine learning utility.

\textbf{TabSyn} \cite{zhangmixed} claims to be the first to explore the application of latent diffusion models for tabular data synthesis (although AutoDiff \cite{suh2023autodiff} also addressed this earlier). The authors emphasize key challenges when applying diffusion models directly to the original data space with mixed feature types: 1) simple encoding strategies, such as one-hot encoding and analog bit encoding, often lead to suboptimal performance; and 2) employing separate models for different feature types hampers the ability to capture cross-feature correlations. To overcome these limitations, TabSyn proposes a diffusion model that operates in a joint latent space where both numerical and categorical features are transformed into continuous embeddings, enabling the model to effectively capture inter-feature dependencies. This approach allows TabSyn to handle a wide range of data types while preserving their inherent correlations. The pipeline of TabSyn is as follows: 
\begin{enumerate}
    \item They first utilize a VAE to transform raw numerical and categorical features into continuous embeddings. Specifically, they learn a unique tokenizer for each feature (after applying one-hot encoding to each categorical feature) and feed the feature-wise embeddings into a VAE (with Transformer-based encoder and decoder) to capture correlations among features.
    \item Once the VAE model is well-trained, they train a score-based diffusion model using SDEs with Gaussian noise in the latent space.
    \item Finally, they learn a detokenizer to recover real feature values from the embeddings. 
\end{enumerate}

\begin{comment}
More concretely, given a sample $\mathbf{x}$, they first apply the feature tokenizer to obtain its embedding $\mathbf{e}$, which is then passed through the VAE encoder to produce a latent representation $\mathbf{z}$. The latent embedding $\mathbf{z}$ is used as input for the diffusion model, generating a synthetic embedding $\hat{\mathbf{z}}$ through the following diffusion and denoising processes:
\begin{equation}
\begin{aligned}
    \mathbf{z}_{t} & = \mathbf{z}_{0} + \sigma(t)\epsilon, \quad \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}), \\
    d\mathbf{z}_{t} & = -2\dot{\sigma}(t)\sigma(t)\nabla_{\mathbf{z}_{t}}\log p(\mathbf{z}_{t})dt + \sqrt{2\dot{\sigma}(t)\sigma(t)}d\bar{\omega}_{t},
\end{aligned}
\end{equation}
where $\mathbf{z}_{0} = \mathbf{z}$, $\mathbf{z}_{t}$ denotes the diffused embedding at time $t$, $\sigma(t) = t$ represents the noise level, and $\dot{\sigma}(t) = d\sigma(t)/dt$. Notably, they employ a simplified variant of the Variance Exploding SDE (VE-SDE) diffusion model introduced in Section~\ref{sec:Background_Math_ScoreSDEs}. After generating the synthetic latent embedding $\hat{\mathbf{z}}$, they feed it into the VAE decoder to obtain the reconstructed embedding $\hat{\mathbf{e}}$. Finally, the detokenizer is applied to $\hat{\mathbf{e}}$ to recover a synthetic sample $\hat{\mathbf{x}}$.
\end{comment}

Note that TabSyn lacks the capability to handle missing values directly.
It requires data preprocessing, including handling missing values and feature transformations, which may introduce bias or noise. The model's performance is evaluated based on fidelity, utility, diversity, and privacy. Notably, ablation studies reveal that substituting the VAE with simple one-hot encoding for categorical features results in the poorest performance, indicating that treating categorical features as continuous is inadequate. Furthermore, their results demonstrate that TabSyn-DDPM operating in the latent space outperforms TabDDPM in the original data space, underscoring the significance of learning latent embeddings to enhance diffusion modeling. However, note that TabSyn can be modified for missing data imputation by drawing an analogy to image inpainting (REPAINT \cite{lugmayr2022repaint}), where missing parts of an image are restored. 


Rather than relying on neural networks, which are commonly used as universal function approximators \cite{hornik1989multilayer} to estimate $S_{\boldsymbol{\theta}}(\cdot)$ or $\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\cdot)$ in diffusion models, \textbf{Forest-Diffusion} \cite{jolicoeur2024generating} employs XGBoost \cite{chen2016xgboost}, leveraging its strength in tabular data prediction and classification. Importantly, XGBoost can naturally handle missing data by learning the best split, allowing the model to be trained directly on incomplete datasets—an advantage over most generative models that require complete data. For categorical data, dummy encoding is applied during pre-processing, and dummy variables are rounded to the nearest class during post-processing. Specifically, Forest-Diffusion adopts the variance-preserving formulation of score-based generative models (see Section~\ref{sec:Background_Math_ScoreSDEs}). To estimate scores using XGBoost instead of neural networks, the method involves four key steps:
\begin{enumerate}
    \item The original dataset $\mathbf{X}$ (size $[N, D]$) is duplicated $n_{\text{noise}}$ times, corresponding to the number of discretized noise levels.
    \item Different noise levels are added to each duplicated dataset, meaning each sample $\mathbf{x}$ receives $n_{\text{noise}}$ noise variants.
    \item Linear interpolations between the original dataset and noise are computed for various time points $t$:
    $
    \mathcal{X}_{i}(t) = t\mathcal{X} + (1 - t)\mathcal{Z}_{i}, \quad \forall t \in [t_{1}, \ldots,$$ t_{n_{t}}], \quad i \in [1, \ldots, n_{\text{noise}}],
    $
    where $\mathcal{Z}_{i}$ represents the noise matrix for the $i$-th duplication.
    \item An XGBoost model is trained per noise level $t$, predicting the score $S(t) := \nabla_{\mathbf{x}_{t}} \log p_{t}(\mathbf{x}_{t} \vert \mathbf{x}_{0})$, resulting in $n_{t}$ models.
\end{enumerate}

For data imputation, Forest-Diffusion draws an analogy to image inpainting, where missing parts of an image are restored. They adopt REPAINT \cite{lugmayr2022repaint}, noting that while image inpainting models are trained on complete images, tabular imputation models operate on incomplete data, which XGBoost can naturally handle. For conditional synthesis, they train a separate XGBoost model for each label in classification tasks. In regression tasks, they follow a strategy similar to TabDDPM by treating the target column as an additional numerical feature. The quality of Forest-Diffusion is evaluated in terms of fidelity, diversity, machine learning utility, and statistical inference.


\textbf{TabDiff} \cite{shi2024tabdiff} identifies several limitations in prior diffusion methods for tabular data: 1) additional encoding overhead in latent diffusion models like TabCSDI \cite{zheng2022diffusion} and TabSyn \cite{zhangmixed}; 2) imperfect discrete-time diffusion modeling approaches, such as those used in TabDDPM \cite{kotelnikov2023tabddpm} and CoDi \cite{lee2023codi}; and 3) insufficient handling of feature-wise distribution heterogeneity in multi-modal frameworks. To address these issues, TabDiff employs a continuous-time diffusion framework with the following key steps:  
\begin{enumerate}
    \item Numerical features are normalized, and categorical features are transformed using one-hot encoding, with an additional [MASK] class.
    \item A joint diffusion process is proposed, where noise schedules for each feature are learnable.
    \item Denoising is performed simultaneously for all features using a single model, followed by inverse transformations to recover the original format.
\end{enumerate}

Despite claiming a ``joint'' diffusion process, TabDiff actually employs separate diffusion models for numerical and categorical features: 1) A single SDEs-based (with VE formulation, see Section~\ref{sec:Background_Math_ScoreSDEs}) diffusion model is used for all numerical features; and 2) For each categorical feature, a separate masking diffusion model (detailed in Section~\ref{subsec:DiffModelInDiscreteMasking}) is employed. Moreover, to better capture individual feature distributions, TabDiff introduces feature-specific noise schedules: 1) A power mean numerical schedule $\sigma^{\text{num}}_{\rho_{i}}(t)$, where $\rho_{i}$ is a learnable parameter for the $i$-th numerical feature; and 2) A log-linear categorical schedule $\sigma^{\text{cat}}_{\kappa_{j}}(t)$, where $\kappa_{j}$ is a learnable parameter for the $j$-th categorical feature. The model’s performance is evaluated using fidelity, machine learning utility, and privacy metrics.

\textbf{TabUnite} \cite{si2024tabunite} identifies feature heterogeneity as a key challenge in tabular data generation, where datasets comprise both numerical and categorical features, often interrelated—e.g., a person's numerical salary may be linked to their categorical education level and age. The authors propose that addressing this challenge requires effective encoding schemes for preprocessing input features before applying tabular generative models, such as diffusion models, which typically depend on continuous transformations for denoising score matching. They highlight several limitations of existing approaches: 1) separate generative processes for numerical and categorical features hinder the modeling of cross-feature correlations; 2) suboptimal encoding heuristics for categorical features—such as one-hot encoding—result in sparse high-dimensional representations \cite{poslavskaya2023encoding}, potentially causing underfitting in generative models \cite{krishnan2018challenges}; and 3) learned latent embeddings are often parameter inefficient.

To address these challenges, they utilize Quantile Transformer for continuous feature encoding, while categorical features are processed using analog bits encoding \cite{chenanalog}, PSK encoding, or dictionary encoding. They integrate these encodings into a unified data space and apply a single diffusion or flow model to generate high-quality tabular data. Notably, PSK and dictionary encodings, proposed by the authors, aim to provide more efficient and compact representations while preserving feature correlations. The model’s performance is evaluated based on fidelity, machine learning utility, and privacy.


\textbf{CDTD} \cite{mueller2024continuous} points out that existing diffusion models that can handle both numerical and categorical features, such as STaSy \cite{kimstasy}, CoDi \cite{lee2023codi}, TabDDPM \cite{kotelnikov2023tabddpm} and Forest-Diffusion \cite{jolicoeur2024generating}, are built upon the advances from the image domain. In other words, the noise schedules are not specifically designed to handle mixed features in tabular data, where different features have different characteristics. As a result, the noise schedules may not be directly transferable from the modalities of images and/or texts to tabular data, and these models may suffer from the following limitations: 1) the diffusion processes and their loss for different types of features are not aligned or balanced; as a result, models that simply combine different diffusion losses (for the numerical and categorical features) may implicitly favor the synthesis of some features or feature types over others \cite{ma2020vaem}; 2) these methods do not scale to large datasets; and 3) these methods do not perform well on categorical features with a large number of categories.


To address these challenges, CDTD enhances diffusion models for tabular data by improving noise schedules and diffusion losses. Specifically, they integrate score matching \cite{hyvarinen2005estimation} and score interpolation \cite{dieleman2022continuous}, embedding categorical features into a continuous space and applying a unified Gaussian diffusion process. This enables better modeling of feature correlations. Three noise scheduling strategies are explored: 
1) a single learnable noise schedule for all features, 
2) separate schedules for numerical and categorical features, and 
3) individual schedules per feature. To further refine mixed-feature handling, they introduce diffusion-specific loss normalization and improved model initialization. Using embeddings instead of one-hot encoding allows scaling to high-cardinality categorical features, while the ODE formulation accelerates sampling. Evaluations based on machine learning utility, fidelity, and privacy demonstrate that CDTD effectively captures feature correlations, with learnable noise schedules significantly enhancing synthetic data quality.




\subsubsection{Diffusion Models for Single Table Synthesis in Healthcare Domain} 
\label{subsubsec:HealthcareTadDiff}
In this part, we review diffusion models for single table synthesis in the healthcare domain, with a primary focus on Electronic Health Records (EHRs) data. EHRs represent a digital collection of patient health information, encompassing a wide variety of data types, including demographics, medical history, diagnoses, medications, lab test results, and treatment outcomes. EHR data is crucial for advancing medical research, enabling predictive modeling, and improving healthcare decision-making \cite{yadav2018mining}. However, due to the sensitive nature of the information, directly sharing EHR data poses significant privacy risks. In this context, synthetic EHR data generated by advanced models, such as diffusion models, can serve as a privacy-preserving alternative \cite{han2024guided}, allowing researchers to conduct exploratory analyses, model development, and validation without compromising patient confidentiality.  In recent years, diffusion models have emerged as a promising approach for generating high-quality synthetic EHR data, which will be reviewed below.

\textbf{MedDiff} \cite{he2023meddiff} is the first to employ diffusion model for generating EHRs. Building on the DDIM framework \cite{song2021denoisingDDIM}, MedDiff accelerates the generation process using Anderson acceleration \cite{anderson1965iterative}, a numerical method that improves the convergence speed of fixed-point iterations. Furthermore, it enables conditional data generation by incorporating the classifier-guided sampling technique \cite{dhariwal2021diffusion}. Specifically, the conditional noise prediction is formulated as:  
$
\hat{\boldsymbol{\epsilon}}_{\boldsymbol{\theta}}(\mathbf{x}_{t}, t, \mathbf{y}) = \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_{t}, t) - \sqrt{1 - \bar{\alpha}_{t}} \nabla_{\mathbf{x}_{t}} \log f_{\boldsymbol{\phi}}(\mathbf{y} \mid \mathbf{x}_{t}),
$  
where $\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_{t}, t)$ is the noise estimator of the unconditional diffusion model, and $f_{\boldsymbol{\phi}}(\cdot)$ represents the trained classifier.  However, MedDiff is limited to handling continuous features, while EHR data often contains both numerical (e.g., blood test results) and categorical (e.g., gender, ethnicity) attributes. The model's performance is evaluated based on fidelity and machine learning utility. Meanwhile, rather than relying on DDIM, \textbf{EHR-TabDDPM} \cite{ceritli2023synthesizing} is the first to utilize DDPM for generating mixed-type tabular EHR data. This approach directly applies TabDDPM \cite{kotelnikov2023tabddpm} to synthesize EHR datasets. The model's quality is assessed based on fidelity, machine learning utility, and privacy.


\textbf{DPM-EHR} \cite{nicholas2023synthetic} highlights the limitations of GANs, such as unstable training and mode collapse, which lead to suboptimal quality and diversity in synthetic data. To overcome these issues, they are among the first to employ diffusion models for generating longitudinal Electronic Health Records (EHR) with mixed-type features. For categorical features, they utilize one-hot encoding followed by the direct application of Gaussian diffusion models, in contrast to other approaches (which will be introduced later) that often adopt multinomial diffusion models. The quality of their method is assessed based on fidelity, diversity, privacy, and machine learning utility.


He et al. \cite{he2024flexible}  highlight that existing diffusion-based EHR models either handle numerical and categorical features separately or lack the capability to generate categorical features altogether. In clinical datasets collected from hospitals, numerical features (e.g., respiration rate) and categorical features (e.g., diagnosis and admission type) often exhibit inherent logical relationships. However, models such as TabDDPM face challenges in accurately capturing and representing these relationships, as they rely on separate diffusion models for numerical and categorical data. Moreover, they indicate two key challenges in generating EHR data: 1) EHRs typically comprise both static tabular measurements and temporal longitudinal records, resulting in heterogeneous features, and 2) EHRs often suffer from missing data. They observe that existing diffusion models struggle to maintain performance when handling missing modalities in such heterogeneous data. 

To overcome these limitations, they propose \textbf{FlexGen-EHR}, a model specifically designed to handle missing modalities while generating both static tabular and temporal longitudinal records simultaneously. Unlike most prior diffusion models that assume complete training data, this work addresses the more realistic scenario of NMCAR (Not Missing Completely At Random), where feature values may be missing for certain modalities across records. To handle this challenge, they introduce an optimal transport module that aligns and emphasizes a common feature space within the heterogeneous structure of EHRs. The FlexGen-EHR framework operates as follows: 1) an LSTM-based encoder processes temporal features, mapping them into a latent space, while an MLP-based encoder processes static features into a separate latent space; 2) these latent representations are concatenated and aligned using the optimal transport module to enhance coherence across modalities; 3) a latent diffusion model is applied in the unified latent space; and 4) two decoders are used to reconstruct the denoised representations back into temporal and static features, respectively. The proposed model is evaluated in terms of fidelity, machine learning utility, and privacy.


\textbf{EHRDiff} \cite{yuan2024ehrdiff} focuses on unconditional EHR data generation, without explicitly modeling conditional temporal structures. Specifically, it represents all features—categorical and continuous—as real values normalized to the range [0, 1], applying one-hot encoding for categorical features and  normalization for continuous ones. Using this representation, EHRDiff employs SDEs based diffusion models, while adapting the reparameterization approach from \cite{karras2022elucidating}. The reverse denoising process is formulated through probability flow ordinary differential equations. The model's performance is assessed using fidelity, machine learning utility, and privacy.

\textbf{EHR-D3PM} \cite{han2024guided} introduces a guided diffusion model for generating discrete tabular medical codes in EHRs, supporting both conditional and unconditional generation. Specifically, in the context of categorical EHRs, the goal is to generate a sequence $\mathbf{x}^{(\text{all})} := [\mathbf{x}^{(1)}, \ldots, \mathbf{x}^{(L)}]$, where each $\mathbf{x}^{(l)}$ represents the one-hot encoding of a categorical feature (i.e., a token). For the unconditional diffusion process, they utilize a D3PM \cite{austin2021structured}  with a multinomial distribution, adding a multinomial noise vector $\mathbf{q}_{\text{noise}}$ independently to each token. Consequently, the diffusion process for the sequence is defined as:
\begin{equation}
    q(\mathbf{x}^{(\text{all})}_{t} \mid \mathbf{x}^{(\text{all})}_{t-1}) = \prod_{l=1}^{L} \text{Cat}\left(\mathbf{x}_{t}^{(l)}; \beta_{t}\mathbf{x}_{t-1}^{(l)} + (1 - \beta_{t})\mathbf{q}_{\text{noise}}\right),
\end{equation}
with a corresponding reverse denoising process.

For conditional generation, they propose a training-free conditional generator: $
    p_{\boldsymbol{\theta}}(\mathbf{x}^{(\text{all})} \mid \mathbf{c}) \propto p_{\boldsymbol{\theta}}(\mathbf{x}^{(\text{all})}) \cdot p(\mathbf{c} \mid \mathbf{x}^{(\text{all})})
$, where $p_{\boldsymbol{\theta}}(\cdot)$ represents an unconditional EHR generator, and $p(\mathbf{c} \mid \mathbf{x}^{(\text{all})})$ is a classifier that approximates the true (but unknown) classifier $q_{\text{data}}(\mathbf{c} \mid \mathbf{x}^{(\text{all})})$. Additionally, they custom-design the energy function and apply energy-guided Langevin dynamics at the latent layer of the predictor network to perform sampling. The model's quality is evaluated in terms of fidelity, machine learning utility, and privacy.

\subsubsection{Diffusion Models for Single Table Synthesis in Finance Domain} In this part, we review diffusion models for single table synthesis in the finance domain, where the generation of realistic tabular data is critical. Financial data typically consists of highly sensitive and heterogeneous information, such as transaction records, account balances, credit histories, and market data. The ability to generate high-quality synthetic financial data has significant implications for various applications, including risk modeling, fraud detection, algorithmic trading, and privacy-preserving data sharing \cite{assefa2020generating}. Recent advancements in diffusion-based financial data synthesis will be reviewed below.

\textbf{FinDiff} \cite{sattarov2023findiff} targets the synthesis of financial tabular data with mixed-type features. The framework comprises four key stages: 1) \textit{Data Pre-processing}: Each categorical feature is transformed into a continuous embedding vector, ensuring that similar categories are positioned closely in the embedding space. These embeddings replace the original categorical values. Meanwhile, numerical features undergo only normalization without embedding; 2) \textit{Embeddings Assembly}: The normalized numerical features, categorical feature embeddings, time embeddings, and label embeddings are concatenated to form a unified input representation; 3) \textit{Model Training and Sampling}: A DDPM model is directly applied to the continuous embedding space for training and generation; and 4) \textit{Post-processing}: The denoised sample is mapped back to the original space. Categorical features are restored by assigning each to the closest category in the embedding space, while numerical features are denormalized. Particularly, by embedding categorical features in a continuous space, FinDiff captures latent structures and correlations among categorical variables. However, this method may be less effective if the semantic meaning of categories is ambiguous or unavailable. The model's performance is assessed in terms of fidelity, machine learning utility, and privacy.

\textbf{EntTabDiff} \cite{liu2024entity} highlights a critical limitation of prior tabular generative models: they often overlook the importance of a special column in financial tabular data that represents entities, such as companies or individuals. These models typically treat the entity column as a standard categorical feature, leading to several drawbacks: 1) increased privacy risks, 2) inability to generate synthetic data conditioned on specific entities, and 3) restriction to generating data only for entities present in the original dataset. To address these challenges, EntTabDiff introduces a novel approach for generating financial tabular data conditioned on entities. The method consists of two key components: 1) learning the distribution of entities in the training data to generate synthetic entities, and 2) generating tabular data conditioned on these synthetic entities, which is achieved by employing a cross-attention mechanism. For pre-processing, it follows the FinDiff approach \cite{sattarov2023findiff} by embedding categorical features and normalizing numerical ones. The model's performance is evaluated across fidelity, machine learning utility, and privacy.

\textbf{Imb-FinDiff} \cite{schreyer2024imb} is the first attempt to employ diffusion models to generate financial tabular data that explicitly addresses the class imbalance challenge. Imb-FinDiff is an extension of FinDiff \cite{sattarov2023findiff}, consisting of three modules: \textit{embedding module}, \textit{diffusion module}, and \textit{prediction module}. 

First, the \textit{embedding module} includes four sub-modules: 1) \textit{categorical feature embedding} that converts individual categorical feature into continuous embeddings, and Gaussian forward diffusion is performed on the obtained embeddings; 2) \textit{numerical feature embedding} that transforms individual numerical feature into continuous embeddings, and Gaussian forward diffusion is performed on the obtained embeddings; 3) \textit{diffusion timestep embedding} that employs positional encodings \cite{vaswani2023attentionneed} to transform timestep into continuous embedding; and 4) \textit{data class embedding} that embeds class labels into continuous embeddings by following TabDDPM \cite{kotelnikov2023tabddpm}. 

Second, the \textit{denoising diffusion module} aims to predict the added noises and class labels in three stages: 1) \textit{embedding projection} that projects the embeddings of categorical features, numerical features, timestep, and class labels from their original embedding spaces into a joint embedding space; 2) \textit{embedding synthesis} that constructs a combined vector of the projected embeddings of categorical features, numerical features, timestep, and class labels in the joint embedding space; 3) \textit{prediction de-embedding} that projects the combined vector back to their original embedding spaces with two projection head functions, one for predicting added noises and another for predicting class labels. These two functions will be processed by the third module as follows. 

The \textit{prediction module} consists of two objectives: 1) \textit{timestep noise loss} that ensures accurate denoising by computing the mean-squared errors between the ground truth noises and the predicted noises; and 2) \textit{class label loss} that ensures class-specific accuracy by measuring the mean-squared errors between the ground truth labels and the predicted labels. By incorporating the class label loss, Imb-FinDiff enhances its capability of generating minority classes samples. The model is evaluated using fidelity and machine learning utility.

\subsection{Diffusion Models for Multi-relational Data Synthesis}
\label{subsec:DataAug_Multi}

Most tabular data synthesis methods focus on single-table generation; however, real-world datasets often consist of multiple interconnected tables, highlighting the importance of multi-relational data synthesis—a challenge that remains largely underexplored. In this subsection, we first present the notation and problem definition. Then, we review related works in chronological order, with a summary provided in Table~\ref{SummaryMultiTableSynthesis}.



\begin{table}[htbp]
\centering
\caption{Overview of Diffusion Models for Multi-relational Data Synthesis}
\label{SummaryMultiTableSynthesis}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Model Name} & \textbf{Year} & \textbf{Venue} & \textbf{Feature Type} & \textbf{Domain}\\ 
\midrule
\textbf{ClavaDDPM} \cite{pang2024clavaddpm}             & 2024         & NeurIPS           & Num by default  & Generic     \\
\textbf{GNN-TabSyn} \cite{hudovernik2024relational}  & 2024         & NeurIPSW            & Num+Cat & Generic \\
\bottomrule
\end{tabular}%
}
\label{tab:tabular_data_synthesis}
\end{table}

In addition to capturing the characteristics of individual tables, multi-relational data synthesis requires modeling relationships across multiple tables that are connected (i.e., constrained by foreign keys). A multi-relational database $\mathcal{R}$ consists of $P$ tables $\{R_{1},...,R_{P}\}$. Each table $R_{p}$ has \textit{primary key} which is the unique identifiers of rows. Given two tables  $R_{p}$ and  $R_{q}$, we say $R_{p}$ refers to $R_{q}$ (or $R_{p}$ has a \textit{foreign key constraint} with $R_{q}$, or $R_{p}$ has a \textit{parent-child relationship} with $R_{q}$ where $R_{q}$ is the \textit{parent} and $R_{p}$ is the \textit{child}) if $R_{p}$ has a feature (called \textit{foreign key}) that refers to the primary key of $R_{q}$. With these notations, the \textit{multi-relational data synthesis problem} can be defined as follows \cite{pang2024clavaddpm}:
\begin{problem}[Multi-relational Data  Synthesis]
   Given $\mathcal{R} = \{R_{1},...,R_{P}\}$, we aim to generate a synthetic database $\tilde{\mathcal{R}} = \{\tilde{R}_{1},...,\tilde{R}_{P}\}$ that preserves the structure, foreign-key constraints, as well as correlations among features (including inter-column correlations with the same table, inter-table correlations, and intra-group correlations with the same foreign key group).
\end{problem}

Rather than focusing on single-table synthesis, \textbf{ClavaDDPM} \cite{pang2024clavaddpm} addresses the generation of multi-relational databases comprising multiple interconnected tables. The authors identify key limitations in existing approaches, particularly regarding scalability for large datasets and the ability to capture long-range dependencies (e.g., correlations across features in different tables). To overcome these challenges, ClavaDDPM introduces clustering labels as intermediaries to model inter-table relationships. Built upon TabDDPM \cite{kotelnikov2023tabddpm} as its backbone, ClavaDDPM differs by employing a unified DDPM model for handling both numerical and categorical features, aiming to reduce computational overhead. Unlike TabDDPM, which uses separate diffusion models for different feature types, ClavaDDPM converts categorical features into a continuous space via label encoding (i.e., mapping categories to integer values) and processes all features within a single diffusion framework. Additionally, it adopts classifier-guided sampling \cite{dhariwal2021diffusion} to enhance sample generation.

ClavaDDPM’s performance is evaluated using both multi-table and single-table metrics. Multi-table metrics include cardinality for intra-group correlation, column-wise density estimation, pair-wise column correlation, and average 2-way dependency to assess both short- and long-range correlations. Single-table metrics include $\alpha$-precision, $\beta$-recall, and machine learning utility.

\textbf{GNN-TabSyn} \cite{hudovernik2024relational} highlights a key limitation of existing data synthesis methods: the lack of explicit modeling for the topological structure of relational databases, which hampers their ability to capture inter-column dependencies across tables. To address this, the authors represent a relational database as a heterogeneous graph, where nodes correspond to rows, and edges represent foreign key relationships connecting these rows, with both nodes and edges assigned specific types. This approach enables the modeling of table relationships induced by foreign key constraints. They employ graph neural networks (GNNs) to learn latent embeddings from this heterogeneous graph, effectively capturing both the structural information and inter-table dependencies. Building on these embeddings, they extend the latent tabular diffusion model, TabSyn \cite{zhangmixed}, to support conditional generation by incorporating the GNN-derived embeddings as conditions to guide the diffusion process. This integration allows the model to generate data that reflects complex relational structures. However, a noted limitation of GNN-TabSyn is its inability to synthesize unseen graph structures. The model’s performance is evaluated using multi-table fidelity (via the DDA metric \cite{hudovernik2024benchmarking}), privacy, and machine learning utility.


\section{Diffusion Models for Data Imputation}
\label{sec:DataImp}

Missing values imputation is a long-standing research problem in data mining and machine learning community \cite{lin2020missing}. These approaches can be divided into two categories \cite{jarrett2022hyperimpute}: 1) \textit{iterative approaches} that estimate the conditional distribution of one feature based on other features (e.g., MICE\cite{van2000multivariate}); and 2) \textit{deep generative approaches} that train a generative model to generate values in missing parts based on observed parts (e.g., MIDA \cite{gondara2018mida} and MIWAE \cite{mattei2019miwae} based on denoisng AEs, HIVAE \cite{nazabal2020handling} based on VAEs, and GAIN \cite{yoon2018gain} based on GANs). We formally define   the missing data imputation problem below.

\begin{problem} [Tabular Data Imputation]
Given a $D$-dimensional training dataset $\mathbf{X} = \{\prescript{}{i}{\mathbf{x}}\}_{i=1}^{N}$, a feature $j \in \{1,...,D\}$ of $\prescript{}{i}{\mathbf{x}}$ is denoted as $\prescript{j}{i}{\mathbf{x}}$, where the $j$-th feature can be numerical or categorical. Besides, any feature $j$ can suffer from missing values. Let $\mathcal{X} = (\mathbb{R}\cup\emptyset)^{D}$ be the input space, where $\mathbb{R}$ denotes the real number space for a numerical feature. For brevity, we also utilize $\mathbb{R}$ to denote the corresponding range if it is  a categorical feature. Data imputation aims to find a function $f(\cdot)$: $\mathcal{X} \to \mathbb{R}^{D}$ which can replace the missing values with reasonable values.
\end{problem}
%Maybe we can replace the problem statement with another one in \cite{chen2024rethinking}

\begin{table}[htbp]
\centering
\caption{Overview of Diffusion Models for Data Imputation}
\label{SummaryDataImputation}
\resizebox{\columnwidth}{!}{
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Model Name} & \textbf{Year} & \textbf{Venue} & \textbf{Feature Type}  & \textbf{Domain} \\ 
\midrule
\textbf{TabCSDI} \cite{zheng2022diffusion}  & 2022         & NeurIPSW            & Num+Cat & Generic\\
%\textbf{TabSyn} \cite{zhangmixed}              & 2024         & ICLR           & Num+Cat    & Generic    \\
\textbf{TabDiff} \cite{shi2024tabdiff}            & 2024         & NeurIPSW   & Num+Cat     & Generic   \\
\textbf{SimpDM} \cite{liu2024self}            & 2024         & CIKM   & Num+Cat    & Generic    \\
\textbf{MTabGen} \cite{villaizan2024diffusion}            & 2024         & ArXiv   & Num+Cat     & Generic  \\
\textbf{DDPM-Perlin} \cite{wibisono2024natural}            & 2024         & KBS   & Num   & Generic    \\
\textbf{NewImp} \cite{chen2024rethinking}            & 2024         & NeurIPS   & Num  & Generic    \\
\textbf{DiffPuter} \cite{zhang2024unleashing}            & 2024         & OpenReview   & Num+Cat  & Generic   \\
\bottomrule
\end{tabular}%
}
\label{tab:tabular_data_synthesis}
\end{table}

\textbf{TabCSDI} \cite{zheng2022diffusion} is the first to apply diffusion models for missing value imputation in tabular data. TabCSDI is an extension of CSDI \cite{tashiro2021csdi}, a method initially designed for time series data that lacks direct support for categorical features. The core idea of CSDI involves partitioning the input $\mathbf{x}$ into an observed part (conditioning set $\mathbf{x}^{\text{con}}$) and an unobserved part (target set $\mathbf{x}^{\text{tar}}$). Instead of unconditionally generating the entire input, a conditional diffusion model is trained using the observed part as a condition. The generative (denoising) process is defined as:
\begin{equation}
\label{TabCSDIReverse}
    p_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t-1}\vert\mathbf{x}^{\text{tar}}_{t},\mathbf{x}^{\text{con}}_{0}) = \mathcal{N}(\mathbf{x}^{\text{tar}}_{t-1};\mu_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t},t\vert \mathbf{x}^{\text{con}}_{0}),\sigma \mathbf{I}),
\end{equation}
where the goal is to learn $\mu_{\boldsymbol{\theta}}(\cdot)$ to predict the missing values.

To enable simultaneous handling of numerical and categorical features, three encoding techniques are explored for categorical features: one-hot encoding, analog bits encoding \cite{chenanalog}, and feature tokenization \cite{gorishniy2021revisiting}. The diffusion process of CSDI is then performed on the pre-processed input $\mathbf{z}_{0}$ rather than the raw input $\mathbf{x}_{0}$, resulting in $\mathbf{z}_{T}$. The denoising process is subsequently applied to obtain $\hat{\mathbf{z}}_{0}$, and the final imputed values $\hat{\mathbf{x}}_{0}$ are recovered from $\hat{\mathbf{z}}_{0}$ using appropriate decoding techniques corresponding to the chosen encoding methods. The quality of imputation is evaluated in terms of machine learning utility.

\textbf{TabDiff} \cite{shi2024tabdiff}, originally proposed for data synthesis (see Section~\ref{subsec:DataAug_Single}), can also be adapted for missing value imputation by employing classifier-free guidance for conditional imputation. Let $\mathbf{y} = \{[\mathbf{y}^{\text{num}}, \mathbf{y}^{\text{cat}}]\}$ denote the set of observed features used as conditions, and $\mathbf{x} = \{[\mathbf{x}^{\text{num}}, \mathbf{x}^{\text{cat}}]\}$ represent the set of features with missing values to be imputed. TabDiff is extended for conditional generation by fixing the observed features $\mathbf{y}_{t}$ as $\mathbf{y}$ while denoising only $\mathbf{x}_{t}$ during the reverse process. For numerical features, the imputation of $\mathbf{x}_{0}^{\text{num}}$ is achieved through an interpolation of the conditional and unconditional estimates in DDPM:
\begin{equation}
    \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}^{\text{num}}_{t}, t) = (1 - \omega) \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}^{\text{num}}_{t}, \mathbf{y}, t) + \omega \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}^{\text{num}}_{t}, t).
\end{equation}
For categorical features, the imputed values $\mathbf{x}_{0}^{\text{cat}}$ are predicted using a weighted interpolation of the conditional and unconditional probabilities:
\begin{equation}
\begin{aligned}
    \log \tilde{p}_{\boldsymbol{\theta}}(\mathbf{x}_{s}^{\text{cat}} \mid \mathbf{x}_{t}^{\text{cat}}, \mathbf{y}) 
    = & \, (1 + \omega) \log p_{\boldsymbol{\theta}}(\mathbf{x}_{s}^{\text{cat}} \mid \mathbf{x}_{t}^{\text{cat}}, \mathbf{y}) \\
    & + \omega \log p_{\boldsymbol{\theta}}(\mathbf{x}_{s}^{\text{cat}} \mid \mathbf{x}_{t}^{\text{cat}}),
\end{aligned}
\end{equation}
where $p_{\boldsymbol{\theta}}(\mathbf{x}_{s}^{\text{cat}} \mid \mathbf{x}_{t}^{\text{cat}})$ is defined as in the Masking Diffusion Model (see Eq.~\ref{DiscreteMaskingReverse}).


Liu et al. \cite{liu2024self} identify that vanilla diffusion models are sub-optimal for tabular data imputation due to two key mismatches between the imputation task and other tasks where diffusion models have shown success. First, there is a \textit{mismatch in learning objectives}: in generation tasks, \textit{diversity} is a critical goal, making diffusion models sensitive to the initial noise $\mathbf{x}_{T}$, where varying $\mathbf{x}_{T}$ results in diverse generated samples $\hat{\mathbf{x}}_{0}$. However, in imputation tasks, \textit{accuracy} is paramount, requiring the imputed values to closely match the ground truth. This sensitivity to initial noise, while promoting \textit{diversity} in generation, can harm \textit{accuracy} in imputation. Second, there is a \textit{data scale mismatch}: unlike image and text domains, which typically have tens of thousands to millions of training samples, tabular datasets often consist of only a few thousand samples. This limited data scale can hinder the diffusion model's ability to fully capture the underlying tabular data manifold, resulting in sub-optimal imputation performance.

To address these mismatch issues, they propose \textbf{SimpDM}, which incorporates self-supervised techniques. To tackle the objective mismatch, a self-supervised alignment mechanism is introduced to regularize the diffusion model's output, ensuring consistent and accurate imputations for the same observed data. To mitigate the data scale mismatch, they adopt a perturbation-based data augmentation strategy to enhance the training set. Specifically, for handling tabular data with both numerical and categorical features, they follow the same approach as TabDDPM \cite{kotelnikov2023tabddpm}, employing joint Gaussian diffusion for numerical features and separate multinomial diffusion models for categorical features. Consequently, SimpDM inherits similar limitations. They evaluate the model’s quality based on the RMSE between actual values and imputed values.

\textbf{MTabGen} \cite{villaizan2024diffusion} extends TabDDPM \cite{kotelnikov2023tabddpm} for handling mixed-type tabular data by employing a Gaussian diffusion model for numerical features and multinomial diffusion models for categorical features. Unlike TabDDPM, MTabGen first applies a Gaussian quantile transformation to normalize numerical features and uses ordinal encoding for categorical features. All heterogeneous features are then projected into a unified continuous latent space, with each feature's embedding modeled independently during the diffusion process, where noise components are sampled separately for each feature.

Particularly, MTabGen introduces three key improvements over prior tabular diffusion models. First, they use a transformer-based encoder-decoder as the denoising model (in contrast to existing methods that rely on MLPs) to better capture feature correlations. Second, they enhance the conditioning mechanism by incorporating condition information into the transformer's attention mechanism. Specifically, they reduce learning bias and improve correlation modeling between unmasked (observed) and masked (missing) features by using a transformer encoder for unmasked feature embeddings, which are then fed into a conditioning attention module. In contrast, prior methods such as TabCSDI \cite{zheng2022diffusion} and TabDDPM \cite{kotelnikov2023tabddpm} directly add condition embeddings to masked feature embeddings. Third, they adopt a unified framework with dynamic masking, enabling the model to handle varying numbers of visible features. Consequently, MTabGen can perform both data synthesis and data imputation within a single framework, treating data synthesis as a special case of data imputation where all feature values are missing. They evaluate the model’s quality in terms of fidelity, machine learning utility, and privacy for data synthesis, and machine learning utility for data imputation.

Similar to the seminal work TabCSDI, \textbf{DDPM-Perlin} \cite{wibisono2024natural} separates the input $\mathbf{x}$ into conditional (observed) part $\mathbf{x}^{\text{con}}$ and target (missing) part $\mathbf{x}^{\text{tar}}$. Based on this, they model the reverse imputation process, using the observed part to impute the unobserved part as follows:
\begin{equation}
\label{DDPM-Perlin}
p_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{0:T} \vert \mathbf{x}^{\text{con}}_{0}) := p(\mathbf{x}_{T}^{\text{tar}}) \prod_{t=1}^{T} p_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t-1} \vert \mathbf{x}^{\text{tar}}_{t}, \mathbf{x}^{\text{con}}_{0}),
\end{equation}
with $p(\mathbf{x}_{T}^{\text{tar}}) \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$, and $p_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t-1} \vert \mathbf{x}^{\text{tar}}_{t}, \mathbf{x}^{\text{con}}_{0})$ is defined as:
\begin{equation}
\label{DDPM-Perlin-Reverse}
p_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t-1} \vert \mathbf{x}^{\text{tar}}_{t}, \mathbf{x}^{\text{con}}_{0}) = \mathcal{N}(\mathbf{x}^{\text{tar}}_{t-1}; \boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t}, t \vert \mathbf{x}^{\text{con}}_{0}), \boldsymbol{\Sigma}_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t}, t \vert \mathbf{x}^{\text{con}}_{0})).
\end{equation}
By connecting this function to the unconditional DDPM model, they define:
\begin{equation}
\begin{aligned}
\boldsymbol{\mu}_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t}, t \vert \mathbf{x}^{\text{con}}_{0}) 
&= \boldsymbol{\mu}_{\boldsymbol{\theta}}^{\text{DDPM}}(\mathbf{x}^{\text{tar}}_{t}, t, \boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t}, t \vert \mathbf{x}^{\text{con}}_{0})); \\
\boldsymbol{\Sigma}_{\boldsymbol{\theta}}(\mathbf{x}^{\text{tar}}_{t}, t \vert \mathbf{x}^{\text{con}}_{0}) 
&= \boldsymbol{\Sigma}_{\boldsymbol{\theta}}^{\text{DDPM}}(\mathbf{x}^{\text{tar}}_{t}, t),
\end{aligned}
\end{equation}
where $\boldsymbol{\mu}_{\boldsymbol{\theta}}^{\text{DDPM}}(\cdot)$ and $\boldsymbol{\Sigma}_{\boldsymbol{\theta}}^{\text{DDPM}}(\cdot)$ are the standard DDPM functions, and $\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\cdot)$ is a neural network predicting the ground truth noise $\boldsymbol{\epsilon}$. In the standard DDPM model, a clean sample $\mathbf{x}_{0}$ is gradually corrupted to a noisy sample $\mathbf{x}_{t}$ by adding Gaussian noise:
\begin{equation}
\label{DDPMCorruptionGaussian}
\mathbf{x}_{t} = \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0} + \sqrt{1 - \bar{\alpha}_{t}} \boldsymbol{\epsilon}, \quad \boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}).
\end{equation}
In this work, they replace Gaussian noise with Perlin noise:
\begin{equation}
\label{DDPMCorruptionPerlin}
\mathbf{x}_{t} = \sqrt{\bar{\alpha}_{t}} \mathbf{x}_{0} + \boldsymbol{\epsilon}_{\text{Perlin}}(\sqrt{1 - \bar{\alpha}_{t}}),  
\end{equation}
where the Perlin noise is defined as:
\begin{equation}
\boldsymbol{\epsilon}_{\text{Perlin}}(x) = \sum_{i=0}^{S} p^{i} \cdot \text{noise}(2^{i} x),
\end{equation}
with $S$ as the number of steps, $p^{i}$ as the persistence degree at step $i$, and $\text{noise}(\cdot)$ as the noise function.

However, it is important to note that Eq.~\ref{DDPMCorruptionPerlin} lacks theoretical support, as Perlin noise does not possess the additive properties of Gaussian noise. Furthermore, this approach is limited to numerical features. They evaluate the model's performance in terms of RMSE under three missing data scenarios: MNAR (missing not at random), MAR (missing at random) and MCAR (missing completely at random).

Chen et al. \cite{chen2024rethinking} highlight that directly applying diffusion models to missing value imputation yields suboptimal performance due to two key issues: 1) the inherent diversity of samples generated by diffusion models hampers accurate inference of missing values, echoing similar concerns raised in \cite{liu2024self}; and 2) the data masking reduces the number of completely observable samples that can be used for model training, and it is challenging to ensure that the masking mechanism aligns well with the true missing mechanism in the testing data. To address these challenges, they propose \textbf{NewImp}, a diffusion-based imputation method under the Wasserstein Gradient Flow framework \cite{mokrov2021large}. This framework reformulates diffusion models to derive a \textit{cost functional} that governs the diffusion process. Specifically, to tackle the first challenge, they incorporate a negative entropy regularization term into the cost functional, aiming to suppress the sample diversity; For the second challenge, they eliminate the needs for data masking by replacing the conditional distribution based cost functional with joint distribution one. However, their model can handle only numerical features. The model's performance is evaluated using MAE (mean absolute error) and WASS (Wasserstein-2 distance) \cite{jarrett2022hyperimpute} under two missing data scenarios: MAR and MCAR.

Broadly, deep learning-based imputation methods can be categorized into two main families: 1) predictive models that infer missing entries based on observed ones, guided by masking mechanisms, and 2) generative models that estimate the joint distribution of both missing and observed entries, enabling imputation via conditional sampling. \textbf{DiffPuter} \cite{zhang2024unleashing} argues that predictive models often outperform generative models in imputation tasks due to the inherent challenge of incomplete likelihood estimation in generative approaches. Specifically, generative models must estimate the joint distribution while the missing entries remain unknown, leading to potential errors in density estimation.

To overcome this limitation, they propose integrating the Expectation-Maximization (EM) algorithm \cite{dempster1977maximum} with diffusion models. The EM-based approach operates as follows: 1) during the M-step, the missing data $\mathbf{x}^{(\text{miss})}$ is fixed, and a diffusion model is trained to learn the joint distribution of observed and missing entries, i.e., \( p_{\boldsymbol{\theta}}(\mathbf{x}) = p_{\boldsymbol{\theta}}(\mathbf{x}^{(\text{obs})}, \mathbf{x}^{(\text{miss})}) \); 2) during the E-step, the model parameters $\boldsymbol{\theta}$ are fixed, and the missing values $\mathbf{x}^{(\text{miss})}$ are updated using the reverse denoising process of the learned diffusion model. They further demonstrate theoretically that the diffusion model’s training process corresponds to the E-step, while its sampling process corresponds to the M-step in the EM framework. For preprocessing, they apply one-hot encoding to transform categorical features into numerical representations, after which a Gaussian diffusion model is applied to all features. They evaluate the performance using MAE and RMSE for continuous features, and Accuracy for discrete features, under three missing data scenarios: MNAR, MAR, and MCAR.

\section{Diffusion Models for Trustworthy Data Synthesis}
\label{sec:TrustSynth}

Trustworthy data synthesis encompasses two key aspects: \textit{privacy-preserving data synthesis} and \textit{fairness-preserving data synthesis}.  The former aims to use diffusion models to generate synthetic tabular data that protects privacy in sensitive datasets (e.g., healthcare, finance) while preserving data utility. For privacy-preserving data synthesis task, we further define \textit{cross-silo tabular synthesis problem} and \textit{federated learning based tabular synthesis problem} as follows, respectively.
\begin{problem}[Cross-Silo Tabular Synthesis]
  Consider there are $L$ distinct parties $\{P_{1},...,P_{L}\}$, each party stores a subset of features $\mathbf{X}_{l} \in \mathbb{R}^{N\times D_{l}}$ with $D_{l}$ the number of features stored at party $P_{l}$. Altogether, we have $\mathbf{X}=\mathbf{X}_{1}||\mathbf{X}_{2}||...||\mathbf{X}_{L}$ (``$||$" means column-wise concatenation) and $\mathbf{X} \in \mathbb{R}^{N\times D}$ with $D=\sum_{l=1}^{L}D_{l}$. The goal of cross-silo tabular synthesis is to generate a synthetic dataset $\tilde{\mathbf{X}}=\tilde{\mathbf{X}}_{1}||\tilde{\mathbf{X}}_{2}||...||\tilde{\mathbf{X}}_{L}$ that are distributionally similar to the original dataset $\mathbf{X}$ while maintaining the private information of the actual values.  
\end{problem}

\begin{problem}[Federated Tabular Synthesis] Consider there are $M$ distinct clients $\{Q_{1},...,Q_{M}\}$, each client stores a subset of samples $\mathbf{X}_{m} \in \mathbb{R}^{N_{m}\times D}$ with $N_{m}$ the number of samples stored at client $Q_{m}$. Altogether, we have $\mathbf{X} = \mathbf{X}_{1} \oplus \mathbf{X}_{2} \oplus \dots \oplus \mathbf{X}_{M}$ (``$\oplus$" denotes row-wise concatenation), and $\mathbf{X} \in \mathbb{R}^{N\times D}$ with with $N=\sum_{m=1}^{M}N_{m}$. The goal of federated learning based tabular synthesis is to generate a synthetic dataset $\tilde{\mathbf{X}}=\tilde{\mathbf{X}}_{1}\oplus\tilde{\mathbf{X}}_{2}\oplus...\oplus\tilde{\mathbf{X}}_{M}$ that are distributionally similar to the original dataset $\mathbf{X}$ while maintaining the private information of the actual values.  
\end{problem}


Meanwhile, \textit{fairness-preserving data synthesis} aims to use diffusion models to generate synthetic tabular data that maintains fairness w.r.t. sensitive features (e.g., age, gender, race, etc.) while preserving data utility. Formally, it is defined as:

\begin{problem} [Fairness-Preserving Data Synthesis]
Given a $D$-dimensional dataset $\mathbf{X} = \{\prescript{}{i}{\mathbf{x}}\}_{i=1}^{N}$, where each data point $\prescript{}{i}{\mathbf{x}} \in \mathbb{R}^{D}$ represents a record with $D$ features, let $\mathbf{S} \subseteq \{1, \ldots, D\}$ denote the set of indices corresponding to sensitive features. The goal is to learn a generative function $g(\cdot)$ to generate synthetic data $\hat{\mathbf{X}} = g(\mathbf{X})$ such that: 1) The synthetic data $\hat{\mathbf{X}} = \{\hat{\prescript{}{i}{\mathbf{x}}}\}_{i=1}^{N}$ preserves fairness with respect to sensitive features in $\mathbf{S}$, ensuring no unfair bias is introduced; and 2) The synthetic data $\hat{\mathbf{X}}$ maintains the statistical properties and utility of the original dataset $\mathbf{X}$ for downstream tasks.
\end{problem}

\begin{table}[htbp]
\centering
\caption{Overview of Diffusion Models for Trustworthy Data Synthesis.}
\label{SummaryPrivacyDataSynthesis}
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Model Name} & \textbf{Year} & \textbf{Venue} & \textbf{Feature Type} & \textbf{Domain} \\ 
\midrule
\textbf{SiloFuse} \cite{shankar2024silofuse}  & 2024         & ICDE & Num+Cat  & Generic \\
\textbf{FedTabDiff} \cite{sattarov2024fedtabdiff}  & 2024         & ArXiv   & Num+Cat  & Generic   \\
\textbf{FairTabDDPM}* \cite{yang2024balanced}            & 2024         & ArXiv   & Num+Cat  & Generic    \\
\textbf{DP-Fed-FinDiff} \cite{sattarov2024differentially}            & 2024         & ArXiv   & Num+Cat  & Finance    \\
\bottomrule
\end{tabular}%
\label{tab:tabular_data_synthesis}
\end{table}

\textbf{SiloFuse} \cite{shankar2024silofuse} is designed to generate tabular data where the features are distributed across multiple silos (known as \textit{feature-partitioned} or \textit{vertically-partitioned} dataset) rather than being centrally stored. To ensure privacy, they employ a distributed latent diffusion model: 1) they utilize autoencoders to convert actual feature values (including sensitive features) into embeddings. In this way, they can also handle categorical features by unifying numerical and categorical features into a shared continuous latent space. As a result, they can capture feature correlations across silos by centralizing the embeddings; 2) they employ a Gaussian diffusion model  to learn to create synthetic embeddings; 3) they leverage stacked distributed training to reduce communication: the autoencoders are trained locally at the client silos, while the latent diffusion model is trained at the central server.

They evaluate the quality of generated data in terms of fidelity, machine learning utility, and privacy. They observe that latent DDPM outperforms TabDDPM on datasets that have many spare features, while TabDDPM performs better on datasets with a small number of features. In other words, combining Gaussian diffusion and multinomial diffusion may work better for datasets with low features size and sparsity, while operating in the latent space could benefit highly sparse datasets with large cardinality in discrete values. 


\textbf{FedTabDiff} \cite{sattarov2024fedtabdiff} addresses the challenge of generating synthetic tabular data with mixed-type features while preserving privacy. They propose a federated learning framework that integrates DDPM with Federated Learning to enhance privacy. Each client $Q_{m}$ locally maintains a decentralized FinDiff model $f_{m}(\cdot)$ \cite{sattarov2023findiff} and collaborates in training a central model $f(\cdot)$ hosted by a trusted entity. A synchronous update scheme is employed for federated optimization over $R$ rounds. In each round, a subset of clients receives the current central model parameters, performs local optimization, and sends updated parameters back for aggregation. For this aggregation, they apply the Federated Averaging (FedAvg) technique \cite{mcmahan2017federated}, computing a weighted average of the model updates. For data pre-processing, they apply quantile transformation to numerical features and use the same embedding method as in FinDiff for categorical features. The framework’s performance is evaluated in terms of fidelity, machine learning utility, diversity, and privacy.

\textbf{DP-Fed-FinDiff} \cite{sattarov2024differentially} integrates differential privacy, federated learning, and DDPM to generate high-fidelity synthetic tabular data while preserving privacy. Similar to FedTabDiff \cite{sattarov2024fedtabdiff}, it assumes sensitive data is distributed across multiple clients and cannot be shared due to privacy regulations. However, it argues that federated learning alone (as in FedTabDiff) is insufficient for ensuring data privacy. To address this, differential privacy is introduced, providing a strict mathematical guarantee that the inclusion of a single sample does not significantly affect analysis outcomes. This allows for precise control over the privacy budget, ensuring individual data confidentiality.

Their approach builds on FinDiff \cite{sattarov2023findiff} as the generative model for mixed-type tabular data. Like FedTabDiff, FinDiff is trained with federated learning, but its parameter update process is modified to incorporate differential privacy via the Gaussian Mechanism \cite{dwork2014algorithmic}. Unlike FinDiff, they apply Quantile Transformation to numerical features while using the same embedding techniques for categorical features. Evaluations on privacy, utility, and fidelity reveal a tradeoff between privacy protection and data utility/fidelity.

\textbf{FairTabDDPM} 
\cite{yang2024balanced} indicates that prior tabular diffusion models may generate biased synthetic data by inheriting bias inherent in the training data. To mitigate bias and maintain synthetic data quality, they propose to build balanced joint distributions of sensitive columns (e.g., gender, race) and target label column. Specifically, FairTabDDPM works as follows: 1) the input data $\mathbf{x}_{0}=(\mathbf{x}_{0}^{\text{num}},\mathbf{x}_{0}^{\text{cat}})$ passes through $T$ steps of diffusion and leads to $\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{T}$. Like TabDDPM, a Gaussian diffusion model is used for numerical features and multinomial diffusion models are used for categorical features separately; 2) the intermediate noisy samples $\mathbf{x}_{1},\mathbf{x}_{2},\cdots,\mathbf{x}_{T}$ are embedded into $\mathbf{z}_{1},\mathbf{z}_{2},\cdots,\mathbf{z}_{T}$ with a MLP encoder; 3) the reverse process denoises $\mathbf{z}_{T}$ sequentially into $\hat{\mathbf{z}}_{T-1},\cdots,\hat{\mathbf{z}}_{1}$ by conditioning on the embeddings (with a MLP) of label column $\mathbf{y}$ and sensitive columns $\mathbf{s}_{1},\cdots,\mathbf{s}_{P}$; Note that while prior tabular diffusion models are mainly unconditional or conditioned solely on the label columns, FairTabDDPM models the data distribution of mixed features conditioned on both label column and multiple sensitive columns ( by extending Eq.~\ref{Equ:ClassifierFreeGuidanceDDPM});
and 4) the denoised latent representation $\hat{\mathbf{z}}_{1}$ is decoded into the reconstructed data $\hat{\mathbf{x}}_{0}$ using an MLP decoder.


For data preprocessing, they apply quantile transformation to normalize numerical features and one-hot encoding for categorical features. The model's performance is evaluated in terms of fidelity, diversity, privacy, machine learning utility, and fairness metrics, including class imbalance within sensitive attributes (demographic parity ratio) and joint distributions with the target label (equalized odds ratio).

\section{Diffusion Models for Anomaly Detection}
\label{sec:AnoDec}

Anomaly detection aims to train diffusion models to learn the ``normal'' distribution of data from the training set and identify anomalies as deviations from this learned distribution in the test data. Formally, it is defined as follows:

\begin{problem} [Tabular Anomaly Detection]
Given a $D$-dimensional training dataset $\mathbf{X}_{\text{train}} = \{\prescript{}{i}{\mathbf{x}}\}_{i=1}^{N_{\text{train}}}$, where each data point $\prescript{}{i}{\mathbf{x}} \in \mathbb{R}^{D}$ represents a record with $D$ features, the objective is to learn a function $f(\cdot): \mathbb{R}^{D} \to \{0, 1\}$ that distinguishes between normal and anomalous data points. During testing, given a $D$-dimensional test dataset $\mathbf{X}_{\text{test}} = \{\prescript{}{j}{\mathbf{x}}\}_{j=1}^{N_{\text{test}}}$, the function $f(\cdot)$ predicts $f(\prescript{}{j}{\mathbf{x}}) = 1$ if $\prescript{}{j}{\mathbf{x}}$ is an anomaly and $f(\prescript{}{j}{\mathbf{x}}) = 0$ if it is normal. Anomalies are identified as data points in $\mathbf{X}_{\text{test}}$ that significantly deviate from the normal patterns learned from $\mathbf{X}_{\text{train}}$.
\end{problem}


\begin{table}[htbp]
\centering
\caption{Overview of Diffusion Models for Anomaly Detection}
\resizebox{\columnwidth}{!}{%
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Model Name} & \textbf{Year} & \textbf{Venue} & \textbf{Feature Type} & \textbf{Domain}\\ 
\midrule
\textbf{TabADM} \cite{zamberg2023tabadm}  & 2023         & ArXiv & Num by default & Generic\\
\textbf{DTE} \cite{livernoche2024diffusion}  & 2024         & ICLR & Num by default & Generic\\
\textbf{SDAD} \cite{li2024self}  & 2024         & Inf. Sci. & Num by default & Generic\\
\textbf{NSCBAD} \cite{anonymous2024anomaly}  & 2024         & OpenReview & Num by default & Generic\\
\textbf{FraudDiffuse} 
\cite{roy2024frauddiffuse}  & 2024         & ICAIF & Num+Cat & Finance\\
\textbf{FraudDDPM} \cite{pushkarenko2024synthetic}  & 2024         & ISIJ & Num+Cat & Finance\\
\bottomrule
\end{tabular}%
}
\label{tab:tabular_data_synthesis}
\end{table}

\textbf{TabADM} \cite{zamberg2023tabadm} is the first diffusion-based approach for unsupervised anomaly detection in tabular data. Unlike semi-supervised methods that require pure normal training data, TabADM handles fully unlabeled training data mixed with anomalies. It estimates the data distribution via a robust diffusion model, assigning anomaly scores to test samples based on their likelihood of being generated by the model. Specifically, TabADM comprises two stages: \textit{training} and \textit{inference}. During training, it follows the standard DDPM process with an additional \textit{sample rejection} procedure to mitigate the influence of anomalies. Specifically, in each batch, it sorts samples by loss in descending order and removes the top $k$ highest-loss samples before gradient descent. During inference, it computes an anomaly score for a test sample $\mathbf{x}$ by evaluating its reconstruction loss: for each timestep $t \in \{1,\dots,T\}$, it generates noise $\boldsymbol{\epsilon}_{t}$, predicts noise $\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)$, and computes the loss as $l_{t}(\mathbf{x}) := \Vert\boldsymbol{\epsilon}_{t} -\boldsymbol{\epsilon}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)\Vert^{2}_{2}$. The final anomaly score is obtained by summing loss values across all timesteps, i.e., $\sum_{t=1}^{T}l_{t}(\mathbf{x})$. Consequently, samples in low-density regions of the learned distribution exhibit high loss values and are classified as anomalies.

Empirical results on 32 datasets, evaluated using AUCROC and AUCPR, demonstrate its superior performance on high-dimensional datasets. However, TabADM has several limitations: 1) it lacks explainability, 2) it incurs high training and inference costs compared to baselines, and 3) it cannot handle missing values.

Livernoche et al. \cite{livernoche2024diffusion} observe that DDPM achieves competitive anomaly detection performance in both unsupervised and semi-supervised settings for tabular data. However, its high computational cost makes it impractical for large datasets or data streams. Since anomaly detection only requires measuring deviations between input and reconstructed samples, they argue that modeling score functions in the reverse process is unnecessary, enabling simplification of diffusion models for this task. A key observation is that the choice of the starting timestep for the reverse diffusion process significantly impacts performance, yet it is often arbitrary in existing methods. They further note that the diffusion (starting) time given a noisy sample follows an inverse Gamma distribution. Based on this, they introduce \textbf{DTE}, which estimates the posterior distribution over diffusion time for noisy input samples. The rationale is that anomalous samples, being distant from the normal data manifold, tend to be assigned higher estimated timesteps, corresponding to more noise and thus indicating higher anomaly likelihood. For each sample, DTE estimates the diffusion time distribution and uses its mode/mean as the anomaly score.

DTE derives an analytical form for the posterior over diffusion time and employs two estimation methods: 1) a non-parametric KNN-based estimator and 2) a parametric neural network-based estimator for improved generalizability and scalability. They explore the interpretability of diffusion-based anomaly detection using deterministic ODE flow for input reconstruction. Notably, they find that leveraging pre-trained embeddings for images significantly enhances performance, highlighting the potential of latent diffusion models. For preprocessing, they standardize data using z-score normalization. They evaluate DTE using ROCAUC and ROCPR but note that it natively supports only numerical features.


While generative models are powerful at modeling complex datasets to capture their distributional patterns,
\textbf{SDAD} \cite{li2024self} point out that existing generative model-based anomaly detection methods suffer from three limitations: 1) they usually prioritize the generative process to generate new synthetic samples resembling the training data, while overlooking the importance of obtaining discriminative representations; 2) VAEs generate low-quality samples and GANs are prone to mode collapse, leading to coarse data reconstructions and hindering the precise identification of anomalies; 3) generative models usually have intricate architecture, requiring extensive training data and limiting their performance on small datasets.

To address these issues, SDAD incorporates self-supervised learning to enhance diffusion-based anomaly detection as follows: 1) it employs an auxiliary module with two pretext tasks to train an encoder, improving the separation between normal and anomalous samples in latent space; 2) using this trained encoder, raw input samples are mapped to the latent space, where a denoising diffusion process models the distribution of normal data; and 3) in inference, test samples with high reconstruction errors in latent space (rather than in original space) are identified as anomalies. SDAD is designed for the semi-supervised setting, where training is performed exclusively on normal samples. It is evaluated using AUCROC and AUCPR. However, its negative sampling strategy in SSL relies on standard Gaussian noise to generate pseudo anomalies, which may not be as representative as real anomalies.

Instead of relying on density estimation $p(\mathbf{x})$ for anomaly detection, \textbf{NSCBAD} \cite{anonymous2024anomaly} leverages the score function $\nabla_{\mathbf{x}}\log p(\mathbf{x})$ to detect anomalies as follows:
1) \textit{Training stage:} They train a Noising Conditional Score Network (NCSN) \cite{song2019generative}, denoted as $S_{\boldsymbol{\theta}}(\cdot)$, to learn the score function characterizing the data distribution. Specifically, they optimize $S_{\boldsymbol{\theta}}(\cdot)$ using a simplified loss function where the score network directly predicts the ground-truth noise $\boldsymbol{\epsilon}$. Mathematically, they demonstrate that the network learns $\sigma_{t}\nabla_{\mathbf{x}}\log p(\mathbf{x}_{t} \vert \mathbf{x}_{0})$; 2) \textit{Inference stage:} Anomaly scores are computed based on the likelihood of a test sample $\mathbf{x}$ within the learned score. Concretely, they measure the distance between the predicted score $S_{\boldsymbol{\theta}^{*}}(\mathbf{x}_{t},t)$ and the ground-truth noise $\boldsymbol{\epsilon}(t)$ at a fixed time $t$, averaging over 70 noise realizations. The anomaly score is given by:
$
\frac{1}{70}\sum_{i=1}^{70}\Vert S_{\boldsymbol{\theta}^{*}}(\mathbf{x}_{t},t) - \boldsymbol{\epsilon}_{i}(t) \Vert^{2}, \quad \boldsymbol{\epsilon}_{i}(t) \sim \mathcal{N}(\mathbf{0,I}), \quad i \in \{1,...,70\}.
$

NSCBAD is evaluated using AUC-ROC, AUC-PR, and F1-score. They highlight that DDPM-based anomaly detection methods rely on a stepwise reverse denoising process (i.e., a sequential Markovian chain for inference), leading to significantly longer inference times compared to NSCBAD.

\textbf{FraudDiffuse} \cite{roy2024frauddiffuse} extends diffusion models to generate synthetic fraudulent transactions, addressing class imbalance and evolving fraud patterns. 

First, they employ a learnable embedding layer (label encoding) to map categorical features into continuous embeddings, which are then concatenated with numerical features in a shared continuous space. Second, they apply a DDPM model in the latent space with the following modifications:
1) Instead of using a Gaussian prior for noise addition, they leverage the distribution of normal transactions to better capture the behaviors of fraudulent transactions at the decision boundary. Specifically, during the forward diffusion process, noise is sampled from $\mathcal{N}(\boldsymbol{\mu}_{\text{nt}}, \boldsymbol{\Sigma}_{\text{nt}})$, where $\boldsymbol{\mu}_{\text{nt}}$ and $\boldsymbol{\Sigma}_{\text{nt}}$ are the mean and covariance estimated from normal transactions. Similarly, in the reverse process, $\mathbf{x}_{T}$ is initialized from $\mathcal{N}(\boldsymbol{\mu}_{\text{nt}}, \boldsymbol{\Sigma}_{\text{nt}})$ rather than from a standard Gaussian distribution.
2) In addition to the typical mean squared error (MSE) loss between predicted and ground-truth noise, they introduce a probability-based loss that quantifies the likelihood of predicted noise originating from $\mathcal{N}(\boldsymbol{\mu}_{\text{nt}}, \boldsymbol{\Sigma}_{\text{nt}})$. The final loss function combines both loss terms.
Third, they incorporate a contrastive loss to enhance the similarity between real and synthetic fraudulent samples, further refining the learned representations.

FraudDiffuse operates under a supervised setting, where both labeled normal and fraudulent transactions are available. Performance is evaluated based on machine learning utility.

Similar to FraudDiffuse \cite{roy2024frauddiffuse}, \textbf{FraudDDPM} \cite{pushkarenko2024synthetic} leverages diffusion models to generate synthetic transaction data, enhancing fraud detection performance. However, unlike FraudDiffuse, they adopt a two-model approach: one diffusion model is trained on normal transaction data to generate synthetic normal transactions, while another is trained on abnormal transactions to synthesize fraudulent data.

They employ Gaussian diffusion models for both numerical and categorical features, where categorical data is transformed into a continuous space using one-hot encoding or embeddings vectors. The generated synthetic normal and fraudulent transactions are then merged to form a synthetic supervised dataset. Finally, this synthetic dataset is combined with real-world transaction data to create a balanced training set, effectively serving as an oversampling technique. This balanced dataset is then used to train fraud detection systems utilizing classical supervised learning algorithms such as Random Forest. They evaluate the quality in terms of machine learning utility.

\section{A review on discrete diffusion models and performance evaluation metrics}
\label{sec:other_related_work}

\subsection{Diffusion Models for Discrete Data.} 
\label{subsec:DiffModelInDiscreteAll}
Diffusion models for discrete data can be divided into two categories \cite{sahoo2024simple}: 1) the first category of works converts discrete structures into a continuous latent space and then directly applies Gaussian diffusion model in the latent space. This line of works include \cite{chenanalog, dieleman2022continuous, gulrajani2024likelihood,han2023ssd,li2022diffusion,lovelace2024latent,strudel2022self,regol2023diffusing}; 2) the second category of works directly defines the diffusion process on discrete structures, and we will focus on this category in the remaining of this section. 
\subsubsection{Binary Diffusion Model} This line of work is for the first time studied in \cite{sohl2015deep}, which explored the scenario of binary random features.



\subsubsection{Multinomial Diffusion Model} Multinomial Diffusion \cite{hoogeboom2021argmax, song2021denoising} that provides transition matrices with uniform distribution; The diffusion and denoising processes have been defined in Section~\ref{sec:Background_Math_DDPM}.
\subsubsection{D3PM Model} D3PM \cite{austin2021structured} generalizes Multinomial Diffusion by providing transition matrices with uniform distribution, transition matrices  that mimic Gaussian kernels in continuous spaces, matrices using nearest neighbors in embedding space, and matrices with absorbing states. Specifically, the Markov forward diffusion process is defined as:
\begin{equation}
    q(\mathbf{x}_{t}\vert\mathbf{x}_{t-1})=\text{Cat}(\mathbf{x}_{t};\bar{Q}_{t}\mathbf{x}_{0})=\text{Cat}(\mathbf{x}_{t};Q_{t}\cdot Q_{t-1}\cdots Q_{1}\mathbf{x}_{0}),
\end{equation}
where $Q_{t}$ can be defined in a general and flexible form to reflect different types of transition matrices.


\subsubsection{Masking (Absorbing) Diffusion Model} \label{subsec:DiffModelInDiscreteMasking} Let $\mathcal{V}=\{\mathbf{x}\in \{0,1\}^{K}:\sum_{i=1}^{K}\mathbf{x}_{i}=1\}$ be the one-hot encoding vectors of all scalar discrete random variables $x$ with $K$ categories (Note that it is reasonable to define $\mathcal{V}$ in this way since in text modeling the words share the same dictionary space. However, in tabular data, different categorical features usually have different number of categories and thus there can be multiple $\mathcal{V}$s). Moreover, let $\text{Cat}(\cdot;\pi)$ the categorical distribution over $K$ categories, where $\pi \in \Delta^{K}$ defines the probabilities and $\Delta^{K}$ is the $K$-simplex. In addition, we assume that the extra $K$-th category/position is a special [MASK] token, and $\mathbf{m} = (0,0,\cdots,1)$ be the one-hot encoding vector for this mask (i.e., $\mathbf{m}_{K}=1$ while $\mathbf{m}_{k}=\mathbf{x}_{k}$ for $k\in \{0,1,\cdots,K-1\}$). For masking (absorbing) diffusion models \cite{sahoo2024simple,shi2024simplified, shi2024tabdiff}, the diffusion process interpolates between the original categorical distribution $\text{Cat}(\cdot;\mathbf{x}_{0})$ (with $\mathbf{x}_{0} \in \mathcal{V}$) and the target distribution $\text{Cat}(\cdot;\mathbf{m})$, namely the input $\mathbf{x}_{0}$ can transition to a masked state with some probability as follows:
\begin{equation}
\label{DiscreteMaskingForward}
    q(\mathbf{x}_{t}\vert \mathbf{x}_{0}) = \text{Cat}(\mathbf{x}_{t};\alpha_{t}\mathbf{x}_{0}+(1-\alpha_{t})\mathbf{m}),
\end{equation}
where $\alpha_{t} \in [0,1]$ is strictly decreasing w.r.t. time $t$, with $\alpha_{0} \approx 1$ while $\alpha_{1} \approx 0$. Importantly, an input will remain in a masked state once it transitions to this state, and all inputs will be masked with probability 1 at time step $T$. The reverse denoising process can be defined as:
\begin{equation}
\label{DiscreteMaskingReverse}
    p_{\boldsymbol{\theta}}(\mathbf{x}_{s} \vert \mathbf{x}_{t}) = 
    \begin{cases} 
        \text{Cat}(\mathbf{x}_{s}; \mathbf{x}_{t}), & \text{if } \mathbf{x}_{t} \neq \mathbf{m}, \\
        \text{Cat}(\mathbf{x}_{s}; \frac{(1-\alpha_{s})\mathbf{m}+(\alpha_{s}-\alpha_{t})\mathbf{x}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)}{1-\alpha_{t}}), & \text{if } \mathbf{x}_{t} = \mathbf{m},
    \end{cases}
\end{equation}
where  $0<s<t<1$, and $\mathbf{x}_{\boldsymbol{\boldsymbol{\theta}}}(\mathbf{x}_{t},t):\mathcal{V}\times[0,1]\rightarrow\Delta^{K}$ is a neural network that approximates the unknown $\mathbf{x}_{0}$. For the denoising model $\mathbf{x}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t)$, the clean input is never input while an masked token is always unchanged during the reverse process. The corresponding loss function is:
\begin{equation}
    \mathcal{L}_{\text{mask}} = \mathbb{E}_{q}\int_{t=0}^{t=1}\frac{\alpha_{t}^{\prime}}{1-\alpha_{t}}\log [\langle\mathbf{x}_{\boldsymbol{\theta}}(\mathbf{x}_{t},t),\mathbf{x}_{0}\rangle]dt,
\end{equation}
where $\langle\cdot\rangle$ means inner product and $\alpha_{t}^{\prime}$ is the first order derivative of $\alpha_{t}$. Note that masking diffusion model is a strict subset of D3PM by setting $Q_{t \vert s}=\alpha_{t\vert s}\mathbf{I}+(1-\alpha_{t\vert s})\mathbf{1}\mathbf{m}^{T}$ for the forward diffusion process, with improvements including parameterization that enables simplified objective and proposing well-engineered training recipes. 

Other recent advancements in this field include 1) auto-regressive diffusion models \cite{hoogeboom2021autoregressive,ye2023diffusion}, 2) editing-based operations for discrete diffusion \cite{jolicoeur2021gotta,reid2022diffuser}, 3) discrete diffusion model in continuous time structure \cite{campbell2022continuous, sun2023score}, and 4) generation acceleration in discrete diffusion model \cite{chen2024fast}.

\subsection{Performance Evaluation Metrics}
The performance of tabular data generative models is often evaluated from various but complementary perspectives, including \textit{fidelity}, \textit{diversity}, \textit{utility}, \textit{privacy}, and \textit{runtime}.


\subsubsection{Fidelity Metrics} they  aim to quantify the similarity between original and synthetic data by assessing whether key properties of the original data are preserved in the synthetic data. This line of evaluation can be further divided into \textit{column-wise density estimation}, which assesses the similarity between a generated column and its original counterpart, and \textit{pairwise column correlation estimation}, which evaluates how well the correlations between columns in the generated data match those in the original data. Moreover, other popular fidelity metrics include \textit{detection metrics}, $\alpha$-Precision, MAE and RMSE (specifically for data imputation task).

\textbf{Column-wise density estimation.} This includes 
\begin{itemize}
    \item Kolmogorov-Sirmov Test (KST) for numerical features. This test evaluates to which extent two one-dimensional probability distributions differ, defined as follows:
    \begin{equation}
        \text{KST} = \sup_{\mathbf{x}}\vert F_{1}(\mathbf{x})-F_{2}(\mathbf{x}) \vert,
    \end{equation}
    where $F_{i}(\mathbf{x})$ is the empirical distribution of $i$-th probability distribution at point $\mathbf{x}$.
    \item Total Variation Distance (TVD) for categorical features. This statistics captures the largest possible difference in the probability of any event under two different (one-dimensional) probability distributions, defined as follows:
    \begin{equation}
        \text{TVD}=\frac{1}{2}\sum_{\mathbf{x}\in \mathbf{X}}\vert p_{1}(\mathbf{x})-p_{2}(\mathbf{x})\vert,
    \end{equation}
    where $p_{1}(\mathbf{x})$ and $p_{2}(\mathbf{x})$ are the probability assigned to $\mathbf{x}$ by these two  probability distributions.
\end{itemize}

\textbf{Pairwise columns correlations estimation.}  This includes 
\begin{itemize}
    \item The difference between Pearson Correlation Coefficient Matrices among numerical features (DPCM) on real data and original data. The Pearson Correlation Coefficient aims to measure the linear correlation between two numerical features $\mathbf{x}$ and $\mathbf{y}$ as follows:
    \begin{equation}
\rho(\mathbf{x},\mathbf{y})=\frac{cov(\mathbf{x},\mathbf{y})}{std(\mathbf{x}),std(\mathbf{y})},
    \end{equation}
    where $cov(\cdot)$ is the covariance and $std(\cdot)$ stands for the standard deviation.
    \item  The difference between Contingency Similarity Matrices among categorical features (DCSM) on real data and original data. The contingency table summarizes the relationship between two categorical features by organizing the data into rows and columns. Each cell in the table represents the frequency of occurrences for a specific combination of the two features.
    \item  The difference between Contingency Similarity Matrices on real data and original data (among categorical and numerical features), where one needs to first group numerical features into discrete intervals via bucketing.
\end{itemize}

\textbf{Detection Metric.} The metric evaluates the difficulty of detecting the synthetic data from the real data by using the classifier-two-sample-test, which often employs a machine learning model to label whether a sample is synthetic or real.

\textbf{$\alpha$-Precision.} It measures the fidelity of synthetic data by indicating whether each synthetic sample resembles the real data \cite{alaa2022faithful}.

\textbf{RMSE and MAE.} These metrics evaluate the performance of data imputation by computing the divergence between the ground-truth missing values and the imputed missing values. Particularly, 
\begin{itemize}
    \item The Mean Absolute Error (MAE) is defined as:
        \begin{equation}
        \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} \left| y_i - \hat{y}_i \right|,
        \end{equation}
        where \( n \) is the total number of data points, \( y_i \) is the actual value, and \( \hat{y}_i \) is the imputed value;
    \item The Root Mean Squared Error (RMSE) is defined as:
    \begin{equation}
    \text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} \left( y_i - \hat{y}_i \right)^2}.
    \end{equation}
\end{itemize}


\subsubsection{Diversity Metrics}  they aim to evaluate the extent to which the synthetic data maintain the entire distribution of the real data. The two most popular metrics are \textit{coverage score} and \textit{$\beta$-Recall}.

\textbf{Coverage Score.} Specifically, for a categorical column, it calculates the number of unique categories from the real data column that are present in the corresponding synthetic column. For a numerical column, this metric evaluates how closely the range (namely the minimum and maximum) of synthetic column aligns with that of real column.

\textbf{$\beta$-Recall.} It evaluates whether the generated samples can cover the entire distribution of real data \cite{alaa2022faithful}.

\subsubsection{Utility Metrics} they aim to quantify the impact on downstream tasks when synthetic data is used to replace original data during the training phase. This metric is usually known as \textit{machine learning utility} and it is defined as follows.

\textbf{Machine Learning Utility.} The machine learning utility is often evaluated using the Training on Synthetic and Testing on Real (TSTR) scheme \cite{choi2017generating}. In this approach, the data is split into training and test sets. A generative model is then used to generate synthetic data of the same size as the training set. Two models are trained: one on the original training data and another on the synthetic data. Both models are subsequently tested on the test set, and their performances are compared. We conclude that the synthetic data has a good machine learning utility if their performance are comparable.

\subsubsection{Privacy Metrics} These metrics aim to quantify the extent to which the identification of original samples is possible. They mainly include Distance to Closet Records (DCR), Attribute Inference Attack \cite{jia2018attriguard}, and Membership Inference Attack \cite{shokri2017membership}.

\textbf{Distance to Closet Records (DCR).} It measures the extent to which the synthetic samples resemble the original data samples. Given a generated sample $\hat{\mathbf{x}}$, its $DCR$ is computed as $DCR(\hat{\mathbf{x}})=\underset{\mathbf{x}\in \mathbf{X}}{\min}~d(\hat{\mathbf{x}},\mathbf{x})$ with $d(\cdot)$ a distance metric. As a result, the DCR of a synthetic dataset is computed as the $DCR$ median of all synthetic samples. Low DCR values indicate that the synthetic sample may violate the privacy requirement. However, it is important to note that random noises can produce higher DCR values (indicating better performances). Therefore, the DCR metric needs to be considered jointly with other evaluation metrics such as machine learning utilities.


\section{Discussion and Conclusions}
\label{sec:conclusions}
Thanks to notable advancements in diffusion models for tabular data, some of the challenges associated with generative models for tabular data, as discussed in Section~\ref{Sec:ChallengeswithTabularData}, have been partially addressed. However, many challenges remain unresolved, and new ones continue to emerge. Below, we outline these challenges and propose potential future research directions:
\begin{itemize}
    \item \textbf{Scalability}: Diffusion models are typically computationally intensive, which can be challenging with high-dimensional tabular data. Therefore, more research efforts should be given to  develop techniques for reducing the computational cost of diffusion models on large tabular datasets, such as efficient sampling and training methods.
    \item \textbf{Evaluation Metrics}: Unlike images where visual quality is a metric, evaluating tabular data generation quality is complex and may require domain-specific measures especially in domains such as healthcare and finance.
    \item \textbf{Privacy Concerns}: While current diffusion models for tabular data synthesis provide basic privacy preservation, research into techniques with strong theoretical guarantees, such as differential privacy, remains limited and requires further exploration.
    \item \textbf{Enhanced Interpretability}: Developing methods to enhance the interpretability of diffusion models is crucial for their practical application in tabular data modeling .
    \item \textbf{Cross-Modality Integration}: Future research into how diffusion models for tabular data might be combined with those for image, text, or time-series data, enabling cross-modal applications.
    \item \textbf{Benchmarking and Standardization}: Standardized benchmarks and evaluation metrics tailored for diffusion models on tabular data are needed.
    \item \textbf{Hybrid Models}:  Diffusion models can be combined with other model architectures (e.g., GANs or VAEs) to better handle tabular data. For instance, AutoDiff \cite{suh2023autodiff} combines Autoencoder with Diffusion Models.
    \item \textbf{Modeling Feature Correlations}: Most existing diffusion models are not optimized to capture feature correlations, especially those among categorical features and those among numerical and categorical features. Ideally, a  tabular diffusion model should be able to model numerical and categorical simultaneously.
\end{itemize}

\begin{table*}[htbp]
\centering
\caption{Overview of Diffusion Models for Tabular Data. The column ``Num" indicates whether it can handle numerical features (`MM' = min-max scaler, 'QT'= quantile transformer, `ZS' = $Z$-score normalization), while ``Cat" indicates whether it can handle categorical features (`OH' = one-hot encoding, `AB'= analog bits encoding, `DC' = dictionary encoding,`DE' = dummy encoding, `IE' = Integer Encoding, `OE' = Ordinal Encoding, `PSK' = PSK encoding, 'FT' = feature tokenization or learned embedding). ``\#Datasets" represents the number of datasets benchmarked. ``?/57 A" indicates the number of datasets used from ADBench~\cite{han2022ADBench}. ``Complete" indicates whether the model requires complete data (i.e., without missing values) to train.}
\label{SummaryPrivacyDataSynthesis}
\resizebox{\linewidth}{!}{
\begin{tabular}{@{}lllllllrllll@{}}
\toprule
\textbf{Model Name} & \textbf{Year} & \textbf{Venue} & \textbf{Num} & \textbf{Cat}  & \textbf{Tasks} & \textbf{Backbone} & \#\textbf{Datasets} & \textbf{Metrics} & \textbf{Code} & \textbf{Complete} & \textbf{Domain}\\ 
\midrule
\textbf{SOS} \cite{kim2022sos} & 2022         & KDD & \checkmark & \ding{55} & Synthesis & SDEs & 6 & Utility & \checkmark & \checkmark & Generic\\
\textbf{STaSy} \cite{kimstasy}              & 2023         & ICLR           & \checkmark (MM) & \checkmark (OH) & Synthesis & SDEs & 15 & Fidelity, Utility, Diversity & \checkmark & \checkmark & Generic\\
\textbf{TabDDPM} \cite{kotelnikov2023tabddpm} & 2023         & ICML & \checkmark (QT) & \checkmark (OH) & Synthesis & DDPM+MLD & 16 & Fidelity, Utility, Privacy & \checkmark & \checkmark & Generic\\
 \textbf{CoDi} \cite{lee2023codi}               & 2023         & ICML & \checkmark (MM) & \checkmark (OH) & Synthesis & DDPM+MLD & 15 & Utility, Diversity & \checkmark & \checkmark & Generic\\ 
 \textbf{AutoDiff} \cite{suh2023autodiff} & 2023         & NeurIPSW & \checkmark  & \checkmark  & Synthesis & Any & 15 & Fidelity, Utility, Privacy & \checkmark & \checkmark & Generic\\ 
 \textbf{MissDiff} \cite{ouyang2023missdiff}               & 2023         & ICMLW  & \checkmark (MM) & \checkmark (OH) & Synthesis & SDEs & 3 & Fidelity, Utility & \ding{55} & \ding{55} & Generic\\

 \textbf{TabSyn} \cite{zhangmixed}              & 2024         & ICLR    & \checkmark & \checkmark (OH) & Synthesis, Imputation & SDEs & 6 & Fidelity, Utility, Diversity, Privacy & \checkmark & \checkmark & Generic\\
 \textbf{Forest-Diffusion} \cite{jolicoeur2024generating} & 2024 & AISTATS  & \checkmark & \checkmark (DE) & Synthesis, Imputation & SDEs & 27 & Fidelity, Diversity, Utility & \checkmark & \ding{55} & Generic\\
 \textbf{TabDiff} \cite{shi2024tabdiff}            & 2024         & NeurIPSW & \checkmark (MM) & \checkmark (OH) & Synthesis, Imputation & SDEs+MSD & 7 & Fidelity, Diversity, Utility & \checkmark &  \checkmark & Generic\\
 \textbf{TabUnite} \cite{si2024tabunite}            & 2024         & OpenReview   &  \checkmark (QT) & \checkmark (AB, PSK, DC) & Synthesis & SDEs+MSD & 10 & Fidelity, Diversity, Utility & \checkmark &  \checkmark & Generic\\
\textbf{CDTD} \cite{mueller2024continuous}& 2024         & OpenReview   &  \checkmark & \checkmark (FT) & Synthesis & Latent SDEs & 11 & Fidelity, Utility, Privacy & \ding{55} &  \checkmark & Generic\\
 \textbf{MedDiff} \cite{he2023meddiff}  & 2023 & ArXiv & \checkmark  & \ding{55} & Synthesis & DDIM & 2 & Fidelity, Utility & \ding{55} &  \checkmark & Healthcare\\
\textbf{EHR-TabDDPM} \cite{ceritli2023synthesizing}  & 2023 & ArXiv &\checkmark (QT)  & \checkmark (OH) & Synthesis & DDPM & 4 & Fidelity, Utility, Privacy & \ding{55} &  \checkmark & Healthcare\\
 \textbf{DPM-EHR}* \cite{nicholas2023synthetic}           & 2023         & NeurIPSW  &\checkmark  & \checkmark (OH) & Synthesis & DDPM & 2 & Fidelity, Diversity, Utility, Privacy & \ding{55} &  \checkmark & Healthcare\\
\textbf{FlexGen-EHR} \cite{he2024flexible} & 2024         & ICLR           &\checkmark + TS (FT)  & \checkmark (FT) & Synthesis & DDPM & 2 & Fidelity, Utility, Privacy & \ding{55} &  \ding{55} & Healthcare\\
\textbf{EHRDiff} \cite{yuan2024ehrdiff} & 2024         & TMLR          &\checkmark (MM)   & \checkmark (OH)  & Synthesis & SDEs & 3 & Fidelity, Utility, Privacy & \checkmark &  \checkmark & Healthcare \\
\textbf{EHR-D3PM} \cite{han2024guided}            & 2024         & ArXiv   &\ding{55}  & \checkmark (OH)   & Synthesis & D3PM & 3 & Fidelity, Utility, Privacy & \ding{55} &  \checkmark & Healthcare \\
\textbf{FinDiff} \cite{sattarov2023findiff}          & 2023         & ICAIF          &\checkmark (ZS)   & \checkmark (OH)   & Synthesis & DDPM & 3 & Fidelity, Utility, Privacy & \checkmark &  \checkmark & Finance \\
\textbf{EntTabDiff} \cite{liu2024entity}            & 2024         & ICAIF   &\checkmark (ZS)   & \checkmark (OH)   & Synthesis & DDPM & 3 & Fidelity, Utility, Privacy & \ding{55} &  \checkmark & Finance \\
\textbf{Imb-FinDiff} \cite{schreyer2024imb}            & 2024         & ICAIF   &\checkmark (ZS)   & \checkmark (OH)   & Synthesis & DDPM & 4 & Fidelity, Utility & \ding{55} &  \checkmark & Finance \\
\textbf{ClavaDDPM} \cite{pang2024clavaddpm}             & 2024         & NeurIPS           &\checkmark   & \checkmark(IE)   & Multi-Relational Synthesis & DDPM & 5 & Fidelity, Diversity, Utility, Dependency & \checkmark &  \checkmark & Generic \\

\textbf{GNN-TabSyn} \cite{hudovernik2024relational}  & 2024         & NeurIPSW             &\checkmark   & \checkmark(IE)   & Multi-Relational Synthesis & DDPM & 6 & Fidelity, Utility, Privacy & \checkmark &  \checkmark & Generic \\
\hline
\textbf{TabCSDI} \cite{zheng2022diffusion}  & 2023         & NeurIPSW            & \checkmark & \checkmark (OH, AB, FT) & Imputation & Conditional DDPM & 7 & Accuracy & \checkmark & \ding{55} & Generic\\
\textbf{TabDiff} \cite{shi2024tabdiff}            & 2024         & NeurIPSW    & \checkmark & \checkmark  & Imputation & Conditional DDPM & 7 & Accuracy & \checkmark & \ding{55} & Generic\\
\textbf{SimpDM} \cite{liu2024self}            & 2024         & CIKM   & \checkmark (QT) & \checkmark (OH)  & Imputation & DDPM+MLD & 17 & Accuracy & \checkmark & \ding{55} &  Generic \\
\textbf{MTabGen} \cite{villaizan2024diffusion}        & 2024         & ArXiv   & \checkmark (QT) & \checkmark (OE)  & Imputation & DDPM+MLD & 10 & Utility & \ding{55} & \ding{55} &  Generic \\
\textbf{DDPM-Perlin} \cite{wibisono2024natural}            & 2024         & KBS    & \checkmark & \ding{55}  & Imputation & DDPM & 10 & Accuracy & \ding{55} & \ding{55} &  Generic \\
\textbf{DiffPuter} \cite{zhang2024unleashing}            & 2024         & OpenReview   & \checkmark & \checkmark (OH)  & Imputation & DDPM & 10 & Accuracy & \checkmark & \ding{55} &  Generic \\
\hline
\textbf{SiloFuse} \cite{shankar2024silofuse}  & 2024         & ICDE  & \checkmark (FT) & \checkmark (FT)  & Trustworthy Synthesis & Latent DDPM & 9 & Fidelity, Utility, Privacy & \ding{55} & \checkmark &  Generic \\
\textbf{FedTabDiff} \cite{sattarov2024fedtabdiff}  & 2024         & ArXiv   & \checkmark (QT) & \checkmark (FT) & Trustworthy Synthesis & DDPM & 2 & Fidelity, Utility, Privacy & \checkmark & \checkmark &  Generic \\
\textbf{FairTabDDPM}* \cite{yang2024balanced}            & 2024         & ArXiv   & \checkmark (QT) & \checkmark (OH) & Trustworthy Synthesis & DDPM & 3 & Fidelity, Diversity, Utility, Privacy, Fairness & \checkmark & \checkmark &  Generic \\
\textbf{DP-Fed-FinDiff} \cite{sattarov2024differentially}            & 2024         & ArXiv   & \checkmark (QT) & \checkmark (FT) & Trustworthy Synthesis & DDPM & 4 & Fidelity, Utility, Privacy & \ding{55} & \checkmark &  Finance \\
\hline
\textbf{TabADM} \cite{zamberg2023tabadm}  & 2023         & ArXiv & \checkmark & \ding{55} & Anomaly Detection & DDPM & 32/57 A & Accuracy & \ding{55} & \checkmark & Generic\\
\textbf{DTE} \cite{livernoche2024diffusion}  & 2024         & ICLR & \checkmark (ZS) & \ding{55}  & Anomaly Detection & DDPM & 57/57 A & Accuracy & \checkmark & \checkmark & Generic\\
\textbf{SDAD} \cite{li2024self}  & 2024         & Inf. Sci. & \checkmark  & \ding{55}  & Anomaly Detection & Latent DDPM & 10/57 A & Accuracy & \checkmark & \checkmark & Generic\\
\textbf{NSCBAD} \cite{anonymous2024anomaly} & 2024         & OpenReview & \checkmark  & \ding{55}  & Anomaly Detection & SDEs & \makecell[r]{57/57 A \\ 15 others}  & Accuracy & \checkmark & \checkmark & Generic \\
\textbf{FraudDiffuse} 
\cite{roy2024frauddiffuse}  & 2024         & ICAIF & \checkmark   & \checkmark (FT)  & Anomaly Detection, Synthesis & Latent DDPM & 2 & Utility & \ding{55} & \checkmark & Finance \\
 \textbf{FraudDDPM} \cite{pushkarenko2024synthetic}  & 2024         & ISIJ & \checkmark  & \checkmark (OH, FT)  & Anomaly Detection, Synthesis & DDPM & 4 & Utility & \ding{55} & \checkmark & Finance \\
\bottomrule
\end{tabular}%
}
\label{tab:tabular_data_synthesis}
\end{table*}

Recent studies indicate that diffusion models can achieve superior performance in tabular data modeling. To support further exploration and progress in this area, we present a comprehensive survey on  diffusion models for tabular data. This survey includes a concise introduction to the fundamental theory behind diffusion models and a categorized review of the existing literature based on their applications. Furthermore, we highlight key challenges and propose potential research directions for advancing the field. We aim for this survey to serve as a valuable resource for researchers and practitioners, encouraging further innovation and development in generative diffusion models for tabular data.



\bibliographystyle{IEEEtran}
\bibliography{references.bib}

%\begin{IEEEbiographynophoto}{Zhong Li} is a postdoc researcher at Leiden University in Netherlands. He received a Ph.D. in Computer Science at Leiden University, during which he focused on trustworthy anomaly detection. He has published papers in leading data mining journals such as TKDE, DAMI, TKDD, and SIGKDD Explorations. He also serves as reviewer for leading conferences/journals such as ICLR, KDD, DAMI, and TKDE，ECMLPKDD. He is paasionate on understanding  generative models，including learning paradigm like self-supervised learning，aiming to make these widely used tehcniques trustworty. He focuses on generative models for tabular data.end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Qi Huang} is a PhD student at Leiden University in Netherlands. \end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Lincen Yang} is a postdoc researcher at Leiden University in Netherlands.\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Jiayang Shi} is .\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Zhao Yang} is .\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Niki van Stein} is .\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Thomas Bäck} is .\end{IEEEbiographynophoto}

%\begin{IEEEbiographynophoto}{Matthijs van Leeuwen} is .\end{IEEEbiographynophoto}
\end{document}



