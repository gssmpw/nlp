% Network design and architecture and hyperparameters
The trained DQN consisted of a neural network with a single hidden layer containing 100 neurons and a fully connected ReLU activation. The action space was two-dimensional, as described in Section~\ref{sec:DOND}, but was flattened into a one-dimensional output vector of length $\vert \cA \vert$ for the network; it is important to note that Agent 1 was restricted from accepting a deal or walking during its first turn, since this would be illogical. The length of the input vector varied, depending on the item pool, valuation distribution, and outside offers that comprised the five different parameter settings of the bargaining game. The exploration policy was $\epsilon$-greedy and was initially set to $\epsilon = 1.0$ and decayed to the final, minimum $\epsilon$ value.
Each player was allotted the same number of training steps to learn the BR. During the DQN's experience replay, a batch size of 64 data points was sampled from the memory, and the Adam optimizer was used to update the network weights. The memory buffer was limited to 200k experiences. We did not allow the network to begin training and updating weights until after 10k timesteps had passed (i.e. at least 10k experiences were stored in the memory buffer). We considered the hyperparameters in Table~\ref{tab:DQN_considered_hyper} as candidates for tuning and found the hyperparameters in Table~\ref{tab:DQN_params} to perform best as the result of a randomized search over the grid space of values. After network training was complete, we used a temperature of 1.0 for the softmax function we use to select the $M$ information sets to which the best response policy label will be added.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l||r|}
    \hline
    \multicolumn{2}{|c|}{Training Parameters} \\
    \hline
        Training Steps & 150000 \\
        Number of Hidden Neurons & 100 \\
        $\epsilon$ Decay & \textsc{Linear} \\
        Minimum $\epsilon$ & 0.02 \\
        Learning Rate $\alpha$ & $1\mathrm{e}{-4}$ \\
        Target Update Frequency & 2 \\
    \hline
    \end{tabular}
    \vspace{1em}
    \caption{Learned Hyperparameters of DQN for Sequential Bargaining Games.}
    \label{tab:DQN_params}
\end{table}

% Table showing hyperparameter tuning ranges
\begin{table}[h!]
    \centering
    \begin{tabular}{|l||r|}
    \hline
    \multicolumn{2}{|c|}{Hyperparameters} \\
    \hline
    % 1\mathrm{e}{-10}
        Training Steps & $\{ 1\mathrm{e}{5}, 1.5\mathrm{e}{5}, 2\mathrm{e}{5}, 5\mathrm{e}{5}, 1\mathrm{e}{6} \}$ \\
        Number of Hidden Neurons & $\{ 50, 100, 200 \}$ \\
        $\epsilon$ Decay & $\{ \textsc{Linear}, \textsc{Exp} \}$ \\
        Minimum $\epsilon$ & $\{ 0.01, 0.02, 0.05 \}$ \\
        Learning Rate $\alpha$ & $\{ 1\mathrm{e}{-4}, 3\mathrm{e}{-4}, 5\mathrm{e}{-4}, 1\mathrm{e}{-3}, 3\mathrm{e}{-3} \}$ \\
        Target Update Frequency & $\{ 1, 2, 5 \}$ \\
    \hline
    \end{tabular}
    \vspace{1em}
    \caption{Hyperparameters of DQN Considered for Sequential Bargaining Games.}
    \label{tab:DQN_considered_hyper}
\end{table}
