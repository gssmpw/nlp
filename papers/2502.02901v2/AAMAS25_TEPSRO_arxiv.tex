%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% LaTeX Template for AAMAS-2025 (based on sample-sigconf.tex)
%%% Prepared by the AAMAS-2025 Program Chairs based on the version from AAMAS-2025. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Start your document with the \documentclass command.


%%% == IMPORTANT ==
%%% Use the first variant below for the final paper (including auithor information).
%%% Use the second variant below to anonymize your submission (no authoir information shown).
%%% For further information on anonymity and double-blind reviewing, 
%%% please consult the call for paper information
%%% https://aamas2025.org/index.php/conference/calls/submission-instructions-main-technical-track/

\documentclass[sigconf]{aamas} 

%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page
\usepackage{bm}
\usepackage{dsfont}
\usepackage{tikz}
\usepackage{oplotsymbl}
\usepackage{marvosym}
\usepackage{savesym}
\savesymbol{Cross}
\usepackage{bbding}
\restoresymbol{bb}{Cross}
\usetikzlibrary{trees}
\usetikzlibrary{quotes}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes}
\usepackage[font=small,labelfont=bf]{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{float}
\usepackage{color}
\usepackage{listings}
\usepackage{setspace}
\usepackage{algorithmicx}
\usepackage[Algorithm,ruled]{algorithm}
\usepackage{algpseudocode}
\usepackage{etoolbox}
\usepackage{enumitem}


\usepackage{xr}
%\externaldocument[arx-]{AAMAS25_TEPSRO_arxiv}



\newcommand{\term}[1]{\textbf{\textit{#1}}}
\newcommand{\T}{\textsc{true}}
\newcommand{\F}{\textsc{false}}
\newcommand{\cA}{\mathcal{A}}
\newcommand{\Udist}{\mathbb{U}}
\newcommand{\cI}{\mathcal{I}}
\newcommand{\cV}{\mathcal{V}}
\newcommand{\cW}{\mathcal{W}}
\newcommand{\cO}{\mathcal{O}}
\newcommand{\cT}{\mathcal{T}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\targprof}{\bm{\Tilde{\sigma}^*}}
\newcommand{\bbR}{\mathbb{R}}
\newcommand{\regret}{\mathsf{Reg}}
\newcommand{\order}{\mathcal{P}}
\newcommand{\dondG}{G_\mathit{bargain}}
\newcommand{\numItems}{m}
\newcommand{\thresh}{\nu}
\newcommand{\offer}{\omega}
\newcommand{\infomap}{\phi}
\newcommand{\piBR}{\pi^*}
\newcommand{\bhexagon}{\mathord{\raisebox{0.6pt}{\tikz{\node[draw,scale=.65,regular polygon, regular polygon sides=6,fill=black](){};}}}}
\newcommand{\given}{\mid}
\newcommand{\gain}{\Gamma}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
% \newcommand{\abs}[1]{\lvert #1 \rvert}

%\newtheorem{theorem}{Theorem}
% \newtheorem{proposition}{Proposition}
% \newtheorem{definition}{Definition}
\newcommand{\expect}{\mathbb{E}}
\newcommand{\Binom}{\mathsf{Binom}}
\newcommand{\Prob}{\mathsf{Pr}}
\providecommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\mc}[1]{{\color{blue}{#1}}}
\newcommand{\gengoof}{\textsc{GenGoof}\xspace}
\newcommand{\gengoofK}[1]{\textsc{GenGoof}$_{#1}$\xspace}
\newcommand{\barg}{\textsc{Bargain}\xspace}

% colorblind friendly palette, add more colors as needed
\definecolor{player1}{RGB}{213, 94, 0}
\definecolor{player1_1}{RGB}{100, 143, 255}
\definecolor{player1_2}{RGB}{0, 153, 255}
\definecolor{player1_3}{RGB}{226, 146, 57}
\definecolor{player1_4}{RGB}{161, 191, 1}
\definecolor{player1_5}{RGB}{121, 71, 186}
\definecolor{player1_6}{RGB}{193, 32, 224}
\definecolor{player1_7}{RGB}{122, 4, 4}
\definecolor{player1_8}{RGB}{37, 124, 163}
\definecolor{player1_9}{RGB}{210, 153, 94}
\definecolor{player1_10}{RGB}{94, 210, 153}
\definecolor{player1_11}{RGB}{107, 65, 147}
\definecolor{player1_12}{RGB}{184, 126, 122}
\definecolor{player1_13}{RGB}{176, 203, 166}
\definecolor{player1_14}{RGB}{143, 175, 107}
\definecolor{player1_15}{RGB}{232, 244, 218}
\definecolor{player1_16}{RGB}{145, 76, 61}
\definecolor{player1_17}{RGB}{193, 94, 194}
\definecolor{player1_18}{RGB}{153, 0, 0}
\definecolor{player1_19}{RGB}{0, 153, 153}
\definecolor{player1_20}{RGB}{48, 233, 186}

\definecolor{player2}{RGB}{0, 114, 189}
\definecolor{player2_1}{RGB}{135, 100, 255}
\definecolor{player2_2}{RGB}{176,224,230}
\definecolor{player2_3}{RGB}{128, 0, 128}
\definecolor{player2_4}{RGB}{255, 178, 255}
\definecolor{player2_5}{RGB}{205,188,198}
\definecolor{player2_6}{RGB}{214, 198, 164}
\definecolor{player2_7}{RGB}{204, 0, 204}
\definecolor{player2_8}{RGB}{204, 204, 0}
\definecolor{player2_9}{RGB}{184, 173, 241}
\definecolor{player2_10}{RGB}{64, 224, 208}
\definecolor{player2_11}{RGB}{204, 255, 0}
\definecolor{player2_12}{RGB}{229, 193, 0}
\definecolor{chance}{RGB}{0, 158, 115}
\definecolor{otherplayer1}{RGB}{204, 121, 167}
\definecolor{otherplayer2}{RGB}{240, 228, 66}
\definecolor{terminal}{RGB}{253, 236, 160}
\definecolor{oldplayer}{RGB}{150, 150, 150}


\definecolor{gg_player1}{RGB}{128, 0, 128}
\definecolor{gg_player1_1}{RGB}{210, 151, 139}
\definecolor{gg_player1_2}{RGB}{192, 222, 150}
\definecolor{gg_player1_3}{RGB}{0, 105, 150}
\definecolor{gg_player1_4}{RGB}{255, 105, 180}
\definecolor{gg_player1_5}{RGB}{142, 59, 0}
\definecolor{gg_player1_6}{RGB}{152, 152, 255}
\definecolor{gg_player1_7}{RGB}{243, 222, 138}
\definecolor{gg_player1_8}{RGB}{1, 137, 223}
\definecolor{gg_player1_9}{RGB}{214, 85, 85}
\definecolor{gg_player1_10}{RGB}{218, 165, 32}
\definecolor{gg_player1_11}{RGB}{80, 200, 120}
\definecolor{gg_player1_12}{RGB}{120, 80, 200}
\definecolor{gg_player1_13}{RGB}{204,161,201}
\definecolor{gg_player1_14}{RGB}{190,209,227}
\definecolor{gg_player1_15}{RGB}{238,251,29}

\definecolor{gg_player2}{RGB}{230, 0, 230}
\definecolor{gg_player2_1}{RGB}{65, 105, 225}
\definecolor{gg_player2_2}{RGB}{68, 170, 153}
\definecolor{gg_player2_3}{RGB}{230, 97, 0}

\definecolor{gg_chance}{RGB}{126, 127, 154}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% AAMAS-2025 copyright block (do not change!)

\setcopyright{ifaamas}
\acmConference[AAMAS '25]{Proc.\@ of the 24th International Conference
on Autonomous Agents and Multiagent Systems (AAMAS 2025)}{May 19 -- 23, 2025}
{Detroit, Michigan, USA}{Y.~Vorobeychik, S.~Das, A.~Nowe (eds.)}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{}
\acmPrice{}
\acmISBN{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% == IMPORTANT ==
%%% Use this command to specify your submission number.
%%% In anonymous mode, it will be printed on the first page.

\acmSubmissionID{917}

%%% Use this command to specify the title of your paper.

\title[Policy Abstraction and Nash Refinement in TE-PSRO]{Policy Abstraction and Nash Refinement in Tree-Exploiting PSRO}

%%% Provide names, affiliations, and email addresses for all authors.

\author{Christine Konicki$^*$}\thanks{$^*$Konicki worked on this paper while a PhD student at the University of Michigan}
\affiliation{
  \institution{Michigan Tech Research Institute}
  \city{Ann Arbor}
  \country{USA}}
\email{ckonicki@mtu.edu}

\author{Mithun Chakraborty}
\affiliation{
  \institution{University of Michigan}
  \city{Ann Arbor}
  \country{USA}}
\email{dcsmc@umich.edu}

\author{Michael P. Wellman}
\affiliation{
  \institution{University of Michigan}
  \city{Ann Arbor}
  \country{USA}}
\email{wellman@umich.edu}

%%% Use this environment to specify a short abstract for your paper.

\begin{abstract}
Policy Space Response Oracles (PSRO) interleaves empirical game-theoretic analysis with deep reinforcement learning (DRL) to solve games too complex for traditional analytic methods. 
Tree-exploiting PSRO (TE-PSRO) is a variant of this approach that iteratively builds a coarsened empirical game model \textit{in extensive form} using data obtained from querying a simulator that represents a detailed description of the game.
We make two main methodological advances to TE-PSRO that enhance its applicability to complex games of imperfect information. 
First, we introduce a scalable representation for the empirical game tree where edges correspond to \textit{implicit policies} learned through DRL.
These policies cover conditions in the underlying game abstracted in the game model, supporting sustainable growth of the tree over epochs. 
Second, we leverage extensive form in the empirical model by employing refined Nash equilibria to direct strategy exploration.
To enable this, we give a modular and scalable algorithm based on generalized backward induction for computing a subgame perfect equilibrium (SPE) in an imperfect-information game. 
We experimentally evaluate our approach on a suite of games including an alternating-offer bargaining game with outside offers; our results demonstrate that TE-PSRO converges toward equilibrium faster when new strategies are generated based on SPE rather than Nash equilibrium, and with reasonable time/memory requirements for the growing empirical model.
\end{abstract}

%%% The code below was generated by the tool at http://dl.acm.org/ccs.cfm.
%%% Please replace this example with code appropriate for your own paper.


%%% Use this command to specify a few keywords describing your work.
%%% Keywords should be separated by commas.

\keywords{Game Theory, Extensive-form Games, PSRO}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Include any author-defined commands here.
         
\newcommand{\BibTeX}{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em\TeX}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

%%% The following commands remove the headers in your paper. For final 
%%% papers, these will be inserted during the pagination process.

\pagestyle{fancy}
\fancyhead{}

%%% The next command prints the information defined in the preamble.

\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec:intro}

\term{Empirical game-theoretic analysis} (EGTA) \citep{Tuyls20,Wellman16} reasons about complex game scenarios through \term{empirical game} models estimated from simulation data.
A popular form of EGTA is the \term{policy space response oracles} (PSRO) framework \citep{psro17} (Fig.~\ref{fig:psro}), in which the empirical game is iteratively extended by adding best responses (BRs) derived from \term{deep reinforcement learning} (DRL).
The vast majority of prior work on EGTA and PSRO \citep{Bighashdel24,Wellman24tg} represents the empirical game in normal form even though the real underlying game consists of agents' strategies interacting via sequential decisions under various unknowns. 
\citet{McAleer21} introduced \term{XDO} as an alternative to PSRO that maintains an empirical game in extensive form, to capture a combinatorial space of strategies with the choice of actions at each decision point in the game tree. 
We originally proposed and evaluated a
\term{tree-exploiting} version of EGTA (TE-EGTA) that maintains an empirical game in an extensive form based on a coarsening of the underlying game \citep{konicki22}. 
This work demonstrated that a significant improvement in model accuracy and strategy exploration, compared to normal-form EGTA, can be achieved by using the tree structure to model even a little of the information-revelation and action-conditioning patterns of the underlying game.
%for , even when the empirical game tree is a highly simplified version of the underlying game. 

\begin{figure}[!htb]
    \centering
    \includegraphics[width=\columnwidth]{figs/PSRO-diagram.pdf}
    \caption{Basic PSRO loop.
    In each iteration (or \term{epoch)}, an empirical game model is extended, based on best responses (BRs) to target profile $\targprof$ derived from the current empirical game by the solver MSS.
    The BRs are computed using deep RL applied to the game simulator. EVAL is a solver (not necessarily the MSS) applied to a model to assess its quality.
    }
    \label{fig:psro}
\end{figure}

A key step in PSRO is augmenting the empirical game model with new BR results.
% and \textit{how} to prevent model size from becoming unmanageable as we extend the strategy space with best responses. 
This is straightforward for a normal-form model: add the new strategies to the game matrix, and estimate payoffs for new profiles using simulation.
For PSRO using an extensive-form model, where we can no longer treat a BR as an atomic entity, we face new  questions such as the following. In what precise sense does the empirical game tree \textit{coarsen} or \textit{abstract} the underlying multiagent scenario? Relatedly, how should we incorporate elements of the BRs (detailed policy specifications) at appropriate places in the empirical game tree?
% Any abstract representation of the underlying game must support a clear semantic interpretation that connects abstract strategies in the empirical game to fully detailed strategies in the simulator.
The way \citet{konicki22} address these challenges in their approach towards tree-exploiting PSRO (TE-PSRO) is by systematically coarsening away select (non-strategic) stochastic events in the underlying game. This approach is not general or scalable in the sense that it relies on the use of stochastic events to model imperfect information in the underlying game.
%chance nodes in the underlying extensive-form game.
In this paper, we reformulate TE-PSRO by developing a method to abstract broad swaths of the \textit{state} and \textit{observation} spaces of the underlying game, providing a more implicit rendering of games with complex information structure, including high degrees of imperfect information. We also introduce other methodological advances that enhance the power of TE-PSRO in many directions.

First, to address the abstraction issue, we use two distinct formulations for the game of interest.
The \textit{underlying game as represented by the simulator} is defined in terms of a state space, agent actions, and the observations, successor states, and rewards (stochastically) resulting from applying actions in a given state.
This formulation is the natural one for DRL algorithms, which can interact directly with the simulator.
On the other hand, the \textit{empirical game model} employs an extensive-form tree representation, which is the natural object of game-solving algorithms.
To bridge the two formulations, the %action 
edges in the empirical game tree correspond to \term{abstract policies} executable in the simulator.
These abstract policies, derived through DRL, map agent observations to actions.%
\footnote{\citet{McAleer21} define a variant of XDO called Neural XDO that likewise employs policies represented as neural networks.
Rather than incorporate these as abstract policies in an empirical game tree, Neural XDO instead relies on methods like neural fictitious self-play \citep{heinrich2016deep} that perform game analysis directly in the space of neural network policies.}
The empirical game includes only select elements of this history, rendering much of the simulator state space and observation space implicit in this abstracted formulation.
At what level to incorporate observation details in the game model is a design choice, entailing tradeoffs in computation and fidelity.

Second, we face additional computational tradeoffs regarding how much to elaborate the game model based on DRL computations. In each iteration (or \textit{epoch}), PSRO solves the current empirical game using a \term{meta-strategy solver} (MSS).
It then derives an approximate best response for each player using DRL, assuming the other players follow the latest MSS solution. 
A straightforward approach would apply the derived response throughout the empirical game \citep{konicki22,McAleer21}.
% \mc{These new strategies generally can be applied throughout the empirical game model, but adding a new action to every information set based on the strategy as in \citet{konicki22}\citet{McAleer21} }
This, however, could lead the game tree to grow at an exponential rate, severely limiting the feasible number of PSRO epochs.
We propose to control this growth by adding the new DRL policies to only a select set of information sets in the empirical game. 

A final question we address regards the choice of MSS for TE-PSRO.
Previous research has considered a range of MSSs that operate on normal-form games, and observed a significant impact on PSRO efficacy \citep{psro17,Wang22mw}. We now have the opportunity to consider new MSSs that exploit the tree structure of an empirical game in extensive form, in particular, refined Nash equilibria.

To illustrate and evaluate our upgraded TE-PSRO approach, we employ two (perfect-recall) games with significantly different imperfect information structures that non-trivially extend stylized games from the literature. In our first game that we call \barg, two players with private valuations over a set of indivisible items negotiate how to split the set up between them through an alternating-offer protocol.
The scenario features imperfect information about the other party's valuation as well as choices for signaling regarding the value of a private \textit{outside option} that each player has recourse to in the event of negotiation failure. This sequential bargaining game extends a well-known two-party multi-issue negotiation task \citep{devault_dond15,lewis_dond17,fatima2014principles,li_dond23}. 

The second game is a general-sum abstraction of the card game Goofspiel \cite{fixx_goof72} that we call \gengoof. As a clean model of multi-round multiagent interactions with considerable strategic depth, Goofspiel has been extensively analyzed in the game-theory literature, more recently serving as a testbed for game-playing AI algorithms \citep{bosansky_sim16}. \gengoof proceeds over an arbitrary number of rounds, each including a discrete stochastic event (defined over a support diminishing every round by the single realized outcome) followed by all players choosing one discrete action each (effectively simultaneously); the payoff of each player at game termination is the sum of arbitrary per-round rewards. 

%(\S\ref{sec:DOND}),
Our key contributions are:
\begin{itemize}
    \item A general scheme for abstract policy representation that supports flexible implementation of TE-PSRO for complex games of imperfect information (\S\ref{sec:abstraction} and~\S\ref{sec:best_response}).
    \item An approach to control the growth of empirical game trees through selective incorporation of best responses at particular information sets (\S\ref{sec:best_response_augment} and App.~\ref{sec:expand_game_tree}).
    \item A new algorithm that computes a subgame perfect equilibrium (SPE) of an imperfect-information game (\S\ref{sec:SPE}).
    \item Experimental demonstration of the efficacy of SPE over NE as MSS for TE-PSRO on a variety of complex sequential games of imperfect information (\S\ref{sec:game_description} and \S\ref{sec:experiments}); our experiments address three aspects of the complete TE-PSRO loop: the effectiveness of our augmentation heuristic in controlling the empirical game growth rate, the power of our SPE computation algorithm, and a comparison of MSS choices including refined equilibria which are feasible only for extensive-form empirical games. 
\end{itemize}
%All appendices referenced below are available in the full version of the paper at <arXiv link>. 
The code base used for our experiments is available at \url{https://github.com/ckonicki-umich/AAMAS25}.

%%%%%%%%%

\section{Technical Preliminaries}\label{sec:prelim}

An \term{extensive-form game} (EFG) is a tuple 
\[G=~\langle~N,~H,~V,~\{\mathcal{I}_j\}_{j=0}^n,~\{A_j\}_{j=1}^n,~X,~P,~u~\rangle,\] 
 where $N = \{0, \dotsc, n\}$ is the set of players or agents; $H$ is a finite tree of histories divided into subsets of \term{terminal nodes} or leaves $Z$ and decision nodes $D$; $V$ is a function assigning each decision node $h$ to an acting player; $\cA_j(\cdot)$ is the set of actions available at each decision node; $u$ is a function mapping each $z \in Z$ to a \term{utility} vector $\{u_j(z)\}_{j=1}^n$;
in games of imperfect information, the set $\cI_j$ is a partition of $V^{-1}(j)$ where each $I \in \cI_j$ is an \term{information set} (or \term{infoset}) of $j$. All nodes $h \in I$ are indistinguishable to player~$j$, meaning their action spaces are also indistinguishable and denoted $A_j(I)$. 
The directed edge connecting any $h \in I$ to its child represents a transition resulting from $V(h)$'s move. We assume %that our information sets are consistent with 
\term{perfect recall} \citep[Definition 5.2.3]{shoham2008multiagent}. 
A node~$h$ where $V(h) = 0$ is called a \term{chance node} controlled by Nature, with a set of possible outcomes $X(h)$ and probability distribution $P(\cdot \mid h)$ over $X(h)$.

Since the underlying or \term{true game} corresponding to the simulator is too large to be represented directly with a tree, we instead express it in a state-action formulation.
A play of the game is a sequence of actions taken by the players (including Nature), where each action leads to a \term{world state} $w \in \cW$. 
The joint space of actions is given by $\cA = \bigotimes_{j = 1}^n \cA_j$, and the set of legal actions for agent $j$ at world state~$w$ is given by $\cA_j(w) \subseteq \cA_j$.
The probability distribution of the world state $w'$ following joint action $a = (a_1, \dotsc, a_n) \in \cA$ taken in world state $w$ is given by a transition function $\cT(w, a) \in \Delta^{\cW}$. 
Upon transitioning to $w'$ from $w$ via $a$, agent $j$ makes a partial \term{observation} $o_j = \cO_j(w, a, w')$ instead of fully observing $w'$. A reward $\cR_j(w)$ is given to agent $j$ at each $w \in \cW$, and the game ends when a \term{terminal world state} is reached. 
In this formulation, a history at time~$t$ is the sequence of world states and actions given by $h = (w^1, a^1, \dotsc, w^t)$; histories $z \in Z$ where $w^t$ is a terminal world state are terminal histories.
It follows that $R_j(h)$ and $\cA_j(h)$ are the reward and action space for agent~$j$ in the last world state of history~$h$.
An \term{information state} (or \term{infostate}) for agent~$j$, denoted $I_j$, is a sequence of agent~$j$'s observations and actions up to a point~$t$ in the game, given by $I_j(h) = (a^1_j, o^2_j, a^2_j, \dotsc, o^t_j) \equiv I_j$. 
Since agent~$j$ cannot distinguish between the histories of $I_j(h)$, it follows that $\cA_j(I_j(h)) = \cA_j(h)$. The complete set of infostates for player~$j$ is again given by $\cI_j$. 
We use hatted symbols to denote components of the empirical game tree (e.g., $\hat{\cI_j}$ for infosets) to distinguish them from analogous components of the true game.


A \term{pure strategy} for player~$j$ specifies the action that $j$ selects at each information set.
A \term{mixed strategy} $\sigma_j$ defines a probability distribution over the action space at each of $j$'s information sets.
A \term{strategy profile} is given by $\bsigma = (\sigma_1, \dotsc, \sigma_n)$, and $\bsigma_{-j}$ denotes the strategies of all players other than $j$. 
$\Sigma_j$ is the set of all strategies available to player~$j$, and $\Sigma = \bigtimes_{j = 1}^n \Sigma_j$ denotes the space of joint strategy profiles. A terminal history $z$ is reached by $\bsigma$ with a \term{reach probability} $r(z, \bsigma) = \prod_{j \in N} r_j(z, \sigma_j)$ where $r_j(z, \sigma_j)$ is the probability that player~$j$ chooses actions that lead to $z$, including Nature's contribution $r_0(z)$. 
The \term{payoff} of $\bsigma$ to player~$j$ is given by its expected utility $U_j(\bsigma)$. 
Player~$j$'s \term{regret} from playing~$\sigma_j$ as part of~$\bsigma$ is given by
$\regret_j(\bsigma) = \max_{\sigma \in \Sigma_j} U_j(\sigma, \bsigma_{-j}) - U_j(\bsigma)$. 
The profile regret of $\bsigma$ is the sum of player regrets: $\regret(\bsigma) = \sum_{j=1}^n \regret_j(\bsigma)$. 
A strategy profile $\bsigma$ with $\regret(\bsigma) = 0$ is a \term{Nash equilibrium} (NE).


%%%%%%%%%
\section{Description of Games Studied}\label{sec:game_description}
%To illustrate and evaluate our extensions to TE-PSRO, we employ two games with significantly different imperfect information structures: (1) a  general-sum abstraction of the extensively studied card game Goofspiel \cite{fixx_goof72} that we call \gengoof, for $K$ a positive integer; (2) a sequential bargaining game over resource division with outside offers that we call \barg.
We will now describe in detail the two games used in our experimental assessment of TE-PSRO in \S\ref{sec:experiments}. The game analyst using TE-PSRO has no direct access to a game description at this level of detail, but can query a simulator based on such a description for data samples relevant to game histories induced by input strategy profiles. 


\subsection{\barg}\label{sec:DOND}

%We employ a sequential bargaining game featuring complex imperfect information. 
% We will also use this game as a running example to illustrate our methodology in \S\ref{sec:method}. 
In this game, two players negotiate the division of $\numItems$ discrete items of $\tau$ %distinct 
types.
We represent the item pool by a vector $\mathbf{p}$ where the $i^\mathrm{th}$ entry $p_i$, $i \in \{1,\dotsc,\tau\} \equiv [\tau]$, is the number of items of type~$i$; $\sum_{i=1}^\tau p_i = \numItems$. 
Each player~$j \in \{1,2\}$ has a private valuation over the items given by a vector $\mathbf{v}_j$ of non-negative integers such that the $i^\mathrm{th}$ entry $v_{j,i}$ is player~$j$'s value for one item of type~$i$. 
In each game instance, $(\mathbf{v}_1, \mathbf{v}_2)$ are sampled uniformly at random from the collection $\cV$ of all vector pairs satisfying three constraints. 
First, for both players, the total value of all items is a constant: $\mathbf{v}_j \cdot \mathbf{p} = \bar{V}$, $j\in\{1,2\}$. 
Second, each item type must have nonzero value for at least one player: $\forall i \in [\tau] . v_{1, i} + v_{2, i} > 0$. 
Finally, some item type must have nonzero value for both players: $\exists i \in [\tau] . v_{1, i} v_{2, i} > 0$.

%\mathbf{v}_1 \odot \mathbf{v}_2 > 0$.
%Finally, each player~$j$ must assign at least one item type positive value, meaning $\sum_{i=1}^\tau v_{i,j}>0$. 

An additional feature of our game is that each player~$j$ has a private \term{outside offer} in the form of a vector of items $\mathbf{o}_j$,
% of the same $\tau$ types as in the above pool, 
defining the fallback payoff the player obtains if no deal is reached.
This offer is drawn from a distribution $P_j(\cdot)$ at the start of each game instance. During negotiation, a player~$j$ may choose to reveal coarsened information about its outside offer to the other player in the form of a binary signal which is $L$ (resp.~$H$) if the value of the offer $\mathbf{o}_j \cdot \mathbf{v}_j$ is at most (resp.\ greater than) a fixed threshold~$\thresh$ where $1 < \thresh < \bar{V}$.

In each of a finite number $T > 0$ of negotiation rounds, the players take turns proposing a partition of the pool between themselves, with player~1 moving first in each round. 
In its turn, a player can accept the latest offer from the other player (\textsc{deal}), end negotiations (\textsc{walk}), or make an offer-revelation combination of the form $(\offer,R)$. 
Offer $\offer \in \{ (\mathbf{p}_1, \mathbf{p}_2) \mid \mathbf{p}_1 + \mathbf{p}_2 = \mathbf{p} \}$ is a proposed partition of the items, with $\mathbf{p}_j$ a vector of $\tau$ non-negative integers representing player~$j$'s share.
Revelation $R\in \{ \T, \F \}$ represents that player's decision to either disclose its signal (\T) in that turn or not (\F). 
We also include a discount factor $\gamma \in (0,1]$ to capture preference for reaching deals sooner. 
Negotiation fails if a player chooses \textsc{walk} in any round $\rho \in\{1,\dotsc,T\}$ or $T$ rounds pass without any player choosing \textsc{deal}. 
In case of failure in round $\rho$, each player~$j$ receives a reward of $\gamma^{\rho} \mathbf{o}_j \cdot \mathbf{v}_j$ from its  outside offer. 
If a proposed partition $(\mathbf{p}_1, \mathbf{p}_2)$ is accepted in round $\rho$, then the reward to~$j$ is $\gamma^{\rho-1} \mathbf{p}_j \cdot \mathbf{v}_j$.
\begin{figure}[h!]
    \centering
\begin{subfigure}[t]{\textwidth}
%\begin{center}
\begin{tikzpicture}[thick,
    level 1/.style = {level distance = 7mm, sibling distance = 35mm},
    level 2/.style = {level distance = 7mm, sibling distance = 18mm},
    level 3/.style = {level distance = 16mm, sibling distance = 10mm},
    engine/.style = {inner sep = 1pt, above}]
    \node [draw, black, fill={chance}, regular polygon, regular polygon sides=3, rotate=180, inner sep=0.08cm] {}
    [black, ->]

    child {node [draw, black, fill={chance}, regular polygon, regular polygon sides=3, rotate=180, inner sep=0.08cm] (1) {} % $H_1$
      child {node [draw, black, fill={player1}, circle] (2) {} % $H_2$
        child {node [draw, black, fill={player2}, diamond] (3) {}
        edge from parent node[engine, sloped] {{\tiny $(\offer_x, \T)$}}}
        child {node [draw, black, fill={player2_2}, diamond] (4) {}
        edge from parent node[engine, sloped] {{\tiny $(\offer_y, \F)$}}}
      edge from parent node[engine, sloped] {$H$}}
      child {node [draw, black, fill={player1}, circle] (6) {} % $L_2$
        child {node [draw, black, fill={player2_5}, diamond] (7) {}
        edge from parent node[engine, sloped] {{\tiny $(\offer_x, \T)$}}}
        child {node [draw, black, fill={player2_3}, diamond] (8) {}
        edge from parent node[engine, sloped] {{\tiny $(\offer_y, \F)$}}}
      edge from parent node[engine, sloped] {$L$}}
      edge from parent node[engine, sloped] {$H$}}
    child {node [draw, black, fill={chance}, regular polygon, regular polygon sides=3, rotate=180, inner sep=0.08cm] (10) {}
    child {node [draw, black, fill={player1_1}, circle] (11) {} % $H_2$
        child {node [draw, black, fill={player2_1}, diamond] (12) {}
        edge from parent node[engine, sloped] {{\tiny $(\offer_x, \T)$}}}
        child {node [draw, black, fill={player2_2}, diamond] (13) {}
        edge from parent node[engine, sloped] {{\tiny $(\offer_y, \F)$}}}
      edge from parent node[engine, sloped] {$H$}}
      child {node [draw, black, fill={player1_1}, circle] (15) {} % $L_2$
        child {node [draw, black, fill={player2_4}, diamond] (16) {}
        edge from parent node[engine, sloped] {{\tiny $(\offer_x, \T)$}}}
        child {node [draw, black, fill={player2_3}, diamond] (17) {}
        edge from parent node[engine, sloped] {{\tiny $(\offer_y, \F)$}}}
      edge from parent node[engine, sloped] {$L$}}
      edge from parent node[engine, sloped] {$L$}
    };

    % \draw[dashed] (4)--(5);
    % \draw[dashed] (7)--(8);
    \path (3) -- node[auto=false]{{\footnotesize \ldots}} (4);
    \path (7) -- node[auto=false]{{\footnotesize \ldots}} (8);
    \path (12) -- node[auto=false]{{\footnotesize \ldots}} (13);
    \path (16) -- node[auto=false]{{\footnotesize \ldots}} (17);
\end{tikzpicture}
%\end{center}
\end{subfigure}



\begin{subfigure}[t]{\textwidth}
\begin{flushleft}
\vspace{0.1em}
% \centering
    \fbox{\begin{tabular}{rlrlrl}
& & & & \scriptsize{\textcolor{player2}{\DiamondSolid} \textcolor{player2_1}{\DiamondSolid} \textcolor{player2_2}{\DiamondSolid}} & \\ 
\scriptsize{\textcolor{chance}{\TriangleDown}} & \scriptsize Chance Nodes &
\scriptsize{\textcolor{player1}{\CircleSolid} \textcolor{player1_1}{\CircleSolid}} & \scriptsize Player 1 Infosets &
\scriptsize{\textcolor{player2_3}{\DiamondSolid} \textcolor{player2_4}{\DiamondSolid} \textcolor{player2_5}{\DiamondSolid}} & \scriptsize Player 2 Infosets
\end{tabular}}
\end{flushleft}
\end{subfigure}

\caption{Part of game tree llustrating the effect of Player 1's decision $R$ on Player 2's infoset structure.
% Player~2 can distinguish between the four action histories shown where player~1 reveals (action of the form $(\offer, \T)$). 
% Player~2 also has two non-singleton information sets, corresponding to paths where player~1 chooses not to reveal.
}
\label{fig:DOND_no_offer_reveal}
\end{figure}

Fig.~\ref{fig:DOND_no_offer_reveal} displays a partial extensive-form representation of a simulation of \barg after each player's valuation has already been sampled.
At the first two levels, the simulator samples outside offer signals for the two players from $\{H,L\}$; if $L$ (resp. $H$) is drawn for player~$j$, its actual outside offer is sampled uniformly at random from all possible item vectors $\mathbf{o}$ of cumulative value $\mathbf{o} \cdot \mathbf{v}_j$ above $0$ and at most the threshold $\thresh$ (resp. above $\thresh$ and at most $\bar{V}$). Thus, setting $P_j$ reduces to picking a probability of the signal being $H$ for player~$j$.
Next, player~$1$ chooses an action, comprising an offer $\offer$ and revelation $R$. Player~$2$ now has four distinguishable histories that result when player~$1$ reveals its signal and two non-singleton information sets that result when player~$1$ chooses not to reveal.
%The effect of this decision is illustrated by the structure of infosets for player~2.
The game continues (not shown) with the action of player~$2$, and alternation between the players for another $T-1$ rounds.




\subsection{\gengoof}\label{sec:GenGoof}
\gengoof is parametrized by a positive integer $K$ which determines the number of game rounds ($K-1$) as well as the size of each player's action space ($K$).

First, a discrete stochastic event with $K$ outcomes occurs at the game root, that is, $\abs{X(\emptyset)}=K$.
Let $e_1$ denote the realized outcome of this event.
Player~$1$ observes $e_1$ and chooses one of $K$ actions, say $a^k_1$, from $\cA_1(I(e_1))=\{ a^k_1 \}_{k=1}^K$. Then, player~$2$ observes $e_1$ but not player~$1$'s action, say $a^k_2$, and also chooses one of $K$ actions from $\cA_2(I(e_1 a^k_1)) = \{ a^k_2 \}_{k=1}^K$, ending round~$1$.
Round~$2$ begins with the realization $e_2$ of a second stochastic event with support $X\left(e_1 a^k_1 a^k_2\right) = X(\emptyset) \setminus \{ e_1 \}$. Player~$1$ then observes the history up to and including $e_2$ before choosing one of $K$ actions, followed by player~$2$ who observes all but player~$1$'s second chosen action. 
This process repeats until the final round $K-1$ where a stochastic event with only $2$ possible outcomes occurs, followed by player~1 and player~2 both observing the history until the final stochastic event realization and picking one of $K$ actions each, ending the game. 

To complete the game description, we define the probability distribution of each stochastic event and each leaf utility as follows (these are also game parameters that are hidden from the game analyst).
For each instance of \gengoof, we sample a single $K$-outcome categorical probability distribution uniformly at random for the stochastic event in round~$1$; for $k\in\{2,3,\dots,K-1\}$, the distribution of the round-$k$ stochastic event is obtained by renormalizing the round-$(k-1)$ over the residual support after eliminating the outcome realized in round $(k-1)$. For example, the probability distribution of the round-$2$ stochastic event given that $e_1$ occurred in round $1$ is
\begin{equation*}
    P\left(e_2 \given e_1 a^k_1 a^k_2 \right) = \frac{P(e_2 \given \emptyset)}{\sum\limits_{e' \in X(\emptyset) \setminus \{ e_1 \}} P(e' \given \emptyset)} \quad \forall e_2 \in X(\emptyset) \setminus \{ e_1 \}.
\end{equation*}

For each possible combination of the stochastic event outcome and the two players' action choices in each round of any game instance, we choose a reward for each player uniformly at random from $[0, u_{\max}]$ for a positive real number $u_{\max}$; we set the leaf utility for each player equal to the sum of the player's rewards over all $K - 1$ rounds in the corresponding history. Thus, for every leaf $z \in Z$ and player $j\in\{1,2\}$, $u_j(z) \sim \Udist([0,u_{\max}(K-1)])$ where $\Udist(\cS)$ denotes the uniform distribution over the set $\cS$.
%of these random utilities, for a range of utilities in the set $U[0, 10 (K-1)]$. 
%In the case of the experiments involving \gengoofK{4}, the maximum payoff either player could attain given any randomly generated instance parameters was 30. 
%All utility and probability parameters of the underlying game were hidden from the game theorist applying EGTA or PSRO to \gengoof{K}.
Figure~\ref{fig:gengoof_chap3} in App.~\ref{sec:gengoof4_diag} illustrates the first round of gameplay in a particular instance of \gengoof.  



%%%%%%%%%%

\section{Tree-Exploiting PSRO}\label{sec:method}
Our domain of interest comprises game instances where expanding the full game out as an extensive-form tree for the purpose of game analysis is infeasible. TE-PSRO tackles this challenge by maintaining a coarsened and abstracted (yet extensive-form) version as its empirical game model.
The full game is specified in the form of a gameplay simulator, which is formalized in terms of the world state framework (\S\ref{sec:prelim}).
A key question for TE-PSRO (Fig.~\ref{fig:psro}) is how to translate new best-response results into elements that can be systematically incorporated into an abstracted empirical game model as part of the model augmentation operation.
Our approach bridges the detailed state-space model of the simulator and the simplified game tree using the concept of abstract policies.

% We now discuss our extensions to TE-PSRO: constructing an empirical game model that represents the true game's state space implicitly rather than coarsening only its stochastic events; using DRL to approximate each player's best response to the MSS output; controlling the growth of the tree as it incorporates the latest DRL policies and associated new simulation data. 

\subsection{Abstract Policies}\label{sec:abstraction}

In the underlying game, the space of possible infostates of player~$j$ is given by $\cI_j$, and a policy $\pi_j$ specifies $j$'s action $a \in \cA_j(I)$ for each $I \in \cI_j$.
We represent such policies in our implementation by neural networks,  encoded as a set of weights for a given architecture.
In the empirical game~$\hat{G}$, player~$j$'s information possibilities are described by its infosets~$\hat{\cI_j}$.
In general, there need be no particular structural relationship between $\cI_j$ and~$\hat{\cI_j}$, though typically they will both be defined in terms of a shared set of primitive observations.
We capture the connection by a function $\infomap:\cI_j\to \hat{\cI_j}$, where $\infomap(I_j)$ is the empirical game infoset corresponding to underlying infostate~$I_j$.

Given the distinct formulations, policies $\pi_j$ are executable in the simulator, but cannot be directly interpreted within the framework of empirical game~$\hat{G}$. 
Nevertheless, we can incorporate them as actions in~$\hat{G}$.
For a given policy $\pi^x_j$, we treat the label ``$\pi^x_j$'' as a potentially allowable action for any infoset~$\hat{\cI_j}$. 
From the empirical game perspective, ``$\pi^x_j$'' is an abstract policy.
An overall game-tree strategy specifies an action for every infoset in~$\hat{\cI_j}$.
To execute a game-tree strategy profile in the simulator, we simply trace through the tree, applying selected abstract policies at each infoset.
The selected abstract policy remains in force for player~$j$ until a new infoset is reached where it is $j$'s turn to move.
Though uninterpreted in the game tree itself, these abstract policies have full access to the information state from the simulation needed for execution.

% The state space of the true game from player~$j$'s perspective is given by $\cI_j$, and a best response policy $\piBR_j(\cdot)$ specifies the optimal action $a \in \cA_j(I)$ for each $I \in \cI_j$ that is reachable given $\bsigma_{-j}$.
% In our framework for the empirical game tree $\hat{G}$, the action spaces of each $I \in \hat{\cI_j}$ are not restricted subsets of the true game's action spaces $\cA_j(\cdot)$. Instead, the learned neural network weights associated with $\piBR_j$ from an iteration of TE-PSRO are saved and mapped to a label $\pi^x_j$. During game simulation, the current state-action history in the true game is translated into the player's partial, abstracted perspective in $\hat{G}$ given by $I \in \hat{\cI}_j$ via a function $\infomap(\cdot)$. Then, when the outgoing edge labelled $\pi^x_j$ is chosen at $I$ to play, the best action to play at $w$ based on the learned weights of policy $\pi^x_j$ is returned as the next move. The aggregated simulation data creates paths in $\hat{G}$ made of these ``policy edges.'' It is important to note that in the case of the bargaining game with outside offers, we wanted to be able to solve for the subgame perfect equilibrium, which requires that $\hat{G}$ can be divided into subgames (\S\ref{sec:SPE}). In order to ensure this, our empirical model abstracts away the chance nodes that output the player's valuation vectors so that subgame roots can exist within $\hat{G}$. Each $\hat{A}_j(\cdot)$ consists of tuples containing a policy label $\pi_x$ \textit{and} the signal revelation (\T\ or \F) so that $\hat{G}$ reproduces the flow of information depicted in Fig.~\ref{fig:DOND_no_offer_reveal}.

\subsection{Best Response: Deep Reinforcement Learning}\label{sec:best_response}

With the ability to simulate profiles over the empirical game strategy space, we can employ the simulator within a deep RL algorithm to derive best responses~$\piBR_j$.
Our implementation employs the DQN algorithm \citep{mnih2015dqn}, which combines a feed-forward neural network parameterized by $\vartheta$ with temporal difference learning and a second target network to estimate Q-values over time given $I \in \cI_j$.

% To learn the best response for player~$j$, we model the true game as a multiagent DRL problem where player~$j$ visits different states in an environment, responds to its observations with actions, and receives rewards for taking an action in a given state (\S\ref{sec:prelim}). The policy derived from DRL in the true game will be that player's best response to the other player's (possibly mixed) strategy. 
% In the true sequential bargaining game, information sets are induced by (1) the other player's valuations being kept private throughout the entire game and (2) the other player's (possible) decision to withhold its outside offer signal from $j$. The world space $\cW$ would include all possible gameplay histories. The observation mapping $\cO_j$ for player $j$ would include gameplay history minus the other player's valuation and outside offer signal if it has not been revealed. Player~$j$'s action space would be $\cA_j$ (\S\ref{sec:DOND}). It is important to note, however, that Player 1's action space for the infostates at the start of the negotiations must be restricted to exclude the all action-tuples containing $\textsc{Walk}$ and $\textsc{Deal}$, since they are nonsensical at the beginning when no offers have been presented yet. The policy parameterized by the parameters $\vartheta$ of our Deep Q-Network (DQN) is a pure strategy map from $\cI_j$ to $\cA$ with label $\pi_{\vartheta}$ in $\hat{G}$.

 As an illustration, we provide details of our deep Q-network for \barg. The input to the neural network representing~$\piBR_j$ for this game is an encoding of player~$j$'s current information state~$I$. 
 $\lceil \log_2(\bar{V}) \rceil$ bits are allotted for player~$j$'s valuation $\mathbf{v}_j[i]$, for each $i \in [\tau]$. One bit is allotted for player~$j$'s outside offer signal. For each player's turn in the game, $\mathbf{p}[i] + 1$ bits are allotted per item type $i \in [\tau]$ to represent a partition of $\mathbf{p}$, plus one bit for the decision to reveal the signal or not. 
 Two bits are allotted to represent the other player's signal: \textsf{00} means no reveal so far, \textsf{01} means~$L$, and \textsf{10} means~$H$. 
 One final bit is allotted to be set to \textsf{1} when negotiations are complete.
 The output of the network is an $\vert \cA_j \vert$-long vector containing the Q-values of each action in $\cA_j$ given the input infostate vector. Our parameter settings, optimized via hyperparameter tuning, are included in App.~\ref{app:dqn}. After successfully training player~$j$'s DQN, the learned weights of $\pi_{\vartheta}$ are saved and mapped to the abstract policy label ``$\pi^x_j$'' in~$\hat{G}$ (\S\ref{sec:abstraction}).
 %For a normal-form game model, each of these policies would just be added as a row/column in the payoff matrix. 

\subsection{Augmenting the Empirical Game Model}\label{sec:best_response_augment}

Given BRs $\piBR$ computed from DRL, the next step is to augment the empirical game~$\hat{G}$ (Fig.~\ref{fig:psro}).
Though an abstract policy is potentially applicable at any point in the game tree, adding $\piBR_j$ to every $I \in \hat{\cI}_j$ could lead to unsustainable growth in~$\hat{G}$.

Our approach is to select a fixed number $M$ of infosets to augment for each player.
Our selection is based on an assessment of the \term{gain}~$\gain$ of playing $\piBR_j$ instead of $\targprof_j$ at candidate infosets $I \in \hat{\cI}_j$ where $\targprof$ is the  BR target at the current TE-PSRO epoch.
% 
Recall (\S\ref{sec:prelim}) that a terminal history in the underlying game is expressed as a sequence of world states and actions: $z = (w^1, a^1, \ldots, w^t)$, and that $R_j(z)$ is the associated reward for player~$j$.
The reach probability of $z$ given that history $h$ of length $t_h$ was reached is
\begin{equation*}
    r(z \mid h, \hat{\bsigma}) = \prod_{\ell = t_h}^{t} \hat{\bsigma}(\infomap(w^{\ell}))(\pi^x) \cdot \mathds{1}\{ \pi^x(w^{\ell}) = a^{\ell} \} \cdot T(w^{\ell}, a^{\ell})(w^{\ell + 1}).
\end{equation*}
The expected payoff to $j$ for playing $\hat{\sigma}_j$ in response to $\targprof_{-j}$ at $I \in \hat{\cI}_j$ is given by
\begin{equation*}
    U_j(\hat{\sigma}_j,\targprof_{-j}, I) = \sum\limits_{\substack{h \in H\mid \infomap(h) \in I}} \sum\limits_{z \in Z} r(z \mid h, \hat{\sigma}_j,\targprof_{-j}) R_j(z).
\end{equation*}

Let $\left.\hat{\bsigma}\right|_{I \rightarrow \piBR_j}$ denote a strategy profile identical to $\hat{\bsigma}$ except that player~$j$ selects $\piBR_j$ at $I$. Gain $K$ is equal to the product of the gain to player~$j$ of $\left.\targprof\right|_{I \rightarrow \piBR_j}$, given that $I$ is reached, and the probability of reaching the set of histories in the underlying game that translate into $I$:
\begin{equation*}
    \gain = r(I, \targprof) \cdot \left( U_j(\left.\targprof\right|_{I \rightarrow \piBR_j}, I) - U_j(\targprof, I) \right).
\end{equation*}

We then perform a softmax selection of $M$ infosets based on the gains $\left[ \gain_I \right]_{I \in \hat{\cI}_j}$.
BR policy $\piBR_j$ is then added as an action edge to each of these infosets.
The process creates new infosets, depending on the observable effects of the abstract policy.
% are then fed through a softmax function to yield a probability distribution, which is then used to select (without replacement) the $M$ infoset action spaces that will now contain $\piBR_j$. 
% If $M$ happens to be greater than $\mid \hat{\cI}_j \mid$ in $\hat{G}$, $\piBR_j$ is added to each action space. 
We illustrate how the empirical game tree is extended by this method in App.~\ref{app:te_psro_snapshots}, using \barg as an illustrative example.

Finally, TE-PSRO updates payoff estimates for the augmented~$\hat{G}$ by simulating the strategy combinations that result from the newly added edges and recording the sampled payoffs. %; new paths through $\hat{G}$ are introduced, and old paths that overlap with the strategy's trajectory are supplemented with additional samples.
All epochs are allocated the same total number of gameplay simulations, called the \textit{simulation budget}, distributed equally among all \textit{new} strategy profiles. Thus, the number of samples per profile is fixed based on the TE-PSRO epoch, independent of the choice of~$M$.


%%%%%%%%%

\section{Computing Refined Nash Equilibria}\label{sec:SPE}

Tree-based game models afford consideration of solution concepts specific to the extensive form.
We specifically investigate the use of \term{subgame perfect equilibrium} (SPE), a refinement of NE that rules out solutions containing non-credible threats.
To make use of SPE, we need a definition that applies to games of imperfect information, and an algorithm that computes such solutions.

\begin{definition}\label{def:subgame}
A \term{subgame} of game $G$ is a directed rooted subtree given by $G'= \langle N, H', V', \{\mathcal{I}'_j\}_{j=0}^n, \{A'_j\}_{j=1}^n, X', P', u' \rangle$ satisfying the following:
\begin{itemize}%[leftmargin=*]
    \item The root $h'$ of tree $H'$ must be the only node in its information set.
    \item As a subtree of $H$, $H'$ must include all nodes in $H$ that succeed~$h'$.
    \item For any $j \in N$ and for all $I \in \mathcal{I}_j$, if $I \in \mathcal{I}'_j$, then the nodes $h \in I$ must all be part of $H'$; if $I \notin \mathcal{I}'_j$, then all its nodes must be part of $H \setminus H'$.
    \item $V'$, $\{\mathcal{I}'_j\}_{j=0}^n$, $\{A'_j\}_{j=1}^n$, $X'$, $P'$, and $u'$ are restrictions to $H'$ of $V$, $\{\mathcal{I}_j\}_{j=0}^n$, $\{A_j\}_{j=1}^n$, $X$, $P$, and $u$, respectively.
\end{itemize}
\end{definition}

% It is important to notice that this definition generalizes to imperfect-information EFGs; further, it implies that the game $G$ is its own subgame and, in case of perfect information, every decision node in $G$ is the root of a subgame.

\begin{definition}[\citep{selten75}]\label{def:SPE}
A \term{subgame perfect equilibrium} (SPE) of game $G$ is an NE of $G$ that also induces NE play in each of $G$'s subgames. 
% That is, for all subgames, no player can increase their payoff by unilaterally deviating to a different strategy in the subgame \citep{selten65,selten75}.
\end{definition}
In finite perfect-information EFGs, an SPE always exists in pure strategies and can be readily computed using the classic backward induction approach. 
With imperfect information, however, a subgame may not admit a pure-strategy NE at all. 
\citet{kaminski19} proposed the \term{generalized backward induction} (GBI) approach for finding the set of SPE for a potentially infinite game of imperfect information.
A key feature of GBI is the re-expression of the game tree as a set of its proper subgames organized by their roots. 
Other crucial implementation details are not fully specified in the original article, in particular how to identify NE of subgames that include non-singleton information sets; a na\"{\i}ve implementation using exhaustive enumeration of strategy profiles for combinations of subgames has a runtime that is exponential in game size.

We provide a practical, modular algorithm for finding an SPE via GBI in a finite, imperfect-information EFG.
Our algorithm combines dynamic programming with a Nash solver subroutine, using \citeauthor{kaminski19}'s [\citeyear{kaminski19}] idea of organizing the game into subgames. 
Alg.~\ref{alg:compute_spe} presents our method.
\textsc{ComputeSPE} uses subroutines described here at a high level (see App.~\ref{app:SPE} for full pseudocode). 

\begin{algorithm}[!htb]
%\renewcommand{\thealgorithm}{}
\small
%\floatname{algorithm}{\textsc{ComputeSPE}}
\caption{: \textsc{ComputeSPE}}
\label{alg:compute_spe}
\begin{algorithmic}[1]
\Require{Input game $G$}

\State{$\Psi \gets \textsc{GetSubgameRoots}(G)$}
\State{$\ell \gets$ height of $h_0$ in $\Psi$}
\State{$\{ \Theta_k \}_{k = 1}^{\ell} \gets \textsc{GetSubgameGroups}(G, \Psi, \ell)$}

\State{$\bm{\sigma}^{SPE} \gets \textsc{GetInitialSPE}(G, \Theta_1)$}
\For{$1 < k \leq \ell$}
\For{$\theta \in \Theta_k$}
\State{Extract $\left.\bm{\sigma}^{SPE}\right|_{G_{\theta}} \gets \{ \bm{\sigma}^{SPE}(I) \mid I \in G_{\theta} \cap \bm{\sigma}^{SPE} \}$}
\State{$\bm{\sigma}^{SPE} \gets \bm{\sigma}^{SPE} \cup \textsc{NashSolver}(G_{\theta}, \left.\bm{\sigma}^{SPE}\right|_{G_{\theta}})$}
\EndFor
\EndFor

\Return $\bm{\sigma}^{SPE}$
\end{algorithmic}
\end{algorithm}
 
 We first call \textsc{GetSubgameRoots} to find the roots of all subgames in the input game $G$ and arrange them into a tree $\Psi$ rooted at $h_0$, the root of $G$. 
 A root in $\Psi$ has a \term{height} $1 \leq k \leq \ell$ where the subgames closest to the leaves of $G$ have height~1 and $h_0$ has height~$\ell$.
 To find the roots, it is sufficient to check which nodes in $H$ are roots of subtrees satisfying the conditions of Definition~\ref{def:subgame}.
 \textsc{GetSubgameGroups} collects all subgame roots in $\Psi$ at height~$k$ into a set~$\Theta_k$, for $1 \leq k \leq \ell$. 
 Then, we use dynamic programming to iterate over the subgames of each $\Theta_k$ and solve each subgame at height $k$ directly via a chosen \textsc{NashSolver}. The union $\bm{\sigma}^{SPE}$ of all SPE found for the subgames in $G$ with height less than $k$ is updated with each new partial SPE. $G_{\theta}$ denotes the subgame rooted at node $\theta \in \Theta_k$. 
 The union of all SPE across all subgames in $G$ by definition must be the SPE of $G$. In order to avoid overwriting the SPE that have been computed for any subgames at smaller heights in $G_{\theta}$, we pass the partial SPE $\left.\bm{\sigma}^{SPE}\right|_{G_{\theta}}$ in as input to \textsc{NashSolver} and restrict the solver to find a solution only for the information sets within~$G_{\theta}$ that are not already included in $\left.\bm{\sigma}^{SPE}\right|_{G_{\theta}}$. 
 This ensures that the runtime of \textsc{ComputeSPE} is linear with respect to the size of $G$ and thus scalable modulo the runtime of \textsc{NashSolver} (see App.~\ref{app:proofs} for runtime analysis). 
 For our experiments (\S\ref{sec:experiments}), we devised an adaptation of the counterfactual regret (CFR) minimization algorithm~\citep{cfrm07}, called $\textsc{SubgameCFR}$, as our \textsc{NashSolver}.

%%%%%%%%%%

\section{Experiments}\label{sec:experiments}
We now report experiments that we conducted to evaluate our TE-PSRO approach by applying it to the two games described in \S\ref{sec:game_description}.

\subsection{Parameter settings}
For \barg (\S\ref{sec:DOND}), we set $\tau=3$, $\bar{V}=10$, $\thresh=5$, $\gamma=0.99$, $T=5$, and $n\in\{5,6,7\}$. 
We generated five unique sets of the remaining parameters $\mathbf{p}$, $(\mathbf{v}_1, \mathbf{v}_2)$, $P_1$, $P_2$ uniformly at random from their respective supports in order to evaluate TE-PSRO's performance on a variety of game instances. For \gengoof (\S\ref{sec:GenGoof}), we set $K=4$, $u_{\max}=10$, calling this instance \gengoofK{4}.  The simulator budget was $100$ samples for \barg and $200$ samples for \gengoofK{4}.


 %All figures include the initial value of each metric for all runs at time-step~0 of TE-PSRO. 
We ran all experiments on our local computing cluster using a single core. 
Runtime and memory requirements depend on the choice of $M \in \{ 1, 2, 4, 8, 16 \}$, which determines the rate of growth of~$\hat{G}$ across TE-PSRO epochs (see App.~\ref{app:great_lakes} for details). %We found that for $M = 1$, running TE-PSRO for 15 epochs (at most) required at most 15 hours and 6.6 gigabytes of memory. Likewise, running TE-PSRO for the same number of epochs with $M = 2$ required 32 hours and 6.7 gigabytes of memory, and running TE-PSRO with $M = 4$ required 37 hours and 9.1 gigabytes of memory. 
Unless otherwise stated, every experiment was performed for five randomly seeded trials for each setting. Error bars in our plots correspond to a 95\% confidence interval.

\subsection{Results}\label{sec:exp_results}
Our first set of experiments assesses space requirements for the empirical game $\hat{G}$ as a function of the number $M$ of infosets augmented per epoch. 
At each epoch of TE-PSRO, we recorded the total number of information sets across players in $\hat{G}$ and the memory required by the emprical model.
We report the average empirical game size for each of the two games studied in terms of the number of player information sets of all players and memory required in megabytes (MB) 
in Fig.~\ref{fig:game_size} and Fig.~\ref{fig:game_size_app} in App.~\ref{app:game_size} respectively, for representative values of $M$; each curve for \barg is averaged over 100 trials per value of $M$ across all five sets of bargaining parameters. The broad takeaway from all plots in this set is the following. Although the rate of increase in the size of the $\hat{G}$ steepens with $M$ in the plots, its size is still manageable after many epochs of TE-PSRO. If we had added a new policy to all information sets rather than to only a subset of size $M$, $\hat{G}$ would grow to as many as $3000$ or $4000$ information sets after only five epochs of TE-PSRO, which is an unsustainable trajectory. See App.~\ref{app:game_size} for further insights on the difference between the two games.
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
     \caption{\barg}
    \includegraphics[width=\textwidth]{figs/game_size_plots/empirical_game_size_6_27.png}
    \label{fig:infosets_barg}
    \end{subfigure}
    \begin{subfigure}[b]{0.36\textwidth}
    \caption{\gengoofK{4}}
    \includegraphics[width=\textwidth]{figs/empirical_game_infosets_4rounds_822.png}
     \label{fig:infosets_gengoof}
    \end{subfigure}
    \caption{Total number of information sets of both players in empirical game $\hat{G}$ over the course of TE-PSRO's runtime, averaged over all combinations of all parameters (except $M$) and seeds.}
    \label{fig:game_size}
\end{figure}

Our second set of experiments provides evidence for the effectiveness of our algorithm \textsc{ComputeSPE}(\S\ref{sec:SPE}) as well as the non-triviality of obtaining an SPE of an imperfect-information extensive-form game.
% game of imperfect information with multiple proper subgames, meaning the sets of NE and SPE were not necessarily identical. 
We ran two suites of TE-PSRO on each of \barg and \gengoofK{4}: each suite consisted of 50 trials for each $M$ setting, using NE as the MSS for 25 trials and SPE as the MSS for the remaining~25.
In one suite, we evaluated intermediate and final $\hat{G}$ (EVAL in Fig.~\ref{fig:psro}) by NE computed using CFR, and the other by SPE using Alg.~\ref{alg:compute_spe}.
We computed the regret of the respective solutions with respect to each subgame of $\hat{G}$ and reported the maximum over subgames, that is, the \term{worst-case subgame regret}; a solution with a lower value of this quantity is a better SPE approximation. Thus,  
Figs.~\ref{fig:subgame_regret_SPE} and~\ref{fig:subgame_regret_abs4_SPE} verify that Alg.~\ref{alg:compute_spe} does indeed produce an approximate SPE
Figs.~\ref{fig:subgame_regret_NE} and~\ref{fig:subgame_regret_abs4_NE} demonstrate that our regular Nash solver does not happen to stumble upon NE that are also subgame-perfect. 
% the SPE in a game tree where the SPE and NE were markedly different, or where major swaths of the game tree contained imperfect information. 
Additionally, the worst-case subgame regret increases with the complexity of $\hat{G}$, reflected in both setting of $M$ and epochs of TE-PSRO. Note that these experiments are not for gauging the quality of the models produced by TE-PSRO; instead, TE-PSRO is used to generate a sequence of empirical games of increasing size and complexity (in terms of the number of non-singleton information sets) that serve as more and more challenging test cases for our game-solving algorithms.


%Larger $\hat{G}$ generally means more paths leading to areas where one or both agents are withholding their signal, and therefore more non-singleton information sets.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
    \caption{\barg: Empirical NE}
    \includegraphics[width=\textwidth]{figs/subgame_regret_plots/subgame_regrets_ne_6_27.png}
    \label{fig:subgame_regret_NE}
    \end{subfigure}\hspace{50pt}
    \begin{subfigure}[b]{0.36\textwidth}
     \caption{\barg: Empirical SPE}
    \includegraphics[width=\textwidth]{figs/subgame_regret_plots/subgame_regrets_spe_6_27.png}
     \label{fig:subgame_regret_SPE}
    \end{subfigure}\\
    \begin{subfigure}[b]{0.36\textwidth}
     \caption{\gengoof{4}: Empirical NE}
    \includegraphics[width=\textwidth]{figs/worst_case_subgame_regret_NE_T500_4rounds.png}
    \label{fig:subgame_regret_abs4_NE}
    \end{subfigure}\hspace{50pt}
    \begin{subfigure}[b]{0.36\textwidth}
    \caption{\gengoof{4}: Empirical SPE}
    \includegraphics[width=\textwidth]{figs/worst_case_subgame_regret_SPE_T500_4rounds.png}
     \label{fig:subgame_regret_abs4_SPE}
    \end{subfigure}
    %\caption{Average worst-case subgame regret of solutions to empirical game $\hat{G}$ in \gengoof{4}.}
    \label{fig:subgame_regret_abs4_chap6}
    \caption{Average worst-case subgame regret of NE  and SPE  solutions to empirical game $\hat{G}$ for the two games studied.
    Note that the scale of vertical axis of (b) is finer than that of (a) by a factor $\approx10^3$ while the corresponding factor $\approx10$ for (d) and (c). 
    }
    \label{fig:subgame_regret}
\end{figure*}

Our final experiment set characterizes TE-PSRO performance in terms of the MSS choice (SPE vs. NE) and different values of~$M$. 
To compare  MSS choices, we computed the profile regret (\S\ref{sec:prelim}) with respect to the underlying game of the solution $\bsigma^*$ returned by EVAL in each epoch of TE-PSRO epoch; we used both NE and SPE as EVAL, giving us two sets of comparison metrics. 

Fig.~\ref{fig:true_regret} depicts our average regret results for \barg under various settings.
We ran TE-PSRO for 25 trials per value of $M$ and four combinations of choices for EVAL and MSS. 
In each trial, TE-PSRO was allowed to run for at most 30 epochs, terminating early when the computed best responses did not yield an improvement greater than 0.1 over the current solution $\bsigma^*$.
Fig.~\ref{fig:regret_M1} shows that TE-PSRO outperforms the normal-form version NF-PSRO, regardless of EVAL/MSS choice, even for $M = 1$.
Figs.~\ref{fig:regret_M8_no_NF} and~\ref{fig:regret_M4_no_NF} show that SPE beats NE as an MSS for $M \in \{ 4, 8 \}$, converging faster to near-zero regret regardless of EVAL.
Finally, Fig.~\ref{fig:regret_spe_mss_ne_eval} compares results for different $M$ settings, with NE as EVAL and SPE as MSS. 
The main takeaway is that intermediate values $M=4$ and $M=8$ outperform lower and higher settings.
Intuitively, a higher $M$ produces $\hat{G}$ with broader coverage earlier, but with a fixed sampling budget, each tree path is estimated less accurately, resulting in a non-monotonic performance with respect to $M$. 
App.~\ref{app:expts} presents the full set of plots over combinations of EVAL, MSS, and $M$.
\begin{figure*}[htb!]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
     \caption{$M = 1$, varying EVAL and MSS, NF-PSRO as baseline}
    \includegraphics[width=\textwidth]{figs/true_regret_plots_by_num_br/true_regret_6_16_M1.png}
    \label{fig:regret_M1}
    \end{subfigure}\hspace{50pt}
    \begin{subfigure}[b]{0.36\textwidth}
    \caption{$M = 8$, varying EVAL and MSS}
    \includegraphics[width=\textwidth]{figs/true_regret_plots_by_num_br/true_regret_7_3_M8_no_NF.png}
    \label{fig:regret_M8_no_NF}
    \end{subfigure}
    \begin{subfigure}[b]{0.36\textwidth}
    \caption{$M = 4$, varying EVAL and MSS}
    \includegraphics[width=\textwidth]{figs/true_regret_plots_by_num_br/true_regret_7_4_M4_no_NF.png}
    \label{fig:regret_M4_no_NF}
    \end{subfigure}\hspace{50pt}
    \begin{subfigure}[b]{0.36\textwidth}
    \caption{NE for EVAL, SPE for MSS, varying $M$}
    \includegraphics[width=\textwidth]{figs/true_regret_plots_by_mss/true_regret_spe_mss_ne_eval_7_3.png}
    \label{fig:regret_spe_mss_ne_eval}
    \end{subfigure}
    \caption{Average regret of solution $\bsigma^*$ of empirical game for \barg over iterations of TE-PSRO, using NE or SPE as the MSS or EVAL and different values of $M$.}
    \label{fig:true_regret}
\end{figure*}

We assessed the statistical significance of the MSS comparisons for \barg using a permutation test, for each setting of $M$ and EVAL. 
Our figure of merit is the area $\cA_\text{MSS}$ under the regret curve, 
calculated starting from TE-PSRO epoch $5$ to avoid the noisy startup phase.
Our test statistic is $\Delta_\text{MSS} = \cA_{\textsc{NE}} - \cA_{\textsc{SPE}}$, resampled over 1000 permutations of the MSSs. 
The $p$-value is the fraction of times the difference under permutation (i.e., a null hypothesis that the MSSs are equally effective) is greater than the observed $\Delta_\text{MSS}$.
For $M = 4$, the superiority of SPE as MSS is significant ($p = 0.007$) with NE as EVAL. 
For $M = 8$, SPE as MSS is significantly better both for NE ($p = 0.006$) and SPE ($p = 0.021$) as EVAL. 
The results are less consistent and less significant for non-optimal $M$ values (App.~\ref{app:p-values}).

For experiments on \gengoofK{4}, we additionally constrained the space of empirical game models induced by TE-PSRO by coarsening away the stochastic events in the last one  two rounds of the full three-round underlying game. We indicate this by $IR$ that stands for \term{included rounds}; it may take values $[0]$, $[0,1]$ or $[0,1,2]$ indicating that each empirical game tree can include (a) stochastic event(s) only in its first round, first two rounds, and all three rounds respectively. 
In Fig.~\ref{fig:abstract_spe_mss_chap8}, we see that $IR = [0]$ and $IR = [0, 1]$ tended to yield the best performance regardless of MSS and that, for both of these settings, SPE outperformed NE as the MSS. App.~\ref{app:IR_gengoof} offers insights on how $IR$ and $M$ jointly impact TE-PSRO performance.

\begin{figure*}[htp!]
    \centering
    \begin{subfigure}[b]{0.36\textwidth}
    \caption{$M = 2$ \label{fig:spe_abs4_M2}}
    \includegraphics[width=\textwidth]{figs/true_regret_4rounds_ne_eval_spe_mss_M2.png}
    \end{subfigure}\hspace{50pt}
    \begin{subfigure}[b]{0.36\textwidth}
     \caption{$M = 4$ \label{fig:spe_abs4_M4}}
    \includegraphics[width=\textwidth]{figs/true_regret_4rounds_ne_eval_spe_mss_M4.png}
    \end{subfigure}
    \caption{Average regret of $\bsigma^*$ evaluated in \gengoofK4\ over the course of TE-PSRO's runtime, using NE or SPE as the MSS.}
    \label{fig:abstract_spe_mss_chap8}
\end{figure*}




%%%%%%%%%%%

\section{Conclusions}\label{sec:conclusion}

We introduced multiple extensions of Tree-Exploiting PSRO, enabling its application to complex games of imperfect information. 
Our main innovation is the treatment of best responses computed by DRL as abstract policies, incorporated as actions in the empirical game tree.
To manage growth of the empirical game as BRs are generated over the course of TE-PSRO, we introduced a hyperparameter~$M$, which controls the number of infosets that can be expanded per epoch.
Finally, we demonstrated that having an extensive-form empirical game model can be leveraged in the form of new meta-strategy solvers based on Nash refinements. 
Toward that end, we developed a modular algorithm for identifying SPE solutions in imperfect-information games. We demonstrated these methods on two carefully constructed complex games, featuring multiple rounds of offer/counteroffer with signaling options.
We showed that TE-PSRO easily outperforms normal-form PSRO in this environment, and that intermediate values of $M$ perform best. 

Particularly intriguing is our finding that exploiting Nash refinement in an MSS offers promise for improving strategy exploration.
Even when the goal is not to find a subgame-perfect solution (i.e., EVAL is NE rather than SPE), targeting best response to SPE rather than NE can be beneficial.
Further work will be required to confirm the scope of and understand the reasons for this advantage.
One intuitive explanation is that empirical game equilibria containing non-credible threats may be particularly exploitable in the underlying game, and thus not the most promising lines to pursue in strategy exploration.



%%%%%%%%%%




\begin{acks}
This work was supported in part by a grant from the Effective Altruism Foundation and by the US National Science Foundation under CRII Award 2153184.
\end{acks}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% The next two lines define, first, the bibliography style to be 
%%% applied, and, second, the bibliography file to be used.

\clearpage
\bibliographystyle{ACM-Reference-Format} 
\bibliography{aamas2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\iffalse
\newpage
\appendix
\onecolumn

\section*{Appendices for AAMAS25 Submission 917}

\section{Illustration of Gameplay in \gengoofK{4}}\label{sec:gengoof4_diag}
\begin{figure}[htbp]
\centering
\begin{subfigure}[t]{0.95\textwidth}
\centering
\begin{tikzpicture}[thick,
    level 1/.style = {level distance = 15mm, sibling distance = 22mm},
    level 2/.style = {level distance = 15mm, sibling distance = 26mm},
    level 3/.style = {level distance = 15mm, sibling distance = 32mm},
    level 4/.style = {level distance = 18mm, sibling distance = 12mm},
    engine/.style = {inner sep = 1pt, above}]
    \node [draw, black, fill={gg_chance}, regular polygon, regular polygon sides=3, rotate=180, inner sep=0.10cm] {}
    [black, ->]
    child { node [draw, black, fill={gg_player1}, circle] (1) {} %A
        child {node [draw, black, fill={gg_player2}, diamond] (2) {} %a_1
            child {node [draw, black, fill={gg_chance}, regular polygon, regular polygon sides=3, rotate=180, inner sep=0.10cm] (3) {}
                child {node [draw, black, fill={gg_player1_4}, circle] (4) {}
                edge from parent node[engine, left] {$B$}}
                child {node [draw, black, fill={gg_player1_5}, circle] (5) {}
                edge from parent node[engine, left] {$C$}}
                child {node [draw, black, fill={gg_player1_6}, circle] (6) {}
                edge from parent node[engine, right] {$D$}}
            edge from parent node[engine, sloped] {$a^1_2$}}
            child {node [draw, black, fill={gg_chance}, regular polygon, regular polygon sides=3, rotate=180, inner sep=0.10cm] (7) {}
                child {node [draw, black, fill={gg_player1_7}, circle] (8) {}
                edge from parent node[engine, left] {$B$}}
                child {node [draw, black, fill={gg_player1_8}, circle] (9) {}
                edge from parent node[engine, left] {$C$}}
                child {node [draw, black, fill={gg_player1_9}, circle] (10) {}
                edge from parent node[engine, right] {$D$}}
        edge from parent node[engine, sloped] {$a^4_2$}}
        edge from parent node[engine, sloped] {$a^1_1$}}
        child {node [draw, black, fill={gg_player2}, diamond] (11) {} %a_1
        edge from parent node[engine, sloped] {$a^4_1$}}
    edge from parent node[engine, sloped] {$A$}}
    child { node [draw, black, fill={gg_player1_1}, circle] (12) {} %B
    edge from parent node[engine, left] {$B$}}
    child { node [draw, black, fill={gg_player1_2}, circle] (13) {} %C
    edge from parent node[engine, right] {$C$}}
    child { node [draw, black, fill={gg_player1_3}, circle] (14) {} %D
        child {node [draw, black, fill={gg_player2_1}, diamond] (17) {} %a_1
        edge from parent node[engine, sloped] {$a^1_1$}}
        child {node [draw, black, fill={gg_player2_1}, diamond] (26) {} %a_1
            child {node [draw, black, fill={gg_chance}, regular polygon, regular polygon sides=3, rotate=180, inner sep=0.10cm] (18) {}
        child {node [draw, black, fill={gg_player1_10}, circle] (19) {}
        edge from parent node[engine, left] {$A$}}
        child {node [draw, black, fill={gg_player1_11}, circle] (20) {}
        edge from parent node[engine, left] {$B$}}
        child {node [draw, black, fill={gg_player1_12}, circle] (21) {}
        edge from parent node[engine, right] {$C$}}
    edge from parent node[engine, sloped] {$a^1_2$}}
    child {node [draw, black, fill={gg_chance}, regular polygon, regular polygon sides=3, rotate=180, inner sep=0.10cm] (22) {}
        child {node [draw, black, fill={gg_player1_13}, circle] (23) {}
        edge from parent node[engine, left] {$A$}}
        child {node [draw, black, fill={gg_player1_14}, circle] (24) {}
        edge from parent node[engine, left] {$B$}}
        child {node [draw, black, fill={gg_player1_15}, circle] (25) {}
        edge from parent node[engine, right] {$C$}}
edge from parent node[engine, sloped] {$a^4_2$}}
        edge from parent node[engine, sloped] {$a^4_1$}}
    edge from parent node[engine, sloped] {$D$}
    };
    \node[below=1cm of 12] (15) {};
    \node[below=1cm of 13] (16) {};
    \node[below=1cm of 11] (27) {};
    \node[below=1cm of 17] (28) {};
    \node[below=1cm of 4] (29) {};
    \node[below=1cm of 5] (30) {};
    \node[below=1cm of 6] (31) {};
    \node[below=1cm of 8] (32) {};
    \node[below=1cm of 9] (33) {};
    \node[below=1cm of 10] (34) {};
    \node[below=1cm of 19] (35) {};
    \node[below=1cm of 20] (36) {};
    \node[below=1cm of 21] (37) {};
    \node[below=1cm of 23] (38) {};
    \node[below=1cm of 24] (39) {};
    \node[below=1cm of 25] (40) {};

    \path (12) -- node[auto=false]{{\LARGE \vdots}} (15);
    \path (13) -- node[auto=false]{{\LARGE \vdots}} (16);
    \path (2) -- node[auto=false]{{\LARGE \ldots}} (11);
    \path (3) -- node[auto=false]{{\LARGE \ldots}} (7);
    \path (11) -- node[auto=false]{{\LARGE \vdots}} (27);
    \path (18) -- node[auto=false]{{\LARGE \ldots}} (22);
    \path (17) -- node[auto=false]{{\LARGE \vdots}} (28);
    \path (4) -- node[auto=false]{{\LARGE \vdots}} (29);
    \path (5) -- node[auto=false]{{\LARGE \vdots}} (30);
    \path (6) -- node[auto=false]{{\LARGE \vdots}} (31);
    \path (8) -- node[auto=false]{{\LARGE \vdots}} (32);
    \path (9) -- node[auto=false]{{\LARGE \vdots}} (33);
    \path (10) -- node[auto=false]{{\LARGE \vdots}} (34);
    \path (19) -- node[auto=false]{{\LARGE \vdots}} (35);
    \path (20) -- node[auto=false]{{\LARGE \vdots}} (36);
    \path (21) -- node[auto=false]{{\LARGE \vdots}} (37);
    \path (23) -- node[auto=false]{{\LARGE \vdots}} (38);
    \path (24) -- node[auto=false]{{\LARGE \vdots}} (39);
    \path (25) -- node[auto=false]{{\LARGE \vdots}} (40);
    
\end{tikzpicture}
\end{subfigure}
\begin{subfigure}[t]{0.9\textwidth}
\centering
%\vspace{0.2em}
\fbox{\begin{tabular}{rr}
\small{\textcolor{gg_chance}{\TriangleDown}} & \footnotesize Chance Nodes \\
\small{\textcolor{gg_player1}{\CircleSolid} \textcolor{gg_player1_1}{\CircleSolid} \textcolor{gg_player1_2}{\CircleSolid} \textcolor{gg_player1_3}{\CircleSolid} \textcolor{gg_player1_4}{\CircleSolid} \textcolor{gg_player1_5}{\CircleSolid} \textcolor{gg_player1_6}{\CircleSolid} \textcolor{gg_player1_7}{\CircleSolid}} \\ 
\small{\textcolor{gg_player1_8}{\CircleSolid} \textcolor{gg_player1_9}{\CircleSolid} \textcolor{gg_player1_10}{\CircleSolid} \textcolor{gg_player1_11}{\CircleSolid} \textcolor{gg_player1_12}{\CircleSolid} \textcolor{gg_player1_13}{\CircleSolid} \textcolor{gg_player1_14}{\CircleSolid} \textcolor{gg_player1_15}{\CircleSolid}} & \footnotesize Player 1 Infosets \\
\small{\textcolor{gg_player2}{\DiamondSolid} \textcolor{gg_player2_1}{\DiamondSolid}} & \footnotesize Player 2 Infosets
\end{tabular}}
    \end{subfigure}
    \caption{Extensive-form representation of the first round of \gengoof{4}, an instance of \gengoof  where the number of stochastic outcomes at the start of the game is $K=4$, and the total number of game rounds is $3$.}
    \label{fig:gengoof_chap3}
\end{figure}



\section{Illustrations of Controlled Empirical Game Tree Expansion With New Best Responses}\label{app:te_psro_snapshots}
\input{app_te_psro_snapshots}

\newpage
\section{GBI Pseudocode and Omitted Subroutines of \textsc{ComputeSPE}}\label{app:SPE}
\input{app_subroutines}

\newpage
\section{Omitted Proofs of Correctness and Runtime for \textsc{ComputeSPE}}\label{app:proofs}
\input{app_omitted_proofs}

\newpage
\section{Space and Runtime Requirements for Experiments}\label{app:great_lakes}
\input{app_experiment_reqs}

\newpage
\section{Omitted Experimental Results and Plots}\label{app:expts}
\input{app_omitted_plots}

% \newpage
\section{Experimental Details for Best Response/DQN}\label{app:dqn}
\input{app_dqn}
%\fi

\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

