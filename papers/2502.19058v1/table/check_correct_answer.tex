\begin{table*}[htbp]
  \centering
  \caption{Error Detection and Error Type Detection in Answers: The best model for each task is highlighted in \textcolor{red}{red}.}
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{cccccccccc}
    \toprule
    \multirow{3}{*}{\textbf{Models}} & \multicolumn{4}{c}{\textbf{Error Detection}} &  \multicolumn{4}{c}{\textbf{Error Type Detection}} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9}
    & \multicolumn{2}{c}{\textbf{GSM8K}} & \multicolumn{2}{c}{\textbf{MATH}} & \multicolumn{2}{c}{\textbf{GSM8K}} & \multicolumn{2}{c}{\textbf{MATH}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    & \textbf{Acc} & \textbf{F1-Score} & \textbf{Acc} & \textbf{F1-Score} & \textbf{Acc} & \textbf{Macro-F1} & \textbf{Acc} & \textbf{Macro-F1} \\
    \midrule
    \multicolumn{9}{c}{\textbf{closed-source Lager Language Models}} \\
    \midrule
    GPT-4o & 80.75 & 88.02 & 77.25 &\textcolor{red}{82.97} & 59.51 & 46.93  & 57.49 & 42.11 \\
    o1-mini & 76.88 & 84.92 & 76.75 & 80.72 & 56.44 & 46.78 & 56.60 & 46.25\\
    o1-preview & 75.38 & 83.57 & \textcolor{red}{77.33} & 81.19 & \textcolor{red}{62.58} & \textcolor{red}{53.73} & 56.15 & 45.17\\
    Claude-3-5-sonnet & 73.12 & 81.98 & 73.83 & 78.84 & 60.12 & 48.17 & 56.38 & 38.21\\
    Gemini & 78.12 & 85.88 & \textcolor{red}{77.33} & 81.32 & 60.12 & 46.46 & \textcolor{red}{59.28} & 46.41\\
    \midrule
    \multicolumn{9}{c}{\textbf{open-source Lager Language Models}} \\
    \midrule
    Llama-3.1-8B-Instruct & 63.25 & 73.80  & 62.83 & 67.25 & 45.40 & 40.39  & 49.22 & 36.40 \\
    Llama-3.1-70B-Instruct & 76.62 & 84.93 & 75.67 &  81.01 & 54.60 & 48.93 & 55.93 & 45.87 \\
    Llama-3.3-70B-Instruct & 79.25 & 86.99 & 76.17 &  82.01 & 57.67 & 51.28 & 58.61 & \textcolor{red}{46.84} \\
    Qwen2.5-7B-Instruct & 81.50 & 88.91  &  73.17  &80.90 & 46.01 & 37.10  & 49.66 & 36.61 \\
    Qwen2.5-Math-7B-Instruct & 77.88 & 86.14 &  71.00 &  78.30 & 38.04 & 27.38  & 43.85 & 32.26\\
    Qwen2.5-72B-Instruct&\textcolor{red}{82.37} & \textcolor{red}{89.23} & 75.08& 82.06 & 50.92  & 47.85   & 55.26 & 45.22\\
    Qwen2.5-Math-72B-Instruct & 79.13 & 87.18  &  71.67 &79.09 & 44.79 & 37.44  & 50.78 & 42.45\\
    QwQ-32B-Preview & 49.75 & 58.39 & 53.00 & 51.63 & 52.76 & 41.31 & 54.59 & 45.29\\
    DeepSeek-R1-Distill-Qwen-7B & 76.38 & 85.01  & 72.58 & 78.14 & 53.37 & 45.63  & 52.35 & 44.06 \\
    DeepSeek-R1-Distill-Qwen-32B & 78.25 & 86.26  & 76.92  & 81.45 & 58.90 & 51.70  & 57.27 & 46.73\\
    DeepSeek-R1 & 76.75 & 84.78 & 76.33  & 80.76  & 57.06 & 36.36 & 57.72 & 43.37\\
    \midrule
    \multicolumn{9}{c}{\textbf{open-source Process Reward Models}} \\
    \midrule
    Math-Shepherd-PRM-7B & 71.88 & 81.04 & 68.92 & 74.75 & - & - & - & -\\
    Qwen2.5-Math-PRM-7B & 76.38 & 84.79 & 75.00 & 79.76 & - & - & - & -\\
    Skywork-PRM-7B & 54.00 & 61.98 & 58.17 & 54.53 & - & - & - & -\\
    % \midrule
    % Vote&&\textbf{82.50} & \textbf{89.33} & 76.17& 82.79 \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:check_correct_answer}%
  \vspace{-2mm}
\end{table*}%

