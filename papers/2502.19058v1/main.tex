%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
%\documentclass[sigconf]{acmart}
\documentclass[sigconf]{acmart}


%%
%% \BibTeX command to typeset BibTeX logo in the docs
% \AtBeginDocument{%
%   \providecommand\BibTeX{{%
%     Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{XX}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%
\let\Bbbk\relax\usepackage{amssymb}
\usepackage{amsmath} 
\usepackage{booktabs} 
\usepackage{array}
\usepackage{graphicx} 
\usepackage{multirow}
\usepackage{color}
%\usepackage{ulem}
\usepackage{makecell} 
\usepackage{float}
\usepackage{subcaption}
\usepackage{colortbl}
\usepackage{placeins}
%\usepackage{algorithm}
%\usepackage{algorithmic}
\usepackage{comment}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}} % checkmark
\newcommand{\xmark}{\ding{55}} % cross

\let\algorithm\relax
\let\endalgorithm\relax
\usepackage[linesnumbered,vlined,ruled,noend]{algorithm2e}
\usepackage{algorithm2e}
\usepackage{amssymb}
\usepackage{float} % Add this to the preamble
%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{MathClean: A Benchmark for Synthetic Mathematical Data Cleaning}

\author{Hao Liang$^\dagger$}
\affiliation{%
  \institution{Peking University}
  \city{Beijing}
  \country{China}}
\email{hao.liang@stu.pku.edu.cn}

\author{Meiyi Qiang$^\dagger$}
\affiliation{%
  \institution{Beijing Institute of Technology}
  \city{Beijing}
  \country{China}}
\email{1120213065@bit.edu.cn}

\author{Yuying Li$^\dagger$}
\affiliation{%
  \institution{Beijing Institute of Technology}
  \city{Beijing}
  \country{China}}
\email{liyuying@bit.edu.cn}

\author{Zefeng He}
\affiliation{%
  \institution{Nanjing University}
  \city{Nanjing}
  \country{China}}
\email{221250021@smail.nju.edu.cn}

\author{Yongzhen Guo}
\affiliation{%
  \institution{Ant Group}
  \city{Beijing}
  \country{China}}
\email{yongzhen.gyz@antgroup.com}

\author{Zhengzhou Zhu}
\affiliation{%
  \institution{Peking University}
  \city{Beijing}
  \country{China}}
\email{zhuzz@pku.edu.cn}

\author{Wentao Zhang$^*$}
\affiliation{%
  \institution{Peking University}
  \city{Beijing}
  \country{China}}
\email{wentao.zhang@pku.edu.cn}

\author{Bin Cui$^*$}
\affiliation{%
  \institution{Peking University}
  \city{Beijing}
  \country{China}}
\email{bin.cui@pku.edu.cn}

% \author{Zheng Liu$^{\dagger\spadesuit}$, Hao Liang$^{\dagger\spadesuit}$, Wentao Xiong$^{\spadesuit}$, Qinhan Yu$^{\spadesuit}$, Linzhuang Sun$^\diamondsuit$, Chong Chen$^\heartsuit$, Conghui He$^\clubsuit$, Wentao Zhang$^\spadesuit$, Bin Cui$^\spadesuit$}
% % \author{Yangyu Tao$^\ddagger$, Bin Cui$^\dagger$}
% \affiliation{
% $^\spadesuit$Peking University~~~~~$^\heartsuit$Huawei Cloud BU~~~~~$^\clubsuit$Shanghai AI Laboratory~~~~~$^\diamondsuit$University of Chinese Academy of Sciences
%  }
% \affiliation{
% $^\dagger$lz030515123@gmail.com, $^\dagger$hao.liang@stu.pku.edu.cn, \{bin.cui, wentao.zhang\}@pku.edu.cn
% }
%\author{Anonymous Author(s)}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
With the rapid development of large language models (LLMs), the quality of training data has become crucial. Among the various types of training data, mathematical data plays a key role in enabling LLMs to acquire strong reasoning abilities. While high-quality open-source data is important, it is often insufficient for pre-training, necessitating the addition of synthetic math problems. However, synthetic math questions and answers can introduce inaccuracies, which may degrade both the training data and web data. Therefore, an effective method for cleaning synthetic math data is essential. In this paper, we propose the MathClean benchmark to evaluate the effectiveness of math data cleaning models. The MathClean benchmark consists of 2,000 correct questions and 2,000 erroneous questions with additional 2,000 correct and erroneous answers sourced from augmented data based on GSM8K and MATH. Moreover, we also annotate error types for each question or answer, since it can assess whether models can correctly identify the error categories for future improvements. Finally, we present comprehensive evaluations using state-of-the-art (SOTA) models. Our results demonstrate that even strong models like GPT-o1 and DeepSeek-R1 perform poorly on this benchmark, highlighting the utility of MathClean. Our code and data is available at \url{https://github.com/YuYingLi0/MathClean}.
\end{abstract}
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Mathematical Data, Synthetic Data Cleaning Benchmark}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle
\footnotetext{$\dagger$: Equal Contribution.}
\footnotetext{$*$: Corresponding Authors}
\vspace{-4mm}
\section{Introduction}
LLMs have demonstrated exceptional performance across a wide range of tasks in various domains~\cite{chatgpt, llama}. It has been established that data plays a crucial role in the success of LLMs~\cite{yang2024qwen2, bai2024survey, abdin2024phi}. Recently, several studies have focused on data cleaning to improve LLM training~\cite{du2023mods, he2020deberta, chen2023alpagasus, xu2023rethinking} and achieved success in training LLMs.
\begin{figure*}[htbp]
    \centering 
    \makebox[\textwidth]{\includegraphics[width=1.00\textwidth]{figures/cover.pdf}}
    \caption{Overview of MathClean benchmark. MathClean proposed challenges in detecting errors in mathematical Questions and Answers, as well as in identifying the specific error types within these mathematical problems.}
    \label{Fig.cover}
    \vspace{-4mm}
\end{figure*}

Among the various types of data for LLMs, mathematical data is one of the most crucial, as it enhances a model’s reasoning capabilities~\cite{yang2024qwen2, guo2025deepseek}. However, collecting large amount of mathematical data presents several challenges: (1) High-quality MathQA data is scarce and often requires data generation~\cite{zhou2024jiuzhang3, toshniwal2024openmathinstruct}. (2) Ensuring the correctness of synthetic mathematical data is challenging~\cite{toshniwal2024openmathinstruct}. (3) Evaluating the correctness of mathematical data remains a complex task~\cite{song2025prmbench,zheng2024processbench,zhou2024your}. To address these challenges, several studies have proposed output reward models (ORMs)~\cite{zhang2024generative} and process reward models (PRMs)~\cite{skyworkopeno12024, wang2024math, song2025prmbench} to assess the correctness of synthetic answers. To evaluate the effectiveness of these ORMs and PRMs, researchers in our community have developed ORM and PRM benchmarks~\cite{song2025prmbench, zheng2024processbench, zhou2024your}. These benchmarks primarily aim to enhance PRM performance, thereby improving mathematical reasoning capabilities. Rather than focusing on inference, our work prioritizes mathematical training data cleaning. We establish the following two tasks as our key objectives for improvement:

\textbf{Correctness of Questions and Answers.}
Recent advancements in synthetic mathematical training data have been significant~\cite{zhou2024jiuzhang3, toshniwal2024openmathinstruct}. OpenMathInstruct1 and 2~\cite{toshniwal2024openmathinstruct} have generated over 14 million QA pairs based on GSM8K and MATH. However, it is reported that approximately 50\% of the generated QA pairs are incorrect. Moreover, OpenMathInstruct1 and 2~\cite{toshniwal2024openmathinstruct}, Given the large volume of data, human evaluation is prohibitively costly. As a result, LLMs are needed to assess the correctness of synthetic math data. Therefore, it is crucial to develop a benchmark for evaluating the correctness of synthetic mathematical problems.

\textbf{Diverse Error Types.}
As noted earlier, LLMs must evaluate the correctness of synthetic math data. Additionally, LLMs need to identify the types of errors in synthetic math problems to facilitate future improvements. However, when determining whether an LLM truly understands the correctness of a question or answer, it is necessary to create a benchmark that incorporates a variety of error types to evaluate the model's ability to recognize different error categories.

To address these two aspects, we introduce \textbf{MathClean}, a benchmark designed to assess models' ability to (1) distinguish between correct and erroneous mathematical data and (2) identify the error type in erroneous questions or answers. Figure~\ref{Fig.cover} present an overview of the error detection and error type detection in the MathClean benchmark. To obtain a more diverse set of questions and answers, we proposed 10 error augmentation prompts and 16 diversity augmentation prompts. Then, we include human annotations to ensure the quality of MathClean. The MathClean benchmark consists of 2,000 correctly formulated questions and 2,000 erroneous ones. Detecting the correctness of questions is crucial, as synthetic math questions cannot yield correct answers if they are inherently flawed. Additionally, it includes 2,000 correct and erroneous answers, specifically designed to assess whether models can accurately identify valid answers corresponding to correct questions. Furthermore, each question and answer is annotated with an error category, posing additional challenges for models. Accurate identification of these categories can guide improvements in the quality of error-laden mathematical data for future use.

The core contributions are summarized as follows:
\begin{itemize}
    \item \textbf{New Benchmark:} We introduce MathClean, a new benchmark designed for cleaning mathematical QA data. Our benchmark comprises 2,000 correct and 2,000 erroneous questions. Additionally, it includes 2,000 correct and erroneous answers. Each erroneous question and answer is annotated with an error type, adding further challenges. 
    %Moreover, identifying these error types can guide improvements in the quality of erroneous mathematical data.

    \item \textbf{Diverse Math Data Synthesis Method:} Following the methodology of MuggleMath, we developed 10 types of question augmentation prompts for error synthesis. Additionally, we created 16 types of prompts—11 more than MuggleMath—to enable more diverse question augmentation. Based on these prompts, we synthesized questions and corresponding answers, ensuring the diversity of our MathClean benchmark. Finally, each augmented question was reviewed by human annotators to ensure data quality.
    
    \item \textbf{Diverse Error Categories:} In addition to the challenges in detecting erroneous questions and answers, we propose a more difficult task: classifying error questions and answers into different categories. This is intended to assess whether models can identify errors in questions and answers, aiding in the future improvement of synthetic data.

    \item \textbf{Comprehensive Evaluation:} We conduct extensive experiments on the MathClean benchmark, evaluating five widely used closed-source LLMs, and eleven open-source LLMs with sizes ranging from 7B to 72B. Moreover, we evaluate our model on three SOTA PRMs. 
    \item \textbf{New Challenges:} Through case studies, we demonstrate that the MathClean benchmark introduces new challenges for LLMs, even for the SOTA models like GPT-o1.
\end{itemize}
\section{Related Work}
\subsection{Mathematical Benchmarks}
% Recent research has seen significant advancements in mathematical reasoning benchmarks aimed at evaluating mathematical abilities. In this summary, we review both pure text and multimodal math benchmarks.
%gao2024omni，glazer2024frontiermath
GSM8K~\cite{cobbe2021training} is a dataset from OpenAI that includes 8.5K high-quality elementary school math word problems, each requiring 2 to 8 steps to solve. These problems primarily involve basic arithmetic operations such as addition, subtraction, multiplication, and division. MATH~\cite{hendrycks2021measuring} offers a dataset of 12,500 problems sourced from high school math competitions. SuperCLUE-Math~\cite{xu2024superclue} is a Chinese benchmark for multi-step reasoning in mathematics, containing over 2,000 problems that require multi-step reasoning and offer natural language solutions. MathBench~\cite{liu2024mathbench} includes 3,709 math problems ranging from basic arithmetic to college-level questions, covering multiple difficulty levels. As LLMs have progressively achieved extremely high accuracy on existing mainstream benchmarks (e.g., MATH, GSM8K), it is critical to propose benchmarks that are more challenging for LLMs. Omni-MATH~\cite{gao2024omni} contains 4428 Olympiad-level math problems sourced from international competitions with rigorous manual annotations. All these problems are categorized into 33 sub-domains in detail, spanning 10 difficulty levels. FrontierMath~\cite{glazer2024frontiermath} is a mathematical benchmark constructed by leading mathematicians, aiming to push AI towards expert-level capabilities. Even the best models solving less than 2\% of problems. It comprises hundreds of purely original, challenging, expert-level mathematical problems covering a broad spectrum of modern mathematical subjects, minimizing the possibility of data contamination. PRMBench~\cite{song2025prmbench} is a benchmark for evaluating the error detection capabilities of PRMs from three perspectives: simplicity, soundness, and sensitivity. It consists of 6,216 questions and 83,456 step-level labels. Similarly, ProcessBench~\cite{zheng2024processbench} consists of 3,400 competition- and olympiad-level math problems that measure the ability to identify the earliest wrong steps in step-level mathematical reasoning. 
%MATHTRAP~\cite{zhao2024exploring} introduces five categories of logical traps, containing a public subset of 1000 problem triples and a private subset of 155 problem triples, focusing on the logical traps.
%PRMBench, Qwen2.5 evaluation 数据集
% All these benchmarks focus exclusively on text-based mathematical tasks. They are designed to evaluate the mathematical capabilities of LLMs through specialized problem sets.
% \paragraph{Multimodal math benchmarks.} MathVista~\cite{lu2023mathvista} is a multimodal benchmark designed to evaluate mathematical reasoning in Visual Contexts. It consists of 6,141 examples, concentrating on symbolic-graphical combinatorial reasoning tasks. Similarly, MathVerse~\cite{zhang2024mathverse} is an all-around visual math benchmark, consisting of 2,612 math problems with diagrams. It is used to thoroughly evaluate whether Multimodal Large Language Models (MLLMs) can genuinely comprehend and utilize visual diagrams in mathematical reasoning.

\subsection{Mathematical LLMs}
Recently, with the emergence of GPT-o1-type models, numerous mathematical large models have been developed. In this paper, we summarize the commonly used mathematical large models.

The GPT series~\cite{radford2019language,brown2020language,ouyang2022training} represents a milestone AI model in humanity's journey toward Artificial General Intelligence (AGI), developed and released by OpenAI. The latest iteration, GPT-4~\cite{achiam2023gpt}, as a large-scale multimodal language model, has demonstrated professional-level proficiency in the field of mathematics. 
OpenAI o1-preview~\cite{chatgpt} integrates slow thinking into the model, designed to allow the model more time for thinking before responding, enabling it to learn from mistakes and refine questions like human cognition. It elevates the capabilities of large language models in complex reasoning and challenging mathematical tasks to a higher level, achieving a score of 83\% on a qualifying exam for the International Mathematics Olympiad (IMO). The Qwen2.5-Math series~\cite{yang2024qwen2} excel in mathematical reasoning. It supports solving mathematical problems in both Chinese and English through Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). 

The QwQ model~\cite{qwq-32b-preview} emphasizes the cultivation of AI's logical reasoning abilities, enabling the model to learn to think, question, and reflect, thereby deepening its understanding of mathematical knowledge and achieving breakthroughs in solving complex mathematical problems. DeepSeek-Math~\cite{shao2024deepseekmath} is an open-source mathematical reasoning model trained on the DeepSeekMath Corpus dataset, available in three 7B versions: base, instruction-tuned, and reinforcement learning. Its performance on the competition-level MATH benchmark approaches that of Gemini Ultra~\cite{team2023gemini} and GPT-4, supporting mathematical problem-solving, tool usage, and theorem proving.
As an open source model, DeepSeek-R1~\cite{guo2025deepseek} achieves impressive performance in mathematical and reasoning tasks which is comparable to OpenAI-o1 with less computational resources. It excels in complex reasoning tasks, specializing in complex mathematical reasoning and competition-level problem solving with detailed step-by-step solutions.

% 数据合成
\subsection{Mathematical Data Synthesis}

The demand for high-quality data in the field of LLMs has spurred the flourishing development of the data synthesis domain. Existing data synthesis approaches can be broadly categorized into two types: those based on large language model distillation and those based on Monte Carlo Tree Search (MCTS).

\textbf{LLM Based Distillation.} MetaMath~\cite{yu2023metamath} leverages GPT-3.5 Turbo models to rewrite existing mathematical problems from multiple perspectives, thereby generating the  MetaMathQA dataset. KPDDS~\cite{huang2024key} utilizes GPT-4 to extract the topics and Key Points from seed questions, processing and sampling them to synthesize new question-answer pairs. JiuZhang3.0~\cite{huang2024key} trains a specialized model for mathematical data synthesis, with the training and retraining datasets generated by GPT-4.

\textbf{MCTS Based.} This approach was first proposed in OpenAI's o1~\cite{chatgpt}. It significantly expands the search space of model outputs. Compared to direct distillation, it demonstrates superior performance in synthesizing datasets for step-by-step solutions to complex mathematical problems. In ReST-MCTS*~\cite{zhang2024rest}, process reward value is utilized to guide MCTS, ensuring the accuracy of the data reasoning process. Meanwhile, rStar~\cite{qi2024mutual} introduces a more extensive action space at each step of reasoning. LLaMA-Berry~\cite{zhang2024llama} implements SR-MCTS (Self-refine), where each leaf node represents a complete problem-solving state, and child nodes correspond to the criticizing and rewriting of parent nodes. Mulberry~\cite{yao2024mulberry} proposes CoMCTS, which leverages collective knowledge from multiple models during inference and constructs a multimodal dataset, Mulberry-260k, for training MLLMs.

% PRM ORM
\subsection{Reward Models}
In the Reinforcement Learning from Human Feedback (RLHF) or MCTS-based inference, Reward Models (RMs) are employed to assess and score the quality of model outputs, thereby guiding the optimization or reasoning path of LLMs. Reward models can be categorized into Process Reward Models (PRMs) and Outcome Reward Models (ORMs).

\textbf{Outcome Reward Models.} ORMs evaluate only the final mathematical results without considering the solution process. For instance, Qwen2.5-Math-RM-72B~\cite{zhang2025lessons}, released by the Qwen team, assigns a single score to each mathematical response.

\textbf{Process Reward Models.} PRMs are more fine-grained, focusing on whether each step of the reasoning path is logical and correct, providing step-level feedback and guidance signals. For example, Math-Shepherd-Mistral-PRM~\cite{wang2024math} is trained on an automatically constructed (rather than manually annotated) process supervision dataset, scoring each step of mathematical reasoning. MATHMinos-PRM~\cite{gao2024llm} introduces a novel two-stage training paradigm and incorporates step-wise natural language feedback labels. EurusPRM~\cite{cui2025process} utilize implicit PRM, where ORM is trained to evaluate response-level labels. Qwen2.5-Math-PRM~\cite{zhang2025lessons}, currently the SOTA PRM, proposes a consensus filtering mechanism combining Monte Carlo estimation and LLM-as-a-judge. Additionally, there are the Skywork-PRM series~\cite{skyworkopeno12024} and RLHFlow-PRM series~\cite{wei2024implementation} models. For more comprehensive LLM-as-a-Judge please refer to the LLM-as-a-Judge survey~\cite{gu2024survey}.

\section{Benchmark for Data Cleaning}

\begin{figure*}[h]
    \centering
    \includegraphics[width=1.00\textwidth]{figures/example.pdf}
    \caption{Two erroneous examples from the MathClean dataset are presented: a simple difficulty question with an unrealistic error and a challenging difficulty problem, which includes both the question and the answer, with a logic error.}
    \label{Fig.example}
    \vspace{-4mm}
\end{figure*}

In this section, we provide a comprehensive description of the construction process for the MathClean benchmark—a dataset comprising 4000 mathematical problems that is divided into a question dataset and an answer dataset. Each dataset is annotated to indicate both correctness and specific error types.

\subsection{Question Dataset Construction}
\subsubsection{Data Preparation}
We used GSM8K and MATH train, two widely recognized mathematical datasets, as seed data to generate mathematical questions with various error types. The problems generated using GSM8K as seed data are relatively easier and are classified as SIMPLE difficulty, while those generated using MATH train as seed data are considered more difficult and are classified as CHALLENGING difficulty. For question synthesis, we employed the open-source model Qwen2.5-72B-Instruct, known for its strong math performance. By carefully designing a diverse set of prompts, we guide the model to generate mathematical questions with the intended error types based on the seed data.

It is important to note that many synthetic questions concurrently exhibit multiple error types and causes. In this study, we begin by analyzing and categorizing four primary error types: expression errors, lack of conditionality, contradictions, and unrealistic scenarios. In order to ensure greater diversity in our dataset, we subsequently designed specific error conditions for each major error type. \textbf{Expression errors} include: (1) misinterpretation due to unclear references, (2) grammatical errors or awkward phrasing, (3) inclusion of redundant or irrelevant content and conditions, and (4) misuse or confusion of terminology. \textbf{Lack of conditions} include: (5) the absence of necessary diagrams or charts, and (6) the omission of essential conditions required to obtain the correct answer. \textbf{Contradictions} include: (7) the presence of mutually exclusive conditions, and (8) the solution is either nonexistent or meaningless. \textbf{Unrealistic scenarios} include: (9) only integer solutions are meaningful, but the conditions or results are non-integer, and (10) a question that violates common sense or fundamental natural laws. This classification provides a systematic framework for analyzing the concrete types of errors in synthetic mathematical questions.

We employ the Qwen2.5-72B-Instruct model to generate questions for each of the 10 error types discussed above. This approach ensures that each generated question contains only one error type, minimizing the complexity of mixing multiple error types within a single question. For each error type, we have iteratively optimized and designed a well-structured prompt, with an example provided for each type. The complete set of prompts is presented in Figure~\ref{Fig.error_question_prompt}.

\subsubsection{Data Annotation}
To ensure that our generated mathematical questions meet the expected requirements, we recruited graduate students majoring in science and engineering from top-tier universities to conduct detailed manual annotations of the data. Prior to annotation, all annotators underwent rigorous screening and assessment of their mathematical abilities. They were required to have university entrance mathematics examination scores above a predetermined baseline, ensuring they possessed sufficient mathematical problem-solving skills and attention to detail.

The annotation process consists of several steps. Annotators first verify whether a question is correct or incorrect, and if it is incorrect, they determine whether it contains a single error type or multiple types of errors. They then annotate the specific error types, which are categorized as: expression errors, lack of information, contradictions, and unrealistic scenarios. All annotation data is reviewed by a team of reviewers to ensure its accuracy. Annotators are required to maintain an accuracy rate of over 90\%.

We filtered math questions that contained only a single error type or where the second error type was not significant, and then decontaminated the filtered labeled dataset. The final problem dataset consists of subsets containing four different error types, combined in a specific ratio, and includes both simple and challenging difficulties. An example of erroneous question, along with the corresponding explanation, is provided in Figure~\ref{Fig.example}.

\subsection{Answer Dataset Construction}
\subsubsection{Data Synthesize}\label{sec:Data_Synthesize}
\begin{table*}[htbp]
  \centering
  \caption{Error Detection and Error Type Detection in Questions: The best model for each task is highlighted in \textcolor{red}{red}.}
  \resizebox{0.9\linewidth}{!}{
    \begin{tabular}{cccccccccc}
    \toprule
    \multirow{3}{*}{\textbf{Models}} & \multicolumn{4}{c}{\textbf{Error Detection}} &  \multicolumn{4}{c}{\textbf{Error Type Detection}} \\
    \cmidrule(lr){2-5} \cmidrule(lr){6-9}
    & \multicolumn{2}{c}{\textbf{GSM8K}} & \multicolumn{2}{c}{\textbf{MATH}} & \multicolumn{2}{c}{\textbf{GSM8K}} & \multicolumn{2}{c}{\textbf{MATH}} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
    & \textbf{Acc} & \textbf{F1-Score} & \textbf{Acc} & \textbf{F1-Score} & \textbf{Acc} & \textbf{Macro-F1} & \textbf{Acc} & \textbf{Macro-F1} \\
    \midrule
    \multicolumn{9}{c}{\textbf{closed-source Lager Language Models}} \\
    \midrule
    % \multirow{2}{*}{Simple Network}&MLP&  76.67 & 12.50  & 70.00 & 47.06 \\
    % &LSTM&  79.17 &  0.00  & 65.56 & 0.00 \\
    % \midrule
    GPT-4o & 72.44&76.38  &72.50&75.26 & \textcolor{red}{79.13}&\textcolor{red}{79.14}&74.00&73.90 \\
    o1-mini & 74.88 & 76.66 & \textcolor{red}{76.96} & \textcolor{red}{78.52} & 76.00 & 75.95 & 74.08 & 74.26\\
    o1-preview & 72.06 & 73.97 & 74.04 & 75.79 & 74.25 & 74.24 & 70.92 & 70.73\\
    Claude-3-5-sonnet & \textcolor{red}{76.31} & \textcolor{red}{78.33} & 75.91 & 78.25 & 74.88 & 74.90 & 70.67 & 70.85\\
    Gemini & 71.13 & 75.43 & 74.54 & 77.74 & 73.75 & 73.60 & 72.67 & 72.75\\
    \midrule
    \multicolumn{9}{c}{\textbf{open-source Lager Language Models}} \\
    \midrule
    Llama-3.1-8B-Instruct & 59.62 & 46.96 & 64.88 & 58.82 & 56.00 & 55.08 & 56.67 & 55.81 \\
    Llama-3.1-70B-Instruct & 69.94 & 72.50 & 69.13 &  70.35 & 69.94 & 72.50 & 72.92 & 73.00\\
    Llama-3.3-70B-Instruct & 71.37 & 76.42 & 69.71 & 74.01 & 75.38 & 75.24 & \textcolor{red}{77.83} & \textcolor{red}{77.90} \\
    Qwen2.5-7B-Instruct & 67.75 & 74.02 & 71.33 & 76.13 & 53.00 & 52.35 & 54.33 & 53.64 \\
    Qwen2.5-Math-7B-Instruct & 67.63 & 71.94 & 68.00 & 71.53 & 46.88 & 44.33 & 51.58 & 49.26\\
    Qwen2.5-72B-Instruct & 72.31 & 77.22 & 73.83 & 78.15 & 77.62 & 77.58 & 73.25 & 73.39\\
    Qwen2.5-Math-72B-Instruct & 68.00 & 74.01 & 68.87 & 73.78 & 67.37 & 67.35 & 65.58 & 65.36\\
    QwQ-32B-Preview & 57.81 & 40.53 & 55.50 & 34.88 & 67.00 & 66.85 & 64.00 & 63.78\\
    DeepSeek-R1-Distill-Qwen-7B & 62.38 & 59.16  & 65.25 & 63.61 & 65.38 & 65.14 & 65.25 & 64.67 \\
    DeepSeek-R1-Distill-Qwen-32B & 72.69 & 76.26 & 71.67 & 73.75 & 74.75 & 74.65 & 74.00 & 73.98\\
    DeepSeek-R1 & 71.25 & 72.46 & 74.50 & 75.94 & 70.37 & 70.21 & 70.75 & 70.50\\
    % \midrule
    % \multicolumn{9}{c}{\textbf{open-source lager language models}} \\
    % \midrule
    % Vote&&\textbf{82.50} & \textbf{89.33} & 76.17& 82.79 \\
    \bottomrule
    \end{tabular}%
    }
  \label{tab:check_correct_questions}%
  \vspace{-2mm}
\end{table*}%
To improve the quality of the answer dataset, it is essential to enhance the diversity of the corresponding questions. To this end, we developed 16 unique question rewriting techniques aimed at maximizing the coverage of mathematical domains within our dataset. This data augmentation step generates novel and diverse questions. Moreover, since all questions in MathClean are derived from newly synthesized data, we effectively mitigate common data contamination issues.

The sixteen data augmentation strategies we have devised are as follows: (1) modification of numerical values, scores, and percentages;   (2) incorporation of algebraic expressions and variables;   (3) adding time variation;   (4) introducing cumulative or increasing-decreasing conditions; (5) having special events or contingencies; (6) having events conform to the laws of statistics and probabilistic analysis; (7) adding error rates and rework; (8) setting cost budgets or resource constraints; (9) introducing multivariate comparisons; (10) establishing conditional loops or recursion; (11) setting goals or schedule requirements; (12) including conditions with multiple outcomes; (13) using non-uniform units; (14) inferring conditions based on the results; (15) storytelling narratives or nested scenarios; and (16) synthesizing interdisciplinary knowledge. Detailed rewriting steps and corresponding examples are provided in Figure~\ref{Fig.question_extension_prompt}.

In order to broaden the answer dataset to include a wider array of error types and a greater range of error severity, we employed a series of models with markedly different performance levels during this process. These included Qwen2.5-Math-72B, Qwen2.5-72B, Llama-3.1-8B-Instruct, and Qwen2.5-Math-1.5B-Instruct. We applied these models to rephrase both simple and challenging problem sets, thereby enabling a comprehensive evaluation across different difficulty levels. In addition, we combine Chain-of-Thought (CoT) and Program-of-Thought (PoT) strategies when generating answers to provide more detailed steps.

\subsubsection{Data Annotation}
Similar to the question dataset, we recruited graduate students with advanced mathematical problem-solving skills to annotate and rigorously review the answer dataset. During this process, annotators were allowed to use various computational tools and reference materials flexibly to ensure the highest accuracy in annotation. 

The annotation process was divided into three steps based on error level and type. First, evaluate the correctness of the question. Second, for questions that are correct, assess the correctness of the corresponding answer. Third, for questions with erroneous answers, label the specific error type and provide a detailed description of the error cause to ensure high annotation accuracy during review. \textbf{Expression errors} included incomplete answers, redundant steps, and grammatically incorrect responses. \textbf{Logic errors} included the use of incorrect mathematical methods, flawed problem-solving approaches, or failure to consider all cases of the problem. \textbf{Computing errors} included errors in the PoT steps and incorrect numerical approximations.
\begin{figure}
\centering 
\includegraphics[width=0.4\textwidth]{figures/question_dataset.pdf} 
\caption{Proportion of different difficulty levels and error types in the Question dataset of MathClean.}
\label{Fig.question_dataset}
\vspace{-4mm}
\end{figure}
Due to the inability to guarantee that all 16 rewriting methods in Section \ref{sec:Data_Synthesize} are applicable to every question scenario, we decontaminated and filtered the annotated data by removing questions that were incorrect or unreasonable. After filtering, we retained only those questions that were correct and had answers that were either correct or exhibited a single or typical error type, thereby forming the answer dataset. An example of an erroneous answer from the dataset is presented in Figure~\ref{Fig.example}.

\subsection{Statistics of MathClean}
\begin{figure}
\centering 
\includegraphics[width=0.4\textwidth]{figures/answer_dataset.pdf} 
\caption{Proportion of different difficulty levels and error types in the Answer dataset of MathClean.}
\label{Fig.answer_dataset}
\vspace{-4mm}
\end{figure}
\subsubsection{Question Dataset} The dataset comprises 2,000 erroneous questions, categorized into four types: expression errors, lack of conditions, contradictions, and unrealistic scenarios, with 500 instances per category. For each error type, 200 seed questions are sourced from GSM8K, representing a simple difficulty level, while the remaining 300 are drawn from MATH train, reflecting a challenging difficulty level comparable to high school mathematics competitions, as illustrated in Figure~\ref{Fig.question_dataset}.

\subsubsection{Answer Dataset} 
% (short)The answer dataset contains 2,000 correct questions paired with corresponding answers, with 40\% categorized as simple and 60\% as challenging. Challenging answers show a higher proportion of errors than simple ones; overall, 30.5\% of the answers are erroneous.

The answer dataset consists of 2,000 correct questions paired with their corresponding answers, with 40\% categorized as simple difficulty and 60\% as challenging difficulty. Challenging answers contain a higher proportion of errors compared to simple ones; overall, 30.5\% of the 2000 answers are erroneous.
 
\input{table/statistics_of_answer_dataset}

% (short)Incorrect answers are divided into three types: logic error, computing error, and expression error. Unlike the question dataset, the distribution of these error types largely corresponds to the error probabilities that tend to occur in the model’s mathematical reasoning. Logic errors are the most common, accounting for 52\%, followed by computing errors at 38\%, while expression errors are the least frequent, comprising approximately 10\%. The distribution of error types across the answers is illustrated in Figure~\ref{Fig.answer_dataset}. The number of answers corresponding to each error extent and type is presented in Table~\ref{Table.answer_dataset_number}.

For error types, incorrect answers are classified as logic, computing, or expression errors. The proportion of each error type closely corresponds to the frequency with which the model tends to commit errors in its mathematical reasoning. Logic errors are the most common (52\%), followed by computing errors (38\%), and expression errors (10\%). Figure~\ref{Fig.answer_dataset} illustrates this distribution, and Table~\ref{Table.answer_dataset_number} details the number of two difficulty answers for each error extent and type.

\section{Experiments}
\input{table/check_correct_answer}
\subsection{Experimental Setting}

\subsubsection{Evaluation Metrics}

The experiment is divided into four subsections: question error detection, answer error detection, question error type detection, and answer error type detection. Error detection can be regarded as a binary classification task, while error type detection can be viewed as a multi-class classification task.

For error detection, we selected Accuracy (Acc) and F1 score as evaluation metrics. Accuracy is a traditional metric for assessing classification tasks, representing the proportion of correctly predicted samples out of the total number of samples. F1 score is the harmonic mean of Precision and Recall, providing a balanced measure of a model's performance. 

For error type detection, we selected both accuracy and the Macro-F1 score as our evaluation metrics. The Macro-F1 score is particularly appropriate for multi-class classification problems, especially in scenarios where class imbalance is evident. Specifically, for an m-class classification problem, the Macro-F1 score is calculated by computing the F1 score for each individual class and subsequently taking the average of these scores as the final Macro-F1 score.
%（为了调整排版的空白，需要修改篇幅所以改了表达） For error type detection, we chose accuracy and Macro-F1 score as evaluation metrics. The Macro-F1 score is particularly suitable for multi-class classification problems, especially in cases of class imbalance. For an m-class classification problem, the Macro-F1 score calculates the F1 score for each class and then takes the average as the final Macro-F1 score.
\subsubsection{Models}
To comprehensively demonstrate the performance of various models on MathClean benchmark, we evaluate the following three categories of models:

\textbf{Closed-Source Large Language Models.} Closed-Source large language models, represented by the GPT series and the o1 series, have shown exceptional performance in mathematical reasoning tasks. In our experiment, we choose GPT-4o~\cite{achiam2023gpt}, o1-preview~\cite{chatgpt}, o1-mini~\cite{chatgpt}, Gemini~\cite{team2023gemini}, and Claude-3-5-sonnet.

\textbf{Open-Source Large Language Models.} Although there remains a slight performance gap between open-source models and closed-source models, many open-source large language models have also achieved excellent results in the mathematical domain. We included the Llama series like Llama-3.1-8B-Instruct, Llama-3.1-70B-Instruct and Llama-3.3-70B-Instruct~\cite{dubey2024llama}, the Qwen series such as Qwen2.5-7B-Instruct, Qwen2.5-Math-7B-Instruct, Qwen2.5-72B-Instruct~\cite{yang2024qwen2} and QwQ-32B-Preview~\cite{qwq-32b-preview}, as well as the DeepSeek series like DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1~\cite{guo2025deepseek}.

\textbf{Open-Source Process Reward Models.} Process Reward Models are designed to evaluate the intermediate steps of a model's reasoning process, making them suitable only for answer error detection tasks. Compared to ORMs, PRMs place greater emphasis on the logical coherence of the reasoning path. Inspired by the idea of training ORM as an implicit PRM~\cite{cui2025process}, we treat the entire reasoning process as a single step, with the PRM evaluating scores for correctness and logic of entire answer. Based on the scores obtained, we set a specific threshold to determine whether the answer is correct or not. The PRMs utilized in this experiment include Math-Shepherd-Mistral-7B-PRM~\cite{wang2024math}, Skywork-PRM-7B~\cite{skyworkopeno12024}, and Qwen2.5-Math-PRM-7B~\cite{zhang2025lessons}.

\begin{figure*}[htbp]
    \centering 
    \makebox[\textwidth]{\includegraphics[width=1.00\textwidth]{figures/case_study.pdf}}
    \caption{Failure case of GPT-o1 in the detection of question correctness in the MathClean benchmark, with explanation. The model fails to recognize the contradiction in the given conditions.}
    \label{Fig.case_study}
    \vspace{-4mm}
\end{figure*}
\subsubsection{Settings}
We utilized the vLLM framework for inference, setting the maximum token parameter to 4000. For generating model outputs, the temperature parameter was set to 0.7, and the top-p parameter to 0.95. The prompts specifically designed for this experiment are shown in Figure~\ref{Fig.experiment_prompt}. In cases where the generated text outputs exhibited formatting errors, the corresponding result was randomly assigned using the random seed 42.

All experiments were conducted on a machine running Ubuntu 22.04, equipped with 8 NVIDIA H800 GPUs, a 192-core CPU, and a total of 960 GB of memory.

\subsection{Error Detection for Questions}\label{sec:Experiment_Question}

% （shortest）As shown in Table \ref{tab:check_correct_questions}, closed-source models achieve strong performance. Open-source models, such as Qwen2.5-72B-Instruct, Llama-3.3-70B-Instruct, and the DeepSeek-R1 series, also perform well, with the DeepSeek series excelling due to its robust Long CoT reasoning ability. Although a few models show impressive results, overall performance on MathClean remains suboptimal. Most models score above 70 in question error detection tasks; however, their performance drops to around 50 in question error type detection tasks, indicating weak detection capabilities. This suggests that current models struggle to identify errors in questions, highlighting a significant opportunity for improvement. Thus, MathClean remains a challenging benchmark for current models.

% (short)As shown in Table 1, closed-source models achieve strong performance. In addition, open-source models, such as Qwen2.5-72B-Instruct, Llama-3.3-70B-Instruct, and the DeepSeek-R1 series, also perform well, with the DeepSeek series notably excelling due to its robust Long CoT reasoning ability. Although a few models show impressive results, the overall performance on MathClean remains suboptimal. Most models score above 70 in question error detection tasks; however, their performance drops to around 50 in question error type detection tasks, indicating relatively weak detection capabilities. These findings suggest that current models still struggle with identifying errors in questions, thereby highlighting a significant opportunity for further improvement. Thus, MathClean remains a challenging benchmark for current models.

As shown in Table \ref{tab:check_correct_questions}, closed-source models achieve strong performance. Open-source models, such as Qwen2.5-72B-Instruct, Llama-3.3-70B-Instruct, and the DeepSeek-R1 series, also achieve good performance. Notably, the DeepSeek series excels due to its strong Long CoT reasoning ability. While a few strong models demonstrate impressive performance on our benchmark, the overall performance of these models on the MathClean benchmark remains suboptimal. On the one hand, most models score above 70 in question error detection tasks; however, their performance drops significantly to around 50 in question error type detection tasks, indicating weak question error detection capabilities. This suggests that existing models struggle to identify errors in questions, highlighting a substantial opportunity for improvement, particularly in question error type detection. Thus, MathClean remains a challenging benchmark for current models.

\subsection{Error Detection for Answers}\label{sec:Experiment_Answers}
As shown in Table \ref{tab:check_correct_answer}, closed-source models continue to deliver strong performance. Some open-source models, such as Qwen2.5-72B-Instruct and Llama-3.3-70B-Instruct, exhibit comparable performance in answer error and type detection tasks, highlighting the potential of open-source models. Additionally, slow-thinking models like GPT-o1-preview and DeepSeek-R1 excel in novel tasks, such as error type detection, but fail to achieve SOTA performance in error detection. We attribute this to their ability to provide human-like reasoning; however, they occasionally suffer from overthinking in error detection tasks. Contrary to expectations, Qwen2.5-7B-Instruct outperforms Qwen2.5-Math-7B-Instruct, likely due to the latter's ineffective instruction-following. Furthermore, open-source PRMs, particularly Qwen2.5-Math-PRM-7B, perform similarly to general-purpose LLMs, such as Llama-3.1-70B-Instruct, in answer error detection, although they still lag behind Qwen2.5-7B-Instruct. This suggests that while PRMs are effective, further improvements are necessary.

Moreover, as shown in Table \ref{tab:check_correct_answer}, SOTA models like o1-preview and DeepSeek-R1 perform poorly (less than 80 points in error detection and less than 70 points in error type detection) on our MathClean benchmark, underscoring MathClean is very challenging.

\subsection{Case Study}
In Sections \ref{sec:Experiment_Question} and \ref{sec:Experiment_Answers}, we found that MathClean poses a significant challenge to models. To further analyze this, we examine a failure case of correctness detection by GPT-o1 on the question dataset as an representative example.

As shown in Figure \ref{Fig.case_study}, this question in MathClean is incorrect because it incorporates multiple geometric conditions, one of which contradicts the information deduced from the others.   During detection, GPT-o1 mistakenly concludes that the condition contributes to solving the problem and classifies the question as “correct.”  In reality, the model fails to further verify the numerical consistency of different conditions, leading to an incorrect judgment. This demonstration indicates that when evaluated on the challenging MathClean benchmark, models with strong mathematical reasoning abilities, such as GPT-o1, also reveal subtle, hard-to-detect flaws in their mathematical logic.

In Appendix~\ref{Appendix.example}, we provide additional error cases related to the correctness and error type detection of questions and answers, in order to further analyze in depth the reasons why LLM fails to achieve excellent performance on the MathClean benchmark.

\section{Conclusion}

In this paper, we introduced the MathClean benchmark as a critical tool for evaluating the overall effectiveness of math data-cleaning models. As the development of LLMs continues to advance, the quality of the mathematical training data has become increasingly important, particularly given the limitations of even high-quality datasets. Synthetic math questions, while necessary to augment training data, can inadvertently introduce errors that hinder model performance. Our benchmark addresses this challenge by offering a comprehensive evaluation of math data cleaning techniques, including both correct and erroneous questions and answers, along with detailed error-type annotations to guide future improvements. Through rigorous testing with SOTA models, we demonstrated that even advanced models like DeepSeek-R1 struggle with cleaning synthetic math data, thus underscoring the significance of MathClean in identifying and addressing errors. Ultimately, MathClean serves as a valuable resource for improving the quality of mathematical datasets and enhancing the robustness of LLMs, offering important insights for future research in data-centric AI development.

\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}
\appendix 
\clearpage

\onecolumn
\section{Example Appendix}\label{Appendix.example}

In this section, we provide several examples in which GPT-o1 fails in either correctness or error type detection on the MathClean benchmark, accompanied by corresponding explanations, in order to more fully and convincingly demonstrate the significant role of the MathClean benchmark in improving the mathematical reasoning capabilities of current models.

\begin{figure*}[htbp]
\centering 
\includegraphics[width=0.98\textwidth]{figures/appendix_example1.pdf} 
\label{Fig.appendix_example1}
\vspace{-4mm}
\end{figure*}

\begin{figure*}[htbp]
\centering 
\includegraphics[width=0.98\textwidth]{figures/appendix_example2.pdf} 
\label{Fig.appendix_example2}
\vspace{-4mm}
\end{figure*}

\begin{figure*}[htbp]
\centering 
\includegraphics[width=0.98\textwidth]{figures/appendix_example4.pdf} 
\label{Fig.appendix_example4}
\vspace{1mm}
\end{figure*}


\begin{figure*}[htbp]
\centering 
\includegraphics[width=0.98\textwidth]{figures/appendix_example3.pdf} 
\label{Fig.appendix_example3}
\vspace{1mm}
\end{figure*}

\section{Prompt Appendix}
% \FloatBarrier

This section provides a detailed list of all the prompts mentioned in this paper, including those for data preparation, data synthesis, and the experimental section. All the prompts were carefully designed and iteratively refined and validated during the experiments, ultimately yielding excellent results that can serve as a valuable reference for researchers.

\begin{figure*}[htbp]
    \centering 
    \makebox[\textwidth]{\includegraphics[width=0.98\textwidth]{figures/experiment_prompt.pdf}}
    \caption{\centering The prompt for the experimental design of question and answer correctness and error type detection.}
    \label{Fig.experiment_prompt}
    \vspace{2mm}
\end{figure*}

\begin{figure*}[htbp]
    \centering 
    \makebox[\textwidth]{\includegraphics[width=0.98\textwidth]{figures/error_question_prompt.pdf}}
    \caption{\centering The prompt for expanding error types during the construction of the question dataset.}
    \label{Fig.error_question_prompt}
    \vspace{-4mm}
\end{figure*}

\FloatBarrier

\begin{figure*}[htbp]
    \centering 
    \makebox[\textwidth]{\includegraphics[width=0.98\textwidth]{figures/question_extension_prompt.pdf}}
    \caption{\centering The prompt for enhancing the diversity of questions during the construction of the answer dataset.}
    \label{Fig.question_extension_prompt}
    \vspace{-4mm}
\end{figure*}


\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
