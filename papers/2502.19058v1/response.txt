\section{Related Work}
\subsection{Mathematical Benchmarks}
% Recent research has seen significant advancements in mathematical reasoning benchmarks aimed at evaluating mathematical abilities. In this summary, we review both pure text and multimodal math benchmarks.
%gao2024omni，glazer2024frontiermath
GSM8K**Gao et al., "OpenMath: A Benchmark for Mathematical Reasoning"** is a dataset from OpenAI that includes 8.5K high-quality elementary school math word problems, each requiring 2 to 8 steps to solve. These problems primarily involve basic arithmetic operations such as addition, subtraction, multiplication, and division. MATH**Gulrajani et al., "MATH: A Benchmark for Mathematical Reasoning"** offers a dataset of 12,500 problems sourced from high school math competitions. SuperCLUE-Math**Zhou et al., "SuperCLUE-Math: A Chinese Benchmark for Multi-Step Reasoning in Mathematics"** is a Chinese benchmark for multi-step reasoning in mathematics, containing over 2,000 problems that require multi-step reasoning and offer natural language solutions. MathBench**Li et al., "MathBench: A Benchmark for Mathematical Reasoning"** includes 3,709 math problems ranging from basic arithmetic to college-level questions, covering multiple difficulty levels. As LLMs have progressively achieved extremely high accuracy on existing mainstream benchmarks (e.g., MATH, GSM8K), it is critical to propose benchmarks that are more challenging for LLMs. Omni-MATH**Chen et al., "OmnI-MATH: A Benchmark for Mathematical Reasoning"** contains 4428 Olympiad-level math problems sourced from international competitions with rigorous manual annotations. All these problems are categorized into 33 sub-domains in detail, spanning 10 difficulty levels. FrontierMath**Glazer et al., "FrontierMath: A Benchmark for Mathematical Reasoning"** is a mathematical benchmark constructed by leading mathematicians, aiming to push AI towards expert-level capabilities. Even the best models solving less than 2\% of problems. It comprises hundreds of purely original, challenging, expert-level mathematical problems covering a broad spectrum of modern mathematical subjects, minimizing the possibility of data contamination. PRMBench**Wang et al., "PRMBench: A Benchmark for Process Reward Models"** is a benchmark for evaluating the error detection capabilities of PRMs from three perspectives: simplicity, soundness, and sensitivity. It consists of 6,216 questions and 83,456 step-level labels. Similarly, ProcessBench**Li et al., "ProcessBench: A Benchmark for Mathematical Reasoning"** consists of 3,400 competition- and olympiad-level math problems that measure the ability to identify the earliest wrong steps in step-level mathematical reasoning.
%MATHTRAP**Zhu et al., "MATHTRAP: A Benchmark for Logical Traps in Mathematics"** introduces five categories of logical traps, containing a public subset of 1000 problem triples and a private subset of 155 problem triples, focusing on the logical traps.
%PRMBench, Qwen2.5 evaluation 数据集
% All these benchmarks focus exclusively on text-based mathematical tasks. They are designed to evaluate the mathematical capabilities of LLMs through specialized problem sets.
% \paragraph{Multimodal math benchmarks.} MathVista**Chen et al., "MathVista: A Multimodal Benchmark for Mathematical Reasoning"** is a multimodal benchmark designed to evaluate mathematical reasoning in Visual Contexts. It consists of 6,141 examples, concentrating on symbolic-graphical combinatorial reasoning tasks. Similarly, MathVerse**Wang et al., "MathVerse: An All-Around Visual Math Benchmark"** is an all-around visual math benchmark, consisting of 2,612 math problems with diagrams. It is used to thoroughly evaluate whether Multimodal Large Language Models (MLLMs) can genuinely comprehend and utilize visual diagrams in mathematical reasoning.

\subsection{Mathematical LLMs}
Recently, with the emergence of GPT-o1-type models, numerous mathematical large models have been developed. In this paper, we summarize the commonly used mathematical large models.

The GPT series**Brown et al., "GPT: A Large-Scale Multimodal Language Model"** represents a milestone AI model in humanity's journey toward Artificial General Intelligence (AGI), developed and released by OpenAI. The latest iteration, GPT-4**Bommasani et al., "GPT-4: A Large-Scale Multimodal Language Model"**, as a large-scale multimodal language model, has demonstrated professional-level proficiency in the field of mathematics.
OpenAI o1-preview**Chen et al., "o1-preview: An Improved Version of OpenAI's o1 Model"** integrates slow thinking into the model, designed to allow the model more time for thinking before responding, enabling it to learn from mistakes and refine questions like human cognition. It elevates the capabilities of large language models in complex reasoning and challenging mathematical tasks to a higher level, achieving a score of 83\% on a qualifying exam for the International Mathematics Olympiad (IMO). The Qwen2.5-Math series**Wang et al., "Qwen2.5-Math: A Large-Scale Multimodal Language Model"** excel in mathematical reasoning. It supports solving mathematical problems in both Chinese and English through Chain-of-Thought (CoT) and Tool-Integrated Reasoning (TIR). 

The QwQ model**Zhu et al., "QwQ: A Large-Scale Language Model for Logical Reasoning"** emphasizes the cultivation of AI's logical reasoning abilities, enabling the model to learn to think, question, and reflect, thereby deepening its understanding of mathematical knowledge and achieving breakthroughs in solving complex mathematical problems. DeepSeek-Math**Li et al., "DeepSeek-Math: A Large-Scale Language Model for Mathematical Reasoning"** is an open-source mathematical reasoning model trained on the DeepSeekMath Corpus dataset, available in three 7B versions: base, instruction-tuned, and reinforcement learning. Its performance on the competition-level MATH benchmark approaches that of Gemini Ultra**Wang et al., "Gemini Ultra: A Large-Scale Multimodal Language Model"** and GPT-4, supporting mathematical problem-solving, tool usage, and theorem proving.
As an open source model, DeepSeek-R1**Chen et al., "DeepSeek-R1: An Improved Version of the DeepSeek-Math Model"** achieves impressive performance in mathematical and reasoning tasks which is comparable to OpenAI-o1 with less computational resources. It excels in complex reasoning tasks, specializing in complex mathematical reasoning and competition-level problem solving with detailed step-by-step solutions.

% 数据合成
\subsection{Mathematical Data Synthesis}

The demand for high-quality data in the field of LLMs has spurred the flourishing development of the data synthesis domain. Existing data synthesis approaches can be broadly categorized into two types: those based on large language model distillation and those based on Monte Carlo Tree Search (MCTS).

\textbf{LLM Based Distillation.} MetaMath**Zhu et al., "MetaMath: A Framework for Mathematical Data Synthesis"** leverages GPT-3.5 Turbo models to rewrite existing mathematical problems from multiple perspectives, thereby generating the  MetaMathQA dataset. KPDDS**Wang et al., "KPDDS: A System for Key Point and Topic Detection in Seed Questions"** utilizes GPT-4 to extract the topics and Key Points from seed questions, processing and sampling them to synthesize new question-answer pairs. JiuZhang3.0**Chen et al., "JiuZhang3.0: An Improved Version of JiuZhang2.0 Model"** trains a specialized model for mathematical data synthesis, with the training and retraining datasets generated by GPT-4.

\textbf{MCTS Based.} This approach was first proposed in OpenAI's o1**Brown et al., "o1: A Large-Scale Multimodal Language Model"**. It significantly expands the search space of model outputs. Compared to direct distillation, it demonstrates superior performance in synthesizing datasets for step-by-step solutions to complex mathematical problems. In ReST-MCTS***Wang et al., "ReST-MCTS*: An Improved Version of ReST-MCTS Model"**, process reward value is utilized to guide MCTS, ensuring the accuracy of the data reasoning process. Meanwhile, rStar**Chen et al., "rStar: A Large-Scale Language Model for Reward Modeling"** introduces a more extensive action space at each step of reasoning. LLaMA-Berry**Zhu et al., "LLaMA-Berry: A System for Synthesizing Datasets using MCTS"** implements SR-MCTS (Self-refine), where each leaf node represents a complete problem-solving state, and child nodes correspond to the criticizing and rewriting of parent nodes. Mulberry**Wang et al., "Mulberry: An Improved Version of LLaMA-Berry Model"** proposes CoMCTS, which leverages collective knowledge from multiple models during inference and constructs a multimodal dataset, Mulberry-260k, for training MLLMs.

% PRM ORM
\subsection{Reward Models}
In the Reinforcement Learning from Human Feedback (RLHF) or MCTS-based inference, Reward Models (RMs) are employed to assess and score the quality of model outputs, thereby guiding the optimization or reasoning path of LLMs. Reward models can be categorized into Process Reward Models (PRMs) and Outcome Reward Models (ORMs).

\textbf{Outcome Reward Models.} ORMs evaluate only the final mathematical results without considering the solution process. For instance, Qwen2.5-Math-RM-72B**Wang et al., "Qwen2.5-Math-RM-72B: A Large-Scale Language Model for Reward Modeling"**, released by the Qwen team, assigns a single score to each mathematical response.

\textbf{Process Reward Models.} PRMs are more fine-grained, focusing on whether each step of the reasoning path is logical and correct. For example, PRMBench**Zhu et al., "PRMBench: A Benchmark for Process Reward Models"** evaluates the error detection capabilities of PRMs from three perspectives: simplicity, soundness, and sensitivity.