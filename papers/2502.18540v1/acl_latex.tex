% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}
\usepackage{multirow}

% Standard package includes
\usepackage{times}
\usepackage{titlesec}
\usepackage{latexsym}
\usepackage{bbding}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{longtable}
\usepackage{lipsum}
\usepackage{array}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{hyperref}
\usepackage{times}
\usepackage{svg}
\usepackage{latexsym}
\usepackage{multirow}
\usepackage{threeparttable}
% \usepackage[table,xcdraw]
\usepackage{colortbl}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{adjustbox}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets
\usepackage{algorithm,algorithmic}
% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{multirow}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\setlist{nosep}

\usepackage{amsmath}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{MA-GTS: A Multi-Agent Framework for Solving Complex Graph Problems in Real-World Applications}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
 \textbf{Zike Yuan\textsuperscript{1,2}},
 \textbf{Ming Liu\textsuperscript{1}},
 \textbf{Hui Wang\textsuperscript{2}},
 \textbf{Bing Qin\textsuperscript{1}}
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
\\
 \textsuperscript{1}Harbin Institute of Technology, Shenzhen, China,\\
 \textsuperscript{2}Peng Cheng Laboratory, Shenzhen, China\\
 \texttt{\{yuanzk,wangh06\}@pcl.ac.cn}\\
 \texttt{\{mliu,qinb\}@ir.hit.edu.cn}
\\
 % \small{
 %   \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
 % }
}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\begin{abstract}
Graph-theoretic problems arise in real-world applications like logistics, communication networks, and traffic optimization. These problems are often complex, noisy, and irregular, posing challenges for traditional algorithms. Large language models (LLMs) offer potential solutions but face challenges, including limited accuracy and input length constraints. To address these challenges, we propose \textbf{MA-GTS} (\textbf{M}ulti-\textbf{A}gent \textbf{G}raph \textbf{T}heory \textbf{S}olver), a multi-agent framework that decomposes these complex problems through agent collaboration. MA-GTS maps the implicitly expressed text-based graph data into clear, structured graph representations and dynamically selects the most suitable algorithm based on problem constraints and graph structure scale. This approach ensures that the solution process remains efficient and the resulting reasoning path is interpretable. We validate MA-GTS using the \textbf{G-REAL} dataset, a real-world-inspired graph theory dataset we created. Experimental results show that MA-GTS outperforms state-of-the-art approaches in terms of efficiency, accuracy, and scalability, with strong results across multiple benchmarks (G-REAL \textbf{94.2\%}, GraCoRe \textbf{96.9\%}, NLGraph \textbf{98.4\%}). MA-GTS is open-sourced at \href{https://github.com/ZIKEYUAN/MA-GTS.git}{https://github.com/ZIKEYUAN/MA-GTS.git}.
\end{abstract}
\begin{figure*}[htbp]
\centering
  \includegraphics[width=0.9\textwidth]{pic/framework_2.pdf}
  \caption{This figure presents MA-GTS framework for solving real-world graph problems, consisting of three layers: Information Extraction, Knowledge Integration, and Algorithm Execution, each with specialized agents to process and solve the problem.}
  \label{fig:1}
  \vspace{-15pt}
\end{figure*}
\section{Introduction}
Graph-theoretic problems have extensive applications in domains such as logistics scheduling, communication networks, production planning, and traffic optimization\cite{suvery}. These problems typically involve a large number of nodes and edges, coupled with complex constraints and dynamic variations, making their solution highly challenging\cite{bondy2008graph}. Despite significant advancements in graph theory and algorithmic design, traditional approaches remain computationally expensive and inefficient when handling large-scale, high-complexity problems. Existing methods, including exact algorithms, greedy strategies, and dynamic programming\cite{bellman1966dynamic}, perform well on small-scale instances. However, as problem size increases, their computational complexity and memory requirements grow exponentially, rendering them impractical for real-world applications. While heuristic methods\cite{kokash2005introduction} can improve performance under specific conditions, they often suffer from local optima and require extensive parameter tuning and model selection. Therefore, developing efficient and scalable solution frameworks capable of addressing the computational demands and structural variability of complex graph-theoretic problems remains a critical research challenge.

Recent advancements in large language models (LLMs) have spurred interest in their applications for graph-theoretic problems. Leveraging their natural language processing (NLP) capabilities, LLMs can serve as  \textbf{scene interpreters} (mapping real-world problems to graph models),  \textbf{graph extractors} (identifying graph structures from unstructured data), and  \textbf{graph algorithm invokers} (assisting in solving and optimizing graph-based problems), addressing certain limitations of traditional algorithms. However, significant challenges remain. \textbf{Firstly}, LLMs rely on statistical pattern matching rather than strict mathematical computations, limiting their reasoning accuracy and making them unreliable for NP-hard problems\cite{hochba1997approximation}.  \textbf{Secondly}, their ability to handle large-scale graphs is limited by the Transformer\cite{vaswani2017attention} architecture's context window and computational complexity, which restricts their capacity to capture global information. \textbf{Finally}, LLMs lack the ability to decompose and map real-world graph theory problems, which often contain complex textual noise and implicit graph structures. As a result, LLMs struggle with accurate denoising and may mismap node information, reducing reasoning interpretability. These limitations highlight the inadequacy of single-agent LLM frameworks for solving complex graph-theoretic problems in real-world applications and underscore the need for more efficient and scalable paradigms.


To tackle these challenges, we propose \textbf{MA-GTS}(\textbf{M}ulti-\textbf{A}gent \textbf{G}raph \textbf{T}heory \textbf{S}olver), an innovative multi-agent framework designed to address complex graph-theoretic problems through agent collaboration and competition. Figure \ref{fig:1} illustrates the framework, which incorporates a multi-agent coordination mechanism allowing agents to perform local searches independently while sharing information and cooperating, thus improving solution efficiency and accuracy. MA-GTS analyzes the original real-world problem textual data, filters out noise, and extracts key graph data and problem-specific details, reducing the text length that LLMs must process and enhancing reasoning efficiency. This coordination mitigates the limitations of LLMs in implicit graph structure modeling, ensuring efficient solutions for complex graph tasks. Additionally, dynamic agent interactions enable the framework to address large-scale problems and adapt to complex constraints and dynamic changes.
% \begin{figure}[t]
%   \includegraphics[width=\columnwidth]{pic/framework.pdf}
%   % \includesvg[width=\textwidth]{pic/fig2_v2.svg}
%   \caption{framework}
%   \label{fig:1}
%      \vspace{-22pt}
% \end{figure}

To validate the effectiveness of the multi-agent framework, we introduce the \textbf{G-REAL} dataset, designed to simulate complex graph theory problems relevant to real-world scenarios. Unlike traditional datasets that rely on simple textual descriptions of graph structures, G-REAL better reflects practical applications for large-scale models. Experiments comparing MA-GTS with state-of-the-art open-source and closed-source LLMs—three closed-source and two open-source models—using both direct and Chain of Thought (CoT) reasoning, show that MA-GTS significantly outperforms existing LLMs in terms of efficiency and accuracy. Notably, it excels in solving large-scale problems with complex constraints, offering superior scalability, robustness, and cost-effectiveness. The primary contributions of this study are as follows:
\begin{itemize}[left=0pt]
\item First, we propose an innovative multi-agent framework, MA-GTS, which overcomes the limitations of traditional graph theory algorithms in large-scale complex problems, achieving state-of-the-art performance in our tests.
\item Second, we constructed a real-world graph theory dataset, G-REAL, that aligns with practical needs, providing the necessary data support for validating the effectiveness of the algorithm.
\item Finally, by introducing novel collaboration mechanisms and strategies, we achieve efficient and precise graph theory problem-solving within the multi-agent system, demonstrating its substantial potential in real-world application scenarios.
\end{itemize}
\begin{figure*}[htbp]
\centering
  \includegraphics[width=1\textwidth]{pic/pipeline.pdf}
  \caption{This figure provides a detailed description of the composition and characteristics of the G-REAL dataset, as well as the entire graph theory problem-solving pipeline of MA-GTS, including specific functions and the formats of input and output.}
  \label{fig:2}
  \vspace{-15pt}
\end{figure*}

\section{Related Work}
\textbf{LLMs for Graph}: Recent advancements in LLMs for graph tasks have led to significant contributions in methodology and evaluation. These tasks are often classified into Enhancer, Predictor, and Alignment types \citep{suvery}. Notably, \citep{2} presents a roadmap for unifying LLMs with Knowledge Graphs (KGs), while \citep{3} proposes an end-to-end method for solving graph-related problems,\cite{cao2024graphinsight} improves LLMs' understanding of graph structures by addressing positional biases and incorporating an external knowledge base. On the evaluation front, several benchmarks have been introduced. NLGraph \citep{NLgraph-6} offers a simple test dataset for graph tasks, and GPT4Graph \citep{gpt4graph-7} evaluates LLM capabilities on semantic tasks. GraCoRe\cite{yuan2025gracore} comprehensively verifies the graph understanding and reasoning capabilities of LLM. Other notable works include \citep{8}, which assesses LLMs in graph data analysis, and \citep{10}, which designs a hint method for graph tasks.

\noindent\textbf{LLM Agents}: In recent years, several multi-agent frameworks have been proposed to enhance the coordination and efficiency of language models in complex tasks. MetaGPT\cite{hong2023metagpt} reduces hallucinations in complex tasks by embedding human workflows into language models. CAMEL\cite{li2023camel} promotes autonomous cooperation among agents, guiding them to align with human goals and studying their interactions. AutoGen\cite{wu2023autogen} is a flexible framework that allows developers to customize agent interactions using both natural language and code, suitable for various fields. In addition, \cite{li2024graphteam} can be used to solve simple graph problems, \cite{li2024anim} is an autonomous agent that uses LLMs to create animated videos from simple narratives.
% \titlespacing*{\subsection}{0pt}{0.5ex plus 0.2ex minus 0.2ex}{0pt}
% \titlespacing*{\section}{0pt}{0.5ex plus 0.2ex minus 0.2ex}{0pt}
\section{MA-GTS}
The MA-GTS framework adopts a hierarchical processing paradigm, comprising three layers: the \textbf{Information Extraction Layer(IEL)}, the \textbf{Knowledge Integration Layer(KIL)}, and the \textbf{Algorithm Execution Layer(AEL)}. These layers interact through a hierarchical collaborative communication mechanism, enabling an end-to-end pipeline that processes unstructured data and solves complex graph-theoretic problems. Additionally, to support the knowledge base of MA-GTS, we have constructed the Graph Theory Knowledge Base and Graph Theory Algorithm Library. More information about them in the Appendix \ref{sec:A}.

The IEL parses textual and structured data, extracts graph-related information, and identifies problem types to provide standardized inputs. The KIL constructs structured graph data, applying graph theory and optimization strategies to improve accuracy and scalability. The AEL calls specified algorithms for computation and performs self-checking to solve complex graph problems efficiently. Figure \ref{fig:2} illustrates the functionality of each agent in their respective layers.

By leveraging agent collaboration, MA-GTS ensures efficient problem-solving, high scalability, and adaptability to complex constraints, offering a novel solution for real-world graph-theoretic challenges. The specific functionalities of each agent are detailed as follows:
\subsection{Information Extraction Layer (IEL)}
The IEL extracts relevant information from text and unstructured data, converting it into a structured format for downstream processing. It also filters out irrelevant content, refining problem-specific information to enhance LLM reasoning. Additionally, it captures implicit graph-structural information, improving problem-solving efficiency and mitigating the impact of text length on model comprehension and inference.

\noindent\textbf{Textual Information Extraction Agent (TIEA)}: The TIEA analyzes real-world graph-theoretic problems, extracting key textual information not directly related to the graph structure or solution objectives. Using NLP techniques, it identifies and structures contextual descriptions, background information, entities, concepts, and definitions. Its goal is to organize context, terminology, and related concepts, providing semantic support for subsequent analysis and problem solving. The extracted information is then output in a standardized format for downstream processing.

\noindent\textbf{Graph Structure Information Extraction Agent (GSIEA)}: The GSIEA extracts implicitly embedded graph-structural information from text, particularly structured formats like tables, lists, adjacency matrices, or edge lists. It parses these inputs to identify nodes, edges, weights, and other topological properties, converting them into standardized graph representations (e.g., adjacency matrices or lists). This transformation enables downstream agents to efficiently use the extracted data for problem solving.

\noindent\textbf{Problem Information Extraction Agent (PIEA)}: The PIEA leverages LLMs’ problem classification capabilities to analyze real-world graph-theoretic problems, identify their types, and extract key components. It classifies problems (e.g., shortest path, network flow, graph matching), extracts relevant constraints and objectives, and outputs the information in a structured format. This guidance improves the accuracy and efficiency of downstream problem-solving agents.

Formally,the operation of IEL is:
\begin{equation}
\begin{aligned}
\mathcal{T} \gets A_{TIEA}(P), \\
\mathcal{P} \gets A_{PIEA}(P), \\
G \gets  A_{GSIEA}(P) \\
IEL_{output} = (\mathcal{T},\mathcal{P},G)
\end{aligned}
\end{equation}
where $P$ is graph theory problem and $A_*$ is a different agent in IEL, $(\mathcal{T},\mathcal{P},G)$ represent the extracted text information, question information and graph structure information respectively.
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\begin{algorithm}[t]
\caption{Pipeline of MA-GTS}
\begin{algorithmic}[1]

\REQUIRE Real-world graph problem $P$, graph theory knowledge base $\mathcal{K_G}$, graph theory algorithm library $\mathcal{L}_{code}$., self check number $N_{check}$
   
\ENSURE Optimized solution $S^n$
\STATE \textbf{Step 1: Information Extraction Layer}
\STATE Extract textual information: $\mathcal{T} \gets A_{TIEA}(P)$
\STATE Identify problem type: $\mathcal{P} \gets A_{PIEA}(P)$
\STATE Extract graph structure: $G \gets  A_{GSIEA}(P)$\;
\STATE Generate extracted information set: $(\mathcal{T}, \mathcal{P}, G)$

\STATE \textbf{Step 2: Knowledge Integration Layer}
\STATE Select best algorithm: 
% \STATE  $Alg^* \gets \arg\max_{Alg_i \in \mathcal{L}} \mathcal{F}(Alg_i, \mathcal{T}, \mathcal{P})$
\STATE \hspace{0.2cm} $\mathcal{L}_{\mathcal{P}} \gets  A_{GTA}(\mathcal{T},\mathcal{P}, \mathcal{K_G})$
\STATE  \hspace{0.2cm} $Alg^* \gets \arg\operatorname{opt}_{Alg_i \in \mathcal{L}_{\mathcal{P}}}  A_{GTA}(Alg_i, \mathcal{T})$
\STATE Get structured graph: $G' \gets A_{SGIA}(G)$
\STATE Define structured problem: $(G', Alg^*)$
\STATE \textbf{Step 3: Algorithm Execution Layer}
\STATE Load algorithm code: 
\STATE \hspace{0.2cm} $Code_{Alg^*} \gets A_{ASA}(Alg^*, \mathcal{L}_{code})$
\STATE Get algorithm output : 
\STATE \hspace{0.2cm} $S_{code} \gets A_{ASA}Coding(Code_{Alg^*}, G')$
\STATE Get optimized solution $S^n$ : 
\STATE  \hspace{0.2cm}$S^0 \gets A_{ASA}(S_{code},Alg^*, G')$;
        \FOR {$i=1,2,\cdots,N_{check}$}
            \STATE  $S^n \gets A_{ASA}(S^{n-1},Alg^*, G')$;
        \ENDFOR
\end{algorithmic}
\end{algorithm}
\subsection{Knowledge Integration Layer (KIL)}
The primary objective of this layer is to construct structured graph data with high representational capacity and integrate graph-theoretic principles for advanced modeling, thus enhancing the efficiency of the solution and the quality of optimization.

\noindent\textbf{Structured Graph Information Agent (SGIA)}: The SGIA standardizes the graph structure data extracted by the GSIEA, ensuring efficiency, consistency, and usability. It converts raw data into standard formats compatible with various environments, while performing data cleaning, deduplication, and format optimization to maintain accuracy. Additionally, it optimizes data storage and indexing based on algorithm requirements, enhancing computational efficiency for large-scale graphs. Without this agent, data inconsistencies, redundancy, and unoptimized structures could hinder algorithm performance. As a key component of MA-GTS, it ensures data standardization and optimization for efficient, scalable problem-solving.

\noindent\textbf{Graph Theory Agent (GTA)}: The GTA integrates information from the TIEA and PIEA to analyze graph-theoretic problems and determine optimal solution strategies using a pre-stored Graph Theory Knowledge Base, enhancing LLM inference efficiency. It models the input problem by extracting key features such as type, constraints, and structural complexity, then queries the \textbf{Graph Theory Knowledge Base} to select the most suitable solution method from classical algorithms (e.g., shortest path, maximum flow, graph matching)\cite{gallo1988shortest,goldberg1988new} and heuristic techniques. By matching problems to algorithms, it reduces inefficient exhaustive searches, cutting computational costs and improving solution quality. Additionally, it provides guidance for multi-agent collaboration, enabling the AEL to invoke the optimal algorithm directly, ensuring efficiency and scalability. Without this agent, LLMs may face suboptimal strategy selection, excessive computation, and reduced efficiency. As a core component of MA-GTS, it is vital for algorithm selection and inference optimization in complex graph-theoretic tasks.

Formally,the operation of KIL is:
\begin{equation}
\begin{aligned}
\mathcal{L}_{\mathcal{P}} \gets  A_{GTA}(\mathcal{T},\mathcal{P}, \mathcal{K_G}), \\
Alg^* \gets \arg\operatorname{opt}_{Alg_i \in \mathcal{L}_{\mathcal{P}}}  A_{GTA}(Alg_i, \mathcal{T}),\\
G' \gets A_{SGIA}(G),\\
KIL_{output} = (G', Alg^*)
\end{aligned}
\end{equation}
where $\mathcal{L}_{\mathcal{P}}$ represents the set of graph theory algorithms selected by GTA based on textual and problem-specific information, $\mathcal{K_G}$ denotes the Graph Theory Knowledge Base, $Alg^*$ refers to the algorithm most suitable for the given graph size, and $G'$ stands for the normalized graph structure data.




\subsection{Algorithm Execution Layer (AEL)}
% The AEL is responsible for solving the target problem by integrating exact algorithms and heuristic methods, ensuring solution feasibility, optimality, and computational efficiency. This layer receives the optimal algorithm selection from the GTA and applies the appropriate solving strategy based on the problem type. For well-structured problems with manageable computational complexity, such as the shortest path, classical exact algorithms—including Dijkstra’s algorithm, Bellman-Ford, and Edmonds-Karp—are prioritized. For computationally intensive or constraint-laden problems, heuristic methods (e.g., greedy algorithms, local search) and metaheuristic optimization techniques (e.g., genetic algorithms, simulated annealing) are employed to enhance efficiency and scalability.
The primary goal of this layer is to integrate multiple algorithmic paradigms, ensuring efficient, scalable, and robust solutions under various constraints. Without it, the MA-GTS framework would rely solely on LLM-based inference, leading to high computational costs, instability, or suboptimal outcomes. As the computational core, the AEL enables the efficient solution of complex graph-theoretic problems across varying scales and complexities.

\noindent\textbf{Algorithm Solving Agent (ASA)}: The ASA is the core computational unit of the AEL, responsible for solving problems by executing algorithmic functions based on the optimal strategy selected by the GTA and the structured graph data processed by the SGIA. It utilizes a \textbf{Graph Theory Algorithm Library} that integrates exact algorithms\cite{noto2000method} and heuristic approaches, ensuring optimal or near-optimal solutions across various problem scenarios. After computation, the agent performs result integration and verification through cross-validation, error analysis, and constraint checking to ensure correctness. It also provides explainable reasoning, offering inference paths, key decision points, and optimization steps to enhance transparency. As the computational core of MA-GTS, the ASA enables efficient, robust, and scalable solutions for complex graph-theoretic problems.

Formally,the operation of AEL is:
\begin{equation}
\begin{aligned}
Code_{Alg^*} \gets A_{ASA}(Alg^*, \mathcal{L}_{code}), \\
S_{code} \gets A_{ASA}Coding(Code_{Alg^*}, G'),\\
S^0 \gets A_{ASA}(S_{code},Alg^*, G'),
\end{aligned}
\end{equation}
where $\mathcal{L}_{code}$ represents the Graph Theory Algorithm Library, $Code_{Alg^*}$ denotes the code obtained after optimal algorithm matching by ASA, $S_{code}$ refers to the output generated by running the code, and $S^0$ represents the interpretable output obtained by combining the code output with problem review. Finally, ASA undergoes $n$ rounds of self-checking, ultimately producing the final suitable result, $S^n$.



\section{G-REAL}
Existing datasets for evaluating LLMs' understanding and reasoning on graph-structured data are explicitly constructed. However, real-world graph-theoretic problems often involve rich textual semantic information and implicitly structured representations. To assess the performance of the MA-GTS framework on practical problems, we introduce \textbf{G-REAL}, a dataset that captures real-world graph problems. This dataset comprises three commonly encountered graph-theoretic challenges: (1) the optimization of logistics and delivery routes, (2) wireless network channel allocation, and (3) network monitoring optimization. These correspond to three fundamental graph problems: the \textbf{Traveling Salesman Problem (TSP)}, the \textbf{Minimum Graph Coloring Problem}, and the\textbf{ Minimum Vertex Cover Problem}, respectively\cite{hoffman2013traveling,jensen2011graph,hochbaum1982approximation}. The composition of G-REAL can be seen briefly in Figure \ref{fig:2}. In this section, we provide a detailed description of the dataset’s composition and construction methodology. More detail about G-REAL in Appendix \ref{sec:C}.



\begin{table}[t]
\centering
\begin{adjustbox}{max width=\linewidth} % 调整表格宽度限制
\begin{tabular}{c|ccc|c|cc}
\hline
\rowcolor[HTML]{EFEFEF} 
                                                    & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{G-REAL}} & \textbf{GraCoRe} & \multicolumn{2}{c}{\cellcolor[HTML]{EFEFEF}\textbf{NLGraph}} \\
\rowcolor[HTML]{EFEFEF} 
                                                    & \textbf{TSP}   & \textbf{Coloring}   & \textbf{Vetex Cover}  & \textbf{TSP}     & \textbf{Shortest Path}            & \textbf{Cycle}           \\ \hline
\cellcolor[HTML]{EFEFEF}\textbf{\#Graph}            & 900            & 900                 & 900                   & 360              & 380                               & 1150                     \\
\cellcolor[HTML]{EFEFEF}\textbf{Node Range}         & 8 - 25        & 8 - 25             & 8 - 25               & 8 - 25          & 5 - 20                           & 5 - 15                  \\
\cellcolor[HTML]{EFEFEF}\textbf{Real-World Problem} & \CheckmarkBold              & \CheckmarkBold                   & \CheckmarkBold                     & \XSolidBold                & \XSolidBold                                 & \XSolidBold                        \\
\cellcolor[HTML]{EFEFEF}\textbf{Text Noise}         & \CheckmarkBold              & \CheckmarkBold                   & \CheckmarkBold                     & \XSolidBold                & \XSolidBold                                 & \XSolidBold                        \\ \hline
\end{tabular}
\end{adjustbox}
     \caption{Differences between different datasets.}
    \label{tab:1}
         \vspace{-20pt}
\end{table}
\begin{table*}[t]
	\centering
	\scriptsize
	\setlength{\extrarowheight}{2.5pt} 
 \resizebox{\linewidth}{!}{
\begin{tabular}{
>{\columncolor[HTML]{EFEFEF}}c |
>{\columncolor[HTML]{EFEFEF}}c |
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c }
\hline
\cellcolor[HTML]{EFEFEF}                                       & \cellcolor[HTML]{EFEFEF}                                  & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{G-REAL}}                                                                             & \cellcolor[HTML]{EFEFEF}\textbf{GraCoRe} & \multicolumn{2}{c}{\cellcolor[HTML]{EFEFEF}\textbf{NLGraph}}                            \\
\multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textbf{Model}}       & \multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textbf{Method}} & \cellcolor[HTML]{EFEFEF}\textbf{TSP}   & \cellcolor[HTML]{EFEFEF}\textbf{Coloring}      & \cellcolor[HTML]{EFEFEF}\textbf{Vetex Cover}   & \cellcolor[HTML]{EFEFEF}\textbf{TSP}     & \cellcolor[HTML]{EFEFEF}\textbf{Shortest Path} & \cellcolor[HTML]{EFEFEF}\textbf{Cycle} \\ \hline
\cellcolor[HTML]{EFEFEF}                                       & \textbf{Direct}                                           & 11.8\%                                 & 80.1\%                                         & 68.7\%                                         & 79.7\%                                   & \textbf{100.0\% }                                       & \textbf{98.9\%}                        \\
\multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textit{o3-mini}}     & \textbf{CoT}                                              & 12.9\%                                 & 83.1\%                                         & 72.8\%                                         & 80.0\%                                   & 98.4\%                                         & \textbf{98.9\%}                        \\ \hline
\cellcolor[HTML]{EFEFEF}                                       & \textbf{Direct}                                           & 2.5\%                                  & 23.4\%                                         & 0.3\%                                          & 1.1\%                                    & 27.3\%                                         & 50.9\%                                 \\
\multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textit{GPT-4o-mini}} & \textbf{CoT}                                              & 3.1\%                                  & 25.1\%                                         & 0.0\%                                          & 1.1\%                                    & 27.6\%                                         & 51.1\%                                 \\ \hline
\cellcolor[HTML]{EFEFEF}                                       & \textbf{Direct}                                           & 0.1\%                                  & 0.7\%                                          & 2.5\%                                          & 1.9\%                                    & 30.5\%                                         & 50.0\%                                 \\
\multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textit{GPT-3.5}}     & \textbf{CoT}                                              & 2.1\%                                  & 7.6\%                                          & 4.8\%                                          & 1.6\%                                    & 34.7\%                                         & 49.9\%                                 \\ \hline
\cellcolor[HTML]{EFEFEF}                                       & \textbf{Direct}                                           & 0.6\%                                  & 16.2\%                                         & 17.4\%                                         & 3.8\%                                    & 22.1\%                                         & 49.6\%                                 \\
\multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textit{Qwen2.5-7b}}  & \textbf{CoT}                                              & 0.6\%                                  & 8.8\%                                          & 8.5\%                                          & 3.0\%                                    & 27.3\%                                         & \cellcolor[HTML]{FFFFFF}52.7\%         \\ \hline
\cellcolor[HTML]{EFEFEF}                                       & \textbf{Direct}                                           & 3.6\%                                  & 10.1\%                                         & 7.2\%                                          & 0.3\%                                    & 12.6\%                                         & 53.7\%                                 \\
\multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textit{Llama 3 -7b}} & \textbf{CoT}                                              & 4.1\%                                  & 14.3\%                                         & 6.7\%                                          & 0.3\%                                    & 19.4\%                                         & 50.9\%                                 \\ \hline
\textit{\textbf{MA-GTS(GPT-4o-mini)}}                          & \textbf{Multi-Agent}                                      & {\color[HTML]{000000} \textbf{94.9\%(\color[HTML]{CB0000}$\uparrow$82\%)}}              & {\color[HTML]{000000} \textbf{94.5\%(\color[HTML]{CB0000}$\uparrow$11.4\%)}}                 & {\color[HTML]{000000} \textbf{93.2\%(\color[HTML]{CB0000}$\uparrow$20.4\%)}} & \textbf{96.9\%(\color[HTML]{CB0000}$\uparrow$14.9\%)}                  & 97.8\%(\color[HTML]{34FF34}$\downarrow$2.2\%)                                                       & \textbf{98.9\%(0\%)}                   \\ \hline
\end{tabular}}
     \caption{The performance comparison of LLMs and MA-GTS on G-REAL and two benchmarks is shown. Red text indicates MA-GTS’s accuracy improvement over the best LLM, while green text highlights the opposite. GPT-4o-mini was used as the base model for MA-GTS.}
    \label{tab:2}
         \vspace{-15pt}
\end{table*}
\subsection{Data Collection}
To mitigate the risk of data contamination in LLMs, which could lead to biased test accuracy due to prior exposure to training data, G-REAL employs several techniques, including randomized node naming, synthetic node descriptions, added textual noise, and randomly structured graph representations. Node names are generated by randomly combining the 26 letters of the alphabet, and synthetic node descriptions are created with arbitrary textual representations. For example, a node may be described as: \textit{"Amber Plaza: A bustling central square surrounded by cafes, boutiques, and street performers."} These fictional descriptions ensure that LLMs cannot leverage prior knowledge, maintaining the integrity of the evaluation.

To improve dataset realism and obscure graph structure, we introduce textual noise to each instance, simulating real-world graph problems embedded in unstructured text. Graph structures are randomly generated, with each node assigned a unique name to reduce prior LLM exposure. Optimal and approximate solutions are generated for each problem type using established algorithms, providing benchmarks for evaluating both LLM and MA-GTS performance.

\subsection{Data Statistics }
To evaluate our framework’s effectiveness in real-world graph-theoretic problems, we construct test datasets with graph sizes from 8 to 25 nodes for each problem type. Each sub-dataset includes 50 instances with distinct structures, offering both optimal and approximate solutions for a comprehensive assessment of robustness and generalization. A statistical summary is provided in Table \ref{tab:1}.




\subsection{Evaluation}
For the TSP, Minimum Graph Coloring Problem, and Minimum Vertex Cover Problem, the unique nature of their outputs requires a dual evaluation approach. Specifically, the results consist of both a set of selected nodes and the final computed solution. To comprehensively assess the graph reasoning capabilities of LLMs, we consider both types of outputs as evaluation metrics. The model's performance is measured by verifying the accuracy of both the selected node set and the computed solution. The methodology for calculating the final accuracy is as follows:
$\text{$ACC_{ALL}$} = 0.5 \cdot ACC_{\text{nodes}} + 0.5 \cdot ACC_{\text{result}},$ where $ACC_{\text{nodes}}$ and $ACC_{\text{result}}$ represent the accuracy of the node set and the predicted values, respectively, with a value of 1 for correct predictions and 0 for incorrect ones.
% $ACC_{(\text{nodes}, \text{result})} = 
% \begin{cases} 
% 1, & \text{if answer is correct}, \\
% 0, & \text{otherwise}.
% \end{cases}$

% \titlespacing*{\subsection}{0pt}{0.5ex plus 0.2ex minus 0.2ex}{0pt}
% \titlespacing*{\section}{0pt}{0.5ex plus 0.2ex minus 0.2ex}{0pt}

\section{Experiments Setup}
\subsection{Datasets}
% To facilitate a comprehensive evaluation of the MA-GTS framework's reasoning capabilities across various types, complexities, and domains of graph-theoretic problems, we employed the \textbf{G-REAL} dataset along with two additional benchmark datasets, \textbf{GraCoRe}\cite{yuan2025gracore} and \textbf{NLGraph}\cite{NLgraph-6}. Together, these datasets encompass six distinct graph-theoretic tasks. From these benchmarks, we selected three sub-tasks for evaluation: the TSP, the shortest path problem, and the Cycle problem. Notably, both the GraCoRe and G-REAL datasets include instances of the TSP; however, the TSP problem in G-REAL is more complex and corresponds to real-world scenarios, implicitly incorporating graph structure data. By comparing the performance of the same model on these two TSP instances, we can assess its ability to handle more intricate graph-theoretic problems. Additionally, the simpler graph-theoretic tasks in NLGraph serve to evaluate the generalization and robustness of the MA-GTS framework. The differences between these three datasets are summarized in Table \ref{tab:1}.
To evaluate the reasoning capabilities of the MA-GTS framework across various graph-theoretic problem types, complexities, and domains, we used the \textbf{G-REAL} dataset alongside two benchmark datasets, \textbf{GraCoRe}\cite{yuan2025gracore} and \textbf{NLGraph}\cite{NLgraph-6}, covering six distinct graph-theoretic tasks. We selected three sub-tasks for evaluation: the TSP, shortest path problem, and Cycle problem in GraCoRe and NLGraph. Notably, both GraCoRe and G-REAL include TSP instances; however, the G-REAL TSP is more complex and reflects real-world scenarios with implicit graph structure data. By comparing performance on these two TSP instances, we assess the model's ability to handle more intricate problems. The simpler tasks in NLGraph evaluate the generalization and robustness of the MA-GTS framework. A summary of the differences between these datasets is provided in Table \ref{tab:1}.
\subsection{Baselines and Foundation Model}
We compared three of OpenAI's latest closed-source models:  \textit{o3-mini},  \textit{GPT-4o-mini}, and  \textit{GPT-3.5}\cite{gpt4}. Additionally, we evaluated two of the most recent open-source models:  \textit{Llama3-7b}\cite{llama} and  \textit{Qwen2.5-7b}\cite{qwen}. For the evaluation methodology, we adopted both direct inference and CoT reasoning approaches. For the foundation model, we selected the GPT-4o-mini model. Regarding the final test results, for each task, we used the accuracy of the final computed solution as the primary evaluation metric. More details about models in Appendix \ref{sec:B}.
\section{Results and Analysis}
In this section, we evaluate the performance of our framework against other LLMs on graph theory problems, with results presented in Table \ref{tab:2}. MA-GTS outperforms all baselines, achieving state-of-the-art results and matching the performance of the leading o3-mini model on simpler problems. We also assess the MA-GTS framework from multiple perspectives.
\begin{table*}[t]
	\centering
	\scriptsize
	\setlength{\extrarowheight}{2.5pt} 
     \resizebox{\linewidth}{!}{
\begin{tabular}{c|ccc|ccc|ccc}
\hline
\rowcolor[HTML]{EFEFEF} 
                                                              & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{TSP}}                 & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{Coloring}}            & \multicolumn{3}{c}{\cellcolor[HTML]{EFEFEF}\textbf{VertexCover}}          \\
\rowcolor[HTML]{EFEFEF} 
                                                              & \textbf{Inp.Tokens(k)} & \textbf{Out.Tokens(k)} & \textbf{Price(\$)}      & \textbf{Inp.Tokens(k)} & \textbf{Out.Tokens(k)} & \textbf{Price(\$)}          & \textbf{Inp.Tokens(k)} & \textbf{Out.Tokens(k)} & \textbf{Price(\$)}          \\ \hline
\cellcolor[HTML]{EFEFEF}\textit{o3-mini}                      & 2.31                   & 4.98                   & 0.0244                  & 0.69                   & 6.83                   & 0.0309                  & 0.6                    & 9.73                   & 0.0443                  \\
\cellcolor[HTML]{EFEFEF}\textit{\textbf{MA-GTS(GPT-4o-mini)}} & 13.32                  & \textbf{4.56\color[HTML]{CB0000}($\downarrow$8.4\%)}   & \textbf{0.0047\color[HTML]{CB0000}($\downarrow$80.7\%)} & 6.79                   & \textbf{2.57\color[HTML]{CB0000}($\downarrow$62.4\%)}  & \textbf{0.0025\color[HTML]{CB0000}($\downarrow$91.9\%)} & 6.39                   & \textbf{2.31\color[HTML]{CB0000}($\downarrow$76.2\%)}  & \textbf{0.0023\color[HTML]{CB0000}($\downarrow$94.8\%)} \\ \hline
\end{tabular}}
     \caption{Comparison of inference costs between MA-GTS and o3-mini model on G-REAL.}
    \label{tab:3}
      \vspace{-15pt}
\end{table*}
\subsection{Performance on real-world problems}
As shown in Table \ref{tab:2}, G-REAL provides three real-world graph theory problems, with the TSP being the most complex. Based on the results from these three problems, MA-GTS demonstrates superior performance, achieving an accuracy rate exceeding 90\% across all tests. Notably, in the case of the TSP, MA-GTS outperforms the o3-mini model by 82\%. Furthermore, when compared to the GPT-4o-mini model, MA-GTS significantly improves its performance from 3.1\% to 94.9\%, marking a substantial increase. This clearly underscores the effectiveness of our framework. Additionally, it is evident that, aside from the o3-mini model, other models exhibit subpar performance on the G-REAL dataset. It is particularly interesting that the performance gap between the two open-source and two closed-source models is minimal, suggesting that the complexity of the problems may lead to a consistent decline in performance, an issue that warrants further investigation. Overall, MA-GTS stands out for its advanced capabilities and generalization when handling complex graph theory problems.
% \titlespacing*{\subsection}{0pt}{0.5ex plus 0.2ex minus 0.2ex}{0pt}
% \titlespacing*{\section}{0pt}{0.5ex plus 0.2ex minus 0.2ex}{0pt}
\subsection{Performance on simple problem}
Table \ref{tab:2} shows that for simpler graph theory problems, such as the Shortest Path and Cycle problems from the NLGraph dataset, the o3-mini model performs exceptionally well, with MA-GTS also showing strong results. Specifically, for the Shortest Path problem, the gap between MA-GTS and o3-mini is just 2.2\%, and MA-GTS performs equally well on the Cycle problem. In contrast, other models perform less satisfactorily. The MA-GTS framework, based on the GPT-4o-mini model, significantly enhances the accuracy of the 4o model, bringing it on par with the o3-mini. Overall, MA-GTS demonstrates excellent performance across diverse textual descriptions and graph structures, highlighting its remarkable generalization capabilities.
\subsection{G-REAL effectiveness analysis}
To evaluate the performance of LLMs and MA-GTS on real-world graph theory problems, we constructed the G-REAL dataset. As shown in Table \ref{tab:2}, the performance of existing LLMs on the G-REAL dataset is suboptimal. To validate the effectiveness of this dataset, we compared it with the TSP problem from the GraCoRe Benchmark, testing problems with node sizes ranging from 8 to 25, consistent with the scale of G-REAL. From this comparison, we observe that on the G-REAL dataset, which includes text complexity, added text noise, and node name shuffling, the o3-mini model performs poorly, with its accuracy dropping from 79.7\% in GraCoRe to 11.8\%. In contrast, the MA-GTS framework appears unaffected by the complexities of real-world graph theory problems, maintaining performance above 90\%. This result indirectly supports the validity of the G-REAL dataset and demonstrates the stability of the MA-GTS framework.
\begin{figure}[t]
\centering
  \includegraphics[width=0.5\textwidth]{pic/graph_size.pdf}
  \caption {Performance of different problems across varying node numbers, showing results for MA-GTS and o3-mini.}
    \label{fig:3} 
\end{figure}
\begin{table}[t]
\centering
\begin{adjustbox}{max width=\linewidth} % 调整表格宽度限制
\begin{tabular}{
>{\columncolor[HTML]{EFEFEF}}c |
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c 
>{\columncolor[HTML]{FFFFFF}}c |
>{\columncolor[HTML]{FFFFFF}}c }
\hline
                               & \multicolumn{3}{c|}{\cellcolor[HTML]{EFEFEF}\textbf{G-REAL}}                                                                    & \cellcolor[HTML]{EFEFEF}                                              \\
                               & \cellcolor[HTML]{EFEFEF}\textbf{TSP} & \cellcolor[HTML]{EFEFEF}\textbf{Coloring} & \cellcolor[HTML]{EFEFEF}\textbf{Vetex Cover} & \multirow{-2}{*}{\cellcolor[HTML]{EFEFEF}\textbf{Average error rate}} \\ \hline
\textit{GPT-4o-mini(Tool use)} & 30.8\%                               & 39.0\%                                    & 4.6\%                                        & 75.0\%                                                                \\ \hline
\textit{w/o IEL}               & 12.5\%                               & 42.2\%                                    & 14.6\%                                       & 19.4\%                                                                \\
\textit{w/o KIL}               & 7.8\%                                & 37.1\%                                    & 12.8\%                                       & 1.0\%                                                                 \\
\textit{w/o AEL}               & 4.6\%                                & 32.1\%                                    & 7.4\%                                        & 3.2\%                                                                 \\ \hline
\textbf{MA-GTS(GPT-4o-mini)}   & \textbf{94.9\%}                      & \textbf{94.5\%}                           & \textbf{93.2\%}                              & \textbf{0.5\%}                                                        \\ \hline
\end{tabular}
\end{adjustbox}
     \caption{Testing Results and Error Analysis of Ablation Experiments for Each Layer of MA-GTS. "Tool use" refers to the utilization of only the algorithm library we have constructed.}
    \label{tab:4}
  \vspace{-15pt}
\end{table}
\subsection{Impact of Node Size}
To evaluate the impact of node scale on LLMs in complex graph theory problems, we tested the performance of MA-GTS and the o3-mini model on four complex graph problem datasets, with node sizes ranging from 8 to 25. The results, shown in Figure \ref{fig:3}, clearly demonstrate that as the number of nodes increases, the performance of the o3-mini model deteriorates, particularly in the TSP problem from G-REAL. For node sizes greater than 20, the o3-mini model is unable to produce correct answers. In contrast, under the MA-GTS framework, the effect of node size is less pronounced. Even with more than 20 nodes, MA-GTS maintains high prediction accuracy and stability. This highlights both the effectiveness and superiority of our framework.
\subsection{Cost Analysis}
Since MA-GTS requires multiple agent calls to model APIs for inference, cost considerations arise. To address this, we compared the inference costs of MA-GTS based on the GPT-4o-mini model with the o3-mini model, as shown in Table \ref{tab:3}. Surprisingly, MA-GTS incurs significantly lower costs than the o3-mini model. The o3-mini model, in contrast, has hidden reasoning tokens during inference, leading to long, concealed reasoning processes even in direct inference scenarios. As shown in the table, the inference cost of MA-GTS is about one-tenth to one-twentieth of the o3-mini model, requiring far fewer inference tokens. Moreover, MA-GTS achieves far better results than o3-mini, demonstrating its high cost-effectiveness in delivering more accurate outcomes at a lower cost.
\subsection{Ablations Studies and Analyses}
To validate the effectiveness of each layer in MA-GTS, we conducted ablation experiments, with results shown in Table \ref{tab:4}. The table demonstrates that each layer is crucial, and removing any layer significantly affects the final results. Although the IEL layer has the smallest impact on accuracy, its absence leads to a substantial increase in error rate (19\%), highlighting its role in maintaining stability. The absence of the AEL layer results in the greatest accuracy loss. Even when a module is removed, MA-GTS still improves the accuracy of the base model, validating the framework's effectiveness. Additionally, when inference is performed using only the GPT-4o-mini model with the constructed algorithm library, accuracy improves, but the error rate remains high (75\%). For graph sizes larger than 10 nodes, the model struggles to correctly invoke algorithms, further demonstrating the robustness and generalizability of MA-GTS.



\section{Conclusion}
This paper introduces MA-GTS, a Multi-Agent Framework for solving real-world graph theory problems, validated using the G-REAL dataset. Performance comparisons across various LLM models show that MA-GTS achieves high accuracy, stability, and cost-effectiveness, excelling in both complex and simpler graph problems. With accuracy consistently above 90\%, MA-GTS outperforms traditional LLM approaches, maintaining stability across different problem scales and being well-suited for larger graphs. Future work will focus on scaling to even larger problems and improving cost-efficiency.
\section*{Limitations}
Although the MA-GTS framework demonstrates significant advantages in addressing complex graph-theoretic problems, several limitations remain. First, while the G-REAL dataset provides valuable support for validating the framework's effectiveness, it may not fully capture the diversity of real-world graph problems, thus limiting the generalizability of the framework. Second, the MA-GTS framework may still require substantial computational resources when handling large-scale problems, particularly in resource-constrained environments. Moreover, despite the improvements made in enhancing LLMs' graph structure modeling capabilities, LLMs may still encounter performance bottlenecks when dealing with graphs that exhibit highly dependent relationships or specialized structures. Finally, the current capabilities of open-source model invocation tools are insufficient, which may impact the stability of the MA-GTS framework.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}
\clearpage
\appendix

\section{MA-GTS Details}
\label{sec:A}
\subsection{Graph Theory Knowledge Base}
\label{subsec:A1}
The Graph Theory Knowledge Base is a graph theory problem database that we have constructed, containing a wide range of common graph theory problems encountered in daily life, including both complex and simple ones. Each problem is associated with multiple optimal or approximate solution algorithms. For each algorithm, we provide a detailed description of its complexity, applicable conditions, and parameter settings, though it does not include corresponding code. This database can serve as a reference book for agents in graph theory. A specific example can be seen in Figure \ref{fig:4}.
\subsection{Graph Theory Algorithm Library}
\label{subsec:A2}
The Graph Theory Algorithm Library is a Python code repository that we have constructed, containing code corresponding to the graph theory algorithms in the Graph Theory Knowledge Base. This ensures the correctness of input parameters and helps maintain the stability of the MA-GTS framework. Each code snippet is accompanied by detailed parameter descriptions and is designed to accommodate various types of graph structure representations. A specific example can be seen in Figure \ref{fig:5}.
\subsection{Prompt Templates}
In this section, I will introduce the prompts for each agent, which will be displayed in Figures \ref{fig:6} to \ref{fig:11}.
\label{subsec:A3}
\section{Details on baseline models}
\label{sec:B}
We evaluated 5 of the latest LLMs, including OpenAI o3-mini reasoning model, launched on January 31, 2025. Table \ref{tab:5} presents more details on the models and their versions.
\section{Details on G-REAL}
\label{sec:C}
Existing graph theory benchmarks do not align with real-world scenarios. To better evaluate the ability of MA-GTS in solving graph theory problems in practical contexts and to test the performance gap between LLMs on structured textual graph data and implicit representations, we constructed the G-REAL dataset. This dataset contains three common real-world problems, with detailed information provided in the G-REAL section. It generates problem graphs of varying scales by randomly encoding node names and structures, with the naming conventions and sample problems illustrated in Figures \ref{fig:12} to \ref{fig:15}.
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/kgb.pdf}
  \caption {Details of Graph Theory Knowledge Base}
    \label{fig:4} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/cdb.pdf}
  \caption {Details of Graph Theory Algorithm Library}
    \label{fig:5} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/TIEA.pdf}
  \caption {Details of TIEA}
    \label{fig:6} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/PIEA.pdf}
  \caption {Details of PIEA}
    \label{fig:7} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/GSIEA.pdf}
  \caption {Details of GSIEA}
    \label{fig:8} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/SGIA.pdf}
  \caption {Details of SGIA}
    \label{fig:9} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/GTA.pdf}
  \caption {Details of GTA}
    \label{fig:10} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/ASA.pdf}
  \caption {Details of ASA}
    \label{fig:11} 
\end{figure*}
\begin{table*}[htbp]
    \centering
    \small
    \resizebox{\linewidth}{!}{

    \begin{tabular}{ccc}
        \toprule
        \textbf{Model}           & \textbf{Version}           & \textbf{Model Link} \\ 
        \midrule
        \textit{OpenAI o3-mini}       & o3-mini                    & \url{https://platform.openai.com/docs/models/o1\#o3-mini} \\
        \textit{GPT-4o-mini}          & gpt-4o-mini                     & \url{https://platform.openai.com/docs/models/gpt-4o-mini} \\

        \textit{GPT-3.5}         & gpt-3.5-turbo              & \url{https://platform.openai.com/docs/models/gpt-3-5-turbo}\\

        \textit{Llama3-ins-8b}   & Meta-Llama-3-8B-Instruct   & \url{https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct}\\

        \textit{Qwen2.5-7b-ins}    & Qwen2.5-7B-Instruct          & \url{https://huggingface.co/Qwen/Qwen2-7B-Instruct}\\

        \bottomrule
    \end{tabular}}
    \caption{More details about models.}
    \label{tab:5}
\end{table*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/place.pdf}
  \caption {Details of Random Places}
    \label{fig:12} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/tsp.pdf}
  \caption {Details of TSP}
    \label{fig:13} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/coloring.pdf}
  \caption {Details of Coloring Problem}
    \label{fig:14} 
\end{figure*}
\begin{figure*}
\centering
  \includegraphics[width=\textwidth]{pic/vertexcover.pdf}
  \caption {Details of Vertex Cover Problem}
    \label{fig:15} 
\end{figure*}
\end{document}



