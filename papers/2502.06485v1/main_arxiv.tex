\documentclass{proc}
\usepackage{xspace}
\usepackage[dvipsnames]{xcolor}

\usepackage{tikz}
\usetikzlibrary{decorations.pathreplacing,calligraphy,calc,positioning, tikzmark, intersections}
\usepackage{kbordermatrix}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs} %
\usepackage{multirow}

\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}


\bibliographystyle{apalike}

\usepackage{hyperref}

\newcommand{\suppmat}{appendix\xspace}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}

\usepackage{algorithm}
\usepackage{algorithmic}


\input{math_commands}

\newcommand{\ourmodel}{\textsc{WyckoffDiff}\xspace}

\title{WyckoffDiff -- A Generative Diffusion Model for Crystal Symmetry}

\author{Filip Ekström Kelvinius, Oskar B. Andersson, Abhijith S. Parackal, 
Dong Qian, \\Rickard Armiento, Fredrik Lindsten
\\
Linköping University
\\
\texttt{\{filip.ekstrom, oskar.andersson\}@liu.se}
}

\begin{document}
\maketitle

\begin{abstract}
Crystalline materials often exhibit a high level of symmetry. However, most generative models do not account for symmetry, but rather model each atom without any constraints on its position or element. We propose a generative model, Wyckoff Diffusion (\ourmodel), which generates symmetry-based descriptions of crystals. This is enabled by considering a crystal structure representation that encodes all symmetry, and we design a novel neural network architecture which enables using this representation inside a discrete generative model framework. In addition to respecting symmetry by construction, the discrete nature of our model enables fast generation. We additionally present a new metric, Fréchet Wrenformer Distance, which captures the symmetry aspects of the materials generated, and we benchmark \ourmodel against recently proposed generative models for crystal generation.
\end{abstract}

\section{Introduction}
\input{figures/graphical_abstract}
Materials science is a field of research that is essential for technological advancement. With machine learning seeing success in a variety of fields, materials science is no exception. In the search for new materials, so called generative models are an attractive class of methods, and a number of models that can generate new materials have been developed \citep[see, e.g.,][for an overview]{park_has_2024}. However, \emph{crystalline} materials are often characterized by their specific symmetries, which are integral to their materials properties. This is an aspect that only recently has been built into generative models \citep{jiao_space_2024,zhu_wycryst_2024,levy_symmcd_2024}. Instead, models without any built-in mechanisms that ensure symmetry in materials have and are still being developed \cite{xie2022crystal, jiao_crystal_2023, merchant_scaling_2023, mattergenNature}. As demonstrated by several works \citep{levy_symmcd_2024, cheetham_artificial_2024, mattergenNature}, materials generated from methods without these explicit constraints often lack the symmetrical characteristics of materials found in databases. For example, \citet{cheetham_artificial_2024} find that roughly 34 \% of the materials generated by the GNoME model \citep{merchant_scaling_2023} belong to four different space groups of which only one exists in the Inorganic Crystal Structure Database \cite{Belsky2002} where it makes up only 1 \%, and \citet{mattergenNature} mention that their MatterGen model tends to generate less symmetric structures than are present in the training data.

The symmetry of a material can be encoded in a \textit{protostructure} description \citep[see also \Cref{sec:crystal_rep}]{parackal_identifying_2024}, where elements occupy Wyckoff positions in crystal structures categorized into space groups. This description avoids specifying the exact atomic coordinates, while maintaining the key structural information, which has been shown to be efficient for searching for novel stable materials by enabling an initial step where candidate crystal structures with high likelihood of being stable are identified based on the symmetry description alone. This step avoids wasting computational resources on exact coordinate calculations across all possible materials \citep{goodall_rapid_2022}. 
Additionally, the infinite space of continuous coordinates also opens the risk of generating degenerate materials or structures outside of the symmetry proximity. 
Since materials of high symmetry are generally the interesting materials to explore, generation of large sets of low symmetry materials is inefficient.
Explicitly encoding symmetry could allow a generative model to only generate within a space of interesting materials of higher symmetry, allowing a symmetry-infused generative model to generate a broader variety of relevant crystalline materials compared to a generative model using exact coordinate representations.%

Explicitly enforcing knowledge about symmetry in generative models for crystal structures is currently an underexplored research direction. %
Our approach is different from previous works in how we specifically target the generation of protostructures using a representation that enables the use of generative models \emph{for discrete data} to generate new materials. Our method shows competitive performance against other methods on various quantitative metrics. The generated protostructures can be used as part of a machine-learning based workflow for materials discovery to find new stable crystal structures. As a proof of concept, we realize a subset of the generated protostructures into crystal structures and from this set we highlight some examples with interesting and varied chemistries ($\mathrm{CsSnF}_6$, $\mathrm{NaNbO}_2$, and $\mathrm{Ca_2PI}$), which are on or below the currently known convex hull of thermodynamically stable compounds. 

\section{Background}
\subsection{Representing Crystals}
\label{sec:crystal_rep}
An ideal crystalline material is commonly represented by its \textit{crystal structure} as an infinitely repeating set of unit cells with atoms of specified \textit{chemical elements} placed at specific atomic positions. In the unit cell, the $M$ atoms are specified by their positions $X\in\R^{M\times 3}$ and elements $Z \in \mathbb{Z}^M$, and the geometry of the unit cell can be specified by three lattice vectors $L\in\R^{3\times 3}$. As an alternative, one can separately specify the symmetry of the atomic positions, and then specify the atomic coordinates only by precise values for the remaining \textit{degrees of freedom}. This representation is discussed in the following.

\vspace{0.5ex}
\textbf{Protostructures} ~ 
All possible combinations of symmetries of crystal structures can be categorized into 230 \textit{space groups} \cite{mullerSymmetryRelationshipsCrystal2013}. The atoms, each a chemical element from the periodic table of elements, can then occupy a so called \textit{Wyckoff position} in the crystal structure, which represents sets of points on which the symmetry operators act in a specific way. Hence, if an atom is specified to sit at a specific Wyckoff position, depending on the nature of that Wyckoff position, this declares it to reside exactly at a specific point; anywhere along a line; in a plane; or in a volume, and the symmetry operators then imply that equivalent atoms sit at a number (the \textit{multiplicity}) of other points in the unit cell, called the \textit{orbit}. 
These different Wyckoff positions are labeled using a letter from the Latin alphabet (a, b, c, etc.). %
The space group completely determines which Wyckoff positions that are available, as tabulated by The Volume of International Tables for Crystallography \citep{ITA2002}.

In this work, we use the term \textit{prototype} as defined for AFLOW prototype labels \citep{mehl_aflow_2017}, i.e., the combination of the spacegroup and how the Wyckoff positions are occupied by unspecified but distinct elements, without additional information about the remaining degrees of freedom for those occupied positions. In more detail, the AFLOW prototype label \texttt{ABC6\_hR24\_166\_a\_b\_h} specifies first the anonymous composition \texttt{AB6C} (i.e., $AB_6C$), then the Pearson symbol \texttt{hR24}, followed by the spacegroup number \texttt{166}, and a list of Wyckoff labels for the positions occupied by the distinct elements in the anonymous formula, \texttt{a\_h\_b} (i.e., positions a, h, and b). Furthermore, following  \citet{parackal_identifying_2024} we use the term \textit{protostructure} to refer to a prototype where specific chemical elements are assigned to the Wyckoff positions (but where the degrees of freedom of the structure remains unspecified).
Protostructures can be labeled by extended AFLOW prototype labels, e.g., \texttt{AB6C\_hR24\_166\_a\_h\_b:Cs-F-Sn}, to indicate that the previously anonymous elements $A$, $B$, and $C$ are \texttt{Cs-F-Sn} (Cs, F, Sn), which occupy the spacegroup 166 Wyckoff positions \texttt{a\_h\_b} (a, h, b)\footnote{Note that the canonicalization of the protostructure compared to the prototype is different, due to protostructures being canonicalized based on alphabetical element order.}.




\subsection{Diffusion Models}
\label{sec:diffmodels_background}
Diffusion models \citep{sohl-dickstein_deep_2015,ho_denoising_2020-2,song_score-based_2021} are a type of generative models that have received tremendous interest lately. In essence, they are based on the idea of starting from a pure noise sample $\x_T$, which is iteratively ``denoised'' to end up with a ``clean'' sample $\x_0$. This denoising is enabled by viewing the data-to-noise (forward) process as a fixed Markov chain
\begin{align}
    q(\x_{0:T}) = q(\x_0)\prod_{t=0}^{T-1}q(\xtplusone|\xt),
\end{align}
where $q(\x_0)$ is the data distribution and the transitions $q(\xtplusone|\xt)$ are designed such that, for large $T$, $q(\x_T)$ converges to a distribution $p(\x_T)$ from which we can easily sample, like a Gaussian distribution in case of continuous variables. The reverse process is then parametrized as
\begin{align}
    p_\theta(\x_{0:T}) = p(\x_T)\prod_{t=0}^{T-1}p_\theta(\xt|\xtplusone),
\end{align}
where $p_\theta(\xt|\xtplusone)$ are fitted such that $p_\theta(\xt|\xtplusone) \approx q(\xt|\xtplusone)$. Sampling according to the reverse process will then give (approximate) samples from the data distribution $q(\x_0)$.

While most diffusion models have been developed for continuous data, there are also several methods designed for the discrete case \citep[e.g.,][]{hoogeboom_argmax_2021-1,austin_structured_2021,campbell_continuous_2022,sun_score-based_2023,lou_discrete_2024}. Conceptually, the idea is the same, but the transitions (both in the forward and backward directions) operate on discrete state-spaces and the limiting distribution $p(\x_T)$ is typically chosen to factorize over the components of $\x_T$ to enable easy sampling. In this work we make explicit use of the method D3PM by \citet{austin_structured_2021}, which we explain in more detail in the context of our model in \Cref{sec:wyckoffdiff}.

\subsection{Related Work}
\paragraph{CDVAE}
The Crystal Diffusion Variational Autoencoder (CDVAE) \citep{xie2022crystal} is a generative model for crystal structures that combines a variational autoencoder (VAE) with a diffusion model. Generation from CDVAE starts with sampling from the VAE: a vector $z\sim\mathcal{N}(0, I)$ is sampled from which the lattice vectors $L$, the number of atoms $M$, and the initial composition are decoded. The positions of the $M$ atoms are randomly initialized, and the elements are randomly assigned according to the decoded composition. The diffusion process then consists of denoising the positions and elements, conditioned on $z$, while keeping $L$ fixed during the full process. The positions and atoms are updated without any explicit or built-in constraints with respect to symmetries. 


\paragraph{DiffCSP and DiffCSP++}
DiffCSP \citep{jiao_crystal_2023} builds upon CDVAE by replacing the VAE with a diffusion model that jointly learns the lattice and coordinates, enabling more precise modeling of crystal geometry. \mbox{DiffCSP++} \cite{jiao_space_2024} further incorporates space group symmetry by leveraging pre-defined structural templates from the training data to learn atomic types and coordinates aligned with these templates. However, this might limit the diversity and novelty of the generated materials.

\paragraph{SymmCD}
To address this limitation, SymmCD \citep{levy_symmcd_2024} introduces a physically-motivated representation of symmetries as binary matrices, enabling efficient information-sharing and generalization across both crystal and site symmetries. By explicitly incorporating crystallographic symmetry into the generative process, SymmCD can generate diverse and valid crystals with realistic symmetries and predicted properties.


\section{Wyckoff Diffusion}
\label{sec:wyckoffdiff}
\subsection{Representing a Protostructure}
\label{sec:wyckoffpos}
Given a space group $s\in G = \{1, \dots, 230\}$, we denote the set of all possible Wyckoff positions as $L(s)$\footnote{All possible Wyckoff positions can be found in \citet{ITA2002}.}. 
To represent a protostructure, we partition the set of Wyckoff positions into the positions without degrees of freedom (i.e., an atom occupying the position is limited to a fixed point in space) and the positions with degrees of freedom (i.e., an atom occupying the position can be positioned anywhere on a line, in a plane, or in a volume). We call these \emph{constrained} and \emph{unconstrained} positions, and use the notation $L_{0}(s) \subset L(s)$ and $L_{\infty}(s) \subset L(s)$ for the respective sets. 
Although unconstrained Wyckoff positions can virtually be occupied by any number of atoms, in our modeling, a maximum of $P$ atoms of each type can occupy an unconstrained Wyckoff position (which means the unit cell has $P$ times the multiplicity of that Wyckoff position of such atoms). We denote $N_a$ as the largest atomic number under consideration. Both $N_a$ and $P$ can be determined from training data. Conditionally on the space group $s$, the unconstrained positions can then be represented by $\num \in \mathbf{M}_{\infty} = \{0, 1, \dots, P\}^{|L_{\infty}(s)| \times N_a}$, i.e., each element $\num_{(i,j)} \in \{0, 1, \dots, P\}$ is the number of atoms of type $j$ occupying the unconstrained Wyckoff position $i$. A constrained position, however, can only be occupied by 0 or 1 atoms (as the positions are restricted to a fixed point in space). Therefore, we represent the elements of the atoms occupying each of these positions as $\type \in \mathbf{M}_0 = \{0, \dots, N_a\}^{|L_{0}(s)|}$, where the value $0$ corresponds to no atom occupying the position. To summarize, a protostructure can be described as the tuple\footnote{For ease of notation, we have omitted the dependence of $\mathbf{M}_{\infty}$ and $\mathbf{M}_0$ on $s$.}%
\begin{align}
    (s, \num, \type) \in
    G 
    \times \mathbf{M}_{\infty}
    \times \mathbf{M}_0. \label{eq:general_description}
\end{align}
\vspace{0.1ex} %

\subsection{Model Overview}
Given our representation of a protostructure in \Cref{eq:general_description}, we now aim to sample from the (unknown) distribution $\pdata(s, \type, \num)$. Since the space group determines the number of Wyckoff positions, we propose to first sample a space group $s$, and then sampling the remaining variables conditioned on $s$. Using the representation $(s, \type, \num)$ ensures that we sample a valid material where constrained positions are occupied by at most one atom.
As an estimation of the distribution of $s$, we can use the empirical training data distribution $\ptrain(s)$, and write our model of $\pdata(s, \type, \num)$ as
\begin{align}
    \ptheta(s, \type, \num) = \ptrain(s) \ptheta(\type, \num|s),
\end{align}
where $\ptheta(\type, \num|s)$ is a diffusion model. We will in the next sections describe how we design $\ptheta(\type, \num|s)$, and when doing so, we will for simplicity use the notation $\x$ as the concatenation $(\type, \num)$, as well as keeping the conditioning on $s$ implicit. \Cref{algo:wyckoffdiff} outlines the full generation of a material using \ourmodel. 

\subsection{Discrete Diffusion}
As both $\type$ and $\num$ are discrete variables, we will use the Discrete Denoising Diffusion Model (D3PM) \citep{austin_structured_2021} as our underlying diffusion model. In this framework, a datapoint is denoted as $\x = (x^1, \dots, x^D)$ where each variable $x^k$ is a discrete variable, and ``noise'' is added independently to each variable according to a discrete Markov chain. By denoting $\xtk$ as a one-hot encoding of the $k$:th variable $x^k$ at sampling time $t$, the Markov forward process (cf. the general description in \Cref{sec:diffmodels_background}) can be written as
\begin{align}
    q(\xtplusonek|\xt) = \xtk Q_{t+1},
\end{align}
with $Q_{t+1}$ being a transition matrix, and $q(\xtplusone|\xt) = \prod_{k=1}^Dq(\xtplusonek|\xt)$. The matrices $Q_{t+1}$ are chosen so that the stationary distribution ($q(\xpriork)$ for large $T$) is a simple distribution (we discuss this choice in \Cref{sec:chooosing_Qt}). The variables $\xtk$ are assumed conditionally independent given $\xtplusone$ in the backward process, i.e., $p_\theta(\xt|\xtplusone) = \prod_{i=1}^D p_\theta(\xtk|\xtplusone)$, and as the backward distribution $q(\xtk|\xtplusone, \x_0^k)$ can be computed exactly, the backward process $p_\theta(\xtk|\xtplusone)$ is parametrized as a marginalization over all possible $\x_0^k$,
\begin{align}
    p_\theta(\xtk|\xtplusone) = \sum_{\x_0^k}q(\xtk|\xtplusone, \x_0^k)p_\theta(\x_0^k|\xtplusone). \label{eq:d3pm_post}
\end{align}
In other words, to use this framework, it is necessary to determine a suitable noise process (i.e., choosing the matrices $Q_{t+1}$), and construct and train a model which can predict the ``clean'' variable $\x_0^k$, given a noisy sample $\xtplusone$ (i.e., the model $p_\theta(\x_0^k|\xtplusone)$).

\subsection{WyckoffGNN -- Neural Network Backbone}
For the parametrization of $p_\theta(\x_0^k|\xtplusone)$, we design a novel neural architecture, WyckoffGNN, that takes a ``noisy'' data point $\xtplusone$ as input, and outputs $D$ different probability vectors, where $D$ is the number of variables. This means that for the Wyckoff representation in \Cref{eq:general_description}, the neural network needs to predict the probabilities for $D=|L_{\infty}(s)| \times N_a + |L_0(s)|$ different categorical distributions. To do this, we view each Wyckoff position in $L(s)$ as a node in a fully connected graph. As different space groups have different number of Wyckoff positions, using the graph representation and processing this with a graph neural network (GNN) gives us the flexibility to utilize a single model for all space groups. The GNN is used to encode each position as a vector in $\R^d$, and we then use a neural network to decode the vectors into the corresponding probability distributions. An illustration of this can be found in \Cref{fig:graph_repr}.

\input{algorithms/WyckoffDiff}
\paragraph{Encoding Wyckoff Positions}
The encoding of Wyckoff positions starts with an initial set of vectors $\{\hidden^0_i\}_{i=1}^{|L(s)|}$, one for each Wyckoff position. These encode the atoms occupying the respective positions, i.e., $d$-dimensional vector embeddings of the atom types on the positions in $L_0$, and the number of each element on the positions $L_\infty$ (see more details in \Cref{app:gnn}). Additionally, we have a set of static vectors $\{\hidden^\text{pos}_i\}_{i=1}^{|L(s)|}$ which encode information about the position like the Wyckoff letter and the number of degrees of freedom, but also the space group $s$ and the sampling time $t$ (again, in the form of high-dimensional embedding vectors, see \Cref{app:gnn}). We then design the $l$:th update of the vectors as first concatenating $\hidden_i^{l-1}$ with its corresponding $\hidden_i^\text{pos}$, and then one layer of a message-passing neural network \citep{gilmer_neural_2017} where first, for each Wyckoff position, a message $\rvm_i^{l}$ is computed as $\rvm_i^l = \sum_{j=1}^{|L(s)|} M_l(\rvw_i, \rvw_j)$, where $\rvw_i$ and $\rvw_j$ are the aforementioned concatenation of vectors. The message $\rvm_i^l$ is hence an aggregation of messages sent between pairs of Wyckoff positions, and the purpose is to propagate information about the full material. As we do not have an inherent graph but rather assume a complete graph, we construct a message function $M_l(\cdot, \cdot)$ inspired by \citet[chapter~5.4]{bronstein_geometric_2021} where we use two multilayer perceptrons (MLPs, or fully connected neural networks). One MLP takes in the neighboring vector $\rvw_j$ and outputs a new vector $\rvw_j' = \MLP_\phi(\rvw_j)$, while the other takes as input a concatenation of $\rvw_i$ and $\rvw_j$ and outputs a scalar $a_{i,j}=\MLP_\theta(\text{cat}(\rvw_i, \rvw_j))$, which is multiplied with $\rvw_j'$, i.e., 
\begin{align}
    M_l(\rvw_i, \rvw_j) = a_{i, j}(\rvw_i, \rvw_j) \rvw_j'(\rvw_j).
\end{align}
The message $\rvm_i^l$ is hence a linear combination of transformations of the neighbor vectors $\rvz_j$. This message is then added to the current vector, so that the updated vector representation becomes $\hidden_i^l = \hidden_i^{l-1} + \rvm_i^l$.  Performing such updates $N$ times (i.e., a neural network with $N$ layers), we obtain our encoded positions as the vector representations $\{\hidden^N_j\}_{j=1}^{|L(s)|}$. Algorithms describing the GNN layer and the message function together with more details on hyperparameter choices can be found in \Cref{app:gnn}.

\paragraph{Decoding the Probabilities}
When we have obtained the encodings $\{\hidden^N_i\}_{i=1}^{|L(s)|}$ of the Wyckoff positions, we need to decode these into vectors of probabilities. For constrained Wyckoff positions, $L_0$, this corresponds to probabilities over which atom type (if any) that is occupying the position. For the unconstrained Wyckoff positions $L_\infty$, it instead corresponds to, for each atom type, the probabilities over the number of atoms of the corresponding atom type that occupies this position. As the output differs between these two types of positions, we use two different MLPs for the decoding. For the constrained positions, an MLP takes as input the representation $\hidden^N_i$ and outputs a single vector of probabilities over atomic numbers, where we use $0$ as ``no atom'' and only consider the atomic numbers $1$ to $N_a=100$, as there are no training data points involving higher atomic numbers. For the unconstrained positions, an MLP instead outputs $N_a$ different probability vectors over number of atoms, one for each atom type. Again, we use a truncated range of $0$ to $P=55$ based on training data. An algorithm outlining the full forward-pass of the neural network can be found in \Cref{algo:gnn_forward} in the \suppmat, together with more details in \Cref{app:gnn}.

\paragraph{Training}
To train our neural network, we start by sampling a time $t$ from the discrete uniform distribution $\text{Uniform}([1, \dots, T])$. Then, to sample $\xt \sim q(\xt|\x_0)$, we sample $\xtk \sim q(\xtk|\x_0) = \text{Categorical}(\rvp=\x_0^k \overline Q_t)$ independently for each $k\in\{1,\,\dots,\,D\}$, where $\overline Q_t = Q_1\cdots Q_{t-1} Q_t$, and the choice of $Q_t$ is described in \Cref{sec:chooosing_Qt}. The neural network takes as input this noisy sample $\xt$, and as in DiGress \citep{vignac_digress_2022}, we optimize the cross-entropy between the true sample $\x_0$ and the predicted distribution $p_\theta(\x_0|\xt)$. We also tried the variational objective by \citet{austin_structured_2021}, but the large state spaces made it unfeasible to fit into GPU-memory.
\input{tables/fwd_and_more}
\subsection{The Choice of $Q_t$}
\label{sec:chooosing_Qt}
\citet{austin_structured_2021} proposes a few different choices of $Q_t$. In our work, we use a matrix of the form
\begin{align}
    Q_t = (1-\beta_t)I + \beta_t \mathbbm{1} \rvm^T,
\end{align}
where $\beta_t$ is given by some user-defined schedule, $\mathbbm{1}$ is a vector of ones, and $\rvm$ is a vector of probabilities. With this transition matrix, a variable stays in its current state with probability $1-\beta_t$, and with probability $\beta_t$ it transitions to a new state sampled from a $\text{Categorical}(\rvp=\rvm)$ distribution. This is a general form for which the choice $\rvm=\mathbbm{1}/D$ gives rise to D3PM-uniform by \citet{austin_structured_2021}. In this general form, for large $T$, the limiting distribution $q(\xpriork)$ becomes $\text{Categorical}(\rvp=\rvm)$, and sampling from D3PM hence starts by sampling each variable $\xpriork$ from this distribution. Although using the uniform distribution could work, in case the data is very ``sparse'', for example in our case where most of the elements in the matrix representation in \Cref{sec:crystal_rep} are 0, using the uniform distribution as the limiting distribution could require many generation steps just to find the correct level of ``sparseness''. \citet{vignac_digress_2022} propose to use the empirical marginal distribution instead of the uniform distribution as $\rvm$. As we show in the experiments section, we find that using a marginal distribution, or a Dirac distribution at zero for all variables (i.e., starting from a material without any atoms at all), greatly improves the performance compared with using the uniform distribution.


\subsection{Evaluation Metric -- Fréchet Wrenformer Distance}\label{sec:fwddefine}
To evaluate a generative model, we strive to find a way of projecting materials into some lower-dimensional space, and draw conclusions about the difference between generated materials and real materials in this space. To do this, we take inspiration from the Fréchet Inception distance used for image generation \cite{heusel_gans_2017}, and propose the metric Fréchet Wrenformer distance (FWD). This metric computes the Wasserstein distance between Gaussian distributions fit with embeddings of the generated materials and training set, respectively, extracted from the pretrained Wrenformer \cite{riebesell2024matbenchdiscoveryframework}, which adapts the GNN-based model by \citet{goodall_rapid_2022} to a Transformer architecture \cite{vaswani_attention_2017} and is distributed with the \texttt{aviary} software\footnote{\url{https://github.com/CompRhys/aviary/tree/main}\label{foot:aviary}}. The FWD metric aims to capture the similarities of the generated materials with the training materials, while being invariant to exact geometry as the Wrenformer only takes into account the protostructure of the material. Similar developments have been done for chemical (Fréchet ChemNet distance, FCD \cite{preuer_frechet_2018}) and biological (Fréchet Biological distance, FBD \cite{stark2024dirichlet}) applications. 

\section{Numerical Evaluations}
\subsection{FWD, Novelty, and Uniqueness}
\label{sec:results_fwd}
The quantitative evaluation of our models uses the WBM dataset \citep{wbmDataset} created by substitution of chemical elements in the crystal structures available from the Materials Project (MP) \cite{jain_commentary_2013} to generate a total of 257k materials. 
We set aside 10k+10k materials as validation and test sets. We start by comparing \ourmodel with CDVAE \cite{xie2022crystal}, DiffCSP++ \citep{jiao_space_2024}, and SymmCD \cite{levy_symmcd_2024} as they constitute examples of models that to different degrees model crystal symmetry. Implementation details of these baseline methods can be found in \Cref{app:compared_methods}. It should be noted that we encountered some numerical issues during generation with SymmCD, resulting in \texttt{NaN} values, and we chose to discard these failed materials ($\sim 4 \%$ of samples, see more details in \Cref{app:compared_methods}). We also found that using \ourmodel with uniform initialization can produce a small amount ($\lesssim0.05\%$) of ``void'' materials with 0 atoms, which we also discarded.

As the focus of our work is on the generation of protostructures and the compared methods all generate full geometries, we convert these materials to AFLOW protostructures \cite{mehl_aflow_2017} using \texttt{aviary}\footref{foot:aviary}, with default tolerance parameters. For all methods, we generate \thsnd{10} protostructures and compute the FWD, novelty (Nov., the fraction of generated protostructures not present in the training set), and uniqueness (Uniq., fraction of unique protostructures among the generated). These results are presented in \Cref{tab:fwd-table}.
It should be noted that, in this discrete setting, we do not expect the novelty to be 1 even for a "perfect model". However, in a practical materials discovery setting we are mainly interested in the novel materials and, since  
FWD is a metric that benefits from sampling materials from the training set, we also compute FWD and uniqueness among only novel materials. To do this, we generate enough materials so that we have obtained \thsnd{10} novel protostructures from all methods. 

From \Cref{tab:fwd-table}, we first conclude that CDVAE, which does not incorporate any knowledge about symmetry, generates materials that are very dissimilar to the training distribution, as indicated by the very high FWD. We also notice that the choice of initial distribution in \ourmodel makes a big difference, and using the uniform distribution severely underperforms compared to initializing from the marginal distribution, or with completely empty materials. This highlights that even if the model is supposed to ``denoise'', starting from something that is closer to the actual data plays a big role. Compared to the baselines, we notice that the novelty for \ourmodel is somewhat lower, which seems to be connected with training time: numbers for models trained with only 10 \% of the number of steps shows a higher novelty. However, looking at sampling speed, \ourmodel is much faster as it does not generate full geometries, and hence, even if the novelty is lower, we produce more novel materials with the same amount of computation time, and we could view this ``novelty filer" as part of the generative procedure. Additionally, when computing FWD on only novel materials, \ourmodel outperforms all baselines, indicating that even if the protostructures are novel, they are to a larger extent faithful to the training distribution.

\subsection{Prototype Uniqueness}
In \Cref{sec:results_fwd}, materials were classified as different if their protostructures were different. Now, we consider only the prototypes
to evaluate the models' abilities to generate structural novelty. Among the \thsnd{10} novel protostructures, we count the number of unique and novel prototypes and present this in \Cref{tab:prototype-uniqueness}. We see that our model indeed generates new prototypes, which highlights that it is not merely learning a ``substitution-algorithm'', where it learns to use an already know structural template (i.e., the prototype) and just replace the elements. We also see that only CDVAE performs better in this regard, but as CDVAE has no restrictions in its generation, this is expected. However, when comparing to DiffCSP++ and SymmCD which do take symmetry into account, \ourmodel produces significantly higher number of unique and novel prototypes, showing its promise as a general generative model for crystal structures. 
\input{tables/prototype_uniqueness}

\subsection{Wren Energies}
To further investigate the protostructures generated by \ourmodel and get a sense of their usefulness, we compare the formation energies (i.e., the energy required to form a material from the pure elements, see \cref{app:novel_stable_materials} for more details) of the generated protostructures with those of the training set. To compute the formation energies, we rely on the same pretrained Wrenformer model as used for FWD (see \cref{sec:fwddefine}), which can predict the formation energy only given a protostructure. \Cref{fig:wren-energies} shows histograms of formation energies of protostructures generated by the zeros-initialization model. We see that the materials in general follow the same distribution as the training set, where the novel materials have a slight shift towards higher energies. A possible explanation is that the training data, ultimately derived from structures seen in experiments, samples the lowest energy structures thoroughly enough that the filtering on novel materials rejects more lower energy structures than higher energy ones. This further suggests the ability of \ourmodel to generate protostructures that are also physically plausible. We see overall the same results for the distributions for the other versions of \ourmodel, and present those in \Cref{app:wren_energies}.
\input{figures/wren_energies}

\input{figures/novel_structures_graphics} %

\section{Materials Discovery Using \ourmodel}
\label{sec:discoverypipeline}
We now demonstrate how \ourmodel fits into a materials discovery pipeline. Starting with a generation of \thsnd{20} novel crystal structures, \thsnd{10} from each of two \ourmodel models (\ourmodel-zeros and a previous iteration of \ourmodel-marginal; see supplementary material \Cref{app:previous-marginal-model}), we extract structures with chemical elements that are not noble gasses and where the underlying computational methods used for the training data are known to be more reliable, i.e., elements from the s-, p-, and d-blocks of the periodic table of elements. 

We then realize the resulting 12\thinspace650 protostructures into crystal structures by a process where we first semi-randomly assign values to the degrees of freedom of the Wyckoff positions using the \texttt{Pyxtal} library \citep{pyxtal} using the implementation in \texttt{aviary}\footref{foot:aviary}. Subsequently, we use the interatomic potential MACE\footnote{\url{https://github.com/ACEsuit/mace-mp/releases/tag/mace\_mpa\_0}\label{foot:mace}} \citep{batatia2023foundation} to perform a constrained relaxation where the energy is minimized while the symmetries set by the protostructure are retained. We repeat this process of realizing and relaxing crystal structures until the two lowest energies seen lies within a small cutoff of $0.01\ \mathrm{eV/atom}$. The lowest energy found is taken as our computationally predicted energy of the material generated by \ourmodel. As is common in materials science, this energy is converted into a formation energy by for each atom subtracting the corresponding energy per atom from a representative elemental solid.

Low formation energies are only indirectly related to stability; the thermodynamically stable material at a composition is the one with the lowest formation energy compared to all alternative competing phases and linear combinations of phases, which spans the so called convex hull of thermodynamical stability 
(see, e.g., \citet{bartel_critical_2020} and \Cref{app:novel_stable_materials} for more details). However, given the indirect relationship, we selected 200 structures with the lowest formation energies to investigate further. We used the high-throughput toolkit (\texttt{httk}) \cite{armiento2020database} to recalculate them with density functional theory (DFT) using the \texttt{VASP} electronic-structure software \cite{kresse1994ab} and evaluated their stability relative to the known convex hull from all materials in the MP \citep{jain_commentary_2013} and WBM \citep{wbmDataset} databases (further details in \ref{app:dft-supplementary}).

Out of the 200 selected materials, we highlight three hand-picked examples with interesting chemistries ($\mathrm{CsSnF}_6$, $\mathrm{NaNbO}_2$, and $\mathrm{Ca_2PI}$), shown in \cref{fig:hull_energies} in their respective composition phase diagrams generated using \texttt{pymatgen} \citep{Ong2013}. The DFT results for these generated materials confirm them to be stable; one is distinctly below, and the other two are \emph{on}, the convex hull. Hence, the generated structure for $\mathrm{CsSnF}_6$ is clearly a new predicted material not present in MP or WBM. The other two materials, $\mathrm{NaNbO}_2$, and $\mathrm{Ca_2PI}$, already exist in MP (i.e., they are part of the known convex hull and therefore on it), and can be traced to experimental works \citep{ROTH1993,Hadenfeldt1988}. These are thus explicit examples of \ourmodel recreating materials outside of its training set
(WBM), which are experimentally confirmed to exist. These results substantiate the ability of the model to generate materials that are physically reasonable.
Furthermore, our investigation of the 200 selected materials finds seven other fluorides confirmed by DFT to be distinctly below the known convex hull from WBM and MP (details presented in the supplementary materials \Cref{app:additional-protostructures}, \Cref{tab:flourides}). The over-representation of new stable fluorides in this set of 200 materials is likely due to that our proof-of-concept methodology of extracting the smallest, i.e., most negative, formation energies may bias towards this chemistry, rather than being a feature of the model.

\section{Discussion \& Conclusions}
In this paper we propose \ourmodel, a novel generative model which leverages a new representation of the symmetrical aspects of materials together with a novel neural network architecture and discrete diffusion to generate new protostructures. Although obtaining the full material requires extra steps, viewing the protostructure and the full geometry as separate processes opens up the possibility of using models tailored for each respective task, and use of computational effort where it is most needed. As we highlight with our proof-of-concept materials discovery pipeline in \Cref{sec:discoverypipeline}, the precise geometry can be uncovered via a pretrained generally applicable interatomic potential such as MACE, only for the most promising materials. \ourmodel shows competitive performance compared to the current state-of-the-art both in terms of novel generated materials/min, structural novelty, and agreement with the data distribution based on the newly proposed Fréchet Wrenformer Distance.

\input{acknowledgments}

\bibliography{references}


\newpage
\appendix
\onecolumn
\input{appendix}


\end{document}
