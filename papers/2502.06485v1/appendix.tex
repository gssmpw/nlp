\section{WyckoffGNN Details}
\label{app:gnn}
\subsection{Architecture}
Here we give some more details on our neural network backbone, WyckoffGNN. As mentioned in the main text, it is based on the message-passing neural network framework \cite{gilmer_neural_2017}, where each node in a graph is represented by a vector $\hidden_i^l$, and each layer corresponds to an update of this representation according to
\begin{subequations}
\label{eq:mpnn}
\begin{align}
    \rvm_i^{l+1} = \sum_{j\in\mathcal{N}(i)}M_l(\hidden_i^l, \hidden_j^l),\\
    \hidden_i^{l+1} = U_l(\hidden_i^l, \rvm_i^{l+1}).
\end{align}
\end{subequations}

\Cref{algo:gnn_forward} describes the full pass through the network. It makes use of $\Embedding$ layers which maps discrete features, like the atom types or number of atoms of a certain atom type, to vectors in some vector space $\R^d$, and $\Linear$ which are affine maps of vectors in $\R^{d_\text{in}}$ to $\R^{d_\text{out}}$, i.e., $\Linear{\x} = \mW\x + \rvb$. The embedding of the number of atoms embeds the number of atoms of each atom type in $\num$ into a scalar which are concatenated and then processed by a linear layer such that all initial representations $\hidden^0$ of all Wyckoff positions are of the same dimension.

\Cref{algo:gnn_layer_forward} describes the update of the hidden representations as in \Cref{eq:mpnn}. As we are working on a fully connected graph, the sum over the neighbors is over all positions. In our case, the input to $M_l$ is not the hidden representations $\hidden_i^l$ and $\hidden_j^l$, but concatenations of the hidden representations and its corresponding \emph{position vector} $\hidden_i^{\text{pos}}$ which contains some general information of the Wyckoff position like the number of degrees of freedom, the letter, but also the space group and sampling timestep $t$. \Cref{algo:gnn_message} outlines how $M_l$ is computed. 

\input{algorithms/GNN_forward}
\input{algorithms/GNN_layer_forward}
\input{algorithms/GNN_message}

\subsection{Choice of $\beta_t$}
As a scheduler for $\beta_t$, we used the cosine scheduler by \citet{hoogeboom_argmax_2021-1}. By defining $\alpha_t = 1 - \beta_t$ and $\bar \alpha_t = \prod_{s=1}^t \alpha_s$, we choose $\beta_t$ such that
\begin{align}
    \bar \alpha_t = \cos\left(\frac{t/T + s}{1 + s}\frac{\pi}{2}\right),
\end{align}
with $s=0.008$.

\subsection{Hyperparameters and Training Details}
Training of a model required approximately 38 hours on a single NVIDIA A100. Hyperparameters for \ourmodel and its training can be found in \Cref{tab:hyperparams}. The activation function SiLU \cite{ramachandran_searching_2017} is given by\footnote{See also, e.g., \url{https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html}}
\begin{align}
    \text{SiLU}(x) = x \frac{\exp(x)}{1 + \exp(x)} \label{eq:silu}.
\end{align}
We did not perform any hyperparameter search. 
\input{tables/hyperparams}

\section{Implementation Details of Compared Methods}
\label{app:compared_methods}
For all methods, we used the official public implementations\footnote{\url{https://github.com/txie-93/cdvae}}\footnote{\url{https://github.com/jiaor17/DiffCSP-PP/}}\footnote{\url{https://github.com/sibasmarak/SymmCD}} and we train all methods for \thsnd{1} epochs. We specify further details below.

\subsection{CDVAE}
For CDVAE, we used the hyperparameters used for the MP20 dataset by the original authors, except for the learning rate which we lowered to $2\cdot10^{-4}$, as the default value led to instabilities in the training.


\subsection{DiffCSP++}
For DiffCSP++, we used the hyperparameters specified by the original authors for the MP20 dataset.


\subsection{SymmCD}
For SymmCD, we used the hyperparameters specified by the original authors for the MP20 dataset, except for the number of training epochs and batch size, which we reduced to \thsnd{1} and 256, respectively, to ensure fair comparisons.

When generating materials using SymmCD, we encountered an issue where the length and angle matrices contained NaN, Inf, or extremely small values. To facilitate subsequent evaluation, we filtered out those invalid materials.


\section{Wren Energy Histograms}
\label{app:wren_energies}

We show the similarities of generated material distrubution across all model versions \ourmodel-marginal, \ourmodel-uniform, and \ourmodel-zeros, in \Cref{fig:wren-energies-supplementary}.

\input{figures/wren_energies_supplementary}

\section{Novel Stable Materials}
\label{app:novel_stable_materials}


Low formation energies are only indirectly related to stability; the thermodynamically stable material at a composition is the one with the lowest formation energy compared to all alternative competing phases and linear combinations of phases, which spans the so called convex hull of thermodynamical stability 
(see, e.g., \citep{bartel_critical_2020}). I.e., in order do determine if a novel material is stable, the formation energy needs to be compared with the convex hull. Deriving the formation energy of a material and computing the convex hull is described below.

\subsection{Formation Energy}
\label{app:form_energy}
\textit{Formation energy} is calculated by taking the total energy of a material and subtracting the sum of elemental solid energy for each element present in the material. A negative formation energy therefore implies a lower energy state of the material relative to its elemental components. In turn, the formation energy proves that the material will not decompose into its elemental components.

\subsection{Convex Hull}
\label{app:convex_hull}
Plotting the formation energies of the materials and its corresponding elemental solid energies in a diagram constructs a \textit{phase diagram}. Materials that holds the lowest formation energy in the phase diagram forms a \textit{convex hull}. The convex hull constructs serves as the line of stable materials, meaning: if a new crystal structure is discovered but has higher formation energy in comparison to the convex hull, the new crystal structure will decompose into its closest stable neighbors on the convex hull; whereas if the new crystal structure has a lower formation energy in comparison to the convex hull, the new material is novel and stable. The novel stable material is then part of a new convex hull, redefining the line of stable materials.


\section{Supplementary Details on Materials Discovery Demonstration}
\label{app:materials-discovery-details}


\subsection{Additional Protostructures}
\label{app:additional-protostructures}
As described in \Cref{sec:discoverypipeline} we performed a selection of three chemically interesting materials, whereas it was noted that there where a total of eight fluorides with distinctly below the convex hull. In \Cref{tab:flourides} we list the materials sorted on energy distance from the convex hull of WBM and Materials Project (MP), up to the final selected structure. 

\input{tables/flourides}

\subsection{Density Functional Theory Supplementary Details}
\label{app:dft-supplementary}
 In order to maintain compatibility with MP and WBM dataset, all DFT calculations and post-corrections (\texttt{MaterialsProjectCompatibility}) \cite{jainFormationEnthalpiesMixing2011a} were performed using INCAR settings, KPOINTS and pseudo-potentials defined by Pymatgen \cite{Ong2013}. Calculations where converged to atleast 1e-4 eV in total energy in electronic steps.

\subsection{Previous \ourmodel-marginal}
\label{app:previous-marginal-model}

The structure \texttt{A2BC\_hR12\_166\_c\_a\_b:Ca-I-P} ($\mathrm{Ca_2PI}$) was found using a previous iteration of the WyckoffGNN architechure where we did not use softmax in the message-function $M_l$, but instead used the raw outputs of the neural network (see \Cref{algo:gnn_message}), and encoded the degrees of freedom of the position using a binary representation, i.e., constrained or unconstrained position instead of 0, 1, 2, or 3 degrees of freedom.




