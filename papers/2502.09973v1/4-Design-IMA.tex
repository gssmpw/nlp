\section{Designing and Prototyping Interactive Digital Items}

\revision{Motivated by the findings of our formative study, we recognize the key attributes of interactivity that should be integrated into digital reconstructions of memorable personal items.}
Thus, we define and elaborate on the concept of \textbf{I}nteractive \textbf{D}igital \textbf{I}tem (IDI) as digital reconstructions with physical interactivity features. 
We proceed to outline the expected features of IDI and introduce \emph{InteRecon}, a user-oriented prototype designed for creating IDI.
InteRecon is designed to cater to individual end-users who have been identified as the most significant users of personal memory archives and extensions. 
Target items for reconstruction should be cherished physical mementos capable of evoking personal memories \cite{kirk2010human,10.1145/3173574.3173998,10.1145/3544549.3585588}. 
IDI emphasizes digitally inheriting the physical interactivity from its physical counterparts to enhance the efficacy of digital reconstructions in presenting personal memories.


\subsection{Concepts and Design Goals}
% For IDI, we use the term `interactive' in a broad conceptual sense to differentiate it from static and non-interactive replications (e.g., images, audio, videos, static 3D models, etc.). 
% % `Digital' denotes that it is presented and preserved in a digital format, such as in VR/AR environment. 
% Target items for reconstruction should be cherished physical mementos capable of evoking personal memories \cite{kirk2010human,10.1145/3173574.3173998,10.1145/3544549.3585588}. 
% IDI emphasizes on digitally inheriting the physical interactivity from its physical counterparts to enhance the efficacy of digital replicas in presenting personal memories.
Based on our formative study, we have formulated three \textbf{D}esign \textbf{G}oals to delineate the physical interactivity of IDI to be reconstructed in the digital realm.
% IDI focuses on digitally inheriting the physical interactivity from its physical counterparts to enhance the efficacy of digital replicas in evoking personal memories.
% Although digital replicas cannot fully retain the value of the original physical mementos \cite{kirk2010human}, we aim to enable IDI to preserve some of the original's value. By doing so, IDI offers new ways to connect to a shared past and frame the family dynamics, functioning similarly to the original physical memento. 

% Based on our formative study, we have formulated three \textbf{D}esign \textbf{G}oals to delineate the physical interactivity of IDI to be integrated, :
% \begin{itemize}
%     \item \textbf{DG1 }Reconstructing physical appearance: From Sec. 3.2.2, we advocate that IDI should consist of a 3D model faithfully replicating the entire physical appearance, instead of downloading a similar 3D model from the internet devoid of the memento's personalized usage traces.
%     \item \textbf{DG2 }Reconstructing interactivity: From Sec. 3.2.3, IDI should incorporate the real-world interactivity of the mementos, simulating realistic interaction effects.
%     \item \textbf{DG3 }Reconstructing embedded content: From Sec. 3.2.4, IDI should become a medium to save and present the embed content and enable users to update or edit the content.
% \end{itemize}







\subsubsection{\textbf{DG1: Reconstructing Geometry}}
IDI should reconstruct geometric properties to accurately mirror the entire physical appearance and transformations of its real-world counterparts (R2). 
In this case, we propose that IDI should feature a 3D model faithfully replicating the entire physical appearance (R2) while also emulating the physical properties, such as gravity and collisions. 
Furthermore, for complex physical artifacts connected by joints (R1), IDI should enable interaction through natural hand movements (e.g., pushing, pulling, manipulating finger joints, etc.) to create realistic motion transformations, including movement and rotation, aligning with the item’s joint mechanism.


%, instead of downloading a similar 3D model from the internet devoid of the memory artifact's personalized traces.
% For types of memory artifacts of physical artifacts (R1, R2), the IDI should stimulate the physical properties (e.g., gravity, collisions, etc.) of the memory artifact. 
% Additionally, for certain complex physical artifacts with interconnected components through joints, IDI should support bare-hand gestures (e.g., pushing, pulling, and finger joint movement) to generate corresponding motion effects (e.g., movement and rotation) based on the artifact's joint mechanism.

% For certain complex physical artifacts with interconnected components through joints, IDI should support bare-hand gestures (e.g., pushing, pulling, and finger joint movement) and generate corresponding physical effects (e.g., movement and rotation) based on the artifact's joint mechanism.

\subsubsection{\textbf{DG2: Reconstructing Interface}}
% IDI ought to recreate the interfaces of its physical counterparts, emulating the interactions that occur within these interfaces (R3). 
For electronic devices, IDI should reconstruct the interfaces of its physical counterpart (R3), including the tangible widgets along with the internal programs (e.g., the functions or effects triggered by certain physical operations) on these devices. 
When users interact with these virtual tangible widgets (e.g., pressing buttons, dragging sliders, etc.), IDI should replicate analogous effects in the virtual interface (e.g., switching to the next song in an MP3 player, moving avatars in a gaming console, etc.) mirroring real-world interactions. 

% IDI should preserve the interactivity of its physical counterpart.
% For digital device mementos, 

% IDI ought to recreate the vintage interfaces of its physical counterparts, emulating the interactions that occur within these interfaces (R3). 
% For instance, IDI should rebuild similar tangible widgets in these interfaces, allowing these widgets to initiate matching effects.
% When users engage with these reconstructed widgets, IDI should produce similar effects in the virtual interface, thereby mirroring the functionality of their physical equivalents.

% IDI ought to recreate the vintage interfaces of its physical counterparts, emulating the interactions that occur within these interfaces. 
% For instance, IDI should rebuild similar tangible widgets in these interfaces, allowing these widgets to initiate matching effects.
% When users engage with these reconstructed widgets (such as pressing buttons, dragging sliders, or rotating knobs), IDI should produce similar effects in the virtual interface (e.g., switching to the next song to in an MP3 player, moving avatars in a gaming console, etc.), thereby mirroring the functionality of their physical equivalents.


% mirroring real-world interactions.

% IDI ought to recreate the vintage interfaces of its physical counterparts, emulating the interactions that occur within these interfaces. 
% For instance, IDI should rebuild similar tangible widgets in these interfaces, allowing these widgets to initiate matching effects.
% When users engage with these reconstructed widgets (such as pressing buttons, dragging sliders, or rotating knobs), IDI is expected to produce similar effects in the virtual interface (e.g., switching to the next song to in an MP3 player, moving avatars in a gaming console, etc.), thereby mirroring the functionality of their physical equivalents.



% From Sec. 3.2.3, IDI should incorporate the real-world interactivity of the mementos, simulating realistic interaction effects. IDI should preserve the interactivity of its physical counterpart. For digital device mementos, IDI should reconstruct analogous tangible widgets and the memento's interface. When users interact with these virtual tangible widgets (e.g., pressing buttons, dragging sliders, etc.), IDI should replicate analogous effects in the virtual interface (e.g., switching to the next song to in an MP3 player, moving avatars in a gaming console, etc.) mirroring real-world interactions. 

\subsubsection{\textbf{DG3: Reconstructing Embedded Content}}
IDI should also preserve the content stored or embedded within the item.
For electronic devices, digital content, such as songs, photos, movies, and games, stored in the devices need to be reconstructed. 
For physical items, the embedded content could be the contextual information (e.g., usage scenarios, associated individuals, and related stories, etc.) in the form of notes, photos, or videos.
Users should be able to access and associate this content with IDI.
For example, consider a bicycle with a scratched frame; in such cases, users can annotate the scratch with a brief note describing the incident that caused the scratch, and users can also give general comments on the bicycle (e.g., the history, interesting events, etc.). 
As validated by our formative study (R4), reproducing the content and making it accessible in IDI could be instrumental for both functional integrity and memory preservation. 


% DG2 was well-articulated, and the supporting functions were effectively demonstrated in the video. However, the implementation of this design goal fell short, likely due to time constraints. The demonstration lacked the compelling and convincing aspects necessary to fully realize the potential of this concept. The disjoint between the head and body, as well as the absence of several demonstrated joints and functions, underscored the need for a more comprehensive implementation.


% \subsubsection{DG2: Interactivity}
% IDI should preserve the interactivity of its physical counterpart.
% For digital device mementos, IDI should reconstruct analogous tangible widgets and the memento's interface.
% When users interact with these virtual tangible widgets (e.g., pressing buttons, dragging sliders, etc.), IDI should replicate analogous effects in the virtual interface (e.g., switching to the next song to in an MP3 player, moving avatars in a gaming console, etc.) mirroring real-world interactions. 
% For physical artifacts, the IDI should mimic the physical properties (e.g., gravity, collisions, etc.) of the memento. 
% For certain complex physical artifacts with interconnected components through joints, IDI should support bare-hand gestures (e.g., pushing, pulling, and finger joint movement) and generate corresponding physical effects (e.g., movement and rotation) based on the memento's joint mechanism.

% \subsubsection{DG3: Embedded content}
% IDI should preserve the content stored or embedded within the memento.
% For digital device mementos, digital content, such as songs, photos, movies, games, software, and operating systems, stored within the memento need to be reconstructed. 
% For physical artifacts, the embedded content should be the contextual information (e.g., usage scenarios, associated individuals, and related stories, etc.) in the form of notes, photos, or videos.
% Users should be able to access and associate this content with IDI.
% For example, consider a bicycle with a scratched frame; in such cases, users can annotate the scratch with a brief note describing the incident that caused the scratch, and users can also give general comments on the bicycle (e.g. the history, interesting events, etc.).

% Motivated by the design goals summarized above, we designed an application to showcase the 
\subsection{InteRecon: An Prototypical Application for IDI Creation}
\label{design_interRecon}
We designed InteRecon, an AR application for end-users to create IDI for the interactivity-aware reconstruction of memorable personal items. InteRecon features four functions corresponding to the above design goals.
% , as detailed below. 

% , to validate the feasibility of our design goals
% InterRecon enables users to create IDI from physical memory artifacts that meet the design goals in Section 4.
% Figure xx illustrates the user workflow using InterRecon. 
% Next, we will introduce individual \textbf{S}teps with InterRecon to create IDI and associate them with the design goals outlined in Section 4 to underscore their design rationale.

\subsubsection{Function 1: Reconstructing 3D Appearance}
\label{sec: Reconstruct the 3D model by 3D scanning}
To support faithfully capturing the physical appearances for IDI (\textbf{DG1}), we developed a mobile application to enable users to reconstruct the 3D model of the target object through 3D scanning.
The Fig. \ref{fig:3dapp} illustrated the entire scanning process. 
First, the user needs to open the application and point it at the target object. 
An automatic bounding box with the length, width, and height of the object is generated before capturing. 
Then the user can move the mobile phone slowly to circle around the object while the application automatically captures the right image for reconstruction. 
The app provided visual guidance on regions where the algorithm needs more images, along with additional feedback messages to help the user capture the best quality shots. 
After finishing one orbit, the user can flip the object to capture the bottom. 
Once scanning for three orbits (front, side, and bottom surface of the bounding box) is completed, the application will proceed to the reconstruction stage, which runs locally on the mobile device. 
A 3D reconstructed model will be ready for further use. 
% After getting the 3D model of the memento, the researcher will help to import it into a software\footnote{https://www.blender.org} to convert to an available format and import to the AR environment.

\begin{figure}[tbh!]
     \centering
     \includegraphics[width=\linewidth]{Figures/3dapp.jpg}
     \vspace{-2ex}
\caption{\textbf{The interactive process for reconstructing 3D appearance. (1) The user opens the application and points it at the target object. (2) An automatic bounding box is generated around the target object in the application interface. (3) Circle around the object with visual guidance on regions to capture more images. (4) A 3D reconstructed model is ready for further use.}}
\Description{Figure 2 has 4 sub-figures (a, b, c, d, from left to right) in a row, showing the interactive process for reconstructing the 3D model. (1) shows a user opens the mobile application and points it at the target object. (2) shows the screen of the mobile application, an automatic bounding box that is generated around the target object in the application interface. (3) shows the screen of the mobile application when scanning objects, a circle around the object with visual guidance on regions where the algorithm needs more IDIs. (4) shows a 3D reconstructed model which is ready for further use.}
\label{fig:3dapp}
\end{figure}



\begin{figure*}[tbh!]
\centering
\includegraphics[width=\textwidth]{Figures/Geo-CHI25.png}
\caption{\textbf{The interactive process of adding physical transforms. (a-1,2) Segment the puppy's model using an automatic approach. (b-1,2) Segment the puppy model in AR by using a segmenting plane and breaking down the model's leg along the plane. (c) Use hands to touch the blue cube to observe the movement features of each demonstrated joint until identify one that resembles the model's leg. Detailed introductions to each joint are listed in Table \ref{tab:joints}, and the numbers close to each joint in the figure correspond to the numbers in Table \ref{tab:joints}. (d) Select the `movable' segment of the model (the leg) using a pinching gesture and confirm the choice by clicking the `movable' button, which is analogous when selecting the `base' (the body). Press the `Apply' button to apply the joint to the model's leg and body. (e) After repeating the above processes for mapping the model's ear joints, shake the model's body and generate similar motions to a real-world toy puppy's legs and ears in AR.}}
\Description{Figure 3 illustrates The interactive process for adding physical transforms. (a-1,2) Segment the puppy model's leg in the software. (b-1,2) Segment the puppy model in AR by using a segmenting plane and breaking down the model's leg along the plane. (c) Use hands to touch the blue cube to observe the movement features of each demonstrated joint until identify one that resembles the model's leg. Detailed introductions to each joint are listed in Table \ref{tab:joints}, and the numbers close to each joint in the figure correspond to the numbers in Table \ref{tab:joints}.
(d) Select the 'movable' segment of the model (the leg) using a pinching gesture and confirm the choice by clicking the 'movable' button, which is analogous when selecting the `base' (the body). Press the `Apply' button to apply the joint to the model's leg and body.
(e) After repeating the above processes for mapping the model's ear joints, shake the model's body and generate similar motions to a real-world toy puppy's legs and ears in AR.}
% To achieve 3D model segmentation, a segmenting plane is provided by InterRecon in AR to support the user in breaking down the mesh model along the plane. 
% The user can use their hands to adjust the size, the rotation, and the location of the plane to better match the potential segment position on the mesh.      }}
\label{fig:geo}
\end{figure*}


\begin{figure*}[tbh!]
\centering
\includegraphics[width=\linewidth]{Figures/Interface.png}
\caption{\textbf{The AR interface for the functions of reconstructing the interface and adding embedded content.  (a) The `interface' in the menu includes four commonly used widget categories: `knob', `screen', `slider', and `button', and a target model category that the user scanned by Function 1 (in this case, the TV model). Press the `knob' and `screen' buttons to select a knob widget and a screen widget and attach them to the TV model to reconstruct its interface. (b) The `content' includes three categories in the menu: `video', `audio', and `picture'. 
Press the `video' button to release the three embedded videos that users uploaded and drag them to the TV model to import. }}
\Description{Figure 4 illustrates the AR interface for the functions of reconstructing the interface and adding embedded content.  (a) The `interface' in the menu includes four pre-defined widget categories: `knob', `screen', `slider', and `button', and a target model category that the user scanned by Function 1 (in this case, the TV model). Press the `knob' and `screen' buttons to select a knob widget and a screen widget and attach them to the TV model to reconstruct its interface. (b) The `content' includes three categories in the menu: `video', `audio', and `picture'. 
Press the `video' button to release the three embedded videos that users uploaded and drag them to the TV model to import. }
\label{fig:inter}
\end{figure*}


% \zisu{Technical clarity: R1 and R2 wanted more details about the system implementation, particularly handling complex object geometries and intricate physical properties. They also requested more technical performance metrics in the evaluation.
% R1: The description of the system implementation is not clear. For example, the handling of complex object geometries or intricate physical properties (e.g., elasticity, texture) is briefly mentioned but not thoroughly explored. The paper does not provide enough technical details to assess how these challenges were overcome or how the system compares to existing AR tools in terms of reconstruction fidelity.}
\subsubsection{Function 2: Adding Physical Transforms}
As proposed by \textbf{DG1}, a further step to realize interactivity reconstruction should focus on the modeling physical dynamics and mechanical properties despite static appearances. 
% We demonstrate how to reconstruct the physical structures of items composed of multiple segments and connecting joints. 
% We allow users to segment the mesh reconstructed from the initial function and apply physical constraints to these segments, simulating the object's physical dynamics and mechanical properties. With these constraints, the model can be directly touched by hands, and its segments will respond just as they would in the real world.
% Physical constraints are great for adding realism to interactions, such as when a character's body reacts naturally to being hit or when objects behave according to real-world physics. However, they lack the precision and control needed for deliberate character animations.
This allows users to segment the mesh reconstructed from the first function and apply physical constraints to these segments, simulating the object's physical dynamics and mechanical properties. Under these constraints, when the model is directly touched by hands, its segments will respond just as they would in the real world.
A two-stage pipeline is devised to achieve this function, as illustrated in Fig. \ref{fig:geo}. 
First, the user should segment the model to get it prepared to apply the physical constraints on it.
% The model of physical segments featuring joints should be segmented into multiple parts to create physical transforms (\textbf{DG1}). 
% Additionally, the widget elements of the electronic device's 3D model should be separated to enable further functions.
To achieve mesh segmentation, a segmenting plane is provided by InteRecon in AR to support the user in breaking down the mesh model along the plane, as is shown in Fig. \ref{fig:geo}(b). 
The user can use their hands to adjust the size, the rotation, and the location of the plane to better match the potential segment position on the mesh. Alternatively, we also incorporated an unsupervised automatic segmentation approach, inspired by Style2Fab~\cite{10.1145/3586183.3606723}. This segmentation method is based on spectral segmentation, which leverages the mesh geometry to predict a mesh-specific number of segments. Users can choose to either segment by providing the planes in AR, or use the automatic segmentation method. 


% Thus, this method doesn't require training and can generalize across a wide range of 3D models. 


% \faraz{To allow for an automatic segmentation method (\autoref{fig:geo}(a)) we also incorporated an unsurpervised segmentation approach, inspired by Style2Fab~\cite{10.1145/3586183.3606723}. This segmentation method is based on spectral segmentation, that leverages the mesh geometry to predict a mesh-specific number of segments. Thus, this method doesn't require training and can generalize across a wide range of 3D models. This segmentation method is available in our user-interface. Users can choose to either segment by providing the planes in the interface, or use the automatic segmentation method.  
% % the model automatically based on its geometric structures. The user could only set a desired number of segments and then get the result. If the result is not desired, the user can further adjust the segments in AR using the `segmenting plane'. 
% }

% We also enabled more precise and complex segmentation operations (e.g., using surfaces to cut non-planar mesh bodies) in Blender~\footnote{www.blender.org} as is shown in Fig. \ref{fig:geo}(a). 
% The user could complete the segmentation in Blender and import the model to the AR environment.
% \faraz{ssss}


% We also enabled more precise and complex segmentation operations (e.g., using surfaces to cut non-planar mesh bodies) in Blender~\footnote{www.blender.org} as is shown in Fig. \ref{fig:geo}(a). 
% The user could complete the segmentation in Blender and import the model to the AR environment.





% \subsection{S3: Mapping transformations}
After acquiring a segmented model in AR, the user could move on to the second stage to further map the physical joints to IDI to simulate the mechanical transforms on the item.
To help the user rapidly map the joints, pre-defined mechanical structures of joints \cite{blake1985design} were implemented in the application, as is shown in the Appendix Table \ref{tab:joints} and their virtual representation in Fig. \ref{fig:geo}(c).
Each of the virtual joints includes two cubes, indicating the relative movement in one degree of freedom and restricting movement in one or more others. 
% We also provided the joints with different resistances for accompanying different cases, such as the 
We also offer different resistance options for each type of joint to accommodate various situations. For instance, although the movement mechanism of the leg joints in soft toys and LEGO figures is similar, the resistance in LEGO figures is greater, which means that under the same amount of force, their range of motion would differ. In order to illustrate relative movements more effectively, the two cubes are distinguished by different colors, with the grey cube representing the `base' cube (non-movable) and the blue cube representing the `movable' cube (capable of motion).
The user could use their hands to touch the blue cube to observe the relative movement features of each demonstrated joint until they identify one that resembles the physical joint.
The user could select the `movable' segment of the mesh model using a pinching gesture and confirm their choice by clicking the `movable' button, as is shown in Fig. \ref{fig:geo}(d).
This process is analogous when selecting the `base'.
To conclude the process, the user can click the `Apply' button next to the identified virtual joint to map it onto the mesh model.
InteRecon also supports the incorporation of multiple joints by repeating the aforementioned process. 


\subsubsection{Function 3: Reconstructing Interface}
InteRecon enables users to reconstruct the interfaces of electronic devices on IDI by integrating widgets to the 3D model (\textbf{DG2}). 
We illustrated the interactive process of this function in Fig. \ref{fig:inter}(a). 
% The 3D model would be obtained from section~\ref{sec: Reconstruct the 3D model by 3D scanning} and imported to the AR environment. 
% After the steps of \textbf{S1} and \textbf{S2}, a widget component model and its parent electronic device model is divided. 
The user can apply \textbf{Fuction 2} on the model of an electronic device to segment the tangible widgets from the main body of the model while defining their mechanical transforms (e.g., rotations for knobs) to simulate real-world effects of the widgets. 
Further, the user can edit widget models and corresponding analogous effects to simulate certain interface logic. They first press the `interface' button to view widget categories and then select widgets from four pre-defined categories: `knob', `screen', `slider', and `button'. 
Each category contains widget instances corresponding to different functions in different device models. 
For example, the `screen' category includes two types: one is a display that can present content such as photos and videos; the other is a camera's viewfinder, allowing you to see what's being photographed.
The user needs to refer to the widgets on the physical electronic device and spawn similar virtual widgets (with the same functionalities compared to the actual buttons) in AR by clicking. 
Afterward, the user can move the position and adjust the size of the virtual widgets to make sure they have a similar relative position and size to their physical counterparts.
The `attach' button allows users to bind virtual widgets generated by InteRecon to the model of the physical widgets.
After binding, the widget model could control the content uploaded from \textbf{Function 4}.
The `invisible' button could make the virtual widgets invisible while preserving their functions that could be triggered in their positions on the surface of the 3D model in AR. 
Thus, the user could trigger the function on the surface of the 3D model without seeing the virtual widgets.





\subsubsection{Function 4: Adding Embedded Content} 
We devise a content management system to help the user better manage the content associated with their memorable items. 
% Such a system is implemented as an iOS APP. 
The user can upload different forms of content, such as songs, videos, pictures, and texts.
Then these contents are synchronized to the `Content' category in the AR environment by our researcher.
The user could select the target content in AR and move it to a specific IDI model to incorporate the embedded content of the item.
% , to a specific IDI model (embedded contents are organized by models). 
% Then these contents are automatically synchronized to the AR environment and bound to specific IDI models, where the user can access them in AR seamlessly. 
Fig. \ref{fig:inter} (b) shows the AR interface of adding embedded content to an IDI model.

% mirrored to the AR environments, bind specific asset. 
% These contents are organized by models XXXX. 

% the user could upload the embedded content in AR from physical artifacts for IDI access (\textbf{DG3}).
% Content can be uploaded via a mobile app we developed, which prepares its formats for compatibility and will be saved in the database of the InterRecon. 
% The user could press the `content' button of the menu in AR and incorporate the target content (e.g., songs, videos, pictures, etc.) to the model of electronic device. 

% \subsubsection{Mapping mechanical components on physical artifacts}
% InteRecon enables users to map mechanical components (e.g., physical joints) of the physical artifact to its static 3D mesh model obtained by the scanning function (mentioned in section~\ref{sec: Reconstruct the 3D model by 3D scanning}). 
% % For physical artifacts which include some mechanical components, the initial 3D model scanned previously is a static mesh model without any physical joints. 
% The Fig. \ref{fig:inter} bottom showed the interactive process of this function.
% In the `Physical' interface in AR, the user can incorporate the simulated physical joints on the model. 
% To achieve this, the mesh model should be broken down into several parts by a segmenting plane for further editing. 
% % A segmenting plane is provided to support the user in breaking down the mesh model along the plane. 
% The user can use their hands to adjust the size, the rotation, and the location of the plane to better match the potential segment position on the mesh. 
% To help the user rapidly map the joints, pre-designed demonstrated virtual joints according to the common mechanical joints design \cite{blake1985design} were provided in this interface (detailed descriptions in Appendix Table \ref{tab:joints}).
% Each of the virtual joints includes two cubes, indicating the relative movement in one degree of freedom and restrict movement in one or more others. 
% In order to illustrate relative movements more effectively, the two cubes are distinguished by different colors, with the grey cube representing the 'base' cube (non-movable) and the green cube representing the 'movable' cube (capable of motion).
% The user could use their hands to touch the green cube to observe the relative movement features of each demonstrated joint until they identify one that resembles the physical artifact.
% When finding a similar one, the user could move the yellow ball (the `Joint' in Fig. \ref{fig:inter} P1) to the joint position of the model.
% The user could select the 'movable' segment of the mesh model using a pinching gesture and confirm their choice by clicking the 'movable' button.
% This process is analogous when selecting the `base'.
% To conclude the process, the user can click the 'Apply' button next to the identified virtual joint to map it onto the mesh model.
% InteRecon also supports the incorporation of multiple joints by repeating the aforementioned process. 

% 1.  Cut the mesh for making joints.
% 2.  Find similar joints.
% 3.  Map the joints.
% 4.  bare hand interaction


% \begin{figure*}[tbh!]
% \centering
% \includegraphics[width=\textwidth]{Figures/content_app.jpg}
% \caption{\textbf{The screenshots of the content application, featuring functions for uploading, editing digital files, and updating in AR.}}
% \Description{Figure 4 shows the screenshots of the content application, featuring functions for uploading, editing digital files, and updating in AR.}
% \label{fig:upload}
% \end{figure*}


% \subsubsection{Enabling content upload for IDI through mobile devices} 
% As shown in Fig. \ref{fig:upload}, we have developed a mobile application enabling users to upload embedded content from physical mementos for IDI access.
% First, the user could import the content (e.g., digital files) from the digital devices mementos to the mobile device.  
% Subsequently, they select the content type (e.g., music, games, pictures, software, operating systems) and incorporate it into a certain model in the AR environment by transmitting the command of selection from the mobile phone to the AR environment.
% The application also provides content editing capabilities in the `content modification' interface, such as playlist creation or editing for MP3 files.



% The usage of this application includes 3 steps with editing embed content in 3 interfaces. The user needs to select the content's type (e.g., music, games, pictures, software, operating systems, etc.) on the type selection interface and upload content from local devices that originally exists in the physical mementos. 


% includes 3 steps with editing in 3 interfaces
% To upload the corresponding content for a certain memento, the user needs to select the type of the memento first. An then choose the exact 

%a type selection interface, an memento selection interface, and a content modification interface. 


% \subsection{S6: Try out and save the reconstructed IDI}
% After completing the scanning, mapping, and uploading process, the user can try out the IDI using free hands in AR.
% This mode can be activated by pressing the 'Display' button.
% For IDI of digital devices, users can test the enabled functions, for example, using the IDI of virtual camera to capture photos in AR or playing music using the IDI of MP3.
% For IDI of physical artifacts, the user can use fingers to touch or gently push, allowing them to experience simulated actual movements of the physical memento within the IDI.
% The user could also save the designed IDI in the database of InterRecon.


% For memory artifacts of electronic devices, embedded content, such as songs, photos, movies, games, software, and operating systems, stored need to be reconstructed and have the capability to be accessed through IDI by the user. 




% From Sec. 3.2.4, IDI should become a medium to save and present the embed content and enable users to update or edit the content.
% IDI should preserve the content stored or embedded within the memento.
% For digital device mementos, digital content, such as songs, photos, movies, games, software, and operating systems, stored within the memento need to be reconstructed. 
% For physical artifacts, the embedded content should be the contextual information (e.g., usage scenarios, associated individuals, and related stories, etc.) in the form of notes, photos, or videos.
% Users should be able to access and associate this content with IDI.
% For example, consider a bicycle with a scratched frame; in such cases, users can annotate the scratch with a brief note describing the incident that caused the scratch, and users can also give general comments on the bicycle (e.g. the history, interesting events, etc.).


% \zisu{Technical clarity: R1 and R2 wanted more details about the system implementation, particularly handling complex object geometries and intricate physical properties. They also requested more technical performance metrics in the evaluation.
% R1: The description of the system implementation is not clear. For example, the handling of complex object geometries or intricate physical properties (e.g., elasticity, texture) is briefly mentioned but not thoroughly explored. The paper does not provide enough technical details to assess how these challenges were overcome or how the system compares to existing AR tools in terms of reconstruction fidelity.}

\subsection{Implementation}
\label{implementation}
We built InteRecon's mobile applications on iPhone 14 Pro to implement InteRecon's functions (\emph{Function 1} and \emph{Function 4}) on mobile devices. For 3D object scanning, we utilized the reality kit (Object Capture API~\footnote{RealityKit: https://developer.apple.com/documentation/realitykit}) from Apple Developer to build an app that runs on iOS 17.0+. 
For the AR interface of InteRecon, we implemented on HoloLens 2, which was connected to a PC (with an Intel i9-12900K CPU and an RTX A6000 24G GPU) using a wireless network using Unreal Engine Version 4.26 \revision{to handle complex geometric meshes}.
We integrated the Mixed Reality Tool Kit (MRTK~\footnote{MRTK: https://github.com/microsoft/MixedReality-UXTools-Unreal}) to handle the hand interaction and UI elements in the application. 

We used UE's built-in physical engine to implement the physical effects such as gravity, collision, physical joints, and hand manipulations, of IDI. 
Specifically, for the manual mesh segmentation, we first converted the scanned mesh to a \emph{Procedural Mesh} and called the \emph{Slice Procedural Mesh} method to cut the mesh with a hand-held cut plane at runtime. 
Segmented meshes were stored both as an array of \emph{Procedural Mesh} in the UE program with further physical operations enabled and as static mesh copies in the disk. 
For automatic segmentation, we use the approach from Style2Fab \cite{10.1145/3586183.3606723}, which uses a spectral decomposition approach. It leverages the spectral properties of a graph representation of the 3D mesh to identify meaningful segments. \revision{By analyzing the eigenvalues and eigenvectors of the graph's normalized Laplacian matrix}, this technique uncovers the mesh's inherent structure, grouping similar vertices together to create a meaningful segmentation of the model. \revision{"Meaningful segments" refers to portions of the mesh that are grouped together based on their similarity or shared properties in geometric, semantic, structural, or functional purpose. For example, in the case of a 3D model of a human body, the model will be divided into segments like limbs and a torso. For an articulated structure, the object will be divided into different components that serve specific functional purposes, such as an elastic linkage part and a non-functional decoration part.}  This method doesn’t require training and can be generalized across a wide range of 3D models.
Joint creation is enabled using the built-in physics engine of the Unreal Engine. 
The hand interaction related to physical effects, such as a slight touch on objects, is implemented by binding a collision sphere to the tip of the finger in AR. 

% \revision{Spectral segmentation is a versatile clustering technique based on the mathematical properties and structure of data, typically represented as a graph. While it doesn't directly identify physical properties like elasticity, it can assist in identifying such features if they are appropriately captured by the chosen characteristics or similarity measures.}
% \revision{For more precise model segmentation in Blender, we use the plug-in of Bool Tool~\footnote{https://docs.blender.org/manual/en/latest/addons/object/bool\_tools.html} to combine or segment 3D models by performing Boolean operations (e.g., union, intersection, and difference). }




% \subsection{Designing the Envisioned Features of IDI}
% In this section, we designed the envisioned features of IDI according to the concepts and the design goals in terms of physical appearance, interactivity, and embedded content. Informed by the two prominent types of physical mementos identified in Sec. 3.2.1, we also considered different specific functions related to interactivity and embedded content.

% % The initial stages of the paper, particularly the formative study, laid a solid foundation for the research. However, DG1 and DG3 seemed somewhat underwhelming. These design goals, in their current state, do not introduce novel concepts and have been previously explored in existing works. To enhance DG2, it would be advisable to incorporate these objectives into more ambitious and impactful design goals. Specifically, integrating geometric operations, such as segmentation techniques, into DG1 would yield a more synergistic workflow.

% % Geometric Operations:
% % The paper could benefit greatly from a robust implementation of geometric operations. Integrating well-known segmentation techniques with human intervention could significantly enhance the joint creation process. These operations, if integrated into DG1, would amplify the synergistic workflow and lead to a more compelling demonstration.

% \subsubsection{DG1: Physical appearance}
% % personal usage marks -> 3D appearance reconstruction
% % user-oriented -> light-weighted , should be fast and low-cost, better in mobile devices
% IDI should be a 3D model with the reconstructed 3D appearance of its physical counterpart that can be easily and quickly be built. 
% The visual properties of the 3D model (e.g., size, shape, texture, material, etc.) should closely resemble those of its physical counterpart.
% The reconstruction accuracy must be high enough to capture scratches and wear on the physical memento to reconstruct the usage traces. 
% % The reconstruction accuracy must be high enough to capture scratches and wear on the memento, enabling the faithful reconstruction of usage traces.
% % We designed to conduct 3D scanning for the memento to get the similar 3D model. 




% DG2 was well-articulated, and the supporting functions were effectively demonstrated in the video. However, the implementation of this design goal fell short, likely due to time constraints. The demonstration lacked the compelling and convincing aspects necessary to fully realize the potential of this concept. The disjoint between the head and body, as well as the absence of several demonstrated joints and functions, underscored the need for a more comprehensive implementation.


% \subsubsection{DG2: Interactivity}
% % 1.for digital devices: tangible widgets
% % 2.for physical artefacts: gestural input and physical aware
% IDI should preserve the interactivity of its physical counterpart.
% For digital device mementos, IDI should reconstruct analogous tangible widgets and the memento's interface.
% When users interact with these virtual tangible widgets (e.g., pressing buttons, dragging sliders, etc.), IDI should replicate analogous effects in the virtual interface (e.g., switching to the next song to in an MP3 player, moving avatars in a gaming console, etc.) mirroring real-world interactions. 
% For physical artifacts, the IDI should mimic the physical properties (e.g., gravity, collisions, etc.) of the memento. 
% For certain complex physical artifacts with interconnected components through joints, IDI should support bare-hand gestures (e.g., pushing, pulling, and finger joint movement) and generate corresponding physical effects (e.g., movement and rotation) based on the memento's joint mechanism.

% \subsubsection{DG3: Embedded content}
% IDI should preserve the content stored or embedded within the memento.
% For digital device mementos, digital content, such as songs, photos, movies, games, software, and operating systems, stored within the memento need to be reconstructed. 
% For physical artifacts, the embedded content should be the contextual information (e.g., usage scenarios, associated individuals, and related stories, etc.) in the form of notes, photos, or videos.
% Users should be able to access and associate this content with IDI.
% For example, consider a bicycle with a scratched frame; in such cases, users can annotate the scratch with a brief note describing the incident that caused the scratch, and users can also give general comments on the bicycle (e.g. the history, interesting events, etc.).


% For example, IDIgine an aging bicycle with a scratched frame; in these situations, users can add a concise note describing the incident that caused the scratch.
% The second category are contents and usage scenarios related to the memento to be reconstructed, as the memento's memorial extension. 
% Digital contents in old devices are previously created or accessed by the user, containing unique value of personal marks. 
% As validated by our formative study (Sec 3.2.4), reproducing these content and making them accessible in IDI could be instrumental for both functional integrity and memory preservation. 
% Two main categories of embedded content, as the inclusion or the extension of the memento itself respectively, are emphasized in IDI. 
% 1.for digital devices: content, software and systems
% 2.for physical artefacts: Usage Scenarios
% To ensure the integrity of IDI, the original content of the reconstructed memento should be embed in IDI and can be accessed through simulating real interactions. Embed content can be songs, photos, software or systems that were created by the owner of the memento or embed in the digital device. 


% Interactive Features and Design Framework: While the interactive features used in the paper were noteworthy, they could benefit from a more formal design framework. A well-defined framework would provide context and structure, ensuring that the interactive elements are effectively harnessed to achieve the desired outcomes.


% \subsection{Designing the Prototype and Workflow for IDI Creation}
% \revision{we developed a prototype, InteRecon to help validate the feasibility of our design for interactivity reconstruction. }
% In this section, we introduce \textit{InteRecon}, an immersive authoring tool designed for end-users to create IDI.
% InteRecon enables users to create IDI from physical mementos that meet the features designed in Sec. 4.2. 
% We described the interactive process within InteRecon's five main categories of functions.
% Two of these functions (mapping tangible widgets and mapping mechanical components), were tailored for two prominent memento types respectively: digital devices and physical artifacts, while the rest of the functions are universally applicable to all types of mementos.



% % Considering the two representative types (digital devices and physical artifacts) as mementos in Sec. 3.2.1, we designed two reconstruction plan including mapping schemes for each type. 


% % an editing tool for end-users to create IDI that empower non-technical users to make their memory more sustainable. 
% % According to the envisioned features of IDI, the overarching aim of building IDI is to allow end-users to map features from physical mementos onto IDI. 


% % Our initial step involves utilizing lightweight 3D scanning to reconstruct physical appearances and then enable further editing. 

% \textbf{User-Oriented editing tool}: Considering digitization for mementos is a highly personal and universally relevant topic \cite{thompson2013autobiographical}, we aimed to enable end-users to effortlessly create their own IDI by InteRecon without the need for professional assistance. 
% % To achieve this, we have designed an IDI editing tool tailored for end-users, which will be further discussed in Section 4.3.

% \revision{delete this para or combine to the following Implemenation session}
% \textbf{Platform}: We propose using mobile and immersive technologies (VR/AR) sequentially to realize the authoring and presentation process of IDI for the following reasons: 
% 1) For DG1, mobile devices allow users to hold and scan physical mementos, providing a simpler method for users to obtain a reconstructed model. 
% Furthermore, some ideas for reconstructing the memento may arise accidentally in daily life. Using mobile devices for 3D scanning enables users to obtain a 3D reconstructed model anytime, anywhere. 
% 2) For DG2, IDI should simulate real-world interactions, such as pulling, pushing, and pinching mementos, potentially causing some objects to shake. 
% It necessitates users to engage in the authoring process without holding the device, employing their bare hands for interaction. 
% Moreover, to attain authenticity, IDI's presentation should be a stereo, see-through environment and conserves the real-world setting. 
% Therefore, we propose to using AR glasses to achieve this goal. 
% 3) For DG3, we propose to enabling mobile devices for users to upload embedded content and integrate it into IDI since digital files can be accessed by mobile devices more easily.

% \subsubsection{Reconstruct the 3D model by 3D scanning}
% \label{sec: Reconstruct the 3D model by 3D scanning}
% We developed a mobile application to enable users to reconstruct the 3D model of the physical memento through 3D scanning.
% The Fig. \ref{fig:3dapp} illustrated the entire scanning process. 
% First, the user needs to open the application and point it at the target object. 
% An automatic bounding box with the length, width, and height of the object is generated before capturing. 
% Then the user can move the mobile phone slowly to circle around the object while the application automatically captures the right images for reconstruction. 
% The app provided visual guidance on regions where the algorithm needs more images, along with additional feedback messages to help the user capture the best quality shots. 
% After finishing one orbit, the user can flip the object to capture the bottom. 
% Once scanning for three orbits (Front, Side \ and bottom surface of the bounding box) is completed, the application will proceed to the reconstruction stage, which runs locally on the mobile device. 
% In just a few minutes, a 3D reconstructed model will be ready for further use. 
% After getting the 3D model of the memento, the researcher will help to import it into a software\footnote{https://www.blender.org} to convert to an available format and import to the AR environment.

% \begin{figure*}[tbh!]
% \centering
% \includegraphics[width=\textwidth]{Figures/3dapp.jpg}
% \caption{\textbf{The interactive process for reconstructing the 3D model. (a) The user opens the application and points it at the target object. (b) An automatic bounding box is generated around the target object in the application interface. (c) Circle around the object with visual guidance on regions where the algorithm needs more images. (d) A 3D reconstructed model is ready for further use. }}
% \Description{Figure 2 has 4 sub-figures (a, b, c, d, from left to right) in a row, showing the interactive process for reconstructing the 3D model. (a) shows a user opens the mobile application and points it at the target object. (b) shows the screen of the mobile application, an automatic bounding box that is generated around the target object in the application interface. (c) shows the screen of the mobile application when scanning objects, a circle around the object with visual guidance on regions where the algorithm needs more images. (d) shows a 3D reconstructed model which is ready for the further use.}
% \label{fig:3dapp}
% \end{figure*}

% \subsubsection{Mapping tangible widgets on digital devices}
% InteRecon enables users to map tangible widgets of a physical memento to a 3D model. 
% We illustrated the interactive process of this function in Fig. \ref{fig:inter} top. 
% The 3D model would be obtained from section~\ref{sec: Reconstruct the 3D model by 3D scanning} and imported to the AR environment. 
% The user could first select the category of the digital devices: we provided two categories of digital devices-\textit{music player} and \textit{game}. Each category has the corresponding virtual widget set that contains the functionalities of its use. 
% % The virtual widgets were enabled the functions by their name.
% For example, if the user clicks the `Play' button, it will activate the function of playing the music in the AR glasses.
% For the category of Music player, we pre-set the virtual buttons of `Play', `Pause', `Next', `Last', `Volume Up', `Volume Down', and `Stop' for users to choose. 
% For the category of Game, we pre-set the virtual buttons of `On/Off', `Up', `Down', `Left', `Right', and `Switch' for users to choose. 
% The user needs to refer to the widgets on the physical digital device and spawn similar virtual widgets (with the same functionalities compared to the actual buttons) in AR by clicking. 
% Afterward, the user can move the position and adjust the size of the virtual widgets and the 3D model, to make sure they have a similar relative position and size to their physical counterparts.
% The `attach' button could help the user fix the position of the virtual widgets on the model. 
% The `invisible' button could make the virtual widgets invisible while preserving their functions that could be triggered in their positions on the surface of the 3D model in AR.
% Thus, the user could trigger the function on the surface of the 3D model without seeing the virtual widgets.

% % \ref{fig:inter} 

% % 1.  Select the function and the style.
% % 2. Spawn a widget in the scene and attach it to a certain location of a mesh.
% % 3. Adjust the size and property (e.g., visibility).

% \begin{figure*}[tbh!]
% \centering
% \includegraphics[width=\textwidth]{Figures/final_ver.jpg}
% \caption{\textbf{The AR interface with functions of mapping tangible widgets for digital devices and mechanical components for physical artifacts. (D1) Find similar virtual widgets (the `Play' button) from a pre-set virtual widgets set (the `Player' set). (D2) Adjust the size and the position of the virtual widget and the 3D model. (D3) Click the `Attach' button to fix the position of the virtual widget on the model and chick the `Invisible' button to make the virtual widget invisible. (D4) Press the surface of the 3D model and trigger the virtual widgets' functions in AR such as `Playing' the music. (P1) Adjust the size, the rotation, and the location of the segment plane to prepare the segmentation for the model. (P2) Segment the model along the plane. (P3) Move the `Joint' ball and map the `Movable' and `Base' segmentation from the pre-designed demonstrated virtual joint to the model. (P4) Try out the mapped joint with free hands.  }}
% \Description{Figure 3 shows an AR interface with functions of mapping tangible widgets for digital devices and mechanical components for physical artifacts. It has 2 rows, each row has 4 images. The first row shows the interaction pipeline of digital buttons: (D1) Find similar virtual widgets (the `Play' button) from a pre-set virtual widget set (the `Player' set). (D2) Adjust the size and the position of the virtual widget and the 3D model. (D3) Click the `Attach' button to fix the position of the virtual widget on the model and click the `Invisible' button to make the virtual widget invisible. (D4) Press the surface of the 3D model and trigger the virtual widgets' functions in AR such as `Playing' the music. For the second row, (P1) Adjust the size, the rotation, and the location of the segment plane to prepare the segmentation for the model. (P2) Segment the model along the plane. (P3) Move the `Joint' ball and map the `Movable' and `Base' segmentation from the pre-designed demonstrated virtual joint to the model. (P4) Try out the mapped joint with free hands.}
% \label{fig:inter}
% \end{figure*}


% \subsubsection{Mapping mechanical components on physical artifacts}
% InteRecon enables users to map mechanical components (e.g., physical joints) of the physical artifact to its static 3D mesh model obtained by the scanning function (mentioned in section~\ref{sec: Reconstruct the 3D model by 3D scanning}). 
% % For physical artifacts which include some mechanical components, the initial 3D model scanned previously is a static mesh model without any physical joints. 
% The Fig. \ref{fig:inter} bottom showed the interactive process of this function.
% In the `Physical' interface in AR, the user can incorporate the simulated physical joints on the model. 
% To achieve this, the mesh model should be broken down into several parts by a segmenting plane for further editing. 
% % A segmenting plane is provided to support the user in breaking down the mesh model along the plane. 
% The user can use their hands to adjust the size, the rotation, and the location of the plane to better match the potential segment position on the mesh. 
% To help the user rapidly map the joints, pre-designed demonstrated virtual joints according to the common mechanical joints design \cite{blake1985design} were provided in this interface (detailed descriptions in Appendix Table \ref{tab:joints}).
% Each of the virtual joints includes two cubes, indicating the relative movement in one degree of freedom and restrict movement in one or more others. 
% In order to illustrate relative movements more effectively, the two cubes are distinguished by different colors, with the grey cube representing the 'base' cube (non-movable) and the green cube representing the 'movable' cube (capable of motion).
% The user could use their hands to touch the green cube to observe the relative movement features of each demonstrated joint until they identify one that resembles the physical artifact.
% When finding a similar one, the user could move the yellow ball (the `Joint' in Fig. \ref{fig:inter} P1) to the joint position of the model.
% The user could select the 'movable' segment of the mesh model using a pinching gesture and confirm their choice by clicking the 'movable' button.
% This process is analogous when selecting the `base'.
% To conclude the process, the user can click the 'Apply' button next to the identified virtual joint to map it onto the mesh model.
% InteRecon also supports the incorporation of multiple joints by repeating the aforementioned process. 

% % 1.  Cut the mesh for making joints.
% % 2.  Find similar joints.
% % 3.  Map the joints.
% % 4.  bare hand interaction


% \begin{figure*}[tbh!]
% \centering
% \includegraphics[width=\textwidth]{Figures/content_app.jpg}
% \caption{\textbf{The screenshots of the content application, featuring functions for uploading, editing digital files, and updating in AR.}}
% \Description{Figure 4 shows the screenshots of the content application, featuring functions for uploading, editing digital files, and updating in AR.}
% \label{fig:upload}
% \end{figure*}


% \subsubsection{Enabling content upload for IDI through mobile devices} 
% As shown in Fig. \ref{fig:upload}, we have developed a mobile application enabling users to upload embedded content from physical mementos for IDI access.
% First, the user could import the content (e.g., digital files) from the digital devices mementos to the mobile device.  
% Subsequently, they select the content type (e.g., music, games, pictures, software, operating systems) and incorporate it into a certain model in the AR environment by transmitting the command of selection from the mobile phone to the AR environment.
% The application also provides content editing capabilities in the `content modification' interface, such as playlist creation or editing for MP3 files.



% % The usage of this application includes 3 steps with editing embed content in 3 interfaces. The user needs to select the content's type (e.g., music, games, pictures, software, operating systems, etc.) on the type selection interface and upload content from local devices that originally exists in the physical mementos. 


% % includes 3 steps with editing in 3 interfaces
% % To upload the corresponding content for a certain memento, the user needs to select the type of the memento first. An then choose the exact 

% %a type selection interface, an memento selection interface, and a content modification interface. 


% \subsubsection{Try out the IDI by hands}
% After completing the scanning, mapping, and uploading process, the user can try out the IDI using bare hands in AR.
% This mode can be activated by pressing the 'Display' button.
% For IDI of digital devices, users can test the enabled functions, such as tapping the 'play' widget on an MP3 IDI for music playback or utilizing the directional pad on a game console IDI for gaming.
% For IDI of physical artifacts, the user can use fingers to touch or gently push, allowing them to experience simulated actual movements of the physical memento within the IDI.









