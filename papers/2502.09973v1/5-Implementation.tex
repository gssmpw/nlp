% \subsection{Implementation}
% We built MemoTool's mobile applications on iPhone 14 Pro for implementing MemoTool's functions (Sec. 4.3.1 and Sec. 4.3.4) on mobile devices. For 3D object scanning, we utilized the reality kit (Object Capture API~\footnote{https://developer.apple.com/documentation/realitykit/guided-capture-sample}) from Apple Developer, to build an app that runs on iOS 17.0+. 
% For the AR interface of MemoTool (Sec. 4.3.2, Sec. 4.3.3 and Sec. 4.3.5), we implemented on HoloLens 2, which was connected to a PC (with an Intel i9-12900K CPU and a RTX A6000 24G GPU) using a wireless network using Unreal Engine Version 4.26. We integrated the Mixed Reality Tool Kit (MRTK~\footnote{https://github.com/microsoft/MixedReality-UXTools-Unreal}) to handle the hand interaction and UI elements in the application. 

% We used UE's built-in physical engine to implement the physical effects such as gravity, collision, physical joints, and hand manipulations, of IDM. 
% Specifically, for mesh segmentation, we first converted the scanned mesh to a \emph{Procedural Mesh} and called the \emph{Slice Procedural Mesh} method to cut the mesh with a hand-held cut plane at runtime. 
% Segmented meshes were stored both as an array of \emph{Procedural Mesh} in the UE program with further physical operations enabled and as static mesh copies in the disk. 
% The joint creation is enabled using the Unreal Engine’s in-built physics engine. 
% The hand interaction related to physical effects such as slightly touch on objects, is implemented by binding a collision sphere on the tip of the finger. 









% The content uploading from mobile devices to AR environment was conducted by Wizard-of-Oz \cite{10.1145/169891.169968}.


% We implemented the 3D object scanning method with an iPhone 14 Pro, which was able to apply the Light Detection and LiDAR scanner. 


% We built MemoTool on iPhone 14 Pro for implementing MemoTool's functions (Sec. 4.3.1 and Sec. 4.3.4) on mobile devices. For 3D object scanning, we utilized the reality kit (Object Capture API~\footnote{https://developer.apple.com/documentation/realitykit/guided-capture-sample}) from Apple Developer, to build an app that runs on iOS 17.0+. For the AR interface of MemoTool (Sec. 4.3.2, Sec. 4.3.3 and Sec. 4.3.5), we implemented on HoloLens 2, which was connected to a PC (with an Intel i9-12900K CPU and an RTX A6000 24G GPU) using a wireless network using Unreal Engine Version 4.26. We integrated the Mixed Reality Tool Kit (MRTK~\footnote{https://github.com/microsoft/MixedReality-UXTools-Unreal}) to handle the hand interaction and UI elements in the application. 
% The physical simulation function is enabled using the Unreal Engine’s in-built physics engine. The content uploading from mobile devices to AR environment was conducted by Wizard-of-Oz \cite{10.1145/169891.169968}.

% collision!!!!

% iphone with lidar
% hololens 2
% \subsubsection{Software}
% realityPro API
% Unreal Engine 4
% MRTK
% Mesh CUT(if any)

% The physics simulation of virtual content in the real world is enabled using the scene-understanding capability of Hololens 2 and Unity’s in-built physics engine. 

% The virtual models and visual efects were downloaded from the Unity asset store [9] and then imported into the system. The real-time sharing of vir- tual content among multiple users wearing Hololens is supported through through Photon Unity Networking (PUN) [5]. The MQTT broker which handles the data transfer between the IoT toolkit and Hololens 2, runs on a PC (Intel Core i7-8700K, 3.7GHz CPU, 64GB RAM, NVIDIA RTX2080Ti GPU) connected to the local area network.