\section{Related Work}
We briefly discuss different batching methods for NNs. The first method is full-batching where the entirety of the dataset is fed through a forward pass of the network. The loss, L, is computed for each of the N datapoints, $(x_i, y_i)$, in the dataset D,

\begin{equation}
\label{eq:full_batch_loss}
L = \sum_{i=1}^{N} L_i(y_i, x_i).
\end{equation}

An example of the loss, for regression, would be the mean squared error. The gradient of the loss with respect to the model weights, $w$, is used to update the weights. The simplest update equation is gradient descent,

\begin{equation}
\label{eq:update_weights}
w' = w + \epsilon \nabla_{w} \sum_{i=1}^{N} L_i,
\end{equation}

where $\epsilon$ represents the step size. Running the entire dataset through a forward pass of the model and computing the gradients of the weight parameters with respect to the loss is expensive on larger datasets for deep neural networks (DNN) where backpropagation is required____. As a result, typically, mini-batches are used for NN training where in each update-step a subset of the data is used to update model weights____. This leads to a batch gradient descent update equation. In the extreme case, known as stochastic (or on-line learning) gradient descent, the batch size is a single datapoint____.

These different batching methods have been thoroughly analyzed for NNs in the literature____ as a function of the batch size. Computation time as a function of batch size has also been explored on GPUs____. In____, large batch sizes were found to typically slow down the convergence of the model parameters. In practice, researchers typically set the batch size as a hyperparameter that is found via cross validation (CV).

For NNs, updating the model weights can take up the bulk of training time through backpropagation____. Typically, NNs operate on numeric data (or data that has been transformed into numeric values). When training the NN with a fixed mini-batch size of numeric data, batches have constant shape and memory requirements. Using Jax, this enables highly efficient training, since the the gradient update step can be compiled once at the start of the training. For convolutional neural networks (CNNs), this is not the case, and images of different pixel dimensions are often padded before being fed into the network____. Similarly, for graph neural networks, the graphs in the dataset typically contain a wide variety of number of nodes and edges. To this end, batching algorithms have emerged, which pad batches of graphs to constant shapes. In this way, the gradient update step for GNNs on batches can be compiled on GPU. Two such methods have become popular, static batching and dynamic batching. The static batching methods always collect a fixed number of graphs while dynamic methods add graphs incrementally to a batch until some padding budget/target is reached.

That said, not all models pad their data. M3GNet____, a GNN trained for interatomic potentials (IAP) is written in TensorFlow and performs batching in a similar way to NNs. Its batching procedure collects a number of graphs corresponding to the batch size, and concatenates the atoms, bonds, states and indices into larger lists for the batch. However, it does not perform any padding after batching data. The pyTorch GNN (ptgnn) library implements a dynamic batching algorithm that also does not use padding____. The algorithm adds graphs until either the batch has the desired batch size (\ie number of graphs) or some safety limit on the number of nodes in the batch has been reached. This safety limit ensures that the batch will fit into memory.

The pyTorch geometric library____ implements a similar dynamic batching algorithm. The user specifies whether to use either (but not both) a total number of nodes as the batch cutoff/limit or total number of edges. The algorithm then adds graphs incrementally to the batch until the target number of graphs are in the batch (\ie the target batch size) or the cutoff has been reached. It also allows the user to skip adding single graphs to the batch that would by themselves exceed the node/edge cutoff.

The Jraph library, which is built on JAX____, implements a dynamic batching algorithm____. Given a batch size, it performs a one-time estimate of the next largest multiple of 64 to which the sum of the nodes/edges in the batch should be padded. We can think of these estimates as the node/edge padding targets for each batch (\ie the number of nodes/edges in a batch after padding). This estimation is done by sampling a random subset of the data. It then iteratively adds one graph at a time to the batch and stops if adding another graph would exceed the node/edge padding targets or the maximum number of graphs (\ie the batch size) is already in the batch.

The Tensorflow GNN library____ implements a static and dynamic batching algorithm. The static batching adds a fixed number of graphs to the batch and then pads to a constant padding value. The dynamic batching method, similar to Jraph, estimates a padding budget (they call it a size constraint) for the batch based on a random subset of data, and then adds graphs incrementally to the batch as long as they do not break this budget.

The static and dynamic algorithms have, to our knowledge, not been described in the literature, but solely within code repositories. Experiments to measure the training time as a function of the algorithm, batch size, model, or dataset are also to our knowledge not found in the literature. Our work seeks to describe the static and dynamic batching algorithms in sufficient detail and perform the aforementioned timing experiments using an implementation based on the Jraph library. 

For datasets with larger graphs, different batching methods have emerged. PipeGCN____ breaks larger training graphs into smaller pieces and trains each partition on a different GPU. Node feature information from different partitions is passed between GPUs across a fast cross-GPU link. Batching methods that break training graphs into partitions are referred to in the literature as mini-batching in GNNs____ as compared to methods that always deal with the entire graph which are called full graph or full-batch methods. These methods are not dealt with in this paper since the methods are meant for datasets which contain very large graphs, unlike the datasets used in this work.