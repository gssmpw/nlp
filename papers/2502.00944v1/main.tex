%%%%%%%% ICML 2024 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newcommand{\ie}{i.e., }
\newcommand{\eg}{e.g., }
\newcommand{\blue}[1]{\color{blue}#1}
\newcommand{\red}[1]{\color{red}#1}
\newcommand{\brown}[1]{\color{brown}#1}
\newcommand{\correct}[2]{\sout{#1}{\red #2}}
\newcommand{\cyan}[1]{\color{cyan}#1}
\newcommand{\magenta}[1]{\color{magenta}#1}
\newcommand{\green}[1]{\color{green}#1}
\newcommand{\exciting}{{\usefont{T1}{lmtt}{b}{n}exciting}}
\newcommand\CoAuthorMark{\footnotemark[\arabic{footnote}]}
\usepackage{algorithm}
\usepackage{algorithmic}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Analysis of static and dynamic batching algorithms for GNNs}




\begin{document}

\twocolumn[
\icmltitle{Analysis of static and dynamic batching algorithms \\
           for graph neural networks}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.

\begin{icmlauthorlist}
\icmlauthor{Daniel Speckhard}{xx,yy}
\icmlauthor{Tim Bechtel}{xx,yy}
\icmlauthor{Sebastian Kehl}{zz}
\icmlauthor{Jonathan Godwin}{aa,xx}
\icmlauthor{Claudia Draxl}{xx,yy}
\end{icmlauthorlist}

\icmlaffiliation{xx}{Physics Department and Center for the Science of Materials, Humboldt-UniversitÃ¤t zu Berlin, Berlin, Germany}
\icmlaffiliation{yy}{Max Planck Institute for Solid State Research, Stuttgart, Germany}
\icmlaffiliation{zz}{Max Planck Computing and Data Facility, Munich, Germany}
\icmlaffiliation{aa}{Orbital Materials, London, United Kingdom} 

\icmlcorrespondingauthor{Daniel Speckhard}{dts@physik.hu-berlin.de}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Graph Neural Networks, Batching, Chemistry data, Materials science data}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Graph neural networks (GNN) have shown promising results for several domains such as materials science, chemistry, and the social sciences. GNN models often contain millions of parameters, and like other neural network (NN) models, are often fed only a fraction of the graphs that make up the training dataset in batches to update model parameters. The effect of batching algorithms on training time and model performance has been thoroughly explored for NNs but not yet for GNNs. We analyze two different batching algorithms for graph based models, namely static and dynamic batching. We use the Jraph library built on JAX to perform our experiments, where we compare the two batching methods for two datasets, the QM9 dataset of small molecules and the AFLOW materials database. Our experiments show that significant training time savings can be found from changing the batching algorithm, but the fastest algorithm depends on the data, model, batch size and number of training steps run. Experiments show no significant difference in model learning between the algorithms. 
\end{abstract}

% Further, we show that it is possible for the dynamic batching algorithm to be faster than the static algorithms, but the performance depends on the size distribution of the graphs in the data set.
\section{Introduction}

% \printinunitsof{in}\prntlen{\textwidth}

Graph neural networks (GNN) models have recently shown great promise in regression and classification tasks, where the input data can be represented as graphs~\cite{kipf2016semi}~\cite{xu2018powerful}. These methods have been applied to predict molecular and material behavior from nanometer to millimeter scale~\cite{schutt2018schnet,jorgensen2018neural, sanchez2020learning, neumann2024orb}. These tasks are of upmost importance to society, since they have opened up a research path towards exploring new molecules for drug design and carbon capture~\cite{choudhary2022graph} and finding new materials for energy storage and generation.~\cite{schaarschmidt2022learned}.

Similar to neural network (NN) models, GNN models typically contain millions of parameters and require large training datasets to achieve sufficient predictive power. In order to effectively train GNNs on large datasets, the graph structured data needs to be batched, otherwise each update over the entire dataset takes too long for the model parameters to converge in a reasonable amount of time. In this paper, we examine two different batching techniques for graph networks, static and dynamic batching. The two algorithms differ in that static batching always grabs the same number of graphs whereas dynamic batching adds graphs to the batch conditionally and seeks to ensure that the memory occupied by the batch of graphs is constant.

With the advent of neural architecture searches in different fields, including with graph structured data, it is often the case that researchers train model candidate architectures several thousand times~\cite{zoph2016neural, gao2021graph, speckhard2023neural}. End-users are likely to re-train the resulting model and re-tune hyper-parameters on new datasets. The total costs involved with the GNN model search and re-training pose a significant computational, financial and environmental  burden~\cite{korolev2024carbon, speckhard2025big,patterson2021carbon}. Therefore, any savings in training time offered by the batching algorithm are significant.

We examine the effect of the algorithms on training time and metrics. For different batch sizes, we compare the computation time required to assemble the graphs in a batch. We also analyze the time to update the model parameter weights using a batch of graphs. Finally, we monitor the effect of the batching techniques on the test metrics of the models. We do this for different batch sizes, models, and datasets. Our main contributions presented in this paper are:

\begin{itemize}
	\item We formally introduce the static and dynamic batching algorithms.
    \item The different padding schemes determine the number of recompilations for a given dataset and batch size.
	\item The performance of the algorithm is dependent on the graph distribution in the dataset, model used, batch size and the number of training steps run.
	\item Our results show that when running enough training steps to converge the model, either the static algorithm with padding to the nearest power of two or padding to the nearest multiple of 64 is fastest in terms of mean training time.
    \item Across the experiments, we observe at most a 33\% speedup in mean time per training step when switching from the slowest algorithm to the fastest.
	\item The experiments show no significant difference in model learning between the algorithms.
\end{itemize}


\section{Related Work}

We briefly discuss different batching methods for NNs. The first method is full-batching where the entirety of the dataset is fed through a forward pass of the network. The loss, L, is computed for each of the N datapoints, $(x_i, y_i)$, in the dataset D,

\begin{equation}
\label{eq:full_batch_loss}
L = \sum_{i=1}^{N} L_i(y_i, x_i).
\end{equation}

An example of the loss, for regression, would be the mean squared error. The gradient of the loss with respect to the model weights, $w$, is used to update the weights. The simplest update equation is gradient descent,

\begin{equation}
\label{eq:update_weights}
w' = w + \epsilon \nabla_{w} \sum_{i=1}^{N} L_i,
\end{equation}

where $\epsilon$ represents the step size. Running the entire dataset through a forward pass of the model and computing the gradients of the weight parameters with respect to the loss is expensive on larger datasets for deep neural networks (DNN) where backpropagation is required~\cite{hastie2009elements}. As a result, typically, mini-batches are used for NN training where in each update-step a subset of the data is used to update model weights~\cite{bishop2006pattern}. This leads to a batch gradient descent update equation. In the extreme case, known as stochastic (or on-line learning) gradient descent, the batch size is a single datapoint~\cite{bottou1991stochastic}.

These different batching methods have been thoroughly analyzed for NNs in the literature~\cite{bottou2007tradeoffs} as a function of the batch size. Computation time as a function of batch size has also been explored on GPUs~\cite{kochura2019batch}. In~\cite{byrd2012sample}, large batch sizes were found to typically slow down the convergence of the model parameters. In practice, researchers typically set the batch size as a hyperparameter that is found via cross validation (CV).

For NNs, updating the model weights can take up the bulk of training time through backpropagation~\cite{lister1995empirical}. Typically, NNs operate on numeric data (or data that has been transformed into numeric values). When training the NN with a fixed mini-batch size of numeric data, batches have constant shape and memory requirements. Using Jax, this enables highly efficient training, since the the gradient update step can be compiled once at the start of the training. For convolutional neural networks (CNNs), this is not the case, and images of different pixel dimensions are often padded before being fed into the network~\cite{tang2019impact}. Similarly, for graph neural networks, the graphs in the dataset typically contain a wide variety of number of nodes and edges. To this end, batching algorithms have emerged, which pad batches of graphs to constant shapes. In this way, the gradient update step for GNNs on batches can be compiled on GPU. Two such methods have become popular, static batching and dynamic batching. The static batching methods always collect a fixed number of graphs while dynamic methods add graphs incrementally to a batch until some padding budget/target is reached.

That said, not all models pad their data. M3GNet~\cite{chen2022universal}, a GNN trained for interatomic potentials (IAP) is written in TensorFlow and performs batching in a similar way to NNs. Its batching procedure collects a number of graphs corresponding to the batch size, and concatenates the atoms, bonds, states and indices into larger lists for the batch. However, it does not perform any padding after batching data. The pyTorch GNN (ptgnn) library implements a dynamic batching algorithm that also does not use padding~\cite{ptgnn2022microsoft}. The algorithm adds graphs until either the batch has the desired batch size (\ie number of graphs) or some safety limit on the number of nodes in the batch has been reached. This safety limit ensures that the batch will fit into memory.

The pyTorch geometric library~\cite{fey2019fast} implements a similar dynamic batching algorithm. The user specifies whether to use either (but not both) a total number of nodes as the batch cutoff/limit or total number of edges. The algorithm then adds graphs incrementally to the batch until the target number of graphs are in the batch (\ie the target batch size) or the cutoff has been reached. It also allows the user to skip adding single graphs to the batch that would by themselves exceed the node/edge cutoff.

The Jraph library, which is built on JAX~\cite{jax2018github}, implements a dynamic batching algorithm~\cite{jraph2020github}. Given a batch size, it performs a one-time estimate of the next largest multiple of 64 to which the sum of the nodes/edges in the batch should be padded. We can think of these estimates as the node/edge padding targets for each batch (\ie the number of nodes/edges in a batch after padding). This estimation is done by sampling a random subset of the data. It then iteratively adds one graph at a time to the batch and stops if adding another graph would exceed the node/edge padding targets or the maximum number of graphs (\ie the batch size) is already in the batch.

The Tensorflow GNN library~\cite{ferludin2023tfgnn} implements a static and dynamic batching algorithm. The static batching adds a fixed number of graphs to the batch and then pads to a constant padding value. The dynamic batching method, similar to Jraph, estimates a padding budget (they call it a size constraint) for the batch based on a random subset of data, and then adds graphs incrementally to the batch as long as they do not break this budget.

The static and dynamic algorithms have, to our knowledge, not been described in the literature, but solely within code repositories. Experiments to measure the training time as a function of the algorithm, batch size, model, or dataset are also to our knowledge not found in the literature. Our work seeks to describe the static and dynamic batching algorithms in sufficient detail and perform the aforementioned timing experiments using an implementation based on the Jraph library. 

For datasets with larger graphs, different batching methods have emerged. PipeGCN~\cite{wan2022pipegcn} breaks larger training graphs into smaller pieces and trains each partition on a different GPU. Node feature information from different partitions is passed between GPUs across a fast cross-GPU link. Batching methods that break training graphs into partitions are referred to in the literature as mini-batching in GNNs~\cite{bajaj2024graph} as compared to methods that always deal with the entire graph which are called full graph or full-batch methods. These methods are not dealt with in this paper since the methods are meant for datasets which contain very large graphs, unlike the datasets used in this work.


\section{Preliminaries} \label{sec:preliminaries}

We now dive deeper into the static and dynamic batching algorithms. In static batching, the method grabs a fixed number of graphs for each batch. It then checks how many nodes/edges are present in the batch. It then adds a dummy graph and adds dummy edges/nodes to this graph to serve as padding. It pads the number of nodes/edges up to the next power of two to ensure that all batches have the same number of nodes/edges (and graphs), so that the memory of the input to the update function is constant and does not require recompilation. This means that, typically, the number of graphs in the batch is the batch size minus one in order to leave space for the dummy graph in the batch. The fact that a power of two is used for padding is arbitrary. We refer to this algorithm as static-$2^N$. We also evaluate static batching with padding to the nearest multiple of 64. We label this algorithm as static-64. A third method is also possible, and implemented in TensorFlow's GNN (TF-GNN) library, where the algorithm first finds the graph with the most nodes/edges in the graph and multiplies this number by the batch size to get the padding target. We refer to this algorithm as static-constant.

Over the course of training, one might encounter a batch that requires a larger power of two than previously used to contain all of the nodes and edges in the batch. This requires the gradient-update function to recompile for a different, \ie larger, input size. The static batching algorithm is shown in Algorithm \ref{alg:static_batching}. The padding method, pad-power-of-2, called in the algorithm, adds a dummy graph with fake nodes/edges to the list of graphs, so that the sum of the nodes/edges in the list of graphs is a power of two. Pseudocode for the method is found in the Appendix, Section~\ref{sec:appedix_algorithms}. The batch method called in the algorithm, concatenates all of the nodes/edges in the list of graphs and creates a single super-graph out of the list of graphs (pseudocode is found in the Appendix).

\begin{algorithm}
\caption{Static batching}\label{alg:static_batching}
\begin{algorithmic}
  \STATE {\bfseries Input:} batch size $B_G$, training graphs $G$, start point $s$
  \STATE $A \leftarrow []$ \COMMENT{Accumulated list of graphs}
  \STATE $n_n \gets 0$ \COMMENT{Accumulated \# of nodes}
  \STATE $n_e \gets 0$ \COMMENT{Accumulated \# of edges}

  % \STATE $r\gets a\bmod b$
  \FOR{$i=s$ {\bfseries to} $s+B_G$}
    % \COMMENT{loop over data graphs from start point}
    \STATE $A \leftarrow \text{$A$.append($G[i]$)}$
    \STATE $n_n \text{ += } \text{$G[i]$.num-nodes}$
    \STATE $n_e \text{ += } \text{$G[i]$.num-edges}$
    \IF{$len(A) == (B_{G} - 1)$}
      \STATE \textbf{return} \text{batch(pad-nearest-power-2}($A$, $n_n$, $n_e$)) 
        \ENDIF
  \ENDFOR
\end{algorithmic}
\end{algorithm}

The dynamic batching algorithm, as implemented in Jraph and TF-GNN, first grabs a random subset of the data (\eg 1000 graphs). It then estimates the mean nodes per graph and mean edges per graph. It uses this to create a padding target (or memory budget), which in effect is the maximum number of nodes and edges allowed to be stored in the batch. The edge (node) padding target is created by multiplying the mean number of edges (nodes) by the batch size and then rounding to the nearest multiple of 64. The padding target estimation is shown in Algorithm \ref{alg:estimate_padding_target}, and is used to determine the padding required for each batch to ensure that each batch uses the same amount of memory.

\begin{algorithm}
\caption{Estimate dynamic batching padding target}
\label{alg:estimate_padding_target}
\begin{algorithmic}
% \COMMENT{Input number of graphs, $N$ list of graphs, $G$, and batch size, $B$}
  \STATE {\bfseries Input:} subset size $N$, list of graphs $G$, batch size $B_{G}$

  \STATE $n \leftarrow 0$ \COMMENT{number of nodes in N graphs}
  \STATE $e \leftarrow 0$ \COMMENT{number of edges in N graphs}

  \STATE $g \leftarrow G[1:N]$ \COMMENT{Grab N graphs from training data}

  \FOR{$i=1$ {\bfseries to} $i=N$}
    \STATE $n$ \text{ += } $G[i].$\text{num-nodes}
    \STATE $e$ \text { += } $G[i].$\text{num-edges}
  \ENDFOR

  \STATE $B_n \leftarrow \text{next-multiple-64}(n*B_{G}/N) $ \COMMENT{padding target}
  \STATE $B_e \leftarrow \text{next-multiple-64}(e*B_{G}/N) $
  \STATE \textbf{return} \textbf{$B_n$, $B_e$, $B_{G}$}
\end{algorithmic}
\end{algorithm}

The dynamic algorithm, given the padding target, appends one graph at a time to the initially empty batch. Before the addition of each graph, it checks if adding the graph would put the batch over the node or edge padding target. If it does, the graph is not added to the batch. Instead, the algorithm adds a dummy graph with fake nodes/edges, the number of which is determined so as to pad the number of nodes/edges to the padding target. It then adds, if necessary, dummy graphs with no nodes/edges to ensure that the number of graphs in the batch is equal to the batch size. A pseudocode representation is shown in Algorithm \ref{alg:dynamic_batching_2}. If, however, during the course of training, a single graph is encountered with more nodes/edges than the padding target, the program terminates. The algorithm must be restarted with a larger node/edge padding target. This issue can be minimized by looking at a larger sample of graphs when estimating the targets. Alternatively, the user can loop through the entire dataset of graphs and find the graph with the largest number of nodes/edges to determine suitable budgets. Note that the algorithm we present here is quite different from the pyTorch-Geometric implementation, whose implementation does not perform a budget estimation (has to be entered by the user) and it does not perform a check on the number of edges, but only the nodes. We find the Jraph/TF-GNN implementation, for which a simplified version is shown in Algorithm \ref{alg:dynamic_batching_2}, to be the more sophisticated, and so it was used for our tests.

\begin{algorithm}
\caption{dynamic batching}\label{alg:dynamic_batching_2}
\begin{algorithmic}
  \STATE {\bfseries Input:} set of training graphs $G$
  \STATE $b \leftarrow \left(n_n, n_e, n_g\right) = \text{estimate\_budget}\left(G\right)$
  \STATE $B \leftarrow \left[\,\right]$
  \STATE $s \leftarrow (0,0,0)$
  \STATE $n_n \gets 0$ \COMMENT{Accumulated \# of nodes}
  \STATE $n_e \gets 0$

  \FOR{$g \text{ in } G$}
    \STATE $s_i \leftarrow \text{graph\_size}\left(g\right)$
    \IF{$s + s_i > b - (0,0,1)$}
        \STATE $\textbf{yield}\,\text{batch(pad-to-target($B$, $n_n$, $n_e$))}$
    \ELSE
        \STATE $B \text{ += } g$
        \STATE $s \text{ += } s_i$
        \STATE $n_n \text{ += } s_i[0]$ \COMMENT{Update node sum}
        \STATE $n_e \text{ += } s_i[1]$

    \ENDIF
  \ENDFOR
\end{algorithmic}
\end{algorithm}

\section{Datasets}
We run profiling experiments on two different datasets. The QM9 dataset~\cite{ramakrishnan2014quantum} contains chemical properties (\eg internal energies, molecular orbital energy levels) of small organic molecules. The AFLOW dataset~\cite{curtarolo2012aflow} is a collection of relevant material properties (\eg formation energies, band gaps, elastic properties). We choose these datasets for two reasons. First, they have served as benchmark datasets for GNN models~\cite{schutt2018schnet, li2024quantum, bechtel2023band}. Second, they have real world applications: QM9 may help models better perform targeted drug discovery, while AFLOW may help models discover more efficient solar cell semiconductors. For QM9 we target the molecular internal energy and for AFLOW we focus on learning the enthalpy of the crystal. 

The datasets are not provided in a graph format. To convert them to graphs, each atom is represented as a node in the graph. For QM9 the edges are added by fully connecting the nodes. For AFLOW, the edges are added with the K-nearest neighbor algorithm based on pairwise distance and setting K equal to 24. This KNN value was chosen from CV studies in the literature~\cite{jorgensen2018neural}. From the AFLOW dataset, we remove duplicates and keep only calculations that contain both the enthalpy and the band structure, following the procedure in~\cite{bechtel2023band}. We visualize the variety in the graph structures in the datasets in Fig. \ref{fig:histogram_of_datasets}. From the figure we can see that the QM9 dataset node distribution appears Gaussian and centered around 17 nodes per graph. AFLOW's node distribution has long tails despite the mean of the distribution being smaller than QM9. The distribution of edges for AFLOW and QM9 are reflective of the distrubiton of the nodes, this is because the number of edges are dependent on the number of nodes when using the KNN algorithm or fully connecting the graphs.

%
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/histogram_of_datasets.png}
\caption{Histograms of the number of nodes (top) and edges (bottom) in the AFLOW and QM9 datasets.}
\label{fig:histogram_of_datasets}
\end{figure}
%

\section{Implementation}

There are several open-source software libraries to choose from to perform these experiments. We opt to use the Jraph library~\cite{jraph2020github}, which is built using the JAX library~\cite{jax2018github}. The JAX library traces out computations into a computation graph that it uses to optimize/compile the code. This compiled auto-differentiation code is optimized to run fast, especially on GPU. To run our experiments, we extend Jraph to run the static-$2^N$, static-64, and static-constant algorithms, log necessary profiling information and run the models we have selected for our tests.

\section{Compute hardware}

As mentioned, the benefit of dynamic batching is that the memory allocated to a batch of graphs is always the same for each batch. This allows the user to compile the update functions only once and optimize the compiled binary to run on GPU. Note that with static-64 and static-$2^N$ batching, the update function can also be compiled, but needs to be recompiled for different multiples of 64 / powers of two (the static-constant algorithm does not have this problem). We time the training of models for different batching algorithms. We run experiments on a cluster that comprises two Intel Xeon IceLake Platinum 8360Y CPUs and 4 NVIDIA A100 GPUs connected via NVLINK. Our experiments use a single GPU node and a single CPU core. This is done so that we can understand the algorithms performance in a controlled setting. We compare these experiments to those done without a GPU and run only on a single Intel Xeon IceLake Platinum 8360Y CPU core.

\section{Models}

We evaluate two models to get insight into any model dependent effects on our profiling results. The models take in a graph, $G$, composed of nodes (or vertices) and edges $G(n, e)$. The nodes are represented by feature vectors $h_n^t$, where the subscript represents the specific node $n$, and the superscript $t$ represents the number of times the vector has been updated. 

The SchNet model~\cite{schutt2018schnet} is a graph convolutional NN. The node feature vectors are updated with a node update equation (or convolution) that convolves the feature vector of the node, $h_i^{t}$, and feature vectors in neighborhood, $N_i$, of the node, $i$. It uses a convolutional kernel, $W(\Vec{r_j}-\Vec{r_i})$, which depends on the euclidean distance between nodes:

\begin{equation}
    h_i^{t+1}=\sum_{j \in N_{i}} h_{j}^{t} \odot W^t(\Vec{r_j}-\Vec{r_i}).
\label{eq:vertex_update_function}
\end{equation}

We also look at a message passing NN model with edge updates (MPEU) ~\cite{jorgensen2018neural, bechtel2023band}. Message passing NNs make use of a message function $M_{ij}^t$, which is defined for a pair of nodes, $i$ and $j$, and their corresponding edge $e_{ij}^t$.

\begin{equation}
M^t_{ij} = f_{m}(h_{i}^t, h_{j}^t, e_{ij}^t)
\label{eq:message_function}
\end{equation}

Here, the messages are aggregated for each node with a permutation-invariant operator. The MPEU aggregates the messages with a sum,

\begin{equation}
m_i^{t+1}=\sum_{j \in N_{i}} M_{ij}^{t}.
\label{eq:message_function_aggregation}
\end{equation}

In the MPEU, the edges between any two connected nodes, $i$ and $j$, are represented by edge vectors $e_{ij}^t$. The message function is a function of the edge vector,
%
\begin{equation}
    M^t_{ij} = f_m^t (h_j^t, e_{ij}^t)=(W_1^t h_j^t) \odot  \sigma(W_3^t \sigma(W_2^t e_{ij}^t)).
\end{equation}
%
where, the feature vectors, $h_i$, are updated in the MPEU as:
\begin{equation}
    h_n^{t+1}=S_t(h_n^t,m_n^{t+1})=h_n^t+W_5^t\sigma (W_4^t m_n^{t+1}).
    \label{eq:state_transition_function}
\end{equation}
%
$\odot$ denotes element-wise multiplication and $\sigma$ represents the shifted soft plus function~\cite{zhao2018novel}.

These two models were chosen, since the SchNet model is often used as a benchmark, and the MPEU has shown better results on the AFLOW dataset in a benchmark study~\cite{bechtel2023band}. These models vary in size. The SchNet model has 84,768 trainable parameters, while the MPEU model contains roughly 30 times more, namely 2,553,472. Large transformer based models were avoided due to computing time constraints on GPU.

\section{Profiling results}

To understand how the batching algorithms work in practice, we analyze the statistics of the number of nodes in a batch before padding occurs. The histograms for both batching algorithms are shown for the QM9 dataset in Fig. \ref{fig:batch_stats_batch_32} for 10,000 batches with a batch size of 32. As stated above, the dynamic batching algorithm checks the node and edge padding target, as well as the number of graphs already in the batch, before adding a graph to the batch. For this dataset and batch size, the dynamic algorithm's node padding target is 576. We can see that all dynamic batches have fewer than 576 nodes before padding. This results in a one-sided distribution, roughly a truncated Gaussian distribution, where the right-hand side is cutoff near the budget. For static batching, however, no such check exists, and we observe a roughly Gaussian distribution of nodes. Similar effects are seen in the figure for the number of edges before padding.

%
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/batch_stats_nodes_edges_size_32_qm9.png}
\caption{Histograms of the number of nodes (top) and edges (bottom) in a batch of size 32 before padding for the static-$2^N$ and dynamic batching algorithms running on the QM9 datset.}
\label{fig:batch_stats_batch_32}
\end{figure}
%

We perform timing experiments on both algorithms for four batch sizes (16, 32, 64, 128) using both datasets and models for two million training steps. This number of training steps was chosen since it resulted in converged models ~\cite{bechtel2023band}. We run each combination of batch size, algorithm, model, and dataset ten times to limit noise effects from the hardware in the profiling results. Each experiment reports the mean batching time, mean gradient update step time, and the sum of the two (combined time).


\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/profiling_2mill_schnet_gpu_aflow_left_qm9_right_mean.png}
\caption{Mean batching time (upper row), mean gradient-update time (middle row), and the mean combined time (bottom row) for varying batch sizes on the AFLOW (left) and QM9 (right) data using the SchNet model. For each datapoint, ten iterations of two million training steps are run.}
\label{fig:profiling_gpu_schnet_2million}
\end{figure}

According to the figure, the static-$2^N$ and static-64 algorithms perform batching at near identical speeds. The dynamic batching, however, is clearly slower. All three algorithms show roughly linear scaling of the batching time with respect to the batch size, however, dynamic batching scales worse for larger batch sizes (\ie has a steeper slope). This reflects the expectation that the time complexity of the batching algorithms is governed by the batch size. The dynamic algorithm is slower due to the added overhead from the bookkeeping of the padding targets. The batching results depend slightly on the dataset, but the rankings do not change and the scaling appears similar. The results for the MPEU model are shown in Fig. \ref{fig:appendix_profiling_2mill_gpu_mpeu_mean} in the Appendix.

The timing results of the gradient-update step at small batch sizes is similar for all algorithms. For batch size, 128, the dynamic batching is fastest for both datasets. The static-64 and dynamic algorithm show roughly linear scaling in the update time with the batch size. The static-$2^N$ algorithm shows poor relative performance especially for larger batch sizes. More work needs to be done to understand the static-$2^N$ scaling behavior. Similar results are seen for the MPEU model in the Appendix, with the gradient-update step being slower for all algorithms for the larger parameter count MPEU model.

The sum of the batching and gradient-update step mean timings, \ie the mean combined time, roughly gives us the mean time per training step. For the SchNet model, the static-$2^N$ model performs the combined steps the fastest for the AFLOW dataset and for the batch size 16 in the QM9 dataset. For the batch size 64 and 128, the static-64 algorithm runs fastest. For the MPEU model the static-$2^N$ algorithm is fastest for batch sizes 16, 32 and 64 on AFLOW but the static-$64$ algorithm is faster for the batch size 128 on AFLOW and for all batch sizes on QM9 data. The poor update step scaling of the static-$2^N$ algorithm becomes the limiting factor for larger batch sizes on the QM9 dataset. The difference between the algorithms is at most 2.9 ms for the QM9 dataset using the MPEU model, from the static-$2^N$ to the static-64 algorithm. This difference represents a 33\% speedup in training time.

For fewer training steps, the number of recompilations required in the gradient-update step is the dominant factor defining the ranking of the algorithms. This number of recompilations is shown in Fig. \ref{fig:recompilation_counter} for the MPEU model on QM9 data for two million training steps. We see that the static-64 algorithm performs several orders of magnitude more recompilations than the static-$2^N$ and dynamic algorithms. This is to be expected, since the dynamic algorithm compiles only once, and if it encounters a graph, which is bigger than the budget, the program terminates. Moreover, for static-64 batching, JAX recompiles the gradient-update step function every time a new nearest multiple of 64 is encountered when padding, and for static-$2^N$ batching every time a new nearest power of two is encountered, which is less likely for large powers of two.  Note that most of the recompilations for the QM9 dataset happen in the first hundred thousand training steps. For example, for the batch size 32, the static-$2^N$ algorithm recompiles four times while the static-64 algorithm recompiles 89 times. From training step one hundred thousand to two million, the static-$2^N$ algorithm does not recompile again and the static-64 algorithm recompiles only an additional twenty times. The effect of recompilations is seen in Fig. \ref{fig:appendix_profiling_100k_gpu_mpeu_mean} and Fig.~\ref{fig:appendix_profiling_100k_gpu_mpeu_median} in the Appendix which show the mean and median combined times after running only one hundred thousand training steps. In the figures, the static-64 algorithm has the lowest median update times while the static-$2^N$ has the lowest mean update times. The mean is affected by outliers while the median is not. This tells us that when removing the effect of outliers, \ie slow gradient-update steps which are caused by recompilations, the static-64 algorithm is fastest to update the gradients. Further, this tells us that the algorithm training time rankings are dependent on the dataset (whose variability in graph sizes cause recompilations) and the of number training steps run.

%
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/recompilation_count_2_million.png}
\caption{Number of recompilations on the QM9 dataset after two million training steps in the gradient-update step as a function of batch size for different batching algorithms.}
\label{fig:recompilation_counter}
\end{figure}
%


The batching algorithm may also affect the learning of the model. Dynamic batching does not always include the same number of graphs in a batch, since one or more fake padded graphs are added when the node or edge budget is reached. These padded graphs do not contribute to the gradient in the update step. The histogram in Fig. \ref{fig:histogram_number_of_graphs} depicts the distribution of the number of graphs before padding for the AFLOW and QM9 dataset after running 10,000 iterations with the dynamic algorithm. While most batches contain 31 graphs for a batch size of 32, some batches for the AFLOW dataset contain as few as ten graphs from the training data. That the QM9 graph node and edge distribution have shorter tails than the AFLOW distribution, as seen in Fig. \ref{fig:histogram_of_datasets}, may explain why the distribution of the number of graphs before padding has shorter tails for QM9 than for AFLOW.

%
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/batch_stats_num_graphs_before_batching_size_32_aflow.png}
\caption{Histogram of number of graphs before padding in a batch of  size 32 for the AFLOW and QM9 dataset.}
\label{fig:histogram_number_of_graphs}
\end{figure}
%

The effect of the algorithm on learning can be discerned by training each model to convergence. The test loss (RMSE) on the test data for both models on the QM9 data is shown in the Appendix Fig. \ref{fig:appendix_test_curves_qm9}. The test loss curves as a function of training time are very similar for the dynamic and static batching algorithms. The same is true for the AFLOW data which can be seen in the accompanying code repository. In our experiments the batching methods do not significantly affect the test metrics despite the fact that the dynamic algorithm often has fewer true training graphs in the batch.

Finally, the static-constant algorithm was also evaluated for a subset of data, models and batch sizes. The results are shown in the Appendix Section \ref{sec:appendix_static_consant}. The static-constant model performs poorly compared to the other static algorithms, especially for the AFLOW dataset. This is because the AFLOW dataset contains a long tailed distribution in terms of the node/edges and as a result the static-constant model uses a large padding target that slows down the timing results. As a result, we did not employ this method for the full set of data combinations in our timing experiments.

\section{Discussion}

Our results show that the batching is slowest for the dynamic algorithm. This is due to the added overhead from the
bookkeeping of the padding targets. All of the algorithms show linear scaling in the batching time with respect to the batch size. The static-64 and dynamic algorithms show linear scaling in the gradient-update step, while the static-$2^N$ algorithm shows exponential scaling. This is likely due to the exponentially larger memory sizes required when using power of two for padding. In our experiments, the static-64 or the static-$2N$ algorithms are the fastest in terms of mean training time. Which of the two is fastest, depends on the batch size, model, number of training steps run, and dataset. Our experiments show that for some datasets and batch sizes, the speedup in training time from the slowest algorithm to the fastest can be over 33\%. The findings in this paper recommend that for each new dataset, model, and batch size, the user should explore possible savings offered by switching the batching algorithm.

The learning curves from the static and dynamic algorithms do not differ significantly for the two datasets examined here. This is despite the fact that the dynamic batching algorithm contains, on average, fewer labeled graphs per batch. Lastly, although the dynamic batching method does not yield the fastest training time results on GPU for our results, it's possible for other datasets, where the graphs in the data vary significantly in size, it outperforms the static methods.

We hope that this work serves to aide users to reduce the computational cost of training graph based models. Further work should target a larger number of datasets (node-level targets as well), sets of models, and compute devices and analyze the effect of using a PyTorch based implementation. Lastly, this work could be extended to combine the batching algorithms with batching schemes that partition large graph networks across GPUs.

\section*{Software and Data}

The software to perform the batching, profiling experiments, parsing of experiments, and plotting is found in \url{https://github.com/speckhard/icml_gnn_batching}.

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

\section*{Impact Statement}

This paper presents work whose goal is to analyze and profile batching algorithms methods that reduce the environmental impact of training graph neural network models.


\bibliography{main}
\bibliographystyle{icml2024}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
% \onecolumn
%\section{You \emph{can} have an appendix here.}

%You can have as much text here as you want. The main body must be at most $8$ pages long.
%For the final version, one more page can be added.
%If you want, you can use an appendix like this one.  

%The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Batch and pad algorithms} \label{sec:appedix_algorithms}
For completeness, we provide pseudocode for the batch algorithm mentioned in Section~\ref{sec:preliminaries}. The pseudocode provided is a simplified version of the method found in Jraph, although the TF-GNN, ptgnn, and pyTorch libraries all perform something similar. The basic principle to create a single disconnected graph that contains all of the information of the individual graphs in the batch. The batch method in Algorithm \ref{alg:appendix_batching}, works by creating six empty lists to store information about the batch. It creates a list to collect information about the individual graphs. The lists collect node/edge feature vectors, number of node/edges, and sender/receivers node indices.  It then loops over the input graphs list, and concatenates the graph's node features, edge features, number of nodes/edges and sender/receiver indices into the respective batch lists. Finally, the method creates a new graph object where the constructor is fed the concatenated lists. Thereby, from a list of graphs, we have created one super-graph containing all of the information about the smaller graphs. The individual graphs can be rebuilt from the super-graph if desired. Note that the sender and receiver node indices need to be offset by a running counter of the number of nodes already added into the batch.

\begin{algorithm}
\caption{Batch}\label{alg:appendix_batching}
\begin{algorithmic}
  \STATE {\bfseries Input:} set of training graphs $G$
  \STATE $l_n \leftarrow []$ \COMMENT{list of node features}
  \STATE $l_e \leftarrow []$ \COMMENT{list of edge features}
  \STATE $l_s \leftarrow []$ \COMMENT{list of receivers}
  \STATE $l_r \leftarrow []$ \COMMENT{list of senders}
  
  \STATE $n_n \leftarrow []$ \COMMENT{list of number of nodes}
  \STATE $n_e \leftarrow []$ \COMMENT{list of number of edges}

  \STATE $o \leftarrow 0$ \COMMENT{offset}

  \FOR{g \text{ in } G}
    \STATE $l_n$ \text{ = } \text{concatenate}($l_n$, \text{g.nodes\_list})
    \STATE $l_e$ \text{ = } \text{concatenate}($l_e$, \text{g.edges\_list})
    \STATE $n_n$ \text{ = } \text{concatenate}($n_n$, \text{g.num\_nodes})
    \STATE $n_e$ \text{ = } \text{concatenate}($n_e$, \text{g.num\_edges})

    % add offsets to sender and reciever nodes
    \FOR{$i=0$ {\bfseries to} $\text{len(g.senders})$}
        \STATE $l_r\text{.append(g.receivers[i] + o)}$ 
        \STATE $l_s\text{.append(g.senders[i] + o) }$
    \ENDFOR
    \STATE o \text{ += } \text{g.num\_nodes})
    
  \ENDFOR   

\STATE \textbf{return} $\text{Graph($l_n$, $l_e$, $n_n$, $n_e$)} $
\end{algorithmic}
\end{algorithm}

The static algorithm's padding method, pad-nearest-power-2, mentioned in Section \ref{sec:preliminaries} works by adding a fake dummy graph to the list of graphs. It adds a graph object with the number of nodes and edges being equal to the number of nodes/edges required to bring the sum of all the nodes/edges in the list of graphs to the nearest power of two. The algorithm is shown in pseudocode in Algorithm \ref{alg:appendix_pad_to_target}. When running the static-64 algorithm the padding is done to the nearest multipe of 64.

\begin{algorithm}
\caption{pad-nearest-power-2}\label{alg:appendix_pad_to_target}
\begin{algorithmic}
  \STATE {\bfseries Input:} set of training graphs $G$, batch size $B$
  \STATE $s_n \leftarrow 0$ \COMMENT{sum of nodes}
  \STATE $s_e \leftarrow 0$ \COMMENT{sum of edges}
  \STATE $m_n \leftarrow 0$ \COMMENT{missing nodes}
  \STATE $m_e \leftarrow 0$ \COMMENT{missing edges}
    
  \FOR{g \text{ in } G}
    \STATE $s_n$ \text{ += } $g[i]$.\text{num\_nodes}
    \STATE $s_e$ \text{ += } $g[i]$.\text{num\_edges} 
  \ENDFOR

  \STATE $t_n$ \text{ = } \text{next\_power\_2}($s_n$)
  \STATE $t_e$ \text{ = } \text{next\_power\_2}($s_e$)
  \STATE $m_n$ \text{ = } $t_n$ \text{ - } $s_n$
  \STATE $m_e$ \text{ = } $t_e$ \text{ - } $s_e$

  \STATE $G'$ \text{ = Graph}($m_n$, $m_e)$
  \STATE \textbf{return } $G$.\text{append}($G'$)
\end{algorithmic}
\end{algorithm}

For the static-constant algorithm, the method first finds the graph with the largest number of nodes and the graph with the largest number of edges. These values are saved and when rounded to the nearest multiple of 64 serve as targets for padding. For the dynamic algorithm, the padding method is slightly more complex than Algorithm \ref{alg:appendix_pad_to_target}. The padding needs to add more dummy graphs if after adding a single dummy graph, the batch has less graphs than the batch size.

\section{Timing experiments setup}

The timing experiments made use of python's time library. Timing statements were executed before batching, before the gradient-update step and after the gradient-update step. Block until ready commands were executed to ensure operations on the GPU had finished before the timing measurements were taken. We also experimented with placing timing measurements before the training loop and after the training loop (\ie after 2 million steps in some cases). We then averaged this time by the number of training steps executed. We found this number to be within a standard deviation of the sum of our batching and gradient-update step timing results. This points to the fact that the library runs batch creation and update-kernel execution consecutively. It is possible to run these steps synchronously but this optimization was not done in this study. More details on the implementation can be found in the code repository.

\section{Mean versus median in timing measurements}

For each timing experiment ten experiments are run and either the mean or median is taken. The median timing results for the MPEU model after running one hundred thousand steps is seen in Fig. \ref{fig:appendix_profiling_100k_gpu_mpeu_median}. The mean results for the same experiments are shown in \ref{fig:appendix_profiling_100k_gpu_mpeu_mean}. The difference in the two figures shows the importance of the number of recompilations in the gradient-update method. This is because the mean is affected by outlier measurements, such as recompilations, while the median is not. The fact that for one hundred thousand training steps, the static-64 algorithm has the best median performance but not the best mean performance is due to the number of recompilations which is highest for this algorithm.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/appendix/profiling_100k_MPEU_gpu_a100_aflow_left_qm9_right_median.png}
\caption{Median batching (top) and median gradient-update times (middle), and median combined time (bottom) for varying batch sizes on the AFLOW (left) and QM9 (right) data using the MPEU model. For each datapoint, ten iterations of one hundred training steps are run.}
\label{fig:appendix_profiling_100k_gpu_mpeu_median}
\end{figure}


\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/appendix/profiling_100k_MPEU_gpu_a100_aflow_left_qm9_right_mean.png}
\caption{Mean batching (top) and mean gradient-update times (middle), and mean combined time (bottom) for varying batch sizes on the AFLOW (left) and QM9 (right) data using the MPEU model.  For each datapoint, ten iterations of one hundred training steps are run.}
\label{fig:appendix_profiling_100k_gpu_mpeu_mean}
\end{figure}


The mean and median timing results for two million steps are shown for the MPEU model in Fig. \ref{fig:appendix_profiling_2mill_gpu_mpeu_mean} and in Fig. \ref{fig:appendix_profiling_2mill_gpu_mpeu_median} respectively. We see the mean static-64 gradient-update step times are shifted higher than the median results, showing that the recompilations still affect the mean results. However, the rankings of the mean combined times are the same for the  median combined times across algorithms, suggesting for longer training time, in this case two million steps, the effect of recompilations is no longer as significant as we saw earlier for one hundred thousand steps.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/appendix/profiling_2mill_MPEU_gpu_aflow_left_qm9_right_mean.png}
\caption{Mean batching time (upper row), mean gradient-update time (middle row), and the mean combined time (bottom row) for varying batch sizes on the AFLOW (left) and QM9 (right) data using the MPEU model. For each datapoint, ten iterations of two million training steps are run.}
\label{fig:appendix_profiling_2mill_gpu_mpeu_mean}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/appendix/profiling_2mill_MPEU_gpu_aflow_left_qm9_right_median.png}
\caption{Median batching time (upper row), median gradient-update time (middle row), and the median combined time (bottom row) for varying batch sizes on the AFLOW (left) and QM9 (right) data using the MPEU model. For each datapoint, ten iterations of two million training steps are run.}
\label{fig:appendix_profiling_2mill_gpu_mpeu_median}
\end{figure}

\section{CPU only timing results}

The algorithm performances are different when running only on CPU. The batching (inclusive of padding) performance is the same as when running on a system that has both a GPU and a CPU which indicates that the batching is executed only on CPU. We experimented with trying to run part of the batching and padding code on GPU using JAX commands but found no speedup. The gradient-update step, however, is much slower on CPU. The mean timing results, from ten iterations of one hundred thousand training steps, are shown in Fig. \ref{fig:profiling_100k_cpu_mpeu_mean} for the MPEU model. The median is shown in Fig. \ref{fig:profiling_100k_cpu_mpeu_median} for the same model. We ran one hundred thousand training steps instead of two million training steps since the computer cluster we used had a time limit of twelve hours for experiments. From the mean results, we see the dynamic batching algorithm is fastest. The median results, however, show that when the effect of recompilations are reduced, the static-64 algorithm is the fastest except for batch size 128 on the AFLOW dataset. This suggests that for longer training times the static-64 algorithm will be the fastest. The results for the SchNet model can be seen in the accompanying code repository.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/appendix/profiling_100k_MPEU_cpu_aflow_left_qm9_right_mean.png}
\caption{Mean batching time (upper row), mean gradient-update time (middle row), and the mean combined time (bottom row) on CPU for varying batch sizes on the AFLOW (left) and QM9 (right) data using the MPEU model. The experiments were run for one hundred thousand training steps.}
\label{fig:profiling_100k_cpu_mpeu_mean}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/appendix/profiling_100k_MPEU_cpu_aflow_left_qm9_right_median.png}
\caption{Median batching time (upper row), median gradient-update time (middle row), and the median combined time (bottom row) on CPU for varying batch sizes on the AFLOW (left) and QM9 (right) data using the MPEU model. The experiments were run for one hundred thousand training steps.}
\label{fig:profiling_100k_cpu_mpeu_median}
\end{figure}

\section{Test metric results}

The test metric (RMSE) curves for each of the ten iterations are averaged for each batch size, model and dataset combination. The resulting mean curve is shown for the two models using the QM9 data in Fig. \ref{fig:appendix_test_curves_qm9}. We do not see significant differences in the test performance for the static-$2^N$ or dynamic batching algorithm. Note that we do not expect nor do we see any noticeable difference in learning from the static-64 to static-$2^N$ algorithms since the difference between the methods is the padding scheme which does not affect the loss. Therefore, the static-64 curve is left out of Fig. \ref{fig:appendix_test_curves_qm9} for visual clarity. The results for AFLOW can be seen in the accompanying code repository.
%
\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/appendix/test_curves_qm9_gpu.png}
\caption{Mean test RMSE curves for the MPEU model (left) and SchNet (right) on QM9 test for batch sizes of 16, 32, 64 and 128 (from top to bottom).}
\label{fig:appendix_test_curves_qm9}
\end{figure}
%

\section{Static-constant batching} \label{sec:appendix_static_consant}

The TF-GNN library tutorial suggests running the static batching algorithm with a constant padding target. As described in Section \ref{sec:preliminaries}, the algorithm first iterates through all the graphs in the dataset and saves the maximum number of graphs/edges seen in a graph and uses this as a padding target. We evaluated this method on a subset of batch sizes, dataset and model combinations, and compared it to the static-$2^N$, static-64 and dynamic algorithms. The results are shown in Table \ref{tab:appendix_static_constant}. Our results show that the static-constant algorithm performs poorly in comparison to the other algorithms, and as a result it is not used in the main text.

\begin{table*}
   \caption{Static-constant timing results compared with the batching algorithms. }\label{tab:appendix_static_constant}
  \centering
    \begin{tabular}{lrrrrrr}\toprule
        Algorithm & Dataset & Model &  Batch size  & Batch time (ms) & Update time (ms) & Combined time (ms) \\\midrule
        static-constant    & AFLOW & MPEU & 16 & 0.63 & 6.34 & 7.30 \\
        static-$2^N$    & AFLOW & MPEU & 16 & 0.49 & 1.89 & 2.38 \\
        static-64    & AFLOW & MPEU & 16 & 0.49 & 1.89 & 2.38 \\    
        % dynamic    & AFLOW & MPEU & 16 & 0.49 & 1.89 & 2.38 \\    
        static-constant    & AFLOW & SchNet & 32 & 0.99 & 6.34 & 7.33 \\
        static-2$^N$ & AFLOW & SchNet & 32 & 0.71 & 1.38 & 2.01 \\
        static-64 & AFLOW & SchNet & 32 & 0.71 & 1.64 & 2.35 \\
        dynamic & AFLOW & SchNet & 32 & 0.87 & 1.30 & 2.17 \\
        static-constant    & QM9 & SchNet & 32 & 0.78 & 2.02 & 2.80 \\
        static-2$^N$ & QM9 & SchNet & 32 & 0.74 & 1.55 & 2.29 \\
        static-64 & QM9 & SchNet & 32 & 0.77 & 1.38 & 2.36 \\
        dynamic & QM9 & SchNet & 32 & 1.03 & 1.38 & 2.41 \\
        static-constant    & QM9 & SchNet & 128 & 2.31 & 5.19 & 7.50 \\
        static-$2^N$    & QM9 & SchNet & 128 & 2.20 & 3.88 & 6.08 \\
        static-$64$    & QM9 & SchNet & 128 & 2.24 & 2.39 & 4.63 \\
        dynamic   & QM9 & SchNet & 128 & 3.38 & 2.13 & 5.51 \\
        \\\bottomrule
    \end{tabular}
\end{table*} 

\section{Node-level target discussion}

Both datasets evaluated in this paper contain graph-level targets, but not node-level targets. The batching step of each algorithm does not change for a node-level target. The gradient step should also not change significantly. Graph-level targets typically feed their node feature vectors into readout functions that sum over the nodes in the graph. For node-level targets, on the other hand, the batch loss is usually computed as the sum of the losses of the node-level targets. Therefore, both target types typically contain a sum over nodes, and we expect the gradient update steps profiling results to remain similar for node-level classification. That said, the learning curves could change for node-level targets, since the true nodes that exist in a dynamic batch are sampled from a roughly truncated Gaussian (as was seen in Figure \ref{fig:batch_stats_batch_32}), whereas the static batch has a Gaussian distribution. This could have an impact on how the model learns, since batches will contain significantly different numbers of targets on average.

\section{Effect on rankings of the number of training steps}

The effect of the number of training steps run on the rankings of the fastest algorithms is depicted in Fig. \ref{fig:time_series_batch_size_32}. We can clearly see that for a smaller number of training steps, the dynamic batching algorithm is faster than the static-64 algorithm which is due to the number of recompilations that happen during the gradient-update step. For more training steps, this effect is subdued and the static-64 algorithm is faster.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{figures/appendix/time_series_batch_size_32_qm9_MPEU.png}
\caption{Running average combined time (gradient-update step and batching) as a function of the total number of training steps run. Here only a single iteration is run for the batch size 32, MPEU model and QM9 dataset for both the dynamic, static-64 and static-$2^N$ algorithms.}
\label{fig:time_series_batch_size_32}
\end{figure}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.