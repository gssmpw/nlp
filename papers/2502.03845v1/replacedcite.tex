\section{Related Works}
\label{S_2}

Multi-agent systems are widespread in life and lie at the intersection of game theory and artificial intelligence in general, which receive much attention from researchers. Communication is essential for MARL to capture inter-agent action dependencies and is proven to enhance exploration and team rewards ____. Agents need to enhance action coordination by learning to communicate with other agents and process the message representations they receive. To address the distributed control and non-stationarity challenges in MARL, numerous methods have achieved notable progress in recent years. Our work builds upon prior research on MARL and communication mechanisms. 


\subsection{MARL without Communication}

Existing MARL approaches can be divided into two categories based on the presence or absence of a communication mechanism. MARL without communication utilizes the CTDE paradigm to tackle multi-agent cooperation problems. This approach has been successfully implemented with both policy-based and value-based algorithms. CTDE allows information sharing during training while ensuring that policies are conditioned solely on the agents' local observations, thus enabling decentralized execution. Various CTDE algorithms have been developed, each offering unique advantages and addressing specific challenges in MARL.

Policy-based MARL methods include centralized policy gradient algorithms where each agent comprises a decentralized actor and a centralized critic. A notable example is multi-agent deep deterministic policy gradient (MADDPG) ____. MADDPG extends the deep deterministic policy gradient (DDPG) ____ algorithm for MARL by conditioning the actor on the history of local observations and training the critic on joint observations and actions to approximate the joint state-action value function. Another significant policy-based method is the counterfactual multi-agent (COMA) ____ policy gradient proposed, which modifies the advantage function in the actorâ€™s loss computation to perform counterfactual reasoning for credit assignment in cooperative MARL. Multi-agent advantage actor-critic (MAA2C) and multi-agent proximal policy optimization (MAPPO) ____ are additional examples, with MAPPO offering enhanced learning efficiency by performing several update epochs pertraining batch.

Value-based MARL methods focus on decomposing the joint state-action value function into individual state-action value functions, adhering to the individual-global-max (IGM) principle. Value decomposition networks (VDN) ____, aim to learn a linear decomposition of the joint Q-value, with each agent maintaining a network to approximate its own state-action values. This method ensures the sufficient condition for the IGM principle and becomes prevalent in MARL due to its simplicity and scalability, inspiring many subsequent approaches. Extending VDN, QMIX ____ introduces a monotonic mixing network to enhance the expressiveness of the decomposed function class, ensuring that the optimal joint action maximizes the joint Q-value, aligning with the individual Q-values of each agent. However, QTRAN ____ aims to represent the entire IGM function class but faces computational intractability, requiring additional soft regularizers and not guaranteeing strict IGM consistency. QPLEX ____ extends the IGM principle into the dueling network architecture, although it has potential limitations in scalability. These value decomposition methods illustrate the flexibility and adaptability of value-based CTDE approaches.

Although these algorithms have shown significant performance in many multi-agent cooperative tasks, their effectiveness relies heavily on the introduction of global state and the setup of centralized controllers. Unlike existing works, our algorithm through communication among agents, reliance on global state is eliminated, and the trainer is set within the agents themselves.


\subsection{MARL with Communication}

MARL with communication methods aim to achieve consensus and cooperation among multiple agents through learning effective communication strategies. Methods incorporating communication in MARL have significantly evolved from early approaches that relied on fixed, static broadcast mechanisms. Initial methods like reinforced inter-agent learning and differentiable inter-agent learning (RIAL \& DIAL) ____ established foundational concepts in learning communication protocols among agents. Communication neural net (CommNet) ____ further advanced communication learning by enabling agents to learn to broadcast messages. To alleviate the local policy burden caused by message flooding, methods like individualized controlled continuous communication model (IC3Net) ____ and attentional communication model (ATOC) ____ introduced gating mechanisms to selectively target communication recipients. Despite these advancements, these mechanisms struggled with modeling directed communication, leading researchers to explore directed graph structures to specify communication targets more effectively. Approaches like graph convolutional reinforcement learning (DGN) ____ and game abstraction mechanism based on two-stage attention network (G2ANet) ____ have shown promise in enhancing cooperation in dynamic and large-scale environments, respectively. However, none of these methods can precisely model the content of agent communication, as they primarily focus on selecting communication targets without considering the specific information being communicated. 

Recent research has made some progress in improving communication efficiency and mitigating the difficulty of policy learning. The first category focuses on generating meaningful messages for the message senders. A straightforward approach in this category is to treat raw local observations or the local information history as messages. For instance, Targeted multi-agent communication (TarMAC) ____ achieves targeted communication through a soft-attention mechanism, where the sender broadcasts a key encoding the agents' properties, and the receiver processes all received messages for a weighted sum to make decisions. Nearly decomposable Q-functions (NDQ) ____ aims to generate minimal messages for different teammates, allowing them to learn decomposable value functions. NDQ optimizes the message generator using two information-theoretic regularizers to ensure expressive communication. The second category of work focuses on efficiently extracting the most useful messages at the receiver's end. An example is multi-agent communication via self-supervised information aggregation (MASIA) ____, which explicitly addresses the optimization of multiple received messages by introducing two self-supervised representation objectives. Multi-agent communication mechanism with Graph Information bottleneck optimization (MAGI) ____ introduces a robust communication learning mechanism, using graph information bottleneck optimization and information-theoretic regularizers to enhance the robustness and efficiency of multi-agent communication and coordination. These objectives aim to ensure that the received information representation abstracts the true states and predicts multi-step future information. These methods highlight the need for balancing the generation and reception of meaningful messages to facilitate effective communication and coordination among agents.

Despite significant advancements, a common limitation in current MARL communication methods is their reliance on sparse reinforcement learning rewards for training both communication and policy networks. This joint training approach often complicates local policies by introducing raw communication data, which increases the complexity of the learning process. Unlike existing methods, our approach focuses on information-level modeling, facilitating effective content extraction and selective integration of agents' local observations through communication for joint policy updates. Additionally, the use of pluggable modules improves computational efficiency, reduces policy network complexity, and enhances algorithm flexibility.


\subsection{Generative Adversarial Network}

Generative adversarial network (GAN) technology has significantly impacted artificial intelligence, especially in computer vision, by enabling the creation of highly realistic synthetic data. Introduced in 2014, GAN ____ consist of a generator and a discriminator network that compete to improve their functions, resulting in data that closely mimics real-world samples. This innovative framework has led to significant progress in applications like image generation, enhancement, and inpainting. One of the earliest and most impactful improvements was the development of deep convolutional GAN (DCGAN) ____, which replaced fully connected layers with convolutional layers. This change enhanced both the stability and quality of generated images, setting a new standard for GAN architectures. Building on this, the introduction of super-resolution GAN (SRGAN) ____ utilized a perceptual loss function based on high-level feature maps from pretrained networks. SRGAN enabled the generation of photo-realistic high-resolution images from low-resolution inputs, significantly improving visual quality over traditional methods. Further advancements are made with progressive growing of GAN (ProGAN) ____, a technique that gradually increased image resolution during training. Starting with low-resolution images and adding layers progressively led to more stable training, addressing issues like mode collapse and training instability. Additionally, the proposal of wasserstein GAN (WGAN) ____ introduces the wasserstein distance as a loss function, providing more meaningful gradients and improving training stability. Recent innovations have also tackled high-resolution image inpainting ____. A multi-scale neural patch synthesis approach combined deep convolutional networks for structural prediction with patch-based synthesis for texture generation, achieving coherent and sharp inpainting results, especially for high-resolution images. Additionally, the introduction of CycleGAN ____ enables unpaired image-to-image translation by enforcing cycle consistency, broadening GAN's applicability in tasks like style transfer and domain adaptation. 

Overall, GAN has significantly advanced computer vision, particularly in generating high-fidelity visual content and enhancing image quality. However, the application of GAN in reinforcement learning remains underexplored. In our research, we find that in a MARL setting based on communication, GAN can effectively complement information. This means that through communication, agents can use GAN to combine received information with their observations, achieving global state information completion and addressing the issue of partial observability.