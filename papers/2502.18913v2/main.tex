% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% zjm add
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{CJKutf8}
\usepackage{ragged2e}
\usepackage{CJKutf8}
\usepackage{multicol}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{CS-Dialogue: A 104-Hour Dataset of Spontaneous Mandarin-English Code-Switching Dialogues for Speech Recognition}
% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\author{
 \textbf{Jiaming Zhou\textsuperscript{1}}, \textbf{Yujie Guo\textsuperscript{1}},  \textbf{Shiwan Zhao\textsuperscript{1}}, \textbf{Haoqin Sun\textsuperscript{1}}, \textbf{Hui Wang\textsuperscript{1}}, \textbf{Jiabei He\textsuperscript{1}},
 \\
   \textbf{Aobo Kong\textsuperscript{1}}, \textbf{Shiyao Wang\textsuperscript{1}},  \textbf{Xi Yang\textsuperscript{2}},  \textbf{Yequan Wang\textsuperscript{2}}, \textbf{Yonghua Lin\textsuperscript{2}}, \textbf{Yong Qin\textsuperscript{1}}\thanks{Yong Qin is the corresponding author.} 
\\
\\
     \textsuperscript{1} College of Computer Science, Nankai University, \\
     \textsuperscript{2} Beijing Academy of Artificial Intelligence, Beijing, China,
\\
 \small{
   \textbf{Correspondence:} \href{mailto:zhoujiaming@mail.nankai.edu.cn}{zhoujiaming@mail.nankai.edu.cn}, \href{mailto:qinyong@nankai.edu.cn}{qinyong@nankai.edu.cn}
 }
}

\begin{document}
\maketitle
\begin{abstract}

Code-switching (CS), the alternation between two or more languages within a single conversation, presents significant challenges for automatic speech recognition (ASR) systems. Existing Mandarin-English code-switching datasets often suffer from limitations in size, spontaneity, and the lack of full-length dialogue recordings with transcriptions, hindering the development of robust ASR models for real-world conversational scenarios.  This paper introduces CS-Dialogue, a novel large-scale Mandarin-English code-switching speech dataset comprising 104 hours of spontaneous conversations from 200 speakers.  Unlike previous datasets, CS-Dialogue provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching patterns in continuous speech.  We describe the data collection and annotation processes, present detailed statistics of the dataset, and establish benchmark ASR performance using state-of-the-art models. Our experiments, using Transformer, Conformer, and Branchformer, demonstrate the challenges of code-switching ASR, and show that existing pre-trained models such as Whisper still have the space to improve. The CS-Dialogue dataset will be made freely available for all academic purposes.

\end{abstract}


\section{Introduction}
Code-switching (CS) refers to the practice of alternating between two or more languages within a single conversation or utterance~\cite{moyer2002bilingual}. It is a common linguistic phenomenon in multilingual communities and occurs in various communication settings, including spoken dialogues, social media, and written texts. The increasing prevalence of code-switching presents significant challenges for automatic speech recognition (ASR) systems, as they must effectively handle complex acoustic and linguistic variations across different languages~\cite{yilmaz2018acoustic}.

Traditional ASR systems, predominantly trained on monolingual data, struggle with code-switched speech due to mismatches in phonetic inventories, syntactic structures, and language switching patterns~\cite{mustafa2022code,zhou2024improving}. These challenges necessitate the development of specialized ASR models and high-quality datasets tailored for code-switching scenarios. Despite recent advancements, existing Mandarin-English code-switching speech corpora remain limited in size, spontaneity, and accessibility, restricting further research and model development.

Table~\ref{tab:datasets} provides an overview of publicly available Mandarin-English code-switching datasets. Many existing corpora~\cite{shen2011cecos,wang2016oc16,TALCS} focus on read speech or constrained domains, lack full transcriptions or are not publicly accessible. Crucially, most datasets comprise isolated code-switching utterances rather than full dialogues, limiting their utility for studying naturalistic speech patterns and contextual dependencies~\cite{10094707}.


To address these gaps, we introduce \textbf{CS-Dialogue}, a novel large-scale Mandarin-English code-switching speech dataset consisting of 104 hours of spontaneous conversations from 200 speakers. Unlike prior work, our dataset provides full-length dialogue recordings with complete transcriptions, capturing naturalistic code-switching phenomena in continuous speech. This dataset enables more comprehensive investigations into code-switching ASR beyond isolated utterances. 
CS-Dialogue is, to the best of our knowledge, the largest publicly available dataset of spontaneous Mandarin-English code-switching dialogues with full transcriptions.


\begin{table*}[!t]
\centering
% \scriptsize
\small
\begin{tabular}{lccccc|c}
\toprule
Dataset & Duration (h) & \#Speakers & Audio Type & Tr. & Avail. & Full-dialogue  \\
\midrule
CECOS~\cite{shen2011cecos}    & 12.1  & 77    & Read           & No  & No         & No \\
OC16-CE80~\cite{wang2016oc16}   & 80    & 1400+ & Read           & Yes & No         & No \\
ASRU~\cite{shi2020asru}       & 240   & N/A      & N/A              & Yes & No         & No \\
TALCS~\cite{TALCS}          & 587   & 100+  & Online Teaching & Yes & Yes        & No \\
DOTA-ME-CS~\cite{li2025dota} & 18.54 & 34    & Read    & Yes & Yes        & No \\
\midrule
SEAME~\cite{SEAME}         & 30    & 157   & Conversation    & Yes & Paid  & No \\
Li et al.~\cite{li2012mandarin} & 36    & N/A      & Conversation    & Partial & No         & No \\
ASCEND~\cite{ASCEND}       & 10.62 & 23    & Conversation    & Yes & Yes        & No \\
Ours                        & 104.02   & 200   & Conversation    & Yes & Yes        & Yes \\
\bottomrule
\end{tabular}
\caption{Comparison of Mandarin-English code-switching speech datasets. "Tr." indicates whether transcripts are available, "Avail." specifies whether the dataset is publicly accessible, and "Full-dialogue" denotes whether full-length dialogue recordings and transcriptions are provided.}
\label{tab:datasets}
\end{table*}


In this paper, we describe the data collection and annotation processes, present key characteristics of the dataset, and evaluate its impact on ASR performance through baseline experiments. Our contributions are summarized as follows:
\begin{itemize}
    \item We construct a large-scale, spontaneous Mandarin-English code-switching speech corpus with full-length dialogue transcriptions, filling the gap of publicly available datasets in this domain.
    \item We detail the data collection and annotation processes, ensuring high transcription accuracy and providing a well-documented resource for future research.
    \item We establish benchmark ASR performance on our dataset using state-of-the-art models, offering insights into the challenges of code-switching ASR.
\end{itemize}




\section{Related Work}

Existing Mandarin-English CS speech datasets can be broadly categorized into read speech and spontaneous speech corpora. Read speech datasets typically contain pre-defined sentences that participants are instructed to read aloud, offering controlled phonetic and linguistic variations but lacking the spontaneity of natural conversations.  

The CECOS dataset~\cite{shen2011cecos} is one of the earliest Mandarin-English CS corpora, comprising 12.1 hours of read speech from 77 speakers at National Cheng Kung University in Taiwan. While it includes code-switching utterances, it lacks publicly available transcriptions and contains non-native speaker accents. OC16-CE80~\cite{wang2016oc16} significantly expands the scale, offering 80 hours of read speech from over 1400 speakers, with transcriptions available but not open-sourced. The ASRU dataset~\cite{shi2020asru}, developed for an ASR challenge, contains 240 hours of predominantly Mandarin speech interspersed with some English. Although transcriptions exist, the dataset is not publicly accessible.  

More recent datasets, such as DOTA-ME-CS~\cite{li2025dota}, offer open-source transcriptions and introduce AI-based augmentation techniques (e.g., timbre synthesis, speed variation, and noise addition) to enhance diversity. However, its scale remains relatively small, with only 18.54 hours from 34 speakers. TALCS~\cite{TALCS} provides a much larger dataset, comprising 587 hours of speech from online teaching scenarios. While it is open-source and valuable for acoustic modeling, its domain-specific nature introduces biases in discourse structure, grammar, and lexical choices, making it less representative of everyday spontaneous conversations.  

Spontaneous CS datasets, in contrast, are essential for modeling real-world language use but present greater challenges in collection and annotation. SEAME~\cite{SEAME} provides approximately 30 hours of spontaneous Mandarin-English conversations from 92 speakers in Singapore and Malaysia. It includes word-level transcriptions with time-aligned language boundaries, making it a valuable resource for code-switching research. However, the dataset is not freely accessible and requires a purchase, which may limit its availability for broader research applications.
\citet{li2012mandarin} compiled 36 hours of spontaneous CS speech across various settings, including conversational meetings and student interviews, but only part-of-speech data is transcribed, limiting its usability for ASR research. ASCEND~\cite{ASCEND} provides a smaller (10.62 hours) yet fully transcribed and open-source dataset of spontaneous, multi-turn CS conversations recorded in Hong Kong, featuring 23 bilingual speakers.  

Despite these advancements, most existing datasets exhibit limitations in scale, availability, or annotation completeness. Many either focus on isolated code-switching utterances rather than full dialogues, or remain inaccessible to the research community. In contrast, our dataset aims to bridge these gaps by providing 104 hours of spontaneous Mandarin-English CS speech, featuring full-length dialogue recordings with comprehensive transcriptions. It captures naturalistic code-switching patterns within extended conversations, making it a valuable resource for both ASR research and broader linguistic analysis.  



\section{Dataset creation}



\subsection{Data Acquisition}



\subsubsection{Speaker Selection}

All speakers were native Chinese citizens with demonstrated fluency in English.  Selection criteria prioritized individuals with significant exposure to English-speaking environments, such as overseas experience or high scores on standardized English proficiency tests (e.g., IELTS 6 or TEM-4). Prospective speakers underwent an audition to ensure adequate speech quality and language proficiency before being included in the recording sessions.

\subsubsection{Ethical Considerations and Compensation}

Prior to participation, all speakers provided informed consent, granting permission for the collection, processing, and potential sharing of their data, including with parties located outside of China. The consent process adhered to ethical guidelines and ensured participants were fully aware of the data's intended use. Each speaker received financial compensation of 300 RMB (approximately 50 USD ) for their contribution to the dataset.

\subsubsection{Topic Selection}

The dataset incorporates seven prevalent topics of daily relevance: personal topics, entertainment, technology, education, job, philosophy, and sports. A detailed overview of these topics could be found in Appendix~\ref{appendix: Topics, Descriptions, and Examples}. To ensure comprehensive coverage, a minimum of 15 distinct speaker pairs engaged in discussions for each topic. Individual speaker pairs selected between two and six topics based on their personal interests, aiming to foster natural and engaging conversations.



\subsubsection{Dialogue Recording Procedure}

The data collection process employed paired dialogues, with each participant equipped with a smartphone microphone to capture individual audio streams in a quiet environment.
A timekeeper facilitated each session, ensuring adherence to the established recording protocol. Each dialogue commenced with brief introductory remarks, transitioning into discussions centered on the pre-selected topics. The linguistic composition of the dialogue progressed systematically: initially in Mandarin Chinese, followed by a period of code-switching between Chinese and English, and concluding with exclusive use of English. Each topic segment was designed to last approximately 20 minutes, with a target allocation of 8 minutes for Chinese, 6 minutes for code-switching, and 6 minutes for English. While the timekeeper provided prompts to maintain the intended schedule, natural variations in pacing were permitted to encourage spontaneous and authentic communication.  A dedicated observer monitored each session, verifying procedural compliance and recording relevant metadata for subsequent analysis.
The entire procedure took approximately 1.5 hours.  All audio files in the dataset are stored in a 16 kHz, 16-bit, mono, PCM WAV format.
The recording structure was as follows:

\begin{enumerate}
    \item Introductory Greetings and Self-Introductions (1 minute).
    \item Topic-Specific Discussion (approximately 20 minutes per topic) segmented into: Mandarin Chinese (8 minutes), code-switching (6 minutes) and English (6 minutes).
    \item Transition to the Next Selected Topic and Repeat Step 2 until all Selected Topics have been Covered.
\end{enumerate}



\subsection{Annotation}


To ensure data quality and facilitate downstream tasks, all audio files underwent a rigorous annotation procedure. This process encompassed manual transcription, detailed annotation of non-lexical events, and stringent quality control measures.  All annotations were performed by a dedicated in-house team following a predefined protocol. An example of a dialogue transcription is provided in Appendix~\ref{appendix_dialogue_case}.






\subsubsection{Transcription Protocol}

The transcription process prioritized accurate representation of the spoken content, focusing on the speaker's actual pronunciation. The following guidelines were implemented to maintain consistency and ensure high transcription quality:



\begin{table}[!t]
\centering
\small
\begin{tabular}{lp{6cm}}
\toprule
Symbol & Definition \\
\midrule
\texttt{**} & Indicates unintelligible words or phrases. \\
\texttt{<FIL/>} & Filled pauses resulting from hesitation. \\
\texttt{<SPK/>} & Speaker-related noises, such as lip smacking, laughter, coughing, or throat clearing. \\
\texttt{<NON/>} & Non-speech noises, such as door slams, knocks, or ringing sounds. \\
\texttt{<NPS/>} & Noises made by individuals other than the designated speakers, including speech or noise. \\
\bottomrule
\end{tabular}
\caption{Annotation Symbols and Definitions}
\label{tab:annotation_symbols}
\end{table}


\begin{enumerate}
    \item \textbf{Word Count Fidelity:} Transcriptions were required to maintain a precise word-for-word correspondence with the spoken utterance, preventing both omissions and additions.
    \item \textbf{Treatment of Disfluencies:} Clear repetitions of sounds or words were transcribed verbatim (e.g., "\begin{CJK}{UTF8}{gbsn}放放假\end{CJK}" transcribed as "\begin{CJK}{UTF8}{gbsn}放放假\end{CJK}"). Partially articulated syllables were transcribed using the most appropriate homophone (e.g., "\begin{CJK}{UTF8}{gbsn}放假\end{CJK}" pronounced as "fu-fang4-jia4" transcribed as "\begin{CJK}{UTF8}{gbsn}夫放假\end{CJK}"). Epenthetic or extremely faint sounds were disregarded.
    \item \textbf{Numerical Representation:} Arabic numerals were converted to their corresponding Chinese characters or English words, depending on the context and pronunciation (e.g., "711" transcribed as "\begin{CJK}{UTF8}{gbsn}七幺幺\end{CJK}" or "Seven Eleven").
    \item \textbf{Accent Accommodation:} Regional accents and variations in pronunciation (e.g., distinctions between retroflex and non-retroflex consonants, nasal finals, or the pronunciation of /h/ and /f/ or /l/ and /n/) were preserved in the transcription without correction.
    \item \textbf{Punctuation Conventions:} Punctuation marks, including both Chinese and English symbols, were applied according to standard grammatical conventions and semantic context to ensure clarity and accurate segmentation.
    \item  \textbf{Spelling conventions:} Spelling followed common English conventions and standards to ensure quality of annotations.
    \item \textbf{Acronym Representation:} Acronyms were transcribed using uppercase letters separated by spaces (e.g., "I B M"). Utterances consisting of three or fewer letters transcribed as an acronym were categorized as Chinese.
\end{enumerate}






\subsubsection{Non-Lexical Annotation}

In addition to the transcription of spoken words, a set of specialized symbols was used to annotate non-lexical events and acoustic phenomena. These symbols, detailed in Table \ref{tab:annotation_symbols}, provided additional information about the acoustic characteristics of the data.

\subsubsection{Quality Control}
Following the initial annotation, a separate quality control team performed a rigorous review process to ensure data accuracy. Discrepancies were resolved through discussion and iterative refinement of the annotation protocol, ensuring the high transcription quality.




\begin{table}[!t]
\centering
\small
\begin{tabular}{@{}l@{\hspace{12pt}}c@{}}
\toprule
Characteristic & Value \\
\midrule
Duration (hrs) & 104.02 \\
\# Speakers & 200 \\
\# Dialogues & 100 \\
\# Raw Recordings & 200 \\
\# Topic Sessions & 320 \\
\# Utterances & 38,917 \\
\# Avg (s) & 9.62 \\
\midrule
\multicolumn{2}{@{}l}{Speaking Rate (tokens/s)} \\
\midrule
\quad English & 1.72 \\
\quad Chinese & 3.45 \\
\quad Mixed & 2.99 \\
\bottomrule
\end{tabular}
\caption{Overview of our dataset}
\label{tab:overview}
\end{table}




\section{Dataset description}

This section provides a comprehensive overview of the CS-Dialogue dataset, including its profile, statistical analysis, and details on speaker demographics, duration, topic distribution, and textual characteristics.

\subsection{Profile}


The CS-Dialogue dataset comprises 104.02 hours of spontaneous Mandarin-English code-switching speech from 200 speakers, structured as 100 dialogues (200 raw recordings, as each dialogue involves two participants).  These dialogues encompass 320 topic sessions, offering a diverse range of conversational contexts. The dataset contains 38,917 utterances. Table~\ref{tab:overview} summarizes the key characteristics of the dataset, including the total duration, number of speakers, dialogues, utterances, and language distribution.  The average speaking rate is 1.72 tokens per second for English, 3.45 tokens per second for Chinese, and 2.99 tokens per second for mixed-language segments. All audio files in the dataset are stored in a 16 kHz, 16-bit, mono, uncompressed PCM format.

For model development and evaluation, the dataset is divided into three speaker-independent sets: training, development, and test. The breakdown of each split is presented in Table~\ref{tab:data_summary}. Critically, these splits are speaker-independent ensuring a robust evaluation of model generalization.

\begin{table}[!t]
\centering
\small
\begin{tabular}{ccccc}
\toprule
Split & \# Spk. & \# Utt. & Dur. (hrs) & Avg. (s) \\
\midrule
Train & 140     & 26,428  & 68.97      & 9.40     \\
Dev   & 30      & 6,196   & 18.30      & 10.63    \\
Test  & 30      & 6,293   & 16.74      & 9.58     \\
\midrule
Total & 200     & 38,917  & 104.02     & 9.62  \\
\bottomrule
\end{tabular}
\caption{Summary of data splits}
\label{tab:data_summary}
\end{table}


\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]
  {img/description/age_gender.pdf}
  \caption{Distribution of speakers by age and gender}
  \label{pic:age_gender}
\end{figure}

\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]
  {img/description/province.pdf}
  \caption{Distribution of speakers by region}
  \label{pic:province}
\end{figure}

\subsection{Statistics}

\subsubsection{Speaker Demographics}
The age and gender distribution of the speakers is illustrated in Figure~\ref{pic:age_gender}. Speaker ages range from 18 to 53, grouped into four-year intervals.  Male speakers are represented in green and female speakers in light green in the stacked bar chart.  A notable trend is the concentration of speakers in the younger age brackets (18-22 and 23-27), with a relatively balanced gender distribution.  The decrease in speaker numbers in older age groups may be attributed to the greater prevalence of Mandarin-English bilingualism among younger generations, or potential challenges in recruiting older participants with the required language proficiency. The data was collected from various regions in China.

The regional distribution of speakers, based on their reported origin, is displayed in Figure~\ref{pic:province}.  Sichuan has the highest representation, followed by Beijing and Guangdong, while the remaining regions have significantly fewer speakers.

\subsubsection{Duration analysis}
Utterance-level and speaker-level duration distributions are presented in Figure~\ref{pic:short_duration}.  Most utterances are under 30 seconds, and the majority of speakers have a total speaking time clustered towards the lower end of the range.  However, a few speakers contribute significantly more data, leading to a long-tailed distribution.

The training, development, and test sets exhibit a consistent proportional distribution of Chinese, English, and mixed-language durations, as shown in Figure~\ref{pic:cstype_duration}.  This balanced representation of each language category within each split ensures that models trained on one split are likely to generalize well to others.  Appendix~\ref{appendix: Full-dialogue duration distribution} details the distribution of full-dialogue durations.

\subsubsection{Dialogue topic analysis}
The dataset's conversation topics are distributed as shown in Table~\ref{tab:topic_freqency}.  Categorized into seven broad themes—Personal Topics, Entertainment, Technology, Education, Job, Philosophy, and Sports—the 320 topic sessions offer diverse conversational contexts (see Appendix~\ref{appendix: Topics, Descriptions, and Examples} for details). 
Personal Topics are the most frequent (24.38\%; 78/320 sessions), while Philosophy is the least frequent (4.69\%), indicating a focus on everyday conversational themes, along with a smaller, yet still significant, representation of more specialized topics.

The distribution of the seven conversation topics across each data split is detailed in Table~\ref{tab:topic_distribution}.  This table presents, for each topic, the total duration, its proportion of the entire dataset, the utterance count, total duration, and average utterance length for each split. While the topic distribution is relatively consistent across the three sets, some variation exists in average utterance lengths. Notably, "Philosophy" tends to have slightly longer utterances than other categories, particularly in the test set (12.95s).

\subsubsection{Text analysis}


An analysis of frequent strings (Appendix~\ref{appendix: top frequent strings}) reveals distinct patterns in language use and code-switching strategies.  Discourse markers like \begin{CJK}{UTF8}{gbsn}"我觉得"\end{CJK} (I think) and \begin{CJK}{UTF8}{gbsn}"比如说"\end{CJK} (for example) characterize the Chinese segments, while phrases like "A LOT OF" and "I THINK IT'S" are frequently used in the English segments.  A crucial observation in the mixed-language segments is the frequent use of function words from one language to frame content words from the other (e.g., "YOU KNOW \begin{CJK}{UTF8}{gbsn}就\end{CJK}"), suggesting that code-switching commonly occurs at clause or phrase boundaries.  Appendix~\ref{appendix:pos_tag} presents the distributions of part-of-speech (POS) tags.

\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]
  {img/description/short_duration.pdf}
  \caption{Utterance-level (left) and speaker-level (right) duration distributions.}
  \label{pic:short_duration}
\end{figure}



\begin{table}[]
\small
\centering

\begin{tabular}{lcc}
\toprule
Topic Name      & Frequency & Proportion \\
\midrule
Personal topics & 78        & 24.38\%        \\
Entertainment   & 60        & 18.75\%        \\
Technology      & 26        & 8.13\%         \\
Education       & 61        & 19.06\%        \\
Job             & 36        & 11.25\%        \\
Philosophy      & 15        & 4.69\%         \\
Sports          & 44        & 13.75\%        \\
Total           & 320       & 100.00\%      \\
\bottomrule
\end{tabular}
\caption{Topic distribution in the full-dialogue recordings}
\label{tab:topic_freqency}
\end{table}

\begin{table*}[!t]
\centering
\small
\begin{tabular}{@{}l@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{}}
\toprule
\multirow{2}{*}{Topic} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Dur. (hrs)\end{tabular}} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Proportion\end{tabular}} & \multicolumn{3}{c}{Train} & \multicolumn{3}{c}{Dev} & \multicolumn{3}{c}{Test} \\
\cmidrule(lr){4-6} \cmidrule(lr){7-9} \cmidrule(lr){10-12}
 &  &  & Count & \begin{tabular}[c]{@{}c@{}}Dur. (hrs)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. (s)\end{tabular} & Count & \begin{tabular}[c]{@{}c@{}}Dur. (hrs)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. (s)\end{tabular} & Count & \begin{tabular}[c]{@{}c@{}}Dur. (hrs)\end{tabular} & \begin{tabular}[c]{@{}c@{}}Avg. (s)\end{tabular} \\ \midrule
Personal   topics      & 21.53                       & 20.69\%                     & 5,865  & 14.59      & 8.95     & 1,348  & 3.55       & 9.48     & 1,487  & 3.39       & 8.22     \\
Education              & 19.20                        & 18.45\%                     & 4,853  & 13.34      & 9.9      & 1,014  & 3.11       & 11.04    & 931   & 2.75       & 10.63    \\
Entertainment          & 23.88                       & 22.95\%                     & 6,609  & 15.85      & 8.63     & 1,509  & 4.39       & 10.47    & 1,402  & 3.64       & 9.34     \\
Sports                 & 14.14                       & 13.59\%                     & 3,476  & 8.64       & 8.95     & 745   & 2.35       & 11.34    & 1,236  & 3.15       & 9.17     \\
Job                    & 11.94                       & 11.48\%                     & 2,555  & 7.37       & 10.38    & 1,052  & 3.02       & 10.35    & 513   & 1.55       & 10.9     \\
Technology             & 8.70                         & 8.36\%                      & 2,151  & 6.09       & 10.19    & 335   & 1.24       & 13.36    & 475   & 1.37       & 10.35    \\
Philosophy             & 4.65                        & 4.47\%                      & 919   & 3.11       & 12.17    & 193   & 0.64       & 11.92    & 249   & 0.9        & 12.95 \\
\bottomrule
\end{tabular}
\caption{Topic distribution across training, development, and test sets: counts, durations, and average utterance lengths}
\label{tab:topic_distribution}
\end{table*}









\begin{table*}[!t]
\centering
\small
\begin{tabular}{@{}l@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}c@{\hspace{6pt}}}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}\# Params\end{tabular}} & \multicolumn{3}{c}{Greedy} & \multicolumn{3}{c}{Beam} & \multicolumn{3}{c}{Attention} & \multicolumn{3}{c}{\begin{tabular}[c]{@{}c@{}}Attention\\ Rescoring\end{tabular}} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8} \cmidrule(lr){9-11} \cmidrule(lr){12-14}
                       &          & CER    & WER    & MER    & CER    & WER    & MER    & CER     & WER     & MER     & CER      & WER      & MER      \\
\midrule
Transformer            & 29M      & 22.56  & 45.34  & 27.21  & 22.24  & 45.19  & 27.01  & 39.23  & 62.80  & 44.05  & 21.60    & 43.44    & 26.06   \\
Branchformer           & 29M      & 18.86  & 39.16  & 23.01  & 18.78  & 39.20  & 22.95  & 44.06  & 60.90  & 47.50  & 18.29    & 37.55    & 22.23   \\
Conformer              & 31M      & 15.91  & 33.67  & 19.54  & 15.88  & 33.60  & 19.50  & 24.98  & 42.75  & 28.61  & 15.45    & 32.36    & 18.91   \\
\bottomrule
\end{tabular}
\caption{Performance of different models training from scratch under various decoding strategies. CER: Character Error Rate (\%); WER: Word Error Rate (\%); MER: Mixed Error Rate (\%).}
\label{tab:decoding_results}
\end{table*}








\section{Experiments}

This section presents our experimental evaluation of the CS-Dialogue dataset. We assess the performance of various ASR models, including those trained from scratch and pre-trained models, with and without fine-tuning on our data.

\subsection{Metrics}
ASR performance on the code-switching dataset is evaluated using three metrics: Mixture Error Rate (MER), Word Error Rate (WER), and Character Error Rate (CER).  Following \cite{shi2020asru}, MER is adopted as the primary metric due to its holistic assessment of ASR accuracy, calculating the edit distance considering both Chinese characters and English words.  In addition to MER, WER and CER are calculated separately for English and Chinese segments to provide more granular insights into per-language performance.

\begin{figure}[!t]
  \centering
  \includegraphics[width=1.0\linewidth]
  {img/description/cstype_duration.pdf}
  \caption{Duration of each language per data split}
  \label{pic:cstype_duration}
\end{figure}


\subsection{Baseline Models}
Two categories of baseline ASR models are evaluated: models trained from scratch on the CS-Dialogue dataset and models pre-trained on large external datasets.  Details regarding model training and hyperparameter configurations are provided in Appendix~\ref{Appendix_hyperparem}.

\subsubsection{Models Trained from Scratch}
Three ASR models are trained from scratch using the open-source WeNet toolkit \citep{wenet}:
\begin{itemize}
    \item \textbf{Transformer} \citep{transformer}:  An attention-based encoder-decoder (AED) model that uses self-attention to capture long-range dependencies.
    \item \textbf{Conformer} \citep{conformer}:  A model that combines convolution and self-attention (via an AED) to effectively model both local and global context.
    \item \textbf{Branchformer} \citep{peng2022branchformer}: A model that introduces a branching mechanism to enhance the modeling of different parts of the speech signal.
\end{itemize}

These models are trained exclusively on the training set of the CS-Dialogue dataset, without incorporating any external data.  Training employs both Connectionist Temporal Classification (CTC) \citep{ctc} and AED \citep{chorowski2014end} loss functions.

\subsubsection{Pre-trained Models}

Several state-of-the-art pre-trained models are also evaluated on the CS-Dialogue dataset:

\begin{itemize}
    \item \textbf{Whisper} \cite{radford2023robust}: A robust, multilingual Transformer-based ASR model pre-trained by OpenAI on 680,000 hours of diverse speech data~\footnote{\url{https://github.com/openai/whisper}} .   
    \item \textbf{Qwen2-Audio} \cite{chu2024qwen2}:  A large-scale audio-language model from Alibaba~\footnote{\url{https://github.com/QwenLM/Qwen2-Audio}} , capable of processing various audio inputs and performing tasks like audio analysis and speech-instruction following.
    \item \textbf{SenseVoice-Small} \cite{an2024funaudiollm}:  A non-autoregressive, encoder-only speech foundation model from Alibaba designed for multilingual, multi-style ASR and other speech understanding tasks~\footnote{\url{https://github.com/FunAudioLLM/SenseVoice}}.
    \item \textbf{FunASR-Paraformer} \cite{gao22b_interspeech}:  A fast and accurate non-autoregressive (NAR) end-to-end ASR model~\footnote{\url{https://github.com/modelscope/FunASR}}.
\end{itemize}


\subsection{Result Analysis}


\begin{table*}[!t]
\small
\begin{tabular}{cccccccc}
\toprule
\multirow{2}{*}{Model} & \multirow{2}{*}{\# Param} & \multirow{2}{*}{CER (\%)} & \multirow{2}{*}{WER (\%)} & \multicolumn{4}{c}{MER (\%)} \\
\cmidrule(lr){5-8}
                       &                           &                           &                           & S     & D     & I     & Overall \\
\midrule
Whisper Large-V2               & 1,550M                    & 10.7                      & 31.11                     & 6.00  & 7.60  & 1.69  & 15.29   \\
Qwen2-Audio            &   8.2B                 & 7.15                      & 19.82                     & 4.32  & 1.82  & 3.62  & 9.76    \\
Paraformer             &  220M                       & 3.70                      & 32.02                     & 6.30  & 0.98  & 2.37  & 9.65    \\
SenseVoice-Small       &  234M                         & 4.42                      & 15.57                     & 3.44  & 1.42  & 1.85  & 6.71    \\
\bottomrule
\end{tabular}
\centering
\caption{Performance comparison of different ASR models on a Mandarin-English code-switching ASR task. CER: Character Error Rate; WER: Word Error Rate; MER: Mixed Error Rate; S: Substitution; D: Deletion; I: Insertion.}
\label{tab:asr_results}
\end{table*}



\begin{table*}[!t]
\centering
\small
\begin{tabular}{cccccccc}
\toprule
\multirow{2}{*}{Whisper} & \multirow{2}{*}{\# Param} & \multicolumn{3}{c}{Zero-shot}  & \multicolumn{3}{c}{Fine-tuning} \\
\cmidrule(lr){3-5} \cmidrule(lr){6-8}
                         &                           & CER (\%) & WER (\%) & MER (\%) & CER (\%)  & WER (\%) & MER (\%) \\
\midrule
Tiny                     & 38M                       & 27.83    & 41.69    & 31.11    & 19.24     & 29.64    & 21.38    \\
Base                     & 74M                       & 19.90    & 37.21    & 23.90    & 15.36     & 27.20    & 17.80    \\
Small                    & 244M                      & 12.82    & 30.81    & 16.76    & 7.51      & 16.09    & 9.26     \\
Medium                   & 769M                      & 11.34    & 32.57    & 15.88    & 6.12      & 13.02    & 7.53    \\
\bottomrule
\end{tabular}
\caption{Zero-shot and fine-tuning performance of different Whisper models on a Mandarin-English code-switching ASR task. CER: Character Error Rate; WER: Word Error Rate; MER: Mixed Error Rate.}
\label{tab:whisper_results}
\end{table*}


The performance comparison of models trained from scratch (Transformer, Branchformer, and Conformer) is presented in Table~\ref{tab:decoding_results}.  Across all decoding methods—greedy decoding, beam search, attention decoding, and attention rescoring, the Conformer consistently outperforms both the Transformer and Branchformer.  Attention rescoring yields the best performance for all models, resulting in the lowest CER, WER, and MER.  For instance, the Conformer achieves a CER of 15.45\%, a WER of 32.36\%, and an MER of 18.91\% with attention rescoring, a substantial improvement over the results obtained with greedy decoding (15.91\% CER, 33.67\% WER, 19.54\% MER).  While the Branchformer generally surpasses the Transformer in performance, it exhibits the highest error rate under the attention decoding strategy.


Table~\ref{tab:asr_results} compares the performance of the pre-trained models (Qwen2-Audio, Paraformer, SenseVoice-Small, and Whisper Large-V2) on the test set.  Paraformer achieves the lowest CER (3.70\%), while SenseVoice-Small obtains the lowest MER (6.71\%).  Whisper Large-V2, despite its large size and extensive pre-training, exhibits the highest error rates among the pre-trained models, particularly in WER (31.11\%) and MER (15.29\%). This highlights the challenges of code-switching ASR, even for models trained on vast amounts of data. Qwen2-Audio shows competitive performance, with 7.15\% CER, 19.82\% WER and 9.76\% MER. Further analysis reveals that all models exhibit a higher rate of substitution errors compared to deletion or insertion errors, as shown in the breakdown of MER into substitution (S), deletion (D), and insertion (I) errors.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=1.0\linewidth]
  {img/description/topic_wer.pdf}
  \caption{Comparison of MER for Whisper Large-V2, Qwen2-Audio, Paraformer, and SenseVoice-Small Across Different Conversation Topics}
  \label{pic:topic_wer}
\end{figure}

Figure~\ref{pic:topic_wer} illustrates the MER of the four pre-trained ASR models across the seven conversation topics.  Model performance varies considerably across topics. SenseVoice-Small consistently achieves the lowest MERs, indicating its superior performance on this task.  Comparing Qwen2-Audio and Paraformer reveals no consistent dominance of one model over the other; instead, their relative performance is topic-dependent. In addition, "Sports" and "Philosophy" tend to have higher MERs for all models, while "Job" and "Technology" generally exhibit lower MERs, suggesting varying levels of difficulty across topics.

The performance of different sized Whisper models, both with and without fine-tuning, is presented in Table~\ref{tab:whisper_results}.  Fine-tuning leads to significant improvements across all model sizes. The Medium model achieves the best performance after fine-tuning, with a CER of 6.12\%, a WER of 13.02\%, and an MER of 7.53\%.  The performance gain from fine-tuning is more pronounced for larger models, demonstrating the effectiveness of fine-tuning for improving performance in code-switching scenarios.

Beyond the quantitative results, a qualitative analysis of the Whisper Medium model's output is provided in Appendix~\ref{Appendix_case_study}. This analysis includes example transcriptions, comparing zero-shot and fine-tuned performance, and highlights common error types.



\section{Conclusion}
In this paper, we introduced CS-Dialogue, a new large-scale dataset for Mandarin-English code-switching speech recognition.  CS-Dialogue addresses several limitations of existing datasets by providing 104 hours of spontaneous, full-length dialogues from 200 speakers, with complete transcriptions. This rich dataset captures naturalistic code-switching patterns and enables more comprehensive investigations into the complexities of code-switching ASR. We detailed the rigorous data collection and annotation procedures, ensuring high-quality data for robust model training and evaluation.  Our baseline experiments with state-of-the-art ASR models (Transformer, Conformer, Branchformer, Whisper, and Qwen2-Audio) highlighted the challenges posed by code-switching, even for models pre-trained on large amounts of data. Fine-tuning significantly improves performance, underscoring the importance of specialized datasets like CS-Dialogue. The dataset, along with the baseline results, serves as a valuable benchmark for future research in code-switching ASR, dialogue modeling, and related areas of natural language processing. We expect that CS-Dialogue will facilitate the development of more robust and natural-sounding ASR systems capable of handling the complexities of multilingual communication.

\section*{Limitations}
While CS-Dialogue represents a significant contribution to the field, it has certain limitations.  First, the dataset focuses exclusively on Mandarin-English code-switching.  While this is a prevalent language pair, future work should expand to include other language combinations to enhance the generalizability of code-switching ASR models. Second, all participants are native Chinese speakers with strong English proficiency. The dataset does not include native English speakers who code-switch into Mandarin, which represents another important aspect of bilingual conversation. Third, although the dialogues are spontaneous, they are still recorded in a controlled environment, which may not fully reflect the acoustic diversity of real-world scenarios (e.g., noisy public spaces, varying microphone quality). Future work could explore data augmentation techniques to simulate a wider range of acoustic conditions.


\section*{Ethics Statement}
The collection and use of the CS-Dialogue dataset were conducted in accordance with established ethical guidelines and regulations for human subjects research. Prior to participation, all speakers were provided with a comprehensive information sheet detailing the study's purpose, data collection procedures, and their rights as participants, including the right to withdraw from the study at any time without penalty.  Informed consent was obtained from each speaker, explicitly authorizing the recording of their conversations, the processing and analysis of their speech data, and the potential sharing of anonymized data with other researchers (including those located outside of China) for research purposes.

Participants were assured that their data would be treated with strict confidentiality and anonymized to protect their privacy.  No personally identifiable information (e.g., names, specific locations) will be included in the released dataset or any associated publications.  Participants received compensation for their time and contribution to the study, commensurate with standard rates for similar research participation.  The research protocol, including the informed consent process     and compensation procedures, was designed to ensure the protection of participants' rights and well-being.  To mitigate potential risks, the topics of discussion during the dialogues were carefully selected to avoid sensitive or potentially harmful content. Participants were given the autonomy to choose topics from a predefined list and were free to pause or stop the recording at any point during the session.

\bibliography{main}




\newpage

\appendix

\section{Dataset details} 

\subsection{Topics, descriptions, and examples} \label{appendix: Topics, Descriptions, and Examples} 
Table~\ref{tab:topics} provides details on the seven conversation topics covered in the dataset, including a brief description of each topic and an example utterance illustrating typical content and code-switching patterns. This information clarifies the thematic scope of the data and provides context for interpreting the experimental results.

\subsection{Dialogue transcription format} \label{appendix_dialogue_case}

\begin{CJK}{UTF8}{gbsn}
Dialogue transcription format are shown in Figure \ref{appendix_dialogue_case} as an example. Note that he names used in this example (e.g., "凯丽", "贝拉") are pseudonyms and do not correspond to the real names of the speakers, ensuring the privacy of participants.
\end{CJK}

\setcounter{figure}{0}  % Reset figure counter
\setcounter{table}{0}   % Reset table counter
\renewcommand{\thefigure}{A.\arabic{figure}}
\renewcommand{\thetable}{A.\arabic{table}}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]
  {img/description/dialogue_case.pdf}
  \caption{A format example of dialogue transcription file (TextGrid)}
  \label{pic:dialogue_case}
\end{figure}

\begin{table*}[htbp]
% \scriptsize
\centering
\begin{tabular}{>{\RaggedRight}p{2cm} >{\RaggedRight}p{5cm} >{\RaggedRight}p{7cm}}
\toprule
Topic & Description & Example \\
\midrule
Personal & Discussions centered on individual experiences, preferences, and relationships. &  "\begin{CJK}{UTF8}{gbsn}就是我听你的描述，感觉你喜欢 Taylor. 因为我其实我有个弟弟也很喜欢 Taylor, 就是但是他性格还确实跟你相差蛮大，就是你给我的感觉 You are very a quiet boy\end{CJK}" \\
\midrule
Entertainment & Conversations focusing on various forms of entertainment and cultural trends. & "\begin{CJK}{UTF8}{gbsn}对，因为我们都想表现的自己非常的 courage, 但其实我小的时候也看着也非常 frighten，然后我会直接<FIL/>放学到 home 之后就一直坐坐在 sofa 上面看到十点都 can't move\end{CJK}" \\
% "\begin{CJK}{UTF8}{gbsn}他呀，就是唱那个 Paris In The Rain 的那个\end{CJK}" \\
\midrule
Technology & Debates and dialogues concerning technological advancements and their impact. & "\begin{CJK}{UTF8}{gbsn}是的，而且他们会通过算法去非常精准地知道你到底想要看一些什么样的东西，所以我感觉其实有的时候 big data 也是一个非常恐怖的东西\end{CJK}" \\
\midrule
Education & Discussions about the academic environment, including challenges and experiences. & "\begin{CJK}{UTF8}{gbsn}那是你们这个 group 自己去想一个 topic 呢，还是这个 professor 会提供一些他的 project 来支撑你们的毕业论文\end{CJK}" \\
\midrule
Job & Conversations about past or present employment situations, work environment, co-workers, etc. &  "\begin{CJK}{UTF8}{gbsn}对对，是如果有有更多的 opportunity 去供我去选择的话，我还是可能会就是放开专业去选择更多的这个看一看，开阔一下我的 horizon\end{CJK}"\\
\midrule
Sports & Discussions on sports activities, athletes, benefits of exercise, etc. & "\begin{CJK}{UTF8}{gbsn}但是这种持续的时间 I couldn't find very long， 就是我也能找到那种 feeling， but very quickly， It disappear only maybe half an hour is a longest period， sometimes 也就十五分钟二十分钟\end{CJK}"\\
\midrule
Philosophy &  Discussions about philosophical ideas and debates on current social issues. & "\begin{CJK}{UTF8}{gbsn}这就是他们现在所处的一个 dilema，It's really classic， I feel like it's happening everywhere。 因为每一个国家都有他各自的 minorities，然后也不得不承认有些地方的 educational resource 真的没有另一些地方更加的 advanced，更加的丰富。\end{CJK}"\\
\bottomrule
\end{tabular}
\caption{Details of topics, descriptions, and examples}
\label{tab:topics}
\end{table*}



\subsection{Annotator information} \label{appendix_annotator_summary}
Table~\ref{tab:annotator_summary} provides a breakdown of the annotators' demographic characteristics, including their age, gender, hometown, and educational background.
\begin{table}[htbp]
\centering
\begin{tabular}{@{}llr@{}}
\toprule
Category & Value & Count/Percentage \\ \midrule
\multirow{2}{*}{Gender} & Male & 6 (40\%) \\
 & Female & 9 (60\%) \\ \midrule
\multirow{2}{*}{Education} & Bachelor's & 12 (80\%) \\
 & Master's & 3 (20\%) \\ \midrule
\multirow{9}{*}{Hometown} & Liaoning & 1 (6.67\%) \\
 & Henan & 1 (6.67\%) \\
 & Shanxi & 3 (20\%) \\
 & Zhejiang & 1 (6.67\%) \\
 & Jiangxi & 2 (13.33\%) \\
 & Beijing & 1 (6.67\%) \\
 & Fujian & 1 (6.67\%) \\
 & Hebei & 3 (20\%) \\
 & Shaanxi & 1 (6.67\%) \\
& Ningxia & 1(6.67\%) \\
 \midrule
\multirow{5}{*}{Age}
& 21 & 5 (33.33\%)\\
& 22 & 1 (6.67\%)\\
& 24 & 4 (26.67\%) \\
& 25 & 3 (20\%)\\
& 28 & 2 (13.33\%) \\
\bottomrule
\end{tabular}
\caption{Summary of Annotator Demographics}
\label{tab:annotator_summary}
\end{table}

\section{Full-dialogue duration distribution} \label{appendix: Full-dialogue duration distribution} 

\setcounter{figure}{0}  % Reset figure counter
\setcounter{table}{0}   % Reset table counter
\renewcommand{\thefigure}{B.\arabic{figure}}
\renewcommand{\thetable}{B.\arabic{table}}

As presented in Figure~\ref{pic:long_duration}, most of full-dialogue recordings are between 2000 to 3000 seconds. This distribution indicates a dataset primarily composed of relatively shorter full-dialogue recordings, with a smaller number of significantly longer recordings.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]
  {img/description/long_duration.pdf}
  \caption{Duration distribution of full-dialogue recordings}
  \label{pic:long_duration}
\end{figure}

\section{Text analysis}

\setcounter{figure}{0}  % Reset figure counter
\setcounter{table}{0}   % Reset table counterC
\renewcommand{\thefigure}{C.\arabic{figure}}
\renewcommand{\thetable}{C.\arabic{table}}

\subsection{Top frequent strings } \label{appendix: top frequent strings} 
To illustrate common linguistic patterns and code-switching behaviors, Table~\ref{tab:frequent_strings} presents the most frequent strings found in the Chinese (CN), English (EN), and mixed-language utterances within the dataset. The table lists the strings and their corresponding frequencies.

\begin{table*}[htbp]
\centering
\tiny
\begin{CJK}{UTF8}{gbsn} % 开始 CJK 环境 (Simplified Chinese)
\begin{tabular}{cccccc}
\toprule
\multicolumn{2}{c}{CN} & \multicolumn{2}{c}{EN} & \multicolumn{2}{c}{MIXED} \\
\cmidrule(lr){1-2} \cmidrule(lr){3-4} \cmidrule(lr){5-6}
String     & Count     & String         & Count & String           & Count  \\
\midrule
我觉得        & 3,391      & A LOT OF       & 304   & YOU KNOW 就       & 20     \\
的时候        & 2,056      & I THINK IT'S   & 208   & KNOW 就是          & 18     \\
比如说        & 1,410      & I WANT TO      & 182   & 这个 AI            & 14     \\
是一个        & 1,173      & DO YOU LIKE    & 178   & SCHOOL 的时        & 12     \\
因为我        & 1,119      & DO YOU HAVE    & 168   & 我的 FRIENDS       & 11     \\
然后我        & 1,033      & I DON'T KNOW   & 143   & 就是 YOU           & 11     \\
的一个        & 982       & YEAH YEAH YEAH & 142   & 就是 AI            & 10     \\
但是我        & 958       & AND I THINK    & 141   & 一个 VERY          & 10     \\
的一些        & 925       & SO I THINK     & 139   & 非常 INTERESTING   & 10     \\
就是我        & 913       & YEAH I THINK   & 132   & I THINK 我        & 9      \\
的就是        & 809       & WHAT DO YOU    & 118   & 常的 HAPPY         & 9      \\
非常的        & 792       & GO TO THE      & 115   & PLAY 麻将          & 9      \\
或者是        & 733       & BUT I THINK    & 110   & HIGH SCHOOL 的    & 8      \\
有一些        & 730       & I THINK I      & 108   & 一个 BIG           & 8      \\
我感觉        & 726       & YOU WANT TO    & 107   & TOGETHER 然后      & 7      \\
\bottomrule
\end{tabular}
\end{CJK} % 结束 CJK 环境
\caption{Top Frequent Strings in Each Language Category}
\label{tab:frequent_strings}
\end{table*}

\subsection{POS Tag} \label{appendix:pos_tag}

Table \ref{tab:pos_tag} in Appendix \ref{appendix:pos_tag} details the distribution of part-of-speech (POS) tags within the dataset. The table lists the counts of each POS tag (e.g., NOUN for noun, VERB for verb, etc.) separately for English and Mandarin Chinese.

\begin{table}[htbp]
\centering
\small
\begin{tabular}{c|c|c}
\toprule
POS tag & EN count & ZH count \\
\midrule
PROPN   & 5,096    & 11,448   \\
NOUN    & 37,654   & 77,592   \\
DET     & 13,856   & -        \\
ADV     & 17,151   & 68,370   \\
ADJ     & 15,930   & 15,003   \\
PRON    & 37,881   & 87,156   \\
VERB    & 28,841   & 115,238  \\
AUX     & 16,845   & 1,949    \\
PART    & 6,743    & 43,710   \\
ADP     & 16,026   & 27,980   \\
CCONJ   & 9,032    & 33,370   \\
SCONJ   & 5,018    & -        \\
NUM     & 2,041    & 25,722   \\
INTJ    & 10,091   & 27,124   \\
X       & 298      & 10,121  \\
\bottomrule
\end{tabular}
\caption{POS tag counts for both languages}
\label{tab:pos_tag}
\end{table}


\section{Experimental configurations} \label{Appendix_hyperparem} 


\setcounter{figure}{0}  % Reset figure counter
\setcounter{table}{0}   % Reset table counter
\renewcommand{\thefigure}{D.\arabic{figure}}
\renewcommand{\thetable}{D.\arabic{table}}


This section provides detailed hyperparameters used for training and fine-tuning ASR models discussed in the paper. All experiments were conducted using four GTX 3090 for several hours. All models utilized in this research are open-source and operate under the MIT License.

\subsection{Training ASR model from scratch}
Table \ref{tab:scratch setting} presents the training hyperparameters for Transformer, Branchformer and Conformer using Wenet toolkit, including batch size, learning rate and epochs.
\begin{table}[!t]
\centering
\small
\begin{tabular}{ccccc}
\toprule
\textbf{Model}    & \textbf{Batch size} & \textbf{Learning rate} & \textbf{Epochs} \\ 
\midrule
Transformer    & Dynamic         & 1.00E-03            & 150              \\
Branchformer   & Dynamic         & 1.00E-03              & 150              \\
Conformer     & Dynamic         & 1.00E-03              & 150              \\
\bottomrule         
\end{tabular}
\caption{Hyperparameters for training ASR models from scratch.}
\label{tab:scratch setting}
\end{table}

\subsection{Fine-tuning ASR Model}
Table~\ref{tab:finetuning setting} presents the hyperparameters used during fine-tuning of the different Whisper model versions.  These parameters include the learning rate and the number of epochs.  A dynamic batch size was utilized, constrained to a maximum of 60,000 frames per batch.

\begin{table}[!t]
\centering
\small
\begin{tabular}{cccc}
\toprule
\textbf{Whisper}           & \textbf{Batch size} & \textbf{Learning rate}         & \textbf{Epochs} \\
\midrule
Tiny              & 16         & 1.00E-05              & 20              \\
Base                &16         & 1.00E-05                  & 20               \\
Small              & 16         & 1.00E-05                 & 20               \\
Medium           & 16         & 1.00E-05                 & 20               \\       
\bottomrule         
\end{tabular}
\caption{Hyperparameters for fine-tuning Whisper.}
\label{tab:finetuning setting}
\end{table}

\section{Case Studies} \label{Appendix_case_study}


\setcounter{figure}{0}  % Reset figure counter
\setcounter{table}{0}   % Reset table counter
\renewcommand{\thefigure}{E.\arabic{figure}}
\renewcommand{\thetable}{E.\arabic{table}}

\begin{figure*}[h]
  \centering
  \includegraphics[width=1.0\linewidth]
  {img/description/some_case.pdf}
  \caption{Examples of ASR output from the Whisper Medium model under zero-shot and fine-tuned conditions, showing ground truth transcriptions and error rates}
  \label{pic:some_case}
\end{figure*}

To illustrate the types of errors made by the Whisper Medium model and the improvements achieved through fine-tuning, Figure~\ref{pic:some_case} presents example transcriptions for several utterances. The figure compares the zero-shot and fine-tuned outputs against the ground truth transcriptions, highlighting differences and providing the associated MER. We obsever that whisper designed for both ASR and S2TT tasks, exhibits an unintended behavior in code-switching ASR scenarios. Specifically, the model occasionally produces translations of the input speech rather than accurate transcriptions, deviating from the expected ASR output.





\end{document}