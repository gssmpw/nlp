We propose to solve the problem defined in Section \ref{sec:probdef} in two stages. First, we learn a 1D dynamical control system, $\mathbf{z}(\bar{x}, t)$, which captures the essential properties of full dynamics of wave $\f$, such as energy distribution in space, scattering, excitation, and dissipation. Second, we derive optimal actions to control the energy distribution of $\f$ via its 1D model $\mathbf{z}(\bar{x}, t)$, which is used as a surrogate in the optimization of Eq. \eqref{eq:fullyobservableloss}. A shorthand notation is used $\mathbf{z} = \mathbf{z}(\bar{x}, t)$.

\subsection{Learning the Reduced Dimensional Model}
Given an initial sensor observation, $X(t_i)$, robot state $d(t_i)$, and actions $a(t)$, the agent constructs $\mathbf{z}$ for $t \in [t_i, t_f]$, a 1D representation of the true function, $\f$, in the reduced spatial coordinates, $\bar{x}\in\Gamma\subset\mathbb{R}$. Construction of $\mathbf{z}$ is accomplished by way of two encoders shown in Figure \ref{fig:Encoders} which output functions defined over $\Gamma$: the wave encoder, $W$, and the robot encoder, $D$:
\begin{align}
&W_{\mu}:X(t)\rightarrow [\mathbf{g}(\bar{x}, t), l(\bar{x})]&\text{(Wave Encoder)} \label{eq:wave-encoder}\\
&D_{\phi}:d(t)\rightarrow c(\bar{x}, t)&\text{(Robot Encoder)} \label{eq:design-encoder}
\end{align}
where $c(\bar{x}, t)$ and $l(\bar{x})$ represent the environment components which are controllable and uncontrollable by the agent's actions, respectively. This way $c(\bar{x}, t)$ and $l(\bar{x})$ correspond to $d(t)$ and $\ell(x)$, respectively, in the original environment, $\f$. This separation is beneficial because it allows for explicit representation of energy excitation and dissipation properties of the original environment (cf., Figure \ref{fig:main_scheme}) within $l(\bar{x})$. Additionally, $W_{\mu}$ outputs $\mathbf{g}(\bar{x}, t)$, which defines the initial conditions of $\mathbf{z}$. We denote the parameters of $W_{\mu}$ and $D_{\phi}$ by $\theta\doteq[\mu, \phi]$.

Given these definitions, we introduce the following optimization problem for $t\in [t_i, t_f]$: 
\begin{align}
    &\min_{\theta}\Biggl\{ \int_{t_i}^{t_f}||\mathcal{T}\bigl(\mathbf{z}(\bar{x}, t')\bigr) - \mathcal{T}(\zeta(x, t')||^2dt'\label{eq:ControlCostLatent}\\
    &+ \int_{t_i}^{t_f}\!\!\!\!\int_\Gamma ||\partial_{t} \mathbf{z}(\bar{x}, t') - \mathcal{N}^\Gamma(\mathbf{z}(\bar{x}, t'); c_{\phi}(\bar{x}, t'), l_{\mu}(\bar{x}), t')||^2 d\bar{x}dt' \label{eq:PhysicsLossLatent}\\
    &\qquad\qquad\;+ \int_\Gamma ||\mathbf{z}(\bar{x}, t_i) - \mathbf{g}_{\mu}(\bar{x}, t_i)||^2 d\bar{x}\label{eq:ICLatent}\Biggr\}\\
    &\qquad\mbox{s.t. }\;\dot{d}(t) = F(d(t), a(t))\nonumber,
\end{align}
where $\mathcal{N}^{\Gamma}$ is a PDE similar to $\mathcal{N}$ that operates on a 1D space $\Gamma$ instead of $\Omega$. Eq. \eqref{eq:ControlCostLatent} is the prediction loss for scattered energy. Eq. \eqref{eq:PhysicsLossLatent} and Eq. \eqref{eq:ICLatent} are the PDE consistency and the initial condition losses, respectively. Notably, $\mathbf{z}$ implicitly depends on $\theta$ through learnable $\mathbf{g}_{\mu}$, and $c_{\phi}$ and $l_{\mu}$.
% , which are the initial conditions, and the properties of physics-informed latent space. 

We chose a deep convolutional neural network to represent $W_\mu$, in Figure \ref{fig:Encoders} (left), which maps the pixels of the sensor observations $X(t)$ onto functions over $\Gamma$. Similarly, the robot encoder is implemented by an artificial neural network $D_\phi$, in Figure \ref{fig:Encoders} (right), maps the robot's trajectory to latent control functions. Since the dynamics of the robot in Eq. \eqref{eq:design_dynamics} are known ahead of time, $D_\phi$ can construct $c(\bar{x}, t)$ from an initial robot state $d(t_i)$ and $a(t)$.

In our method, $\mathbf{z}$ is constructed using numerical integration of $\mathcal{N}^{\Gamma}$. Assuming prior knowledge of the governing equation of $\f$ in lower dimensional space, the encoded initial conditions, boundary conditions, and exogenous forces specify a solution $\mathbf{z}$. Relying on numerical integration to generate $\mathbf{z}$ has the unique benefit of guaranteeing that it is a solution to $\mathcal{N}^{\Gamma}$, while introducing no trainable parameters aside from the encoders $W_\mu$ and $D_\phi$. This is in contrast to the approaches that require an additional network (e.g., NODE) to represent the dynamics $\mathcal{N}^{\Gamma}$ of $\mathbf{z}$, which increases sample and computational complexity.

Our method requires the following data for training:
\begin{align}
\mathcal{I}=\{\bigl(X(t_i), d(t_i), a(t), \sigma(t)\bigr)\}_{k=1}^N,\label{eq:dataset}   
\end{align}
where $X(t_i)$, $d(t_i)$, $a(t)$, and $\sigma(t)$, are a sensor observation and a robot state observed at time $t_i$, and actions and corresponding target signal in time period $t\in \left[t_i, t_{f} \right]$. Algorithm~\ref{alg:cPILS_NI} summarizes the learning of physics-informed model. 
% We generate $\mathcal{I}$ using a random policy to select actions, however, in principle it could be generated using an arbitrary policy.

\begin{figure}[t!]
\centering
\includegraphics[width=\linewidth]{Figures/Encoders.jpg}
\caption{Encoding Scheme. Left: Sensor observation $X(t_i)$ of the PDE $\f$ is encoded by $W$ to the latent initial condition $\mathbf{g}(\bar{x}, t_i)$ and exogenous function $l(\bar{x})$. Right: sequences of robot configurations, $d(t_i), d(t_{i+1}), \cdots$, produced by agent's actions, $a_i(t)$ are individually encoded to latent control functions $c(\bar{x}, t_i), c(\bar{x}, t_{i+1}), \cdots$.}\label{fig:Encoders}
\end{figure}
\begin{algorithm}[h!]
\caption{Learning Physics-Informed Model}
\begin{algorithmic}[1]\label{alg:cPILS_NI}

\REQUIRE Training data, $\mathcal{I}$, (cf., Eq. \eqref{eq:dataset}), parameters of $W_{\mu}$ and $D_{\phi}$, denoted by $\theta = (\mu, \phi)$, and design dynamics $F$.

\STATE Randomly initialize $\theta$
\REPEAT
    \setlength\itemsep{0.5em}
    \STATE \textbf{Sample} a data point from $\mathcal{I}$:\hfill\COMMENT{Eq. \eqref{eq:dataset}}
    % $X(t_i), d(t_i), \vec{a}_i(t), \sigma_i(t)\sim \mathcal{I}$
    $X(t_i), d(t_i), a(t), \sigma(t)\sim \mathcal{I}$
    \STATE \textbf{Encode} a Sensor Reading (wave image):\hfill\COMMENT{Eq. \eqref{eq:wave-encoder}}
    $\mathbf{g}_{\mu}(\bar{x}, t_i), l_{\mu}(\bar{x}) = W_{\mu}(X(t_i))$
    % \STATE \textbf{Integrate} a Design Trajectory from $d(t_i)$:\\
    % $\vec{d}_{i}(t)=\bigl[ d_{i}(t), d_{i+1}(t), \dots,d_{i+k}(t), \dots, d_{i+h}(t)\bigr]$\\
    % with $d_{i+k}(t)=d(t_i) + \int_{t_i}^{t_{i+k}} F(d(t'), \vec{a}_{i}(t'))dt'$
    \STATE \textbf{Integrate} a Design Trajectory from $d(t_i)$:\\
    $d(t) = d(t_i) + \int_{t_i}^{t_f}F(d(t'), a(t'))dt'$
    \STATE \textbf{Encode} a Design Trajectory:\hfill\COMMENT{Eq. \eqref{eq:design-encoder}}
    % $c_{\phi}(\bar{x}, t) = D_{\phi}(\vec{d}_i(t))$
    $c_\phi(\bar{x}, t) = D_\phi(d(t))$
    \STATE \textbf{Integrate} a Latent Trajectory from $\mathbf{g}(\bar{x}, t_i)$:\vspace{-0.5cm}\\
    \begin{align*}
        &\mathbf{z}_{\theta}(\bar{x}, t) = \\
        &\quad\mathbf{g}_{\mu}(\bar{x}, t_i)+ \int_{t_i}^{t} \mathcal{N}^{\Gamma}(\mathbf{z}(\bar{x}, t'); c_{\phi}(\bar{x}, t'), l_{\mu}(\bar{x}), t')dt'
    \end{align*}
    \vspace{-0.5cm}
    \STATE \textbf{Calculate} the Prediction Loss:\hfill\COMMENT{Eq. \eqref{eq:ControlCostLatent}}\\
    $\mbox{loss}(\theta) = \int_{t_i}^{t_f} \bigl(\mathcal{T}\bigl(\mathbf{z}_{\theta}(\bar{x}, t')\bigr) - \sigma(t') \bigr)^2dt'$
    \STATE \textbf{Perform Gradient Descent} on  $\mbox{loss}(\theta)$
\UNTIL{Convergence}
\RETURN $W^*_{\mu}$, $D^*_{\phi}$
\end{algorithmic}
\end{algorithm}
Once the robot learns how its sparse control affect the wave, $\f$, through $\mathbf{z}$, optimal actions can be selected according to Model Predictive Control, as explained below.

\subsection{Model Predictive Control of Wave Energy}
% \cite{RICHALET1978_MPC, MORARI1988_MPC}, which is demonstrated in Section \ref{sec:experiments}.

\iffalse
We apply MPC in our setting by first defining a sequence of continuous actions $a_\tau(t) \doteq \{ a(t): t_\tau \leq t < t_{\tau+1} \}$. Dividing the time interval $t\in[ t_i, t_f ]$\ST{do you mean $t\in[ t_{\tau}, t_{\tau+1} ]$} into equal length segments allows our agent to operate at a slower timescale than the wave. Next, utilization of the lower dimensional representation of $\zeta$ from which $\sigma(t)$ can be efficiently computed allows for tractable optimization of Eq. \eqref{eq:fullyobservableloss} in the form of the objective in Eq. \eqref{eq:latent_control_cost}.
% \begin{equation}
%     \underset{a_i(\cdot), a_{i+1}(\cdot), \cdots, a_{f-1}(\cdot)}{\min} \int_{t_i}^{t_f}||\mathcal{T}(\mathbf{z}(\bar{x}, t')) - \sigma^*(t')||^2 + \beta||a(t')||^2 dt' \label{eq:latent_control_cost}
% \end{equation}
% \begin{multline}
%     \underset{a(\cdot)}{\min} \int_{t_i}^{t_f}||\mathcal{T}(\mathbf{z}(\bar{x}, t')) - \sigma^*(t')||^2dt' \\
%     + \beta\sum_{\tau=i}^{f-1}\int_{t_\tau}^{t_{\tau+1}}||a_\tau(t')||^2
%     \label{eq:latent_control_cost}
% \end{multline}
\begin{equation}
    \underset{a(\cdot)}{\min} \int_{t_i}^{t_f}||\mathcal{T}(\mathbf{z}(\bar{x}, t')) - \sigma^*(t')||^2dt'
    + \beta\sum_{\tau=i}^{f-1}\int_{t_\tau}^{t_{\tau+1}}||a_\tau(t')||^2dt'
    \label{eq:latent_control_cost}
\end{equation}
\fi

We apply MPC \cite{bakaravc2018_random_shooting,RICHALET1978_MPC, MORARI1988_MPC} for control of wave energy over the time interval $[t_i, t_f]$, by dividing a control trajectory, $a(t)$, to a sequence of $N_a$ piece-wise constant actions, $a(t)=\{a_{\tau} : \mbox{ if }t_\tau\le t < t_{\tau+1}\,\forall \tau \}$. This time discretization leads to a series of time intervals $\Delta_{\tau} = (t_{\tau+1}-t_{\tau})$, with boundary conditions $t_{\tau = 1} = t_i$ and $t_{\tau = N_a + 1} = t_f$, which reflects the slow time scale of robotic actuation in comparison to the high speed of wave propagation. The objective below with the reference control signal $\sigma^*(t)$, allows for the derivation of the optimal control sequence $\{a^*_{\tau}\}_{\tau=1}^{N_a}$:   
\begin{align}
 &\underset{\{a_{\tau}\}_{\tau=1}^{N_a}}{\min} \int_{t_i}^{t_f}||\mathcal{T}(\mathbf{z}(\bar{x}, t')) - \sigma^*(t')||^2dt'
    + \beta\sum_{\tau=1}^{N_a}||a_{\tau}||^2\Delta_{\tau}    \label{eq:latent_control_cost}\\
    &\mbox{s.t.} \begin{cases}
        \partial_{t}\mathbf{z}(\bar{x}, t) = \mathcal{N}^\Gamma(\mathbf{z}(\bar{x}, t); c_{\phi^*}(\bar{x}, t), l_{\mu^*}(\bar{x}), t)\\
        \mathbf{z}(\bar{x}, t_i) = \mathbf{g}_{\mu^*}(\bar{x}, t_i)
    \end{cases}
\end{align}
where the constraints are the latent 1D dynamics with the optimal parameters $\theta^* = [\mu^*, \phi^*$] in Eq.~\eqref{eq:PhysicsLossLatent}-\eqref{eq:ICLatent}, in addition to F, which is known to the agent. The agent's power is constrained through hyperparameter $\beta$.

% We choose to optimize action trajectories through a random shooting algorithm  which represents one of the simplest control strategies. We defer exploration of more advanced control strategies to future works.

To the best of our knowledge, this proposed solution has not been studied prior to our work, while the most relevant works \cite{ericaislanantonelo_2022_physicsinformed, nicodemus_2022_physicsinformed} considered ODEs only, which is a restricted class of dynamics. Moreover, there are no efficient data-driven methods for controlling acoustic PDEs in open space with sparse control and partially observable state.
