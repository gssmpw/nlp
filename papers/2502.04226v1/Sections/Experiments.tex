\section{Experiments}
\label{sec:Experiments}
In this section, we evaluate the proposed SCP on six widely challenging image clustering datasets. A series of initial quantitative and qualitative comparisons, ablation studies, and hyper-parameter analyses will be carried out to investigate the effectiveness and robustness of the method.

\subsection{Experimental Setup}
We first introduce the datasets and metrics used for evaluation and then provide the implementation details of SCP.

\subsubsection{Datasets}
To evaluate the performance of SCP, we apply it to six widely used image clustering datasets: STL-10 \cite{coates2011analysis}, CIFAR-10 \cite{krizhevsky2009learning}, CIFAR-20 \cite{krizhevsky2009learning}, CIFAR-100 \cite{krizhevsky2009learning}, ImageNet-10 \cite{chang2017deep}, and ImageNet-Dogs \cite{chang2017deep}, which is a subset from ImageNet-1k \cite{deng2009imagenet}. The brief information of all datasets used in our evaluation is summarized in Table~\ref{tab:datasets}.

\begin{table}[h]
\caption{Characteristics of the benchmark datasets used in our evaluation.}
\label{tab:datasets}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{l c c c c}
\toprule
\textbf{\textcolor{red}{Dataset}} & \textbf{\textcolor{red}{Split (Train/Test)}} & \textbf{\textcolor{red}{\# Training}} & \textbf{\textcolor{red}{\# Testing}} & \textbf{\textcolor{red}{\# Classes}} \\
\midrule
STL-10 & Train/Test & 5,000 & 8,000 & 10 \\
CIFAR-10 & Train/Test & 50,000 & 10,000 & 10 \\
CIFAR-20 & Train/Test & 50,000 & 10,000 & 20 \\
CIFAR-100 & Train/Test & 50,000 & 10,000 & 100 \\
ImageNet-10 & Train/Val & 13,000 & 500 & 10 \\
ImageNet-Dogs & Train/Val & 19,500 & 750 & 15 \\
\bottomrule
\end{tabular}%
\end{sc}
\end{small}
\end{center}
\end{table}

\subsubsection{Implementation Details}
In most experiments, we utilized the CLIP \cite{radford2021learning} with the backbone ViT-B/32 \cite{dosovitskiy2020image}. For a fair comparison with TEMI \cite{adaloglou2023exploring} and CPP \cite{chu2024image}, we replaced the CLIP backbone with ViT-L/14 to match the architectures used in these methods. The cluster head $g$ is a five-layer MLP with dimensions: $D \rightarrow 1024 \rightarrow 786 \rightarrow 512 \rightarrow 1024 \rightarrow K$, where $D$ denotes the output dimension of the pre-trained model and $K$ represents the number of clusters.

We train the model using the Adam optimizer with a cosine annealing learning rate schedule, starting from an initial rate of \(1 \times 10^{-3}\) for 30 epochs. The batch size is set to 512 for all datasets. For regularization, we set $\alpha = 2$ for CIFAR-20 and ImageNet-Dogs, $\alpha = 3$ for CIFAR-100 to account for its larger number of clusters, and $\alpha = 1$ for the remaining datasets.
 All experiments are conducted on a single NVIDIA L4 GPU. Under this setup, training SCP on CIFAR-10 takes approximately one minute, excluding data augmentation.





\subsection{Main Results}
We compare our method with state-of-the-art baselines on six widely-used image clustering datasets, with feature visualizations to show the competitiveness of our proposed SCP.


\subsubsection{Comparison against classical and text-based methods}
We first evaluate our pipeline on five widely used datasets and compare it with 18 deep clustering baselines. Since most earlier baselines adopt ResNet-34 or ResNet-18 as the backbone, we mainly focus on comparisons with CLIP and CLIP-based methods. Specifically, CLIP (zero-shot) uses CLIP’s pre-trained image and text encoders to classify images by matching them to text prompts, while CLIP (K-means) clusters directly on CLIP image features via K-means. Our approach includes SCP-CLIP, which employs a ViT-B/32 backbone like other CLIP-based methods, and SCP-DINO, which uses a ViT-B/8 backbone. Each experiment is repeated 50 times, and the best results are reported.


\begin{table}[htbp]
\caption{%
Clustering performance of various baseline methods on multiple datasets 
(all metrics are multiplied by 100). 
We highlight the best and second-best results in {\first{boldfaced red}} and {\second{underlined in blue}}, respectively. 
CLIP (K-means), CLIP (zero-shot), SCP-CLIP, SIC, and TAC \textbf{all adopt a ViT-B/32 backbone.} 
SCP-DINO uses \textbf{a Vit-B/8 backbone.}
}
\label{tab:performance_comparison_textfree}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c c ccc ccc ccc ccc ccc}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Text-Free} & \multicolumn{3}{c}{STL-10} & \multicolumn{3}{c}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-20} & \multicolumn{3}{c}{ImageNet-10} & \multicolumn{3}{c}{ImageNet-Dogs} \\
\cmidrule(r){3-5} \cmidrule(r){6-8} \cmidrule(r){9-11} \cmidrule(r){12-14} \cmidrule(r){15-17}
 &  & NMI & ACC & ARI & NMI & ACC & ARI & NMI & ACC & ARI & NMI & ACC & ARI & NMI & ACC & ARI \\ 
\midrule
JULE \cite{yang2016jointunsupervisedlearningdeep} 
  & \ding{51} 
  & 18.2 & 27.7 & 16.4 
  & 19.2 & 27.2 & 13.8 
  & 10.3 & 13.7 & 3.3 
  & 17.5 & 30.0 & 13.8 
  & 5.4 & 13.8 & 2.8 \\

DEC \cite{xie2016unsuperviseddeepembeddingclustering} 
  & \ding{51}
  & 27.6 & 35.9 & 18.6 
  & 25.7 & 30.1 & 16.1 
  & 13.6 & 18.5 & 5.0 
  & 28.2 & 38.1 & 20.3 
  & 12.2 & 19.5 & 7.9 \\

DAC \cite{chang2017deep}
  & \ding{51}
  & 36.6 & 47.0 & 25.7 
  & 39.6 & 52.2 & 30.6 
  & 18.5 & 23.8 & 8.8 
  & 39.4 & 52.7 & 30.2 
  & 21.9 & 27.5 & 11.1 \\

DCCM \cite{wu2019deepcomprehensivecorrelationmining} 
  & \ding{51}
  & 37.6 & 48.2 & 26.2 
  & 49.6 & 62.3 & 40.8 
  & 28.5 & 32.7 & 17.3 
  & 60.8 & 71.0 & 55.5 
  & 32.1 & 38.3 & 18.2 \\

IIC \cite{ji2019invariantinformationclusteringunsupervised} 
  & \ding{51}
  & 49.6 & 59.6 & 39.7 
  & 51.3 & 61.7 & 41.1 
  & 22.5 & 25.7 & 11.7 
  & - & - & - 
  & - & - & - \\

PICA \cite{huang2020deep} 
  & \ding{51}
  & 61.1 & 71.3 & 53.1 
  & 59.1 & 69.6 & 51.2 
  & 31.0 & 33.7 & 17.1 
  & 80.2 & 87.0 & 76.1 
  & 35.2 & 35.3 & 20.1 \\

CC \cite{li2021contrastive} 
  & \ding{51}
  & 76.4 & 85.0 & 72.6 
  & 70.5 & 79.0 & 63.7 
  & 43.1 & 42.9 & 26.6 
  & 85.9 & 89.3 & 82.2 
  & 44.5 & 42.9 & 27.4 \\

IDFD \cite{tao2021clusteringfriendlyrepresentationlearninginstance} 
  & \ding{51}
  & 64.3 & 75.6 & 57.5 
  & 71.1 & 81.5 & 66.3 
  & 42.6 & 42.5 & 26.4 
  & 89.8 & 95.4 & 90.1 
  & 54.6 & 59.1 & 41.3 \\

SCAN \cite{van2020scan}
  & \ding{51}
  & 69.8 & 80.9 & 64.6 
  & 79.7 & 88.3 & 77.2 
  & 48.6 & 50.7 & 33.3 
  & - & - & - 
  & 61.2 & 59.3 & 45.7 \\

MiCE \cite{tsai2020mice} 
  & \ding{51}
  & 63.5 & 75.2 & 57.5 
  & 73.7 & 83.5 & 69.8 
  & 43.6 & 44.0 & 28.0 
  & - & - & - 
  & 42.3 & 43.9 & 28.6 \\

GCC \cite{zhong2021graph} 
  & \ding{51}
  & 68.4 & 78.8 & 63.1 
  & 76.4 & 85.6 & 72.8 
  & 47.2 & 47.2 & 30.5 
  & 84.2 & 90.1 & 82.2 
  & 49.0 & 52.6 & 36.2 \\

NNM \cite{dang2021nearest} 
  & \ding{51}
  & 66.3 & 76.8 & 59.6 
  & 73.7 & 83.7 & 69.4 
  & 48.0 & 45.9 & 30.2 
  & - & - & - 
  & 60.4 & 58.6 & 44.9 \\

TCC \cite{shen2021you}
  & \ding{51}
  & 73.2 & 81.4 & 68.9 
  & 79.0 & 90.6 & 73.3 
  & 47.9 & 49.1 & 31.2 
  & 84.8 & 89.7 & 82.5 
  & 55.4 & 59.5 & 41.7 \\

SPICE \cite{niu2022spice}
  & \ding{51}
  & 81.7 & 90.8 & 81.2 
  & 73.4 & 83.8 & 70.5 
  & 44.8 & 46.8 & 29.4 
  & 82.8 & 92.1 & 83.6 
  & 57.2 & 64.6 & 47.9 \\ 
\midrule
SIC \cite{cai2023semantic}
  &  
  & 95.3 & 98.1 & 95.9
  & 84.7 & 92.6 & 84.4 
  & 59.3 & 58.3 & 43.9 
  & 97.0 & 98.2 & 96.1 
  & 69.0 & 69.7 & 55.8 \\

TAC \cite{li2023image}
  &  
  & \second{95.5} & \second{98.2} & \second{96.1} 
  & 83.3 & 91.9 & 83.1 
  & \first{61.1} & \first{60.7} & \first{44.8} 
  & \first{98.5} & \first{99.2} & \first{98.3} 
  & \first{80.6} & \first{83.0} & \first{72.2} \\ 
\midrule
SCP-CLIP 
  & \ding{51}
  & 94.6 & 97.9 & 95.1 
  & \second{85.5} & \second{93.2} & \second{85.7} 
  & \second{59.5} & \second{60.1} & \second{44.3} 
  & 97.4 & 98.5 & 97.9 
  & 56.3 & 59.6 & 44.6 \\

SCP-DINO
  & \ding{51}
  & \first{95.8} & \first{98.4} & \first{96.5} 
  & \first{89.2} & \first{95.2} & \first{89.8} 
  & 58.9 & 59.3 & 42.8 
  & \second{95.9} & \second{98.8} & \second{96.7} 
  & \second{77.1} & \second{80.5} & \second{70.5} \\

CLIP (K-means) 
  & \ding{51}
  & 92.2 & 94.5 & 89.5 
  & 71.3 & 75.2 & 62.6 
  & 50.8 & 48.1 & 30.6 
  & 96.9 & 98.2 & 96.1 
  & 39.8 & 38.1 & 20.1 \\

CLIP (zero-shot) 
  & 
  & 93.9 & 97.1 & 93.7 
  & 80.7 & 90.0 & 79.3 
  & 55.3 & 58.3 & 39.8 
  & 95.8 & 97.6 & 94.9 
  & 73.5 & 72.8 & 58.2 \\
\bottomrule
\end{tabular}%
} % end resizebox
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}

\begin{figure}[htbp]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/step1.png}
        \caption{Step 1, $ARI=4.5$}
        \label{fig:step1}
    \end{subfigure}
    \hfill    
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/step10.png}
        \caption{Step 10, $ARI=24$}
        \label{fig:step10}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/step50.png}
        \caption{Step 50, $ARI=57$}
        \label{fig:step50}
    \end{subfigure}
    \hfill  
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/step100.png}
        \caption{Step 100, $ARI=59$}
        \label{fig:step100}
    \end{subfigure}
\caption{%
Visualization of representations from different training steps learned by SCP-DINO on the ImageNet-Dogs training set, 
along with the corresponding clustering ARI (multiplied by 100). 
(a) Image embeddings directly from the DINO image encoder, with clusters obtained using K-means. 
(b)--(d) Image logits learned by SCP-DINO across different steps. 
There are $77$ steps per epoch.
}
\label{fig:epoch_comparison_tSNE}
\end{figure}


As shown in Table~\ref{tab:performance_comparison_textfree}, our method significantly enhances clustering performance across multiple benchmark datasets. For example, on CIFAR-10, CLIP (K-means) achieves an ACC of 75.2\%, whereas SCP-CLIP improves this by 18\%, surpassing TAC by 1.3\%. On CIFAR-20, SCP-CLIP attains an ACC of 60.1\%, outperforming SIC by 1.8\%. Moreover, SCP-DINO boosts performance on four benchmarks, achieving state-of-the-art results on STL-10 and the second-best performance on both ImageNet-10 and ImageNet-Dogs. The lower performance of SCP-CLIP on ImageNet-Dogs may stem from the difficulty of capturing discriminative features in fine-grained images, particularly without aligned text features.

Notably, SCP relies exclusively on visual features but still surpasses CLIP zero-shot on all benchmarks and outperforms TAC on both STL-10 and CIFAR-10. Results underscore its effectiveness in extracting and leveraging visual information for clustering. Furthermore, our pipeline is text-free, featuring a simpler architecture with fewer hyperparameters—making it especially advantageous for adaptation to pure visual pre-trained models.

Overall, these results demonstrate that beyond zero-shot classification or multi-modal frameworks, a simpler approach can still effectively utilize the power of pre-trained models for image clustering.




\subsubsection{Comparison with recent text-free methods}
To supplement our answers to Question~\eqref{Q1}, we introduce SCP-MIX, which leverages the concatenated visual features from both CLIP and DINO. Alongside SCP-CLIP, we compare this method against two recent text-free approaches, TEMI \cite{adaloglou2023exploring} and CPP \cite{chu2024image}, as reported in Table~\ref{tab:performance_comparison2}.
\begin{table}[ht]
\caption{%
Clustering performance comparison for recent text-free methods. The best and second-best results are highlighted in {\first{boldface red}} and {\second{underlined in blue}}, respectively. \textbf{All methods use the ViT-L/14 backbone, and SCP-MIX adopts the concatenation (or ensemble) of features learned by CLIP (ViT-L/14) and DINO (ViT-B/8)}.
}
\label{tab:performance_comparison2}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{c c c c c c c c}
\toprule
\multirow{2}{*}{Method} & \multirow{2}{*}{Text-Free} 
& \multicolumn{2}{c}{CIFAR-10} 
& \multicolumn{2}{c}{CIFAR-20} 
& \multicolumn{2}{c}{CIFAR-100} \\
\cmidrule(r){3-4} \cmidrule(r){5-6} \cmidrule(r){7-8}
 &  & NMI & ACC & NMI & ACC & NMI & ACC \\
\midrule
TEMI \cite{adaloglou2023exploring}
  & \ding{51}
  & 92.6 & 96.9
  & 64.5 & 61.8
  & 79.9 & 73.7 \\

CPP \cite{chu2024image}
  & \ding{51}
  & \second{93.6} & {97.4}
  & \second{72.5} & {64.2}
  & \first{81.8} & \second{74.0} \\

SCP-CLIP
  & \ding{51}
  & \second{93.6} & \second{97.5}
  & {69.2} & \second{65.8}
  & {80.1} & \first{74.1} \\

SCP-MIX
  & \ding{51}
  & \first{95.0} & \first{98.0}
  & \first{72.9} & \first{67.5}
  & \second{80.3} & {73.8} \\
\bottomrule
\end{tabular}
}% end of resizebox
\end{sc}
\end{small}
\end{center}
\end{table}

SCP-MIX outperforms CPP and TEMI on both CIFAR-10 and CIFAR-20, while SCP-CLIP achieves the highest accuracy on CIFAR-100. These results underscore the competitive performance of SCP, highlighting effectiveness and simplicity.  Notably, SCP achieves these outcomes without relying on additional feature layers, support sets, self-distillation, or exponential moving average strategies. Thus, unimodal text-free deep clustering method can rival multimodal pipelines.

\subsection{Visualization}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\columnwidth]{images/SCP-main.png}
    \caption{The visualization of clustering performance for SCP-CLIP with \textbf{ViT-B/32 backbone}. \textit{(Left)}: An example of an image-to-image search on STL-10, showing clusters produced by CLIP (Top) and SCP (Bottom); \textit{(Right)}: Visualization of clustering performance. SCP-CLIP effectively enhances CLIP's clustering performance through a cluster head.}
    \label{fig:visualization}
\end{figure}
To provide an intuitive understanding of the clustering results, we visualize the clustering performance obtained from SCP in Fig. \ref{fig:visualization}, along with learned representations on the CIFAR-10 dataset in Fig. \ref{fig:clustering_comparison_tSNE}. The image logits, representing SCP outputs before the final $\operatorname{softmax}$  function, are used for visualization, with t-SNE applied to reduce the feature dimensions. As shown, compared to embeddings directly learned by pre-trained models, SCP effectively forms well-separated clusters, leading to a higher ACC score. Without aligned text, SCP successfully extracts image features by incorporating a clustering head, resulting in superior within-cluster compactness and between-cluster separability. The visualization supports our quantitative results and highlights SCP's effectiveness in learning discriminative features suitable for lightweight clustering tasks. Furthermore, Fig. \ref{fig:epoch_comparison_tSNE} demonstrates that SCP requires only a few steps to establish a clustering structure, effectively separating certain image embeddings within 100 steps. For instance, from step 10 to step 50, SCP learns to push the light-green cluster away from the blue cluster.


\begin{figure}[htbp]
    \centering

    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/DINO.png}
        \caption{DINO, $ACC=80.5\%$}
    \end{subfigure}
    \hspace{0.05\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CLIP.png}
        \caption{SCP-Dino, $ACC=95.4\%$}
    \end{subfigure}
    \vspace{1em}

    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CAC-DINO.png}
        \caption{CLIP, $ACC=78.5\%$}
    \end{subfigure}
    \hspace{0.05\textwidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/CAC-CLIP.png}
        \caption{SCP-Clip, $ACC=93.5\%$}
    \end{subfigure}

    \caption{Visualization of representations learned by different methods on the CIFAR-10 training set,
along with the corresponding clustering accuracy (ACC). 
(a) Image embeddings directly from the DINO image encoder, with clusters obtained by K-means. 
(b) Image logits from SCP-DINO. 
(c) Image embeddings directly from the CLIP image encoder, again clustered by K-means. 
(d) Image logits from SCP-CLIP.}
    \label{fig:clustering_comparison_tSNE}
\end{figure}






\subsection{Ablation Studies}
To assess the effectiveness of the three loss terms, $L_{\text{e}}$, $L_{\text{con}}$, and $H(Y)$, we evaluate the performance of SCP using different combinations of these losses, as presented in Table \ref{AS1}. The results reveal several important insights: i) $H(Y)$ plays a crucial role in preventing cluster collapse. Without $H(Y)$, SCP tends to assign most images to only a few clusters, resulting in the model collapse on CIFAR-10 and CIFAR-20. ii) $L_{\text{con}}$ provides a slight boost in performance. The reason is that the cluster assignments would be less confident when the cluster number increases. iii) The combination of all losses effectively learns image information, leading to the best clustering performance in this table.

\begin{table}[ht]
\caption{%
Clustering results for different losses on CIFAR-10 and CIFAR-20. A '-' indicates model collapse.
}
\label{AS1}
\begin{center}
\begin{small}
\begin{sc}
\resizebox{0.7\textwidth}{!}{%
\begin{tabular}{c c c c c c c c c}
\toprule
\multirow{2}{*}{$L_{\text{e}}$} & \multirow{2}{*}{$L_{\text{con}}$} & \multirow{2}{*}{$H(Y)$} 
& \multicolumn{3}{c}{CIFAR-10} & \multicolumn{3}{c}{CIFAR-20} \\ 
\cmidrule(r){4-6} \cmidrule(r){7-9}
 &  &  & NMI & ACC & ARI & NMI & ACC & ARI \\
\midrule
\ding{51} &      &      & -    & -    & -    & -    & -    & -    \\ 
\ding{51} & \ding{51} &      & -    & -    & -    & -    & -    & -    \\ 
\ding{51} & \ding{51} & \ding{51} & \first{85.5} & \first{93.2} & \first{85.7} & \first{59.5} & \first{60.1} & \first{44.3} \\ 
     & \ding{51} &      & -    & -    & -    & -    & -    & -    \\ 
     & \ding{51} & \ding{51} & 83.0 & 90.4 & 81.4 & \second{55.7} & \second{53.1} & \second{37.6} \\ 
     &      & \ding{51} & 16.0 & 22.6 & 6.5  & 16.0 & 16.5  & 4.6  \\ 
\ding{51} &      & \ding{51} & \second{83.9} & \second{91.8} & \second{82.9} & 54.7 & 52.9 & 37.0 \\ 
\bottomrule
\end{tabular}
}% end of resizebox
\end{sc}
\end{small}
\end{center}
\end{table}


\subsection{Parameter Analyses}
To show how the scale of \( H(Y) \) influences the performance of SCP, we evaluate it under various choices of \( \alpha \) on training sets of CIFAR-10, CIFAR-20, CIFAR-100 and ImageNet-Dogs under 5 random seeds. The average results and the corresponding standard deviations are presented in Fig.~\ref{fig:comparison}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=.6\textwidth]{images/PA3.png}
    \vspace{-.5em}
    \caption{Comparison of different loss weights \( \alpha \), the solid line is the mean ARI across five runs, and the shaded region plots the standard deviation across our runs.}
    \label{fig:comparison}
\end{figure}

We observe that setting the loss weight \(\alpha\) too low (e.g., below 0.5) makes it difficult for our method to converge. For CIFAR-10, the performance remains relatively stable and is not sensitive to large \(\alpha\) values. In contrast, for ImageNet-dogs and CIFAR-20, choosing \(\alpha = 2\) leads to better results, suggesting it provides an optimal balance between regularization and scale. For CIFAR-100, which involves a larger number of clusters, a higher \(\alpha\) value is more suitable. Based on these findings, we select \(\alpha = 3\) for CIFAR-100, \(\alpha = 2\) for CIFAR-20 and ImageNet-dogs, and \(\alpha = 1\) for all other datasets. It is also consistent with the prior knowledge of the number of clusters in each dataset.




% \begin{table}[h!]
% \centering
% \begin{tabular}{lccc}
% \toprule
% \textbf{Methods} & \textbf{CIFAR-10} & \textbf{CIFAR-100} & \textbf{STL-10} \\
% \midrule
% K-means & 83.5 & 47.3 & 94.5 \\
% NNM \cite{dang2021nearest} & 84.3 & 47.7 & 80.8 \\
% PCL \cite{li2020prototypical} & 87.4 & 52.6 & 41.0 \\
% SCAN \cite{van2020scan} & 88.3 & 50.7 & 80.9 \\
% SPICE \cite{niu2022spice} & 92.6 & 53.8 & 93.8 \\
% ProPos \cite{huang2022learning} & 94.3 & 61.4 & 86.7 \\
% \textbf{TEMI} \cite{adaloglou2023exploring} & 96.9 & 73.7 & 98.5 \\
% \textbf{CPP} \cite{chu2024image} & 97.4 & 74.0 & -- \\
% \textbf{CAEL} \footnotesize{(Our method)} & \textbf{97.2} & \textbf{62.4} & \textbf{99.6} \\
% \bottomrule
% \end{tabular}
% \vspace{0.2cm}
% \caption{\textbf{Clustering accuracy on CIFAR-10, CIFAR-20, and STL-10 datasets.} The K-means performance is based on features pre-processed by CLIP. TEMI (2023) \cite{adaloglou2023exploring} and CPP (2024 April) \cite{chu2024image} are the latest models that also applied CLIP, while other methods trained their own encoder. Other accuracies are collected from their original papers.}
% \label{tab:performance_comparison}
% \end{table}


