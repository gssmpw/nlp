\section{Related Work}
In this section, we provide a broad overview of self-supervised learning research that has inspired our work, along with recent trends in image clustering using pre-trained models.


\subsection{Self-Supervised Learning}
Self-supervised learning learns representations from data without explicit labels. The objective is to create a representation space where positive pairs are closer together, while negative pairs are pushed farther apart \cite{geiping2023cookbook}.

SimCLR \cite{chen2020simple} uses data augmentations, such as flipping and colour jittering, to create positive and negative pairs for optimizing objectives. It also introduces a projection head that maps embeddings into a space where contrastive loss is applied. BYOL \cite{grill2020bootstrap} shows that high-quality representations can be learned by simply maximizing agreement between two augmented views of the same input, without requiring negative pairs. Building on these advancements, SimSiam \cite{chen2020exploringsimplesiameserepresentation} eliminates the need for both negative pairs and momentum encoders by introducing a stop-gradient operation, which effectively prevents representational collapse. Inspired by these methods, we adopt similar ideas to develop a simple and effective self-supervised framework for image clustering.

\subsection{Pre-trained Models in Vision} 
Building on advances in self-supervised learning, CLIP \cite{radford2021learning} introduced a paradigm of contrastive pre-training that aligns images with corresponding textual descriptions. This approach enables broad task generalization without task-specific fine-tuning. DINO \cite{caron2021emerging}, which stands for self-distillation with no labels, demonstrates a self-supervised method for optimizing a student network from a teacher network based on vision input data only.

One of the key advantages of pre-trained models like CLIP is their ability to eliminate the need for training models from scratch for downstream tasks, significantly reducing computational costs and time. Instead of training a self-supervised neural network from the ground up, pre-trained models provide high-quality feature representations out of the box, leading to faster experimentation and improved performance on a variety of tasks. The scalability of CLIP has been further validated by openCLIP \cite{Cherti_2023}, which extended CLIP using the larger Vision Transformer models \cite{dosovitskiy2020image}. Similarly, models such as DINO~\cite{9709990} and DINOv2~\cite{oquab2024dinov2learningrobustvisual} are capable of processing visual data and mapping it to high-quality latent representations.

\subsection{Image Clustering via Pre-trained Models}
To address the challenges of scaling to modern image datasets, methods such as NMCE \cite{li2022neural} and MLC \cite{deng2023acp} have integrated deep learning with manifold clustering using the minimum coding rate principle \cite{Arthur_Vassilvitskii_2007}. Building on this idea, CPP \cite{chu2024image} further refines CLIP features and estimates the optimal number of clusters when unknown. TEMI \cite{adaloglou2023exploring} improves clustering by leveraging associations between image features, introducing a variant of pointwise mutual information with instance weighting. Unlike our approach, TEMI utilizes a nearest-neighbors set and an exponential moving average for parameter optimization.

SIC \cite{cai2023semantic} leverages multi-modality by mapping images to a semantic space and generating pseudo-labels based on image-semantic relationships. More recently, TAC~\cite{li2023image} utilizes the textual semantics of WordNet~\cite{miller1995wordnet} to enhance image clustering by selecting and retrieving nouns that best distinguish the images, facilitating collaboration between text and image modalities through mutual cross-modal neighborhood distillation.

Current pre-trained approaches often rely on heavy or complex architectures to ensure consistency, motivating us to develop a simple yet effective pipeline for image clustering. Our method requires only a simple clustering head and basic data augmentations, demonstrating strong competitiveness among recent models.







