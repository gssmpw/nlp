\section{Our Method}
\label{sec:Our Method}
Recent methods such as CC \cite{li2021contrastive} and CPP \cite{chu2024image} decouple the latent space into clustering and feature spaces. TAC \cite{li2023image} utilizes text information, and TEMI \cite{adaloglou2023exploring} employs self-distillation networks to enhance clustering. In contrast, our approach remains simple and efficient without relying on these techniques. As shown in Fig.~\ref{fig:AE}, SCP consists of only two components: a pre-trained frozen backbone for pair construction, denoted as $f(.)$, and a trainable cluster head $g(.)$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\columnwidth]{images/scp.png}
    \caption{%
    A overall pipeline for SCP. During training, two augmented views \(T^a\) and \(T^b\) of an image are generated from the dataset and processed by a frozen feature extractor \(f\) and a trainable cluster head \(g\) (a five-layer MLP). The objective is to minimize the cross-entropy loss between the outputs of the cluster head \(g\) for the two augmented views.
    }
    \label{fig:AE}
\end{figure}

Briefly, SCP performs data augmentations and extracts features from the augmented images using pre-trained models. The cluster head then projects these features into a cluster space, where the dimension equals the number of clusters. After training, outputs in the cluster space provide the soft assignments for clustering.


\subsection{Pair Construction Backbone}

The success of BYOL demonstrates that we can maximize the similarities of positive pairs without negative ones. In SCP, the positive pairs consist of samples augmented from the same instance.

Given a data instance \(x_i\), we apply two stochastic transformations \(T^a\) and \(T^b\), independently selected from the augmentation family \(\mathcal{T}\). This produces two correlated views: \(x_i^a = T^a(x_i)\) and \(x_i^b = T^b(x_i)\). 

An appropriate augmentation strategy is vital for better downstream performance. In our work, we adopt only two simple augmentations: \texttt{RandomCrop} and \texttt{GaussianBlur}. This choice aligns with the preprocessing techniques used in training pre-trained models, ensuring compatibility with their learned representations. \texttt{RandomCrop} randomly crops the image to a specified size, and \texttt{GaussianBlur} applies a Gaussian filter to blur the image. For each image, these two augmentations are applied independently, each with a 50\% probability. We then use a pre-trained model \(f(\cdot)\), such as CLIP, to extract features from the augmented images: $h_i^a = f(x_i^a)$ and $h_i^b = f(x_i^b).$

\subsection{Cluster Head}
Following the “label as representation” concept \cite{li2021contrastive}, when a data sample is projected into a space whose dimensionality matches the number of clusters \(K\), the \(k\)-th component of its feature vector (after applying a $\operatorname{softmax}$  function) can be interpreted as the probability that the sample belongs to the \(k\)-th cluster. We employ a five-layer non-linear MLP as the clustering head \(g(\cdot)\), producing a  \(K\)-dimensional feature that is normalized with a $\operatorname{softmax}$  over the dimension of the cluster.
\[
y_i^a = g(h_i^a), 
\quad 
y_i^b = g(h_i^b).
\]
Hence, \(y_i^a\) and \(y_i^b\) are both \(K\)-dimensional vectors, whose components \(y_{i,k}^a\) and \(y_{i,k}^b\) indicate the probability of assigning the \(i\)-th sample to the \(k\)-th cluster. Formally, let \(Y^a, Y^b \in \mathbb{R}^{N \times K}\) be the outputs of the clustering head for all samples. Then, we have the following matrices:
\[
% Y^a = \begin{bmatrix}
% y^a_1 \\
% y^a_2 \\
% \vdots \\
% y^a_N
% \end{bmatrix}
% \quad
% Y^b = \begin{bmatrix}
% y^b_1 \\
% y^b_2 \\
% \vdots \\
% y^b_N
% \end{bmatrix}.
Y^a = \begin{bmatrix}
y^i_1 \\
% y^a_2 \\
\vdots \\
y^a_N
\end{bmatrix}
\quad
Y^b = \begin{bmatrix}
y^b_1 \\
% y^b_2 \\
\vdots \\
y^b_N
\end{bmatrix}.
\]
To maximize row-wise similarity, we adopt the following cross-entropy loss function instead of the commonly used InfoNCE loss \cite{oord2018representation}, as SCP only have positive pairs that should share similar soft assignments:
\begin{equation}
L_e = - \sum_{i=1}^{N} \sum_{k=1}^{K} y^{a}_{i,k} \log y^{b}_{i,k}.
\end{equation}
Inspired by the effective regularizations in TAC \cite{li2023image}, we further introduce the following confidence loss to make the soft labels \( y^{a}_i \) and \( y^{b}_i \) more confident, approaching one-hot vectors:
\begin{equation}
L_{\text{con}} = - \log \sum_{i=1}^N {y^a_i}^\top y^b_i.
\end{equation}
This loss ensures that the cluster head assigns higher probabilities to its top predicted clusters, thereby increasing confidence in the assignments. 

In addition, following TAC \cite{li2023image}, we introduce an entropy term \( H(Y) \) to prevent model collapse, defined as follows:
\begin{equation}
H(Y) = - \sum_{k=1}^{K} \left[ P^{a}_k \log P^{a}_k + P^{b}_k \log P^{b}_k \right],
\end{equation}
where
\[
P^{a}_k = \frac{1}{N} \sum_{i=1}^{N} y^{a}_{i,k}, \quad P^{b}_k = \frac{1}{N} \sum_{i=1}^{N} y^{b}_{i,k}.
\]
$H(Y)$ encourages uniform soft assignments across clusters, thereby mitigating the issue of empty clusters. 

Hence, we define the overall objective function of SCP as
\begin{equation}
L_{\text{clu}} = L_e + L_{\text{con}} - \alpha H(Y),
\end{equation}
where the balancing weight $\alpha$ modulates the influence of $H(Y)$, especially when the number of clusters is large. By maximizing consistency between different augmented views with regularizations, SCP effectively prevents trivial solutions and achieves competitive performance. We provide algorithm \ref{alg:CAC} to explain our pipeline.

\begin{algorithm}[H]
\caption{Simple Clustering via Pre-trained Models (SCP)}
\label{alg:CAC}
\begin{algorithmic}[1]
    \Require Dataset $\mathcal{X} = \{ x_i \}_{i=1}^{N}$, Pre-trained model $f(\cdot)$, number of clusters $K$, batch size $B$, loss weight $\alpha$
    \State Initialize cluster head $g(\cdot)$
    \For{each epoch}
        \For{each mini-batch $\{x_i\}_{i=1}^{B}$}
            \State \textbf{Pair Construction:}
            \For{each data instance $x_i$ in the mini-batch}
                \State Apply stochastic transformations $T^a$, $T^b$ to obtain:
                \State \quad $x_i^a = T^a(x_i)$, \quad $x_i^b = T^b(x_i)$
                \State Extract features using pre-trained model:
                \State \quad $h_i^a = f(x_i^a)$, \quad $h_i^b = f(x_i^b)$
            \EndFor
            \State \textbf{Cluster Space Encoding:}
            \For{each feature $h_i^a$, $h_i^b$}
                \State Compute soft assignments: \quad $y_i^a = g(h_i^a)$, \quad $y_i^b = g(h_i^b)$
            \EndFor
            \State \textbf{Compute Losses:}
            \State Compute total clustering loss:
            \State \quad $L_{\text{clu}} = L_e + L_{\text{con}} - \alpha H(Y)$
            \State \textbf{Update} cluster head $g(\cdot)$ parameters by minimizing $L_{\text{clu}}$
        \EndFor
    \EndFor
    \State \Return soft assignments $y_i = g(h_i)$ for each $x_i \in \mathcal{X}$
\end{algorithmic}
\end{algorithm}


% Inspired by the work \cite{dwibedi2021little}, we recognize the advantages of incorporating positives from other instances in the dataset. Therefore, we construct a support set by sampling the nearest neighbours from the dataset in the CLIP feature space, treating them as additional positives. This approach provides extra semantic variations and aligns with the concept of stochastic neighbour embeddings. We assume that these nearest neighbours should be close enough as well after the projections.

% For each image feature \( h_i^a \), we find its \( M \) nearest neighbours \( \{ \tilde{h}_{i,j} \}_{j=1}^M \) in the set of \( \{ h_{i}^a \}_{i=1}^N \) and compute a weighted sum to obtain the aggregated neighbour representation \( \tilde{h}_i \):

% \begin{equation}
% \tilde{h}_i = \sum_{j=1}^M p(\tilde{h}_{i,j} \mid h_i^a) \, \tilde{h}_{i,j},
% \label{nnequ}
% \end{equation}

% where the weights \( p(\tilde{h}_{i,j} \mid h_i^a) \) are defined as:

% \begin{equation}
% p(\tilde{h}_{i,j} \mid h_i^a) = \frac{\exp\left( \text{sim}(h_i^a, \tilde{h}_{i,j}) / \tau \right)}{\sum_{k=1}^M \exp\left( \text{sim}(h_i^a, \tilde{h}_{i,k}) / \tau \right)}.
% \end{equation}

% Here, \( \text{sim}(\cdot, \cdot) \) denotes cosine similarity:

% \begin{equation}
% \text{sim}(h_i^a, \tilde{h}_{i,j}) = \frac{(h_i^a)^\top \tilde{h}_{i,j}}{\|h_i^a\| \, \|\tilde{h}_{i,j}\|},
% \end{equation}

% \( \tau \) is a temperature parameter, and \( M \) is the number of top nearest neighbours selected from  \( \{ h_{i}^a \}_{i=1}^N \). This weighting mechanism ensures that the influence of each neighbour is proportional to its similarity to the query image feature \( h_i^a \), preventing the nearest neighbours of different images from collapsing to the same point. We set the selected number of nearest neighbours as 10 for all experiments. To further clarify, this method allows each image to dynamically weigh its nearest neighbours based on similarity, enhancing the model's ability to capture fine-grained semantic relationships within the dataset. Finally, we incorporate these aggregated neighbour representations into the learning process by treating \( \tilde{h}_i \) as additional positive samples for \( h_i^a \). Thus, the support set loss is defined by:

% \begin{equation}
% L_s = - \sum_{i=1}^{N} \sum_{k=1}^{K} \tilde{y}_{i,k} \log y^{a}_{i,k},
% \end{equation}

% where \( \tilde{y}_{i} = g(\tilde{h}_{i}) \). This enhances the model's ability to learn from semantically nearest instances, improving clustering performance. To keep simplicity, we don't calculate the support set loss for another augmented view \( \{ h_{i}^b \}_{i=1}^N \).


% \subsection{Cluster Space Decoder}
% To encourage the encoder to learn meaningful and helpful latent representations and additionally avoid trivial solutions, we attach a decoder that aims to reconstruct the original image features. In summary, the cluster space decoder minimizes the reconstruction loss:

% \begin{equation}
% L_{d}=  - \sum_{i=1}^{N} \left( \text{sim}(h_i^a, h_{i}^{'a}) + \text{sim}(h_i^b, h_{i}^{'b}) \right),
% \end{equation}

% where $h_{i}^{'a} = d(y^{a}_{i})$ and $h_{i}^{'b} = d(y^{b}_{i})$, and $d(\cdot)$ denotes the decoder. $sim$ is still measured by cosine similarity.

% Finally, we arrive at the overall objective function of CAC, which takes the form:

% \begin{equation}
% L_{\text{CAC}} = \alpha \cdot L_{d} + L_s + L_{\text{clu}},
% \end{equation}

% where $\alpha = 0.01$ is a weight parameter set for all current experiments. This is small scale because it is expected that the decoder cannot perfectly reconstruct the same features, only knowing which cluster they belong to. 

% In section 4.5, we show our method is robust to different $\alpha$. Although the multi-task loss looks overwhelming there are fewer parameters to tune compared to previous works like \cite{li2023image}.

% In the end, we provide a pseudo code to better explain our pipeline:

% \begin{algorithm}[H]
% \caption{CAC: CLIP-based Auto-Encoder Clustering}
% \label{alg:CAC}
% \begin{algorithmic}[1]
% \Require Dataset $\mathcal{X} = \{ x_i \}_{i=1}^{N}$, number of clusters $K$, batch size $B$, number of nearest neighbors $M$, temperature parameter $\tau$, weight parameter $\alpha$
% \State Initialize encoder network $g(\cdot)$ and decoder network $d(\cdot)$
% \For{each epoch}
%     \For{each mini-batch $\{ x_i \}_{i=1}^{B}$}
%         \State \textbf{Pair Construction:}
%         \For{each data instance $x_i$ in the mini-batch}
%             \State Apply stochastic transformations $T^a$, $T^b$ to obtain:
%             \State \quad $x_i^a = T^a(x_i)$, \quad $x_i^b = T^b(x_i)$
%             \State Extract features using CLIP backbone:
%             \State \quad $h_i^a = f(x_i^a)$, \quad $h_i^b = f(x_i^b)$
%         \EndFor
%         \State \textbf{Cluster Space Encoding:}
%         \For{each feature $h_i^a$, $h_i^b$}
%             \State Compute soft labels: \quad $y_i^a = g(h_i^a)$, \quad $y_i^b = g(h_i^b)$
%         \EndFor
%         \For{each feature $h_i^a$}
%             \State Compute aggregated neighbor representation $h_i^N$
%             \State Compute soft label $y_i^N = g(h_i^N)$
%         \EndFor
%         \State \textbf{Cluster Space Decoding:}
%         \For{each soft label $y_i^a$, $y_i^b$}
%             \State Reconstruct features:
%             \State \quad $h_{i}^{'a} = d(y^{a}_{i})$, \quad $h_{i}^{'b} = d(y^{b}_{i})$
%         \EndFor
%         \State \textbf{Compute Total Loss:}
%         \State \quad $L_{\text{CAC}} = \alpha \cdot L_{d} + L_s + L_{\text{clu}}$
%         \State \textbf{Update} encoder $g(\cdot)$ and decoder $d(\cdot)$ parameters by minimizing $L_{\text{CAC}}$
%     \EndFor
% \EndFor
% \end{algorithmic}
% \end{algorithm}
% Clip-based Auto-Encoder Learning (CAEL) aims to learn a mapping from $R^{768}$ into $R^{512}$. We minimize the Euclidean distance in the latent space $R^512$ to preserve the original structure between two views of images because they represent the same semantic meaning:

% \begin{equation}
% l_1 = \frac{1}{N} \sum_{n=1}^{N}  \left\| g_{w}(h^a_n) - g_{w}(h^b_n) \right\|_2^2
% \end{equation}

% where the parameters $w$ of the encoder $g_w(\cdot)$ and parameter $\theta$ of the decoder $d_\theta(\cdot)$ are further jointly learned by optimizing the following problem to avoid collapse:

% \begin{equation}
% l_2 =  \frac{1}{2N} \sum_{n=1}^{N} \left( \left\|  h^{'a}_n - h^a_n \right\|_2^2 + \left\|  h^{'b}_n - h^b_n \right\|_2^2 \right),
% \end{equation}

% where $ h^{'a}_n = f_{\theta}(g_{W}(h^a_n))$, $ h^{'b}_n = f_{\theta}(g_{W}(h^b_n))$. 

% Furthermore, inspired by the pioneering work \cite{dwibedi2021little} as well as the similar idea of stochastic neighbour embedding, we construct the support set $S$ from the CLIP representations $h^b$, where $S = \{ {h^b}_n \}_{n=1}^N$. Thus, we aim to find the nearest neighbour of $h^a$ in the support set $S$, then minimize the Euclidean distance between the view $h^a$ and its nearest neighbour $S_{\text{near}}: h^a_{\text{near}} \in S$. (For now, it is not plotted in figure 2)

% \begin{equation}
% l_3 = \frac{1}{N} \sum_{n=1}^{N} \left\| g_{w}(S_{\text{near}}) - g_{w}(h^b_n) \right\|_2^2
% \end{equation}

% Thus, for $g_w(\cdot)$ and $d_\theta(\cdot)$, our method is trying to optimize the following objection functions:

% \begin{equation}
% \min_{w,\theta} l_1 + l_2 + \alpha l_3
% \end{equation}

% we set $\alpha = 0.1$ after analysis. 

% After sufficient training, such as 150 epochs on a benchmark dataset, for the images in the test set $Y$, we obtain their CLIP representations $Y_c = f(Y)$ without the need for data augmentation. We then use the encoder to embed $Y_c$ into $Z_c = g(Y_c)$. Finally, we perform K-means clustering on $Z_c$ to obtain the clusters $\mathbf{C}$, represented by an $M \times k$ one-hot matrix, where $k$ is the number of clusters and $M$ is the number of instances in the test set.

% 

% \begin{algorithm}
% \caption{Clip-based Auto-Encoder Learning (CAEL)}
% \begin{algorithmic}[1]
% \State \textbf{Input:} Dataset \(X\); Two augmented views \(x^a\) and \(x^b\); Number of epochs \(T\); Number of epochs \(T_{\text{refine}}\); Support set \(S = \{ h^b_n \}_{n=1}^{N}\); Weights \(\alpha\); CLIP model \(f(\cdot)\); Encoder \(g_w(\cdot)\); Decoder \(d_\theta(\cdot)\); Projector \(f_p(\cdot)\); Identity matrix \(I_k\)
% \State \textbf{Output:} Cluster assignments \(\mathbf{C}\)

% \vspace{0.3cm}
% -------------------------------------------------------------

% \State \textit{Phase 1: Training}
% \vspace{0.3cm}

% \State Initialize parameters \(w\) and \(\theta\)
% \For{each epoch \(t = 1\) to \(T\)}
%     \For{each image \(x \in X\)}
%         \State Obtain CLIP representations: \(h^a = f(x^a)\) and \(h^b = f(x^b)\)
        
%         \vspace{0.3cm}
%         \State Obtain latent representations: \(z^a = g_w(h^a)\) and \(z^b = g_w(h^b)\)
%          \State Compute loss \(l_1 = \frac{1}{N} \sum_{n=1}^{N} \left\| g_w(h^a_n) - g_w(h^b_n) \right\|_2^2\)

%           \vspace{0.3cm}
    
%         \State Obtain reconstruct representations: \(h^{'a} = d_\theta(z^a)\) and \(h^{'b} = d_\theta(z^b)\)
%           \State Compute loss \(l_2 = \frac{1}{2N} \sum_{n=1}^{N} \left( \left\| h^{'a}_n - h^a_n \right\|_2^2 + \left\| h^{'b}_n - h^b_n \right\|_2^2 \right)\)
    
%         \vspace{0.3cm}

%              \State Find nearest neighbor \(S_{\text{near}} = h^a_{\text{near}} \in S\)
%         \State Compute loss \(l_3 = \frac{1}{N} \sum_{n=1}^{N} \left\| g_w(S_{\text{near}}) - g_w(h^b_n) \right\|_2^2\)
        
%          \vspace{0.3cm}
        

        
%         \vspace{0.3cm}
        
%         \State Update parameters \(w\) and \(\theta\) by minimizing \(l = l_1 + l_2 + \alpha l_3\)
%         \vspace{0.3cm}
        
%     \EndFor
% \EndFor

% \vspace{0.3cm}
% -------------------------------------------------------------

% \State \textit{Phase 2: Clustering and refinement}
% \vspace{0.3cm}


% \State Obtain CLIP representations without data augmentation  \(H = f(X)\)
% \vspace{0.3cm}
% \State Obtain latent representations: \(Z_c = g_w(H)\)
% \vspace{0.3cm}
% \State Perform K-means clustering on \(Z_c\) to get initial clusters \(\mathbf{C}\)
% \vspace{0.3cm}

% \For{each epoch \(t = 1\) to \(T_{\text{refine}}\)}
%     \For{each latent representation \(z_n \in Z_c\)}
%     \vspace{0.3cm}
    
%         \State obtain \(k\)-simplex representation: \(P_c = f_p(Z_c)\)
%         \State Compute the loss \(l_{\text{refine}} = \frac{1}{M} \sum_{m=1}^{M} \left\| P_c - \mathbf{C}_{m \times k} \right\|_2^p\)
%         \vspace{0.3cm}
%         \State Update parameters of \(f_p\) to minimize \(l_{\text{refine}}\)
        
%         \vspace{0.3cm}
% \EndFor
% \EndFor
% \vspace{0.3cm}
% \State Reassign instances to clusters based on the nearest distance to identity matrix \(I_k\):
% \State \(C_k \gets \{z_n : \|P_c - \mathbf{e}_k\| \le \min_{j=1,\dots,k} \|P_c - \mathbf{e}_j\| \}\)
% \vspace{0.3cm}
% \State \Return \(\mathbf{C}\)
% \end{algorithmic}
% \end{algorithm}
