\section{Introduction}
Powered by the expressive capabilities of neural networks, deep clustering~(DC) has redefined the state of the art~(SOTA) in image clustering, outperforming traditional algorithms by indisputable margins. These DC pipelines uncover subtle features that can significantly enhance the power of downstream learners. A highly non-exhaustive list of algorithms redefining their respective SOTA includes DC-powered methods for community detection~\cite{saeedi2018novel, yaroslavtsev2018massively}, anomaly detection~\cite{pang2021deep}, image segmentation~\cite{minaee2021image, liang2023clustsegclusteringuniversalsegmentation}, object detection~\cite{zhao2019object, kim2024garfieldgroupradiancefields}, and medical applications~\cite{zhao2020joint}. However, many recent DC pipelines rely on \textit{massive} models, often powered by large language models~(LLMs), which may be unnecessarily compute-intensive for clustering tasks, as well as complicated training schemes. This motivates our main research question: 

\begin{equation}
\label{Q1}
\tag{Q1}
% \small
\resizebox{0.5\hsize}{!}{$
    \mbox{\textit{Is SOTA performance achievable with a simple DC pipeline?}}
$}
\end{equation}

Indeed, we show that competitive performance, SOTA and near-SOTA, can be achieved using only a small adapter modifying the features generated by a pre-trained text-free, image encoder, i.e.\ not requiring any additional input text from the user, LLMs, or any other variants of text encoders.  The result is a \textit{lightweight} and \textit{simple} end-to-end DC pipeline, which is computationally cheap enough to be run on a standard L4 GPU. Hence, our framework is easy to deploy in real-world tasks, as it is computationally accessible to most practitioners and does not require text-image pairs, which may not always be readily available in practical clustering applications.

\subsection{Theoretical Motivation}

The intuition behind why a text-free classifier should, at least theoretically, be able to match the power of a classifier using text embeddings together with the raw pixel data is rooted in our theoretical results. If $X$ represents a random image and $Z$ is its corresponding semantic description, which itself is a compressed representation of the information \textit{only in} the random image $X$, then any classification task $Y$ that depends on both the image $X$ and its textual information can ultimately be expressed as a function of the image $X$ alone.  
We denote the $\sigma$-algebra generated by the random variable $X$ by $\sigma(X)$.
\begin{proposition}[\textbf{Lossless Amortization Principle (LAR)}]
\label{prop:LosslessAmort}
Fix $d,D,C\in \mathbb{N}$.
Let $X,Z$ be random variables respectively, taking values in $\mathbb{R}^d$ and in $\mathbb{R}^D$, both of which are defined on a common probability space $(\Omega,\mathcal{F},\mathbb{P})$, and suppose that $Z$ is $\sigma(X)$-measurable.  
For every $\{0,1,\dots,C-1\}$-valued random variable $Y$ on $(\Omega,\mathcal{F},\mathbb{P})$ if:
there exists a Borel measurable function $f:
(\mathbb{R}^{d+D},\mathcal{B}(\mathbb{R}^{d+D})) \to (\mathbb{R}^d,\mathcal{B}(\mathbb{R}^d))$ representing $Y$ by
\begin{equation}
\label{eq:uncompressed}
% \tag{Class}
\underbrace{
Y = 
f(X,Z)
}_{\text{Depends both on $X$ and $Z$}}
\end{equation}
then, there is a Borel map $F: (\mathbb{R}^d,\mathcal{B}(\mathbb{R}^d))\to 
(\mathbb{R}^n,\mathcal{B}(\mathbb{R}^n))$ providing the following lossless amortized representation
\begin{equation}
\label{eq:compression}
\tag{LAR}
% \underbrace{
    % \mathbb{E}[
    Y
    % |X] 
% }_{\text{Depends both on $X$ and $Z$}}
= 
\underbrace{
    F(X)
.
}_{\text{Only depends on $X$}}
\end{equation}
\end{proposition}
\begin{proof}
See Appendix~\ref{s:Proofs}.
\end{proof}

Further, the proposition above seems to align with the \textit{Platonic Representation Hypothesis} proposed in~\citep{pmlr-v235-huh24a}, which suggests that latent representations in deep networks converge despite being trained on distinct data modalities. This supports our claim that, as long as the backbone vision model provides sufficiently good latent representations, the addition of extra modality inputs may be unnecessary.

Note that we acknowledge that, generally speaking, using multimodal representations and additional information for our networks to make predictions empirically tends to yield better results. Rather than disputing this, we aim to theorize whether, in principle and under ideal conditions, clustering could be achieved using the simplest model possible, without cumbersome multimodal data collection.


Our LAR principle (Proposition~\ref{prop:LosslessAmort}) shows that any semantically-powered image classifier may be realized by a text-free classifier ``$F$''.  However, \textit{can this theoretical classifier be practically realized, at-least approximately?}
Our main theoretical result guarantees that the theoretical text-free classifiers can indeed be approximately implemented by an MLP with a $\operatorname{softmax}$ output activation.  Our guarantee holds for multiclass MLP classifiers using any standard activation function such as $\operatorname{ReLU}$, $\operatorname{Swish}$, or $\operatorname{softplus}$.

\begin{theorem}[\textbf{Text-Free DC is Powerful Enough}]
\label{thrm:DCplus}
In the setting of Proposition~\ref{prop:LosslessAmort}, let $\rho:\mathbb{R}\to \mathbb{R}$ be an activation function with at least one point of continuous and non-zero differentiability.  
Then, for every $0<\varepsilon\le 1$, there is an MLP $\hat{F}:\mathbb{R}^d\to \mathbb{R}^C$, with activation function $\rho$, satisfying
\begin{equation}
        \big|
            \underbrace{
                f(X,Z)
            }_{\text{text-dependent classes}}
            -
            \underbrace{
                \,\,
                \operatorname{softmax}\circ \hat{F}(X)
            }_{\text{text-free deep learner}}
        \big|
    <
        \varepsilon
\end{equation}
with probability at least $1-\varepsilon$.
\end{theorem}
\begin{proof}
See Appendix~\ref{s:Proofs}.
\end{proof}

\subsection{Simplifying Contemporary Deep Clustering}

Modern clustering pipelines, e.g.\ \cite{van2020scan,ding2023unsupervised} usually involve: \textbf{(i)} learning powerful initial latent representations through self-supervised techniques, e.g.\ joint-embedding approaches \cite{chen2020simple,he2020momentum}, and \textbf{(ii)} gradually refining the representation and clustering membership by minimizing an objective function \cite{caron2019deepclusteringunsupervisedlearning}. 

\paragraph{Representation Learning in Deep Clustering} 
The emergence of large-scale pre-trained models optimized according to a contrastive learning objective using both natural language such as CLIP~\cite{radford2021learning} and purely image-based encoders like DINO~\cite{caron2021emerging,oquab2024dinov2learningrobustvisual}, has proven highly effective in capturing rich, general-purpose feature representations for a variety of downstream tasks. 
Most SOTA-achieving DC pipelines leverage these powerful representations alongside text-embedding~\cite{li2023image,cai2023semantic} (the $Z$ in Theorem~\ref{thrm:DCplus}) for effective latent feature alignment.  However, as we will see in Section~\ref{sec:Experiments}, these text-based representations seem to contain redundant features, given that similar performance can be achieved without them. As we have already mentioned, being able to eliminate the need for text would make downstream applications, particularly in data-scarce regimes, much more accessible to the end user.
% While this approach adds multimodal connections, it also greatly increases the computational overhead and complexity. 
% Moreover, reliance on text data limits flexibility, making extending to purely advanced vision-based models difficult. 

% Briefly, CLIP has exhibited remarkable performance in diverse visual tasks, whereas DINO and DINO-v2 are trained on images without a paired text encoder, indicating their potential as a strong text-free backbone for clustering applications that require robust visual feature extraction.



\paragraph{Self-Supervised Training}
Typically, modern DC pipelines are trained using self-supervised methods by optimizing contrastive losses~\cite{oord2018representation}. While contrastive methods rely on both positive and negative pairs, other approaches like BYOL~\cite{grill2020bootstrap} have demonstrated that it is possible to learn effective representations without negative pairs by simply maximizing the agreement between two views of the same data. This suggests that maximizing similarities of positive pairs alone can yield high-quality representations. We will follow a similar approach in this work too, see Section~\ref{sec:Our Method}.


% Additionally, several contemporary DC pipeline~\cite{li2023image,cai2023semantic} incorporate external text data to enhance performance. While this approach adds multimodal connections, it also greatly increases the computational overhead and complexity. Moreover, reliance on text data limits flexibility, making extending to purely advanced vision-based models difficult. 
% Our pipeline, however, demonstrates that the potential of internal image information alone is underestimated by current methods.

\paragraph{From Theory to Practice} Even if the model is approximately optimal and its parameters are perfectly adjusted, it is not clear that any real-world unsupervised training procedure can achieve the theoretical expressivity guaranteed by Theorem~\ref{thrm:DCplus}. Hence, we would like to empirically answer whether \textit{unimodal text-free DC pipelines can rival multimodal ones}. We would like to emphasize that obtaining powerful DC performance, with the additional requirement that our DC pipeline is simple, unimodal, and text-free, is highly nontrivial since even the current SOTA in deep clustering exhibits a \textit{substantial} performance gap with respect to supervised image classification models.  Indeed, supervised methods typically achieve accuracies well-above $90\%$ on image datasets, while unsupervised clustering approaches rarely surpassed 50\% clustering accuracy on CIFAR-20, and many fall below $50\%$~\cite{dang2021nearest,li2021contrastive,shen2021you,niu2022spice}. This is because most unsupervised clustering pipelines struggle with complex natural images due to high intra-class variability.

% To address the challenges posed by clustering complex natural image datasets and maintain simple and light-scale structures, we leverage advances in pre-trained models and self-supervised learning to develop a novel pipeline named  

\paragraph{Contributions} To address the challenges discussed so far, we propose Simple Clustering via Pre-trained models (SCP). Our main contributions are as follows:

\begin{itemize}
\item \textbf{Simple DC Pipeline:}
Our method yields competitive results while maintaining a simple yet effective architecture, requiring no additional feature layers, support sets, teacher-student networks, text information, or exponential moving averages.
\item \textbf{Text-Free (Unimodal) Encodings:}
We integrate the powerful image encoder from CLIP (and also experiment with DINO) into a self-supervised clustering framework and show that this approach achieves competitive clustering performance on benchmark datasets such as CIFAR-10 \cite{krizhevsky2009learning}, CIFAR-20 \cite{krizhevsky2009learning} and STL-10 \cite{krizhevsky2009learning} to current SOTA.
\item \textbf{Principled Approach:} 
Our text-free approach is principled, based on the approximate representation result previously presented in Theorem~\ref{thrm:DCplus}, which guarantees that text-free embeddings are theoretically enough.
\end{itemize}








% Although many promising results have been achieved, most classical algorithms yield inferior results on complex datasets due to insufficient representability. Deep clustering employs neural networks to extract image information to improve performance. So far, few methods have achieved above 50\% clustering accuracy on ImageNet-1k or TinyImageNet-200: e.g., Van Gansbeke et al. (2020a); Li et al. (2021); Niu \& Wang (2021); Sadeghi et al. (2022); Ding et al. (2023) all achieve accuracy smaller than 50\%.

% For example, DeepCluster \cite{caron2018deep} iteratively groups features with k-means and utilizes pseudo-labels to update the deep network. However, this iterative learning method may suffer from incorrect clustering results, leading to sub-optimal performance. Recently, foundation models, such as CLIP \cite{radford2021learning}, have been introduced in image clustering. CLIP maps images and texts into a unified space, but simply mapping images to the nearest semantics does not provide good results. A few works were proposed to absorb text information as internal or external guidance to help clustering. But it forced to input some prior text or introduce extra workflows such as word filtering and word embedding. These pioneering methods have undoubtedly made remarkable progress. However, they often involve a degree of intricate engineering and parameter tuning. While these complexities are inherent to their design and have enabled them to achieve their goals, they may present challenges when implementing and scaling these methods to larger datasets.
% Therefore, in the task of image clustering with foundation models, we wanted to solve two key problems:
% \begin{enumerate}
%     \item How to integrate with foundation models to learn proper representations that can improve clustering?
%     \item Can we still use the image encoder individually to achieve comparable results?
%     % (Or it could be: what is the efficient way to apply text information during clustering.)
% \end{enumerate}
% To address these issues, we propose an image clustering method named Clip-based Auto-Encoder Learning (CAEL) (Temporary name). It shares the idea of self-supervised learning, maps the given images to a latent space and clustering via performing K-means on those representations. Our main contributions are summarized as follows:
% \begin{itemize}
%     \item Unlike existing models using internal or external text information, we create a straightforward pipeline that learns representations without any prior text information. Despite this, our approach still achieves competitive results on some benchmark datasets, compared to several state-of-the-art methods and zero-shot learning with CLIP.


    
%     \item The significance of CAEL is two-fold. On the one hand, it proves the effectiveness and superiority of the proposed straightforward clustering paradigm without text information. On the other hand, it suggests the presence of more simple but effective strategies for mining the zero-shot learning ability inherent in VLP models. (Copy from ICML paper)
    
%     % \item The novel loss of the support set takes advantage of the idea of stochastic neighbour embedding.
%     % \item We come up with a light-scale and helpful clustering refinement strategy.
%     % \item Experimental results on five benchmark datasets clearly show that CAEL is comparable to several state-of-the-art methods and zero-shot learning with CLIP.
    
% \end{itemize}