%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
\usepackage{hyperref}
\usepackage{url}
% \usepackage{floatrow}
\usepackage{graphicx}
\usepackage{amsmath}
% \usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{multirow}
\usepackage[lined,ruled]{algorithm2e}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage[nomargin,inline,marginclue,draft]{fixme}
\usepackage{enumitem}
\usepackage{subfig}
\usepackage{multirow}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\renewcommand\footnotetextcopyrightpermission[1]{}
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
% %% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}
% %%
% %%  Uncomment \acmBooktitle if the title of the proceedings is different
% %%  from ``Proceedings of ...''!
% %%
% %%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
% %%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{A Large-scale Dataset with Behavior, Attributes, and Content of Mobile Short-video Platform}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

\author{Yu Shang}
\affiliation{%
  \institution{Department of Electronic Engineering\\ Tsinghua University}
  \city{Beijing}
  \country{China}}
\email{shangy21@mails.tsinghua.edu.cn}

\author{Chen Gao}
\affiliation{%
  \institution{BNRist\\ Tsinghua University}
  \city{Beijing}
  \country{China}}
\email{chgao96@tsinghua.edu.cn}


\author{Nian Li}
\affiliation{%
  \institution{Department of Electronic Engineering\\ Tsinghua University}
  \city{Beijing}
  \country{China}}
\email{linian21@mails.tsinghua.edu.cn}

\author{Yong Li}
\affiliation{%
  \institution{Department of Electronic Engineering\\ Tsinghua University}
  \city{Beijing}
  \country{China}}
\email{liyong07@tsinghua.edu.cn}


%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
% \renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
   Short-video platforms show an increasing impact on peopleâ€™s daily lives nowadays, with billions of active users spending plenty of time each day. The interactions between users and online platforms give rise to many scientific problems across computational social science and artificial intelligence. However, despite the rapid development of short-video platforms, currently there are serious shortcomings in existing relevant datasets on three aspects: inadequate user-video feedback, limited user attributes and lack of video content. To address these problems, we provide a large-scale dataset with rich user behavior, attributes and video content from a real mobile short-video platform. This dataset covers 10,000 voluntary users and 153,561 videos, and we conduct four-fold technical validations of the dataset. First, we verify the richness of the behavior and attribute data. Second, we confirm the representing ability of the content features. 
   Third, we provide benchmarking results on recommendation algorithms with our dataset. Finally, we explore the filter bubble phenomenon on the platform using the dataset.
   We believe the dataset could support the broad research community, including but not limited to user modeling, social science, human behavior understanding, etc. 
   The dataset and code is available at \href{https://github.com/tsinghua-fib-lab/ShortVideo_dataset}{https://github.com/tsinghua-fib-lab/ShortVideo\_dataset}.
% Our full dataset is available at \href{http://101.6.70.16:8080/}{http://101.6.70.16:8080/}\footnote{Username: videodata Password: ShortVideo@10000} and the code is available at \href{https://github.com/tsinghua-fib-lab/ShortVideo_dataset}{https://github.com/tsinghua-fib-lab/ShortVideo\_dataset}.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large-scale dataset, Behavior, Attributes, Video content, Mobile short-video platform}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Short-video platforms such as Tiktok are redefining how users access information on the web. 
% It is reported by Demand Sage\footnote{https://www.demandsage.com/tiktok-user-statistics/} that Tiktok has attracted 1 billion active users who spend 1.5 hours each day on the platform on average.
With the widely deployed algorithms of personalized recommendation, the platform can infer user preferences from users' behavioral logs and users' demographics, which helps alleviate information overload.
However, the intelligence algorithm has also brought critical social concerns including echo chamber~\citep{ge2020understanding}, filter bubble~\citep{nguyen2014exploring}, user addiction~\citep{montag2018internet}, etc. 
% For example, it has been demonstrated that recommendation algorithms might lean to recommend popular items~\citep{abdollahpouri2020multi} and this algorithmic bias could be amplified over time when users interact with recommender systems.
% The bias, in turn, sets constraints to the preference learning of algorithms, leading to the \textit{feedback loop}~\citep{mansoury2020feedback}, which further causes these concerns. 

Despite the fast growth of short-video platforms, there is no publicly available dataset that can well support the research of improving user modeling and alleviating the negative impact caused by AI algorithms.
The existing public datasets of short-video platforms are released by researchers and engineers of recommendation algorithms. However, they only focus on user feedback while are quite limited in supporting the research areas beyond recommendation algorithms.
In specific, 
% MicroVideo-1.7M~\citep{chen2018temporal} records millions of interactions between users and videos with 10,986 users and 1,704,880 for micro-video recommendation tasks. 
KuaiRec~\citep{gao2022kuairec} is a fully observed micro-video dataset with additional user and video attributes collected from Kuaishou platform. 
REASONER~\citep{chen2023reasoner} is an explainable recommendation dataset, containing some basic attributes and ground truths for multiple explanation purposes. 
Tenrec~\citep{yuantenrec} is a large-scale multipurpose benchmark dataset for recommendation tasks, which also contains interactions between users and videos.
MicroLens~\citep{ni2023content} is a recently released micro-video dataset with various modality content of videos.
Although these datasets may support the research of machine learning models for personalized user modeling or recommendation. 
% they have not well covered the critical content such as video content and rich user attributes, leading to limited value for broader research, such as the study of filter bubble or user addiction.
They are suffering from limitations including (1) lack of video content (2) inadequate user-video feedback and (3) the shortage of user attributes.
% The video content is the most informative video-side data, which is neglected by most existing datasets, in which there is only coarse-grained categorical information of videos is provided.
% \item \textbf{Inadequate user-video feedback.} The existing datasets have limitations in the scale of the user set or item set, and the interaction number is limited. 
% For example, REASONER~\citep{yuantenrec} covers only 2,997 users, 4,672 videos and 58,497 interactions. KuaiRec~\citep{gao2022kuairec} includes only 1,411 users, 3,327 videos, and 4,676,570 interactions, in which user feedback is limited to play-finished and like.
% \item \textbf{Shortage of user attributes.} User attributes \textit{i.e., demographics} are the basis of the research of understanding and further addressing the social-concerned issues in short-video platforms such as user addiction, however, these data are not sufficient in most existing datasets such as MicroVideo-1.7M~\citep{chen2018temporal}, Tenrec~\citep{yuantenrec} and MicroLens~\citep{ni2023content}.
% \end{itemize}

\begin{figure*}[t!]
\centering
% \vspace{-8mm}
\subfloat[User interface and behaviors on the platform.]{\includegraphics[width=0.5\linewidth]{fig/interface2.pdf}\label{fig:interface}}
\subfloat[Dataset Overview.]
{\includegraphics[width=0.5\linewidth]{fig/data_overview.pdf}\label{fig:data_overview}
}
\caption{The illustration of user interface and behaviors on the platform (a) and an overview of the dataset (b).}
% \vspace{-11mm}
\label{fig:data_overview}
% \vspace{-8mm}
\end{figure*}

In this paper, we provide a large-scale short-video dataset from 10,000 volunteers on one of the largest short-video platforms. The data collection procedure strictly follows privacy and ethical regulations. 
In terms of data scale, our dataset covers 1,019,568 interactions between 10,000 users and 153,561 videos, which is larger than most existing released datasets, e.g., Kuairec~\citep{gao2022kuairec} and REASONER~\citep{chen2023reasoner}.
% In real industrial scenarios, it is common to first employ an offline dataset with a relatively small scale to conduct experiments validating the effectiveness of proposed methods and then extend to full-scale online testing, which has been practiced by many works for industrial applications[3,4]. 
Besides, compared with datasets used in real industrial scenarios, our dataset has a similar scale to the datasets used to conduct offline experiments and analysis, such as the dataset of WeChat Channel~\footnote{https://algo.weixin.qq.com/2021/problem-description} and Tiktok~\footnote{https://www.biendata.xyz/competition/icmechallenge2019}. 
Therefore, our dataset can satisfy the requirements of both academic and industrial scenarios.
Our dataset outperforms existing public datasets in three aspects: user behavior, user/video attributes, and video content, which is detailed in Section 2. 
% It includes rich user behavior data such as historical viewing records, six types of explicit feedback (like, follow, forward, collect, comment, hate), and implicit feedback (watching time). 
% For attributes, we gather extensive user and video information often overlooked by prior datasets. User attributes include demographic data (age, gender), geographical details (city, community), and other factors (e.g., phone price). Video attributes include three-level content categories, author info (ID, fan count), and video title/tags. In contrast, previous datasets~\citep{chen2018temporal,li2019routing} cover only a subset, such as age, gender, and single-level categories.
% In terms of content data, our dataset provides the raw video files which are very rare in existing datasets but essential for describing and understanding video content. Moreover, we provide extracted visual features and ASR results of videos for convenient use.
We further conduct a four-fold validation of the dataset usability in Section 3, including data distribution analysis, video content quality validation, benchmarking recommendation algorithms and the filter bubble phenomenon study.
% Finally, we discuss how our dataset benefits the broad research community from three aspects: user modeling, social science, and human behavior understanding. 
Our key contributions are as follows:
\begin{itemize}[leftmargin=*]
    \item We collect a large-scale dataset from a real mobile short-video platform, covering rich user behavior, attributes and video content, which are scarce in existing datasets.
    \item We conduct comprehensive technical validation of the wide coverage of behavior and attribute data and the quality of content data in the dataset.
    \item We provide a sufficient exploration of the potential research directions with our dataset, covering broad research communities such as user modeling and studying filter bubbles.
\end{itemize}

% \begin{figure}[t!]
% \centering
% {\includegraphics[width=\linewidth]{fig/interface2.pdf}
% }
% % \vspace{-5mm}
% \caption{Illustration of the user interface and behaviors on the platform.}
% \label{fig:interface}
% % \vspace{-3mm}
% \end{figure}

\section{Data Description}
We collected six months of interaction data from 10,000 hired volunteers (excluding users under 20) with their consent, in the paper we focus on analyzing the first weekâ€™s data for a quick release. 
% All participants signed informed consent forms, agreeing to the use of anonymized data for research. 
% They have the right to opt out and request to remove their data at any time.
% User IDs were hashed and mapped to numbers between 1 and 10,000 to protect anonymity. 
% The data collection was conducted strictly following the local ethical regulations which have similar privacy regulations as GDPR and was supervised by the Science and Technology Ethics Committee.
% Our dataset possesses much richer information than existing datasets on behavior, attribute and content data. 
% First, the dataset contains large-scale interaction records and multi-type user behavior, which are helpful for user interest modeling and behavior analysis. Second, the dataset contains various user attribute data which has never been covered by existing datasets. Third, the video content is available in the dataset, which could support further information extraction and analysis. 
An overview of the dataset is shown in Figure \ref{fig:data_overview}. 
% The structure and detailed data description are presented in the supplementary material.


% \floatsetup[figure]{style=plain,subcapbesideposition=top}

\subsection{Behavior Data}
% There are 153,561 involved videos and 1,019,568 interaction records. 
% The behavior data is stored in \emph{video\_rec.csv}, in which each row represents one interaction. 
We asked the volunteers to install a proxy agent on their mobile devices to record the interactions, which was fully acknowledged by the volunteers.
For each interaction record, we record the basic information including anonymous user/video ID and timestamp of the interaction. More importantly, we have collected rich user behavior which could convey diverse preference signals of users including explicit feedback (like, comment, follow, forward, collect and hate behavior) and implicit feedback (watching time).  
% On the platform we use, users can watch a variety of short-form videos, which are organized by recommending streaming where each time the user can see only one video, which is known as the single-column user interface. In this scenario, the video will automatically play, during which users can choose to stop viewing the current video by exiting this interface or switching to the next video. 
% There are mainly two types of feedback in this scenario: implicit feedback and explicit feedback.

% Implicit feedback refers to the user's watching behavior, more specifically, the watching time, which is a vital signal to yield user satisfaction~\citep{zheng2022dvr,liu2021concept}. 
% Traditional datasets usually focus on discrete user feedback, such as ratings ~\citep{koren2008factorization,koren2009matrix} and click behavior~\citep{liu2020autofis,song2019autoint}. 
% Differently, in the scenario of short-video platforms, users will watch the current video recurrently until they switch to the next video as Figure~\ref{fig:interface} shows, in which case user engagement is mainly reflected on the continuous watching time~\citep{salganik2006experimental}. Intuitively, users tend to continue watching if they are interested in the current video, otherwise, they will skip to the next video. Put another way, the watching time could serve as a critical indicator of user interest.
% The behavior data not only records the precise watching time of each interaction but also provides an "effective view" label by comparing watching time and a threshold (set as 3 seconds in our dataset). 
\par
% Explicit feedback, provided by users actively, usually conveys clear preference or disgust. For example, if the user collects or forwards a video, we could infer that they are interested in the video content with high confidence. Conversely, if a user conveys hate to the video, it could be assumed reasonably that they show no interest in it.
% The behavior data records 6 kinds of explicit feedback including like, comment, follow, forward, collect and hate. 
% These feedback signals correspond to the buttons in the single-column user interface as shown in Figure \ref{fig:interface}. 
% We present the distribution of 6 kinds of explicit feedback in the supplementary material, which is sparse while effective in conveying user interest.
% The structure and detailed description of the behavior data is shown in Table~\ref{tab:behavior}.

\subsection{Attribute Data}
Compared with existing datasets, the dataset contains richer attribute information about the users and videos. Specifically, there are 9 video-side attributes and 6 user-side attributes in the dataset.

For the video side, we have fully exploited the key attributes of micro-videos including video category, author information, duration, video text and video tag. 
\begin{itemize}[leftmargin=*]
\item \textbf{Video category}. 
% According to the regulation of the short-video platform, video authors are required to provide the title and tag for uploaded videos for better management and recommendation. 
We establish the hierarchical categories by conducting K-means clustering of the video title and manually labeling their categories, which can cover most of the collected videos. 
The category of videos is divided into 3 levels hierarchically (Category \uppercase\expandafter{\romannumeral1}, Category \uppercase\expandafter{\romannumeral2}, Category \uppercase\expandafter{\romannumeral3}). 
For example, for a video recording a \textit{hockey game}, Category \uppercase\expandafter{\romannumeral1} is ``\textit{sports}'', Category \uppercase\expandafter{\romannumeral2} is ``\textit{ball game}'' and Category \uppercase\expandafter{\romannumeral3} is ``\textit{hockey}''. Statistically there are 37 kinds of Category \uppercase\expandafter{\romannumeral1}, 281 kinds of Category \uppercase\expandafter{\romannumeral2} and 382 kinds of Category \uppercase\expandafter{\romannumeral3}. 
% It is worth mentioning that the number of Category \uppercase\expandafter{\romannumeral2} and Category \uppercase\expandafter{\romannumeral3} are similar since some Category \uppercase\expandafter{\romannumeral2} could be further divided into several Category \uppercase\expandafter{\romannumeral3} while some others could not. 
The hierarchical category organization possesses richer semantic information compared with existing datasets, which could support multi-level video content understanding. 
% In fact, both coarse-grained and fine-grained categories are indispensable. For example, in video advertising, the coarse-grained category is more appropriate to enlarge the audience group. While in tasks such as video retrieval, fine-grained categories are necessary to ensure retrieval accuracy. Besides, the relation between different categories is valuable, for example, if a user likes videos about delicious food, the platform could reasonably recommend videos about cooking, improving the diversity of the recommendation list. 

\item \textbf{Author information}. 
The dataset includes 81,870 video authors, with collected information such as author ID and number of fans.
\item \textbf{Duration}. The dataset includes videos of varying durations, ranging from under 30 seconds to over 5 minutes, with most being relatively short (less than one minute). 

\item \textbf{Video title and tags}. Our dataset includes 140,341 unique titles across all videos. Video tag is added manually by authors shown in each video for brief video descriptions, such as "\textit{delicious food recommendation}" and "\textit{travel tips}". 
% Note that video tags are collected from the platform while categories are processed and classified through our human effort. 
 % In practice, it has been utilized to summarize video content and support video understanding~\citep{shamma2007watch,mazeikawould}.
 There are a total of 79,705 unique tags in all collected videos. 
\end{itemize}

For the user side data, the dataset records comprehensive user attributes including demographical, and geographical characteristics, which are not adequate in existing datasets. Demographic and geographic data were coarse-grained, such as city-level location, ensuring that users could not be identified.
\begin{itemize}[leftmargin=*]
\item \textbf{Demographic attributes}. The demographic attributes include gender and age of users, which are basic for the social-concerned research on short-video platforms like user addiction. 
% For example, in the research on the fairness of recommender systems~\citep{geyik2019fairness,wu2022selective}, gender and age are vital sensitive attributes for the selection of protected user groups.
In the data analysis of the paper, we do not exclude the minor users but their data has been removed in the actual dataset.
\item \textbf{Geographical attributes}. The geographical attributes contain the city each user lives in, the city's level (from first-tier to fifth-tier) and the community type (country, urban area and town). The geographical characteristics could complement the user-side profile and support the analysis of regional user differences.
\item \textbf{Other attributes}. 
Our dataset records the price of user devices, which is seldom covered by previous datasets.  Considering privacy protection, we only collect the phone's model name from volunteers and then retrieve the corresponding phone price.
\end{itemize}

\subsection{Content Data}
The video content is the most novel component of our dataset, which has always been neglected by existing datasets. Our dataset includes the raw files of 153,561 videos watched by users, totaling 3,998 hours in duration and 3.2 TB in size.
% The size (height*width) of video frames, frame rate and audio sampling frequency are 1280*720, 22.050 kHz and 60 FPS, respectively. 
We preprocess the raw videos for easier use. Each video is divided into 8 equal-length clips, and for each clip, we extract a 256-dimensional visual feature using both
pre-trained ResNet and ViT. Additionally, we provide bilingual (Chinese and English) ASR text generated with SenseVoice-Small~\citep{speechteam2024funaudiollm} and LLaMA3-8B for translation.

\section{Technical Validation and Applications}
To ensure the technical validity of the data, we validate it from four aspects. First, we assess the richness and diversity of the data, including interaction distribution and attribute diversity. Second, we validate the quality of the content data, particularly the preprocessed video features, using clustering visualization. Third, we establish a benchmark for recommendation algorithms. Lastly, we explore the filter bubble phenomenon within our dataset.

\subsection{Data Richness Validation}
\begin{figure}[!t]
\centering
\subfloat[User-side interaction distribution.]{\includegraphics[width=0.5\columnwidth]{fig/user_inter.pdf}\label{fig:u_inter}}
\subfloat[Video-side interaction distribution.]{\includegraphics[width=0.5\columnwidth]{fig/video_inter.pdf}\label{fig:v_inter}}
\caption{Interaction number distribution of (a) users and (b) videos.}
\vspace{-3mm}
\label{fig:inter}
\end{figure}

% \begin{figure*}[!t]
% \centering
% \subfloat[Correlation of the number of likes and watching time.]
% {\includegraphics[width=0.3\linewidth]{fig/like_watch.pdf}
% \label{fig:l_watch}}\hfill
% \subfloat[Correlation of the number of likes and comments.]{
% \includegraphics[width=0.3\linewidth]{fig/like_comment.pdf} 
% \label{fig:l_comment}}\hfill
% \subfloat[Correlation of the number of likes and follows.]{
% \includegraphics[width=0.3\linewidth]{fig/like_follow.pdf} 
% \label{fig:l_follow}}

% \subfloat[Correlation of the number of likes and collects.]
% {\includegraphics[width=0.3\linewidth]{fig/like_collect.pdf}
% \label{fig:l_collect}}\hfill
% \subfloat[Correlation of the number of likes and forwards.]{
% \includegraphics[width=0.3\linewidth]{fig/like_forward.pdf}
% \label{fig:l_forward}}\hfill
% \subfloat[Correlation of the number of likes and hates.]{
% \includegraphics[width=0.3\linewidth]{fig/like_hate.pdf} 
% \label{fig:l_hate}}

% \caption{Correlation of the number of likes and the number of (a) watching time, (b) comments,(c) follows, (d) collects, (e) forwards, (f) hates per user.} 
% \label{fig:relation_behav}
% \end{figure*}

% In the validation of the richness and diversity of behavior data, we conduct a two-fold analysis: the interaction frequency distribution, and the distribution of explicit and implicit feedback.
\subsubsection{Interaction Distribution Analysis} 
We categorize users and videos based on their interaction frequency, as shown in Figure \ref{fig:inter}, illustrating a wide range of user activity and video exposure. As depicted in Figure \ref{fig:u_inter}, the number of users decreases with higher interaction frequency, reflecting the fact that highly active users are a minority. For videos, Figure \ref{fig:v_inter} shows varying interaction counts, ranging from 1 to 1,193. In summary, the dataset encompasses a diverse set of users and videos in terms of interaction frequency.
% \subsubsection{Explicit and Implicit Feedback Distribution}
\begin{figure}[t]
\centering
% \vspace{-6mm}
\subfloat[User gender distribution.]
{\centering\includegraphics[width=0.48\linewidth]{fig/gender.pdf}
\label{fig:gender}}
% \hspace{6mm}
% \subfloat[User age distribution.]{
% \includegraphics[width=0.31\linewidth]{fig/age.pdf} 
% \label{fig:age}}
% \hspace{1mm}
% \subfloat[Phone price distribution.]{
% \includegraphics[width=0.31\linewidth]{fig/price.pdf} 
% \label{fig:price}}
% \subfloat[City distribution (only top 10 are shown).]
% {\includegraphics[width=0.3\linewidth]{fig/city.pdf}
% \label{fig:city}}\hfill
\subfloat[User City-level distribution.]{
\includegraphics[width=0.48\linewidth]{fig/city_level.pdf} 
\label{fig:city-level}}\hfill
% \subfloat[Community type distribution of users.]{
% \includegraphics[width=0.3\linewidth]{fig/com_type.pdf} 
% \label{fig:com}}

\caption{Distribution of some key fields in user attributes.} 
\label{fig:u_attr}
\vspace{-5mm}
\end{figure}
% The user behavior on the platform is divided into two parts: explicit feedback and implicit feedback.
% Explicit feedback such as like, comment and hate on the platform is sparse. By comparison, implicit feedback (\textit{i.e.,} continuous watching behavior) is much more common. 
% The distribution of user-side and video-side watching time is shown in the supplementary material, from which it can be seen that most volunteers watched within 1,000 seconds in the 7 days, and the videos that have been watched for less than 50 seconds are the most common. 

% Furthermore, aiming to get a deeper understanding of user behavior habits, we conduct a detailed analysis of the relation between explicit and implicit feedback by calculating Pearson correlation and the p-value between them, and the results are shown in Figure \ref{fig:relation_behav}. 
% In this part, the number of explicit feedback is counted during the whole data collection process (i.e., the accumulated number on the seventh day). 
% From the result, the following conclusions can be derived:
% \begin{itemize}[leftmargin=*]
%     \item \textbf{Watching time and like behavior have weak positive relations with statistical significance.} The Pearson correlation is 0.20 (significant at the 0.001 level). The users providing more like behaviors usually have a long watching time. The reason might be that these active users usually watch more videos, thus having more opportunities to find satisfying videos on the platform. 
%     \item \textbf{Like behavior and follow/collect/comment behavior have weak positive relations with statistical significance.} The Pearson correlation with follow/collect/comment are 0.18 (significant at the 0.001 level), 0.19 (significant at the 0.001 level) and 0.09 (significant at the 0.01 level). The users with more like behavior also show higher frequency in follow and collect behavior. It is reasonable because these behaviors all convey user interest and usually happen simultaneously. 
%     \item \textbf{Like behavior and forward/hate behavior have no statistically significant relations.} The Pearson correlation between the number of likes and forwards/hates are 0.04 and  -0.03 but both are not statistically significant. 
% \end{itemize}

\subsubsection{User Attribute Coverage Analysis}
In order to validate the diversity of users, we have analyzed the distribution of some key attributes including demographic data (gender) and geographical data (city level) as shown in Figure~\ref{fig:u_attr}. Among the volunteers, 57.1\% are male and 42.9\% female, reflecting a nearly balanced distribution. This aligns closely with the official platform report of 56\% male and 44\% female users, validating the reliability of the dataset. Geographically, the dataset covers diverse users from 373 cities and 5 city levels (first to fifth tier).

% \begin{itemize}[leftmargin=*]
% \item \textbf{Distribution of demographic attributes.}
% The demographic attributes cover the gender and age of users in the dataset. In all volunteers, the ratio of male and female users is 57.1\% and 42.9\% respectively, which exhibits a relatively uniform distribution. The user age ranges from 5 to 79 and the 10-20 year-old users occupy the largest proportion (26.07\%) showing that young users are important user groups of the platform. 
% According to the official report, there were users with 44\% female and 56\% male, which is highly consistent with the distribution in our dataset without obvious bias. This provides evidence for the reliability of the overall collected data.
% \item \textbf{Distribution of geographical attributes.}
% As for the geographical attributes, the dataset covers 373 cities (we only show the top 10 due to the limit of space), 3 community types(country, urban area, town) and 5 city levels (first-tier, second-tier, third-tier, fourth-tier and fifth-tier). In this dataset, most users come from the country, occupying 36.92\%. The users from urban areas and towns are 19.27\% and 11.39\%, respectively. 32.43\% of users don't provide this information and we label their community type as "unknown".
% \end{itemize}

% For the video side, we focus on the validation of two important attributes: category and duration. 
% \begin{itemize}[leftmargin=*]
% \item \textbf{Video category.} 
% Overall, the dataset covers 37 primary categories(Category \uppercase\expandafter{\romannumeral1}), 281 secondary categories(Category \uppercase\expandafter{\romannumeral2}) and 382 tertiary categories(Category \uppercase\expandafter{\romannumeral3}). The distribution of Category \uppercase\expandafter{\romannumeral1} is presented in the Appendix. 
% \item \textbf{Duration.} 
% The dataset includes videos with a wide range of duration from 2.7 to 1734.9 seconds, and the distribution of the duration is shown in the supplementary material. In the dataset, most videos are relatively short with less than 30 seconds. Overall, the number of videos decreases with the increasing duration, which is consistent with the concept of short-video platforms.
% \end{itemize}
% In summary, the dataset includes rich attribute data, which largely extends the range of user and video information compared with existing datasets. On the whole, the distribution of the attributes is comparatively even which could reduce the analysis bias.

\subsection{Quality Validation of Video Content Data}
\begin{figure}[t!]
\centering
\subfloat[Category \uppercase\expandafter{\romannumeral1}]
{\includegraphics[width=0.5\linewidth]{fig/pca_root1.pdf}
\label{fig:ts1}}
% \hspace{3mm}
\subfloat[Category \uppercase\expandafter{\romannumeral3}]
{\includegraphics[width=0.5\linewidth]{fig/pca_category.pdf}
\label{fig:ts3}}
\caption{Embedding visualization of videos with different (a) Category \uppercase\expandafter{\romannumeral1} and (b) Category \uppercase\expandafter{\romannumeral3} through t-SNE.}
\label{fig:ts}
\vspace{-3mm}
\end{figure}

Our dataset differs from existing ones by providing raw video files and preprocessed visual features. To assess the quality of these features, we use t-SNE for embedding visualization. We randomly select five video categories from both Category \uppercase\expandafter{\romannumeral1} and Category \uppercase\expandafter{\romannumeral3}. As shown in Figure~\ref{fig:ts}, videos from different categories exhibit distinct content features, confirming that the visual features effectively capture the semantic information of the videos. 

% To further validate the utility of the visual features, we test them in a downstream recommendation task. We evaluate four multimodal recommendation methods with and without video features, as shown in Table~\ref{tab:video-feature-exp}. The results reveal a consistent performance drop when video features are absent, highlighting their importance.

% \begin{table}[!t]
%     \centering
%     \caption{\label{tab:video-feature-exp} Performance comparison of 4 typical multimodal recommendation methods with and without video features to validate the practical usage of video features.}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{ccccc}
%     \hline
%         \textbf{Model} & \textbf{Recall@10} & \textbf{Recall@20} & \textbf{NDCG@10} & \textbf{NDCG@20}  \\ \hline
%         VBPR & 0.0184 & 0.0283  & 0.0140 & 0.0172  \\ \hline
%         VBPR (w/o video feature) & 0.0099 & 0.0163  & 0.0082 & 0.0103  \\ \hline
%         MMGCN & 0.0105 & 0.0187  & 0.0088 & 0.0114  \\ \hline
%         MMGCN (w/o video feature) & 0.0088 & 0.0141  & 0.0068 & 0.0085  \\ \hline
%         GRCN & 0.0119 & 0.0224  & 0.0089 & 0.0125  \\ \hline
%         GRCN (w/o video feature) & 0.0055 & 0.0112  & 0.0040 & 0.0058  \\ \hline
%         BM3 & 0.0238 & 0.0364  & 0.0178 & 0.0218  \\ \hline
%         BM3 (w/o video feature) & 0.0231 & 0.0352 & 0.0171 & 0.0208  \\ \hline
%     \end{tabular}
%     }
% \end{table}

\subsection{Benchmarking Recommendation Algorithms}
\begin{table}[!t]
    \centering
    \caption{Benchmark results on 8 recommendation algorithms (including general and multimodal methods).}
    \label{tab:rec-results}
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
    \hline
        \textbf{Model} & \textbf{Recall@10} & \textbf{Recall@20}  & \textbf{NDCG@10} & \textbf{NDCG@20}  \\ \hline
        BPR & 0.0113 & 0.0218 & 0.0078 & 0.0114  \\ \hline
        LightGCN & 0.0223 & 0.0358  & 0.0169 & 0.0211  \\ \hline
        LayerGCN & 0.0208 & 0.0336  & 0.0160 & 0.0198  \\ \hline
        VBPR & 0.0184 & 0.0283  & 0.0140 & 0.0172  \\ \hline
        MMGCN & 0.0105 & 0.0187 & 0.0088 & 0.0114  \\ \hline
        GRCN & 0.0119 & 0.0224 & 0.0089 & 0.0125 \\ \hline
        LGMRec & 0.0159 & 0.0257 & 0.0131 & 0.0162 \\ \hline
        BM3 & \textbf{0.0238} & \textbf{0.0364} & \textbf{0.0178} & \textbf{0.0218}  \\ \hline
    \end{tabular}
    }
\vspace{-5mm}
\end{table}
Accurate user modeling and personalized recommendations are key focuses in both academia and industry. On short-video platforms, leveraging multimodal video information improves user modeling and recommendation quality. 
We evaluate 8 recommendation algorithms on our dataset in Table~\ref{tab:rec-results} to show the effectiveness of our dataset for user modeling. These algorithms are categorized into general and multimodal methods. The general methods include BPR~\citep{rendle2009bpr}, LightGCN~\citep{he2020lightgcn}, and LayerGCN~\citep{zhou2023layer}. The multimodal methods include VBPR~\citep{he2016vbpr}, MMGCN~\citep{wei2019mmgcn}, GRCN~\citep{wei2020graph}, BM3~\citep{zhou2023bootstrap}, and LGMRec~\citep{guo2024lgmrec}. 
We split the dataset into training, validation, and test sets with an 8:1:1 ratio, following the pipeline in~\citep{zhou2023comprehensive}. Results show that BM3 outperforms other methods by effectively utilizing multimodal data, consistent with results from other multimodal recommendation benchmarks~\citep{zhou2023comprehensive}. This validates the practical usage of our dataset for benchmarking recommendation algorithms.

\subsection{Filter Bubble Study} 
The filter bubble phenomenon, where algorithms on online platforms reduce information diversity, has recently gained significant attention. Recommendation algorithms, which prioritize users' past preferences, tend to reinforce existing interests and limit exposure to diverse content, negatively impacting user experience and platform growth.
To measure content coverage, our dataset uses a 3-level video category system. Coverage for category \( c \) is defined as \( \frac{N_{seen}(u,c)}{N_{all}(c)} \), where \( N_{seen}(u,c) \) represents the number of categories a user \( u \) has interacted with, and \( N_{all}(c) \) is the total available categories at level \( c \). A userâ€™s filter bubble extent is assessed by comparing this metric to the median value across all users. We track the evolution of filter bubble ratios over 7 days, distinguishing between active users ($\geq$3 videos/day) and inactive users (<3 videos/day). Results, shown in Figures~\ref{fig:filter bubble}, indicate that active users' filter bubble ratios remain stable with minor fluctuations, while inactive users' ratios steadily increase over time. This finding aligns with previous research~\citep{fu2024heavy}, suggesting that more engaged users do not necessarily have limited content preferences.


% \begin{figure}[h]
% \centering
% {\includegraphics[width=\linewidth]{fig/filter_bubble_inactive_users.pdf}
% }
% \vspace{-5mm}
% \caption{Analysis of the filter bubble ratio of inactive users over time in our dataset.}
% % \vspace{-5mm}
% \label{fig:filter bubble inactive}
% \end{figure}

% \begin{figure}[ht!]
% \centering
% % \vspace{-5mm}
% \subfloat[Active users]
% {\includegraphics[width=0.55\linewidth]{fig/filter_bubble_active_users.pdf}
% \label{fig:filter bubble active}}
% \subfloat[Inactive users]{
% \includegraphics[width=0.55\linewidth]{fig/filter_bubble_inactive_users.pdf} 
% \label{fig:filter bubble inactive}}
% \caption{Analysis of the filter bubble ratio over time.} 
% % \vspace{-3mm}
% \end{figure}


% \section{More Potential Research Directions} \label{use}
% % \subsubsection{Intended Use}. \label{use}
% In addition to the user modeling (or recommendations) and filter bubble study we have introduced, we discuss more intended research directions that our dataset could support as follows:
% \begin{itemize}[leftmargin=*]
% \item \textbf{User modeling and personalization.}
% Nowadays making accurate user modeling and personalized recommendations has become one of the main focuses of academia and industry~\citep{wu2022federated,wu2022removing,volkovs2017dropoutnet}. On the short-video platform, introducing rich multimodal information from videos can help achieve better user modeling and personalization. However, these data are not well supported by existing datasets due to the shortage of some necessary information such as video content. By comparison, our dataset covers broader content information of users and videos compared with existing datasets. For example, the available raw video files could support more fine-grained multi-modal feature extraction and enhance content-based recommendation. 
% The critical user attributes (\textit{e.g.} geographical attributes) covered by the dataset could be introduced into user interest modeling, enriching the user representation learning. 
%     \item \textbf{Fairness of AI algorithms.} It has long been blamed that artificial intelligence algorithms might lead to and amplify inequity and unfairness~\citep{kusner2020long,espin2022inequality}.
%     One important concern with short-video recommender systems is the potential for bias or discrimination towards different users or video groups. For the user side, recommender systems might exhibit unintentional discrimination across groups with different genders or ages.
%     For the video side, if the recommender system favors certain categories of content or authors, it might lead to unequal exposure for different groups of videos.  Our dataset covers necessary user-side (\textit{e.g.} gender, age, living city) and video-side information (\textit{e.g.} hierarchical categories and raw video content) for multi-aspect group division, which could support the research on the fairness issue. 
%     \item \textbf{Polarization on online platforms.} The polarization phenomenon on social networking platforms might occur when users are exposed to limited content that reinforces their existing beliefs~\citep{sikder2020minimalistic,santos2021link}. One potential way to analyze the polarization phenomenon is to identify clusters of users who are exposed to similar content and show similar attitudes towards the same video. Our dataset covers various kinds of feedback representing their clear beliefs and interests, which could support the identification and analysis of the polarization phenomenon on micro-video platforms.
%     \item \textbf{User addiction on micro-video platforms.} User addiction on the Internet is a growing concern nowadays~\citep{montag2018internet,brand2022can}. Analysis of user addiction relies on the tracking of time spent on the platform and the consumed content. Our dataset covers large-scale records of users' watching history on the micro-video platform, together with an accurate timestamp and content information, which can be used to conduct analysis on the user addiction problem.
% \end{itemize}
\section{Ethical Statement}
All participants have signed informed consent forms, agreeing to the use of their anonymized data for research. They have the right to opt out and request to remove their data at any time. User information has been anonymized to avoid user identification. The data collection was conducted strictly following the local ethical regulations similar to GDPR and was supervised by the Science and Technology Ethics Committee of Tsinghua University. 
The videos are collected from the public micro-video platform Kuaishou, we provide a collection of the existing public online videos with the ImageNet license. To avoid copyright issues, we will provide the original video URLs which can be accessed publicly. 
\section{Conclusion and Future Work}
In this paper, we introduce a large-scale dataset with rich user behavior, attributes and video content from a real mobile short-video platform, where we detail the data collection process, data characteristics and quality validation. Our dataset can support broad research communities such as user modeling, social science, human behavior understanding and so on. We have released the whole dataset and codes for data character analysis to facilitate relevant research. In the future, we plan to provide more fine-grained video content like semantic objects and sentiment labels so as to better describe video content. Moreover, we would like to process more data and provide user interactions with longer periods on the platform.

% \begin{figure}[t]
% \centering
% {\includegraphics[width=\linewidth]{fig/filter_bubble_active_users.pdf}
% }
% \vspace{-5mm}
% \caption{Analysis of the filter bubble ratio of active users over time in our dataset.}
% \vspace{-5mm}
% \label{fig:filter bubble active}
% \end{figure}

\begin{figure}[!t]
\centering
\subfloat[Active users.]{\includegraphics[width=0.51\linewidth]{fig/filter_bubble_active_users.pdf}
}
\subfloat[Inactive users.]{\includegraphics[width=0.51\linewidth]{fig/filter_bubble_inactive_users.pdf}}
\caption{Analysis of the filter bubble ratio of active users (a) and inactive users (b) over time in our dataset.}
\vspace{-3mm}
\label{fig:filter bubble}
\end{figure}
%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
