%In this section, we derive the consistency and robustness guarantee of the CMP mechanism in the general $\lqnorm[2](\R^d)$ space with arbitrary $d\geq 2$...

%In the rest of this section, for the normalization of parameters, we inherit all the assumption on $\f$ and the total number of points in the previous section while replacing the assumption on $\m$ with $\cmp=\mathbf{0}=(0,\cdots,0)$.
We discuss in this section how to extend our analysis of the median mechanism to the generalized median mechanism of~\cite{AgrawalBGTX22} in strategic facility location problem with prediction. In the mechanism design setting with prediction, the mechanism $\mech$ receives  reports $\locs=(\loc_1,\loc_2,\cdots,\loc_n)$ of $n$ strategic agents along with the learning-augmented advice for the placement of the optimal facility $\pred\in\reals^d$. When prediction $\pred$ is accurate, the mechanism $\mech(\locs,\pred)$ is expected to perform better than the worst-case approximation guarantee. The approximation ratio of $\mech$ in the case $\pred=\facility(\locs)$ is called \emph{consistency guarantee}. On the other hand, the prediction $\pred$ might also be highly inaccurate. In this case, the mechanism is expected to retain some of the worst-case performance even when $\pred$ has arbitrary bad social cost compared to the optimum $\facility(\locs)$ for the actual locations of the agents. The worst-case approximation ratio of $\mech(\locs,\pred)$ over all possible predictions $\pred\in\reals^d$
is called \emph{robustness guarantee} of $\mech$.  
Agrawal, Balkanski, Gkatzelis, Ou, and Tan~\cite{AgrawalBGTX22} have shown that the following generalized median mechanism called $\CMP(c)$ parametrized with $c\in[0,1]$ achieves best possible trade-offs for the consistency and robustness in $\lqnorm[2](\reals^2)$: add $c\cdot n$ copies of the predicted point $\pred$ to the input $\locs$ and calculate coordinate-wise median 
\[
\cmp=\median(\loc_1,\loc_2,\cdots,\loc_n, c\cdot n\text{ copies of}\ \pred).
\]
We would like to understand how the consistency and robustness approximation guarantees of $\CMP(c)$ degrade in 
arbitrary Euclidean spaces $\lqnorm[2](\reals^d)$. 
% \begin{itemize}
%     \item prediction $\pred$ 
%     \item the coordinate-wise median mechanism(CMP):
%     $$\CMP=median(\loc_1,\loc_2,\cdots,\loc_n, \text{cn copies of}\ \pred)$$
%     \item extra assumption: $\cmp=\mathbf{0}=(0,\cdots,0)$.(replace the assumption that $\mathbf{m}=\mathbf{0}=(0,\cdots,0)$)
% \end{itemize}

\begin{theorem}
    \label{thm:consistency_guarantee}
    The consistency guarantee of $\CMP(c)$ mechanism in $\lqnorm[2](\R^d)$ is at most
    \[
    \begin{cases}
    \left( 4\sqrt{2c+3}\cdot c + 6\sqrt{2c+3} - 10c - 8 \right)^{1/2} / (c+1), & \quad\text{when }c \in [0, 1/2)\\
    \sqrt{\frac{2}{c+1}} & \quad\text{when }c \in [1/2, 1).
    \end{cases}
    \]
\end{theorem}
\begin{proof}
We make the same assumptions as in \eqref{eq:main} about optimal facility $\facility$ (that $\facility_i>0$ for each $i\in[d]$), which is the same as prediction $\facility(\locs)=\pred$, and that the generalized median $\cmp=\vect{0}$. As the constraint of $\cmp$ being located at $\mathbf{0}$ can be written as $\sum_{i\in[n]}\sigma(\loc_i)=-c\cdot n$, we get the following optimization problem
% In the consistency scenario, the $c\cdot n$ copies of $\pred$ are located precisely at the optimal facility location $\facility$ with $\facility_i>0$. The constraint of $\cmp$ located at $\mathbf{0}$ can be formally stated as $\sum_{i\in[n]}\sigma(\loc_i)=-c\cdot n$. Hence, our optimization problem for consistency guarantee can be written as follows.
\begin{align}
 \label{eq:consistency main}
 \min\limits_{\locs}& \sum_{i\in[n]} g(\loci), &\ %\text{where }
 where~ g(\loci)\eqdef\qnorm[2]{\loci-\facility}-\lambda\qnorm[2]{\loci} \nonumber\\
 \text{s.t.}&  \sum_{i\in [n]}\sigma(\loci) = -cn\cdot\vect{1}, & 
\end{align}
We follow the same plan as in our analysis of the median mechanism with minor modifications. Namely, we have the same function $g(\loc)$ as in \eqref{eq:main} defined for $q=2$; apply Lemma~\ref{lm:expression for local optimum} for each $\loci\in\locs$, to express $g(\loci)$ as a function of $x_i$ as defined in \eqref{eq:relaxed main}. 
The only modification we need to make comes from a different constraint of \eqref{eq:main}, which becomes
% \begin{multline}
%     \label{eq: consistency relaxed_median_constraint}
%     \sum\limits_{i\in[n]}x_i=\sum\limits_{i\in[n]}\energy[S(\loci)]=
%     \sum_{i\in[n]}\sum_{j:p_{i,j}\ge 0^+}f_j^2
%     \\
%     =\sum_{j\in[d]}f_j^2\cdot\sum_{i:p_{i,j}\ge 0^+}1
%     =\frac{n}{2}\cdot\sum_{j\in[d]}f_j^2=\frac{1-c}{2}\cdot n.
% \end{multline}
\begin{equation}
    \label{eq: consistency relaxed_median_constraint}
    \sum\limits_{i\in[n]}x_i=\sum\limits_{i\in[n]}\energy[S(\loci)]=
    \sum_{i\in[n]}\sum_{j:p_{i,j}\ge 0^+}f_j^2
    =\sum_{j\in[d]}f_j^2\cdot\sum_{i:p_{i,j}\ge 0^+}1
    =\frac{n}{2}\cdot\sum_{j\in[d]}f_j^2=\frac{1-c}{2}\cdot n.
\end{equation}
% We can simply relax the constraint in \eqref{eq:consistency main} to \eqref{eq: consistency relaxed_median_constraint} and solve the corresponding minimization problem. 
Thus, we get the following analog of~\eqref{eq:relaxed main} for $q=2$.
% can simply relax the constraint in \eqref{eq:consistency main} to \eqref{eq: consistency relaxed_median_constraint} and solve the corresponding minimization problem. 
\begin{align}
 \label{eq:consistency relaxed main}
 \min\limits_{\vx}& \sum_{i\in[n]} h(x_i)
 &\text{where }h(x_i)
 \eqdef \lambda\cdot\left(\delta\cdot(1-x_i)^{1/2}- x_i^{1/2}\right)\nonumber\\
 \text{s.t.}&  \sum_{i\in [n]}x_i = \frac{1-c}{2}\cdot n,&
\forall i\in[n]~x_i\in[0,1]
\end{align}
As before, $z\eqdef\delta^{-2/3}/(\delta^{-2/3}+1)<1/2$ for $q=2$; and by Lemma~\ref{optimal value of x_i}, the optimal value of $x_i$ lies in $\{ a,1\}$, where $a\in[0,z]$ and the space of optimal solution can be narrowed down to 2 parameters $a$ and $|\{i: x_i=a\}|$\footnote{Actually, Lemma~\ref{optimal value of x_i} helps to narrow down the optimal value of $x_i$ to 3 possible values $\{a,b,1\}$, but with the same argument as for the median mechanism, we can ignore the effect of $x_i=b$.}. 

As our new constraint is $\sum_{i=1}^n x_i = (1-c)\cdot n/2$, we get that the numbers $|\{i: x_i=a\}|$ and $|\{i: x_i=1\}|$ must be, respectively, $\frac{1+c}{2(1-a)}\cdot n$ and $\frac{1-2a-c}{2(1-a)}\cdot n$. Moreover, the number $|\{i: x_i=1\}|\geq 0$ implies that $1-2a-c\geq 0$. Therefore, we need to find $\lambda$ such that
\begin{multline}
    \label{eq: consistency optimization over a}   \min_{a\in\left[0,\min\{z,\frac{1-c}{2}\}\right]}\left[\frac{1+c}{2(1-a)}\cdot n\cdot h(a) + \frac{1-2a-c}{2(1-a)}\cdot n\cdot h(1)\right]\\
    =\min_{a\in\left[0,\min\{z,\frac{1-c}{2}\}\right]}\bigg[\frac{n\cdot\lambda}{2(1-a)}\cdot\Big((1+c)\cdot\left(\delta\cdot(1-a)^{1/q}-a^{1/q}\right)-1+2\cdot a+c\Big)\bigg]\ge 0.
\end{multline}
Thus, we need to minimize $$u_1(a)\eqdef(1+c)\cdot\left(\delta\cdot(1-a)^{1/q}-a^{1/q}\right)-1+2\cdot a+c.$$
Let $a_1\eqdef \argmin\limits_{a\in\left[0,\min\{z,\frac{1-c}{2}\}\right]} u_1(a)$. We observe that
\begin{lemma}
    \label{lm:consistency optimal solution for a_1}
    If $u_1(a_1)=0$, then $a_1$ is optimal $\Rightarrow u_1'(a_1)=0\ or\ a_1=\frac{1-c}{2}$.
\end{lemma}
The proof of Lemma~\ref{lm:consistency optimal solution for a_1} is 
essentially the same as the proof of Lemma~\ref{lm:optimal solution for a}:
we simply verify that if $u_1(a_1)=0$, $a_1\in\{0,z\}$ are not optimal, and that $u_1'(a)=0$ has unique 
solution on $(0,z)$ (the only difference is that we have an extra constraint $a_1\le \frac{1-c}{2}$, which is appropriately reflected in the statement of Lemma~\ref{lm:consistency optimal solution for a_1}).
Proof with full details is deferred to the Appendix. 


To conclude the proof of Theorem~\ref{thm:consistency_guarantee}, we need to find $\lambda_1$ (analog of $\lambda^*$ from the proof of Theorem~\ref{thm:UB}) such that the minimum of \eqref{eq:consistency relaxed main} and respectively \eqref{eq: consistency optimization over a} is equal to $0$. This will ensure an $1/\lambda_1$-consistency guarantee. By Lemma~\ref{lm:consistency optimal solution for a_1}, the optimal $\lambda_1$ and $a_1$ must satisfy: (i) $u_1(a_1)=0$ and (ii) $u_1'(a_1)=0$ or $a_1=(1-c)/2$. Then by denoting $\delta_1=\delta(\lambda_1)$, we find  $a_1$, $\delta_1$, $\lambda_1$ in the same way as before. We move detailed calculations to Appendix and conclude that
\begin{equation*}
%\label{eq:optimal lambda}
    \lambda_1=\left\{
    \begin{aligned}
    &(c+1)/\left(4\sqrt{2c+3}\cdot c+6\sqrt{2c+3}-10c-8\right)^{\frac{1}{2}},&c\in[0,1/2)\\
    &\sqrt{\frac{c+1}{2}},&c\in[1/2,1).
    \end{aligned}
    \right.
\end{equation*}
% For the same argument as in the proof of Theorem~\ref{thm:UB}, the final step in the proof of Theorem~\ref{thm:consistency_guarantee} is to find $\lambda_1$ such that the minimum of \eqref{eq:consistency relaxed main} and respectively \eqref{eq: consistency optimization over a} is equal to $0$, which ensures us an $1/\lambda_1$-consistency guarantee. By Lemma~\ref{lm:consistency optimal solution for a_1}, the optimal $\lambda_1$ and $a_1$ must satisfy: (i) $u_1(a_1)=0$ and (ii) $u_1'(a_1)=0$ or $a_1=(1-c)/2$. Then by denoting $\delta_1=\delta_1(\lambda_1)$, we can solve for $a_1$, $\delta_1$, $\lambda_1$ in a straightforward way. We move the detailed calculations to Appendix and can conclude that
% \begin{equation*}
% %\label{eq:optimal lambda}
%     \lambda_1=\left\{
%     \begin{aligned}
%     &(c+1)/\left(4\sqrt{2c+3}\cdot c+6\sqrt{2c+3}-10c-8\right)^{\frac{1}{2}},&c\in[0,1/2)\\
%     &\sqrt{\frac{c+1}{2}},&c\in[1/2,1).
%     \end{aligned}
%     \right.
% \end{equation*}
\end{proof}

In the robustness scenario, instead of having $\pred=\facility$, the prediction $\pred$ can be anywhere in $\reals^d$. Thus, we first need to understand what is the worst-case placement of $\pred$ for $\cmp(\locs,\pred)$. Now, if we normalize the problem so that $\cmp=\vect{0}$, our constraint in \eqref{eq:main} becomes $\sum_{i\in[n]}\sigma(\loc_i)=-cn\cdot\sigma(\pred)$. Since this constraint only depends on the signature of $\pred$, the respective mathematical program for the robustness is as follows.
% For a given $\pred$, to locate $\cmp$ at $\mathbf{0}$ is equivalent to having the constraint that $\sum_{i\in[n]}\sigma(\loc_i)=-cn\cdot\sigma(\pred)$. Since only the signature of $\pred$ affects the outcome of our mechanism, the optimization problem for robustness guarantee can be stated as follows.
\begin{align}
 \label{eq:optimization for sigma}
 \min\limits_{\locs,\sigma(\pred)}&\sum_{i\in[n]} g(\loci) &\text{where }g(\loci)=\qnorm[2]{\loci-\facility}-\lambda\cdot\qnorm[2]{\loci} \nonumber\\
 \text{s.t.}&  \sum_{i\in [n]}\sigma(\loci) = -cn\cdot\sigma(\pred),& 
\end{align}
Let $\sigma^*$ and $\loc^*$ be the optimal solution to \eqref{eq:optimization for sigma}. It is rather straightforward to see that $\sigma^*=\sigma(\pred)=(-1,\ldots,-1)$.
 % Denote $\sigma^*$ and $\loc^*$ as the optimal solution for $\sigma(\pred)$. We observe
\begin{lemma} 
    \label{lm:optimal solution for sigma(pred)}
    Without loss of generality we can assume that 
    $\sigma^*=(-1,-1,\cdots,-1)$.
\end{lemma}
\begin{proof} Assume towards the contradiction that there is a coordinate $k$ with $\sigma^*_k=1$. Then we shall change $\sigma^*_k$ from $1$ to $-1$ ($\sigma'=(\sigma^*_{\text{-}k},-1)$) and also modify the set of locations to $\locs'$, such that $(\locs',\sigma')$ satisfy the constraint and achieve even smaller objective value $\sum_{i\in[n]}g(\loci')\le\sum_{i\in[n]}g(\loci)$. 

We construct $\locs'$ as follows. For any $j\neq k$ and $i\in[n]$, let $p'_{i,j}=p^*_{i,j}$. For coordinate $k$, we select any $cn$ locations $\loc^*_{(1)},\loc^*_{(2)},\cdots,\loc^*_{(cn)} \in \locs^*$ with the signature of the $k$-th coordinate being -1 and let $p'_{(i),k}=-p^*_{(i),k}$. The $k$-th coordinate remains the same between $\locs'$ and $\locs^*$ for the rest $(1-c)\cdot n$ locations. Now, since $f_j>0$ for all $j\in d$, $|p'_{(i),k}-f_k|\leq|p^*_{(i),k}-f_k|$. Therefore, 
    \begin{align*}
        \sum_{i\in[n]} g(\loci')&=\sum_{i\in[n]}\qnorm[2]{\loci'-\facility}-\lambda\cdot\qnorm[2]{\loci'}\\
        &\leq \sum_{i\in[n]}\qnorm[2]{\loci^*-\facility}-\lambda\cdot\qnorm[2]{\loci^*}\\
        &=\sum_{i\in[n]} g(\loci^*).
    \end{align*}
     I.e., $\locs'$ produce a smaller or equal objective value.
\end{proof}
Hence, \eqref{eq:optimization for sigma} can be written as
% With the optimal condition of $\pred$ showed in Lemma~\ref{lm:optimal solution for sigma(pred)}, we can rewrite the optimization problem ~\eqref{eq:optimization for sigma} into
\begin{align}
 \label{eq:robustness main}
\min\limits_{\locs}& \sum_{i\in[n]} g(\loci), &\ %\text{where }
 where~ g(\loci)\eqdef\qnorm[2]{\loci-\facility}-\lambda\qnorm[2]{\loci} \nonumber\\
 \text{s.t.}&  \sum_{i\in [n]}\sigma(\loci) = cn\cdot\vect{1}, &
\end{align}
Notice that the optimization problem~\eqref{eq:robustness main} is very similar to~\eqref{eq:consistency main}. We implement the same reasoning as in the proof of Theorem~\eqref{thm:consistency_guarantee} and obtain the following robustness guarantee 
\begin{theorem}
    \label{thm:robustness_guarantee}
    The robustness guarantee of $\CMP(c)$ mechanism in $\lqnorm[2](\R^d)$ is at most
    \begin{align*}
    \left( -4\sqrt{3-2c}\cdot c + 6\sqrt{3-2c} + 10c - 8 \right)^{1/2} / (1-c).
    \end{align*}
\end{theorem}

 We defer the detailed proof to Appendix.


\subsection*{Comparison between $\CMP(c)$ in $\reals^d$ and $\reals^2$}
The consistency and robustness approximation guarantees of the coordinate-wise median mechanism from~\cite{AgrawalBGTX22} are respectively $\sqrt{2c^2+2}/(c+1)$ and $\sqrt{2c^2+2}/(1-c)$ in $\reals^2$. We compare these results to the performance of $\CMP(c)$ in $\lqnorm[2](\reals^d)$ for arbitrary $d$, given by our Theorems~\ref{thm:consistency_guarantee} and~\ref{thm:robustness_guarantee}.
%We generalize their settings from $\R^2$ to arbitrary $\R^d$. 
% In this subsection, we compare the consistency and robustness guarantees under both cases ($\R^2$ and $\R^d$) to see how much performance loss the CMP mechanism suffers from the generalization.

In figure~\ref{fig:plot of $r_a$ and $r_b$}, we plot two curves corresponding to the ratio $r_a\eqdef$ consistency guarantee in $\reals^d$/consistency guarantee in $\reals^2$ and to the ratio $r_b\eqdef$ robustness guarantee in $\reals^d$/robustness guarantee in $\reals^2$ depending on the parameter $c\in[0,1)$. Specifically,

% In figure ~\ref{fig:plot of $r_a$ and $r_b$}, we plot the ratio $r_a\eqdef$ consistency guarantee in $\reals^d$/consistency guarantee in $\reals^2$ and $r_b\eqdef$ robustness guarantee in $\reals^d$/robustness guarantee in $\reals^2$ against $c\in[0,1)$. More specific,
\begin{equation*}
r_a=
\left\{
\begin{aligned}
&(2\sqrt{2c+3}\cdot c+3\sqrt{2c+3}-5c-4)^{\frac{1}{2}}/(c^2+1)^{\frac{1}{2}}, &c\in[0,1/2)\\
&(c+1)^{\frac{1}{2}}/(c^2+1)^{\frac{1}{2}},  &c\in[1/2,1)
\end{aligned}
\right.
\end{equation*}
and,
\begin{equation*}
r_b=(-2\sqrt{3-2c}\cdot c+3\sqrt{3-2c}+5c-4)^{\frac{1}{2}}/(c^2+1)^{\frac{1}{2}}.
\end{equation*}
When $c=0$, $r_a=r_b=\sqrt{6\sqrt{3}-8}/\sqrt{2}\approx1.09$. From figure~\ref{fig:plot of $r_a$ and $r_b$}, $r_a$ increases at first and then decreases to $1$, but it never exceeds $1.11$, while $r_b$ decreases monotonously from roughly $1.094$ to $1$. This indicates that the generalization of the CMP mechanism from $\R^2$ to $\R^d$ won't suffer a performance loss (both for consistency and for robustness) more than $11\%$.
\begin{figure}[htb] 
		\centering
		\includegraphics[width=5.0in]{ra_rb.png}
		\caption{plot of $r_a$ and $r_b$}
\label{fig:plot of $r_a$ and $r_b$}		
\end{figure}

On the other hand, when comparing asymptotic convergence rate of both consistency and robustness for confidence parameter $c$ approaching $c\rightarrow 1$ of $\CMP(c)$ in $\reals^d$ against $\CMP(c)$ in $\reals^2$, we observe that robustness guarantees are essentially the same, while consistency guarantees converge to $1$ at different rates. Specifically, both robustness guarantees are of order $\Theta(1/(1-c))$, and the Taylor expansions of the consistency guarantee for $\eps=1-c$ is  $\sqrt{2/(c+1)}=1+\frac{\eps}{4}+O(\eps^2)$ in $\reals^d$, and 
$\sqrt{2c^2+2}/(c+1)=1+\frac{\eps^2}{8}+O(\eps^3)$ in $\reals^2$.

% In addition, we also compare the asymptotic convergence rate of both consistency and robustness guarantees when $c\rightarrow 1$ under $\R^d$ and $\R^2$. For robustness, both cases converge at a rate of $O(1/(1-c))$. For consistency, we calculate their taylor expansion respectively when $c\rightarrow 1$. In $\R^d$ space, $\sqrt{2/(c+1)}=1+\frac{1-c}{4}+O((1-c)^2)$. However, in $\R^2$ space, $\sqrt{2c^2+2}/(c+1)=1+\frac{(1-c)^2}{8}+O((1-c)^3)$, indicating a faster convergence rate when $c\rightarrow1$.
% To conclude, besides the asymptotic convergence rate of the consistency guarantee when $c\rightarrow 1$, the CMP mechanism only suffers a small loss in performance when generalized from $\R^2$ to $\R^d$.  