\section{Related work}
Our work introduces a novel \textit{concept-based interpretability} method that adapts \textit{SAEs} to discover \textit{universal concepts}. We now review the most relevant works in each of these fields.

\noindent\textbf{Concept-based interpretability}~\cite{kim2018interpretability} emerged as a response to the limitations of attribution methods~\cite{simonyan2013deep,zeiler2014visualizing,bach2015pixel,springenberg2014striving,smilkov2017smoothgrad,sundararajan2017axiomatic,selvaraju2017grad,fong2019extremal,fel2021sobol,muzellec2023gradient}, which, despite being widely used for explaining model predictions, often fail to provide a structured or human-interpretable understanding of internal model computations~\cite{hase2020evaluating,hsieh2020evaluations,nguyen2021effectiveness,fel2021cannot,kim2021hive,sixt2020explanations}. Attribution methods highlight input regions responsible for a given prediction, the \textit{where}, but do not explain \textit{what} the model has learned at a higher level. In contrast, concept-based approaches aim to decompose internal representations into human-understandable \textit{concepts}~\cite{genone2012concept}. The main components of concept-based interpretability approaches can generally be broken down into two parts~\cite{fel2023holistic}: (\textbf{\textit{i}}) concept discovery, which extracts and visualizes the interpretable units of computation and (\textbf{\textit{ii}}) concept importance estimation, which quantifies the importance of these units to the model output. Early work explored `closed-world' concept settings in which they evaluated the existence of pre-defined concepts in model neurons~\cite{bau2017network} or layer activations~\cite{kim2018interpretability}. Similar to our work, `open-world' concept discovery methods do not assume the set of concepts is known a priori. These methods pass data through the model and cluster the activations to discover concepts and then apply a concept importance method on these discoveries~\cite{ghorbani2019towards,zhang2021invertible,fel2023craft,graziani2023concept,vielhaben2023multi,kowal2024understanding,kowal2024visual}. 




\noindent\textbf{Sparse Autoencoders} (SAEs)~\cite{cunningham2023sparse, bricken2023monosemanticity, rajamanoharan2024jumping, gao2024scaling,menon2024analyzing} are a specific instance of dictionary learning~\cite{rubinstein2010dictionaries,elad2010sparse,tovsic2011dictionary,mairal2014sparse,dumitrescu2018dictionary} that has regained attention~\cite{chen2021low,tasissa2023kds,baccouche2012spatio, tariyal2016deep,papyan2017convolutional,mahdizadehaghdam2019deep,yu2023white} for its ability to uncover interpretable concepts in DNN activations. This resurgence stems from evidence that individual neurons are often \textit{polysemantic}—i.e., they activate for multiple, seemingly unrelated concepts~\cite{nguyen2019understanding,elhage2022toy}—suggesting that deep networks encode information in \textit{superposition}~\cite{elhage2022toy}. SAEs tackle this by learning a sparse~\cite{hurley2009comparing,eamaz2022building} and \textit{overcomplete} representation, where the number of concepts exceeds the latent dimensions of the activation space, encouraging disentanglement and interpretability. 
While SAEs and clustering bear mathematical resemblance, SAEs benefit from gradient-based optimization, enabling greater scalability and efficiency in learning structured concepts.  
Though widely applied in natural language processing (NLP)~\cite{wattenberg2024relational, lwcomposition,chanin2024absorption,tamkin2023codebook}, SAEs have also been used in vision~\cite{fel2023holistic, surkov2024unpacking,bhalla2024interpreting}. Early work compared SAEs to clustering and analyzed early layers of Inception v1~\cite{mordvintsev2015Inceptionism,gorton2024missing}, revealing hypothesized but hidden features. More recently, SAEs have been leveraged to construct text-based concept bottleneck models~\cite{koh2020concept} from CLIP representations~\cite{radford2021learning,rao2024discover,parekh2024concept,bhalla2024towards}, showcasing their versatility across modalities. Unlike prior work that apply SAEs independently to %
models, here we consider a joint application of SAEs fit simultaneously 
across diverse models. %


\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/usae_method.jpg}
    \vspace{-18pt}
    \caption{\textbf{USAE training process.} In each forward pass during training, an encoder of model $i$ is randomly selected to encode a batch of that model's activations, $\Z = \encoder^{(i)}(\A^{(i)})$. The concept space, $ \Z $, is then decoded to reconstruct every model's activations, $ \widehat{\A}^{(j)} $, using their respective decoders, $ \D^{(j)}$.}
    \vspace{-6mm}
    \label{fig:method}
\end{figure}

\noindent\textbf{Feature Universality} studies the shared information across different DNNs. One approach, Representational Alignment, quantifies the mutual information between different sets of representations—whether across models or between biological and artificial systems~\cite{kriegeskorte2008representational,sucholutsky2023getting}. Typically, these methods rely on paired data (e.g., text-image pairs) to compare encodings across modalities. Recent work suggests that foundation models, regardless of their training modality, may be converging toward a shared, \textit{Platonic} representation of the world~\cite{huh2024platonic}. Another line of research focuses on identifying universal features across models trained on different tasks. Rosetta \textit{Neurons}~\cite{dravid2023rosetta} identify image regions with correlated activations across models, while Rosetta \textit{Concepts}~\cite{kowal2024understanding} extract concept vectors from video transformers by analyzing shared exemplars. These methods perform post-hoc mining of universal concepts rather than learning a shared conceptual space. This reliance on retrospective discovery is computationally prohibitive for many models and prevents direct concept translation between architectures.  
A concurrent study~\cite{lindsey2024sparse} explores training SAEs (termed \textit{crosscoders}) between different states of the same model before and after fine-tuning. In contrast, our work discovers universal concepts shared \textit{across} distinct model architectures for vision tasks.





