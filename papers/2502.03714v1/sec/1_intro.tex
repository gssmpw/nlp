
\vspace{-6mm}
\section{Introduction}
\vspace{1mm}

In this work, we focus on discovering interpretable concepts shared among multiple pretrained deep neural networks (DNNs).
The goal is to learn a \emph{universal concept space} -- a joint space of concepts -- that provides a unified lens into the hidden representations of diverse models. We define concepts as the abstractions each network captures that transcend individual data points—spanning low-level features (e.g., colors and textures) to high-level attributes (e.g., emotions like \emph{horror} and ideas like \emph{holidays}).



Grasping the underlying representations within DNNs is crucial for mitigating risks during deployment~\cite{buolamwini2018gender,hansson2021self}, fostering the development of innovative model architectures~\cite{darcet2023vision}, and abiding by regulatory frameworks~\cite{euro2021laying,whitehouse2023president}. 
Prior interpretability efforts often center on dissecting a single model for a specific task, leaving risk management unmanageable when each network is analyzed in isolation.
With a growing number of capable DNNs, finding a canonical basis for understanding model internals may yield more tractable strategies for managing potential risks.

Recent work supports this possibility.
The core idea behind `foundation models'~\cite{henderson2023foundation} presupposes that any DNN trained on a large enough dataset should encode concepts that generalize to an array of downstream tasks for that modality. Moreover, recent work has shown that there is a substantial amount of shared information between DNNs trained independently for \textit{different} tasks or modalities~\cite{huh2024platonic}, and recent studies~\cite{dravid2023rosetta, kowal2024understanding} have found shared concepts across vision models, implying that universality may be more widespread than previously assumed.
However, current techniques for identifying universal features
~\cite{dravid2023rosetta,huh2024platonic,kowal2024understanding} 
typically operate 
\emph{post-hoc},
extracting concepts from individual models and then matching them through labor-intensive filtering or optimization. This approach is limited in scalability, lacks the efficiencies of gradient-based training, and precludes \emph{translation} between models within a unified concept space. Consequently, tasks that require simultaneous interaction across multiple models, e.g., \emph{coordinated activation maximization} shown later, %
 become more cumbersome.



To overcome these challenges, we introduce a \emph{universal sparse autoencoder} (USAE), Fig.~\ref{figure:method_overview}, designed to jointly encode and reconstruct activations from multiple DNNs. Through qualitative and quantitative evaluations, we show that the resulting concept space captures interpretable features shared across all models. Crucially, a USAE imposes concept alignment during its end-to-end training, differing from conventional post-hoc methods. 
We apply USAEs to three diverse vision models and make several interesting findings %
about shared concepts: (i) We discover a \textit{broad range of universal concepts}, at low and high levels of abstraction. (ii) We observe a strong correlation between concept \textit{universality} and \textit{importance}. (iii) We provide quantitative and qualitative evidence that DinoV2~\cite{oquab2023dinov2} admits \textit{unique features} compared to other considered vision models. (iv) Universal training admits shared representations \textit{not uncovered} in model-specific SAE training.

\noindent\textbf{Contributions.} 
Our main contributions are as follows.  
First, we introduce %
USAEs: 
a framework that learns a shared, interpretable concept space spanning multiple models, with %
focus on
visual tasks. 
Second, we present a detailed analysis contrasting universal concepts against model-specific concepts, offering new insights into how large vision models—trained on diverse tasks and datasets—compare and diverge in their internal representations.  Finally, we demonstrate a novel USAE application,  %
\emph{coordinated activation maximization}, showcasing simultaneous visualization of universal concepts across models.

