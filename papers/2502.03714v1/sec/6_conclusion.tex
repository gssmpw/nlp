\vspace{-3.5mm}
\section{Conclusion}
\vspace{-1.5mm}
In this work, we introduced \emph{Universal Sparse Autoencoders} (USAEs), a framework for learning a unified concept space that faithfully reconstructs and interprets activations from multiple deep vision models at once. Our experiments revealed several important findings: (i) qualitatively, we discover diverse concepts, from low-level primitives like colors, shapes and textures, to compositional, semantic, and abstract concepts like groupings, object parts, and faces, (ii) many concepts turn out to be both \emph{universal} (firing consistently across different architectures and training objectives) and \emph{highly important} (responsible for a large proportion of each model’s reconstruction), (iii) certain models, such as DinoV2, encode unique features even as they share much of their conceptual basis with others, and (iv) while universal training recovers a significant fraction of the concepts learned by independent single-model SAEs, it also uncovers new shared representations that do not appear to emerge in model-specific training. Finally, we demonstrated a novel application of USAEs—\emph{coordinated activation maximization}—that enables simultaneous visualization of a universal concept across multiple networks. Altogether, our USAE framework offers a practical and powerful tool for multi-model interpretability, shedding light on the commonalities and distinctions that arise when different architectures, tasks, and datasets converge on shared high-level abstractions.


