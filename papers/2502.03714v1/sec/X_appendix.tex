\newpage
\appendix
\onecolumn
\section{Appendix}

\subsection{SAE Training Implementation details}~\label{appendix:imp_details}
We modify the TopK Sparse Autoencoder (SAE)~\cite{gao2024scaling} by replacing the $\ell_2$ loss with an $\ell_1$ loss, as we find that this adjustment improves both training dynamics and the interpretability of the learned concepts. The encoder consists of a single linear layer followed by batch normalization~\cite{ioffe2015batch} and a ReLU activation function, while the decoder is a simple dictionary matrix.

For all experiments, we use a dictionary of size $8 \times 768 = 6144$ which is an expansion factor of $8$ multiplied by the largest feature dimension in any of the three models, $768$. All SAE encoder-decoder pairs have independent Adam optimizers~\cite{kingma2014adam}, each with an initial learning rate of $3\mathrm{e}{-4}$, which decays to $1\mathrm{e}{-6}$ following a cosine schedule with linear warmup. To account for variations in activation scales caused by architectural differences, we standardize each model's activations using 1000 random samples from the training set. Specifically, we compute the mean and standard deviation of activations for each model and apply standardization, thereby preserving the relative relationship between activation magnitudes and directions while mitigating scale differences.

Since SigLIP does not incorporate a class token, we remove class tokens from DinoV2 and ViT to ensure consistency across models.
Additionally, we interpolate the DinoV2 token count to match a patch size of $16 \times 16$ pixels, aligning it with SigLIP and ViT. We train all USAEs on a single NVIDIA RTX 6000 GPU, with training completing in approximately three days.

\subsection{Discovering Unique Concepts with USAEs}
With our universal training objective, we are in a unique position to explore concepts that may arise independently in one model, but not in others. Using metrics for universality, Eqs.~\ref{eq:cofire_metric_probs} and~\ref{eq:cofire_metric_entropy}, we can search for concepts that fire with a \textit{low entropy}, thereby isolating firing distributions whose probability mass is allocated to a single model. We explore this direction by isolating unique concepts for DinoV2 and SigLIP, both of which have been studied for their unique generalization capabilities to different downstream tasks~\cite{amir2021deep,zhai2023sigmoid}.

\subsubsection{Unique DinoV2 Concepts}~\label{appendix:unique_dino}
DinoV2's unique concepts are presented in Figures~\ref{fig:qual_app_perspective} and~\ref{fig:qual_app_depth}. Interestingly, we find concepts that solely fire for DinoV2 related to \textit{depth} and \textit{perspective} cues. These features follow surfaces and edges to vanishing points as in concept 3715 and 4189, demonstrating features for converging perspective lines. Further, we find features for object groupings placed in the scene at varying depths in concept 4756, and background depth cues related to uphill slanted surfaces in concept 1710. We also find features that suggest a representation of view invariance, such as concepts related to the angle or tilt of an image (Fig.~\ref{fig:qual_app_tilt}) for both left (concept 3003) and right views (concept 2562). Lastly, we observe unique geometric features in Fig.~\ref{fig:qual_app_geometry} that suggest some low-level 3D understanding, such as concept 4191 that fires for the top face of rectangular prisms, concept 3448 for brim lines that belong to dome shaped objects, as well as concept 1530 for corners of objects resembling rectangular prisms. 

View invariance, depth cues, and low-level geometric concepts are all features we expect to observe unique to DinoV2's training regime and architecture~\cite{oquab2023dinov2}. Specifically, self-distillation across different views and crops at the image level emphasizes geometric consistency across viewpoints. This, in combination with the masked image modelling iBOT objective~\cite{zhou2021ibot} that learns to predict masked tokens in a student-teacher distillation framework, would explain the sensitivity of DinoV2 to perspective and geometric properties, as well as view-invariant features. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Appendix/perspective_figure.jpg}
    \caption{\textbf{Qualitative results of DinoV2 low-entropy concepts.} These concepts fire frequently for DinoV2, depicting converging perspective lines to the right (concept 3715, above) and to the left (concept 4189, below). }
    
    \label{fig:qual_app_perspective}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Appendix/tilt_figure.jpg}
    \caption{\textbf{Qualitative results of low-entropy concepts that fire for DinoV2.} We discover concepts related to view-invariance, such as angled scenes both right (above) and left (below) in concept 2562 and 3003, respectively. }
    \label{fig:qual_app_tilt}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Appendix/depth_figure.jpg}
    \caption{\textbf{Qualitative results of low-entropy concepts that fire for DinoV2.} We discover features related to depth cues for foreground objects as well as background in concept 4756 (above) and 1710 (below).}
    \label{fig:qual_app_depth}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Appendix/geom_figure.jpg}
    \caption{\textbf{Qualitative results for low-entropy concepts that fire for DinoV2.} We discover DinoV2 independent features that are not universal suggesting 3D understanding like corners (concepts 1530), top face of rectangular prism (concept 4191), and brim of dome (concept 3448).}
    \label{fig:qual_app_geometry}
\end{figure*}


\subsubsection{Unique SigLIP Concepts}~\label{appendix:unique_siglip}
Similar to DinoV2, we isolate concepts with low firing-entropy where probability mass is concentrated for SigLIP. Example concepts are presented in Fig.~\ref{fig:qual_app_siglip}. We observe concepts that fire for both visual and textual elements of the same concept. Concept 5718 fires for both the shape of a star, as well as regions of images with the word or even just a subset of letters on a bottlecap and sign at different scales. Additionally, concept 2898 fires broadly for various musical instruments, as well as music notes, while concept 923 fires for the letter `C'. For each of these concepts, the coordinated activation maximization visualization has both the physical semantic representation of the concept, as well as printed text. The presence of image and textual elements are expected given SigLIP is trained as a vision-language model with a contrastive learning objective, where the aim is to align image and text latent representations from separate image and language encoders. While we do not train on any activations directly from the language model, we still observe textual concepts in our image-space visualizations.   

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Appendix/siglip_figure.jpg}
    \caption{\textbf{Qualitative results of low-entropy SigLIP concepts.} We consistently find concepts that fire for abstract concepts in image space such as images or text of `star' (concept 923), letters (concept 5718), and music notes (concept 2958).
    }
    \label{fig:qual_app_siglip}
\end{figure*}



\subsection{Additional Results}

\subsubsection{Additional Quantitative Results}~\label{sec:appendix_quant}
Figure~\ref{fig:roc1000_appendix} presents concept consistency distributions across models for the top 1,000 co-firing concepts. We observe consistent findings with Sec.~\ref{sec:consistency}, mainly that ViT has the strongest concept overlap with $35\%$ of its concepts having a cosine similarity $>0.5$ with its independent counterpart. USAEs again achieve far better performance than the baseline for all models, suggesting that universal training preserves meaningful concept alignments rather than learn entirely new representations. The lower proportion of overlap for SigLIP and DinoV2 indicates that \textbf{universal training discovers universal concepts that may not emerge in independent training}. Universal training favors concepts that are consistently represented across all models, as these concepts more effectively reduce overall reconstruction loss. This may lead to a bias toward fundamental visual concepts that are commonly learned by all models. In contrast, independently trained SAEs lack this selection pressure, allowing them to learn any concept that aids reconstruction, including those specific to a particular architecture or objective, rather than universally shared ones.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{images/Appendix/ROC_curve1000cofiring.pdf}
    \caption{\textbf{Top 1000 co-firing concept consistency between independent SAEs and Universal SAEs.} Our universal training objective discovers universal concepts that have overlap (i.e., cosine similarity) with those discovered with independent training. ViT again has noticeably more overlap, suggesting its simpler architecture and training objective may yield activations that naturally encode fundamental and universal visual concepts.}
    \label{fig:roc1000_appendix}
\end{figure*}




\subsubsection{Additional Qualitative Results}
We provide additional universal concept visualizations for the top activating images for that concept across each model. Specifically, we showcase low-level concepts in Fig.~\ref{fig:qual_app_texture} related to texture like shell and wood for concepts 1716 and 2533, respectively, as well as tiling for concept 5563. We also showcase high-level concepts in Fig.~\ref{fig:qual_app_highlevel} related to environments like auditoriums in concept 4691, object interactions like ground contact in concept 5346, as well as facial features like snouts in concept 3479. 


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Appendix/texture_figure.jpg}
    \caption{\textbf{Qualitative results of universal concepts.} We depict low-level visual features related to textures, such as shells (concept 1716), wood (concept 2533), and tiling (concept 5563).}
    \label{fig:qual_app_texture}
\end{figure*}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{images/Appendix/usae_highlevel_ex_500.jpg}
    \caption{\textbf{Qualitative results of universal concepts.} We depict high-level visual features related to environments, such as auditoriums (concept 4691), ground contact (concept 5346), and animal snouts (concept 3479).}
    \label{fig:qual_app_highlevel}
\end{figure*}




\subsection{Limitations}

Our universal concept discovery objective successfully discovers fundamental visual concepts encoded between vision models trained under distinct objectives and architectures, and allows us to explore features that fire distinctly for a particular model of interest under our regime. However, we note some limitations that we aim to address in future work. We notice some sensitivity to hyperparameters when increasing the number of models involved in universal training, and use hyperparameter sweeps to find an optimal configuration. 
We also constrain our problem to discovering features at the last layer of each vision model. We choose to do so as a tractable first step in this novel paradigm of \emph{learning} to discover universal features. We leave an exploration of universal features across different layer depths for future work. 
Lastly, we do find qualitatively that a small percentage of concepts are uninterpretable. They may be still stored in superposition \cite{elhage2022toy} or they could be useful for the model but simply difficult for humans to make sense of. This is a phenomena that independently trained SAEs suffer from as well.
Many of the limitations of our approach are tightly coupled to the limitations of training independent SAEs, an active area of research. 
