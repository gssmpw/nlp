
\vspace{-3mm}
\section{Method}

\vspace{-1mm}
\paragraph{Notations.}
Let $ \|\cdot\|_2 $ and $ \|\cdot\|_F $ denote the $\ell_2$ and Frobenius norms, respectively, and set $ [n] = \{1, \dots, n\} $. We focus on a broad representation learning paradigm, where a DNN, $ \f : \SX \to \SA $, maps data from $ \SX $ into a feature space, $ \SA \subseteq \mathbb{R}^d $. Given a dataset, $ \X \subseteq \SX $ of size $ n $, these activations are collated into a matrix $ \A \in \mathbb{R}^{n \times d} $. Each row $ \A_i $ (for $ i \in [n] $) corresponds to the feature vector of the $ i $-th sample.

\vspace{-3.5mm}
\paragraph{Background.} The main goal of a Sparse Autoencoder (SAE) 
is to find a sparse re-interpretation of the feature representations. Concretely, given a set of $n$ inputs, $\X$ (e.g., images or text) and their encoding, $\A = \f(\X) \in \mathbb{R}^{n \times d}$, an SAE learns an encoder $\encoder(\cdot)$ that maps $\A$ to \emph{codes} $\Z = \encoder(\A) \in \mathbb{R}^{n \times m}$, forming a sparse representation. This sparse representation must still allow faithful reconstruction of $\A$ through a learned \emph{dictionary} (decoder) $\D \in \mathbb{R}^{m \times d}$, i.e., $\Z \D$ must be close to $\A$. If $m > d$, we say $\D$ is \textit{overcomplete}. In this work, we specifically consider an (overcomplete) TopK SAE~\cite{gao2024scaling}, defined as
\begin{equation}
\Z = \encoder(\A) = \mathrm{TopK}\!\bigl(\bm{W}_{\text{enc}}\,(\A - \bm{b}_{\text{pre}})\bigr),  
\hat{\A} = \Z \D,
\end{equation}
where $\W_{\text{enc}} \in \mathbb{R}^{m \times d}$ and $\bm{b}_{\text{pre}} \in \mathbb{R}^{d}$ are learnable weights. The $\mathrm{TopK}(\cdot)$ operator enforces $ \|\Z_i\|_0 \le K $ for all $ i \in [m] $. The final training loss is given by the Frobenius norm of the reconstruction error:
\begin{equation}
    \mathcal{L}_{\text{SAE}} = \|\f(\X) - \encoder\bigl(\f(\X)\bigr)\D\|_F = \|\A - \Z \D\|_F, 
\end{equation}
with the $K$-sparsity constraint applied to the rows of $ \Z. $



\vspace{-2mm}
\subsection{Universal Sparse Autoencoders (USAEs)}
\vspace{-1mm}
Contrasting standard SAEs, which reinterpret the internal representations of a \emph{single} model, \emph{universal} sparse autoencoders (USAEs) extend this notion across $M$ different models, each with its own feature dimension, $d_i$ (see Fig.~\ref{fig:method}).  Concretely, for model $i \in [M]$, let $\A^{(i)} \in \mathbb{R}^{n \times d_i}$ denote the matrix of activations for $n$ samples.  The key insight of USAEs is to learn a shared sparse code, $\Z \in \mathbb{R}^{n \times m}$, which allows every model to be reconstructed from the same sparse embedding. Specifically, each activation from model $i$ in $\A^{(i)}$ is encoded via a model-specific encoder $\encoder^{(i)}$, as
\begin{equation}
    \Z = \encoder^{(i)}(\A^{(i)}) = \text{TopK}\!\bigl(\W_{\text{enc}}^{(i)}(\A^{(i)} - \bm{b}^{(i)}_{\text{pre}})\bigr).
\end{equation}
 Crucially, once encoded into $\Z$, each row of any model $j \in [M]$ can be reconstructed by a model-specific dictionary, $\D^{(j)} \in \mathbb{R}^{d_j \times m}$, as
\begin{equation}
    \widehat{\A}^{(j)} = \Z \D^{(j)}.
\end{equation}
By jointly learning all encoder-decoder pairs, $\{(\encoder^{(i)}, \D^{(i)})\}_{i=1}^M$, the USAE enforces a unified concept space, $\Z$, that aligns the internal representations of all $M$ models. This shared code not only promotes consistency and interpretability across model architectures, but also ensures each model’s features can be faithfully recovered from a \emph{common} set of sparse `concepts'.


\vspace{-2mm}
\subsection{Training USAEs}
\vspace{-1mm}
Recall that $\X \subseteq \SX$ is our dataset of size $n$, mapped into their respective feature space using DNNs $\f^{(1)}, \ldots, \f^{(M)}$. A naive approach to train our respective encoder and decoder would simultaneously encode and decode the features of all $M$ models, which quickly grows expensive in memory and computation. 
Conversely, randomly sampling a pair of models to encode and decode results in slow convergence. To balance these concerns, we adopt an intermediate strategy (pseudocode detailed in Figure~\ref{code:usae}) that updates a single encoder and decoder at each iteration with a reconstruction loss computed through \emph{all} decoders. Concretely, at each mini-batch iteration, a single model $i \in [M]$ is selected at random, and a batch of features, $\A^{(i)} \in \mathbb{R}^{n \times d_i}$, is sampled and encoded into the shared code space, $\Z = \encoder^{(i)}(\A^{(i)})$.
This code space, $\Z$, is then used to reconstruct the feature representation $\A^{(j)}$ of every model $j \in [M]$ via its decoder:
$\widehat{\A}^{(j)} = \Z \D^{(j)}$,
where $\D^{(j)}$ is the model-$j$ decoder. All reconstructions are aggregated to form the total loss:
\vspace{-2mm}
\begin{align}
\mathcal{L}_{\text{Universal}} &= \sum_{j=1}^M \|\A^{(j)} - \widehat{\A}^{(j)}\|_F \\
&= \sum_{j=1}^M \|\A^{(j)} - \encoder(\A^{(i)})\D^{(j)} \|_F.
\end{align}
Using this universal loss, backpropagation updates the chosen encoder $\encoder^{(i)}$ and decoder $\D^{(i)}$. This method promotes concept alignment, ensures an equal number of updates between encoders and decoders, and strikes a practical balance between training speed and memory usage.


\begin{figure}[h]
\centering
\noindent\begin{minipage}{0.45\textwidth}
\begin{RoundedListing}
def train_usae($\encoder$, $\D$, $\A$, $T$, Optimizers):
    M = len($\encoder$)
    for t in range($T$):
        i = random(M)
        $\Z$ = $\encoder^{(i)}$($\A^{(i)}$)
        $\mathcal{L}$ = $0.0$
        for $j$ in range($M$):
            $\widehat{\A}^{(j)}$ = $\Z$ @ $\D^{(j)}$
            $\mathcal{L}$ += ($\A^{(j)}$ - $\widehat{\A}^{(j)}$).norm(p='fro')
        $\mathcal{L}$.backward()
        Optimizers[i].step()
    return $\encoder$, $\D$
\end{RoundedListing}
\end{minipage}
\caption{\textbf{Training Universal Sparse Autoencoder.} During each training iteration, $\mathcal{L}_{\text{Universal}}$ is the aggregated error computed from decoding each activation $\widehat{A}^{(j)}$. We then take an optimizer step for randomly selected encoder $\encoder^{(i)}$ and associated dictionary $\D^{(i)}$. }
\label{code:usae}
\vspace{-7mm}
\end{figure}






\subsection{Application: Coordinated Activation Maximization}\label{sec:application}
A common technique for interpreting individual neurons or latent dimensions in deep networks is \textit{Activation Maximization (AM)}~\cite{olah2017feature, tsipras2018robustness, santurkar2019image, engstrom2019adversarial, ghiasi2021plug, ghiasi2022vision, fel2023unlocking, hamblin2024feature}. AM involves synthesizing an input that maximally activates a specific component of a model—such as a neuron, channel, or concept vector~\cite{cogsci1986, mahendran2015understanding, kim2018interpretability, fel2023craft}. However, in the case of a USAE, the learned latent space is explicitly structured to capture \textit{shared concepts} across multiple models. This shared space enables a novel extension of AM: \textit{Coordinated Activation Maximization}, where a common concept index, $k$, is simultaneously maximized across all aligned models.


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/Heatmaps_Main_v1.pdf}
    \vspace{-12pt}
\caption{\textbf{Qualitative results of universal concepts.} We discover and visualize heatmaps of universal concepts across a broad range of visual abstractions, where bright green denotes a stronger activation of a given concept. We observe colors, basic shapes, foreground-background, parts, objects and their groupings across \textit{all considered models}. 
    }
    \label{fig:qualitative_universal}
        \vspace{-12pt}
\end{figure*}

Given $M$ models, our objective is to optimize one input per model, $\x^{(1)}_{\star}, \dots, \x^{(M)}_{\star}$, ensuring that all inputs maximally activate the same concept dimension $k$. This approach enables the visualization of how a single concept manifests across different models. By comparing these optimized inputs, we can identify both \textit{consistent} and \textit{divergent} representations of the same underlying concept. Let $\x^{(i)}$ denote the input to model $i$, and let $\f^{(i)}(\x^{(i)}) \in \R^{d_i}$ represent its internal activations. Each model is associated with a USAE encoder $\encoder^{(i)}$, which maps activations to the shared concept space. The activation of concept $k$ for model $i$ given input $\x^{(i)}$ is defined as
\vspace{-2mm}
\begin{align}
    \Z_k^{(i)}(\x) = \left[\encoder^{(i)}\left(\f^{(i)}(\x)\right)\right]_k,
\end{align}
where $k$ indexes the universal concept dimension in the USAE. The goal is to independently optimize each $\x^{(i)}$ such that it maximizes the activation of the same concept $k$ across all $M$ models:
\begin{align}
    \x^{(i)}_{\star} = \argmax_{\x \in \SX} \Z_k^{(i)}(\x^{(i)}) - \lambda \mathcal{R}(\x^{(i)}),
\end{align}
where $\mathcal{R}(\x)$ is a %
regularizer
that promotes natural and interpretable inputs (e.g., total variation, $\ell_2$ penalty, or data priors), and $\lambda$ controls its strength.
In all experiments, we follow the optimization and regularization strategy of Maco~\cite{fel2023unlocking}, which optimizes the input phase while preserving its magnitude. Once the optimized inputs $\x^{(i)}_{\star}$ are obtained for each model, they reveal the specific structures or features (e.g., model- or task-specific biases) that model $i$ associates with this universal concept.














