\vspace{-3mm}
\section{Experimental Results}
\vspace{-1mm}
This section is split into six parts. We first provide experimental implementation details. Then, we qualitatively analyze universal concepts discovered by USAEs (Sec.~\ref{sec:universal_visualizations}). Next, we provide a quantitative analysis of USAEs through the validation of activation reconstruction (Sec.~\ref{sec:val_reconstruct}), measuring the universality and importance of concepts (Secs.~\ref{sec:concept_universality}), and investigating the consistency between concepts in USAEs and individually trained SAE counterparts (Sec.~\ref{sec:consistency}). Finally, we provide a finer-grained analysis via the application of USAEs to \textit{coordinated activation maximization} (Sec.~\ref{sec:act_max_results}).

\vspace{-2mm}
\paragraph{Implementation Details.} 
We train a USAE on the final layer activations of three popular vision models: DinoV2~\cite{oquab2023dinov2,darcet2023vision}, SigLIP~\cite{zhai2023sigmoid}, and ViT~\cite{dosovitskiy2020image} (trained on ImageNet~\cite{deng2009imagenet}). These models, sourced from the \texttt{timm} library~\cite{wightman2019pytorch}, were selected due to their diverse training paradigms—image and patch-level discriminative learning (DinoV2), image-text contrastive learning (SigLIP), and supervised classification (ViT).
For all experiments, we train the USAE on the ImageNet training set, while the validation set is reserved for qualitative visualizations and quantitative evaluations. 
Our USAE is trained on the final layer representations of each vision model, as previous work showed final-layer features facilitate improved concept extraction and yield accurate estimates of feature importance~\cite{fel2023holistic}. We base our SAE off of the TopK Sparse Autoencoder (SAE)~\cite{gao2024scaling} and for all experiments, use a dictionary of size $6144$. We train all USAEs on a single Nvidia RTX 6000 GPU, with training completing in approximately three days (see Appendix~\ref{appendix:imp_details} for more implementation details).



\vspace{-3mm}
\subsection{Universal Concept Visualizations}\label{sec:universal_visualizations}
\vspace{-1mm}
We qualitatively validate 
the most important universal concepts found by USAEs. We determine concept importance by measuring its relative energy towards reconstruction~\cite{gillis2020nonnegative}, where the energy of a concept $k$ is defined as
\begin{equation}
    \text{Energy}(k) = \|\mathbb{E}_{\x}[\Z_k(\x)] \D_k\|_2^2. 
\end{equation}
This measures how much each concept contributes to reconstructing the original features -- formally, the squared $\ell_2$ norm of the average activation of a concept multiplied by its dictionary element. Higher energy concepts have a greater impact on the reconstruction.

Figure~\ref{fig:qualitative_universal} presents eight representative concepts selected from the 100 most important USAE concepts. These concepts span a diverse range of ImageNet categories, demonstrating the ability of USAEs to capture meaningful features across multiple levels of abstraction and complexity~\cite{olah2017feature,fel2024understanding}.
At lower levels, the USAE extracts fundamental color concepts, such as `yellow' and `blue', activating over broad spatial regions across multiple classes. Notably, the blue bottle caps example highlights a precisely captured checkerboard pattern, demonstrating spatial precision. 
At intermediate levels, the USAE uncovers structural relationships consistent across models, such as foreground-background contrasts (e.g., birds against the sky) and thin, wiry objects, independent of model architecture.
At higher levels, it identifies object-part concepts, like `dog face', excluding eye regions, and `bolts', which activate across materials like metal and rubber.
Finally, the USAE reveals fine-grained, compositional concepts such as `mouth-open animal jaws' and `faces of animals in a group', which generalize across ImageNet classes and persist even in ViT, despite its lack of explicit structured supervision.


Overall, these findings show that USAEs discover robust, generalizable concepts that persist across different architectures, training tasks, and datasets. This highlights their ability to reveal invariant, semantically meaningful representations that transcend the specifics of any single model.



\begin{figure}
    \centering
    \includegraphics[width=0.6\linewidth]{images/R2_matrix_v2.pdf}
    \vspace{-0.43cm}
    \caption{\textbf{Cross model activation reconstruction.} 
    Each entry \((i, j)\) represents the average \( R^2 \) score when activations from model \( \A^{(i)} \) are encoded into the shared code space, \( \Z \), then decoded via \( \D^{(j)} \) to reconstruct \( \widehat{\A}^{(j)} \). Positive off-diagonal \( R^2 \) scores indicate the presence of shared features across models captured by USAEs.}
      \vspace{-20pt}
    \label{fig:R2matrix}
  
\end{figure}



\begin{figure}
    \begin{minipage}{0.49\linewidth}  %
    \centering
    \includegraphics[width=\linewidth]{images/firing_entropy_dist_resize_v4.pdf}
    \end{minipage}
        \begin{minipage}{0.499\linewidth}  %
    \centering
    \includegraphics[width=\linewidth]{images/firing_proportions_top1k_resize_v7.pdf}
    \end{minipage}
    \begin{minipage}{1\linewidth}  %
    \centering
    \includegraphics[width=\linewidth]{images/cofire_importance_coerr_v7.pdf}
    \end{minipage}
    \vspace{-15pt}
    \caption{\textbf{Quantitative analysis of universality and importance of USAE concepts via co-firing rates.}
    (a) Histogram of firing entropy across all $k$ concepts. We observe a bimodal distribution over firing entropy with peaks at $H_k = 1$ and $H_k = 0.6$, demonstrating a group of concepts that fire uniformly across models and a group that preferentially activates for some models.
    (b) Proportion of concept co-fires for the top 1000 energetic concepts per model. The first 200 concepts co-fire between $60-80\%$ of the time suggesting high universality. 
    (c) Relationship between concept co-firing frequency and concept energy. We show all concepts (left) and only frequently co-firing concepts $(\geq 1000 \text{ co-fires})$ (right). The correlation strengthens ($r=0.63$ vs $r=0.89$) when focusing on high-frequency concepts, suggesting a strong correlation between how energetic a concept is and its universality.}
    \vspace{-6mm}
    \label{fig:quant_entropy}
\end{figure}




\vspace{-2mm}
\subsection{Validation of Cross-Model Reconstruction}\label{sec:val_reconstruct}
\vspace{-1mm}
A viable universal space of concepts should enable the reconstruction of activations from any model.
To quantify the reconstruction performance, we use the coefficient of determination, or $R^2$ score \cite{wright1921correlation}, which measures the proportion of variance in the original activations that is captured by the reconstructed activations, relative to the mean activation baseline, $\bar{\A}$. The $R^2$ score is defined as
\begin{equation}
    R^2 = 1 - \|\A - \widehat{\A}\|_F^2/\|\A - \bar{\A}\|_F^2,
\end{equation}
where \( ||\A - \widehat{\A}||_F^2 \) represents the residual sum of squares (the reconstruction error), and \( ||\A - \bar{\A}||_F^2 \) is the total sum of squares (the variance of the original activations relative to their mean).
A higher $R^2$ indicates better reconstruction quality, with a score of one corresponding to a perfect reconstruction.

Figure~\ref{fig:R2matrix} shows the $R^2$ scores as a confusion matrix across all three models. As expected, self-reconstruction along the diagonal achieves the highest explained variance, confirming the USAE’s effectiveness when encoding and decoding within the same model. More notably, positive off-diagonal $R^2$ scores indicate successful cross-model reconstruction, suggesting the USAE captures shared, likely universal, features. DinoV2 exhibits the highest self-reconstruction performance, aligning with individual SAE results where its $R^2$ score averages 0.8, compared to 0.7 for SigLIP and ViT. This suggests DinoV2 features are sparser and more decomposable, a trend further supported in Secs.~\ref{sec:concept_universality} and~\ref{sec:act_max_results}.



\vspace{-2mm}
\subsection{Measuring Concept Universality and Importance}\label{sec:concept_universality}
\vspace{-1mm}
Having established the efficacy of cross-model reconstruction, we now assess concept \textit{universality} using \emph{firing entropy} and \emph{co-firing} metrics. We further examine the relationship between \textit{universality} and \textit{importance} in reconstructing ground truth activations.

Let $\tau$ be a threshold value and $\mathcal{V}$ be the ImageNet validation set of patches. Given data points $\x \in \mathcal{V}$, let $\Z^{(i)}(\x) = \encoder^{(i)}(\f^{(i)}(\x))$ denote the sparse code from model $i \in [M]$. We define a concept firing for dimension $k$ when $\Z_k^{(i)}(\x) > \tau$. A co-fire occurs when a concept fires simultaneously across all models for the same input. Formally, for concept dimension $k$, the set of co-fires is defined as
\vspace{-1mm}
\begin{equation}
\mathcal{C}_k = \{\x \in \mathcal{V} : \min_{i \in [M]} \Z_k^{(i)}(\x) > \tau\}.
\vspace{-2mm}
\end{equation}
Similarly, let $\mathcal{F}^{(i)}_k = \{\x \in \mathcal{V} : \Z_k^{(i)}(\x) > \tau\}$ denote the set of ``fires'' for model $i$ and concept $k$. 
We are now ready to introduce our two metrics (\textbf{\textit{i}}) Firing Entropy (\textbf{FE}) and (\textbf{\textit{ii}}) Co-Fire Proportion (\textbf{CFP}). 

\vspace{-1mm}
\noindent\textbf{Firing Entropy ({FE})}  
measures, for each concept $k$, the normalized entropy across models, as
\vspace{-2mm}
\begin{equation}
\text{\textbf{FE}}_k = -\frac{1}{\log M}\sum_{i=1}^M p^{(i)}_k \log p^{(i)}_k,
\label{eq:cofire_metric_entropy}
\vspace{-1mm}
\end{equation}
where
\vspace{-2mm}
\begin{equation}
p^{(i)}_k = {|\mathcal{F}^{(i)}_k|}/{\sum_{j=1}^M |\mathcal{F}^{(j)}_k|}.
\label{eq:cofire_metric_probs}
\vspace{-1mm}
\end{equation}
The normalization ensures $\text{\FE}_k \in [0,1]$, with $\FE = 1$ indicating a shared concept with uniform firing across models and
low entropy indicating that a concept has a model bias and fires for a single architecture or subset. 

\begin{figure}
    \centering
    \begin{minipage}{0.48\linewidth}  %
        \centering
        \includegraphics[width=\linewidth]{images/ROC_curve_v5.pdf}
    \end{minipage}%
    \hfill
    \begin{minipage}{0.48\linewidth} %
        \centering
        \resizebox{\linewidth}{!}{
        \begin{tabular}{@{}lcc@{}}
            \toprule
            Model & AUC & \% $\Z>$ 0.5 \\
            \midrule
            SigLIP & 0.30 & 0.23 \\
            DinoV2 & 0.36 & 0.26 \\
            ViT & 0.41 & 0.38 \\
            Baseline & 0.13 & 0.00 \\
            \bottomrule
        \end{tabular}}
    \end{minipage}
    \vspace{-0.3cm}
    \caption{\textbf{Concept consistency between independent SAEs and Universal SAEs.} (left) Our universal training objective discovers concepts that have overlap (i.e., cosine similarity) with those discovered with independent training. Specifically, ViT has noticeably more overlap, suggesting its simpler architecture and training objective may yield activations that naturally encode fundamental and universal visual concepts. (right) We consider a cosine similarity $>0.5$ as a match between concepts in the SAE and USAE learned dictionaries. Across each vision model used in training, $23-37\%$ of the highly universal concepts discovered by our approach exist in independently trained SAEs.}
    \label{fig:ROC_MCS}
    \vspace{-15pt}
\end{figure} 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{images/tf_act_max.jpg}
        \vspace{-13pt}
    \caption{\textbf{Coordinated Activation Maximization.} We show results for the three model USAE along with dataset exemplars, where bright green denotes stronger activation of the concept. We visualize the maximally activating input for a broad range of concepts, including basic shape compositions, textures, and various objects.
    }
    \vspace{-15pt}
    \label{fig:act_max_results}
\end{figure*}

Figure~\ref{fig:quant_entropy} (a) shows a histogram of firing entropies across all concept dimensions $K$. Fully universal concepts should have a maximum entropy of one, indicating uniform firing across models. 
Our results exhibit a bimodal distribution, with over 1000 concepts at peak entropy, confirming the USAE learns a strongly universal concept space.
A second group shows moderate entropy, indicating concepts that favor two models but not all three.
Few concepts fall in the low-entropy range (0.0–0.2), suggesting most are shared rather than model-specific. Appendix~\ref{appendix:unique_dino} further examines these low-entropy concepts, revealing DinoV2’s unique encoding of \textit{geometric} features as well as SigLIP's encoding of \textit{textual} features.

\noindent \textbf{Co-Fire Proportion (\CFP)} quantifies how often concepts fire together for the same input. While previous results show many concepts fire uniformly across models, they do not reveal how frequently they co-fire on the same tokens. For each model $i$ and concept $k$, we compute the proportion of total fires that are also co-fires:
\begin{equation}
\text{\CFP}^{(i)}_k = |\mathcal{C}_k|/|\mathcal{F}^{(i)}_k|.
\end{equation}
High co-fire proportions indicate concepts that are more universal, i.e., when one model detects the concept, others tend to as well. 


Figure~\ref{fig:quant_entropy} (b) shows the \textbf{CFP} for the top $1000$ concepts per model. The first ${\sim}100$ concepts exhibit high co-firing $(>0.5)$, activating together 50–80\% of the time, indicating a core set of consistently recognized concepts across networks. The gradual decline in \textbf{CFP} suggests a spectrum of universality, from widely shared to model-specific. For our chosen models, we again notice a pattern distinguishing DinoV2, which has a lower co-firing proportion (0.266) compared to SigLIP (0.344) and ViT (0.326), suggesting the latter two share more concepts. This may stem from DinoV2’s architecture and distillation-based training, which enhance its adaptability to diverse vision tasks~\cite{amir2021deep}. These findings also hint at a correlation between co-firing and concept importance, raising the question: How important are these highly co-firing features?


To answer this, we plot the co-fire frequency of all concepts as well as their energy-based importance in Fig.~\ref{fig:quant_entropy} (c). We see a moderate positive correlation \(r=0.63\text{, slope}=0.23\); however, zooming into concepts with $>1000$ co-fires, shows a much stronger correlation. Indeed, past a certain threshold, co-firing frequency becomes highly predictable of concept importance. This suggests that \textbf{the most important concept are also highly universal}, firing consistently across models. 



















\vspace{-2mm}
\subsection{Concept Consistency Between USAEs and SAEs}\label{sec:consistency}
\vspace{-1mm}
How many concepts discovered under our universal training regime are present in an independently trained SAE for a single model? Further, what percentage of highly universal concepts appear in these same independently trained SAEs? 
To assess the alignment between independently-trained and universal SAEs, we analyze the similarity of their learned conceptual spaces. 
We quantify concept overlap by computing pairwise cosine similarities between decoder vectors and use the Hungarian algorithm~\cite{kuhn1955hungarian} to optimally align concepts, measuring consistency across models.








Figure~\ref{fig:ROC_MCS} presents concept consistency distributions across models. For a baseline to compare against, we sample concept vectors from normal distributions, where the mean and variance are those of each independent model's dictionary. We observe that ViT has the strongest concept overlap with $38\%$ of its concepts having a cosine similarity $>0.5$ with its independent counterpart. This suggests ViT's conceptual representation under the independent SAE objective is most well preserved under universal training. USAEs achieve far better performance than the baseline (Area Under the Curve (AUC)=$0.13$) across models, suggesting that universal training preserves meaningful concept alignments rather than learn entirely new representations. On the other hand the relatively low proportion of overlap ($23\%$ and $26\%$ for SigLIP and DinoV2, respectively) for concepts indicates that \textbf{universal training discovers concepts that may not emerge in independent training}. 
Importantly, this distribution remains when looking at the \textit{top 1,000 co-firing} concepts (see Sec.~\ref{sec:appendix_quant}). 
Universal training naturally selects for concepts that are well-represented across all models, since these will better minimize the total reconstruction loss, biasing towards discovering fundamental visual concepts that all models have learned to represent. Independently trained SAEs have no such selection pressure, learning to represent any concept that helps reconstruction, including architecture or objective specific concepts that are not universal.   

\vspace{-3mm}
\subsection{Coordinated Activation Maximization}\label{sec:act_max_results}
\vspace{-1mm}
Figure \ref{fig:act_max_results}
shows a visual comparison of several universal concepts and their corresponding coordinated activation maximization inputs. Our method produces interpretable visualizations for a given USAE dimension across all models for a broad range of visual concepts. We show examples of all models encoding low-level visual primitives, e.g., `curves' and `crosses'. Other basic entities are also shown, like `brown grass' texture and `round objects'. Finally, we visualize higher-level concepts corresponding to `objects from above' and `keypads'. In all cases, %
our coordinated activation maximization method produces plausible visual phenomenon that can be used to \textit{identify differences between how each model encodes the same concept}. 

For example, we note an interesting difference between DinoV2 and the other models: low-mid level concepts (i.e., left two columns) appear at a much \textit{larger scale} than the other models. %
Further, as shown in Fig.~\ref{figure:method_overview}, DinoV2 exhibits stronger activation for the ‘curves’ concept, particularly for larger curves, compared to the other models.
Additionally, while `brown grass' activates on grass in our heatmaps, some models' activation maximizations include birds, suggesting animals also influence the concept's activation. 












