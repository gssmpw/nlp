\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{comment}
\usepackage{booktabs} %

\usepackage{hyperref}
\usepackage{float}
\usepackage{caption}

\newcommand{\theHalgorithm}{\arabic{algorithm}}


\usepackage[accepted]{icml2025}
\input{preamble}
\usepackage{code}
\usepackage[capitalize,noabbrev]{cleveref}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}




\icmltitlerunning{Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment}

\begin{document}

\twocolumn[{
\icmltitle{Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment}

\begin{icmlauthorlist}
\icmlauthor{Harrish Thasarathan}{york,vector}
\icmlauthor{Julian Forsyth}{york}
\icmlauthor{Thomas Fel}{harvard}
\icmlauthor{Matthew Kowal}{york,vector,far}
\icmlauthor{Konstantinos Derpanis}{york,vector}
\end{icmlauthorlist}

\icmlaffiliation{york}{EECS York University, Toronto, Canada}
\icmlaffiliation{harvard}{Kempner Institute, Harvard University, Boston, USA}
\icmlaffiliation{vector}{Vector Institute, Toronto, Canada}
\icmlaffiliation{far}{FAR AI}
\icmlcorrespondingauthor{Harrish Thasarathan}{harryt@yorku.ca}

\vspace{5mm} 

\begin{center}
    \includegraphics[width=0.98\textwidth]{images/big_picture_v6.jpg}
    \vspace{-0.4cm}
    \captionof{figure}{\textbf{Overview of Universal Sparse Autoencoders.} (A) We introduce \textit{Universal Sparse Autoencoders} (USAEs), a method for discovering common concepts across multiple different deep neural networks. USAEs are simultaneously trained on the activations of multiple models and are constrained to share an aligned and interpretable dictionary of discovered concepts. (B) We also demonstrate one immediate application of USAEs, \textit{Coordinated Activation Maximization}, where optimizing the inputs of multiple models to activate the same concepts reveals how different models encode the same concept. Visualization reveals interesting concepts at various levels of abstraction, such as `curves' (top), `animal haunch' (middle) and `the faces of crowds' (bottom). Better viewed with zoom.}
    \label{figure:method_overview}
    \vspace{-5mm}
\end{center}

\icmlkeywords{Machine Learning, ICML}
\vskip 0.3in
}]

\printAffiliationsAndNotice{} %





\input{sec/0_abstract}   
\input{sec/1_intro}
\input{sec/2_related_work}
\input{sec/3_method}
\input{sec/4_experiments}
\input{sec/6_conclusion}




\bibliography{main,dictionarylearning,refs_shorthand}
\bibliographystyle{icml2025}



\input{sec/X_appendix}


\end{document}
