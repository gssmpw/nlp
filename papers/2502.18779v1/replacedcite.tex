\section{Related Works}
\label{se:related_work}
\subsection{Draft Model Design}
Numerous studies have explored the design of better draft models for speculative decoding. In principle, any autoregressive probabilistic model can serve as a draft model. The simplest approaches include using n-gram models ____ or document retrieval as draft models ____. Small transformer-based language models have also been employed ____, often with distillation techniques to further increase the overlap between the draft and target models ____.


The design of a good draft model involves a trade-off between its similarity to the target model and its computational complexity. More complex draft models lead to higher acceptance rates due to their closer resemblance to the target model, but they also incur higher computational overhead. To achieve a better trade-off, some works have proposed reusing the target model's computational results. For example, ____ use the original model with ``look ahead" tokens, while ____ add new heads to the last hidden layer of the original model to predict tokens further ahead. ____ reuse the last layer hidden state computation of the large model and introduce a new attention layer to predict the next token. ____ employ the target model with a partial key-value cache as the draft model.


\subsection{Multi-Draft Speculative Decoding}\label{sec:related_mdsd}
Many related works on Multi-Draft Speculative Decoding (MDSD) have been introduced in other sections. This paper focuses on the single-step Multi-Draft scenario. When MDSD generates multiple steps, with each step involving multiple drafts, it forms a tree structure. Sequoia ____ propose a dynamic programming algorithm to search for the optimal tree topology.

As the tree grows deeper, the acceptance probability of certain branches decreases. Cascade Speculative Drafting ____ addresses this issue by assigning the largest draft model to generate draft tokens at shallower levels, which are more likely to be accepted, and gradually using smaller models to generate drafts for less relevant branches.

____ studied the optimal acceptance rate for special case of sampling with replacement for $n=2$ drafts, and obtained the following result:
\begin{equation}
\alpha^\ast(p,p_{\opn{draft}}) = \min_{H \subset\Sigma} \left\{
\sum_{i\in H} p(i)+\left(\sum_{i\in \Sigma\setminus H} q(s)\right)^2+2\left(\sum_{i\in H} q(s)\right)\left(\sum_{i\in \Sigma\setminus H} q(s)\right)
\right\}
~.
\end{equation}

This is essentially the same as our result \eqref{eq:subset} under this special case. 
However, our theory is more general, without any assumption on the draft sampling methods or the number of draft tokens.

\subsection{Multi-Step Speculative Decoding}\label{sec:related_multi_step}
The basic single-step, single-draft speculative decoding, as introduced in \Cref{sec:prelim_basic_sps}, can be applied to multiple steps, with each step having only one draft and an independent verification process ____. However, such an approach of repeatedly applying single-step verification is not optimal for the multi-step scenario. Some works, such as ____, have designed better verification algorithms specifically for the multi-step setting. These algorithms are tailored for the multi-step scenario while remaining compatible with the single-step case, reducing to the basic speculative sampling algorithm when applied to a draft sequence of length 1.

\begin{figure}[h]
\centering
\begin{tikzpicture}[
    box/.style={draw, rectangle, minimum width=4.5cm, minimum height=1.2cm, align=center},
    smallbox/.style={draw, rectangle, minimum width=3.5cm, minimum height=1.2cm, align=center},
    arrow/.style={->, thick},
]

\node[box] (base) at (0,0) {Speculative Decoding \\ ____ \\ ____};

\node[smallbox] (ms1) at (6,0) {____ \\ ____ \\ ____};

\node[box] (md1) at (0,-3) {____ \\ ____ \\ This paper};



\draw[arrow] (base) -- node[above] {multi-step} (ms1);
\draw[arrow] (base) -- node[left] {multi-draft} (md1);


\end{tikzpicture}

\caption{Different directions for improving speculative decoding.}
\label{fig:related_work}
\end{figure}


Multi-step speculative decoding and multi-draft speculative decoding represent different directions for improvement.

As shown in \Cref{fig:related_work}, ____ and our work improve speculative decoding from the multi-draft perspective. When there is only a single draft, it reduces to the case in ____. On the other hand, ____ enhance speculative decoding from the multi-step perspective. When there is only a single step, it reduces to the case in ____.

Combining both improvements in the multi-draft and multi-step scenario would be ideal, and could be a direction for future research.