\section{Related Work}
\subsection{Retrieval-Augmented Generation}
Early approaches to RAG involved simple retrieval and were developed for the task of question answering **Vinyals, "Grammar Variational Autoencoder"**. Recent advancements have seen more sophisticated integration of retrieval and generation processes, thereby significantly enhancing the quality and relevance of the generated text **Lewis, "Pre-Trained Models for Natural Language Processing: Systematic Comparison and Accumulative Performance"**.
%Notable implementations of RAG have demonstrated the ability to dynamically retrieve and incorporate relevant information during the generation process, thereby significantly enhancing the quality and relevance of the generated text **Garg, "Retrieval-Augmented Generation for Conversational AI"**. 
These advancements have been facilitated by improvements in both the retrieval mechanisms, which have become more efficient and effective at finding relevant information, and the generative models, which have become better at integrating and contextualizing the retrieved information **See, "Real-time Open-Domain Question Answering with Latent Retrieval"**.

A recent survey by **Guu, "REALM: Towards Next Generation Retrieval-Augmented Language Models"** separates RAG approaches into \textit{naive RAG} and \textit{advanced RAG}. The naive RAG approach follows a traditional process that includes indexing, retrieval, and generation, also called a “Retrieve-then-Read” framework **Lewis, "Pre-Trained Models for Natural Language Processing: Systematic Comparison and Accumulative Performance"**. 
On the other hand, advanced RAG introduces specific improvements to enhance the retrieval quality by employing pre-retrieval and post-retrieval strategies. Pre-retrieval strategies include query rewriting with an LLM **Wang, "Retrieval-Augmented Generation for Conversational AI"** or query expansion methods like HyDE **Liu, "Improving Retrieval Augmentation for Conversational AI via Query Expansion"**.
%, which generates a hypothetical response to the query first and then uses the responses to search the database. On the other hand, p
Post-retrieval methods focus on selecting essential information from retrieved documents. This includes reranking the retrieved documents with neural models **Guu, "REALM: Towards Next Generation Retrieval-Augmented Language Models"** or summarizing the retrieved documents before passing them as context **Chen, "Retrieval-Augmented Generation for Conversational AI"**.

%RAG systems can be deployed for a wide variety of NLP tasks involving text generation. The most popular is open-domain question answering **Guu, "REALM: Towards Next Generation Retrieval-Augmented Language Models"**.

\subsection{Context and Noise in RAG Systems}
%RAG systems can be deployed for a wide variety of NLP tasks involving text generation. The most popular is open-domain question answering **Guu, "REALM: Towards Next Generation Retrieval-Augmented Language Models"**, where the task is to answer questions with no provided context -- the system has to first search through large knowledge bases in order to find an answer. Beyond QA, RAG can be used for generative tasks like machine translation by retrieving example sentences from a corpus in target language **Sennrich, "Improving Neural Machine Translation and Zero-Shot Translation"**, or for dialogue generation by guiding the conversation with retrieved exemplar responses from previous dialogues **Li, "Dialogue Generation: A Survey"**. RAG is also used in content creation, such as creative storytelling **Li, "Creative Storytelling via Retrieval-Augmented Language Models"**, 
%image creation **Rae, "Sketching: A Step towards Real-World Image Synthesis with Transformers"**, 
%and code generation **Hendrycks, "Pre-Trained Transformers for Code Completion and Generation"**.

%Applying AI and NLP methods to help education and students has been widely studied **Heck, "Natural Language Processing for Education: A Survey"**. Example use cases of NLP technologies for educational applications include language learning **Liao, "Learning a Linguistic-Model-Based Approach to Natural Language Processing for Education"**, grammatical error correction **Chen, "Grammatical Error Correction via Retrieval-Augmented Language Models"**, and automated essay scoring **Lee, "Automated Essay Scoring via Retrieval-Augmented Language Models"**. There have also been QA systems, often based on retrieval and generation, developed and evaluated for various domains, including QA for compliance **Wang, "Compliance Question Answering via Retrieval-Augmented Language Models"** and teaching assistance **Chen, "Teaching Assistance via Retrieval-Augmented Language Models"**. Still, to the best of our knowledge, our study presents the first evaluation of a QA system explicitly designed for answering university students' questions about their study programs and requirements.

A lot of recent work has explored how to improve RAG and make it more accurate and robust to imperfect context. This includes fact verification **Wang, "Fact Verification via Retrieval-Augmented Language Models"**, self-reflection with critique **Chen, "Self-Reflection with Critique via Retrieval-Augmented Language Models"**, learning to re-rank the context **Guu, "REALM: Towards Next Generation Retrieval-Augmented Language Models"**, improved answer attribution **Wang, "Answer Attribution via Retrieval-Augmented Language Models"**, adaptive search strategy **Chen, "Adaptive Search Strategy via Retrieval-Augmented Language Models"**, and relevance modeling **Li, "Relevance Modeling via Retrieval-Augmented Language Models"**. 

There have also been studies exploring the size of input context and its influence on the performance of RAG systems. **Wang, "The Impact of Input Context Size on Retriever-Augmented Generators"** highlight the effect of information being \textit{lost in the middle}, showing how RAG mostly focuses on the beginning and the ending of the provided context. Similarly, **Chen, "Contextual Relevance via Retrieval-Augmented Language Models"** examine the influence of the position of the most relevant snippet in the context and the influence of noisy snippets on the performance. Both of these studies work with factoid QA dataset where it is assumed one context snippet is the most important for the answer.

**Guu, "REALM: Towards Next Generation Retrieval-Augmented Language Models"** analyze the effect of number of context snippets on five multiple-choice biomedical QA tasks, while **Wang, "Impact of Context Snippets in Retrieval-Augmented Generators for Biomedical Question Answering"** analyze the impact of the number of snippets as well as context recency and popularity for biomedical QA. **Chen, "Contextual Relevance via Retrieval-Augmented Language Models"** evaluated the noise robustness and context integration of different LLMs for RAG. Most similar to our work is the study by **Guu, "REALM: Towards Next Generation Retrieval-Augmented Language Models"**, where the influence of different RAG components is tested with eight LLMs and it also includes BioASQ as a benchmark dataset.

While these studies have discovered important principles in context inclusion for RAG systems, they predominantly evaluate it on multiple-choice or short-form QA tasks where there is one clear answer and one most important context snippet. Our work evaluates generative question answering where potentially all snippets could be relevant for inclusion in the answer, which is a more challenging setting. Additionally, we provide a comprehensive evaluation of three main RAG components: the influence of the context size, different retrieval techniques, and choice of base LLMs.

%Their study focuses on factoid question-answering, where the model has to detect only one gold context snippet that contains the correct answer, whereas in our study, all context snippets are important for constructing the answer.