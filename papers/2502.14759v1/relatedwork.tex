\section{Related Work}
\subsection{Retrieval-Augmented Generation}
Early approaches to RAG involved simple retrieval and were developed for the task of question answering \cite{chen-etal-2017-reading}. Recent advancements have seen more sophisticated integration of retrieval and generation processes, thereby significantly enhancing the quality and relevance of the generated text \cite{lewis2020retrieval}.
%Notable implementations of RAG have demonstrated the ability to dynamically retrieve and incorporate relevant information during the generation process, thereby significantly enhancing the quality and relevance of the generated text \cite{lewis2020retrieval}. 
These advancements have been facilitated by improvements in both the retrieval mechanisms, which have become more efficient and effective at finding relevant information, and the generative models, which have become better at integrating and contextualizing the retrieved information \cite{cai2022recent}.

A recent survey by \citet{gao2024retrievalaugmented} separates RAG approaches into \textit{naive RAG} and \textit{advanced RAG}. The naive RAG approach follows a traditional process that includes indexing, retrieval, and generation, also called a “Retrieve-then-Read” framework \cite{zhu2021retrieving}. 
On the other hand, advanced RAG introduces specific improvements to enhance the retrieval quality by employing pre-retrieval and post-retrieval strategies. Pre-retrieval strategies include query rewriting with an LLM \cite{ma-etal-2023-query} or query expansion methods like HyDE \cite{gao-etal-2023-precise}.
%, which generates a hypothetical response to the query first and then uses the responses to search the database. On the other hand, p
Post-retrieval methods focus on selecting essential information from retrieved documents. This includes reranking the retrieved documents with neural models \cite{glass-etal-2022-re2g} or summarizing the retrieved documents before passing them as context \cite{an2021retrievalsum}. 

%RAG systems can be deployed for a wide variety of NLP tasks involving text generation. The most popular is open-domain question answering \cite{siriwardhana2023improving}.

\subsection{Context and Noise in RAG Systems}
%RAG systems can be deployed for a wide variety of NLP tasks involving text generation. The most popular is open-domain question answering \cite{siriwardhana2023improving}, where the task is to answer questions with no provided context -- the system has to first search through large knowledge bases in order to find an answer. Beyond QA, RAG can be used for generative tasks like machine translation by retrieving example sentences from a corpus in target language \cite{cai-etal-2021-neural}, or for dialogue generation by guiding the conversation with retrieved exemplar responses from previous dialogues \cite{gupta-etal-2021-controlling}. RAG is also used in content creation, such as creative storytelling \cite{wen-etal-2023-grove}, 
%image creation \cite{Chen2022ReImagenRT}, 
%and code generation \cite{lu-etal-2022-reacc}.

%Applying AI and NLP methods to help education and students has been widely studied \cite{zhang2021ai, kasneci2023chatgpt}. Example use cases of NLP technologies for educational applications include language learning \cite{settles2018second}, grammatical error correction \cite{grundkiewicz2019neural}, and automated essay scoring \cite{ramesh2022automated}. There have also been QA systems, often based on retrieval and generation, developed and evaluated for various domains, including QA for compliance \cite{abualhaija2022automated} and teaching assistance \cite{zylich2020exploring}. Still, to the best of our knowledge, our study presents the first evaluation of a QA system explicitly designed for answering university students' questions about their study programs and requirements.

A lot of recent work has explored how to improve RAG and make it more accurate and robust to imperfect context. This includes fact verification \cite{li-etal-2024-llatrieval}, self-reflection with critique \cite{asai2024selfrag}, learning to re-rank the context \cite{yu2024rankrag}, improved answer attribution \cite{kdir24vladika}, adaptive search strategy \cite{jeong-etal-2024-adaptive}, and relevance modeling \cite{wang-etal-2024-rear}. 

There have also been studies exploring the size of input context and its influence on the performance of RAG systems. \citet{liu-etal-2024-lost} highlight the effect of information being \textit{lost in the middle}, showing how RAG mostly focuses on the beginning and the ending of the provided context. Similarly, \citet{cuconasu2024power} examine the influence of the position of the most relevant snippet in the context and the influence of noisy snippets on the performance. Both of these studies work with factoid QA dataset where it is assumed one context snippet is the most important for the answer.

\citet{xiong-etal-2024-benchmarking} analyze the effect of number of context snippets on five multiple-choice biomedical QA tasks, while \citet{vladika-matthes-2024-improving} analyze the impact of the number of snippets as well as context recency and popularity for biomedical QA. \citet{10.1609/aaai.v38i16.29728} evaluated the noise robustness and context integration of different LLMs for RAG. Most similar to our work is the study by \citet{hsia2024ragged}, where the influence of different RAG components is tested with eight LLMs and it also includes BioASQ as a benchmark dataset.

While these studies have discovered important principles in context inclusion for RAG systems, they predominantly evaluate it on multiple-choice or short-form QA tasks where there is one clear answer and one most important context snippet. Our work evaluates generative question answering where potentially all snippets could be relevant for inclusion in the answer, which is a more challenging setting. Additionally, we provide a comprehensive evaluation of three main RAG components: the influence of the context size, different retrieval techniques, and choice of base LLMs.

%Their study focuses on factoid question-answering, where the model has to detect only one gold context snippet that contains the correct answer, whereas in our study, all context snippets are important for constructing the answer.