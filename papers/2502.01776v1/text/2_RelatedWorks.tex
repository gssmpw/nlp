% Related Works
% 1. Mention:
%   PAB, FasterCache, TeaCache, DiTFastAttn
%   TinyFusion, 
%   Latent Consistent Model/Distillation
% 2. Copy from SVDQuant
%   However, diffusion models suffer from extremely slow inference speed due to their long denoising sequences and intense computation. To address this, various approaches have been proposed, including few-step samplers (Zhang & Chen, 2022; Zhang et al., 2022; Lu et al., 2022) or distilling fewer-step models from pre-trained ones (Salimans & Ho, 2021; Meng et al., 2022a; Song et al., 2023; Luo et al., 2023; Sauer et al., 2023; Yin et al., 2024b; a; Kang et al., 2024). Another line of works choose to optimize or accelerate computation via efficient architecture design (Li et al., 2023b; 2020; Cai et al., 2024; Liu et al., 2024a), quantization (Shang et al., 2023; Li et al., 2023a), sparse inference (Li et al., 2022; Ma et al., 2024b; a), and distributed inference (Li et al., 2024b; Wang et al., 2024c; Chen et al., 2024b). This work focuses on quantizing the diffusion models to 4 bits to reduce the computation complexity. Our method can also be applied to the few-step diffusion models to further reduce the latency (see Section 5.2).
% 3. Copy from TeaCache
%   Despite the notable performance of Diffusion models in image and video synthesis, their significant inference costs hinder practical applications. Efforts to accelerate Diffusion model inference fall into two primary categories. First, techniques such as DDIM [45] allow for fewer sampling steps without sacrificing quality. Additional research has focused on efficient ODE or SDE solvers [19, 20, 26, 27, 46], using pseudo numerical methods for faster sampling. Second, approaches include distillation [36, 51], quantization [15, 25, 39, 43], and distributed inference [22] are employed to reduce the workload and inference time. However, these methods often demand additional resources for fine-tuning or optimization. Some training-free approaches [5, 49] streamline the sampling process by reducing input tokens, thereby eliminating redundancy in image synthesis. Other methods reuse intermediate features between successive timesteps to avoid redundant computations [42, 54, 58]. DeepCache [55] and Faster Diffusion [23] utilize feature caching to modify the UNet Diffusion, thus enhancing acceleration. FORA [38] and △- DiT [11] adapts this mechanism to DiT by caching residuals between attention layers. PAB [59] caches and broadcasts intermediate features at various timestep intervals based on different attention block characteristics for video synthesis. While these methods have improved Diffusion efficiency, enhancements for DiT in visual synthesis remain limited.
% 4. Copy from MInference
%   Sparse Attention Due to the quadratic complexity of the attention mechanism, many previous works have focused on sparse attention to improve the efficiency of Transformers. These methods include static sparse patterns, cluster-based sparse approaches, and dynamic sparse attention. Static sparse patterns include techniques such as sliding windows [JSM+23, AJA+24], dilated attention [CGRS19, SGR+21, DMD+23], and mixed sparse patterns [BPC20, ZGD+20, LCSR21]. Cluster-based sparse methods include hash-based [KKL20] and kNN-based [RSVG21, NŁC+24] methods. All of the above methods require pre-training the model from scratch, which makes them infeasible to be directly used as a plugin for reay-to-use LLMs. Recently, there has been work [DG24, ZAW24] to unify state space models [GGR22, GD23, DG24], and linear attention [KVPF20, SDH+23] into structured masked attention. Additionally, some works [WZH21, LQC+22, RCHG+24] leverage the dynamic nature of attention to predict sparse patterns dynamically. However, these approaches often focus on low-rank hidden states during the dynamic pattern approximation or use post-statistical methods to obtain the sparse mask, which introduce substantial overhead in the estimation step, making them less useful for long-context LLMs.
% 5. Copy from PAB
%   Advancements in video diffusion models have demonstrated their potential for high-quality video generation, yet their practical application is often limited by slow inference speeds. Previous research about speeding up diffusion model inference can be broadly classified into three categories. First, reducing the sampling time steps has been explored through methods such as DDIM (Song et al., 2020), which enables fewer sampling steps without compromising generation quality. Other works also explore efficient solver of ODE or SDE (Song et al., 2021; Jolicoeur-Martineau et al., 2021; Lu et al., 2022; Karras et al., 2022; Lu et al., 2023), which employs a pseudo numerical method to achieve faster sampling. Second, researchers aimed at reducing the workload and inference time at each sampling step, including distillation (Salimans & Ho, 2022; Li et al., 2023d), quantization (Li et al., 2023c; He et al., 2023; So et al., 2023a; Shang et al., 2023), distributed inference (Li et al., 2024). Third, jointly optimized methods simultaneously optimize network and sampling methods (Li et al., 2023a; Liu et al., 2023). Moreover, researchers modify the model structure indirectly by using the cache mechanism to reduce computation. Cache (Smith, 1982) in computer systems is a method to temporarily store frequently accessed data from the main memory to improve processing speed and efficiency. Based on the findings that high-level features usually change minimally between consecutive steps, researchers reuse the high-level features in U-Net structure while updating the low-level ones (Ma et al., 2024c; Li et al., 2023b; Wimbauer et al., 2024; So et al., 2023b). Besides, (Zhang et al., 2024) cache the redundant cross-attention in the fidelity-improving stage. However, previous methods mainly focus on the U-Net structure and image domain. The most similar work are (Ma et al., 2024b), (Chen et al., 2024b), (Zhang et al., 2024) and (Li et al., 2024). (Ma et al., 2024b) skip the computation of a large proportion of feedforward layers in DiT models through post-training. (Zhang et al., 2024) cache the self-attention in the initial stage and reuses cross-attention in the fidelity-improving phase. (Chen et al., 2024b) caches feature offsets of DiT blocks. (Li et al., 2024) reduce the latency of single-sample generation by running convolutionbased diffusion models across multiple devices in parallel while sacrificing quality and efficiency for parallel. Different from previous works, we aim at real-time DiT-based video generation models using training-free acceleration methods. We utilize pyramid attention and broadcast sequence parallel to accelerate video generation without loss of quality.
% 6. Copy from DiTFastAttn abstract:
% Diffusion Transformers (DiT) excel at image and video generation but face computational challenges due to the quadratic complexity of self-attention operators. We propose DiTFastAttn, a post-training compression method to alleviate the computational bottleneck of DiT. We identify three key redundancies in the attention computation during DiT inference: (1) spatial redundancy, where many attention heads focus on local information; (2) temporal redundancy, with high similarity between the attention outputs of neighboring steps; (3) conditional redundancy, where conditional and unconditional inferences exhibit significant similarity. We propose three techniques to reduce these redundancies: (1) Window Attention with Residual Sharing to reduce spatial redundancy; (2) Attention Sharing across Timesteps to exploit the similarity between steps; (3) Attention Sharing across CFG to skip redundant computations during conditional generation. We apply DiTFastAttn to DiT, PixArt-Sigma for image generation tasks, and OpenSora for video generation tasks. Our results show that for image generation, our method reduces up to 76% of the attention FLOPs and achieves up to 1.8× end-to-end speedup at high-resolution (2k × 2k) generation.


% Outline
% Acceleration of Diffusion Models:
% 
% 1. Fewer Sampling Steps
%   a) DDIM
%   b) 
% 2. Training
%   a) Distillation 
%   b) Other architectures
% 3. Quantization
%   a) SVDQuant
%   b) 
% 4. Distributed Inference
% 5. Cache-based Methods
%   a) PAB
%   b) FasterCache
%   c) TeaCache
%   d) DeepCache
% 6. Pruning
%   a) TinyFusion
% Consider LongVU?
% Sparse Attention in Transformers
% 1. Static attention
% 2. Dynamic attention
% 3. Attention sink

\section{Related Work}
\label{sec:related_works}

\subsection{Efficient diffusion models}
% Diffusion models essentially learn to estimate the gradient of the data distribution \citep{song2019generative}, producing high-quality, diverse samples but inefficient~\citep{ho2020denoising, meng2022sdedit}. Common strategies to boost efficiency include (1) reducing denoising steps, (2) compressing model size, and (3) system-level optimizations.

\noindent\textbf{Decreasing the denoising steps.}
Most diffusion models employ SDEs that require many sampling steps \citep{song2019generative, ho2020denoising, meng2022sdedit}. To address this, DDIM \citep{song2020denoising} approximates them with an ODE; subsequent techniques refine ODE paths and solvers \citep{lu2022dpm, lu2022dpm++, liu2022flow, liu2023instaflow} or incorporate consistency losses \citep{song2023consistency, luo2023latent}. Distillation-based methods \citep{yin2024improved, yin2024one} train simpler, few-step models. However, these require expensive re-training or fine-tuning—impractical for most video use cases. In contrast, our approach directly uses off-the-shelf pre-trained models without any additional training.

\noindent\textbf{Diffusion model compression.}
Weight compression through quantization is a common tactic \citep{li2023q, zhao2024vidit, li2024svdquant}, pushing attention modules to INT8 \citep{zhang2025sageattention} or even INT4/FP8 \citep{zhang2024sageattention2}. Other work proposes efficient architectures \citep{xie2024sana,cai2024condition,chen2025pixart} or high-compression autoencoders \citep{chen2024deep} to improve performance. Our Sparse VideoGen is orthogonal to these techniques and can incorporate them for additional gains.

\noindent\textbf{Efficient system implementation.}
System-level optimizations focus on dynamic batching \citep{kodaira2023streamdiffusion, liang2024looking}, caching \citep{chen2024delta, zhao2024pab}, or hybrid strategies \citep{lv2024fastercache, liu2024timestep}. While these methods can improve throughput, their output quality often drops below a PSNR of 22. By contrast, our method preserves a PSNR above 30, thus substantially outperforming previous approaches in maintaining output fidelity.

% \subsection{Efficient Diffusion Models}\label{subsec:efficient_diffusion}
% Diffusion Models function primarily as denoising models that are trained to estimate the gradient of the data distribution \citep{song2019generative}. Although these models are capable of generating samples with high quality and diversity, they are known as inefficient. To enhance the efficiency of diffusion models, researchers often focus on three primary approaches: (1) decreasing the number of denoising steps, (2) reducing the model size, and (3) optimizing system implementation for greater efficiency.


% \paragraph{Decreasing the denoising steps.} 
% The main diffusion models rely on stochastic differential equations (SDEs) that learn to estimate the gradient of the data distribution through Langevin dynamics \citep{ho2020denoising, meng2022sdedit}. Consequently, these models generally require numerous sampling steps (\textit{, e.g.,} 1,000). To improve sample efficiency, DDIM \citep{song2020denoising} approximates SDE-based diffusion models within an ordinary differential equation (ODE) framework. Expanding on this concept, DPM \citep{lu2022dpm}, DPM++ \citep{lu2022dpm++}, and Rectified Flows \citep{liu2022flow, liu2023instaflow} enhance ODE paths and solvers to further reduce the number of denoising steps. Furthermore, Consistency Models \citep{song2023consistency, luo2023latent} integrate the ODE solver into training using a consistency loss, allowing diffusion models to replicate several denoising operations with fewer iterations. In addition, approaches grounded in distillation \citep{yin2024improved,yin2024one} represent another pivotal strategy. This involves employing a simplified, few-step denoising model to distill a more complex, multi-step denoising model, thereby improving overall efficiency.

% Nevertheless, all these approaches necessitate either re-training or fine-tuning the complete models on image or video datasets. For video generation models, this is largely impractical due to the significant computational expense involved, which is prohibitive for the majority of users. In this work, our primary focus is on a method to enhance generation speed that requires no additional training.

% \paragraph{Diffusion Model Acceleration}
% A common approach to enhancing the efficiency of diffusion models involves compressing their weights through quantization. Q-Diffusion~\citep{li2023q} introduced a W8A8 strategy, implementing quantization in these models. Building on this foundation, ViDiT-Q~\citep{zhao2024vidit} proposed a timestep-aware dynamic quantization method that effectively reduces the bit-width to W4A8. Furthermore, SVDQuant~\citep{li2024svdquant} introduced a cost-effective branch designed to address outlier problems in both activations and weights, thus positioning W4A4 as a feasible solution for diffusion models. SageAttention~\citep{zhang2025sageattention} advanced the field by quantizing the attention module to INT8 precision via a smoothing technique. SageAttention V2~\citep{zhang2024sageattention2} extended these efforts by pushing the precision boundaries to INT4 and FP8. Another common approach is to design efficient diffusion model architectures \cite{xie2024sana,cai2024condition,chen2025pixart} and high-compression autoencoders \cite{chen2024deep} to boost efficiency. Our Sparse VideoGen are orthogonal to these techniques and can utilize them as supplementary methods to enhance efficiency.

% \paragraph{Efficient System Implementation}
% In addition to enhancing the efficiency of diffusion models by either retraining the model to decrease the number of denoising steps or compressing the model size, efficiency improvements can also be achieved at the system level. For instance, strategies such as dynamic batching are employed in StreamDiffusion~\citep{kodaira2023streamdiffusion} and StreamV2V~\citep{liang2024looking} to effectively manage streaming inputs in diffusion models, thereby achieving substantial throughput enhancements. Other approaches include: DeepCache~\citep{ma2024deepcache}, which leverages feature caching to modify the UNet Diffusion; $\Delta-DiT$~\citep{chen2024delta}, which implements this mechanism by caching residuals between attention layers in DiT to circumvent redundant computations; and PAB~\citep{zhao2024pab}, which caches and broadcasts intermediary features at distinct timestep intervals. FasterCache~\citep{lv2024fastercache} identifies significant redundancy in CFG and enhances the reuse of both conditional and unconditional outputs. Meanwhile, TeaCache~\cite{liu2024timestep} recognizes that the similarity in model inputs can be used to forecast output similarity, suggesting an improved machine strategy to amplify speed gains.

% Despite these advanced methodologies, they often result in the generated output diverging significantly from the original, as indicated by a PSNR falling below 22. In contrast, our method consistently achieves a PSNR exceeding 30, thus ensuring substantially superior output quality compared to these previously mentioned strategies.


% (1) Accelerating diffusion via reducing the denoising steps (Chenfeng Todo)
% Consistency model, LCM, DPM, DPM++, rectical flow, DMD, etc.

% (2) Accelerating diffusion via reducing the model size (Xiuyu todo)
% QDiffusion, SVDquant, SageAttention, ViDIT-Q, Xiuyu to add more.

% (3) Accelerating diffusion via efficient system implementation. (Chenfeng, Xiuyu, someone else help)
% StreamDiffusion, StreamV2V, Deepcahce, delta-dit, Faster-Cache, TeaCache (Xiuyu and others add more)


\subsection{Efficient attention methods}\label{subsec:efficient_attention}
% (1) accelerating attention with sparsity (mainly about LLM) (Andy, Haocheng)

% (2) Accelerating attention with linear approximation. (Andy, Haocheng)

\looseness=-1
\noindent\textbf{Sparse attention in LLMs.}
Recent research on sparse attention in language models reveals diverse patterns to reduce computational overhead. StreamingLLM \cite{xiao2023efficient} and LM-Infinite \cite{han2023lm} observe that attention scores often concentrate on the first few or local tokens, highlighting temporal locality. H2O \cite{zhang2023h2o}, Scissorhands \cite{liu2024scissorhands} and DoubleSparsity \cite{yang2024posttrainingsparseattentiondouble} identify a small set of ``heavy hitter'' tokens dominating overall attention scores. TidalDecode \cite{yang2024tidaldecode} shows that attention patterns across layers are highly correlated, while DuoAttention \cite{xiao2024duoattention} and MInference \cite{jiang2024minference} demonstrate distinct sparse patterns across different attention heads. However, these methods focus on token-level sparsity and do not leverage the inherent redundancy of video data.

\noindent\textbf{Linear and low-bit attention.}
Another direction involves linear attention \cite{cai2023efficientvit,xie2024sana,wang2020linformer,choromanski2020rethinking,yu2022metaformer,katharopoulos2020transformers}, which lowers complexity from quadratic to linear, and low-bit attention \cite{zhang2025sageattention,zhang2024sageattention2}, which operates in reduced precision to accelerate attention module. Sparse VideoGen is orthogonal to both approaches: it can be combined with techniques like FP8 attention while still benefiting from the video-specific spatial and temporal sparsity in video diffusion models.

% \paragraph{Sparse Attention in LLM} Recent works on sparse attention have uncovered a variety of patterns in language models that help reduce computational costs by focusing on specific subsets of tokens. 
% StreamingLLM \cite{xiao2023efficient} and LM-Infinite \cite{han2023lm} identify that attention scores are often concentrated on the first few tokens and local tokens, emphasizing the temporal locality of attention during decoding. 
% H2O \cite{zhang2023h2o} and Scissorhands \cite{liu2024scissorhands} observe that attention predominantly focuses on a small subset of "heavy hitter" tokens, which dominate the overall attention scores. 
% TidalDecode \cite{yang2024tidaldecode} highlights the correlation of attention patterns across layers, showing that information learned in earlier layers can help guide attention sparsity in subsequent layers.
% DuoAttention \cite{xiao2024duoattention} and MInference \cite{jiang2024minference} demonstrate that different attention heads can exhibit distinct sparse patterns, with some focusing on specific key tokens while others prioritize broader contextual information.
% While these sparse attention mechanisms have shown great success in LLMs, they are limited to token-level sparsity and fail to leverage the unique redundancy inherent in video data.

% \paragraph{Linear and Low-bit Attention}In addition to sparse attention, there has been considerable progress in improving attention efficiency through linear attention \cite{cai2023efficientvit,xie2024sana} and low-bit attention techniques \cite{zhang2024sageattention}. 
% Linear attention methods, such as Linformer \cite{wang2020linformer}, Performer \cite{choromanski2020rethinking}, MetaFormer \cite{yu2022metaformer}, and LinearAttention \cite{katharopoulos2020transformers}, transform the quadratic complexity of standard attention into linear complexity.
% Low-bit attention methods aim to reduce computational overhead by operating in lower precision. 
% For example, SageAttention \cite{zhang2024sageattention} uses INT8 precision to significantly improve efficiency without introducing substantial performance degradation.

% Sparse VideoGen, as a \textbf{sparse attention} method, is \textbf{orthogonal} to both linear attention and low-bit attention techniques. 
% Moreover, it can be integrated with low-bit attention methods, such as FP8 attention, to further enhance computational efficiency. 
% By leveraging video-specific spatial and temporal sparsity, Sparse VideoGen effectively addresses challenges unique to video diffusion models while remaining compatible with broader efficiency frameworks.



% \subsection{Diffusion Models for Video Generation}
% After the success of diffusion models in image generation, applying diffusion models to generate videos has become a very hot topic. Diffusion Transformer (DiT) is the dominant model architecture people use for video diffusion models. Some early works uses temporal and spatial attention (2D + 1D) to deal with video modality, and the most recent open-source models all apply 3D full attention since it gives much better performance and details. However, 3D full Attention incurs a very long context length, making the computation relatively slow. For example, CogVideoX V1.5 needs 10 minutes to generate a 10 second video. These makes the efficiency problem very important.

% \subsection{Efficient Diffusion Models}

% \TODO{Do not discuss them in details. Discuss them all. Say that they are orthogonal with ours.}

% \paragraph{Quantization}
% Quantization has been prove to be effective speedup the training and inference of large language models. Nowadays, this techniques has been extended to diffusion models for inference speedup. Q-Diffusion first propose a W8A8 solution to apply quantization to diffusion models. ViDiT-Q propose a timestep-aware dynamic quantization method to reduce the bit-width to W4A8. SVDQuant introduce a low-cost branch to alleviate the outlier issue for both activations and weights, making W4A4 a practical solution for diffusion models. SageAttention quantize the attention module into INT8 precision by proposing a smoothing technique. SageAttention V2 further push the limit to INT4 and FP8.

% \paragraph{Distributed Inference}
% Distrifusion, X-DiT

% \paragraph{Cache-based Methods}
% DeepCache utilize feature caching to modify the UNet Diffusion, $\Delta-DiT$ adapts this mechanism to DiT by caching residuals
% between attention layers to avoid redundant computation. PAB caches and broadcasts intermediate features at various timestep intervals. FasterCache observe that there's a huge redundant in CFG and optimizes the reuse of conditional and unconditional outputs. TeaCache finds that the similarity of model inputs can be used to predict the similarity of model outputs, and therefore propose a better machine strategy to achieve a better speedup. However, these methods usually makes the generated output diverge from the original output (PSNR smaller than 22). In comparison, our method can achieve a PSNR > 30, which makes the quality of our method much higher than these methods.

% \subsection{Sparse Attention Methods}
% Sparse attention has been widely used in large language models (StreamLLM, H2O, MInference, FlexPrefill). As the context length of DiT becomes very long, utilizing sparsity to accelerate diffusion models becomes very important. We are the first to study the sparsity problem in video generation.

% \TODO{Punish LLM sparse method here (MInference)}
% \TODO{Andy}