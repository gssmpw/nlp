\section{Motivation and Analysis}
\label{sec:sparse_patterns}


\subsection{\Attn{} shows instinct sparsity}
\label{subsec:sparse-pattern}
We identify that \attn{} possess inherent sparsity, characterized by different distinct sparse patterns tailored for different functions~\citep{xiao2024duoattention}. We deeply investigate the sparsity nature across various text-to-video and image-to-video models and identify two types of attention heads based on sparse patterns: \textit{\SpatialHead{}} and \textit{\TemporalHead{}}, as shown in Figure.~\ref{fig:spatial-temporal-illustration}.

% We identify \attn{} possess intrinsic sparsity, characterized by two distinct sparse patterns tailored for separate functions~\cite{xiao2024duoattention}. To deeply investigate this, we analyze attention maps from various text-to-video and image-to-video models, as shown in Figure~\ref{fig:spatial-temporal-illustration} illustrates that most heads can be categorized into two pattern types: \textit{\SpatialHead{}} and \textit{\TemporalHead{}}.

\looseness=-1
\textbf{\SpatialHead{}.} As illustrated in Figure~\ref{fig:spatial-temporal-illustration}(a-b), \spatialhead{} primarily focuses its attention scores on spatially-local tokens. This leads to the attention map exhibiting a \textit{block-wise layout}. Since pixels within the same frame are tokenized into contiguous tokens, \spatialhead{} attends exclusively to pixels within the same frame and its adjacent frames. This property is essential for maintaining the spatial consistency of generated videos. In \spatialhead{}, the block size relates to the \textit{number of tokens per frame}. 

\looseness=-1
\textbf{\TemporalHead{}.} In contrast, \temporalhead{} exhibits a \textit{slash-wise layout} with a constant interval (Figure~\ref{fig:spatial-temporal-illustration}(c-d)). Since each frame is tokenized into a fixed number of tokens $L$, pixels occupying the same spatial position across different frames are arranged at a stride of $L$. Consequently, \temporalhead{} captures information from the token with the same spatial position across multiple frames. This pattern is important for ensuring temporal consistency in video generation\footnote{We hypothesize that this occurs because the majority of the training data consists of slow-motion videos, making the temporal head's focus on tokens with the same spatial position in several frames adequate to maintain temporal consistency.}.


In addition to the spatial and temporal patterns, we observe that the text prompts and the first frame hold significant attention scores for both spatial and \temporalhead{}, which aligns with previous investigations~\cite{xiao2024efficientstreaminglanguagemodels,shen2024longvuspatiotemporaladaptivecompression,su2025akvqvlattentionawarekvcache}. Therefore, we include these tokens in both the spatial and temporal head.


\subsection{Sparse attention achieves lossless accuracy}
\label{subsec:sparse-accurate}
Furthermore, we find that directly applying sparse patterns to corresponding heads does not hurt the quality of generated videos.
We demonstrate this by evaluating CogVideoX-v1.5 and HunyuanVideo on VBench~\citep{huang2023vbenchcomprehensivebenchmarksuite} with sparse attention. We determine the sparse pattern by computing full attention along with two different sparse mechanisms (\spatialhead{} and \temporalhead{}) for each attention head and denoising step. The sparse pattern with the lowest mean squared error (MSE) relative to full attention is chosen for further inference. This approach achieves a PSNR over 29, showing that the right sparse pattern maintains the high quality of generated videos.

However, despite its accuracy, this strategy does not provide practical efficiency benefits, as full attention computation is still required to determine the optimal sparse pattern. We will address this issue in Sec~\ref{subsec:sampling_based_pattern_selection}.


\subsection{Sparse attention promises theoretical speedup}
\label{sec:sparse-theoretical-speedup}
\looseness=-1
Instead of computing full attention, sparse attention selectively processes only the important tokens based on sparsity patterns, leading to significant computational savings. We analyze the theoretical computation saving below.

Given a model configuration of hidden dimension $H$, number of tokens per frame $L$, and number of total frames $N$, the total computation (FLOPS) for each full attention is $2\cdot2\cdot(LN)^2\cdot H=4L^2N^2H$. For \spatialhead{}, assuming each head only attends to nearby $c_s$ frames, the computation is reduced to $(2\cdot2\cdot L^2H)\cdot c_sN$, resulting in a sparsity of $\frac{c_s}{N}$. For \temporalhead{}, assuming each token only attends $c_t$ tokens across all the frames, the computation is $(2\cdot2\cdot N^2 H)\cdot c_tL$, with a sparsity of $\frac{c_t}{L}$. 
Since both $c_s$ and $c_t$ are typically much smaller compared to $N$ and $L$ respectively, the sparsity can easily achieve $30$\%.
%\cf{why do we specific 30\%? can provide some practical numbers like $c_s$=? $L$=?, it seems that we can even achieve lower sparsity like 13\% in table. 4}.
E.g., the aforementioned CogVideoX-v1.5-T2V achieves a sparsity of $31$\% for both spatial and \temporalhead{} while maintaining an average of $29.99$ PSNR.


Despite the theoretical speedup, the temporal head can not be directly translated into real speedup since the pattern is hardware-inefficient. We will discuss our practical solution in Sec.~\ref{subsec:frame_token_rearrangement} and prove it can achieve theoretical speedup in Sec.~\ref{subsec:kernel_level_efficiency}. Note that we do not include the text prompts and first frame in the theoretical calculation for simplicity, as they are constant and small compared to the remaining part.

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.9\columnwidth]{figure/head-pattern.pdf} 
%     \caption{Spatial and temporal patterns of attention heads are universal across different video generation models.\TODO{ref this figure, hunyuan is placeholder}}
%     \label{fig:head-patterns-circle-figure}
% \end{figure}

%Current video diffusion models widely employ 3D full attention to capture spatial-temporal relationships. Nonetheless, this method significantly increases computational demands, especially with long contextual inputs like high-resolution video sequences. As shown in Figure \ref{fig:intro-op-breakdown}, 3D full attention is the key efficiency bottleneck. This work aims to address this challenge through sparse attention. 

%In this section, we analyze 3D full attention's sparsity patterns. We offer two crucial findings: (1) a \textit{Arrow-shaped} pattern that shows spatial correlation and (2) a \textit{Zebra-striped} pattern that exhibits temporal correlation. Our findings reveal that these patterns are notably distinctive and consistently present across various attention heads. To adeptly capture these correlations, we implement two types of sparse masks: the \textbf{Arrow Mask}, which is crafted to depict spatial correlation, and the \textbf{Zebra Mask}, which is specifically designed to represent temporal correlation.

% We offer an explanation for the observed spatial correlation phenomenon. In language models, it is commonly observed that elements on the main diagonal often exhibit very high attention scores, indicating a strong tendency for a token to focus on itself. Similarly, in diffusion models, we propose that each frame tends to attend to itself, resulting in significant attention scores for tiles on the main diagonal. Furthermore, due to the requirement of motion smoothness, neighboring frames exhibit similarities, causing a frame to likely attend to adjacent frames as well. Consequently, the tiles immediately above and below the main diagonal (superdiagonal and subdiagonal tiles) also demonstrate relatively high attention scores.

% In addition, we discover that the initial frame along the key axis holds significant importance. This is due to the concept of frame-level attention sink \cf{cite StreamLLM, or explain what attention sink is, I don't think diffusion model field people understand it.}, where attention scores tend to focus on the first frame. Building on these insights, we introduce an attention mask aimed at covering the majority of attention scores while maintaining maximal sparsity. This mask is developed at the tile level, with each tile representing the cross-attention between two frames. As illustrated in Figure~\ref{fig:spatial-temporal-illustration} (b), we retain the main-diagonal, super-diagonal, sub-diagonal tiles \cf{I also do not understand what main-diagonal, super-diagonal, sub-diagonal means if I do not read the figure. Is this some specific terminology? If so cite wikipedia}, as well as the tiles corresponding to the first frame along the key axis. Recognizing the critical role of text prompts, we also ensure that all text token-related information is included in the attention mask.

% In addition to spatial correlation, we identify another distinct pattern in attention maps, peculiar to video diffusion models, termed \textit{temporal correlation}.

% As illustrated in Figure~\ref{fig:spatial-temporal-illustration} (b), the attention map reveals a prominent slanted stripe pattern, where each slanted stripe aligns with the diagonals of frame tiles. The central stripe's straightforward interpretation is due to each token having a higher likelihood of attending to itself. Observing the stripes adjacent to the central stripe, we can see they comprise the diagonal of the tile from the second row and first column, the diagonal of the tile from the third row and second column, and so forth, continuing until the diagonal of the tile in the (n+1)-th row and n-th column. This indicates that each token will attend to its corresponding token on the adjacent frame. By extending this observation to include all stripes, it becomes clear that a token at one frame also concentrates on the identical token at other frames. We label this pattern as \textit{Zebra-striped, temporal correlated} sparse pattern because it signifies the interaction between frames. Drawing from these insights, we create an attention mask to accurately depict the "Temporal Correlation" effect, as shown in Figure~\ref{fig:spatial-temporal-illustration} (d).

% We find that each pattern leads to significant quality degradation independently. However, by optimally combining these two attention masks, we can achieve nearly lossless video generation.

% We start from an oracle study to determine the optimal sparse attention mask for each specific attention head. We evaluate which sparse pattern yields a lower mean squared error (MSE) regarding the attention output. Initially, we compute the attention output for the 3D full attention mechanism. Subsequently, we derive two separate attention outputs corresponding to the two distinct sparse patterns. As depicted in Figure ?, the method of oracle-based selection facilitates a performance that is nearly lossless, as evidenced by a PSNR exceeding 30.


% Inspired by the oracle study, we attempt to explore an efficient way to find the (nearly) optimal sparsity pattern for every attention head. A straightforward solution is to perform static pattern selection, \textit{i.e.,} we can decide which kind of sparse patterns are used for each attention head via comparing MSE between the sparse attention map and the dense attention map in an offline manner. However, the static pattern selection strategy is ineffective in real practice because the optimal attention pattern varies significantly across different prompts. Therefore, we require an online selection algorithm to determine the optimal sparsity pattern for each attention head.