\section{Preliminaries}

\subsection{Diffusion Transformers}
% A Diffusion Transformer Model (DiT) comprises three key components: a text encoder, a variational autoencoder (VAE), and the transformer itself. 

Diffusion models typically consist of three main elements: a text encoder, a variational autoencoder (VAE), and a denoising model designed to iteratively handle noise within the latent space. Diffusion Transformers specifically employ a transformer model for the denoising task.  
During the sampling phase, the text encoder interprets textual inputs to generate conditioning embeddings, while the VAE reconstructs the generated latent representation into pixel space. 
The denoising process involves the model incrementally improving a noisy latent representation over a sequence of timesteps. 
In each step, video diffusion transformers utilize self-attention to identify dependencies both within individual frames and across multiple frames, and employ feed-forward layers to enhance the latents.
By repetitively executing these steps over all timesteps, video diffusion transformers convert the random noise latent into a coherent video. 

% These components are run once, whereas the transformer operates iteratively at every denoising timestep, comprising more than 99\% of the total computational cost.


% A Diffusion Transformer Model comprises three key components: a text encoder, a variational autoencoder (VAE), and the transformer itself. During the sampling process, the text encoder and VAE are invoked only once, while the transformer is called at every timestep. 
% Consequently, the text encoder and VAE account for less than 1\% of the total sampling time. 
% Therefore, in the following discussion, we focus exclusively on the transformer component.

% Diffusion models simulate visual generation through a sequence of iterative denoising steps. Typically, the diffusion process is divided into a thousand timesteps during the training phase and reduced to just a few dozen timesteps during the inference phase.


\subsection{Video Generation with 3D Full Attention}
Recent video diffusion models commonly adopt 3D full attention that integrates temporal and spatial attention into a unified framework. While this approach often delivers superior performance, it comes at the cost of significantly higher computational demands, particularly when the context length is extensiveâ€”such as in cases of long video sequences or high-resolution content. We denote the total context length as $S$, the length of the text prompt as $T$, the number of frame as $F$, and the token per frame as $C$. In 3D full Attention, we have 
\[ S = T + F \times C. \]
As we visualize in Figure~\ref{fig:attention proportion}, since the 3D full attention's compute time is quadratic to context length but linear layers' compute time is linear to it, attention takes most of the computation when the context length is longer than 20k. 

\begin{figure}[t]
    \centering
    \begin{minipage}[t]{\linewidth}
        \includegraphics[width=0.95\linewidth]
        {figure/AttentionPortionBarChartGray2.pdf}
        \caption{Time proportion of every component in DiT. Attention module's time proportion increases rapidly in long context setting. 
        }
        \label{fig:attention proportion}
    \end{minipage}
\end{figure}
