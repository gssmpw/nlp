% \section{Appendix}

% \begin{algorithm}[h]
% \caption{ Sparse Attention.}
% \label{alg:code_inter_frame}
% \definecolor{codeblue}{rgb}{0.25,0.65,0.5}
% \definecolor{codeblue2}{rgb}{0,0,1}
% \lstset{
%   backgroundcolor=\color{white},
%   basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
%   columns=fixed,
%   breaklines=true,
%   captionpos=b,
%   commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
%   keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue2},
%     emph={frame_token_rearrange, token_frame_rearrange,transpose},
%     emphstyle={\color[RGB]{255,52,179}},
% }
% \begin{lstlisting}[language=python]
% # Q: [B, H, T + F * C, D] - query
% # K: [B, H, T + F * C, D] - key
% # V: [B, H, T + F * C, D] - value
% # O: [B, H, T + F * C, D] - output
% # T                       - Prompt Token Length
% # F                       - Total Frame Number
% # C                       - Token Per Frame

% # Token rearrangement for QKV
% Q_i = Q[:,:,T:,:].view(B, H, F, C, D)
% K_i = K[:,:,T:,:].view(B, H, F, C, D)
% V_i = V[:,:,T:,:].view(B, H, F, C, D)
% Q_i = Q_i.transpose(2,3)
% K_i = K_i.transpose(2,3)
% V_i = V_i.transpose(2,3)
% Q[:,:,T:,:] = Q_i.view(B, H, F * C, D)
% K[:,:,T:,:] = K_i.view(B, H, F * C, D)
% V[:,:,T:,:] = V_i.view(B, H, F * C, D)

% # Compute diagonal block sparse attention
% mask = generate_diagonal_mask()
% O = block_sparse_attention(Q, K, V, mask)

% # Output rearrangement
% O_i = O[:,:,T:,:].view(B, H, C, F, D)
% O_i = O_i.transpose(2,3)
% O[:,:,T:,:] = O_i.view(B, H, F * C, D)
    
% \end{lstlisting}
% \end{algorithm}


\section{A full version of related work}


\subsection{Efficient Diffusion Models}\label{subsec:efficient_diffusion}
Diffusion Models function primarily as denoising models that are trained to estimate the gradient of the data distribution \citep{song2019generative}. Although these models are capable of generating samples with high quality and diversity, they are known as inefficient. To enhance the efficiency of diffusion models, researchers often focus on three primary approaches: (1) decreasing the number of denoising steps, (2) reducing the model size, and (3) optimizing system implementation for greater efficiency.


\paragraph{Decreasing the denoising steps.} 
The main diffusion models rely on stochastic differential equations (SDEs) that learn to estimate the gradient of the data distribution through Langevin dynamics \citep{ho2020denoising, meng2022sdedit}. Consequently, these models generally require numerous sampling steps (\textit{, e.g.,} 1,000). To improve sample efficiency, DDIM \citep{song2020denoising} approximates SDE-based diffusion models within an ordinary differential equation (ODE) framework. Expanding on this concept, DPM \citep{lu2022dpm}, DPM++ \citep{lu2022dpm++}, and Rectified Flows \citep{liu2022flow, liu2023instaflow} enhance ODE paths and solvers to further reduce the number of denoising steps. Furthermore, Consistency Models \citep{song2023consistency, luo2023latent} integrate the ODE solver into training using a consistency loss, allowing diffusion models to replicate several denoising operations with fewer iterations. In addition, approaches grounded in distillation \citep{yin2024improved,yin2024one} represent another pivotal strategy. This involves employing a simplified, few-step denoising model to distill a more complex, multi-step denoising model, thereby improving overall efficiency.

Nevertheless, all these approaches necessitate either re-training or fine-tuning the complete models on image or video datasets. For video generation models, this is largely impractical due to the significant computational expense involved, which is prohibitive for the majority of users. In this work, our primary focus is on a method to enhance generation speed that requires no additional training.

\paragraph{Diffusion Model Compreesion}
A common approach to enhancing the efficiency of diffusion models involves compressing their weights through quantization. Q-Diffusion~\citep{li2023q} introduced a W8A8 strategy, implementing quantization in these models. Building on this foundation, ViDiT-Q~\citep{zhao2024vidit} proposed a timestep-aware dynamic quantization method that effectively reduces the bit-width to W4A8. Furthermore, SVDQuant~\citep{li2024svdquant} introduced a cost-effective branch designed to address outlier problems in both activations and weights, thus positioning W4A4 as a feasible solution for diffusion models. SageAttention~\citep{zhang2025sageattention} advanced the field by quantizing the attention module to INT8 precision via a smoothing technique. SageAttention V2~\citep{zhang2024sageattention2} extended these efforts by pushing the precision boundaries to INT4 and FP8. Another common approach is to design efficient diffusion model architectures \cite{xie2024sana,cai2024condition,chen2025pixart} and high-compression autoencoders \cite{chen2024deep} to boost efficiency. Our Sparse VideoGen is orthogonal to these techniques and can utilize them as supplementary methods to enhance efficiency.

\paragraph{Efficient System Implementation}
In addition to enhancing the efficiency of diffusion models by either retraining the model to decrease the number of denoising steps or compressing the model size, efficiency improvements can also be achieved at the system level. For instance, strategies such as dynamic batching are employed in StreamDiffusion~\citep{kodaira2023streamdiffusion} and StreamV2V~\citep{liang2024looking} to effectively manage streaming inputs in diffusion models, thereby achieving substantial throughput enhancements. Other approaches include: DeepCache~\citep{ma2024deepcache}, which leverages feature caching to modify the UNet Diffusion; $\Delta-DiT$~\citep{chen2024delta}, which implements this mechanism by caching residuals between attention layers in DiT to circumvent redundant computations; and PAB~\citep{zhao2024pab}, which caches and broadcasts intermediary features at distinct timestep intervals. FasterCache~\citep{lv2024fastercache} identifies significant redundancy in CFG and enhances the reuse of both conditional and unconditional outputs. Meanwhile, TeaCache~\cite{liu2024timestep} recognizes that the similarity in model inputs can be used to forecast output similarity, suggesting an improved machine strategy to amplify speed gains.

Despite these advanced methodologies, they often result in the generated output diverging significantly from the original, as indicated by a PSNR falling below 22. In contrast, our method consistently achieves a PSNR exceeding 30, thus ensuring substantially superior output quality compared to these previously mentioned strategies.

\subsection{Efficient Attention}


\paragraph{Sparse Attention in LLM} Recent studies on sparse attention in language models have identified patterns that reduce computational costs by targeting specific token subsets. StreamingLLM \cite{xiao2023efficient} and LM-Infinite \cite{han2023lm} reveal concentration on initial and local tokens, highlighting temporal locality in decoding. H2O \cite{zhang2023h2o} and Scissorhands \cite{liu2024scissorhands} note attention focuses mainly on a few dominant tokens. TidalDecode \cite{yang2024tidaldecode} shows cross-layer attention pattern correlations, aiding in attention sparsity. DuoAttention \cite{xiao2024duoattention} and MInference \cite{jiang2024minference} find distinct sparse patterns among attention heads, with varying focus on key tokens and context. Despite their success in LLMs, these mechanisms are constrained to token-level sparsity and miss the redundancy unique to video data.

\paragraph{Linear and Low-bit Attention} Significant advancements have been achieved in enhancing attention efficiency, notably through linear attention \cite{cai2023efficientvit,xie2024sana} and low-bit attention techniques \cite{zhang2025sageattention,zhang2024sageattention2}. Linear attention models, including Linformer \cite{wang2020linformer}, Performer \cite{choromanski2020rethinking}, MetaFormer \cite{yu2022metaformer}, and LinearAttention \cite{katharopoulos2020transformers}, reduce the quadratic complexity of traditional attention to linear. Low-bit attention approaches decrease computational demands by utilizing lower precision, with SageAttention \cite{zhang2025sageattention} employing INT8 precision to enhance efficiency without notable performance loss.

Sparse VideoGen, as a \textbf{sparse attention} method, is \textbf{orthogonal} to both linear attention and low-bit attention techniques. 
Moreover, it can be integrated with low-bit attention methods, such as FP8 attention, to further enhance computational efficiency. 
