\section{Experiment}
\label{subsec:experiments}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{figure/visualization.pdf} 
        \captionof{figure}{Examples of generated videos by \sys{} and original implementation on CogVideoX-v1.5-I2V and HunyuanVideo-T2V. We showcase four different scenarios: (a) minor scene changes, (b) significant scene changes, (c) rare object interactions, and (d) frequent object interactions. \sys{} produces videos highly consistent with the originals in all cases, maintaining high visual quality.}
        \label{fig:SVG-visualization} 
\end{figure*}

\subsection{Setup}
\label{subsec:experiment_setup}

\textbf{Models.} We evaluate \sys{} on open-sourced state-of-the-art video generation models including CogVideoX-v1.5-I2V, CogVideoX-v1.5-T2V, and HunyuanVideo-T2V to generate $720$p resolution videos. After 3D VAE, CogVideoX-v1.5 consumes $11$ frames with $4080$ tokens per frame in \attn{}, while HunyuanVideo works on $33$ frames with $3600$ tokens per frame.


\textbf{Metrics.} We assess the quality of the generated videos using the following metrics. We use Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS)~\citep{zhang2018perceptual}, Structural Similarity Index Measure (SSIM) to evaluate the generated video's similarity, and use VBench Score~\citep{huang2023vbenchcomprehensivebenchmarksuite} to evaluate the video quality, following common practices in community~\citep{5596999,zhao2024pab,li2024svdquant,li2024distrifusion}. Specifically, we report the imaging quality and subject consistency metrics, represented by VBench-1 and VBench-2 in our table.

\textbf{Datasets.} For CogVideoX-v1.5, we generate video using the VBench dataset after prompt optimization, as suggested by CogVideoX~\cite{yang2024cogvideox}. 
For HunyuanVideo, we benchmark our method using the prompt in Penguin Video Benchmark released by HunyuanVideo~\cite{kong2024hunyuanvideo}.

% We follow standard practices in evaluating video generation models.
% Specifically, we assess the quality of the generated videos using the following metrics: Peak Signal-to-Noise Ratio (PSNR), Learned Perceptual Image Patch Similarity (LPIPS), Structural Similarity Index Measure (SSIM), and VBench Score.
% PSNR measures pixel-level fidelity by quantifying the difference between generated and ground-truth frames, where higher scores indicate better preservation of fine details. 
% LPIPS evaluates perceptual similarity based on feature representations, while SSIM assesses the structural similarity within video frames. 
% VBench provides a comprehensive evaluation of video quality that aligns closely with human perception. 
% Among these metrics, our method achieves notably high PSNR, demonstrating superior pixel fidelity while maintaining perceptual and structural quality.

\textbf{Baselines.} We compare \sys{} against sparse attention algorithms DiTFastAttn~\cite{yuan2024ditfastattnattentioncompressiondiffusion} and MInference~\cite{jiang2024minference}. As DiTFastAttn can be considered as \spatialhead{} only algorithm, we also manually implement a \temporalhead{} only baseline named \textit{Temporal-only attention}. We also include a cache-based DiT acceleration algorithm PAB~\cite{zhao2024pab} as a baseline.


\textbf{Parameters.} For MInference and PAB, we use their official configurations. For \sys{}, we choose $c_s$ as $4$ frames and $c_t$ as $1224$ tokens for CogVideoX-v1.5, while $c_s$ as $10$ frames and $c_t$ as $1200$ tokens for HunyuanVideo. Such configurations lead to approximately $30$\% sparsity for both \spatialhead{} and \temporalhead{}, which is enough for lossless generation in general. We skip the first $25$\% denoising steps for all baselines as they are critical to generation quality, following previous works~\cite{zhao2024pab,li2024distrifusion,lv2024fastercache,liu2024timestep}.



\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\columnwidth]{figure/efficiency-breakdown.pdf} 
    \caption{The breakdown of end-to-end runtime of HunyuanVideo when generating a $5.3$s, $720$p video. \sys{} effectively reduces the end-to-end inference time from $2253$ seconds to $968$ seconds through system-algorithm co-design. Each design point contributes to a considerable improvement, with a total $2.33\times$ speedup.}
    \label{fig:efficiency-breakdown-figure}
\end{figure}



\subsection{Quality evaluation}
\label{subsec:quality_benchmark}
We evaluate the quality of generated videos by \sys{} compared to baselines and report the results in Table~\ref{table:accuracy_efficiency_benchmark}. Results demonstrate that \sys{} \textbf{consistently outperforms} all baseline methods in terms of PSNR, SSIM, and LPIPS while achieving \textbf{higher end-to-end speedup}.


Specifically, \sys{} achieves an average PSNR exceeding \textbf{29.55} on HunyuanVideo and \textbf{29.99} on CogVideoX-v1.5-T2V, highlighting its exceptional ability to maintain high fidelity and accurately reconstruct fine details.
For a visual understanding of the video quality generated by \sys{}, please refer to Figure \ref{fig:SVG-visualization}.

\sys{} maintains both \textbf{spatial and temporal consistency} by adaptively applying different sparse patterns, while all other baselines fail. E.g., since the mean-pooling block sparse cannot effectively select slash-wise temporal sparsity (see Figure~\ref{fig:spatial-temporal-illustration}), MInference fails to account for temporal dependencies, leading to a substantial PSNR drop. Besides, PAB skips computation of \attn{} by reusing results from prior layers, which greatly hurts the quality.


Moreover, \sys{} is compatible with \textbf{FP8 attention quantization}, incurring only a $0.1$ PSNR drop on HunyuanVideo. Such quantization greatly boosts the efficiency by $1.3\times$. Note that we do not apply FP8 attention quantization on CogVideoX-v1.5, as its head dimension of $64$ limits the arithmetic intensity, offering no on-GPU speedups.


% \begin{table*}[t]
% \centering
% \caption{Quality and Efficiency Benchmark for Video Models.}
% \label{table:accuracy_efficiency_benchmark}
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{c|l|ccccc|cccc}
% \toprule
% \textbf{Type} & \textbf{Method} & \multicolumn{5}{c|}{\textbf{Quality}} & \multicolumn{4}{c}{\textbf{Efficiency}} \\
% \cmidrule(lr){3-7} \cmidrule(lr){8-11}
% & & PSNR $\uparrow$ & SSIM $\uparrow$ & LPIPS $\downarrow$ & VBench-1 $\uparrow$ & VBench-2 $\uparrow$ & FLOPS $\downarrow$ & Peak Memory $\downarrow$ & Latency $\downarrow$ & Speedup $\uparrow$ \\
% \midrule
% \textbf{I2V} & CogVideoX-v1.5 (720p, 10s, 80 frames) & - & - & - & 70.09\% & 95.37\% & 147.87 PFLOPs &  & 528s & 1x \\
% \midrule
% & DiTFastAttn (Spatial-only) & 24.591 & 0.836 & 0.167 & 70.44\% & 95.29\% & 78.86 PFLOPs &  & 338s  & 1.56x \\
% & Temporal-only & 23.839 & 0.844 & 0.157 & 70.37\% & 95.13\% & 70.27 PFLOPs &  & 327s & 1.61x \\
% & MInference & 22.489 & 0.743 & 0.264 & 58.85\% & 87.38\% & 84.89 PFLOPs &  &  &  \\
% & PAB & 23.234 & 0.842 & 0.145 & 69.18\% & 95.42\% & 105.88 PFLOPs &  &  &  \\
% \rowcolor{lightblue}
% & Ours & \textbf{\textcolor{darkgreen}{28.165}} & \textbf{\textcolor{darkgreen}{0.915}} & \textbf{\textcolor{darkgreen}{0.104}} & 70.41\% & 95.29\% & 74.57 PFLOPs &  & 237s & \textcolor{darkgreen}{2.23x} \\
% % \rowcolor{lightblue}
% % & Ours + FP8 & 26.709 & 0.890 & 0.122 &  &  &  &  & \\
% \midrule
% \textbf{T2V} & CogVideoX-v1.5 (720p, 10s, 80 frames) & - & - & - & 62.42\% & 98.66\% & 147.87 PFLOPs &  & 528s & 1x \\
% \midrule
% & DiTFastAttn (Spatial-only) & 23.202 & 0.741 & 0.256 & 62.22\% & 96.95\% & 78.86 PFLOPs &  & 338s & 1.56x \\
% & Temporal-only & 23.804 & 0.811 & 0.198 & 62.12\% & 98.53\% & 70.27 PFLOPs &  & 327s & 1.61x \\
% & MInference & 22.451 & 0.691 & 0.304 & 54.87\% & 91.52\% & 84.89 PFLOPs &  &  &  \\
% & PAB & 22.486 & 0.740 & 0.234 & 57.32\% & 98.76\% & 400.04 PFLOPs &  &  &  \\
% \rowcolor{lightblue}
% & Ours & \textbf{\textcolor{darkgreen}{29.989}} & \textbf{\textcolor{darkgreen}{0.910}} & \textbf{\textcolor{darkgreen}{0.112}} & 63.01\% & 98.67\% & 74.57 PFLOPs &  & 232s & \textbf{\textcolor{darkgreen}{2.28x}} \\
% % \rowcolor{lightblue}
% % & Ours + FP8 &  &  &  &  &  &  &  &  \\
% \midrule
% \textbf{T2V} & HunyuanVideo (720p, 5.33s, 128 frames) & - & - & - & 66.11\% & 93.69\% & 612.37 PFLOPs &  & 2253s & 1x \\
% \midrule
% & DiTFastAttn (Spatial-only) & 21.416 & 0.646 & 0.331 & 67.33\% & 90.10\% & 260.48 PFLOPs &  & 1238s & 1.82x \\
% & Temporal-only & 25.851 & 0.857 & 0.175 & 62.12\% & 98.53\% & 259.10 PFLOPs &  & 1231s & 1.83x \\
% & MInference & 23.157 & 0.823 & 0.163 &  &  & 293.87 PFLOPs &  &  &  \\
% & PAB & - & - & - & - &  & - & \color{red}OOM & - & - \\
% \rowcolor{lightblue}
% & Ours & \textbf{\textcolor{darkgreen}{29.546}} & \textbf{\textcolor{darkgreen}{0.907}} & \textbf{\textcolor{darkgreen}{0.127}} & 65.90\% & 93.51\% & 259.79 PFLOPs &  & 1171s & 1.92x \\
% \rowcolor{lightblue}
% & Ours + FP8 & \textbf{\textcolor{darkgreen}{29.452}} & \textbf{\textcolor{darkgreen}{0.906}} & \textbf{\textcolor{darkgreen}{0.128}} & 65.70\% & 93.51\% & 259.79 PFLOPs &  & 968s & \textbf{\textcolor{darkgreen}{2.33x}} \\
% \bottomrule
% \end{tabular}%
% }
% \end{table*}



\subsection{Efficiency evaluation}
\label{subsec:efficiency_benchmark}

To demonstrate the feasibility of \sys{}, we prototype the entire framework with dedicated CUDA kernels based on FlashAttention~\cite{dao2022flashattentionfastmemoryefficientexact}, FlashInfer~\cite{ye2025flashinferefficientcustomizableattention}, and Triton~\cite{Tillet2019TritonAI}. We first showcase the end-to-end speedup of \sys{} compared to baselines on an H100-80GB-HBM3 with CUDA 12.4. Besides, we also conduct a kernel-level efficiency evaluation. Note that all baselines adopt FlashAttention-2~\cite{dao2022flashattentionfastmemoryefficientexact}.


\begin{table}[t]
\small
\centering
\caption{Inference speedup of customized QK-norm and RoPE compared to PyTorch implementation with different number of frames. We use the same configuration of CogVideoX-v1.5, i.e. $4080$ tokens per frame, $96$ attention heads.}
\label{table:small-kernel-speedup-comparison}
\begin{tabular}{c|cccc}
\toprule
Frame Number & 8 & 9 & 10 & 11  \\
\midrule
%LayerNorm & 7.436× & 7.448× & 7.464× & 7.474×  \\
QK-norm & 7.44× & 7.45× & 7.46× & 7.47×  \\
\midrule
RoPE & 14.50× & 15.23× & 15.93× & 16.47×   \\
\bottomrule
\end{tabular}
\end{table}


\textbf{End-to-end speedup benchmark.} We incorporate the end-to-end efficiency metric including FLOPS, latency, and corresponding speedup into Table~\ref{table:accuracy_efficiency_benchmark}. \sys{} consistently outperforms all baselines by achieving an average speedup of $2.28\times$ while maintaining the highest generation quality. We further provide a detailed breakdown of end-to-end inference time on HunyuanVideo in Figure~\ref{fig:efficiency-breakdown-figure} to analyze the speedup. Each design point described in Sec~\ref{sec:methodology} contributes significantly to the speedup, with sparse attention delivering the most substantial improvement of $1.81\times$.

\textbf{Kernel-level efficiency benchmark.}\label{subsec:kernel_level_efficiency} We benchmark individual kernel performance including QK-norm, RoPE, and block sparse attention with unit tests in Table~\ref{table:small-kernel-speedup-comparison}. Our customized QK-norm and RoPE achieve consistently better throughput across all frame numbers, with an average speedup of $7.4\times$ and $15.5\times$, respectively. For the sparse attention kernel, we compare the latency of our customized kernel with the theoretical speedup across different sparsity. As shown in Figure~\ref{fig:kernel-efficiency-sparse-attention}, our kernel achieves theoretical speedup, enabling practical benefit from sparse attention.


\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figure/LayourTransformSpeed3.pdf} 
    % \vspace{-2pt}
    \caption{Latency comparison of different implementations of sparse attention. Our hardware-efficient \reorder{} optimizes the sparsity pattern of \temporalhead{} for better contiguity, which is $1.7$× faster than naive sparse attention (named original), approaching the theoretical speedup.}
    \label{fig:kernel-efficiency-sparse-attention}
    \vspace{-5pt}
\end{figure}

\begin{table}[t]
\centering
\caption{Sensitivity test on \onlinesample{} ratios. Profiling just $1$\% tokens achieves generation quality comparable to the oracle ($100$\%) while introducing only negligible overhead.}
\label{table:sensitivity-sampling}
\begin{tabular}{l|ccc}
\toprule
\textbf{Ratios} & \textbf{PSNR $\uparrow$} & \textbf{SSIM $\uparrow$} & \textbf{LPIPS $\downarrow$} \\
\midrule
\multicolumn{4}{c}{\textbf{CogVideoX-v1.5-I2V (720p, 10s, 80 frames)}} \\
\midrule
profiling 0.1\% & 30.791 & 0.941 & 0.0799 \\
profiling 1\% & 31.118 & 0.945 & 0.0757\\
profiling 5\% & 31.008 & 0.944 & 0.0764\\
profiling 100\% & 31.324 & 0.947 & 0.0744 \\
% \midrule
% \multicolumn{4}{l}{\textbf{CogVideoX V1.5 (720p, 10s, 80 frames)}} \\
% \midrule
% No threshold & 31.118 & 0.945 & 0.0757\\
% threshold=10 & 31.304 & 0.949 & 0.0722\\
% threshold=1 & 31.322 & 0.949 & 0.0717\\
% threshold=0.1 & 31.217 & 0.949& 0.0720\\
\bottomrule
\end{tabular}
\end{table}

\subsection{Sensitivity test}
\label{subsec:sensitivity-test}
In this section, we conduct a sensitivity analysis on the hyperparameter choices of \sys{}, including the \onlinesample{} ratios (Sec~\ref{subsec:sampling_based_pattern_selection}) and the sparsity ratios $c_s$ and $c_t$ (Sec~\ref{subsec:frame_token_rearrangement}). Our goal is to demonstrate the robustness of \sys{} across various efficiency-accuracy trade-offs.


\textbf{\Onlinesample{} ratios.} We evaluate the effectiveness of \onlinesample{} with different profiling ratios on CogVideoX-v1.5 using a random subset of VBench in Table~\ref{table:sensitivity-sampling}. In our experiments, we choose to profile only 1\% of the input rows, which offers a comparable generation quality comparable to the oracle profile (100\% profiled) with negligible overhead.

%Profiling only $1$\% of the input data achieves nearly the same generation quality as the oracle profiling ($100$\% sampling), with only a $0.2$ PSNR reduction. Therefore, we adopt this scheme as the default setting, as it provides accuracy comparable to the oracle with negligible overhead.


\textbf{Generation quality over different sparsity ratios.} As discussed in Sec~\ref{sec:sparse-theoretical-speedup}, different sparsity ratio of the \spatialhead{} and \temporalhead{} can be set by choosing different $c_s$ and $c_t$, therefore reaching different trade-offs between efficiency and accuracy. We evaluate the LPIPS of HunyuanVideo over a random subset of VBench with different sparsity ratios. As shown in Table~\ref{table:sensitivity-sparsity-ratios}, \sys{} consistently achieves decent generation quality across various sparsity ratios. E.g., even with a sparsity of $13$\%, \sys{} still achieves $0.154$ LPIPS. We leave the adaptive sparsity control for future work.


\subsection{Ablation study}
\label{subsec:ablation}
We conduct the ablation study to evaluate the effectiveness of the proposed hardware-efficient \reorder{} (as discussed in Sec~\ref{subsec:frame_token_rearrangement}). Specifically, we profile the latency of the sparse attention kernel with and without the transformation under the HunyuanVideo configuration. As shown in Figure~\ref{fig:kernel-efficiency-sparse-attention}, the sparse attention with \reorder{} closely approaches the theoretical speedup, whereas the original implementation without \reorder{} falls short. For example, at a sparsity level of $10$\%, our method achieves an additional $1.7\times$ speedup compared to the original approach, achieving a $3.63\times$ improvement.

\begin{table}[t]
\small
\centering
\caption{Video quality of HunyuanVideo on a subset of VBench when varying sparsity ratios. LPIPS decreases as the sparse ratio increases, achieving trade-offs between efficiency and accuracy.}
\label{table:sensitivity-sparsity-ratios}
\begin{tabular}{c|cccccc}
\toprule
Sparsity$\downarrow$ & 0.13 & 0.18 & 0.35 & 0.43 & 0.52 \\
\midrule
LPIPS$\downarrow$ & 0.154 & 0.135 & 0.141 & 0.129 & 0.116 \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\end{table}



% \paragraph{Robustness of Sparse Attention} To further assess the robustness of our sparse attention mechanism, we examine its performance under different MSE thresholds. 
% As discussed in Section \ref{sec:sparse_patterns}, approximately 10\% of attention heads exhibit high MSE values ($\ge$0.1) under both Arrow Mask and Zebra Mask. 
% To address these edge cases, we calculate full attention for heads with MSE values exceeding a given threshold (0.1, 1, or 10). 
% As shown in Table \ref{table:ablation_study}, the PSNR remains consistent across all threshold settings, indicating that these rare corner cases do not significantly impact overall performance.

% \paragraph{Impracticality of Offline Calibration} We explore whether sparse pattern selection can be pre-determined through offline calibration. 
% A visual comparison of sparse patterns selected for two videos generated by CogVideoX is presented in Figure \ref{}. 
% The patterns show no clear correlation between the two videos, indicating that sparse attention patterns vary significantly depending on the content and context of each video. This result demonstrates that offline calibration is infeasible for video generation tasks, further validating the need for our online sampling-based method.