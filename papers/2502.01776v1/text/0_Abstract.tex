\begin{abstract}

% Diffusion Transformers with 3D full attention have emerged as a powerful framework for video generation by leveraging both spatial and temporal dependencies to produce high-quality outputs. 
% However, their computational efficiency is severely limited by the quadratic complexity of self-attention with respect to sequence length, particularly in large-scale video models. 
% For example, CogVideoX v1.5 requires 20 minutes to generate a 10-second video on an NVIDIA A100 GPU, with 73\% of the computation spent on attention, while HunyuanVideo takes one hour for a 5-second video, with attention dominating 83\% of the total cost.
% Our key observation is that attention patterns in 3D-DiT exhibit inherent sparsity, falling into two categories: spatial-correlated patterns that focus on local tokens within frames and temporal-correlated patterns that align tokens across consecutive frames. 
% Despite this sparsity, two main challenges arise: (1) the dynamic nature of attention patterns, which vary across heads and timesteps, and (2) the hardware inefficiency of sparse layouts on GPUs.
% To address these challenges, we propose SparseI2V, a training-free acceleration framework that achieves a 3Ã— speedup for 3D-DiT video generation without sacrificing video quality. 
% SparseI2V dynamically identifies and applies sparse patterns through an online sampling-based strategy and reorders tokens to optimize for hardware efficiency. 
% By leveraging these innovations, SparseI2V significantly reduces computational overhead, making 3D-DiT more practical for real-world video generation tasks.


% Diffusion Transformers (DiTs) with \attn{} have recently achieved remarkable success in video generation. %offering high fidelity and flexible control via images or text. 
% However, their high computational cost severely limits real-world applicability, with mainstream DiT-based approaches requiring tens of minutes on high-performance GPUs to generate just a few seconds of video. In this paper, we identify and mitigate the core efficiency bottleneck as the quadratic computational complexity of \attn{}.


% We observe two distinct sparse patterns of attention across a wide range of text-to-video and image-to-video models: (1) arrow-shaped, spatially correlated patterns within individual frames, and (2) zebra-striped, temporally correlated patterns across consecutive frames. Despite this inherent sparsity that provides the possibility of acceleration, two challenges persist: (a) determining which pattern to leverage at inference, given the dynamic nature of attention across heads and timesteps, and (b) efficiently accelerating 3D full attention based on these sparsity patterns.
% To address these challenges, we propose a simple yet effective training-free framework. Our method adaptively identifies and applies the appropriate sparse patterns through an online sampling-based strategy and further reorders tokens to optimize for hardware efficiency. Experimental results show that our approach significantly accelerates mainstream video diffusion models such as \cf{Remember to check the number} CogVideoX and Hunyuan by 3.31x and 2.65x respectively, while preserving video generation fidelity. We will release the code in the near future.

Diffusion Transformers (DiTs) dominate video generation but their high computational cost severely limits real-world applicability, usually requiring tens of minutes to generate a few seconds of video even on high-performance GPUs.
% Diffusion Transformers (DiTs) with \attn{} have recently achieved remarkable success in video generation. However, their high computational cost severely limits real-world applicability, requiring tens of minutes on high-performance GPUs to generate a few seconds of video.
This inefficiency primarily arises from the quadratic computational complexity of \attn{} with respect to the context length. In this paper, we propose a training-free framework termed Sparse VideoGen (\textbf{\sys{}}) that leverages the inherent sparsity in \attn{} to boost inference efficiency. We reveal that the attention heads can be dynamically classified into two groups depending on distinct sparse patterns: (1) \textit{\SpatialHead{}}, where only spatially-related tokens within each frame dominate the attention output, and (2) \textit{\TemporalHead{}}, where only temporally-related tokens across different frames dominate. Based on this insight, \sys{} proposes an \onlinesample{} to capture the dynamic sparse patterns and predicts the type of attention head. Combined with a novel hardware-efficient tensor \reorder{} and customized kernel implementations, \sys{} achieves up to $2.28\times$ and $2.33\times$ end-to-end speedup on CogVideoX-v1.5 and HunyuanVideo, respectively, while preserving generation quality.

% Based on this insight, \sys{} performs sparse attention by computing only important tokens of a given attention head, thus saving substantial computation. 

% To identify the important tokens, \sys{} proposes an online sampling-based strategy that adaptively captures the dynamic sparse patterns. Combined with a novel hardware-efficient layout transformation and customized kernel implementations, \sys{} achieves up to \fixme{} and \fixme{} end-to-end speedup on CogVideoX-v1.5 and Hunyuan respectively, while preserving generation quality. Our code will be open-sourced soon.


\end{abstract}

% \TODO{DEMO: Dense vs sparsev2 vs sparsev4 vs ours}