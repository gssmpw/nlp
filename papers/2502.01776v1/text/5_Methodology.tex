\section{Methodology}
\label{sec:methodology}


\begin{algorithm}[t]
\caption{Online Profiling Strategy}
\label{alg:sample_based_pattern_selection}
\definecolor{codeblue}{rgb}{0.25,0.65,0.5}
\definecolor{codeblue2}{rgb}{0,0,1}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fixed,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue2},
    emph={mask},
    emphstyle={\color[RGB]{255,52,179}},
}
\begin{lstlisting}[language=python]
# Q, K, V, O: [B, H, S, D] - query, key, value, output
# S:              - Total Token Number E.g., 18k
# t:              - Sampled Token Number. E.g., 32

# Sample the Indices
indices = sample_indices(S, t) # (t,)
Q_i = Q[:, :, indices, :]

# Get the attention masks
mask_spatial = gen_spatial_mask()[:, :, indices, :]
mask_temporal = gen_temporal_mask()[:, :, indices, :]

# Compute sampled attention score
# Shape: [B, H, t, D]
O_full = mask_attention(Q_i, K, V, None)
O_spatial = mask_attention(Q_i, K, V, mask_spatial)
O_temporal = mask_attention(Q_i, K, V, mask_temporal)

# Calculate MSE and get best mask
# Shape: [B, H]
MSE_s = (O_full - O_spatial).norm().mean(dim=(2,3))
MSE_t = (O_full - O_temporal).norm().mean(dim=(2,3))
best_mask_config = (MSE_s < MSE_t)

\end{lstlisting}
\end{algorithm}


In this section, we introduce \sys{}, a training-free framework designed to exploit the sparse patterns of \attn{} while addressing practical deployment challenges through careful design. To identify sparse patterns, \sys{} employs an \onlinesample{} (Sec~\ref{subsec:sampling_based_pattern_selection}). To effectively utilize sparsity, \sys{} introduces a hardware-efficient \reorder{}, which enables real-world hardware acceleration (Sec~\ref{subsec:frame_token_rearrangement}). Additionally, by integrating techniques such as customized kernels and quantization (Sec~\ref{subsec:other-optimization}), \sys{} significantly accelerates video generation without compromising generation quality.


\subsection{\Onlinesample{} for sparsity identification}
\label{subsec:sampling_based_pattern_selection}

As discussed in Sec~\ref{subsec:sparse-pattern}, all attention heads can be classified and sparsified into \spatialhead{} and \temporalhead{}. However, we find that such sparse patterns can be \textit{highly dynamic} across different denoising steps and input data. E.g., a certain head can be a \spatialhead{} for one prompt while being a \temporalhead{} given another. This dynamic nature necessitates an efficient online sparsity identification method, which classifies attention heads on the fly without extra overhead. 


To this end, \sys{} proposes an \textit{\onlinesample{}}. Instead of computing the entire full attention to identify sparse attention, \sys{} only samples a subset of input rows ($x$\%) and calculates results with both the spatial and temporal sparsity patterns. By choosing the one with the lower MSE compared to full attention, \sys{} can efficiently approximate the oracle identification method discussed in Sec~\ref{subsec:sparse-accurate}. We detail the profiling process in Algorithm~\ref{alg:sample_based_pattern_selection}.


To demonstrate the effectiveness of the proposed method, we conduct a sensitivity test on profiling ratio $x$ with CogVideoX-v1.5-I2V. As shown in Table~\ref{table:sensitivity-sampling}, profiling only $1$\% can achieve up to $31.1$ PSNR, with only $3$\% runtime overhead compared to full attention.

\input{text/6_Table_main}

\subsection{Hardware-efficient \reorder{}}
\label{subsec:frame_token_rearrangement}

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{figure/layout-transformation.pdf} 
    \caption{Visualization of hardware-efficient \reorder{}. (a) Non-contiguous sparsity layout of \temporalhead{}, which is hardware inefficient due to the contiguous layout required by hardware accelerators. (b) Contiguous layout generated by transposing the token-major tensor into a frame-major one, which can be efficiently handled by block sparse attention.}
    \label{fig:frame-token-rearrangement-figure}
\end{figure}

\looseness=-1
Despite the high sparsity in attention computation, speedups are limited without a hardware-efficient sparsity layout~\cite{ye2023sparsetircomposableabstractionssparse,zheng2023pitoptimizationdynamicsparse}. For instance, NVIDIAâ€™s Tensor Core, a matrix-matrix multiplication accelerator, requires at least $16$ contiguous elements along each dimension to use. However, \temporalhead{} exhibits a sparse layout of non-contiguous elements with a stride of $L$ (i.e., number of tokens per frame). This sparsity pattern prevents effective utilization of Tensor Core, thereby constraining overall efficiency.


To tackle this, \sys{} introduces a \textit{\reorder{}} strategy that transforms the sparsity layout of \temporalhead{} into a hardware-efficient one. As illustrated in Figure~\ref{fig:frame-token-rearrangement-figure}, this strategy transposes a token-major tensor into a frame-major one, which makes the tokens across different frames into a contiguous layout. Such transformation maintains a mathematically equivalent output as attention computation is associative~\cite{dao2022flashattentionfastmemoryefficientexact,dao2019butterfly}. We ablate the effectiveness of the proposed method in Sec~\ref{subsec:kernel_level_efficiency}.


\subsection{Other optimizations}
\label{subsec:other-optimization}

\textbf{Efficient kernel customization.} We notice that current implementations of QK-norm and RoPE suffer from performance issues, due to limited parallelism on small head dimensions (e.g., $64$ in CogVideoX-v1.5). Therefore, we customize those operations with CUDA by a sub-warp reduction implementation, providing up to $5\times$ speedup compared to torch implementation (see Table~\ref{table:small-kernel-speedup-comparison}). We also use Triton to implement fused \onlinesample{} and \reorder{} kernels, followed by a block sparse attention kernel with FlashInfer \citep{ye2025flashinferefficientcustomizableattention}.


\textbf{Quantization.} We further incorporate \textbf{FP8 quantization} into sparse attention~\cite{zhang2025sageattention,zhang2024sageattention2,zhao2024atom}, which further boosts up to $1.3\times$ throughput with minimal accuracy drop as shown in Table~\ref{table:accuracy_efficiency_benchmark}. We also customize an attention kernel that supports both FP8 quantization and block sparse computation.


%Our method emphasizes an efficient system design to fully utilize GPU hardware and maximize acceleration.  The core of our approach is a high-performance block sparse attention kernel, implemented using flashinfer, which achieves runtime performance very close to the theoretical speedup. To dynamically select the optimal sparse pattern, we implemented the sampling-based mechanism with a custom Triton kernel that ensures QK computation is performed only once, reducing redundant operations and maintaining high efficiency. Additionally, we optimized QK-norm and RMS-norm operations for video diffusion models like HunyuanVideo and CogVideoX, which perform these computations along the head dimension (128 and 64, respectively). Unlike token-dimension norm operations in language models, head-dimension operations can underutilize hardware. To address this, we designed efficient norm kernels that significantly improve computational efficiency.

%\paragraph{Advanced Optimizations} To maintain high video quality while optimizing efficiency, we adopt several techniques to handle critical stages of the generation process. Inspired by works like SVDQuant, PAB, and Quest, we compute full attention during the earlier denoising steps and in the first transformer layer, ensuring that important high-fidelity information is preserved. We also integrate state-of-the-art methods, including FP8 attention for accelerating sparse attention and SVDQuant for optimizing linear layers such as the MLP. As attention acceleration reduces its runtime proportion, the MLP becomes a more prominent bottleneck, accounting for nearly half of the overall computation. By applying SVDQuant to the MLP, we achieve additional speedup while maintaining high-quality video generation.

% Specifically, if we denote the rearrangement in a matrix form as $P$, then for $Q, K, V$, we calculate the attention as 
% {\small
% $\begin{array}{ll}
%     \operatorname{softmax}(\frac{(PQ)(PK)^T}{\sqrt{d}})(PV) &= P\operatorname{softmax}(\frac{QK^T}{\sqrt{d}})P^T(PV) \\
%     &= P\operatorname{softmax}(\frac{QK^T}{\sqrt{d}})V.
% \end{array}$
% }
% Such an arrangement effectively harnesses inter-frame correlations, allowing each token to focus on others across different frames. This generates an attention map concentrated along the main diagonal and allows the continued application of block sparse attention with high efficiency. Note that we only apply rearrangement to video tokens and leave text tokens unchanged. Our algorithm is formalized as Algorithm~\ref{alg:code_inter_frame}.
% To get the original output $O$, we revert it back by multiplying an $P^\top$ as $O \;=\; P^\top \,\Bigl[\mathrm{Attention}\bigl(PQ,\; PK,\; PV\bigr)\Bigr]$. 