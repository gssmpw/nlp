

\section{Related Work}
\label{sec:related_work}
\input{020related}


\section{Indices Shuffling Algorithm}
\label{sec:indices_shuffling_algorithm}

To avoid overlapping subject IDs when loading data from multiple datasets, we first count the number of subjects in each dataset and assign each subject a unique subject ID starting from 1. As a result, each sample \(\bm{x}_i\) has a corresponding new subject ID \(\bm{s}_i\), where \(\bm{s}_i = \bm{s}_j\) indicates that \(\bm{x}_i\) and \(\bm{x}_j\) are from the same subject. In real-world scenarios, as the number of subjects increases, the likelihood of samples with the same subject ID appearing in the same training batch decreases. This situation may hinder subject-level contrastive learning. 

To address this issue, we develop an indices shuffling algorithm, which is called every epoch by passing it to the \textit{sampler} parameter in the PyTorch \textit{DataLoader}. We first sort the indices by subject IDS and split the sorted indices of the entire training set into small groups (where the group size is much smaller than the batch size), each containing indices of samples from the same subject ID. We then randomly shuffle these groups rather than shuffle the individual samples. After shuffling the groups, we split the shuffled indices into batches and shuffle the indices within each batch. This two-step shuffling process ensures the randomness of the samples in each training epoch while maintaining a relatively balanced number of positive pairs for subject-level contrastive learning. The pseudocode for indices shuffling is provided in Algorithm~\ref{alg:indices_shuffling}.


\begin{algorithm}[t]
\caption{Pseudo code of Indices Shuffling.}
\label{alg:indices_shuffling}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
\lstset{
  backgroundcolor=\color{white},
  basicstyle=\fontsize{7.2pt}{7.2pt}\ttfamily\selectfont,
  columns=fullflexible,
  breaklines=true,
  captionpos=b,
  commentstyle=\fontsize{7.2pt}{7.2pt}\color{codeblue},
  keywordstyle=\fontsize{7.2pt}{7.2pt},
%  frame=tb,
}
\begin{lstlisting}[language=python]
import numpy as np
# ids: subject IDs array
# bs: pseudo batch size for shuffling
# gs: pseudo group size for shuffling

def shuffle_indices(ids, batch_size=128, group_size=2)
    # indices sorted by subject IDs
    indices = np.argsort(subject_ids)
    length = len(indices)

    # split indices into groups
    groups = [indices[i:i + group_size] for i in range(0, length, group_size)]

    # shuffle groups
    np.random.shuffle(groups) 
    # concatenate groups
    indices = np.concatenate(groups) 

    # split indices into batches
    batches = [indices[i:i + batch_size] for i in range(0, length, batch_size)]

    # shuffle indices in the batch
    for batch in batches: 
        np.random.shuffle(batch)
    # concatenate batches
    indices = np.concatenate(batches) 

    return indices
\end{lstlisting}
\end{algorithm}



\section{Data Augmentation Banks}
\label{sec:data_augmentation_banks}

We apply data augmentation for self-supervised contrastive pretraining and some supervised learning methods. We utilize a bank of data augmentation techniques to enhance the model's robustness and generalization. During the forward pass in the training of each iteration, one augmentation method will be picked from available augmentation options with equal probability. The data augmentation methods include temporal flipping, temporal masking, frequency masking, channel masking, jittering, and dropout, and can be further expanded to more choices. We provide a detailed description of each technique below.


\textbf{Temporal Flippling.}
We reverse the EEG data along the temporal dimension. The probability of applying this augmentation is controlled by a parameter \textit{prob}, with a default value of 0.5.

\textbf{Temporal Masking.}
We randomly mask timestamps across all channels. The proportion of timestamps masked is controlled by the parameter \textit{ratio}, with a default value of 0.1. 

\textbf{Frequency Masking.} 
First introduced in~\cite{zhang2022self} for contrastive learning, this method involves converting the EEG data into the frequency domain, randomly masking some frequency bands, and then converting it back. The proportion of frequency bands masked is controlled by the parameter \textit{ratio}, with a default value of 0.1. 

\textbf{Channel Masking.}
We randomly mask channels across all timestamps. The proportion of channel masked is controlled by the parameter \textit{ratio}, with a default value of 0.1. 

\textbf{Jittering.} Random noise, ranging from 0 to 1, is added to the raw data. The intensity of the noise is adjusted by the parameter \textit{scale}, which is set by default to 0.1. 

\textbf{Dropout.} Similar to the dropout layer in neural networks, this method randomly drops some values. The proportion of values dropped is controlled by the parameter \textit{ratio}, with a default value of 0.1.





\begin{figure*}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/methods/channel_alignment.pdf}
    \caption{\textbf{Channel Alignment.} We perform channel alignment to ensure that all datasets have the standard 19 channels according to the 10-20 international system. For datasets with more than 19 channels, we select the 19 channels based on their names. For datasets with fewer than 19 channels, we perform channel interpolation. In cases of channel name mismatches, we select the closest channels as alternatives by calculating their 3D coordinates.
    }
    \label{fig:channel_alignment}
    \vspace{-5mm}
\end{figure*}


\section{Datasets Preprocessing}
\label{sec:datasets_preprocessing}



We refer to datasets that include Alzheimer's Disease (AD) subjects as AD datasets, while datasets that do not include AD subjects are called non-AD datasets. In total, we have 9 AD datasets and 7 non-AD datasets. Note that all the non-AD datasets have one commonality: the label is assigned to each subject, which adapts to the subject-level contrastive learning. Among the AD datasets, \textbf{ADFTD}, \textbf{BrainLat}, \textbf{CNBPM}, \textbf{Cognision-ERP}, and \textbf{Cognision-rsEEG} are selected for downstream evaluations due to their high quality, larger number of subjects, and sufficiently long recording trials per subject, which provide a more robust assessment. The remaining 4 AD datasets, as well as all the non-AD datasets, are used for self-supervised pretraining to learn general EEG patterns and disease-specific patterns related to neurological diseases. 

For datasets where the raw channel names or numbers do not match the 19 standard channels (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, O2) in the 10-20 international system, we perform channel alignment to generate two versions of the processed data: a channel-aligned version with 19 channels and a raw-channel version with either more or fewer channels. The channel-aligned version is used for pretraining and unified fine-tuning across multiple datasets, while the raw-channel version without channel alignment is used for supervised learning on individual datasets. These two versions aim to demonstrate the effectiveness of the pretraining and unified fine-tuning method pipeline compared to supervised learning on individual datasets, even if it involves a trade-off where some channel information is lost in certain datasets. Note that the channels T7, T8, P7, and P8 are the same as the channels T3, T4, T5, and T6 in the international 10-20 and 10-10 system~\cite{acharya2016american}. Figure~\ref{fig:channel_alignment} illustrates the channel alignment process. The statistics of the processed datasets are presented in Table~\ref{tab:processed_data}.

There are three main reasons for aligning all datasets to the same 19 channels. First, these 19 channels are the most commonly used in EEG-based AD detection~\cite{aviles2024machine}, matching the goal of low-cost and convenient AD detection through EEG. Second, the standard 19 channels cover all the brain regions, preserving enough temporal and spatial information. This channel alignment approach avoids the trade-off between computational resources and patch length in existing methods that seek to capture both spatial and fine-grained temporal features~\cite{yang2024biot,wangeegpt,jiang2024large}, as we discuss later in Appendix~\ref{sec:discussion}. Third, using the same channels across datasets enables unified fine-tuning for downstream AD datasets together in one run, which significantly improves performance compared to fine-tuning on individual datasets, a benefit we demonstrate later in the ablation study~\ref{tab:single_dataset_finetune_validate}.

The final processed datasets are organized into two folders: \textit{Feature/} and \textit{Label/}. The \textit{Feature/} folder contains files named in the format \textit{feature\_ID.npy} for all subjects, where \textit{ID} represents the subject ID. Each \textit{feature\_ID.npy} file contains samples belonging to the same subject, stacked into a 3-D array with the shape \([N_{\text{sample}}, T, C]\), where \(N_{\text{sample}}\) denotes the number of samples in this subject, \(T\) denotes the number of timestamps per sample, and \(C\) denotes the number of channels. Note that different subjects may have different numbers of samples. The \textit{Label/} folder contains a file named \textit{label.npy}, which is a 2-D array with the shape \([N_{\text{subject}}, 2]\), where \(N_{\text{subject}}\) is the total number of subjects. The first column contains the subject's label (e.g., healthy or AD), and the second column contains the subject ID, which ranges from 1 to \(N_{\text{subject}}\).


\subsection{AD Datasets}
\label{sub:ad_datasets}


\subsubsection{AD-Auditory.} 
The AD-Auditory (40Hz Auditory Entrainment) is a publicly available EEG dataset on the OpenNEURO website\footnote{\url{https://openneuro.org/datasets/ds005048/versions/1.0.0}} from the paper~\cite{lahijanian2024auditory}. It contains 35 subjects, including 17 AD, 6 MCI, 10 healthy controls, and 2 unknown subjects. This dataset aims to investigate the effect of entrainment on brain oscillations using EEG signal recordings during auditory brain stimulation for distinguish Alzheimer’s Disease. All the data are recorded using 19 monopolar channels (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) based on the standard 10/20 system, with a sampling rate set to 250Hz. The dataset's authors preprocess the data using the EEGLab toolbox in Matlab, which includes bandpass filtering, noise removal, artifact removal, re-referencing, and interpolating rejected channels, as described in their paper and on the data website. We perform secondary data preprocessing to match the pipeline of our method.

Each subject has one recording trial. For each raw trial, we first downsample the trials from 250Hz to 128Hz. Then, we segment all the trials into 1-second samples with 128 timestamps. We drop the last sample if it is shorter than 128 timestamps. This results in a total of 37,425 1-second, 128Hz samples. We apply bandpass filtering ranging from 0.5Hz to 45Hz, followed by standard normalization on each channel. We perform preliminary evaluations on this dataset and find substantial variability among subjects. We suspect the limited number of subjects and potential data and label quality issues cause this variability. As a result, we decide to use this dataset for pre-training, although it is an AD dataset.


\subsubsection{ADFSU.} 
This is a publicly available dataset provided by Dr. Dennis Duke of Florida State University~\cite{vicchietti2023computational,nour2024novel}, as we name it to ADFSU\footnote{\url{https://osf.io/2v5md/}}. It contains data from 80 AD subjects and 12 healthy subjects. Each subject has a recording with a sampling frequency of 128Hz and an 8-second trial collected across 19 standard channels (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) during both eye-open and eye-closed resting-state conditions. The preprocessing steps includes band-pass filtering within the range of 0.5–30 Hz, and an experienced EEG expert removes artifacts caused by movements. We perform secondary preprocessing to match the pipeline of our method.

The data files are organized into "AD" and "Healthy" folders, with each folder containing "Eyes\_open" and "Eyes\_closed" subfolders to indicate the different tasks. Each task folder contains subject folders labeled with ID numbers, and all the data are stored in channel\_name.txt files. Note that the eye-open data for the healthy subject with ID 5 is empty. For simplicity, we manually copy the eyes-closed data for this subject and use it as the eye-open data to avoid handling empty files in the code. For each subject, we first load all the channel text files and concatenate them into two recording trials: eyes-open and eyes-closed. We segment the data for each trial into 1-second, half-overlapping samples with 128 timestamps. We discard the last sample if it is shorter than 128 timestamps. This results in a total of 2,760 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. Due to the extreme imbalance between AD and healthy subjects, and the limited length of each subject’s recording trial, we use this dataset for pretraining rather than downstream evaluation.


\subsubsection{ADFTD.} 
The ADFTD (A dataset of EEG recordings from Alzheimer's disease, Frontotemporal dementia and Healthy subjects) is a publicly available EEG dataset on the OpenNEURO website\footnote{\url{https://openneuro.org/datasets/ds004504/versions/1.0.8}} from the paper~\cite{miltiadous2023dataset,miltiadous2023dice}. It contains 88 subjects, including 36 AD, 23 Frontotemporal Dementia (FTD), and 29 healthy controls. For recording, a Nihon Kohden EEG 2100 clinical device is used, with 19 scalp electrodes (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) according to the 10-20 international system and 2 reference electrodes (A1 and A2) placed on the mastoids for impedance check, according to the manual of the device. Each recording is performed according to the clinical protocol, with participants sitting with their eyes closed. The collection sampling rate is 500Hz. The dataset's authors preprocess the data using the EEGLab toolbox in Matlab, which includes bandpass filtering, noise removal, artifact removal, re-referencing, and interpolating rejected channels, as described in their paper and on the data website. We perform secondary data preprocessing to match the pipeline of our method.

Each subject has one recording trial. This paper only uses 65 subjects that are AD and healthy control subjects. For each raw trial, we first downsample the trials from 500Hz to 128Hz. Then, we segment all the trials into 1-second samples with 128 timestamps. We drop the last sample if it is shorter than 128 timestamps. This results in a total of 53,215 1-second, 128Hz samples. We apply bandpass filtering ranging from 0.5Hz to 45Hz, followed by standard normalization on each channel. This AD dataset is used for downstream evaluation as it contains enough subjects, and each subject has a long enough recording to be segmented into samples.


\subsubsection{ADSZ.} 
The ADSZ (Alzheimer's Disease and Schizophrenia) dataset is a public EEG dataset\footnote{\url{https://figshare.com/articles/dataset/Alzheimer_s_disease_and_Schizophrenia/19091771}} from the paper~\cite{alves2022eeg,pineda2020quantile}. We use only the sub-dataset for Alzheimer's disease (AD) available in the download link. This dataset contains data from 48 subjects, including 24 AD subjects and 24 healthy elderly subjects. The data are collected from 19 standard channels (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) during eyes-open and eyes-closed resting states, with a sampling frequency of 128Hz. Most subjects have an EEG trial duration of 8 seconds, although some trials last 10, 12, or 14 seconds, with timestamps ranging from 1,024 to 1,792. The preprocessing of the signals includes band-pass filtering within the range of 1–30 Hz, and an experienced EEG technician removes artifacts caused by subject movements. We perform secondary preprocessing to match the pipeline of our method.

For each trial, we segment the data into 1-second, half-overlapping samples with 128 timestamps. We discard the last sample if it is shorter than 128 timestamps. This results in a total of 768 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. Due to the limited number of subjects and the short duration of each subject’s recording trials, we use this dataset for pretraining rather than downstream evaluation.



\subsubsection{APAVA.} 
The APAVA (Alzheimer’s Patients’ Relatives Association of Valladolid) dataset\footnote{\url{https://osf.io/jbysn/}}, referenced in the study by~\cite{escudero2006analysis}, is a publicly available EEG dataset consisting of 23 subjects, including 12 AD subjects and 11 healthy elderly subjects. The data are recorded using 16 channels (Fp1, Fp2, F7, F3, F4, F8, T3, C3, C4, T4, T5, P3, P4, T6, O1, and O2) with a sampling frequency of 256Hz. Each subject has multiple trials, with each trial lasting 5 seconds, corresponding to 1,280 timestamps. A specialist physician visually inspects the recordings to select data with minimal movement, electromyographic activity, or electrooculographic artifacts. A bandpass filter is applied with 0.5 Hz and 40 Hz cut-off frequencies. We perform secondary preprocessing to match the pipeline of our method.

Since this dataset has only 16 channels compared to the 19 standard channels, we perform channel alignment using the Python MNE EEG processing tools. Specifically, by checking the file information stored in the Matlab files, we found that the three missing channels are Fz, Cz, and Pz. We set these channels as "bad channels" and interpolate them using the montage "standard\_1020." After this, we have recording trials in all 19 standard channels. We then downsample all the trials from 256Hz to 128Hz. We segment the data into 1-second, half-overlapping samples for each trial with 128 timestamps. We discard the last sample if it is shorter than 128 timestamps. This results in a total of 5,967 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. Besides, we also process a raw 16-channel data version for supervised learning on individual datasets by not doing channel alignment and keeping other procedures the same. Due to the limited number of subjects and the short duration of each subject’s recording trials, we use this dataset for pretraining rather than downstream evaluation.



\subsubsection{BrainLat.} 
The BrainLat\footnote{\url{https://www.synapse.org/Synapse:syn51549340/wiki/624187}} (Latin American Brain Health Institute) dataset comprises multimodal neuroimaging data from 780 participants from Latin America~\cite{prado2023brainlat}. It contains two modalities: EEG and MRI. It includes five classes of subjects: Alzheimer's disease (AD), behavioral variant frontotemporal dementia (bvFTD), multiple sclerosis (MS), Parkinson's disease (PD), and healthy controls (HC). For EEG data recording, subjects are recorded in an eye-closed resting state inside a dimly lit, sound-attenuated, and electromagnetically shielded EEG room. They are instructed to remain still and awake, with a 128-channel Biosemi Active-two acquisition system (pin-type, active, sintered Ag-AgCl electrodes). The data are band-pass filtered between 0.5 and 40 Hz using a zero-phase shift Butterworth filter of order 8. The data are then downsampled to 512 Hz, and Independent Component Analysis (ICA) is used to correct EEG artifacts induced by blinking and eye movements. We perform secondary preprocessing to match the pipeline of our method.

In this paper, we use only the EEG data from the AD and HC classes for research. EEG data for each subject are stored in folders labeled AR and CL, representing the subjects' countries: Argentina and Chile. It is important to note that some subjects cannot read for unknown reasons, such as the subject named "sub-100013" (at least when we downloaded the dataset, which the data version was last modified by Dr. Pavel Prado on 7/2/2024). Additionally, not all subjects have EEG data; most subjects only have MRI datasets. In total, there are 135 functional subjects with EEG data across all five classes, with 67 subjects(35 AD and 32 HC) used for this paper.

Since this dataset uses the Biosemi128 montage instead of the standard\_1020 montage, which has an entirely different electrode naming and positioning scheme, we perform channel alignment using the Python MNE EEG processing tools. Specifically, we use the 3-D coordinates of the channels to identify 19 channels in the Biosemi128 montage closest to the 19 standard channels in the 10-20 system. The closest channels are C29, C16, D7, D4, C21, C4, C7, D24, D19, A1, B22, B14, A10, A18, A19, B4, B7, A16, and A29, which we use as replacements for the 19 standard channels. We then downsample all trials from 512Hz to 128Hz. We segment the data into 1-second, 128Hz samples with 128 timestamps. We discard the last sample if it is shorter than 128 timestamps. This results in a total of 29,788 1-second, 128Hz samples. We apply bandpass filtering from 0.5Hz to 45Hz, followed by standard normalization on each channel. We also process a raw 128-channel data version for supervised learning on individual datasets by not performing channel alignment while keeping all other procedures. This high-quality dataset has enough subjects and trial recording length; we use it for downstream evaluations.



\subsubsection{CNBPM.}
The CNBPM is a large private EEG dataset provided by the AI-LAB laboratory at the University Mediterranea of Reggio Calabria, Italy, referenced in studies~\cite{ieracitano2019time, amezquita2019novel}. It consists of 63 subjects with Alzheimer's Disease (AD), 63 with Mild Cognitive Impairment (MCI), and 63 Healthy Control (HC) subjects. The data are collected using 19 standard channels (Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2) with an initial sampling rate of 1024Hz. A frequency-band filter is applied to filter the frequency bands between 0.5 and 32 Hz, followed by downsampling to reduce the sampling rate to 256Hz. Visible blinks affected by artifacts are visually inspected and removed by an EEG expert. We perform secondary preprocessing to match the pipeline of our method.

In this paper, we use only the EEG data of 63 AD and 63 HC for research. For each subject's recording trial, which ranges from several minutes to over half an hour, we first downsample all trials from 256Hz to 128Hz. We then segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the trial's edge. This results in a total of 46,336 1-second, 128Hz samples. We apply bandpass filtering ranging from 0.5Hz to 45Hz, followed by standard normalization on each channel. Since this dataset is sufficiently large and high-quality, with more than 100 subjects and long recording trials per subject, we use it for downstream evaluations.




\subsubsection{Cognision-ERP.} 
The Cognision-ERP is a private Event-Related Potential (ERP) EEG dataset from the \textit{Cognision}\footnote{\url{https://www.cognision.com/}} company, referenced in the study~\cite{cecchi2015clinical}. It contains 177 subjects, including 90 Alzheimer's Disease (AD) subjects and 87 Healthy Control (HC) subjects. The total number of samples is 61,300, with each subject having either 300 or 400 samples, and each sample containing 149 timestamps. The sampling rate is 125Hz, and there are 7 channels (Fz, Cz, Pz, F3, P3, F4, P4). Artifacts, such as eye movements, are visually inspected and removed by an EEG expert. We perform secondary preprocessing to match the pipeline of our method.

Since this dataset has only 7 channels compared to the 19 standard channels, we perform channel alignment using the Python MNE EEG processing tools. Specifically, the 12 missing channels are Fp1, Fp2, F7, F8, T3, C3, C4, T4, T5, T6, O1, and O2. We set these channels as "bad channels" and interpolate them using the montage "standard\_1020." After this, we have recording samples with all 19 standard channels. We then upsample all the samples from 125Hz to 128Hz, which increases the number of timestamps per sample from 149 to 153. We take the middle 128 timestamps as the new sample. This results in a total of 61,300 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. We also process a raw 7-channel data version for supervised learning on individual datasets by not performing channel alignment while keeping all other procedures the same. Since this dataset is sufficiently large and high-quality, with more than 100 subjects and many samples per subject, we use it for downstream evaluations.




\subsubsection{Cognision-rsEEG.} 
The Cognision-rsEEG is a private EEG dataset from the \textit{Cognision} company. Unlike Cognision-ERP, Cognision-rsEEG consists of resting-state EEG data. It contains 180 subjects, including 97 Alzheimer's Disease (AD) subjects and 83 Healthy Control (HC) subjects. Each subject has a recording trial with 22,524 timestamps collected at 125Hz. The number of channels is 7 (Fz, Cz, Pz, F3, P3, F4, P4). Artifacts, such as eye movements, are visually inspected and removed by an EEG expert. We perform secondary preprocessing to match the pipeline of our method.

Since this dataset has only 7 channels compared to the 19 standard channels, we perform channel alignment using the Python MNE EEG processing tools. Specifically, the 12 missing channels are Fp1, Fp2, F7, F8, T3, C3, C4, T4, T5, T6, O1, and O2. We set these channels as "bad channels" and interpolate them using the "standard\_1020" montage. After this, we obtain recording samples with all 19 standard channels. We then upsample all the samples from 125Hz to 128Hz. Next, we segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the edges of the trial. This results in a total of 32,400 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. Since this dataset is sufficiently large and high-quality, with more than 100 subjects and many samples per subject, we use it for downstream evaluations.





\subsection{Non-AD Datasets}
\label{sub:non_ad_datasets}


\subsubsection{Depression.}
The Depression (EEG: Depression rest) dataset is a publicly available EEG dataset on the OpenNEURO website\footnote{\url{https://openneuro.org/datasets/ds003478/versions/1.1.0}} from the paper~\cite{cavanagh2019multiple,cavanagh2021eeg}. It contains data from 122 college-age subjects with healthy and different degrees of depression. The EEG data are recorded in a resting state, with instructions for eyes open and eyes closed, triggering one-minute spans of either open or closed eyes. Each subject's depression level is labeled based on their score on the Beck Depression Inventory (BDI). The raw data sampling frequency is 500Hz. We perform secondary preprocessing to match the pipeline of our method.

Each subject has one or multiple recording trials. We check the channel information for each trial and find that some trials have 66 channels and others have 67 channels. We perform channel alignment by selecting the 19 standard channels: Fp1, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, P8, O1, and O2. Note that the channels T7, T8, P7, and P8 are the same as the channels T3, T4, T5, and T6 in the international 10-20 and 10-10 system~\cite{acharya2016american}. After alignment, we obtain recording trials with all 19 standard channels. We then downsample all the trials from 500Hz to 128Hz. Next, we segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the edges of the trial. This results in a total of 24,014 1-second, 128Hz samples. This 19-channel processed dataset is used for self-supervised contrastive pretraining. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. We also process a raw 66-channel data version for supervised learning on individual datasets by not performing channel alignment while keeping all other procedures the same. The 66 channels are picked by the most common channel names among trials.




\subsubsection{PEARL-Neuro.}
The PEARL-Neuro (A Polish Electroencephalography, Alzheimer’s Risk-genes, Lifestyle and Neuroimaging) dataset is a publicly available EEG dataset on the OpenNEURO website\footnote{\url{https://openneuro.org/datasets/ds004796/versions/1.0.9}}, referenced in the paper~\cite{dzianok2024pearl}. The full dataset contains data from 192 self-reported healthy middle-aged (50-63) subjects, with a balanced female-to-male ratio. Of these, 79 subjects are publicly available, and the dataset includes two modalities: EEG and fMRI. Other information, such as blood tests, demographics, and other health conditions, are also provided. The dataset aims to identify genetic variations associated with brain anatomical and functional phenotype imaging genomics, which could be potential biomarkers for predicting the risk of developing neurological and psychiatric disorders. This could lead to earlier diagnoses, more targeted treatments, and improved patient outcomes. EEG data are recorded using Brain Products systems, including an actiCHamp amplifier and high-density actiCAP electrode caps with 128 electrodes (Brain Products GmbH, Munich, Germany). The FCz electrode is used as an online reference, and the sampling rate is set to 1000Hz with a low-pass filter at 280Hz. The dataset includes three different tasks: the Sternberg memory task (Sternberg), the Multi-source interference task (MSIT), and resting-state (rest). In this paper, we use only the resting-state EEG data. We perform secondary preprocessing to match the pipeline of our method.

For resting-state trials from each subject, we first align the channels by selecting 19 standard channels: Fp1, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, P8, O1, and O2, where T7, T8, P7, and P8 correspond to the channels T3, T4, T5, and T6 in the international 10-20 system. After alignment, we obtain recording trials with all 19 standard channels. We then downsample all trials from 1000Hz to 128Hz. Next, we segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the edges of the trial. This results in a total of 51,670 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. This 19-channel processed dataset is used for self-supervised contrastive pretraining. Additionally, we process a 127-channel data version for supervised learning on individual datasets without channel alignment but keeping all other procedures the same. 




\subsubsection{REEG-BACA.}
The REEG-BACA (Resting-state EEG data before and after cognitive activity across the adult lifespan and a 5-year follow-up) dataset is a publicly available EEG dataset on the OpenNEURO website\footnote{\url{https://openneuro.org/datasets/ds005385/versions/1.0.2}}, referenced in the paper~\cite{getzmann2024resting}. According to the paper’s description, this dataset consists of 64 channels based on the 10-20 system, with the FCz electrode as an online reference. It includes resting-state EEG recordings from 608 subjects aged between 20 and 70 years, along with follow-up measurements of 208 subjects approximately 5 years later, starting in 2021. The EEG data are recorded with eyes open and eyes closed before and after a 2-hour block of cognitive experimental tasks. The EEG data are recorded at a 1000Hz sampling rate and filtered online using a 250Hz low-pass filter. This dataset aims to study the aging of brain activity in a resting state and provide a normal distribution of healthy subjects’ resting-state EEG for comparison with clinically relevant disorders. We perform secondary preprocessing to match the pipeline of our method.

For resting-state trials in both the eye-open and eye-closed conditions from each subject, we first align the channels by selecting the 19 standard channels: Fp1, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, P8, O1, and O2. Note that T7, T8, P7, and P8 are the same channels as T3, T4, T5, and T6 in the international 10-20 and 10-10 systems. After alignment, we obtain recording trials with all 19 standard channels. We then downsample all trials from 1000Hz to 128Hz. Next, we segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the edges of the trial. This results in a total of 611,269 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. This 19-channel processed dataset is used for self-supervised contrastive pretraining. Additionally, we process a 65-channel data version for supervised learning on individual datasets by not performing channel alignment while keeping all other procedures the same. 



\subsubsection{REEG-PD.}
The REEG-PD (Rest eyes open) dataset is a publicly available EEG dataset on the OpenNEURO website\footnote{\url{https://openneuro.org/datasets/ds004584/versions/1.0.0}}, referenced in the paper~\cite{singh2023evoked}. This dataset includes 149 subjects, with 100 Parkinson's disease (PD) subjects and 49 Healthy controls (HC) subjects. According toe the description in their paper, the EEG data is recorded with a 64-channel BrainVision cap in a resting state with their eyes open for two minutes. The sampling frequency is set to 500Hz, and a 0.1Hz high-pass filter is applied to the EEG recordings. The Fully Automated Statistical Thresholding for EEG artifact Rejection (FASTER) algorithm rejects the bad channels and trials with greater than +/- 3 Z-scores on key metrics and pop\_rejchan function from EEGLAB. Bad channels are interpolated except the mid-frontal Cz channel, which is never interpolated. Eye blink artifacts are removed following independent component analysis(ICA). We perform secondary preprocessing to match the pipeline of our method.

For the trials in each subject, we first align the channels into 19 channels Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2 by selecting from the existing channels in the data. Channel Pz is not included in the existing channels, so we use the closest channel, POz, as a replacement. Besides, T7, T8, P7, P8 are the same channels as T3, T4, T5, T6. Finally, we select Fp1, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, POz, P4, P8, O1, and O2 as the 19 standard channels. After alignment, we obtain recording trials with all 19 standard channels. We then downsample all trials from 500Hz to 128Hz. Next, we segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the edges of the trial. This results in a total of 23,839 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. This 19-channel processed dataset is used for self-supervised contrastive pretraining. Additionally, we process a raw 60-channel data version for supervised learning on individual datasets by not performing channel alignment while keeping all other procedures the same. Since some trials have mismatched channels, the 60 channels are picked by the most common channel names among trials. 



\subsubsection{REEG-SRM.}
The REEG-SRM (SRM Resting-state EEG) dataset is a publicly available EEG dataset on the OpenNEURO website\footnote{\url{https://openneuro.org/datasets/ds003775/versions/1.2.1}}, referenced in the paper~\cite{hatlestad2022bids}. This dataset contains resting-state EEG extracted from the experimental paradigm used in the Stimulus-Selective Response Modulation (SRM) project at the Department of Psychology, University of Oslo, Norway. The EEG data are recorded using 64 electrodes with a BioSemi ActiveTwo system, following the positional scheme of the 10-10 system. The dataset includes 111 healthy control subjects, with some subjects having one trial and others having multiple trials. The sampling rate is set to 1024Hz. Preprocessing steps are applied to the raw data, including bad channel interpolation, artifact rejection, and bandpass filtering from 1Hz to 45Hz. We perform secondary preprocessing to match the pipeline of our method.

We exclude two subjects who cannot read, identified as "sub-029" and "sub-104." For the remaining 109 subjects, we perform channel alignment by selecting the 19 standard channels: Fp1, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, P8, O1, and O2. Note that T7, T8, P7, and P8 are the same channels as T3, T4, T5, and T6 in the international 10-20 and 10-10 systems. After alignment, we obtain recording trials with all 19 standard channels. We then downsample all trials from 1024Hz to 128Hz. Next, we segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the edges of the trial. This results in a total of 32,760 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. The 19-channel processed dataset is used for self-supervised contrastive pretraining. Additionally, we process a raw 64-channel data version for supervised learning on individual datasets by not performing channel alignment but keeping all other procedures the same. Since some trials have mismatched channels, the 60 channels are selected based on the most common channel names across trials.




\subsubsection{TDBrain.}
The TDBrain (Two Decades-Brainclinics Research Archive for Insights in Neurophysiology) dataset\footnote{\url{https://brainclinics.com/resources/}}\footnote{\url{https://www.synapse.org/Synapse:syn25671079/wiki/610278}}, referenced in the paper~\cite{van2022two}, is a large permission-available EEG time series dataset recording brain activities of 1274 subjects with 33 channels. Researchers need to send requests to the authors by filling out the application forms to get access to this dataset. This dataset aims to research neurological or psychiatric dysfunction, such as Major Depressive 
Disorder (MDD), attention deficit hyperactivity disorder (ADHD), Subjective Memory Complaints (SMC), obsessive-compulsive disorder (OCD), Parkinson's disease (PD), and many other brain disorders. The EEG data is recorded in resting-states in eye-open and eye-closed states. The sampling rate is 500Hz. Preprocessing steps are applied to the raw data, including artifact rejection, 50Hz notch-frequency removal, and bandpass filtering from 0.5Hz to 100Hz. We perform secondary preprocessing to match the pipeline of our method.


We exclude subjects with "REPLICATION" and "NaN" indications in the datasets, which are left for validation and testing for the researcher's model by contacting them, as described in their paper. For the remaining 911 subjects, we perform channel alignment by selecting the 19 standard channels: Fp1, Fp2, F7, F3, Fz, F4, F8, T7, C3, Cz, C4, T8, P7, P3, Pz, P4, P8, O1, and O2. Note that T7, T8, P7, and P8 are the same channels as T3, T4, T5, and T6 in the international 10-20 and 10-10 systems. After alignment, we obtain recording trials with all 19 standard channels. We then downsample all trials from 500Hz to 128Hz. Next, we segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the edges of the trial. This results in a total of 231,689 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel. The 19-channel processed dataset is used for self-supervised contrastive pretraining. Additionally, we process a raw 33-channel data version for supervised learning on individual datasets by not performing channel alignment but keeping all other procedures the same.


\subsubsection{TUEP.}
The TUEP\footnote{\url{https://isip.piconepress.com/projects/nedc/html/tuh_eeg/}} is one of the datasets in The Temple University Hospital (TUH) Electroencephalography (EEG) Corpus, which is the world’s largest open-source EEG corpus. Researchers can access this dataset by submitting a request via an application form to the authors. This dataset is a subset of TUEG and contains data from 100 subjects with epilepsy and 100 subjects without epilepsy, as determined by a certified neurologist. We perform data preprocessing to align the data with our method.

Each subject has one or more trials, and some trials may have different numbers of channels and sampling rates. We first select subjects who have 19 standard channels: Fp1, Fp2, F7, F3, Fz, F4, F8, T3, C3, Cz, C4, T4, T5, P3, Pz, P4, T6, O1, and O2, in all their trials. A total of 179 subjects meet this requirement. For each trial from these 179 subjects, the majority have a 256Hz sampling rate. We downsample or upsample all trials to 128Hz. Next, we segment the trials into 1-second, non-overlapping samples, excluding those shorter than 1 second at the trial edges. Some subjects have a large number of trials, resulting in more than 10,000 samples in total per subject. Since our goal for pretraining is to learn general EEG features and disease-related features across subjects, we aim to avoid the model overfitting to subject-specific features. Therefore, we set 800 as the maximum number of samples per subject, randomly selecting 800 samples if the total number exceeds this threshold. This results in a total of 143,200 1-second, 128Hz samples. We apply bandpass filtering in the range of 0.5Hz to 45Hz, followed by standard normalization on each channel.





\section{Implementation Details}
\label{sec:implementation_details}

\input{tables/training_setups}
\input{tables/training_params}


\subsection{LEAD}
\label{sub:lead}


We perform pretraining on 11 datasets: \textbf{AD-Auditory}~\cite{lahijanian2024auditory}, \textbf{ADFSU}~\cite{vicchietti2023computational}, \textbf{ADSZ}~\cite{alves2022eeg,pineda2020quantile}, \textbf{APAVA}~\cite{escudero2006analysis,smith2017accounting}, \textbf{Depression}~\cite{cavanagh2019multiple,cavanagh2021eeg}, \textbf{PEARL-Neuro}~\cite{dzianok2024pearl}, \textbf{REEG-BACA}~\cite{getzmann2024resting}, \textbf{REEG-PD}~\cite{singh2023evoked}, \textbf{REEG-SRM}~\cite{hatlestad2022bids}, \textbf{TDBrain}~\cite{van2022two}, and \textbf{TUEP}~\cite{veloso2017big}, and fine-tuning on 5 downstream datasets: \textbf{ADFTD}~\cite{miltiadous2023dataset,miltiadous2023dice}, \textbf{BrainLat}~\cite{prado2023brainlat}, \textbf{CNBPM}~\cite{ieracitano2019time,amezquita2019novel}, \textbf{Cognision-ERP}~\cite{cecchi2015clinical}, and \textbf{Cognision-rsEEG}. The details of datasets and preprocessing steps are described in the previous section~\ref{sec:datasets_preprocessing}. The pretraining datasets include 7 non-AD brain diseases or healthy subjects and 4 AD datasets, totaling 2,354 subjects and 1,165,361 1-second, 128Hz samples. All the downstream datasets are binary classifications between Alzheimer's disease and healthy controls, totaling 615 subjects and 223,039 1-second, 128Hz samples. The rationale for selecting these datasets for pretraining or fine-tuning is presented in~\ref{sub:datasets_selection}.

We compare our method with 10 baselines, including 5 supervised, 3 self-supervised learning, and 2 large EEG foundational models. These selected baselines are state-of-the-art methods or have shown strong performance in EEG or time series classification tasks. The 5 supervised learning methods include \textbf{TCN}~\cite{bai2018empirical}, vanilla \textbf{Transformer}~\cite{vaswani2017attention}, \textbf{Conformer}~\cite{song2022eeg}, \textbf{TimesNet}~\cite{wu2023timesnet}, and \textbf{Medformer}~\cite{wang2024medformer}. The 3 self-supervised learning methods are \textbf{TS2Vec}~\cite{yue2022ts2vec}, \textbf{BIOT}~\cite{yang2024biot}, and \textbf{EEG2Rep}~\cite{mohammadi2024eeg2rep}. The 2 large EEG foundational models are \textbf{LaBraM}~\cite{jiang2024large} and \textbf{EEGPT}~\cite{wangeegpt}. 


Our method has three variants based on training setups: \textbf{LEAD-Vanilla(3.21M)}, \textbf{LEAD-Sup(3.21M)}, and \textbf{LEAD-Base(3.41M)}. The LEAD-Vanilla model is trained fully supervised on a single dataset without channel alignment, such as the 7-channel version of the Cognision-ERP dataset. LEAD-Sup and LEAD-Base use datasets with alignment to 19 channels. LEAD-Sup is the model trained unified supervised on 5 AD datasets together without pre-training. For LEAD-Base, we first perform self-supervised pre-training on 11 pre-training datasets. The trained model is then used for unified fine-tuning on 5 downstream AD datasets. Note that for both LEAD-Sup and LEAD-Base, the 5 downstream AD datasets are unified trained and evaluated in one run, which is different from the usual approach where supervised training or fine-tuning occurs on a single dataset. The five supervised learning baselines, including TCN, Transformer, Conformer, TimesNet, and Medformer, use the same setup as LEAD-Vanilla. The three self-supervised learning baselines, including TS2Vec, BIOT, and EEG2Rep, follow LEAD-Base's setup. For the two large EEG foundational models, LaBraM and EEGPT, we load their pre-trained models and use the same unified fine-tuning setup as our LEAD-Base. The training setups, including single-dataset or unified training, whether the training is fully supervised or self-supervised, and whether the channel-aligned dataset version is used are summarized in Table~\ref{tab:training_setups}.

All baseline methods and our method's variants, except for LaBraM and EEGPT, are trained using the same code framework and pipelines. The training epoch for self-supervised pretraining is fixed at 50 epochs, with no early stopping mechanism. Linear probing is applied on all downstream datasets every five epochs to monitor relative performance. For fully supervised learning or fine-tuning, the training epoch is set to 100, with early stopping after 15 epochs of patience based on the best F1 score. The batch sizes for pretraining, fully supervised learning, and fine-tuning are set to 512, 128, and 128, respectively. The optimizer used is AdamW. The initial learning rates for pretraining, fully supervised learning, and fine-tuning are set to 0.0002, 0.0001, and 0.0001, respectively, with the CosineAnnealingLR learning scheduler. Gradient norm clipping is set to 4.0, and Stochastic Weight Averaging (SWA)\cite{izmailov2018averaging} is enabled to improve cross-subject representation learning. The parameters are summarized in Table~\ref{tab:training_params}.

For LaBraM and EEGPT, we use their public code frameworks and load their pre-trained models for fine-tuning, as both are large EEG foundational models. The selection of pre-training datasets is also an integral part of their method. The five fine-tuning AD datasets are all preprocessed to the same shape and sampling frequency to align with their pre-trained models as described in their paper. Further training details for these two methods are provided in the following subsections for each respective method.

We employ four evaluation metrics: sample-level accuracy, sample-level F1 score (macro-averaged), subject-level accuracy, and subject-level F1 score (macro-averaged) after majority voting, as described in ~\ref{sub:important_setups}. In the self-supervised pre-training stage, all subjects in the pre-training datasets are used for training. In the supervised learning or fine-tuning classification stage, the training, validation, and test sets are split based on the subject-independent setup with a ratio of 6:2:2 for each dataset, where each subject appears exclusively in one of these three sets. There is no dataset overlapping between the pretraining and finetuning datasets. The training process is conducted with five random seeds (41-45) on fixed training, validation, and test sets to compute the mean and standard deviation of the models. All experiments are run on an NVIDIA RTX 4090 GPU and a server with 4 RTX A5000 GPUs, using Python 3.8 and PyTorch 2.0.0 + cu118. 



\subsection{TCN}
\label{sub:tcn}
Temporal Convolutional Networks (TCN)~\cite{bai2018empirical} are a specialized type of convolutional network designed for time series tasks such as forecasting and classification. TCNs use causal dilated convolutions to expand the receptive field of the network while preventing information leakage from the past. Based on our experience, TCNs typically offer fast training speeds and relatively good performance in many time series classification tasks~\cite{yue2022ts2vec,wang2024contrast}, including EEG classification. This is a fully supervised method, and we train it on datasets without channel alignment. We set \textit{e\_layers} = 6. The method specified parameters are \textit{hidden\_dims} = 128, \textit{output\_dims} = 320, and \textit{kernel\_size} = 3.



\subsection{Transformer}
\label{sub:transformer}
Transformer~\cite{vaswani2017attention}, commonly known as the vanilla transformer, is introduced in the well-known paper "Attention is All You Need." It can also be applied to time series by embedding each cross-channel timestamp as a token and performing self-attention among these input tokens. This is a fully supervised method, and we train it on datasets without channel alignment. We set \textit{e\_layers} = 6, \textit{n\_heads} = 8, \textit{d\_model} = 128, and \textit{d\_ff} = 256.



\subsection{Conformer}
\label{sub:conformer}

EEG-Conformer~\cite{song2022eeg} is specifically designed for EEG classification by combining convolutional networks and self-attention modules. They first use convolutional modules to learn low-level local features and embeds the raw data into patches. A self-attention module is applied to these patches to capture global features. This is a fully supervised method, and we train it on datasets without channel alignment. We set \textit{e\_layers} = 6, \textit{n\_heads} = 8, \textit{d\_model} = 128, and \textit{d\_ff} = 256.



\subsection{TimesNet}
\label{sub:timesNet}
TimesNet~\cite{wu2023timesnet} is designed for general time series analysis. Instead of using 1D raw time series data, it first transforms the data into a 2D format based on multiple periods. This transformation embeds intra-period and inter-period variations into the columns and rows of the 2D tensors, respectively, to capture more robust features with 2D convolutions. According to a recent survey~\cite{wang2024deep}, TimesNet achieves the best classification performance in many time series benchmarks. This is a fully supervised method, and we train it on datasets without channel alignment. We set \textit{e\_layers} = 2, \textit{d\_model} = 32, and \textit{d\_ff} = 64. The method specified parameter \textit{top\_k} is set to 3.



\subsection{Medformer}
\label{sub:medformer}


Medformer~\cite{wang2024medformer} is designed for biomedical time series classification, including EEG and ECG. They utilize three mechanisms: cross-channel patching, multi-granularity embedding, and intra-inter granularity self-attention. These mechanisms enable it to capture channel correlations and multi-granularity temporal features effectively. This is a fully supervised method, and we train it on datasets without channel alignment. We set \textit{e\_layers} = 6, \textit{d\_model} = 128, and \textit{d\_ff} = 256. The method specified parameters \textit{patch\_len\_list} is set to [2, 4, 8].



\subsection{TS2Vec}
\label{sub:ts2vec}
TS2Vec~\cite{yue2022ts2vec} is a well-known self-supervised contrastive method designed for time series analysis. They effectively capture fine-grained features in time series at the timestamp level. Unlike other contrastive frameworks in computer vision domains (e.g., SimCLR, MOCO), which compute contrastive loss on denser representations after the projection head, TS2Vec computes the contrastive loss directly on the representations after the encoder, at both the timestamp and sample levels. Since the loss computation for each timestamp is required in this method, backbone models that fuse timestamps into patches are not suitable. Therefore, we replace the TCN backbone model used in their paper with a vanilla Transformer to better align with the large foundation model training. We pre-train on 11 datasets and then perform unified fine-tuning on the five downstream AD datasets together. We set \textit{e\_layers} = 20, \textit{n\_heads} = 12, \textit{d\_model} = 128, and \textit{d\_ff} = 256.


\subsection{BIOT}
\label{sub:biot}
BIOT~\cite{yang2024biot} is the first large foundation model for biosignals. They employ single-channel patching techniques to handle biosignals with varying numbers of channels. Each patch is mapped into tokens, with segment embedding, channel embedding, and positional embedding added to incorporate channel and positional information, making the tokens distinguishable from each other. A self-supervised contrastive framework is used to pretrain the model. We first pretrain on 11 datasets and then perform unified fine-tuning on the five downstream AD datasets. We set \textit{e\_layers} = 20, \textit{n\_heads} = 12, \textit{d\_model} = 128, and \textit{d\_ff} = 256.



\subsection{EEG2Rep}
\label{sub:eeg2rep}
EEG2Rep~\cite{mohammadi2024eeg2rep} is a self-supervised learning method that uses context-level masking and reconstruction instead of raw data-level reconstruction. The raw data is embedded into patches, and the masking operations are performed on the patch embeddings. They employ two networks: the context network, which is used as a query, and the target network, which serves as a key for calculating the L2 loss. A cross-attention predictor is used to align the output shapes of the context and target networks. We first pre-train on 11 datasets and then perform unified fine-tuning on the five downstream AD datasets. We set \textit{e\_layers} = 20, \textit{n\_heads} = 12, \textit{d\_model} = 128, and \textit{d\_ff} = 256.



\subsection{LaBraM}
\label{sub:labram}
LaBraM~\cite{jiang2024large} is the first large foundational model for EEG. They design three-step pre-training strategies. They first pre-train a vector-quantified neural code book that encodes single-channel EEG patches into compact neural codes representations. Then, they pre-train a neural transformer by predicting the original neural codes for the masked EEG patches. Last, the encoder part of the pre-trained neural transformer is reused and a new classification head is added for finetuning on new datasets. We use the base version of the model checkpoint with \textit{e\_layers} = 12, \textit{n\_heads} = 10, \textit{d\_model} = 200, and \textit{d\_ff} = 800. We preprocess the five downstream AD datasets into 8-second, 1600 timestamps, and 200Hz samples to match the pre-trained model of their methods.


\subsection{EEGPT}
\label{sub:eegpt}
EEGPT~\cite{wangeegpt} is a state-of-the-art large foundational model for EEG. They design a combination of an alignment loss between encoded tokens and momentum-encoded tokens and a reconstruction loss between reconstructed patches and masked patches. A spatiotemporal embedding is used to encode single-channel patches and by adding channel embedding and patch embedding together. The pre-trained model we used is the large version, which has \textit{e\_layers} = 8, \textit{n\_heads} = 8, \textit{d\_model} = 512, and \textit{d\_ff} = 2048. We preprocess the five downstream AD datasets into 4-second, 1024 timestamps, and 200Hz samples to match the pre-trained model of their methods. 




\section{Ablation Study}
\label{sec:ablation_studies}


\subsection{Ablation Study of Non-AD Datasets}
\label{sub:ablation_study_non_ad_datasets}


\input{tables/ablation_study/non_ad_datasets}

\begin{figure*}[h]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/plots/Non-AD-Ablation.png}
    \caption{\textbf{Ablation Study of Non-AD datasets.} It shows the performance change when adding more non-AD datasets in pre-training.
    }
    \label{fig:non_ad_ablation}
    \vspace{-4mm}
\end{figure*}


\textbf{Setup.} We conduct an ablation study to evaluate the performance changes when adding more Non-AD datasets to self-supervised contrastive pretraining. We begin by pretraining on five datasets: ADSZ, APAVA, ADFSU, AD-Auditory, and TDBRAIN, and fine-tuning on five datasets: ADFTD, CNBPM, Cognision-rsEEG, Cognision-ERP, and BrainLat, following the same setups as the model LEAD-Base. The \textbf{P} denotes pretraining, and \textbf{F} denotes fine-tuning, with the following number indicating the number of datasets used. We then gradually add two more pretraining datasets in the following pairs for the ablation study: (TUEP, REEG-PD), (PEARL-Neuro, Depression), and (REEG-SRM, REEG-BACA). The model uses 11 pre-training datasets is the same as LEAD-Base in Table~\ref{tab:baseline_comparison}. This study aims to investigate whether adding more Non-AD datasets to pre-training improves the model's ability to discriminate AD patterns during downstream fine-tuning.


\textbf{Results.} The results are presented in Table~\ref{tab:ablation_non_ad_datasets}, and a figure showing the performance changes is displayed in Figure~\ref{fig:non_ad_ablation}. We observe that the overall performance at both the subject- and sample-level improves for four out of the five datasets, excluding ADFTD. Performance increases for the last two pretraining datasets, with dramatic improvements seen in BrainLat, Cognision-ERP, and Cognision-rsEEG. Specifically, the subject-level F1 scores improved by 5.72\%, 12.8\%, and 6.63\%, respectively. This improvement is intuitive, as the last two added datasets, REEG-SRM and REEG-BACA, have the largest number of samples, totaling more than 600K samples. The performance improvement in Cognision-ERP and Cognision-rsEEG also demonstrates that self-supervised pretraining benefits datasets, especially those with fewer channels (7 in the raw datasets), helping them distinguish general EEG or other brain disease features from AD-specific features.

For datasets that initially demonstrated good performance, such as CNBPM, adding more Non-AD datasets to pretraining results in gradual improvements, with a total improvement of 6.16\% in the F1 score from 5 to 11 pretraining datasets. As for the performance drop in ADFTD, it is consistent with the finding in Table~\ref{tab:baseline_comparison} that the performance of LEAD-Sup using supervised learning is much better than that of LEAD-Base using self-supervised learning. The reasons for this drop are unclear and require further investigation in future research.




\subsection{Ablation Study of AD Datasets}
\label{sub:ablation_study_ad_datasets}

\input{tables/ablation_study/ad_datasets}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/plots/AD-Ablation.png}
    \caption{\textbf{Ablation Study of AD datasets.} It shows the results change of ADFTD when adding more AD datasets in supervised learning.
    }
    \label{fig:ad_ablation}
    \vspace{-4mm}
\end{figure}

\textbf{Setup.} We conduct an ablation study to evaluate the performance changes when adding more AD datasets into unified supervised learning. All AD datasets are added to the training set one by one in the following order: ADFTD, BrainLat, CNBPM, Cognision-ERP, and Cognision-rsEEG, with validation and testing conducted on a single dataset: ADFTD. The models are named from S-1-V-ADFTD-Sup to S-5-V-ADFTD-Sup, where \textbf{S} denotes supervised learning and \textbf{V} denotes validation, with the following number indicating the number of datasets used. Note that the performance of S-5-V-ADFTD-Sup on ADFTD may differ slightly from LEAD-Sup in Table~\ref{tab:baseline_comparison}, as S-5-V-ADFTD-Sup validates only on ADFTD to select the best-performing model, rather than selecting the model with the best weighted F1 score across all five datasets. This study investigates whether high-quality AD datasets can benefit each other in unified supervised learning compared to single-dataset supervised learning, even without self-supervised pretraining.


\textbf{Results.}
The results are presented in Table~\ref{tab:ablation_ad_datasets}, and a figure showing the performance changes is displayed in Figure~\ref{fig:ad_ablation}. We observe a gradual improvement in performance at the sample-level classification, with the F1 score increasing from 73.75 to 82.39. For subject-level results, the overall trend is upward. However, we believe the fluctuations are due to the limited number of subjects compared to the total number of samples. Additionally, the imbalanced number of samples per subject in the ADFTD dataset may contribute to the larger variability observed.



\subsection{Contrastive Learning Modules}
\label{sub:contrastive_learning_modules}

\input{tables/ablation_study/contrastive_modules}


\textbf{Setup.}
We conduct an ablation study to investigate the functionality of each contrastive module. Specifically, we use either sample-level contrast or subject-level contrast for self-supervised pre-training, while keeping the other setups the same as the LEAD-Base model. Recall that sample-level contrast involves instance discrimination, where different views of the same sample are treated as positive pairs, and the rest are treated as negative pairs, similar to setups used in other domains like computer vision. In contrast, subject-level contrast is specifically designed for biomedical time series data, such as EEG, which contains subject information. We consider samples segmented from the same subject as positive pairs and samples from different subjects as negative pairs, thus performing a subject-discrimination task. We believe subject-level contrast helps to reduce subject-specific noise, leading to more robust general feature learning. This approach makes the embedding space more uniform across subjects and improves inter-subject classification in downstream tasks.


\textbf{Results.} 
The results are presented in Table~\ref{tab:contrastive_modules}. The \textbf{All} is the same as the LEAD-Base model. We observe that subject-level contrast significantly outperforms sample-level contrast for four out of five datasets (except for ADFTD), with around a 5\% improvement in F1 score for subject-level classification. Subject-level contrast performs poorly on ADFTD, likely due to unknown factors, and this could be the reason that adding self-supervised pre-training does not work well for ADFTD. We plan to investigate this in future work and explore alternative self-supervised training methods for the ADFTD dataset, such as mask and reconstruction. Overall, in most cases, subject-level contrast performs better than sample-level contrast, and using both contrastive modules together achieves comparable or better results than subject-level contrast alone.





\section{Supplementary Experiments}
\label{sec:supplementary_experiments}



\subsection{Finetune or Validate on One Dataset}
\label{sub:finetune_validate_one_dataset}

\input{tables/supplementary_experiment/single_dataset_finetune_validate}

\textbf{Setup.} 
We conduct experiments that fine-tune on a single dataset and fine-tune on multiple datasets while validating on a single dataset. In the LEAD-Base model, we perform unified fine-tuning, which involves fine-tuning five downstream AD datasets simultaneously and using the mean F1 score across all five datasets for early stopping based on the best performance. We aim to compare the functionality of unified fine-tuning against single-dataset fine-tuning, and investigate whether validating on a specific dataset could improve performance. Two new models are named with P-11-F-1-Base and P-11-F-5-V-1-Base. Other setups are the same as the LEAD-Base model.



\textbf{Results.}
The results are presented in Table~\ref{tab:single_dataset_finetune_validate}. For ADFTD, BrainLat, and CNBPM, we observe a performance improvement in sample-level classification when using single-dataset fine-tuning. However, fine-tuning across the five AD datasets together still yields better performance in subject-level classification on these three datasets. This supports the idea that unified fine-tuning leads to more balanced results among subjects, reducing the likelihood of overfitting to specific subjects. For the two Cognision datasets, performance drops significantly when fine-tuning on a single dataset. Since the raw channels for these two datasets are fewer compared to other datasets and inherently contain less information, we can infer that datasets with fewer channels rely more heavily on unified fine-tuning.

When validating on a single dataset, we observe no improvement, except for a subtle improvement in Cognision-ERP. We believe this is due to using the average metrics across datasets for early stopping, which helps alleviate the risk of overfitting to the validation set on specific datasets, leading to more robust performance on the test set.




\subsection{Public Datasets Training}
\label{sub:public_datasets_training}

\input{tables/supplementary_experiment/public_datasets_training}


\textbf{Setup.} 
We conduct an experiment where we train only on public datasets. Since all the pretraining datasets are public, we modify only the fine-tuning or unified fully supervised learning setups. Specifically, we use ADFTD and BrainLat for unified supervised learning by training the two datasets together, referred to as S-2-Sup. All other setups remain the same as the LEAD-Sup model. We also perform unified fine-tuning by fine-tuning the two datasets together, referred to as P-11-F-2-Base, with all other setups remaining the same as the LEAD-Base model.



\textbf{Results.}
The results are presented in Table~\ref{tab:public_datasets_training}. We observe that unified supervised learning or fine-tuning still benefits performance compared to the LEAD-vanilla model, which is trained on a single dataset with raw channel numbers. The improvement is not as pronounced for ADFTD, but it is significant for BrainLat, with approximately 15\% and 13\% F1 score improvements in sample-level and subject-level classification, respectively.




\subsection{Public Datasets Fine-tuning}
\label{sub:public_datasets_finetuning}

\input{tables/supplementary_experiment/public_datasets_finetuning}


\textbf{Setup.}  
We conduct an experiment where we leave the 2 public AD datasets (ADFTD and BrainLat) for fine-tuning and pre-train the remaining 14 datasets, including the 3 private AD datasets that were originally used for fine-tuning in LEAD-Base. This new model is named P-14-F-2-Base, with all other configurations kept the same as the LEAD-Base.

\textbf{Results.}  
The results are presented in Table~\ref{tab:public_datasets_finetuning}. For ADFTD, the result is better than LEAD-Base but worse than LEAD-Sup, which is consistent with previous findings that our self-supervised pre-training does not benefit ADFTD. For BrainLat, the results are surprisingly better than LEAD-Base, achieving a 91.40\% F1 score on subject-level classification.



\subsection{All Datasets Pre-training}
\label{sub:all_dataset_pretraining}

To benefit future research on EEG-based AD detection or other neurological disease detection, we also pre-train a model named \textbf{LEAD-All}, which performs self-supervised pre-training on all the subjects across 16 datasets. This includes subjects in the training, validation, or test sets, as well as subjects beyond AD and healthy classes that were originally excluded during data preprocessing in LEAD-Base (such as Frontotemporal Dementia subjects in ADFTD).

Since the LEAD-All model is trained using all the available data, there is no separate downstream dataset to evaluate its performance. Consequently, its performance is not guaranteed to be superior to the LEAD-Sup and LEAD-Base models. Theoretically, the LEAD-All model should have learned more general EEG features. Note that the new dataset used for the downstream task fine-tuning on LEAD-All should have no overlap with the 16 datasets used for pre-training the LEAD-All model to ensure no data leakage.



\subsection{Frequency Bands Analysis}
\label{sub:frequency_bands_analysis}

\input{tables/supplementary_experiment/frequency_bands_analysis}


\textbf{Setup.} 
We conduct experiments that fine-tune on AD datasets across different frequency bands. Recall that in the data preprocessing stage, we filter the frequency band from 0.5–45Hz. In this setup, the pretraining remains unchanged, but we filter the downstream AD datasets into different frequency bands. Other fine-tuning setups are the same as in the LEAD-Base model. Specifically, the bands are Delta ($\delta$) (0.5–4Hz), Theta ($\theta$) (4–7Hz), Alpha ($\alpha$) (8–12Hz), Beta ($\beta$) (12–30Hz), and Gamma ($\gamma$) (30–45Hz). We perform unified fine-tuning using the data filtered into these different bands.



\textbf{Results.}
The results for different frequency bands are presented in Table~\ref{tab:frequency_bands_analysis}. The \textbf{All} model is the same as the LEAD-Base model. We observe that the Theta ($\theta$) (4–7Hz), Alpha ($\alpha$) (8–12Hz), and Beta ($\beta$) (12–30Hz) bands are usually the most critical for classification performance. These three bands consistently yield the highest sample-level F1 scores, as well as strong performance in subject-level classification. However, there are two exceptions. First, for subject-level classification in BrainLat, the highest performance is achieved in the Gamma ($\gamma$) (30–45Hz) band. We speculate that this may be due to the imbalanced number of subjects per sample, which could cause overfitting to specific subjects. The second exception is the Cognision-ERP dataset, where the highest performance comes from the Delta ($\delta$) (0.5–4Hz) band. This result is plausible since Cognision-ERP is an event-related potentials dataset rather than resting-state EEG. Previous studies have shown that biomarkers in continuous attention tasks may manifest in this band~\cite{kirmizi2006comparative}.




\subsection{Channels Importance Analysis}
\label{sub:channels_importance_analysis}

\input{tables/supplementary_experiment/channels_importance_analysis}


\textbf{Setup.} 
We conduct experiments to assess the importance of different brain regions. Specifically, we keep the training stage the same, but mask the channels in specific regions during testing. The trained LEAD-Base model is used for this experiment. We perform this research on the CNBPM dataset, as it achieves the highest results and can mostly alleviate the interference of other factors, such as data quality. We define the masked region that causes the highest performance drop as the most critical region. The regions include: Frontopolar (Fp1, Fp2), Frontal (F7, F3, Fz, F4, F8), Temporal (T3, T4, T5, T6), Parietal (P3, Pz, P4), Occipital (O1, O2), and Central (C3, Cz, C4).



\textbf{Results.}
The results are presented in Table~\ref{tab:channel_importance_analysis}. The \textbf{All Regions} is the same as the LEAD-Base model. We observe that the Frontal region is the most important, causing the most significant performance drop when masked, with a 24.26\% and 25.32\% F1 score reduction in sample-level and subject-level classification, respectively. The second most important region is the Occipital region, which causes a 9.56\% and 8.56\% F1 score drop in sample-level and subject-level classification. However, the performance drop here is not as pronounced as that seen with the Frontal region.




\section{Discussion}
\label{sec:discussion}

\subsection{Comparison With Existing Large EEG Model}
\label{sub:comparison_with_existing_large_EEG_model}



To address the challenges posed by heterogeneous channels in EEG large foundational model pre-training, existing methods typically use single-channel patches as embedding tokens for transformers~\cite{yang2024biot, jiang2024large, wangeegpt}. However, this approach introduces two trade-offs concerning flexibility and computational resources.

First, there is a trade-off between model flexibility and unified dataset training. While single-channel patching allows the model to train on EEG data with varying numbers of channels, the token embedding methods are fixed, limiting the model to adopt backbone architectures different from BIOT~\cite{yang2024biot}. In such architectures, spatial features among channels can only be learned by attaching channel embeddings to each patch embedding. This approach restricts the model's ability to capture more prosperous spatial relationships, thus limiting the ability to extract spatial features.

Second, there is a trade-off between patch length and computational resources. With single-channel patch embedding, the channel number becomes a factor in determining the final number of input tokens. For instance, if each sample has 1024 timestamps and 19 channels, and the patch size is 32, the total number of patches would be 19 × (1024 / 32) = 608. As the patch size decreases, the number of patches increases, making the model computationally expensive to train. For example, if we choose a smaller patch length like 4, the number of patches increases significantly, which requires additional computational power to map all these patches into the d\_model dimension tokens. This computational cost leads many existing methods to opt for larger patch sizes, such as 64 or 200~\cite{jiang2024large, wangeegpt}, which limits the model’s ability to learn fine-grained temporal features.

To avoid these trade-offs, we perform channel alignment during data preprocessing. This strategy offers more flexibility for choosing the backbone model, reduces the computational burden, and enables unified fine-tuning on all downstream datasets in one run, which enhances model performance without compromising efficiency.


\subsection{Effectiveness of Subject-Level Contrast}
\label{sub:subject_contrast_effectiveness}


We speculate that there are three potential reasons why subject-level contrastive pre-training significantly benefits downstream AD detection: \textbf{1)} Purification of Noise within the Subject: By treating all samples from a single subject as positive pairs, subject-level contrastive learning forces the model to make the sample representations within each subject more similar. This helps the model focus on the subject's inherent characteristics and purifies the noise caused by irrelevant subject-specific features such as artifacts.  \textbf{2)} Learning General Features Associated with Subjects: Subject-level contrasting aims to differentiate subjects by pushing different subjects apart uniformly in the embedding space. This subject-discrimination task encourages the model to learn general features related to the subject, such as brain structure, age, gender, EEG devices, and brain health. These features are essential for downstream tasks where the goal is to classify AD based on subject-specific patterns.  \textbf{3)} Compare with SimCLR: In computer vision, SimCLR is designed to perform sample-discrimination tasks for image classification during pre-training. In contrast, our ultimate goal is to classify subjects for AD detection. Treating each subject's EEG data as one "sample," subject-level contrastive learning becomes analogous to SimCLR, making it reasonable to perform subject-level contrasting during pre-training for better subject-level classification during downstream tasks.


\subsection{Limitations and Future Works}
\label{sub:limitations_and_future_works}

In this paper, to enable pre-training on various EEG datasets and perform unified fine-tuning, we align all the EEG data to 19 standard channels in data preprocessing. For datasets with more than 19 channels, we simply drop the extra channels, which may result in some potential information loss. However, we have demonstrated that this trade-off is manageable, as channel alignment still significantly benefits the overall training pipeline. Moving forward, we plan to explore methods to better utilize the information from additional channels, aiming to minimize any loss and enhance model performance. Additionally, while this study focuses on the potential of contrastive-based pre-training for AD detection, we also recognize the potential of other techniques, such as combining contrastive learning with mask-reconstruction modules or adopting a decoder-only architecture. These avenues will be explored in future works to enhance our model's performance.