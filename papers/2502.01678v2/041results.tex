


\subsection{Comparison with Baselines}
\label{sub:baseline_comparsion}

\textbf{Setup.} Our method has three variants based on training setups: \textbf{LEAD-Vanilla(3.21M)}, \textbf{LEAD-Sup(3.21M)}, and \textbf{LEAD-Base(3.41M)}. The LEAD-Vanilla model is trained fully supervised on a single dataset without channel alignment, such as the 7-channel version of the Cognision-ERP dataset. LEAD-Sup and LEAD-Base use datasets with alignment to 19 channels. LEAD-Sup is the model trained unified supervised on 5 AD datasets together without pre-training. For LEAD-Base, we first perform self-supervised pre-training on 11 pre-training datasets. The trained model is then used for unified fine-tuning on 5 downstream AD datasets. Note that for both LEAD-Sup and LEAD-Base, the 5 downstream AD datasets are unified trained and evaluated in one run, which is different from the usual approach where supervised training or fine-tuning occurs on a single dataset. The five supervised learning baselines, including TCN, Transformer, Conformer, TimesNet, and Medformer, use the same setup as LEAD-Vanilla. The three self-supervised learning baselines, including TS2Vec, BIOT, and EEG2Rep, follow LEAD-Base's setup. For the two large EEG foundational models, LaBraM and EEGPT, we load their pre-trained models and use the same fine-tuning setup as our LEAD-Base. Appendix~\ref{sec:implementation_details} provides more details about the implementation setups.


\textbf{Results.}
The results are presented in Table~\ref{tab:baseline_comparison}. Our method significantly improves accuracy and F1 score compared with all baselines for both sample-level and subject-level classification. Specifically, our method outperforms the best baseline methods by 6.9\%, 5.72\%, 3.85\%, 7.81\%, and 11.16\% in F1 score at the subject-level on the ADFTD, BrainLat, CNBPM, Cognision-ERP, and Cognision-rsEEG datasets, respectively. The comparison between our method and the supervised learning baselines highlights the effectiveness of channel alignment; although some information might be lost during alignment, the ability to allow unified training still demonstrates substantial performance improvements compared to supervised learning methods on raw-channel datasets. The comparison with self-supervised learning baselines underscores the effectiveness of our contrastive learning approach. The sample-level and subject-level contrasting show a strong learning ability for inter-subject classification. The two large EEG models perform poorly on the ADFTD and BrainLat datasets, achieving almost random results. The comparison between ours and their methods emphasizes the importance of selecting proper pre-training datasets. Our selection of healthy and neurological disease datasets for pre-training contributes significantly to the downstream classification between AD and healthy controls.

Among the three variants of our methods, the LEAD-Base achieves the best performance in most cases, except for the ADFTD dataset, where LEAD-Sup performs better. The comparison between LEAD-Vanilla and LEAD-Sup shows that leveraging more AD datasets for training benefits performance, even in a fully supervised learning manner. The comparison between LEAD-Sup and LEAD-Base indicates that proper self-supervised pre-training methods dramatically reduce the interference of subject features and improve inter-subject classification ability. Besides, We observe that subject-level classification results are typically better than sample-level classification results for almost all methods. This demonstrates that majority voting does alleviate noise interference from outlier samples within a subject. The improvement is particularly notable in the two Cognision datasets. The best performance for these two datasets on the sample-level is around 70\% F1 score, but increases to approximately 90\% with majority voting. Since these datasets were collected in an industrial pipeline with a balanced number of samples per subject (300 or 400), we can infer that the more balanced the number of samples per subject, the greater the improvement introduced by majority voting.






\subsection{Ablation Study and Supplementary Experiments}
\label{sub:ablation_study_supplementary_experiments}
We conduct comprehensive ablation studies, including the effectiveness of non-AD and AD datasets, and contrastive learning modules research, see Appendix~\ref{sec:ablation_studies}. Besides, we conduct additional experiments for more training setups and brain interpretability analysis, including frequency bands analysis and channels analysis. See Appendix~\ref{sec:supplementary_experiments}.






