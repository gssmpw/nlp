




\subsection{Problem Formulation}
\label{sub:problem_formulation}


\textbf{Sample-Level Classification.}  
Consider an input EEG sample $\bm{x} \in \mathbb{R}^{T \times C}$\, where $T$ denotes the number of timestamps and $C$ represents the number of channels. Our objective is to learn an encoder that generates a representation $\bm{h}$\, which can be used to predict the corresponding label $\bm{y} \in \mathbb{R}$\ for the input sample. Specifically, the label $\bm{y}$\ corresponds to either Alzheimer's Disease or Healthy controls.

\textbf{Subject-Level Classification.}
In addition to the corresponding label $\bm{y} \in \mathbb{R}$\, each input EEG sample also has a subject ID $\bm{s} \in \mathbb{R}$\ that indicates which subject the sample belongs to. The ultimate goal of EEG-based AD detection is to determine whether a subject has Alzheimer's Disease. For subject-level classification, we use a majority voting scheme, where the subject is assigned the label corresponding to the majority label of all samples from that subject.



\subsection{Datasets Selection}
\label{sub:datasets_selection}

\textbf{AD Datasets.} We review EEG-based AD detection papers published between 2018 and 2024 to identify potentially available datasets. We find 6 publicly available datasets containing AD subjects: \textbf{AD-Auditory}\cite{lahijanian2024auditory}, \textbf{ADFSU}\cite{vicchietti2023computational}, \textbf{ADFTD}\cite{miltiadous2023dataset,miltiadous2023dice}, \textbf{ADSZ}\cite{alves2022eeg,pineda2020quantile}, \textbf{APAVA}\cite{escudero2006analysis,smith2017accounting}, and \textbf{BrainLat}\cite{prado2023brainlat}. Additionally, we use 3 private datasets: \textbf{Cognision-ERP}\cite{cecchi2015clinical}, \textbf{Cognision-rsEEG}, and \textbf{CNBPM}\cite{ieracitano2019time,amezquita2019novel}, bringing the total number of AD datasets to 9, and the total number of subjects to 813.  We perform preliminary experiments on each dataset individually to assess their quality. For smaller datasets or those showing large performance variability across subjects, we use them for pre-training to alleviate potential data quality issues such as mislabeled subjects, interference from artifacts, collection devices, and collection methods. Five high-quality AD datasets, \textbf{ADFTD}, \textbf{CNBPM}, \textbf{Cognision-rsEEG}, \textbf{Cognision-ERP}, and \textbf{BrainLat}, are used for downstream tasks to evaluate the model performance.

\textbf{Non-AD Datasets.} To enhance the learning of general EEG and AD-specific features, we use datasets of healthy subjects and other neurological diseases for self-supervised pretraining. We aim to increase the diversity of brain conditions, including healthy and diseased states, and increase the number of subjects used for training to reduce the interference of subject-specific patterns. Note that all the non-AD datasets have one commonality: the label is assigned to the subject, which adapts to the subject-level feature extraction. Datasets such as sleep stage detection and mental state classification are unsuitable here. We select publicly available datasets from sources like OpenNEURO\footnote{\url{https://openneuro.org/}}, Temple University Hospital\footnote{\url{https://isip.piconepress.com/projects/tuh_eeg/}}, and Brainclinics\footnote{\url{https://www.brainclinics.com/resources}}. We choose datasets collected in a resting-state condition or involving resting-state tasks with either eyes open or closed to ensure consistency with most downstream AD datasets. In total, we select 7 proper large datasets, each with hundreds or even thousands of subjects. They are \textbf{Depression}~\cite{cavanagh2019multiple,cavanagh2021eeg}, \textbf{PEARL-Neuro}~\cite{dzianok2024pearl}, \textbf{REEG-BACA}~\cite{getzmann2024resting}, \textbf{REEG-PD}~\cite{singh2023evoked}, \textbf{REEG-SRM}~\cite{hatlestad2022bids}, \textbf{TDBrain}~\cite{van2022two}, and \textbf{TUEP}~\cite{veloso2017big}.




\subsection{Data Preprocessing}
\label{sub:data_preprocessing}

Two key challenges in training a large foundation model for time-series-like data are varying channel/variate numbers and heterogeneous sampling frequencies~\cite{liu2024timer,woo2024unified,yang2024biot}. However, we can easily align channels based on their names in EEG and align sampling frequency by resampling. More details and reasons for preprocessing steps are provided in Appendix~\ref{sec:datasets_preprocessing}.

\textbf{Artifacts Removal.} Some datasets have already undergone preprocessing steps during data collection, such as artifact removal and filtering. We perform a secondary preprocessing to align all datasets uniformly for training. All the fine-tuning datasets are guaranteed to be artifacts-free.

\textbf{Channel Alignment.} We align all datasets to a standard set of 19 channels, which include Fp1, Fp2, F7, F3, Fz, F4, F8, T3/T7, C3, Cz, C4, T4/T8, T5/P7, P3, Pz, P4, T6/P8, O1, and O2, based on the international 10-20 system\footnote{\url{https://en.wikipedia.org/wiki/10-20_system_(EEG)}}. For datasets with fewer than 19 channels, we interpolate the missing channels using the MNE EEG processing package\footnote{\url{https://mne.tools/stable/index.html}}. For datasets with more than 19 channels, we select the 19 channels based on the channel name and discard the others. In cases where datasets use different channel montages, such as the Biosemi headcaps with 32, 64, 128 channels\footnote{\url{https://www.biosemi.com/headcap.htm}}, we select the 19 closest channels by calculating the Euclidean distance between their 3D coordinates. The channel alignment allows us to pre-train the models on different datasets with any backbone encoder and perform unified fine-tuning on all AD datasets in one run.
 
\textbf{Frequency Alignment.} In addition to channel alignment, we resample all datasets to a uniform sampling frequency of 128Hz, which is commonly used and preserves the key frequency bands ($\delta$, $\theta$, $\alpha$, $\beta$, $\gamma$), while also reducing noise.

\textbf{Sample Segmentation.} For deep learning training, we segment the EEG trials within each subject into 1-second samples, which results in 128 timestamps per sample, as the sampling frequency is aligned to 128Hz. 

\textbf{Frequency Filtering.} We then apply frequency filtering to each sample, ranging from 0.5Hz to 45Hz, to remove frequency bands that do not correspond to brain activities.

\textbf{Standard Normalization.} After frequency filtering, we perform standard normalization on each sample, applied individually to each channel, to ensure that the data is centered and scaled consistently across all samples and channels.




\begin{figure*}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/methods/lead_details.pdf}
    \caption{\textbf{Details of LEAD method.} \textbf{a)} The pipeline for our method includes data preprocessing, sample indices shuffling, self-supervised pre-training, multi-dataset fine-tuning, sample-level classification, and subject-level AD detection. \textbf{b)} The flowchart of the self-supervised pre-training. A batch of samples is applied with two augmentations $a$ and $b$ to generate two augmented views, $\bm{x}_i^a$ and $\bm{x}_i^b$. The number in each sample is the subject ID. The representation $\bm{z}_i^a$ and $\bm{z}_i^b$ after encoder $f(\cdot)$ and projection head $g(\cdot)$ are used for contrastive learning. Two augmented views from the same sample are positive pairs for sample-level contrast. For subject-level contrast, samples with the same subject IDs are positive pairs. \textbf{c)} The backbone encoder $f(\cdot)$ includes two branches. The temporal branch takes cross-channel patches to embed as tokens. The spatial branch takes the whole series of channels to embed as tokens. The two branches are computed in parallel.
    }
    \label{fig:lead_details}
    \vspace{-5mm}
\end{figure*}



\subsection{Self-Supervised Pretraining}
\label{sub:self_supervised_pretraining}

The region b) in figure~\ref{fig:lead_details} shows the flowchart of self-supervised contrastive pre-training.



\textbf{Representation Learning.} For an input EEG sample $\bm{x}_i$, where $i$ denotes the index of the sample $\bm{x}_i$, we apply data augmentation methods $a$ and $b$ to generate two augmented views, $\bm{x}_i^a$ and $\bm{x}_i^b$. Given a backbone encoder $f(\cdot)$ and a projection head $g(\cdot)$, we compute their representations $\bm{h}_i^a = f(\bm{x}_i^a)$ and $\bm{h}_i^b = f(\bm{x}_i^b)$ after the encoder $f(\cdot)$, and further obtain denser representations $\bm{z}_i^a = g(\bm{h}_i^a)$ and $\bm{z}_i^b = g(\bm{h}_i^b)$ through the projection head $g(\cdot)$. The projection head is designed to benefit contrastive learning, as described in~\cite{chen2020simple}, and it will be discarded during downstream tasks, with only the encoder being used for the downstream task.




\textbf{Sample-Level Contrasting.} Sample-level contrasting is the most widely used framework in contrastive learning, as seen in SimCLR~\cite{chen2020simple} and MOCO~\cite{he2020momentum}. The goal is to perform sample/instance discrimination and learn a representation that can distinguish one sample from others~\cite{wu2018unsupervised}. A pre-trained model using this approach can capture general patterns in EEG data, benefiting downstream tasks by improving performance and reducing the need for labeled data. In this work, we adopt the SimCLR architecture, which treats different augmented views of the same sample as positive pairs and views from different samples as negative pairs. For an input sample $\bm{x}_i \in \mathcal{B}$ in a batch, our sample-level InfoNCE contrastive loss is defined as follows:

\begin{equation}
\label{eq:sample_loss}
\mathcal{L}_{Sam} = 
\mathbb{E}_{\bm{x}_i}
    \left[
    -\textrm{log}
        \frac
            {\textrm{exp}( \textrm{sim}( \bm{z}_i^a, \bm{z}_i^b ) / \tau)}
            {
            \sum_{j}
                \left(
                \textrm{exp}( \textrm{sim}( \bm{z}_i^a, \bm{z}_j^b ) / \tau)
                \right)
            }
    \right]
\end{equation}

where $j$ denotes the index of other samples in the batch $\mathcal{B}$, and $\textrm{sim} (\bm{u}, \bm{v}) = \frac{\bm{u}^T \bm{v}}{\|\bm{u}\| \|\bm{v}\|}$ denotes the cosine similarity between vectors $\bm{u}$ and $\bm{v}$. The parameter $\tau$ is a temperature parameter that adjusts the similarity scale. 




\textbf{Subject-Level Contrasting.} In EEG-based Alzheimerâ€™s disease (AD) detection, each subject is typically associated with a stable medical state. Specifically, once a subject has AD or preclinical signs of AD, all EEG samples from that subject should exhibit features related to AD, meaning they share the same label during deep learning training. This prior knowledge allows us to perform subject-level contrasting, a concept first defined in~\cite{wang2024contrast} and successfully applied in EEG and ECG-based disease detection~\cite{kiyasseh2021clocs, wang2024contrast, abbaspourazad2023large}. In subject-level contrasting, we treat samples from the same subject as positive pairs and samples from different subjects as negative pairs. With an increasing number of subjects used in pre-training, we aim for the model to learn diverse feature types and reduce interference from unrelated subject-specific features during downstream classification. Appendix~\ref{sub:contrastive_learning_modules} and~\ref{sub:subject_contrast_effectiveness} provide more details on the effectiveness and analysis of subject-level contrasting. For an input sample $\bm{x}_i \in \mathcal{B}$ in a batch, our subject-level InfoNCE contrastive loss is defined as follows:

\begin{equation}
\label{eq:subject_loss}
\mathcal{L}_{Sub} = 
\mathbb{E}_{\bm{x}_i}
    \left[
    \mathbb{E}_{\bm{x}_k}
    \left[
    -\textrm{log}
        \frac
            {\textrm{exp}( \textrm{sim}( \bm{z}_i^a, \bm{z}_k^b ) / \tau)}
            {
            \sum_{j}
                \left(
                \textrm{exp}( \textrm{sim}( \bm{z}_i^a, \bm{z}_j^b ) / \tau)
                \right)
            }
    \right]
    \right]
\end{equation}

where $\bm{x}_k$ denotes samples from the same subject as $\bm{x}_i$ in the batch, with the same subject ID $\bm{s}_k = \bm{s}_i$. The function $\textrm{sim} (\bm{u}, \bm{v}) = \frac{\bm{u}^T \bm{v}}{\|\bm{u}\| \|\bm{v}\|}$ represents the cosine similarity, and $\tau$ is a temperature parameter that adjusts the scale. Note that not all neurological diseases can utilize subject-level contrasting. For instance, seizures are a condition where the EEG patterns during a seizure phase differ significantly from those in the regular phase for the same subject.



\textbf{Overall Loss Function.}
The overall loss function is the weighted sum of the sample-level and subject-level contrastive losses is defined as follows: 

\begin{equation}
\label{eq:joint_loss}
\mathcal{L} = 
    {\lambda_1} \mathcal{L}_{Sam} + 
    {\lambda_2} \mathcal{L}_{Sub}  
\end{equation}
where ${\lambda_1} + {\lambda_2} = 1$ are hyper-coefficients that control the relative importance and adjust the scales of each level's loss.


\textbf{Indices Shuffling.} In real-world scenarios, the likelihood of samples with the same subject ID appearing in the same training batch decreases as the number of subjects increases. This can hinder subject-level contrastive learning. To address this issue, we develop an indices shuffling algorithm that shuffles the order of samples in each epoch. The goal is to ensure that samples with the same subject ID are present in the batch while introducing randomness in the sample order every epoch. More algorithm description and Pseudo code details are presented in Appendix~\ref{sec:indices_shuffling_algorithm}.






\subsection{Backbone Encoder Architecture}
\label{sub:backbone_encoder}
We use a simplified version of ADformer~\cite{wang2024adformer} as the backbone encoder $ f(\cdot) $, adopting single-granularity learning only. This architecture is designed for EEG-based AD detection and efficiently captures temporal features along the time dimension and spatial features among channels, as both are critical for EEG feature representation learning. For simplicity, we omit the subscript $ i $ for the input sample $ \bm{x} $ in this subsection, as it is not necessary for the illustration. The temporal and spatial branches are computed in parallel before the projection head or classifier. Both branches use the standard encoder-only transformer, including self-attention, layer normalization, and feed-forward networks. The region c) in figure~\ref{fig:lead_details} illustrates the architecture of the backbone encoder.

\textbf{Temporal Branch.}
Given an input EEG sample $\bm{x} \in \mathbb{R}^{T \times C}$ and patch length $L$, where $T$ and $C$ denote the number of timestamps and channels, respectively. We first segment the input sample into $N$ cross-channel non-overlapping patches to obtain $\bm{x}^{\text{t}} \in \mathbb{R}^{N \times (L \cdot C)}$. Zero padding is applied to ensure that the number of timestamps $T$ is divisible by $L$, resulting in $N = \left\lceil \frac{T}{L} \right\rceil$. The patches $\bm{x}^{\text{t}}$ are then mapped into $D$-dimensional patch embeddings using a linear projection $\bm{W}$, and a fixed positional embedding $\bm{W}_{\text{pos}}$~\cite{vaswani2017attention} is added to produce the final patch embeddings: $\bm{e}^{\text{t}} = \bm{x}^{\text{t}} \bm{W} + \bm{W}_{\text{pos}}$, where $\bm{e}^{\text{t}} \in \mathbb{R}^{N \times D}$, $\bm{W} \in \mathbb{R}^{(L \cdot C) \times D}$, and $\bm{W}_{\text{pos}} \in \mathbb{R}^{N \times D}$. The final patch embeddings $\bm{e}^{\text{t}}$ are used as input tokens for the standard encoder-only transformer. After $M$ encoding layers, we obtain the temporal branch's final representations $\bm{h}^{\text{t}}$.



\textbf{Spatial Branch.}
Given an input EEG sample $\bm{x} \in \mathbb{R}^{T \times C}$, we first transpose the sample and add a fixed channel-wise positional embedding $\bm{W}_{\text{pos}}$ to obtain $\bm{x}^{\text{c}} = \operatorname{Transpose}(\bm{x}) + \bm{W}_{\text{pos}}$, where $\bm{x}^{\text{c}}, \bm{W}_{\text{pos}} \in \mathbb{R}^{C \times T}$. Unlike the temporal branch, where positional embeddings are added after embedding, we add channel-wise positional embeddings on the raw input EEG data since the subsequent up-dimension process destroys the information of raw channel order. For a target channel number $F$ and embedding dimension $D$, we first perform an up-dimensional transformation using a 1-D convolution $\bm{W}_1$ to increase the channel number. Then, we map the entire series of each channel into a latent embedding using a linear projection $\bm{W}_2$ to get the final channel embeddings: $\bm{e}^{\text{c}} = (\bm{W}_1 \bm{x}^{\text{c}}) \bm{W}_2$, where $\bm{e}^{\text{c}} \in \mathbb{R}^{F \times D}$, $\bm{W}_1 \in \mathbb{R}^{F \times C}$, and $\bm{W}_2 \in \mathbb{R}^{T \times D}$. The final channel embeddings $\bm{e}^{\text{c}}$ are used as input tokens for the standard encoder-only transformer. After $M$ encoding layers, we obtain the spatial branch's final representation $\bm{h}^{\text{c}}$. 




\textbf{Projection Head and Classifier.}
For an input EEG sample $\bm{x}$, we obtain the temporal branch's representation $\bm{h}^t$ and the spatial branch's representation $\bm{h}^c$. We concatenate the \textbf{last token} from both representations to form the final representation $\bm{h} = \left[\bm{h}^t[-1] \,||\, \bm{h}^c[-1]\right]$ of the backbone encoder $f(\cdot)$, where $\left[\cdot \,||\, \cdot\right]$ denotes concatenation and $\bm{h} \in \mathbb{R}^{2D}$. For contrastive pre-training, $\bm{h}$ is further projected into a denser representation $\bm{z} \in \mathbb{R}^{D}$ using a projection head $g(\cdot)$, where $g(\cdot)$ consists of a two-layer fully connected network. For downstream classification tasks, $\bm{h}$ is used directly to classify the output label $\bm{y}$ via a linear classifier $c(\cdot)$.

\input{tables/processed_datasets}


\subsection{Important Setups}
\label{sub:important_setups}

\textbf{Subject-Independent.}  
Two main setups are commonly used for evaluation in the EEG-based AD detection domain: subject-dependent~\cite{nour2024novel,kumar2023eegalzheimer} and subject-independent~\cite{watanabe2024deep,chen2024multi}. In the subject-dependent setup, all samples are mixed together and split into training, validation, and test sets, allowing samples from the same subject to appear in all three sets. In contrast, the subject-independent setup splits the training, validation, and test sets based on subjects, ensuring that samples from the same subject are exclusively assigned to one set~\cite{wang2024medformer}. Unlike many existing works that use the subject-dependent setup, we use the subject-independent setup. The subject-dependent setup is unsuitable for real-world scenarios and leads to significant data leakage~\cite{wang2024evaluate}.


\textbf{Unified Fine-tuning.} 
The channel alignment in our data preprocessing step enables us to pre-train the model on various datasets and then fine-tune it on all downstream datasets simultaneously. We refer to this as "unified fine-tuning," where the model is fine-tuned across all downstream AD datasets in one run. The best model is then selected based on the weighted performance across the downstream datasets, ensuring that the model performs optimally on all tasks.


\textbf{Majority Voting.}  
For subject-level EEG-based AD detection, we apply a majority voting scheme to determine the final classification label for each subject. Specifically, for all the samples from one subject (with the same subject ID $\bm{s}$), we find the majority label of these samples and assign this label to this subject. For example, if a subject has 100 samples and more than 50 are classified as AD, the subject will be labeled "AD." The voting mechanism alleviates the interference of outlier samples in a subject.
