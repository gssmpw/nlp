

\input{tables/baseline_comparison}



\textbf{Datasets.}
We pre-train on 11 datasets: \textbf{AD-Auditory}~\cite{lahijanian2024auditory}, \textbf{ADFSU}~\cite{vicchietti2023computational}, \textbf{ADSZ}~\cite{pineda2020quantile}, \textbf{APAVA}~\cite{escudero2006analysis}, \textbf{Depression}~\cite{cavanagh2019multiple}, \textbf{PEARL-Neuro}~\cite{dzianok2024pearl}, \textbf{REEG-BACA}~\cite{getzmann2024resting}, \textbf{REEG-PD}~\cite{singh2023evoked}, \textbf{REEG-SRM}~\cite{hatlestad2022bids}, \textbf{TDBrain}~\cite{van2022two}, and \textbf{TUEP}~\cite{veloso2017big}, and fine-tuning on 5 downstream datasets: \textbf{ADFTD}~\cite{miltiadous2023dataset}, \textbf{BrainLat}~\cite{prado2023brainlat}, \textbf{CNBPM}~\cite{amezquita2019novel}, \textbf{Cognision-ERP}~\cite{cecchi2015clinical}, and \textbf{Cognision-rsEEG}. The pre-training datasets include 7 non-AD neurological diseases or healthy subjects and 4 AD datasets, totaling \textit{2,354 subjects and 1,165,361 1-second, 128Hz samples}. All downstream datasets are binary classifications between AD patients and healthy subjects, totaling \textit{615 subjects and 223,039 1-second, 128Hz samples}. The nine AD datasets used for pretraining or fine-tuning consist of \textit{813 subjects} in total. The rationale behind selecting these datasets for pre-training and fine-tuning is discussed in \ref{sub:datasets_selection}. The unified processing pipeline for each dataset is detailed in \ref{sub:data_preprocessing}, with a more detailed description available in Appendix~\ref{sec:datasets_preprocessing}. The statistics for the processed datasets are summarized in Table~\ref{tab:processed_data}.




\textbf{Baselines.}
We compare our method with 10 baselines, including 5 supervised, 3 self-supervised learning, and 2 large EEG foundational models. These selected baselines are state-of-the-art methods or have shown strong performance in EEG or time series classification tasks. The 5 supervised learning methods include \textbf{TCN}~\cite{bai2018empirical}, vanilla \textbf{Transformer}~\cite{vaswani2017attention}, \textbf{Conformer}~\cite{song2022eeg}, \textbf{TimesNet}~\cite{wu2023timesnet}, and \textbf{Medformer}~\cite{wang2024medformer}. The 3 self-supervised learning methods are \textbf{TS2Vec}~\cite{yue2022ts2vec}, \textbf{BIOT}~\cite{yang2024biot}, and \textbf{EEG2Rep}~\cite{mohammadi2024eeg2rep}. The 2 large EEG foundational models are \textbf{LaBraM}~\cite{jiang2024large} and \textbf{EEGPT}~\cite{wangeegpt}. 



\textbf{Implementation.}
All baseline methods and our method's variants, except for LaBraM and EEGPT, are trained under the same code framework. The training epoch for self-supervised pretraining is fixed at 50 epochs, with no early stopping mechanism. The training epoch is set to 100 for fully supervised learning or fine-tuning, with early stopping after 15 epochs of patience based on the best F1 score. The batch sizes for pretraining, fully supervised learning, and fine-tuning are set to 512, 128, and 128, respectively. The optimizer is AdamW. The initial learning rates for pretraining, fully supervised learning, and fine-tuning are set to 0.0002, 0.0001, and 0.0001, respectively, with the CosineAnnealingLR learning scheduler. Gradient norm clipping is set to 4.0, and Stochastic Weight Averaging (SWA)~\cite{izmailov2018averaging} is enabled to benefit inter-subject representation learning. For LaBraM and EEGPT, we use their public code and load their pre-trained model for fine-tuning. We employ four evaluation metrics: sample-level accuracy and F1 score (macro-averaged), and subject-level accuracy and F1 score (macro-averaged) after majority voting, as described in ~\ref{sub:important_setups}. In the self-supervised pre-training stage, all subjects in the datasets are used for training. The ${\lambda_1}$ and ${\lambda_2}$ are both set to 0.5. In the supervised learning or fine-tuning classification stage, the training, validation, and test sets are split based on the subject-independent setup with a ratio of 6:2:2 for each dataset, where each subject appears exclusively in one of these three sets. There is no dataset overlapping between the pre-training and fine-tuning datasets. The training process is conducted with 5 random seeds (41-45) on fixed training, validation, and test sets to compute the mean and standard deviation of the models. All experiments are run on an RTX 4090 GPU and a server with 4 RTX A5000 GPUs, using Python 3.8 and PyTorch 2.0.0 + cu118. Appendix~\ref{sec:implementation_details} provides more details about each method's implementations.
