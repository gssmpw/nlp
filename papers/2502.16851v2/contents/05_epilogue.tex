% \section{Related Work}
% \pagebreak
\section{Key Takeaways}\label{sec:tackaway}
We highlight the key takeaways from a practical perspective: %for optimizing memory-bound kernels:

\begin{itemize}
    \item Identifying kernel characteristics (compute-bound/ memory-bound) (Section~\ref{sec:workload}). 
    \item Compute-bound
    \begin{itemize}
        \item Tensor cores remain advantageous for compute-bound operations.
    \end{itemize}
    \item Memory-bound
        \begin{itemize}
        \item Prioritizing CUDA cores for memory-bound kernels due to their simplicity and effectiveness (Section~\ref{sec:eval}).
        \item Focusing on memory access optimizations, e.g. cache-aware algorithms and reducing memory traffic~\cite{10.1145/3577193.3593716,10.1145/3577193.3593705}.
        \item Prioritizing pipeline and overlap optimizations before considering tensor core adoption (Section~\ref{sec:anaovl}). 
        \item Considering theoretical limits: tensor cores provide at most $1.33$× speedup in double precision, with a $2$× ceiling assuming infinite tensor core computational speedup (Section~\ref{sec:anaunovl}).
    \end{itemize}
\end{itemize}

\section{Conclusion}\label{sec:conclude}

Through systematic theoretical and empirical analysis, we demonstrate that leveraging the tensor cores for computation in memory-bound kernels fails to deliver sound performance benefits. 
Our theoretical analysis establishes an upper bound of $1.33$× speedup for double-precision memory-bound kernels, while empirical results across SCALE, SpMV, and stencil show that tensor core implementations usually underperform their CUDA core counterparts. 
While these findings may temper expectations for using tensor cores in memory-bound kernels, efforts leveraging tensor cores still provide valuable insights for the design and utilization of matrix processing units in broader contexts.

% researching these applications still provides valuable insights for the design and utilization of matrix processing units in broader contexts.

% In this research we conducted systematical analysis in memory-bound kernels and then empirically debunk the speedup in tensor-core over cuda core in such kernels. We conclude that pury porting memory-bound kernel from cuda core to tensor core do not bring performance benefit. Though our empirical analysis might temper the enthusiasm of using tensor core, this line of research can still be valuable to other matrix processing units. 