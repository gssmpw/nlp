
% We use the same stencil benchmark as ConvStencil~\cite{10.1145/3627535.3638476}. Details are listed in Table~\ref{tab:stencilbench}. Here we only conduct experiment in A100 platform because both ConvStencil's and LoRAStencil's AD/AE code has bugs in GH200.
% ConvStencil transformed stencil computation to matrix-matrix multiplication to use the tensor core. To better use the power of the tensor core, it applied kernel fusion, a.k.a temporal blocking in stencil. 

% For evaluation, we use the default setting (including the default domain size) in their papers

% Lorastencil innovatively uses Low-Rank adaptation in stencil computation to reduce redundant computation in stencil. However, if what the author's claim being correct, the roofline plot of LoraStencil would be moved left to be even more memory-bound. 

% We intended to include this new sota in our comparison. However as we checked the AD/AE, we found that the error rate of the implementation is unacceptable (Root Mean Square $RMS\approx 109$ in $1024^2$ domain while  $RMS\approx 0.49$ in $512^2$ domain). We decided to refrain from using the AD/AE code for evaluation. Instead, we used a "visual measurement" from their paper~\cite{lorastencil} for comparison. 

\subsection{Iterative Stencils}
\input{Table/StencilBenchs}
Stencil implementations were evaluated using ConvStencil~\cite{10.1145/3627535.3638476} benchmark suite (Table~\ref{tab:stencilbench}). %Due to ConvStencil and LoRAstencil having bugs in GH200, our experiments are limited to the A100 platform.
Due to bugs in both ConvStencil's and LoRAStencil's Artifact Description/Artifact Evaluation (AD/AE) on the GH200 platform, our experiments are restricted to the A100 platform.
% Due to ConvStencil and LoRAStencil having bugs in GH200, our experiments are limited to the A100 platform.

\noindent\textbf{ConvStencil~\cite{10.1145/3627535.3638476} (Tensor Core):}
ConvStencil leverages tensor cores by transforming stencil computation into matrix-matrix multiplication, incorporating temporal blocking through kernel fusion. We evaluate using their default configuration and domain sizes.

\noindent\textbf{LoRAStencil~\cite{lorastencil} (Tensor Core):} LoRAStencil applies Low-Rank adaptation to reduce stencil computational redundancy. While innovative, their artifact evaluation relies on assumptions about the rank of stencil weights, which limits its practical applicability. Due to these constraints, we use their published performance results for comparison.
%we observed significant accuracy issues in its AD/AE
% ~\footnote {$error=\tfrac{\| \hat{A} - A_{ref} \|_F}{\| A_{ref} \|_F}$.
% For a $512^2$ domain, the error was $0.01$; for a $1024^2$ domain, the error increased to $2.19$; and for a $10240^2$ domain, the error escalated to $73.03$.}
% . 
% 

\noindent\textbf{Brick~\cite{zhao2019exploiting} (CUDA Core):} Baseline CUDA Core implementation without temporal blocking. We evaluated it using the default configuration.

\noindent\textbf{EBISU~\cite{10.1145/3577193.3593716} (CUDA Core):} EBISU is a state-of-the-art CUDA Core implementation that incorporates temporal blocking. To ensure a fair comparison, we configured EBISU's temporal blocking parameters to match those of ConvStencil.

% ~\footnote {\[
% error=\frac{\| \hat{A} - A_{ref} \|_\infty}{\| A_{ref} \|_\infty}
% \]
% In domain  $512^2$ error = 0.01, in domain $1024^2$  error= 6.90, in domain $10240^2$ error= 182.72.}
% For evaluation, we use the default setting (including the default domain size) in their papers.


