\begin{figure}[t!]
\vspace{-8pt}
    \centering
    \includegraphics[width=\linewidth]{FIG/scale_eva.pdf}
    \vspace{-25pt}
    \caption{SCALE performance evaluation on A100 (top) and GH200 (bottom). We show the speedup geometric mean (GEOMEAN) of the CUDA cores over the tensor cores is reported for input domain sizes larger and smaller than half of the L2 cache, respectively.}
    \label{fig:scaleeval}
\end{figure}
\begin{figure*}[ht!]
\vspace{-15pt}
    \centering
    \includegraphics[width=\linewidth]{FIG/spmv_eva.pdf}
    \vspace{-30pt}
    \caption{Comparison of cuSPARSE (CUDA Core) and DASP (Tensor Core) in sparse matrix-vector multiplication on A100 (top) and GH200 (bottom). (a) \& (c) on the left report the performance in effective flops; (b) \& (d) on the right report the geometric mean speedup of cuSPARSE over DASP.}
    \label{fig:spmv}
\end{figure*}
% \vspace{-40pt}
\begin{figure*}[ht!]
\vspace{-12pt}
    \centering
    \includegraphics[width=\linewidth]{FIG/stencil_eva.pdf}
    \vspace{-30pt}
    \caption{Comparison of EBISU and Brick and ConvStencil and LoraStencil on A100 using a suite of stencil benchmarks. For LoRAStencil~\cite{lorastencil}, performance data and their assumed rank values from their artifact evaluation are included.}
    \label{fig:stencilev}
\end{figure*}

\subsection{Evaluation}

% \subsection{Evaluation and Observations}

Figures~\ref{fig:scaleeval}, \ref{fig:spmv}, and \ref{fig:stencilev} present performance comparisons for SCALE, SpMV, and stencil kernels respectively. 
% We show both workload-specific and general observations.

% \subsubsection{Primary Observations} 
\noindent\textbf{SCALE:} Figure~\ref{fig:scaleeval} reveals consistent, though modest, performance degradation when using tensor cores compared to CUDA cores. Given that the computational time difference is negligible, this performance gap likely arises from suboptimal memory access patterns associated with tensor core usage on current GPU architectures.

\noindent\textbf{SpMV:} Figure~\ref{fig:spmv} demonstrates that for datasets exceeding the L2 cache size, cuSPARSE (CUDA core) outperforms DASP (tensor core) on average.

\noindent\textbf{Stencils:} Figure~\ref{fig:stencilev} shows that equivalently optimized tensor core implementations generally underperform their CUDA core counterparts.

\noindent\textbf{Summary:} While our goal was to verify the theoretical bounds, empirical evaluation reveals that tensor core implementations usually underperform their CUDA core counterparts.

% \noindent\textbf{Summary:} Though we intend to verify the theoretical bound. Yet the evaluation shows that tensor core implementations usually underperform their CUDA core counterparts. 

% to see a slight speedup of within $1.33$x speedup. The evaluation 

\subsection{Other Observations}
% \subsection{General Observations}
\noindent\textbf{L2 Cache Impact:} L2 cache interactions exhibit nontrivial between implementations. For SCALE, tensor core performance degradation intensifies within L2 cache bounds. Conversely, DASP demonstrates improved performance for cache-resident data, suggesting crucial performance implications of L2 cache optimization.% (ranging from 30\% slowdown to 20\% speedup in A100).

\noindent\textbf{Compute-Bound Cases:} The 2d49pt stencil, which is compute-bound on A100, shows comparable performance between tensor and CUDA core implementations. However, on GH200, the same kernel becomes memory-bound, where CUDA cores theoretically maintain superior performance.

\noindent\textbf{Resource-Constrained Cases:} 3D stencils and high-order 2D stencils typically encounter bottlenecks beyond memory or compute limitations, such as register capacity, cache capacity, or cache bandwidth~\cite{10.1145/3577193.3593716}. Since tensor core optimizations target only computational aspects, they provide no inherent advantage for these resource-constrained workloads. As expected, our evaluation shows no performance benefits from tensor core implementations in these stencil benchmarks.



% However, this same kernel becomes memory-bound on GH200, where CUDA cores maintain superior performance in theory.
% The evaluation result of SCALE kernel, spmv, and stencil is shown in Figure~\ref{fig:scaleeval}, Figure~\ref{fig:spmv} and Figure~\ref{fig:stencilev} respectively. In the following section, we first discuss the main observation from each workload and then discuss the topic-specific observations.
%we consider this performance difference mainly come from the tensor memory access being not desirable in GPU. 
% \subsubsection{3D Stencils.} 
% 3D stencils and high-order 2D stencils are usually not bound by memory or compute. Instead, it might be bounded by register capacity, cache capacity, or cache bandwidth. As tensor core implementation only tackle computation, it is reasonable that tensor core implementation can not benefit 3D stencils. 
% \subsubsection{SCALE} In Figure~\ref{fig:scaleeval}, we observe a consistent performance degradation when using a tensor core, even though the performance difference is insignificant. 
% \subsubsection{Spmv} In Figure~\ref{fig:spmv}, we observe that when data size is larger than the L2 cache, cuSPARSE (representing CUDA core) is on average faster than DASP (representing tensor core).
% \subsubsection{Stencils} In Figure~\ref{fig:stencilev} we observe that, generally, tensor core implementations could not beat their cuda core counterparts if it is the same level well optimized. 

% \subsubsection{L2 Cache} If we only consider the domain that is within the L2 cache, we can observe that in the SCALE kernel, tensor core implementation performs worse than exceeding the L2 cache. At the same time, for DASP, it is the opposite. This means special care needs to be taken for the tensor core when the L2 cache becomes the bottleneck and the performance benefit of considering that might be non-trivial (from 30\% slow down to 20\% speedup in A100).

% \subsubsection{Compute-Bound Stencil} 2d49pt is a compute-bound stencil in A100, tensor core implementation shows compatible performance, which is not surprising. However, as this kernel becomes memory-bound in GH200, the CUDA core counterpart might still perform much better.

% \noindent\textbf{Consistently Performance Degrade:} We observe that Tensor Core is consistently slower, this might be because some parts of the computation can not be overlapped by memory access and thus the performance disadvantage of tensor core implementation used in this research influenced the performance.

% \noindent\textbf{Worse L2 Cache Performance:} When the data range is within L2 cache, performance degraded from using tensor core is enlarged. This might mean the tensor core might not be L2 cache-friendly without special care. 



 % The observations are as follows:


% \noindent\textbf{General Size cuSPARSE Perform Better:} We observe that cuSPARSE in cases where the dataset is larger than the L2 cache, it on average performs better than DASP. And the performance difference is larger than that of the STREAM kernel. This can directly lead to a conclusion that generally cuSPARSE with CUDA Core still be the better choice for spmv. 




% \noindent\textbf{Compute-Bound Stencils.} The only situation tensor core shows compatible performance is 2d49pt stencil. However, as this kernel becomes memory-bound in GH200, the CUDA core counterpart might still perform much better. 



% We can see that tensor core implementation in 3D stencil is still not compatible with its CUDA core counterpart. This  