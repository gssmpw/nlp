\subsection{SCALE}

\noindent\textbf{Tensor Core Implementation:}
Inspired by work~\cite{navarro2020gpu}, we implement tensor core SCALE as matrix multiplication $A=B(qI)$, where $I$ is the identity matrix, as shown in Figure~\ref{fig:scaleimp}.
% \noindent\textbf{Performance Drawback:}
However, this implementation utilizes only $\tfrac{1}{max(m,n)}$ of the tensor core's compute capacity ($m$, $n$ = tensor core dimensions). For A100 and H100's $8\times4$ double precision tensor cores, only 1/8 of compute power is used:
$\mathbb{P}_{A100}(TC,SCALE)=2.4$, TFLOPS/s $\mathbb{P}_{GH200}(TC,SCALE)=8.37$ TFLOPS/s. This is lower than CUDA core performance. However, this should not significantly impact SCALE kernel performance, with or without overlap, given its extremely low operational intensity $\mathbb{I}$ (According to Section~\ref{sec:theory}).

% This implementation utilizes only $\tfrac{1}{max(m,n)}$ of the tensor core's compute capacity, where $m$ and $n$ represent the tensor core dimensions. For A100 and H100's double precision tensor cores (8Ã—4), this results in utilizing only 1/8 of available compute power. Consequently, the effective tensor core peak performance becomes $\mathbb{P}_{A100}(TC,SCALE)=2.4$ TFLOPS/s and $\mathbb{P}_{GH200}(TC,SCALE)=8.37$ TFLOPS/s, lower than CUDA core performance on both architectures. However, if the computation can be fully overlapped, this computational performance difference should not impact overall kernel performance for memory-bound operations.

\begin{figure}[t!]
    \centering
    \includegraphics[width=\linewidth]{FIG/scale_imp.pdf}
    \vspace{-24pt}
    \caption{Tensor core SCALE implementation.}
    \label{fig:scaleimp}
\end{figure}

\noindent\textbf{CUDA Core Implementation:}
The CUDA core baseline uses a ChatGPT-generated STREAM implementation{\footnote{\url{https://chatgpt.com/share/67570ae8-4554-8007-9f91-48f01722af85}}}, modified only to include warmup iterations.


% In this way, we only use $\tfrac{1}{max(m,n)}$ of tensor core compute power, where $m$ and $n$ are the row and column sizes of the given tensor core. In A100 and H100, the double precision tensor core size is $8\times 4$. So we only use $1/8$ of Tensor Core computing power, making the effective tensor core peak to be: $\mathbb{P}_{TC,A100}(SCALE)=2.4$ TFLOPS/s and $\mathbb{P}_{TC,GH200}(SCALE)=8.37$ TFLOPS/s, slower than CUDA core in both architecture. Nevertheless, if the roofline predicts correctly, this performance difference would not influence performance.
% STREAM kernels are widely used and simple. Here we use ChatGPT to do the favor. Note that we only added warmup section to the code generated by ChatGPT.

