% \vspace{-15pt}
\section{Introduction}
Since Nvidia introduced tensor cores in their Volta architecture in 2017~\cite{8344474}, researchers have extensively explored their applications across various domains, including dense linear algebra~\cite{bhaskaracharya2020automatickernelgenerationvolta}, sparse linear algebra~\cite{10.1145/3620666.3651378,okanovic2024high}, and spectral methods~\cite{9563043,9460474}. The low-precision computation capabilities of tensor cores, which offer substantially higher performance, have spawned innovative approaches such as mixed-precision algorithms~\cite{haidar2018harnessing,lu2024amgt} and precision recovery~\cite{10.1145/3650200.3656634,ootomo2022recovering,ootomo2024dgemm}. However, despite this broad adoption, fundamental analysis of tensor core performance remains limited, with only a few studies focusing on microbenchmarking~\cite{9931992,10579250,9926299}.

% Since 2017, when Nvidia first introduced the tensor cores in Volta~\cite{8344474}, a burst of efforts has been made to utilize it, including Dense Linear~\cite{bhaskaracharya2020automatickernelgenerationvolta}, Sparse Linear~\cite{10.1145/3620666.3651378,okanovic2024high}, Spectral Methods~\cite{9563043,9460474}. Additionally, Tensor Core usually features low-precision computation that is tremendously faster, some efforts were to leverage this feature with mix-precision algorithm~\cite{haidar2018harnessing,lu2024amgt} or precision recovering~\cite{ootomo2022recovering,ootomo2024dgemm}. Despite many research already done in this area, yet very few analysis research to understand the performance of tensor core. Only few research was done to microbenchmark the performance of Tensor Core~\cite{9931992,10579250}. 

While tensor cores represent a powerful tool for program acceleration, their effective utilization requires a thorough understanding of their performance characteristics. This research addresses this knowledge gap by focusing specifically on memory-bound kernels, which constitute a significant portion of HPC workloads~\cite{austinsystem}. Our study seeks to answer: % two critical questions:

% Tensor Core as a potential tool to accelerate program can be very powerful. However, not understanding the tools might lead to misusing or not fully utilizing the tools. We thereafter initialize this research. 

% In this paper, we mainly tackle the feature of memory-bound kernel as HPC workloads are mainly memory-bound~\cite{austinsystem}. In this study, we aim to answer the questions:

\begin{itemize}
\item What is the theoretical performance ceiling for tensor cores when applied to memory-bound kernels?
\item Do current tensor core implementation strategies provide real performance benefits for memory-bound kernels?

\end{itemize}
To address these questions, we make the following contributions:
\begin{itemize}
\item A comprehensive theoretical analysis of tensor core performance for memory-bound kernels.
\item An empirical evaluation comparing tensor core implementations against their CUDA core counterparts across representative memory-bound kernels.
\end{itemize}

% The rest of the paper is organized as follows. Section~\ref{sec:background} introduces backgrounds for understanding this paper; Section~\ref{sec:workload} introduces the memory-bound workloads used in this research; In Section~\ref{sec:theory}, we leverage a thorough analysis on two extreme scenery and draw the up bound for using tensor core over CUDA core. In Section~\ref{sec:eval}, we empirically analyze representative memory-bound kernels and verify that leveraging tensor core for computation in memory-bound kernels fail to deliver sound performance benefits. 

The rest of the paper is organized as follows. Section~\ref{sec:background} provides essential background concepts. Section~\ref{sec:workload} introduces our studied memory-bound workloads. Section~\ref{sec:theory} analyzes two extreme scenarios to establish tensor core performance bounds. Section~\ref{sec:eval} empirically substantiates our claims using representative memory-bound kernels. Finally, we present key takeaways and conclusions.

% % To answer these questions, our contributions are:
% \begin{itemize}
%     \item We provide a systematic node-level theoretical performance analysis for memory-bound kernels. 
%     \item We conducted an empirical study on memory-bound kernels, comparing the performance of tensor core-based implementation and its cuda core counterparts. 
% \end{itemize}

