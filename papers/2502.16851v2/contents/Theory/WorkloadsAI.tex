% This section introduces the workload studied in this paper i.e., SCALE (Section~\ref{sec:theoscale}), Spmv (Section~\ref{sec:theospmv}), and stencil (Section~\ref{sec:theostencil}).
% We conducted an analysis based on operation intensity $\mathbb{I}$. Since our main interest lies in high-performance workloads, our analysis focuses on double-precision situations (data size $\mathbb{D}=8$), yet the analysis can be further generalized to lower-precision situations. 


\section{Workloads: Memory-Bound Kernels}\label{sec:workload}

This section examines three representative memory-bound kernels: SCALE (Section~\ref{sec:theoscale}), Sparse Matrix-Vector Multiplication (SpMV) (Section~\ref{sec:theospmv}), and Stencil (Section~\ref{sec:theostencil}). We analyze these kernels through the lens of operational intensity ($\mathbb{I}$), focusing on double-precision operations (data size $\mathbb{D}=8$ bytes). % to align with high-performance computing requirements. 

While our analysis centers on double precision, the methodology can be extended to lower-precision scenarios.

\subsection{SCALE}\label{sec:theoscale}
SCALE, one of the STREAM benchmark~\cite{McCalpin2007}, is defined as
\begin{equation}\footnotesize
a_i = qb_i, \quad \forall i \in {1,\ldots,n}, \quad a,b \in \mathbb{R}^n, \quad q \in \mathbb{R}
\end{equation}
Each element operation requires one load, one store, and one computation, yielding: $\mathbb{W}(\text{SCALE})=1$, $\mathbb{Q}(\text{SCALE})=2\times\mathbb{D}$, and consequently $\mathbb{I}(\text{SCALE})=\tfrac{1}{16}$. STREAM benchmark is commonly used to measure sustainable memory bandwidth due to its low computational intensity.

\subsection{Sparse Matrixâ€“Vector Multiplication (SpMV)} \label{sec:theospmv}
SpMV, crucial for iterative solvers, has format-dependent operational intensity. In this section, we begin by analyzing dense matrix-vector multiplication (GEMV) as a baseline.

\noindent\textbf{GEMV:}
For matrix $A \in \mathbb{R}^{m \times n}$ and vectors $x \in \mathbb{R}^n$, $y \in \mathbb{R}^m$, GEMV is defined as:
\begin{equation}\footnotesize
y = Ax
\end{equation}
With computing $\mathbb{W}(\text{GEMV})=m\times n\times2$ operations and memory traffic $\mathbb{Q}(\text{GEMV})=(m\times n+m+n)\times\mathbb{D}$ yields:
\begin{equation}\footnotesize
\mathbb{I}(\text{GEMV})=\frac{m\times n\times2}{(m\times n+m+n)\times\mathbb{D}}
\approx \frac{2}{\mathbb{D}}=\frac{1}{4}
\end{equation}

\noindent\textbf{SpMV:}
For sparse matrices with $nnz$ non-zeros, SpMV is defined as:
\begin{equation}\footnotesize
\begin{aligned}
y &= Ax, \
A \in \mathbb{R}^{m \times n}, \quad \text{nnz}(A) &\ll mn, \quad
x \in \mathbb{R}^n, \quad y \in \mathbb{R}^m
\end{aligned}
\end{equation}
With computation $\mathbb{W}(\text{SpMV})=2\times nnz$ and memory traffic including coordinate information $\alpha\mathbb{I}$ or packed values $\beta\mathbb{Z}$:
\begin{equation}\footnotesize
\mathbb{I}(\text{SpMV})=\frac{nnz\times2}{(nnz+m+n)\times\mathbb{D}+\alpha\mathbb{I} +\beta\mathbb{Z}}
\end{equation}
Given $nnz\ll m\times n$, we have $\mathbb{I}(\text{SpMV})<\mathbb{I}(\text{GEMV})$.
% \begin{equation}\footnotesize
% \mathbb{I}(\text{SpMV})<\mathbb{I}(\text{GEMV})
% \end{equation}

\noindent\textbf{Compressed Sparse Row (CSR) format:}
CSR format, the most common sparse representation, requires storing column indices and row pointers. With memory traffic $\mathbb{Q}(\text{SpMV,CSR})=(nnz+m+n)\times\mathbb{D}+(nnz+m+1)\times\mathbb{I}$ and computation $\mathbb{W}(\text{SpMV,CSR})=2\times nnz$:
\begin{equation}\footnotesize
\begin{split}
\mathbb{I}(\text{SpMV,CSR})&=\frac{2\times nnz}{(nnz+m+n)\times\mathbb{D}+(nnz+m+1)\times\mathbb{I}} \\
&\approx \frac{2}{\mathbb{D}+\mathbb{I}}=\frac{1}{6}<\mathbb{I}(\text{GEMV})
\end{split}
\end{equation}
This analysis confirms SpMV's memory-bound nature, consistent with prior works~\cite{10.1145/1816038.1816021,10.1145/3577193.3593705}.

% Spmv is at the heart of many iterative solvers. The operation intensity of Spmv depends on the sparse format 
% chosen. We initialize the analysis in this subsection by analyzing gemv: dense matrix-vector multiplication. 
% We can assume a matrix $A$ with $m\times n$. The input vector $x$ has $n$ elements and the output vector $y$ has $m$ elements. The gemv operation can be formulated as:

% \begin{equation}
% y = Ax, \quad A \in \mathbb{R}^{m \times n}, \quad x \in \mathbb{R}^n, \quad y \in \mathbb{R}^m
% \end{equation}

% We will have $\mathbb{W}_{gemv}=m\times n\times2$. Ideally, we need to load the whole matrix, and input vector from the main memory, and store the output vector back to the main memory. So we have: $\mathbb{Q}_{gemv}=(m\times n+m+n)\times\mathbb{D}$. Thus we have:
% \begin{equation}\footnotesize
%     \mathbb{I}_{gemv}=\frac{m\times n\times2}{(m\times n+m+n)\times\mathbb{D}}
%      \approx \frac{2}{\mathbb{D}}=\frac{1}{4}
% \end{equation}


% The operation intensity of spmv depends on the sparse format chosen. We assume that we have non-zero values $nnz$. The formulation of spmv would be:

% \begin{equation}
% \begin{aligned}
% y &= Ax, \\
% A \in \mathbb{R}^{m \times n}, \quad \text{nnz}(A) &\ll mn, \quad
% x \in \mathbb{R}^n, \quad y \in \mathbb{R}^m
% \end{aligned}
% \end{equation}

% So we have $\mathbb{Q}_{spmv}=2\times nnz$. When it comes to memory traffic, we need either additional information of coordinate $\alpha\mathbb{I}$ (e.g. CSR format~\cite{7013050}) or need to pack additional values $\beta\mathbb{Z}$ (SELL-C-$\sigma$~\cite{doi:10.1137/130930352}). So we have $\mathbb{Q}_{spmv}=(nnz+m+n)\times\mathbb{D}+\alpha\mathbb{I} +\beta\mathbb{Z}$. Thus:
% \begin{equation}\footnotesize
%    \mathbb{I}_{spmv}=\frac{nnz\times2}{(nnz+m+n)\times\mathbb{D}+\alpha\mathbb{I} +\beta\mathbb{Z}}
% \end{equation}
% Because $nnz\ll m\times n$. We have:
% \begin{equation}\footnotesize
%    \mathbb{I}_{spmv}<\mathbb{I}_{gemv}
% \end{equation}


% CSR format (or Compressed Sparse Column (CSC)) is the most used sparse matrix format. Assume that we have non-zero values $nnz$. We have the same amount of value and index for the column index. We need to record the starting point and end point of each row of $m+1$ values. So we have: $\mathbb{Q}_{spmv}(CSR)=(nnz+m+n)*\mathbb{D}+(nnz+m+1)\times\mathbb{I})$. As for computation, the effective computation is $\mathbb{Q}_{spmv}(CSR)=2\times nnz$. So, we have:

% \begin{equation}\footnotesize
% \begin{split}
%     \mathbb{I}_{spmv}(CSR)&=\frac{2\times nnz}{(nnz+m+n)*\mathbb{D}+(nnz+m+1)\times\mathbb{I})} \\
%     &\approx \frac{2}{\mathbb{D}+\mathbb{I}}=\frac{1}{6}<\mathbb{I}_{gemv}
% \end{split}
% \end{equation}


% So spmv kernel is a memory-bound kernel with very low operation intensity. This analysis result consists with existing research~\cite{10.1145/1816038.1816021,10.1145/3577193.3593705}. 






\subsection{Iterative Stencils}\label{sec:theostencil}

Stencil computations is common in HPC~\cite{hagedorn2018high}. For 2D stencil, we have:
\begin{equation}\footnotesize
v(i,j) = \sum_{(p,q) \in \mathbb{S}} w_{p,q} \cdot u(i+p,j+q)
\end{equation}
where $v(i,j)$ and $u(i,j)$ are updated and original values at point $(i,j)$, and $\mathbb{S}$ defines relative offsets (e.g., 5-point stencil: $(-1,0)$, $(1,0)$, $(0,1)$, $(0,-1)$, $(0,0)$).
Ideally, only one load of $u$ and one store of $v$ are necessary:
\begin{equation}\footnotesize
\mathbb{Q}=2\times\mathbb{D}, \quad \mathbb{W}=2\times|\mathbb{S}|, \quad \mathbb{I}=\frac{|\mathbb{S}|}{\mathbb{D}}
\end{equation}
For a 2d5pt stencil where $|\mathbb{S}
(\text{2d5pt})|=5$, $\mathbb{I}(\text{2d5pt})=\tfrac{5}{8}$.

% \subsubsection{Temporal Blocking}
\noindent\textbf{Temporal blocking~\cite{10.1145/3577193.3593716,10.1145/3368826.3377904}} combines $t$ timesteps together:
\begin{equation}\footnotesize
\mathbb{W}_{t}=t\times2\times|\mathbb{S}|, \quad \mathbb{I}_{t}=t\times\frac{|\mathbb{S}|}{\mathbb{D}}
\end{equation}
While temporal blocking can theoretically transform memory-bound stencils into compute-bound kernels by increasing operational intensity, practical limitations exist. 

For a 2d5pt stencil on GH200 ($\mathbb{B}_{GH200}=9.99$), compute-bound behavior requires:
\begin{equation}\footnotesize
t\times \mathbb{I}(\text{2d5pt})>\mathbb{B}_{GH200} \implies t\times 0.625 > 9.99 \implies t > 15.98
\end{equation}

However, deep temporal blocking (e.g., $t > 16$) usually faces hardware limits from register pressure~\cite{10.1145/3577193.3593716,10.1145/3368826.3377904}. 

Thus, shallow temporal blocking ($t < 16$) 2d5pt stencil remains memory-bound, while deep temporal blocking might make stencil kernel register-bound. 



% Iterative stencils are widely used in HPC~\cite{hagedorn2018high}. To simplify the discussion, we use 2D stencil as an example. Let $u(i,j)$ represent the value of the grid at point (i,j), we can formulate 2d stencil computation as:
% \begin{equation}
% v(i,j) = \sum_{(p,q) \in \mathbb{S}} w_{p,q} \cdot u(i+p,j+q)
% \end{equation}
% where:
% \begin{itemize}
%     \item $v(i,j)$: The updated value at grid point $(i,j)$
%     \item $u(i,j)$: The original value at grid point $(i,j)$
%     \item $\mathbb{S}$: The set of relative offsets defining the stencil points (e.g., $(-1,0)$, $(1,0)$, $(0,1)$, $(0,-1)$, $(0,0)$ for a 5-point stencil)
% \end{itemize}

% Ideally, we only load $u$ and store $v$ once. So we have: 
% \begin{equation}\footnotesize
%     \mathbb{Q}=2\times\mathbb{D}
% \end{equation}
% The workload is directly correlated to the number of points in $\mathbb{S}$. We have:
% \begin{equation}\footnotesize
%     \mathbb{W}=2\times|\mathbb{S}|
% \end{equation}
% And:
% \begin{equation}\footnotesize
%     \mathbb{I}=\frac{|\mathbb{S}|}{\mathbb{D}}
% \end{equation}

% As an example, for 2d5pt stencil $|\mathbb{S}_{2d5pt}|=5$, we have $\mathbb{I}_{2d5pt}=\tfrac{5}{8}$. 
