\paragraph{\faCheck \space Trust} 
To prevent over-reliance on automatic translations, lay users must calibrate an \emph{appropriate level} of (dis)trust. Indeed, they risk accepting potentially flawed translations at face value, and trust may be misplaced when an output appears believable but is inaccurate—an issue that is especially harmful in high-stakes contexts \citep{mehandru-etal-2023-physician}.

Prior research on MT has shown that \textit{fluency} and \textit{dialogue coherence} can falsely signal reliability, while LLMs amplify this issue with their overly persuasive tone, even when incorrect \citep{xiong2024llmsexpressuncertaintyempirical, trust}. As general-purpose models increasingly replace domain-specific applications, providing mechanisms for trust calibration becomes even more urgent \citep{deng2022generalpurposemachinetranslation, litschko-etal-2023-establishing}.

To harness the benefits of MT systems while avoiding over-reliance on flawed translations, lay users often turn to back-translation\footnote{i.e., automatically translating a text to a \emph{target} language and then back to the \emph{source} language.} as a strategy to improve confidence~\citep{back-2007, zouhar-etal-2021-backtranslation, mehandru-etal-2023-physician}. However, back-translation is typically performed manually due to the lack of dedicated functionalities, and its effectiveness in reliably assessing translation accuracy remains debated \citep{robertson2022understanding}.

A critical factor in fostering appropriate trust is transparency—e.g., communicating uncertainty and providing explanations~\citep{liao2023aitransparencyagellms}. While explainability is growing \citep{sarti-bisazza-2022-indee}, ensuring that explanations are informative and digestible to lay users rather than just developers is not trivial. Moreover, how to effectively integrate such signals into the development of MT systems and their user interfaces is still an open question.






prevent over-reliance on automatic translations, lay users must calibrate an \emph{appropriate level} of (dis)trust. 
Indeed, lay users risks accepting (potentially flawed) automatic translations at face value, and trust might be displaced when an output is believable but inaccurate, which is especially harmful in high-stakes contexts \citep{mehandru-etal-2023-physician}.  
In this regard, prior research on  MT  has shown that \textit{fluency} and \textit{dialogue coherence} can falsely signal reliability, while LLMs amplify this issue with their overly persuasive tone, even when they are wrong \citep{xiong2024llmsexpressuncertaintyempirical, trust}. 
As general-purpose models increasingly replace domain-specific application, providing mechanisms for trust calibration becomes even more urgent \citep{deng2022generalpurposemachinetranslation, litschko-etal-2023-establishing}. 
To harness the benefits of MT systems while avoiding over-reliance on flawed translations, lay users often adopt 
back-translation\footnote{i.e. automatically translating a text to a \emph{target} language and then back to the \emph{source} language.} as a strategy to improve confidence~\citep{back-2007, zouhar-etal-2021-backtranslation,mehandru-etal-2023-physician}. However, 
back-translation is performed manually as dedicated functionalities are often unavailable, and it the effectiveness of this method to get a correct estimate is debated \citep{robertson2022understanding}. 
One critical factor to support appropriate trust is transparency, e.g. communicating uncertainty and by providing explanations~\citep{liao2023aitransparencyagellms}. While explanability is growing, it is not trivial to make sure that the latter are digestible for lay users and not just for developers, and how to concretely handle this signals 
%What is currently missing is how to define and quantify an appropriate level of trust and how to handle it concretely in 
the development of MT systems and the user interfaces in which they are embedded.




Details on the functional capacity and origin of a system~\citep{davis-1979-applications} (i.e. intrinsic factors of \emph{system trustworthiness} such as model behavior) also help to strengthen user trust.

—not just for developers but for lay users who need accessible explanations and uncertainty cues to gauge reliability \citep{liao2023aitransparencyagellms}. Additionally, revealing intrinsic factors of \emph{system trustworthiness}, such as model behavior and provenance \citep{davis-1979-applications}, can strengthen informed trust.










For lay users to avoid over-reliance on MT, it is paramount to be able able to calibrate an \emph{appropriate level} of (dis)trust. 
Indeed, lay users risks accepting (potentially flawed) automatic translations at face value. 
This has serious consequences, since trust might be displaced when an output is believable but inaccurate, especially in high-stakes contexts 
\citep{mehandru-etal-2023-physician}. Prior work in MT as well as in monoligual LLM setting indeed found \textit{fluency} or \textit{dialogue flaw} as particularly confounding factors as an evidence of reliable output, 
as well as in LLM due to their overly persuasive nature and tone even when wrong \citep{xiong2024llmsexpressuncertaintyempirical, trust}. And yet, the more general purpose they are, the more support to calibrate trust should relevant as we are leaving dedicated contexutlly grounded models \citep{deng2022generalpurposemachinetranslation, litschko-etal-2023-establishing}. 

Details on the functional capacity and origin of a system~\citep{davis-1979-applications} (i.e. intrinsic factors of \emph{system trustworthiness} such as model behavior) also help to strengthen user trust.
and \emph{transparency} is a fundamental concept to help lay users modulate it it, e.g. by communicating uncertainty and explanations~\citep{liao2023aitransparencyagellms}, making sure that the latter are 
digestible for lay users and not just for developers. What is currently missing is how to define and quantify an appropriate level of trust and how to handle it concretely in the development of MT systems and the user interfaces in which they are embedded.










To harness the benefits of MT systems while avoiding over-reliance on flawed translations, lay users often adopt 
back-translation\footnote{i.e. automatically translating a text to a \emph{target} language and then back to the \emph{source} language.} as a strategy to improve confidence~\citep{back-2007, zouhar-etal-2021-backtranslation,mehandru-etal-2023-physician}. However, 
back-translation is performed manually as dedicated functionalities are often unavailable, and it the effectiveness of this method to get a correct estimate is debated. 

For trust to be established, lay users should be provided with 
information and tools to understand when to use or discard translations when interacting with it 
\citep{litschko-etal-2023-establishing}. 
However, 

Details on the functional capacity and origin of a system~\citep{davis-1979-applications} (i.e. intrinsic factors of \emph{system trustworthiness} such as model behavior) also help to strengthen user trust.
In user-facing technologies such as MT, it is paramount to understand how lay users  
modulate their trust with respect to flawed translations.
In this regard, 
\citet{martindale-carpuat-2018-fluency} 
show that users give more weight to \emph{fluency} rather than \emph{adequacy} errors. This has serious consequences, since trust might be displaced when an output is believable but inaccurate, especially in high-stakes contexts 
\citep{mehandru-etal-2023-physician}. Although chat-based LLMs open opportunities for adjusting outputs based on user requirements 
(i.e. via instructions), 
LLMs' efficacy, reliability, and coverage of users' feedback is currently unexplored.

To harness the benefits of MT systems while avoiding over-reliance on flawed translations, lay users often adopt 
back-translation\footnote{i.e. automatically translating a text to a \emph{target} language and then back to the \emph{source} language.} as a strategy to improve confidence~\citep{zouhar-etal-2021-backtranslation,mehandru-etal-2023-physician}. However, 
back-translation is performed manually due to the lack of dedicated functionalities. 
%in current MT interfaces. 
The need for an \emph{appropriate level} of trust in MT outputs is becoming central~\citep{martindale-carpuat-2018-fluency,deng2022generalpurposemachinetranslation} and \emph{transparency} is a fundamental concept to help lay users gain it, e.g. by communicating uncertainty and explanations~\citep{liao2023aitransparencyagellms}, making sure that the latter are 
digestible for lay users and not just for developers. What is currently missing is how to define and quantify an appropriate level of trust and how to handle it concretely in the development of MT systems and the user interfaces in which they are embedded.