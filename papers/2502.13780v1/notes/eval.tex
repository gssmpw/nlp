
Though MT-mediated communication has been approached as a form of human-machine interaction for professionals \citep{green2015natural,o2012translation}, 
little consideration has been given to how lay users can enhance this interaction and apply control strategies to overcome MT limits.
This implies a degree of critical agency rather than passive consumption. 
%knowledge of how to engage with MT and be agentic rather than passive consumer. 
Work in this area, however, showed how a system that impeded the exchange of potentially flawed translations  disregarded user judgments causing discomfort, thus suggesting that users favor warnings and interactive guidance over outright message blocking. Still, while warnings serve as an initial signal, users---usually constrained by their knowledge of either source or target language---need to understand how to proceed to recover from MT errors.
%when relying on MT remains a necessity.

To address this, \citet{bowker2019towards} introduced the concept of MT \textit{literacy}, a digital skill intended to provide users with the knowledge and strategies to interact more effectively with MT, for instance by pre-editing 
the input text to overcome typical failures (e.g., using short sentences or active voice). While such an approach yielded positive results in academic contexts, extending these strategies beyond students to reach more vulnerable populations and underserved languages is nontrivial \citep{Liebling}.

Focusing on target language comprehension, instead, \citep{robertson} explores the role of interfaces offering dictionary access and assistive bots. 
While LLMs encourage participation through chat and interactive queries, their reliability in this role remains an open question, as current LLM-powered systems  might impact cognitive attention for critical engagement \cite{}. Also, MT literacy might require evolving to incorporate and understand different failures brought by LLMs (e.g., hallucinations, cascading errors over multiple requests). 



%%
% \citep{Bowker01092020} showed positive results 

% In many communication settings, users possess some knowledge of the source language and can improve translations by using techniques such as short sentences, consistent terminology, and active voice \citep{Bowker01092020}. However, a key challenge is extending these strategies beyond students to reach more vulnerable populations, whose needs have been found to be underserved \citep{Liebling}.

must now extend to understanding that LLMs learn from past inputs and face challenges such as hallucinations and cascading errors (e.g., translating, shortening, and refining style iteratively). Moreover, recent research suggests that LLMs may reduce cognitive attention, raising further concerns about their role in fostering true user engagement.
%%%%%%%%%%%%%%%%%%%%%%%%%%%




Though MT has been viewed as human-machine interaction in professional translation, little attention has been given to how lay users can effectively engage with it. Research indicates that users value agency and prefer to participate actively rather than depend entirely on automated translations.

For example, \todo{(Maybe et al., 2010)} tested a system that only allowed accurate messages, which overlooked user judgment and caused discomfort. This suggests users prefer warnings and interactive guidance over message blocking. While uncertainty communication serves as an initial alert, users also need guidance on proceeding when relying on MT.

\citet{bowker2019towards} introduced MT literacy, equipping users with knowledge and strategies for effective interaction with MT. Users with some source language knowledge can enhance translations using techniques like short sentences and consistent terminology. However, extending these strategies to vulnerable populations and underserved languages remains a challenge \citep{Liebling}.
Additionally, \todo{Robertson}  

Further engagement strategies have been explored by \todo{Robertson}, who looked into interfaces with dictionary access and assistive bots for better comprehension. While LLMs promote participation, their reliability is still under study. MT literacy now also involves understanding LLMs' learning from past inputs and challenges like hallucinations and cascading errors, alongside concerns about diminishing cognitive attention impacting user engagement.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Though MT has traditionally been viewed as a form of human-machine interaction in professional translation, little attention has been paid to how lay users can be supported in making this interaction more effective and applying control measures within their linguistic competence. However, research suggests that lay users value agency and prefer contributing actively rather than relying entirely on automated translations.

For instance, \todo{(Maybe et al., 2010)} tested a system that only allowed accurate messages to be exchanged. However, this approach disregarded user judgment and caused discomfort, indicating that users favor warnings and interactive guidance over outright message blocking. While uncertainty communication—discussed in the previous section—serves as an initial warning, users must also understand how to proceed when relying on MT remains necessary.

To address this, \citet{bowker2019towards} introduced the concept of MT literacy, which provides users with the knowledge and strategies to interact effectively with MT. In many communication settings, users possess some knowledge of the source language and can improve translations by using techniques such as short sentences, consistent terminology, and active voice \citep{Bowker01092020}. However, a key challenge is extending these strategies beyond students to reach more vulnerable populations, whose needs have been found to be underserved \citep{Liebling}.

Further support through active participation has been explored by \todo{Robertson}, who investigated interfaces offering dictionary access and assistive bots to enhance target language comprehension. While LLMs encourage participation through chat and interactive queries, their reliability in this role remains under study. MT literacy must now extend to understanding that LLMs learn from past inputs and face challenges such as hallucinations and cascading errors (e.g., translating, shortening, and refining style iteratively). Moreover, recent research suggests that LLMs may reduce cognitive attention, raising further concerns about their role in fostering true user engagement.


%%%%%%%%%%%%%%%%%%%%%%
MT has often been seen as a human-machine interaction in professional translation, but less attention has been given to how lay users can enhance this interaction and exert control over their linguistic abilities. Research shows that users value their agency and prefer to engage actively rather than rely solely on automated translations.

For example, \todo{(Maybe et al., 2010)} found that a system allowing only accurate messages caused discomfort among users, highlighting their preference for warnings and interactive guidance instead of outright message blocking. While uncertainty communication serves as an initial warning, users need to know how to proceed when relying on MT.

To address this, \citet{bowker2019towards} introduced MT literacy, equipping users with the skills to interact effectively with MT. Many users have some knowledge of the source language and can improve translations with strategies like short sentences and consistent terminology \citep{Bowker01092020}. However, a significant challenge remains in reaching underserved and vulnerable populations \citep{Liebling}.

Further support has been explored by \todo{Robertson}, looking at interfaces that provide dictionary access and assistive bots to aid comprehension. While LLMs encourage interaction, their reliability is still being studied. Users must understand that LLMs learn from past inputs and have issues like hallucinations. Recent research also indicates that LLMs might decrease cognitive attention, raising concerns about their effectiveness in promoting genuine user engagement.













Literacy
%%%%%%%%%%%%%%%%%%%%%%%%%%
- oltre a dover fare ricerca su come allineare systemi ed eval a user, e far capire quando rely o not rely on a model, un altra componente fondamentale 
e' gli user possano sapere quando intervenire per migliore e use effectively. Questa cos devi mettertela via, perche'ci sara' sempre il rischio che sia sbagliato qualcosa. 

- E questo e' particolarmente vero  per certe lingue, certi domain, e certi aspetti, e.g. bias. 

- Se connsideriamo MT come una forma di interaction, cosi' come si faceva con i translators, allora dobbiamo cercare dei modi anche per empower their role come active participants. 

- Crucially, MT literacy is tied to \textit{agency}; involving lay users as active participants is essential because the mere accessibility of MT does not guarantee its effective use.

- Ora come ora non e' molto incoraggiato: two box onlin are mostly copy and paste, e prove su bloccare messagi sbagliayi con errori hanno upsate user, advocating for solutions that grant them more agency. 

- Bowewker, ha introdotto questa notizione, e yielded postive results with students, ma e' limited there. Ma spesso le eprsone + impattate che non riescono a fare ricorso sono proprio le + vulnerable a qui bisogna arrivare. 

- This requires some level of literacy, that is knowledge on control measures, and it is thighy lined to the notion of agency. Qyesta anche in MT systems online non e' particolarlye, che non permettono molto + di copy and clik. In casi pero' in cui le persone conoscebano il src language, literacy cna help pre-edit (short sentence, active voice etc)

- Again, drawing da quello che succedea con i trasnalto studies, offer other notions of the target language tipo chat-input, o dictionaries. 

- questo puo' aiutare con i LLM. Ma bisogna anche capire i) come imparare nuovi tipi di errori (e.g. hallucinations), ii) e saperliinterrogare, che non e' detto si sappia fare ora. + questo complica things, MT literacy extends to understanding that LLMs learn and adapt based on past inputs and that they face specific challenges, such as hallucinations and cascading errors (for example, translating, then shortening, then refining the style).


%%%%%%%%%%%%%%%%%
Though MT has long been viewed as an area of human-machine interaction in the context of professional translators and methods and solutions have been devoted to enhance it such, little attention has been payde to how lay users themselves could be supported to to make that interaction more fruitful and intervene with control measures, even if of course bounded within the user language comptence. 
Works however suggests that lay user might prefer some degree of agency and contribute,  
In this area,  \todo{(Maybe et al., 2010)}, a system was tested that only allowed accurate messages to be exchanged. However, this approach disregarded user judgment and caused discomfort, suggesting that users prefer warnings and interactive guidance on how to proceed over outright message blocking.
WHil uncertaintiy communication--as discussed in the previous seciton--can represent a first level of warning, than users need to know of to procceed from there if they need to still to rely on MT. 
To count this, \citet{bowker2019towards} introduced the concept of MT literacy, aimed at providing notions of understanding and strategies to interact with MT. Realistically, in communication settings, the user knows the src language, and could (e.g., using short sentences, consistent terminology, and the active voice). This was proved useful by \citet{Bowker01092020}, but key challenges remain on how to reach out and adapt it beside students and rather get in contact with most vulnerable populations, whose need were found unsupported by Liebling. 
Also, other support based on active participation were suggested by todo{Robertson}'s work explores interfaces that provide access to dictionaries and assistive bots, which can support on the target language knwlegde for assimilation purposes. In this area LLM are indeed increasing participation via chat and interative questions, but i) how reliable they are in this is open to investigation,  MT literacy extends to understanding that LLMs learn and adapt based on past inputs and that they face specific challenges, such as hallucinations and cascading errors (for example, translating, then shortening, then refining the style). Also because recent work showed that LLM might actually descrese cognitive attention. 


To make the best use of MT, users must be aware of its limitations and where it might be flawed or risky, so they can also implement control measures.

A key factor in overcoming MT’s limitations, and more generally in having more fruitful interactions with MT, is \textit{literacy}. If we consider MT as a form of human-computer interaction, it works best when users have a degree of knowledge about the system and are empowered to exercise their agency. This means users should engage actively with the technology rather than rely on it passively.

\citet{bowker2019towards} introduced the concept of MT literacy, initially defined as understanding how MT works, its implications, and strategies for improving translation quality. While it was initially aimed at the academic community, we advocate for a broader application to the general public. MT literacy involves being an informed and critical user—thinking about whether, when, why, and how to use MT rather than simply copying, pasting, and clicking.
%
Crucially, MT literacy is tied to \textit{agency}; involving lay users as active participants is essential because the mere accessibility of MT does not guarantee its effective use.

\citet{Bowker01092020} reports on a workshop with Chinese speakers that increased awareness of different MT tools and strategies. Participants learned that various online MT systems can exhibit different behaviors and results, and they were taught how to improve input to enhance output (e.g., using short sentences, consistent terminology, and the active voice). Additionally, awareness of MT biases can improve user expectations and decision-making. The workshop yielded positive results, with participants reporting increased knowledge and confidence in using MT. While not everyone can attend formal courses, online resources are available to help develop MT literacy---although they are mostly targeting students right now.\footnote{See \url{https://sites.google.com/view/machinetranslationliteracy/}.}

Agency can also be fostered during MT use by communicating uncertainties in a model, which has been shown to improve success rates in achieving goals \citep{zhao-etal-2024-successfully}. \todo{Robertson}'s work explores interfaces that provide access to dictionaries and assistive bots to help users create better inputs. Other research investigates MT warnings for potential cultural misunderstandings, indicating that alerts can improve awareness and reduce misinterpretations \todo{cit}. In a different study \todo{(Maybe et al., 2010)}, a system was tested that only allowed accurate messages to be exchanged. However, this approach disregarded user judgment and caused discomfort, suggesting that users prefer warnings and interactive guidance over outright message blocking.

What does literacy and agency mean for large language models (LLMs)? To a certain extent, agency is more encouraged through chat. However, there is still much we do not know about literacy in the context of LLMs. MT literacy extends to understanding that LLMs learn and adapt based on past inputs and that they face specific challenges, such as hallucinations and cascading errors (for example, translating, then shortening, then refining the style).

%%%%%%%%%%%%%%%%%

Crucially, MT literacy is tied to \textit{agency}; involving lay users as active participants is essential because the mere accessibility of MT does not guarantee its effective use.


To make the best use of machine translation (MT), users should be aware of its limitations and implement control measures. A crucial factor in improving interactions with MT is \textit{literacy}, which allows users to engage actively with the technology instead of relying on it passively. 

\citet{bowker2019towards} introduced MT literacy as understanding how MT works, its implications, and strategies for improving translation quality. While initially aimed at the academic community, it is important for the general public as well. Being an informed user involves critically considering when and how to use MT instead of simply copying and pasting.

MT literacy is linked to \textit{agency}; engaging users as active participants is essential since accessibility alone does not guarantee effective use. For example, a workshop highlighted different MT tools, showing participants how input quality can affect output and raising awareness of MT biases. Many online resources exist to help develop MT literacy, even if they mostly target students.

Agency can be enhanced by conveying uncertainties in models, and research indicates that alerts for potential cultural misunderstandings can improve user awareness. Preferences suggest that users favor warnings and guidance over outright blocking of messages.

In the context of large language models (LLMs), agency is more encouraged through chat, but literacy remains less understood. While MT literacy includes awareness of LLMs' learning and adaptation processes, users must also recognize challenges like hallucinations and cascading errors.
















%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MT usage Evolution \todo{\textit{todo}: find better title--and to shorten}}
MT has evolved from a specialized tool to a democratized technology, enabling global communication at an unprecedented scale. As it integrates into more complex systems and publicly available LMs, its societal impacts and risks require greater attention. This section examines the converging societal and technical factors that have transformed MT from a niche tool to a professional application and, ultimately, a mainstream, user-facing technology with a vast lay-user base \bs{(see Table \ref{tab:mt_users_profile})}. We argue that this shift necessitates a broader perspective---one that refocuses on the perspectives and needs of lay users, considering the entire ecosystem of user interaction, usability, and social implications. User-centric concerns, long overdue \citep{green2015natural, Schuff_Vanderlyn_Adel_Vu_2023}, are ignited by the rise of LLMs, which might redefine 
traditional NLP tasks \citep{ouyang2023shifted}---including MT related ones---%
%MT-related tasks and 
make them more flexible, accessible, and engaging for non-experts through intuitive chat interfaces and conversational capabilities across multiple languages \bs{(see Figure \ref{fig:trend} for the increasing user-focus within LLM studies)}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



























==============================================

%Rethinking Machine Translation Evaluation: From Model Performance to Usability

Converging societal and technical factors
%--such as global interconnectedness, the explosion of digital content, and advances in core technology---
have transformed language technologies into user-facing applications employed by millions across numerous languages. 
As a cornerstone task in NLP, 
Machine translation (MT) has become a global tool, virtually accessible to anyone with an internet connection.
% ---and with cross-lingual services now also supported by dialogue systems powered by multilingual Large Language Models (LLMs).    
This accessibility has expanded the reach of MT to a vast base of \textit{lay~users},  with little to no expertise in the languages or the \hl{task} of translation itself. Despite this,
the understanding of MT consumed by lay users%
% MT for the general public, and how it serves lay users
% the understanding of MT end users---particularly lay users
---their needs, experiences, and interactions with these systems---remains limited. 


This paper first revisits the evolution of MT user-profiles and usages, and the potential impact of LLMs 

from a specialized to an increasingly diverse lay audience, 
and foregrounds the impact of LLMs on the 

users and usages by the general public, and the implications of 

and user profiles, emphasizing the increasing diversity of users, contexts, and the potential for transformative impact of Large Language Models (LLMs).



Converging societal and technical factors
%--such as global interconnectedness, the explosion of digital content, and advances in core technology---
have transformed language technologies into user-facing applications employed by millions across numerous languages. 
%As a cornerstone task in NLP, 
Machine translation (MT) has become a global tool, virtually accessible to anyone with an internet connection.
% ---and with cross-lingual services now also supported by dialogue systems powered by multilingual Large Language Models (LLMs).    
This accessibility has expanded the reach of MT to a vast base of \textit{lay~users},  with little to no expertise in the languages or the \hl{task} of translation itself. Despite this,
the understanding of MT consumed by lay users%
% MT for the general public, and how it serves lay users
% the understanding of MT end users---particularly lay users
---their needs, experiences, and interactions with these systems---remains limited. 

In this paper, we first examine the evolution of MT user profiles and usage from a specialized to a lay audience, the implications of this shift, and the potential impact of cross-lingual services now also supported by dialogue systems powered by multilingual Large Language Models (LLMs).

In this paper, we first trace the evolution of MT usage and user profiles by examining the shift to non-expert users and how their interactions with MT might change with the advent of LLMs. We identify three key factors—usability, trust, and literacy—that shape these interactions and must be addressed to better align MT with end-user needs. By exploring these dimensions, we offer actionable insights to guide future MT solutions and user research.


%Questions persist regarding how effectively MT addresses real-world challenges and incorporates human factors into its design. 
This paper examines the evolution of MT usage and user profiles, emphasizing the increasing diversity of users, contexts, and the potential for transformative impact of Large Language Models (LLMs). We identify three critical \bs{human factors}---\bs{usability, trust, and literacy}—that must be addressed to better align MT with end-user perspectives. By exploring these dimensions, we offer \hl{actionable} insights to guide future MT solutions paired with MT user research. 








===============================

\paragraph{Performance evaluations vs utility and usability}
Evaluation is critical, by setting the standard and direction for future improvements and advancements. But improvements and advancements for whom? Satisfying which criteria and desiderata? Ideally, assessments should reflect utility for users and feedback into MT design. 
%
%
Also, to make informed decisions about how and when to use a system, lay users would need to receive clear communications about their utility and capabilities, ideally along meaningful and comprehensible dimensions, i.e. by reflecting values and criteria that are important and understandable for users. 
%
%
Admittedly, however, current evaluation approaches are based on standardized benchmarks detached from realistic usage, and measures that are primarily opaque to non-experts. 
%
%
%
Indeed, prior work has discussed the limitations of performance-based
evaluation approaches in MT---and NLP research more broadly---as they tend
%leaderboards, which are widely used in NLP and MT research. These leaderboards tend 
to pursue  abstract notion of accuracy and quality above the practical utility of a model or other relevant values, e.g. fairness (Jurafsky et al., 2021).
%
These values are often contextual; e.g.  (Williams et al.) discuss how values \textit{robustness} to errors and misspellings might be detrimental if using MT as a learning tool. Conversely--- though \textit{faithfulness} is normally assumed to be a key aspect of ``MT quality'', in contexts such as literary translation or subtliting, \textit{enjoyability} may take precedence over strict fidelity to the source text (cit.).

Perhaps unsurprisingly, many limitations on how we currently operationalize and approach evaluation have emerged in the context of bias, which is mainly concerned with the downstream potential impact of technology on users. Indeed, \citep{Savoldi} found how evaluation approaches overlook user involvement by occasionally involving lay people (i.e. crowdworkers)  as manual evaluators to gain model-centric insights rather than user experiences. Along this line, Delobelle (cit)  underscores how current measurements fail to provide actionable insights and/or guide decision making, and there is increasing concern about benchmarks and test structures that do not reflect real-world scenarios. 
%
% fail to provide actionable insights, and rely on benchmarks that do not reflect real-world scenarios. Consequently, metrics often yield opaque scores that are hard for users to interpret.
Indeed, MT metrics---either reference-based or neutral ones---offer overall, coarse scores of ``generic'' performance, that lend themselves to ranking and comparisons across models, but are opaque for the receiver of this technology. 
%
% Many of our metrics offer opaque overall, coarse-grained scores of "generic" performance notion\footnote{Boldgett criticizes this a lacking a sufficient explanation of the construct.}, which serve ranking and comparisons but are hard to interpret and are definitely not meaningful to users. 
%
In this area, prior work has explored the effect of providing quality estimates (QE) to end-users, but results remain inconclusive in professional settings (Turchi et al., 2015). For lay users, 
others show that non-experts struggle to interpret probability scores (Miller, 2018; Vodrahalli et al., 2022). As such, they should be converted into textual explanations improve understanding (Mehandru et al., 2023), but (Mehandru et al., 2023) finds that such scores have rather an effect on user confidence rather than an informative value about the translation itself. 
%QE remains imperfect and can overestimate translation quality.
%

To counter coarse opaque scores, tools like OpenKiwi (Kepler et al., 2019) attempt to visualize quality at the phrase level. Similarly, 
the use of Error analysis frameworks like MQM (Lommel et al., 2014) 
employed in automatic approaches such as XCOMET, or LLM-based evaluation can
offer more detailed insights, by integrates both sentence-level evaluation and error span detection capabilities. However, such taxonomies and approaches reflect linguistic criteria of quality, and assume bilingual audience---or evaluatuors---as a target, but it is unclear if and how they might support lay users with limited language skills. 

% but recent trends toward LLM-based evaluation (Kocmi & Federmann, 2023; Fernandes et al., 2023) still prioritize annotators over end-users. While LLM-driven scoring correlates with human judgment, it assumes a bilingual audience—neglecting users who depend on MT to overcome language barriers. In general, this is again related to high-level notions of linguistic quality that can be useful if we want to treat people to taxonomy model behavior but still do not reflect their experiences or criteria important for them. 
% %
% Quality Estimation (QE) studies have produced mixed results. While some research suggests that QE supports human translation workflows (Stewart et al., 2020), its effectiveness for lay users remains uncertain. Prior studies indicate that users struggle to interpret probability and confidence estimates (Miller, 2018; Vodrahalli et al., 2022). Tsai et al. (2015) found that numeric quality estimates can support (bilingual) non-expert users, but Zouhar et al. (2021) observed that while QE feedback increased user confidence, it did not improve task performance. Mehandru et al. (2023) examined QE in clinical settings and found that textual explanations were more interpretable than numerical scores. However, while QE improved confidence, it did not ensure that translations effectively supported users’ needs. Furthermore, QE labels can be flawed, sometimes overestimating translation quality.

Now, although eval has been long discussed and criticized, also for MT (cit), But now since the advent of LLMs, researchers such as Liao et al.,  have even argued that now, with LLMs, we are facing an evaluation crisis, because our methods and pre-defined benchmarks for modular tasks might be obsolete, and not account for utility in real-world settings, and disregard other values and criteria that might be relevant (e.g. fairness, ease of usage). Besides, the risk is that we are opening the socio-technical gap, that is  there are divergences between what the community—or certain parts of the MT community—consider to be super relevant strengths versus actual useful features for lay users on practical user cases.
%
Evaluation has long been a topic of discussion and criticism, including in MT (cit). However, with the advent of LLMs, researchers such as Liao et al. have argued that we are now facing an evaluation crisis. Existing methods and predefined benchmarks for modular tasks may be obsolete, failing to capture real-world utility. This raises the risk of widening the socio-technical gap—where the priorities of the research community, or certain parts of the MT community, diverge from the actual needs of lay users in practical applications. 
%
A shift from "quality" to "usability" is essential. Usability, as defined by ISO 9241-11 (ISO, 1998), measures how effectively, efficiently, and satisfactorily users achieve their goals. Prioritizing usability ensures evaluations (i) provide meaningful guidance, (ii) encourage outputs that benefit real users, and (iii) assess entire systems, not just models. Bridging the gap between metrics and usability is crucial to making MT reliable and accessible for end-users.
%
% We also need to consider that depending on the context, the criteria might change: e.g.  For instance, in the context of subtitles or literary/artistic translation, enjoyability might be particularly relevant, even at the cost of less adherent translations. Along this line, Prasanna Parthasarathi discusses how sometimes we want ungrammatical sentences: it might be better to have very literal but faithful translations over translations that are robust to errors and optimize for fluency. 
% %
\bs{Overall, we have key issues: (i) evaluation scores are opaque and disconnected from practical use, (ii) the criteria being assessed lack clarity, and (iii) users are rarely involved in the evaluation process. Importantly, research focuses on models in isolation, rather than on usability within a complete system.}



% \paragraph{Performance Evaluations vs. Utility and Usability}

% Evaluation plays a crucial role in setting standards for future advancements. However, it raises the question of whose desiderata and criteria are met, and catering which needs.

% To help lay users make informed decisions about using a system, clear communication about capabilities and meaningful metrics is essential. Unfortunately, current evaluation methods primarily serve developers, lacking the clarity and insights necessary for lay users.

% Criticism has been directed at performance-based leaderboards common in NLP and MT research, which often prioritize abstract notions of accuracy over practical utility (Jurafsky et al., 2021). Many evaluation approaches overlook user involvement, fail to provide actionable insights, and rely on benchmarks that do not reflect real-world scenarios. Consequently, metrics often yield opaque scores that are hard for users to interpret.

% Tools like OpenKiwi (Kepler et al., 2019) aim to visualize quality at the phrase level, but their effectiveness in guiding user decisions remains uncertain (Turchi et al., 2015). Quality Estimation (QE) research shows mixed outcomes, with some findings suggesting QE supports workflows (Stewart et al., 2020), while others reveal that non-experts struggle to interpret probability scores (Miller, 2018; Vodrahalli et al., 2022). Even with textual explanations improving understanding (Mehandru et al., 2023), QE can still overestimate translation quality.

% Recent trends in LLM-based evaluations (Kocmi & Federmann, 2023; Fernandes et al., 2023) prioritize annotators over end-users, often ignoring those who rely on machine translation (MT) for language access. This creates a disconnect between evaluations and user experiences.

% Moving from a focus on "quality" to "usability" is essential. Usability, defined by ISO 9241-11 (ISO, 1998), measures how effectively users achieve their goals. Prioritizing usability means that evaluations should: (i) provide meaningful guidance, (ii) benefit actual users, and (iii) consider entire systems rather than isolated models.

% Criteria for evaluation can change based on context; for instance, enjoyability may be more important in literary translation than strict fidelity. In summary, key issues persist: evaluation scores are often opaque and disconnected from practical use, assessed criteria lack clarity, and user involvement is minimal. Research must shift focus from isolated models to usability within complete systems.
