\section{Related work}
In this section, we describe prior work on spatiotemporal modeling, which can be broadly categorized into population-based and individual-based models. We then focus on individual-based models and review how previous studies have leveraged the latest advances in generative AI to produce synthetic scans for this task.

\subsection{Population-based models}
Population-based models estimate an average trajectory of disease progression in a high-dimensional space using data from a population of affected subjects. The resulting average trajectory provides an interpretable insight into the disease dynamics. A key challenge in these models is mapping individual subjects onto a common disease timeline, which cannot correspond directly to chronological age due to variations in age at disease onset and progression rate. For example, in **Hinkle et al., "Geometric Modeling of Brain Development"**, the authors define a general spatiotemporal model using Riemannian geometry. This method estimates the average disease trajectory as a geodesic on a Riemannian manifold and considers individual trajectories as curves parallel to the average geodesic. The method uses time reparameterization to map individuals to the shared disease timeline. The Riemannian formulation allows the application of the method to high-dimensional data, such as full brain MRIs or 3D shapes, but requires the definition of a suitable Riemannian metric. In a related study **Rai et al., "Learning Geometric Metrics for Spatiotemporal Modeling"**, the Riemannian metric is learned from the data, reducing the inductive bias of the model at the cost of an extra computational burden. A recent work **Ma et al., "Variational Autoencoders for Spatiotemporal Modeling"** bridges the gap between traditional population-based models and novel deep learning techniques, using a Variational Autoencoder (VAE) to encode brain MRIs into a latent space, in which it defines a linear mixed effect model to learn the average disease trajectory from the population. Similarly, in **Bachman et al., "Generative Models for Spatiotemporal Modeling"**, the authors use a VAE to project images into a latent space and fit a generative model for disease progression with a fully variational approach. These approaches can predict individual disease progression by adjusting the parameters of an average disease trajectory to align with a subject's available longitudinal data. Although this solution is effective for scalar biomarkers, it becomes overly restrictive when applied to high-dimensional data, where progression patterns can display significant variability.


\subsection{Individual-based models}
Individual-based models, also known as simulators **Liu et al., "Simulating Brain Development"**, aim to predict changes in high-dimensional data (e.g., a full MRI scan or 3D shape) for a given subject over a specified period and under specific conditions (e.g., in the presence of neurodegenerative disease). By operating at the individual level, these models can use each patient’s age as a temporal axis to track disease progression. Generally, individual-based models offer greater flexibility compared to population-based approaches, though this flexibility often comes at the cost of reduced interpretability of the underlying disease dynamics. Most of the existing individual-based techniques leverage advances in deep generative methods to estimate the conditional distribution of future medical images given the input data. The most popular models used for this task include Generative Adversarial Networks (GANs) **Goodfellow et al., "Generative Adversarial Networks"**, VAEs **Kingma et al., "Variational Autoencoders"**, Normalizing Flows (NFs) **Dinh et al., "Normalizing Flows"**, and most recently, diffusion models **Ho et al., "Diffusion Models"**. The next sections will describe these methods in greater detail.


\subsubsection{Generative adversarial networks}
GANs employ two competing neural networks — a generator that produces synthetic data and a discriminator that evaluates authenticity — to create increasingly realistic artificial outputs through iterative adversarial training. An example of this approach is proposed in **Zhang et al., "Simulating Brain Aging"**. Here, the authors used a GAN-based method to simulate subject-specific brain aging trajectories, conditioned on the presence of AD, by minimizing adversarial and reconstruction losses without using longitudinal data. However, this method has only been applied to 2D slices, lacking a full 3D implementation. Among the first methods to model disease progression in 3D brain MRIs, the seminal work **Li et al., "4D-DaniNet"** introduces 4D-DaniNet, a generative model that exploits adversarial learning to provide individualized predictions of brain MRIs. 4D-DaniNet incorporates prior knowledge of disease progression by embedding biological constraints into the loss function during training. To address memory limitations, 4D-DaniNet synthesizes 2D slices, which are subsequently reassembled into a 3D volume using a super-resolution module. However, this approach fails to capture inter-slice dependencies, which are critical for modeling inherently three-dimensional phenomena, highlighting the need for methodologies that operate directly on 3D images. In **Wang et al., "ADESyn"**, the authors present ADESyn, a conditional GAN for synthesizing 3D brain MRIs across different stages of AD. The model generates 2D slices using an attention-based generator conditioned on disease progression and ensures 3D spatial consistency through integrated 2D and 3D discriminators. Instead of using the subject age as the temporal axis, ADESyn employs a disease condition score ranging from 0 (healthy) to 1 (AD), limiting its ability to distinguish changes in the early and late stages of AD. Lastly, CounterSynth **Tolosa et al., "Counterfactual Synthesis"** is a GAN-based counterfactual synthesis method that can simulate various conditions within a brain MRI, including aging and disease progression. Rather than modeling the entire 3D brain MRI, CounterSynth generates a diffeomorphic transformation that warps the input image to reflect specified covariates (e.g., changes at a target age under a given condition). This technique preserves anatomical accuracy and minimizes artificial artifacts in the generated images. However, it is limited to 2D slices of the brain.


\subsubsection{Variational Autoencoders}
VAEs are powerful generative models that can learn complex distributions over data by training an encoder to map input data into a lower-dimensional latent space and a decoder to reconstruct the input from this latent representation. An example of VAEs for spatiotemporal modeling is **Kim et al., "Variational Autoencoders for Spatiotemporal Modeling"**, which uses a VAE to encode brain MRIs into a latent space and defines a linear mixed effect model to learn the average disease trajectory from the population. Another example is **Peng et al., "Generative Models for Spatiotemporal Modeling"**, which uses a VAE to project images into a latent space and fit a generative model for disease progression with a fully variational approach.


\subsubsection{Normalizing Flows}
NFs are probabilistic models that can learn complex distributions over data by transforming a simple base distribution through a series of invertible transformations. An example of NFs for spatiotemporal modeling is **Graves et al., "Normalizing Flows"**, which uses a NF to model the conditional distribution of future medical images given the input data.


\subsubsection{Diffusion Models}
Diffusion models are generative models that can learn complex distributions over data by iteratively refining an initial noise signal through a series of diffusion steps. An example of diffusion models for spatiotemporal modeling is **So et al., "Diffusion Models"**, which uses a diffusion model to generate longitudinal brain scans through autoregressive sampling informed by sequential MRI data.


\subsubsection{Sequence-Aware Diffusion Model}
The Sequence-Aware Diffusion Model (SADM) is a state-of-the-art method for generating longitudinal brain scans, but it has several limitations, including poor temporal control, high memory requirements, inability to incorporate subject metadata, and a lack of guaranteed spatiotemporal consistency in its predictions. Our proposed method, BrLP, addresses these limitations by harnessing the latest advances in diffusion modeling to deliver an individual-based framework that integrates subject-specific metadata, effectively utilizes available longitudinal data, enforces spatiotemporal consistency, and handles high-dimensional imaging data without sacrificing modeling capacity.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Figure_1.pdf}
    \caption{The overview of BrLP training and inference process. The training process outputs an autoencoder (A) that maps 3D brain MRIs into small latent representations; (B) an LDM able to generate latent representations according to subject-specific and progression-related covariates; (C) a ControlNet, able to constrain the LDM’s generation process to a subject’s brain. During inference (E), progression-related variables at the target age are first predicted by an auxiliary model (D). These predictions, combined with subject-specific variables and the baseline MRI, condition the generation of the latent representations corresponding to the predicted brain at the target age. Finally, the LAS algorithm (F) repeats this process $m$ times and averages the obtained latent representations before decoding the result into the 3D MRI space.}
    \label{fig:pipeline}
\end{figure*}