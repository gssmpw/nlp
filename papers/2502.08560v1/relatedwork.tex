\section{Related work}
In this section, we describe prior work on spatiotemporal modeling, which can be broadly categorized into population-based and individual-based models. We then focus on individual-based models and review how previous studies have leveraged the latest advances in generative AI to produce synthetic scans for this task. 

\subsection{Population-based models}
Population-based models estimate an average trajectory of disease progression in a high-dimensional space using data from a population of affected subjects. The resulting average trajectory provides an interpretable insight into the disease dynamics. A key challenge in these models is mapping individual subjects onto a common disease timeline, which cannot correspond directly to chronological age due to variations in age at disease onset and progression rate. For example, in~\citep{schiratti2015learning}, the authors define a general spatiotemporal model using Riemannian geometry. This method estimates the average disease trajectory as a geodesic on a Riemannian manifold and considers individual trajectories as curves parallel to the average geodesic. The method uses time reparameterization to map individuals to the shared disease timeline. The Riemannian formulation allows the application of the method to high-dimensional data, such as full brain MRIs or 3D shapes, but requires the definition of a suitable Riemannian metric. In a related study~\citep{sauty2022riemannian} the Riemannian metric is learned from the data, reducing the inductive bias of the model at the cost of an extra computational burden. A recent work~\citep{sauty2022progression} bridges the gap between traditional population-based models and novel deep learning techniques, using a Variational Autoencoder (VAE) to encode brain MRIs into a latent space, in which it defines a linear mixed effect model to learn the average disease trajectory from the population. Similarly, in~\citep{chadebec2022image}, the authors use a VAE to project images into a latent space and fit a generative model for disease progression with a fully variational approach. These approaches can predict individual disease progression by adjusting the parameters of an average disease trajectory to align with a subject's available longitudinal data. Although this solution is effective for scalar biomarkers, it becomes overly restrictive when applied to high-dimensional data, where progression patterns can display significant variability.


\subsection{Individual-based models}
Individual-based models, also known as simulators~\citep{ravi2022degenerative}, aim to predict changes in high-dimensional data (e.g., a full MRI scan or 3D shape) for a given subject over a specified period and under specific conditions (e.g., in the presence of neurodegenerative disease). By operating at the individual level, these models can use each patient’s age as a temporal axis to track disease progression. Generally, individual-based models offer greater flexibility compared to population-based approaches, though this flexibility often comes at the cost of reduced interpretability of the underlying disease dynamics. Most of the existing individual-based techniques leverage advances in deep generative methods to estimate the conditional distribution of future medical images given the input data. The most popular models used for this task include Generative Adversarial Networks (GANs)~\citep{goodfellow2020generative}, VAEs~\citep{kingma2013auto}, Normalizing Flows (NFs)~\citep{papamakarios2021normalizing}, and most recently, diffusion models~\citep{ho2020denoising}. The next sections will describe these methods in greater detail.


\subsubsection{Generative adversarial networks}
GANs employ two competing neural networks — a generator that produces synthetic data and a discriminator that evaluates authenticity — to create increasingly realistic artificial outputs through iterative adversarial training. An example of this approach is proposed in~\citep{xia2021learning}. Here, the authors used a GAN-based method to simulate subject-specific brain aging trajectories, conditioned on the presence of AD, by minimizing adversarial and reconstruction losses without using longitudinal data. However, this method has only been applied to 2D slices, lacking a full 3D implementation. Among the first methods to model disease progression in 3D brain MRIs, the seminal work~\citep{ravi2022degenerative} introduces 4D-DaniNet, a generative model that exploits adversarial learning to provide individualized predictions of brain MRIs. 4D-DaniNet incorporates prior knowledge of disease progression by embedding biological constraints into the loss function during training. To address memory limitations, 4D-DaniNet synthesizes 2D slices, which are subsequently reassembled into a 3D volume using a super-resolution module. However, this approach fails to capture inter-slice dependencies, which are critical for modeling inherently three-dimensional phenomena, highlighting the need for methodologies that operate directly on 3D images. In~\citep{jung2021conditional}, the authors present ADESyn, a conditional GAN for synthesizing 3D brain MRIs across different stages of AD. The model generates 2D slices using an attention-based generator conditioned on disease progression and ensures 3D spatial consistency through integrated 2D and 3D discriminators. Instead of using the subject age as the temporal axis, ADESyn employs a disease condition score ranging from 0 (healthy) to 1 (AD), limiting its ability to distinguish changes in the early and late stages of AD. Lastly, CounterSynth~\citep{pombo2023equitable} is a GAN-based counterfactual synthesis method that can simulate various conditions within a brain MRI, including aging and disease progression. Rather than modeling the entire 3D brain MRI, CounterSynth generates a diffeomorphic transformation that warps the input image to reflect specified covariates (e.g., changes at a target age under a given condition). This technique preserves anatomical accuracy and minimizes artificial artifacts in the generated images. However, it is limited to modeling only structural changes.

\subsubsection{Variational autoencoders}
VAEs are deep generative models that learn to map data to and from a structured probabilistic latent space, enabling both compression and generation. In~\citep{he2024individualized}, the authors propose a double-encoder conditional VAE for predicting future MRI scans based on baseline MRI and subject metadata. The model also facilitates disease classification by estimating the posterior distribution when both baseline and follow-up MRIs are provided. Although demonstrated using 3D brain MRIs, the experiments are performed at low resolution ($2\text{ mm}^3$) to mitigate computational costs. Furthermore, the model does not address other challenges, such as the utilization of longitudinal data and enforcing the spatiotemporal consistency of the predicted progression.

\subsubsection{Normalizing flows}
NFs use sequences of invertible transformations to convert simple distributions into complex ones while maintaining exact likelihood evaluation. Based on this, \citep{wilms2022invertible} introduces a bidirectional NF model that links brain morphology to age, potentially incorporating additional variables, such as disease diagnosis. This approach enables the generation of follow-up images at a target age and the estimation of brain age from a given image. Their model leverages NFs and, similar to CounterSynth~\citep{pombo2023equitable}, utilizes diffeomorphic deformations to represent structural changes. While this bidirectional framework holds promise, it cannot utilize longitudinal data if available, and it lacks mechanisms to enforce spatiotemporal consistency.


\subsubsection{Diffusion models}\label{sec:diffusionmodels}
In recent years, diffusion models have emerged as a major breakthrough in generative AI, gaining significant attention for their state-of-the-art image synthesis capabilities and stable training processes, which avoid the challenges associated with adversarial approaches. A Denoising Diffusion Probabilistic Model (DDPM)~\citep{ho2020denoising} is a deep generative model with two Markovian processes: forward diffusion and reverse diffusion. In the forward process, Gaussian noise is incrementally added to the original image $x_0$ over $T$ steps. At each step $t$, noise is introduced to the current image $x_{t-1}$ by sampling from a Gaussian transition probability defined as $q(x_t \mid x_{t-1}) \coloneqq \mathcal{N}(x_t; \sqrt{1 - \beta_t}x_{t-1}, \beta_t I)$, where $\beta_t$ follows a variance schedule. If $T$ is sufficiently large, $x_T$ will converge to pure Gaussian noise $x_T \sim \mathcal{N}(0, I)$. The reverse diffusion process aims to revert each diffusion step, allowing the generation of an image from the target distribution starting from pure noise $x_T$. The reverse transition probability has a Gaussian closed form, $q(x_{t-1} \mid x_t, x_0) = \mathcal{N}(x_{t-1} \mid \tilde\mu(x_0, x_t), \tilde\beta_t)$, conditioned on the real image $x_0$. As $x_0$ is not available during generation, a neural network is trained to approximate $\mu_\theta(x_t, t) \approx \tilde\mu(x_0, x_t)$. Following the work proposed in~\citep{ho2020denoising}, it is possible to reparameterize the mean in terms of $x_t$ and a noise term $\epsilon$, and then use a neural network to predict the noise $\epsilon_\theta(x_t, t) \approx \epsilon$, optimized with the following objective:

\begin{equation}
\mathcal{L}_{\epsilon} \coloneqq \mathbb{E}_{t, x_t, \epsilon \sim \mathcal{N}(0, I)} \left[ 
\lVert \epsilon - \epsilon_\theta(x_t, t) \rVert^2 \right].
\label{eqn:ddpmloss}
\end{equation}

An LDM~\citep{rombach2022high} extends the DDPM by applying the diffusion process to a latent representation $z$ of the image $x$, rather than to the image itself. This approach reduces the high memory demand while preserving the quality and flexibility of the models. The latent representation is obtained by training an autoencoder, composed of an encoder $\mathcal{E}$ and a decoder $\mathcal{D},$ such that the encoder maps the sample $x$ to the latent space $z = \mathcal{E}(x),$ and the decoder recovers it as $x = \mathcal{D}(z)$.

Diffusion models have been successfully applied to 3D medical image synthesis, showing promising results~\citep{pinaya2022brain}. In the context of spatiotemporal modeling, the Sequence-Aware Diffusion Model (SADM)~\citep{yoon2023sadm} marks a significant advancement by allowing for the generation of longitudinal brain scans through autoregressive sampling informed by sequential MRI data. While SADM improves upon previous spatiotemporal methods by leveraging longitudinal data, it has several limitations, including poor temporal control, high memory requirements, inability to incorporate subject metadata, and a lack of guaranteed spatiotemporal consistency in its predictions. Other works have applied diffusion-based approaches to spatiotemporal modeling, but they are either limited to 2D slices of the brain~\citep{litrico2024tadm} or focused on different diseased organs~\citep{kim2022diffusion}. Our proposed method, BrLP, addresses the limitations of existing approaches by harnessing the latest advances in diffusion modeling to deliver an individual-based framework that integrates subject-specific metadata, effectively utilizes available longitudinal data, enforces spatiotemporal consistency, and handles high-dimensional imaging data without sacrificing modeling capacity. A key innovation of our approach is its treatment of prior knowledge: unlike 4D-DaniNet, which incorporates prior knowledge of disease progression by embedding biological constraints into the loss function during training, BrLP takes a more direct approach by integrating this prior knowledge directly into the image generation process itself.


\begin{figure*}[t!]
    \centering
    \includegraphics[width=\textwidth]{Figure_1.pdf}
    \caption{The overview of BrLP training and inference process. The training process outputs an autoencoder (A) that maps 3D brain MRIs into small latent representations; (B) an LDM able to generate latent representations according to subject-specific and progression-related covariates; (C) a ControlNet, able to constrain the LDM’s generation process to a subject’s brain. During inference (E), progression-related variables at the target age are first predicted by an auxiliary model (D). These predictions, combined with subject-specific variables and the baseline MRI, condition the generation of the latent representations corresponding to the predicted brain at the target age. Finally, the LAS algorithm (F) repeats this process $m$ times and averages the obtained latent representations before decoding the result into the 3D MRI space.}
    \label{fig:pipeline}
\end{figure*}