\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
% \usepackage[round]{natbib}

\usepackage{makecell}

\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{float}
\usepackage{multirow}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{6cm}
%
% and set <dim> to something 5cm or larger.


\usepackage{authblk}

\author[1]{Vincent Li}
\author[2]{Yule Fu}
\author[3]{Tim Knappe}
\author[4,5]{Kevin Han}
\author[5]{Kevin Zhu}

\affil[1]{Boston University \\ \texttt{vinli@bu.edu}}
\affil[2]{Duke University \\ \texttt{yule.fu@duke.edu}}
\affil[3]{Provadis \\ \texttt{cs.timknappe@gmail.com}}
\affil[5]{Carnegie Mellon University}
\affil[4]{Algoverse AI Research}




\begin{document}

\title{Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs}
\maketitle




% \author{
%   \textbf{Vincent Li * \textsuperscript{1}},
%   \textbf{Yule Fu * \textsuperscript{2}},
%   \textbf{Patrick Zhang * \textsuperscript{3}},
%   \textbf{Serene Xue * \textsuperscript{4}},
% \\
%   \textbf{Tim Knappe * \textsuperscript{7}}
%   \textbf{Kevin Zhu\textsuperscript{5}},
%   \textbf{Kevin Han\textsuperscript{6}},
% \\
%   \textsuperscript{1}Boston University, vinli@bu.edu
%   \\
%   \textsuperscript{2}Duke University, yule.fu@duke.edu
%   \\
%   \textsuperscript{3}Carnegie Mellon University,
%   \\
%   \textsuperscript{4}Affiliation 4
%   \\
%   \textsuperscript{5}Affiliation 5
%   \\
%   \textsuperscript{5}Affiliation 5
% \\
%   \small{
%     \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%   }
% \\
%   \small{*Authors contributed equally.}
% }


\begin{abstract}
Large Language Models have demonstrated remarkable capabilities in natural language processing tasks, including mathematical problem-solving that requires multi-step logical reasoning. However, challenges persist in automating the identification of key mathematical concepts, understanding their interrelations, and formalizing proofs within a rigorous framework. We present a novel framework that leverages knowledge graphs to augment LLMs to construct and formalize mathematical proofs. Our results demonstrate significant performance improvements across multiple datasets, with using knowledge graphs, achieving up to a 34\% success rate on the MUSTARDSAUCE dataset on o1-mini and consistently outperforming baseline approaches by 2-11\% across different models. We show how this approach bridges the gap between natural language understanding and formal logic proof systems and achieve elevated results for foundation models over baseline. 

\end{abstract}

\section{Introduction}

The advent of Large Language Models has revolutionized natural language processing, enabling machines to perform complex reasoning tasks \citep{peters-etal-2018-deep, brown2020languagemodelsfewshotlearners, srivastava2023imitationgamequantifyingextrapolating}. These models have shown promise in mathematical problem-solving, which inherently requires multi-step logical inference and precise understanding of abstract concepts. Despite these advancements, significant challenges remain in automating the identification of mathematical concepts, understanding their interrelations, and formalizing proofs within a mathematical framework \citep{hendrycks2021measuringmathematicalproblemsolving}.

% Recent efforts in AI-driven mathematics have focused on enhancing the reasoning capabilities of LLMs and integrating them with formal verification systems. For instance, DeepMind's AlphaGo and AlphaZero have been adapted for theorem proving, leading to the development of systems like \textit{DeepMath} and \textit{HOList} \cite{Bansal2019}. These systems leverage reinforcement learning and neural networks to navigate the vast search space inherent in theorem proving, demonstrating the potential of AI in formal mathematics.

% Another notable project is MUSTARD (Mathematics Understanding through Semantic Theory and Reasoning Development), which explores the use of semantic representations to improve mathematical understanding in AI \cite{Johnson2020}. MUSTARD focuses on creating structured representations of mathematical knowledge, enabling machines to reason about mathematical concepts more effectively. By integrating semantic parsing with knowledge representation, MUSTARD aims to enhance the machine's ability to comprehend and generate mathematical proofs.

% Moreover, work by \cite{Polu2020} introduced \textit{Generative Language Modeling for Automated Theorem Proving}, where language models are trained to generate proofs in formal languages. The approach demonstrates that LLMs can learn the syntax and semantics of formal mathematical languages to a certain extent, but it also highlights limitations in handling complex mathematical reasoning without structured guidance.

Recent advancements in AI-driven mathematics focus on improving reasoning capabilities in language models and integrating them with formal verification systems. Novel approaches leverage reinforcement learning to address theorem proving \citep{Bansal2019}. Others enhance the understanding of mathematical concepts by combining semantic parsing with knowledge representation \citep{Johnson2020}. Training language models to generate formal proofs has underscored the difficulties of handling complex reasoning tasks without structured guidance \citep{Polu2020}.

\begin{figure}[h] \centering \includegraphics[width=\linewidth]{diagram2.jpeg} \caption{Overview of the proof generation and formalization workflow.} \label{fig:workflow} \end{figure}
Nonetheless, existing methodologies often lack a comprehensive approach to extracting and structuring mathematical content from extensive knowledge bases, such as ProofWiki, and leveraging this structured data for proof generation \citep{ProofWiki}.

In this paper, we propose a framework that automates mathematical proof construction by combining Large Language Models with knowledge graphs. Our approach parses constructs a knowledge graph that captures mathematical relationships. We then leverage this graph to formalize proofs, integrating with Lean \citep{Moura2021} a theorem prover and programming language designed for formal proof verification.

More specifically we:

\begin{itemize}
    \item Built a knowledge graph of over 60,000 nodes and 300,000 edges that represents mathematical concepts and their interrelations, facilitating traversal to alike subjects.
    \item Integrate a framework of multiple language models working together towards a correct formal proof, achieving a 7-10\% absolute gain over baseline approaches.
    \item Utilize feedback-like approaches granting additional traversals for failure correction, allowing our knowledge graph method to consistently outperform the baseline.
\end{itemize}

% Building upon prior research and incorporating advanced techniques in natural language processing and knowledge representation,
Our work aims to bridge the gap between natural language understanding and formal logic. We provide a detailed methodology, evaluate the effectiveness of our framework, and discuss its scalability and potential impact on the field of automated theorem proving.



\section{Related Work}

The intersection of artificial intelligence and mathematics has garnered significant attention, with various research efforts aiming to automate mathematical reasoning and proof generation.

DeepMind introduced \textit{HOList} for higher-order theorem proving. By integrating reinforcement learning with neural networks, the system learns to predict useful tactics for proving theorems in the HOL Light theorem prover \citep{Bansal2019, Harrison2012}. Further advancements include \textit{DeepMath}, which combines LLMs with reinforcement learning to navigate proof spaces \citep{Lample2022}.
% By training on a large corpus of formal mathematical proofs.

The MUSTARD project \citep{Johnson2020} aims to improve AI's understanding of mathematics through semantic representation. By first generating a problem, constructing an informal proof, converting it into Lean \citep{Moura2015} format, and verifying the proof with a Lean interpreter. 

Recent work explored using generative language models for automated theorem proving, by training transformer models on formal mathematical languages, equipping models such as DeepSeek-Prover-V1.5 with Methods like proof-assistant feedback to improve performance\citep{Polu2020}.

While previous research has made significant strides in automating aspects of mathematical reasoning, challenges remain in integrating natural language mathematical knowledge with formal proof systems \citep{yang2023leandojotheoremprovingretrievalaugmented}.

\section{Methodology}

Our framework automates mathematical proof generation by integrating Large Language Models with a knowledge graph constructed from ProofWiki. We employ a multi-stage approach combining retrieval-augmented generation with a two-agent system for proof formalization. The system consists of three main components: search strategy implementation, proof generation, and proof formalization. Figure~\ref{fig:workflow} illustrates the overall workflow.

% \begin{figure}[h] \centering \includegraphics[width=\linewidth]{diagram.jpeg} \caption{Overview of the proof generation and formalization workflow.} \label{fig:workflow} \end{figure}

\subsection{Overall Workflow}

Our main workflow integrates the components as follows, given a mathematical problem input:

\begin{enumerate}  
\item \textbf{Context Retrieval}: Semantic search retrieves relevant context from the knowledge graph. 
\item \textbf{Informal Proof Generation}: The LLM generates an informal proof using the context. 
\item \textbf{Formal Proof Generation}: The Autoformalizer converts the informal proof into a formal proof. 
\item \textbf{Verification}: The formal proof is verified using  a proof verifier (e.g Lean) \item \textbf{Iterative Refinement}: If verification fails, we retrieve another node and the process is iterated to improve the proof. 
\end{enumerate}


\subsection{Knowledge Graph}
\subsubsection{Retrieval}

To retrieve relevant context for a given problem, we implemented a semantic search strategy leveraging the embeddings, by generating an embedding vector $\mathbf{v}_q$ for problem statements and comparing to the problem embedding $\mathbf{v}_q$ and the embeddings $\mathbf{v}_i$ of the nodes in the knowledge graph using the cosine similarity function :
    \begin{align*}
        sim(v_q,v_i)=\frac{v_q\cdot v_i}{\|v_q\|\|v_i\|}
    \end{align*}
     % Where
% \begin{itemize}
%     \item $v_q$ and $v_i$ signify the given embedding vectors
%     \item $\|v_n\|$ represents the Euclidean norm
% \end{itemize}
We then select the top $k$ nodes with the highest similarity scores, retrieving the most relevant concepts related to the problem. The content of these selected nodes is injected to the context that will be provided to the language model. To further enrich the context with foundational concepts, we introduce a depth parameter 
$d$, which controls the traversal of the graph by limiting or expanding the allowed steps between nodes. This enables the inclusion of proofs or relationships that are located further from the root node.

\subsubsection{Graph database}

We parsed ProofWiki to extract mathematical definitions, theorems, proofs, and related content, focusing on name-spaces corresponding to definitions, axioms, and proofs\footnote{Our constructed dataset shape can be referred to in Appendix \ref{appendix:dataset_sample}}.
We use Neo4j, as a graph database, to store and manage the nodes and relationships, forming our knowledge graph \citep{Webber2012}. Nodes are created with their respective properties, and relationships are established based on internal links within the content, capturing the inter-dependencies among concepts. We store the nodes in Neo4j alongside with their embedding vectors, enabling queries based on semantic similarity.
Relationships between nodes were established\footnote{An example entry from our nodes collection can be found in Appendix \ref{appendix:nodes_example}}.


\subsection{Proof Generation with LLMs}

\subsubsection{Informal Proof Generation}
The Informal proof generation integrates retrieved context into the language model prompt and uses the LLM to create a proof based on this enhanced input. If the proof is unsuccessful or incomplete, the framework iteratively deepens the context by one level in the knowledge graph, selecting the top-$k$ semantically closest neighboring nodes to uncover missing key concepts. The updated context is then used for subsequent proof generation attempts.


\subsubsection{Formal Proof Generation}

The Autoformalizer generates the formal proof by first preparing the prompt\footnote{The prompting framework can be found in Appendix \ref{appendix:prompt_example}}, which involves combining the code prefix and the informal proof. It then invokes the model to generate the formal proof based on this prompt. Finally, it parses the model's response to extract the Lean 4 code.

\subsection{Lean 4 integration}
To ensure the formal correctness of the proofs generated by our framework
we adopted the Lean verification method from DeepSeek-Prover-V1.5 to enhance the formalization step in our proof generation process utilizing RLPAF to refine our model's ability to generate proofs that are verifiable in Lean \citep{Jiang2024}. By integrating proof-assistant feedback, our models are more robust in producing proofs that adhere to the strict syntactic and logical requirements of Lean.

The formal proofs were verified using Lean 4 to ensure correctness. The generated proof code was submitted to Lean, and the results were analyzed. If verification failed, error messages were extracted and used to refine the proof iteratively. The Autoformalizer adjusted the prompt or proof based on these errors, repeating the process up to a set attempt limit until the proof passed verification or the limit was reached.

% \subsection{Search Strategies within the Knowledge Graph}

% To optimize the process of automated proof generation, we explored different methods for navigating the constructed knowledge graph. Specifically, we implemented two primary search strategies: Breadth-First Search (BFS) and semantic search using vector embeddings. This section elaborates on these methodologies, their implementation in our framework, and analyzes their respective advantages and disadvantages in our scenario.

% \subsubsection{Breadth-First Search (BFS)}

% Breadth-First Search is a classic graph traversal algorithm that systematically explores the vertices of a graph in layers, starting from a given root node and expanding outward to neighboring nodes at increasing depths.
% In our framework, BFS was utilized as follows:

% \begin{enumerate}
%     \item \textbf{Zero-Shot Prompting}: We initially present the problem statement directly to the GPT model without any additional context, requesting a proof in a zero-shot setting.
%     \item \textbf{First-Level Traversal}: If the zero-shot attempt is unsuccessful, we perform a BFS to explore the immediate neighboring nodes of the problem statement node. Specifically, we retrieve up to the nearest 50 nodes connected directly to the root node.
%     \item \textbf{Contextual Prompting}: We then prompt the GPT model again, providing the problem statement along with the content from the retrieved neighboring nodes to supply additional context for proof generation.
%     \item \textbf{Iterative Expansion}: If the proof remains incomplete or incorrect, we extend the BFS to the next level by including nodes that are two edges away from the root, effectively expanding the context window before re-prompting the GPT model.
% \end{enumerate}

% The advantage of BFS is that it allows for a systematic exploration of the knowledge graph, ensuring that all nodes within a certain depth are considered, which may uncover relevant but non-obvious connections. By incrementally increasing the depth of traversal, we can control the amount of additional information provided to the GPT model, potentially improving the quality of the generated proof.

% However, BFS can be computationally expensive, especially in densely connected graphs, as the number of nodes grows exponentially with each additional level of depth. Including a broad set of neighboring nodes may introduce irrelevant or redundant information, which could overwhelm the GPT model and hinder its ability to generate a coherent proof.

% \subsubsection{Semantic Search Using Embeddings}

% Semantic search leverages vector embeddings to identify nodes that are semantically similar to a given query \cite{Neelakantan2022}. Each node in our knowledge graph is associated with a high-dimensional embedding vector, enabling similarity computations.

% Our semantic search approach involves the following steps:

% 
%     \item \textbf{Embedding Generation}: We use an OpenAI's \texttt{text-embedding-3-large} to generate a 3,072-dimensional vector representation for each node in the knowledge graph, capturing the semantic content of the node's mathematical concept.
%     \item \textbf{Similarity Computation}: For the given problem statement, we generate its embedding vector and compute the cosine similarity between this vector and the embeddings of all nodes in the graph.
%     \item \textbf{Node Selection}: We select the nodes with the highest similarity scores to the problem statement, identifying them as the most semantically relevant concepts.



% \begin{enumerate}
%     \item \textbf{Hierarchical Prompting}: Similar to the BFS approach, we begin with a zero-shot prompt. If unsuccessful, we incrementally include the most similar nodes into the context when re-prompting the GPT model, effectively performing one-shot, two-shot prompting, and so on.
% \end{enumerate}

% Semantic search is computationally less intensive than BFS, as it avoids exhaustive traversal and focuses only on nodes with high semantic relevance. By prioritizing nodes that are semantically similar to the problem statement, we provide the GPT model with highly pertinent information, potentially improving proof generation quality. The disadvantages are that the effectiveness of semantic search is contingent upon the embedding model's ability to accurately capture mathematical semantics, which may be challenging for complex or abstract concepts. Important nodes that are not semantically similar based on the embedding (e.g., foundational axioms or lemmas) may be overlooked, potentially omitting crucial information required for the proof.


% Regardless of the search method used, we adopted an iterative prompting strategy with the GPT model. This approach allows us to manage the amount of information provided to the GPT model, aiming to strike a balance between context richness and the model's capacity to process and utilize the information effectively.


\section{Experiment Design}
\subsection{Models}
\subsubsection*{Embedding Models}
To create semantic representations in the form of embeddings, we used OpenAI's \texttt{text-embedding-3-large} model \citep{Neelakantan2022}.

\subsubsection*{Generator Models}
For informal proof generation we used GPT-4o-mini, as well as Claude 3.5 Sonnet and a collection of LLAMA 3 models \citep{OpenAI2024, anthropic2024claude35sonnetaddendum, grattafiori2024llama3herdmodels}. Additionally we measure performance on the COT-reasoning model Deepseek-R1.\footnote{All generator models are evaluated together with a custom prompt. The prompt can be found in Appendix \ref{appendix:prompt_example2} that was designed to provide a clear problem statement and incorporating the retrieved context}\citep{deepseekai2025deepseekr1}

\subsubsection*{Autoformalizer}

As an Autoformalizer we use DeepSeek-Prover-V1.5 \citep{Jiang2024} which is an open-source language model designed for theorem proving in Lean \citep{Moura2021}.

\subsection{Datasets}
To evaluate the effectiveness of our framework, we conducted experiments on multiple benchmarks commonly used in automated theorem proving: \textbf{miniF2F}, \textbf{ProofNet} and \textbf{MUSTARD} \citep{Zheng2022, Azerbayev2023,huang2024mustardmasteringuniformsynthesis}.  MiniF2F is a benchmark dataset of formal mathematics problems sourced from undergraduate-level mathematics competitions, specifically the International Mathematical Olympiad (IMO). ProofNet is a large-scale dataset of mathematical proofs and theorem statements, ranging in difficulty and domain. MUSTARDSAUCE is the dataset MUSTARD generated itself using GPT-4.


All datasets present their samples with natural language and a formal statement in Lean, which we use as ground truth to compare against. Our exact dataset configuration can be found in Appendix \ref{appendix:benchmarks}.

% \begin{itemize}
%     \item \textbf{miniF2F} is a dataset comprising mathematical problems sourced from high school and undergraduate-level mathematics competitions. % covering domains like algebra, geometry, number theory, and calculus.  Each problem is presented in both natural language and a formal statement in Lean 4, which we use as ground truth to compare against. miniF2F is designed to benchmark the capabilities of automated theorem provers in handling complex mathematical reasoning tasks.
%     \item \textbf{ProofNet} is a large-scale dataset of formal mathematical statements and proofs collected from extensive mathematical libraries, including Lean's \texttt{mathlib}. % It encompasses a wide range of mathematical domains, providing both the formal statements and their corresponding proofs in Lean 4. ProofNet serves as a comprehensive benchmark for evaluating the performance of proof generation and autoformalization systems, particularly in terms of scalability and the ability to handle advanced mathematical concepts.
%     \item \textbf{mustard} is a formal proof dataset designed to evaluate systems' ability to handle diverse and modular mathematical reasoning tasks. % The dataset includes proofs and statements spanning multiple domains, focusing on modular arithmetic, symbolic computation, and reasoning with definitions and transformations. mustard emphasizes the importance of structuring proofs into reusable components, making it particularly suited for benchmarking the modularity and adaptability of automated theorem provers. The dataset is structured in Lean 4 and includes a mix of simple and complex proofs to challenge systems at various levels of reasoning depth.
 
% \end{itemize}


% \begin{table*}[h!t]
% \centering
% \begin{tabular}{l l|c c c c c c}
% \toprule
% \textbf{Dataset} & \textbf{Method} & \makecell{\textbf{Claude 3.5}\\\textbf{Sonnet}} & 
% \makecell{\textbf{Deepseek}\\\textbf{R1}} & 
% \makecell{\textbf{Llama 3.1}\\\textbf{8B}} & 
% \makecell{\textbf{Llama 3.3}\\\textbf{70B}} & 
% \textbf{GPT 4o} & 
% \makecell{\textbf{o1}\\\textbf{-mini}} \\
% \midrule
% \multirow{3}{*}{\textbf{ProofNet}} 
%  & Base   & 2.69 & 2.69 & 3.76 & 2.15 & 3.23 & 3.76 \\
%  & RAG    & 3.76 & tbd & 3.76 & 3.76  & 5.38 & 5.91 \\
%  & Graphs & 4.84 & tbd & 4.30  & 4.30  & 6.45 & 6.99 \\
% \midrule
% \multirow{3}{*}{\textbf{miniF2F}} 
%  & Base   & 22.95 & 20.08 & 20.49 & 25.00 & 23.36 & 23.77 \\
%  & RAG    & 28.69 & 22.54 & 24.59 & 24.59  & 28.69 & 28.28 \\
%  & Graphs & 31.15 & tbd & 31.97  & 30.74  & 30.74 & 30.74 \\
% \midrule
% \multirow{3}{*}{\textbf{mustard-sauce}} 
%  & Base   & tbd & tbd & tbd & tbd & tbd & tbd \\
%  & RAG    & tbd & tbd & tbd & tbd & tbd & tbd \\
%  & Graphs & tbd & tbd & tbd & tbd & tbd & tbd \\
% \bottomrule
% \end{tabular}
% \caption{Comparison of models across ProofNet, miniF2F, and mustard-sauce datasets for Base, RAG, and Graphs methods. Accuracy scores reflect performance of a single run with a maximum of three attempts per proof. (measured in \%)}
% \label{tab:model_dataset_accuracy}
% \end{table*}

\begin{table*}[h!]
\centering
\begin{tabular}{l l|c c c c c c}
\toprule
\textbf{Dataset ($\uparrow$)} & \textbf{Method} & \makecell{\textbf{Claude 3.5}\\\textbf{Sonnet}} & 
\makecell{\textbf{Deepseek}\\\textbf{R1}} & 
\makecell{\textbf{Llama 3.1}\\\textbf{8B}} & 
\makecell{\textbf{Llama 3.3}\\\textbf{70B}} & 
\textbf{GPT 4o} & 
\makecell{\textbf{o1}\\\textbf{-mini}} \\
\midrule
\multirow{3}{*}{\textbf{ProofNet}} 
 & Base   & 2.69\% & 2.69\% & 3.76\% & 2.15\% & 3.23\% & 3.76\% \\
 & RAG    & 3.76\% & 3.76\%& 3.76\% & 3.76\%  & 5.38\% & 5.91\% \\
 & Graphs & 4.84\% & 5.38\%& 4.30\%  & 4.30\%  & 6.45\% & \textbf{6.99\%} \\
\midrule
\multirow{3}{*}{\textbf{miniF2F}} 
 & Base   & 22.95\% & 20.08\% & 20.49\% & 25.00\% & 23.36\% & 23.77\% \\
 & RAG    & 28.69\% & 22.54\% & 24.59\% & 24.59\%  & 28.69\% & 28.28\% \\
 & Graphs & 31.15\% & 28.28\%& \textbf{31.97\%}  & 30.74\%  & 30.74\% & 30.74\% \\
\midrule
\multirow{3}{*}{\textbf{MUSTARDSAUCE}} 
 & Base   & 28.00\%& 20.00\%& 24.00\%& 25.60\%& 28.00\%& 24.80\%\\
 & RAG    & 28.40\%& 25.00\%& 28.00\%& 28.8\%& 28.00\%& 26.80\%\\
 & Graphs & 30.00\%& 27.00\%& 27.60\%& 32.5\%& 30.00\%& \textbf{34.00\%}\\
\bottomrule
\end{tabular}
\caption{Comparison of models across ProofNet, miniF2F, and MUSTARDSAUCE datasets. Accuracy scores reflect performance of a single run with a maximum of three attempts per proof, measured as percentage of successful proof generations. The bolded numbers show the largest performance gain from baseline to knowledge graphs for each dataset, achieving more than 11\% gain.}
\label{tab:model_dataset_accuracy}
\end{table*}


\section{Results}

\subsection{Model Performance}
As visualized in Table \ref{tab:model_dataset_accuracy}, knowledge graphs consistently outperform baseline proof-systems and over Retrieval Augmented Generation. Performance gains of knowledge graphs ranged from 2-11\% across different models. Notionally, Llama 3.1 8B achieved 31.97\% success rate on miniF2F, compared to 20.49\% baseline.

ProofNet represents the most challenging dataset with lowest overall performance (2-7\% success rates). This can be attributed to the difficulty of the problems. They require higher abstract mathematical reasoning and more intricate proof structures. The miniF2F dataset showed moderate performance (20-31\% success) because it includes more structured mathematical problems, intermediate complexity of proofs, and more predictable reasoning patterns.

MUSTARDSAUCE demonstrated moderate performance as well (24-34\% success). MUSTARDSAUCE was created by prompting GPT-4 on different levels of mathematics (from elementary to college level) and on different fields \citep{huang2024mustardmasteringuniformsynthesis}. Since these problems were created by GPT-4, there may be inherent biases that reflects GPT-4's internal reasoning patterns and align with GPT-4o's problem-solving approach. Thus, this dataset is potentially optimized for large language model reasoning.

% \subsubsection{Comparative Analysis Factors}
% Performance variations can be explained by:
% \begin{enumerate}
% \item Model architectural differences
% \item Training data diversity
% \item Semantic reasoning capabilities
% \item Knowledge graph construction complexity
% \item Problem representation strategies
% \end{enumerate}

However, it is important to note that we only ran a single run. A difference of a percentage point could be due to statistical variance or model initialization randomness.

\subsection{Graph Networks}

Our framework successfully constructed a knowledge graph comprising 60,535 nodes and 305,452 relationships, implemented into an easily reproducible framework towards proof retrieval\footnote{Our code can be accessed via [ANONYMIZED FOR REVIEW]}.


\subsection{Failure Scenarios}

Although we see strong performance across multiple proof benchmarks, there are certain scenarios in which models \& techniques fail to function optimally. Across multiple runs, we found the following possible errors: 
\begin{itemize}
    \item The informal proof is correct but the conversion into a formal proof fails.
    \item The required knowledge is not in the graph and other topics are too briefly related to extrapolate.
\end{itemize}

Through manual analysis we observed that 35\% of the questions fail because the formal proof is incorrect even when informal proof is correct. By examining specific questions, we find that informal English language proofs often contain implicit assumptions, and use high-level reasoning, whereas Lean 4 demands explicit steps, well-defined quantifiers, and precise theorem applications. Common errors include missing hypotheses, ambiguous references to theorems, and incorrect translations of algebraic manipulations or induction arguments. Additionally, informal proofs tend to use flexible language constructs, such as "it follows that" or "by symmetry," which lack direct formal counterparts. These issues indicate that these failures are often not due to a lack of mathematical knowledge but rather the inability to impose a structured, machine-verifiable logic onto loosely written informal reasoning.

It is rare that traversal doesn't gather relevant information or that the knowledge is not available and only apparent on particularly hard questions. 
% However, for difficult questions, such as those proposed by the International Math Olympiad, the graph cannot find the most relevant nodes.

\section{Additional Studies}

\subsection{Best of N Tree Search}
The system generates multiple candidates for each math problem by using parallel generations with slightly varied sampling parameters. A dedicated GPT-4o model acts as a judge, evaluating each candidate proof across dimensions of mathematical correctness, clarity, and reasoning completeness. The judge assigns scores from 0-10 and provides justification for each evaluation. Candidates are then sorted by their scores, with the highest-scoring proof selected as the optimal solution.

The tree search process begins by generating an initial beam of candidate proofs. For each candidate, the system attempts formal Lean verification and generates refinements based on verification feedback. These refinements are then scored and ranked, with the top k candidates retained for subsequent iterations. The process repeats for a predetermined number of depths, ultimately returning the best overall candidate proof.

On GPT-4o our run of the best of n tree search yielded an accuracy of \textbf{45\%} on the minif2f benchmark indicating that the performance of formalizations in the context of our Graph system can be significantly improved by integrating a judging process into the proof framework.

\subsection{Traversal Depth}
To allow the model for failure correction and improvement we give the Graph system, three consecutive attempts defined as $r$. Each attempt allows the model to traverse further in Graph and explore more nodes. As more Proofs get injected into the context and the model gets more tries to correct initial mistakes the accuracy scales higher per iterative refinement step. This effect is most predominant in smaller parameter models, such as Llama 3.3 8b. This behavior is captured in Table \ref{table:results_per_attempt}. 

% \begin{table}[h!tbp]
%     \centering

%     \begin{tabular}{l|c|c|c}
%         \toprule
%         \textbf{Model} & \textbf{$r = 1$} & \textbf{$r = 2$} & \textbf{$r = 3$}  \\
%         \midrule
%         Claude 3.5  Sonnet    & tbd            & tbd            & tbd              \\
%         Deepseek R1    & tbd            & tbd            & tbd                      \\
%         Llama 3.1 8B   & tbd            & tbd            & tbd                       \\
%         Llama 3.3 70B  & tbd            & tbd            & tbd                        \\
%         GPT 4o             & tbd            & tbd            & tbd                     \\
%         o1-mini        & tbd            & tbd            & tbd                          \\
%         \bottomrule
%     \end{tabular}
%     \caption{Graph Model Performance in minif2f, measured in percent for each retry.}
%     \label{table:results_per_attempt}
% \end{table}

\begin{table}[h!tbp]
    \centering

    \begin{tabular}{l|c|c|c}
        \toprule
        \textbf{Dataset ($\uparrow$) } & \textbf{$r = 1$} & \textbf{$r = 2$} & \textbf{$r = 3$}  \\
        \midrule
        ProofNet& 2.96\%& 4.17\%& 5.38\%\\
        minif2f& 24.39\%& 29.20\%& 30.60\%\\
        MUSTARDSAUCE   & 18.16\%& 26.48\%& 30.18\%\\
        \bottomrule
    \end{tabular}
    \caption{Performance on all datasets, measured in percent for each retry, averaged over all models.}
    \label{table:results_per_attempt}
\end{table}



% \subsection{Qualitative Analysis}
% \subsubsection{Node retrieval}

% \subsubsection{Proof generation}
% The conversion into Lean4 with the AutoFormalizer often requires specific formulations. With the loose formulations often employed by Models, the conversion into Lean4 failed, due missing information constraints or lacking context at times. We exhibit that, with the use of Nodes integrated into the system, these limitations diminish.

\section{Conclusion \& Discussion}
We present a framework that automates mathematical proof generation by integrating LLMs with a knowledge graph to utilize inter-dependencies across mathematical proofs. Our approach demonstrates the potential of combining multiple mathematical concepts in an intertwined graph and show that by doing so, language models can be effectively guided towards correct proof generation. This leads them to exhibit improved accuracy and enhanced abilities in formalizing proofs according to standards such as Lean4.

We establish that existing foundation models can achieve similar or higher performing results as finetuned ones, by simple context injections of related concepts during inference time, without requiring any additional pretraining, expert iteration or training system of any kind.


\section{Limitations}
Despite the advancements in capturing semantic relationships in text via vectorized embeddings, embeddings can potentially suffer from issues such as loss of fine-grained logical structure, and difficulties in preserving contextual dependencies across larger passages. This can lead to challenges in accurately retrieving relevant mathematical statements, especially in formalized settings where precise definitions and logical consistency are crucial. In addition, while we filter and discard irrelevant details, signs and other minutiae, XML dumps such as the collected one from ProofWiki can introduce noise that might disrupt of affect the semantic search and embeddings.

While our approach successfully formalizes proofs from structured datasets, its performance on entirely novel or highly abstract mathematical problems remains uncertain. Models trained on existing proofs may struggle with creative problem-solving or unconventional mathematical approaches.

Large Language Models have finite context windows, meaning lengthy or complex proofs may exceed the model's processing capacity. This might result in incomplete reasoning, loss of critical details, or forgetting earlier steps in multi-stage proofs.

Future work may enhance the knowledge graph and improving the autoformalization process to handle more complex mathematical concepts.

\section{Reproducibility Statement}
Our experiments were conducted using publicly available Datasets and Models. GPT-4o, 4o-mini, o1-mini and text-embedding-3-large can be accessed via \url{https://openai.com/api/}. Both Deepseek-R1 and the LLAMA 3 collection are open-sourced models. Claude models can be accessed via their respective api endpoits, under \url{https://www.anthropic.com/api}. 

\hyperlink{https://github.com/zhangir-azerbayev/ProofNet}{ProofNet} and \hyperlink{https://github.com/openai/miniF2F}{miniF2F}, and \hyperlink{https://github.com/Eleanor-H/MUSTARD}{MUSTARDSAUCE} are publicly available datasets. Our Code is publicly available on GitHub, we encourage anyone to validate and extend our findings.
The used Neo4j-based graph database can be used under \url{https://neo4j.com} and could potentially be replaced with alternative graph databases as desired.


\section{Ethical Considerations \& Risks}
Our knowledge base is derived from ProofWiki, an open database for formal proofs. While the page is moderated, adversaries could attempt to incorporate harmful content or incorrect factual information into the extracted pages. However, we consider this risk to be unlikely.

Although alignment work continues to progress Large Language Models can introduce biases towards certain marginalized groups or other minorities. All of our introduced models are moderated and have content filters that should prevent models from generating harmful content. However said filters aren't perfect, models can still be exploited via sophisticated prompting and other adversarial techniques. Given our contribution in the framework, we expect no increased risk in any of the given safety evaluation measured proposed.

\subsection{GPU usage}

\begin{table}[h]
    \centering
    \begin{tabular}{c|c|c}
        GPU model & Watts & approx. usage Time \\
        \midrule
        Nvidia A40 & 300 W & 300 hours \\
        Nvidia RTX A5000 & 300 W & 50 hours \\
    \end{tabular}
    \caption{Estimated GPU usage for all Evaluations.}
    \label{tab:my_label}
\end{table}

The shown GPU usage may only partially reflect and accurate measure of the computational resources required, as major models are only available through API endpoints. We estimate the inference time on said APIs to roughly 150 hours.




% \bibliographystyle{plainnat}
\bibliography{custom}

\clearpage

\appendix

\section{Introduced Hyperparameters}
For our evaluations we introduce multiple parameters that can be varied.

In our evaluations: 
\begin{itemize}
    \item $r$ signifies the provided amount of attempts on one individual proof.
    \item $d$ defines the depth the retriever is allowed to traverse in the knowledge graph.
    \item $k$ signifies the amount of selected nodes from the current depth descending based on semantic similarity.
\end{itemize}

For our experiments we evaluate three attempts, with a max depth set to $d=2$ and a $top-k=5$ or smaller depending on the amount of related nodes.

\section{Structural Improvement}
Few shot learning, even with briefly related examples has shown to improve performance across a variety of tasks and domains. 

Therefore we hypothesize that even only partly related proof nodes will improve the not only the proof understanding, but will also benefit the structured formalization that is required for the correct interpretation and conversion of informal natural language into lean4.
% To test this theory we select 3 unaligned arbitrary nodes out of the Graph and insert them manually into the prompt. Then we evaluate our prompt following the baseline without retrieval.

% \begin{table}[h!]
%     \centering
%     \begin{tabular}{l|c|c}
%         \toprule
%         \textbf{Model} & \textbf{Baseline} & \textbf{few-shot} \\
%         \midrule
%         Claude 3.5  Sonnet    & tbd            & tbd                                 \\
%         Deepseek R1    & tbd            & tbd                                \\
%         Llama 3 450B   & tbd            & tbd                                  \\
%         Llama 3.3 70B  & tbd            & tbd                               \\
%         o1             & tbd            & tbd                                \\
%         o1-mini        & tbd            & tbd                            \\
%         \bottomrule
%     \end{tabular}
%     \caption{Model few shot performance compared to baseline}
% \end{table}

\section{Deterministic Evaluations}
Unless specified otherwise we use greedy decoding for all of our experiments. Additionally the semantic search in our Graph knowledge base will yield identical outputs, given that the input doesn't change in between different runs. 

While this behavior can be favorable in some situations, other evaluations may benefit from slight variations in different seeds. To introduce a slight stochasticity other evaluation may vary the temperature parameter of the employed models, and use the introduced method in Appendix \ref{appendix:knowledgeGraphStochastic} to introduce randomness into our knowledge graph.

\subsection{Knowledge Graph Stochasticity}
\label{appendix:knowledgeGraphStochastic}
To mitigate fully repetitive outputs Nodes from the knowledge graph we propose top-k shuffling, where we retrieve the k-highest ranked nodes, shuffle them, and select a subset. This method ensures diversity in individual generations. We favor this implementation over random sampling over a broader set of candidate nodes, selecting from a pool beyond the strict top-k. Due to the potentially less relevant knowledge, trading off precision for increased coverage.

The level of stochasticity can be tuned dynamically based on confidence scores or response variance metrics


\section{Node example}
\label{appendix:nodes_example}
% \begin{verbatim}
% 166, def, Definition:Set, Set, 
% "Definition content..."
% \end{verbatim}

\begin{itemize}
    \item \textbf{from\_id}: The ID of the current node.
    \item \textbf{to\_id}: The ID of the linked node (found using the title-name-to-ID mapping).
    \item \textbf{type}: There are 6 different relationship categories: 
    \end{itemize}
    \begin{verbatim}
    USES_DEFINITION, 
    RELATED_DEFINITION, 
    USES_AXIOM, 
    SIMILAR_PROOF, 
    PROOF_DEPENDENCY, 
    PROOF_TECHNIQUE.
    \end{verbatim}
An example from our relationships collection:

\begin{verbatim}
from_id, to_id, type
149, 167, LINK
149, 41289, PROOF_TECHNIQUE
67015,6780, USES_DEFINITION
\end{verbatim}

\section{Prompt Examples}
\subsection{Prompt Example 1}

 The model was provided with the informal proof and a code template, and it generated the corresponding formal proof in Lean 4. Each element was processed to extract the title, namespace, and content.

\label{appendix:prompt_example}

\begin{verbatim} 
You are a Lean 4 code generator. 
We have:
HEADER:
{header}

INFORMAL PROOF:
{informal_proof}

PREFIX:
{informal_prefix}

STATEMENT:
{formal_statement}

GOAL (optional):
{goal}

INSTRUCTIONS:
1. Output exactly one triple-backtick code 
block containing valid Lean 4 code.
2. Do not include any text or explanations 
outside the code block.
3. Make sure it compiles in Lean 4.

Required Format:
# Start
```lean4
<Lean code here>
```  
# End
\end{verbatim}

\subsection{Prompt Example 2}
\label{appendix:prompt_example2}
\begin{verbatim} 
You are a mathematics expert focused on 
generating clear informal proofs.

Given the following mathematical problem 
and context, generate a clear and detailed 
informal proof in natural language.

Context: [Retrieved context]

Problem: [Problem statement]

Provide your proof in the following format:

Informal Proof:
[Your proof here] \end{verbatim}

\section{Graph Dataset}
We parsed an XML dump of ProofWiki, where each \texttt{<page>} element was processed. Irrelevant sections were filtered, and the wikitext was cleaned to obtain structured content.
\subsection{Node structure}
\label{appendix:dataset_sample}
We represented each mathematical concept as a node in the knowledge graph, storing attributes such as:

\begin{itemize} 
    \item \textbf{id}: Unique identifier. 
    \item \textbf{type}: Content type (e.g., definition, theorem). 
    \item \textbf{title}: Page title. 
    \item \textbf{name}: Extracted from the title. 
    \item \textbf{content}: Main text content. 
\end{itemize}

\section{Benchmarks}
\label{appendix:benchmarks}
By utilizing miniF2F, ProofNet amd MUSTARDSAUCE, we assess our framework's ability to generate and formalize proofs across diverse mathematical problems. The datasets provided a standardized evaluation setting, allowing us to compare our results uniformly with existing approaches and to analyze the strengths and limitations of our Method. However it is possible that our setup deviates from the ones introduced in the respective papers of the dataset, which explains a varied performance across tasks, which is especially apparent on MUSTARDSAUCE. To setup a comparable evaluation, we compute the baseline of our setup as well rather then taking the previous State-of-the-Art. 
\subsection{Used splits}
We ran on 186 problems from the test split of ProofNet, 244 problems from the test split of miniF2F, and randomly selected 250 theorem proving problems from MUSTARDSAUCE.

\end{document}