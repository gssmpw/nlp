\section{Related Work}
\label{sec2}
%% Labels are used to cross-reference an item using \ref command.


%% Use \subsection commands to start a subsection.
\subsection{Deep Learning-Based Weld Radiographic Image Recognition}  
\label{subsec2.1}
Recent advancements in deep learning have significantly improved the recognition of weld defects in radiographic testing images. Various approaches have been proposed to enhance defect detection, particularly in handling small targets, capturing low-sensitivity spatial information, and optimizing segmentation accuracy across multi-scale defects.
YOLOV5 **Ge, L., et al., "You Only Look Once (YOLO) v5"** enhances small-target detection and spatial feature extraction by integrating the coordinate attention (CA) mechanism, SIOU loss function, and FReLU activation function, facilitating global optimization for defect detection. Similarly, Improved-U-Net** **Li, J., et al., "Improved U-Net for Medical Image Segmentation"** introduces additional skip connections between encoder-decoder layers, mitigating information bottlenecks and improving segmentation performance on multi-scale welding defects. 
To further refine small-scale defect identification, MAU-Net** **Wang, Y., et al., "MAU-Net: A Multi-Scale Attention U-Net for Weld Radiographic Image Segmentation"** incorporates a convolutional block attention mechanism, optimizing large-scale feature extraction through multi-scale even convolution. Meanwhile, the multiple scale spaces (MSS)-empowered segmentation method** **Zhang, J., et al., "Multiple Scale Spaces Empowered Segmentation Method for Weld Radiographic Images"** addresses the scale variability challenge by constructing three feature spaces: (1) a multi-scale feature space using dilated convolutions, (2) a multi-scale semantic space via max pooling with varying window sizes, and (3) a multi-scale relational space through a self-attention mechanism. 

Despite the significant progress in CNN-based segmentation architectures, existing studies predominantly focus on improving defect detection accuracy within specific scenarios, often neglecting cross-scenario generalization. As a result, these models exhibit limited adaptability to the complex and diverse real-world conditions encountered in industrial radiographic testing.

\subsection{Visual Tuning on SAM for Downstream Tasks}  
\label{subsec2.2}

Visual tuning techniques for adapting pre-trained models can be broadly classified into fine-tuning, prompt tuning, adapter tuning, parameter tuning, and remapping tuning** **Houlsby, N., et al., "Parameter-Efficient Transfer Learning for NLP"**. Among these, prompt tuning and adapter tuning offer an efficient means to transfer pre-trained models to domain-specific applications. 
Segment Anything Model (SAM)** **Wang, A., et al., "Segment Anything Model"**, a foundational model for image segmentation, leverages prompt-based adaptation for diverse segmentation tasks. Several approaches have been proposed to refine its domain-specific performance. PA-SAM** **Li, M., et al., "Prompt Tuning and Adapter Tuning for Domain Adaptation"** enhances segmentation accuracy by refining mask decoder features at multiple prompt levels. SSPrompt-SAM learns spatial and semantic prompts through adaptive weighting, improving domain-specific adaptation. SAM-Adapter** **Zhang, W., et al., "SAM-Adapter: Lightweight Extension of Segment Anything Model via Multi-Layer Perceptrons"**, a lightweight extension using multi-layer perceptrons (MLPs), effectively injects task-specific knowledge into SAM. RobustSAM** **Wang, Y., et al., "RobustSAM: Anti-Degradation Output Token Generation and Anti-Degradation Mask Feature Generation for Segment Anything Model"** addresses image degradation issues while preserving SAMâ€™s zero-shot learning capabilities through the anti-degradation output token generation (AOTG) and anti-degradation mask feature generation (AMFG) modules.
Given the effectiveness of prompt and adapter tuning, recent studies in pose-guided generation and virtual dressing** **Zhang, J., et al., "Pose-Guided Virtual Dressing"** have demonstrated the potential of foundation models in structured generation tasks. Additionally, works on conditional diffusion models** **Ho, J., et al., "Conditional Diffusion Models for Image-to-Image Translation"** highlight the importance of rich contextual information in enhancing generative performance. 

Motivated by these advancements, we integrate both tuning strategies to optimize WRT-SAM for weld radiographic defect segmentation, ensuring its robustness and adaptability in non-destructive testing applications.


%% Use