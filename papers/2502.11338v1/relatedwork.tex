\section{Related Work}
\label{sec2}
%% Labels are used to cross-reference an item using \ref command.


%% Use \subsection commands to start a subsection.
\subsection{Deep Learning-Based Weld Radiographic Image Recognition}  
\label{subsec2.1}
Recent advancements in deep learning have significantly improved the recognition of weld defects in radiographic testing images. Various approaches have been proposed to enhance defect detection, particularly in handling small targets, capturing low-sensitivity spatial information, and optimizing segmentation accuracy across multi-scale defects.
YOLOV5 ~\cite{xu2023defect} enhances small-target detection and spatial feature extraction by integrating the coordinate attention (CA) mechanism, SIOU loss function, and FReLU activation function, facilitating global optimization for defect detection. Similarly, Improved-U-Net~\cite{yang2021automatic} introduces additional skip connections between encoder-decoder layers, mitigating information bottlenecks and improving segmentation performance on multi-scale welding defects. 
To further refine small-scale defect identification, MAU-Net~\cite{wang2024new} incorporates a convolutional block attention mechanism, optimizing large-scale feature extraction through multi-scale even convolution. Meanwhile, the multiple scale spaces (MSS)-empowered segmentation method~\cite{liu2023multiple} addresses the scale variability challenge by constructing three feature spaces: (1) a multi-scale feature space using dilated convolutions, (2) a multi-scale semantic space via max pooling with varying window sizes, and (3) a multi-scale relational space through a self-attention mechanism. 

Despite the significant progress in CNN-based segmentation architectures, existing studies predominantly focus on improving defect detection accuracy within specific scenarios, often neglecting cross-scenario generalization. As a result, these models exhibit limited adaptability to the complex and diverse real-world conditions encountered in industrial radiographic testing.

\subsection{Visual Tuning on SAM for Downstream Tasks}  
\label{subsec2.2}

Visual tuning techniques for adapting pre-trained models can be broadly classified into fine-tuning, prompt tuning, adapter tuning, parameter tuning, and remapping tuning~\cite{yu2024visual}. Among these, prompt tuning and adapter tuning offer an efficient means to transfer pre-trained models to domain-specific applications. 
Segment Anything Model (SAM)~\cite{kirillov2023segment}, a foundational model for image segmentation, leverages prompt-based adaptation for diverse segmentation tasks. Several approaches have been proposed to refine its domain-specific performance. PA-SAM~\cite{xie2024pa} enhances segmentation accuracy by refining mask decoder features at multiple prompt levels. SSPrompt-SAM learns spatial and semantic prompts through adaptive weighting, improving domain-specific adaptation. SAM-Adapter~\cite{chen2023sam}, a lightweight extension using multi-layer perceptrons (MLPs), effectively injects task-specific knowledge into SAM. RobustSAM~\cite{chen2024robustsam} addresses image degradation issues while preserving SAMâ€™s zero-shot learning capabilities through the anti-degradation output token generation (AOTG) and anti-degradation mask feature generation (AMFG) modules.
Given the effectiveness of prompt and adapter tuning, recent studies in pose-guided generation and virtual dressing~\cite{shen2024imagpose, shen2024imagdressing} have demonstrated the potential of foundation models in structured generation tasks. Additionally, works on conditional diffusion models~\cite{shen2023advancing, shen2024boosting} highlight the importance of rich contextual information in enhancing generative performance. 

Motivated by these advancements, we integrate both tuning strategies to optimize WRT-SAM for weld radiographic defect segmentation, ensuring its robustness and adaptability in non-destructive testing applications.


%% Use