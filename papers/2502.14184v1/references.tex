@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@article{ripley1977modelling,
  title={Modelling spatial patterns},
  author={Ripley, Brian D},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  volume={39},
  number={2},
  pages={172--192},
  year={1977},
  publisher={Wiley Online Library}
}
@article{getis1984interaction,
  title={Interaction modeling using second-order analysis},
  author={Getis, Arthur},
  journal={Environment and Planning A},
  volume={16},
  number={2},
  pages={173--183},
  year={1984},
  publisher={SAGE Publications Sage UK: London, England}
}
@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

@article{marcon2009generalizing,
  title={Generalizing Ripley's K function to inhomogeneous populations},
  author={Marcon, Eric and Puech, Florence},
  year={2009}
}
@article{dixon2012ripley,
  title={Ripley’s K function},
  author={Dixon, Philip M and El-Shaarawi, Abdel H and Piegorsch, Walter W},
  journal={Encyclopedia of Environmetrics},
  volume={3},
  year={2012},
  publisher={John Wiley \& Sons Ltd.}
}

@techreport{burns2012description,
  title={Description of the tritium-producing burnable absorber rod for the commercial light water reactor TTQP-1-015 Rev 19},
  author={Burns, Kimberly A and Love, Edward F and Thornhill, Cheryl K},
  year={2012},
  institution={Pacific Northwest National Lab.(PNNL), Richland, WA (United States)}
}
@article{pazdernik2020microstructural,
  title={Microstructural classification of unirradiated LiAlO2 pellets by deep learning methods},
  author={Pazdernik, Karl and LaHaye, Nicole L and Artman, Conor M and Zhu, Yuanyuan},
  journal={Computational Materials Science},
  volume={181},
  pages={109728},
  year={2020},
  publisher={Elsevier}
}

@article{senor2018science,
  title={Science and technology in support of the tritium sustainment program},
  author={Senor, DJ},
  journal={PNNL report, PNNL-27216},
  year={2018}
}

@article{kendall_bayesian_2016,
	title = {Bayesian {SegNet}: {Model} {Uncertainty} in {Deep} {Convolutional} {Encoder}-{Decoder} {Architectures} for {Scene} {Understanding}},
	shorttitle = {Bayesian {SegNet}},
	url = {http://arxiv.org/abs/1511.02680},
	abstract = {We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixelwise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3\% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a signiﬁcant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets.},
	language = {en},
	urldate = {2020-09-14},
	journal = {arXiv:1511.02680 [cs]},
	author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
	month = oct,
	year = {2016},
	note = {arXiv: 1511.02680},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {Kendall et al. - 2016 - Bayesian SegNet Model Uncertainty in Deep Convolu.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\993KE9E2\\Kendall et al. - 2016 - Bayesian SegNet Model Uncertainty in Deep Convolu.pdf:application/pdf},
}

@article{badrinarayanan_segnet_2016,
	title = {{SegNet}: {A} {Deep} {Convolutional} {Encoder}-{Decoder} {Architecture} for {Image} {Segmentation}},
	shorttitle = {{SegNet}},
	url = {http://arxiv.org/abs/1511.00561},
	abstract = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classiﬁcation layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1]. The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classiﬁcation. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Speciﬁcally, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable ﬁlters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3], DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efﬁcient both in terms of memory and computational time during inference. It is also signiﬁcantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efﬁcient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
	language = {en},
	urldate = {2020-09-14},
	journal = {arXiv:1511.00561 [cs]},
	author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
	month = oct,
	year = {2016},
	note = {arXiv: 1511.00561},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Badrinarayanan et al. - 2016 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\KGAKXYBS\\Badrinarayanan et al. - 2016 - SegNet A Deep Convolutional Encoder-Decoder Archi.pdf:application/pdf},
}

@article{tao_hierarchical_2020,
	title = {Hierarchical {Multi}-{Scale} {Attention} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/2005.10821},
	abstract = {Multi-scale inference is commonly used to improve the results of semantic segmentation. Multiple images scales are passed through a network and then the results are combined with averaging or max pooling. In this work, we present an attention-based approach to combining multi-scale predictions. We show that predictions at certain scales are better at resolving particular failures modes, and that the network learns to favor those scales for such cases in order to generate better predictions. Our attention mechanism is hierarchical, which enables it to be roughly 4x more memory efficient to train than other recent approaches. In addition to enabling faster training, this allows us to train with larger crop sizes which leads to greater model accuracy. We demonstrate the result of our method on two datasets: Cityscapes and Mapillary Vistas. For Cityscapes, which has a large number of weakly labelled images, we also leverage auto-labelling to improve generalization. Using our approach we achieve a new state-of-the-art results in both Mapillary (61.1 IOU val) and Cityscapes (85.1 IOU test).},
	urldate = {2020-09-14},
	journal = {arXiv:2005.10821 [cs]},
	author = {Tao, Andrew and Sapra, Karan and Catanzaro, Bryan},
	month = may,
	year = {2020},
	note = {arXiv: 2005.10821
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\UELNPJGX\\2005.html:text/html;arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\4FBLSF2N\\Tao et al. - 2020 - Hierarchical Multi-Scale Attention for Semantic Se.pdf:application/pdf},
}

@inproceedings{krishnan_efficient_2019,
	address = {Seoul, Korea (South)},
	title = {Efficient {Priors} for {Scalable} {Variational} {Inference} in {Bayesian} {Deep} {Neural} {Networks}},
	isbn = {978-1-72815-023-9},
	url = {https://ieeexplore.ieee.org/document/9022308/},
	doi = {10.1109/ICCVW.2019.00102},
	abstract = {Stochastic variational inference for Bayesian deep neural networks (DNNs) requires specifying priors and approximate posterior distributions for neural network weights. Specifying meaningful weight priors is a challenging problem, particularly for scaling variational inference to deeper architectures involving high dimensional weight space. Based on empirical Bayes approach, we propose Bayesian MOdel Priors Extracted from Deterministic DNN (MOPED) method to choose meaningful prior distributions over weight space using deterministic weights derived from the pretrained DNNs of equivalent architecture. We empirically evaluate the proposed approach on real-world applications including image classiﬁcation, video activity recognition and audio classiﬁcation tasks with varying complex neural network architectures. The proposed method enables scalable variational inference with faster training convergence and provides reliable uncertainty quantiﬁcation.},
	language = {en},
	urldate = {2020-09-11},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} {Workshop} ({ICCVW})},
	publisher = {IEEE},
	author = {Krishnan, Ranganath and Subedar, Mahesh and Tickoo, Omesh},
	month = oct,
	year = {2019},
	pages = {773--777},
	file = {Krishnan et al. - 2019 - Efficient Priors for Scalable Variational Inferenc.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\7WFBHDXJ\\Krishnan et al. - 2019 - Efficient Priors for Scalable Variational Inferenc.pdf:application/pdf},
}

@article{li_automated_2018,
	title = {Automated defect analysis in electron microscopic images},
	volume = {4},
	copyright = {2018 The Author(s)},
	issn = {2057-3960},
	url = {https://www.nature.com/articles/s41524-018-0093-8},
	doi = {10.1038/s41524-018-0093-8},
	abstract = {Electron microscopy and defect analysis are a cornerstone of materials science, as they offer detailed insights on the microstructure and performance of a wide range of materials and material systems. Building a robust and flexible platform for automated defect recognition and classification in electron microscopy will result in the completion of analysis orders of magnitude faster after images are recorded, or even online during image acquisition. Automated analysis has the potential to be significantly more efficient, accurate, and repeatable than human analysis, and it can scale with the increasingly important methods of automated data generation. Herein, an automated recognition tool is developed based on a computer vison–based approach; it sequentially applies a cascade object detector, convolutional neural network, and local image analysis methods. We demonstrate that the automated tool performs as well as or better than manual human detection in terms of recall and precision and achieves quantitative image/defect analysis metrics close to the human average. The proposed approach works for images of varying contrast, brightness, and magnification. These promising results suggest that this and similar approaches are worth exploring for detecting multiple defect types and have the potential to locate, classify, and measure quantitative features for a range of defect types, materials, and electron microscopic techniques.},
	language = {en},
	number = {1},
	urldate = {2020-09-16},
	journal = {npj Computational Materials},
	author = {Li, Wei and Field, Kevin G. and Morgan, Dane},
	month = jul,
	year = {2018},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {1--9},
	file = {Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\V24YDPRA\\Li et al. - 2018 - Automated defect analysis in electron microscopic .pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\SXD97BLP\\s41524-018-0093-8.html:text/html},
}

@article{grimes_explanation_nodate,
	title = {Explanation of {Unintended} {Radiated} {Emission} {Classification} via {LIME}},
	abstract = {Unintended radiated emissions arise during the use of electronic devices. Identifying and mitigating the effects of these emissions is a key element of modern power engineering and associated control systems. Signal processing of the electrical system can identify the sources of these emissions. A dataset known as Flaming Moes includes captured unintended radiated emissions from consumer electronics. This dataset was analyzed to construct next-generation methods for device identification. To this end, a neural network based on applying the ResNet-18 image classification architecture to the short time Fourier transforms of short segments of voltage signatures was constructed. Using this classifier, the 18 device classes and background class were identified with close to 100 percent accuracy. By applying LIME to this classifier and aggregating the results over many classifications for the same device, it was possible to determine the frequency bands used by the classifier to make decisions. Using ensembles of classifiers trained on very similar datasets from the same parent data distribution, it was possible to recover robust sets of features of device output useful for identification. The additional understanding provided by the application of LIME enhances the trainability, trustability, and transferability of URE analysis networks.},
	language = {en},
	author = {Grimes, Tom F and Church, Eric D and Pitts, William K and Wood, Lynn S},
	pages = {7},
	file = {Grimes et al. - Explanation of Unintended Radiated Emission Classi.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\GTYDP3L9\\Grimes et al. - Explanation of Unintended Radiated Emission Classi.pdf:application/pdf},
}

@article{bernstein_comparison_2019,
	title = {A comparison of material flow strength models using {Bayesian} cross-validation},
	volume = {169},
	issn = {0927-0256},
	url = {http://www.sciencedirect.com/science/article/pii/S0927025619303891},
	doi = {10.1016/j.commatsci.2019.109098},
	abstract = {Predicting material flow strength over a range of conditions, such as temperature and strain rate, is necessary for many engineering applications. This paper considers how to compare the predictiveness of several different strength models using a statistical technique called Bayesian cross-validation. Given a dataset of flow strength measurements obtained from mechanictesting experiments, the procedure consists of performing a Bayesian calibration of each strength model on a subset of the data and evaluating how well the trained models predict the remaining data. The predictiveness of a calibrated strength model is quantifiable probabilistically, which provides an interpretable metric for comparing the different models. As an illustrative example, we compare the Johnson-Cook (JC), Zerilli-Armstrong (ZA), Preston-Tonks-Wallace (PTW), and Mechanical Threshold Stress (MTS) flow strength models for the tantalum stress-strain curve data from Chen and Gray (1996). We show that prediction intervals for the four strength models cover the held-out data at most experimental conditions, but also that prediction interval coverage and prediction uncertainty varies by model and experimental condition. The analysis further allows us to identify experimental regimes for which one of the strength models predicts better than the other three.},
	language = {en},
	urldate = {2020-10-12},
	journal = {Computational Materials Science},
	author = {Bernstein, Jason and Schmidt, Kathleen and Rivera, David and Barton, Nathan and Florando, Jeffrey and Kupresanin, Ana},
	month = nov,
	year = {2019},
	keywords = {Bayesian, Flow strength, Prediction, Strength model, Uncertainty quantification, Validation},
	pages = {109098},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\QQUU9FEH\\Bernstein et al. - 2019 - A comparison of material flow strength models usin.pdf:application/pdf},
}

@article{pazdernik_microstructural_2020,
	title = {Microstructural classification of unirradiated {LiAlO2} pellets by deep learning methods},
	volume = {181},
	issn = {0927-0256},
	url = {http://www.sciencedirect.com/science/article/pii/S0927025620302196},
	doi = {10.1016/j.commatsci.2020.109728},
	abstract = {Microstructural features and defects can greatly impact material properties and performance in a wide range of application areas. Recognition and characterization of microstructural features is essential to the understanding and prediction of material performance under various operational conditions, including irradiation. In this work, we tested a collection of Deep Convolutional Neural Network (DCNN) architectures that have been optimized for image segmentation and selected the best performer to obtain pixel-level classification of the main microstructural features in unirradiated LiAlO2 pellets, including grains, grain boundaries, voids, precipitates, and zirconia impurities. LiAlO2 is an important material that is used as a tritium producer for the Tritium Sustainment Program. While LiAlO2 pellets have been employed in tritium-producing burnable absorber rods (TPBARs) for years, comprehensive microstructural analysis of unirradiated LiAlO2, and therefore time-dependent tritium release from the material during irradiation, has not been established. A full understanding of unirradiated LiAlO2 microstructure and how it evolves as a result of neutron irradiation is necessary to produce an integrated performance model to predict in-reactor behavior as well as to target strategic experiments. This work aims at developing a fast and quantitative analysis method to classify various microstructural features in unirradiated LiAlO2 pellets that are visualized by scanning electron microscopy (SEM). Given classification results obtained, statistical analysis was then carried out to evaluate the performance of the DCNN classification and to describe the properties of the microstructural features as a whole, based on standard aggregation and spatial point-process methodology. Our results show improved performance over a baseline heuristic approach. Also, the computational efficiency of the computer-aided analytical method allows for quantitative characterization of a larger volume of SEM images than was previously possible using manual segmentation.},
	language = {en},
	urldate = {2020-11-17},
	journal = {Computational Materials Science},
	author = {Pazdernik, Karl and LaHaye, Nicole L. and Artman, Conor M. and Zhu, Yuanyuan},
	month = aug,
	year = {2020},
	keywords = {Deep convolutional neural network, Image segmentation, Scanning electron microscopy, Spatial point process},
	pages = {109728},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\UCI84R2M\\Pazdernik et al. - 2020 - Microstructural classification of unirradiated LiA.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\IHLTT652\\S0927025620302196.html:text/html},
}

@article{si_material_2019,
	title = {Material {Degradation} {Modeling} and {Failure} {Prediction} {Using} {Microstructure} {Images}},
	volume = {61},
	issn = {0040-1706},
	url = {https://www.tandfonline.com/doi/10.1080/00401706.2018.1514327},
	doi = {10.1080/00401706.2018.1514327},
	abstract = {Degradation data, frequently along with low-dimensional covariate information such as scalar-type covariates, are widely used for asset reliability analysis. Recently, many high-dimensional covariates such as functional and image covariates have emerged with advances in sensor technology, containing richer information that can be used for degradation assessment. In this article, motivated by a physical effect that microstructures of dual-phase advanced high strength steel strongly influence steel degradation, we propose a two-stage material degradation model using the material microstructure image as a covariate. In Stage 1, we show that the microstructure image covariate can be reduced to a functional covariate while statistical properties of the image are preserved up to the second order. In Stage 2, a novel functional covariate degradation model is proposed, based on which the time-to-failure distribution in terms of degradation level passages is derived. A penalized least squares estimation method is developed to obtain the closed-form point estimator of model parameters. Analytical inferences on interval estimation of the model parameters, the mean degradation levels, and the distribution of the time-to-failure are also developed. Simulation studies are implemented to validate the developed methods. Physical experiments on dual-phase advanced high strength steel are designed and conducted to demonstrate the proposed model. The results show that a significant improvement is achieved for material failure prediction by using material microstructure images compared with multiple benchmark models.},
	number = {2},
	urldate = {2020-12-04},
	journal = {Technometrics},
	author = {Si, Wujun and Yang, Qingyu and Wu, Xin},
	month = apr,
	year = {2019},
	note = {Publisher: Taylor \& Francis},
	pages = {246--258},
	file = {Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\C5G4RMPG\\00401706.2018.html:text/html},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	abstract = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
	language = {en},
	urldate = {2020-12-09},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
	file = {He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\IAVV9RVU\\He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf:application/pdf},
}

@article{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more eﬃciently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caﬀe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
	language = {en},
	urldate = {2020-12-09},
	journal = {arXiv:1505.04597 [cs]},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv: 1505.04597},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\HAW2PNB8\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf},
}

@article{tran_practical_nodate,
	title = {Practical {Uncertainty} {Estimation} \& {Out}-of-{Distribution} {Robustness} in {Deep} {Learning}},
	language = {en},
	author = {Tran, Dustin and Snoek, Jasper and Lakshminarayanan, Balaji},
	pages = {109},
	file = {Tran et al. - Practical Uncertainty Estimation & Out-of-Distribu.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\PCWK4AWP\\Tran et al. - Practical Uncertainty Estimation & Out-of-Distribu.pdf:application/pdf},
}

@article{hagen2022dbcal,
  title={DBCal: Density based calibration of classifier predictions for uncertainty quantification},
  author={Hagen, Alex and Pazdernik, Karl and LaHaye, Nicole and Oostrom, Marjolein},
  journal={arXiv preprint arXiv:2204.00150},
  year={2022}
}

@article{thiagarajan_designing_2020,
	title = {Designing accurate emulators for scientific processes using calibration-driven deep models},
	volume = {11},
	copyright = {2020 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-19448-8},
	doi = {10.1038/s41467-020-19448-8},
	abstract = {Predictive models that accurately emulate complex scientific processes can achieve speed-ups over numerical simulators or experiments and at the same time provide surrogates for improving the subsequent analysis. Consequently, there is a recent surge in utilizing modern machine learning methods to build data-driven emulators. In this work, we study an often overlooked, yet important, problem of choosing loss functions while designing such emulators. Popular choices such as the mean squared error or the mean absolute error are based on a symmetric noise assumption and can be unsuitable for heterogeneous data or asymmetric noise distributions. We propose Learn-by-Calibrating, a novel deep learning approach based on interval calibration for designing emulators that can effectively recover the inherent noise structure without any explicit priors. Using a large suite of use-cases, we demonstrate the efficacy of our approach in providing high-quality emulators, when compared to widely-adopted loss function choices, even in small-data regimes.},
	language = {en},
	number = {1},
	urldate = {2021-01-11},
	journal = {Nature Communications},
	author = {Thiagarajan, Jayaraman J. and Venkatesh, Bindya and Anirudh, Rushil and Bremer, Peer-Timo and Gaffney, Jim and Anderson, Gemma and Spears, Brian},
	month = nov,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {5622},
	file = {Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\4C22EWQB\\Thiagarajan et al. - 2020 - Designing accurate emulators for scientific proces.pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\GI8IH8S4\\s41467-020-19448-8.html:text/html},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {A} {Survey} on {Bayesian} {Deep} {Learning}},
	url = {https://paperswithcode.com/paper/towards-bayesian-deep-learning-a-survey},
	abstract = {Implemented in one code library.},
	language = {en},
	urldate = {2021-01-12},
	file = {Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\4Z3PKW2F\\towards-bayesian-deep-learning-a-survey.html:text/html},
}

@article{wang_survey_2021,
	title = {A {Survey} on {Bayesian} {Deep} {Learning}},
	url = {http://arxiv.org/abs/1604.01662},
	abstract = {A comprehensive artificial intelligence system needs to not only perceive the environment with different `senses' (e.g., seeing and hearing) but also infer the world's conditional (or even causal) relations and corresponding uncertainty. The past decade has seen major advances in many perception tasks such as visual object recognition and speech recognition using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. In recent years, Bayesian deep learning has emerged as a unified probabilistic framework to tightly integrate deep learning and Bayesian models. In this general framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in turn, the feedback from the inference process is able to enhance the perception of text or images. This survey provides a comprehensive introduction to Bayesian deep learning and reviews its recent applications on recommender systems, topic models, control, etc. Besides, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as Bayesian treatment of neural networks. For a constantly updating project page, please refer to https://github.com/js05212/BayesianDeepLearning-Survey.},
	urldate = {2021-01-12},
	journal = {arXiv:1604.01662 [cs, stat]},
	author = {Wang, Hao and Yeung, Dit-Yan},
	month = jan,
	year = {2021},
	note = {arXiv: 1604.01662
version: 4},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\EQN7E3C8\\Wang and Yeung - 2021 - A Survey on Bayesian Deep Learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\VKY4Q9D2\\1604.html:text/html},
}

@article{brock_high-performance_2021,
	title = {High-{Performance} {Large}-{Scale} {Image} {Recognition} {Without} {Normalization}},
	url = {http://arxiv.org/abs/2102.06171},
	abstract = {Batch normalization is a key component of most image classification models, but it has many undesirable properties stemming from its dependence on the batch size and interactions between examples. Although recent work has succeeded in training deep ResNets without normalization layers, these models do not match the test accuracies of the best batch-normalized networks, and are often unstable for large learning rates or strong data augmentations. In this work, we develop an adaptive gradient clipping technique which overcomes these instabilities, and design a significantly improved class of Normalizer-Free ResNets. Our smaller models match the test accuracy of an EfficientNet-B7 on ImageNet while being up to 8.7x faster to train, and our largest models attain a new state-of-the-art top-1 accuracy of 86.5\%. In addition, Normalizer-Free models attain significantly better performance than their batch-normalized counterparts when finetuning on ImageNet after large-scale pre-training on a dataset of 300 million labeled images, with our best models obtaining an accuracy of 89.2\%. Our code is available at https://github.com/deepmind/ deepmind-research/tree/master/nfnets},
	urldate = {2021-02-16},
	journal = {arXiv:2102.06171 [cs, stat]},
	author = {Brock, Andrew and De, Soham and Smith, Samuel L. and Simonyan, Karen},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.06171},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\SUSFZGF5\\Brock et al. - 2021 - High-Performance Large-Scale Image Recognition Wit.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\FCRQ4JS3\\2102.html:text/html},
}

@article{zhang_resnest_2020,
	title = {{ResNeSt}: {Split}-{Attention} {Networks}},
	shorttitle = {{ResNeSt}},
	url = {http://arxiv.org/abs/2004.08955},
	abstract = {It is well known that featuremap attention and multi-path representation are important for visual recognition. In this paper, we present a modularized architecture, which applies the channel-wise attention on different network branches to leverage their success in capturing cross-feature interactions and learning diverse representations. Our design results in a simple and unified computation block, which can be parameterized using only a few variables. Our model, named ResNeSt, outperforms EfficientNet in accuracy and latency trade-off on image classification. In addition, ResNeSt has achieved superior transfer learning results on several public benchmarks serving as the backbone, and has been adopted by the winning entries of COCO-LVIS challenge. The source code for complete system and pretrained models are publicly available.},
	urldate = {2021-02-16},
	journal = {arXiv:2004.08955 [cs]},
	author = {Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
	month = dec,
	year = {2020},
	note = {arXiv: 2004.08955
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\ABXP5M3M\\Zhang et al. - 2020 - ResNeSt Split-Attention Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\KDRW4PQ8\\2004.html:text/html},
}

@article{dosovitskiy_image_2020,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2021-03-01},
	journal = {arXiv:2010.11929 [cs]},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11929},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\7ZVJMETM\\Dosovitskiy et al. - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\AS78BKFM\\2010.html:text/html},
}

@article{luo_combining_2021,
	title = {Combining {Geometric} and {Topological} {Information} for {Boundary} {Estimation}},
	url = {http://arxiv.org/abs/1910.04778},
	abstract = {A fundamental problem in computer vision is boundary estimation, where the goal is to delineate the boundary of objects in an image. In this paper, we propose a method which jointly incorporates geometric and topological information within an image to simultaneously estimate boundaries for objects within images with more complex topologies. We use a topological clustering-based method to assist initialization of the Bayesian active contour model. This combines pixel clustering, boundary smoothness, and potential prior shape information to produce an estimated object boundary. Active contour methods are knownto be extremely sensitive to algorithm initialization, relying on the user to provide a reasonable starting curve to the algorithm. In the presence of images featuring objects with complex topological structures, such as objects with holes or multiple objects, the user must initialize separate curves for each boundary of interest. Our proposed topologically-guided method can provide an interpretable, smart initialization in these settings, freeing up the user from potential pitfalls associated with objects of complex topological structure. We provide a detailed simulation study comparing our initialization to boundary estimates obtained from standard segmentation algorithms. The method is demonstrated on artificial image datasets from computer vision, as well as real-world applications to skin lesion and neural cellular images, for which multiple topological features can be identified.},
	urldate = {2021-03-25},
	journal = {arXiv:1910.04778 [cs, eess, stat]},
	author = {Luo, Hengrui and Strait, Justin},
	month = jan,
	year = {2021},
	note = {arXiv: 1910.04778},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\V5RYV5MV\\Luo and Strait - 2021 - Combining Geometric and Topological Information fo.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\X5YUSXUG\\1910.html:text/html},
}

@article{mukhoti_evaluating_2019,
	title = {Evaluating {Bayesian} {Deep} {Learning} {Methods} for {Semantic} {Segmentation}},
	url = {http://arxiv.org/abs/1811.12709},
	abstract = {Deep learning has been revolutionary for computer vision and semantic segmentation in particular, with Bayesian Deep Learning (BDL) used to obtain uncertainty maps from deep models when predicting semantic classes. This information is critical when using semantic segmentation for autonomous driving for example. Standard semantic segmentation systems have well-established evaluation metrics. However, with BDL's rising popularity in computer vision we require new metrics to evaluate whether a BDL method produces better uncertainty estimates than another method. In this work we propose three such metrics to evaluate BDL models designed specifically for the task of semantic segmentation. We modify DeepLab-v3+, one of the state-of-the-art deep neural networks, and create its Bayesian counterpart using MC dropout and Concrete dropout as inference techniques. We then compare and test these two inference techniques on the well-known Cityscapes dataset using our suggested metrics. Our results provide new benchmarks for researchers to compare and evaluate their improved uncertainty quantification in pursuit of safer semantic segmentation.},
	urldate = {2021-03-30},
	journal = {arXiv:1811.12709 [cs]},
	author = {Mukhoti, Jishnu and Gal, Yarin},
	month = mar,
	year = {2019},
	note = {arXiv: 1811.12709},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\Q6A5XY4I\\Mukhoti and Gal - 2019 - Evaluating Bayesian Deep Learning Methods for Sema.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\WUNGDJ2L\\1811.html:text/html},
}

@article{kendall_what_2017,
	title = {What {Uncertainties} {Do} {We} {Need} in {Bayesian} {Deep} {Learning} for {Computer} {Vision}?},
	url = {http://arxiv.org/abs/1703.04977},
	abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
	urldate = {2021-03-30},
	journal = {arXiv:1703.04977 [cs]},
	author = {Kendall, Alex and Gal, Yarin},
	month = oct,
	year = {2017},
	note = {arXiv: 1703.04977},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\7LIHRMQA\\Kendall and Gal - 2017 - What Uncertainties Do We Need in Bayesian Deep Lea.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\K65CUG7X\\1703.html:text/html},
}

@article{hu_uncertainty-aware_nodate,
	title = {Uncertainty-{Aware} {Learning} for {Zero}-{Shot} {Semantic} {Segmentation}},
	abstract = {Zero-shot semantic segmentation (ZSS) aims to classify pixels of novel classes without training examples available. Recently, most ZSS methods focus on learning the visual-semantic correspondence to transfer knowledge from seen classes to unseen classes at the pixel level. Yet, few works study the adverse effects caused by the noisy and outlying training samples of the seen classes. In this paper, we identify this challenge and address it with a novel framework that learns to discriminate noisy samples based on Bayesian uncertainty estimation. Speciﬁcally, we model the network outputs with Gaussian and Laplacian distributions, with the variances accounting for the observation noise and uncertainty of input samples. Learning objectives are then derived with the estimated variances playing as adaptive attenuation for individual samples in training. Consequently, our model learns more attentively from representative samples of seen classes while suffering less from noisy and outlying ones, thus providing better reliability and generalization toward unseen categories. We demonstrate the effectiveness of our framework through comprehensive experiments on multiple challenging benchmarks, and show that our method achieves signiﬁcant accuracy improvement over previous approaches for large-scale open-set segmentation.},
	language = {en},
	author = {Hu, Ping and Sclaroff, Stan and Saenko, Kate},
	pages = {12},
	file = {Hu et al. - Uncertainty-Aware Learning for Zero-Shot Semantic .pdf:C\:\\Users\\pazd068\\Zotero\\storage\\G22BH4GN\\Hu et al. - Uncertainty-Aware Learning for Zero-Shot Semantic .pdf:application/pdf},
}

@article{boluki_learnable_nodate,
	title = {Learnable {Bernoulli} {Dropout} for {Bayesian} {Deep} {Learning}},
	abstract = {In this work, we propose learnable Bernoulli dropout (LBD), a new model-agnostic dropout scheme that considers the dropout rates as parameters jointly optimized with other model parameters. By probabilistic modeling of Bernoulli dropout, our method enables more robust prediction and uncertainty quantiﬁcation in deep models. Especially, when combined with variational auto-encoders (VAEs), LBD enables ﬂexible semi-implicit posterior representations, leading to new semi-implicit VAE (SIVAE) models. We solve the optimization for training with respect to the dropout parameters using Augment-REINFORCE-Merge (ARM), an unbiased and low-variance gradient estimator. Our experiments on a range of tasks show the superior performance of our approach compared with other commonly used dropout schemes. Overall, LBD leads to improved accuracy and uncertainty estimates in image classiﬁcation and semantic segmentation. Moreover, using SIVAE, we can achieve stateof-the-art performance on collaborative ﬁltering for implicit feedback on several public datasets.},
	language = {en},
	author = {Boluki, Shahin and Ardywibowo, Randy and Dadaneh, Siamak Zamani and Zhou, Mingyuan and Qian, Xiaoning},
	pages = {11},
	file = {Boluki et al. - Learnable Bernoulli Dropout for Bayesian Deep Lear.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\5MIIBZ2M\\Boluki et al. - Learnable Bernoulli Dropout for Bayesian Deep Lear.pdf:application/pdf},
}

@article{yildirim_bayesian_2021,
	title = {Bayesian {Particle} {Instance} {Segmentation} for {Electron} {Microscopy} {Image} {Quantification}},
	volume = {61},
	issn = {1549-9596},
	url = {https://doi.org/10.1021/acs.jcim.0c01455},
	doi = {10.1021/acs.jcim.0c01455},
	abstract = {Automating the analysis portion of materials characterization by electron microscopy (EM) has the potential to accelerate the process of scientific discovery. To this end, we present a Bayesian deep-learning model for semantic segmentation and localization of particle instances in EM images. These segmentations can subsequently be used to compute quantitative measures such as particle-size distributions, radial- distribution functions, average sizes, and aspect ratios of the particles in an image. Moreover, by making use of the epistemic uncertainty of our model, we obtain uncertainty estimates of its outputs and use these to filter out false-positive predictions and hence produce more accurate quantitative measures. We incorporate our method into the ImageDataExtractor package, as ImageDataExtractor 2.0, which affords a full pipeline to automatically extract particle information for large-scale data-driven materials discovery. Finally, we present and make publicly available the Electron Microscopy Particle Segmentation (EMPS) data set. This is the first human-labeled particle instance segmentation data set, consisting of 465 EM images and their corresponding semantic instance segmentation maps.},
	number = {3},
	urldate = {2021-03-30},
	journal = {Journal of Chemical Information and Modeling},
	author = {Yildirim, Batuhan and Cole, Jacqueline M.},
	month = mar,
	year = {2021},
	note = {Publisher: American Chemical Society},
	pages = {1136--1149},
	file = {Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\RVQGAENU\\Yildirim and Cole - 2021 - Bayesian Particle Instance Segmentation for Electr.pdf:application/pdf;ACS Full Text Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\N3Q72XS2\\acs.jcim.html:text/html},
}

@article{he_edge_2020,
	title = {Edge {Prior} {Multilayer} {Segmentation} {Network} {Based} on {Bayesian} {Framework}},
	volume = {2020},
	issn = {1687-725X},
	url = {https://www.hindawi.com/journals/js/2020/6854260/},
	doi = {10.1155/2020/6854260},
	abstract = {In recent years, methods based on neural network have achieved excellent performance for image segmentation. However, segmentation around the edge area is still unsatisfactory when dealing with complex boundaries. This paper proposes an edge prior semantic segmentation architecture based on Bayesian framework. The entire framework is composed of three network structures, a likelihood network and an edge prior network at the front, followed by a constraint network. The likelihood network produces a rough segmentation result, which is later optimized by edge prior information, including the edge map and the edge distance. For the constraint network, the modified domain transform method is proposed, in which the diffusion direction is revised through the newly defined distance map and some added constraint conditions. Experiments about the proposed approach and several contrastive methods show that our proposed method had good performance and outperformed FCN in terms of average accuracy for 0.0209 on ESAR data set.},
	language = {en},
	urldate = {2021-03-30},
	journal = {Journal of Sensors},
	author = {He, Chu and Shi, Zishan and Fang, Peizhang and Xiong, Dehui and He, Bokun and Liao, Mingsheng},
	month = feb,
	year = {2020},
	note = {Publisher: Hindawi},
	pages = {e6854260},
	file = {Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\VVH4MZPI\\He et al. - 2020 - Edge Prior Multilayer Segmentation Network Based o.pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\AU96C45W\\6854260.html:text/html},
}

@article{saidu_active_2021,
	title = {Active {Learning} with {Bayesian} {UNet} for {Efficient} {Semantic} {Image} {Segmentation}},
	volume = {7},
	issn = {2313-433X},
	url = {https://www.mdpi.com/2313-433X/7/2/37},
	doi = {10.3390/jimaging7020037},
	abstract = {We present a sample-efﬁcient image segmentation method using active learning, we call it Active Bayesian UNet, or AB-UNet. This is a convolutional neural network using batch normalization and max-pool dropout. The Bayesian setup is achieved by exploiting the probabilistic extension of the dropout mechanism, leading to the possibility to use the uncertainty inherently present in the system. We set up our experiments on various medical image datasets and highlight that with a smaller annotation effort our AB-UNet leads to stable training and better generalization. Added to this, we can efﬁciently choose from an unlabelled dataset.},
	language = {en},
	number = {2},
	urldate = {2021-03-30},
	journal = {Journal of Imaging},
	author = {Saidu, Isah Charles and Csató, Lehel},
	month = feb,
	year = {2021},
	pages = {37},
	file = {Saidu and Csató - 2021 - Active Learning with Bayesian UNet for Efficient S.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\GAMB3Y4K\\Saidu and Csató - 2021 - Active Learning with Bayesian UNet for Efficient S.pdf:application/pdf},
}

@article{goan_bayesian_2020,
	title = {Bayesian {Neural} {Networks}: {An} {Introduction} and {Survey}},
	volume = {2259},
	shorttitle = {Bayesian {Neural} {Networks}},
	url = {http://arxiv.org/abs/2006.12024},
	doi = {10.1007/978-3-030-42553-1_3},
	abstract = {Neural Networks (NNs) have provided state-of-the-art results for many challenging machine learning tasks such as detection, regression and classification across the domains of computer vision, speech recognition and natural language processing. Despite their success, they are often implemented in a frequentist scheme, meaning they are unable to reason about uncertainty in their predictions. This article introduces Bayesian Neural Networks (BNNs) and the seminal research regarding their implementation. Different approximate inference methods are compared, and used to highlight where future research can improve on current methods.},
	urldate = {2021-03-30},
	journal = {arXiv:2006.12024 [cs, stat]},
	author = {Goan, Ethan and Fookes, Clinton},
	year = {2020},
	note = {arXiv: 2006.12024},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {45--87},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\EPX9PETW\\Goan and Fookes - 2020 - Bayesian Neural Networks An Introduction and Surv.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\URNPGBFM\\2006.html:text/html},
}

@article{hao_brief_2020,
	title = {A {Brief} {Survey} on {Semantic} {Segmentation} with {Deep} {Learning}},
	volume = {406},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231220305476},
	doi = {10.1016/j.neucom.2019.11.118},
	abstract = {Semantic segmentation is a challenging task in computer vision. In recent years, the performance of semantic segmentation has been greatly improved by using deep learning techniques. A large number of novel methods have been proposed. This paper aims to provide a brief review of research efforts on deep-learning-based semantic segmentation methods. We categorize the related research according to its supervision level, i.e., fully-supervised methods, weakly-supervised methods and semi-supervised methods. We also discuss the common challenges of the current research, and present several valuable growing research points in this field. This survey is expected to familiarize readers with the progress and challenges of semantic segmentation research in the deep learning era.},
	language = {en},
	urldate = {2021-03-30},
	journal = {Neurocomputing},
	author = {Hao, Shijie and Zhou, Yuan and Guo, Yanrong},
	month = sep,
	year = {2020},
	keywords = {Deep learning, Semantic segmentation},
	pages = {302--321},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\T56QAGWL\\Hao et al. - 2020 - A Brief Survey on Semantic Segmentation with Deep .pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\MYYU75GQ\\S0925231220305476.html:text/html},
}

@article{minaee_image_2021,
	title = {Image {Segmentation} {Using} {Deep} {Learning}: {A} {Survey}},
	issn = {1939-3539},
	shorttitle = {Image {Segmentation} {Using} {Deep} {Learning}},
	doi = {10.1109/TPAMI.2021.3059968},
	abstract = {Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of Deep Learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions.},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Minaee, S. and Boykov, Y. Y. and Porikli, F. and Plaza, A. J. and Kehtarnavaz, N. and Terzopoulos, D.},
	year = {2021},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Deep learning, Computational modeling, Semantics, Image segmentation, Computer architecture, convolutional neural networks, deep learning, encoder-decoder models, Generative adversarial networks, generative models, instance segmentation, Logic gates, medical image segmentation, panoptic segmentation, recurrent models, semantic segmentation},
	pages = {1--1},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\JWJFFNMH\\Minaee et al. - 2021 - Image Segmentation Using Deep Learning A Survey.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\pazd068\\Zotero\\storage\\H5EJ7PL4\\9356353.html:text/html},
}

@article{aguiar_bringing_2020,
	title = {Bringing nuclear materials discovery and qualification into the 21 st century},
	volume = {11},
	copyright = {2020 This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-16406-2},
	doi = {10.1038/s41467-020-16406-2},
	abstract = {Time horizons for nuclear materials development and qualification must be shortened to realize future nuclear energy concepts. Inspired by the Materials Genome Initiative, we present an integrated approach to materials discovery and qualification to insert new materials into service.},
	language = {en},
	number = {1},
	urldate = {2021-04-06},
	journal = {Nature Communications},
	author = {Aguiar, Jeffery A. and Jokisaari, Andrea M. and Kerr, Matthew and Allen Roach, R.},
	month = may,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {2556},
	file = {Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\WVXT5NCX\\Aguiar et al. - 2020 - Bringing nuclear materials discovery and qualifica.pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\TMSLG4IA\\s41467-020-16406-2.html:text/html},
}

@article{tran_bayesian_2019,
	title = {Bayesian {Layers}: {A} {Module} for {Neural} {Network} {Uncertainty}},
	shorttitle = {Bayesian {Layers}},
	url = {http://arxiv.org/abs/1812.03973},
	abstract = {We describe Bayesian Layers, a module designed for fast experimentation with neural network uncertainty. It extends neural network libraries with drop-in replacements for common layers. This enables composition via a unified abstraction over deterministic and stochastic functions and allows for scalability via the underlying system. These layers capture uncertainty over weights (Bayesian neural nets), pre-activation units (dropout), activations ("stochastic output layers"), or the function itself (Gaussian processes). They can also be reversible to propagate uncertainty from input to output. We include code examples for common architectures such as Bayesian LSTMs, deep GPs, and flow-based models. As demonstration, we fit a 5-billion parameter "Bayesian Transformer" on 512 TPUv2 cores for uncertainty in machine translation and a Bayesian dynamics model for model-based planning. Finally, we show how Bayesian Layers can be used within the Edward2 probabilistic programming language for probabilistic programs with stochastic processes.},
	urldate = {2021-04-12},
	journal = {arXiv:1812.03973 [cs, stat]},
	author = {Tran, Dustin and Dusenberry, Michael W. and van der Wilk, Mark and Hafner, Danijar},
	month = mar,
	year = {2019},
	note = {arXiv: 1812.03973},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Programming Languages},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\MLXFWJBI\\Tran et al. - 2019 - Bayesian Layers A Module for Neural Network Uncer.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\W8U3D5H5\\1812.html:text/html},
}

@article{gordon-rodriguez_continuous_2020,
	title = {The continuous categorical: a novel simplex-valued exponential family},
	shorttitle = {The continuous categorical},
	url = {http://arxiv.org/abs/2002.08563},
	abstract = {Simplex-valued data appear throughout statistics and machine learning, for example in the context of transfer learning and compression of deep networks. Existing models for this class of data rely on the Dirichlet distribution or other related loss functions; here we show these standard choices suffer systematically from a number of limitations, including bias and numerical issues that frustrate the use of flexible network models upstream of these distributions. We resolve these limitations by introducing a novel exponential family of distributions for modeling simplex-valued data - the continuous categorical, which arises as a nontrivial multivariate generalization of the recently discovered continuous Bernoulli. Unlike the Dirichlet and other typical choices, the continuous categorical results in a well-behaved probabilistic loss function that produces unbiased estimators, while preserving the mathematical simplicity of the Dirichlet. As well as exploring its theoretical properties, we introduce sampling methods for this distribution that are amenable to the reparameterization trick, and evaluate their performance. Lastly, we demonstrate that the continuous categorical outperforms standard choices empirically, across a simulation study, an applied example on multi-party elections, and a neural network compression task.},
	urldate = {2021-06-07},
	journal = {arXiv:2002.08563 [cs, stat]},
	author = {Gordon-Rodriguez, Elliott and Loaiza-Ganem, Gabriel and Cunningham, John P.},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.08563},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\UR99427C\\Gordon-Rodriguez et al. - 2020 - The continuous categorical a novel simplex-valued.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\QUMTRHI9\\2002.html:text/html},
}

@article{frankle_lottery_2019,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} {Sparse}, {Trainable} {Neural} {Networks}},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {http://arxiv.org/abs/1803.03635},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	urldate = {2021-06-07},
	journal = {arXiv:1803.03635 [cs]},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = mar,
	year = {2019},
	note = {arXiv: 1803.03635},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\YH4ZBHHA\\Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\2PJUEN7D\\1803.html:text/html},
}

@article{peters_point--set_2020,
	title = {Point-to-set distance functions for weakly supervised segmentation},
	url = {http://arxiv.org/abs/2007.13251},
	abstract = {When pixel-level masks or partial annotations are not available for training neural networks for semantic segmentation, it is possible to use higher-level information in the form of bounding boxes, or image tags. In the imaging sciences, many applications do not have an object-background structure and bounding boxes are not available. Any available annotation typically comes from ground truth or domain experts. A direct way to train without masks is using prior knowledge on the size of objects/classes in the segmentation. We present a new algorithm to include such information via constraints on the network output, implemented via projection-based point-to-set distance functions. This type of distance functions always has the same functional form of the derivative, and avoids the need to adapt penalty functions to different constraints, as well as issues related to constraining properties typically associated with non-differentiable functions. Whereas object size information is known to enable object segmentation from bounding boxes from datasets with many general and medical images, we show that the applications extend to the imaging sciences where data represents indirect measurements, even in the case of single examples. We illustrate the capabilities in case of a) one or more classes do not have any annotation; b) there is no annotation at all; c) there are bounding boxes. We use data for hyperspectral time-lapse imaging, object segmentation in corrupted images, and sub-surface aquifer mapping from airborne-geophysical remote-sensing data. The examples verify that the developed methodology alleviates difficulties with annotating non-visual imagery for a range of experimental settings.},
	urldate = {2021-07-29},
	journal = {arXiv:2007.13251 [cs, eess]},
	author = {Peters, Bas},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.13251},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, 68T45},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\H5I3G6LU\\Peters - 2020 - Point-to-set distance functions for weakly supervi.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\TJXE79FI\\2007.html:text/html},
}

@article{zoph_rethinking_2020,
	title = {Rethinking {Pre}-training and {Self}-training},
	url = {http://arxiv.org/abs/2006.06882},
	abstract = {Pre-training is a dominant paradigm in computer vision. For example, supervised ImageNet pre-training is commonly used to initialize the backbones of object detection and segmentation models. He et al., however, show a surprising result that ImageNet pre-training has limited impact on COCO object detection. Here we investigate self-training as another method to utilize additional data on the same setup and contrast it against ImageNet pre-training. Our study reveals the generality and flexibility of self-training with three additional insights: 1) stronger data augmentation and more labeled data further diminish the value of pre-training, 2) unlike pre-training, self-training is always helpful when using stronger data augmentation, in both low-data and high-data regimes, and 3) in the case that pre-training is helpful, self-training improves upon pre-training. For example, on the COCO object detection dataset, pre-training benefits when we use one fifth of the labeled data, and hurts accuracy when we use all labeled data. Self-training, on the other hand, shows positive improvements from +1.3 to +3.4AP across all dataset sizes. In other words, self-training works well exactly on the same setup that pre-training does not work (using ImageNet to help COCO). On the PASCAL segmentation dataset, which is a much smaller dataset than COCO, though pre-training does help significantly, self-training improves upon the pre-trained model. On COCO object detection, we achieve 54.3AP, an improvement of +1.5AP over the strongest SpineNet model. On PASCAL segmentation, we achieve 90.5 mIOU, an improvement of +1.5\% mIOU over the previous state-of-the-art result by DeepLabv3+.},
	urldate = {2021-09-10},
	journal = {arXiv:2006.06882 [cs, stat]},
	author = {Zoph, Barret and Ghiasi, Golnaz and Lin, Tsung-Yi and Cui, Yin and Liu, Hanxiao and Cubuk, Ekin D. and Le, Quoc V.},
	month = nov,
	year = {2020},
	note = {arXiv: 2006.06882
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\U6D3BZ8N\\Zoph et al. - 2020 - Rethinking Pre-training and Self-training.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\EN5NXFJF\\2006.html:text/html},
}

@article{smith_evaluation_1984,
	title = {Evaluation of candidate blanket materials for fusion reactor blanket applications},
	volume = {122},
	issn = {0022-3115},
	url = {https://www.sciencedirect.com/science/article/pii/0022311584905737},
	doi = {10.1016/0022-3115(84)90573-7},
	abstract = {This paper summarizes the primary candidate materials currently considered for major components, viz., structure, breeder, coolant, etc. Compatible combinations of materials that offer the most potential for satisfactory performance and the projected ranges of operating parameters are indicated. Critical properties of the candidate materials that limit blanket performance are defined and the areas of greatest uncertainty are identified. General characteristics of the materials systems that are potentially favorable or unfavorable for the specific application are evaluated. Based on the above assessment, a perspective of the relative importance of various materials limitations and design criteria on the projected performance and blanket lifetime is presented.},
	language = {en},
	number = {1},
	urldate = {2021-10-15},
	journal = {Journal of Nuclear Materials},
	author = {Smith, Dale L.},
	month = may,
	year = {1984},
	pages = {51--65},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\QEZYRVEQ\\Smith - 1984 - Evaluation of candidate blanket materials for fusi.pdf:application/pdf},
}

@article{chou_defect_2006,
	title = {Defect characterizations of γ-{LiAlO2} single crystals},
	volume = {291},
	issn = {0022-0248},
	url = {https://www.sciencedirect.com/science/article/pii/S0022024806002752},
	doi = {10.1016/j.jcrysgro.2006.03.038},
	abstract = {LiAlO2 is a potential substrate for growing III-nitride semiconductors since the lattice mismatch between LiAlO2 and GaN is only 1.4\%. GaN grown on (100) LiAlO2 substrates is along the (101¯0) M-plane which does not have the spontaneous polarization. This paper describes the growth of (100) LiAlO2 single crystals using the Czochralski method. The as-grown crystals revealed a serious chemical decomposition problem in the cone area and some cracks in the body of the boule. Cracks indicate strain in the crystal and also relieve the strain. The crystal's structure was identified as the γ-phase by X-ray diffraction analysis. No phase transition was found. The thermal expansion coefficients of LiAlO2 was found to be 6.50×10−6°C−1 along (100) axis, and 14.90×10−6°C−1 along (001) axis. Stress distribution was investigated using an optical polarizer and no residual stress was found. Chemical etching and electron microcopy revealed a multi-domain structure along the a-axis. This domain structure is due to the polarity inversion. An unique cross-hatched pattern was also found in (001) TEM images.},
	language = {en},
	number = {2},
	urldate = {2021-10-15},
	journal = {Journal of Crystal Growth},
	author = {Chou, Mitch M. C. and Huang, Hul Chun and Gan, Der-Shin and Hsu, Chuck W. C.},
	month = jun,
	year = {2006},
	keywords = {A1. Defect, A1. Substrate, A2. Czochralski, B1. GaN, B1. LiAlO},
	pages = {485--490},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\733RRZ4C\\Chou et al. - 2006 - Defect characterizations of γ-LiAlO2 single crysta.pdf:application/pdf},
}

@article{jiang_irradiation_2017,
	title = {Irradiation effects and hydrogen behavior in {H2}+ and {He}+ implanted γ-{LiAlO2} single crystals},
	volume = {484},
	issn = {0022-3115},
	url = {https://www.sciencedirect.com/science/article/pii/S0022311516300885},
	doi = {10.1016/j.jnucmat.2016.03.014},
	abstract = {Gamma-phase lithium aluminate (γ-LiAlO2) is a breeder material for tritium, a necessary substance for strategic stockpile and fusion power systems. A fundamental study of structural evolution and tritium diffusion in γ-LiAlO2 under displacive irradiation is needed to fully assess the material performance. This study utilizes ion implantation of protium (surrogate for tritium) and helium in γ-LiAlO2 single crystals at elevated temperatures to emulate the irradiation effects. The results show that at 573 K there are two distinct disorder saturation stages to 1 dpa without full amorphization; overlapping implantation of H2+ and He+ ions suggests possible formation of gas bubbles. For irradiation to 1021 H+/m2 (0.36 dpa at peak) at 773 K, amorphization occurs at surface with H diffusion and dramatic Li loss; the microstructure contains bubbles and cubic LiAl5O8 precipitates with sizes up to 200 nm or larger. In addition, significant H diffusion and release are observed during thermal annealing.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Journal of Nuclear Materials},
	author = {Jiang, Weilin and Zhang, Jiandong and Kovarik, Libor and Zhu, Zihua and Price, Lloyd and Gigax, Jonathan and Castanon, Elizabeth and Wang, Xuemei and Shao, Lin and Senor, David J.},
	month = feb,
	year = {2017},
	pages = {374--381},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\N2SH5ZJ7\\Jiang et al. - 2017 - Irradiation effects and hydrogen behavior in H2+ a.pdf:application/pdf},
}

@article{tanaka_-situ_1989,
	title = {{IN}-situ tritium release experiments from solid breeding materials ({TTTEx}) — tritium diffusion coefficients and surface reaction on lithium aluminate},
	volume = {8},
	issn = {0920-3796},
	url = {https://www.sciencedirect.com/science/article/pii/S0920379689801000},
	doi = {10.1016/S0920-3796(89)80100-0},
	abstract = {In-situ tritium release experiments were performed for γ-LiAlO2 polycrystalline powder (5–9, 6–7, 9–16, 10–14 mesh) and pellets (8 mm diameter × 10 mm high) with three bulk densities of 55\% T.D., 78\% T.D. and 90\% T.D. at 470–780 °C using sweep gas of He+10; 100; 1000; 10000; 30000 ppm H2. The particle size dependence of tritium residence time in the sample was measured using sweep gas of He+10000 ppm H2. The experimental results were analyzed using a two-step diffusion model. Tritium diffusivities in grain (Dg) and for intergrain (Dint) were separately evaluated. The value of Dg was Dg[cm2/s] = 1.82 × 10−7 exp(−84.5[kJ/mol]/RT), and Dint was found to have a strong correlation with the bulk density. The tritium residence time decreased with the increase of H2 concentration in the sweep gas. The major part of the released tritium was the water-forming component in the regime below 1000 ppm H2, while the non-water-forming component was dominant in the regime above 10000 ppm H2.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Fusion Engineering and Design},
	author = {Tanaka, Satoru and Kawamoto, Atsushi and Yamawaki, Michio and Terai, Takayuki and Takahashi, Yoichi and Kawamura, Hiroshi and Saito, Minoru},
	month = jan,
	year = {1989},
	pages = {155--160},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\8I56YQ4T\\Tanaka et al. - 1989 - IN-situ tritium release experiments from solid bre.pdf:application/pdf},
}

@article{oztel_deep_2018,
	title = {Deep learning approaches in electron microscopy imaging for mitochondria segmentation},
	volume = {21},
	issn = {1748-5673, 1748-5681},
	url = {http://www.inderscience.com/link.php?id=96398},
	doi = {10.1504/IJDMB.2018.096398},
	language = {en},
	number = {2},
	urldate = {2021-10-15},
	journal = {International Journal of Data Mining and Bioinformatics},
	author = {Oztel, Ismail and Yolcu, Gozde and Ersoy, Ilker and White, Tommi A. and Bunyak, Filiz},
	year = {2018},
	pages = {91},
}

@misc{noauthor_latent_nodate,
	title = {Latent {Dirichlet} allocation for image segmentation and source finding in radio astronomy images {\textbar} {Proceedings} of the 27th {Conference} on {Image} and {Vision} {Computing} {New} {Zealand}},
	url = {https://dl.acm.org/doi/abs/10.1145/2425836.2425918},
	urldate = {2021-10-15},
	file = {Latent Dirichlet allocation for image segmentation and source finding in radio astronomy images | Proceedings of the 27th Conference on Image and Vision Computing New Zealand:C\:\\Users\\pazd068\\Zotero\\storage\\S92SW45M\\2425836.html:text/html},
}

@article{urquhart_graph_1982,
	title = {Graph theoretical clustering based on limited neighbourhood sets},
	volume = {15},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/0031320382900693},
	doi = {10.1016/0031-3203(82)90069-3},
	abstract = {A method for clustering data according to a visual model of clusters is proposed. The method uses either of two graphs which are defined according to relative distance and based on the Gabriel graph and the relative neighbourhood graph respectively. The method is locally sensitive, hierarchic and based on the concept of limited neighbourhood sets. Clusters that are either disjoint or homogeneous and separable by sharp changes in point density may be detected.},
	language = {en},
	number = {3},
	urldate = {2021-10-15},
	journal = {Pattern Recognition},
	author = {Urquhart, Roderick},
	month = jan,
	year = {1982},
	keywords = {Clustering, Delaunay triangulation, Geometrical complexity, Grabriel graph, Hierarchic clustering, Limited neighbourhood sets, Minimal spanning tree, Pattern recognition, Region of influence, Relative neighbourhood graph},
	pages = {173--187},
	file = {ScienceDirect Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\IT9BBJ3U\\0031320382900693.html:text/html},
}

@article{shi_normalized_2000,
	title = {Normalized cuts and image segmentation},
	volume = {22},
	issn = {0162-8828},
	doi = {10.1109/34.868688},
	abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
	language = {English},
	number = {8},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shi, J. and Malik, J.},
	year = {2000},
	pages = {888--905},
	file = {Submitted Version:C\:\\Users\\pazd068\\Zotero\\storage\\APQDJLHA\\Shi and Malik - 2000 - Normalized cuts and image segmentation.pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\U9MR5SNL\\display.html:text/html},
}

@article{felzenszwalb_efficient_2004,
	title = {Efficient graph-based image segmentation},
	volume = {59},
	issn = {0920-5691},
	doi = {10.1023/B:VISI.0000022288.19776.77},
	abstract = {This paper addresses the problem of segmenting an image into regions. We define a predicate for measuring the evidence for a boundary between two regions using a graph-based representation of the image. We then develop an efficient segmentation algorithm based on this predicate, and show that although this algorithm makes greedy decisions it produces segmentations that satisfy global properties. We apply the algorithm to image segmentation using two different kinds of local neighborhoods in constructing the graph, and illustrate the results with both real and synthetic images. The algorithm runs in time nearly linear in the number of graph edges and is also fast in practice. An important characteristic of the method is its ability to preserve detail in low-variability image regions while ignoring detail in high-variability regions.},
	language = {English},
	number = {2},
	journal = {International Journal of Computer Vision},
	author = {Felzenszwalb, P.F. and Huttenlocher, D.P.},
	year = {2004},
	keywords = {Clustering, Graph algorithm, Image segmentation, Perceptual organization},
	pages = {167--181},
	file = {Submitted Version:C\:\\Users\\pazd068\\Zotero\\storage\\C58E3KII\\Felzenszwalb and Huttenlocher - 2004 - Efficient graph-based image segmentation.pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\9WYS6MPZ\\display.html:text/html},
}

@article{ciresan_deep_2010,
	title = {Deep, big, simple neural nets for handwritten digit recognition},
	volume = {22},
	issn = {1530-888X},
	doi = {10.1162/NECO_a_00052},
	abstract = {Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35\% error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning. © 2010 Massachusetts Institute of Technology.},
	language = {English},
	number = {12},
	journal = {Neural Computation},
	author = {Cireşan, D.C. and Meier, U. and Gambardella, L.M. and Schmidhuber, J.},
	year = {2010},
	pages = {3207--3220},
	file = {Submitted Version:C\:\\Users\\pazd068\\Zotero\\storage\\WZD78GKB\\Cireşan et al. - 2010 - Deep, big, simple neural nets for handwritten digi.pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\656ASTYB\\display.html:text/html},
}

@article{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution ﬁlters, which shows that a signiﬁcant improvement on the prior-art conﬁgurations can be achieved by pushing the depth to 16–19 weight layers. These ﬁndings were the basis of our ImageNet Challenge 2014 submission, where our team secured the ﬁrst and the second places in the localisation and classiﬁcation tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	language = {en},
	urldate = {2021-10-15},
	journal = {arXiv:1409.1556 [cs]},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv: 1409.1556},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\9P5WDZCW\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf},
}

@inproceedings{long_fully_2015,
	title = {Fully convolutional networks for semantic segmentation},
	doi = {10.1109/CVPR.2015.7298965},
	abstract = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet [20], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by fine-tuning [3] to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20\% relative improvement to 62.2\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
	month = jun,
	year = {2015},
	note = {ISSN: 1063-6919},
	keywords = {Adaptation models, Computer architecture, Convolution, Deconvolution, Image segmentation, Semantics, Training},
	pages = {3431--3440},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\HXZRUQ9J\\Long et al. - 2015 - Fully convolutional networks for semantic segmenta.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\pazd068\\Zotero\\storage\\W5EJ3JN6\\7298965.html:text/html},
}

@article{chen_deeplab_2018,
	title = {{DeepLab}: {Semantic} {Image} {Segmentation} with {Deep} {Convolutional} {Nets}, {Atrous} {Convolution}, and {Fully} {Connected} {CRFs}},
	volume = {40},
	issn = {1939-3539},
	shorttitle = {{DeepLab}},
	doi = {10.1109/TPAMI.2017.2699184},
	abstract = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
	number = {4},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L.},
	month = apr,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {atrous convolution, Computational modeling, conditional random fields, Context, Convolution, Convolutional neural networks, Image resolution, Image segmentation, Neural networks, semantic segmentation, Semantics},
	pages = {834--848},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\ZYFN6U2Y\\Chen et al. - 2018 - DeepLab Semantic Image Segmentation with Deep Con.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\pazd068\\Zotero\\storage\\NYYX8K2S\\7913730.html:text/html},
}

@inproceedings{li_deep_2018,
	title = {Deep learning based gastric cancer identification},
	doi = {10.1109/ISBI.2018.8363550},
	abstract = {Gastric cancer is one of the most common cancers, which causes the second largest deaths worldwide. Manual pathological inspection of gastric slice is time-consuming and usually suffers from inter-observer variations. In this paper, we proposed a deep learning based framework, namely GastricNet, for automatic gastric cancer identification. The proposed network adopts different architectures for shallow and deep layers for better feature extraction. We evaluate the proposed framework on publicly available BOT gastric slice dataset. The experimental results show that our deep learning framework performs better than state-of-the-art networks like DenseNet, ResNet, and achieved an accuracy of 100\% for slice-based classification.},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	author = {Li, Yuexiang and Li, Xuechen and Xie, Xinpeng and Shen, Linlin},
	month = apr,
	year = {2018},
	note = {ISSN: 1945-8452},
	keywords = {Cancer, classification, deep learning network, Feature extraction, Gastric cancer, Machine learning, Pathology, Testing, Training, Tumors},
	pages = {182--185},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\6H2AMVSM\\Li et al. - 2018 - Deep learning based gastric cancer identification.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\pazd068\\Zotero\\storage\\IALZQTD9\\8363550.html:text/html},
}

@article{nelson2017assessing,
  title={Assessing probabilistic inference by comparing the generalized mean of the model and source probabilities},
  author={Nelson, Kenric P},
  journal={Entropy},
  volume={19},
  number={6},
  pages={286},
  year={2017},
  publisher={MDPI}
}
@techreport{senor2013recommendations,
  title={Recommendations for Tritium Science and Technology Research and Development in Support of the Tritium Readiness Campaign, TTP-7-084},
  author={Senor, David J},
  year={2013},
  institution={Pacific Northwest National Lab.(PNNL), Richland, WA (United States)}
}

@inproceedings{folmsbee_active_2018,
	title = {Active deep learning: {Improved} training efficiency of convolutional neural networks for tissue classification in oral cavity cancer},
	shorttitle = {Active deep learning},
	doi = {10.1109/ISBI.2018.8363686},
	abstract = {Deep learning has yielded impressive performance on a variety of difficult machine learning tasks due to large, widely available annotated datasets. Unfortunately, acquiring such datasets is difficult in medical imaging. In particular, labels for computational pathology are tedious to create and require expert pathologists. In this work, we explore methods for efficiently training convolutional neural networks (CNNs) for tissue classification using Active Learning (AL) instead of the more common Random Learning (RL). Our dataset consists of 143 digitized images of hematoxylin and eosin-stained whole oral cavity cancer sections. We compare both AL and RL training in the task of using a CNN to identify seven tissue classes (stroma, lymphocytes, tumor, mucosa, keratin pearls, blood, and background / adipose). We find that the AL strategy provides an average 3.26\% greater performance than RL for a given training set size.},
	booktitle = {2018 {IEEE} 15th {International} {Symposium} on {Biomedical} {Imaging} ({ISBI} 2018)},
	author = {Folmsbee, Jonathan and Liu, Xulei and Brandwein-Weber, Margaret and Doyle, Scott},
	month = apr,
	year = {2018},
	note = {ISSN: 1945-8452},
	keywords = {Cancer, Cavity resonators, Image segmentation, Machine learning, Pathology, Training, Tumors},
	pages = {770--773},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\RE5IU5VE\\Folmsbee et al. - 2018 - Active deep learning Improved training efficiency.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\pazd068\\Zotero\\storage\\3YF46UWX\\8363686.html:text/html},
}

@article{al_arif_fully_2018,
	title = {Fully automatic cervical vertebrae segmentation framework for {X}-ray images},
	volume = {157},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260717305126},
	doi = {10.1016/j.cmpb.2018.01.006},
	abstract = {The cervical spine is a highly flexible anatomy and therefore vulnerable to injuries. Unfortunately, a large number of injuries in lateral cervical X-ray images remain undiagnosed due to human errors. Computer-aided injury detection has the potential to reduce the risk of misdiagnosis. Towards building an automatic injury detection system, in this paper, we propose a deep learning-based fully automatic framework for segmentation of cervical vertebrae in X-ray images. The framework first localizes the spinal region in the image using a deep fully convolutional neural network. Then vertebra centers are localized using a novel deep probabilistic spatial regression network. Finally, a novel shape-aware deep segmentation network is used to segment the vertebrae in the image. The framework can take an X-ray image and produce a vertebrae segmentation result without any manual intervention. Each block of the fully automatic framework has been trained on a set of 124 X-ray images and tested on another 172 images, all collected from real-life hospital emergency rooms. A Dice similarity coefficient of 0.84 and a shape error of 1.69 mm have been achieved.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Al Arif, S. M. Masudur Rahman and Knapp, Karen and Slabaugh, Greg},
	month = apr,
	year = {2018},
	keywords = {Cervical vertebrae, Deep learning, FCN, Localization, Segmentation, UNet, X-ray},
	pages = {95--111},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\TDIUS3CH\\Al Arif et al. - 2018 - Fully automatic cervical vertebrae segmentation fr.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\D32Y4NU8\\S0169260717305126.html:text/html},
}

@article{litjens_survey_2017,
	title = {A survey on deep learning in medical image analysis},
	volume = {42},
	issn = {1361-8415},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841517301135},
	doi = {10.1016/j.media.2017.07.005},
	abstract = {Deep learning algorithms, in particular convolutional networks, have rapidly become a methodology of choice for analyzing medical images. This paper reviews the major deep learning concepts pertinent to medical image analysis and summarizes over 300 contributions to the field, most of which appeared in the last year. We survey the use of deep learning for image classification, object detection, segmentation, registration, and other tasks. Concise overviews are provided of studies per application area: neuro, retinal, pulmonary, digital pathology, breast, cardiac, abdominal, musculoskeletal. We end with a summary of the current state-of-the-art, a critical discussion of open challenges and directions for future research.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Medical Image Analysis},
	author = {Litjens, Geert and Kooi, Thijs and Bejnordi, Babak Ehteshami and Setio, Arnaud Arindra Adiyoso and Ciompi, Francesco and Ghafoorian, Mohsen and van der Laak, Jeroen A. W. M. and van Ginneken, Bram and Sánchez, Clara I.},
	month = dec,
	year = {2017},
	keywords = {Convolutional neural networks, Deep learning, Medical imaging, Survey},
	pages = {60--88},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\IRLTDKHJ\\Litjens et al. - 2017 - A survey on deep learning in medical image analysi.pdf:application/pdf},
}

@article{kondo_microstructure_2017,
	title = {Microstructure recognition using convolutional neural networks for prediction of ionic conductivity in ceramics},
	volume = {141},
	issn = {1359-6454},
	url = {https://www.sciencedirect.com/science/article/pii/S1359645417307383},
	doi = {10.1016/j.actamat.2017.09.004},
	abstract = {Convolutional neural networks (CNNs) have recently exhibited state-of-the-art performance with respect to image recognition tasks. In the present study, we adopt CNNs to link experimental microstructures with corresponding ionic conductivities. The results reveal that CNNs can be trained using only seven micrographs, and their performance exceeds the conventional scheme using hand-crafted features. While the main drawback in the use of CNNs is poor interpretability of their highly abstracted features, we propose a feature visualization method that is suitable for the proposed training scheme, assuming that all of the cropped images from a macroscopic image have the representative macroscopic property. The visualization results showed that the present CNNs automatically extract semantic features having a large correlation with macroscopic properties, such as the number of voids and the area without voids. By analyzing these features, we find an optimized size of the representative volume element to ensure the prediction accuracy of the CNNs, providing useful guidance in preparation for the training set.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Acta Materialia},
	author = {Kondo, Ruho and Yamakawa, Shunsuke and Masuoka, Yumi and Tajima, Shin and Asahi, Ryoji},
	month = dec,
	year = {2017},
	keywords = {Deep learning, EBSD, Image analysis, Microstructure, Superionic conductor},
	pages = {29--38},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\3DX7K58T\\Kondo et al. - 2017 - Microstructure recognition using convolutional neu.pdf:application/pdf},
}

@article{napoletano_anomaly_2018,
	title = {Anomaly {Detection} in {Nanofibrous} {Materials} by {CNN}-{Based} {Self}-{Similarity}},
	volume = {18},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1424-8220/18/1/209},
	doi = {10.3390/s18010209},
	abstract = {Automatic detection and localization of anomalies in nanofibrous materials help to reduce the cost of the production process and the time of the post-production visual inspection process. Amongst all the monitoring methods, those exploiting Scanning Electron Microscope (SEM) imaging are the most effective. In this paper, we propose a region-based method for the detection and localization of anomalies in SEM images, based on Convolutional Neural Networks (CNNs) and self-similarity. The method evaluates the degree of abnormality of each subregion of an image under consideration by computing a CNN-based visual similarity with respect to a dictionary of anomaly-free subregions belonging to a training set. The proposed method outperforms the state of the art.},
	language = {en},
	number = {1},
	urldate = {2021-10-15},
	journal = {Sensors},
	author = {Napoletano, Paolo and Piccoli, Flavio and Schettini, Raimondo},
	month = jan,
	year = {2018},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {anomaly detection, convolutional neural networks, defect detection, industrial quality inspection, nanofibrous materials, quality control},
	pages = {209},
	file = {Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\C55BG2RF\\209.html:text/html;Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\LD9EEEJ3\\Napoletano et al. - 2018 - Anomaly Detection in Nanofibrous Materials by CNN-.pdf:application/pdf},
}

@article{azimi_advanced_2018,
	title = {Advanced steel microstructural classification by deep learning methods},
	volume = {8},
	issn = {2045-2322},
	doi = {10.1038/s41598-018-20037-5},
	abstract = {The inner structure of a material is called microstructure. It stores the genesis of a material and determines all its physical and chemical properties. While microstructural characterization is widely spread and well known, the microstructural classification is mostly done manually by human experts, which gives rise to uncertainties due to subjectivity. Since the microstructure could be a combination of different phases or constituents with complex substructures its automatic classification is very challenging and only a few prior studies exist. Prior works focused on designed and engineered features by experts and classified microstructures separately from the feature extraction step. Recently, Deep Learning methods have shown strong performance in vision applications by learning the features from data together with the classification step. In this work, we propose a Deep Learning method for microstructural classification in the examples of certain microstructural constituents of low carbon steel. This novel method employs pixel-wise segmentation via Fully Convolutional Neural Network (FCNN) accompanied by a max-voting scheme. Our system achieves 93.94\% classification accuracy, drastically outperforming the state-of-the-art method of 48.89\% accuracy. Beyond the strong performance of our method, this line of research offers a more robust and first of all objective way for the difficult task of steel quality appreciation. © 2018 The Author(s).},
	language = {English},
	number = {1},
	journal = {Scientific Reports},
	author = {Azimi, S.M. and Britz, D. and Engstler, M. and Fritz, M. and Mücklich, F.},
	year = {2018},
	file = {Full Text:C\:\\Users\\pazd068\\Zotero\\storage\\FSLY2YAS\\Azimi et al. - 2018 - Advanced steel microstructural classification by d.pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\B33G2354\\display.html:text/html},
}

@article{chowdhury_image_2016,
	title = {Image driven machine learning methods for microstructure recognition},
	volume = {123},
	issn = {0927-0256},
	url = {https://www.sciencedirect.com/science/article/pii/S0927025616302695},
	doi = {10.1016/j.commatsci.2016.05.034},
	abstract = {Computer vision and machine learning methods were applied to the challenge of automatic microstructure recognition. Here, a case study on dendritic morphologies was performed. Two classification tasks were completed, and involved distinguishing between micrographs that depict dendritic morphologies from those that do not contain this particular microstructural feature (Task 1), and from those micrographs identified as depicting dendrites, different cross-sectional views (longitudinal or transverse) were identified (Task 2). Data sets were comprised of images taken over a range of magnifications, from materials with different compositions and varying orientations of microstructural features. Feature extraction and dimensionality reduction were performed prior to training machine learning algorithms to classify microstructural image data. Visual bag of words, texture and shape statistics, and pre-trained convolutional neural networks (deep learning algorithms) were used for feature extraction. Classification was then performed using support vector machine, voting, nearest neighbors, and random forest models. For each model, classification was completed using full (original size) and reduced feature vectors for each feature extraction method tested. Performance comparisons were done to evaluate all possible combinations of feature extraction, selection, and classifiers for the task of micrograph classification. Results demonstrate that pre-trained neural networks represent microstructure image data well, and when used for feature extraction yield the highest classification accuracies for the majority of classifier and feature selection methods tested. Thus, deep learning algorithms can successfully be applied to micrograph recognition tasks. Maximum classification accuracies of 91.85±4.25\% and 97.37±3.33\% for Tasks 1 and 2 respectively, were achieved. This work is a broad investigation of computer vision and machine learning methods that acts as a step towards applying these established methods to more sophisticated materials recognition or characterization tasks. The approach presented here could offer improvements over established stereological measurements by removing the requirement of expert knowledge (bias) for interpretation of image data prior to characterization.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Computational Materials Science},
	author = {Chowdhury, Aritra and Kautz, Elizabeth and Yener, Bülent and Lewis, Daniel},
	month = oct,
	year = {2016},
	keywords = {Classification, Computer vision, Convolutional neural networks, Machine learning, Micrograph, Microstructure},
	pages = {176--187},
}

@article{decost_high_2019,
	title = {High throughput quantitative metallography for complex microstructures using deep learning: {A} case study in ultrahigh carbon steel},
	volume = {25},
	issn = {1431-9276, 1435-8115},
	shorttitle = {High throughput quantitative metallography for complex microstructures using deep learning},
	url = {http://arxiv.org/abs/1805.08693},
	doi = {10.1017/S1431927618015635},
	abstract = {We apply a deep convolutional neural network segmentation model to enable novel automated microstructure segmentation applications for complex microstructures typically evaluated manually and subjectively. We explore two microstructure segmentation tasks in an openly-available ultrahigh carbon steel microstructure dataset: segmenting cementite particles in the spheroidized matrix, and segmenting larger fields of view featuring grain boundary carbide, spheroidized particle matrix, particle-free grain boundary denuded zone, and Widmanst{\textbackslash}"atten cementite. We also demonstrate how to combine these data-driven microstructure segmentation models to obtain empirical cementite particle size and denuded zone width distributions from more complex micrographs containing multiple microconstituents. The full annotated dataset is available on materialsdata.nist.gov (https://materialsdata.nist.gov/handle/11256/964).},
	number = {1},
	urldate = {2021-10-15},
	journal = {Microscopy and Microanalysis},
	author = {DeCost, Brian L. and Lei, Bo and Francis, Toby and Holm, Elizabeth A.},
	month = feb,
	year = {2019},
	note = {arXiv: 1805.08693},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {21--29},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\D53RQ5FD\\DeCost et al. - 2019 - High throughput quantitative metallography for com.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\8ACTZL7T\\1805.html:text/html},
}

@article{ly_new_2019,
	title = {A new approach for quantifying morphological features of {U3O8} for nuclear forensics using a deep learning model},
	volume = {517},
	issn = {0022-3115},
	url = {https://www.sciencedirect.com/science/article/pii/S0022311518313497},
	doi = {10.1016/j.jnucmat.2019.01.042},
	abstract = {Morphological features have proven to be a useful signature in determining the process histories of uranium oxides. Historically, morphological analysis has relied on using image analysis software to segment fully visible particles in Scanning Electron Microscopy (SEM) images of a particular sample and then compute attributes such as circularity, area, perimeter, ellipse aspect ratio of these segmented particles. One such software is Morphological Analysis for MAterial (MAMA) developed by Los Alamos National Laboratory. MAMA provides both segmentation and quantification functionality. Unfortunately, SEM images of nuclear materials can be difficult to segment due to overlapping particles, charging effects, and image clarity. It requires significant user inputs, which is time-consuming and tedious, to segment only fully visible particles. In this study, an alternative segmentation method, using a deep learning model, was used to segment fully visible particles. The deep learning model used in this study is a modified version of a well-known segmentation model in the computer vision community referred to as U-net. This model was able to produce the segmentation results similar to manual segmentation results obtained using MAMA with at least 85\% accuracy in intersection over union metric. Furthermore, the model achieved a similar statistical relevance as manual segmentation under Kolmogorov-Smirnov (K-S) test.},
	language = {en},
	urldate = {2021-10-15},
	journal = {Journal of Nuclear Materials},
	author = {Ly, Cuong and Olsen, Adam M. and Schwerdt, Ian J. and Porter, Reid and Sentz, Kari and McDonald, Luther W. and Tasdizen, Tolga},
	month = apr,
	year = {2019},
	keywords = {Convolutional neural networks, Machine learning, Nuclear forensics, Quantitative morphology, Segmentation, Triuranium octoxide},
	pages = {128--137},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\pazd068\\Zotero\\storage\\CP3RL7M2\\Ly et al. - 2019 - A new approach for quantifying morphological featu.pdf:application/pdf},
}

@article{badrinarayanan2017segnet,
  title={Segnet: A deep convolutional encoder-decoder architecture for image segmentation},
  author={Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={12},
  pages={2481--2495},
  year={2017},
  publisher={IEEE}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@misc{trypag,
  author = {trypag},
  title = {pytorch-unet-segnet},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/trypag/pytorch-unet-segnet/tree/master}}
}

@misc{AdeelH,
  author = {AdeelH},
  title = {Multi-class Focal Loss},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AdeelH/pytorch-multi-class-focal-loss}}
}
@misc{Ganaye,
  author = {
Pierre-Antoine Ganaye},
  title = {pytorch-unet-segnet},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/trypag/pytorch-unet-segnet/tree/master}}
}
@misc{say4n,
  author = {say4n},
  title = {pytorch-segnet: Implementation of SegNet architecture in Pytorch},
  year = {2020},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/say4n/pytorch-segnet/tree/master}}
}

@article{buck2018monitoring,
  title={Monitoring bromide effect on radiolytic yields using in situ observations of uranyl oxide precipitation in the electron microscope},
  author={Buck, Edgar C and Wittman, Richard S and Soderquist, Chuck Z and McNamara, Bruce K},
  journal={RSC advances},
  volume={8},
  number={33},
  pages={18227--18233},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@article{roberts_deep_2019,
	title = {Deep {Learning} for {Semantic} {Segmentation} of {Defects} in {Advanced} {STEM} {Images} of {Steels}},
	volume = {9},
	issn = {2045-2322},
	doi = {10.1038/s41598-019-49105-0},
	abstract = {Crystalline materials exhibit long-range ordered lattice unit, within which resides nonperiodic structural features called defects. These crystallographic defects play a vital role in determining the physical and mechanical properties of a wide range of material systems. While computer vision has demonstrated success in recognizing feature patterns in images with well-defined contrast, automated identification of nanometer scale crystallographic defects in electron micrographs governed by complex contrast mechanisms is still a challenging task. Here, building upon an advanced defect imaging mode that offers high feature clarity, we introduce DefectSegNet - a new convolutional neural network (CNN) architecture that performs semantic segmentation of three common crystallographic defects in structural alloys: dislocation lines, precipitates and voids. Results from supervised training on a small set of high-quality defect images of steels show high pixel-wise accuracy across all three types of defects: 91.60 ± 1.77\% on dislocations, 93.39 ± 1.00\% on precipitates, and 98.85 ± 0.56\% on voids. We discuss the sources of uncertainties in CNN prediction and the training data in terms of feature density, representation and homogeneity and their effects on deep learning performance. Further defect quantification using DefectSegNet prediction outperforms human expert average, presenting a promising new workflow for fast and statistically meaningful quantification of materials defects. © 2019, The Author(s).},
	language = {English},
	number = {1},
	journal = {Scientific Reports},
	author = {Roberts, G. and Haile, S.Y. and Sainju, R. and Edwards, D.J. and Hutchinson, B. and Zhu, Y.},
	year = {2019},
	file = {Full Text:C\:\\Users\\pazd068\\Zotero\\storage\\DVQG2JKU\\Roberts et al. - 2019 - Deep Learning for Semantic Segmentation of Defects.pdf:application/pdf;Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\YNX4VEGY\\display.html:text/html},
}

@inproceedings{agarwal_multi-class_2018,
	address = {Houston, United States},
	title = {Multi-class segmentation of neuronal electron microscopy images using deep learning},
	isbn = {978-1-5106-1637-0 978-1-5106-1638-7},
	url = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/10574/2293940/Multi-class-segmentation-of-neuronal-electron-microscopy-images-using-deep/10.1117/12.2293940.full},
	doi = {10.1117/12.2293940},
	abstract = {Study of connectivity of neural circuits is an essential step towards a better understanding of functioning of the nervous system. With the recent improvement in imaging techniques, high-resolution and high-volume images are being generated requiring automated segmentation techniques. We present a pixel-wise classiﬁcation method based on Bayesian SegNet architecture. We carried out multi-class segmentation on serial section Transmission Electron Microscopy (ssTEM) images of Drosophila third instar larva ventral nerve cord, labeling the four classes of neuron membranes, neuron intracellular space, mitochondria and glia / extracellular space. Bayesian SegNet was trained using 256 ssTEM images of 256 x 256 pixels and tested on 64 diﬀerent ssTEM images of the same size, from the same serial stack. Due to high class imbalance, we used a class-balanced version of Bayesian SegNet by re-weighting each class based on their relative frequency. We achieved an overall accuracy of 93\% and a mean class accuracy of 88\% for pixel-wise segmentation using this encoder-decoder approach. On evaluating the segmentation results using similarity metrics like SSIM and Dice Coeﬃcient, we obtained scores of 0.994 and 0.886 respectively. Additionally, we used the network trained using the 256 ssTEM images of Drosophila third instar larva for multi-class labeling of ISBI 2012 challenge ssTEM dataset.},
	language = {en},
	urldate = {2021-10-15},
	booktitle = {Medical {Imaging} 2018: {Image} {Processing}},
	publisher = {SPIE},
	author = {Agarwal, Chirag and Khobragade, Nivedita},
	editor = {Angelini, Elsa D. and Landman, Bennett A.},
	month = mar,
	year = {2018},
	pages = {101},
	file = {Agarwal and Khobragade - 2018 - Multi-class segmentation of neuronal electron micr.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\GTBA7AZF\\Agarwal and Khobragade - 2018 - Multi-class segmentation of neuronal electron micr.pdf:application/pdf},
}

@article{kim_hadamard_2017,
	title = {Hadamard {Product} for {Low}-rank {Bilinear} {Pooling}},
	url = {http://arxiv.org/abs/1610.04325},
	abstract = {Bilinear models provide rich representations compared with linear models. They have been applied in various visual tasks, such as object recognition, segmentation, and visual question-answering, to get state-of-the-art performances taking advantage of the expanded representations. However, bilinear representations tend to be high-dimensional, limiting the applicability to computationally complex tasks. We propose low-rank bilinear pooling using Hadamard product for an efficient attention mechanism of multimodal learning. We show that our model outperforms compact bilinear pooling in visual question-answering tasks with the state-of-the-art results on the VQA dataset, having a better parsimonious property.},
	urldate = {2021-10-15},
	journal = {arXiv:1610.04325 [cs]},
	author = {Kim, Jin-Hwa and On, Kyoung-Woon and Lim, Woosang and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
	month = mar,
	year = {2017},
	note = {arXiv: 1610.04325},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:C\:\\Users\\pazd068\\Zotero\\storage\\PK8KSVB9\\Kim et al. - 2017 - Hadamard Product for Low-rank Bilinear Pooling.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\EFUKQGJ6\\1610.html:text/html},
}

@article{he_convolutional_nodate,
	title = {Convolutional {Neural} {Networks} at {Constrained} {Time} {Cost}},
	abstract = {Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and timeconsuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of ﬁlters, ﬁlter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8\% top-5 error, 10-view test), yet is 20\% faster than “AlexNet” [14] (16.0\% top-5 error, 10-view test).},
	language = {en},
	author = {He, Kaiming and Sun, Jian},
	pages = {8},
	file = {He and Sun - Convolutional Neural Networks at Constrained Time .pdf:C\:\\Users\\pazd068\\Zotero\\storage\\XTMUP5UJ\\He and Sun - Convolutional Neural Networks at Constrained Time .pdf:application/pdf},
}

@article{kiryati_probabilistic_1991,
	title = {A probabilistic {Hough} transform},
	volume = {24},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/003132039190073E},
	doi = {10.1016/0031-3203(91)90073-E},
	abstract = {The Hough Transform for straight line detection is considered. It is shown that if just a small subset of the edge points in the image, selected at random, is used as input for the Hough Transform, the performance is often only slightly impaired, thus the execution time can be considerably shortened. The performance of the resulting “Probabilistic Hough Transform” is analysed. The analysis is supported by experimental evidence.},
	language = {en},
	number = {4},
	urldate = {2021-10-15},
	journal = {Pattern Recognition},
	author = {Kiryati, N. and Eldar, Y. and Bruckstein, A. M.},
	month = jan,
	year = {1991},
	keywords = {Computer vision, Feature extraction, Hough Transform, Monte-Carlo methods, Probabilistic algorithms},
	pages = {303--316},
	file = {ScienceDirect Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\F3IU6A2Q\\003132039190073E.html:text/html},
}

@article{srivastava_highway_2015,
	title = {Highway {Networks}},
	url = {http://arxiv.org/abs/1505.00387},
	abstract = {There is plenty of theoretical and empirical evidence that depth of neural networks is a crucial ingredient for their success. However, network training becomes more difﬁcult with increasing depth and training of very deep networks remains an open problem. In this extended abstract, we introduce a new architecture designed to ease gradient-based training of very deep networks. We refer to networks with this architecture as highway networks, since they allow unimpeded information ﬂow across several layers on information highways. The architecture is characterized by the use of gating units which learn to regulate the ﬂow of information through a network. Highway networks with hundreds of layers can be trained directly using stochastic gradient descent and with a variety of activation functions, opening up the possibility of studying extremely deep and efﬁcient architectures.},
	language = {en},
	urldate = {2021-10-15},
	journal = {arXiv:1505.00387 [cs]},
	author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, Jürgen},
	month = nov,
	year = {2015},
	note = {arXiv: 1505.00387},
	keywords = {68T01, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, G.1.6, I.2.6},
	file = {Srivastava et al. - 2015 - Highway Networks.pdf:C\:\\Users\\pazd068\\Zotero\\storage\\GDBVFVQ3\\Srivastava et al. - 2015 - Highway Networks.pdf:application/pdf},
}

@book{riemann_grundlagen_1867,
	title = {Grundlagen fur eine allgemeine {Theorie} der {Functionen} einer veraenderlichen complexen {Groesse}},
	language = {de},
	publisher = {Adalbert Rente},
	author = {Riemann, Bernhard},
	year = {1867},
	note = {Google-Books-ID: tBUOAAAAQAAJ},
}

@article{ripley_modelling_1977,
	title = {Modelling {Spatial} {Patterns}},
	volume = {39},
	issn = {2517-6161},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01615.x},
	doi = {10.1111/j.2517-6161.1977.tb01615.x},
	abstract = {Spatial point processes may be analysed at two levels. Quadrat and distance methods were designed for the sampling of a population in the field. In this paper we consider those situations in which a map of a spatial pattern has been produced at some cost and we wish to extract the maximum possible information. We review the stochastic models which have been proposed for spatial point patterns and discuss methods by which the fit of such a model can be tested. Certain models are shown to be the equilibrium distributions of spatial–temporal stochastic processes. The theory is illustrated by several case studies.},
	language = {en},
	number = {2},
	urldate = {2021-10-15},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Ripley, B. D.},
	year = {1977},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01615.x},
	keywords = {cell shapes, hard-core model, markov point process, mathematical morphology, point process, spatial pattern},
	pages = {172--192},
	file = {Snapshot:C\:\\Users\\pazd068\\Zotero\\storage\\DA65HCZ2\\j.2517-6161.1977.tb01615.html:text/html},
}


@article{bessag1977comments,
  title={Comments on Ripley’s paper},
  author={Bessag, JE},
  journal={Journal of the Royal Statistical Society B},
  volume={39},
  pages={193--195},
  year={1977}
}


@article{senor2017recommendations,
  title={Recommendations for Science and Technology in Support of the Tritium Sustainment Program},
  author={Senor, DJ},
  journal={PNNL-27216, PNNL},
  year={2017}
}

@article{Nelson2017,
	author = {Nelson, Kenric P.},
	doi = {10.3390/E19060286},
	file = {:Users/hage581/Documents/lit/Nelson_2017_Assessing Probabilistic Inference by Comparing the Generalized Mean of the Model and Source Probabilities.pdf:pdf},
	journal = {Entropy 2017, Vol. 19, Page 286},
	keywords = {Bayesian,generalized mean,inference,information theory,probability},
	month = {jun},
	number = {6},
	pages = {286},
	publisher = {Multidisciplinary Digital Publishing Institute},
	title = {{Assessing Probabilistic Inference by Comparing the Generalized Mean of the Model and Source Probabilities}},
	url = {https://www.mdpi.com/1099-4300/19/6/286/htm https://www.mdpi.com/1099-4300/19/6/286},
	volume = {19},
	year = {2017}
}


@inproceedings{borges2019physics,
  title={Physics-informed brain MRI segmentation},
  author={Borges, Pedro and Sudre, Carole and Varsavsky, Thomas and Thomas, David and Drobnjak, Ivana and Ourselin, Sebastien and Cardoso, M Jorge},
  booktitle={International Workshop on Simulation and Synthesis in Medical Imaging},
  pages={100--109},
  year={2019},
  organization={Springer}
}

@inproceedings{borges2019physics,
  title={Physics-informed brain MRI segmentation},
  author={Borges, Pedro and Sudre, Carole and Varsavsky, Thomas and Thomas, David and Drobnjak, Ivana and Ourselin, Sebastien and Cardoso, M Jorge},
  booktitle={International Workshop on Simulation and Synthesis in Medical Imaging},
  pages={100--109},
  year={2019},
  organization={Springer}
}

@article{hu2019topology,
  title={Topology-preserving deep image segmentation},
  author={Hu, Xiaoling and Li, Fuxin and Samaras, Dimitris and Chen, Chao},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{pazdernik2020microstructural,
  title={Microstructural classification of unirradiated LiAlO2 pellets by deep learning methods},
  author={Pazdernik, Karl and LaHaye, Nicole L and Artman, Conor M and Zhu, Yuanyuan},
  journal={Computational Materials Science},
  volume={181},
  pages={109728},
  year={2020},
  publisher={Elsevier}
}

@article{baddeley2005spatstat,
  title={Spatstat: an R package for analyzing spatial point patterns},
  author={Baddeley, Adrian and Turner, Rolf},
  journal={Journal of statistical software},
  volume={12},
  pages={1--42},
  year={2005}
}

@article{jiang2024microstructural,
  title={Microstructural and compositional evolutions in $\gamma$-LiAlO2 pellets during ion irradiation at an elevated temperature},
  author={Jiang, Weilin and Kovarik, Libor and Wirth, Mark G and Hu, Zhihan and Shao, Lin and Casella, Andrew M and Senor, David J},
  journal={Journal of Nuclear Materials},
  volume={591},
  pages={154925},
  year={2024},
  publisher={Elsevier}
}

@ARTICLE{2022ApJ...935..167A,
       author = {{Astropy Collaboration} and {Price-Whelan}, Adrian M. and {Lim}, Pey Lian and {Earl}, Nicholas and {Starkman}, Nathaniel and {Bradley}, Larry and {Shupe}, David L. and {Patil}, Aarya A. and {Corrales}, Lia and {Brasseur}, C.~E. and {N{\"o}the}, Maximilian and {Donath}, Axel and {Tollerud}, Erik and {Morris}, Brett M. and {Ginsburg}, Adam and {Vaher}, Eero and {Weaver}, Benjamin A. and {Tocknell}, James and {Jamieson}, William and {van Kerkwijk}, Marten H. and {Robitaille}, Thomas P. and {Merry}, Bruce and {Bachetti}, Matteo and {G{\"u}nther}, H. Moritz and {Aldcroft}, Thomas L. and {Alvarado-Montes}, Jaime A. and {Archibald}, Anne M. and {B{\'o}di}, Attila and {Bapat}, Shreyas and {Barentsen}, Geert and {Baz{\'a}n}, Juanjo and {Biswas}, Manish and {Boquien}, M{\'e}d{\'e}ric and {Burke}, D.~J. and {Cara}, Daria and {Cara}, Mihai and {Conroy}, Kyle E. and {Conseil}, Simon and {Craig}, Matthew W. and {Cross}, Robert M. and {Cruz}, Kelle L. and {D'Eugenio}, Francesco and {Dencheva}, Nadia and {Devillepoix}, Hadrien A.~R. and {Dietrich}, J{\"o}rg P. and {Eigenbrot}, Arthur Davis and {Erben}, Thomas and {Ferreira}, Leonardo and {Foreman-Mackey}, Daniel and {Fox}, Ryan and {Freij}, Nabil and {Garg}, Suyog and {Geda}, Robel and {Glattly}, Lauren and {Gondhalekar}, Yash and {Gordon}, Karl D. and {Grant}, David and {Greenfield}, Perry and {Groener}, Austen M. and {Guest}, Steve and {Gurovich}, Sebastian and {Handberg}, Rasmus and {Hart}, Akeem and {Hatfield-Dodds}, Zac and {Homeier}, Derek and {Hosseinzadeh}, Griffin and {Jenness}, Tim and {Jones}, Craig K. and {Joseph}, Prajwel and {Kalmbach}, J. Bryce and {Karamehmetoglu}, Emir and {Ka{\l}uszy{\'n}ski}, Miko{\l}aj and {Kelley}, Michael S.~P. and {Kern}, Nicholas and {Kerzendorf}, Wolfgang E. and {Koch}, Eric W. and {Kulumani}, Shankar and {Lee}, Antony and {Ly}, Chun and {Ma}, Zhiyuan and {MacBride}, Conor and {Maljaars}, Jakob M. and {Muna}, Demitri and {Murphy}, N.~A. and {Norman}, Henrik and {O'Steen}, Richard and {Oman}, Kyle A. and {Pacifici}, Camilla and {Pascual}, Sergio and {Pascual-Granado}, J. and {Patil}, Rohit R. and {Perren}, Gabriel I. and {Pickering}, Timothy E. and {Rastogi}, Tanuj and {Roulston}, Benjamin R. and {Ryan}, Daniel F. and {Rykoff}, Eli S. and {Sabater}, Jose and {Sakurikar}, Parikshit and {Salgado}, Jes{\'u}s and {Sanghi}, Aniket and {Saunders}, Nicholas and {Savchenko}, Volodymyr and {Schwardt}, Ludwig and {Seifert-Eckert}, Michael and {Shih}, Albert Y. and {Jain}, Anany Shrey and {Shukla}, Gyanendra and {Sick}, Jonathan and {Simpson}, Chris and {Singanamalla}, Sudheesh and {Singer}, Leo P. and {Singhal}, Jaladh and {Sinha}, Manodeep and {Sip{\H{o}}cz}, Brigitta M. and {Spitler}, Lee R. and {Stansby}, David and {Streicher}, Ole and {{\v{S}}umak}, Jani and {Swinbank}, John D. and {Taranu}, Dan S. and {Tewary}, Nikita and {Tremblay}, Grant R. and {Val-Borro}, Miguel de and {Van Kooten}, Samuel J. and {Vasovi{\'c}}, Zlatan and {Verma}, Shresth and {de Miranda Cardoso}, Jos{\'e} Vin{\'\i}cius and {Williams}, Peter K.~G. and {Wilson}, Tom J. and {Winkel}, Benjamin and {Wood-Vasey}, W.~M. and {Xue}, Rui and {Yoachim}, Peter and {Zhang}, Chen and {Zonca}, Andrea and {Astropy Project Contributors}},
        title = "{The Astropy Project: Sustaining and Growing a Community-oriented Open-source Project and the Latest Major Release (v5.0) of the Core Package}",
      journal = {\apj},
     keywords = {Astronomy software, Open source software, Astronomy data analysis, 1855, 1866, 1858, Astrophysics - Instrumentation and Methods for Astrophysics},
         year = 2022,
        month = aug,
       volume = {935},
       number = {2},
          eid = {167},
        pages = {167},
          doi = {10.3847/1538-4357/ac7c74},
archivePrefix = {arXiv},
       eprint = {2206.14220},
 primaryClass = {astro-ph.IM},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2022ApJ...935..167A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{zhang2022resnest,
  title={Resnest: Split-attention networks},
  author={Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2736--2746},
  year={2022}
}

@article{kendall2015bayesian,
  title={Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding},
  author={Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
  journal={arXiv preprint arXiv:1511.02680},
  year={2015}
}

@inproceedings{zhang2022resnest,
  title={Resnest: Split-attention networks},
  author={Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Lin, Haibin and Zhang, Zhi and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2736--2746},
  year={2022}
}

@article{goa2010,
  title={Nuclear Weapons: National Nuclear Security Administration 
Needs to Ensure Continued Availability of Tritium for the Weapons 
Stockpile},
  author={United States Government Accountability Office},
  journal={GAO report, number GAO-11-100},
  year={2010},
url = {https://www.gao.gov/assets/a311096.html}
}

@article{senor2018science,
  title={Science and technology in support of the tritium sustainment program},
  author={Senor, DJ},
  journal={PNNL report, PNNL-27216},
  year={2018},
url = { https://www.energy.gov/sites/prod/files/2019/06/f63/Science-and-Technology-Supporting-the-Tritium-Sustainment-Program.pdf}
}

@article{goa2010,
  title={Nuclear Weapons: National Nuclear Security Administration 
Needs to Ensure Continued Availability of Tritium for the Weapons 
Stockpile},
  author={United States Government Accountability Office},
  journal={GAO report, number GAO-11-100},
  year={2010},
url = {https://www.gao.gov/assets/a311096.html}
}

@article{jiang2022microstructural,
  title={Microstructural evolution and precipitation in $\gamma$-LiAlO2 during ion irradiation},
  author={Jiang, Weilin and Kovarik, Libor and Zhu, Zihua and Varga, Tamas and Bowden, Mark E and Matthews, Bethany E and Hu, Zhihan and Shao, Lin and Senor, David J},
  journal={Journal of Applied Physics},
  volume={131},
  number={21},
  year={2022},
  publisher={AIP Publishing}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{jiang2020quantitative,
  title={A quantitative study of retention and release of deuterium and tritium during irradiation of $\gamma$-LiAlO2 pellets},
  author={Jiang, Weilin and Luscher, Walter G and Wang, Tianyao and Zhu, Zihua and Shao, Lin and Senor, David J},
  journal={Journal of Nuclear Materials},
  volume={542},
  pages={152532},
  year={2020},
  publisher={Elsevier}
}

@inproceedings{lin2017focal,
  title={Focal loss for dense object detection},
  author={Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2980--2988},
  year={2017}
}


@article{kendall2015bayesian,
  title={Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding},
  author={Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
  journal={arXiv preprint arXiv:1511.02680},
  year={2015}
}


@article{yao2023dual,
  title={Dual vision transformer},
  author={Yao, Ting and Li, Yehao and Pan, Yingwei and Wang, Yu and Zhang, Xiao-Ping and Mei, Tao},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  year={2023},
  publisher={IEEE}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@article{huang2024segment,
  title={Segment anything model for medical images?},
  author={Huang, Yuhao and Yang, Xin and Liu, Lian and Zhou, Han and Chang, Ao and Zhou, Xinrui and Chen, Rusi and Yu, Junxuan and Chen, Jiongquan and Chen, Chaoyu and others},
  journal={Medical Image Analysis},
  volume={92},
  pages={103061},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{nilsback2008automated,
  title={Automated flower classification over a large number of classes},
  author={Nilsback, Maria-Elena and Zisserman, Andrew},
  booktitle={2008 Sixth Indian conference on computer vision, graphics \& image processing},
  pages={722--729},
  year={2008},
  organization={IEEE}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{liu2021efficient,
  title={Efficient training of visual transformers with small datasets},
  author={Liu, Yahui and Sangineto, Enver and Bi, Wei and Sebe, Nicu and Lepri, Bruno and Nadai, Marco},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23818--23830},
  year={2021}
}

@article{li2024machine,
  title={Machine learning quantification of grain boundary defects for high efficiency perovskite solar cells},
  author={Li, Xiaohui and Mai, Yongxiang and Meng, Haogang and Bi, Huan and Ng, Chi Huey and Teo, Siow Hwa and Lan, Chunfeng and Zhang, Putao and Li, Shengjun},
  journal={Advanced Composites and Hybrid Materials},
  volume={7},
  number={6},
  pages={1--8},
  year={2024},
  publisher={Springer}
}

@article{jacobs2022performance,
  title={Performance and limitations of deep learning semantic segmentation of multiple defects in transmission electron micrographs},
  author={Jacobs, Ryan and Shen, Mingren and Liu, Yuhan and Hao, Wei and Li, Xiaoshan and He, Ruoyu and Greaves, Jacob RC and Wang, Donglin and Xie, Zeming and Huang, Zitong and others},
  journal={Cell Reports Physical Science},
  volume={3},
  number={5},
  year={2022},
  publisher={Elsevier}
}

@article{roberts2019deep,
  title={Deep learning for semantic segmentation of defects in advanced STEM images of steels},
  author={Roberts, Graham and Haile, Simon Y and Sainju, Rajat and Edwards, Danny J and Hutchinson, Brian and Zhu, Yuanyuan},
  journal={Scientific reports},
  volume={9},
  number={1},
  pages={12744},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{ronneberger2015u,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={Medical image computing and computer-assisted intervention--MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18},
  pages={234--241},
  year={2015},
  organization={Springer}
}

@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@inproceedings{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4015--4026},
  year={2023}
}


@article{zou2024segment,
  title={Segment everything everywhere all at once},
  author={Zou, Xueyan and Yang, Jianwei and Zhang, Hao and Li, Feng and Li, Linjie and Wang, Jianfeng and Wang, Lijuan and Gao, Jianfeng and Lee, Yong Jae},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}


