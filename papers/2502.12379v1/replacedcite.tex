\section{Related Work}
\label{sec:related}
	
	Recent years have witnessed substantial progress in medical imaging analysis, driven by the convergence of deep learning algorithms, large-scale computing resources, and continuously expanding datasets. Table~\ref{tab:relatedsummary} provides a concise overview of representative methods in this domain, illustrating the historical dominance of CNNs, emerging Transformer-based approaches, and ongoing debates surrounding transfer learning strategies.
	
	\begin{table}[ht]
		\centering
		\caption{Representative Related Works in Medical Imaging and Transfer Learning}
		\label{tab:relatedsummary}
		\begin{tabular}{p{3.2cm}p{3.6cm}p{5cm}p{1.2cm}}
			\toprule
			\textbf{Approach / Model} 
			& \textbf{Dataset / Modality} 
			& \textbf{Key Findings} 
			& \textbf{Refs} \\
			\midrule
			\textbf{CNN for OCT lesions}  
			& Public OCT datasets  
			& High accuracy but data-hungry; limited long-range context modeling  
			& ____ \\
			
			\textbf{Data augmentation \& GAN}  
			& Liver lesion, etc.  
			& Synthetic data helps mitigate class imbalance and small-sample issues  
			& ____ \\
			
			\textbf{Transformer (NLP to CV)}
			& ImageNet, COCO  
			& Global attention outperforms or competes with CNN, yet needs large data  
			& ____ \\
			
			\textbf{ViT in medical imaging}  
			& Brain MRI, CT  
			& Hybrid or specialized Transformer variants proposed for better data efficiency  
			& ____ \\
			
			\textbf{OCT Domain Gap}  
			& Retinal OCT, OCTA  
			& Domain-specific structures hamper direct use of standard CNN/ViT models  
			& ____ \\
			
			\textbf{ImageNet pre-training}  
			& Various medical tasks  
			& Speeds up convergence; domain gap can lessen benefits  
			& ____ \\
			
			\textbf{Scratch vs. Pre-trained}  
			& Medical tasks (general)  
			& Under certain conditions, scratch training can match or exceed pre-trained  
			& ____ \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\subsection{CNNs in Medical Imaging}
	Deep convolutional neural networks (CNNs) have been a approach for various medical imaging tasks, including lesion detection in CT/MRI ____ and automated diagnosis in OCT ____. By leveraging hierarchical feature extraction, CNNs can yield high accuracy when provided with sufficiently large datasets. However, many medical image collections remain limited or imbalanced, increasing the risk of overfitting ____. To counter this, researchers often employ data augmentation or generative adversarial networks (GAN) for synthetic data generation ____, as well as transfer learning from ImageNet-pretrained models.
	
	In particular, Optical Coherence Tomography (OCT) is a crucial modality for ophthalmic diagnosis, enabling cross-sectional visualization of the retina that distinctly differs from natural images in contrast, texture, and noise characteristics ____. Although CNN-based pipelines have shown success on OCT datasets ____, the unique domain-specific features of retinal scans can limit the direct transferability of features learned from natural-image datasets.
	
	\subsection{Vision Transformers}
	Transformers, originally successful in NLP ____, were introduced to computer vision through the Vision Transformer (ViT) ____, achieving performance on par with or surpassing CNNs in large-scale settings such as ImageNet. Unlike convolution-based models, ViT treats an image as a sequence of patches, allowing self-attention to capture long-range dependencies. This global modeling property can be particularly beneficial for identifying subtle lesions or anatomical variations in medical scans. 
	
	Nevertheless, ViT’s data-hungry nature poses challenges for medical imaging, where annotated datasets are often limited. To address these issues, hybrid or specialized Transformer architectures have been proposed, combining convolutional layers with attention blocks to strike a balance between local and global feature learning ____. Recent studies also explore self-supervised or large-scale pre-training tailored for medical images, showing promising results in reducing domain gaps and improving downstream tasks ____.
	
	\subsection{Pre-training in Medical Imaging}
	Transfer learning from ImageNet ____ remains a popular strategy to overcome data scarcity, but the degree of improvement often depends on how closely medical images align with natural-image distributions ____. Indeed, certain studies have reported that, given a sufficiently large in-domain dataset, training from scratch can match or outperform ImageNet-pretrained models ____. In the context of OCT imaging, the domain gap—arising from differences in acquisition physics, layer structures, and brightness distributions—can be substantial ____. As summarized in Table~\ref{tab:relatedsummary}, such a gap may necessitate specialized model architectures or alternative pre-training strategies.
	
	Hence, whether ImageNet-pretrained Vision Transformers consistently yield better results on OCT classification tasks remains an open question. Although preliminary work suggests the benefits of domain-specific pre-training or self-supervised approaches ____, no large-scale, publicly available ViT models pre-trained specifically on OCT data currently exist. This gap motivates our systematic comparison of \textbf{ViT (Pre-trained)} and \textbf{ViT (Scratch)} across different OCT dataset sizes and categories to clarify the role of pre-training in this specialized medical domain.
	%-------------------------------------------------------