\onecolumn
\par\noindent\rule{\textwidth}{1pt}
\begin{center}
{\Large \bf Appendix}
\end{center}
\vspace{-0.1in}
\par\noindent\rule{\textwidth}{1pt}
\appendix

\section{Discussion on Related Work}
\label{sec:discussion}

\paragraph{Implicit Bias of Edge of Stability.} Edge of stability was first coined by \cite{cohen2021gradient}, where they showed that the Hessian of the training loss plateaus around $2/\eta$ when deep models were trained using GD. However, \cite{jastrzebski2020break,jastrzkebski2018relation} previously demonstrated that the step size influences the sharpness along the optimization trajectory. Due to the important practical implications of the edge of stability, there has been an explosion of research dedicated to understanding this phenomenon and its implicit regularization properties. Here, we survey a few of these works.~\cite{damian2023selfstabilization} explained edge of stability through a mechanism called ``self-stabilization'', where they showed that during the momentary divergence of the iterates along the sharpest eigenvector direction of the Hessian, the iterates also move along the negative direction of the gradient of the curvature, which leads to stabilizing the sharpness to $2/\eta$. \cite{agarwala2022second} proved that second-order regression models (the simplest class of models after the linearized NTK model) demonstrate progressive sharpening of the NTK eigenvalue towards a slightly different value
than $2/\eta$.
\cite{arora2022understanding} mathematically analyzed the edge of stability, where they showed that the GD updates evolve along some deterministic flow on the manifold of the minima. 
\cite{lyu2022understanding} showed that the normalization layers had an important role in the edge of stability -- they showed that these layers encouraged GD to reduce the sharpness of the loss surface and enter the EOS regime. \cite{ahn2024learning} established the phenomenon in two-layer networks and find phase transitions for step-sizes in which networks fail to learn ``threshold'' neurons.~\cite{wang2022analyzing} also analyze a two-layer network, but provide a theoretical proof for the change in sharpness across four different phases. \cite{even2024s} analyzed the edge of stability in diagonal linear networks and found that oscillations occur on the sparse support of the vectors. Lastly,~\cite{wu2024implicit} analyzed the convergence at the edge of stability for constant step size GD for logistic regression on linearly separable data. 


\paragraph{Edge of Stability in Toy Functions.}

To analyze the edge of stability in slightly simpler settings, many works have constructed scalar functions to analyze the prevalence of this phenomenon. For example,~\cite{chen2023edge} studied a certain class of scalar functions and identified conditions in which the function enters the edge of stability through a two-step convergence analysis.~\cite{wang2023good} showed that the edge of stability occurs in specific scalar functions, which satisfies certain regularity conditions and developed a global convergence theory for a family of non-convex functions without globally Lipschitz continuous gradients.  \cite{minimal_eos} analyzed local oscillatory behaviors for 4-layer scalar networks with balanced initialization. \cite{song2023trajectory,kalra2023universal} provide analyses of learning dynamics at the EOS in simplified settings such as two-layer networks. \cite{zhu2022quadratic,chen2023stability} study GD dynamics for quadratic models in large learning rate regimes.
Overall, all of these works showed that the necessary condition for the edge of stability to occur is that the second derivative of the loss function is non-zero, even though they assumed simple scalar functions. Our work takes one step further to analyze the prevalence of the edge of stability in DLNs. Although our loss simplifies to a loss in terms of the singular values, they precisely characterize the dynamics of the DLNs for the deep matrix factorization problem.


%Although there exists a rich body of literature analyzing EOS, none of the works are close to the practical setting of deeper networks (either scalar functions or two linear networks are analyzed). In this work, we provide an extensive characterization of EOS in deep linear networks through the deep matrix factorization problem where target matrix is low rank $r$. We theoritically analyze in deep linear models, that GD preserves invariances by aligning the singular vectors of each factor layers and has a balancing effect on the $r$ significant singular values. These implicit biases of GD further allows us to prove the oscillatory phenomenon in EOS for deep linear networks.  Recent work in \cite{zhu2023catapults} \textit{empirically} demonstrated that catapaults in training loss for GD/SGD occur in a low-dimensional subspace spanned by the top eigenvectors of the tangent kernel. Our work theoritically analyzes this phenomenon in deep linear networks trained with GD. 

\paragraph{Deep Linear Networks.} 
Over the past decade, many existing works have analyzed the learning dynamics of DLNs as a surrogate for deep nonlinear networks to study the effects of depth and implicit regularization~\citep{saxe2014exact, arora2018optimization, implicit_dmf,zhang2024structure}. Generally, these works focus on unveiling the dynamics of a phenomenon called ``incremental learning'', where small initialization scales induce a greedy singular value learning approach~\citep{kwon, gissin2020the, saxe2014exact}, analyzing the learning dynamics via gradient flow~\citep{saxe2014exact, CHOU2024101595, implicit_dmf}, or showing that the DLN is biased towards low-rank solution~\citep{yaras2024compressible, implicit_dmf, kwon}, amongst others.
However, these works do not consider the occurence of the edge of stability in such networks. 
On the other hand, while works such as those by~\cite{yaras2024compressible} and~\cite{kwon} have similar observations in that the weight updates occur within an invariant subspace as shown by Proposition~\ref{prop:one_zero_svs_set}, they do not analyze the edge of stability regime.


\section{Additional Results}
\label{sec:additional_exp}

%In Section~\ref{sec:exp_details}, we provide experimental details regarding the experiments in the main text. 

\subsection{Experimental Details}
\label{sec:extra_details}

\paragraph{Bifurcation Plot.}
In this section, we provide additional details regarding the experiments used to generate the figures in the main text. For Figure~\ref{fig:bifurcation}, we consider a rank-3 target matrix $\mbf{M}_\star \in \mbb{R}^{5\times 5}$ with ordered singular values $10, 6, 3$. We use a $3$-layer DLN to fit the target matrix. Since $\sigma_{\star, 1} = 10$, the network enters the EOS regime at
\begin{align*}
    \eta = \frac{2}{L\sigma_{\star, 1}^{2- 2/L}} = 0.0309.
\end{align*}
We show that there exists a two-period orbit after $0.0309 / 2 = 0.0154$, as we do not have a scaling of $1/2$ in the objective function for the code used to generate the figures.

\paragraph{Contour Plots.}
In Figure~\ref{fig:contour}, we considered the toy example 
$$f(\sigma_1, \sigma_2) = \frac{1}{2}(\sigma_2 \cdot \sigma_1 - \sigma_{*})^2,$$
which corresponds to a scalar two-layer network. By Lemma~\ref{lemma:hessian_eigvals}, the stability limit is computed as $\eta = 0.2$, as $L=2$ and $\sigma_{\star} = 5$. To this end, for GD beyond EOS, we use a learning rate of $\eta = 0.2010$, where as we use a learning rate of $\eta = 0.1997$ for GD at EOS. For GF, we plot the conservation flow, and use a learning rate of $\eta = 0.1800$ for stable GD.

\paragraph{DLN and Holder Table Function Plots.}
In Figure~\ref{fig:landscape} and~\ref{fig:figure_grid}, we compared the landscape of DLNs with that of a more complicated non-convex function such as the Holder table function.
To mimic the DLN, we considered the loss function
\begin{align}
\label{eqn:2d_example}
    z = L(x, y) = (x^{4}-0.8)^2 + (y^{4}-1)^2,
\end{align}
which corresponds to a 4-layer network.
Here the eigenvector of the Hessian at the global minima coincides with the $x, y$-axis. We calculate the eigenvalues $\lambda_{1}$ and $\lambda_{2}$ at the minimum $(0.8^{0.25},1)$ 
and plot the dynamics of the iterates for step size range $\frac{2}{\lambda_{2}}> \eta >  \frac{2}{\lambda_{1}}$ and $\eta >  \frac{2}{\lambda_{2}}$. When $\frac{2}{\lambda_{2}}> \eta >  \frac{2}{\lambda_{1}}$ the $x$-coordinate stays fixed at the minima $0.8^{0.25}$ and the $y$-coordinate oscillates around its minimum at $y=1$. This is evident in the landscape figure. Similarly, when $\eta >  \frac{2}{\lambda_{2}}$, oscillations occur in both the $x$ and $y$ direction. The loss landscape $z =L(x,y)$ does not have spurious local minima, so sustained oscillations take place in the loss basin. 

\begin{figure}[t!]
    \centering
    % First Row
    \caption*{Oscillation along Y-axis: $2/\lambda_2>\eta > 2/\lambda_1$}
    \begin{subfigure}{0.245\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/linear_layers/007.png}
        %\caption{Loss Landscape}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/sharpness_plot_dln_gd_h00.1_post10_lr0.075.png}
        %\caption{Sharpness}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.37\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/x_trajectory_plot_dln_gd_h00.1_post10_lr0.075.png}
        %\caption{Oscillatory Components}
    \end{subfigure}
    
    \vspace{0.3cm} % Space between rows
    
    % Second Row
    \caption*{Oscillation along both X and Y-axis: $\eta > 2/\lambda_2$}
    \begin{subfigure}{0.25\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/linear_layers/008.png}
        \caption*{Loss Landscape}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3675\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/linear_layers/sharpness_plot_dln_gd_h00.1_post10_lr0.088.png}
        \caption*{Sharpness}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.3675\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/linear_layers/x_trajectory_plot_dln_gd_h00.1_post10_lr0.088.png}
        \caption*{Oscillatory Components}
    \end{subfigure}
    
    \caption{Demonstration of the EOS dynamics of a 2-dimensional depth-4 scalar network as shown in Equation~(\ref{eqn:2d_example}). $X, Y$ axes are the eigenvectors of the Hessian with eigenvalues $\lambda_{1}$ and $\lambda_{2}$ respectively. Top: when $\eta > 2/\lambda_1$, the $X$ component remains fixed, while the $Y$ component oscillates with a  periodicity of 2. Bottom: for $\eta > 2/\lambda_2$, the iterates oscillation in both directions.}
    \label{fig:figure_grid}
\end{figure}


\begin{figure}[h!]
    \centering
    \begin{subfigure}[t!]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/new_lr_cycle.png}
        
    \end{subfigure}%
    \hfill
    \begin{subfigure}[t!]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/holder1.png}
        %\caption{Iterates catapults out of a local basin whenever $\eta$ is increased and jumps out to surface where sharpness is just about $\frac{2}{\eta}$.}
    \end{subfigure}
    
    \caption{EOS dynamics at various step learning rates from the Holder table function. Left: plot of the learning rate steps and sharpness, showing that 
        sharpness follows the EOS limit $2/\eta$. Right: Plot showing that the iterates catapult out of a local basin when the learning rate is increased and jumps out to a surface where the sharpness is about $2/\eta$.}
    \label{fig:two_figures}
\end{figure}




For the non-convex landscape as shown in Figure~\ref{fig:landscape} and~\ref{fig:two_figures}, we consider the Holder table function: 
\begin{align*}
    f(x, y) = - \left| \sin(x) \cos(y) \exp \left( 1 - \frac{\sqrt{x^2 + y^2}}{\pi} \right) \right|.
\end{align*}
By observation, we initialize near a sharp minima and run GD with an increasing learning rate step size as shown in the lefthand side of Figure~\ref{fig:two_figures}.
When the learning rate is fixed, we observe that oscillations take place inside the local valley, but when learning rate is increased, it jumps out of the local valley to find a flatter basin. Similar to the observations by \cite{cohen2021gradient}, the sharpness of the GD iterates are ``regulated'' by the threshold $2/\eta$, as it seems to closely follow this value as shown in Figure~\ref{fig:two_figures}.

Overall, these examples aim to highlight the difference in linear and complex loss landscapes. The former consists of \emph{only} saddles and global minima, and hence (stably) oscillate about the global minimum. However, in more complicated non-convex landscapes, sharpness regularization due to large learning rates enable catapulting to flatter loss basins, where sharpness is smaller than $2/\eta$.



\subsection{Initialization Outside Singular Vector Invariant Set}


In this section, we present an initialization example that is outside the singular vector stationary set. We consider the following initialization:
\begin{align}
    \mbf{W}_L(0) = \mbf{0}, \quad \quad\quad \mbf{W}_\ell(0) = \alpha \mbf{P}_\ell, \quad \forall \ell \in [L-1],
\end{align}
where $\mbf{P}_\ell \in \mbb{R}^{d\times d}$ is an orthogonal matrix. Note that here for $\ell>1$, the singular vectors do not align and lies outside the SVS set we defined in Proposition~\ref{prop:one_zero_svs_set}. 
We consider the deep matrix factorization problem with a target matrix $\mbf{M}_\star \in \mbb{R}^{d\times d}$, where $d=100$, $r=5$, and $\alpha = 0.01$. We empirically obtain that the decomposition after convergence admits the form:
    \begin{align}
        \mbf{W}_L(t) &= 
        \mbf{U}^\star
        \begin{bmatrix}
            \mbf{\Sigma}_L(t) & \mbf{0} \\
            \mbf{0} & \mbf{0}
        \end{bmatrix} \left[\left(\prod_{i=L-1}^1{\mbf{P}_{i}}\right)\mbf{V^\star}\right]^{\top}, \\
        \mbf{W}_{\ell}(t) &= \left[\left(\prod_{i=\ell}^1{\mbf{P}_{i}}\right)\mbf{V^\star}\right]
        \begin{bmatrix}
            \mbf{\Sigma}_{\ell}(t) & \mbf{0} \\
            \mbf{0} & \alpha\mbf{I}_{d-r}
        \end{bmatrix} \left[\left(\prod_{i=\ell-1}^1{\mbf{P}_{i}}\right)\mbf{V^\star}\right]^{\top},
        \quad \forall \ell \in [2, L-1], \\
        \mbf{W}_{1}(t) &= \mbf{P}_{1}\mbf{V}^{\star} \begin{bmatrix}
            \mbf{\Sigma}_{1}(t) & \mbf{0} \\
            \mbf{0} & \alpha\mbf{I}_{d-r}
        \end{bmatrix} \mbf{V}^{\star\top},
    \end{align} 
    where  
    $\mbf{W}_L(0) = \mbf{0}$ and $\mbf{W}_{\ell}(0) = \alpha \mbf{P}_{l}$, $\forall\ell \in [L-1]$.
    The decomposition after convergence lies in the SVS set as the singular vectors now align with each other. This demonstrates an example where even when the initialization is made outside the SVS set, GD aligns the singular vectors such that after certain iterations it lies in the SVS set.
    


\begin{figure}[h!]
    \centering
     \begin{subfigure}[b]{0.495\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/left_svec_error.png}
         \caption*{Left Singular Vectors}
     \end{subfigure}
         \begin{subfigure}[b]{0.495\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/right_svec_error.png}
         \caption*{Right Singular Vectors}
     \end{subfigure}
    \caption{Empirical verification of the decomposition for initialization with orthogonal matrices (lying outside SVS set) in that after some GD iterations, the singular vectors of the intermediate matrices align to lie within SVS set, displaying singular vector invariance.}
    \label{fig:verify_conj}
\end{figure}



\begin{figure}[h!]
    \centering
     \begin{subfigure}[b]{0.315\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/eos_balance_001.png}
         \caption*{$\alpha = 0.01$}
     \end{subfigure}
     \begin{subfigure}[b]{0.315\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/eos_balance_010.png}
         \caption*{$\alpha = 0.10$}
     \end{subfigure}
         \begin{subfigure}[b]{0.315\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/eos_balance_030.png}
         \caption*{$\alpha = 0.30$}
     \end{subfigure}
    \caption{Observing the balancedness between the singular value initialized to $0$ and a singular value initialized to $\alpha$. The scattered points are successive GD iterations (going left to right). The initial gap between the two values is larger for a larger $\alpha$, but quickly gets closer over more GD iterations.}
    \label{fig:assumption}
\end{figure}

\begin{figure}[t!]
    \centering
     \begin{subfigure}[t!]{\textwidth}
         \centering
        \includegraphics[width=0.95\textwidth]{figures/eos_init_0_01_lr3.pdf}
     \end{subfigure}
          \newline
     \centering
     \begin{subfigure}[t!]{\textwidth}
         \centering
         \includegraphics[width=0.95\textwidth]{figures/eos_init_0_01_lr1.pdf}
         
     \end{subfigure}
     \newline
     \centering
     \begin{subfigure}[t!]{\textwidth}
         \centering
         \includegraphics[width=0.95\textwidth]{figures/eos_init_0_01_lr2.pdf}
         
     \end{subfigure}
    \caption{Plots of the training loss, singular value magnitude, and the balancing gap over iterations for different learning rates: $\eta = 0.030, 0.032, 0.034$ (top to bottom). When the learning rate is stable ($\eta < 0.031$ since the top singular value is $\sigma_{\star, 1} = 10$), the balancing gap plateaus, whereas the balancing gap goes strictly to zero when the oscillations occur. }
    \label{fig:oscillations_period}
\end{figure}



\subsection{Additional Experiments for Balancing, Singular Vector Invariance, and Theory}
\label{sec:extra_balance_svs}

Our theory relied on two tools and assumptions: balancing of singular values and stationarity of the singular vectors. In this section, we investigate how the dynamics at EOS are affected if these two assumptions do not hold.

\paragraph{Balancing.}
First, we present additional experimental results on Proposition~\ref{prop:balancing} and how close the iterates become for different initialization scales. To this end, we consider the same setup from the previous section, where we have a target matrix $\mbf{M}_\star \in \mbb{R}^{d\times d}$, where $d=100$, $r=5$, and varying initialization $\alpha$.  In Figure~\ref{fig:assumption}, we observe that for larger values of $\alpha$, the balancing quickly occurs, whereas for smaller values of $\alpha$, the balancing is almost immediate. This is to also highlight that our bound on $\alpha$ in Proposition~\ref{prop:balancing} may be an artifact of our analysis, and can choose larger values of $\alpha$ in practice.

To this end, we also investigate how large $\alpha$ can be until Proposition~\ref{prop:balancing} no longer holds. We consider the dynamics of a $3$-layer DLN to fit a target matrix $\mbf{M}_\star \in \mbb{R}^{10 \times 10}$ of rank-3 with ordered singular values $10, 8, 6$. We use a learning rate of $\eta = 0.0166$, which corresponds to oscillations in the top-2 singular values. In Figure~\ref{fig:no_balance_hold}, we show the dynamics of when the initialization scale is $\alpha = 0.01$ and $\alpha = 0.5$, where balancing holds theoretically for the former but not for the latter. Clearly, we observe that balancing does not hold for $\alpha = 0.5$. However, examining the middle plots reveals that the oscillations in the singular values still have the same amplitude in both cases and for both singular values. 


\paragraph{Singular Vector Stationarity.} 


Throughout this paper, we considered two initializations in Equations~(\ref{eqn:balanced_init}) and~(\ref{eqn:unbalanced_init}), where balancing holds immediately and one where balancing holds for a sufficiently small initialization scale. In this section, we investigate different initializations with aim to observe (i) if they do not converge to the SVS set and (ii) how they affect the oscillations if they do not belong to the SVS set. To this end, we consider the following:
\begin{align}
    &\mbf{W}_L(0) = \mbf{0},  \quad \mbf{W}_\ell(0) = \alpha \mbf{I}_d, \quad \forall \ell \in [L-1],\tag{Original} \\
    &\mbf{W}_L(0) = \mbf{0},  \quad \mbf{W}_\ell(0) = \alpha \mbf{P}_\ell, \quad \forall \ell \in [L-1],\tag{Orthogonal} \\
    &\mbf{W}_L(0) = \mbf{0},  \quad \mbf{W}_\ell(0) = \alpha \mbf{H}_\ell, \quad \forall \ell \in [L-1],\tag{Random}
\end{align}
where $\mbf{P}_\ell$ is an orthogonal matrix and $\mbf{H}_\ell$ is a random matrix with Gaussian entries. 
For all of these initialization schemes, we consider the same setup as in the balancing case, with an initialization scale of $\alpha = 0.01$. To observe if singular vector stationarity holds, we consider the subspace distance as follows:
\begin{align}
\label{eqn:subs_dist}
    \mathrm{Subspace \,\, Distance} = \|\mbf{U}_{\ell-1, r}^\top \mbf{V}_{\ell, r} - \mbf{I}_r\|_{\mathsf{F}},
\end{align}
where $\mbf{U}_{\ell,r}$ and $\mbf{V}_{\ell, r}$ are the top-$r$ left and right singular vectors of layer $\mbf{W}_\ell$, respectively. Since Proposition~\ref{prop:svs_set} implies that the intermediate singular vectors cancel, the initialization converges to the SVS set if the subspace distance goes to zero. 
In Figure~\ref{fig:svs_set_test}, we plot the dynamics for all of the initializations. Generally, we observe that the subspace distance for all cases go to zero, validating the use of the SVS set for analysis purposes.

\begin{figure}[t!]
    \centering
     \begin{subfigure}[t!]{\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/eos_init_0_01.pdf}
     \end{subfigure}
     \newline
     \centering
     \begin{subfigure}[t!]{\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/eos_init_0_9.pdf}
         
     \end{subfigure}
    \caption{Top: EOS dynamics of a 3-layer DLN with initialization scale $\alpha=0.01$, where balancing theoretically holds. Bottom: EOS dynamics of the DLN with initialization scale $\alpha = 0.5$. While the balancing does not hold for $\alpha=0.5$, the oscillations in the singular values are still prevalent, with the same amplitude.}
\label{fig:no_balance_hold}
\end{figure}



\begin{figure}[t!]
    \centering
     \begin{subfigure}[t!]{\textwidth}
         \centering
        \includegraphics[width=0.75\textwidth]{figures/eos_init_eye.pdf}
     \end{subfigure}
          \newline
     \centering
     \begin{subfigure}[t!]{\textwidth}
         \centering
         \includegraphics[width=0.75\textwidth]{figures/eos_init_ortho.pdf}
         
     \end{subfigure}
     \newline
     \centering
     \begin{subfigure}[t!]{\textwidth}
         \centering
         \includegraphics[width=0.75\textwidth]{figures/eos_init_rand.pdf}
         
     \end{subfigure}
    \caption{EOS dynamics of a 3-layer DLN for different initializations where it all converges to the SVS set. The subspace distance is defined in Equation~(\ref{eqn:subs_dist}). Top: Dynamics with the original identity initialization. Middle: Dynamics with orthogonal initialization. Bottom: Dynamics with random initialization.}
    \label{fig:svs_set_test}
\end{figure}



\paragraph{Additional Results.}

In this section, we provide more experimental results to corroborate our theory. Recall that in Lemma~\ref{lemma:hessian_eigvals}, we proved that the learning rate needed to enter the EOS is a function of the depth, and that deeper networks can enter EOS using a smaller learning rate. To verify this claim, we provide an additional experiment where the target matrix is $\mbf{M}_\star \in \mbb{R}^{5\times 5}$ with the top singular value set to $\sigma_{\star, 1} = 0.5$. We use an initialization scale of $\alpha = 0.01$. In Figure~\ref{fig:depth_lr}, we can clearly see that shallower networks need a larger learning rate, and vice versa to enter EOS. Here, black refers to stable learning and white refers to regions in which oscillations occur (EOS regime).

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/depth_vs_lr.pdf}
    \caption{Demonstrating that deeper networks requires a smaller learning rate to enter the EOS regime for DLNs, as implied by Proposition~\ref{prop:balancing}, for a target matrix with top singular value $\sigma_{\star,1} = 0.5$ and initialization $\alpha = 0.01$. Black refers to stable learning and white refers to regions in which oscillations in the loss and singular values occur. The EOS limit exactly matches $\eta = 2/L \sigma^{2-\frac{2}{L}}_{\star,i} $.}
    \label{fig:depth_lr}
\end{figure}





\subsection{Periodic and Free Oscillations}

In this section, we present additional experiments on oscillation and catapults in both deep linear and nonlinear networks to supplement the results in the main paper. First, we consider a 3-layer MLP without bias terms for the weights, with each hidden layer consisting of 1000 units. The network is trained using MSE loss with a learning rate of $\eta = 4$, along with random weights scaled by $\alpha = 0.01$ and full-batch gradient descent on a 5K subset of the MNIST dataset, following~\cite{cohen2021gradient}. The motivation for omitting bias terms comes from the findings of~\cite{zhang2024when}, where they provably show that a ReLU network without bias terms behaves similarly to a linear network. With this in mind, we aimed to investigate how oscillations manifest in comparison to deep linear networks (DLNs). In Figure~\ref{fig:mlp_bias_free}, we plot the training loss, top-5 singular values, and sharpness throughout training. Interestingly, despite the non-convexity of the loss landscape, the oscillations appear to be almost periodic across all three plots. It would be of great interest to theoretically study the behavior of EOS for this network architecture and determine whether our analyses extend to this case as well.




\begin{figure}[h!]
    \centering
    \includegraphics[width=\textwidth]{figures/nonlinear/fig_relu_mnist.png}
    \caption{Plot of the training loss, singular values, and sharpness for an MLP network with no bias. Similar to the DLN case, there are oscillations in each of the plots throughout iterations.}
    \label{fig:mlp_bias_free}
\end{figure}

Next, we consider the DLN setting to corroborate our result from Theorem~\ref{thm:align_thm}. We consider modeling rank-3 target matrix with singular values $\sigma_{\star, i} = \{10, 9, 8\}$ with a 3-layer DLN with initialization scale $\alpha = 0.1$. By computing the sharpness under these settings, notice that $2 / \lambda_1 = L\sigma_{\star, 1}^{2 - \frac{2}{L}} \approx 0.01547$ and $2/\lambda_2 \approx 0.01657$. In Figure~\ref{fig:progressive_eta_dln}, we use learning rates near these values, and plot the oscillations in the singular values. Here, we can see that the oscillations follow exactly our theory. 








Lastly, we provide additional experiments demonstrating stronger oscillation in feature directions as measured by the singular values. To this end, we consider a 4-layer MLP with ReLU activations with hidden layer size in each unit of 200 for classification on a subsampled 20K set on MNIST and CIFAR-10. In Figure~\ref{fig:non-lin}, we show that the oscillations in the training loss are artifacts of jumps only in the top singular values, which is also what we observe in the DLN setting.


\begin{figure}[h!]
    \centering
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/nonlinear/complete_fig_data_mnist-20k_lr_0.8_arch_fc-relu-depth4.png}
        \caption*{MNIST Dataset with 4-Layer MLP}
        \label{fig:first_image}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.485\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/nonlinear/complete_fig_data_cifar10-20k_lr_0.8_arch_fc-relu-depth4.png}
        \caption*{CIFAR-10 Dataset with 4-Layer MLP}
        \label{fig:second_image}
    \end{subfigure}
    \caption{Prevalence of oscillatory behaviors in top subspaces in $4$-layer networks with ReLU activations on two different datasets.}
    \label{fig:non-lin}
\end{figure}



\begin{figure}[ht]
    \centering
    %\captionsetup{justification=centering}
    % Specify the filename for each figure
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/train_loss_lr_030.pdf}
        \caption*{\footnotesize  Train Loss ($\eta = 0.0300$) }
        \label{fig:1}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer1_lr_030.pdf}
        \caption*{\footnotesize   Layer 1 $\sigma_i$ ($\eta = 0.0300$)}
        \label{fig:2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer2_lr_030.pdf}
        \caption*{\footnotesize  Layer 2 $\sigma_i$ ($\eta = 0.0300$)}
        \label{fig:3}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer3_lr_030.pdf}
        \caption*{\footnotesize  Layer 3 $\sigma_i$ ($\eta = 0.0300$)}
        \label{fig:4}
    \end{subfigure}

    \par\bigskip % Adds space between the rows
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/train_loss_lr_031.pdf}
        \caption*{\footnotesize  Train Loss ($\eta = 0.031$) }
        \label{fig:1}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer1_lr_031.pdf}
        \caption*{\footnotesize  Layer 1 $\sigma_i$ ($\eta = 0.0310$)}
        \label{fig:2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer2_lr_031.pdf}
        \caption*{\footnotesize  Layer 2 $\sigma_i$ ($\eta = 0.0310$)}
        \label{fig:3}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer3_lr_031.pdf}
        \caption*{\footnotesize  Layer 3 $\sigma_i$ ($\eta = 0.0310$)}
        \label{fig:4}
    \end{subfigure}
        \par\bigskip % Adds space between the rows
        
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/train_loss_lr_0325.pdf}
        \caption*{\footnotesize  Train Loss ($\eta = 0.0325$) }
        \label{fig:1}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer1_lr_0325.pdf}
        \caption*{\footnotesize  Layer 1 $\sigma_i$ ($\eta = 0.0325$)}
        \label{fig:2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer2_lr_0325.pdf}
        \caption*{\footnotesize  Layer 2 $\sigma_i$ ($\eta = 0.0325$)}
        \label{fig:3}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer3_lr_0325.pdf}
        \caption*{\footnotesize  Layer 3 $\sigma_i$ ($\eta = 0.0325$)}
        \label{fig:4}
    \end{subfigure}


        \par\bigskip % Adds space between the rows
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/train_loss_lr_0335.pdf}
        \caption*{\footnotesize  Train Loss ($\eta = 0.0335$) }
        \label{fig:1}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer1_lr_0335.pdf}
        \caption*{\footnotesize  Layer 1 $\sigma_i$ ($\eta = 0.0335$)}
        \label{fig:2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer2_lr_0335.pdf}
        \caption*{\footnotesize  Layer 2 $\sigma_i$ ($\eta = 0.0335$)}
        \label{fig:3}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer3_lr_0335.pdf}
        \caption*{\footnotesize  Layer 3 $\sigma_i$ ($\eta = 0.0335$)}
        \label{fig:4}
    \end{subfigure}

            \par\bigskip % Adds space between the rows
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/train_loss_lr_035.pdf}
        \caption*{\footnotesize  Train Loss ($\eta = 0.0350$) }
        \label{fig:1}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer1_lr_035.pdf}
        \caption*{\footnotesize  Layer 1 $\sigma_i$ ($\eta = 0.0350$)}
        \label{fig:2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer2_lr_035.pdf}
        \caption*{\footnotesize  Layer 2 $\sigma_i$ ($\eta = 0.0350$)}
        \label{fig:3}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer3_lr_035.pdf}
        \caption*{\footnotesize  Layer 3 $\sigma_i$ ($\eta = 0.0350$)}
        \label{fig:4}
    \end{subfigure}

            \par\bigskip % Adds space between the rows
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/train_loss_lr_036.pdf}
        \caption*{\footnotesize  Train Loss ($\eta = 0.0360$) }
        \label{fig:1}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer1_lr_036.pdf}
        \caption*{\footnotesize  Layer 1 $\sigma_i$ ($\eta = 0.0360$)}
        \label{fig:2}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer2_lr_036.pdf}
        \caption*{\footnotesize  Layer 2 $\sigma_i$ ($\eta = 0.0360$)}
        \label{fig:3}
    \end{subfigure}\hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/layerwise_oscs/layer3_lr_036.pdf}
        \caption*{\footnotesize  Layer 3 $\sigma_i$ ($\eta = 0.0360$)}
        \label{fig:4}
    \end{subfigure}
    
    \caption{Depiction of the training loss and the singular values of each weight matrix for fitting a rank-3 matrix with singular values $10, 9.5, 9$. The weights enter the EOS regime based on the learning rate $\eta > 2/K$, where $K = L\sigma_{\star, i}^{2-2/L}$ and $L=3$. For a sufficiently large learning rate (e.g., $\eta = 0.04$), the singular values start to enter a period-4 orbit. 
    %When $\eta=2/L\sigma^{2-\frac{2}{L}}_{\star,1} \approx 0.0154$, oscillation occur on the first singular value. When $\eta=2/\sum_{\ell=0}^{L-1} \left(\sigma_{\star, 1}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, 2}^{\frac{1}{L}\ell}\right)^2 \approx 0.0165$, oscillation occur on second singular value and so on.
    }
    \label{fig:progressive_eta_dln}
\end{figure}

