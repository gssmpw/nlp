 \section{Deep Matrix Factorization Beyond the Edge of Stability}



When using a large learning rate, the learning dynamics can typically be separated into two distinct stages: (i) progressive sharpening and (ii) the edge of stability. Within the progressive sharpening stage, the sharpness lies below $2/\eta$ and tends to continually rise.  Our goal is to analyze the EOS stage under the deep matrix factorization formulation. Here, we observe that the training loss fluctuates due to layerwise singular value oscillations, as illustrated in Figure~\ref{fig:ps_eos}.

%Here, we observe that the training loss oscillates in a two-period orbit rather than decreasing over iterations, as illustrated in Figure~\ref{fig:ps_eos} and rigorously shown in the following sections.


\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/svector_alignment_diff_lr.png}
    \caption{Illustrations of the singular vector and value evolution of the end-to-end DLN. The singular vectors of the network remain static across all iterations, as suggested by the singular vector stationary set, regardless of the learning rate. The angle between the true singular vectors and those of the network remains aligned throughout. The first singular values undergo oscillations in the large $\eta$ regime, whereas they remain constant in the small $\eta$ regime.}
    \label{fig:svec_alignment}
\end{figure}

\subsection{Main Results}


Before we present our main results, we provide a definition of what we refer to as a strict balanced state of the singular values of the weight matrices. The parameters are said to be in a strict balanced state if the singular values of each weight matrix take the same values across all layers.

\begin{definition}[Strict Balanced State]
The parameters $\mbf{\Theta}$ of the DLN from Equation~(\ref{eqn:deep_mf}) are said to be in a strict balanced state if for some $t\geq 0$
    \begin{align*}
        \sigma_i(\mbf{W}_\ell(t)) = \sigma_i(\mbf{W}_k(t)), \quad  \forall i \in [r], \quad \forall \ell, k \in [L],
    \end{align*}
    where $\sigma_{i}(\mbf{W}_{\ell})$ denotes the $i$-th singular value of the $\ell$-th layer and $r$ is the rank of the matrix $\mbf{M}_\star$.
\end{definition}

It is straightforward to show that the parameters are in a strictly balanced state for all $t\geq 0$ if we initialize the singular values to be the same across all weight matrices $\mbf{W}_\ell$. Hence, it immediately holds that the balanced initialization is in a strictly balanced state. However, the one-zero initialization in Equation~(\ref{eqn:init}) sets the singular values of $\mbf{W}_L$ to zero, meaning the parameters are not initially in a strictly balanced state.
However, in Lemma~\ref{lemma:balancing} of Section~\ref{sec:used_tools}, we prove that the balancing increasingly occurs throughout GD iterations within the EOS regime. Consequently, we assume a strictly balanced state for the remainder of this paper and analyze the EOS regime in relation to the balanced minimum.
Next, we derive the eigenvalues of the Hessian at convergence, such that we can identify the learning rate needed to enter the EOS regime for DLNs.

\begin{lemma}[Eigenvalues of Hessian at the Balanced Minimum]
\label{lemma:hessian_eigvals}
    Consider running GD on the deep matrix factorization loss $f(\mbf{\Theta})$ defined in Equation~(\ref{eqn:deep_mf}). The \edit{set of all} non-zero eigenvalues of the training loss Hessian \emph{at the balanced minimum} is given by
    \begin{align*}
       \lambda_{\mbf{\Theta}} = \underbrace{\left\{L \sigma_{\star, i}^{2 - \frac{2}{L}}, \sigma_{\star, i}^{2 - \frac{2}{L}}\right\}_{i=1}^r }_{\text{self-interaction}} \, \bigcup \, \underbrace{\left\{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, i}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, j}^{\frac{1}{L}\ell}\right)^2\right\}_{i\neq j}^{r}}_{\text{interaction with other singular values}} \,\bigcup \, \underbrace{\left\{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, k}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \alpha^{\ell}\right)^2\right\}_{k = 1}^{r}}_{\text{interaction with initialization}} 
    \end{align*}
    where $\sigma_{\star, i}$ is the $i$-th singular value of the target matrix $\mbf{M}_\star \in \mbb{R}^{d\times d}$,  $\alpha \in \mbb{R}$ is the initialization scale, $L$ is the depth of the network, and the second element of the set under ``self-interaction'' has a multiplicity of $d-r$.
\end{lemma}


We defer all of the proofs to Appendix~\ref{sec:proofs}.
Let $\lambda_i$ denote the $i$-th largest eigenvalue of the Hessian. 
By Lemma~\ref{lemma:hessian_eigvals}, we observe that the sharpness is equal to $\lambda_1 = \|\nabla^2 f(\mbf{\Theta})\|_2 = L\sigma_{\star, 1}^{2- \frac{2}{L}}$ at the balanced minimum. \edit{In Lemma~\ref{lemma:flattest}, we show that the sharpness at the balanced minimum is the flattest, and hence $L\sigma_{\star, 1}^{2 - \frac{2}{L}}$ represents the smallest sharpness value among all global minima.
Thus, if $\eta$ is set such that $\eta > 2/\lambda_1$, oscillations in the loss will occur, as the step size is large enough to induce oscillations even in the flattest region.}   Interestingly, notice that all non-zero eigenvalues are a function of network depth. For a deeper network, the sharpness will be larger, implying that a smaller learning rate can be used to drive the DLN into EOS. This provides a unique perspective on how the learning rate should be chosen as networks become deeper and explains the observation made by~\cite{cohen2021gradient}, who observed that sharpness scales with the depth of the network.
Equipped with the eigenvalues, we show in the following result that oscillations occur in a two-period orbit along the $i$-th eigenvector direction, given that the learning rate is set to be greater than $2/\lambda_i$.

\begin{theorem}[Stable Subspace Oscillations]
\label{thm:stable_oscillations}
Let $\alpha' \coloneqq \left( \ln\left( \frac{2\sqrt{2}}{\eta \lambda_1} \right) \cdot \frac{ \sigma_{\star, 1}^{4/L}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{1/4}$.
Consider running GD on the loss in~(\ref{eqn:deep_mf}) with initialization scale $0<\alpha < \alpha'$.
    If $\eta = \frac{2}{K}$ with $\lambda_i \leq K< \lambda_{i+1}$, then $2$-period orbit oscillation occurs in the direction of $\Delta_i$, where $\lambda_i$ and $\Delta_i$ denote the $i$-th largest eigenvalue and  eigenvector of the Hessian at the balanced minimum, respectively.
\end{theorem}


The complete proof is provided in Appendix~\ref{sec:proofs_oscillations}, where we derive all eigenvectors at the balanced minimum and demonstrate that the necessary conditions from Lemma~\ref{lemma:chen-bruna} (restated from~\cite{chen2023edge}) are satisfied for a two-period orbit. The condition on the initialization scale is chosen to ensure balanced behavior, as demonstrated in Lemma~\ref{lemm:balancing}. \qq{fix}
To understand Theorem~\ref{thm:stable_oscillations} more clearly, consider the first eigenvector of the Hessian, which is
\begin{align*}
        \Delta_1 = \frac{1}{\sqrt{L}}\cdot \mathrm{vec}\left(\mbf{u}_{\star, 1} \mbf{v}_{\star, 1}^\top, \mbf{v}_{\star, 1}  \mbf{v}_{\star, 1}^\top, \ldots, \mbf{v}_{\star, 1}  \mbf{v}_{\star, 1}^\top \right),
\end{align*}
where $\mbf{u}_{\star, 1}, \mbf{v}_{\star, 1} \in \mbb{R}^d$ are the first left and right singular vectors of $\mbf{M}_\star$, respectively. By Proposition~\ref{prop:one_zero_svs_set}, we know that the each weight layer takes the form  $\mbf{W}_\ell = \mbf{V}_\star \mbf{\Sigma}_\ell \mbf{V}_\star$ \qq{transpose?} for all $\ell \in [L-1]$ and $\mbf{W}_L = \mbf{U}_\star \mbf{\Sigma}_L \mbf{V}_\star$ at convergence, starting from the unbalanced initialization. By stacking and flattening these weights, consider the direction
\begin{align*}
    \widetilde{\Delta} \coloneqq \sum_{i=1}^d \mathrm{vec}\left(\sigma_{L, i}\cdot\mbf{u}_{\star, i} \mbf{v}_{\star, i}^\top,\sigma_{L-1, i}\cdot \mbf{v}_{\star, i}  \mbf{v}_{\star, i}^\top, \ldots, \sigma_{1, i}\cdot\mbf{v}_{\star, i}  \mbf{v}_{\star, i}^\top \right).
\end{align*}
Since $\widetilde{\Delta}^\top \Delta_1$ is only non-zero in the rank-$1$ components of $\widetilde{\Delta}$, this implies that if $\lambda_1 \leq K < \lambda_2$ from Theorem~\ref{thm:stable_oscillations},
the oscillations only occur in the rank-$1$ components of the weights. The following result substantiates this claim by demonstrating that, with an appropriately chosen learning rate, oscillations occur in the singular values in the top-$r$ directions.

\begin{theorem}[\edit{Rank-$p$} Oscillations]
\label{thm:align_thm}
\edit{Let $\mbf{M}_\star = \mbf{U}_\star \mbf{\Sigma}_\star \mbf{V}_\star^\top$ denote the SVD of the target matrix and define $S_p\coloneqq L \sigma^{2-\frac{2}{L}}_{\star,p}$ and $K'_p \coloneqq \mathrm{max} \left\{ S_{p+1},\frac{S_p}{2\sqrt{2}}\right\}$.
If we run GD on the deep matrix factorization loss with initialization scale $\alpha < \alpha'$ from Theorem~\ref{thm:stable_oscillations} and learning rate $\eta = \frac{2}{K}$, where $K'_p < K< S_p$, then under strict balancing, each weight matrix $\mbf{W}_{\ell} \in \mbb{R}^{d\times d}$ oscillates around the balanced minimum in a $2$-period fixed orbit ($j \in \{1,2\}$) as follows:
\begin{align*}
    \mbf{W}_L &= \sum_{i=1}^p\rho_{i, j} \cdot \mbf{u}_{\star, i}\mbf{v}_{\star, i}^{\top} + \sum_{k=p+1}^d \sigma_{\star, k}^{1/L} \cdot \mbf{u}_{\star, k}\mbf{v}_{\star, k}^{\top}, \\
    \mbf{W}_\ell &=\underbrace{\sum_{i=1}^p\rho_{i, j} \cdot \mbf{v}_{\star, i}\mbf{v}_{\star, i}^{\top} }_{\text{oscillation subspace}}+ \underbrace{\sum_{k=p+1}^r \sigma_{\star, k}^{1/L}\cdot \mbf{u}_{\star, k}\mbf{v}_{\star, k}^{\top} + \sum_{m=r+1}^d \alpha\cdot \mbf{u}_{\star, m}\mbf{v}_{\star, m}^{\top}}_{\text{stationary subspace}}, \quad \forall\ell \in [L-1],
\end{align*}
where $\rho_{i, 1} \in \left(0, \sigma_{\star, i}^{1/L}\right)$ and $\rho_{i, 2} \in \left(\sigma_{\star, i}^{1/L}, (2\sigma_{\star, i})^{1/L}\right)$} are the two real roots of the polynomial $g(\rho_i)=0$, where
\begin{align*}
    g(\rho_i) = \rho_i^L\cdot\frac{1+\left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right)^{2L-1}}{1+\left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right)^{L-1}} - \sigma_{\star, i}.
\end{align*}
\end{theorem}

%\ag{Should we have the general oscillation as a lemma?
%Also where are we having the digoanl liner network oscillation?}
%\paragraph{Remarks.}
In contrast to Theorem~\ref{thm:stable_oscillations}, Theorem~\ref{thm:align_thm} explicitly identifies the subspaces that exhibit a two-period orbit based on the range of the learning rate. It also provides a rough characterization of the oscillation amplitude, which is determined by $\rho_{i, 1}$ and $\rho_{i, 2}$—values below and above the balanced minimum, respectively. 
%Hence, the oscillations occur around the minima.
Since there is no closed-form solution for an arbitrary higher-order polynomial, $\rho_{i, 1}$ and $\rho_{i, 2}$ are defined as solutions to the polynomial $g(\rho_i)$.
Overall, this aims to theoretically explain why (i) oscillations occur primarily within the top subspaces of the network, as observed by~\cite{zhu2023catapults}, and (ii) oscillations are more pronounced in the directions of stronger features, as measured by the magnitudes of their singular values.


\begin{figure}[t!]
    \centering
     \begin{subfigure}[t!]{0.495\textwidth}
         \centering
        \includegraphics[width=0.85\textwidth]{figures/eos_balanced_contour_1.pdf}
        \caption*{$\eta < 0.2$}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t!]{0.495\textwidth}
         \centering
         \includegraphics[width=0.85\textwidth]{figures/eos_balanced_contour_2.pdf}
         \caption*{$\eta > 0.2$}
     \end{subfigure}
    \caption{Illustration of the GD trajectories for two different learning rates for minimizing the function $f(\sigma_1, \sigma_2) = \frac{1}{2}(\sigma_2 \cdot \sigma_1 - 5)^2$, starting from an unbalanced initial point. The sharpness at the balanced minimum is $10$, and so the learning rate needed to enter EOS is $\eta = 0.2$. These plots show that while GD initially arrives at an unbalanced solution in both cases, the iterates oscillate about the other minima to arrive at the balanced solution within the EOS regime.
    }
    
    \label{fig:contour}
\end{figure}



Notice that Theorem~\ref{thm:align_thm} demonstrates the existence of a two-period orbit only around the balanced minimum. While this assumption was initially made for ease of analysis, we empirically observe that the two-period orbit actually \emph{only} occurs around the balanced solution.
To illustrate this claim, in Figure~\ref{fig:contour}, we provide a plot of the GD trajectory for minimizing a two-layer scalar loss $f(\sigma_1, \sigma_2) = \frac{1}{2}(\sigma_2 \cdot \sigma_1 - 5)^2$, starting from an unbalanced initial point $(\sigma_1, \sigma_2) = (1.5, 2.25)$. Notice that, by Lemma~\ref{lemma:hessian_eigvals}, the sharpness around the balanced minimum is $L\sigma_{\star,1}^{2-2/L} = 10$, and thus the necessary learning rate to enter the EOS regime is $\eta = 0.2$.
We plot the GD trajectory under two cases for the learning rate: (i) slightly below the EOS learning rate, $\eta = 0.1999$, and (ii) slightly above it, $\eta = 0.2010$. When $\eta = 0.2010$, GD first arrives at an unbalanced solution, then oscillates until it reaches the balanced minimum, where it sustains a two-period orbit around the balanced solution. The other minima are too narrow to sustain the oscillations induced by the large learning rate, causing the GD iterates to bounce out of those minima and settle at the flattest, balanced solution.
When the learning rate is slightly below $\eta = 0.2$, it remains large enough to induce oscillations around other unbalanced minima, but ultimately GD converges to the balanced solution, where it exhibits no oscillations, as predicted by Theorem~\ref{thm:align_thm}. This empirical observation suggests that two-period oscillations and balancing occur simultaneously, which we leave for future investigation.

Finally, we conclude by remarking that our result also aims to generalize the recent theoretical findings of~\cite{chen2023edge}, where they proved the existence of a certain class of scalar functions \( f(x) \) for which GD does not diverge even when operating beyond the stability threshold.
%\( \eta > \frac{2}{f^{''}(\hat{x})} \), where \( \hat{x} \) is a local minimum of \( f(x) \). Specifically, they showed that for a function dependent $\epsilon > 0$, there exists a range \( \eta \in \left( \frac{2}{f^{''}(\hat{x})}, \frac{2}{f^{''}(\hat{x})}(1+\epsilon) \right) \), 
They demonstrated that there exists a range in which the loss oscillates around the local minima with a certain periodicity. These oscillations gradually progress into higher periodic orbits (e.g., 2, 4, 8 periods), transition into chaotic behavior, and ultimately result in divergence. In our work, we prove that this oscillatory behavior beyond the stability threshold also occurs in DLNs.



% For the initialization scale, we require an upper bound on $\alpha$ to satisfy the balancing condition in Lemma~\ref{lemma:balancing}.


\subsection{Assumptions and Analytical Tools}
\label{sec:used_tools}

This section presents the two main tools used in our analyses: the singular vector stationary set and
singular value balancedness. First, we present the singular vector stationary set, which allows us to
encompass a wider range of initialization schemes. This set defines a broad class of initialization for
which singular vector alignment occurs, simplifying the dynamics to only singular values.


\begin{proposition}[Singular Vector Stationary Set]
\label{prop:svs_set}
Consider the deep matrix factorization loss in Equation~(\ref{eqn:deep_mf}). Let $\mbf{M}_\star = \mbf{U}_\star \mbf{\Sigma}_\star \mbf{V}_\star^\top$ and 
$\mbf{W}_\ell(t) = \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t)$ denote the compact SVD for the target matrix and the $\ell$-th layer weight matrix at time $t$, respectively. For any time $t\geq 0$, if $\dot{\mbf{U}}_\ell(t) = \dot{\mbf{V}}_\ell(t) = 0$ for all $\ell \in [L]$, then the singular vector stationary points for each weight matrix are given by
\begin{align*}
\mathrm{SVS}(f(\mbf{\Theta})) = 
\begin{cases}
    (\mbf{U}_L, \mbf{V}_L) &= (\mbf{U}_\star, \mbf{Q}_L), \\
    (\mbf{U}_\ell, \mbf{V}_\ell) &= (\mbf{Q}_{\ell+1}, \mbf{Q}_\ell), \quad\forall \ell \in [2, L-1], \\
    (\mbf{U}_1, \mbf{V}_1) &= (\mbf{Q}_2, \mbf{V}_\star),
\end{cases}
\end{align*}
where \(\{\mbf{Q}_\ell\}_{\ell=2}^{L}\) are any set of orthogonal matrices. 
\end{proposition}


The singular vector stationary set states that for any set of weights where the gradients with respect to the singular vectors become zero, the singular vectors become fixed points for subsequent iterations. Once the singular vectors become stationary, running GD further isolates the dynamics on the singular values. Hence, throughout our analysis, we re-write and consider the loss 
\begin{align}\label{eqn:simplified_loss}
    \frac{1}{2} \left\|\mbf{W}_{L:1}(t) - \mbf{M}^\star\right\|^2_{\mathsf{F}} = \frac{1}{2} \|\mbf{\Sigma}_{L:1} - \mbf{\Sigma}^\star\|^2_{\mathsf{F}} = \frac{1}{2} \sum_{i=1}^r \left(\sigma_i(\mbf{\Sigma}_{L:1}(t)) - \sigma_{\star, i}\right)^2,
\end{align}
where $\mbf{\Sigma}_{L:1}$ are the singular values of $\mbf{W}_{L:1}$. This allows us to decouple the dynamics of the singular vectors and singular values, focusing on the periodicity that occurs in the singular values within the EOS regime. \edit{In Propositions~\ref{prop:one_zero_svs_set} and~\ref{prop:balanced_svs_set}, we prove that both the unbalanced and balanced initializations belong to this set respectively,
with an illustration in Figure~\ref{fig:svec_alignment}.
Specifically, we show that the singular vectors belongs to the singular vector stationary set after GD iteration $t=1$ (far before entering the EOS regime) with $\mbf{Q}_\ell = \mbf{V}_\star$,
allowing us to consider the loss in Equation~(\ref{eqn:simplified_loss}).}
Next, we present a result to validate our use of the strictly balanced assumption \edit{on the unbalanced initialization case} by showing that the singular values become increasingly balanced throughout the GD iterations.

\begin{lemma}[Balancing]
\label{lemma:balancing}
   Let $\sigma_{\star, i}$ and $\sigma_{\ell, i}(t)$ denote the $i$-th singular value of $\mbf{M}_\star \in \mbb{R}^{d\times d}$ and $\mbf{W}_\ell(t)$, respectively and define $S_i \coloneqq L \sigma^{2-\frac{2}{L}}_{\star,i}$.
    Consider GD on the $i$-th index of the simplified loss in~(\ref{eqn:simplified_loss}) with unbalanced initialization in  (\ref{eqn:init}) and learning rate $\frac{2}{S_i} < \eta < \frac{2\sqrt{2}}{S_i}$. If the initialization scale $\alpha$ satisfies
    $0 < \alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta S_i} \right) \cdot \frac{ \sigma_{\star, i}^{4/L}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{1/4}$, \edit{then there exists a constant $c \in (0, 1]$ such that} for all $\ell \in [L-1]$, we have $\left| \sigma^2_{L, i}(t+1) - \sigma^2_{\ell, i}(t+1)\right| < \edit{c}\left| \sigma^2_{L, i}(t) - \sigma^2_{\ell, i}(t)\right|$.
\end{lemma}

\begin{wrapfigure}{r}{0.385\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{figures/balancing_gap.pdf}
    
  \end{center}
  \caption{Plot of $| \sigma^2_{L, i}(t) - \sigma^2_{\ell, i}(t)|$ 
 showing strict balancing.}
\label{fig:balancing}
\end{wrapfigure}
This result has been shown to hold similarly for two-layer matrix factorization~\citep{wang2021large,ye2021global,chen2023edge}, and our analysis extends it to the deep case.
Lemma~\ref{lemm:balancing} considers the scalar loss for a single singular value index and states that, as long as $\alpha$ is chosen below a threshold dependent on $\sigma_{\star, i}$, the $i$-th singular value will become increasingly balanced. To ensure that balancing occurs for all singular values of the loss in~(\ref{eqn:simplified_loss}), we can select the learning rate to induce oscillations in all singular values (assuming they remain below the divergence limit) and choose $\alpha$ with $\sigma_{\star, 1}$ such that it is the smallest $\alpha$ that satisfies the condition for all singular values $\sigma_{\star, i}$.


If the constant were $c < 1$, note that the balancing gap would approach zero infinitesimally. However, our analysis currently only shows that 
$c<1$ when the product of singular values across all layers $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$ and but $c=1$ when $\sigma_i(\mbf{\Sigma}_{L:1}) > \sigma_{\star, i}$.
Since we start from a small initialization scale, we generally mostly operate within the regime $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$,
and only transition to the latter regime when oscillations occur.
Note that $\sigma_i(\mbf{\Sigma}_{L:1}) = \sigma_{\star, i}$ cannot occur since the learning rate is chosen to be within the EOS regime -- equality
can only arise in the stable regime, where balancing does not occur. 
%\edit{To summarize, Lemma~\ref{lemm:balancing} states that, provided $\alphay$ is chosen below a certain threshold, the top-$r$ singular values of the weights across all layers become increasingly balanced during GD, even if they are unbalanced as in the initialization of Equation~(\ref{eqn:init}).  Our result is an extension of these analyses, but to the deep matrix factorization case. Our analysis shows that the constant $c$ changes for two different cases\footnote{We exclude the case  $\sigma_i(\mbf{\Sigma}_{L:1}) = \sigma_{\star, i}$, as this occurs with probability 0 in the presence of oscillations in the EOS regime.}: (i) $0<c<1$ when the product of singular values across all layers $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$ and (ii) $c=1$ when $\sigma_i(\mbf{\Sigma}_{L:1}) > \sigma_{\star, i}$. In the literature, it has been widely shown that the dynamics of DLNs (along with diagonal linear networks) exhibit an incremental learning phenomenon, where the singular values $\sigma_i(\mbf{W}_\ell)$ start from $\alpha$ and increase to the target singular value one-by-one~\citep{gissin2020the, inclearndiag, kwon, jacot2021saddle}. Empirically, this implies that we often operate in the regime of $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$, as the oscillations begin to occur once we reach and about the minima. Hence, throughout most of the learning trajectory, $0<c<1$ holds, and the balancing gap becomes infinitesimally small.}
In Figure~\ref{fig:balancing},
we plot the balancing gap between the top-3 singular values of a weight matrix initialized to zero and those initialized to $\alpha$ for a rank-3 matrix.
 This plot shows that the gap decreases and goes to zero empirically, and this is consistently the case across all of our experiments, with additional results provided in Appendix~\ref{sec:additional_exp}.
 This provides empirical evidence that our analysis can be further improved such that $c<1$ for both cases. 
 To this end, we use this insights to assume that strict balancing holds for both initializations in Equation~(\ref{eqn:init}). This allows us to write the loss of the singular values into the form $\sigma_i(\mbf{\Sigma}_{L:1}(t)) = \sigma^L_{i}(t)$, which allows us to focus on the dynamics in the singular values.





%Also in Appendix~\ref{sec:additional_exp}, we show that even for a slightly larger initialization scale, the balancing still holds in practice. Furthermore, we note that very similar results for imbalanced initialization have been proven by~\cite{wang2021large}, where our result can be viewed as an extension to multiple variables.
%Finally, by assuming strict balancing, we can simplify the loss even further, particularly in the form $\sigma_i(\mbf{\Sigma}_{L:1}(t)) = \sigma^L_{i}(t)$, which allows us to solely analyze the dynamics of the singular values within the EOS regime. 

%Recall that by our initialization scheme, we initialize to $\sigma_{L, i}(0) = 0$ and $\sigma_{\ell, i}(0) = \alpha$ for all $\ell \in [L-1]$. By considering the singular value scalar loss in Equation~(\ref{eqn:simplified_loss}), the gradient with respect to each $\sigma_{\ell, i}(t)$ are all the same. Thus, except for $\sigma_{L, i}(t)$, all of the other singular values across all weight matrices remain balanced throughout all iterations of GD. Lemma~\ref{lemma:balancing} states that throughout the course of GD, as long as $\alpha$ is below a certain threshold, $\sigma_{L, i}(t)$ becomes increasingly balanced to the rest of the singular values regardless of the learning rate. Since the balancing gap strictly decreases and $| \sigma^2_{i,L} - \sigma^2_{i,\ell} |$ is lower bounded by zero, it converges to 0 as $t \to \infty$. 


%However, it is worth noting that (i)~Lemma~\ref{lemma:balancing} does not state that the difference goes to zero, but rather that it becomes increasingly balanced and (ii) the bound on $\alpha$ may be too conservative and is an artifact of our analysis. 








\begin{comment}
    
\begin{figure}[htbp]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deep_martix_figs/oscillation_ranges.png}
        \caption{Figure 1}
        \label{fig:figure1}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deep_martix_figs/layer_1_singular_values_0.01546.png}
        \caption{$\eta=\frac{1}{L\sigma^{2-\frac{2}{L}}_{\star,1}}$}
        \label{fig:figure2}
    \end{minipage}%
    
    \vspace{0.5cm} % Adds some vertical space between rows

    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deep_martix_figs/layer_1_singular_values_0.0167.png}
        \caption{$\eta=\frac{1}{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, 1}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, 2}^{\frac{1}{L}\ell}\right)^2}$}
        \label{fig:figure3}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deep_martix_figs/layer_1_singular_values_0.02.png}
        \caption{$\eta>\frac{1}{L\sigma^{2-\frac{2}{L}}_{\star,2}}$}
        \label{fig:figure4}
    \end{minipage}
    \caption{}
\end{figure}





\end{comment}




%it is possible to have $r$-dimensional oscillations as long as the learning rate is chosen correctly. 



%\paragraph{Remarks.} By adopting this proof technique, we show that both conditions are true in each eigenvector direction considered in Theorem~\ref{thm:stable_oscillations}. Hence, Theorem~\ref{thm:stable_oscillations} that it is possible to have $r$-dimensional oscillations as long as the learning rate is chosen correctly. 












\subsection{Relation to Diagonal Linear Networks}

\label{sec:diagonal}

%Due to singular vector stationarity and balancing, DLNs may appear equivalent to diagonal linear networks at first glance. In this section, we characterize an explicit distinction between the two networks by deriving the eigenvalues of diagonal linear networks from the Hessian at convergence and explaining how they contribute to periodic oscillations in the EOS regime.

In this section, we derive the necessary conditions and characteristics of two-period oscillations in diagonal linear networks to establish their similarities with DLNs.

\begin{theorem}[Subspace Oscillation for Diagonal Linear Networks]
\label{thm:diag_lin_thm}
\edit{Consider an $L$-layer diagonal linear network on the loss 
\begin{align}
\label{eqn:diag_lin_loss}
        \mathcal{L}\left(\{\mbf{s}_\ell\}_{\ell=1}^L \right) \coloneqq \frac{1}{2} \|\mbf{s}_1 \odot \ldots \odot \mbf{s}_{L} - \mbf{s}_{\star}\|_2^2, 
 \end{align}
where $\mbf{s}_\star \in \mbb{R}^{d}$ be an $r$-sparse vector with ordered coordinates such that $s_{\star,1} >\ldots>s_{\star,d}$ and define $S_p \coloneqq L s_{\star,p}^{2-\frac{2}{L}}$ and $\alpha' \coloneqq \left( \ln\left( \frac{2\sqrt{2}}{\eta L s_{\star, 1}^{2 - \frac{2}{L}}} \right) \cdot \frac{ s_{\star, 1}^{\frac{4}{L}}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{\frac{1}{4}}$.
For any $p < r-1$ and $\alpha < \alpha'$, suppose we run GD on Equation~(\ref{eqn:diag_lin_loss}) with learning rate $\eta = \frac{2}{K}$, where $S_{p}\geq K> S_{p+1}$ with initialization 
$\mbf{s}_\ell = \alpha  \mbf{1}_d$ for all $\ell \in [L-1]$ and $\mbf{s}_{L}=\mbf{0}_d$.} Then, under strict balancing, the top-$p$ coordinates of $\mbf{s}_\ell$ oscillate within a $2$-period fixed orbit around the minima in the form
 \begin{align*}
     s_{\ell, i}(t) = \rho_{i, j}(t), \quad \forall i < p,  \, \forall \ell \in [L],
 \end{align*}
where $\rho_{i, j}(t) \in\{\rho_{i, 1}, \rho_{i, 2}\}$, $\rho_{i, 1} \in \left(0, s_{\star, i}^{1/L} \right)$ and $\rho_{i, 2} \in \left(s_{\star, i}^{1/L}, (2s_{\star, i})^{1/L} \right)$ are two real roots of the polynomial $h(\rho)=0$:
\begin{align*}
    h(\rho) = \rho^L\cdot \frac{1+\left(1 + \eta L(s_{\star,i}  - \rho^L)\cdot \rho^{L-2} \right)^{2L-1}}{1+\left(1 + \eta L(s_{\star,i}  - \rho^L)\cdot \rho^{L-2} \right)^{L-1}}- s_{\star,i}.
\end{align*}

\end{theorem}

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \caption*{\footnotesize Rank-$1$ Oscillation}
\includegraphics[width=\textwidth]{figures/eos_osc_region_1.pdf}
\caption*{$S_1 >K>S_2$}
\end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \caption*{\footnotesize Rank-$2$ Oscillation}
\includegraphics[width=\textwidth]{figures/eos_osc_region_2.pdf}
\caption*{$S_2 >K>S_3$}
\end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \caption*{\footnotesize Rank-$3$ Oscillation}
         \includegraphics[width=\textwidth]{figures/eos_osc_region_3.pdf}
         \caption*{$S_3 >K>S_4$}
\end{subfigure}
\caption{Evolution of the singular values of the end-to-end $3$-layer network for fitting a rank-3 target matrix with singular values $10$, $9.5$, and $9$. We use a learning rate of $\eta = 2/S_i$ with $S_i \coloneqq L\sigma_{\star, i}^{2 - 2/L}$. The oscillations occur exactly with learning rate ranges specified in Theorem~\ref{thm:align_thm}.}
\label{fig:thm2_osc}
\end{figure}


From Lemma~\ref{lemma:hessian_eigvals}, we observe that DLNs exhibit additional dominant curvature directions that are not present in diagonal linear networks—specifically, the eigenvalues corresponding to the ``interaction with other singular values'' are absent in diagonal linear networks. These eigenvalues arise due to the zero off-diagonal elements of the singular value diagonal matrix in~(\ref{eqn:simplified_loss}).
However, despite these extra directions, Theorem~\ref{thm:diag_lin_thm} demonstrates that the behavior of diagonal linear networks and DLNs is essentially the same. This can be understood using the argument from Theorem~\ref{thm:stable_oscillations}: the eigenvectors corresponding to these directions are always orthogonal to the flattened weights $\widetilde{\Delta}$, thereby making the behaviors of the two network types synonymous, even in the EOS regime.


%Similar to Theorem~\ref{thm:align_thm}, each coordinate of the diagonal linear network undergoes periodic oscillations with an appropriately chosen learning rate. However, as shown in the proof of Theorem~\ref{thm:diag_lin_thm}, the main difference lies in the eigenvalues themselves -- the number non-zero eigenvalues for diagonal linear networks are much smaller than those of the DLN. The set of eigenvalues corresponding to the interaction with other singular values in Lemma~\ref{lemma:hessian_eigvals} are missing for diagonal linear networks. Therefore, the top two eigenvalues of the diagonal linear network are $L s_{\star, 1}^{2 - \frac{2}{L}}$ and $L s_{\star, 2}^{2 - \frac{2}{L}}$, whereas the top two eigenvalues of the DLN are  $L s_{\star, 1}^{2 - \frac{2}{L}}$ and $\sum_{\ell=0}^{L-1} \left(s_{\star, 1}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot s_{\star, 2}^{\frac{1}{L}\ell}\right)^2$. Hence, the dynamics in the EOS regime between the two networks differ significantly (see Figure~\ref{fig:diff_dln_diag}). \edit{The primary difference in the landscape arises from the zero off-diagonal elements of the singular value diagonal matrix of the DLN which introduces additional curvature directions despite singular vector invariance.}


% In the DLN, some eigenvalues are a function of a ``mixture'' of singular values, which arise from the off-diagonal elements in the decoupled loss in Equation~(\ref{eqn:simplified_loss}). In the diagonal linear network, they depend only on a single coordinate (i.e., $s_{\star,i}$) or the initialization scale $\alpha$. Hence, in the diagonal linear network, a much larger learning rate is required to observe oscillations in two or more coordinates compared to the DLN. 

