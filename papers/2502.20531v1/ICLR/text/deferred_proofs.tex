
\section{Deferred Proofs}
\label{sec:proofs}

\subsection{Deferred Proofs for Oscillations}
\label{sec:proofs_oscillations}

\begin{manualpropositioninner}[Singular Vector Stationary Set]
Consider the deep matrix factorization loss in Equation~(\ref{eqn:deep_mf}). Let $\mbf{M}_\star = \mbf{U}_\star \mbf{\Sigma}_\star \mbf{V}_\star^\top$ and 
$\mbf{W}_\ell(t) = \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t)$ denote the compact SVD for the target matrix and the $\ell$-th layer weight matrix at time $t$, respectively. For any time $t\geq 0$, if $\dot{\mbf{U}}_\ell(t) = \dot{\mbf{V}}_\ell(t) = 0$ for all $\ell \in [L]$, then the singular vector stationary points for each weight matrix are given by
\begin{align*}
\mathrm{SVS}(f(\mbf{\Theta})) = 
\begin{cases}
    (\mbf{U}_L, \mbf{V}_L) &= (\mbf{U}_\star, \mbf{Q}_L), \\
    (\mbf{U}_\ell, \mbf{V}_\ell) &= (\mbf{Q}_{\ell+1}, \mbf{Q}_\ell), \quad\forall \ell \in [2, L-1], \\
    (\mbf{U}_1, \mbf{V}_1) &= (\mbf{Q}_2, \mbf{V}_\star),
\end{cases}
\end{align*}
where \(\{\mbf{Q}_\ell\}_{\ell=2}^{L}\) can be any orthogonal matrices. \label{svs-set}
\end{manualpropositioninner}

\begin{proof}

Let us consider the dynamics of $\mbf{W}_\ell(t)$ in terms of its SVD with respect to time:
\begin{align}
\label{eqn:svd_rynamics}
    \dot{\mbf{W}}_\ell(t) &= \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t) + \mbf{U}_\ell(t) \dot{\mbf{\Sigma}}_\ell(t) \mbf{V}_\ell^\top(t) + \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_\ell^\top(t).
\end{align}
By left multiplying by \(\mbf{U}_\ell^\top(t)\) and right multiplying by \(\mbf{V}_\ell(t)\), we have
\begin{align}
    \mbf{U}_\ell^\top(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) &= \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t) + \dot{\mbf{\Sigma}}_\ell(t) + \mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t), 
\end{align}
where we used the fact that \(\mbf{U}_\ell(t)\) and \(\mbf{V}_\ell(t)\) have orthonormal columns. Now, note that we also have
\begin{align*}
    \mbf{U}_\ell^\top(t) \mbf{U}_\ell(t) = \mbf{I}_r \implies \dot{\mbf{U}}_\ell^\top(t) \mbf{U}_\ell(t) + \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) = \mbf{0},
\end{align*}
which also holds for $\mbf{V}_\ell(t)$. This implies that $\dot{\mbf{U}}_\ell^\top(t) \mbf{U}_\ell(t)$ is a skew-symmetric matrix, and hence have zero diagonals. 
Since \(\mbf{\Sigma}_\ell(t)\) is diagonal, \(\mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t)\) and \(\mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t)\) have zero diagonals as well. On the other hand, since \(\dot{\mbf{\Sigma}}_\ell(t)\) is a diagonal matrix, we can write
\begin{align}
\label{eqn:diag_inv}
    \hat{\mbf{I}}_r \odot \left(\mbf{U}_\ell^\top(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t)\right) &= \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t) + \mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t), 
\end{align}
where \(\odot\) stands for the Hadamard product and \(\hat{\mbf{I}}_r\) is a square matrix holding zeros on its diagonal and ones elsewhere. Taking transpose of Equation~(\ref{eqn:diag_inv}), while recalling that \(\mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t)\) and \(\mbf{V}_\ell^\top(t) \dot{\mbf{V}}_\ell(t)\) are skew-symmetric, we have
\begin{align}
\label{eqn:diag_inv_transpose}
    \hat{\mbf{I}}_{r} \odot \left(\mbf{V}_\ell^\top(t) \dot{\mbf{W}}_\ell^\top(t) \mbf{U}_\ell(t)\right) &= -\mbf{\Sigma}_\ell(t) \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) - \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell(t). 
\end{align}
Then, by right multiplying Equation~(\ref{eqn:diag_inv}) by \(\mbf{\Sigma}_\ell(t)\), left-multiply Equation~(\ref{eqn:diag_inv_transpose}) by \(\mbf{\Sigma}_\ell(t)\), and by adding the two terms, we get
\begin{align*}
    \hat{\mbf{I}}_{r} \odot \biggl(\mbf{U}_\ell^\top(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell(t) + \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t) &\dot{\mbf{W}}_\ell^\top(t) \mbf{U}_\ell(t)\biggr) \\
    &= \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell^2(t) - \mbf{\Sigma}_\ell^2(t) \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t). 
\end{align*}
Since we assume that the singular values of $\mbf{M}_\star$ are distinct, the top-$r$ diagonal elements of \(\mbf{\Sigma}_{\ell}^2(t)\) are also distinct (i.e., $\Sigma^2_{r}(t) \neq \Sigma^2_{r'}(t) \text{ for } r \neq r'$). This implies that
\begin{align*}
    \mbf{U}_{\ell}^\top(t) \dot{\mbf{U}}_{\ell}(t) &= \mbf{H}(t) \odot \left[\mbf{U}_{\ell}^\top(t) \dot{\mbf{W}}_{\ell}(t) \mbf{V}_{\ell}(t) \mbf{\Sigma}_{\ell}(t) + \mbf{\Sigma}_{\ell}(t) \mbf{V}_{\ell}^\top(t) \dot{\mbf{W}}_{\ell}^\top(t) \mbf{U}_{\ell}(t)\right], 
\end{align*}

where the matrix \(\mbf{H}(t) \in \mathbb{R}^{d\times d}\) is defined by:
\begin{align}
    H_{r,r'}(t) := 
    \begin{cases}
    \left(\Sigma^2_{r'}(t) - \Sigma^2_r(t)\right)^{-1}, & r \neq r', \\
    0, & r = r'.
    \end{cases}
\end{align}

Then, multiplying from the left by \(\mbf{U}_{\ell}(t)\) yields
\begin{align}
    \mbf{P}_{\mbf{U}_{\ell}(t)} \dot{\mbf{U}}_{\ell}(t) &= \mbf{U}_{\ell}(t) \left(\mbf{H}(t) \odot \left[\mbf{U}_{\ell}^\top(t) \dot{\mbf{W}}_{\ell}(t) \mbf{V}_{\ell}(t) \mbf{\Sigma}_{\ell}(t) + \mbf{\Sigma}_{\ell}(t) \mbf{V}_{\ell}^\top(t) \dot{\mbf{W}}_{\ell}^\top(t) \mbf{U}_{\ell}(t)\right]\right), 
\end{align}
with \(\mbf{P}_{\mbf{U}_{\ell}(t)} := \mbf{U}_{\ell}(t) \mbf{U}_{\ell}^\top(t)\) being the projection onto the subspace spanned by the (orthonormal) columns of \(\mbf{U}_{\ell}(t)\). Denote by \(\mbf{P}_{\mbf{U}_{{\ell}\perp}(t)}\) the projection onto the orthogonal complement ( i.e., $\mbf{P}_{\mbf{U}_{\ell\perp}(t)} := \mbf{I}_r - \mbf{U}_{\ell}(t) \mbf{U}_{\ell}^\top(t)$). Apply \(\mbf{P}_{\mbf{U}_{\ell\perp}(t)}\) to both sides of Equation~(\ref{eqn:svd_rynamics}):
\begin{align}
    \mbf{P}_{\mbf{U}_{\ell\perp}(t)}\dot{\mbf{U}}_{\ell}(t)  = \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \dot{\mbf{U}}_{\ell}(t) \mbf{\Sigma}_\ell(t) \mbf{V}_{\ell}^\top(t) &+ \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \mbf{U}_\ell(t) \dot{\mbf{\Sigma}}_{\ell}(t) \mbf{V}_{\ell}^\top(t)\\ &+ \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_{\ell}^\top(t). 
\end{align}

Note that \(\mbf{P}_{\mbf{U}_{\ell\perp}(t)} \mbf{U}_\ell(t) = 0\), and multiply from the right by \(\mbf{V}_\ell(t) \mbf{\Sigma}_{\ell}^{-1}(t)\) (the latter is well-defined since we have the compact SVD and the top-$r$ elements are non-zero):
\begin{align}
    \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \dot{\mbf{U}}_\ell(t) &= \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell^{-1}(t) = (\mbf{I}_r - \mbf{U}_\ell(t)\mbf{U}^\top(t)) \dot{\mbf{W}}(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell^{-1}(t). 
\end{align}
Then by adding the two equations above, we obtain an expression for \(\dot{\mbf{U}}(t)\):
\begin{align}
    \dot{\mbf{U}}_\ell(t) &= \mbf{P}_{\mbf{U}_\ell(t)} \dot{\mbf{U}}_\ell(t) + \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \dot{\mbf{U}}_\ell(t) \nonumber \\
    &= \mbf{U}_\ell(t)\left(\mbf{H}(t) \odot \left[\mbf{U}_\ell^\top(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell(t) + \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t) \dot{\mbf{W}}_\ell^\top(t) \mbf{U}_\ell(t)\right]\right) \nonumber \\
    &\quad + (\mbf{I}_r - \mbf{U}_\ell(t) \mbf{U}_\ell^\top(t)) \dot{\mbf{W}}(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell^{-1}(t). 
\end{align}
We can similarly derive the dynamics for $\dot{\mbf{V}}_\ell(t)$ and $\dot{\mbf{\Sigma}}_\ell(t)$:
\begin{align}
\dot{\mbf{V}}_\ell(t) = \mbf{V}_\ell(t)\left(\mbf{H}(t) \odot \left[\mbf{\Sigma}_\ell(t) \mbf{U}^\top_\ell(t) \dot{\mbf{W}_{\ell}}(t) \mbf{V}_\ell(t) + \mbf{V}^\top_\ell(t) \dot{\mbf{W}_{\ell}}^\top(t) \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t)\right]\right) \\
+ \left(\mbf{I}_{r} - \mbf{V}_\ell(t)\mbf{V}^\top_\ell(t)\right) \dot{\mbf{W}_{\ell}}^\top(t) \mbf{U}_\ell(t) \mbf{\Sigma}_\ell^{-1}(t), \label{vdiff}
\end{align}
\begin{align*}
   \dot{\mbf{\Sigma}}_\ell(t) = \mbf{I}_r \odot \left[ \mbf{U}^\top_\ell(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \right].
\end{align*}

Now, we will left multiply $\dot{\mbf{U}}_\ell(t)$ and $\dot{\mbf{V}}_\ell(t)$ with $\mbf{U}_\ell^\top(t)$ and $\mbf{V}_\ell^\top(t)$, respectively, to obtain
\begin{align*}
    \mbf{U}^\top_\ell(t) \dot{\mbf{U}}_\ell(t) &= -\mbf{H}(t) \odot \left[\mbf{U}^\top_\ell(t)\nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell(t) + \mbf{\Sigma}_\ell(t) \mbf{V}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{U}_\ell(t)\right], \\
    \mbf{V}^\top_\ell(t) \dot{\mbf{V}}_\ell(t) &= -\mbf{H}(t) \odot \left[\mbf{\Sigma}_\ell(t) \mbf{U}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{V}_\ell(t) + \mbf{V}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t)\right],
\end{align*}
where we replaced $\dot{\mbf{W}}_\ell(t) \coloneqq -\nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta})$, as $\dot{\mbf{W}}_\ell(t)$ is the gradient of $f(\mbf{\Theta})$ with respect to $\mbf{W}_\ell$ by definition. By rearranging and multiplying by $\mbf{\Sigma}_\ell(t)$, we have
\begin{align}
\label{eqn:diagonal_grad}
      \mbf{U}^\top_\ell(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t) -   \mbf{\Sigma}_\ell(t) \mbf{V}^T (t) \dot{\mbf{V}}_\ell(t) = -  \hat{\mbf{I}}_{r} \odot [\mbf{U}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{V}_\ell(t)].
\end{align}
Hence, when $\dot{\mbf{U}}_\ell(t)=0$ and $\dot{\mbf{V}}_\ell(t)=0$, it must be that the left-hand side is zero and so $\mbf{U}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{V}_\ell(t)$ is a diagonal matrix. 

Now, notice that for the given loss function $f(\mbf{\Theta})$, we have
\begin{align*}
   -\dot{\mbf{W}}_\ell(t) = \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}(t)) = \mbf{W}^{\top}_{L:\ell+1}(t) \cdot \left(\mbf{W}_{L:1}(t) - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}(t). 
\end{align*}
Then, from Equation~(\ref{eqn:diagonal_grad}), when the singular vectors are stationary, we have
\begin{align*}
    \mbf{U}_\ell^\top(t)\mbf{W}^{\top}_{L:\ell+1}(t) \cdot \left(\mbf{W}_{L:1}(t) - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}(t)\mbf{V}_\ell(t)
\end{align*}
must be a diagonal matrix for all $\ell \in [L]$. The only solution to the above should be (since the intermediate singular vectors need to cancel to satisfy the diagonal condition), is the set
\begin{align*}
\mathrm{SVS}(f(\mbf{\Theta})) = 
\begin{cases}
    (\mbf{U}_L, \mbf{V}_L) &= (\mbf{U}_\star, \mbf{Q}_L), \\
    (\mbf{U}_\ell, \mbf{V}_\ell) &= (\mbf{Q}_{\ell+1}, \mbf{Q}_\ell), \quad\forall \ell \in [2, L-1], \\
    (\mbf{U}_1, \mbf{V}_1) &= (\mbf{Q}_2, \mbf{V}_\star),
\end{cases}
\end{align*}
where \(\{\mbf{Q}_\ell\}_{\ell=2}^{L}\) are any set of orthogonal matrices. Then, notice that when the singular vectors are stationary, the dynamics become isolated on the singular values: \begin{align*}
   \dot{\mbf{\Sigma}}_\ell(t) = \mbf{I}_r \odot \left[ \mbf{U}^\top_\ell(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \right],
\end{align*} 
since $\left[ \mbf{U}^\top_\ell(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \right]$ is diagonal. This completes the proof.


\end{proof}


\begin{manualtheoreminner}
[Stable Subspace Oscillations]
Let $\alpha' \coloneqq \left( \ln\left( \frac{2\sqrt{2}}{\eta \lambda_1} \right) \cdot \frac{ \sigma_{\star, 1}^{4/L}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{1/4}$.
Consider running GD on the loss in~(\ref{eqn:deep_mf}) with initialization scale $0<\alpha < \alpha'$.
    If $\eta = \frac{2}{K}$ with $\lambda_i \leq K< \lambda_{i+1}$, then under strict balancing, $2$-period orbit oscillation occurs in the direction of $\Delta_i$, where $\lambda_i$ and $\Delta_i$ denote the $i$-th largest eigenvalue and eigenvector of the Hessian at the balanced minimum, respectively.
      \label{thm:stable_sub}
\end{manualtheoreminner}
\begin{proof}

Define $f_{\Delta_i}$ as the 1-D function at the cross section of the loss landscape and the line
following the direction of $\Delta_i$ passing the (balanced) minima. To prove the result, we will invoke Lemma~\ref{lemma:chen-bruna}, which states that two-period orbit oscillation occurs in the direction of $\Delta_i$ if the minima of $f_{\Delta_i}$ satisfies $f_{\Delta_i}^{(3)}>0$ and $3[f_{\Delta_i}^{(3)}]^2 - f_{\Delta_i}^{(2)}f_{\Delta_i}^{(4)} > 0$, for $\eta>\frac{2}{\lambda_{i}}$. Recall that the initialization condition is an artifact of Proposition~\ref{prop:balancing}, which allows us to consider the balanced minimum. 


First, we will derive the eigenvectors of the Hessian of the training loss at convergence (i.e., $\mbf{M}_\star = \mbf{W}_{L:1}$).
    To obtain the eigenvectors of the Hessian of parameters $(\mbf{W}_L, \ldots, \mbf{W}_2, \mbf{W}_1)$, consider a small perturbation of the parameters:
    \begin{align*}
        \mbf{\Theta} \coloneqq \left(\Delta \mbf{W}_\ell +  \mbf{W}_\ell \right)_{\ell=1}^L =  (\mbf{W}_L + \Delta \mbf{W}_L, \ldots, \mbf{W}_2+ \Delta \mbf{W}_2, \mbf{W}_1+ \Delta \mbf{W}_1).
    \end{align*}

    
    Given that $\mbf{W}_{L:1} = \mbf{M}_\star$, consider and evaluate the loss function at this minima: 
    \begin{align}
        \mathcal{L}(\mbf{\Theta}) = \frac{1}{2} \biggl\| &\sum_{\ell} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \\
        &+ \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1}   +   \ldots   +  \Delta \mbf{W}_{L:1}\biggr\|^2_{\mathsf{F}}.
    \end{align}
    By expanding each of the terms and splitting by the orders of $\Delta \mbf{W}_\ell$ (perturbation), we get that the second-order term is equivalent to
    \begin{align*}
        \Theta&\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^2\right): \,\, \frac{1}{2} \biggl\| \sum_{\ell} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \biggr\|^2_{\mathsf{F}} \\
        \Theta&\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^3\right): \,\, \mathrm{tr}\left[\left(\sum_{\ell} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left( \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right)\right] \\
        \Theta&\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^4\right): \,\, \frac{1}{2} \| \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1}\|^2_{\mathsf{F}}\\
        &+ \mathrm{tr}\left[\sum_{l} \left(\mbf{W}_{L:\ell+1}\Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left(\sum_{l<m<p} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:p+1} \Delta \mbf{W}_{p} \mbf{W}_{p-1:1} \right)\right]
    \end{align*}

The direction of the steepest change in the loss at the minima correspond to the largest eigenvector direction of the Hessian. Since higher order terms such as $ \Theta\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^3\right)$ are insignifcant compared to the second order terms $  \Theta\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^2\right)$, finding the direction that maximizes the second order term leads to finding the eigenvector of the Hessian.
    Then, the eigenvector corresponding to the maximum eigenvalue of  $\nabla^2 \mathcal{L}$ is the solution of 
    \begin{align}
        \Delta_{1} \coloneqq \mathrm{vec}(\Delta \mbf{W}_L, \ldots \Delta \mbf{W}_1) = \underset{\|\Delta \mbf{W}_L\|^2_{\mathsf{F}} + \ldots + \|\Delta \mbf{W}_1\|^2_{\mathsf{F}} = 1}{\mathrm{arg max}} \, f\left(\Delta \mbf{W}_L, \ldots, \Delta \mbf{W}_1 \right),\label{max-eig}
    \end{align}
    where 
    \begin{align}
        f(\Delta \mbf{W}_L, \ldots, \Delta \mbf{W}_1) \coloneqq \frac{1}{2} \|\Delta \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta \mbf{W}_1\|^2_{\mathsf{F}}.
    \end{align}

While the solution of Equation~(\ref{max-eig}) gives the maximum eigenvector direction of the Hessian, $\Delta_{1}$, the other eigenvectors can be found by solving
\begin{align}
    \Delta_{r} \coloneqq  \underset{
    \substack{
    \|\Delta \mbf{W}_L\|^2_{\mathsf{F}} + \ldots + \|\Delta \mbf{W}_1\|^2_{\mathsf{F}} = 1, \\
    \Delta_{r}\perp \Delta_{r-1},.., \Delta_{r}\perp \Delta_{1}
    }
    }{\mathrm{argmax}} \, f\left(\Delta \mbf{W}_L, \ldots, \Delta \mbf{W}_1 \right).\label{other-eig}
\end{align}

    By expanding $f(\cdot)$, %subject to the constraint ${\|\Delta \mbf{W}_L\|^2_{\mathsf{F}}} + \ldots + \|\Delta \mbf{W}_1\|^2_{\mathsf{F}} = 1$ ,
    we have that % we don't expand subject to a constraint
    \begin{align}
        f(\Delta \mbf{W}_L, &\ldots, \Delta \mbf{W}_1) = \|\Delta\mbf{W}_L \mbf{W}_{L-1:1}\|^2_{\mathsf{F}} +\ldots+ \|\mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1}\|^2_{\mathsf{F}}  + \|\mbf{W}_{L:2} \Delta \mbf{W}_1\|^2_{\mathsf{F}} \notag \\
        &+ \mathrm{tr}\left[\left(\Delta\mbf{W}_L \mbf{W}_{L-1:1} \right)^\top \left(\mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} + \ldots +\mbf{W}_{L:2} \Delta \mbf{W}_1 \right)\right] + \ldots + \notag \\
        &\mathrm{tr}\left[\left(\mbf{W}_{L:2} \Delta \mbf{W}_1\right)^\top \left(\mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} + \ldots +\mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} \right)\right].     \label{expansion}
    \end{align}

We can solve Equation~(\ref{max-eig}) by maximizing each of the terms, which can be done in two steps:
\begin{enumerate}[label=(\roman*)]
\item 
Each Frobenius term in the expansion is maximized when the left singular vector of $\Delta \mbf{W}_{\ell}$ aligns with $\mbf{W}_{L:\ell+1}$ and the right singular vector aligns with $\mbf{W}_{\ell-1:1}$. This is a result of Von Neumann's trace inequality~\citep{mirsky1975trace}. Similarly, each term in the trace is maximized when the singular vector of the perturbations align with the products. 
\item Due to the alignment, Equation~(\ref{max-eig}) can be written in just the singular values. Let $\Delta s_{\ell, i}$ denote the $i$-th singular value of the perturbation matrix $\Delta\mbf{W}_\ell$. Recall that all of the singular values of $\mbf{M}_\star$ are distinct (i.e., $\sigma_{\star, 1} > \ldots>\sigma_{\star, r}$). Hence, it is easy to see that
Equation~(\ref{max-eig}) is maximized when $\Delta s_{\ell,i} = 0$ (i.e, all the weight goes to $\Delta s_{\ell,1}$). Thus, each perturbation matrix must be rank-$1$.
\end{enumerate}
Now since each perturbation is rank-$1$, we can write each perturbation as 
    \begin{align}
        \Delta\mbf{W}_{\ell} = \Delta s_{\ell} \Delta \mbf{u}_{\ell} \Delta\mbf{v}_\ell^\top, \quad \forall \ell \in [L],
    \end{align}
    % Then, notice that since the top-$r$ singular values of $\mbf{M}_\star$ are unique (i.e., $\sigma_{\star, 1} > \ldots > \sigma_{\star, r}$), the singular values of each $\mbf{W}_{\ell}$ are also unique, and hence the solution for each $\Delta\mbf{W}_{\ell}$ will be rank-$1$. We can express each solution as \
    for $\Delta s_{\ell} > 0$ and orthonormal vectors $\Delta \mbf{u}_{\ell} \in \mbb{R}^d$ and  $\Delta \mbf{v}_{\ell} \in \mbb{R}^d$ with $\sum_{\ell=1}^L \Delta s^2_{\ell} = 1$.
    Plugging this in each term, we obtain:
   \begin{align*}
        \|\mbf{W}_{L:\ell+1} \Delta_1 \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|_2^2 = \Delta_1 s_\ell^2\cdot \biggl\|\underbrace{\mbf{V}_\star \mbf{\sigma}^{\frac{L-\ell}{L}}_\star \mbf{V}^\top_\star \Delta \mbf{u}_\ell}_{\eqqcolon \mbf{a}}\underbrace{ \Delta \mbf{v}_\ell^\top \mbf{V}_\star \mbf{\sigma}^{\frac{\ell-1}{L}}_\star \mbf{V}^\top_\star}_{\eqqcolon \mbf{b}^\top}\biggr\|_2^2.
    \end{align*}

    Since alignment maximizes this expression as discussed in first point, we have:
        % We first derive the leading eigenvector of the Hessian, denoted by $\Delta_1$. This makes repeated use of Von-Neumann trace inequality~, in that $\Delta \mbf{W}_\ell$ will have their singular vectors align.
    

    % To maximize $f(\cdot)$, we can find $\Delta \mbf{W}_\ell$ that maximizes each term in its expansion. Consider the Frobenius norm terms (e.g., $\|\mbf{W}_{L:\ell+1} \Delta \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|^2_{\mathsf{F}}$). Since each $\Delta\mbf{W}_\ell$ solution is rank-$1$, it follows that each one of the matrices within the norm must also be at most rank-$1$. Then, we have
    % \begin{align*}
    %     \|\mbf{W}_{L:\ell+1} \Delta_1 \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|^2_{\mathsf{F}} = \|\mbf{W}_{L:\ell+1} \Delta_1 \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|^2_{2} &\leq \|\mbf{W}_{L:\ell+1}\|_2^2 \cdot \|\Delta_1 \mbf{W}_{\ell}\|_2^2 \cdot \|\mbf{W}_{\ell-1:1}\|_2^2 \\
    %     &= \Delta_1 s_{\ell}^{2} \cdot \sigma_{\star, 1}^{2 - \frac{2}{L}}.
    % \end{align*}
    % To obtain this upper bound, consider the following:
    % \begin{align*}
    %     \|\mbf{W}_{L:\ell+1} \Delta_1 \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|_2^2 = \Delta_1 s_\ell^2\cdot \biggl\|\underbrace{\mbf{V}_\star \mbf{\sigma}^{\frac{L-\ell}{L}}_\star \mbf{V}^\top_\star\mbf{u}_\ell}_{\eqqcolon \mbf{a}}\underbrace{\mbf{v}_\ell^\top \mbf{V}_\star \mbf{\sigma}^{\frac{\ell-1}{L}}_\star \mbf{V}^\top_\star}_{\eqqcolon \mbf{b}^\top}\biggr\|_2^2.
    % \end{align*}
     $\mbf{u}_\ell =\mbf{v}_\ell = \mbf{v}_{\star, 1}$ for all $\ell \in [2, L-1]$, then
    \begin{align*}
        \mbf{a} = \sigma_{\star, 1}^{\frac{L-\ell}{L}}\mbf{v}_{\star, 1} \quad \text{and} \quad \mbf{b}^\top = \sigma_{\star, 1}^{\frac{\ell - 1}{L}}\mbf{v}_{\star, 1}^\top \implies \mbf{ab}^\top = \sigma_{\star, 1}^{1 - \frac{1}{L}} \cdot \mbf{v}_{\star, 1}\mbf{v}_{\star, 1}^\top.
    \end{align*}
    The very same argument can be made for the trace terms.
    Hence, in order to maximize $f(\cdot)$, we must have
    \begin{align*}
        \mbf{v}_L &= \mbf{v}_{\star, 1}, \quad \text{and} \quad \mbf{u}_1 = \mbf{v}_{\star, 1}, \\
        \mbf{u}_\ell &= \mbf{v}_\ell = \mbf{v}_{\star, 1}, \quad \forall \ell \in [2, L-1].
    \end{align*}
    To determine $\mbf{u}_L$ and $\mbf{v}_1$, we can look at one of the trace terms:
    \begin{align*}
    \mathrm{tr}\left[\left(\Delta_1\mbf{W}_L \mbf{W}_{L-1:1} \right)^\top \left(\mbf{W}_{L:3}\Delta_1 \mbf{W}_2 \mbf{W}_{1} + \ldots +\mbf{W}_{L:2} \Delta_1 \mbf{W}_1 \right)\right] \leq \left(\frac{L-1}{L} \right)\cdot\sigma_{\star, 1}^{2 - \frac{2}{L}}.
    \end{align*}
    To reach the upper bound, we require $\mbf{u}_L = \mbf{u}_{\star, 1}$ and $\mbf{v}_1 = \mbf{v}_{\star, 1}$. Finally, as the for each index, the singular values are balanced, we will have  $\Delta_1 s_{\ell} = \frac{1}{\sqrt{L}}$ for all $\ell \in [L]$ to satisfy the constraint. Finally, we get that the leading eigenvector is
    \begin{align*}
        \Delta_1 \coloneqq \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_1 \mbf{v}_1^\top, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top \right).
    \end{align*}
    Notice that we can also verify that $f(\Delta_1) = L\sigma_{\star, 1}^{2- \frac{2}{L}}$, which is the leading eigenvalue (or sharpness) derived in Lemma~\ref{lemma:hessian_eigvals}.  

    To derive the remaining eigenvectors, we need to find all of the vectors in which $\Delta_i^\top \Delta_j = 0$ for $i\neq j$, where
    \begin{align*}
        \Delta_i = \mathrm{vec}(\Delta_i \mbf{W}_L, \ldots \Delta_i \mbf{W}_1),
    \end{align*}
    and $f(\Delta_i) = \lambda_i$, where $\lambda_i$ is the $i$-th largest eigenvalue. By repeating the same process as above, we find that the eigenvector-eigenvalue pair as follows:
    \begin{align*}
        \Delta_1 &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_1 \mbf{v}_1^\top, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top \right) , \quad\lambda_{1} =  L\sigma_{\star, 1}^{2- \frac{2}{L}} \\
        \Delta_2 &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_1 \mbf{v}_2^\top, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_2^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_2^\top \right), \quad\lambda_{2} =  \left(\sum_{i=0}^{L-1} \sigma_{\star, 1}^{1-\frac{1}{L}-\frac{1}{L}i} \cdot \sigma_{\star, 2}^{\frac{1}{L}i} \right) \\
        \Delta_3 &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_2 \mbf{v}_1^\top, \frac{1}{\sqrt{L}}\mbf{v}_2 \mbf{v}_1^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_2 \mbf{v}_1^\top \right), \quad\lambda_{3} =  \left(\sum_{i=0}^{L-1} \sigma_{\star, 1}^{1-\frac{1}{L}-\frac{1}{L}i} \cdot \sigma_{\star, 2}^{\frac{1}{L}i} \right) \\
        \quad\quad\quad&\vdots \\
         \Delta_d &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_2 \mbf{v}_2^\top, \frac{1}{\sqrt{L}}\mbf{v}_2 \mbf{v}_2^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_2 \mbf{v}_2^\top \right), \quad\lambda_{d} =  L\sigma_{\star, 2}^{2- \frac{2}{L}} \\     
        \quad\quad\quad&\vdots \\
        \Delta_{dr+r} &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_d \mbf{v}_r^\top, \frac{1}{\sqrt{L}}\mbf{v}_d \mbf{v}_r^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_d \mbf{v}_r^\top \right),
    \end{align*}
    which gives a total of $dr + r$ eigenvectors.

    Second, equipped with the eigenvectors, let us consider the 1-D function $f_{\Delta_i}$ generated by the cross-section of the loss landscape and each eigenvector $\Delta_i$ passing the minima:
    \begin{align*}
        f_{\Delta_i}(\mu) &= \mathcal{L}(\mbf{W}_L + \mu\Delta \mbf{W}_L, \ldots, \mbf{W}_2+ \mu\Delta \mbf{W}_2, \mbf{W}_1+ \mu\Delta \mbf{W}_1), \\
        &= \mu^2\cdot \frac{1}{2} \|\Delta \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta \mbf{W}_1\|^2_{\mathsf{F}}\\
        \quad&+\mu^3 \cdot \sum_{\ell=1, \ell< m}^L\mathrm{tr}\left[\left(\mbf{W}_{L:\ell+1}\Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left( \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right)\right] \\
        \quad&+\mu^4\cdot \frac{1}{2} \left\|  \left( \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right) \right\|^2_{\mathsf{F}}\\
        &+ \mu^4 \cdot\sum_{\ell<m<p} ^L \mathrm{tr}\left[\left(\mbf{W}_{L:\ell+1}\Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top  \left(\mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:p+1} \Delta \mbf{W}_{p} \mbf{W}_{p-1:1} \right)\right].
    \end{align*}
    Then, the several order derivatives of $f_{\Delta_i}(\mu)$ at $\mu = 0$ can be obtained from Taylor expansion as
    \begin{align*}
        f_{\Delta_i}^{(2)}(0) &= \|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} = \lambda^2_{i}\\
        f_{\Delta_i}^{(3)}(0) &= 6\sum_{\ell=1}^L\mathrm{tr}\left[\left(\mbf{W}_{L:\ell+1}\Delta_i \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left(\mbf{W}_{L:\ell+2}\Delta_i \mbf{W}_{\ell+1} \mbf{W}_\ell\Delta_i\mbf{W}_{\ell-1} \mbf{W}_{\ell-2:1} \right)\right] \\
        & = 6 \biggl\| \sum_{\ell} \mbf{W}_{L:\ell+1}\Delta_i \mbf{W}_\ell \mbf{W}_{\ell-1:1} \biggr\|_{\mathsf{F}}\cdot \biggl\| \left( \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right) \biggr\|_{\mathsf{F}}\\
        & \coloneqq 6  \lambda_{i}\cdot\beta_{i} \\
        f_{\Delta_i}^{(4)}(0) &= 12\|\Delta_i \mbf{W}_L \Delta_i\mbf{W}_{L-1}\mbf{W}_{L-2:1} + \ldots + \mbf{W}_{L:4}\Delta_i \mbf{W}_3 \mbf{W}_{2}\Delta_i\mbf{W}_1 + \mbf{W}_{L:3}\Delta_i\mbf{W}_2 \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
        &+ 24\sum_{\ell=1}^L \mathrm{tr}\left[\left(\mbf{W}_{L:\ell+1}\Delta_i \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left(\sum_{l<m<p} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:p+1} \Delta \mbf{W}_{p} \mbf{W}_{p-1:1} \right)\right] \\
        &\coloneqq 12\beta^2_{i} + 24\lambda_{i}\cdot\delta_{i},
    \end{align*}
  
    % \begin{align*}
    %     3[f_{\Delta_i}^{(3)}]^2 - f_{\Delta_i}^{(2)}f_{\Delta_i}^{(4)} &= 108\beta\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
    %     &- 12\beta\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
    %     &- 24\gamma\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
    %     &= 96\beta\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
    %     &- 24\gamma\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}}.
    % \end{align*}

    where we defined 
    \begin{align*}
        & \lambda_{i} = \biggl\| \sum_{\ell} \mbf{W}_{L:\ell+1} \Delta_{i} \mbf{W}_\ell \mbf{W}_{\ell-1:1}  \biggr\|_{\mathsf{F}} \quad \tag{Total $L\choose 1$ terms}\\
        & \beta_{i} =\biggl\| \left( \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right)\biggr\|_{\mathsf{F}} \quad \tag{Total $L\choose 2$ terms}\\
        & \delta_{i} = \biggl\| \left(\sum_{l<m<p} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:p+1} \Delta \mbf{W}_{p} \mbf{W}_{p-1:1} \right) \biggr\|_{\mathsf{F}}, \quad \tag{Total $L\choose 3$ terms}
    \end{align*}
and used the fact that $\mathrm{tr}(\mbf{A}^\top \mbf{B}) = \| \mbf{A} \|_{\mathsf{F}}\cdot \| \mbf{B}\|_{\mathsf{F}}$ under singular vector alignment.

Then, since $\beta_{i} $ has $L\choose 2$ terms inside the sum, when the Frobenium term is expanded, it will have $\frac{{L\choose 2}\left({L\choose 2}+1\right)}{2}$ number of terms. 
Under alignment and balancedness, $\beta^{2}_{i} = \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i}  \times \frac{{L\choose 2}\left({L\choose 2}+1\right)}{2}$ and $\lambda_{i} \delta_{i} =  \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i}  \times {L\choose 3} L$. Thus, we have the expression
\begin{align*}
   2\beta^{2}_{i} - \lambda_{i} \delta_{i} &= \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i} \left( 2 \frac{\binom{L}{2}\left(\binom{L}{2} + 1\right)}{2} -  \binom{L}{3} L \right) \\
   &= \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i}  \binom{L}{3} L \times \left( \frac{3\left(\frac{L(L-1)}{2}+1\right)}{L(L-2)} -1 \right) \\
   & =  \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i} \frac{2 \binom{L}{3} L }{L(L-2)} \times \left( (L-1)^2  + 5\right) > 0,\\
\end{align*}
for any depth $L>2$. Finally, the condition of stable oscillation of 1-D function is
\begin{align*}
       &  3[f_{\Delta_i}^{(3)}]^2 - f_{\Delta_i}^{(2)}f_{\Delta_i}^{(4)} =   108 \lambda^2_{i}\beta^{2}_{i} -  ( \lambda^{2}_{i})( 12\beta^{2}_{i} + 24(2\lambda_{i})(\delta_{i})) = 48 \lambda^{2}_{i} ( 2\beta^{2}_{i} - \lambda_{i} \delta_{i}  ) > 0,
\end{align*}
which we have proven to be positive for any depth $L>2$, for all the eigenvector directions corresponding to the non-zero eigenvalues. This completes the proof. 
    % Then, notice that from the first part of the proof,
    % \begin{align*}
    %     \|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} = 2\lambda_i > 0,
    % \end{align*}
    
\end{proof}



\begin{manualtheoreminner}[Rank-$p$ Periodic Subspace Oscillations]

Let $\mbf{M}_\star = \mbf{U}_\star \mbf{\Sigma}_\star \mbf{V}_\star^\top$ denote the SVD of the target matrix and define $S_p\coloneqq L \sigma^{2-\frac{2}{L}}_{\star,p}$ and $K'_p \coloneqq \mathrm{max} \left\{ S_{p+1},\frac{S_p}{2\sqrt{2}}\right\}$.
If we run GD on the deep matrix factorization loss with initialization scale $\alpha < \alpha'$ from Theorem~\ref{thm:stable_oscillations} and learning rate $\eta = \frac{2}{K}$, where $K'_p < K< S_p$, then under strict balancing, the top-$p$ singular values of the end-to-end DLN oscillates in a $2$-period orbit ($j \in \{1,2\}$) around the balanced minimum and admits the following decomposition:
\begin{align}
    \mbf{W}_{L:1} = \underbrace{\sum_{i=1}^p\rho_{i, j}^L \cdot \mbf{u}_{\star, i}\mbf{v}_{\star, i}^{\top} }_{\text{oscillation subspace}}+ \underbrace{\sum_{k=p+1}^d \sigma_{\star, k}\cdot \mbf{u}_{\star, k}\mbf{v}_{\star, k}^{\top}}_{\text{stationary subspace}}, \quad j \in \{1,2\}, \quad \forall\ell \in [L-1],
\end{align}
where $\rho_{i, 1} \in \left(0, \sigma_{\star, i}^{1/L}\right)$ and $\rho_{i, 2} \in \left(\sigma_{\star, i}^{1/L}, (2\sigma_{\star, i})^{1/L}\right)$ are the two real roots of the polynomial $g(\rho_i)=0$ and
\begin{align*}
    g(\rho_i) = \rho_i^L\cdot\frac{1+\left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right)^{2L-1}}{1+\left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right)^{L-1}} - \sigma_{\star, i}.
\end{align*}
\label{main:thm}
\end{manualtheoreminner}

\begin{proof}

To prove the result, we will consider the GD step on the $i$-th singular value and show that the $2$-period orbit condition holds given the learning rate $\eta = \frac{2}{K}$.
For ease of exposition, let us denote the $i$-th singular value of each $\mbf{W}_\ell$ as $\sigma_{i} \coloneqq \sigma_{\ell, i}$. Under balancing, consider the two-step GD update on the first singular value:
\begin{align*}
    \sigma_i(t+1) &= \sigma_i(t) + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)\right)\cdot \sigma^{L-1}_{i}(t) \\
      \sigma_i(t) = \sigma_i(t+2) &= \sigma_i(t+1) + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t+1)\right)\cdot \sigma^{L-1}_{i}(t+1). \tag{By 2-period orbit}
\end{align*}
Define $z \coloneqq \left(1 + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)\right)\cdot \sigma^{L-2}_{i}(t) \right)$ and by plugging in $\sigma_i(t+1)$ for $\sigma_i(t)$, we have
\begin{align*}
    \sigma_i(t) &= \sigma_i(t) z + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)z^L \right) \cdot \sigma_i^{L-1}(t)z^{L-1} \\
    \implies 1 &= z + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)z^L \right) \cdot \sigma_i^{L-2}(t)z^{L-1} \\
    \implies 1 &= \left(1 + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)\right)\cdot \sigma^{L-2}_{i}(t) \right) + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)z^L \right) \cdot \sigma_i^{L-2}(t)z^{L-1} \\
    \implies 0 &= \left(\sigma_{\star, i} - \sigma_i^L(t)\right) + \left(\sigma_{\star, i} - \sigma_i^L(t)z^L \right) \cdot z^{L-1}
\end{align*}
Simplifying this expression further, we have
\begin{align*}
    &0 = \sigma_{\star, i} - \sigma_i^L(t) + \sigma_{\star, i} z^{L-1} - \sigma_i^L(t) z^{2L-1} \\
    \implies &\sigma_i^L(t) + \sigma_i^L(t) z^{2L-1} =  \sigma_{\star, i} + \sigma_{\star, i} z^{L-1} \\
    \implies &\sigma_i^L(t)\cdot\left(1 + z^{2L - 1} \right) = \sigma_{\star, i}\cdot\left(1 + z^{L - 1} \right) \\
    \implies &\sigma_i^L(t)\frac{\left(1 + z^{2L - 1} \right)}{\left(1 + z^{L - 1} \right)} = \sigma_{\star, i},
\end{align*}
and by defining $\rho_i \coloneqq \sigma_i(t)$, we obtain the polynomial
\begin{align*}
    \sigma_{\star, i} = \rho_i^L\frac{1+z^{2L-1}}{1+z^{L-1}}, \quad \text{where  } \, z \coloneqq \left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right).
\end{align*}
Next, we show the existence of (real) roots within the ranges for $\rho_{i,1}$ and $\rho_{i, 2}$. We note that these roots only exist within the EOS regime.
First, consider $\rho_{i, 1} \in \left(0, \sigma_{\star, i}^{1/L} \right)$. We will show that for two values within this range, there is a sign change for all $L \geq 2$. More specifically, we show that there exists $\rho_i \in \left(0, \sigma_{\star, i}^{1/L} \right)$ such that
\begin{align*}
     \rho_i^L\frac{1+z^{2L-1}}{1+z^{L-1}} - \sigma_{\star, i} > 0 \quad \text{and} \quad \rho_i^L\frac{1+z^{2L-1}}{1+z^{L-1}} - \sigma_{\star, i} < 0.
\end{align*}
For the positive case, consider $\rho_i = (\frac{1}{2}\sigma_{\star, i})^{1/ L}$. We need to show that 
\begin{align*}
    \frac{1+z^{2L-1}}{1+z^{L-1}}  = \frac{1 + \left(1+\eta L\cdot\left(\frac{\sigma_{\star, i}}{2}\right)\frac{\sigma_{\star, i}^{1-\frac{2}{L}}}{2^{1 - \frac{2}{L}}}\right)^{2L-1}}{1 + \left(1+\eta L\cdot\left(\frac{\sigma_{\star, i}}{2}\right)\frac{\sigma_{\star, i}^{1-\frac{2}{L}}}{2^{1 - \frac{2}{L}}}\right)^{L-1}} > 2.
\end{align*}
To do this, we will plug in the smallest possible value of $\eta = \frac{2}{L\sigma_{\star, i}^{2 - \frac{2}{L}}}$ to show that the fraction is still greater than $2$, which gives us
\begin{align}
\label{eqn:first_range_pos}
    u(L) \coloneqq \frac{1 + \left(1+\frac{1}{ 2^{1 - \frac{2}{L}}} \right)^{2L-1}}{1 + \left(1+\frac{1}{ 2^{1 - \frac{2}{L}}} \right)^{L-1}},
\end{align}
which is an increasing function of $L$ for all $L\geq 2$. Since $u(2) > 2$, Equation~(\ref{eqn:first_range_pos}) must always be greater than $2$. For the negative case, we can simply consider $\rho_i = 0$.
Hence, since the polynomial is continuous, by the Intermediate Value Theorem (IVT), there must exist a root within the range $\rho_i \in \left(0, \sigma_{\star, i}^{1/L} \right)$.


Next, consider the range $\rho_{i, 2} \in \left(\sigma_{\star, i}^{1/L}, (2\sigma_{\star, i})^{1/L}\right)$. Similarly, we will show sign changes for two values in $\rho_{i, 2}$.
For the positive case, consider $\rho_i = \left(\frac{3}{2} \sigma_{\star, i}\right)^{1/L}$. For $\eta$, we can plug in the smallest possible value within the range to show that this value of $\rho_i$  provides a positive quantity. Specifically, we need to show that
\begin{align*}
    \frac{1+z^{2L-1}}{1+z^{L-1}} > \frac{2}{3} \implies \frac{1+\left(1+\frac{2}{\sigma_{\star, i}^{2- \frac{2}{L}}}\cdot(\sigma_{\star, i} - \frac{3}{2}\sigma_{\star, i})\cdot \left(\frac{3}{2}\sigma_{\star, i}\right)^{1 - \frac{2}{L}} \right)^{2L-1}}{1+\left(1+\frac{2}{\sigma_{\star, i}^{2- \frac{2}{L}}}\cdot(\sigma_{\star, i} - \frac{3}{2}\sigma_{\star, i})\cdot \left(\frac{3}{2}\sigma_{\star, i}\right)^{1 - \frac{2}{L}} \right)^{L-1}} > \frac{2}{3}.
\end{align*}
We can simplify the fraction as follows:
\begin{align*}
    \frac{1+\left(1+\frac{2}{\sigma_{\star, 1}^{2- \frac{2}{L}}}\cdot(\sigma_{\star, 1} - \frac{3}{2}\sigma_{\star, 1})\cdot \left(\frac{3}{2}\sigma_{\star, 1}\right)^{1 - \frac{2}{L}} \right)^{2L-1}}{1+\left(1+\frac{2}{\sigma_{\star, 1}^{2- \frac{2}{L}}}\cdot(\sigma_{\star, 1} - \frac{3}{2}\sigma_{\star, 1})\cdot \left(\frac{3}{2}\sigma_{\star, 1}\right)^{1 - \frac{2}{L}} \right)^{L-1}} = 
    \frac{1+\left(1-(\frac{3}{2})^{1 - \frac{2}{L}} \right)^{2L-1}}{1+\left(1-(\frac{3}{2})^{1 - \frac{2}{L}} \right)^{L-1}}.
\end{align*}
Then, since we are subtracting by $(\frac{3}{2})^{1 - \frac{2}{L}}$, we can plug in its largest value for $L\geq 2$, which is $3/2$. This gives us 
\begin{align*}
    \frac{1+\left(-0.5\right)^{2L-1}}{1+\left(-0.5 \right)^{L-1}} > \frac{2}{3},
\end{align*}
as for odd values of $L$, the function increases to $1$ starting from $L=2$, and decreases to $1$ for even $L$. 
To check negativity, let us define
\begin{align*}
    h(\rho) \coloneqq \frac{f(\rho)}{g(\rho)} \coloneqq \frac{\rho^L \left(1 + z^{2L-1} \right)}{1 + z^{L-1}}.
\end{align*}
We will show that $h'\left(\sigma_{\star, i}^{1/L} \right) < 0$:
\begin{align*}
h'\left(\sigma_{\star, i}^{1/L} \right) &= \frac{f'\left(\sigma_{\star, i}^{1/L} \right)g\left(\sigma_{\star, i}^{1/L} \right) - f\left(\sigma_{\star, i}^{1/L} \right)g'\left(\sigma_{\star, i}^{1/L} \right)}{g^2\left(\sigma_{\star, i}^{1/L} \right)} \\
&= \frac{f'\left(\sigma_{\star, i}^{1/L} \right) - \sigma_{\star, i}\cdot g'\left(\sigma_{\star, i}^{1/L} \right)}{2} \\
&= \frac{L\sigma_{\star, i}^{1 - \frac{1}{L}} - \sigma_{\star, i}(2L-1)\left(\eta L^2 \sigma_{\star, i}^{2 -\frac{3}{L}} \right) - \sigma_{\star, i}(L-1)\left(\eta L^2 \sigma_{\star, i}^{2 -\frac{3}{L}} \right) }{2} \\
&= \frac{L\sigma_{\star, i}^{1 - \frac{1}{L}} - (3L-2)\left(\eta L^2 \sigma_{\star, i}^{3 -\frac{3}{L}} \right) }{2} < 0,
\end{align*}
as otherwise we need $\eta \leq \frac{\sigma_{\star, i}^{2/L - 2}}{3L^2 - 2L}$, which is out of the range of interest. Since $h'(\rho)< 0$, it follows that there exists a $\delta > 0$ such that $h(\rho) > h(x)$ for all $x$ such that $\rho < x < \rho+\delta$. Lastly, since $h(\rho) - \sigma_{\star, i} = 0$ for $\rho = \sigma_{\star, i}^{1/L}$, it follows that $h(\rho) - \sigma_{\star, i}$ must be negative at $\rho + \delta$.
Similarly, by IVT, there must exist a root within the range 
$\rho_{i,2} \in \left(\sigma_{\star, i}^{1/L}, (2\sigma_{\star, i})^{1/L}\right)$. This proves that the $i$-th singular value undergoes a two-period orbit with the roots $\rho_{i, 1}$ and $\rho_{i, 2}$. Then, notice that if the learning rate is large enough to induce oscillations in the $i$-th singular value, then it is also large enough to have oscillations in all singular values from $1$ to the $(i-1)$-th singular value (assuming that it is not large enough for divergence). Finally, at the (balanced) minimum, we can express the dynamics as 
\begin{align}
    \mbf{W}_{L:1} = \underbrace{\sum_{i=1}^p\rho_{i, j}^L \cdot \mbf{u}_{\star, i}\mbf{v}_{\star, i}^{\top} }_{\text{oscillation subspace}}+ \underbrace{\sum_{k=p+1}^d \sigma_{\star, k}\cdot \mbf{u}_{\star, k}\mbf{v}_{\star, k}^{\top}}_{\text{stationary subspace}}, \quad j \in \{1,2\}, \quad \forall\ell \in [L-1].
\end{align}
This completes the proof.


\end{proof}







\begin{manuallemmainner}[Balancing]
  Let $\sigma_{\star, i}$ and $\sigma_{\ell, i}(t)$ denote the $i$-th singular value of $\mbf{M}_\star \in \mbb{R}^{d\times d}$ and $\mbf{W}_\ell(t)$, respectively and define $S_i \coloneqq L \sigma^{2-\frac{2}{L}}_{\star,i}$.
    Consider GD on the $i$-th index of the simplified loss in~(\ref{eqn:simplified_loss}) with the unbalanced initialization and learning rate $\frac{2}{S_i} < \eta < \frac{2\sqrt{2}}{S_i}$. If the initialization scale $\alpha$ satisfies
    $0 < \alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta S_i} \right) \cdot \frac{ \sigma_{\star, i}^{4/L}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{1/4}$, then there exists a constant $c \in (0, 1]$ such that for all $\ell \in [L-1]$, we have $\left| \sigma^2_{L, i}(t+1) - \sigma^2_{\ell, i}(t+1)\right| < c\cdot \left| \sigma^2_{L, i}(t) - \sigma^2_{\ell, i}(t)\right|$.
\end{manuallemmainner}

\begin{proof}



To prove the result, we adopt the analysis from~\cite{kreisler2023gradient}, where they use the notion of gradient flow solution (GFS) sharpness defined below:
\begin{definition}[GFS Sharpness]
    The GFS sharpness denoted by $\psi(x)$ is the sharpness achieved by the global minima which lies in the same GF trajectory of $x$ (i.e., $\|\nabla^2 L(z)\|$ such that $L(z)=0$ and $z = GF(x)$, where $GF(\cdot)$ denotes the gradient flow solution).
\end{definition}

\noindent Then, let us consider the $i$-th index of the simplified loss in~(\ref{eqn:simplified_loss}):
\begin{align*}
    \frac{1}{2} \left(\prod_{\ell=1}^L \sigma_{\ell, i} - \sigma_{\star, i}  \right)^2 \eqqcolon \frac{1}{2} \left(\prod_{\ell=1}^L \sigma_{\ell} - \sigma_{\star}  \right)^2  ,
\end{align*}
and omit the dependency on $i$ for ease of exposition. Our goal is to show that the $L$-th singular value $\sigma_L$ initialized to zero become increasingly balanced to $\sigma_\ell$ which are initialized to $\alpha$.
%So, first we show that in a singular value scalar loss, for a learning rate $\eta > \frac{2}{S_{i}}$, oscillations occur provably for any initialization range $0<\alpha$. We do this in the following steps: 1) show that the sharpness around the global minima is the least when iterates are balanced, 2) By using the descent lemma, if $\eta>\frac{2}{S_{i}}$, then oscillations start occuring at the balanced point, 3) By lemma-1, if oscillations can occur at balanced point, then oscillation can occur anywhere around the global minima. So, $\eta>\frac{2}{S_{i}}$ is a sufficient condition to show oscillation. \footnote{Oscillations can also start occuring for some $\eta<\frac{2}{S_{i}}$ but will eventually subside once it reaches close to balanced minima. }
To that end, let us define the balancing dynamics between $\sigma_i$ and $\sigma_j$ as $b_{i,j}^{(t+1)} \coloneqq \left(\sigma_i^{(t+1)}\right)^2 - \left(\sigma_j^{(t+1)}\right)^2$ and $\pi^{(t)} \coloneqq \prod_{\ell=1}^L \sigma_{\ell}(t)$ for the product of singular values at iteration $t$.
We can simplify the balancing dynamics as follows:
% \begin{align*}
%     \pi^{(t+1)} = \prod_{i=1}^{L} \sigma_{i}^{(t+1)} 
%     &= \prod_{i=1}^{D} \left(\sigma_i^{(t)} - \eta \left(\pi^{(t)} - 1 \right)\frac{\pi^{(t)}}{\sigma_i^{(t)}} \right) \\
%     &= \pi^{(t)} \prod_{i=1}^{L} \left(1 - \eta \left(\pi^{(t)} - 1 \right) \frac{\pi^{(t)}}{\left(\sigma_i^{(t)}\right)^2}\right) \\
%     &= \pi^{(t)} + \sum_{m=1}^{L} \eta^m (1 - \pi^{(t)})^m \pi^{(t)} \sigma_m(\sigma^{(t)}).
% \end{align*}

% Defining  to be the balance,
% The dynamic of the balances then becomes:
\begin{align*}
    b_{i,j}^{(t+1)} &= \left(\sigma_i^{(t+1)}\right)^2 - \left(\sigma_j^{(t+1)}\right)^2 \\
    &= \left(\sigma_i^{(t)} - \eta\left(\pi^{(t)} - \sigma_{\star}\right)\frac{\pi^{(t)}}{\sigma_i^{(t)}}\right)^2 - \left(\sigma_j^{(t)} - \eta\left(\pi^{(t)} - \sigma_{\star}\right)\frac{\pi^{(t)}}{\sigma_j^{(t)}}\right)^2 \\
    &= \left(\sigma_i^{(t)}\right)^2 - \left(\sigma_j^{(t)}\right)^2 + \eta^2 \left(\pi^{(t)} - \sigma_{\star}\right)^2 \left(\frac{\left(\pi^{(t)}\right)^2}{\left(\sigma_i^{(t)}\right)^2} - \frac{\left(\pi^{(t)}\right)^2}{\left(\sigma_j^{(t)}\right)^2}\right) \\
    &= \left(\left(\sigma_i^{(t)}\right)^2 - \left(\sigma_j^{(t)}\right)^2 \right) \left( 1 - \eta^2 (\pi^{(t)} - \sigma_{\star})^2 \frac{\left(\pi^{(t)}\right)^2}{\left(\sigma_i^{(t)}\right)^2 \left(\sigma_j^{(t)}\right)^2} \right) \\
    &= b_{i,j}^{(t)} \left( 1 - \eta^2 (\pi^{(t)} - \sigma_{\star})^2 \frac{\left(\pi^{(t)}\right)^2}{\left(\sigma_i^{(t)}\right)^2 \left(\sigma_j^{(t)}\right)^2} \right).
\end{align*}
Then, in order to show that $ \left|b_{i,j}^{(t+1)}\right| < c\left|b_{i,j}^{(t)}\right|$ for some $0<c \leq 1$, we need to prove that
\begin{align*}
    \left | 1 - \eta^2 (\pi^{(t)} -\sigma_{\star} )^2 \frac{\left(\pi^{(t)}\right)^2}{\left(\sigma_i^{(t)}\right)^2 \left(\sigma_j^{(t)}\right)^2} \right| < c,
\end{align*}
 for all iterations $t$. Note that for our case, it is sufficient to show the result for $i=L$ and $j=\ell$ for any $\ell \neq L$.
 %Now we introduce  a definition called gradient flow solution sharpness (GFS sharpness) before we proceed.
We complete this using the following two steps:
\begin{enumerate}[label=(\roman*)]
    \item We show that for all scalars $\sigma$ in the trajectory, if $\psi(\sigma)< \frac{2\sqrt{1+c}}{\eta}$ and $\sigma > 0$, then it holds that $\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - \sigma_{\star} )^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2} \leq 1+c$, where $\pi(\sigma)$ denotes the product given the trajectory of all $\sigma_i$. This case is analyzed when  \(\pi(\sigma) \in [0,\sigma_{\star} )\) where $0<c<1$ and when $\pi(\sigma) > \sigma_{\star}$ where $c=1$.
    \item If $\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - \sigma_{\star} )^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2} \leq 1+c$, then iterates become more balanced, i.e,  $|b_{i,j}^{(t+1)}| <c |b_{i,j}^{(t)}|$.
\end{enumerate}

\noindent We prove (i) in Lemma \ref{GFS-1} and (ii) in Lemma \ref{GFS-2}. Both of the proofs are originally from \cite{kreisler2023gradient}, which we adapted using our notation for ease of the reader. Then, in Lemma~\ref{GFS-3}, we show that for each $\sigma_\star$, as long as the initialization scale satisfies
\begin{align*}
    \alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{ \sigma_{\star}^{\frac{4}{L}}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{\frac{1}{4}} ,
\end{align*}
then it holds that the GFS sharpness satisfies $\psi(\sigma) < \frac{2{\sqrt{2}}}{\eta}$, which is the necessary condition for balancing. Then, to satisfy this condition for all singular values $\sigma_{\star, i}$ for all $i \in [r]$, we need
\begin{align}
\label{eqn:balance_condition}
    \alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star, 1}^{2 - \frac{2}{L}}} \right) \cdot \frac{ \sigma_{\star, 1}^{\frac{4}{L}}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{\frac{1}{4}}  \implies \eta < \frac{2\sqrt{2}}{L \sigma_{\star, 1}^{2 - \frac{2}{L}}},
\end{align}
 for the validity of the initialization scale. Thus, as long as the conditions in Equation~(\ref{eqn:balance_condition}) hold, we will have balancing. This completes the proof. 
\end{proof}



\begin{manuallemmainner}
[Hessian Eigenvalues at Balanced Minimum]
     Consider running GD on the deep matrix factorization loss $f(\mbf{\Theta})$ defined in Equation~(\ref{eqn:deep_mf}). The set of all non-zero eigenvalues of the training loss Hessian at the balanced minimum is given by
    \begin{align*}
       \lambda_{\mbf{\Theta}} = \underbrace{\left\{L \sigma_{\star, i}^{2 - \frac{2}{L}}, \sigma_{\star, i}^{2 - \frac{2}{L}}\right\}_{i=1}^r }_{\text{self-interaction}} \, \bigcup \, \underbrace{\left\{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, i}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, j}^{\frac{1}{L}\ell}\right)^2\right\}_{i\neq j}^{r}}_{\text{interaction with other singular values}} \,\bigcup \, \underbrace{\left\{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, k}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \alpha^{\ell}\right)^2\right\}_{k = 1}^{r}}_{\text{interaction with initialization}} 
    \end{align*}
    where $\sigma_{\star, i}$ is the $i$-th singular value of the target matrix $\mbf{M}_\star \in \mbb{R}^{d\times d}$,  $\alpha \in \mbb{R}$ is the initialization scale, $L$ is the depth of the network, and the second element of the set under ``self-interaction'' has a multiplicity of $d-r$.
\end{manuallemmainner}

\begin{proof}
By Proposition~\ref{prop:one_zero_svs_set}, notice that we can re-write the loss in Equation~(\ref{eqn:deep_mf}) as 
    \begin{align*}
        \frac{1}{2} \left\|\mbf{W}_{L:1} - \mbf{M}_\star\right\|^2_{\mathsf{F}} = \frac{1}{2} \|\mbf{\Sigma}_{L:1} - \mbf{\Sigma}_\star\|^2_{\mathsf{F}},
    \end{align*}
    where $\mbf{\Sigma}_{L:1}$ are the singular values of $\mbf{W}_{L:1}$. We will first show that the eigenvalues of the Hessian with respect to the weight matrices $\mbf{W}_\ell$ are equivalent to those of the Hessian taken with respect to its singular values $\mbf{\Sigma}_\ell$.
    To this end, consider the vectorized form of the loss:
    \begin{align*}
        f(\mbf{\Theta}) \coloneqq \frac{1}{2}\|\mbf{W}_{L:1} - \mbf{M}_\star\|^2_{\mathsf{F}} = \frac{1}{2}\|  \text{vec}(\mbf{W}_{L:1}) - \text{vec}(\mbf{M}_\star)\|^2_2.
    \end{align*}
    Then, each block of the Hessian $ \nabla_{\mbf{\Theta}}^2 f(\mbf{\Theta}) \in \mbb{R}^{d^2 L \times d^2 L}$ with respect to the vectorized parameters is given as
    \begin{align*}
    \left[\nabla_{\mbf{\Theta}}^2 f(\mbf{\Theta})\right]_{m, \ell} = \nabla_{\text{vec}(\mbf{W}_{m})} \nabla^\top_{\text{vec}(\mbf{W}_{\ell})} f(\mbf{\Theta}) \in \mbb{R}^{d^2 \times d^2}.
    \end{align*}
    %Here, notice that 
%Note that each of the Hessian wrt product combinatinos $\nabla_{\text{vec}(\mbf{W_{l}})} \nabla_{\text{vec}(\mbf{W}_{m})^\top} f(\mbf{\Theta})  \in \mathbf{R}^{n^2 \times n^2} $ which makes  $  \nabla_{\mbf{\Theta}}^2 f(\mbf{\Theta}) \in \mathbf{R}^{n^2 L \times n^2 L}$.
By the vectorization trick, each vectorized layer matrix has an SVD of the form $\text{vec}(\mbf{W}_{\ell}) = \text{vec} (\mbf{U}_{\ell} \mbf{\Sigma}_{\ell} \mbf{ V}^\top_{\ell}) = (\mbf{V}_{\ell} \otimes \mbf{U}_{\ell}) \cdot \text{vec}(\mbf{\Sigma}_{\ell})$.
Then, notice that we have
\begin{align*}
    \nabla_{\text{vec}(\mbf{W}_{\ell})} f(\mbf{\Theta}(t)) = (\mbf{ V}_{\ell} \otimes \mbf{U}_{\ell})  \cdot \nabla_{\text{vec}(\mbf{ \Sigma}_{\ell})}f(\mbf{\Theta}(t)),
\end{align*}
which gives us that 
each block of the Hessian is given by  
%Similarly, calculating the $ml$ block product matrix of the Hessian, we have:
\begin{align*}
    \left[\nabla_{\mbf{\Theta}}^2 f(\mbf{\Theta})\right]_{m, \ell} &= \nabla_{\text{vec}(\mbf{W}_{m})} \nabla^\top_{\text{vec}(\mbf{W}_{\ell})} f(\mbf{\Theta})\\
  &=  (\mbf{V}_{m} \otimes \mbf{U}_{m})\cdot \underbrace{\nabla_{\text{vec}(\mbf{\Sigma}_{m})}   \nabla^\top_{\text{vec}(\mbf{\Sigma}_{\ell})} f(\mbf{\Theta})}_{\eqqcolon \mbf{H}_{m, \ell}}\cdot (\mbf{V}_{\ell} \otimes \mbf{U}_\ell)^\top.
\end{align*}
Then, since the Kronecker product of two orthogonal matrices is also an orthogonal matrix by Lemma~\ref{lemma:kronecker_ortho}, we can write the overall Hessian matrix as  
    \begin{align*}
        \widetilde{\mbf{H}} =
        \begin{bmatrix}
            \mbf{R}_1\mbf{H}_{1, 1}\mbf{R}_1 & \mbf{R}_1\mbf{H}_{1, 2}\mbf{R}_2 & \ldots & \mbf{R}_1\mbf{H}_{1, L}\mbf{R}_L \\
            \mbf{R}_2\mbf{H}_{2, 1}\mbf{R}_1& \mbf{R}_2 \mbf{H}_{2, 2}\mbf{R}_2 & \ldots & \mbf{R}_2\mbf{H}_{2, L} \mbf{R}_L\\
            \vdots & \vdots & \ddots & \vdots \\
             \mbf{R}_L\mbf{H}_{L, 1}\mbf{R}_1 & \mbf{R}_L\mbf{H}_{L, 2}\mbf{R}_2 & \ldots & \mbf{R}_L\mbf{H}_{L, L}\mbf{R}_L
        \end{bmatrix},
    \end{align*}
 for orthogonal matrices $\{\mbf{R}_\ell\}_{\ell=1}^L$. Then, by Lemma~\ref{lem:relationship_lemma}, the eigenvalues of $\widetilde{\mbf{H}}$ are the same as those of $\mbf{H}$, where $\mbf{H} \in \mbb{R}^{d^2 L \times d^2 L}$ is the Hessian matrix with respect to the vectorized $\mbf{\Sigma}_\ell$:
\begin{align*}
    \mbf{H} = \begin{bmatrix}
            \mbf{H}_{1,1} & \mbf{H}_{1, 2} & \hdots &\mbf{H}_{L, 1}\\
            \mbf{H}_{2,1} & \mbf{H}_{2,2} & \hdots & \mbf{H}_{L, 2} \\
            \vdots & \vdots & \ddots & \vdots \\
            \mbf{H}_{1, L} & \mbf{H}_{2, L} & \hdots &\mbf{H}_{L, L}
        \end{bmatrix}.
\end{align*}
   Now, we can consider the following vectorized loss:
    \begin{align*}
        f(\mbf{\Theta}) = \frac{1}{2} \|\mbf{\Sigma}_{L:1} - \mbf{\Sigma}_\star\|_\mathsf{F}^2 &= \frac{1}{2} \left\|\mathrm{vec}\left(\mbf{\Sigma}_{L:1} - \mbf{\Sigma}_\star\right)\right\|_2^2 \\&= \frac{1}{2} \| \underbrace{\left(\mbf{\Sigma}^\top_{\ell-1:1} \otimes \mbf{\Sigma}_{L:\ell+1} \right)}_{\eqqcolon \mbf{A}_{\ell}}\cdot\mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{\Sigma}_\star) \|_2^2. 
    \end{align*}
    Then, the gradient with respect to $\mathrm{vec}(\mbf{\Sigma}_{\ell})$ is given by
    \begin{align*}
        \nabla_{\mathrm{vec}(\mbf{\Sigma}_{\ell})} f(\mbf{\Theta}) = \mbf{A}_{\ell}^\top \left( \mbf{A}_{\ell}\cdot \mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{\Sigma}_\star)\right).
    \end{align*}
   
    Then, for $m=\ell$, we have
    \begin{align*}
        \mbf{H}_{\ell, \ell} = \nabla^2_{\mathrm{vec}(\mbf{\Sigma}_{\ell})} f(\mbf{\Theta}) &= \mbf{A}_{\ell}^{\top}\mbf{A}_{\ell}. 
    \end{align*}
For $m\neq \ell$, we have
\begin{align*}
   & \mbf{H}_{m, \ell} = \nabla_{\mathrm{vec}(\mbf{\Sigma}_{m})}  \nabla_{\mathrm{vec}(\mbf{\Sigma}_{\ell})} f(\mbf{\Theta}) =  \nabla_{\mathrm{vec}(\mbf{\Sigma}_{m})} \left[\mbf{A}_{\ell}^\top (\mbf{A}_{\ell} \mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{M}^{\star})) \right] \\
   & = \nabla_{\mathrm{vec}(\mbf{\Sigma}_{m})} \mbf{A}_{\ell}^\top \cdot \underbrace{(\mbf{A}_\ell \mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{M}^{\star}))}_{=0 \text{ 
 at convergence}} + \mbf{A}_{\ell}^{\top} \cdot  \nabla_{\mathrm{vec}(\mbf{\Sigma}_{m})} (\mbf{A}_{\ell} \mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{M}^{\star})) \\
   & = \mbf{A}_{\ell}^\top \mbf{A}_{m},
\end{align*}
where we have used the product rule along with the fact that $\mbf{A}_{\ell} \mathrm{vec}(\mbf{\Sigma}_{\ell}) = \mbf{A}_m \mathrm{vec}(\mbf{\Sigma}_{m})$.

Overall, the Hessian at convergence for GD is given by
\begin{align*}
    \mbf{H} =
    \begin{bmatrix}
        \mbf{A}_{1}^\top \mbf{A}_{1} & \mbf{A}_{1}^\top \mbf{A}_{2} & \ldots & \mbf{A}_{1}^\top \mbf{A}_{L} \\
        \mbf{A}_{2}^\top \mbf{A}_{1} & \mbf{A}_{2}^\top \mbf{A}_{2} & \ldots & \mbf{A}_{2}^\top \mbf{A}_{L} \\
        \vdots & \vdots & \ddots & \vdots \\
        \mbf{A}_{L}^\top \mbf{A}_{1} & \mbf{A}_{L}^\top \mbf{A}_{2} & \ldots & \mbf{A}_{L}^\top \mbf{A}_{L}
    \end{bmatrix}
\end{align*}
Now, we can derive an explicit expression for each $\mbf{A}_{m, \ell}$ by considering the implicit balancing effect of GD in Proposition~\ref{prop:balancing}. Under balancing and Proposition~\ref{prop:one_zero_svs_set}, we have that at convergence,
    \begin{align*}
        \mbf{\Sigma}_{L:1} = \mbf{\Sigma}_\star \implies \mbf{\Sigma}_{\ell} = \begin{bmatrix}
            \mbf{\Sigma}^{1/L}_{\star, r} & \mbf{0} \\
            \mbf{0} & \alpha \cdot \mbf{I}_{d-r}
        \end{bmatrix}, \quad \forall \ell \in [L-1], \quad \text{and} \,\,\, \mbf{\Sigma}_L = \mbf{\Sigma}^{1/L}_{\star}.
    \end{align*}
    Thus, we have
    \begin{align*}
        \mbf{H}_{m, \ell} = \begin{cases}
            \mbf{\Sigma}_{\ell}^{2(\ell -1)} \otimes \mbf{\Sigma}_{\star}^{\frac{2(L-\ell)}{L}} \quad& \text{for } \,m=\ell, \\
            \mbf{\Sigma}_\ell^{m+\ell - 2} \otimes \mbf{\Sigma}_{\star}^{2L -m-\ell} \quad& \text{for }\, m\neq\ell. \\
        \end{cases}
    \end{align*}
        Now, we are left with computing the eigenvalues of $\mbf{H} \in \mbb{R}^{d^2 L \times d^2 L}$. To do this, let us block diagonalize $\mbf{H}$ into $\mbf{H} = \mbf{PCP}^\top$, where $\mbf{P}$ is a permutation matrix and 
    \begin{align*}
        \mbf{C} = 
        \begin{bmatrix}
            \mbf{C}_{1} & & \\
            & \ddots & \\
            &&\mbf{C}_{d^2}
        \end{bmatrix} \in \mbb{R}^{d^2 L \times d^2 L},
    \end{align*}
    where each $(i,j)$-th entry of $\mbf{C}_k \in \mbb{R}^{L \times L}$ is the $k$-th diagonal element of $\mbf{H}_{i, j}$. Since $\mbf{C}$ and $\mbf{H}$ are similar matrices, they have the same eigenvalues.
    Then, since $\mbf{C}$ is a block diagonal matrix, its eigenvalues (and hence the eigenvalues of $\mbf{H}$) are the union of each of the eigenvalues of its blocks. 

    By observing the structure of $\mbf{H}_{m, \ell}$, notice that each $\mbf{C}_k$ is a rank-$1$ matrix. Hence, when considering the top-$r$ diagonal elements of $\mbf{H}_{m, \ell}$ corresponding to each Kronecker product to construct $\mbf{C}_k$, each $\mbf{C}_k$ can be written as an outer product $\mbf{uu}^{\top}$, where $\mbf{u} \in \mbb{R}^L$ is
    \begin{align}
        \mbf{u}^{\top} = \begin{bmatrix}
            \sigma_{\star, i}^{1 - \frac{1}{L}} \sigma_{\star, j}^{0} & \sigma_{\star, i}^{1 - \frac{2}{L}} \sigma_{\star, j}^{\frac{1}{L}} & \sigma_{\star, i}^{1 - \frac{3}{L}} \sigma_{\star, j}^{\frac{2}{L}} & \ldots & \sigma_{\star, i}^{0} \sigma_{\star, j}^{1 - \frac{1}{L}} 
        \end{bmatrix}^{\top}.
    \end{align}
    Then, the non-zero eigenvalue of this rank-$1$ matrix is simply $\|\mbf{u}\|_2^2$, which simplifies to 
    \begin{align*}
        \|\mbf{u}\|_2^2 = \sum_{\ell=0}^{L-1} \left(\sigma_{\star, i}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, j}^{\frac{1}{L}\ell}\right)^2.
    \end{align*}
    Next, we can consider the remaining $d-r$ components of each Kronecker product of $\mbf{H}_{m, \ell}$. Notice that for $m = \ell = L$, we have
    \begin{align*}
        \mbf{H}_{L, L} = \begin{bmatrix}
            \sigma_{\star, 1}^{\frac{2(L-1)}{L}} \cdot \mbf{I}_d & & & \\
            & \ddots & & \\
            & & \sigma_{\star, r}^{\frac{2(L-1)}{L}} \cdot \mbf{I}_d  & \\
            & & & \alpha^{2(L-1)}\mbf{I}_{d-r} \otimes \mbf{I}_d
        \end{bmatrix}. 
    \end{align*}
    This amounts to a matrix $\mbf{C}_k$ with a single element  $\sigma_{\star, i}^{\frac{2(L-1)}{L}}$ and $0$ elsewhere. This gives an eigenvalue $\sigma_{\star, i}^{\frac{2(L-1)}{L}}$  for all $i \in [r]$, with multiplicity $d-r$. 

    Lastly, we can consider the diagonal components of $\mbf{H}_{m, \ell}$ that is a function of the initialization scale $\alpha$. For this case, each $\mbf{C}_k$ can be written as an outer product $\mbf{vv}^{\top}$, where 
    \begin{align}
        \mbf{v}^{\top} = \begin{bmatrix}
            \sigma_{\star, i}^{1 - \frac{1}{L}} \alpha^{0} & \sigma_{\star, i}^{1 - \frac{2}{L}} \alpha& \sigma_{\star, i}^{1 - \frac{3}{L}} \alpha^{2} & \ldots & \sigma_{\star, i}^{0} \alpha^{L-1}
        \end{bmatrix}^{\top}.
    \end{align}
    Similarly, the non-zero eigenvalue is simply $\|\mbf{v}\|_2^2$, which corresponds to
    \begin{align*}
        \|\mbf{v}\|_2^2 = \sum_{\ell=0}^{L-1} \left(\sigma_{\star, k}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \alpha^{\ell}\right)^2.
    \end{align*}
    This completes the proof.
\end{proof}



\begin{manuallemmainner}
\label{GFS-1}
    If the GFS sharpness $\psi(\sigma) \leq \frac{2\sqrt{1+c}}{\eta}$ and $\sigma > 0$, then  $\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma)-\sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{[L-i]}^2 \sigma_{[D]}^2} \leq (1+c)$ for some $0<c\leq1$.
\end{manuallemmainner}

\begin{proof}

We will consider two cases: (i) $\pi(\sigma) \in [0, \sigma_\star)$ and (ii) $\pi(\sigma) > \sigma_\star$ \footnote{We ignore the case $\pi^{(t)} =\sigma_{\star}$ when we get $b_{i,j}^{(t+1)} = b_{i,j}^{(t)}$. Since the occurence $\pi^{(t)} =\sigma_{\star}$ holds with a probability of zero. } .\\

\noindent \textbf{Case 1:} Let \(\sigma \in \mathbb{R}^D\) and consider the case where \(\pi(\sigma) \in [0,\sigma_{\star} )\). Then, we have
\[
\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - \sigma_{\star} )^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2 } 
\leq \frac{\eta^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2}.
\]
Our goal is to show that if \(\psi(\sigma) \leq \frac{2\sqrt{1+c}}{\eta}\) for some $0<c<1$ then, 
\[
\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - 1)^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2}  \leq \frac{\eta^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2}\leq 1+c.
\]

Since the GFS sharpness is constant for all the weights on the gradient flow (GF) trajectory by definition, we can focus on the singular values (or weights) at the global minima.
Consider $z = \mathrm{GF}(\sigma)$, the GF solution of $\sigma$. In Lemma \ref{gf-unbalanced}, we proved that GF preserves unbalancedness, such that $\sigma^2_{l} - \sigma^2_{m} = z^2_{l} - z^2_{m}$ for all layers. Hence, it is sufficient to show that $\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \pi(z)^2}{z_{L-i}^2 z_{L}^2} \leq 1+c$ in order to ensure $\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2} \leq 1+c$. Note that $\pi(z)=\sigma_{\star}$, since it lies on the global minima. Then,
\begin{align}
    \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \pi^2(z)}{z^{2}_{L-i} z^2_{L}} 
= \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \sigma^2_{\star}}{z^{2}_{L-i} z^2_{L}}.
\end{align}
From Lemma~\ref{1d-sharp}, we know that the sharpness at the global minima is given as
\begin{align}
\label{eqn:helper1}
\psi(\sigma)=\left\| \nabla^2 L(z) \right\| = \sum_{i=1}^{L} \frac{ \sigma^2_{\star}}{z_i^2}.
\end{align}
This immediately implies that \(\frac{\sigma^2_{\star}}{z^2_{L}} \leq \psi(\sigma)\) and equivalently, \(\exists \alpha \in [0,1]\) such that $\frac{\sigma^2_{\star}}{z^2_{L}} = \alpha \psi(\sigma)$.
Therefore, we have
\begin{align}
\label{eqn:helper2}
    \sum_{i=1}^{\min\{2,L-1\}} \frac{\sigma^2_{\star}}{z^2_{L-i}} \leq (1 - \alpha) \psi(\sigma).
\end{align}
Substituting Equations~(\ref{eqn:helper1}) and~(\ref{eqn:helper2}) into the expression we aim to bound, we obtain

\[
\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - \sigma^2_{\star})^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2}
= \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \sigma^2_{\star}}{z^{2}_{L-i} z^2_{L}} 
\leq \eta^2 \alpha (1 - \alpha) \psi^2(\sigma) \leq \frac{\eta^2}{4} \psi^2(\sigma) \leq 1+c,
\]
where we used the fact that the maximum of $\alpha(1-\alpha) $ is $\frac{1}{4}$ when \(\alpha = \frac{1}{2}\) and \(\psi(\sigma) \leq \frac{2\sqrt{1+c}}{\eta}\).
Thus, if \(\psi(\sigma) \leq \frac{2\sqrt{1+c}}{\eta}\), then for every weight $\sigma$ lying on its GF trajectory, we have
\begin{align*}
    \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2} \leq 1+c.
\end{align*}


\noindent \textbf{Case 2:} Consider the case in which $\pi(\sigma) > \sigma_{\star}$. We already have that $\sigma>0$ throughout the trajectory (refer to Lemma 3.11 in \cite{kreisler2023gradient}) and so  $\pi(\sigma)>0$. So, the GD update from $\sigma_{i}$ will also stay positive
\begin{align*}
    \sigma_{i}-\eta (\pi(\sigma) - \sigma_{\star})\pi(\sigma) \frac{1}{\sigma_{i}} >0.
\end{align*}
From this, we get 
\begin{align*}
  2 >  \frac{\eta (\pi(\sigma) - \sigma_{\star})\pi(\sigma)}{\sigma^2_{i}}>0,
\end{align*}
This implies $\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2} \leq (1+c)$ with $c=1$. This completes the proof.
\end{proof}




\begin{manuallemmainner}
\label{GFS-2}
    If $\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2} \leq 1+c$ for $i,j \in [L]$ for some $0<c\leq1$, then $ \left|b_{i,j}^{(t+1)}\right| < c \left|b_{i,j}^{(t)}\right|$.
\end{manuallemmainner}

\begin{proof}

Recall that the condition for balancing was given by
\begin{align}
\label{balance}
 b_{i,j}^{(t+1)}  = b_{i,j}^{(t)} \left( 1 - \eta^2 (\pi^{(t)} - \sigma_{\star})^2 \frac{\pi^{(t)2}}{\left(\sigma_i^{(t)}\right)^2 \left(\sigma_j^{(t)}\right)^2} \right).
\end{align}
WLOG, suppose that the $\sigma$ are sorted such that $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_L$. We know that  $$\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-i}^2 \sigma_{L}^2} \leq 1+c,$$ 
which implies 
\begin{align}
        \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-1}^2 \sigma_{L}^2} < 1+c \quad \text{and} \quad 
        \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{i}^2 \sigma_{j}^2} < \frac{1+c}{2},
\end{align}
for all $i \in [L]$, $j \in [L-2] $ and $ i < j$. Notice that the latter inequality comes from the fact that 
\begin{align*}
    \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-2}^2 \sigma_{L}^2} + \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-2}^2 \sigma_{L}^2} &< \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-1}^2 \sigma_{L}^2} + \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-2}^2 \sigma_{L}^2} \\
    &< 1+c,
\end{align*}
which implies that
\begin{align*}
    2\frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-2}^2 \sigma_{L}^2} < 1+c \implies \frac{\eta^2 (\pi(\sigma) - \sigma_{\star})^2 \pi^2(\sigma)}{\sigma_{L-2}^2 \sigma_{L}^2} < \frac{1+c}{2},
\end{align*}
and since $\sigma$ are sorted, it holds for all other $\sigma$.
Therefore from Equation~(\ref{balance}), we have for all $i \in [L-2]$,
\begin{align}
    b^{(t+1)}_{i,i+1} < c b^{(t)}_{i,i+1} \quad \text{and}\quad b^{(t+1)}_{L-2,L} < c b^{(t)}_{L-2,L}
    \quad \text{and} \quad -c b^{(t)}_{L-1,L}  <b^{(t+1)}_{L-1,L} < c b^{(t)}_{L-1,L} .
\end{align}
%Now, we prove balancing considering two cases where $b^{(t+1)}_{L-1,L} \geq 0 $ and $b^{(t+1)}_{L-1,L} \leq 0 $. 
%First, consider the case $b^{(t+1)}_{L-1,L} \geq 0 $ then using the fact for all $i \in [L-2]$,
%$b^{(t+1)}_{i,i+1} <  b^{(t)}_{i,i+1}$, $b^{(t+1)}_{i,i+1} <  b^{(t)}_{i,i+1}$.
Then, notice that since we initialized all of the singular values $\sigma_\ell$ for $\ell \in [L-1]$ to be the same, they follow the same dynamics. Since we already showed that $|b^{(t+1)}_{L-1,L}| < c |b^{(t)}_{L-1,L}|$, it must follow that
\begin{align*}
    \left|b_{i,j}^{(t+1)}\right| < c \left|b_{i,j}^{(t)}\right| \quad \text{for } \, i,j \in [L].
\end{align*}
This completes the proof.

%Hence we prove balancing for all $i,j \in [L]$, we have $ b_{i,j}^{(t+1)} \leq b_{i,j}^{(t)}$. 


%Now considering the case $b^{(t+1)}_{L-1,L} \leq 0 $, we will only obtain till $i \in [L-3]$, that $b^{(t+1)}_{i,i+1} <  b^{(t)}_{i,i+1}$ and $b^{(t+1)}_{L-2,L} <  b^{(t)}_{L-2,L} $
%Now considering the case $b^{(t+1)}_{L-1,L} \leq 0 $, we will only obtain till $i \in [L-3]$, that $b^{(t+1)}_{i,i+1} <  b^{(t)}_{i,i+1}$ and $b^{(t+1)}_{L-1,L} <  b^{(t)}_{L-1,L} $. This completes the proof. 
\end{proof}




\begin{manuallemmainner}
\label{GFS-3}
    Consider running GD with learning rate $\eta$ in Equation~(\ref{eqn:gd}) on the scalar loss 
    \begin{align*} 
        \mathcal{L}(\{\sigma_i\}_{i=1}^d)
 = \frac{1}{2} \left( \prod_{i=1}^L \sigma_{i} - \sigma_{\star} \right)^2,
    \end{align*} 
    with initialization
    $\sigma_{L}(0) = 0$ and $\sigma_{\ell}(0) = \alpha$ for all $\ell \in [L-1]$.
    If $\alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{ \sigma_{\star}^{\frac{4}{L}}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{\frac{1}{4}}$, then
    the GFS sharpness $\psi(\sigma) \leq \frac{2\sqrt{1+c}}{\eta}$ for some $0<c<1$.
\end{manuallemmainner} 


\begin{proof}
    %We prove that the initialization set $\sigma_{L}(0) = 0$ and $\sigma_{\ell}(0) = \alpha$ for all $\ell \in [L-1]$ has a GFS sharpness $\psi(\sigma) \leq \frac{2\sqrt{2}}{\eta}$ if $\alpha \leq \log(\frac{2\sqrt{2}}{\eta}) $. This means that starting from this initialization the solution that GF finds will have sharpness less than $\frac{2\sqrt{2}}{\eta}$. For this initialization set, gradient descent with step-size $\eta$ will always balance in the next iterate. 

    %Note that at initialization $(L-1)$ iterates are equal. So, throughout the GD trajectory they are always equal, i.e, $\sigma_{L-1}(t) = \sigma_{L-2}(t)= \ldots= \sigma_{1}(t) \eqqcolon y$ for all t. Further by lemma \ref{balance}, $\sigma^{2}_{L}(t)-\sigma^{2}_{l}(t)=-\alpha^2$ for all $t$. 
    Since the singular values $\sigma_\ell$ for all $\ell \in [L-1]$ are initialized to $\alpha$, note that they all follow the same dynamics. Then, let us define 
    \begin{align*}
        y \coloneqq \sigma_1 = \ldots = \sigma_{L-1} \quad \text{and} \quad x \coloneqq \sigma_L.
    \end{align*}
    The gradient flow (GF) solution is the intersection between
    \begin{align*}
       xy^{L-1}=\sigma_{\star} \quad \text{and} \quad x^{2} - y^{2}= -\alpha^2,
    \end{align*}
    where the first condition comes from convergence and the second comes from the conservation flow law of GF which we prove in Lemma \ref{gf-unbalanced}.  
    Then, if we can find a solution at the intersection such that
    \begin{align}
    \label{constraint-set}
        (\hat{x}(\alpha),\hat{y}(\alpha)) = \begin{cases}
            xy^{L-1}=\sigma_{\star} \\
            x^{2} - y^{2}= -\alpha^2,
        \end{cases} 
    \end{align}
    solely in terms of $\alpha$, then we can plug in $(\hat{x}(\alpha),\hat{y}(\alpha))$ into the GFS\footnote{Note that throughout the proof $(\hat{x}(\alpha),\hat{y}(\alpha))$ denotes the gradient flow solution as function of $\alpha$. It does
not refer to the GF trajectory.} from Lemma \ref{1d-sharp}
    \begin{align*}
        \psi(\hat{x}(\alpha),\hat{y}(\alpha)) 
 = \psi(\sigma) = \sum_{i=1}^{L} \frac{\sigma^2_{\star}}{\sigma^2_{i}} = \sigma^2_{\star}\left(\frac{1}{\hat{x}(\alpha)^2} + \frac{L-1}{\hat{y}(\alpha)^2}\right) < \frac{2\sqrt{2}}{\eta}
    \end{align*}
and solve to find an upper bound on $\alpha$. The strict inequality ensures that we can find a $c$ in $c \in [0,1)$ such that $ \psi(\alpha) <\frac{2\sqrt{1+c}}{\eta} $. However, the intersection $(\hat{x}(\alpha),\hat{y}(\alpha))$ is a $2L$-th order polynomial in $\hat{y}(\alpha)$ which does not have a straightforward closed-form solution solely in terms of $\alpha$. Hence, we aim to find the upper bound on $\alpha$ by using a calculus of variations. By plugging in $x$, the solution $\hat{y}(\alpha)$ satisfies
\begin{align*}
    y^{2L} - \alpha^2 y^{2L-2} =  \sigma^2_{\star}.
\end{align*}
Then, by differentiating the relation with respect to $\alpha$, we obtain the following variational relation:
\begin{align}
   & 2L y^{2L-1}dy - \alpha^2 2 (L-1) y^{2L-3}dy - 2\alpha y^{2L-2} d\alpha = 0 \notag  \\ 
   & \implies y^{2L-3} (y^2 L - \alpha^2 (L-1)) dy = \alpha y^{2(L-1)} d\alpha \notag \\ \
   & \implies dy = \frac{y \alpha}{ (y^2 L -\alpha^2 (L-1))} d\alpha,
\end{align}
where we used the fact that $y^{2L-2} > 0 $ from Lemma~\ref{GFS-1} in the last line. Then, in order to have $\frac{dy}{d \alpha}>0$, we need $y > \sqrt{\frac{L-1}{L}} \alpha$, which is always true since $y > \alpha$ from initialization. Then, since $\alpha \to 0$, 
$\lim_{\alpha \rightarrow 0} \hat{y}(\alpha) = \sigma^{\frac{1}{L}}_{\star}$ and $\lim_{\alpha \rightarrow 0} \hat{x}(\alpha) = \sigma^{\frac{1}{L}}_{\star}$, as it corresponds to exact balancing. Hence, $\frac{dy}{d \alpha}>0$ implies as $\alpha$ increases from $0$, $\hat{y}(\alpha)$ would increase from $\sigma^{\frac{1}{L}}_{\star}$ and $\hat{y}(\alpha)$ is an increasing function of $\alpha$.
%Note that $\lim_{\alpha \rightarrow 0} \hat{y}(\alpha) = \sigma^{\frac{1}{L}}_{\star}$ and $\lim_{\alpha \rightarrow 0} \hat{x}(\alpha) = \sigma^{\frac{1}{L}}_{\star}$ , since $\alpha \rightarrow 0$ correspond to exact balancing. 
%So, $\frac{dy}{d \alpha}>0$ implies $y > \sqrt{\frac{L-1}{L}} \alpha$, which is always true since $y> \alpha$ as it is the intialization. So, $\frac{dy}{d \alpha}>0$, implies as $\alpha$ increases from $0$, $\hat{y}(\alpha)$ would increase from $\sigma^{\frac{1}{L}}_{\star}$. 
%So, $\hat{y}(\alpha)$ is an increasing function of $\alpha$. 
Similarly, the intersection at the global minima would satisfy the following relation for $ \hat{x}(\alpha)$:
\begin{align}
    & x^{(2+ \frac{2}{L-1})} + x^{\frac{2}{(L-1)}} \alpha^2 = \sigma^{\frac{2}{L-1}}_{\star} \notag \\ 
    & \implies \left(2+\frac{2}{L-1} \right) x^{1+ \frac{2}{L-1}}dx + \left(\frac{2}{L-1}\right)\alpha^2 x^{\frac{2}{L-1}-1} dx  + x^{\frac{2}{L-1}}(2\alpha d\alpha) = 0 \notag \\
    & \implies dx = \frac{-\alpha}{\left(\frac{L}{L-1}x + \frac{\alpha^2}{L-1}\frac{1}{x}\right)} d\alpha.
\end{align}

Note that since $x>0$, we will always have $\frac{dx}{d\alpha}<0$. Then, since $\lim_{\alpha \rightarrow 0} \hat{x}(\alpha) = \sigma^{\frac{1}{L}}_{\star}$,  $\frac{dx}{d\alpha}<0$ implies that as $\alpha$ increases, $\hat{x}(\alpha)$ would decrease from $\sigma^{\frac{1}{L}}_{\star}$.
Now, with the variational relations  $\frac{d\hat{x}}{d\alpha}$ and  $\frac{d \hat{y}}{d\alpha}$ in place, we aim to find $\frac{d \psi}{d \alpha}$:
\begin{align*}
   &  \Psi(\alpha) \coloneqq  \psi(\hat{x}(\alpha),\hat{y}(\alpha)) =  \sigma_{\star}^2 \left(\frac{1}{\hat{x}(\alpha)^2} + \frac{L-1}{\hat{y}(\alpha)^2} \right)  \\ 
   & \implies d \Psi = \sigma^2_{\star} \left(-\frac{2 }{\hat{x}^3} d\hat{x} - \frac{2 (L-1) }{\hat{y}^3}d\hat{y} \right)  \\ 
   & \implies d \Psi = \frac{1}{\hat{x}^3} \left[\frac{2 \alpha \sigma^2_{\star}}{\left(\frac{L}{L-1}\right)\hat{x} +\left(\frac{\alpha^2}{L-1}\right)\frac{1}{\hat{x}} } \right] d\alpha 
   - \left[\frac{(L-1)}{\hat{y}^3} \frac{2 \alpha \hat{y} \sigma^2_{\star}}{(\hat{y}^2L - \alpha^2 (L-1))} \right] d \alpha \\
   & \implies d \Psi = \left [\frac{1}{\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 } - \frac{1}{(\hat{y}^4 - \alpha^2 \hat{y}^2\frac{(L-1)}{L})} \right] 2 \frac{(L-1)\sigma^2_{\star}}{L} \alpha d \alpha \\
   & \implies d \Psi  = G(\alpha) d\alpha,
\end{align*}
where we defined $G(\alpha) \coloneqq \left [\frac{1}{\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 } - \frac{1}{(\hat{y}^4 - \alpha^2 \hat{y}^2\frac{(L-1)}{L})} \right] 2 \frac{(L-1)\sigma^2_{\star}}{L} \alpha$ and used the notation $\hat{x} = \hat{x}(\alpha)$ and $\hat{y} = \hat{y}(\alpha)$ for simplicity. \\

\noindent Next, we will show the three following steps:
\begin{enumerate}[label=(\roman*)]
    \item Prove that $G(\alpha) > 0$ for all $\alpha > 0$ to show that the sharpness $\Psi(\alpha)$ is an increasing function of $\alpha$.
    \item Solve the differential $d\Psi$ to find the relationship between $d\Psi$ and $\Psi(\alpha)$.
    \item Find an upper bound on a part of $\frac{d\Psi}{\Psi(\alpha)}$ found in Step 2.
\end{enumerate}
\  These series of steps comes from the fact that the intersection does not have a closed-form solution. The goal is to find a function in which we can upper bound $\frac{d\Psi}{\Psi(\alpha)}$ with a function with a closed-form solution to find a bound on $\alpha$ such that the sharpness $\psi(\alpha) < \frac{2\sqrt{2}}{\eta}$. \\

% \ag{There are a few steps to go from here. }

% \begin{enumerate}
%     \item Show that  $ G(\hat{x}(\alpha),\hat{y}(\alpha),\alpha)$ is always positive, meaning sharpness increases as $\alpha$ increases. This is true since we already proved balanced iterates have the least sharpness. 
%     \item Find an upper bound on $ G(\hat{x}(\alpha),\hat{y}(\alpha),\alpha)$ (say $H(\alpha)$), so that the PDE $d \Psi  = H(\alpha) d\alpha$ has a known closed form solution (since we know initial condition). 
%     \item By continuity, the solution of PDE $d \Psi  = H(\alpha) d\alpha$ will always be above the curve $d \Psi  = G(\hat{x}(\alpha),\hat{y}(\alpha),\alpha) d\alpha$. 
%     \item Since both are increasing function and $d \Psi  = H(\alpha) d\alpha$ is an upper bound on $d \Psi  = G(\hat{x}(\alpha),\hat{y}(\alpha),\alpha) d\alpha$, the initialization condition found by solving $d \Psi  = H(\alpha) d\alpha$ would be also true for the original PDE. 
% \end{enumerate}

\noindent \textbf{Step 1}: \underline{Prove $ G(\alpha)>0 $ to show sharpness $ \Psi(\alpha)$ is an increasing function of $\alpha$}. \\

\noindent There have been several lines of work such as those by \cite{kreisler2023gradient} and \cite{marion2024deep} which showed that GD would decrease the sharpness of the solution. The more balanced the solution (which corresponds to smaller $\alpha$), the smaller the sharpness. We prove this again here:
\begin{align*}
    G(\alpha) > 0 
    & \implies \hat{y}^4 - \alpha^2 \hat{y}^2\frac{(L-1)}{L} > \hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 \\
    & \implies (\hat{y}^4  - \hat{x}^4) > \alpha^2 \left(\frac{1}{L}\hat{x}^2 + \hat{y}^2\frac{(L-1)}{L}\right) \\
    &  \implies \underbrace{(\hat{y}^2  - \hat{x}^2)}_{=\alpha^2}(\hat{y}^2  + \hat{x}^2) > \alpha^2 \left(\frac{1}{L}\hat{x}^2 + \hat{y}^2\frac{(L-1)}{L}\right) \\
    & \implies  \hat{x}^2 (1- \frac{1}{L}) + \hat{y}^2\frac{1}{L} >0,
\end{align*}
where the last inequality always holds since we have $L>2$. This proves that $\Psi$ is an increasing function of $\alpha$ since for  $d \Psi  = G(\alpha) d\alpha$, as it always holds that $ G(\alpha) >0$ for any $L>2$ and $\alpha > 0$. \\

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/objective_vs_a_L_variation.png}
    \caption{Sharpness $\Psi(\alpha)$ as a function of initialization $\alpha$. The theoretical approximation bound $ \Psi = \Psi_{0} \exp\left( \frac{  L^2\cdot 2^{\frac{2(L-1)}{L}}}{2\sigma_{\star}^{\frac{4}{L}}} \alpha^4 \right)$ serves as proxy upper bound to this increasing function.}
    \label{fig:sharp-init}
\end{figure}


\noindent \textbf{Step 2}: \underline{Solve the differential to establish the relation between $\Psi(\alpha)$ and $\alpha$}. \\

\noindent Rewriting the expression for sharpness and establishing an equation we have
\begin{align}
    \Psi(\alpha) = \sigma_{\star}^2 \left(\frac{1}{\hat{x}(\alpha)^2} + \frac{L-1}{\hat{y}(\alpha)^2} \right) 
    & \implies  \Psi(\alpha) = \sigma_{\star}^2 \left(\frac{\hat{y}^2 + (L-1)\hat{x}^2}{\hat{x}^2\hat{y}^2} \right)\\ 
    & \implies \frac{\hat{y}^2}{L} + \left(1-\frac{1}{L} \right) \hat{x}^2 = \frac{\Psi(\alpha) \hat{x}^2 \hat{y}^2}{L \sigma_{\star}^2}. \label{mid-eq}
\end{align}
Now, we revisit the original differential between $\Psi(\alpha)$ and $\alpha$:
\begin{align}
   &  d \Psi  = \left [\frac{1}{(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 )} - \frac{1}{(\hat{y}^4 - \alpha^2 \hat{y}^2\frac{(L-1)}{L})} \right] 2 \frac{(L-1)\sigma^2_{\star}}{L} \alpha d\alpha \notag \\
   & \implies d \Psi = \frac{\hat{y}^4 - \hat{x}^4 -\alpha^2 (\frac{\hat{x}^2}{L} + (1-\frac{1}{L}\hat{y}^2 )) }{(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 )(\hat{y}^4 - \alpha^2 \hat{y}^2\frac{(L-1)}{L}) } 2 \left(1-\frac{1}{L} \right) \sigma^2_{\star} \alpha d \alpha \notag \\
   &  \implies   d \Psi =   \frac{\alpha^2  \left( \frac{\hat{y}^2}{L} + (1-\frac{1}{L})\hat{x}^2\right) }{(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 )(\hat{y}^4 - \alpha^2 \hat{y}^2\frac{(L-1)}{L}) } 2 \left(1-\frac{1}{L}\right) \sigma^2_{\star} \alpha d \alpha    \label{mid-eq2}   
\end{align}
Using the expression for $  \frac{\hat{y}^2}{L} + (1-\frac{1}{L})\hat{x}^2$ derived in Equation~(\ref{mid-eq}) and plugging it into Equation~(\ref{mid-eq2}), we obtain
\begin{align}
    &    d \Psi = \frac{\alpha^2 \left( \frac{\Psi(\alpha) \hat{x}^2 \hat{y}^2}{L \sigma_{\star}^2} \right)}{(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2)(\hat{y}^4 - \alpha^2 \hat{y}^2 \frac{(L-1)}{L})} 2 \left(1-\frac{1}{L}\right) \sigma_{\star}^2 \alpha \, d \alpha \notag \\
    & \implies \frac{d \Psi }{\Psi(\alpha)} =   \frac{2}{(\hat{x}^2 + \frac{\alpha^2}{L})(\hat{y}^2 - \alpha^2\frac{(L-1)}{L})}    \left(\frac{1}{L} - \frac{1}{L^2} \right)\alpha^{3} d \alpha \notag  \\ 
    & \implies \frac{d \Psi }{\Psi(\alpha)} =   \frac{2}{(\hat{x}^2 + \frac{\alpha^2}{L} )^2 }    \left(\frac{1}{L} - \frac{1}{L^2} \right)\alpha^{3} d \alpha  \\
    & \implies \frac{d \Psi }{\Psi(\alpha)} = P(\alpha) \alpha^3 d\alpha, \\
\end{align}
where we have defined $P(\alpha) \coloneqq \frac{2}{(\hat{x}^2 + \frac{\alpha^2}{L} )^2 }    \left(\frac{1}{L} - \frac{1}{L^2} \right)$.

\noindent Solving the differential $ \frac{d \Psi }{\Psi(\alpha)} = P(\alpha) \alpha^{3}d\alpha$ in exact closed-form is difficult since $\hat{x}$ is also an function of $\alpha$. However, in Step 1, we proved that $\Psi(\alpha)$ is an increasing function of $\alpha$, and so instead of solving exactly, we can find a differential equation $ \frac{d \Psi }{\Psi(\alpha)} = F(\alpha) \alpha^{3} d\alpha$ with $F(\alpha)> P(\alpha) $ such that $F(\alpha)$ is more increasing, and use it to solve the PDE instead.
Though, note that the initialization limit on $\alpha$ that would be found after solving the surrogate PDE $ \frac{d \Psi }{\Psi(\alpha)} = F(\alpha)\alpha^{3} d\alpha$ would be smaller than the $\alpha$ if it was found using the original PDE $ \frac{d \Psi }{\Psi(\alpha)} = P(\alpha)\alpha^{3} d\alpha$. \\ 

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{figures/objective_vs_a_variation.png}
    \caption{$P(\alpha)$ for GF has a unique maxima at $\alpha = \frac{\sigma_{*}^{\frac{1}{L}}}{\left(\frac{1}{\sqrt{L(L-2)}} \left(1+\frac{1}{L(L-2)}\right)^{\frac{L-1}{2}}\right)^{\frac{1}{L}}}$ for $L>2$.}
    \label{fig:P_obj}
\end{figure}


\noindent \textbf{Step 3}: \underline{Finding an upper bound function and solving for initialization}. \\
\noindent Note that the original coefficient in $ \frac{d \Psi }{\Psi(\alpha)} = P(\alpha)\alpha^{3} d\alpha$ is of the form
\begin{align}
    P(\alpha)  = \frac{2}{(\hat{x}^2 + \frac{\alpha^2}{L})^2 }    \left(\frac{1}{L} - \frac{1}{L^2}\right) \label{ref-dif}
\end{align}
Let us consider the two corner cases for $\alpha$. We showed before that $\lim_{\alpha \rightarrow 0} \hat{x}(\alpha) = \sigma^{\frac{1}{L}}_{\star}$, so \[
\lim_{\alpha \rightarrow 0} P(\alpha) = \frac{2 \left(\frac{1}{L} - \frac{1}{L^2}\right)}{\sigma_{\star}^{\frac{4}{L}} }
\]
As $\alpha \rightarrow \infty $, we have $\lim_{\alpha \rightarrow \infty} P(\alpha) \rightarrow 0 $ since $\lim_{\alpha \rightarrow \infty} \hat{x} = 0$. Furthermore, we have 

\begin{align*}
 &   P'(\alpha) = \frac{-4}{(\hat{x}^2 + \frac{\alpha^2}{L})^3 }    \left(\frac{1}{L} - \frac{1}{L^2}\right)(2\hat{x} \frac{d \hat{x}}{d \alpha} + \frac{2 \alpha}{L} ) \\
 & = \frac{-4 \left(\frac{1}{L} - \frac{1}{L^2}\right)}{(\hat{x}^2 + \frac{\alpha^2}{L})^3 }    \left(2\hat{x} ( \frac{-\alpha}{\left(\frac{L}{L-1}\hat{x} + \frac{\alpha^2}{L-1}\frac{1}{\hat{x}}\right)}) + \frac{2 \alpha}{L} \right ) \\
 & = \frac{8 \alpha \left(\frac{1}{L} - \frac{1}{L^2}\right)}{(\hat{x}^2 + \frac{\alpha^2}{L})^3 }  \left( \frac{L-1}{L+ \frac{\alpha^2}{\hat{x}^2}} - \frac{1}{L}\right)
\end{align*}
For $L=2$, we always have $P'(\alpha)<0$. Hence, choosing $F(\alpha) = \lim_{\alpha \rightarrow 0} P(\alpha) = \frac{2 \left(\frac{1}{L} - \frac{1}{L^2}\right)}{\sigma_{\star}^{\frac{4}{L}} } $, will serve as the correct upper bound. 

Then, let us consider $L>2$. We can see that $\alpha$ at which $\hat{x}(\alpha)= \frac{\alpha}{\sqrt{L(L-2)}} $ is the critical point of $P(\alpha)$. Further, we note that when $\hat{x}(\alpha)< \frac{\alpha}{\sqrt{L(L-2)}} $, $P'(\alpha)<0 $ meaning $P(\alpha)$ is decreasing. Since $\hat{x}(\alpha)$ is itself decreasing in $\alpha$, this states there for any $\alpha>\alpha_{crit}$, $P(\alpha)$ is decreasing. $\alpha_{crit}$ is the solution of $\hat{x}(\alpha)= \frac{\alpha}{\sqrt{L(L-2)}}$. 


For any $\alpha<\alpha_{crit}$, $ P'(\alpha) > 0$, so $P(\alpha)$ is increasing. So, $P(\alpha_{crit})$ corresponds to the maximum of $P$ in $\alpha$. Choosing $F(\alpha) = P(\alpha_{crit}) $, a constant allows us to find an upper bound function for the function in Equation~(\ref{ref-dif}). Furthermore, note that since $\frac{d \hat{x}}{d \alpha} <0$ and $\hat{x}>0$, $\alpha>0$, there must be only one critical point of $P(\alpha)$ which is at $\alpha_{crit}$. 

Hence, we have $\hat{x}(\alpha_{crit}) = \frac{\alpha_{crit}}{\sqrt{L(L-2)}}$. From Equation~(\ref{constraint-set}), we also get $\hat{y} = \alpha_{crit} \sqrt{1+\frac{1}{L(L-2)}}$ and 
\begin{align*}
    & \left(\frac{\alpha_{\text{crit}}}{\sqrt{L(L-2)}}\right) \left( \alpha_{\text{crit}} \sqrt{1+\frac{1}{L(L-2)}}\right)^{L-1} = \sigma_{*} \\ 
    & \implies \alpha_{\text{crit}}  = \frac{\sigma_{*}^{\frac{1}{L}}}{\left(\frac{1}{\sqrt{L(L-2)}} \left(1+\frac{1}{L(L-2)}\right)^{\frac{L-1}{2}}\right)^{\frac{1}{L}}} \\
\end{align*}

Using $\alpha_{\text{crit}}$, we obtain the maximum of $P(\alpha)$ to be 

\begin{align*}
& P(\alpha_{crit}) =  \frac{2}{(\hat{x}(\alpha_{crit})^2 + \frac{\alpha_{crit}^2}{L})^2 }    \left(\frac{1}{L} - \frac{1}{L^2}\right) \\
& = \frac{2}{ \left(\alpha^2_{crit} (\frac{1}{L(L-2)} +\frac{1}{L}) \right)^2 }    \left(\frac{1}{L} - \frac{1}{L^2}\right)  \\
& = \frac{2}{\sigma^{\frac{4}{L}}_{*}} g(L)
\end{align*}
where $g(L)= \frac{(\frac{1}{L} - \frac{1}{L^2}) [ \frac{1}{\sqrt{L(L-2)}} \left(1+\frac{1}{L(L-2)}\right)^{\frac{L-1}{2}}]^{\frac{4}{L}}}{(\frac{1}{L(L-2)} +\frac{1}{L})^2} $.

Now, chosing $F(\alpha)= P(\alpha_{crit})$, we integrate the upper bound function as 

\begin{align*}
    & \int \frac{d \Psi}{\Psi} = \frac{2 g(L)}{\sigma_{\star}^{\frac{4}{L}}} \int \alpha^3 \, d\alpha \\
    & \implies \mathrm{ln}(\frac{\Psi}{\Psi_{0}}) = \frac{ g(L)}{2\sigma_{\star}^{\frac{4}{L}}} (\alpha^4) \\ 
    & \implies \Psi = \Psi_{0} \exp\left( \frac{ g(L)}{2\sigma_{\star}^{\frac{4}{L}}} \alpha^4 \right)\\
\end{align*}

where $\Psi_{0}= \lim_{\alpha \rightarrow 0}\Psi = L\sigma^{2-\frac{2}{L}}_{\star} $. 
We verify this upper bound empirically from Figure \ref{fig:sharp-init}, where we see a near exponential growth in sharpness as function of $\alpha$. 

Now, note that the function $\Psi = \Psi_{0} \exp\left( \frac{ g(L)}{2\sigma_{\star}^{\frac{4}{L}}} \alpha^4 \right)$ acts an upper bound to the original sharpness function of $\alpha$ and both are increasing in $\alpha$ (Step 1). So, solving for an initialization $\alpha$-upper limit with  $\Psi = \frac{2\sqrt{2}}{\eta}$ would mean that the original sharpness with this initialization would be less than $\frac{2\sqrt{1+c}}{\eta}$ for some $0<c<1$.  Hence, $\alpha$ is restricted to
\begin{align*}
    \alpha < \left( \ln\left( \frac{\frac{2\sqrt{2}}{\eta}}{L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{2 \sigma_{\star}^{\frac{4}{L}}}{g(L)} \right)^{\frac{1}{4}}.
\end{align*}
We can simplify the bound further by finding an upper bound on $g(L)$ \footnote{This includes the case for $L=2$ since $(\frac{1}{L}- \frac{1}{L^2}) < L^2\cdot 2^{\frac{2(L-1)}{L}}$ for $L=2$.}:
\begin{align*}
    g(L) \leq \frac{\left(\left(1 + \frac{1}{L(L-2)}\right)^{\frac{L-1}{2}}\right)^{\frac{4}{L}}}{\left(\frac{1}{L(L-2)}+\frac{1}{L}\right)^2} \leq L^2 \cdot \left(\left(1 + \frac{1}{L(L-2)}\right)^{\frac{L-1}{2}}\right)^{\frac{4}{L}} \leq L^2\cdot 2^{\frac{2(L-1)}{L}}.
\end{align*}
Then, we get obtain a lower bound on $\alpha$:
\begin{align*}
    \alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{2 \sigma_{\star}^{\frac{4}{L}}}{L^2 \cdot 2^{\frac{2(L-1)}{L}}} \right)^{\frac{1}{4}} = \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{ \sigma_{\star}^{\frac{4}{L}}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{\frac{1}{4}} 
\end{align*}
 Hence, as long as $\alpha$ satisfies this upper bound, we will have balancing. This completes the proof.
 % And as $\alpha \rightarrow \infty$, we will have $\hat{x}(\alpha) \rightarrow 0$, since we proved that  $\hat{x}(\alpha)$ is a decreasing function of $\alpha$ but the product $\hat{x}^2(\alpha)\alpha^2$ is finite, so $\lim_{\alpha \rightarrow \infty}P(\alpha) < \infty$.  So, we chose $F(\alpha) = \sup_{0< \alpha<\infty} P(\alpha)$ and use it to solve the PDE.

% We find the critical points of $ g(\alpha) = (\hat{x}(\alpha)^2 + \frac{\alpha^2}{L} )^2$ to analyze the behavior of $P(\alpha)$. 

% \begin{align*}
%    & \frac{\partial g}{\partial \alpha} = 2 (\hat{x}^2 + \frac{\alpha^2}{L} )(2\hat{x} \frac{\partial x}{\partial \alpha} +\frac{2 \alpha }{L} )  \\
%    & = 2 (\hat{x}^2 + \frac{\alpha^2}{L} )(2\hat{x} \frac{-\alpha}{[\frac{L}{L-1}\hat{x} + \frac{\alpha^2}{L-1}\frac{1}{\hat{x}}]} +\frac{2 \alpha }{L} ) \\
%    & = -4 \alpha  (\hat{x}^2 + \frac{\alpha^2}{L} )(\frac{L-1}{L+\frac{\alpha^2}{\hat{x}^2}} -\frac{1}{L})
% \end{align*}


% The critical points of $\frac{\partial g}{\partial \alpha}$ are solved as:
% % Using the expression for $\frac{\partial \hat{x}}{\partial \alpha}$ and solving for $\hat{x}$, we obtain
% \begin{align*}
%     \hat{x}^2 = \frac{\alpha^2}{L(L-1)-L}
% \end{align*} which gives $P(\alpha)$ at critical point to be :

% \begin{align*}
%     P(\alpha) = \frac{2}{\alpha^4 (\frac{1}{L(L-1)-L} + \frac{1}{L} )^2}\left(\frac{1}{L} - \frac{1}{L^2}\right)
% \end{align*}
% We note that this expression is larger than both $\lim_{\alpha \rightarrow \infty} P(\alpha)$ and $\lim_{\alpha \rightarrow 0}  P(\alpha)$, since we have $\sigma_{\star} > \alpha^{L} (\frac{1}{L(L-1)-L} + \frac{1}{L})^2 $.

% So, we will use $F(\alpha) = \frac{2}{\alpha^4 (\frac{1}{L(L-1)-L} + \frac{1}{L} )^2}\left(\frac{1}{L} - \frac{1}{L^2}\right) $ and integrate both sides as follows:

% \begin{align*}
%   &  \int \frac{d \Psi}{\Psi(\alpha)} = \frac{2}{ \left( \frac{1}{L(L-1)-1} + \frac{1}{L} \right)^2}\left(\frac{1}{L} - \frac{1}{L^2}\right) \int  \frac{d \alpha}{\alpha}  \\
%   & 
% \end{align*}

\end{proof}

\begin{manuallemmainner}
\label{gf-unbalanced}
    Consider the minimizing the loss         \begin{align*} 
        \mathcal{L}\left(\{\sigma_\ell\}_{\ell=1}^L\right)
 = \frac{1}{2} \left( \prod_{\ell=1}^L \sigma_{\ell} - \sigma_{\star} \right)^2,
    \end{align*}  
    using gradient flow. Then, the balancedness between two singular values defined by $\sigma^2_{ \ell} (t) - \sigma^2_{m} (t)$ for all $m, \ell \in [L]$ is constant for all $t$.
\end{manuallemmainner}

\begin{proof}
Notice that the result holds specifically for gradient flow and not descent. The dynamics of each scalar factor for gradient flow can be written as
    \begin{align*}
        \dot{\sigma}_{\ell}(t) = - \left(\prod_{\ell=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right)\cdot \prod_{i\neq \ell}^L \sigma_{i}(t)
    \end{align*}
Then, the time derivative of balancing is given as
\begin{align*}
  & \frac{\partial}{\partial t} (\sigma^2_{ \ell} (t) - \sigma^2_{m} (t)) = \sigma_{ \ell} (t)\dot{\sigma}_{\ell}(t)  - \sigma_{m} (t)\dot{\sigma}_{m}(t)  \\
  & = - \sigma_{ \ell} (t)\left(\prod_{\ell=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right)\cdot \prod_{i\neq \ell}^L \sigma_{i}(t) + \sigma_{m} (t)\left(\prod_{m=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right)\cdot \prod_{j\neq m}^L \sigma_{j}(t). \\
  & = 0.
\end{align*}
Hence, the quantity $\sigma^2_{ \ell} (t) - \sigma^2_{m} (t) $ remains constant for all time $t$, hence preserving unbalancedness. 
\end{proof}

\begin{manuallemmainner}
\label{1d-sharp}
    Consider the scalar loss     \begin{align*} 
        \mathcal{L}(\{\sigma_i\}_{i=1}^d)
 = \frac{1}{2} \left( \prod_{i=1}^L \sigma_{i} - \sigma_{\star} \right)^2,
    \end{align*}
    The sharpness at the global minima is given as $\| \nabla^2 \mathcal{L} \|_{2} = \sum_{i=1}^{L} \frac{\sigma^2_{\star}}{\sigma^2_{i}}$.
\end{manuallemmainner}

\begin{proof}
The gradient is given by
\begin{align*}
    \nabla_{\sigma_{i}}  \mathcal{L} = \left(\prod_{\ell=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right) \prod_{j\neq i}^L \sigma_{j}(t).
\end{align*}
Then, 
\begin{align*}
     \nabla_{\sigma_{j}}  \nabla_{\sigma_{i}}  \mathcal{L} =  \prod_{\ell\neq i}^L \sigma_{\ell}(t)  \prod_{\ell\neq j}^L \sigma_{\ell}(t) + \left(\prod_{\ell=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right)  \prod_{\ell\neq j, \ell \neq i}^L \sigma_{\ell}(t)
\end{align*}
 Let $\pi(t)=  \prod_{i=1}^L \sigma_{i}(t)$. Then, at the global minima, we have
\begin{align*}
     \nabla_{\sigma_{j}}  \nabla_{\sigma_{i}}  \mathcal{L} =  \frac{\pi^2}{\sigma_{i} \sigma_{j}} = \frac{\sigma_{\star}^2}{\sigma_{i} \sigma_{j}}
\end{align*}
Thus, the sharpness of the largest eigenvalue is given as $ \| \nabla^2 \mathcal{L} \|_{2} = \sum_{i=1}^{L} \frac{\sigma^2_{\star}}{\sigma^2_{i}}$. 
\end{proof}

\begin{manuallemmainner}
\label{lemma:flattest}
Consider the loss \begin{align*} 
        \mathcal{L}\left(\{\sigma_i\}_{i=1}^L\right)
 = \frac{1}{2} \left( \prod_{i=1}^L \sigma_{i} - \sigma_{\star} \right)^2.
    \end{align*} 
The balanced minimum $\sigma_i = \sigma_\star^{1/L}$ has the smallest sharpness amongst all global minima with a value of $\|\nabla^2 \mathcal{L}\|_2 = L\sigma_\star^{2-2/L}$.
\end{manuallemmainner}
\begin{proof}
    By Lemma~\ref{1d-sharp}, recall that the sharpness at the global minima is given in the form
    \begin{align*}
        \|\nabla^2 \mathcal{L}\|_2 = \sum_{i=1}^L \frac{\sigma_\star^2}{\sigma_i^2}.
    \end{align*}
    To show that the balanced minimum is the flattest (i.e., it has the smallest sharpness amongst all global minima), we will show that KKT stationarity condition of the constrained objective
    \begin{align*}
        \underset{\{\sigma_i\}_{i=1}^L }{\mathrm{min}} \,  \sum_{i=1}^L \frac{\sigma_\star^2}{\sigma_i^2} \quad\,\mathrm{s.t.} \,\, \prod_{i=1}^L \sigma_i = \sigma_\star,
    \end{align*}
    are only met at the balanced minimum, 
    which gives us the sharpness value $\| \nabla^2 \mathcal{L} \|_{2} = L\sigma_\star^{2-2/L}$.
    The Lagrangian is given by
    \begin{align*}
        L(\sigma_1, \ldots, \sigma_L, \mu) = \sum_{i=1}^L \frac{\sigma_\star^2}{\sigma_i^2} + \mu\left( \prod_{i=1}^L \sigma_i - \sigma_\star \right).
    \end{align*}
Then, the stationary point conditions of the Langrangian is given by 
    \begin{align}
    \label{eqn:station1}
        \frac{\partial L}{\partial \sigma_i} &= -\frac{2\sigma_\star^2}{\sigma_i^3} + \mu \prod_{j\neq i} \sigma_j = 0, \\
    \label{eqn:station2}
    \frac{\partial L}{\partial \mu} &= \prod_{i=1}^L \sigma_i - \sigma_\star = 0.
    \end{align}
    From Equation~(\ref{eqn:station1}), the solution of the stationary point gives
    \begin{align*}
        \frac{2\sigma_\star^2}{\sigma_i^3} = \mu \prod_{j\neq i} \sigma_j \implies \mu =  \frac{2\sigma_\star^2}{\sigma_i^3 \prod_{j\neq i} \sigma_j} = \frac{2\sigma_\star^2}{\sigma_i^2 \sigma_\star} = \frac{2\sigma_\star}{\sigma_i^2}.
    \end{align*}
    This also indicates that at the stationary point, $\sigma_{i} = \sqrt{\frac{2\sigma_\star}{\mu}}$ for all $i \in [L]$, which means that the condition is \emph{only} satisfied at the balanced minimum, i.e, $\sigma_{i} = \sigma_\star^{1/L}$. Furthermore, notice that
    \begin{align*}
        \nabla^2 f(\sigma_i) = 6 \sigma_\star^2 \cdot \diag\left(\frac{1}{\sigma^4_{i}}\right) \succ \mathbf{0},
    \end{align*}
    where $f(\sigma_i) = \sum_{i=1}^L \frac{\sigma_\star^2}{\sigma_i^2}$, indicating that $f$ only has a minimum. Notice that Equation~(\ref{eqn:station2}) holds immediately. Thus, the balanced minimum has the smallest shaprness (flattest), which plugging into $f$ gives a sharpness of $\|\nabla^2 \mathcal{L}\|_2 = L\sigma_\star^{2-2/L}$. 

    
    % Then, plugging in the balanced minimum and $\mu^*$ into Equation~(\ref{eqn:station1}), we have
    % \begin{align*}
    %     -\frac{2\sigma_\star^2}{\sigma_\star^{3/L}} + \frac{2\sigma_\star}{\sigma_\star^{2/L}} \cdot \left(\sigma_\star^{1/L}\right)^{L-1} = -2\sigma_\star^{\frac{2L-3}{L}} + 2\sigma_\star^{\frac{2L-3}{L}} = 0.
    % \end{align*}
    % Notice that Equation~(\ref{eqn:station2}) holds immediately. Thus, the balanced minimum $\sigma_i = \sigma_\star^{1/L}$ satisfies both the stationary point conditions, which completes the proof.
\end{proof}








\begin{comment}

As  $\mbf{s}^\star \in \mbb{R}^{d}$ is a $r$-sparse vector, we denote the support set $I$ as the indices set where  $\mbf{s}^\star$ is non-zero. Let $I^{C}$ denote the complement set. Let us denote each diagonal vector layer $\mbf{s}_l =  \text{vec}\left( \{ s_{l,i} \}_{i=1}^{d} \right)$.  We specify vector $\mbf{q}_{i} = \text{vec}\left( \{ s_{l,i} \}_{l=1}^{L} \right)$, which is a vector containing the $i^{th}$ index for all layers $l-1,.,L$. 

With these notation, we can equivalently write the loss of diagonal linear network as $\mathcal{L}(\mbf{s}) =  \sum_{i=1}^d L(\mbf{q}_{i})  = \frac{1}{2} \sum_{i} (s_{1,i}s_{2,i}..s_{L,i} - s_{\star,i} )^2$. 

 The loss of diagonal linear networks can be decomposed in the sum of each scalar losses on each index   , where $\mathbf{s}_{l} = \text{vec}\left( \{ s_{l,i} \}_{i=1}^{d} \right)$ and $L(\mbf{q}_{i}) = (s_{1,i}s_{2,i}..s_{L,i} - s_{\star,i} )^2  $. So the Hessian of the end to end loss can be written as:

 \begin{align*}
    \nabla^2 \mathcal{L}(\mbf{s}) = \begin{bmatrix}
        \nabla^2 \mathcal{L}(\mbf{q}_{1})& & \\
        & \ddots & \\
        & & \nabla^2 \mathcal{L}(\mbf{q}_{d})
    \end{bmatrix}
 \end{align*}

where $\nabla^2 \mathcal{L}(\mbf{q}_{i})$ denote the Hessian of the loss $L(\mbf{q}_{i}) = (s_{1,i}s_{2,i}..s_{L,i} - s_{\star,i} )^2  $ and we have that $\nabla_{(\mbf{q}_{i})} \nabla_{(\mbf{q}_{j})} \mathcal{L} = \mbf{0}$ since the loss is separable. 

Due to the balancednes ( $\alpha< \log({\frac{2\sqrt{2}}{h}})$), we will have $\mbf{q}_{i} =  s^{\frac{1}{L}}_{\star,i} \mbf{1}_{L}$ if $i \in I$ and otherwise $\mbf{q}_{i} =   \text{vec}\left( \alpha, \alpha, ..,\alpha, 0  \right)$ if $i \in I^{C}$. 

With this each diagonal block of the Hessian is calculated as follows:
$ \nabla^2 \mathcal{L}(\mbf{q}_{i}) =  s^{2(1 - \frac{1}{L})}_{\star,i} \mbf{1}_{L} \mbf{1}_{L}^T $ if $ i \in I$.




If $ i\in I^{C}$,  $ \nabla^2 \mathcal{L}(\mbf{q}_{i}) = \begin{bmatrix}
            0 & \hdots & 0 & \alpha^{2(L-1)} \\
            \vdots & \ddots & \vdots & \vdots \\
            0 & \hdots & 0 & \alpha^{2(L-1)} \\
            \alpha^{2(L-1)} & \hdots & \alpha^{2(L-1)} & \alpha^{2(L-1)}
        \end{bmatrix} $.
        
The top $r$ (corresponding to the $r$-support set) eigenvectors of $\nabla^2 \mathcal{L}(\mbf{s})$ and eigenvalue pairs are then given as 

\begin{align*}
    \Delta_1 &= \mathrm{vec}\left( \frac{1}{\sqrt{L}} \mathbf{1}_{L}, \mathbf{0}, \dots, \mathbf{0} \right), \quad \lambda_{1} = L s_{\star,1}^{2 - \frac{2}{L}} \\
    \Delta_2 &= \mathrm{vec}\left( \mathbf{0}, \frac{1}{\sqrt{L}} \mathbf{1}_{L}, \dots, \mathbf{0} \right), \quad \lambda_{2} = L s_{\star,2}^{2 - \frac{2}{L}} \\
    \quad\quad\quad&\vdots \\
    \Delta_r &= \mathrm{vec}\left( \mathbf{0}, \dots, \frac{1}{\sqrt{L}} \mathbf{1}_{L}, \dots, \mathbf{0} \right), \quad \lambda_{r} = L s_{\star,r}^{2 - \frac{2}{L}} \\
\end{align*}

Followed by eigenvector-eigenvalue pair for the support set complement where $\mathbf{e}_{r+j} $ is the eigenvector of the matrix $\begin{bmatrix}
            0 & \hdots & 0 & \alpha^{2(L-1)} \\
            \vdots & \ddots & \vdots & \vdots \\
            0 & \hdots & 0 & \alpha^{2(L-1)} \\
            \alpha^{2(L-1)} & \hdots & \alpha^{2(L-1)} & \alpha^{2(L-1)}
        \end{bmatrix} $
:
\begin{align*}
    \Delta_{r+j} &= \mathrm{vec}\left( \mathbf{0}, \dots, \mathbf{e}_{r+j}, \dots, \mathbf{0} \right) , \quad \lambda_{r+j} = \frac{-\alpha^{2(L-1)} \pm \sqrt{(4L-3)\cdot \alpha^{4(L-1)^2}}}{-2}
\end{align*}


\begin{align*}
     \Delta_{r+j} = \mathrm{vec}\left( \mathbf{0}, \dots, \mbf{e}_{r+j},\dots, \mathbf{0} \right)
\end{align*}
In each 1D eigenvector direction, we can analyze the loss and verify if it satisifies the stable condition or not. 
Infact, in each eigen-vector cross-section, the 1D loss is exactly $L(\mbf{q}_{i}) =(s_{1,i}s_{2,i}..s_{L,i} - s_{\star,i} )^2  $. Due to balancing the 1D loss boils down to a multilayer scalar loss $L(x) = (x^{L} - s_{\star,i})^2$. Using Corollary-5 in \cite{chen2023edge} or restated lemma \ref{lemma:chen-bruna} on the 1D scalar function $L(x) = (x^{L} - s_{\star,i})^2$ , we prove that this 1D loss is amenable to stable oscillation when learning rate $ \eta >\frac{2}{\lambda_{i}} $. To prove the uniqueness and existence of two period orbit fixed point for $ \eta >\frac{2}{\lambda_{i}} $, we show that the polynomial obtained by solving two step fixed point has a real root. This is the same loss we analyzed in Theorem \ref{thm:stable_oscillations}, where we showed that the oscillations are real root of the polynomial

\begin{align*}
    \sigma_{\star, 1} = \rho^L\frac{1+z^{2L-1}}{1+z^{L-1}}, \quad \text{where  } \, z \coloneqq \left(1 + \eta L(\sigma_{\star, 1} - \rho^L)\cdot \rho^{L-2} \right).
\end{align*}


$\rho_1 \in \left(0, \sigma_{\star, 1}^{1/L}\right)$ and $\rho_2 \in \left(\sigma_{\star, 1}^{1/L}, (2\sigma_{\star, 1})^{1/L}\right)$ are the two real roots of the polynomial which exists and are unique.

So, whenever the learning rate $\eta$ lies between $[\frac{2}{\lambda_{i}},\frac{2}{\lambda_{i+1}}]$, we will have oscilaltions in all the eigenvector directions 
\end{proof}
\end{comment}



\begin{comment}

\section{Auxiliary Results}


\begin{customlemma}{4}\label{lem:relationship_lemma} 
    Let $\mbf{U}, \mbf{V}, [\mbf{\sigma}_{l}]_{l=2}^L \in \mathbb{R}^{n\times n}$ be $L$ orthogonal matrices and $\mbf{H}_{i, j} \in \mathbb{R}^{n^2 \times n^2}$ be diagonal matrices. Consider the two following block matrices:
    \begin{align*}
       & \widetilde{\mbf{H}} = \begin{bmatrix}
            \mbf{H}_{1,1} & \mbf{H}_{1, 2} & \hdots &\mbf{H}_{L, 1}\\
            \mbf{H}_{2,1} & \mbf{H}_{2,2} & \hdots & \mbf{H}_{L, 2} \\
            \vdots & \vdots & \ddots & \vdots \\
            \mbf{H}_{1, L} & \mbf{H}_{2, L} & \hdots &\mbf{H}_{L, L}
        \end{bmatrix} \quad\quad\quad \\
      &  \mbf{W} = \begin{bmatrix}
                 (\mbf{\sigma}_{L} \otimes \mbf{U})\mbf{H}_{1,1}(\mbf{\sigma}_{L} \otimes \mbf{U})^{\top} & (\mbf{\sigma}_{L} \otimes \mbf{U}) \mbf{H}_{1, 2}(\mbf{\sigma}_{L-1} \otimes \mbf{\sigma}_{L})^{\top} & \hdots &  (\mbf{\sigma}_{L} \otimes \mbf{U}) \mbf{H}_{1, L}  (\mbf{V}  \otimes \mbf{\sigma}_{2})^{\top}\\
            (\mbf{\sigma}_{L-1} \otimes \mbf{\sigma}_{L}) \mbf{H}_{2,1}(\mbf{\sigma}_{L} \otimes \mbf{U})^{\top} & (\mbf{\sigma}_{L-1} \otimes \mbf{\sigma}_{L}) \mbf{H}_{2,2} (\mbf{\sigma}_{L-1} \otimes \mbf{\sigma}_{L})^{\top} & \hdots & (\mbf{\sigma}_{L-1} \otimes \mbf{\sigma}_{L}) \mbf{H}_{2, L}  (\mbf{V}  \otimes \mbf{\sigma}_{2})^{\top}\\
            \vdots & \vdots & \ddots & \vdots \\
                (\mbf{V}  \otimes \mbf{\sigma}_{2}) \mbf{H}_{L, 1}(\mbf{\sigma}_{L} \otimes \mbf{U})^{\top} &   (\mbf{V}  \otimes \mbf{\sigma}_{2}) \mbf{H}_{L, 2}(\mbf{\sigma}_{L-1} \otimes \mbf{\sigma}_{L})^{\top}  & \hdots &  (\mbf{V}  \otimes \mbf{\sigma}_{2}) \mbf{H}_{L, L} (\mbf{V}  \otimes \mbf{\sigma}_{2})^{\top}
        \end{bmatrix}.
    \end{align*}
    Then, the eigenvalues of $\widetilde{\mbf{H}} \in \mbb{R}^{n^2 L \times n^2 L}$ are the same as those of $\mbf{W} \in \mbb{R}^{n^2 L \times n^2 L}$. $\mbf{W}$ is the Hessian of the deep matrix factorization loss with SVS singular vectors and $\widetilde{\mbf{H}}$ is the Hessian computed for product of diagonal matrices.
\end{customlemma}


\begin{proof}

\ag{I will change the order of L and 1 later, it does not change the proof}.
    For sake of notation, say $\mbf{R_{L}} =  (\mbf{\sigma}_{L} \otimes \mbf{U})$, $\mbf{R_{L-1}} =  (\mbf{\sigma}_{L-1} \otimes \mbf{\sigma}_{L}) $,..., $ \mbf{R_{l}} = (\mbf{\sigma}_{l} \otimes \mbf{\sigma}_{l+1})$ and $\mbf{R}_{1} = (\mbf{V}  \otimes \mbf{\sigma}_{2})$. Then for sake of notation we rewrite $\mbf{W}$ as:

\begin{align*}
    \mbf{W} = \begin{bmatrix}
        \mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^{\top} & \mbf{R}_{L}\mbf{H}_{1, 2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L}\mbf{H}_{1, L}\mbf{R}_{1}^{\top}\\
        \mbf{R}_{L-1}\mbf{H}_{2,1}\mbf{R}_{L}^{\top} & \mbf{R}_{L-1}\mbf{H}_{2,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L-1}\mbf{H}_{2, L}\mbf{R}_{1}^{\top}\\
        \vdots & \vdots & \ddots & \vdots \\
        \mbf{R}_{1}\mbf{H}_{L,1}\mbf{R}_{L}^{\top} & \mbf{R}_{1}\mbf{H}_{L,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{1}\mbf{H}_{L,L}\mbf{R}_{1}^{\top}
    \end{bmatrix}.
\end{align*}

In lemma, we show that $\mbf{R}_{l}$ is an orthogonal matrix. With this, it suffices to show that $\mbf{W}$ and $ \widetilde{\mbf{H}}$ have the same characteristic polynomial. Let us define 

Let us define
    \begin{align*}
        \mbf{W} \coloneqq \begin{bmatrix}
            \mbf{A} & \mbf{B} \\
            \mbf{C} & \mbf{D}
        \end{bmatrix},
    \end{align*}
    where 
    \begin{alignat}{3}
        &\mbf{A} \coloneqq  \mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^{\top} \quad\quad\quad &\mbf{B} &\coloneqq \begin{bmatrix}
            \mbf{R}_{L}\mbf{H}_{1, 2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L}\mbf{H}_{1, L}\mbf{R}_{1}^{\top}
        \end{bmatrix} \\
        &\mbf{C} \coloneqq \begin{bmatrix}
            \mbf{R}_{L-1}\mbf{H}_{2,1}\mbf{R}_{L}^{\top} \\
            \vdots \\
           \mbf{R}_{1}\mbf{H}_{L,1}\mbf{R}_{L}^{\top}
        \end{bmatrix}  \quad\quad\quad
        &\mbf{D} &\coloneqq \begin{bmatrix}
           \mbf{R}_{L-1}\mbf{H}_{2,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L-1}\mbf{H}_{2, L}\mbf{R}_{1}^{\top}\\ \\
            \vdots & \ddots & \vdots \\
           \mbf{R}_{1}\mbf{H}_{L,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{1}\mbf{H}_{L,L}\mbf{R}_{1}^{\top}
        \end{bmatrix}.
    \end{alignat}
    Then, we have
    \begin{align*}
        \det(\mbf{W} - \lambda \mbf{I}) &= \det\left(\begin{bmatrix}
            \mbf{A} - \lambda\mbf{I} & \mbf{B} \\
            \mbf{C} & \mbf{D} - \lambda\mbf{I}
        \end{bmatrix}\right) \\
        &= \det(\mbf{A} - \lambda \mbf{I}) \cdot \det((\mbf{D} - \lambda \mbf{I}) - \mbf{C}(\mbf{A} - \lambda \mbf{I})^{-1}\mbf{B}),
    \end{align*}
    where the second equality is by the Schur complement. Notice that
    \begin{align*}
        (\mbf{A} - \lambda \mbf{I})^{-1} = (\mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^\top  - \lambda \mbf{I})^{-1} = (\mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^\top  - \lambda \mbf{\mbf{R}_{L}\mbf{R}_{L}}^\top)^{-1} = \mbf{R}_{L} \cdot (\mbf{H}_{1,1} - \lambda\mbf{I})^{-1} \cdot \mbf{R}_{L}^\top.
    \end{align*}
    Then, we also see that, 
    \begin{align*}
        \mbf{C}(\mbf{A} - \lambda \mbf{I})^{-1}\mbf{B} = \underbrace{\begin{bmatrix}
            \mbf{R}_{L-1} & & \\
            & \ddots & \\
            & & \mbf{R}_{1}
        \end{bmatrix}}_{\eqqcolon \widehat{\mbf{V}}}\cdot\, 
        \mbf{E}\cdot
        \underbrace{\begin{bmatrix}
            \mbf{R}_{L-1}^\top & & \\
            & \ddots & \\
            & & \mbf{R}_{1}^\top
        \end{bmatrix}}_{\eqqcolon \widehat{\mbf{V}}^\top}.
    \end{align*}
    where
    \begin{align*}
        \mbf{E}\coloneqq
        \begin{bmatrix}
            \mbf{H}_{2, 1} \cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1,2} & \hdots & \mbf{H}_{2,1}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, L} \\
            \vdots & \ddots & \vdots \\
            \mbf{H}_{L, 1}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, 2} & \hdots & \mbf{H}_{L, 1}\cdot (\mbf{H}_{1, 1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, L}
        \end{bmatrix}.
    \end{align*}
    Similarly, we can write $\mbf{D}$ as 
    \begin{align*}
        \mbf{D} = \widehat{\mbf{V}}
        \underbrace{\begin{bmatrix}
            \mbf{H}_{2,2} & \hdots & \mbf{H}_{2, L} \\
            \vdots & \ddots & \vdots \\
            \mbf{H}_{L, 2} & \hdots & \mbf{H}_{L, L}
        \end{bmatrix}}_{\eqqcolon \mbf{F}}
        \widehat{\mbf{V}}^\top.
    \end{align*}
    Then, we have
    \begin{align*}
        \det(\mbf{W} - \lambda \mbf{I}) &= \det(\mbf{R}_{L}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})\cdot\mbf{R}_{L}^\top) \cdot \det\left(\widehat{\mbf{V}} \cdot (\mbf{E} - \mbf{F})\cdot \widehat{\mbf{V}}^\top \right) \\
       &= \det(\mbf{H}_{1,1} - \lambda \mbf{I}) \cdot \det(\mbf{E} - \mbf{F}),
    \end{align*}
    which is not a function of $\mbf{U}, \mbf{V}, [\mbf{\sigma}_{l}]_{l=2}^L$. In lemma, we proved that $\mbf{R}_{L}$ and $\widehat{\mbf{V}}$ are orthogonal matrices which do not change the value of determinant.
    
    By doing the same for $\widetilde{\mbf{H}}$, we can show that both $\widetilde{\mbf{H}}$ and $\mbf{W}$ have the same characteristic polynomials, and hence the same eigenvalues. This completes the proof. This shows that the singular vector of the target matrix does not affect the eigenvalues of the Hessian. 
 
\end{proof}


\begin{lemma}
    If $\mbf{R}_{L}$, $\mbf{R}_{L-1}$,.., $\mbf{R}_{1}$ are orthogonal matrices then $ \mbf{R}_{l-1} \otimes \mbf{R}_{l}$ for $l=1,..,L$ is an orthogonal matrix and the block matrix  $\begin{bmatrix}
            \mbf{R}_{L-1} & & \\
            & \ddots & \\
            & & \mbf{R}_{1}
        \end{bmatrix}$ is also an orthogonal matrix. 
\end{lemma}

\begin{proof}

\begin{align*}
   &  (\mbf{R}_{l-1} \otimes \mbf{R}_{l})^{T} (\mbf{R}_{l-1} \otimes \mbf{R}_{l}) \\ 
   & = (\mbf{R}_{l-1}^{T}\mbf{R}_{l-1} ) \otimes (\mbf{R}_{l}^{T}\mbf{R}_{l} ) = \mathbf{I}_{d} \otimes \mathbf{I}_{d} \\
   & = \mathbf{I}_{d^2} 
\end{align*}
    It similarly holds that $ (\mbf{R}_{l-1} \otimes \mbf{R}_{l}) (\mbf{R}_{l-1} \otimes \mbf{R}_{l})^{T} =\mathbf{I}_{d^2} $. 
So, $\mbf{R}_{l-1} \otimes \mbf{R}_{l}$ is orthogonal if  $\mbf{R}_{L}$, $\mbf{R}_{L-1}$,.., $\mbf{R}_{1}$ are orthogonal matrices. 

Similarly, 

$\begin{bmatrix}
            \mbf{R}_{L-1} & & \\
            & \ddots & \\
            & & \mbf{R}_{1}
        \end{bmatrix}^{T} \begin{bmatrix}
            \mbf{R}_{L-1} & & \\
            & \ddots & \\
            & & \mbf{R}_{1}
        \end{bmatrix} = \begin{bmatrix}
            \mbf{R}^{T}_{L-1}\mbf{R}_{L-1}  & & \\
            & \ddots & \\
            & & \mbf{R}^{T}_{1}\mbf{R}_{1} 
        \end{bmatrix} = \mathbf{I}_{d^2} 
    $
    So, the block matrix $\begin{bmatrix}
            \mbf{R}_{L-1} & & \\
            & \ddots & \\
            & & \mbf{R}_{1}
        \end{bmatrix}$ is orthogonal. 
\end{proof}

\end{comment}




\clearpage

\subsection{Deferred Proofs for Singular Vector Invariance}


\begin{manualpropositioninner}
    
\label{prop:one_zero_svs_set}
    Let $\mbf{M}_\star = \mbf{U}_\star\mbf{\Sigma}_\star \mbf{V}_\star^\top$ denote the SVD of the target matrix. The initialization in Equation~(\ref{eqn:init}) is a member of the singular vector stationary set in Proposition~\ref{prop:svs_set}, where $\mbf{Q}_L = \ldots = \mbf{Q}_2 = \mbf{V}_\star$.
\end{manualpropositioninner}
\begin{proof}
Recall that the initialization is given by
    \begin{align*}
        \mbf{W}_L(0) = 0 \quad \text{and} \quad \mbf{W}_\ell(0) = \alpha\mbf{I}_d \quad \forall \ell \in [L-1].
    \end{align*}
    We will show that under this initialization, each weight matrix admits the following decomposition for all $t \geq 1$:
    \begin{align}
        \mbf{W}_L(t) = \mbf{U}_\star \begin{bmatrix}
            \widetilde{\mbf{\Sigma}}_L(t) & \mbf{0} \\
            \mbf{0} & \mbf{0}
        \end{bmatrix} \mbf{V}_\star^\top,
        \quad\quad
        \mbf{W}_{\ell}(t) = \mbf{V}_\star \begin{bmatrix}
            \widetilde{\mbf{\Sigma}}(t) & \mbf{0} \\
            \mbf{0} & \alpha\mbf{I}_{d-r}
        \end{bmatrix} \mbf{V}_\star^\top,
        \quad \forall \ell \in [L-1],
    \end{align}
where
\begin{align*}
    \widetilde{\mbf{\Sigma}}_L(t) &= \widetilde{\mbf{\Sigma}}_L(t-1) - \eta \cdot\left(\widetilde{\mbf{\Sigma}}_L(t-1) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t-1) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-1}(t-1) \\
    \widetilde{\mbf{\Sigma}}(t) &= \widetilde{\mbf{\Sigma}}(t-1)\cdot  \left(\mbf{I}_r- \eta\cdot\widetilde{\mbf{\Sigma}}_L(t-1)\cdot\left(\widetilde{\mbf{\Sigma}}_L(t-1) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t-1) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-3}(t-1)\right),
\end{align*}
where $\widetilde{\mbf{\Sigma}}_L(t), \widetilde{\mbf{\Sigma}}(t) \in \mbb{R}^{r\times r}$ is a diagonal matrix with $\widetilde{\mbf{\Sigma}}_L(1) = \eta \alpha^{L-1}\cdot \mbf{\Sigma}_{r,\star}$ and $\widetilde{\mbf{\Sigma}}(1) = \alpha \mbf{I}_r$. 

This will prove that the singular vectors are stationary with $\mbf{\Sigma}_L = \ldots =\mbf{\Sigma}_2 = \mbf{V}_\star$. We proceed with mathematical induction. 

\paragraph{Base Case.} For the base case, we will show that the decomposition holds for each weight matrix at $t=1$. The gradient of $f(\mbf{\Theta})$ with respect to $\mbf{W}_{\ell}$ is
\begin{align*}
    \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) = \mbf{W}^{\top}_{L:\ell+1} \cdot \left(\mbf{W}_{L:1} - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}. 
\end{align*}
For $\mbf{W}_L(1)$, we have
\begin{align*}
    \mbf{W}_L(1) &= \mbf{W}_L(0) - \eta \cdot \nabla_{\mbf{W}_{L}} f(\mbf{\Theta}(0)) \\
    &= \mbf{W}_L(0) - \eta \cdot \left(\mbf{W}_{L:1}(0) - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{L-1:1}(0)\\
    &= \eta \alpha^{L-1}\mbf{\Sigma}_\star \\
    &= \mbf{U}_\star \cdot \left( \eta \alpha^{L-1} \cdot \mbf{\Sigma}_\star \right) \cdot \mbf{V}_\star^\top \\
    &= \mbf{U}_\star
    \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(1) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix}
    \mbf{V}_\star^\top.
\end{align*}
Then, for each $\mbf{W}_{\ell}(1)$ in $\ell \in [L-1]$, we have
\begin{align*}
\mbf{W}_{\ell}(1)&= \mbf{W}_{\ell}(0) - \eta \cdot \nabla_{\mbf{W}_{\ell}}f(\mbf{\Theta}(0)) \\
&= \alpha\mbf{I}_d,
\end{align*}
where the last equality follows from the fact that $\mbf{W}_L(0) = \mbf{0}$. Finally, we have
\begin{align*}
    \mbf{W}_{\ell}(1) = \alpha \mbf{V}_\star \mbf{V}_\star^\top = \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}(1) & \mbf{0} \\
        \mbf{0} & \alpha\mbf{I}_{d-r}
     \end{bmatrix}\mbf{V}_\star^\top, \quad \forall \ell \in [L-1].
\end{align*}

\paragraph{Inductive Step.} By the inductive hypothesis, suppose that the decomposition holds. Then, notice that we can simplify the end-to-end weight matrix to
\begin{align*}
    \mbf{W}_{L:1}(t) = \mbf{U}_\star
    \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix}
    \mbf{V}_\star^\top,
\end{align*}
for which we can simplify the gradients to
\begin{align*}
    \nabla_{\mbf{W}_{L}} f(\mbf{\Theta}(t)) &= \left(\mbf{U}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r} & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top\right) \cdot  \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}^{L-1}(t) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix}\mbf{V}_\star^\top \\
    &= \mbf{U}_\star \begin{bmatrix}
        \left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top,
\end{align*}
for the last layer matrix, and similarly,
\begin{align*}
     \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}(t)) &= \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t)\cdot\left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-2}(t)  & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top, \quad \ell \in [L-1],
\end{align*}
for all other layer matrices. Thus, for the next GD iteration, we have
\begin{align*}
    \mbf{W}_L(t+1) &= \mbf{W}_{L}(t) - \eta \cdot \nabla_{\mbf{W}_L}(\mbf{\Theta}(t)) \\
    &= \mbf{U}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t) - \eta \cdot\left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top \\
    &= \mbf{U}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t+1) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top.
\end{align*}
Similarly, we have
\begin{align*}
    \mbf{W}_\ell(t+1) &= \mbf{W}_{\ell}(t) - \eta \cdot \nabla_{\mbf{W}_\ell}(\mbf{\Theta}(t)) \\
    &= \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}(t) - \eta\cdot\widetilde{\mbf{\Sigma}}_L(t)\cdot\left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-2}(t)  & \mbf{0} \\
        \mbf{0} & \alpha \mbf{I}_{d-r}
    \end{bmatrix} \mbf{V}_\star^\top \\
     &= \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}(t)\cdot  \left(\mbf{I}_r- \eta\cdot\widetilde{\mbf{\Sigma}}_L(t)\cdot\left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-3}(t)\right)  & \mbf{0} \\
        \mbf{0} & \alpha \mbf{I}_{d-r}
    \end{bmatrix} \mbf{V}_\star^\top \\
    &= \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}(t+1) & \mbf{0} \\
        \mbf{0} & \alpha \mbf{I}_{d-r}
    \end{bmatrix} \mbf{V}_\star^\top,
\end{align*}
for all $\ell \in [L-1]$. This completes the proof.
\end{proof}




\begin{manualpropositioninner}
\label{prop:balanced_svs_set}

 Let $\mbf{M}_\star = \mbf{V}_\star\mbf{\Sigma}_\star \mbf{V}_\star^\top \in \mbb{R}^{d\times d}$ denote the SVD of the target matrix. The balanced initialization in Equation~(\ref{eqn:init}) is a member of the singular vector stationary set in Proposition~\ref{prop:svs_set}, where  $\mbf{U}_L = \mbf{Q}_L = \ldots = \mbf{Q}_2 = \mbf{V}_1 = \mbf{V}_\star$.
 
\end{manualpropositioninner}

\begin{proof}
    
Using mathematical induction, we will show that with the balanced initialization in Equation~(\ref{eqn:init}), each weight matrix admits a decomposition of the form
\begin{align}
    \mbf{W}_\ell(t) = \mbf{V}_\star \mbf{\Sigma}_\ell(t) \mbf{V}_\star^\top,
\end{align}
which implies that the singular vectors are stationary for all $t$ such that $\mbf{U}_L = \mbf{Q}_L = \ldots = \mbf{Q}_2 = \mbf{V}_1 = \mbf{V}_\star$.

\paragraph{Base Case.} Consider the weights at iteration $t=0$. By the initialization scheme, we can write each weight matrix as
\begin{align*}
    \mbf{W}_\ell(0) = \alpha \mbf{I}_d \implies  \mbf{W}_\ell(0) = \alpha \mbf{V}_\star \mbf{V}_\star^\top,
\end{align*}
which implies that $\mbf{W}_\ell(0) = \mbf{V}_\star \mbf{\Sigma}_\ell(0)\mbf{V}_\star^\top$ with $\mbf{\Sigma}_\ell(0) = \alpha \mbf{I}_d$.

\paragraph{Inductive Step.} By the inductive hypothesis, assume that the decomposition holds for all $t \geq 0$. We will show that it holds for all iterations $t+1$. Recall that the gradient of $f(\mbf{\Theta})$ with respect to $\mbf{W}_{\ell}$ is
\begin{align*}
    \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) = \mbf{W}^{\top}_{L:\ell+1} \cdot \left(\mbf{W}_{L:1} - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}. 
\end{align*}
Then, for $\mbf{W}_\ell(t+1)$, we have
\begin{align*}
    \mbf{W}_\ell(t+1) &= \mbf{W}_\ell(t) - \eta \cdot \nabla_{\mbf{W}_{L}} f(\mbf{\Theta}(t)) \\
    &=  \mbf{V}_\star \mbf{\Sigma}_\ell(t)\mbf{V}_\star^\top - \eta \mbf{W}^\top_{L:\ell+1}(t) \cdot \left(\mbf{W}_{L:1}(t) - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}(t)\\
    &=  \mbf{V}_\star \mbf{\Sigma}_\ell(t)\mbf{V}_\star^\top - \eta \mbf{V}_\star\cdot \left(  \mbf{\Sigma}^{L-\ell}_\ell(t)\cdot \left(\mbf{\Sigma}_\ell^{L}(t) - \mbf{\Sigma}_\star \right)\cdot \mbf{\Sigma}_\ell^{\ell-1}(t) \right) \cdot\mbf{V}^{\top}_{\star}\\
    &=  \mbf{V}_\star\cdot \left(\mbf{\Sigma}_\ell(t) - \eta\cdot  \mbf{\Sigma}^{L-\ell}_\ell(t)\cdot \left(\mbf{\Sigma}_\ell^{L}(t) - \mbf{\Sigma}_\star \right)\cdot \mbf{\Sigma}_\ell^{\ell-1}(t) \right) \cdot\mbf{V}^{\top}_{\star}\\
    &= \mbf{V}_\star
   \mbf{\Sigma}(t)
    \mbf{V}_\star^\top,
\end{align*}
where $\mbf{\Sigma}(t) = \mbf{\Sigma}_\ell(t) - \eta\cdot  \mbf{\Sigma}^{L-\ell}_\ell(t)\cdot \left(\mbf{\Sigma}_\ell^{L}(t) - \mbf{\Sigma}_\star \right)\cdot \mbf{\Sigma}_\ell^{\ell-1}(t)$. This completes the proof.
\end{proof}



\subsection{Auxiliary Results}


\begin{manuallemmainner}
\label{lem:relationship_lemma} 
    Let $\{\mbf{R}_{\ell}\}_{\ell=1}^L \in \mathbb{R}^{n\times n}$ be orthogonal matrices and $\mbf{H}_{i, j} \in \mathbb{R}^{n^2 \times n^2}$ be diagonal matrices. Consider the two following block matrices:
    \begin{align*}
       \mbf{H} &= \begin{bmatrix}
            \mbf{H}_{1,1} & \mbf{H}_{1, 2} & \hdots &\mbf{H}_{L, 1}\\
            \mbf{H}_{2,1} & \mbf{H}_{2,2} & \hdots & \mbf{H}_{L, 2} \\
            \vdots & \vdots & \ddots & \vdots \\
            \mbf{H}_{1, L} & \mbf{H}_{2, L} & \hdots &\mbf{H}_{L, L}
        \end{bmatrix} \\  \widetilde{\mbf{H}} &=
      \begin{bmatrix}
        \mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^{\top} & \mbf{R}_{L}\mbf{H}_{1, 2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L}\mbf{H}_{1, L}\mbf{R}_{1}^{\top}\\
        \mbf{R}_{L-1}\mbf{H}_{2,1}\mbf{R}_{L}^{\top} & \mbf{R}_{L-1}\mbf{H}_{2,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L-1}\mbf{H}_{2, L}\mbf{R}_{1}^{\top}\\
        \vdots & \vdots & \ddots & \vdots \\
        \mbf{R}_{1}\mbf{H}_{L,1}\mbf{R}_{L}^{\top} & \mbf{R}_{1}\mbf{H}_{L,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{1}\mbf{H}_{L,L}\mbf{R}_{1}^{\top}
    \end{bmatrix}.
    \end{align*}
    Then, the two matrices $\mbf{H}$ and $\widetilde{\mbf{H}}$ are similar, in the sense that they have the same eigenvalues.
\end{manuallemmainner}


\begin{proof}
It suffices to show that $\mbf{H}$ and $ \widetilde{\mbf{H}}$ have the same characteristic polynomials. Let us define 
    \begin{align*}
        \widetilde{\mbf{H}} \coloneqq \begin{bmatrix}
            \mbf{A} & \mbf{B} \\
            \mbf{C} & \mbf{D}
        \end{bmatrix},
    \end{align*}
    where 
    \begin{alignat}{3}
        &\mbf{A} \coloneqq  \mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^{\top} \quad\quad\quad &\mbf{B} &\coloneqq \begin{bmatrix}
            \mbf{R}_{L}\mbf{H}_{1, 2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L}\mbf{H}_{1, L}\mbf{R}_{1}^{\top}
        \end{bmatrix} \\
        &\mbf{C} \coloneqq \begin{bmatrix}
            \mbf{R}_{L-1}\mbf{H}_{2,1}\mbf{R}_{L}^{\top} \\
            \vdots \\
           \mbf{R}_{1}\mbf{H}_{L,1}\mbf{R}_{L}^{\top}
        \end{bmatrix}  \quad\quad\quad
        &\mbf{D} &\coloneqq \begin{bmatrix}
           \mbf{R}_{L-1}\mbf{H}_{2,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L-1}\mbf{H}_{2, L}\mbf{R}_{1}^{\top}\\ \\
            \vdots & \ddots & \vdots \\
           \mbf{R}_{1}\mbf{H}_{L,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{1}\mbf{H}_{L,L}\mbf{R}_{1}^{\top}
        \end{bmatrix}.
    \end{alignat}
    Then, we have
    \begin{align*}
        \det(\widetilde{\mbf{H}} - \lambda \mbf{I}) &= \det\left(\begin{bmatrix}
            \mbf{A} - \lambda\mbf{I} & \mbf{B} \\
            \mbf{C} & \mbf{D} - \lambda\mbf{I}
        \end{bmatrix}\right) \\
        &= \det(\mbf{A} - \lambda \mbf{I}) \cdot \det((\mbf{D} - \lambda \mbf{I}) - \mbf{C}(\mbf{A} - \lambda \mbf{I})^{-1}\mbf{B}),
    \end{align*}
    where the second equality is by the Schur complement. Notice that
    \begin{align*}
        (\mbf{A} - \lambda \mbf{I})^{-1} = (\mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^\top  - \lambda \mbf{I})^{-1} &= (\mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^\top  - \lambda \mbf{\mbf{R}_{L}\mbf{R}_{L}}^\top)^{-1} \\&= \mbf{R}_{L} \cdot (\mbf{H}_{1,1} - \lambda\mbf{I})^{-1} \cdot \mbf{R}_{L}^\top.
    \end{align*}
    Then, we also see that, 
    \begin{align*}
        \mbf{C}(\mbf{A} - \lambda \mbf{I})^{-1}\mbf{B} = \underbrace{\begin{bmatrix}
            \mbf{R}_{L-1} & & \\
            & \ddots & \\
            & & \mbf{R}_{1}
        \end{bmatrix}}_{\eqqcolon \widehat{\mbf{V}}}\cdot\, 
        \mbf{E}\cdot
        \underbrace{\begin{bmatrix}
            \mbf{R}_{L-1}^\top & & \\
            & \ddots & \\
            & & \mbf{R}_{1}^\top
        \end{bmatrix}}_{\eqqcolon \widehat{\mbf{V}}^\top}.
    \end{align*}
    where
    \begin{align*}
        \mbf{E}\coloneqq
        \begin{bmatrix}
            \mbf{H}_{2, 1} \cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1,2} & \hdots & \mbf{H}_{2,1}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, L} \\
            \vdots & \ddots & \vdots \\
            \mbf{H}_{L, 1}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, 2} & \hdots & \mbf{H}_{L, 1}\cdot (\mbf{H}_{1, 1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, L}
        \end{bmatrix}.
    \end{align*}
    Similarly, we can write $\mbf{D}$ as 
    \begin{align*}
        \mbf{D} = \widehat{\mbf{V}}
        \underbrace{\begin{bmatrix}
            \mbf{H}_{2,2} & \hdots & \mbf{H}_{2, L} \\
            \vdots & \ddots & \vdots \\
            \mbf{H}_{L, 2} & \hdots & \mbf{H}_{L, L}
        \end{bmatrix}}_{\eqqcolon \mbf{F}}
        \widehat{\mbf{V}}^\top.
    \end{align*}
    Then, we have
    \begin{align*}
        \det(\widetilde{\mbf{H}} - \lambda \mbf{I}) &= \det(\mbf{R}_{L}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})\cdot\mbf{R}_{L}^\top) \cdot \det\left(\widehat{\mbf{V}} \cdot (\mbf{E} - \mbf{F})\cdot \widehat{\mbf{V}}^\top \right) \\
       &= \det(\mbf{H}_{1,1} - \lambda \mbf{I}) \cdot \det(\mbf{E} - \mbf{F}),
    \end{align*}
    which is not a function of $\mbf{U}, \mbf{V},\{\mbf{R}_{\ell}\}_{\ell=1}^L$. By doing the same for $\mbf{H}$, we can show that both $\widetilde{\mbf{H}}$ and $\mbf{H}$ have the same characteristic polynomials, and hence the same eigenvalues. This completes the proof.


\end{proof}


\begin{manuallemmainner}
\label{lemma:kronecker_ortho}
   Let $\mbf{A}, \mbf{B} \in \mbb{R}^{d\times d}$ be two orthogonal matrices. Then, the Kronecker product of $\mbf{A}$ and $\mbf{B}$ is also an orthogonal matrix:
   \begin{align*}
       (\mbf{A} \otimes \mbf{B})^\top (\mbf{A} \otimes \mbf{B}) = (\mbf{A} \otimes \mbf{B})(\mbf{A} \otimes \mbf{B})^\top = \mbf{I}_{d^2}.
   \end{align*}
\end{manuallemmainner}

\begin{proof}
We prove this directly by using properties of Kronecker products:
\begin{align*}
    (\mbf{A} \otimes \mbf{B})^\top (\mbf{A} \otimes \mbf{B}) &= \mbf{A}^\top \mbf{A} \otimes \mbf{B}^\top \mbf{B} \\
    &= \mbf{I}_d \otimes \mbf{I}_d = \mbf{I}_{d^2}.
\end{align*}
Similarly, we have
\begin{align*}
    (\mbf{A} \otimes \mbf{B}) (\mbf{A} \otimes \mbf{B})^\top &= \mbf{A} \mbf{A}^\top \otimes \mbf{B} \mbf{B}^\top \\
    &= \mbf{I}_d \otimes \mbf{I}_d = \mbf{I}_{d^2}.
\end{align*}
This completes the proof.
\end{proof}

\begin{manuallemmainner}
\label{lemm:seq_converge}
    Let $\{a(t)\}_{t=1}^N$ be a sequence such that $a(t) \geq 0$ for all $t$. 
    If there exists a constant $c \in (0,1)$ such that $a(t+1) < c \cdot a(t)$ for all $t$, 
    then $\lim_{t \to \infty} a(t) = 0$.

\end{manuallemmainner}

\begin{proof}
   We prove this by direct reasoning. 
    From the assumption $a(t+1) < c \cdot a(t)$ for some $c \in (0,1)$, we can iteratively expand this inequality:
    \[
    a(t+1) < c \cdot a(t), \quad a(t+2) < c \cdot a(t+1) < c^2 \cdot a(t),
    \]
    and, more generally, by induction:
    \[
    a(t+k) < c^k \cdot a(t), \quad \text{for all } k \geq 0.
    \]
    Since $c \in (0,1)$, the sequence $\{c^k\}_{k=0}^\infty$ converges to $0$ as $k \to \infty$. Hence:
    \[
    0\leq \lim_{k \to \infty} a(t+k) \leq \lim_{k \to \infty} c^k \cdot a(t) = 0.
    \]
    Therefore, by the squeeze theorem, the sequence $\{a(t)\}$ converges to $0$ as $t \to \infty$.
\end{proof}





\begin{manuallemmainner}
[\cite{chen2023edge}]
\label{lemma:chen-bruna}
Consider any 1-D differentiable function $f(x)$ around a local minima $\bar{x}$, satisfying (i) $f^{(3)}(\bar{x}) \neq 0$, and (ii) $3[f^{(3)}]^2 - f'' f^{(4)} > 0$ at $\bar{x}$. Then, there exists $\epsilon$ with sufficiently small $|\epsilon|$ and $\epsilon \cdot f^{(3)} > 0$ such that: for any point $x_0$ between $\bar{x}$ and $\bar{x} - \epsilon$, there exists a learning rate $\eta$ such that $F_{\eta}^2(x_0) = x_0$, and
\end{manuallemmainner}

\[
\frac{2}{f''(\bar{x})} < \eta < \frac{2}{f''(\bar{x}) - \epsilon \cdot f^{(3)}(\bar{x})}.
\]

\clearpage

\begin{manualtheoreminner}
    Let $\eta \geq \frac{2}{K}$, where $K = L\sigma_{\star}^{2- 2/L}$. If we run GD on the singular value loss with learning rate $\eta$, then there exists $\delta_1 \in (0, 1)$ and $\delta_2 \in (1, 2)$ such that if $\sigma_{\ell}(t) \in \left((\delta_1 \sigma_\star)^{1/L}, \sigma_\star^{1/L}\right)$, then for some $t \geq 0$,
    \begin{align*}
    \sigma_{\ell}(t+1) \in \left(\sigma_\star^{1/L}, (\delta_2 \sigma_\star)^{1/L} \right)
    \quad \text{and}
    \quad \sigma_{\ell}(t+2) \in \left((\delta_1 \sigma_\star)^{1/L}, \sigma_\star^{1/L} \right), \quad \forall \ell \in [L].
    \end{align*}

    \noindent Moreover, if $\sigma_{\ell}(t)$ are balanced for all $\ell \in [L]$, then the singular values oscillates in a $2$-period orbit ($j \in \{1,2\}$) around the balanced minimum and admits the following decomposition:
\begin{align}
    \mbf{W}_{L:1} = \underbrace{\sum_{i=1}^p\rho_{i, j}^L \cdot \mbf{u}_{\star, i}\mbf{v}_{\star, i}^{\top} }_{\text{oscillation subspace}}+ \underbrace{\sum_{k=p+1}^d \sigma_{\star, k}\cdot \mbf{u}_{\star, k}\mbf{v}_{\star, k}^{\top}}_{\text{stationary subspace}}, \quad j \in \{1,2\}, \quad \forall\ell \in [L-1],
\end{align}
where $\rho_{i, 1} \in \left(0, \sigma_{\star, i}^{1/L}\right)$ and $\rho_{i, 2} \in \left(\sigma_{\star, i}^{1/L}, (2\sigma_{\star, i})^{1/L}\right)$ are the two real roots of the polynomial $g(\rho_i)=0$ and
\begin{align*}
    g(\rho_i) = \rho_i^L\cdot\frac{1+\left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right)^{2L-1}}{1+\left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right)^{L-1}} - \sigma_{\star, i}.
\end{align*}
\end{manualtheoreminner}

\begin{proof}
    Suppose that for all $\ell \in [L]$,  $\sigma_{\ell}(t) \in \left((\delta_1 \sigma_\star)^{1/L}, \sigma_\star^{1/L}\right)$ for some $\delta_1 \in (0, 1)$. Then, for any $a_\ell \in (0, 1)$, we can write $\sigma_\ell(t) = (a_\ell \sigma_\star)^{1/L}$. First, we will show that 
    \begin{align}
    \label{eqn:sval_gd}\sigma_\ell(t+1) = \sigma_\ell(t) - \eta \cdot \left(\sigma_L(t)\cdot \ldots \cdot \sigma_1(t) - \sigma_\star\right) \cdot \prod_{i\neq \ell} \sigma_i(t),
    \end{align}
    for $\sigma_\ell(t+1) \in \left(\sigma_\star^{1/L}, (2\sigma_\star)^{1/L} \right)$. It suffices to show that $\sigma_\ell(t+1) = (b_\ell \sigma_\star)^{1/L}$ for some $b_\ell \in (1,2)$ for all $\ell \in [L]$.

    By plugging into Equation~(\ref{eqn:sval_gd}), we have
    \begin{align*}
        (b_\ell \sigma_\star)^{1/L} &= (a_\ell \sigma_\star)^{1/L} - \eta \left(\sigma_\star\cdot \prod_{k=1}^L a_k^{1/L} - \sigma_\star\right) \cdot \prod_{i\neq \ell} (a_i \sigma_\star)^{1/L} \\
        &= (a_\ell \sigma_\star)^{1/L} + \eta \sigma_\star \cdot \left(1- \prod_{k=1}^L a_k^{1/L}\right) \cdot \sigma_\star^{(L-1)/ L}\cdot \prod_{i\neq \ell} a_i ^{1/L} \\
        &= \sigma_\star^{1/L} \cdot \left( a_\ell^{1/L} + \eta \sigma_\star^{(L-2)/ L} \cdot\left(1- \prod_{k=1}^L a_k^{1/L}\right) \cdot  \prod_{i\neq \ell} a_i ^{1/L} \right) \\
        \implies b_\ell^{1/L} &= a_\ell^{1/L} + \eta \sigma_\star^{(L-2)/ L} \cdot\left(1- \prod_{k=1}^L a_k^{1/L}\right) \cdot  \prod_{i\neq \ell} a_i ^{1/L} \\
        &\geq  a_\ell^{1/L} + \frac{2}{L} \cdot\left(1- \prod_{k=1}^L a_k^{1/L}\right) \cdot  \prod_{i\neq \ell} a_i ^{1/L} \tag{$\eta \geq \frac{2}{K}$} \\
        \implies b_\ell &\geq \left(a_\ell^{1/L} + \frac{2}{L} \cdot\left(1- \prod_{k=1}^L a_k^{1/L}\right) \cdot  \prod_{i\neq \ell} a_i ^{1/L}  \right)^L.
    \end{align*}
    
    
\end{proof}


\begin{manuallemmainner}
    
    Let $S_i \coloneqq \sigma_{\star, i}^{2 - 2/L}$. Suppose we run GD with learning rate $\eta = \frac{2}{LK}$, where $\mathrm{max}\{S_{i+1}, \frac{S_i}{\sqrt{2}}\}<K\leq S_i$ on the simplified loss in~(\ref{eqn:simplified_loss}). Suppose that the singular values are oscillating such that if $\sigma_\ell(t) \in (0, \sigma_\star^{1/L})$, we have
    \begin{align*}
        \sigma_\ell(t+1) \in (\sigma_\star^{1/L}, (2\sigma_\star)^{1/L}) \quad \text{and} \quad \sigma_\ell(t+2) \in (0, \sigma_\star^{1/L}). 
    \end{align*}
    Then, the balancing gap defined as $|b_{i, j}(t)| \coloneqq |\sigma_i^2(t) - \sigma_j^2(t)|$ decreases as follows:
    \begin{align*}
        |b_{i, j}(t+1)| = c\cdot |b_{i, j}(t)|,
    \end{align*}
    for some constant $c \in (0, 1)$.    
\end{manuallemmainner}
\begin{proof}
    Let us define $\pi(t) \coloneqq \sigma_L(t) \cdot \ldots \cdot \sigma_1(t)$. The balancing dynamics can be simplified to
    \begin{align*}
        b_{i, j}(t+1) =  b_{i, j}(t) \cdot \underbrace{\left| 1 - \eta^2 (\pi(t) - \sigma_\star)^2 \cdot \frac{\pi^2(t)}{\sigma^2_i(t) \sigma^2_j(t)} \right|}_{\eqqcolon c(t)}.
    \end{align*}
    We need to show that $c(t) \in (0, 1)$ for all $t$. Suppose that $\sigma_\ell(t) \in (0, \sigma_\star^{1/L})$ for all $\ell \in [L]$. That is, let $\sigma_\ell = (a_\ell \sigma_\star)^{1/L})$ for any constant $a_\ell \in (0, 1)$. Then, $c(t)$ is given as 
    \begin{align*}
        c(t) &=  \left| 1 - \eta^2 (\pi(t) - \sigma_\star)^2 \cdot \frac{\pi^2(t)}{\sigma^2_i(t) \sigma^2_j(t)} \right| \\
        &= \left| 1 - \eta^2 (\sigma_\star\prod_\ell a_\ell^{1/L} - \sigma_\star)^2 \cdot \frac{\sigma_\star^2 \cdot \prod_\ell a_\ell^{2/L}}{((a_i a_j)^{2/L})\cdot \sigma_\star^{4/L}}\right| \\
        &= \left| 1 - \eta^2 \sigma_\star^2( a_\ell^{1/L} - 1)^2 \cdot \frac{\sigma_\star^2 \cdot \prod_\ell a_\ell^{2/L}}{((a_i a_j)^{2/L})\cdot \sigma_\star^{4/L}}\right| \\
        &= \left|1- \eta^2 \frac{( a_\ell^{1/L} - 1)^2\cdot \prod_\ell a_\ell^{2/L}}{((a_i a_j)^{2/L})} \cdot \sigma_\star^{\frac{4L -4}{L}} \right|.
    \end{align*}
    Now, notice that since each $a_\ell < 1$, it follows that the quantity
    \begin{align*}
        \frac{( a_\ell^{1/L} - 1)^2\cdot \prod_\ell a_\ell^{2/L}}{((a_i a_j)^{2/L})} < 1 \implies \frac{( a_\ell^{1/L} - 1)^2\cdot \prod_\ell a_\ell^{2/L}}{((a_i a_j)^{2/L})} \cdot \frac{4}{L^2} < 1,
    \end{align*}
    \smk{need to verify the fraction / denominator}
    where the $\frac{4}{L^2}$ term comes from $\eta$ and holds since $L\geq 2$.
    Since the above quantity is less than 1, that means that we need to guarantee that 
    \begin{align*}
        \frac{\sigma_\star^{\frac{4L-4}{L}}}{K^2} < 2,
    \end{align*}
    which is satisfied as long as $K > \sqrt{\frac{\sigma_\star^{\frac{4L-4}{L}}}{2} } = \frac{\sigma_\star^{\frac{2L-2}{L}}}{\sqrt{2}}$.

Similarly, consider $a_\ell \in (1, 2)$. This boils down to showing that 
\begin{align*}
    \frac{( a_\ell^{1/L} - 1)^2\cdot \prod_\ell a_\ell^{2/L}}{((a_i a_j)^{2/L})} \cdot \frac{4}{L^2} < 1.
\end{align*}

\end{proof}


\begin{comment}
    
Let $\mbf{U} \in \mbb{R}^{d\times r}$ represent an $r$-dimensional basis and $\mbf{U}_\perp \in \mbb{R}^{d\times d-r}$ be its orthogonal complement. Define $\beta_{\text{task}} \sim f_{\text{task}} \coloneqq \mathcal{N}(0, \mbf{UU}^\top)$ and $\beta_{\text{ood}} \sim f_{\text{ood}} \coloneqq \mathcal{N}(0, \mbf{U}_\perp \mbf{U}_\perp^\top)$. Given feature vectors $\mbf{x} \sim \mathcal{N}(0, \mbf{I}_d)$, suppose we pretrain a GPT2 model with $y = \beta_{\text{task}}^\top \mbf{x}$. Given a query say $\mbf{x}_{n+1}$, based on the literature, ICL should be able to ``complete'' the prompt for any $\alpha_{\text{task}} \in f_{\text{task}}$. That is, even if we train using $\beta_{\text{task}}$, ICL should be able to perform well on the in-context prompt
\begin{align*}
    \mbf{Z} = \begin{bmatrix}
        \mbf{x}_1 & \ldots & \mbf{x}_n & \mbf{x}_{n+1} \\
        \alpha_{\text{task}}^\top\mbf{x}_1 & \ldots & \alpha_{\text{task}}^\top\mbf{x}_n & 0
    \end{bmatrix},
\end{align*}
since $\beta_{\text{task}}$ and $\alpha_{\text{task}}$ are drawn from the same distribution.

If ICL can truly do out-of-distribution generalization, that means that based on this framework, we can construct a query say
\begin{align*}
    \mbf{Z} = \begin{bmatrix}
        \mbf{x}_1 & \ldots & \mbf{x}_n & \mbf{x}_{n+1} \\
        \beta_{\text{ood}}^\top\mbf{x}_1 & \ldots & \beta_{\text{ood}}^\top\mbf{x}_n & 0
    \end{bmatrix},
\end{align*}
and it should perform well on this prompt. Though, there exists a few work in the literature that states that ICL does not perform well on out-of-distribution tasks (e.g., the ICLR paper I sent). In fact, they state that even when you increase the number of pre-training tasks, ICL cannot generalize. I wonder if we can demonstrate this using this framework. For example, we can increase $r \to d$ and observe if ICL can ``increasingly'' do better on ood tasks as $r$ gets closer to $d$, as that would be analagous to ``increasing the number of training tasks'' (I think).
\end{comment}
