\section{Deferred Proofs}
\label{sec:proofs}

In this section, we present the deferred proofs from the main manuscript.


\subsection{Proofs for Singular Vector Stationarity}


\subsubsection{Proof of Proposition~\ref{prop:svs_set}}


\begin{proof}

Let us consider the dynamics of $\mbf{W}_\ell(t)$ in terms of its SVD with respect to time:
\begin{align}
\label{eqn:svd_rynamics}
    \dot{\mbf{W}}_\ell(t) &= \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t) + \mbf{U}_\ell(t) \dot{\mbf{\Sigma}}_\ell(t) \mbf{V}_\ell^\top(t) + \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_\ell^\top(t).
\end{align}
By left multiplying by \(\mbf{U}_\ell^\top(t)\) and right multiplying by \(\mbf{V}_\ell(t)\), we have
\begin{align}
    \mbf{U}_\ell^\top(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) &= \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t) + \dot{\mbf{\Sigma}}_\ell(t) + \mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t), 
\end{align}
where we used the fact that \(\mbf{U}_\ell(t)\) and \(\mbf{V}_\ell(t)\) have orthonormal columns. Now, note that we also have
\begin{align*}
    \mbf{U}_\ell^\top(t) \mbf{U}_\ell(t) = \mbf{I}_r \implies \dot{\mbf{U}}_\ell^\top(t) \mbf{U}_\ell(t) + \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) = \mbf{0},
\end{align*}
which also holds for $\mbf{V}_\ell(t)$. This implies that $\dot{\mbf{U}}_\ell^\top(t) \mbf{U}_\ell(t)$ is a skew-symmetric matrix, and hence have zero diagonals. 
Since \(\mbf{\Sigma}_\ell(t)\) is diagonal, \(\mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t)\) and \(\mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t)\) have zero diagonals as well. On the other hand, since \(\dot{\mbf{\Sigma}}_\ell(t)\) is a diagonal matrix, we can write
\begin{align}
\label{eqn:diag_inv}
    \hat{\mbf{I}}_r \odot \left(\mbf{U}_\ell^\top(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t)\right) &= \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t) + \mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t), 
\end{align}
where \(\odot\) stands for the Hadamard product and \(\hat{\mbf{I}}_r\) is a square matrix holding zeros on its diagonal and ones elsewhere. Taking transpose of Equation~(\ref{eqn:diag_inv}), while recalling that \(\mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t)\) and \(\mbf{V}_\ell^\top(t) \dot{\mbf{V}}_\ell(t)\) are skew-symmetric, we have
\begin{align}
\label{eqn:diag_inv_transpose}
    \hat{\mbf{I}}_{r} \odot \left(\mbf{V}_\ell^\top(t) \dot{\mbf{W}}_\ell^\top(t) \mbf{U}_\ell(t)\right) &= -\mbf{\Sigma}_\ell(t) \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) - \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell(t). 
\end{align}
Then, by right multiplying Equation~(\ref{eqn:diag_inv}) by \(\mbf{\Sigma}_\ell(t)\), left-multiply Equation~(\ref{eqn:diag_inv_transpose}) by \(\mbf{\Sigma}_\ell(t)\), and by adding the two terms, we get
\begin{align*}
    \hat{\mbf{I}}_{r} \odot \biggl(\mbf{U}_\ell^\top(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell(t) + \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t) &\dot{\mbf{W}}_\ell^\top(t) \mbf{U}_\ell(t)\biggr) \\
    &= \mbf{U}_\ell^\top(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell^2(t) - \mbf{\Sigma}_\ell^2(t) \dot{\mbf{V}}_\ell^\top(t) \mbf{V}_\ell(t). 
\end{align*}
Since we assume that the singular values of $\mbf{M}_\star$ are distinct, the top-$r$ diagonal elements of \(\mbf{\Sigma}_{\ell}^2(t)\) are also distinct (i.e., $\Sigma^2_{r}(t) \neq \Sigma^2_{r'}(t) \text{ for } r \neq r'$). This implies that
\begin{align*}
    \mbf{U}_{\ell}^\top(t) \dot{\mbf{U}}_{\ell}(t) &= \mbf{H}(t) \odot \left[\mbf{U}_{\ell}^\top(t) \dot{\mbf{W}}_{\ell}(t) \mbf{V}_{\ell}(t) \mbf{\Sigma}_{\ell}(t) + \mbf{\Sigma}_{\ell}(t) \mbf{V}_{\ell}^\top(t) \dot{\mbf{W}}_{\ell}^\top(t) \mbf{U}_{\ell}(t)\right], 
\end{align*}

where the matrix \(\mbf{H}(t) \in \mathbb{R}^{d\times d}\) is defined by:
\begin{align}
    H_{r,r'}(t) := 
    \begin{cases}
    \left(\Sigma^2_{r'}(t) - \Sigma^2_r(t)\right)^{-1}, & r \neq r', \\
    0, & r = r'.
    \end{cases}
\end{align}

Then, multiplying from the left by \(\mbf{U}_{\ell}(t)\) yields
\begin{align}
    \mbf{P}_{\mbf{U}_{\ell}(t)} \dot{\mbf{U}}_{\ell}(t) &= \mbf{U}_{\ell}(t) \left(\mbf{H}(t) \odot \left[\mbf{U}_{\ell}^\top(t) \dot{\mbf{W}}_{\ell}(t) \mbf{V}_{\ell}(t) \mbf{\Sigma}_{\ell}(t) + \mbf{\Sigma}_{\ell}(t) \mbf{V}_{\ell}^\top(t) \dot{\mbf{W}}_{\ell}^\top(t) \mbf{U}_{\ell}(t)\right]\right), 
\end{align}
with \(\mbf{P}_{\mbf{U}_{\ell}(t)} := \mbf{U}_{\ell}(t) \mbf{U}_{\ell}^\top(t)\) being the projection onto the subspace spanned by the (orthonormal) columns of \(\mbf{U}_{\ell}(t)\). Denote by \(\mbf{P}_{\mbf{U}_{{\ell}\perp}(t)}\) the projection onto the orthogonal complement ( i.e., $\mbf{P}_{\mbf{U}_{\ell\perp}(t)} := \mbf{I}_r - \mbf{U}_{\ell}(t) \mbf{U}_{\ell}^\top(t)$). Apply \(\mbf{P}_{\mbf{U}_{\ell\perp}(t)}\) to both sides of Equation~(\ref{eqn:svd_rynamics}):
\begin{align}
    \mbf{P}_{\mbf{U}_{\ell\perp}(t)}\dot{\mbf{U}}_{\ell}(t)  = \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \dot{\mbf{U}}_{\ell}(t) \mbf{\Sigma}_\ell(t) \mbf{V}_{\ell}^\top(t) &+ \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \mbf{U}_\ell(t) \dot{\mbf{\Sigma}}_{\ell}(t) \mbf{V}_{\ell}^\top(t)\\ &+ \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t) \dot{\mbf{V}}_{\ell}^\top(t). 
\end{align}

Note that \(\mbf{P}_{\mbf{U}_{\ell\perp}(t)} \mbf{U}_\ell(t) = 0\), and multiply from the right by \(\mbf{V}_\ell(t) \mbf{\Sigma}_{\ell}^{-1}(t)\) (the latter is well-defined since we have the compact SVD and the top-$r$ elements are non-zero):
\begin{align}
    \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \dot{\mbf{U}}_\ell(t) &= \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell^{-1}(t) = (\mbf{I}_r - \mbf{U}_\ell(t)\mbf{U}^\top(t)) \dot{\mbf{W}}(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell^{-1}(t). 
\end{align}
Then by adding the two equations above, we obtain an expression for \(\dot{\mbf{U}}(t)\):
\begin{align}
    \dot{\mbf{U}}_\ell(t) &= \mbf{P}_{\mbf{U}_\ell(t)} \dot{\mbf{U}}_\ell(t) + \mbf{P}_{\mbf{U}_{\ell\perp}(t)} \dot{\mbf{U}}_\ell(t) \nonumber \\
    &= \mbf{U}_\ell(t)\left(\mbf{H}(t) \odot \left[\mbf{U}_\ell^\top(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell(t) + \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t) \dot{\mbf{W}}_\ell^\top(t) \mbf{U}_\ell(t)\right]\right) \nonumber \\
    &\quad + (\mbf{I}_r - \mbf{U}_\ell(t) \mbf{U}_\ell^\top(t)) \dot{\mbf{W}}(t) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell^{-1}(t). 
\end{align}
We can similarly derive the dynamics for $\dot{\mbf{V}}_\ell(t)$ and $\dot{\mbf{\Sigma}}_\ell(t)$:
\begin{align}
\dot{\mbf{V}}_\ell(t) = \mbf{V}_\ell(t)\left(\mbf{H}(t) \odot \left[\mbf{\Sigma}_\ell(t) \mbf{U}^\top_\ell(t) \dot{\mbf{W}_{\ell}}(t) \mbf{V}_\ell(t) + \mbf{V}^\top_\ell(t) \dot{\mbf{W}_{\ell}}^\top(t) \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t)\right]\right) \\
+ \left(\mbf{I}_{r} - \mbf{V}_\ell(t)\mbf{V}^\top_\ell(t)\right) \dot{\mbf{W}_{\ell}}^\top(t) \mbf{U}_\ell(t) \mbf{\Sigma}_\ell^{-1}(t), \label{vdiff}
\end{align}
\begin{align*}
   \dot{\mbf{\Sigma}}_\ell(t) = \mbf{I}_r \odot \left[ \mbf{U}^\top_\ell(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \right].
\end{align*}

Now, we will left multiply $\dot{\mbf{U}}_\ell(t)$ and $\dot{\mbf{V}}_\ell(t)$ with $\mbf{U}_\ell^\top(t)$ and $\mbf{V}_\ell^\top(t)$, respectively, to obtain
\begin{align*}
    \mbf{U}^\top_\ell(t) \dot{\mbf{U}}_\ell(t) &= -\mbf{H}(t) \odot \left[\mbf{U}^\top_\ell(t)\nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{V}_\ell(t) \mbf{\Sigma}_\ell(t) + \mbf{\Sigma}_\ell(t) \mbf{V}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{U}_\ell(t)\right], \\
    \mbf{V}^\top_\ell(t) \dot{\mbf{V}}_\ell(t) &= -\mbf{H}(t) \odot \left[\mbf{\Sigma}_\ell(t) \mbf{U}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{V}_\ell(t) + \mbf{V}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t)\right],
\end{align*}
where we replaced $\dot{\mbf{W}}_\ell(t) \coloneqq -\nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta})$, as $\dot{\mbf{W}}_\ell(t)$ is the gradient of $f(\mbf{\Theta})$ with respect to $\mbf{W}_\ell$ by definition. By rearranging and multiplying by $\mbf{\Sigma}_\ell(t)$, we have
\begin{align}
\label{eqn:diagonal_grad}
      \mbf{U}^\top_\ell(t) \dot{\mbf{U}}_\ell(t) \mbf{\Sigma}_\ell(t) -   \mbf{\Sigma}_\ell(t) \mbf{V}^T (t) \dot{\mbf{V}}_\ell(t) = -  \hat{\mbf{I}}_{r} \odot [\mbf{U}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{V}_\ell(t)].
\end{align}
Hence, when $\dot{\mbf{U}}_\ell(t)=0$ and $\dot{\mbf{V}}_\ell(t)=0$, it must be that the left-hand side is zero and so $\mbf{U}^\top_\ell(t) \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) \mbf{V}_\ell(t)$ is a diagonal matrix. 

Now, notice that for the given loss function $f(\mbf{\Theta})$, we have
\begin{align*}
   -\dot{\mbf{W}}_\ell(t) = \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}(t)) = \mbf{W}^{\top}_{L:\ell+1}(t) \cdot \left(\mbf{W}_{L:1}(t) - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}(t). 
\end{align*}
Then, from Equation~(\ref{eqn:diagonal_grad}), when the singular vectors are stationary, we have
\begin{align*}
    \mbf{U}_\ell^\top(t)\mbf{W}^{\top}_{L:\ell+1}(t) \cdot \left(\mbf{W}_{L:1}(t) - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}(t)\mbf{V}_\ell(t)
\end{align*}
must be a diagonal matrix for all $\ell \in [L]$. The only solution to the above should be (since the intermediate singular vectors need to cancel to satisfy the diagonal condition), is the set
\begin{align*}
\mathrm{SVS}(f(\mbf{\Theta})) = 
\begin{cases}
    (\mbf{U}_L, \mbf{V}_L) &= (\mbf{U}_\star, \mbf{Q}_L), \\
    (\mbf{U}_\ell, \mbf{V}_\ell) &= (\mbf{Q}_{\ell+1}, \mbf{Q}_\ell), \quad\forall \ell \in [2, L-1], \\
    (\mbf{U}_1, \mbf{V}_1) &= (\mbf{Q}_2, \mbf{V}_\star),
\end{cases}
\end{align*}
where \(\{\mbf{Q}_\ell\}_{\ell=2}^{L}\) are any set of orthogonal matrices. Then, notice that when the singular vectors are stationary, the dynamics become isolated on the singular values: \begin{align*}
   \dot{\mbf{\Sigma}}_\ell(t) = \mbf{I}_r \odot \left[ \mbf{U}^\top_\ell(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \right],
\end{align*} 
since $\left[ \mbf{U}^\top_\ell(t) \dot{\mbf{W}}_\ell(t) \mbf{V}_\ell(t) \right]$ is diagonal. This completes the proof.


\end{proof}


\subsubsection{Supporting Results}


\begin{proposition}
    
\label{prop:one_zero_svs_set}
    Let $\mbf{M}_\star = \mbf{U}_\star\mbf{\Sigma}_\star \mbf{V}_\star^\top$ denote the SVD of the target matrix. The initialization in Equation~(\ref{eqn:unbalanced_init}) is a member of the singular vector stationary set in Proposition~\ref{prop:svs_set}, where $\mbf{Q}_L = \ldots = \mbf{Q}_2 = \mbf{V}_\star$.
\end{proposition}
\begin{proof}
Recall that the initialization is given by
    \begin{align*}
        \mbf{W}_L(0) = 0 \quad \text{and} \quad \mbf{W}_\ell(0) = \alpha\mbf{I}_d \quad \forall \ell \in [L-1].
    \end{align*}
    We will show that under this initialization, each weight matrix admits the following decomposition for all $t \geq 1$:
    \begin{align}
        \mbf{W}_L(t) = \mbf{U}_\star \begin{bmatrix}
            \widetilde{\mbf{\Sigma}}_L(t) & \mbf{0} \\
            \mbf{0} & \mbf{0}
        \end{bmatrix} \mbf{V}_\star^\top,
        \quad\quad
        \mbf{W}_{\ell}(t) = \mbf{V}_\star \begin{bmatrix}
            \widetilde{\mbf{\Sigma}}(t) & \mbf{0} \\
            \mbf{0} & \alpha\mbf{I}_{d-r}
        \end{bmatrix} \mbf{V}_\star^\top,
        \quad \forall \ell \in [L-1],
    \end{align}
where
\begin{align*}
    \widetilde{\mbf{\Sigma}}_L(t) &= \widetilde{\mbf{\Sigma}}_L(t-1) - \eta \cdot\left(\widetilde{\mbf{\Sigma}}_L(t-1) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t-1) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-1}(t-1) \\
    \widetilde{\mbf{\Sigma}}(t) &= \widetilde{\mbf{\Sigma}}(t-1)\cdot  \left(\mbf{I}_r- \eta\cdot\widetilde{\mbf{\Sigma}}_L(t-1)\cdot\left(\widetilde{\mbf{\Sigma}}_L(t-1) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t-1) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-3}(t-1)\right),
\end{align*}
where $\widetilde{\mbf{\Sigma}}_L(t), \widetilde{\mbf{\Sigma}}(t) \in \mbb{R}^{r\times r}$ is a diagonal matrix with $\widetilde{\mbf{\Sigma}}_L(1) = \eta \alpha^{L-1}\cdot \mbf{\Sigma}_{r,\star}$ and $\widetilde{\mbf{\Sigma}}(1) = \alpha \mbf{I}_r$. 

This will prove that the singular vectors are stationary with $\mbf{\Sigma}_L = \ldots =\mbf{\Sigma}_2 = \mbf{V}_\star$. We proceed with mathematical induction. 

\paragraph{Base Case.} For the base case, we will show that the decomposition holds for each weight matrix at $t=1$. The gradient of $f(\mbf{\Theta})$ with respect to $\mbf{W}_{\ell}$ is
\begin{align*}
    \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) = \mbf{W}^{\top}_{L:\ell+1} \cdot \left(\mbf{W}_{L:1} - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}. 
\end{align*}
For $\mbf{W}_L(1)$, we have
\begin{align*}
    \mbf{W}_L(1) &= \mbf{W}_L(0) - \eta \cdot \nabla_{\mbf{W}_{L}} f(\mbf{\Theta}(0)) \\
    &= \mbf{W}_L(0) - \eta \cdot \left(\mbf{W}_{L:1}(0) - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{L-1:1}(0)\\
    &= \eta \alpha^{L-1}\mbf{\Sigma}_\star \\
    &= \mbf{U}_\star \cdot \left( \eta \alpha^{L-1} \cdot \mbf{\Sigma}_\star \right) \cdot \mbf{V}_\star^\top \\
    &= \mbf{U}_\star
    \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(1) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix}
    \mbf{V}_\star^\top.
\end{align*}
Then, for each $\mbf{W}_{\ell}(1)$ in $\ell \in [L-1]$, we have
\begin{align*}
\mbf{W}_{\ell}(1)&= \mbf{W}_{\ell}(0) - \eta \cdot \nabla_{\mbf{W}_{\ell}}f(\mbf{\Theta}(0)) \\
&= \alpha\mbf{I}_d,
\end{align*}
where the last equality follows from the fact that $\mbf{W}_L(0) = \mbf{0}$. Finally, we have
\begin{align*}
    \mbf{W}_{\ell}(1) = \alpha \mbf{V}_\star \mbf{V}_\star^\top = \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}(1) & \mbf{0} \\
        \mbf{0} & \alpha\mbf{I}_{d-r}
     \end{bmatrix}\mbf{V}_\star^\top, \quad \forall \ell \in [L-1].
\end{align*}

\paragraph{Inductive Step.} By the inductive hypothesis, suppose that the decomposition holds. Then, notice that we can simplify the end-to-end weight matrix to
\begin{align*}
    \mbf{W}_{L:1}(t) = \mbf{U}_\star
    \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix}
    \mbf{V}_\star^\top,
\end{align*}
for which we can simplify the gradients to
\begin{align*}
    \nabla_{\mbf{W}_{L}} f(\mbf{\Theta}(t)) &= \left(\mbf{U}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r} & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top\right) \cdot  \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}^{L-1}(t) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix}\mbf{V}_\star^\top \\
    &= \mbf{U}_\star \begin{bmatrix}
        \left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top,
\end{align*}
for the last layer matrix, and similarly,
\begin{align*}
     \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}(t)) &= \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t)\cdot\left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-2}(t)  & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top, \quad \ell \in [L-1],
\end{align*}
for all other layer matrices. Thus, for the next GD iteration, we have
\begin{align*}
    \mbf{W}_L(t+1) &= \mbf{W}_{L}(t) - \eta \cdot \nabla_{\mbf{W}_L}(\mbf{\Theta}(t)) \\
    &= \mbf{U}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t) - \eta \cdot\left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top \\
    &= \mbf{U}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}_L(t+1) & \mbf{0} \\
        \mbf{0} & \mbf{0}
    \end{bmatrix} \mbf{V}_\star^\top.
\end{align*}
Similarly, we have
\begin{align*}
    \mbf{W}_\ell(t+1) &= \mbf{W}_{\ell}(t) - \eta \cdot \nabla_{\mbf{W}_\ell}(\mbf{\Theta}(t)) \\
    &= \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}(t) - \eta\cdot\widetilde{\mbf{\Sigma}}_L(t)\cdot\left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-2}(t)  & \mbf{0} \\
        \mbf{0} & \alpha \mbf{I}_{d-r}
    \end{bmatrix} \mbf{V}_\star^\top \\
     &= \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}(t)\cdot  \left(\mbf{I}_r- \eta\cdot\widetilde{\mbf{\Sigma}}_L(t)\cdot\left(\widetilde{\mbf{\Sigma}}_L(t) \cdot \widetilde{\mbf{\Sigma}}^{L-1}(t) - \mbf{\Sigma}_{\star,r}\right)\cdot \widetilde{\mbf{\Sigma}}^{L-3}(t)\right)  & \mbf{0} \\
        \mbf{0} & \alpha \mbf{I}_{d-r}
    \end{bmatrix} \mbf{V}_\star^\top \\
    &= \mbf{V}_\star \begin{bmatrix}
        \widetilde{\mbf{\Sigma}}(t+1) & \mbf{0} \\
        \mbf{0} & \alpha \mbf{I}_{d-r}
    \end{bmatrix} \mbf{V}_\star^\top,
\end{align*}
for all $\ell \in [L-1]$. This completes the proof.
\end{proof}




\begin{proposition}
\label{prop:balanced_svs_set}

 Let $\mbf{M}_\star = \mbf{V}_\star\mbf{\Sigma}_\star \mbf{V}_\star^\top \in \mbb{R}^{d\times d}$ denote the SVD of the target matrix. The balanced initialization in Equation~(\ref{eqn:balanced_init}) is a member of the singular vector stationary set in Proposition~\ref{prop:svs_set}, where  $\mbf{U}_L = \mbf{Q}_L = \ldots = \mbf{Q}_2 = \mbf{V}_1 = \mbf{V}_\star$.
 
\end{proposition}

\begin{proof}
    
Using mathematical induction, we will show that with the balanced initialization in Equation~(\ref{eqn:balanced_init}), each weight matrix admits a decomposition of the form
\begin{align}
    \mbf{W}_\ell(t) = \mbf{V}_\star \mbf{\Sigma}_\ell(t) \mbf{V}_\star^\top,
\end{align}
which implies that the singular vectors are stationary for all $t$ such that $\mbf{U}_L = \mbf{Q}_L = \ldots = \mbf{Q}_2 = \mbf{V}_1 = \mbf{V}_\star$.

\paragraph{Base Case.} Consider the weights at iteration $t=0$. By the initialization scheme, we can write each weight matrix as
\begin{align*}
    \mbf{W}_\ell(0) = \alpha \mbf{I}_d \implies  \mbf{W}_\ell(0) = \alpha \mbf{V}_\star \mbf{V}_\star^\top,
\end{align*}
which implies that $\mbf{W}_\ell(0) = \mbf{V}_\star \mbf{\Sigma}_\ell(0)\mbf{V}_\star^\top$ with $\mbf{\Sigma}_\ell(0) = \alpha \mbf{I}_d$.

\paragraph{Inductive Step.} By the inductive hypothesis, assume that the decomposition holds for all $t \geq 0$. We will show that it holds for all iterations $t+1$. Recall that the gradient of $f(\mbf{\Theta})$ with respect to $\mbf{W}_{\ell}$ is
\begin{align*}
    \nabla_{\mbf{W}_{\ell}} f(\mbf{\Theta}) = \mbf{W}^{\top}_{L:\ell+1} \cdot \left(\mbf{W}_{L:1} - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}. 
\end{align*}
Then, for $\mbf{W}_\ell(t+1)$, we have
\begin{align*}
    \mbf{W}_\ell(t+1) &= \mbf{W}_\ell(t) - \eta \cdot \nabla_{\mbf{W}_{L}} f(\mbf{\Theta}(t)) \\
    &=  \mbf{V}_\star \mbf{\Sigma}_\ell(t)\mbf{V}_\star^\top - \eta \mbf{W}^\top_{L:\ell+1}(t) \cdot \left(\mbf{W}_{L:1}(t) - \mbf{M}_\star \right) \cdot \mbf{W}^{\top}_{\ell-1:1}(t)\\
    &=  \mbf{V}_\star \mbf{\Sigma}_\ell(t)\mbf{V}_\star^\top - \eta \mbf{V}_\star\cdot \left(  \mbf{\Sigma}^{L-\ell}_\ell(t)\cdot \left(\mbf{\Sigma}_\ell^{L}(t) - \mbf{\Sigma}_\star \right)\cdot \mbf{\Sigma}_\ell^{\ell-1}(t) \right) \cdot\mbf{V}^{\top}_{\star}\\
    &=  \mbf{V}_\star\cdot \left(\mbf{\Sigma}_\ell(t) - \eta\cdot  \mbf{\Sigma}^{L-\ell}_\ell(t)\cdot \left(\mbf{\Sigma}_\ell^{L}(t) - \mbf{\Sigma}_\star \right)\cdot \mbf{\Sigma}_\ell^{\ell-1}(t) \right) \cdot\mbf{V}^{\top}_{\star}\\
    &= \mbf{V}_\star
   \mbf{\Sigma}(t)
    \mbf{V}_\star^\top,
\end{align*}
where $\mbf{\Sigma}(t) = \mbf{\Sigma}_\ell(t) - \eta\cdot  \mbf{\Sigma}^{L-\ell}_\ell(t)\cdot \left(\mbf{\Sigma}_\ell^{L}(t) - \mbf{\Sigma}_\star \right)\cdot \mbf{\Sigma}_\ell^{\ell-1}(t)$. This completes the proof.
\end{proof}





\subsection{Proofs for Balancing}

In this section, we present our proof of Proposition~\ref{prop:balancing} along with supporting results. Throughout these results, we use the notion of the gradient flow solution (GFS) and the GFS sharpness as presented by~\cite{kreisler2023gradient}, which we briefly recap. \\

\noindent Consider minimizing a smooth loss function  $\mathcal{L}:\mbb{R}^d \to \mbb{R}$ using gradient flow (GF):
\begin{align*}
    \dot{\mbf{w}}(t) = -\nabla\mathcal{L}(\mbf{w}(t)).
\end{align*}
The GFS denoted by $S_{\mathrm{GF}}(\mbf{w})$ is the limit of the gradient flow trajectory when initialized at $\mbf{w}$. Furthermore, the GFS sharpness denoted by $\psi(\mbf{w})$ is defined to be the sharpness of $S_{\mathrm{GF}}(\mbf{w})$, i.e., the largest eigenvalue of $\nabla^2 \mathcal{L}\left( S_{\mathrm{GF}}(\mbf{w}) \right)$. 




\subsubsection{Supporting Lemmas}

\begin{lemma}[Conservation of Balancedness in GF]
\label{gf-unbalanced}
    Consider the singular value scalar loss         \begin{align*} 
        \mathcal{L}\left(\{\sigma_\ell\}_{\ell=1}^L\right)
 = \frac{1}{2} \left( \prod_{\ell=1}^L \sigma_{\ell} - \sigma_{\star} \right)^2.
    \end{align*}  
    Under gradient flow, the balancedness between two singular values defined by $\sigma^2_{ \ell} (t) - \sigma^2_{m} (t)$ for all $m, \ell \in [L]$ is constant for all $t\geq 0$.
\end{lemma}

\begin{proof}
Notice that the result holds specifically for gradient flow and not descent. The dynamics of each scalar factor for gradient flow can be written as
    \begin{align*}
        \dot{\sigma}_{\ell}(t) = - \left(\prod_{\ell=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right)\cdot \prod_{i\neq \ell}^L \sigma_{i}(t)
    \end{align*}
Then, the time derivative of balancing is given as
\begin{align*}
  & \frac{\partial}{\partial t} (\sigma^2_{ \ell} (t) - \sigma^2_{m} (t)) = \sigma_{ \ell} (t)\dot{\sigma}_{\ell}(t)  - \sigma_{m} (t)\dot{\sigma}_{m}(t)  \\
  & = - \sigma_{ \ell} (t)\left(\prod_{\ell=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right)\cdot \prod_{i\neq \ell}^L \sigma_{i}(t) + \sigma_{m} (t)\left(\prod_{m=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right)\cdot \prod_{j\neq m}^L \sigma_{j}(t). \\
  & = 0.
\end{align*}
Hence, the quantity $\sigma^2_{ \ell} (t) - \sigma^2_{m} (t) $ remains constant for all time $t\geq 0$, hence preserving balancedness. 
\end{proof}

\begin{lemma}[Sharpness at Minima]
\label{1d-sharp}
    Consider the singular value scalar loss     \begin{align*} 
        \mathcal{L}(\{\sigma_i\}_{i=1}^d)
 = \frac{1}{2} \left( \prod_{i=1}^L \sigma_{i} - \sigma_{\star} \right)^2,
    \end{align*}
    The sharpness at the global minima is given as $\| \nabla^2 \mathcal{L} \|_{2} = \sum_{i=1}^{L} \frac{\sigma^2_{\star}}{\sigma^2_{i}}$.
\end{lemma}

\begin{proof}
The gradient is given by
\begin{align*}
    \nabla_{\sigma_{i}}  \mathcal{L} = \left(\prod_{\ell=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right) \prod_{j\neq i}^L \sigma_{j}(t).
\end{align*}
Then, 
\begin{align*}
     \nabla_{\sigma_{j}}  \nabla_{\sigma_{i}}  \mathcal{L} =  \prod_{\ell\neq i}^L \sigma_{\ell}(t)  \prod_{\ell\neq j}^L \sigma_{\ell}(t) + \left(\prod_{\ell=1}^L \sigma_{ \ell} (t) - \sigma_{\star} \right)  \prod_{\ell\neq j, \ell \neq i}^L \sigma_{\ell}(t)
\end{align*}
 Let $\pi(t)=  \prod_{i=1}^L \sigma_{i}(t)$. Then, at the global minima, we have
\begin{align*}
     \nabla_{\sigma_{j}}  \nabla_{\sigma_{i}}  \mathcal{L} =  \frac{\pi^2}{\sigma_{i} \sigma_{j}} = \frac{\sigma_{\star}^2}{\sigma_{i} \sigma_{j}}
\end{align*}
Thus, the sharpness of the largest eigenvalue is given as $ \| \nabla^2 \mathcal{L} \|_{2} = \sum_{i=1}^{L} \frac{\sigma^2_{\star}}{\sigma^2_{i}}$. 
\end{proof}

\begin{lemma}[Balanced Minima is the Flattest]
\label{lemma:flattest}
Consider the singular value scalar loss \begin{align*} 
        \mathcal{L}\left(\{\sigma_i\}_{i=1}^L\right)
 = \frac{1}{2} \left( \prod_{i=1}^L \sigma_{i} - \sigma_{\star} \right)^2.
    \end{align*} 
The balanced minimum (i.e., $\sigma_i = \sigma_\star^{1/L}$ for all $i \in [L]$)  has the smallest sharpness amongst all global minima with a value of $\|\nabla^2 \mathcal{L}\|_2 = L\sigma_\star^{2-2/L}$.
\end{lemma}
\begin{proof}
    By Lemma~\ref{1d-sharp}, recall that the sharpness at the global minima is given in the form
    \begin{align*}
        \|\nabla^2 \mathcal{L}\|_2 = \sum_{i=1}^L \frac{\sigma_\star^2}{\sigma_i^2}.
    \end{align*}
    To show that the balanced minimum is the flattest (i.e., it has the smallest sharpness amongst all global minima), we will show that KKT stationarity condition of the constrained objective
    \begin{align*}
        \underset{\{\sigma_i\}_{i=1}^L }{\mathrm{min}} \,  \sum_{i=1}^L \frac{\sigma_\star^2}{\sigma_i^2} \quad\,\mathrm{s.t.} \,\, \prod_{i=1}^L \sigma_i = \sigma_\star,
    \end{align*}
    are only met at the balanced minimum, 
    which gives us the sharpness value $\| \nabla^2 \mathcal{L} \|_{2} = L\sigma_\star^{2-2/L}$.
    The Lagrangian is given by
    \begin{align*}
        L(\sigma_1, \ldots, \sigma_L, \mu) = \sum_{i=1}^L \frac{\sigma_\star^2}{\sigma_i^2} + \mu\left( \prod_{i=1}^L \sigma_i - \sigma_\star \right).
    \end{align*}
Then, the stationary point conditions of the Langrangian is given by 
    \begin{align}
    \label{eqn:station1}
        \frac{\partial L}{\partial \sigma_i} &= -\frac{2\sigma_\star^2}{\sigma_i^3} + \mu \prod_{j\neq i} \sigma_j = 0, \\
    \label{eqn:station2}
    \frac{\partial L}{\partial \mu} &= \prod_{i=1}^L \sigma_i - \sigma_\star = 0.
    \end{align}
    From Equation~(\ref{eqn:station1}), the solution of the stationary point gives
    \begin{align*}
        \frac{2\sigma_\star^2}{\sigma_i^3} = \mu \prod_{j\neq i} \sigma_j \implies \mu =  \frac{2\sigma_\star^2}{\sigma_i^3 \prod_{j\neq i} \sigma_j} = \frac{2\sigma_\star^2}{\sigma_i^2 \sigma_\star} = \frac{2\sigma_\star}{\sigma_i^2}.
    \end{align*}
    This also indicates that at the stationary point, $\sigma_{i} = \sqrt{\frac{2\sigma_\star}{\mu}}$ for all $i \in [L]$, which means that the condition is \emph{only} satisfied at the balanced minimum, i.e, $\sigma_{i} = \sigma_\star^{1/L}$. Furthermore, notice that
    \begin{align*}
        \nabla^2 f(\sigma_i) = 6 \sigma_\star^2 \cdot \diag\left(\frac{1}{\sigma^4_{i}}\right) \succ \mathbf{0},
    \end{align*}
    where $f(\sigma_i) = \sum_{i=1}^L \frac{\sigma_\star^2}{\sigma_i^2}$, indicating that $f$ only has a minimum. Notice that Equation~(\ref{eqn:station2}) holds immediately. Thus, the balanced minimum has the smallest shaprness (flattest), which plugging into $f$ gives a sharpness of $\|\nabla^2 \mathcal{L}\|_2 = L\sigma_\star^{2-2/L}$. 

    
    % Then, plugging in the balanced minimum and $\mu^*$ into Equation~(\ref{eqn:station1}), we have
    % \begin{align*}
    %     -\frac{2\sigma_\star^2}{\sigma_\star^{3/L}} + \frac{2\sigma_\star}{\sigma_\star^{2/L}} \cdot \left(\sigma_\star^{1/L}\right)^{L-1} = -2\sigma_\star^{\frac{2L-3}{L}} + 2\sigma_\star^{\frac{2L-3}{L}} = 0.
    % \end{align*}
    % Notice that Equation~(\ref{eqn:station2}) holds immediately. Thus, the balanced minimum $\sigma_i = \sigma_\star^{1/L}$ satisfies both the stationary point conditions, which completes the proof.
\end{proof}




\begin{lemma}
\label{GFS-3}
    Let $\mbf{s} \coloneqq \begin{bmatrix}
        \sigma_1  & \sigma_2 & \ldots & \sigma_L
    \end{bmatrix} \in \mbb{R}^L$ and define the singular value scalar loss as 
    \begin{align*} 
        \mathcal{L}(\mbf{s})
 = \frac{1}{2} \left( \prod_{i=1}^L \sigma_{i} - \sigma_{\star} \right)^2,
    \end{align*} 
    for some $\sigma_\star > 0$. If $\sigma \in \mbb{R}^L$ are initialized such that
    \begin{align*}
        \sigma_L(0) = 0 \quad \text{and} \quad \sigma_\ell(0) = \alpha, \quad \forall \ell \in [L-1],
    \end{align*}
    where $0<\alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{ \sigma_{\star}^{\frac{4}{L}}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{\frac{1}{4}}$ and $\eta > 0$, then
    the GFS sharpness satisfies $\psi(\mbf{s}) \leq \frac{2\sqrt{1+c}}{\eta}$ for some $0<c<1$.
\end{lemma} 


\begin{proof}
    We will show that the necessary condition for the GFS sharpness 
    to satisfy $\psi(\mbf{s}) \leq \frac{2\sqrt{1+c}}{\eta}$ for some $\eta >0$ and $0<c<1$ to hold is that the initialization scale $\alpha$ must satisfy $0<\alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{ \sigma_{\star}^{\frac{4}{L}}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{\frac{1}{4}}$. \\
    
    Since the singular values $\sigma_\ell$ for all $\ell \in [L-1]$ are initialized to $\alpha$, note that they all follow the same dynamics. Then, let us define the following for simplicity in exposition:
    \begin{align*}
        y \coloneqq \sigma_1 = \ldots = \sigma_{L-1} \quad \text{and} \quad x \coloneqq \sigma_L,
    \end{align*}
    and so $\prod_{\ell=1}^L \sigma_\ell = xy^{L-1}$.
    Then, note that the  gradient flow (GF) solution is the intersection between
    \begin{align*}
       xy^{L-1}=\sigma_{\star} \quad \text{and} \quad x^{2} - y^{2}= -\alpha^2,
    \end{align*}
    where the first condition comes from convergence and the second comes from the conservation flow law of GF from in Lemma \ref{gf-unbalanced}.  
    Then, if we can find a solution at the intersection such that
    \begin{align}
    \label{constraint-set}
        (\hat{x}(\alpha),\hat{y}(\alpha)) = \begin{cases}
            xy^{L-1}=\sigma_{\star} \\
            x^{2} - y^{2}= -\alpha^2,
        \end{cases} 
    \end{align}
    solely in terms of $\alpha$, we can plug in $(\hat{x}(\alpha),\hat{y}(\alpha))$ into the GFS\footnote{Note that throughout the proof $(\hat{x}(\alpha),\hat{y}(\alpha))$ denotes the gradient flow solution as function of $\alpha$. It does
not refer to the GF trajectory.}:
    \begin{align}
\label{eqn:psi_diff}
        \psi(\hat{x}(\alpha),\hat{y}(\alpha)) 
 = \psi(\mbf{s}) \stackrel{(i)}{=}\sum_{i=1}^{L} \frac{\sigma^2_{\star}}{\sigma^2_{i}} = \sigma^2_{\star}\left(\frac{1}{\hat{x}(\alpha)^2} + \frac{L-1}{\hat{y}(\alpha)^2}\right) < \frac{2\sqrt{2}}{\eta},
    \end{align}
and solve to find an upper bound in terms of $\alpha$, where (i) comes from Lemma \ref{1d-sharp}.
The strict inequality ensures that we can find a $c$ in $c \in [0,1)$ such that $ \psi(\mbf{s}) \leq\frac{2\sqrt{1+c}}{\eta} $. However, the intersection $(\hat{x}(\alpha),\hat{y}(\alpha))$ is a $2L$-th order polynomial in $\hat{y}(\alpha)$ which does not have a straightforward closed-form solution solely in terms of $\alpha$. 
To this end, we aim to find a more tractable upper bound on $\psi(\hat{x}(\alpha), \hat{y}(\alpha))$ by using variational calculus, and use that to find a bound on $\alpha$ instead. Specifically, we will compute the differential $d\psi$, upper bound $d\psi$ with a tractable function, and then integrate to obtain our new function $\psi'$ for which we use to set  $\psi' < \frac{2\sqrt{2}}{\eta}$.

\paragraph{Computing the Differentials $d\hat{x}$ and $d\hat{y}$.} 

Before computing the differential $d\psi$, we need to derive the differentials of $\hat{x}(\alpha)$ and $\hat{y}(\alpha)$. We drop the $\alpha$ notation and use $\hat{x}$ and $\hat{y}$ where applicable. 
%For now, we assume that $\alpha < \sigma_\star$, but we will see later that our bound on $\alpha$ already encompasses this case.
By plugging in $\hat{x}$ into Equation~(\ref{constraint-set}), the solution $\hat{y}$ satisfies
\begin{align*}
    \hat{y}^{2L} - \alpha^2 \hat{y}^{2L-2} =  \sigma^2_{\star}.
\end{align*}
Then, by differentiating the relation with respect to $\alpha$, we obtain the following variational relation:
\begin{align}
   &2L  \hat{y}^{2L-1}d \hat{y} - \alpha^2 2 (L-1) \hat{y}^{2L-3}d\hat{y} - 2\alpha \hat{y}^{2L-2} d\alpha = 0 \notag  \\ 
   & \implies \hat{y}^{2L-3} (\hat{y}^2 L - \alpha^2 (L-1)) d\hat{y} = \alpha \hat{y}^{2(L-1)} d\alpha \notag \\ \
   & \implies d\hat{y} = \frac{\hat{y} \alpha}{ (\hat{y}^2 L -\alpha^2 (L-1))} d\alpha,
\end{align}
where we used Lemma~3.10 of~\cite{kreisler2023gradient} to deduce that $\hat{y} > 0$ and so $\hat{y}^{2L-2} > 0$. Then, notice that we have $\hat{y} > \sqrt{\frac{L-1}{L}} \alpha$ from initialization, and so we
$\frac{d\hat{y}}{d \alpha}>0$, (i.e., $\hat{y}(\alpha)$ is an increasing function of $\alpha$). Then, we also have
\begin{align*}
    \underset{\alpha \to 0}{\lim} \, \hat{y}(\alpha) = \sigma_\star^{1/L} \quad \text{and} \quad \underset{\alpha \to 0}{\lim} \, \hat{x}(\alpha) = \sigma_\star^{1/L}, 
\end{align*}
as it corresponds to exact balancing. Hence, as $\alpha$ increases from 0, $\hat{y}(\alpha)$ increases from $\sigma_\star^{1/L}$.

Similarly, the intersection at the global minima satisfies the following relation for $ \hat{x}$:
\begin{align}
    & \hat{x}^{\left(2+ \frac{2}{L-1} \right)} + \hat{x}^{\frac{2}{L-1}} \alpha^2 = \sigma^{\frac{2}{L-1}}_{\star} \notag \\ 
    & \implies \left(2+\frac{2}{L-1} \right) \hat{x}^{\left({ \frac{2}{L-1}+1}\right)} d\hat{x} + \left(\frac{2}{L-1}\right)\alpha^2 \hat{x}^{\left(\frac{2}{L-1}-1\right)} d\hat{x}  + 2\alpha\hat{x}^{\frac{2}{L-1}}d\alpha = 0 \notag \\
    & \implies d\hat{x} = \frac{-\alpha}{\left(\frac{L\hat{x}}{L-1} + \frac{\alpha^2}{(L-1)\hat{x}}\right)} d\alpha.
\end{align}

Note that since $\hat{x}>0$, we have $\frac{dx}{d\alpha}<0$. This implies that as $\alpha$ increases from 0, $\hat{x}(\alpha)$ decreases from $\sigma_{\star}^{1/L}$.



\paragraph{Computing the Differential $d\psi$.} Now we are position to derive the differential $d\psi$. Let us define $\Psi(\alpha) \coloneqq \psi(\hat{x}(\alpha),\hat{y}(\alpha)) $ as we ultimately want the behavior in terms of $\alpha$. Let us simplify $\Psi(\alpha)$ first:
\begin{align}
   \Psi(\alpha) \coloneqq  \psi(\hat{x}(\alpha),\hat{y}(\alpha)) &=  \sigma_{\star}^2 \left(\frac{1}{\hat{x}(\alpha)^2} + \frac{L-1}{\hat{y}(\alpha)^2} \right)  \tag{From Equation~(\ref{eqn:psi_diff})} \\ 
    &= \sigma_{\star}^2 \left(\frac{\hat{y}(\alpha)^2 + (L-1)\hat{x}(\alpha)^2}{\hat{x}(\alpha)^2\hat{y}(\alpha)^2} \right) \\
\label{eqn:psi_alpha_simplify}&\implies \frac{\hat{y}^2}{L} + \left(1-\frac{1}{L} \right) \hat{x}^2 = \frac{\Psi(\alpha) \hat{x}^2 \hat{y}^2}{L \sigma_{\star}^2}.
\end{align}
Then, computing the differential, we have the following:
\begin{align}
   d \Psi &= \sigma^2_{\star} \left(-\frac{2 }{\hat{x}^3} d\hat{x} - \frac{2 (L-1) }{\hat{y}^3}d\hat{y} \right)  \\ 
   &=  \frac{1}{\hat{x}^3} \left[\frac{2 \alpha \sigma^2_{\star}}{\frac{L\hat{x}}{L-1} +\frac{\alpha^2}{(L-1)\hat{x}}} \right] d\alpha 
   - \left[\frac{(L-1)}{\hat{y}^3} \frac{2 \alpha \hat{y} \sigma^2_{\star}}{(\hat{y}^2L - \alpha^2 (L-1))} \right] d \alpha \tag{Substitute $d\hat{x}, d\hat{y}$} \\
   &= \left[\frac{1}{\hat{x}^4 + \left(\frac{\alpha^2}{L}\right)\hat{x}^2 } - \frac{1}{\hat{y}^4 - \alpha^2 \hat{y}^2\left(\frac{L-1}{L}\right)} \right] \cdot \frac{2\alpha(L-1)\sigma^2_{\star}}{L} d \alpha \\
    &=   \left[\frac{\hat{y}^4 - \hat{x}^4 -\alpha^2 \left(\frac{\hat{x}^2}{L} + \left(1 - \frac{1}{L} \right)\hat{y}^2  \right) }{\left(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 \right)\cdot\left(\hat{y}^4 - \alpha^2 \hat{y}\left(\frac{L-1}{L}\right) \right) } \right] \cdot \frac{2\alpha(L-1)\sigma^2_{\star}}{L} d \alpha. %\notag 
    %\\&=   \frac{\alpha^2  \left( \frac{\hat{y}^2}{L} + (1-\frac{1}{L})\hat{x}^2\right) }{\left(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 \right)\cdot\left(\hat{y}^4 - \alpha^2 \hat{y}\left(\frac{L-1}{L}\right) \right) } 2 \left(1-\frac{1}{L}\right) \sigma^2_{\star} \alpha d \alpha  
\end{align}
Then, recall the intersection constraint:
\begin{align}
\label{eqn:fourth_order}
\hat{y}^2 - \hat{x}^2 = \alpha^2 &\implies (\hat{y}^2 - \hat{x}^2)(\hat{y}^2 + \hat{x}^2) = \alpha^2(\hat{y}^2 + \hat{x}^2) \\
\label{eqn:fourth_order}
&\implies \hat{x}^4 - \hat{y}^4 = \alpha^2 \cdot (\hat{x}^2 + \hat{y}^2).
\end{align}
By substituting in Equation~(\ref{eqn:fourth_order}), we can simplify further:
\begin{align*}
    d \Psi =   \left[\frac{\alpha^2  \left( \frac{\hat{y}^2}{L} + \left(1-\frac{1}{L} \right)\hat{x}^2\right) }{(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 )(\hat{y}^4 - \alpha^2 \hat{y}^2\left(\frac{L-1}{L}\right)) } \right] \cdot \frac{2\alpha(L-1)\sigma^2_{\star}}{L} d \alpha.
\end{align*}
Now, we can plug in Equation~(\ref{eqn:psi_alpha_simplify}) into the numerator:
\begin{align*}
    d \Psi &=   \left[\frac{\alpha^2  \left( \frac{\hat{y}^2}{L} + \left(1-\frac{1}{L} \right)\hat{x}^2\right) }{(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 )(\hat{y}^4 - \alpha^2 \hat{y}^2\left(\frac{L-1}{L}\right)) } \right] \cdot \frac{2\alpha(L-1)\sigma^2_{\star}}{L} d \alpha\\
    &= \left[\frac{\alpha^2  \left( \frac{\Psi(\alpha) \hat{x}^2\hat{y}^2}{L\sigma_\star^2}\right) }{(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 )(\hat{y}^4 - \alpha^2 \hat{y}^2\left(\frac{L-1}{L}\right)) } \right] \cdot \frac{2\alpha(L-1)\sigma^2_{\star}}{L} d \alpha \\
    &= \left[\frac{\alpha^2\Psi(\alpha) \hat{x}^2\hat{y}^2}{L\sigma_\star^2(\hat{x}^4 + \frac{\alpha^2}{L}\hat{x}^2 )(\hat{y}^4 - \alpha^2 \hat{y}^2\left(\frac{L-1}{L}\right)) } \right] \cdot \frac{2\alpha(L-1)\sigma^2_{\star}}{L} d \alpha \\
    &= \left[\frac{2\Psi(\alpha)}{(\hat{x}^2 + \frac{\alpha^2}{L} )(\hat{y}^2 - \alpha^2 \left(\frac{L-1}{L}\right)) } \right] \cdot\left(\frac{1}{L} - \frac{1}{L^2} \right)\alpha^{3} \, d \alpha. 
\end{align*}
Finally, notice that from the conservation flow, we also have
\begin{align*}
     \hat{y}^2 - \hat{x}^2 = \alpha^2 \implies \hat{x}^2 + \frac{\alpha^2}{L} =\hat{y}^2 - \alpha^2 \left(\frac{L-1}{L}\right),
\end{align*}
and so
\begin{align*}
    d\Psi = \left[\frac{2\Psi(\alpha)}{(\hat{x}^2 + \frac{\alpha^2}{L})^2} \right] \cdot\left(\frac{1}{L} - \frac{1}{L^2} \right)\alpha^{3} \, d \alpha \implies \frac{d\Psi}{\Psi(\alpha)} &= \underbrace{\left[\frac{2}{(\hat{x}^2 + \frac{\alpha^2}{L})^2} \right] \cdot\left(\frac{1}{L} - \frac{1}{L^2} \right)}_{\eqqcolon P (\alpha)}\alpha^{3} \, d \alpha \\
    &= P(\alpha)\alpha^3 \, d\alpha.
\end{align*}

\paragraph{Upper Bounding the Differential.}
Note that it is difficult to directly solve for $\alpha$ from $P(\alpha)$, as $\hat{x}$ is also a function of $\alpha$. Hence, we can upper bound $P(\alpha)$ by a function $F(\alpha)$ such that $F(\alpha) \geq P(\alpha)$ for all $\alpha > 0$, and use this to solve for $\alpha$. We proceed by looking at the derivative of $P(\alpha)$:
\begin{align*}
    P'(\alpha) &= \left[\frac{-4}{(\hat{x}^2 + \frac{\alpha^2}{L})^3 } \right]   \left(\frac{1}{L} - \frac{1}{L^2}\right) \left(2\hat{x} \frac{d \hat{x}}{d \alpha} + \frac{2 \alpha}{L} \right) \\
    &= \left[\frac{-4}{(\hat{x}^2 + \frac{\alpha^2}{L})^3 } \right]   \left(\frac{1}{L} - \frac{1}{L^2}\right) \left(\frac{2 \alpha}{L} -  \frac{2\hat{x}\alpha}{\frac{L\hat{x}}{L-1} + \frac{\alpha^2}{(L-1)\hat{x}}} \right) \\
    &= \frac{8 \alpha}{(\hat{x}^2 + \frac{\alpha^2}{L})^3 }  \left(\frac{1}{L} - \frac{1}{L^2}\right) \left( \frac{L-1}{L+ \frac{\alpha^2}{\hat{x}^2}} - \frac{1}{L}\right)
\end{align*}
\paragraph{Case 1: $L=2$.} Consider the case when $L=2$. Then, notice that for all $\alpha > 0$, $P'(\alpha) < 0$. Thus, we can choose $F(\alpha)$ as such:
\begin{align*}
    F = \underset{\alpha \to 0}{\lim} \, P(\alpha) = \frac{2}{\sigma_\star^{4/L}} \left(\frac{1}{L} - \frac{1}{L^2} \right),
\end{align*}
which is constant in $\alpha$ that upper bounds $P(\alpha)$.

\paragraph{Case 2: $L>2$.} Now consider the general case. Notice that 
\begin{align*}
    \hat{x}(\alpha) = \frac{\alpha}{\sqrt{L(L-2)}}
\end{align*}
is the only critical point of $P(\alpha)$ (since $\hat{x} > 0$). Furthermore, we have
\begin{align*}
    \hat{x}(\alpha) < \frac{\alpha}{\sqrt{L(L-2)}} \implies P'(\alpha) < 0,
\end{align*}
implying that $P(\alpha)$ is decreasing. Then, since $\hat{x}(\alpha)$ is also a decreasing function in $\alpha$, this means that there exists an $\alpha_{\text{crit}}$ such that for all $\alpha > \alpha_{\text{crit}}$, $P(\alpha)$ is always decreasing. We can find 
$\alpha_{\text{crit}}$ as such:
\begin{align*}
    \hat{x}(\alpha_{\text{crit}}) = \frac{\alpha_{\text{crit}}}{\sqrt{L(L-2)}} \implies \hat{y}(\alpha_{\text{crit}}) = \alpha_{\text{crit}} \sqrt{1 + \frac{1}{L(L-2)}}.
\end{align*}
By plugging these into our constraint set, we obtain
\begin{align*}
    &\left(\frac{\alpha_{\text{crit}}}{\sqrt{L(L-2)}} \right) \left(\alpha_{\text{crit}} \sqrt{1 + \frac{1}{L(L-2)}} \right)^{L-1} = \sigma_\star \\
    &\implies \alpha_{\text{crit}}^L\left( \sqrt{1 + \frac{1}{L(L-2)}} \right)^{L-1} = \sigma_\star\sqrt{L(L-2)} \\
    &\implies \alpha_{\text{crit}}^L = \frac{\sigma_\star\sqrt{L(L-2)}}{\left( \sqrt{1 + \frac{1}{L(L-2)}} \right)^{L-1}} \\
    &\implies \alpha_{\text{crit}} = \frac{\sigma_{*}^{1/L}}{\left(\frac{1}{\sqrt{L(L-2)}} \left(1+\frac{1}{L(L-2)}\right)^{\frac{L-1}{2}}\right)^{1/L}}.
\end{align*}
Next, also note that for any $\alpha < \alpha_{\text{crit}}$, $P'(\alpha) > 0$, and so $P(\alpha)$ is increasing. Hence, $P(\alpha_{\text{crit}})$ corresponds to the maximum value of $P$. Therefore, we can choose $F = P(\alpha_{\text{crit}})$ as a constant function that upper bounds $P(\alpha)$. This leads to
\begin{align*}
    F = P(\alpha_{\text{crit}}) &= \left[\frac{2}{(\hat{x}(\alpha_{\text{crit}})^2 + \frac{\alpha_{\text{crit}}^2}{L})^2} \right] \cdot\left(\frac{1}{L} - \frac{1}{L^2} \right) \\
    &= \left[\frac{2}{\left(\frac{\alpha_{\text{crit}}^2}{L(L-2)} + \frac{\alpha_{\text{crit}}^2}{L} \right)^2} \right] \cdot\left(\frac{1}{L} - \frac{1}{L^2} \right) \\
    &= \left[\frac{2}{\left(\frac{(L-1)\alpha_{\text{crit}}^2}{L(L-2)}\right)^2} \right] \cdot\left(\frac{1}{L} - \frac{1}{L^2} \right) \\
    &= \frac{2}{\sigma_\star^{4/L}}  \cdot\underbrace{\left(\frac{1}{L} - \frac{1}{L^2} \right) \left(\frac{L(L-2)}{L-1} \right)^2 \left(\frac{1}{\sqrt{L(L-2)}} \left(1+\frac{1}{L(L-2)}\right)^{\frac{L-1}{2}}\right)^{4/L}}_{\eqqcolon h(L)} \\
    &= \frac{2h(L)}{\sigma_\star^{4/L}}.
\end{align*}

\paragraph{Combining Both Cases.} To avoid using two separate functions $F$ for different values of $L$, we can upper bound the function $h(L)$ to encompass both cases. This yields the following upper bound:
\begin{align*}
    h(L) \leq L^2 \cdot \left(\left(1 + \frac{1}{L(L-2)}\right)^{\frac{L-1}{2}}\right)^{\frac{4}{L}} \leq L^2\cdot 2^{\frac{2(L-1)}{L}} \eqqcolon g(L).
\end{align*}
Finally, we are left with the new differential
\begin{align*}
    \frac{d\Psi}{\Psi(\alpha)} = \frac{2g(L)}{\sigma_\star^{4/L}}\alpha^3 \, d\alpha.
\end{align*}



\begin{figure}[t!]
    \centering
     \begin{subfigure}[t!]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/P_alpha_L2.pdf}
     \caption*{$L=2$}
     \end{subfigure}
 \begin{subfigure}[t!]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/P_alpha_L3.pdf}
     \caption*{$L=3$}
     \end{subfigure}
 \begin{subfigure}[t!]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/P_alpha_L4.pdf}
     \caption*{$L=4$}
     \end{subfigure}
    \caption{Plot of $P(\alpha)$ along with its upper bound evaluated at $F = P(\alpha_{\text{crit}})$ for different depths. The critical point occurs exactly at the computed value of $\alpha_{\text{crit}}$ and the function $F \geq P(\alpha)$ for all $\alpha > 0$.}
    
    \label{fig:p_upper_bound}
\end{figure}


\paragraph{Finding Upper Bound on $\alpha$.} Firstly, we integrate the new differential:
\begin{align*}
    \int\frac{d\Psi}{\Psi(\alpha)} = \frac{2g(L)}{\sigma_\star^{4/L}}\int\alpha^3 \, d\alpha &\implies \mathrm{ln}\left(\frac{\Psi}{\Psi_{0}} \right)  = \frac{g(L)\alpha^4}{2\sigma_\star^{4/L}} \\
    &\implies \Psi = \Psi_{0} \exp\left( \frac{g(L)\alpha^4}{2\sigma_\star^{4/L}} \right),
\end{align*} 
where $\Psi_{0} = \underset{\alpha \to 0}{\lim} \, \Psi(\alpha) = L\sigma^{2-\frac{2}{L}}_{\star} $. Now, we can solve for $\alpha$:
\begin{align*}
    L\sigma^{2-\frac{2}{L}}_{\star}\exp\left( \frac{g(L)\alpha^4}{2\sigma_\star^{4/L}} \right) < \frac{2\sqrt{2}}{\eta} &\implies \exp\left( \frac{g(L)\alpha^4}{2\sigma_\star^{4/L}} \right) < \frac{2\sqrt{2}}{\eta L\sigma^{2-\frac{2}{L}}_{\star}} \\
    &\implies \alpha < \left( \ln\left( \frac{\frac{2\sqrt{2}}{\eta}}{L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{2 \sigma_{\star}^{4/L}}{g(L)} \right)^{1/4} \\
    &\implies \alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{2 \sigma_{\star}^{4/L}}{L^2 \cdot 2^{\frac{2(L-1)}{L}}} \right)^{1/4} 
\end{align*}
Simplifying further, we obtain 
\begin{align*}
    \alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{ \sigma_{\star}^{4/L}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{1/4}, 
\end{align*}
which gives us the desired bound. This completes the proof.
\end{proof}


\begin{lemma}
\label{GFS-1}
    Let $\pi(\mbf{s}) \coloneqq \prod_{\ell=1}^L \sigma_\ell$ denote the end-to-end product of $\mbf{s} \in \mbb{R}^L$ and suppose that each $\sigma_\ell > 0$.
    If the GFS sharpness $\psi(\mbf{s}) \leq \frac{2\sqrt{1+c}}{\eta}$ for some $c \in (0, 1]$, then  
    \begin{align*}
        \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\mbf{s})-\sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{[L-i]}^2 \sigma_{[D]}^2} \leq 1+c.
    \end{align*}
\end{lemma}

\begin{proof}

We consider two cases: (i) $\pi(\mbf{s}) \in [0, \sigma_\star)$ and (ii) $\pi(\mbf{s}) > \sigma_\star$. Note that we ignore the case of $\pi(\mbf{s}) = \sigma_\star$ as this occurs with probability zero at EoS.
%\footnote{We ignore the case $\pi(\mbf{s}(t)) =\sigma_{\star}$ when we get $b_{i,j}(t+1) = b_{i,j}(t)$. Since the occurence $\pi(\mbf{s}(t)) =\sigma_{\star}$ holds with a probability of zero. } .\\

\paragraph{Case 1 $(\pi(\mbf{s}) \in [0, \sigma_\star))$.} For this case, notice that we have
\[
\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star} )^2 \pi^2(\mbf{s})}{\sigma_{L-i}^2 \sigma_{L}^2 } 
\leq \frac{\eta^2 \pi^2(\mbf{s})}{\sigma_{L-i}^2 \sigma_{L}^2}. \tag{$\pi(\mbf{s}) < \sigma_\star$}
\]
Then, note that the GFS sharpness is constant for all weights on the GF trajectory, as it is defined to be the sharpness at the limit of the GF trajectory (i.e., the GFS). 
Hence, we can focus on the weights at the solution, or global minima.

Define the GFS as $\mbf{z} \coloneqq S_{\mathrm{GF}}(\mbf{s})$. By Lemma~\ref{gf-unbalanced}, each coordinate in $\mbf{z} \in \mbb{R}^L$ (and hence $\mbf{s} \in \mbb{R}^L$) is balanced across layers under GF, and so we have that
\begin{align*}
    \sigma^2_{\ell} - \sigma^2_{m} = z^2_{\ell} - z^2_{m} \quad\quad \forall \ell, m \in [L].
\end{align*}
Hence, it is suffices to show that 
\begin{align*}
    \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \pi(\mbf{z})^2}{z_{L-i}^2 z_{L}^2} \leq 1+c \implies\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \pi^2(\mbf{s})}{\sigma_{L-i}^2 \sigma_{L}^2} \leq 1+c.
\end{align*}
Then, note that $\pi(\mbf{z})=\sigma_{\star}$, since it lies on the global minima, and so we have
\begin{align}
    \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \pi^2(\mbf{z})}{z^{2}_{L-i} z^2_{L}} 
= \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \sigma^2_{\star}}{z^{2}_{L-i} z^2_{L}}.
\end{align}
From Lemma~\ref{1d-sharp}, the sharpness at the global minima is given as
\begin{align}
\label{eqn:helper1}
\psi(\mbf{s})=\left\| \nabla^2 \mathcal{L}(\mbf{z}) \right\| = \sum_{i=1}^{L} \frac{ \sigma^2_{\star}}{z_i^2}.
\end{align}
This immediately implies that \(\frac{\sigma^2_{\star}}{z^2_{L}} \leq \psi(\mbf{s})\) and equivalently, \(\exists \beta \in [0,1]\) such that $\frac{\sigma^2_{\star}}{z^2_{L}} = \beta \psi(\mbf{s})$.
Therefore, we have
\begin{align}
\label{eqn:helper2}
    \sum_{i=1}^{\min\{2,L-1\}} \frac{\sigma^2_{\star}}{z^2_{L-i}} \leq (1 - \beta) \psi(\mbf{s}).
\end{align}
Substituting Equations~(\ref{eqn:helper1}) and~(\ref{eqn:helper2}) into the expression we aim to bound, we obtain

\[
\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\mbf{s}) - \sigma^2_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-i}^2 \sigma_{L}^2}
= \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 \sigma^2_{\star}}{z^{2}_{L-i} z^2_{L}} 
\leq \eta^2 \beta (1 - \beta) \psi^2(\mbf{s}) \leq \frac{\eta^2}{4} \psi^2(\mbf{s}) \leq 1+c,
\]
where we used the fact that the maximum of $\beta(1-\beta) $ is $\frac{1}{4}$ when \(\beta = \frac{1}{2}\) and \(\psi(\mbf{s}) \leq \frac{2\sqrt{1+c}}{\eta}\).
Thus, if \(\psi(\mbf{s}) \leq \frac{2\sqrt{1+c}}{\eta}\), then for every weight $\mbf{s} \in \mbb{R}^L$ lying on its GF trajectory, we have
\begin{align*}
    \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-i}^2 \sigma_{L}^2} \leq 1+c.
\end{align*}


\paragraph{Case 2 $(\pi(\mbf{s}) > \sigma_{\star})$.} Consider the case in which $\pi(\mbf{s}) > \sigma_{\star}$. 
%We already have that $\sigma>0$ throughout the trajectory (refer to Lemma 3.11 in \cite{kreisler2023gradient}) and so  $\pi(\mbf{s})>0$. 
By assumption, note that we have $\sigma_i > 0$, which implies that each GD update will also remain positive:
\begin{align*}
    \sigma_{i}-\eta (\pi(\mbf{s}) - \sigma_{\star})\pi(\mbf{s}) \frac{1}{\sigma_{i}} >0.
\end{align*}
From this, we get 
\begin{align*}
  2 >  \frac{\eta (\pi(\mbf{s}) - \sigma_{\star})\pi(\mbf{s})}{\sigma^2_{i}}>0,
\end{align*}
This implies that
$$\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-i}^2 \sigma_{L}^2} \leq (1+c),$$
with $c=1$. 

Putting both cases together, we have that 
$$\sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-i}^2 \sigma_{L}^2} \leq (1+c),$$
for $c \in (0, 1]$, which completes the proof.

\end{proof}



\subsubsection{Proof of Proposition~\ref{prop:balancing}}
\label{sec:proof_of_balancing}

\begin{proof}
    Consider the $i$-th index of the simplified loss in~(\ref{eqn:simplified_loss}):
\begin{align*}
    \frac{1}{2} \left(\prod_{\ell=1}^L \sigma_{\ell, i} - \sigma_{\star, i}  \right)^2 \eqqcolon \frac{1}{2} \left(\prod_{\ell=1}^L \sigma_{\ell} - \sigma_{\star}  \right)^2  ,
\end{align*}
and omit the dependency on $i$ for ease of exposition. Our goal is to show that the $L$-th singular value $\sigma_L$ initialized to zero become increasingly balanced to $\sigma_\ell$ which are initialized to $\alpha > 0$.
%So, first we show that in a singular value scalar loss, for a learning rate $\eta > \frac{2}{S_{i}}$, oscillations occur provably for any initialization range $0<\alpha$. We do this in the following steps: 1) show that the sharpness around the global minima is the least when iterates are balanced, 2) By using the descent lemma, if $\eta>\frac{2}{S_{i}}$, then oscillations start occuring at the balanced point, 3) By lemma-1, if oscillations can occur at balanced point, then oscillation can occur anywhere around the global minima. So, $\eta>\frac{2}{S_{i}}$ is a sufficient condition to show oscillation. \footnote{Oscillations can also start occuring for some $\eta<\frac{2}{S_{i}}$ but will eventually subside once it reaches close to balanced minima. }
To that end, let us define the balancing dynamics between $\sigma_i$ and $\sigma_j$ as $b_{i,j}(t+1) \coloneqq \left(\sigma_i^{(t+1)}\right)^2 - \left(\sigma_j^{(t+1)}\right)^2$ and $\pi(\mbf{s}(t)) \coloneqq \prod_{\ell=1}^L \sigma_{\ell}(t)$ for the product of singular values at iteration $t$.
Then, we can simplify the balancing dynamics as such:
% \begin{align*}
%     \pi^{(t+1)} = \prod_{i=1}^{L} \sigma_{i}^{(t+1)} 
%     &= \prod_{i=1}^{D} \left(\sigma_i(t) - \eta \left(\pi(\mbf{s}(t)) - 1 \right)\frac{\pi(\mbf{s}(t))}{\sigma_i(t)} \right) \\
%     &= \pi(\mbf{s}(t)) \prod_{i=1}^{L} \left(1 - \eta \left(\pi(\mbf{s}(t)) - 1 \right) \frac{\pi(\mbf{s}(t))}{\left(\sigma_i(t)\right)^2}\right) \\
%     &= \pi(\mbf{s}(t)) + \sum_{m=1}^{L} \eta^m (1 - \pi(\mbf{s}(t)))^m \pi(\mbf{s}(t)) \sigma_m(\sigma^{(t)}).
% \end{align*}

% Defining  to be the balance,
% The dynamic of the balances then becomes:
\begin{align}
    b_{i,j}(t+1) &= \left(\sigma_i(t+1)\right)^2 - \left(\sigma_j(t+1)\right)^2 \\
    &= \left(\sigma_i(t) - \eta\left(\pi(\mbf{s}(t)) - \sigma_{\star}\right)\frac{\pi(\mbf{s}(t))}{\sigma_i(t)}\right)^2 - \left(\sigma_j(t) - \eta\left(\pi(\mbf{s}(t)) - \sigma_{\star}\right)\frac{\pi(\mbf{s}(t))}{\sigma_j(t)}\right)^2 \\
    &= \left(\sigma_i(t)\right)^2 - \left(\sigma_j(t)\right)^2 + \eta^2 \left(\pi(\mbf{s}(t)) - \sigma_{\star}\right)^2 \left(\frac{\pi^2(\mbf{s}(t))}{\left(\sigma_i(t)\right)^2} - \frac{\pi^2(\mbf{s}(t))}{\left(\sigma_j(t)\right)^2}\right) \\
    &= \left(\left(\sigma_i(t)\right)^2 - \left(\sigma_j(t)\right)^2 \right) \left( 1 - \eta^2 (\pi(\mbf{s}(t)) - \sigma_{\star})^2 \frac{\pi^2(\mbf{s}(t))}{\left(\sigma_i(t)\right)^2 \left(\sigma_j(t)\right)^2} \right) \\
    \label{eqn:simplified_balanced}
    &= b_{i,j}(t) \left( 1 - \eta^2 (\pi(\mbf{s}(t)) - \sigma_{\star})^2 \frac{\pi^2(\mbf{s}(t))}{\left(\sigma_i(t)\right)^2 \left(\sigma_j(t)\right)^2} \right).
\end{align}
Then, in order to show that $ \left|b_{i,j}(t+1)\right| < c\left|b_{i,j}(t)\right|$ for some $0<c \leq 1$, we need to prove that
\begin{align*}
    \left | 1 - \eta^2 (\pi(\mbf{s}(t)) -\sigma_{\star} )^2 \frac{\pi^2(\mbf{s}(t))}{\left(\sigma_i(t)\right)^2 \left(\sigma_j(t)\right)^2} \right| < c,
\end{align*}
 for all iterations $t$. Note that for our case, it is sufficient to show the result for $i=L$ and $j=\ell$ for any $\ell \neq L$. WLOG, suppose that the $\sigma$ are sorted such that $\sigma_1 \geq \sigma_2 \geq \ldots \geq \sigma_L$. By assumption, since our initialization scale satisfies
\begin{align*}
    0<\alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star}^{2 - \frac{2}{L}}} \right) \cdot \frac{ \sigma_{\star}^{4/L}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{1/4},
\end{align*}
by Lemma~\ref{GFS-3}, we have that the GFS sharpness $\psi(\cdot)$ for positive $\mbf{s} = \begin{bmatrix}
    \sigma_1 & \ldots & \sigma_L
\end{bmatrix} \in \mbb{R}^L$ (i.e., each element $\sigma_\ell > 0$) satisfies $\psi(\mbf{s}) < \frac{2\sqrt{2}}{\eta}$. Then, by Lemma~\ref{GFS-1}, we have
\begin{align}
\label{eqn:conseq_gfs1}
    \sum_{i=1}^{\min\{2,L-1\}} \frac{\eta^2 (\pi(\mbf{s})-\sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{[L-i]}^2 \sigma_{[D]}^2} \leq 1+c,
\end{align}
for some $c \in [0, 1)$. Then, notice that Equation~(\ref{eqn:conseq_gfs1}) implies that \begin{align}
        \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-1}^2 \sigma_{L}^2} < 1+c \quad \text{and} \quad 
        \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{i}^2 \sigma_{j}^2} < \frac{1+c}{2},
\end{align}
for all $i \in [L]$, $j \in [L-2] $ and $ i < j$. Notice that the latter inequality comes from the fact that 
\begin{align*}
    \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-2}^2 \sigma_{L}^2} + \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-2}^2 \sigma_{L}^2} &< \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-1}^2 \sigma_{L}^2} + \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-2}^2 \sigma_{L}^2} \\
    &< 1+c,
\end{align*}
which implies that
\begin{align*}
    2\frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-2}^2 \sigma_{L}^2} < 1+c \implies \frac{\eta^2 (\pi(\mbf{s}) - \sigma_{\star})^2 \pi^2(\mbf{s})}{\sigma_{L-2}^2 \sigma_{L}^2} < \frac{1+c}{2},
\end{align*}
and since $\sigma$ are sorted, it holds for all other $\sigma$.
Therefore from Equation~(\ref{eqn:simplified_balanced}), we have for all $i \in [L-2]$,
\begin{align*}
    b_{i,i+1}(t+1) < c \cdot b_{i,i+1}(t) \quad \text{and} \quad b(t+1)_{L-2,L} < c \cdot b_{L-2,L}(t),
\end{align*}
as well as 
\begin{align*}
    -c \cdot b_{L-1,L} (t) <b_{L-1,L}(t+1) < c \cdot b_{L-1,L}(t).
\end{align*}
%Now, we prove balancing considering two cases where $b(t+1)_{L-1,L} \geq 0 $ and $b(t+1)_{L-1,L} \leq 0 $. 
%First, consider the case $b(t+1)_{L-1,L} \geq 0 $ then using the fact for all $i \in [L-2]$,
%$b(t+1)_{i,i+1} <  b(t)_{i,i+1}$, $b(t+1)_{i,i+1} <  b(t)_{i,i+1}$.
Then, notice that since we initialized all of the singular values $\sigma_\ell$ for $\ell \in [L-1]$ to be the same, they follow the same dynamics. Since we already showed that $|b_{L-1,L}(t+1)| < c \cdot|b_{L-1,L}(t)|$, it must follow that
\begin{align*}
    \left|b_{i,j}(t+1)\right| < c \cdot \left|b_{i,j}(t)\right|, \quad \forall \, i,j \in [L].
\end{align*}
This completes the proof.


\end{proof}




\subsection{Proofs for Periodic Orbits}

Before presenting our proof for Theorem~\ref{thm:align_thm}, we first show that the required condition from~\cite{chen2023edge} for stable oscillations to occur (see Lemma~\ref{lemma:chen-bruna}) is also satisfied for DLNs beyond the EOS, as shown in Appendix~\ref{subsec:supporting_lemmas_orbits}.

\subsubsection{Supporting Lemmas}
\label{subsec:supporting_lemmas_orbits}

\begin{lemma}
[Stable Subspace Oscillations]

Define $S_p\coloneqq L \sigma^{2-\frac{2}{L}}_{\star,p}$ and $K'_p \coloneqq \mathrm{max} \left\{ S_{p+1},\frac{S_p}{2\sqrt{2}}\right\}$.
%If we run GD on the deep matrix factorization loss with initialization scale $\alpha < \alpha'$ from Theorem~\ref{prop:balancing} and learning rate $\eta = \frac{2}{K}$, where $K'_p < K< S_p$, then at the steady state limit $t \rightarrow \infty$, 
If we run GD on the deep matrix factorization loss in~(\ref{eqn:deep_mf}) with learning rate $\eta = \frac{2}{K}$, where $K'_p < K< S_p$, then $2$-period orbit oscillation occurs in the direction of $\Delta_{S_p}$, where $\Delta_{S_p}$ denotes the eigenvector associated with the eigenvalue $S_p$ of the Hessian at the balanced minimum.
    \label{thm:stable_sub}
\end{lemma}
\begin{proof}

Define $f_{\Delta_i}$ as the 1-D function at the cross section of the loss landscape and the line
following the direction of $\Delta_i$ passing the (balanced) minima, where $\Delta_i$ is the $i$-th eigenvector of the training loss Hessian at the balanced minimum. To prove the result, we will invoke Lemma~\ref{lemma:chen-bruna}, which states that two-period orbit oscillation occurs in the direction of $\Delta_i$ if the minima of $f_{\Delta_i}$ satisfies $f_{\Delta_i}^{(3)}>0$ and $3[f_{\Delta_i}^{(3)}]^2 - f_{\Delta_i}^{(2)}f_{\Delta_i}^{(4)} > 0$, for $\eta>\frac{2}{\lambda_{i}}$. We show that while the condition holds for all of the eigenvector directions, the oscillations can only occur specifically in the directions of $\Delta_{S_i}$.
%Recall that the initialization condition is an artifact of Proposition~\ref{prop:balancing}, which allows us to consider the balanced minimum. 


First, we will derive the eigenvectors of the Hessian of the training loss at convergence (i.e., $\mbf{M}_\star = \mbf{W}_{L:1}$).
    To obtain the eigenvectors of the Hessian of parameters $(\mbf{W}_L, \ldots, \mbf{W}_2, \mbf{W}_1)$, consider a small perturbation of the parameters:
    \begin{align*}
        \mbf{\Theta} \coloneqq \left(\Delta \mbf{W}_\ell +  \mbf{W}_\ell \right)_{\ell=1}^L =  (\mbf{W}_L + \Delta \mbf{W}_L, \ldots, \mbf{W}_2+ \Delta \mbf{W}_2, \mbf{W}_1+ \Delta \mbf{W}_1).
    \end{align*}

    
    Given that $\mbf{W}_{L:1} = \mbf{M}_\star$, consider and evaluate the loss function at this minima: 
    \begin{align}
        \mathcal{L}(\mbf{\Theta}) = \frac{1}{2} \biggl\| &\sum_{\ell} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \\
        &+ \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1}   +   \ldots   +  \Delta \mbf{W}_{L:1}\biggr\|^2_{\mathsf{F}}.
    \end{align}
    By expanding each of the terms and splitting by the orders of $\Delta \mbf{W}_\ell$ (perturbation), we get that the second-order term is equivalent to
    \begin{align*}
        \Theta&\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^2\right): \,\, \frac{1}{2} \biggl\| \sum_{\ell} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \biggr\|^2_{\mathsf{F}} \\
        \Theta&\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^3\right): \,\, \mathrm{tr}\left[\left(\sum_{\ell} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left( \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right)\right] \\
        \Theta&\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^4\right): \,\, \frac{1}{2} \| \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1}\|^2_{\mathsf{F}}\\
        &+ \mathrm{tr}\left[\sum_{l} \left(\mbf{W}_{L:\ell+1}\Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left(\sum_{l<m<p} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:p+1} \Delta \mbf{W}_{p} \mbf{W}_{p-1:1} \right)\right]
    \end{align*}

The direction of the steepest change in the loss at the minima correspond to the largest eigenvector direction of the Hessian. Since higher order terms such as $ \Theta\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^3\right)$ are insignifcant compared to the second order terms $  \Theta\left(\sum_{\ell=1}^L\|\Delta \mbf{W}_\ell\|^2\right)$, finding the direction that maximizes the second order term leads to finding the eigenvector of the Hessian.
    Then, the eigenvector corresponding to the maximum eigenvalue of  $\nabla^2 \mathcal{L}$ is the solution of 
    \begin{align}
        \Delta_{1} \coloneqq \mathrm{vec}(\Delta \mbf{W}_L, \ldots \Delta \mbf{W}_1) = \underset{\|\Delta \mbf{W}_L\|^2_{\mathsf{F}} + \ldots + \|\Delta \mbf{W}_1\|^2_{\mathsf{F}} = 1}{\mathrm{arg max}} \, f\left(\Delta \mbf{W}_L, \ldots, \Delta \mbf{W}_1 \right),\label{max-eig}
    \end{align}
    where 
    \begin{align}
        f(\Delta \mbf{W}_L, \ldots, \Delta \mbf{W}_1) \coloneqq \frac{1}{2} \|\Delta \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta \mbf{W}_1\|^2_{\mathsf{F}}.
    \end{align}

While the solution of Equation~(\ref{max-eig}) gives the maximum eigenvector direction of the Hessian, $\Delta_{1}$, the other eigenvectors can be found by solving
\begin{align}
    \Delta_{r} \coloneqq  \underset{
    \substack{
    \|\Delta \mbf{W}_L\|^2_{\mathsf{F}} + \ldots + \|\Delta \mbf{W}_1\|^2_{\mathsf{F}} = 1, \\
    \Delta_{r}\perp \Delta_{r-1},.., \Delta_{r}\perp \Delta_{1}
    }
    }{\mathrm{argmax}} \, f\left(\Delta \mbf{W}_L, \ldots, \Delta \mbf{W}_1 \right).\label{other-eig}
\end{align}

    By expanding $f(\cdot)$, %subject to the constraint ${\|\Delta \mbf{W}_L\|^2_{\mathsf{F}}} + \ldots + \|\Delta \mbf{W}_1\|^2_{\mathsf{F}} = 1$ ,
    we have that % we don't expand subject to a constraint
    \begin{align}
        f(\Delta \mbf{W}_L, &\ldots, \Delta \mbf{W}_1) = \|\Delta\mbf{W}_L \mbf{W}_{L-1:1}\|^2_{\mathsf{F}} +\ldots+ \|\mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1}\|^2_{\mathsf{F}}  + \|\mbf{W}_{L:2} \Delta \mbf{W}_1\|^2_{\mathsf{F}} \notag \\
        &+ \mathrm{tr}\left[\left(\Delta\mbf{W}_L \mbf{W}_{L-1:1} \right)^\top \left(\mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} + \ldots +\mbf{W}_{L:2} \Delta \mbf{W}_1 \right)\right] + \ldots + \notag \\
        &\mathrm{tr}\left[\left(\mbf{W}_{L:2} \Delta \mbf{W}_1\right)^\top \left(\mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} + \ldots +\mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} \right)\right].     \label{expansion}
    \end{align}

We can solve Equation~(\ref{max-eig}) by maximizing each of the terms, which can be done in two steps:
\begin{enumerate}[label=(\roman*)]
\item 
Each Frobenius term in the expansion is maximized when the left singular vector of $\Delta \mbf{W}_{\ell}$ aligns with $\mbf{W}_{L:\ell+1}$ and the right singular vector aligns with $\mbf{W}_{\ell-1:1}$. This is a result of Von Neumann's trace inequality~\citep{mirsky1975trace}. Similarly, each term in the trace is maximized when the singular vector of the perturbations align with the products. 
\item Due to the alignment, Equation~(\ref{max-eig}) can be written in just the singular values. Let $\Delta s_{\ell, i}$ denote the $i$-th singular value of the perturbation matrix $\Delta\mbf{W}_\ell$. Recall that all of the singular values of $\mbf{M}_\star$ are distinct (i.e., $\sigma_{\star, 1} > \ldots>\sigma_{\star, r}$). Hence, it is easy to see that
Equation~(\ref{max-eig}) is maximized when $\Delta s_{\ell,i} = 0$ (i.e, all the weight goes to $\Delta s_{\ell,1}$). Thus, each perturbation matrix must be rank-$1$.
\end{enumerate}
Now since each perturbation is rank-$1$, we can write each perturbation as 
    \begin{align}
        \Delta\mbf{W}_{\ell} = \Delta s_{\ell} \Delta \mbf{u}_{\ell} \Delta\mbf{v}_\ell^\top, \quad \forall \ell \in [L],
    \end{align}
    % Then, notice that since the top-$r$ singular values of $\mbf{M}_\star$ are unique (i.e., $\sigma_{\star, 1} > \ldots > \sigma_{\star, r}$), the singular values of each $\mbf{W}_{\ell}$ are also unique, and hence the solution for each $\Delta\mbf{W}_{\ell}$ will be rank-$1$. We can express each solution as \
    for $\Delta s_{\ell} > 0$ and orthonormal vectors $\Delta \mbf{u}_{\ell} \in \mbb{R}^d$ and  $\Delta \mbf{v}_{\ell} \in \mbb{R}^d$ with $\sum_{\ell=1}^L \Delta s^2_{\ell} = 1$.
    Plugging this in each term, we obtain:
   \begin{align*}
        \|\mbf{W}_{L:\ell+1} \Delta_1 \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|_2^2 = \Delta_1 s_\ell^2\cdot \biggl\|\underbrace{\mbf{V}_\star \mbf{\sigma}^{\frac{L-\ell}{L}}_\star \mbf{V}^\top_\star \Delta \mbf{u}_\ell}_{\eqqcolon \mbf{a}}\underbrace{ \Delta \mbf{v}_\ell^\top \mbf{V}_\star \mbf{\sigma}^{\frac{\ell-1}{L}}_\star \mbf{V}^\top_\star}_{\eqqcolon \mbf{b}^\top}\biggr\|_2^2.
    \end{align*}

    Since alignment maximizes this expression as discussed in first point, we have:
        % We first derive the leading eigenvector of the Hessian, denoted by $\Delta_1$. This makes repeated use of Von-Neumann trace inequality~, in that $\Delta \mbf{W}_\ell$ will have their singular vectors align.
    

    % To maximize $f(\cdot)$, we can find $\Delta \mbf{W}_\ell$ that maximizes each term in its expansion. Consider the Frobenius norm terms (e.g., $\|\mbf{W}_{L:\ell+1} \Delta \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|^2_{\mathsf{F}}$). Since each $\Delta\mbf{W}_\ell$ solution is rank-$1$, it follows that each one of the matrices within the norm must also be at most rank-$1$. Then, we have
    % \begin{align*}
    %     \|\mbf{W}_{L:\ell+1} \Delta_1 \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|^2_{\mathsf{F}} = \|\mbf{W}_{L:\ell+1} \Delta_1 \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|^2_{2} &\leq \|\mbf{W}_{L:\ell+1}\|_2^2 \cdot \|\Delta_1 \mbf{W}_{\ell}\|_2^2 \cdot \|\mbf{W}_{\ell-1:1}\|_2^2 \\
    %     &= \Delta_1 s_{\ell}^{2} \cdot \sigma_{\star, 1}^{2 - \frac{2}{L}}.
    % \end{align*}
    % To obtain this upper bound, consider the following:
    % \begin{align*}
    %     \|\mbf{W}_{L:\ell+1} \Delta_1 \mbf{W}_{\ell} \mbf{W}_{\ell-1:1}\|_2^2 = \Delta_1 s_\ell^2\cdot \biggl\|\underbrace{\mbf{V}_\star \mbf{\sigma}^{\frac{L-\ell}{L}}_\star \mbf{V}^\top_\star\mbf{u}_\ell}_{\eqqcolon \mbf{a}}\underbrace{\mbf{v}_\ell^\top \mbf{V}_\star \mbf{\sigma}^{\frac{\ell-1}{L}}_\star \mbf{V}^\top_\star}_{\eqqcolon \mbf{b}^\top}\biggr\|_2^2.
    % \end{align*}
     $\mbf{u}_\ell =\mbf{v}_\ell = \mbf{v}_{\star, 1}$ for all $\ell \in [2, L-1]$, then
    \begin{align*}
        \mbf{a} = \sigma_{\star, 1}^{\frac{L-\ell}{L}}\mbf{v}_{\star, 1} \quad \text{and} \quad \mbf{b}^\top = \sigma_{\star, 1}^{\frac{\ell - 1}{L}}\mbf{v}_{\star, 1}^\top \implies \mbf{ab}^\top = \sigma_{\star, 1}^{1 - \frac{1}{L}} \cdot \mbf{v}_{\star, 1}\mbf{v}_{\star, 1}^\top.
    \end{align*}
    The very same argument can be made for the trace terms.
    Hence, in order to maximize $f(\cdot)$, we must have
    \begin{align*}
        \mbf{v}_L &= \mbf{v}_{\star, 1}, \quad \text{and} \quad \mbf{u}_1 = \mbf{v}_{\star, 1}, \\
        \mbf{u}_\ell &= \mbf{v}_\ell = \mbf{v}_{\star, 1}, \quad \forall \ell \in [2, L-1].
    \end{align*}
    To determine $\mbf{u}_L$ and $\mbf{v}_1$, we can look at one of the trace terms:
    \begin{align*}
    \mathrm{tr}\left[\left(\Delta_1\mbf{W}_L \mbf{W}_{L-1:1} \right)^\top \left(\mbf{W}_{L:3}\Delta_1 \mbf{W}_2 \mbf{W}_{1} + \ldots +\mbf{W}_{L:2} \Delta_1 \mbf{W}_1 \right)\right] \leq \left(\frac{L-1}{L} \right)\cdot\sigma_{\star, 1}^{2 - \frac{2}{L}}.
    \end{align*}
    To reach the upper bound, we require $\mbf{u}_L = \mbf{u}_{\star, 1}$ and $\mbf{v}_1 = \mbf{v}_{\star, 1}$. Finally, as the for each index, the singular values are balanced, we will have  $\Delta_1 s_{\ell} = \frac{1}{\sqrt{L}}$ for all $\ell \in [L]$ to satisfy the constraint. Finally, we get that the leading eigenvector is
    \begin{align*}
        \Delta_1 \coloneqq \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_1 \mbf{v}_1^\top, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top \right).
    \end{align*}
    Notice that we can also verify that $f(\Delta_1) = L\sigma_{\star, 1}^{2- \frac{2}{L}}$, which is the leading eigenvalue (or sharpness) derived in Lemma~\ref{lemma:hessian_eigvals}.  

    To derive the remaining eigenvectors, we need to find all of the vectors in which $\Delta_i^\top \Delta_j = 0$ for $i\neq j$, where
    \begin{align*}
        \Delta_i = \mathrm{vec}(\Delta_i \mbf{W}_L, \ldots \Delta_i \mbf{W}_1),
    \end{align*}
    and $f(\Delta_i) = \lambda_i$, where $\lambda_i$ is the $i$-th largest eigenvalue. By repeating the same process as above, we find that the eigenvector-eigenvalue pair as follows:
    \begin{align*}
        \Delta_1 &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_1 \mbf{v}_1^\top, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top \right) , \quad\lambda_{1} =  L\sigma_{\star, 1}^{2- \frac{2}{L}} \\
        \Delta_2 &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_1 \mbf{v}_2^\top, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_2^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_2^\top \right), \quad\lambda_{2} =  \left(\sum_{i=0}^{L-1} \sigma_{\star, 1}^{1-\frac{1}{L}-\frac{1}{L}i} \cdot \sigma_{\star, 2}^{\frac{1}{L}i} \right) \\
        \Delta_3 &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_2 \mbf{v}_1^\top, \frac{1}{\sqrt{L}}\mbf{v}_2 \mbf{v}_1^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_2 \mbf{v}_1^\top \right), \quad\lambda_{3} =  \left(\sum_{i=0}^{L-1} \sigma_{\star, 1}^{1-\frac{1}{L}-\frac{1}{L}i} \cdot \sigma_{\star, 2}^{\frac{1}{L}i} \right) \\
        \quad\quad\quad&\vdots \\
         \Delta_d &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_2 \mbf{v}_2^\top, \frac{1}{\sqrt{L}}\mbf{v}_2 \mbf{v}_2^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_2 \mbf{v}_2^\top \right), \quad\lambda_{d} =  L\sigma_{\star, 2}^{2- \frac{2}{L}} \\     
        \quad\quad\quad&\vdots \\
        \Delta_{dr+r} &= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_d \mbf{v}_r^\top, \frac{1}{\sqrt{L}}\mbf{v}_d \mbf{v}_r^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_d \mbf{v}_r^\top \right),
    \end{align*}
    which gives a total of $dr + r$ eigenvectors.

    Second, equipped with the eigenvectors, let us consider the 1-D function $f_{\Delta_i}$ generated by the cross-section of the loss landscape and each eigenvector $\Delta_i$ passing the minima:
    \begin{align*}
        f_{\Delta_i}(\mu) &= \mathcal{L}(\mbf{W}_L + \mu\Delta \mbf{W}_L, \ldots, \mbf{W}_2+ \mu\Delta \mbf{W}_2, \mbf{W}_1+ \mu\Delta \mbf{W}_1), \\
        &= \mu^2\cdot \frac{1}{2} \|\Delta \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta \mbf{W}_1\|^2_{\mathsf{F}}\\
        \quad&+\mu^3 \cdot \sum_{\ell=1, \ell< m}^L\mathrm{tr}\left[\left(\mbf{W}_{L:\ell+1}\Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left( \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right)\right] \\
        \quad&+\mu^4\cdot \frac{1}{2} \left\|  \left( \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right) \right\|^2_{\mathsf{F}}\\
        &+ \mu^4 \cdot\sum_{\ell<m<p} ^L \mathrm{tr}\left[\left(\mbf{W}_{L:\ell+1}\Delta \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top  \left(\mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:p+1} \Delta \mbf{W}_{p} \mbf{W}_{p-1:1} \right)\right].
    \end{align*}
    Then, the several order derivatives of $f_{\Delta_i}(\mu)$ at $\mu = 0$ can be obtained from Taylor expansion as
    \begin{align*}
        f_{\Delta_i}^{(2)}(0) &= \|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} = \lambda^2_{i}\\
        f_{\Delta_i}^{(3)}(0) &= 6\sum_{\ell=1}^L\mathrm{tr}\left[\left(\mbf{W}_{L:\ell+1}\Delta_i \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left(\mbf{W}_{L:\ell+2}\Delta_i \mbf{W}_{\ell+1} \mbf{W}_\ell\Delta_i\mbf{W}_{\ell-1} \mbf{W}_{\ell-2:1} \right)\right] \\
        & = 6 \biggl\| \sum_{\ell} \mbf{W}_{L:\ell+1}\Delta_i \mbf{W}_\ell \mbf{W}_{\ell-1:1} \biggr\|_{\mathsf{F}}\cdot \biggl\| \left( \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right) \biggr\|_{\mathsf{F}}\\
        & \coloneqq 6  \lambda_{i}\cdot\beta_{i} \\
        f_{\Delta_i}^{(4)}(0) &= 12\|\Delta_i \mbf{W}_L \Delta_i\mbf{W}_{L-1}\mbf{W}_{L-2:1} + \ldots + \mbf{W}_{L:4}\Delta_i \mbf{W}_3 \mbf{W}_{2}\Delta_i\mbf{W}_1 + \mbf{W}_{L:3}\Delta_i\mbf{W}_2 \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
        &+ 24\sum_{\ell=1}^L \mathrm{tr}\left[\left(\mbf{W}_{L:\ell+1}\Delta_i \mbf{W}_\ell \mbf{W}_{\ell-1:1} \right)^\top \left(\sum_{l<m<p} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:p+1} \Delta \mbf{W}_{p} \mbf{W}_{p-1:1} \right)\right] \\
        &\coloneqq 12\beta^2_{i} + 24\lambda_{i}\cdot\delta_{i},
    \end{align*}
  
    % \begin{align*}
    %     3[f_{\Delta_i}^{(3)}]^2 - f_{\Delta_i}^{(2)}f_{\Delta_i}^{(4)} &= 108\beta\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
    %     &- 12\beta\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
    %     &- 24\gamma\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
    %     &= 96\beta\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} \\
    %     &- 24\gamma\|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}}.
    % \end{align*}

    where we defined 
    \begin{align*}
        & \lambda_{i} = \biggl\| \sum_{\ell} \mbf{W}_{L:\ell+1} \Delta_{i} \mbf{W}_\ell \mbf{W}_{\ell-1:1}  \biggr\|_{\mathsf{F}} \quad \tag{Total $L\choose 1$ terms}\\
        & \beta_{i} =\biggl\| \left( \sum_{\ell<m} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:1} \right)\biggr\|_{\mathsf{F}} \quad \tag{Total $L\choose 2$ terms}\\
        & \delta_{i} = \biggl\| \left(\sum_{l<m<p} \mbf{W}_{L:\ell+1} \Delta \mbf{W}_\ell \mbf{W}_{\ell-1:m+1} \Delta \mbf{W}_{m} \mbf{W}_{m-1:p+1} \Delta \mbf{W}_{p} \mbf{W}_{p-1:1} \right) \biggr\|_{\mathsf{F}}, \quad \tag{Total $L\choose 3$ terms}
    \end{align*}
and used the fact that $\mathrm{tr}(\mbf{A}^\top \mbf{B}) = \| \mbf{A} \|_{\mathsf{F}}\cdot \| \mbf{B}\|_{\mathsf{F}}$ under singular vector alignment.

Then, since $\beta_{i} $ has $L\choose 2$ terms inside the sum, when the Frobenium term is expanded, it will have $\frac{{L\choose 2}\left({L\choose 2}+1\right)}{2}$ number of terms. 
Under alignment and balancedness, $\beta^{2}_{i} = \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i}  \times \frac{{L\choose 2}\left({L\choose 2}+1\right)}{2}$ and $\lambda_{i} \delta_{i} =  \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i}  \times {L\choose 3} L$. Thus, we have the expression
\begin{align*}
   2\beta^{2}_{i} - \lambda_{i} \delta_{i} &= \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i} \left( 2 \frac{\binom{L}{2}\left(\binom{L}{2} + 1\right)}{2} -  \binom{L}{3} L \right) \\
   &= \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i}  \binom{L}{3} L \times \left( \frac{3\left(\frac{L(L-1)}{2}+1\right)}{L(L-2)} -1 \right) \\
   & =  \Delta s^2_{\ell} \sigma^{2-\frac{4}{L}}_{i} \frac{2 \binom{L}{3} L }{L(L-2)} \times \left( (L-1)^2  + 5\right) > 0,\\
\end{align*}
for any depth $L>2$. Finally, the condition of stable oscillation of 1-D function is
\begin{align*}
       &  3[f_{\Delta_i}^{(3)}]^2 - f_{\Delta_i}^{(2)}f_{\Delta_i}^{(4)} =   108 \lambda^2_{i}\beta^{2}_{i} -  ( \lambda^{2}_{i})( 12\beta^{2}_{i} + 24(2\lambda_{i})(\delta_{i})) = 48 \lambda^{2}_{i} ( 2\beta^{2}_{i} - \lambda_{i} \delta_{i}  ) > 0,
\end{align*}
which we have proven to be positive for any depth $L>2$, for all the eigenvector directions corresponding to the non-zero eigenvalues. 
Lastly, by Proposition~\ref{prop:one_zero_svs_set}, notice that we can write the vectorized weights in the form
\begin{align*}
    \widetilde{\Delta} &\coloneqq \mathrm{vec}\left( \mbf{W}_L, \mbf{W}_{L-1}, \ldots, \mbf{W}_1\right)\\
    &= \mathrm{vec}\left( \mbf{U}_\star\mbf{\Sigma}_L \mbf{V}_\star^\top, \mbf{V}_\star\mbf{\Sigma}_{L-1} \mbf{V}_\star^\top, \ldots, \mbf{V}_\star\mbf{\Sigma}_1 \mbf{V}_\star^\top\right)\\
    &=\sum_{i=1}^d \mathrm{vec}\left(\sigma_{L, i}\cdot\mbf{u}_{\star, i} \mbf{v}_{\star, i}^\top,\sigma_{L-1, i}\cdot \mbf{v}_{\star, i}  \mbf{v}_{\star, i}^\top, \ldots, \sigma_{1, i}\cdot\mbf{v}_{\star, i}  \mbf{v}_{\star, i}^\top \right).
\end{align*}
Then, $\Delta_i^\top \widetilde{\Delta} \neq 0$ only in the eigenvector directions that correspond to the eigenvalues of the form $S_i = L\sigma_{\star,i}^{2 - 2/L}$. Hence, the oscillations can only occur in the direction of $\Delta_{S_i}$, where $\Delta_{S_i}$ are the eigenvectors corresponding to the eigenvalues $S_i$.
This completes the proof. 
    % Then, notice that from the first part of the proof,
    % \begin{align*}
    %     \|\Delta_i \mbf{W}_L \mbf{W}_{L-1:1} + \ldots + \mbf{W}_{L:3}\Delta_i \mbf{W}_2 \mbf{W}_{1} + \mbf{W}_{L:2} \Delta_i \mbf{W}_1\|^2_{\mathsf{F}} = 2\lambda_i > 0,
    % \end{align*}
    
\end{proof}



\subsubsection{Proof of Lemma~\ref{lemma:hessian_eigvals}}
\label{sec:proof_of_hess_eigvals}

\begin{proof}
By Proposition~\ref{prop:one_zero_svs_set}, notice that we can re-write the loss in Equation~(\ref{eqn:deep_mf}) as 
    \begin{align*}
        \frac{1}{2} \left\|\mbf{W}_{L:1} - \mbf{M}_\star\right\|^2_{\mathsf{F}} = \frac{1}{2} \|\mbf{\Sigma}_{L:1} - \mbf{\Sigma}_\star\|^2_{\mathsf{F}},
    \end{align*}
    where $\mbf{\Sigma}_{L:1}$ are the singular values of $\mbf{W}_{L:1}$. We will first show that the eigenvalues of the Hessian with respect to the weight matrices $\mbf{W}_\ell$ are equivalent to those of the Hessian taken with respect to its singular values $\mbf{\Sigma}_\ell$.
    To this end, consider the vectorized form of the loss:
    \begin{align*}
        f(\mbf{\Theta}) \coloneqq \frac{1}{2}\|\mbf{W}_{L:1} - \mbf{M}_\star\|^2_{\mathsf{F}} = \frac{1}{2}\|  \text{vec}(\mbf{W}_{L:1}) - \text{vec}(\mbf{M}_\star)\|^2_2.
    \end{align*}
    Then, each block of the Hessian $ \nabla_{\mbf{\Theta}}^2 f(\mbf{\Theta}) \in \mbb{R}^{d^2 L \times d^2 L}$ with respect to the vectorized parameters is given as
    \begin{align*}
    \left[\nabla_{\mbf{\Theta}}^2 f(\mbf{\Theta})\right]_{m, \ell} = \nabla_{\text{vec}(\mbf{W}_{m})} \nabla^\top_{\text{vec}(\mbf{W}_{\ell})} f(\mbf{\Theta}) \in \mbb{R}^{d^2 \times d^2}.
    \end{align*}
    %Here, notice that 
%Note that each of the Hessian wrt product combinatinos $\nabla_{\text{vec}(\mbf{W_{l}})} \nabla_{\text{vec}(\mbf{W}_{m})^\top} f(\mbf{\Theta})  \in \mathbf{R}^{n^2 \times n^2} $ which makes  $  \nabla_{\mbf{\Theta}}^2 f(\mbf{\Theta}) \in \mathbf{R}^{n^2 L \times n^2 L}$.
By the vectorization trick, each vectorized layer matrix has an SVD of the form $\text{vec}(\mbf{W}_{\ell}) = \text{vec} (\mbf{U}_{\ell} \mbf{\Sigma}_{\ell} \mbf{ V}^\top_{\ell}) = (\mbf{V}_{\ell} \otimes \mbf{U}_{\ell}) \cdot \text{vec}(\mbf{\Sigma}_{\ell})$.
Then, notice that we have
\begin{align*}
    \nabla_{\text{vec}(\mbf{W}_{\ell})} f(\mbf{\Theta}(t)) = (\mbf{ V}_{\ell} \otimes \mbf{U}_{\ell})  \cdot \nabla_{\text{vec}(\mbf{ \Sigma}_{\ell})}f(\mbf{\Theta}(t)),
\end{align*}
which gives us that 
each block of the Hessian is given by  
%Similarly, calculating the $ml$ block product matrix of the Hessian, we have:
\begin{align*}
    \left[\nabla_{\mbf{\Theta}}^2 f(\mbf{\Theta})\right]_{m, \ell} &= \nabla_{\text{vec}(\mbf{W}_{m})} \nabla^\top_{\text{vec}(\mbf{W}_{\ell})} f(\mbf{\Theta})\\
  &=  (\mbf{V}_{m} \otimes \mbf{U}_{m})\cdot \underbrace{\nabla_{\text{vec}(\mbf{\Sigma}_{m})}   \nabla^\top_{\text{vec}(\mbf{\Sigma}_{\ell})} f(\mbf{\Theta})}_{\eqqcolon \mbf{H}_{m, \ell}}\cdot (\mbf{V}_{\ell} \otimes \mbf{U}_\ell)^\top.
\end{align*}
Then, since the Kronecker product of two orthogonal matrices is also an orthogonal matrix by Lemma~\ref{lemma:kronecker_ortho}, we can write the overall Hessian matrix as  
    \begin{align*}
        \widetilde{\mbf{H}} =
        \begin{bmatrix}
            \mbf{R}_1\mbf{H}_{1, 1}\mbf{R}_1 & \mbf{R}_1\mbf{H}_{1, 2}\mbf{R}_2 & \ldots & \mbf{R}_1\mbf{H}_{1, L}\mbf{R}_L \\
            \mbf{R}_2\mbf{H}_{2, 1}\mbf{R}_1& \mbf{R}_2 \mbf{H}_{2, 2}\mbf{R}_2 & \ldots & \mbf{R}_2\mbf{H}_{2, L} \mbf{R}_L\\
            \vdots & \vdots & \ddots & \vdots \\
             \mbf{R}_L\mbf{H}_{L, 1}\mbf{R}_1 & \mbf{R}_L\mbf{H}_{L, 2}\mbf{R}_2 & \ldots & \mbf{R}_L\mbf{H}_{L, L}\mbf{R}_L
        \end{bmatrix},
    \end{align*}
 for orthogonal matrices $\{\mbf{R}_\ell\}_{\ell=1}^L$. Then, by Lemma~\ref{lem:relationship_lemma}, the eigenvalues of $\widetilde{\mbf{H}}$ are the same as those of $\mbf{H}$, where $\mbf{H} \in \mbb{R}^{d^2 L \times d^2 L}$ is the Hessian matrix with respect to the vectorized $\mbf{\Sigma}_\ell$:
\begin{align*}
    \mbf{H} = \begin{bmatrix}
            \mbf{H}_{1,1} & \mbf{H}_{1, 2} & \hdots &\mbf{H}_{L, 1}\\
            \mbf{H}_{2,1} & \mbf{H}_{2,2} & \hdots & \mbf{H}_{L, 2} \\
            \vdots & \vdots & \ddots & \vdots \\
            \mbf{H}_{1, L} & \mbf{H}_{2, L} & \hdots &\mbf{H}_{L, L}
        \end{bmatrix}.
\end{align*}
   Now, we can consider the following vectorized loss:
    \begin{align*}
        f(\mbf{\Theta}) = \frac{1}{2} \|\mbf{\Sigma}_{L:1} - \mbf{\Sigma}_\star\|_\mathsf{F}^2 &= \frac{1}{2} \left\|\mathrm{vec}\left(\mbf{\Sigma}_{L:1} - \mbf{\Sigma}_\star\right)\right\|_2^2 \\&= \frac{1}{2} \| \underbrace{\left(\mbf{\Sigma}^\top_{\ell-1:1} \otimes \mbf{\Sigma}_{L:\ell+1} \right)}_{\eqqcolon \mbf{A}_{\ell}}\cdot\mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{\Sigma}_\star) \|_2^2. 
    \end{align*}
    Then, the gradient with respect to $\mathrm{vec}(\mbf{\Sigma}_{\ell})$ is given by
    \begin{align*}
        \nabla_{\mathrm{vec}(\mbf{\Sigma}_{\ell})} f(\mbf{\Theta}) = \mbf{A}_{\ell}^\top \left( \mbf{A}_{\ell}\cdot \mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{\Sigma}_\star)\right).
    \end{align*}
   
    Then, for $m=\ell$, we have
    \begin{align*}
        \mbf{H}_{\ell, \ell} = \nabla^2_{\mathrm{vec}(\mbf{\Sigma}_{\ell})} f(\mbf{\Theta}) &= \mbf{A}_{\ell}^{\top}\mbf{A}_{\ell}. 
    \end{align*}
For $m\neq \ell$, we have
\begin{align*}
   & \mbf{H}_{m, \ell} = \nabla_{\mathrm{vec}(\mbf{\Sigma}_{m})}  \nabla_{\mathrm{vec}(\mbf{\Sigma}_{\ell})} f(\mbf{\Theta}) =  \nabla_{\mathrm{vec}(\mbf{\Sigma}_{m})} \left[\mbf{A}_{\ell}^\top (\mbf{A}_{\ell} \mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{M}^{\star})) \right] \\
   & = \nabla_{\mathrm{vec}(\mbf{\Sigma}_{m})} \mbf{A}_{\ell}^\top \cdot \underbrace{(\mbf{A}_\ell \mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{M}^{\star}))}_{=0 \text{ 
 at convergence}} + \mbf{A}_{\ell}^{\top} \cdot  \nabla_{\mathrm{vec}(\mbf{\Sigma}_{m})} (\mbf{A}_{\ell} \mathrm{vec}(\mbf{\Sigma}_{\ell}) - \mathrm{vec}(\mbf{M}^{\star})) \\
   & = \mbf{A}_{\ell}^\top \mbf{A}_{m},
\end{align*}
where we have used the product rule along with the fact that $\mbf{A}_{\ell} \mathrm{vec}(\mbf{\Sigma}_{\ell}) = \mbf{A}_m \mathrm{vec}(\mbf{\Sigma}_{m})$.

Overall, the Hessian at convergence for GD is given by
\begin{align*}
    \mbf{H} =
    \begin{bmatrix}
        \mbf{A}_{1}^\top \mbf{A}_{1} & \mbf{A}_{1}^\top \mbf{A}_{2} & \ldots & \mbf{A}_{1}^\top \mbf{A}_{L} \\
        \mbf{A}_{2}^\top \mbf{A}_{1} & \mbf{A}_{2}^\top \mbf{A}_{2} & \ldots & \mbf{A}_{2}^\top \mbf{A}_{L} \\
        \vdots & \vdots & \ddots & \vdots \\
        \mbf{A}_{L}^\top \mbf{A}_{1} & \mbf{A}_{L}^\top \mbf{A}_{2} & \ldots & \mbf{A}_{L}^\top \mbf{A}_{L}
    \end{bmatrix}
\end{align*}
Now, we can derive an explicit expression for each $\mbf{A}_{m, \ell}$ by considering the implicit balancing effect of GD in Proposition~\ref{prop:balancing}. Under balancing and Proposition~\ref{prop:one_zero_svs_set}, we have that at convergence,
    \begin{align*}
        \mbf{\Sigma}_{L:1} = \mbf{\Sigma}_\star \implies \mbf{\Sigma}_{\ell} = \begin{bmatrix}
            \mbf{\Sigma}^{1/L}_{\star, r} & \mbf{0} \\
            \mbf{0} & \alpha \cdot \mbf{I}_{d-r}
        \end{bmatrix}, \quad \forall \ell \in [L-1], \quad \text{and} \,\,\, \mbf{\Sigma}_L = \mbf{\Sigma}^{1/L}_{\star}.
    \end{align*}
    Thus, we have
    \begin{align*}
        \mbf{H}_{m, \ell} = \begin{cases}
            \mbf{\Sigma}_{\ell}^{2(\ell -1)} \otimes \mbf{\Sigma}_{\star}^{\frac{2(L-\ell)}{L}} \quad& \text{for } \,m=\ell, \\
            \mbf{\Sigma}_\ell^{m+\ell - 2} \otimes \mbf{\Sigma}_{\star}^{2L -m-\ell} \quad& \text{for }\, m\neq\ell. \\
        \end{cases}
    \end{align*}
        Now, we are left with computing the eigenvalues of $\mbf{H} \in \mbb{R}^{d^2 L \times d^2 L}$. To do this, let us block diagonalize $\mbf{H}$ into $\mbf{H} = \mbf{PCP}^\top$, where $\mbf{P}$ is a permutation matrix and 
    \begin{align*}
        \mbf{C} = 
        \begin{bmatrix}
            \mbf{C}_{1} & & \\
            & \ddots & \\
            &&\mbf{C}_{d^2}
        \end{bmatrix} \in \mbb{R}^{d^2 L \times d^2 L},
    \end{align*}
    where each $(i,j)$-th entry of $\mbf{C}_k \in \mbb{R}^{L \times L}$ is the $k$-th diagonal element of $\mbf{H}_{i, j}$. Since $\mbf{C}$ and $\mbf{H}$ are similar matrices, they have the same eigenvalues.
    Then, since $\mbf{C}$ is a block diagonal matrix, its eigenvalues (and hence the eigenvalues of $\mbf{H}$) are the union of each of the eigenvalues of its blocks. 

    By observing the structure of $\mbf{H}_{m, \ell}$, notice that each $\mbf{C}_k$ is a rank-$1$ matrix. Hence, when considering the top-$r$ diagonal elements of $\mbf{H}_{m, \ell}$ corresponding to each Kronecker product to construct $\mbf{C}_k$, each $\mbf{C}_k$ can be written as an outer product $\mbf{uu}^{\top}$, where $\mbf{u} \in \mbb{R}^L$ is
    \begin{align}
        \mbf{u}^{\top} = \begin{bmatrix}
            \sigma_{\star, i}^{1 - \frac{1}{L}} \sigma_{\star, j}^{0} & \sigma_{\star, i}^{1 - \frac{2}{L}} \sigma_{\star, j}^{\frac{1}{L}} & \sigma_{\star, i}^{1 - \frac{3}{L}} \sigma_{\star, j}^{\frac{2}{L}} & \ldots & \sigma_{\star, i}^{0} \sigma_{\star, j}^{1 - \frac{1}{L}} 
        \end{bmatrix}^{\top}.
    \end{align}
    Then, the non-zero eigenvalue of this rank-$1$ matrix is simply $\|\mbf{u}\|_2^2$, which simplifies to 
    \begin{align*}
        \|\mbf{u}\|_2^2 = \sum_{\ell=0}^{L-1} \left(\sigma_{\star, i}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, j}^{\frac{1}{L}\ell}\right)^2.
    \end{align*}
    Next, we can consider the remaining $d-r$ components of each Kronecker product of $\mbf{H}_{m, \ell}$. Notice that for $m = \ell = L$, we have
    \begin{align*}
        \mbf{H}_{L, L} = \begin{bmatrix}
            \sigma_{\star, 1}^{\frac{2(L-1)}{L}} \cdot \mbf{I}_d & & & \\
            & \ddots & & \\
            & & \sigma_{\star, r}^{\frac{2(L-1)}{L}} \cdot \mbf{I}_d  & \\
            & & & \alpha^{2(L-1)}\mbf{I}_{d-r} \otimes \mbf{I}_d
        \end{bmatrix}. 
    \end{align*}
    This amounts to a matrix $\mbf{C}_k$ with a single element  $\sigma_{\star, i}^{\frac{2(L-1)}{L}}$ and $0$ elsewhere. This gives an eigenvalue $\sigma_{\star, i}^{\frac{2(L-1)}{L}}$  for all $i \in [r]$, with multiplicity $d-r$. 

    Lastly, we can consider the diagonal components of $\mbf{H}_{m, \ell}$ that is a function of the initialization scale $\alpha$. For this case, each $\mbf{C}_k$ can be written as an outer product $\mbf{vv}^{\top}$, where 
    \begin{align}
        \mbf{v}^{\top} = \begin{bmatrix}
            \sigma_{\star, i}^{1 - \frac{1}{L}} \alpha^{0} & \sigma_{\star, i}^{1 - \frac{2}{L}} \alpha& \sigma_{\star, i}^{1 - \frac{3}{L}} \alpha^{2} & \ldots & \sigma_{\star, i}^{0} \alpha^{L-1}
        \end{bmatrix}^{\top}.
    \end{align}
    Similarly, the non-zero eigenvalue is simply $\|\mbf{v}\|_2^2$, which corresponds to
    \begin{align*}
        \|\mbf{v}\|_2^2 = \sum_{\ell=0}^{L-1} \left(\sigma_{\star, k}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \alpha^{\ell}\right)^2.
    \end{align*}
    This completes the proof.
\end{proof}


\subsubsection{Proof of Theorem~\ref{thm:align_thm}}
\label{sec:proof_of_orbits}


\begin{proof}

To prove the result, we will consider the GD step on the $i$-th singular value and show that the $2$-period orbit condition holds given the learning rate $\eta = \frac{2}{K}$.
For ease of exposition, let us denote the $i$-th singular value of each $\mbf{W}_\ell$ as $\sigma_{i} \coloneqq \sigma_{\ell, i}$. Under balancing, consider the two-step GD update on the first singular value:
\begin{align*}
    \sigma_i(t+1) &= \sigma_i(t) + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)\right)\cdot \sigma^{L-1}_{i}(t) \\
      \sigma_i(t) = \sigma_i(t+2) &= \sigma_i(t+1) + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t+1)\right)\cdot \sigma^{L-1}_{i}(t+1). \tag{By 2-period orbit}
\end{align*}
Define $z \coloneqq \left(1 + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)\right)\cdot \sigma^{L-2}_{i}(t) \right)$ and by plugging in $\sigma_i(t+1)$ for $\sigma_i(t)$, we have
\begin{align*}
    \sigma_i(t) &= \sigma_i(t) z + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)z^L \right) \cdot \sigma_i^{L-1}(t)z^{L-1} \\
    \implies 1 &= z + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)z^L \right) \cdot \sigma_i^{L-2}(t)z^{L-1} \\
    \implies 1 &= \left(1 + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)\right)\cdot \sigma^{L-2}_{i}(t) \right) + \eta L \cdot \left(\sigma_{\star, i} - \sigma_i^L(t)z^L \right) \cdot \sigma_i^{L-2}(t)z^{L-1} \\
    \implies 0 &= \left(\sigma_{\star, i} - \sigma_i^L(t)\right) + \left(\sigma_{\star, i} - \sigma_i^L(t)z^L \right) \cdot z^{L-1}
\end{align*}
Simplifying this expression further, we have
\begin{align*}
    &0 = \sigma_{\star, i} - \sigma_i^L(t) + \sigma_{\star, i} z^{L-1} - \sigma_i^L(t) z^{2L-1} \\
    \implies &\sigma_i^L(t) + \sigma_i^L(t) z^{2L-1} =  \sigma_{\star, i} + \sigma_{\star, i} z^{L-1} \\
    \implies &\sigma_i^L(t)\cdot\left(1 + z^{2L - 1} \right) = \sigma_{\star, i}\cdot\left(1 + z^{L - 1} \right) \\
    \implies &\sigma_i^L(t)\frac{\left(1 + z^{2L - 1} \right)}{\left(1 + z^{L - 1} \right)} = \sigma_{\star, i},
\end{align*}
and by defining $\rho_i \coloneqq \sigma_i(t)$, we obtain the polynomial
\begin{align*}
    \sigma_{\star, i} = \rho_i^L\frac{1+z^{2L-1}}{1+z^{L-1}}, \quad \text{where  } \, z \coloneqq \left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right).
\end{align*}
Next, we show the existence of (real) roots within the ranges for $\rho_{i,1}$ and $\rho_{i, 2}$. We note that these roots only exist within the EOS regime.
First, consider $\rho_{i, 1} \in \left(0, \sigma_{\star, i}^{1/L} \right)$. We will show that for two values within this range, there is a sign change for all $L \geq 2$. More specifically, we show that there exists $\rho_i \in \left(0, \sigma_{\star, i}^{1/L} \right)$ such that
\begin{align*}
     \rho_i^L\frac{1+z^{2L-1}}{1+z^{L-1}} - \sigma_{\star, i} > 0 \quad \text{and} \quad \rho_i^L\frac{1+z^{2L-1}}{1+z^{L-1}} - \sigma_{\star, i} < 0.
\end{align*}
For the positive case, consider $\rho_i = (\frac{1}{2}\sigma_{\star, i})^{1/ L}$. We need to show that 
\begin{align*}
    \frac{1+z^{2L-1}}{1+z^{L-1}}  = \frac{1 + \left(1+\eta L\cdot\left(\frac{\sigma_{\star, i}}{2}\right)\frac{\sigma_{\star, i}^{1-\frac{2}{L}}}{2^{1 - \frac{2}{L}}}\right)^{2L-1}}{1 + \left(1+\eta L\cdot\left(\frac{\sigma_{\star, i}}{2}\right)\frac{\sigma_{\star, i}^{1-\frac{2}{L}}}{2^{1 - \frac{2}{L}}}\right)^{L-1}} > 2.
\end{align*}
To do this, we will plug in the smallest possible value of $\eta = \frac{2}{L\sigma_{\star, i}^{2 - \frac{2}{L}}}$ to show that the fraction is still greater than $2$, which gives us
\begin{align}
\label{eqn:first_range_pos}
    u(L) \coloneqq \frac{1 + \left(1+\frac{1}{ 2^{1 - \frac{2}{L}}} \right)^{2L-1}}{1 + \left(1+\frac{1}{ 2^{1 - \frac{2}{L}}} \right)^{L-1}},
\end{align}
which is an increasing function of $L$ for all $L\geq 2$. Since $u(2) > 2$, Equation~(\ref{eqn:first_range_pos}) must always be greater than $2$. For the negative case, we can simply consider $\rho_i = 0$.
Hence, since the polynomial is continuous, by the Intermediate Value Theorem (IVT), there must exist a root within the range $\rho_i \in \left(0, \sigma_{\star, i}^{1/L} \right)$.


Next, consider the range $\rho_{i, 2} \in \left(\sigma_{\star, i}^{1/L}, (2\sigma_{\star, i})^{1/L}\right)$. Similarly, we will show sign changes for two values in $\rho_{i, 2}$.
For the positive case, consider $\rho_i = \left(\frac{3}{2} \sigma_{\star, i}\right)^{1/L}$. For $\eta$, we can plug in the smallest possible value within the range to show that this value of $\rho_i$  provides a positive quantity. Specifically, we need to show that
\begin{align*}
    \frac{1+z^{2L-1}}{1+z^{L-1}} > \frac{2}{3} \implies \frac{1+\left(1+\frac{2}{\sigma_{\star, i}^{2- \frac{2}{L}}}\cdot(\sigma_{\star, i} - \frac{3}{2}\sigma_{\star, i})\cdot \left(\frac{3}{2}\sigma_{\star, i}\right)^{1 - \frac{2}{L}} \right)^{2L-1}}{1+\left(1+\frac{2}{\sigma_{\star, i}^{2- \frac{2}{L}}}\cdot(\sigma_{\star, i} - \frac{3}{2}\sigma_{\star, i})\cdot \left(\frac{3}{2}\sigma_{\star, i}\right)^{1 - \frac{2}{L}} \right)^{L-1}} > \frac{2}{3}.
\end{align*}
We can simplify the fraction as follows:
\begin{align*}
    \frac{1+\left(1+\frac{2}{\sigma_{\star, 1}^{2- \frac{2}{L}}}\cdot(\sigma_{\star, 1} - \frac{3}{2}\sigma_{\star, 1})\cdot \left(\frac{3}{2}\sigma_{\star, 1}\right)^{1 - \frac{2}{L}} \right)^{2L-1}}{1+\left(1+\frac{2}{\sigma_{\star, 1}^{2- \frac{2}{L}}}\cdot(\sigma_{\star, 1} - \frac{3}{2}\sigma_{\star, 1})\cdot \left(\frac{3}{2}\sigma_{\star, 1}\right)^{1 - \frac{2}{L}} \right)^{L-1}} = 
    \frac{1+\left(1-(\frac{3}{2})^{1 - \frac{2}{L}} \right)^{2L-1}}{1+\left(1-(\frac{3}{2})^{1 - \frac{2}{L}} \right)^{L-1}}.
\end{align*}
Then, since we are subtracting by $(\frac{3}{2})^{1 - \frac{2}{L}}$, we can plug in its largest value for $L\geq 2$, which is $3/2$. This gives us 
\begin{align*}
    \frac{1+\left(-0.5\right)^{2L-1}}{1+\left(-0.5 \right)^{L-1}} > \frac{2}{3},
\end{align*}
as for odd values of $L$, the function increases to $1$ starting from $L=2$, and decreases to $1$ for even $L$. 
To check negativity, let us define
\begin{align*}
    h(\rho) \coloneqq \frac{f(\rho)}{g(\rho)} \coloneqq \frac{\rho^L \left(1 + z^{2L-1} \right)}{1 + z^{L-1}}.
\end{align*}
We will show that $h'\left(\sigma_{\star, i}^{1/L} \right) < 0$:
\begin{align*}
h'\left(\sigma_{\star, i}^{1/L} \right) &= \frac{f'\left(\sigma_{\star, i}^{1/L} \right)g\left(\sigma_{\star, i}^{1/L} \right) - f\left(\sigma_{\star, i}^{1/L} \right)g'\left(\sigma_{\star, i}^{1/L} \right)}{g^2\left(\sigma_{\star, i}^{1/L} \right)} \\
&= \frac{f'\left(\sigma_{\star, i}^{1/L} \right) - \sigma_{\star, i}\cdot g'\left(\sigma_{\star, i}^{1/L} \right)}{2} \\
&= \frac{L\sigma_{\star, i}^{1 - \frac{1}{L}} - \sigma_{\star, i}(2L-1)\left(\eta L^2 \sigma_{\star, i}^{2 -\frac{3}{L}} \right) - \sigma_{\star, i}(L-1)\left(\eta L^2 \sigma_{\star, i}^{2 -\frac{3}{L}} \right) }{2} \\
&= \frac{L\sigma_{\star, i}^{1 - \frac{1}{L}} - (3L-2)\left(\eta L^2 \sigma_{\star, i}^{3 -\frac{3}{L}} \right) }{2} < 0,
\end{align*}
as otherwise we need $\eta \leq \frac{\sigma_{\star, i}^{2/L - 2}}{3L^2 - 2L}$, which is out of the range of interest. Since $h'(\rho)< 0$, it follows that there exists a $\delta > 0$ such that $h(\rho) > h(x)$ for all $x$ such that $\rho < x < \rho+\delta$. Lastly, since $h(\rho) - \sigma_{\star, i} = 0$ for $\rho = \sigma_{\star, i}^{1/L}$, it follows that $h(\rho) - \sigma_{\star, i}$ must be negative at $\rho + \delta$.
Similarly, by IVT, there must exist a root within the range 
$\rho_{i,2} \in \left(\sigma_{\star, i}^{1/L}, (2\sigma_{\star, i})^{1/L}\right)$. This proves that the $i$-th singular value undergoes a two-period orbit with the roots $\rho_{i, 1}$ and $\rho_{i, 2}$. Then, notice that if the learning rate is large enough to induce oscillations in the $i$-th singular value, then it is also large enough to have oscillations in all singular values from $1$ to the $(i-1)$-th singular value (assuming that it is not large enough for divergence). Finally, at the (balanced) minimum, we can express the dynamics as 
\begin{align}
    \mbf{W}_{L:1} = \underbrace{\sum_{i=1}^p\rho_{i, j}^L \cdot \mbf{u}_{\star, i}\mbf{v}_{\star, i}^{\top} }_{\text{oscillation subspace}}+ \underbrace{\sum_{k=p+1}^d \sigma_{\star, k}\cdot \mbf{u}_{\star, k}\mbf{v}_{\star, k}^{\top}}_{\text{stationary subspace}}, \quad j \in \{1,2\}, \quad \forall\ell \in [L-1].
\end{align}
This completes the proof.


\end{proof}


\subsection{Auxiliary Results}


\begin{lemma}
\label{lem:relationship_lemma} 
    Let $\{\mbf{R}_{\ell}\}_{\ell=1}^L \in \mathbb{R}^{n\times n}$ be orthogonal matrices and $\mbf{H}_{i, j} \in \mathbb{R}^{n^2 \times n^2}$ be diagonal matrices. Consider the two following block matrices:
    \begin{align*}
       \mbf{H} &= \begin{bmatrix}
            \mbf{H}_{1,1} & \mbf{H}_{1, 2} & \hdots &\mbf{H}_{L, 1}\\
            \mbf{H}_{2,1} & \mbf{H}_{2,2} & \hdots & \mbf{H}_{L, 2} \\
            \vdots & \vdots & \ddots & \vdots \\
            \mbf{H}_{1, L} & \mbf{H}_{2, L} & \hdots &\mbf{H}_{L, L}
        \end{bmatrix} \\  \widetilde{\mbf{H}} &=
      \begin{bmatrix}
        \mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^{\top} & \mbf{R}_{L}\mbf{H}_{1, 2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L}\mbf{H}_{1, L}\mbf{R}_{1}^{\top}\\
        \mbf{R}_{L-1}\mbf{H}_{2,1}\mbf{R}_{L}^{\top} & \mbf{R}_{L-1}\mbf{H}_{2,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L-1}\mbf{H}_{2, L}\mbf{R}_{1}^{\top}\\
        \vdots & \vdots & \ddots & \vdots \\
        \mbf{R}_{1}\mbf{H}_{L,1}\mbf{R}_{L}^{\top} & \mbf{R}_{1}\mbf{H}_{L,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{1}\mbf{H}_{L,L}\mbf{R}_{1}^{\top}
    \end{bmatrix}.
    \end{align*}
    Then, the two matrices $\mbf{H}$ and $\widetilde{\mbf{H}}$ are similar, in the sense that they have the same eigenvalues.
\end{lemma}


\begin{proof}
It suffices to show that $\mbf{H}$ and $ \widetilde{\mbf{H}}$ have the same characteristic polynomials. Let us define 
    \begin{align*}
        \widetilde{\mbf{H}} \coloneqq \begin{bmatrix}
            \mbf{A} & \mbf{B} \\
            \mbf{C} & \mbf{D}
        \end{bmatrix},
    \end{align*}
    where 
    \begin{alignat}{3}
        &\mbf{A} \coloneqq  \mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^{\top} \quad\quad\quad &\mbf{B} &\coloneqq \begin{bmatrix}
            \mbf{R}_{L}\mbf{H}_{1, 2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L}\mbf{H}_{1, L}\mbf{R}_{1}^{\top}
        \end{bmatrix} \\
        &\mbf{C} \coloneqq \begin{bmatrix}
            \mbf{R}_{L-1}\mbf{H}_{2,1}\mbf{R}_{L}^{\top} \\
            \vdots \\
           \mbf{R}_{1}\mbf{H}_{L,1}\mbf{R}_{L}^{\top}
        \end{bmatrix}  \quad\quad\quad
        &\mbf{D} &\coloneqq \begin{bmatrix}
           \mbf{R}_{L-1}\mbf{H}_{2,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{L-1}\mbf{H}_{2, L}\mbf{R}_{1}^{\top}\\ \\
            \vdots & \ddots & \vdots \\
           \mbf{R}_{1}\mbf{H}_{L,2}\mbf{R}_{L-1}^{\top} & \hdots & \mbf{R}_{1}\mbf{H}_{L,L}\mbf{R}_{1}^{\top}
        \end{bmatrix}.
    \end{alignat}
    Then, we have
    \begin{align*}
        \det(\widetilde{\mbf{H}} - \lambda \mbf{I}) &= \det\left(\begin{bmatrix}
            \mbf{A} - \lambda\mbf{I} & \mbf{B} \\
            \mbf{C} & \mbf{D} - \lambda\mbf{I}
        \end{bmatrix}\right) \\
        &= \det(\mbf{A} - \lambda \mbf{I}) \cdot \det((\mbf{D} - \lambda \mbf{I}) - \mbf{C}(\mbf{A} - \lambda \mbf{I})^{-1}\mbf{B}),
    \end{align*}
    where the second equality is by the Schur complement. Notice that
    \begin{align*}
        (\mbf{A} - \lambda \mbf{I})^{-1} = (\mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^\top  - \lambda \mbf{I})^{-1} &= (\mbf{R}_{L}\mbf{H}_{1,1}\mbf{R}_{L}^\top  - \lambda \mbf{\mbf{R}_{L}\mbf{R}_{L}}^\top)^{-1} \\&= \mbf{R}_{L} \cdot (\mbf{H}_{1,1} - \lambda\mbf{I})^{-1} \cdot \mbf{R}_{L}^\top.
    \end{align*}
    Then, we also see that, 
    \begin{align*}
        \mbf{C}(\mbf{A} - \lambda \mbf{I})^{-1}\mbf{B} = \underbrace{\begin{bmatrix}
            \mbf{R}_{L-1} & & \\
            & \ddots & \\
            & & \mbf{R}_{1}
        \end{bmatrix}}_{\eqqcolon \widehat{\mbf{V}}}\cdot\, 
        \mbf{E}\cdot
        \underbrace{\begin{bmatrix}
            \mbf{R}_{L-1}^\top & & \\
            & \ddots & \\
            & & \mbf{R}_{1}^\top
        \end{bmatrix}}_{\eqqcolon \widehat{\mbf{V}}^\top}.
    \end{align*}
    where
    \begin{align*}
        \mbf{E}\coloneqq
        \begin{bmatrix}
            \mbf{H}_{2, 1} \cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1,2} & \hdots & \mbf{H}_{2,1}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, L} \\
            \vdots & \ddots & \vdots \\
            \mbf{H}_{L, 1}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, 2} & \hdots & \mbf{H}_{L, 1}\cdot (\mbf{H}_{1, 1} - \lambda \mbf{I})^{-1} \cdot \mbf{H}_{1, L}
        \end{bmatrix}.
    \end{align*}
    Similarly, we can write $\mbf{D}$ as 
    \begin{align*}
        \mbf{D} = \widehat{\mbf{V}}
        \underbrace{\begin{bmatrix}
            \mbf{H}_{2,2} & \hdots & \mbf{H}_{2, L} \\
            \vdots & \ddots & \vdots \\
            \mbf{H}_{L, 2} & \hdots & \mbf{H}_{L, L}
        \end{bmatrix}}_{\eqqcolon \mbf{F}}
        \widehat{\mbf{V}}^\top.
    \end{align*}
    Then, we have
    \begin{align*}
        \det(\widetilde{\mbf{H}} - \lambda \mbf{I}) &= \det(\mbf{R}_{L}\cdot (\mbf{H}_{1,1} - \lambda \mbf{I})\cdot\mbf{R}_{L}^\top) \cdot \det\left(\widehat{\mbf{V}} \cdot (\mbf{E} - \mbf{F})\cdot \widehat{\mbf{V}}^\top \right) \\
       &= \det(\mbf{H}_{1,1} - \lambda \mbf{I}) \cdot \det(\mbf{E} - \mbf{F}),
    \end{align*}
    which is not a function of $\mbf{U}, \mbf{V},\{\mbf{R}_{\ell}\}_{\ell=1}^L$. By doing the same for $\mbf{H}$, we can show that both $\widetilde{\mbf{H}}$ and $\mbf{H}$ have the same characteristic polynomials, and hence the same eigenvalues. This completes the proof.


\end{proof}


\begin{lemma}
\label{lemma:kronecker_ortho}
   Let $\mbf{A}, \mbf{B} \in \mbb{R}^{d\times d}$ be two orthogonal matrices. Then, the Kronecker product of $\mbf{A}$ and $\mbf{B}$ is also an orthogonal matrix:
   \begin{align*}
       (\mbf{A} \otimes \mbf{B})^\top (\mbf{A} \otimes \mbf{B}) = (\mbf{A} \otimes \mbf{B})(\mbf{A} \otimes \mbf{B})^\top = \mbf{I}_{d^2}.
   \end{align*}
\end{lemma}

\begin{proof}
We prove this directly by using properties of Kronecker products:
\begin{align*}
    (\mbf{A} \otimes \mbf{B})^\top (\mbf{A} \otimes \mbf{B}) &= \mbf{A}^\top \mbf{A} \otimes \mbf{B}^\top \mbf{B} \\
    &= \mbf{I}_d \otimes \mbf{I}_d = \mbf{I}_{d^2}.
\end{align*}
Similarly, we have
\begin{align*}
    (\mbf{A} \otimes \mbf{B}) (\mbf{A} \otimes \mbf{B})^\top &= \mbf{A} \mbf{A}^\top \otimes \mbf{B} \mbf{B}^\top \\
    &= \mbf{I}_d \otimes \mbf{I}_d = \mbf{I}_{d^2}.
\end{align*}
This completes the proof.
\end{proof}

\begin{lemma}
\label{lemm:seq_converge}
    Let $\{a(t)\}_{t=1}^N$ be a sequence such that $a(t) \geq 0$ for all $t$. 
    If there exists a constant $c \in (0,1)$ such that $a(t+1) < c \cdot a(t)$ for all $t$, 
    then $\lim_{t \to \infty} a(t) = 0$.

\end{lemma}

\begin{proof}
   We prove this by direct reasoning. 
    From the assumption $a(t+1) < c \cdot a(t)$ for some $c \in (0,1)$, we can iteratively expand this inequality:
    \[
    a(t+1) < c \cdot a(t), \quad a(t+2) < c \cdot a(t+1) < c^2 \cdot a(t),
    \]
    and, more generally, by induction:
    \[
    a(t+k) < c^k \cdot a(t), \quad \text{for all } k \geq 0.
    \]
    Since $c \in (0,1)$, the sequence $\{c^k\}_{k=0}^\infty$ converges to $0$ as $k \to \infty$. Hence:
    \[
    0\leq \lim_{k \to \infty} a(t+k) \leq \lim_{k \to \infty} c^k \cdot a(t) = 0.
    \]
    Therefore, by the squeeze theorem, the sequence $\{a(t)\}$ converges to $0$ as $t \to \infty$.
\end{proof}





\begin{lemma}
[\cite{chen2023edge}]
\label{lemma:chen-bruna}
Consider any 1-D differentiable function $f(x)$ around a local minima $\bar{x}$, satisfying (i) $f^{(3)}(\bar{x}) \neq 0$, and (ii) $3[f^{(3)}]^2 - f'' f^{(4)} > 0$ at $\bar{x}$. Then, there exists $\epsilon$ with sufficiently small $|\epsilon|$ and $\epsilon \cdot f^{(3)} > 0$ such that: for any point $x_0$ between $\bar{x}$ and $\bar{x} - \epsilon$, there exists a learning rate $\eta$ such that $F_{\eta}^2(x_0) = x_0$, and
\end{lemma}

\[
\frac{2}{f''(\bar{x})} < \eta < \frac{2}{f''(\bar{x}) - \epsilon \cdot f^{(3)}(\bar{x})}.
\]
