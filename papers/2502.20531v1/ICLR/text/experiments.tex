
\section{Experimental Results}


%Section~\ref{sec:oscillations_exp} presents experiments that corroborate our theory. Section~\ref{sec:unexplained_exp} discusses \edit{(i) phenomena in non-linear networks currently unexplained in the literature and how our theory can account for them in deep linear networks} \edit{and (ii) how landscape in DLNs behave at EOS compared to more complicated non-convex landscapes.} 



\begin{comment}

\begin{figure}[t]
    \centering
    \hfill
    \raisebox{1.4cm}{\rotatebox{90}{\footnotesize DLN}}
    \hfill
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \caption*{\footnotesize Stable Region}
\includegraphics[width=\textwidth]{figures/oscillation/eos_sval_none.pdf}
        
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
        \caption*{\footnotesize 1-D Oscillation Region}
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_1.pdf}
     \end{subfigure}
         \begin{subfigure}[b]{0.32\textwidth}
         \centering
        \caption*{\footnotesize 2-D Oscillation Region}\includegraphics[width=\textwidth]{figures/oscillation/eos_sval_2.pdf} 
     \end{subfigure}
     \hfill
        \newline
        \hfill
        \raisebox{1.5cm}{\rotatebox{90}{\footnotesize	 Diagonal LN}}
        \hfill
      \begin{subfigure}[b]{0.32\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_diag_none.pdf}
         \caption*{$K > \lambda_1$}
     \end{subfigure}
     \begin{subfigure}[b]{0.32\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_diag_1.pdf}
         \caption*{ $\lambda_1>K>\lambda_2$}
     \end{subfigure}
         \begin{subfigure}[b]{0.32\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_diag_2.pdf} 
         \caption*{$\lambda_1 > \lambda_2 > K$}
     \end{subfigure}   
     \hfill
    \caption{
    Dynamics of the singular values of the end-to-end DLN (top) and diagonal linear network (bottom) trained using a learning rate $\eta = 2/K$. $\lambda_1 = L\sigma_{\star, 1}^{2 - 2/L}$ and $\lambda_2 = \sum_{\ell=0}^{L-1} \left(\sigma_{\star, 1}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, 2}^{\frac{1}{L}\ell}\right)^2$ are the top two eigenvalues of the Hessian of the training loss at convergence for the DLN. By Theorem~\ref{thm:align_thm}, the DLN has corresponding oscillations for these two regimes, while the diagonal linear network does not due to the difference in the curvature of the landscape.}
    \label{fig:diff_dln_diag}
\end{figure}
\end{comment}


\subsection{Subspace Oscillations in Deep Networks}
\label{sec:oscillations_exp}


Firstly, we provide experimental results corroborating Theorem~\ref{thm:align_thm}. 
We let the target matrix be $\mbf{M}_\star \in \mbb{R}^{50 \times 50}$ with rank 3, with dominant singular values $\sigma_{\star} = {10, 9.5, 9}$.
For the DLN, we consider a 3-layer network, with each layer as $\mbf{W}_\ell \in \mbb{R}^{50\times 50}$ and use an initialization scale of $\alpha = 0.01$.
%For the diagonal linear network, we consider a similar setup, with initialization scale $\alpha = 0.01$ and the top-3 elements of $\mbf{s}_\star \in \mbb{R}^{50}$ to be $10, 9, 6$. 
\begin{wrapfigure}{r}{0.35\textwidth}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{figures/eos_osc_range.pdf}
  \end{center}
  \caption{Oscillation range as a function of the learning rate.}
\label{fig:osc_range}
\end{wrapfigure}
In Figure~\ref{fig:thm2_osc}, we present the behaviors of the singular values of the end-to-end network under different learning rate regimes. Recall that by Theorem~\ref{thm:align_thm}, the $i$-th singular value undergoes periodic oscillations when $K$ is set to be $S_i < K < S_{i+1}$, where $S_i = L\sigma_{\star, i}^{2-2/L}$. Figure~\ref{fig:thm2_osc} illustrates this clearly -- we only observe oscillations in the $i$-th coordinate depending on the learning rate. Interestingly, notice that $\sigma_2$ also begins to oscillate in the rank-$1$ oscillation region before settling at a minimum. This occurs because, while the learning rate is large enough to catapult out of a minimum, it is not sufficiently large to induce periodic oscillations.




%However, the difference in the loss landscape plays a role in oscillations for the second component.
%When $\eta > 2/\lambda_2$, where $\lambda_2$ is the second largest eigenvalue of the DLN, the diagonal linear network does not experience oscillations in the second component, as its second largest eigenvalue is much smaller. 
%This highlights a distinction between the two networks and how the landscape changes between them. 

In Figure~\ref{fig:osc_range}, we present an experiment demonstrating the relationship between the oscillation range and the learning rate by plotting the amplitude of the singular value oscillations in the end-to-end network. The oscillations begin to occur starting from each region $\eta = 2/S_i$, and the oscillation range (or amplitude) increases as the learning rate increases. This can also be observed in Figure~\ref{fig:thm2_osc}; the amplitude of $\sigma_1$ increases as we move from the rank-$1$ to the rank-$3$ oscillation region.

%\edit{In Figure~\ref{fig:osc_range} (right), we present an experiment showing that deeper networks require a smaller $\eta$ to enter the EOS regime, as predicted by the eigenvalue computation in Lemma~\ref{lemma:hessian_eigvals}. Here, we consider the same setup as above, but with a leading target singular value of $\sigma_{\star, 1} = 0.5$.}









\subsection{\edit{Similarities and Differences Between Linear and Nonlinear Nets at EOS}}
\label{sec:unexplained_exp}

%In this section, we draw parallels between observations made in deep nonlinear networks and our theory on DLNs.



\paragraph{Mild Sharpening.} \edit{``Mild'' sharpening refers to the sharpness not rising to $2/\eta$ throughout learning, and generally occurs in tasks with low complexity as discussed in Caveat 2 of \citep{cohen2021gradient}.} We illustrate mild sharpening in Figure~\ref{fig:combined_figures}, 
where we plot sharpness in two settings: (i) regression with simple images and (ii) classification with an MLP using a subset of the CIFAR-10 dataset. 
\begin{wrapfigure}{l}{0.35\textwidth}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{figures/intro/eos_nuance.pdf}
    \end{center}
    \caption{DLNs do not enter EOS regime if $L\sigma^{2-\frac{2}{L}}_{1}< 2/\eta$.}
    \label{fig:dln_mild_sharpening}
\end{wrapfigure}
For the regression task, we minimize the loss $\mathcal{L}(\Theta) = \|G(\Theta) - \mbf{y}_{\mathrm{image}}\|_2^2$, where $G(\Theta)$ is a UNet parameterized by $\Theta$, and $\mbf{y}_{\mathrm{image}}$ denotes one of the images in Figure~\ref{fig:img_used}. 
We observe that when $\mbf{y}_{\mathrm{image}}$ is a smooth, low-frequency image, the sharpness of the loss generally remains low. However, when $\mbf{y}_{\mathrm{image}}$ has higher frequency content, the sharpness increases and enters the EOS regime (Figure~\ref{fig:eos-dip}). Similarly, for the classification task, we train a 2-layer fully connected neural network on $N$ labeled training images from the CIFAR-10 dataset using MSE loss and plot the sharpness in Figure \ref{fig:small_Sharp}. The sharpness links to $N$, the number of data points used for training. For small $N$ values, such as 100 or 200, the network learns only a limited set of latent features, resulting in mild sharpening, and it does not reach the EOS threshold. However, when $N$ exceeds 1000, the sharpness increases and reaches the EOS threshold. \edit{The intrinsic dimension update in neural networks for such low complexity tasks is usually smaller \citep{li2018measuring} which could cause the sharpness to be small.}
\edit{Similar} observations can also be seen in DLNs. In Figure~\ref{fig:dln_mild_sharpening}, we show that the sharpness reaches $L\sigma_{\star, 1}^{2-\frac{2}{L}}$, where $\sigma_{\star, 1}$ is the singular value of the target matrix. Whenever $L\sigma_{\star, 1}^{2-\frac{2}{L}} < 2/\eta$, the network will not enter the EOS regime. This can be viewed as low-complexity learning, as $\sigma_{\star, 1}$ corresponds to the magnitude of the strongest feature of the target matrix. Hence, when $\sigma_{\star, 1}$ is not large enough, the sharpness will not rise to $2/\eta$. \edit{While these observations do not fully explain mild sharpening, our experiments demonstrate that interpreting sharpness as a measure of complexity, combined with our findings from DLNs, marks an important first step toward fully understanding this phenomenon.}


\paragraph{\edit{Difference in Oscillation Behaviors}.}

Here, we discuss the differences in oscillations that arise in DLNs compared to catapults that occur in practical deep nonlinear networks. 
The main difference lies in the loss landscapeâ€”at convergence, the Hessian for DLNs is positive semi-definite, as shown in Lemma~\ref{lemma:hessian_eigvals}, meaning there are only directions of positive curvature and flat directions (in the null space of the Hessian). 
In this landscape, oscillations occur because the basin walls bounce off, without
the direction of escape. 
However, in deep nonlinear networks, it has been frequently observed that the Hessian at the minima has negative eigenvalues \citep{ghorbani2019investigation, sagun2016eigenvalues}. This enables an escape direction along the negative curvature, preventing sustained oscillations. 
\begin{wrapfigure}{r}{0.57\textwidth}
    \centering
     \begin{subfigure}[t]{0.3\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/holder1.png}
     \end{subfigure}
     \begin{subfigure}[t]{0.25\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/dln.png}
         %\caption*{Deep Linear Network}
     \end{subfigure}
     
    \caption{Loss landscape of the Holder table function and DLNs, respectively (left--right). The Holder table function is non-convex which allows catapulting to other minima, whereas DLNs do not have spurious local minima.}
    \label{fig:landscape}
\end{wrapfigure}
\edit{In Figure~\ref{fig:landscape}, we demonstrate these two differences by visualizing the loss landscapes and the iterates throughout GD marked in red. The Holder table function Figure~\ref{fig:landscape} (left) exhibits numerous local minima, causing the loss to exhibit a sharp ``catapult'' when a large learning rate is used. In contrast, for DLNs (shown in the right) the loss oscillates in a periodic orbit around the minima since there are no spurious local minima \citep{ge2016matrix,kawaguchi2016deep,lu2017depth,zhang2019depth,yun2018small}. 
%In Appendix~\ref{sec:extra_details}, we provide experimental details.
}


Lastly,~\cite{damian2023selfstabilization} studies self-stabilization, where sharpness decreases below $2/\eta$ after initially exceeding $2/\eta$. \edit{Their analysis requires assumptions such as  $\nabla L(\theta) \cdot u(\theta) = 0$ and $\nabla S(\theta)$ lies in the null space of the Hessian, where $S(\theta)$ and $u(\theta)$ denotes the sharpness and its corresponding eigenvector respectively. These assumptions do not hold exactly in DLNs. Rather, the sharpness oscillates about $2/\eta$ as shown in Figure~\ref{fig:ps_eos} as the condition for stable oscillation holds along each eigenvector of the Hessian.}


\begin{comment}
\subsection{Explaining Sharpening Phenomenon in Deep Nonlinear Network}
\label{sec:unexplained_exp}

Based on our analysis, we draw parallels to two common observation in deep non-linear networks:\\

1) \textbf{Mild Sharpening in low complexity tasks:} The extent of sharpening in deep neural networks is often related to the complexity of the learning task. To illustrate this, we consider two commonly used tasks: regression and classification.\\
\textit{Regression Task}: In Figure \ref{fig:eos-dip}, we minimize the loss function $L(\boldsymbol{\theta})= \| G(\boldsymbol{\theta}) - \mbf{y}_{im} \|_{2}^2$, fitting an image $\mbf{y}_{im}$ using a deep image generator network like Unet $G(\boldsymbol{\theta})$, with Gradient Descent. We observe that the degree of sharpening in this network depends on the target image $\mbf{y}_{im}$. For example, when $\mbf{y}_{im}$ is a smooth, low-frequency image, the sharpness of the loss $ \| \nabla^2 L(\boldsymbol{\theta}) \|_{2}$ remains low. In contrast, when fitting a detailed image, such as white noise with high-frequency content, the sharpness increases and enters the EOS regime $\frac{2}{\eta}$. In particular,we observe a steady correlation between the sharpness rise (Figure-\ref{fig:eos-dip}) and the high frequency content of the fitting image (Figure-\ref{fig:img_used}). \\
\textit{Classification task}:  In Figure \ref{fig:small_Sharp}, we train a 2-layer fully connected neural network on $N$ pairs of labeled training images from the CIFAR-10 dataset using MSE loss. The sharpness of the loss is linked to $N$, the number of data points used for training. For small $N$ values, such as 100 or 200, the network learns only a limited set of latent features, resulting in mild sharpening, and the network does not reach the EOS threshold. However, when $N$ exceeds 1000, the sharpness increases and reaches the EOS threshold $\frac{2}{\eta}$. Furthermore, Figure-16 in \cite{cohen2021gradient} reported that sharpness also increases with depth of the network. So, the sharpness limit correlates both with network depth and complexity of the learning problem. \\
These two occurences in non-linear network is partially addressed by our analysis of deep matrix factorization at EOS. We show that sharpness rises to $L\sigma^{2-\frac{2}{L}}_{1}$, where $\sigma_{1}$ is the top singular value of the target matrix and $L$ is the depth. In deep matrix factorization problem, $\sigma_{1}$ corresponds to the magnitude of the strongest feature in the data and is related to the complexity of learning. Whenever, $L\sigma^{2-\frac{2}{L}}_{1}<\frac{2}{\eta}$, the network do not enter the EOS regime (Figure-\ref{fig:dln_mild_sharpening}). Similarly, in both regression and classification experiments in Figure \ref{fig:combined_figures}, whenever data has low complexity, sharpness do not rises and network fails to reach EOS. Furthermore, sharpness $L\sigma^{2-\frac{2}{L}}_{1}$ scales with depth of the network $L$, supporting the observation by \cite{cohen2021gradient}.

2) \textbf{Oscillation range in strong feature direction:}
\end{comment}

\begin{figure}[t!]    
\centering
    \begin{subfigure}[t]{0.32\textwidth} % Subfigure (a) - increased size
        \centering
        \includegraphics[width=\textwidth]{figures/dip_eos/eos.pdf}
        \caption{Sharpness plots for training image generator networks using SGD with learning rate $\eta = 2\times 10^{-4}$.}
        \label{fig:eos-dip}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth} % Subfigure (b) - increased size
        \centering
        \includegraphics[width=\textwidth]{figures/dip_eos/img_used.pdf}
        \caption{Target images (denoted as $\mbf{y}_{\mathrm{image}}$) with different frequencies used for training.}
        \label{fig:img_used}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth} % Subfigure (c) - increased size
        \centering
        \includegraphics[width=\textwidth]{figures/small_sharpness.png}
        \caption{2-layer FC network trained with small number $N$ of CIFAR-10 dataset with $\eta=10^{-2}$}
        \label{fig:small_Sharp}
    \end{subfigure}
    \caption{Illustration of Caveat 2 by~\cite{cohen2021gradient} on how mild sharpening occurs on simple datasets and network. (a) Regression task showing the evolution of the sharpness when an UNet (with fixed initialization) is trained to fit a single image shown in (b). (c) Evolution of the minimal progressive sharpening on a classification task of a 2-layer MLP trained on a  subset of CIFAR-10.}
    \label{fig:combined_figures}
\end{figure}

\begin{comment}

\subsection{Difference of EOS dynamics in non-linear networks and deep matrix factorization }

Although mild sharpening and strong directional oscillation phenomenon is partially explained from analyzing deep matrix factorization, we highlight some critical differences between EOS dynamics in non-linear networks and liner networks:

\begin{itemize}
    \item \textbf{Sustained oscillation in DMFs vs catapaults in non-linear networks}: In deep matrix factorization (DMF) loss whenever $\eta > \frac{2}{L \sigma_{1}^{2-\frac{2}{L}}}$, we observe free oscillation in 2-period orbit \footnote{At higher learning rate, iterates enter higher period orbit oscillations, chaos and divergence}. However, in nonlinear networks we observe catapaults or sudden spikes. This is primarily because of difference in loss landscape. At global minima, Hessian of the loss in DMF is provably Positive Semi-Definite (PSD)\footnote{we explicitly derived all positive eigenvalues}. So there are only directions of positive curvature and flat directions (in null space of the Hessian). In this landscape,\textit{ oscillations occur due to bouncing of the basin of the wall with no direction of escape.} But in deep non-linear networks, it is has been frequently observed that the Hessian at the minima has negative eigenvalues \cite{ghorbani2019investigation,sagun2016eigenvalues}. This enables an escape direction in the direction of the negative curvature preventing sustained/free oscillation. We believe understanding the role of non-linear activation layers in changing this loss landscape is the key to address this difference.
    \item \textbf{Absence of self-stabilization effect \cite{damian2023selfstabilization} in deep matrix factorization}
    Damian et al. studied self-stabilization, where sharpness decreases below $\frac{2}{\eta}$ after exceeding $\frac{2}{\eta}$. They assume that $\nabla L(\theta) \cdot u(\theta) = 0$ and $\nabla S(\theta)$ lie in the null-space of the Hessian. However, in deep linear networks, \textit{these assumptions are not satisfied}. Due to singular vector alignment and balancing, the dynamics reduce to a one-dimensional oscillating trajectory, where both $\nabla L(\theta)$ and $\nabla S(\theta)$ lie in the range space of the Hessian. Letting $\pi(\theta)$ represent the scalar product, $\nabla L(\theta) \cdot u(\theta) = (\pi(\theta) - s)\pi(\theta)|w^{-1}|$, which isn't zero unless the loss is zero (not the case in EoS). Hence, in deep linear networks, self-stabilization does not occur, and the iterates and sharpness oscillate above $\frac{2}{\eta}$.

\end{itemize}
\end{comment}
\section{Conclusion, Limitations and Future Work}

In this paper, we presented a fine-grained analysis of the learning dynamics of deep matrix factorization with the aim of understanding unexplained phenomena in deep nonlinear networks within the EOS regime. Our analysis revealed that within EOS, DLNs exhibit periodic oscillations in small subspaces, where the subspace dimension is exactly characterized by the learning rate. 
%This allowed us to explain phenomena such as mild sharpening, oscillations occurring only within the top singular values, and to distinguish the differences between the behaviors of DLNs and diagonal linear networks. 
There are two limitations to our work:  we require (i) the dynamics converge to the singular vector stationary set, and (ii) strict balancing of the singular values. 
However, we provide thorough empirical evidence validating the use of these assumptions, along with more results in Appendix~\ref{sec:additional_exp}.
For the balancing assumption, we leave for future work on alleviating the assumption of strict balancing, and rigorously show that this holds before entering the EOS regime.


%there may exist cases where oscillations begin before strict balance is achieved. We leave it to future work to find subspace oscillations without assuming strict balancing before oscillation starts.
