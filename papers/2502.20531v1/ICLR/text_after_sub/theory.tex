 \section{Deep Matrix Factorization Beyond the Edge of Stability}




\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/dln_ps_eos_lr172.pdf}
    \caption{Depiction of the two phases of learning in the deep matrix factorization problem for a network of depth $3$. It appears that upon escaping the first saddle point, the GD iterates enter the edge of stability regime, where the sharpness hovers just above $2/\eta$ when the learning rate is chosen to be above $2/\|\nabla^2 f(\mbf{\Theta})\|_2$ at convergence.
    }
    \label{fig:ps_eos}
\end{figure}

When using a large learning rate, the learning dynamics can typically be separated into two distinct stages: (i) progressive sharpening and (ii) the edge of stability. Within the progressive sharpening stage, the sharpness lies below $2/\eta$ and tends to continually rise. In the edge of stability regime, the sharpess hovers just above $2/\eta$, and the training loss oscillates
over short timescales, yet consistently decreases over long timescales~\citep{cohen2021gradient}. Our goal is to analyze the EOS stage under the deep matrix factorization formulation, with the aim of understanding phenomena that arise in DLNs. Our result extends the theoritical findings of \cite{chen2023edge}, where the authors proved the existence of a certain class of scalar functions \( f(x) \) for which gradient descent (GD) does not diverge even when operating beyond the stability threshold \( \eta > \frac{2}{f^{''}(\hat{x})} \), where \( \hat{x} \) is a local minimum of \( f(x) \). Specifically, the authors showed that there exists a range \( \eta \in \left( \frac{2}{f^{''}(\hat{x})}, \frac{2}{f^{''}(\hat{x})}(1+\epsilon) \right) \), with a function-dependent \( \epsilon > 0 \), where the loss oscillates around the local minima $(\hat{x})$ with a certain periodicity. As \( \eta \) increases beyond \( \frac{2}{f^{''}(\hat{x})} \), the oscillations gradually enter higher periodic orbits (e.g., 2, 4, 8 periods), then transition into chaotic behavior, and ultimately lead to divergence. In our work, we prove that this oscillatory behaviour beyond the stability threshold occurs even for deep linear networks. Our key observation is that due to two specific implicit bias of GD namely 1) singular vector alignment and 2) balancing, stable oscillations takes place in the top $r$ eigenvectors of the Hessian of the loss, where $r$ is dependent on $\eta$. 
Throughout the analysis, we only limit ourselves to analyzing two periodic orbit oscillation (Figure~\ref{fig:ps_eos}) and show higher period oscillations in the appendix.

%Here, we observe that the training loss oscillates in a two-period orbit rather than decreasing over iterations, as illustrated in Figure~\ref{fig:ps_eos} and rigorously shown in the following sections.


\subsection{Main Results}
In this section, we present our main results rigorously describing the learning dynamics of DLNs in the edge of stability regime. Before doing so, we give a proof brief sketch on how we analyze EOS for deep linear networks:

\begin{itemize}
    \item We state the two implicit biases of GD in deep matrix factorization in Proposition~\ref{prop:svs_set} and Lemma~ \ref{lemma:balancing}. These allows us to find the local minima in the non-convex problem and find the eigenvectors and eigenvalues of the Hessian. 
   \item We prove that oscillation in each eigenvector direction satisfies the stable oscillation condition which we prove in Theorem-\ref{thm:stable_oscillations}. We also show the existence and uniqueness of two point fixed period orbit along each eigenvector direction in Theorem~\ref{thm:align_thm}. 
\end{itemize}


\subsection{Tools used in the Analyses}

This section presents the two main tools used in our analyses: the singular vector stationary set and balancedness. First, we present the singular vector stationary set, which allows us to encompass a wider range of initialization schemes. This set defines a broad class of initialization for which singular vector alignment occurs, simplifying the dynamics to only singular values.

\begin{proposition}[Singular Vector Stationary Set]
\label{prop:svs_set}
Consider the deep matrix factorization loss in Equation~(\ref{eqn:deep_mf}). Let $\mbf{M}_\star = \mbf{U}_\star \mbf{\Sigma}_\star \mbf{V}_\star^\top$ and 
$\mbf{W}_\ell(t) = \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t) \mbf{V}_\ell(t)^\top$ denote the compact SVD for the target matrix and the $\ell$-th weight matrix at time $t$, respectively. Then the set of all time stationary points $\dot{\mbf{U}}_\ell(t) = \dot{\mbf{V}}_\ell(t) = 0$ for all $\ell \in [L]$ are given by
\begin{align*}
\mathrm{SVS}(f(\mbf{\Theta})) = 
\begin{cases}
    (\mbf{U}_L, \mbf{V}_L) &= (\mbf{U}_\star, \mbf{Q}_L), \\
    (\mbf{U}_\ell, \mbf{V}_\ell) &= (\mbf{Q}_{\ell+1}, \mbf{Q}_\ell), \quad\forall \ell \in [2, L-1], \\
    (\mbf{U}_1, \mbf{V}_1) &= (\mbf{Q}_2, \mbf{V}_\star),
\end{cases}
\end{align*}
where \(\{\mbf{Q}_\ell\}_{\ell=2}^{L}\) are any set of orthogonal matrices. 
\end{proposition}


The singular vector stationary set states that for any set of weights where the gradients with respect to the singular vectors become zero, the singular vectors become fixed points for subsequent iterations. Once the singular vectors become stationary, running GD further isolates the dynamics on the singular values. Hence, throughout our analysis, we re-write and consider the loss 
\begin{align}\label{eqn:simplified_loss}
    \frac{1}{2} \left\|\mbf{W}_{L:1}(t) - \mbf{M}^\star\right\|^2_{\mathsf{F}} = \frac{1}{2} \|\mbf{\Sigma}_{L:1} - \mbf{\Sigma}^\star\|^2_{\mathsf{F}} = \frac{1}{2} \sum_{i=1}^r \left(\sigma_i(\mbf{\Sigma}_{L:1}(t)) - \sigma_{\star, i}\right)^2,
\end{align}
where $\mbf{\Sigma}_{L:1}$ are the singular values of $\mbf{W}_{L:1}$. This allows us to decouple the dynamics of the singular vectors and singular values, focusing on the periodicity that occurs in the singular values within the EOS regime. In Proposition~\ref{prop:one_zero_svs_set}, we prove that the one-zero initialization we consider belongs to the singular vector stationary set with an illustration in Figure~\ref{fig:svec_alignment}.
In Appendix~\ref{sec:additional_exp}, we provide additional examples of initialization that belong to this set. Next, we present a result to validate our use of the strictly balanced assumption by showing that the singular values become increasingly balanced throughout the GD iterations.

\begin{lemma}[Balancing]
\label{lemma:balancing}
    Suppose we run GD on the deep matrix factorization loss in Equation~(\ref{eqn:deep_mf}) with learning rate $\eta < \frac{2\sqrt{2}}{L \sigma^{2-\frac{2}{L}}_{\star,1}}$, where $\sigma_{\star, 1}$ is the first singular value of $\mbf{M}_\star \in \mbb{R}^{d\times d}$. Let $\sigma_{i,\ell}$ denote the $i$-th singular value of the $\ell$-th layer matrix.  If the initialization scale $\alpha$ satisfies
    $0< \alpha < \left( \log\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star, 1}^{2 - \frac{2}{L}}} \right) \cdot 2 \sigma_{\star, 1}^{\frac{4}{L}} \right)^{\frac{1}{4}}$,  we have $| \sigma^2_{i,L}(t+1) - \sigma^2_{i,\ell}(t+1)| < | \sigma^2_{i,L}(t) - \sigma^2_{i,\ell}(t)|$. 
\end{lemma}


\begin{definition}[Strict Balanced State]
    The parameters $\mbf{\Theta}$ in the deep matrix factorization loss are said to be in a strict balanced state if the singular values of the top $r$ indices for each layer are the same. Specifically, for all $1 \leq i \leq r$ and for all layers $\ell, k = 1, 2, \dots, L$, we have $\sigma_{i}(\mbf{W_{\ell}}) = \sigma_{i}(\mbf{W_{k}})$, where $\sigma_{i}(\mbf{W_{\ell}})$ denotes the $i$-th singular value of the $\ell$-th layer.
\end{definition}

\begin{lemma}[Eigenvalues of Hessian at Convergence]
\label{lemma:hessian_eigvals}
    Consider running GD on the deep matrix factorization loss $f(\mbf{\Theta})$ defined in Equation~(\ref{eqn:deep_mf}). Under  strict balancing with any stationary point $\Theta$ with $\nabla_{\mbf{\Theta}} f(\mbf{\Theta}) = 0$, the non-zero eigenvalues of the training loss Hessian are given by
    \begin{align*}
        \lambda_{\mbf{\Theta}} = \left\{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, i}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, j}^{\frac{1}{L}\ell}\right)^2\right\}_{i,j = 1}^{r} \,\bigcup \, \,\left\{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, k}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \alpha^{\ell}\right)^2\right\}_{k = 1}^{r} \, \bigcup \, \,\left\{\sigma_{\star, m}^{2-\frac{2}{L}}\right\}_{m=1}^r,
    \end{align*}
    where $\sigma_{\star, i}$ is the $i$-th singular value of the target matrix $\mbf{M}_\star \in \mbb{R}^{d\times d}$,  $\alpha \in \mbb{R}$ is the initialization scale, $L$ is the depth of the network, and the last set of eigenvalues each have a multiplicity of $d-r$.
\end{lemma}

We defer all of the proofs to Appendix~\ref{sec:proofs}. Since we are interested in analyzing the dynamics when the learning rate is $\eta > 2 / \|\nabla^2 f(\mbf{\Theta})\|_2$, we need to exactly quantify the sharpness value $\|\nabla^2 f(\mbf{\Theta})\|_2$. By Lemma~\ref{lemma:hessian_eigvals}, we can see that the sharpness is exactly $\|\nabla^2 f(\mbf{\Theta})\|_2 = L\sigma_{\star, 1}^{2- \frac{2}{L}}$, assuming strict balance. Hence, as long as $\eta$ is set to $\eta > 2 / L\sigma_{\star, 1}^{2- \frac{2}{L}}$, we will observe oscillations in the loss.

Interestingly, notice that all non-zero eigenvalues are a function of network depth. For a deeper network, the sharpness will be larger, implying that a smaller learning rate can be used to drive the DLN into EOS. This provides a unique perspective on how the learning rate should be chosen as networks become deeper. Next, we present our result on the dynamics of DLNs within EOS.

Lastly, notice that the Lemma~\ref{lemma:hessian_eigvals} was established under a strict balancing assumption, meaning the singular values across all weight matrices are the same in the EOS regime. In Lemma~\ref{lemma:balancing}, we show that as long as the initialization scale $\alpha$ satisfies the upper bound, the singular values do indeed become balanced as $t \to \infty$, but not necessarily before oscillation starts.
However, we validate with experimental results that the singular values become balanced even before entering the EOS regime, supporting the use of the strict balancing assumption.



\begin{comment}
    
Interestingly, Lemma~\ref{lemma:hessian_eigvals} also reveals that the loss landscape of DLNs differs from that of the widely studied diagonal linear networks. In diagonal linear networks, each eigenvalue is characterized by a coordinate of the target vector (which is equivalent to the singular values of the target matrix in DLNs), whereas in our case, there is a ``mixture'' of singular values in the eigenvalues that arise primarily from the off-diagonal terms. This presents a unique distinction between our analysis and those on diagonal networks~\citep{even2024s}.
Next, we present our main result on $2$-period subspace oscillations in the singular values of the DLN.
\end{comment}

\begin{theorem}[1-D Subspace Oscillation]
\label{thm:align_thm}
Suppose we run GD on the deep matrix factorization loss with learning rate $\eta = \frac{2}{K}$, where $\mathrm{max} \left\{ \sum_{\ell=0}^{L-1} \left(\sigma_{\star, 1}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, 2}^{\frac{1}{L}\ell}\right)^2,L\sigma_{\star, 1}^{2-\frac{2}{L}}/\sqrt{2}\right\}< K< L\sigma_{\star, 1}^{2-\frac{2}{L}}$ and with initialization scale $\alpha < \left( \log\left( \frac{2\sqrt{2}}{\eta L \sigma_{\star, 1}^{2 - \frac{2}{L}}} \right) \cdot 2 \sigma_{\star, 1}^{\frac{4}{L}} \right)^{\frac{1}{4}}$. Then, under strict balancing, each weight matrix $\mbf{W}_{\ell}(t) \in \mbb{R}^{d\times d}$ oscillates around the minima in a $2$-period fixed orbit as follows:
\begin{align*}
    \mbf{W}_L(t) &= \rho_i(t)\cdot \mbf{u}_{\star, 1}\mbf{v}_{\star, 1}^{\top} + \sum_{j=1}^r \sigma_{\star, i}\mbf{u}_{\star, j}\mbf{v}_{\star, j}^{\top}, \quad i = 1, 2, \\
    \mbf{W}_\ell(t) &= \rho_i(t) \cdot\mbf{v}_{\star, 1}\mbf{v}_{\star, 1}^{\top} + \sum_{j=1}^r \sigma_{\star, i}\mbf{v}_{\star, j}\mbf{v}_{\star, j}^{\top}, \quad i = 1, 2, \quad \forall\ell \in [L-1],
\end{align*}
where $\mbf{M}_\star = \mbf{U}_\star \mbf{\Sigma}_\star \mbf{V}_\star^\top$ denotes the SVD of the target matrix , $\rho_i(t) \in \{\rho_1, \rho_2\}$
and $\rho_1 \in \left(0, \sigma_{\star, 1}^{1/L}\right)$ and $\rho_2 \in \left(\sigma_{\star, 1}^{1/L}, (2\sigma_{\star, 1})^{1/L}\right)$ are the two real roots of the polynomial $g(\rho)=0$, where
\begin{align*}
    g(\rho) = \rho^L\frac{1+\left(1 + \eta L(\sigma_{\star, 1} - \rho^L)\cdot \rho^{L-2} \right)^{2L-1}}{1+\left(1 + \eta L(\sigma_{\star, 1} - \rho^L)\cdot \rho^{L-2} \right)^{L-1}} - \sigma_{\star, 1}.
\end{align*}
\end{theorem}

%\ag{Should we have the general oscillation as a lemma?
%Also where are we having the digoanl liner network oscillation?}
\paragraph{Remarks.} Let $\lambda_i$ denote the $i$-th largest eigenvalue of the training loss Hessian. In Theorem~\ref{thm:align_thm}, we showed that if the learning rate is chosen such that $\eta = \frac{2}{K}$ with $\lambda_2 < K< \lambda_1$ (assuming that $\lambda_2 > \lambda_1  / \sqrt{2}$), then the largest singular value of the DLN will undergo periodic oscillations, where the amplitude is governed by $\rho_1$ and $\rho_2$. This theoretically shows why (i) oscillations only occur within top subspaces of the network as observed by~\cite{zhu2023catapults} and (ii) oscillations are more prevalent in the direction of the stronger features (measured by the magnitude of the singular values). 



% For the initialization scale, we require an upper bound on $\alpha$ to satisfy the balancing condition in Lemma~\ref{lemma:balancing}.

To show oscillations in two or more subspaces, we can easily extend Theorem~\ref{thm:align_thm} and set the learning rate such that $\lambda_i \geq K > \lambda_{i+1}$ for $i \geq 2$. For example, to observe oscillations in the top-$2$ subspaces, one needs to set $\lambda_2 \geq K > \lambda_3$, according to the eigenvalues computed in Lemma~\ref{lemma:hessian_eigvals}. Since Theorem~\ref{thm:align_thm} already shows that there must exist real roots for $K < \lambda_1$, a more careful treatment of the range of $\rho_1$ and $\rho_2$ is needed to rigorously extend this to $r$-dimensional oscillations. The upper bound of learning rate $\eta< \frac{2\sqrt{2}}{L\sigma^{2-\frac{2}{L}}_{\star,1}}$ depends only on the top singular value $\sigma_{\star,1}$ for which balancing fails to hold in all scalar singular value losses. 
% Lastly, note that we implicitly assumed the learning rate is chosen below some divergence limit. We leave for future work the task of precisely quantifying the learning rate for divergence, in order to obtain a more accurate bound on which learning rates contribute to the EOS regime.

In the following, we demonstrate that it is indeed possible to observe oscillations in all $r$-dimensional subspaces. From Lemma~\ref{lemma:chen-bruna} (restated from~\cite{chen2023edge}), the necessary condition for stable two-period orbit is that $f_{\Delta_i}^{(3)}(x) \neq 0$ and $3[f_{\Delta_i}^{(3)}(x)]^2 - f_{\Delta_i}^{(2)}(x)f_{\Delta_i}^{(4)}(x) > 0$ around a local minima $x$, where $\Delta_i$ is the $i$-th eigenvector of the Hessian and $f_{\Delta_i}$ is the loss function restricted to the line $\{y: y=x+ t \Delta_i, t\in \mbb{R}\}$.
In Theorem~\ref{thm:stable_oscillations}, we prove that this condition holds for each eigenvector direction.
%it is possible to have $r$-dimensional oscillations as long as the learning rate is chosen correctly. 


\begin{theorem}[Stable Subspace Oscillations]
\label{thm:stable_oscillations}
    Consider running GD on the deep matrix factorization loss in Equation~(\ref{eqn:deep_mf}) and denote the SVD of the target matrix as  $\mbf{M}_\star = \mbf{U}_\star\mbf{\Sigma}_\star\mbf{V}^\top_\star $, with distinct singular values $\sigma_{\star, 1} > \ldots > \sigma_{\star, r}$. Let $\Delta_i$  denote the $i$-th eigenvector of the Hessian with unit norm, $\lambda_{i}$ the corresponding eigenvalue after strict balancing occurs and denote $f_{\Delta_i}$ as the 1-D function at the cross section of the loss landscape and the line
following the direction of $\Delta_i$ passing the minima.
Then, if the minima of $f_{\Delta_i}$ satisfy $f_{\Delta_i}^{(3)}>0$ and $3[f_{\Delta_i}^{(3)}]^2 - f_{\Delta_i}^{(2)}f_{\Delta_i}^{(4)} > 0$, then 2-period orbit oscillation occurs in direction of $\Delta_i$ if $\eta>\frac{2}{\lambda_{i}}$. 
\end{theorem}
%\paragraph{Remarks.} By adopting this proof technique, we show that both conditions are true in each eigenvector direction considered in Theorem~\ref{thm:stable_oscillations}. Hence, Theorem~\ref{thm:stable_oscillations} that it is possible to have $r$-dimensional oscillations as long as the learning rate is chosen correctly. 


\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/svector_alignment_diff_lr.png}
    \caption{Illustrations of the singular vector and value evolution of the end-to-end DLN. The singular vectors of the network remain static across all iterations, as suggested by the singular vector stationary set, regardless of the learning rate. The angle between the true singular vectors and those of the network remains aligned throughout. The first singular values undergo oscillations in the large $\eta$ regime, whereas they remain constant in the small $\eta$ regime.}
    \label{fig:svec_alignment}
\end{figure}

\vspace{-0.25in}



\begin{wrapfigure}{r}{0.385\textwidth}
  \begin{center}
    \includegraphics[width=\linewidth]{figures/balancing_gap.pdf}
    
  \end{center}
  \caption{Plot of $| \sigma^2_{i,L}(t) - \sigma^2_{i,\ell}(t)|$ for initialization scale $\alpha = 0.01$ showing strict balancing.}
\label{fig:balancing}
\end{wrapfigure}
Recall that by our initialization scheme, we initialize to $\sigma_{L, i}(0) = 0$ and $\sigma_{\ell, i}(0) = \alpha$ for all $\ell \in [L-1]$. By considering the singular value scalar loss in Equation~(\ref{eqn:simplified_loss}), the gradient with respect to each $\sigma_{\ell, i}(t)$ are all the same. Thus, except for $\sigma_{L, i}(t)$, all of the other singular values across all weight matrices remain balanced throughout all iterations of GD. Lemma~\ref{lemma:balancing} states that throughout the course of GD, as long as $\alpha$ is below a certain threshold, $\sigma_{L, i}(t)$ becomes increasingly balanced to the rest of the singular values regardless of the learning rate. Since the balancing gap strictly decreases and $| \sigma^2_{i,L} - \sigma^2_{i,\ell} |$ is lower bounded by zero, it converges to 0 as $t \to \infty$. In Figure~\ref{fig:balancing}, we show that the gap indeed goes to zero empirically, and this is consistently the case across all of our experiments, with additional results provided in Appendix~\ref{sec:additional_exp}. Also in Appendix~\ref{sec:additional_exp}, we show that even for a slightly larger initialization scale, the balancing still holds in practice. Furthermore, we note that very similar results for imbalanced initialization have been proven by~\cite{wang2021large}, where our result can be viewed as an extension to multiple variables.
Finally, by assuming strict balancing, we can simplify the loss even further, particularly in the form $\sigma_i(\mbf{\Sigma}_{L:1}(t)) = \sigma^L_{i}(t)$, which allows us to solely analyze the dynamics of the singular values within the EOS regime. 


%However, it is worth noting that (i)~Lemma~\ref{lemma:balancing} does not state that the difference goes to zero, but rather that it becomes increasingly balanced and (ii) the bound on $\alpha$ may be too conservative and is an artifact of our analysis. 








\begin{comment}
    
\begin{figure}[htbp]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deep_martix_figs/oscillation_ranges.png}
        \caption{Figure 1}
        \label{fig:figure1}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deep_martix_figs/layer_1_singular_values_0.01546.png}
        \caption{$\eta=\frac{1}{L\sigma^{2-\frac{2}{L}}_{\star,1}}$}
        \label{fig:figure2}
    \end{minipage}%
    
    \vspace{0.5cm} % Adds some vertical space between rows

    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deep_martix_figs/layer_1_singular_values_0.0167.png}
        \caption{$\eta=\frac{1}{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, 1}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, 2}^{\frac{1}{L}\ell}\right)^2}$}
        \label{fig:figure3}
    \end{minipage}%
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/deep_martix_figs/layer_1_singular_values_0.02.png}
        \caption{$\eta>\frac{1}{L\sigma^{2-\frac{2}{L}}_{\star,2}}$}
        \label{fig:figure4}
    \end{minipage}
    \caption{}
\end{figure}





\end{comment}







\subsection{Relation to Diagonal Linear Networks}
Due to singular vector stationarity and balancing, DLNs may appear equivalent to diagonal linear networks at first glance. In this section, we characterize an explicit distinction between the two networks by deriving the eigenvalues of diagonal linear networks from the Hessian at convergence and explaining how they contribute to periodic oscillations in the EOS regime.

\begin{theorem}[Subspace Oscillation for Diagonal Linear Networks]
\label{thm:diag_lin_thm}
Let $\mbf{s}_\star \in \mbb{R}^{d}$ be an $r$-sparse vector with ordered coordinates such that $s_{\star,1} >\ldots>s_{\star,d}$. 
%Denote $I$ as the support set of $\mbf{s}_\star \in \mbb{R}^d$, $I^{C}$ as its complementary set, and $P$ denote the set containing $p$ largest elements in  $\mbf{s}^\star$. 
Suppose we run GD (\ref{eqn:gd}) with learning rate $\eta = \frac{2}{K}$, where $L s_{\star,p}^{2-\frac{2}{L}}\geq K>L s_{\star,p+1}^{2-\frac{2}{L}}$
for any $p < r$ and $\frac{s_{\star, p}}{s_{\star, 1}}> (\frac{1}{2})^{\frac{1}{4-4/L}} $ on the loss 
\begin{align*}
        \mathcal{L}\left(\{\mbf{s}_\ell\}_{\ell=1}^L \right) \coloneqq \frac{1}{2} \|\mbf{s}_1 \odot \ldots \odot \mbf{s}_{L} - \mbf{s}_{\star}\|_2^2, 
 \end{align*}
 with initialization $\mbf{s}_\ell = \alpha  \mbf{1}_d$ for all $\ell \in [L-1]$ where $\alpha < \left( \log\left( \frac{2\sqrt{2}}{\eta L s_{\star, 1}^{2 - \frac{2}{L}}} \right) \cdot 2 s_{\star, 1}^{\frac{4}{L}} \right)^{\frac{1}{4}}$, and $\mbf{s}_{L}=\mbf{0}_d$. Then, under strict balancing, the top-$p$ coordinates of $\mbf{s}_\ell$ oscillate within a $2$-period fixed orbit around the minima in the form
 \begin{align*}
     s_{\ell, i}(t) = \rho_{i, j}(t), \quad \forall i < p,  \, \forall \ell \in [L],
 \end{align*}
where $\rho_{i, j}(t) \in\{\rho_{i, 1}, \rho_{i, 2}\}$, $\rho_{i, 1} \in \left(0, s_{\star, i}^{1/L} \right)$ and $\rho_{i, 2} \in \left(s_{\star, i}^{1/L}, (2s_{\star, i})^{1/L} \right)$ are two real roots of the polynomial $h(\rho)=0$:
\begin{align*}
    h(\rho) = \rho^L\frac{1+\left(1 + \eta L(s_{\star,i}  - \rho^L)\cdot \rho^{L-2} \right)^{2L-1}}{1+\left(1 + \eta L(s_{\star,i}  - \rho^L)\cdot \rho^{L-2} \right)^{L-1}}- s_{\star,i}.
\end{align*}

\end{theorem}
\paragraph{Remarks.}

Similar to Theorem~\ref{thm:align_thm}, each coordinate of the diagonal linear network undergoes periodic oscillations as long as the learning rate is set to $\eta > 2/\lambda_1$, where $\lambda_1$ is the largest eigenvalue of the training loss Hessian. However, as shown in the proof of Theorem~\ref{thm:diag_lin_thm}, the main difference lies in the eigenvalues themselves -- the non-zero eigenvalue set of the diagonal linear network is much smaller than those of the DLN. For example, the top two eigenvalues of the diagonal linear network are $L s_{\star, 1}^{2 - \frac{2}{L}}$ and $L s_{\star, 2}^{2 - \frac{2}{L}}$, whereas the top two eigenvalues of the DLN are  $L s_{\star, 1}^{2 - \frac{2}{L}}$ and $\sum_{\ell=0}^{L-1} \left(s_{\star, 1}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot s_{\star, 2}^{\frac{1}{L}\ell}\right)^2$. Hence, while the dynamics for diagonal linear networks and DLNs both occur in $r$-dimensional subspaces based on our initialization, the two loss landscapes differ significantly (see Figure~\ref{fig:diff_dln_diag}). This implicit bias is absent for gradient flow or GD in the stable regime. 


% In the DLN, some eigenvalues are a function of a ``mixture'' of singular values, which arise from the off-diagonal elements in the decoupled loss in Equation~(\ref{eqn:simplified_loss}). In the diagonal linear network, they depend only on a single coordinate (i.e., $s_{\star,i}$) or the initialization scale $\alpha$. Hence, in the diagonal linear network, a much larger learning rate is required to observe oscillations in two or more coordinates compared to the DLN. 

