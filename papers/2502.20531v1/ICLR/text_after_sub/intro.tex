\section{Introduction}

Understanding generalization in deep neural networks requires an understanding of the optimization process in gradient descent (GD).
In the literature, it has been empirically observed that the learning rate $\eta$ plays a key role in driving generalization~\citep{hayou2024lora,lewkowycz2020large}.%, with larger learning rates often leading to better generalization performance. 
The ``descent lemma'' from classical optimization theory says that for a $\beta$-smooth loss $\mathcal{L}(\mathbf{\Theta})$ parameterized by $\mathbf{\Theta}$, gradient descent (GD) iterates satisfy
\begin{align*}
    \mathcal{L}(\mathbf{\Theta}(t+1)) \leq \mathcal{L}(\mathbf{\Theta}(t)) - \frac{\eta(2-\eta \beta)}{2} \|  \nabla \mathcal{L}(\mathbf{\Theta}(t)) \|_{2}^2,
\end{align*}
and so the learning rate should be chosen as $\eta < 2/\beta$ to monotonically decrease the loss.
%However, classical optimization theory on GD restricts itself to small infinitesimal learning rate and unrealistic convergence assumptions which barely holds for training deep networks in practice. Traditional analysis of GD show that for $L$-smooth loss, only if the step-size $\eta$ is selected to be less than $\frac{2}{L}$, the training is stable and the loss decreases monotonically.
However, many recent works have shown that the training loss decreases even for $\eta > 2/\beta$, albeit non-monotonically.
Surprisingly, it has been observed that choosing such a learning rate often provides better generalization over smaller ones that lie within the stability threshold. This observation has led to a series of works analyzing the behavior of GD within a regime dubbed ``the edge of stability'' (EOS). By letting $\mbf{\Theta}$ be a deep network, we formally define EOS as follows:
%Consider GD is run with learning rate $\eta$ to minimize a loss $l(\mbf{\theta})$ with a deep network. \cite{cohen2021gradient} observed a surprising and a commonly occuring phenomenon in deep networks coined "Edge of Stability" which is formally defined:
\begin{definition}[Edge of Stability~\citep{cohen2021gradient}]
During training, the sharpness of the loss, defined as $S(\mbf{\Theta}):= \| \nabla^2 \mathcal{L}(\mbf{\Theta})\|_{2}$, continues to grow until it reaches $2/\eta$, after which it stabilizes around $2/\eta$. During this process, the training loss behaves non-monotonically over short timescales but consistently decreases over long timescales.
\end{definition}

Using a large learning rate to operate within the EOS regime is hypothesized to give better generalization performance by inducing ``catapults'' in the training loss~\citep{zhu2023catapults}. Intuitively, whenever the sharpness $S(\mbf{\Theta})$ exceeds the local stability limit $2/\eta$, the GD iterates momentarily diverge (or catapults) out of a sharp region and self-stabilizes~\citep{damian2023selfstabilization} to settle for a flatter region where the sharpness is below $2/\eta$, which is known to generalize better~\citep{keskar2017on, izmailov2019averaging, petzka2021relative, foret2021sharpnessaware, gatmiry2023inductive}. Of course, the dynamics within EOS differ based on the loss landscape. For example, as illustrated in Figure~\ref{fig:catapults_vs_osc}, when the loss landscape is highly non-convex with many local valleys, catapults may occur, whereas sustained oscillations may exist for ``nicer'' landscapes. It is of great interest to understand the behaviors of such oscillations (or catapults) within different network architectures to further our understanding of EOS and how these behaviors can lead to better generalization.

From a theoretical perspective, there have been many recent efforts to understand EOS. These works generally focus on analyzing ``simple'' functions, examples including scalar losses \citep{minimal_eos,wang2023good,kreisler2023gradient}, quadratic regression models \citep{agarwala2022second}, diagonal linear networks \citep{even2024s} and two-layer matrix factorization \citep{chen2023edge}. 
However, the simplicity of these functions cannot fully capture the behaviors of deep neural networks within the EOS regime. Specifically, the following observations remain unexplained by existing analyses: (i) mild (or no) sharpening occurs when deep networks are trained using ``simple'' datasets~(Caveat 2 from~\citep{cohen2021gradient}); (ii) the oscillations and catapults in the weights occur within a low-rank subspace or within the top singular values of each weight matrix~\citep{zhu2024catapultssgdspikestraining}; and (iii) the oscillations are stronger within weight subspaces that generate more prominent features in the output fitting the data. For example, \cite{zhu2024catapultssgdspikestraining} observed that large catapults occurred within the top eigenspace of the NTK, followed by mild (or no) catapults in the complementary space, and \cite{rosenfeld2024outliers} showed that network inputs with large-magnitude features and strong opposing signals oscillate with larger spikes for those particular inputs.%
%\qq{need to discuss the limitation of previous results, especially for the last part} \smk{actually the limitation is that they cannot explain the phenomena}


\begin{comment}
    
These works developed our understanding of EOS but are far from capturing EOS dynamics occuring in deep neural networks. The formulation of deep neural networks is very close to deep matrix factorization problem which only exempts the use of non-linear activations and with a practical assumption that the underlying data is low rank. However, existing literature in deep matrix factorization focused on gradient flow dynamics \cite{arora2019implicit,saxe2014exact} or GD operating in the stable regime \cite{chou2024gradient,gidel} where training loss decreases monotoncally. We enlist important observations made while training deep network in the EOS regime which are currently unexplained by existing analysis in the literature:

\begin{enumerate}
\item  (Obs-1:) \textbf{Mild-sharpening} or no sharpening occurs 1) when deep networks are trained to fit \textit{simple} datasets (Caveat-2 \cite{cohen2021gradient}) (refer Figure) or 2) when the depth of the network is small.  % 
\item (Obs-2:) \textbf{Subspace Oscillation}: Oscillation/catapaults in the weight space occurs in a low rank subspace or within the top few sinuglar values of weight matrix in each layer \cite{zhu2024catapultssgdspikestraining}, hence preserving \textit{structure amidst chaos}.%
\item (Obs-3:) \textbf{Stronger oscillations} occur along weight subspace that generates prominent features in the output fitting the data. \cite{zhu2024catapultssgdspikestraining} observed that large catapaults occured in the top eigenspace of the NTK followed by mild/no catapaults in complementary space. \cite{rosenfeld2024outliers} showed that network inputs with large magnitude features and strong opposing signals oscillate with larger spikes for that particular input. %
\end{enumerate}

Our analysis of EOS on deep matrix factorization aims at explaining these 3 key observations and thereby attempting to bridge the gap between theory and practice. But before understanding EOS dynamics, it is important to understand gradient flow dynamics in deep matrix factorization. 
\end{comment}

\begin{figure}[t!]
    \centering
     \begin{subfigure}[b]{0.2475\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/intro/dln_train_loss.pdf}
        \caption{Linear Networks}
     \end{subfigure}
     \begin{subfigure}[b]{0.2475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/intro/mlp_train_loss.pdf}
         \caption{ReLU Networks}
     \end{subfigure}
        \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=0.95\textwidth]{figures/intro/lora_osc.pdf}
         \caption{BERT using LoRA}
     \end{subfigure}
     \vspace{-0.1in}
    \caption{Illustration of instability in the training loss for different network architectures. When the loss landscape is more complex with many local valleys (b, c), catapults occur, whereas free/sustained oscillation occur in (a). Parts with oscillations/catapults are zoomed in.}
    
    \label{fig:catapults_vs_osc}
\end{figure}

In this work, we attempt to bridge the gap between theory and practice by providing a fine-grained analysis of the learning dynamics of deep linear networks (DLNs) within the EOS regime. DLNs share many characteristics with their nonlinear counterparts, making them a useful surrogate for analyzing behaviors in deep neural networks~\citep{wang2024understandingdeeprepresentationlearning, zhang2024when}. For example,~\cite{zhang2024when} has provably shown that bias-free ReLU networks behave similarly to linear networks.
Generally, existing works on DLNs focus on analyzing the effect of depth and the initialization scale, and how they implicitly bias the trajectory of gradient flow towards low-rank solutions when the learning rate is chosen to be stable~\citep{saxe2014exact, arora2018optimization, implicit_dmf, zhang2024structure, pesme2023saddle, jacot2022saddletosaddle}.
Our analysis builds upon these works to show that DLNs exhibit intricate and interesting behaviors outside the stability regime. Our main contributions can be summarized as follows: 
%particularly periodic oscillations within a small subspace, where the subspace dimension is precisely characterized by the step size. 
%\qq{need a short title for each bullet point}
\vspace{-0.1in}
\begin{itemize}
\item \textbf{Periodic Subspace Oscillations.} We show that there exist periodic oscillations within $r$-dimensional subspaces in DLNs, where the rank $r$ is precisely characterized by the learning rate. This effectively explains why oscillations occur within top subspaces as observed by~\cite{zhu2023catapults}. We also prove that the learning rate needed to enter EOS is a function of the network depth, further revealing the role of depth in deep networks.
\item \textbf{Difference in DLNs and Diagonal Linear Networks.} We prove that the behavior of DLNs and  diagonal linear networks differ within the EOS regime. This also provides a unique perspective on how the landscape changes between the two networks, despite its similarities.
\item \textbf{Catapults in LoRA.} Lastly, we empirically demonstrate how oscillations and catapults may occur in different network architectures and their effects. We show that, for a sufficiently large learning rate, updating low-rank adaptors (LoRA) \citep{hu2022lora} during the fine-tuning of LLMs can drive the entire network into the EOS regime, where catapults can potentially improve generalization.
    %\item  The dynamics in deep matrix factorization and diagonal linear networks differ in the EOS regime, unlike in gradient flow (GF). These dynamics depend on the dimensionality of the layers (whether vector in diagonal linear networks, matrix in deep matrix factorization, or tensor in tensor factorization), which is not the case in GF.
    %\item  We prove that in deep linear networks, only the depth and the largest singular value of the target matrix determine whether the network will operate in the EOS regime.
    %\item The condition to enter the EOS regime is independent of the initialization scale and the dimensionality of the layers, dimension the rank of the target matrix. 
\end{itemize}

\begin{comment}
    
\subsection{Implicit regularization in deep linear models with gradient flow}
Over the past few years, several works have analyzed the effect of depth and initialization in deep linear networks \cite{saxe2014exact, arora2018optimization, implicit_dmf, zhang2024structure}. It has been observed that small initialization and larger depth make the trajectory of gradient flow implicitly biased towards low-rank solutions. Similar observations have been made in diagonal linear networks \cite{woodworth2020kernel, pesme2021implicit}, where the scale of initialization determines whether the dynamics of the iterates are in the rich regime or the kernel regime. Smaller initialization promotes rich regime dynamics, where features are learned incrementally through saddle-to-saddle dynamics \cite{pesme2023saddle, jacot2022saddletosaddle}. Larger depth increases the duration of these saddle points and has a stronger implicit bias towards exact low-rank solutions by suppressing the residual singular values through the depth factor \cite{arora2018optimization}. In deep matrix factorization gradient flow aligns the singular vectors of successive layer factors \cite{du2018algorithmic}. 

Due to this alignment, the flow dynamics in deep matrix factorization are the same as those in deep linear networks, but act on the singular values of the matrices.

Our analysis and experiments reveal certain surprising (yet explainable) phenoemons that is critical to understand Edge of Stability:

\begin{itemize}
    \item  The dynamics in deep matrix factorization and diagonal linear networks (or even tensor factorization) differ in the EOS regime, unlike in gradient flow (GF). These dynamics depend on the dimensionality of the layers (whether vector in diagonal linear networks, matrix in deep matrix factorization, or tensor in tensor factorization), which is not the case in GF.
    \item  We prove that in deep linear networks, only the depth and the largest singular value of the target matrix determine whether the network will operate in the EOS regime.
    \item The condition to enter the EOS regime is independent of the initialization scale and the dimensionality of the layers, dimension the rank of the target matrix. 
\end{itemize}


\ag{I am unsure whether it is better to state it after notations otherwise it will be a reptitition. Probably we should state this in the contribution section?}


\end{comment}





%\paragraph{Related Works.} We briefly survey a few related works to highlight their differences, and provide a detailed discussion in \Cref{app:related}. 
%    DLNs are often used as prototypes to study the behaviors of nonlinear networks~\citep{benign, saxe2014exact, wang2024understanding, nc}. The most relevant literature on DLNs are those by Yaras et al.~\citep{yaras2023law, yaras2024compressible} and Kwon et al.~\citep{kwon}, who reveal that the weight updates of deep networks occur within an invariant subspace. Our work differs from that of Yaras et al.~\citep{yaras2023law} in that we fully capture the learning dynamics of DLNs throughout the entire GD process. While Kwon et al.~\citep{kwon} observe invariant weight updates, they use this observation for model compression and do not study the learning dynamics with large learning rates. Regarding the edge of stability, the most relevant works are those that analyze scalar functions to demonstrate that the edge of stability occurs on such functions, which have a non-zero third-order derivative and satisfy certain regularity conditions~\citep{chen2023edge,wang2023good,minimal_eos}. However, as mentioned previously, these works do not capture the more complicated models that we consider in this work.


\section{Notation and Problem Setup}

%\qq{we need to be consistent, some place we call it deep linear networks, other places we call it deep matrix factorization. Let us call it "deep linear network", and training loss for DLN}

\paragraph{Notation.}
We denote vectors with bold lower-case letters (e.g., $\mbf{x}$)
and matrices with bold upper-case letters (e.g., $\mbf{X}$).
%Given any $n \in \N$,
We use $\mbf{I}_n$ to denote an identity matrix of size $n \in \N$.
We use $[L]$ to denote the set $\{1, 2, \ldots, L\}$. 
We use the notation $\sigma_i(\mbf{A})$ to denote the $i$-th singular value of the matrix $\mbf{A}$.

\begin{comment}
\paragraph{Deep linear model}
Consider the problem of least squares fitting problem with multidimensional output. We want to learn a mapping $f(.):\mbb{R}^{d}\rightarrow \mbb{R}^{d}  $ given $d$ number of input vectors $\mbf{x}_{i} \in \mbb{R}^{d}$ to same number of target vector $\mbf{y}_{i} \in \mbb{R}^{d}$. Considering $f(.)$ to be deep linear model with set of parameters  $\mbf{\Theta} = \left(\mbf{W}_1, \mbf{W}_2, \ldots, \mbf{W}_L \right)$, then the fitting regression problem can be formulated as minimizing the loss wrt $\frac{1}{2} \|\mbf{Y} - \mbf{X} \mbf{W}_L \cdot \ldots \cdot \mbf{W}_1\|^2_{F} $ where $\mbf{X}$ and $\mbf{Y}$ are column stacking of the input  $\left\{\mathbf{x}_{i}\right\}_{i=1}^d$ and target $ \left\{\mathbf{y}_{i}\right\}_{i=1}^d$. Assuming the data matrix $\mbf{X}$ to be whitened matrix with dominant features such that $\mbf{X}^T \mbf{Y} = \mbf{M}$ (say) is low rank, then the optimization can be formulated as a low rank recovery problem. Throughout the paper, we will restrict ourselves to analyze the deep matrix factorization problem, which we introduce next.
\end{comment}


 \paragraph{Deep Matrix Factorization Loss.}
The objective in deep matrix factorization is to model a low-rank matrix $\mbf{M}_\star \in \mbb{R}^{d\times d}$ with $\rank(\mbf{M}_\star) = r$ via a DLN parameterized by a set of parameters $\mbf{\Theta} = \left(\mbf{W}_1, \mbf{W}_2, \ldots, \mbf{W}_L \right)$, which can be estimated by solving
\begin{align}\label{eqn:deep_mf}
    \underset{\mbf{\Theta}}{\rm{arg min}} \, f(\mbf{\Theta}) \coloneqq \frac{1}{2}\|\underbrace{\mbf{W}_L \cdot \ldots \cdot \mbf{W}_1}_{\eqqcolon \mbf{W}_{L:1}} - \mbf{M}_\star\|^2_{\mathsf{F}},
\end{align}
where we adopt the abbreviation $\mbf{W}_{j:i} = \mbf{W}_{j}\cdot \ldots \cdot \mbf{W}_i$ to denote the end-to-end DLN and is identity when $j < i$. We assume that each weight matrix has dimensions $\mbf{W}_\ell \in \mbb{R}^{d\times d}$ to observe the effects of overparameterization.





\paragraph{Optimization.}
We update each weight matrix $\mbf W_\ell \in \mbb{R}^{d\times d}$ using GD with iterations given by
\begin{align}\label{eqn:gd}
    \mbf{W}_\ell(t) = \mbf{W}_\ell(t-1) - \eta\cdot \nabla_{\mbf{W}_\ell}f(\mbf{\Theta}(t-1)), \quad \forall \ell \in [L],
\end{align}
where $\eta > 0$ is the learning rate and $\nabla_{\mbf{W}_\ell}f(\mbf{\Theta}(t))$ is the gradient of $f(\mbf{\Theta})$ with respect to the $\ell$-th weight matrix at the $t$-th GD iterate. We initialize the network as the following:
\begin{align}\label{eqn:init}
    \mbf{W}_L(0) = \mbf{0}, \quad \quad\quad \mbf{W}_\ell(0) = \alpha \mbf{I}_d, \quad \forall \ell \in [L-1],
\end{align}
where $\alpha > 0$ is a small constant. The zero weight layer can be viewed as the limiting case of initializing the weights with a (very) small constant $\alpha' \ll \alpha$, and has been similarly explored by~\cite{two_layer_bias}, albeit for two-layer networks. 

Previous works on both shallow and deep linear networks assume a zero-balanced initialization (i.e., $\mbf{W}_i^\top(0)\mbf{W}_i(0) = \mbf{W}_j(0)\mbf{W}^\top_j(0)$ for $i \neq j$). This introduces the invariant $\mbf{W}_i^\top(t)\mbf{W}_i(t) = \mbf{W}_j(t)\mbf{W}^\top_j(t)$ for all $t > 0$, ensuring two (degenerate) conditions throughout the training trajectory: (i) the singular vectors of each of the layers remain aligned and (ii) the singular values stay balanced.

Our initialization lifts condition (ii). 
Rather than staying balanced, we show that the singular values become increasingly balanced (see Lemma~\ref{lemma:balancing}).
We also show that our analysis is not limited to this specific initialization but applies to \emph{any initialization} that converges to the singular vector stationary set (see Proposition~\ref{prop:svs_set}). To the best of our knowledge, it is common to assume that the singular vectors remain aligned, as many existing works make the same assumption~\citep{two_layer_bias, implicit_dmf, saxe2014exact, gidel,chou2024gradient, kwon}. 



%In the following section (as well as Appendix~\ref{sec:additional_exp}), we provide examples of such initializations, which we adopt for our analyses as well.



%\smk{I am pretty adamant that we should have this section like this; it has clear motivation.} \ag{Slightly debatable, but let's hear opinions of others. }
%\ag{@Qing: would it be better if we specify the general class of initialization instead of the zero-one initilaization in the main text?}\smk{Note that we never prove that any initialization belongs in the SVS set! And we need to then change all of our proofs to be $\mbf{Q}$ orthogonal matrices rather than the target matrix $\mbf{V}$ like we have now.}
 % For deep linear networks, it is common to use an orthogonal-type initialization rather than a random one~\citep{kwon, kwon2024on, yaras2023law}, as it has been provably shown that random initialization slows down convergence~\citep{Hu2020Provable}.