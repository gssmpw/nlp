\section{Experimental Results}
In this section, we present experimental results to supplement our theory along with experiments on nonlinear networks within the EOS regime. Section~\ref{sec:oscillations_exp} presents experiments that corroborate our theory. Section~\ref{sec:lora_exp} shows experiments where oscillations occur when using low-rank adaptors and their relation to generalization. Finally, Section~\ref{sec:unexplained_exp} discusses phenomena currently unexplained in the literature and how our theory can account for them.



\begin{figure}[t!]
    \centering
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_none.pdf}
     \end{subfigure}
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_1.pdf}
     \end{subfigure}
         \begin{subfigure}[b]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_2.pdf} 
     \end{subfigure}
        \newline
      \begin{subfigure}[b]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_diag_none.pdf}
         \caption*{$\eta < 2/\lambda_1$}
     \end{subfigure}
     \begin{subfigure}[b]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_diag_1.pdf}
         \caption*{$2/\lambda_1 <\eta < 2/\lambda_2$}
     \end{subfigure}
         \begin{subfigure}[b]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/oscillation/eos_sval_diag_2.pdf} 
         \caption*{$\eta > 2/\lambda_2 > 2/\lambda_1$}
     \end{subfigure}   
    \caption{Top: Plots of the singular values of the end-to-end DLN; Bottom: plots of the end-to-end diagonal linear network, where $\lambda_i$ denotes the $i$-th largest eigenvalue of the Hessian. While the largest eigenvalue for both networks are the same $\lambda_1 = L\sigma_{\star,1}^{2- \frac{2}{L}}$, the second largest eigenvalue is different, and we use
    $\lambda_2 = \sum_{\ell=0}^{L-1} \left(\sigma_{\star, 1}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, 2}^{\frac{1}{L}\ell}\right)^2$, which is the second largest eigenvalue of the DLN. While the DLN undergoes oscillations in the top-2 components, the diagonal linear network only shows oscillations in 1.}
    \label{fig:diff_dln_diag}
\end{figure}

\subsection{Subspace Oscillations in Deep Networks}
\label{sec:oscillations_exp}

\begin{wrapfigure}{r}{0.35\textwidth}
  \begin{center}
    \includegraphics[width=0.35\textwidth]{figures/oscillation_range.pdf}
  \end{center}
  \caption{Oscillation range as a function of the learning rate.}
\label{fig:osc_range}
\end{wrapfigure}
Firstly, we provide experimental results corroborating Theorem~\ref{thm:align_thm} and Theorem~\ref{thm:diag_lin_thm} to highlight their differences. We let the target matrix be $\mbf{M}_\star \in \mbb{R}^{50 \times 50}$ with rank 3, with dominant singular values $\sigma_{\star} = {10, 9, 6}$.
For the DLN, we consider a 3-layer network, with each layer as $\mbf{W}_\ell \in \mbb{R}^{50\times 50}$ and use an initialization scale of $\alpha = 0.01$. For the diagonal linear network, we consider a similar setup, with initialization scale $\alpha = 0.01$ and the top-3 elements of $\mbf{s}_\star \in \mbb{R}^{50}$ to be $10, 9, 6$. 
In Figure~\ref{fig:diff_dln_diag}, we present the behaviors of both end-to-end networks under different learning rate regimes. By Theorem~\ref{thm:align_thm} and Theorem~\ref{thm:diag_lin_thm}, the largest eigenvalue $\lambda_1$ is the same for both networks, and thus both undergo oscillations in the largest component when $\eta > 2/\lambda_1$. 
However, the difference in the loss landscape plays a role in oscillations for the second component.
When $\eta > 2/\lambda_2$, where $\lambda_2$ is the second largest eigenvalue of the DLN, the diagonal linear network does not experience oscillations in the second component, as its second largest eigenvalue is much smaller. 
This highlights a distinction between the two networks and how the landscape changes between them. In Figure~\ref{fig:osc_range}, we present an experiment demonstrating the relationship between the range of oscillations and the learning rate by plotting the amplitude of the singular value oscillations in the end-to-end network.
We can clearly see that there exists no oscillations when $\eta < 2/\lambda_1$, but begins to occur as soon as $\eta > 2/\lambda_1 $. As the learning rate increases, the amplitude also increases, which can similarly be observed in Figure~\ref{fig:diff_dln_diag}. 






\subsection{Oscillations in Low-Rank Adaptors}
\label{sec:lora_exp}


Low-rank adaptation (LoRA)~\citep{lora} has arguably become one of the most popular methods for fine-tuning deep neural networks. The main idea behind LoRA is that rather than training from scratch, we can update two low-rank factor matrices to ``append'' onto an existing weight matrix.
That is, give a pre-trained weight matrix $\mathbf{W}_0 \in \mathbb{R}^{d_1 \times d_2}$, LoRA involves updating two low-rank factors commonly referred to as ``adaptors'':
\begin{align*}
    \underbrace{\mathbf{W}_\star}_{\text{new weight}} = \underbrace{\mathbf{W}_0}_{\text{pre-trained weight}} + \underbrace{\mathbf{AB}^\top}_{\text{adaptors}}.
\end{align*}
Upon training only $\mathbf{A} \in \mbb{R}^{d_1 \times r}$ and $\mathbf{B} \in \mbb{R}^{d_2 \times r}$, $\mathbf{W}_\star \in \mbb{R}^{d_1 \times d_2}$ is used for inference. By viewing the adaptations as individual low-rank matrix factorization problems, then this formulation closely aligns with our theoretical setup with a depth of $2$. Hence, we investigate (i) how oscillations may appear in such a setup and (ii) what these oscillations may imply in terms of generalization.


To this end, we follow the setup used by~\cite{yaras2024compressible} and consider a pre-trained BERT~\citep{wang2018glue} base model and apply
adaptation on all attention and feedforward weights in the transformer, resulting in 72 adapted
layers in total. 
We choose an initialization scale of $\alpha = 10^{-3}$ for the adaptors and randomly sample $512$ examples from the STS-B~\citep{wang2018glue} dataset for fine-tuning.  We choose a batch size of 64  with a maximum sequence length of 128 tokens. 
First, we experiment how large the rank of the adaptors must be to drive the entire network to EOS. Using a learning rate of $\eta = 10^{-4}$, Figure~\ref{fig:rank_vs_correlation} shows oscillatory behavior across all ranks. However, this behavior may also be an artifact of the stochasticity induced by updating with only a batch of samples. Notably, for ranks $r = 4$ and $r = 8$, there are catapults (or jumps) that occur very early in the iterations, which do not occur for $r = 1$ or $r = 2$. Then, using a rank of $r=8$, we experiment what the oscillations and catapults might imply in terms of generalization.
\begin{wrapfigure}{l}{0.375\textwidth}
  \begin{center}
    \includegraphics[width=0.375\textwidth]{figures/bert/train_loss_ranks.pdf}
  \end{center}
  \caption{Catapults in the training loss for different ranks for LoRA.}
\label{fig:rank_vs_correlation}
\end{wrapfigure}
 In Figure~\ref{fig:lr_vs_correlation}, we present the training loss and Pearson correlation for different learning rates.
 When the learning rate is large enough to induce catapults (as seen for $\eta = 10^{-4}$), we observe that the training loss decreases rapidly, resulting in a very high Pearson correlation. In contrast, when the learning rate is small, convergence takes much longer, as the model seems to bounce around within the same local basin before reaching a low training loss. These results suggest that there may exist a combination of rank and learning rate that can converge to an optimal solution (as measured by Pearson correlation) more quickly and with better generalization by inducing catapults in the loss through EOS, as the learning rate of $10^{-4}$ appears to yield the highest Pearson correlation.
We leave for future work a careful study of this observation, aiming to accurately select the learning rate to maximize the efficiency of LoRA.

 \begin{comment}
    \begin{figure}[h!]
    \centering
     \begin{subfigure}[t!]{0.495\textwidth}
         \centering
        \includegraphics[width=0.87\textwidth]{figures/bert/train_loss.pdf}
     \end{subfigure}
     \begin{subfigure}[t!]{0.495\textwidth}
         \centering
         \includegraphics[width=0.9\textwidth]{figures/bert/test_loss.pdf}
     \end{subfigure}
    \caption{Illustration of the different behaviors in the training loss for various ranks of the adaptors at a fixed learning rate of $\eta = 10^{-4}$. While each choice of rank exhibits oscillations, higher ranks show more ``catapults'', which appear to be correlated with generalization as measured by Pearson correlation.}
    
    \label{fig:rank_vs_correlation}
\end{figure}
\end{comment}

\begin{figure}[t!]
    \centering
     \begin{subfigure}[t!]{0.495\textwidth}
         \centering
        \includegraphics[width=0.75\textwidth]{figures/bert/train_loss_diff_lr.pdf}
     \end{subfigure}
     \begin{subfigure}[t!]{0.495\textwidth}
         \centering
         \includegraphics[width=0.75\textwidth]{figures/bert/test_loss_diff_lr.pdf}
     \end{subfigure}
    \caption{Illustration of different behaviors in the training loss for various learning rates with a fixed rank of $r=8$ for fine-tuning BERT using LoRA. When the learning rate is large enough to induce catapults in the loss (visible early in the stages for $\eta = 10^{-4}$), the adaptors converge to a point with high Pearson correlation much more rapidly.}
    
    \label{fig:lr_vs_correlation}
\end{figure}

\subsection{Deep Linear Networks to Explain Phenomena in Nonlinear Networks}
\label{sec:unexplained_exp}

%In this section, we draw parallels between observations made in deep nonlinear networks and our theory on DLNs.



\paragraph{Mild Sharpening.} \cite{cohen2021gradient} discussed in Caveat 2 of their work that sharpness does not always rise to $2/\eta$, especially in tasks with low complexity. We re-illustrate this in Figure~\ref{fig:combined_figures}, 
\begin{wrapfigure}{r}{0.375\textwidth}
  \begin{center}
    \includegraphics[width=0.375\textwidth]{figures/intro/eos_nuance.pdf}
    \end{center}
    \caption{DLNs do not enter EOS regime if $L\sigma^{2-\frac{2}{L}}_{1}< 2/\eta$.}
    \label{fig:dln_mild_sharpening}
\end{wrapfigure}
where we plot sharpness in two settings: (i) regression with simple images and (ii) classification with an MLP using a subset of the CIFAR-10 dataset. For the regression task, we minimize the loss $\mathcal{L}(\Theta) = \|G(\Theta) - \mbf{y}_{\mathrm{image}}\|_2^2$, where $G(\Theta)$ is a UNet parameterized by $\Theta$, and $\mbf{y}_{\mathrm{image}}$ denotes one of the images in Figure~\ref{fig:img_used}. We observe that when $\mbf{y}_{\mathrm{image}}$ is a smooth, low-frequency image, the sharpness of the loss generally remains low. However, when $\mbf{y}_{\mathrm{image}}$ has higher frequency content, the sharpness increases and enters the EOS regime (Figure~\ref{fig:eos-dip}). Similarly, for the classification task, we train a 2-layer fully connected neural network on $N$ labeled training images from the CIFAR-10 dataset using MSE loss and plot the sharpness in Figure \ref{fig:small_Sharp}. The sharpness of the loss is linked to $N$, the number of data points used for training. For small $N$ values, such as 100 or 200, the network learns only a limited set of latent features, resulting in mild sharpening, and it does not reach the EOS threshold. However, when $N$ exceeds 1000, the sharpness increases and reaches the EOS threshold.
These observations can also be seen in DLNs. In Figure~\ref{fig:dln_mild_sharpening}, we show that the sharpness reaches $L\sigma_{\star, 1}^{2-\frac{2}{L}}$, where $\sigma_{\star, 1}$ is the singular value of the target matrix. Whenever $L\sigma_{\star, 1}^{2-\frac{2}{L}} < 2/\eta$, the network will not enter the EOS regime. This can be viewed as low-complexity learning, as $\sigma_{\star, 1}$ corresponds to the magnitude of the strongest feature of the target matrix. Hence, when $\sigma_{\star, 1}$ is not large enough, the sharpness will not rise to $2/\eta$. Lastly,~\cite{cohen2021gradient} also observed that sharpness scales with the depth of the network, which is also directly supported by our analysis.




\paragraph{Sustained vs. Free Oscillations.}


Here, we discuss the differences in oscillations between ``simpler'' networks such as DLNs and those studied in the literature, versus oscillations and catapults in practical deep nonlinear networks. The main difference lies in the loss landscape—at convergence, the Hessian for DLNs is positive semi-definite, as shown in Lemma~\ref{lemma:hessian_eigvals}, meaning there are only directions of positive curvature and flat directions (in the null space of the Hessian). In this landscape, oscillations occur due to bouncing off the basin walls, with no direction of escape. However, in deep nonlinear networks, it has been frequently observed that the Hessian at the minima has negative eigenvalues \citep{ghorbani2019investigation, sagun2016eigenvalues}. This enables an escape direction along the negative curvature, preventing sustained or free oscillations. We believe understanding the role of nonlinear activation layers in altering this loss landscape is key to addressing this difference.

Lastly,~\cite{damian2023selfstabilization} studied self-stabilization, where sharpness decreases below $2/\eta$ after initially exceeding $2/\eta$. Let $u(\theta)$ be the top eigenvector direction and $\nabla S(\theta)$ denote the change in sharpness. They assume that $\nabla L(\theta) \cdot u(\theta) = 0$ and that $\nabla S(\theta)$ lies in the null space of the Hessian. However, \emph{these assumptions are not satisfied for DLNs}. Due to Proposition~\ref{prop:svs_set} and Lemma~\ref{lemma:balancing}, the dynamics reduce to a one-dimensional oscillating trajectory, where both $\nabla L(\theta)$ and $\nabla S(\theta)$ lie in the range space of the Hessian. By letting $\pi(\theta)$ represent the scalar product of singular values, $\nabla L(\theta) \cdot u(\theta) = (\pi(\theta) - s)\pi(\theta)\cdot|\theta^{-1}|$, which is not zero unless the loss is zero (which is not the case in EOS). Hence, in DLNs, self-stabilization does not occur, and the iterates and sharpness oscillate above $2/\eta$.


\begin{comment}
\subsection{Explaining Sharpening Phenomenon in Deep Nonlinear Network}
\label{sec:unexplained_exp}

Based on our analysis, we draw parallels to two common observation in deep non-linear networks:\\

1) \textbf{Mild Sharpening in low complexity tasks:} The extent of sharpening in deep neural networks is often related to the complexity of the learning task. To illustrate this, we consider two commonly used tasks: regression and classification.\\
\textit{Regression Task}: In Figure \ref{fig:eos-dip}, we minimize the loss function $L(\boldsymbol{\theta})= \| G(\boldsymbol{\theta}) - \mbf{y}_{im} \|_{2}^2$, fitting an image $\mbf{y}_{im}$ using a deep image generator network like Unet $G(\boldsymbol{\theta})$, with Gradient Descent. We observe that the degree of sharpening in this network depends on the target image $\mbf{y}_{im}$. For example, when $\mbf{y}_{im}$ is a smooth, low-frequency image, the sharpness of the loss $ \| \nabla^2 L(\boldsymbol{\theta}) \|_{2}$ remains low. In contrast, when fitting a detailed image, such as white noise with high-frequency content, the sharpness increases and enters the EOS regime $\frac{2}{\eta}$. In particular,we observe a steady correlation between the sharpness rise (Figure-\ref{fig:eos-dip}) and the high frequency content of the fitting image (Figure-\ref{fig:img_used}). \\
\textit{Classification task}:  In Figure \ref{fig:small_Sharp}, we train a 2-layer fully connected neural network on $N$ pairs of labeled training images from the CIFAR-10 dataset using MSE loss. The sharpness of the loss is linked to $N$, the number of data points used for training. For small $N$ values, such as 100 or 200, the network learns only a limited set of latent features, resulting in mild sharpening, and the network does not reach the EOS threshold. However, when $N$ exceeds 1000, the sharpness increases and reaches the EOS threshold $\frac{2}{\eta}$. Furthermore, Figure-16 in \cite{cohen2021gradient} reported that sharpness also increases with depth of the network. So, the sharpness limit correlates both with network depth and complexity of the learning problem. \\
These two occurences in non-linear network is partially addressed by our analysis of deep matrix factorization at EOS. We show that sharpness rises to $L\sigma^{2-\frac{2}{L}}_{1}$, where $\sigma_{1}$ is the top singular value of the target matrix and $L$ is the depth. In deep matrix factorization problem, $\sigma_{1}$ corresponds to the magnitude of the strongest feature in the data and is related to the complexity of learning. Whenever, $L\sigma^{2-\frac{2}{L}}_{1}<\frac{2}{\eta}$, the network do not enter the EOS regime (Figure-\ref{fig:dln_mild_sharpening}). Similarly, in both regression and classification experiments in Figure \ref{fig:combined_figures}, whenever data has low complexity, sharpness do not rises and network fails to reach EOS. Furthermore, sharpness $L\sigma^{2-\frac{2}{L}}_{1}$ scales with depth of the network $L$, supporting the observation by \cite{cohen2021gradient}.

2) \textbf{Oscillation range in strong feature direction:}
\end{comment}

\begin{figure}[t!]    
\centering
    \begin{subfigure}[t]{0.32\textwidth} % Subfigure (a) - increased size
        \centering
        \includesvg[width=\textwidth]{figures/dip_eos/eos.svg}
        \caption{Sharpness plots for training image generator networks using SGD with learning rate $\eta = 2^{-4}$.}
        \label{fig:eos-dip}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth} % Subfigure (b) - increased size
        \centering
        \includesvg[width=\textwidth]{figures/dip_eos/img_used.svg}
        \caption{Target images (denoted as $\mbf{y}_{\mathrm{image}}$) with different frequencies used for training.}
        \label{fig:img_used}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.32\textwidth} % Subfigure (c) - increased size
        \centering
        \includegraphics[width=\textwidth]{figures/small_sharpness.png}
        \caption{2-layer FC network trained with small number $N$ of CIFAR-10 dataset with $\eta=1e-2$}
        \label{fig:small_Sharp}
    \end{subfigure}
    \caption{Illustration of Caveat 2 by~\cite{cohen2021gradient} on how mild sharpening occurs on simple datasets and network. (a) Regression task showing the evolution of the sharpness when an UNet (with fixed initialization) is trained to fit a single image shown in (b). (c) Evolution of the minimal progressive sharpening on a classification task of a 2-layer MLP trained on a  subset of CIFAR-10.}
    \label{fig:combined_figures}
\end{figure}

\begin{comment}

\subsection{Difference of EOS dynamics in non-linear networks and deep matrix factorization }

Although mild sharpening and strong directional oscillation phenomenon is partially explained from analyzing deep matrix factorization, we highlight some critical differences between EOS dynamics in non-linear networks and liner networks:

\begin{itemize}
    \item \textbf{Sustained oscillation in DMFs vs catapaults in non-linear networks}: In deep matrix factorization (DMF) loss whenever $\eta > \frac{2}{L \sigma_{1}^{2-\frac{2}{L}}}$, we observe free oscillation in 2-period orbit \footnote{At higher learning rate, iterates enter higher period orbit oscillations, chaos and divergence}. However, in nonlinear networks we observe catapaults or sudden spikes. This is primarily because of difference in loss landscape. At global minima, Hessian of the loss in DMF is provably Positive Semi-Definite (PSD)\footnote{we explicitly derived all positive eigenvalues}. So there are only directions of positive curvature and flat directions (in null space of the Hessian). In this landscape,\textit{ oscillations occur due to bouncing of the basin of the wall with no direction of escape.} But in deep non-linear networks, it is has been frequently observed that the Hessian at the minima has negative eigenvalues \cite{ghorbani2019investigation,sagun2016eigenvalues}. This enables an escape direction in the direction of the negative curvature preventing sustained/free oscillation. We believe understanding the role of non-linear activation layers in changing this loss landscape is the key to address this difference.
    \item \textbf{Absence of self-stabilization effect \cite{damian2023selfstabilization} in deep matrix factorization}
    Damian et al. studied self-stabilization, where sharpness decreases below $\frac{2}{\eta}$ after exceeding $\frac{2}{\eta}$. They assume that $\nabla L(\theta) \cdot u(\theta) = 0$ and $\nabla S(\theta)$ lie in the null-space of the Hessian. However, in deep linear networks, \textit{these assumptions are not satisfied}. Due to singular vector alignment and balancing, the dynamics reduce to a one-dimensional oscillating trajectory, where both $\nabla L(\theta)$ and $\nabla S(\theta)$ lie in the range space of the Hessian. Letting $\pi(\theta)$ represent the scalar product, $\nabla L(\theta) \cdot u(\theta) = (\pi(\theta) - s)\pi(\theta)|w^{-1}|$, which isn't zero unless the loss is zero (not the case in EoS). Hence, in deep linear networks, self-stabilization does not occur, and the iterates and sharpness oscillate above $\frac{2}{\eta}$.

\end{itemize}
\end{comment}
\section{Conclusion, Limitations and Future Work}

In this paper, we presented a fine-grained analysis of the learning dynamics of deep matrix factorization with the aim of understanding unexplained phenomena in deep nonlinear networks within the EOS regime. Our analysis revealed that within EOS, DLNs exhibit periodic oscillations in small subspaces, where the subspace dimension is exactly characterized by the learning rate. 
%This allowed us to explain phenomena such as mild sharpening, oscillations occurring only within the top singular values, and to distinguish the differences between the behaviors of DLNs and diagonal linear networks. 

There are two limitations to our work: (i) we require that the dynamics converge to the singular vector stationary set, and (ii) we require strict balancing of the singular values. Regarding the singular vector stationary set, we leave it for future work to analyze cases where initialization occurs outside the singular vector stationary set, which requires jointly analyzing the oscillation in both singular vectors and singular values. It should be noted, however, that the oscillations largely occur due to the singular values, so our results still accurately capture the behavior of DLNs at EOS.
As for balancing, there may exist cases where oscillations begin before strict balance is achieved. We leave it to future work to find subspace oscillations without assuming strict balancing before oscillation starts.
