\documentclass[11pt]{article}

%%% ready for submission %%%
\input{ICLR/math_commands.tex}
\input{ICLR/commands}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{tgpagella}
\usepackage{tcolorbox}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{svg}

\usepackage{enumitem}
\usepackage{authblk}
\usepackage{natbib}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}
\usepackage{url}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{lipsum}  
\usepackage{cleveref}
\usepackage{xcolor}  
\usepackage{wrapfig}% colors
%\usepackage{natbib}
\usepackage{multirow}
% \bibliographystyle{achemso}

\newcommand{\conv}{\circledast}
\newcommand{\cconv}{  \boxasterisk }
\newcommand{\mb}{\mathbf}
\newcommand{\mc}{\mathcal}
\newcommand{\mf}{\mathfrak}
\newcommand{\md}{\mathds}
\newcommand{\bb}{\mathbb}
\newcommand{\magnitude}[1]{ \left| #1 \right| }
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\condset}[2]{ \left\{ #1 \;\middle|\; #2 \right\} }
\newcommand{\tbW}{\widetilde{\bm{W}}}
\newcommand{\bWcomp}{\bm{W}^{\textrm{comp}}}
\newcommand{\DS}[1]{{\color{magenta} (DS: #1)}}
\newcommand{\ZK}[1]{{\color{cyan} [{\em Zekai:} #1]}}
\newcommand{\SMK}[1]{{\color{red} [{\em SMK:} #1]}}

\newcommand{\qq}[1]{\textcolor{blue}{\bf [{\em Qing:} #1]}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
%%% to compile a preprint version, e.g., for submission to arXiv, add the [preprint] option %%%
%\usepackage[preprint]{cpal_2024}
%\newcommand{\ag}[1]{{\color{blue} [\textbf{AG:} #1]}}

%%% to compile a camera-ready version, add the [final] option %%%
%\usepackage[final]{cpal_2024}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}


%add packages
\usepackage{url}

%\title{Compressing Overparameterized Deep Models by Harnessing Low-Dimensional Learning Dynamics}
\title{Learning Dynamics of Deep Linear Networks Beyond the Edge of Stability}

%\author{%
%  Soo Min Kwon\textsuperscript{1}\thanks{The first two authors have contributed equally to this work. Correspondence to \texttt{kwonsm@umich.edu}.}, ~Zekai Zhang\textsuperscript{2}$^*$, ~Dogyoon Song\textsuperscript{1}, ~Laura Balzano\textsuperscript{1}, ~Qing Qu\textsuperscript{1} \\
%  \vspace{12pt}
%  \textsuperscript{1} Department of EECS, University of Michigan \\
%  \vspace{3pt}
%  \textsuperscript{2} Department of Automation, Tsinghua University \\
%}
\author[1]{Avrajit Ghosh\footnote{The first two authors contributed to this work equally. Correspondence to \texttt{ghoshavr@msu.edu}, \texttt{kwonsm@umich.edu}. Code to reproduce the results is available at \href{https://github.com/soominkwon/dln-at-eos}{\texttt{https://github.com/soominkwon/dln-at-eos}}.}}
\author[2]{Soo Min Kwon$^*$}
\author[1]{Rongrong Wang}
\author[1]{\\Saiprasad Ravishankar}
\author[2]{Qing Qu}

\newcommand\CoAuthorMark{\footnotemark[\arabic{footnote}]}
\affil[1]{Computational Mathematics Science and Engineering, Michigan State University}
\affil[2]{Department of Electrical Engineering \& Computer Science, University of Michigan}


%\date{\today}

\begin{document}


\maketitle


\begin{abstract}
Deep neural networks trained using gradient descent with a fixed learning rate $\eta$ often operate in the regime of ``edge of stability'' (EOS), where the largest eigenvalue of the Hessian equilibrates about the stability threshold $2/\eta$. In this work, we present a fine-grained analysis of the learning dynamics of (deep) linear networks (DLNs) within the deep matrix factorization loss beyond EOS. For DLNs, loss oscillations beyond EOS follow a period-doubling route to chaos.
We theoretically analyze the regime of the 2-period orbit and show that the loss oscillations occur within a small subspace, with the dimension of the subspace precisely characterized by the learning rate.
The crux of our analysis lies in showing that the symmetry-induced conservation law for gradient flow, defined as the balancing gap among the singular values across layers, breaks at EOS and decays monotonically to zero.
Overall, our results contribute to explaining two key phenomena in deep networks: (i) shallow models and simple tasks do not always exhibit EOS~\citep{cohen2021gradient}; and (ii) oscillations occur within top features~\citep{zhu2023catapults}. We present experiments to support our theory, along with examples demonstrating how these phenomena occur in nonlinear networks and how they differ from those which have benign landscape such as in DLNs.
  

%Lastly, we experiment with using low-rank adaptors to fine-tune deep networks with large learning rates, showing that the resulting catapult dynamics in the loss can potentially improve generalization, which may be of independent interest.
 
%Lastly, we also demonstrate that fine-tuning deep networks using low-rank adaptation with large learning rates induces catapult dynamics in the loss, which has the potential to improve generalization.
 
 

%In this work, we investigate the training dynamics of deep networks with a large learning rate $\eta$, commonly used in machine learning practice for improved empirical performance.
%In this work, we provide a fine-grained analysis of the training dynamics of weight matrices with a large learning rate $\eta$, commonly used in machine learning practice for improved empirical performance. 
%This regime is also known as the edge of stability, where the largest eigenvalue of the Hessian hovers around $2/\eta$, and the training loss oscillates yet decreases over long timescales. Within this regime, we observe an intriguing phenomenon: the oscillations in the training loss are artifacts of the oscillations of \emph{only} a few leading singular values of the weight matrices within a tiny
%small invariant
%subspace. Theoretically, we analyze this behavior based on the deep matrix factorization problem, showing that this oscillation behavior closely follows that of its nonlinear counterparts. 
%Our analysis reveals that for $\eta$ within a specific range,
%We provably show that for $\eta$ within a specific range, 
%\edit{the leading subspaces of each weight matrix oscillate} within a 2-period fixed orbit. \edit{Empirically, we corroborate our theory by demonstrating that these oscillations occur not only in both linear and nonlinear networks but also during the fine-tuning of low-rank adaptors, and we discuss the practical implications of these findings.}
%\edit{}

%We extensively corroborate our theory with empirical justifications, namely in that (i) deep linear and nonlinear networks share many properties in their learning dynamics and (ii) our model captures the nuances that occur at the edge of stability which other models do not, providing deeper insights into this phenomenon. 

\end{abstract}



\tableofcontents


\input{intro}
\input{theory}
\input{experiments}

%\clearpage
% Reference
% For natbib users:
%\bibliography{reference}

\clearpage
{\small 
\bibliography{main}
\bibliographystyle{iclr2025_conference}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\clearpage



\include{ICLR/text/extended_exps}
\include{ICLR/text/revised_proofs}





\end{document}
