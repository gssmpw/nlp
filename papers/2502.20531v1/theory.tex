\begin{figure}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/dln_ps_eos_lr172_new.pdf}
    \caption{Depiction of the two phases of learning in the deep matrix factorization problem for a network of depth $3$. Left: Plot of the training loss undergoing saddle jumps, followed by periodic oscillations. Right: Plot of the corresponding sharpness of the DLN. Upon escaping the first saddle point, the GD iterates enter the edge of the stability regime, where the sharpness hovers just about $2/\eta$.
    }
    \label{fig:ps_eos}
\end{figure}
 
 \section{Deep Matrix Factorization Beyond the Edge of Stability}


When using a large learning rate, the learning dynamics can typically be separated into two distinct stages: (i) progressive sharpening and (ii) the edge of stability. Within the progressive sharpening stage, the sharpness lies below $2/\eta$ and tends to continually rise.  Our goal is to analyze the EOS stage under the deep matrix factorization formulation. Here, we observe that the training loss fluctuates due to layerwise singular value oscillations, as illustrated in Figure~\ref{fig:ps_eos}. 

%Here, we observe that the training loss oscillates in a two-period orbit rather than decreasing over iterations, as illustrated in Figure~\ref{fig:ps_eos} and rigorously shown in the following sections.

%Our result extends the theoritical findings of \cite{chen2023edge}, where the authors proved the existence of a certain class of scalar functions \( f(x) \) for which gradient descent (GD) does not diverge even when operating beyond the stability threshold \( \eta > \frac{2}{f^{''}(\hat{x})} \), where \( \hat{x} \) is a local minimum of \( f(x) \). Specifically, the authors showed that there exists a range \( \eta \in \left( \frac{2}{f^{''}(\hat{x})}, \frac{2}{f^{''}(\hat{x})}(1+\epsilon) \right) \), with a function-dependent \( \epsilon > 0 \), where the loss oscillates around the local minima $(\hat{x})$ with a certain periodicity. As \( \eta \) increases beyond \( \frac{2}{f^{''}(\hat{x})} \), the oscillations gradually enter higher periodic orbits (e.g., 2, 4, 8 periods), then transition into chaotic behavior, and ultimately lead to divergence. In our work, we prove that this oscillatory behaviour beyond the stability threshold occurs even for deep linear networks. Our key observation is that due to two specific implicit bias of GD namely 1) singular vector alignment and 2) balancing, stable oscillations takes place in the top $r$ eigenvectors of the Hessian of the loss, where $r$ is dependent on $\eta$. 
%Throughout the analysis, we only limit ourselves to analyzing two periodic orbit oscillation (Figure~\ref{fig:ps_eos}) and show higher period oscillations in the appendix.




\subsection{Assumptions and Analytical Tools}

Before we present our main result, we introduce two key analytical tools used in our analyses: the singular vector stationary set and singular value balancedness. 
First, we introduce the singular vector stationary set, which allows us to consider a wider range of initialization schemes. This set defines a broad class of weights for which singular vector alignment occurs, simplifying the dynamics of weights to those that only involve singular values.





\subsubsection{Singular Vector Alignment}

\begin{proposition}[Singular Vector Stationary Set]
\label{prop:svs_set}
Consider the deep matrix factorization loss in Equation~(\ref{eqn:deep_mf}). Let $\mbf{M}_\star = \mbf{U}_\star \mbf{\Sigma}_\star \mbf{V}_\star^\top$ and 
$\mbf{W}_\ell(t) = \mbf{U}_\ell(t) \mbf{\Sigma}_\ell(t) \mbf{V}_\ell^\top(t)$ denote the compact SVD for the target matrix and the $\ell$-th layer weight matrix at time $t$, respectively. For any time $t\geq 0$, if $\dot{\mbf{U}}_\ell(t) = \dot{\mbf{V}}_\ell(t) = 0$ for all $\ell \in [L]$, then the singular vector stationary (SVS) points for each weight matrix are given by
\begin{align*}
\mathrm{SVS}(f(\mbf{\Theta})) = 
\begin{cases}
    (\mbf{U}_L, \mbf{V}_L) &= (\mbf{U}_\star, \mbf{Q}_L), \\
    (\mbf{U}_\ell, \mbf{V}_\ell) &= (\mbf{Q}_{\ell+1}, \mbf{Q}_\ell), \quad\forall \ell \in [2, L-1], \\
    (\mbf{U}_1, \mbf{V}_1) &= (\mbf{Q}_2, \mbf{V}_\star),
\end{cases}
\end{align*}
where \(\{\mbf{Q}_\ell\}_{\ell=2}^{L}\) can be any orthogonal matrices. 
\end{proposition}

The singular vector stationary set states that for any set of weights where the gradients with respect to the singular vectors become zero, the singular vectors become fixed points for subsequent iterations. Once the singular vectors become stationary, running GD further isolates the dynamics on the singular values. Hence, throughout our analysis, we re-write and consider the loss 
\begin{align}\label{eqn:simplified_loss}
    \frac{1}{2} \left\|\mbf{W}_{L:1}(t) - \mbf{M}^\star\right\|^2_{\mathsf{F}} = \frac{1}{2} \|\mbf{\Sigma}_{L:1} - \mbf{\Sigma}^\star\|^2_{\mathsf{F}} = \frac{1}{2} \sum_{i=1}^r \left(\sigma_i(\mbf{\Sigma}_{L:1}(t)) - \sigma_{\star, i}\right)^2,
\end{align}
where $\mbf{\Sigma}_{L:1}$ are the singular values of $\mbf{W}_{L:1}$. This allows us to decouple the dynamics of the singular vectors and singular values, focusing on the periodicity that occurs in the singular values within the EOS regime. In Propositions~\ref{prop:one_zero_svs_set}~and~\ref{prop:balanced_svs_set}, we prove that both the unbalanced and balanced initializations considered here  belong to this set respectively,
with an illustration in Figure~\ref{fig:svec_alignment}. Specifically, we show that the balanced initialization in (\ref{eqn:balanced_init}) belongs to the singular vector stationary set for all $t\geq 0$, while the unbalanced initialization in (\ref{eqn:unbalanced_init}) belongs to the set for all $t\geq 1$ (far before entering the EOS regime) with $\mbf{Q}_\ell = \mbf{V}_\star$,
allowing us to consider the loss in Equation~(\ref{eqn:simplified_loss}). 

\begin{figure}[t!]
    \centering
    \includegraphics[width=\textwidth]{figures/svector_alignment_diff_lr.png}
    \caption{Illustrations of the singular vector and value evolution of the end-to-end DLN starting from the unbalanced initialization. The singular vectors of the network remain static across all iterations, as suggested by the singular vector stationary set, regardless of the learning rate. The angle between the true singular vectors and those of the network remains aligned throughout. The first singular values undergo oscillations in the large $\eta$ regime, whereas they remain constant in the small $\eta$ regime.}
    \label{fig:svec_alignment}
\end{figure}


%\footnote{When the initialization for the singular vectors are made outside this set, further tracking the singular vector alignment $(\mathbf{V}^{T}_{l}(t)\mathbf{U}_{l-1}(t) -\mathbf{I}_{r})$ is required, which is currently outside the scope of the paper. However, in appendix (Figure~\ref{fig:svs_set_test}), we show that at EOS, the singular vectors gradually align to converge to this set.}






\subsubsection{Balancing of Singular Values Across Layers}

Equipped with the loss in (\ref{eqn:simplified_loss}), notice that the balanced initialization in Equation~(\ref{eqn:balanced_init}) makes the learning dynamics such that for all $t \geq 0$, 
\begin{align*}
\sigma_i(\mbf{W}_\ell(t)) = \sigma_i(\mbf{W}_k(t)), \quad  \forall i \in [d], \quad \forall \ell, k \in [L],
\end{align*}
 where $\sigma_{i}(\mbf{W}_{\ell})$ denotes the $i$-th singular value of the $\ell$-th layer. This allows us to couple the dynamics and analyze the behavior of a single variable: $\sigma_i(\mbf{\Sigma}_{L:1}(t)) = \sigma_i^L(t)$. However, this is certainly not the case for the unbalanced initialization in Equation~(\ref{eqn:unbalanced_init}). Since the singular values of the $\mbf{W}_L(0)$ are initialized to zero, there is a non-negligible gap of $\alpha > 0$ between the singular values of the $L$-th layer and the other layers. However, in the following result, we prove that as long as $\alpha$ is small, GD will monotonically decrease the gap to balance the singular values across layers at the EOS. This will allow us to couple the dynamics in the limiting case for the unbalanced initialization as well.

 


 


%Next, we provide a definition of what we refer to as a \emph{strict balanced state} of the singular values for the weight matrices. If the parameters are said to be in a strictly balanced state, then the singular values of each weight are balanced in the sense that they take the same values across all weight layers.




\begin{comment}
    
\begin{definition}[Strict Balanced State]
The parameters $\mbf{\Theta}$ of the DLN from Equation~(\ref{eqn:deep_mf}) are said to be in a strict balanced state if for some $t\geq 0$
    \begin{align*}
        \sigma_i(\mbf{W}_\ell(t)) = \sigma_i(\mbf{W}_k(t)), \quad  \forall i \in [r], \quad \forall \ell, k \in [L],
    \end{align*}
    where $\sigma_{i}(\mbf{W}_{\ell})$ denotes the $i$-th singular value of the $\ell$-th layer and $r$ is the rank of the matrix $\mbf{M}_\star$.
\end{definition}


It is straightforward to show that the parameters are in a strictly balanced state for all $t\geq 0$ if we initialize the singular values to be the same across all weight matrices $\mbf{W}_\ell$. Hence, it immediately holds that the balanced initialization is always in a strict balanced state. However, the one-zero (i.e., unbalanced) initialization in Equation~(\ref{eq:unbalanced_init}) sets the singular values of $\mbf{W}_L$ to zero, and so the parameters are not initially in a strictly balanced state. The following result demonstrates that, with an appropriately chosen initialization scale $\alpha$, the singular values of each weight matrix become \emph{increasingly} balanced, even starting from an unbalanced initialization.
\end{comment}

\begin{proposition}[Balancing of Singular Values]
\label{prop:balancing}
   Let $\sigma_{\star, i}$ and $\sigma_{\ell, i}(t)$ denote the $i$-th singular value of $\mbf{M}_\star \in \mbb{R}^{d\times d}$ and $\mbf{W}_\ell(t)$, respectively and define $S_i \coloneqq L \sigma^{2-\frac{2}{L}}_{\star,i}$.
    Consider GD on the $i$-th index of the simplified loss in~(\ref{eqn:simplified_loss}) with the unbalanced initialization and learning rate $\frac{2}{S_i} < \eta < \frac{2\sqrt{2}}{S_i}$. If the initialization scale $\alpha$ satisfies
    $0 < \alpha < \left( \ln\left( \frac{2\sqrt{2}}{\eta S_i} \right) \cdot \frac{ \sigma_{\star, i}^{4/L}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{1/4}$, then there exists a constant $c \in (0, 1]$ such that for all $\ell \in [L-1]$, we have $\left| \sigma^2_{L, i}(t+1) - \sigma^2_{\ell, i}(t+1)\right| < c\cdot \left| \sigma^2_{L, i}(t) - \sigma^2_{\ell, i}(t)\right|$.
\end{proposition}
The proof is available in Appendix~\ref{sec:proof_of_balancing}.
This result has been shown to hold similarly for two-layer matrix factorization~\citep{wang2021large,ye2021global,chen2023edge}, and our analysis extends it to the deeper case. Precisely, it  considers the scalar loss for a singular value index and states that, as long as $\alpha$ is chosen below a threshold dependent on $\sigma_{\star, i}$, the $i$-th singular value across layers will become increasingly balanced. At the steady state limit of EOS, this balancing gap will decrease to zero. While this result is presented as a tool for the main result, it also has interesting implications for the dynamics of GD at EOS. 


\begin{figure}[t!]
    \centering
     \begin{subfigure}[t!]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/contour_gf.pdf}
        \caption{GF and Stable GD}
        \label{subfig:contour_gf}
     \end{subfigure}
     \hfill\begin{subfigure}[t!]{0.325\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/contour_gd_below_stab.pdf}
        \caption{GD at EOS}
        \label{subfig:contour_at_eos}
     \end{subfigure}
     \hfill
     \begin{subfigure}[t!]{0.325\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/eos_balanced_contour_2.pdf}
         \caption{ GD Beyond EOS %($\eta>\frac{2}{L\sigma^{2-\frac{2}{L}}_{*}}$)
         }
         \label{subfig:contour_beyond_eos}
     \end{subfigure}
    \caption{Illustration of the GD trajectories for three different learning rates regimes for minimizing the function $f(\sigma_1, \sigma_2) = \frac{1}{2}(\sigma_2 \cdot \sigma_1 - \sigma_{*})^2$, starting from an unbalanced initial point. %The sharpness at the balanced minimum is $L\sigma^{2-\frac{2}{L}}_{*}= 10$, and so the learning rate needed to enter EOS is $\eta = \frac{2}{L\sigma^{2-\frac{2}{L}}_{*}} = 0.2$. 
    Gradient flow conserves the balancing gap $|\sigma_{1}^{2}(t)-\sigma_{2}^{2}(t)|$ throughout its trajectory. GD at EOS decreases the gap, but stagnates once the oscillations no longer occur. GD beyond EOS  decreases the gap monotonically to zero by oscillating towards and about the balanced minimum.
     }
    
    \label{fig:contour}
\end{figure}


Firstly, it is well known that for gradient flow (GF), the balancing gap is conserved throughout its trajectory (see Lemma~\ref{gf-unbalanced} and Figure~\ref{subfig:contour_gf}). While GD with small learning rates approximately conserves the gap, GD at the EOS (i.e., a learning rate close but below the stability limit) breaks this conservation. However, for a learning rate below the stability limit $2/\|\nabla^2 f(\mbf{\Theta})\|_2$, the GD iterates may converge to an unbalanced global minimum (e.g., Figure~\ref{subfig:contour_at_eos}, and the balancing gap will cease to decrease any further. In contrast, for GD beyond the stability limit (i.e., beyond the EOS), Proposition~\ref{prop:balancing} states that the balancing gap decreases monotonically to zero. This highlights a key distinction between the learning dynamics of GF, GD, and GD at EOS.
To illustrate these differences, we consider a toy example of minimizing a two-layer scalar function $f(\sigma_1, \sigma_2) = \frac{1}{2}(\sigma_2 \cdot \sigma_1 - \sigma_{*})^2$, where $\sigma_{*} = 5$ in Figure~\ref{fig:contour}. Once stable GD arrives at an unbalanced global minimum, it settles there and the balancing gap do not decrease further. For GD just below the stability limit (Figure~\ref{subfig:contour_at_eos}), the iterates oscillate, but once they cease oscillating and settle at an unbalanced global minimum, the gap also stagnates. On the other hand, GD beyond EOS (Figure~\ref{subfig:contour_beyond_eos}) drives the balancing gap strictly to zero, as the GD iterates oscillate toward and around the balanced minimum.

\begin{wrapfigure}{r}{0.385\textwidth}
\vspace{-0.2 in}
  \begin{center}
    \includegraphics[width=\linewidth]{figures/gd_balancing_diff.pdf}
    
  \end{center}
  \vspace{-0.2 in}
  \caption{Plot of $| \sigma^2_1(t) - \sigma^2_2(t)|$ on a toy example,
 showing a decaying balancing gap beyond EOS.}
\label{fig:balancing}
\end{wrapfigure}
Secondly, for deep matrix factorization, we prove that the balanced minimum (i.e., the minimum where all of the singular values across layers are the same) corresponds to the flattest minimum (see Lemma~\ref{lemma:flattest}). Since GD at EOS monotonically decreases the balancing gap, this also implies that GD implicitly walks from a sharper minima to the flattest minima. This also suggests an algorithmic trick: one can initially use a large learning rate to oscillate toward a flatter region and subsequently decrease the learning rate to settle at a flat minimum, as also highlighted by~\cite{chen2023edge}.



Next, note that if the constant in Proposition~\ref{prop:balancing} were to be strictly $c<1$, by Lemma~\ref{lemm:seq_converge}, the gap would approach zero infinitesimally. Our analysis shows the existence of $c$ in two cases: (i) $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$ and (ii) $\sigma_i(\mbf{\Sigma}_{L:1}) > \sigma_{\star, i}$. While we provably show that $c < 1$ for the first case, we have that $c = 1$ for the second case. This implies that when the GD iterates are below ($\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$) and approaching the minima, the balancing gap will monotonically decrease, but is not guaranteed to decrease to zero when we overshoot above the minima ($\sigma_i(\mbf{\Sigma}_{L:1}) > \sigma_{\star, i}$). However, note that in the EOS regime, we oscillate below and above the minima as shown in Figure~\ref{fig:contour} (since for the case $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$, we have $c<1$). 
This indicates that we alternate between the two cases, and hence, the balancing gap will overall decrease to zero as depicted in Figure~\ref{fig:balancing}.
Since oscillations do not occur or are not sustained in GF and stable GD, the gap does not go to zero in most cases, making this a distinct characteristic of GD beyond EOS.

Finally, we remark that Proposition~\ref{prop:balancing} considers only the loss of a single singular value index, whereas Equation~(\ref{eqn:simplified_loss}) is the sum over multiple indices. For Proposition~\ref{prop:balancing} to hold for all indices, we can simply choose $\alpha$ with $\sigma_{\star, 1}$ such that it is the smallest $\alpha$ satisfying the condition for all singular values $\sigma_{\star, i}$.
To this end, in the following sections, we rigorously analyze the behavior of singular value oscillations around the balanced minimum. This can be viewed as the behavior of GD in the steady-state limit, as Proposition~\ref{prop:balancing} implies that the singular values become balanced as $t \to \infty$.

%Several works (\citet{wang2023good,minimal_eos}) states the regime $\eta<\frac{2}{L\sigma^{2-\frac{2}{L}}_{*}}$ (Figure~\ref{fig:contour}-b) as the EOS regime, where iterates initiallly oscillates but finally settles down to sharpness where it is less than $\frac{2}{\eta}$. To be consistent with the literature, we denote the regime $\eta>\frac{2}{L\sigma^{2-\frac{2}{L}}_{*}}$ as "Beyond EOS" where we encounter stable oscillation. This regime is the prime focus in this paper. 



%If the constant were to be strictly $c < 1$,  the balancing gap would approach zero infinitesimally. However, our analysis currently only shows that $c<1$ when the product of singular values across all layers $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$ and but $c=1$ when $\sigma_i(\mbf{\Sigma}_{L:1}) > \sigma_{\star, i}$. Since we start from a small initialization scale, we generally mostly operate within the regime $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$, and only transition to the latter regime when oscillations occur. Note that $\sigma_i(\mbf{\Sigma}_{L:1}) = \sigma_{\star, i}$ cannot occur since the learning rate is chosen to be within the EOS regime. The equality ($c=1$) can only arise in the stable regime, where exact balancing does not occur. We discuss the three phases of balancing in Section-. 
%\edit{To summarize, Lemma~\ref{lemm:balancing} states that, provided $\alphay$ is chosen below a certain threshold, the top-$r$ singular values of the weights across all layers become increasingly balanced during GD, even if they are unbalanced as in the initialization of Equation~(\ref{eqn:init}).  Our result is an extension of these analyses, but to the deep matrix factorization case. Our analysis shows that the constant $c$ changes for two different cases\footnote{We exclude the case  $\sigma_i(\mbf{\Sigma}_{L:1}) = \sigma_{\star, i}$, as this occurs with probability 0 in the presence of oscillations in the EOS regime.}: (i) $0<c<1$ when the product of singular values across all layers $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$ and (ii) $c=1$ when $\sigma_i(\mbf{\Sigma}_{L:1}) > \sigma_{\star, i}$. In the literature, it has been widely shown that the dynamics of DLNs (along with diagonal linear networks) exhibit an incremental learning phenomenon, where the singular values $\sigma_i(\mbf{W}_\ell)$ start from $\alpha$ and increase to the target singular value one-by-one~\citep{gissin2020the, inclearndiag, kwon, jacot2021saddle}. Empirically, this implies that we often operate in the regime of $\sigma_i(\mbf{\Sigma}_{L:1}) < \sigma_{\star, i}$, as the oscillations begin to occur once we reach and about the minima. Hence, throughout most of the learning trajectory, $0<c<1$ holds, and the balancing gap becomes infinitesimally small.} In Figure~\ref{fig:balancing}, we plot the balancing gap between the top-3 singular values of a weight matrix initialized to zero and those initialized to $\alpha$ for a rank-3 matrix. This plot shows that the gap decreases and goes to zero empirically at Edge of Stability, and this is consistently the case across all of our experiments, with additional results provided in Appendix~\ref{sec:additional_exp}. This provides empirical evidence that our analysis can be further improved such that $c<1$ for both cases. 


 
 
 % This allows us to write the loss of the singular values into the form $\sigma_i(\mbf{\Sigma}_{L:1}(t)) = \sigma^L_{i}(t)$, which allows us to focus on the coupled dynamics in the singular values.




\subsection{Main Results}


Using our analytical tools, we present our main results describing the learning dynamics of DLNs about the balanced solution beyond the EOS. First, we present a result characterizing the set of all eigenvalues $\lambda_{\mbf{\Theta}}$ of the DLN with respect to the flattened Hessian of the training loss at the balanced minimum. 
%\qq{For clarity, it would be better to introduce the notation of eigenvalues $\lambda$ in detail first, before introducing the lemma below. It would be easier for the readers to differentiate between sigma and lambda }

\begin{lemma}[Eigenvalues of Hessian at the Balanced Minimum]
\label{lemma:hessian_eigvals}
 The set of all non-zero eigenvalues of the training loss Hessian of the deep matrix factorization loss $f(\mbf{\Theta})$ defined in Equation~(\ref{eqn:deep_mf}) at the balanced minimum is given by
    \begin{align*}
       \lambda_{\mbf{\Theta}} = \left\{L \sigma_{\star, i}^{2 - \frac{2}{L}}, \sigma_{\star, i}^{2 - \frac{2}{L}}\right\}_{i=1}^r  \, \bigcup \, \left\{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, i}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \sigma_{\star, j}^{\frac{1}{L}\ell}\right)^2\right\}_{i\neq j}^{r}\,\bigcup \, \left\{\sum_{\ell=0}^{L-1} \left(\sigma_{\star, k}^{1-\frac{1}{L} - \frac{1}{L}\ell} \cdot \alpha^{\ell}\right)^2\right\}_{k = 1}^{r}
    \end{align*}
    where $\sigma_{\star, i}$ is the $i$-th singular value of the target matrix $\mbf{M}_\star \in \mbb{R}^{d\times d}$,  $\alpha \in \mbb{R}$ is the initialization scale, $L$ is the depth of the network, and the second element of the set has a multiplicity of $d-r$. 
    %However, the trajectory of iterates at EOS only lie in an r-dimensional subspace whose eigenvalues are given by $\lambda_{\Theta} \;=\;\Bigl\{{L\,\sigma_{\star,i}^{2-\frac{2}{L}}}\Bigr\}_{i=1}^r.$
\end{lemma}

The proof is deferred to Appendix~\ref{sec:proof_of_hess_eigvals}.
Let $\lambda_i$ denote the $i$-th largest eigenvalue of the Hessian. 
By Lemma~\ref{lemma:hessian_eigvals}, we observe that the sharpness is equal to $\lambda_1 = \|\nabla^2 f(\mbf{\Theta})\|_2 = L\sigma_{\star, 1}^{2- \frac{2}{L}}$ at the balanced minimum. In Lemma~\ref{lemma:flattest}, we show that among all the points on the global minima, the sharpness at the balanced minimum is the smallest. 
Thus, if $\eta$ is set such that $\eta > 2/\lambda_1$, oscillations in the loss will occur, as the step size is large enough to induce oscillations even in the flattest region. 
Notice that this was alluded to in Figure~\ref{fig:contour}—for GD beyond EOS (i.e., when $\eta > 2/\lambda_1$), there is stable oscillation around the minima, whereas for GD at EOS, the iterates eventually settle down after transient oscillations. Furthermore, notice that all non-zero eigenvalues are a function of network depth. For a deeper network, the sharpness will be larger, implying that a smaller learning rate can be used to drive the DLN into EOS. This provides a unique perspective on how the learning rate should be chosen as networks become deeper and explains the observation made by~\cite{cohen2021gradient}, who observed that sharpness scales with the depth of the network. %Although Lemma~\ref{lemma:hessian_eigvals} derives all the eigenvalues of the Hessian at the balanced minima, oscillations only take place along eigenvector directions with eigenvalues $\lambda_{i} = L \sigma_{\star, i}^{2 - \frac{2}{L}}$. 
Equipped with the eigenvalues, we show in the following result that oscillations actually occur in a two-period orbit about the balanced minimum within a rank-$p$ subspace, where the rank is dependent on the choice of the learning rate. 

%given that the learning rate is set to be greater than $2/\lambda_i$.


\begin{comment}
    
\begin{theorem}[Stable Subspace Oscillations]
\label{thm:stable_oscillations}
Let $\alpha' \coloneqq \left( \ln\left( \frac{2\sqrt{2}}{\eta \lambda_1} \right) \cdot \frac{ \sigma_{\star, 1}^{4/L}}{L^2 \cdot 2^{\frac{2L-3}{L}}} \right)^{1/4}$.
Consider running GD on the loss in~(\ref{eqn:deep_mf}) with initialization scale $0<\alpha < \alpha'$.
    If $\eta = \frac{2}{K}$ with $\lambda_i \leq K< \lambda_{i+1}$, then $2$-period orbit oscillation may occur in the direction of $\Delta_i$, where $\lambda_i$ and $\Delta_i$ denote the $i$-th largest eigenvalue and  eigenvector of the Hessian at the balanced minimum, respectively.
\end{theorem}


The complete proof is provided in Appendix~\ref{sec:proofs_oscillations}, where we derive all eigenvectors at the balanced minimum and demonstrate that the necessary conditions from Lemma~\ref{lemma:chen-bruna} (restated from~\cite{chen2023edge}) are satisfied for a two-period orbit. Recall that the condition on the initialization scale is chosen to ensure balanced behavior, as demonstrated in Proposition~\ref{prop:balancing}. \ag{I did not get this part.}
To understand Theorem~\ref{thm:stable_oscillations} more clearly, consider the first eigenvector of the Hessian, which we derived to be
\begin{align*}
        \Delta_1 = \frac{1}{\sqrt{L}}\cdot \mathrm{vec}\left(\mbf{u}_{\star, 1} \mbf{v}_{\star, 1}^\top, \mbf{v}_{\star, 1}  \mbf{v}_{\star, 1}^\top, \ldots, \mbf{v}_{\star, 1}  \mbf{v}_{\star, 1}^\top \right),
\end{align*}
where $\mbf{u}_{\star, 1}, \mbf{v}_{\star, 1} \in \mbb{R}^d$ are the first left and right singular vectors of $\mbf{M}_\star$, respectively. By Proposition~\ref{prop:one_zero_svs_set}, we know that the each weight layer takes the form  $\mbf{W}_\ell = \mbf{V}_\star \mbf{\Sigma}_\ell \mbf{V}_\star^\top$ for all $\ell \in [L-1]$ and $\mbf{W}_L = \mbf{U}_\star \mbf{\Sigma}_L \mbf{V}_\star^\top$ at convergence, starting from the unbalanced initialization. By stacking and flattening these weights, consider the direction
\begin{align*}
    \widetilde{\Delta} \coloneqq \sum_{i=1}^d \mathrm{vec}\left(\sigma_{L, i}\cdot\mbf{u}_{\star, i} \mbf{v}_{\star, i}^\top,\sigma_{L-1, i}\cdot \mbf{v}_{\star, i}  \mbf{v}_{\star, i}^\top, \ldots, \sigma_{1, i}\cdot\mbf{v}_{\star, i}  \mbf{v}_{\star, i}^\top \right).
\end{align*}
Since $\widetilde{\Delta}^\top \Delta_1$ is only non-zero in the rank-$1$ components of $\widetilde{\Delta}$, this implies that if $\lambda_1 \leq K < \lambda_2$ from Theorem~\ref{thm:stable_oscillations},
the oscillations only occur in the rank-$1$ components of the weights. The following result substantiates this claim by demonstrating that, with an appropriately chosen learning rate, oscillations occur in the singular values in the top-$p$ directions given that $p \leq r$.
\end{comment}


\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \caption*{\footnotesize Rank-$1$ Oscillation}
\includegraphics[width=\textwidth]{figures/eos_osc_region_1.pdf}
\caption*{$ \frac{2}{S_2} >\eta>\frac{2}{S_1}$}
\end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \caption*{\footnotesize Rank-$2$ Oscillation}
\includegraphics[width=\textwidth]{figures/eos_osc_region_2.pdf}
\caption*{$ \frac{2}{S_3} >\eta>\frac{2}{S_2}$}
\end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
         \centering
         \caption*{\footnotesize Rank-$3$ Oscillation}
         \includegraphics[width=\textwidth]{figures/eos_osc_region_3.pdf}
         \caption*{$ \frac{2}{S_4} >\eta>\frac{2}{S_3}$}
\end{subfigure}
\caption{Evolution of the singular values of the end-to-end $3$-layer network for fitting a rank-3 target matrix with singular values $10$, $9.5$, and $9$. We use a learning rate of $\eta = 2/S_i$ with $S_i \coloneqq L\sigma_{\star, i}^{2 - 2/L}$. The oscillations occur as a two-period orbit about the balanced minimum exactly with learning rate ranges specified in Theorem~\ref{thm:align_thm} for rank-$p$ oscillations ($p=1,2, 3$).}
\label{fig:thm2_osc}
\end{figure}


\begin{theorem}[Rank-$p$ Periodic Subspace Oscillations]
\label{thm:align_thm}
Let $\mbf{M}_\star = \mbf{U}_\star \mbf{\Sigma}_\star \mbf{V}_\star^\top$ denote the SVD of the target matrix and define $S_p\coloneqq L \sigma^{2-\frac{2}{L}}_{\star,p}$ and $K'_p \coloneqq \mathrm{max} \left\{ S_{p+1},\frac{S_p}{2\sqrt{2}}\right\}$.
%If we run GD on the deep matrix factorization loss with initialization scale $\alpha < \alpha'$ from Theorem~\ref{prop:balancing} and learning rate $\eta = \frac{2}{K}$, where $K'_p < K< S_p$, then at the steady state limit $t \rightarrow \infty$, 
If we run GD on the deep matrix factorization loss with learning rate $\eta = \frac{2}{K}$, where $K'_p < K< S_p$,
%If the singular values across layers are balanced,
then the top-$p$ singular values of the end-to-end DLN oscillates in a $2$-period orbit ($j \in \{1,2\}$) around the balanced minimum and admits the following decomposition:
\begin{align}
    \mbf{W}_{L:1} = \underbrace{\sum_{i=1}^p\rho_{i, j}^L \cdot \mbf{u}_{\star, i}\mbf{v}_{\star, i}^{\top} }_{\text{oscillation subspace}}+ \underbrace{\sum_{k=p+1}^d \sigma_{\star, k}\cdot \mbf{u}_{\star, k}\mbf{v}_{\star, k}^{\top}}_{\text{stationary subspace}}, \quad j \in \{1,2\}, \quad \forall\ell \in [L-1],
\end{align}
where $\rho_{i, 1} \in \left(0, \sigma_{\star, i}^{1/L}\right)$ and $\rho_{i, 2} \in \left(\sigma_{\star, i}^{1/L}, (2\sigma_{\star, i})^{1/L}\right)$ are the two real roots of the polynomial $g(\rho_i)=0$ and
\begin{align*}
    g(\rho_i) = \rho_i^L\cdot\frac{1+\left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right)^{2L-1}}{1+\left(1 + \eta L(\sigma_{\star, i} - \rho_i^L)\cdot \rho_i^{L-2} \right)^{L-1}} - \sigma_{\star, i}.
\end{align*}
\end{theorem}

%\ag{Should we have the general oscillation as a lemma?
%Also where are we having the digoanl liner network oscillation?}
%\paragraph{Remarks.}
The proof is available in Appendix~\ref{sec:proof_of_orbits}. 
Theorem~\ref{thm:align_thm} explicitly identifies the subspaces that exhibit a two-period orbit based on the range of the learning rate. It also provides a rough characterization of the oscillation amplitude, which is determined by $\rho_{i, 1}$ and $\rho_{i, 2}$—values below and above the balanced minimum, respectively. 
%Hence, the oscillations occur around the minima.
Since there is no closed-form solution for an arbitrary higher-order polynomial, $\rho_{i, 1}$ and $\rho_{i, 2}$ are defined as solutions to the polynomial $g(\rho_i)$.
Overall, this aims to theoretically explain why (i) oscillations occur primarily within the top subspaces of the network, as observed by~\cite{zhu2023catapults}, and (ii) oscillations are more pronounced in the directions of stronger features, as measured by the magnitudes of their singular values.

Notice that the range of the learning rate depends on the eigenvalues of the form $S_p = L\sigma_{\star, p}^{2 - 2/L}$ rather than on all eigenvalues in Lemma~\ref{lemma:hessian_eigvals}. This is because the eigenvectors associated with the other eigenvalues are orthogonal to the weights of the DLN at the balanced minimum, so oscillations will never occur in those particular eigendirections. They are only non-orthogonal in the directions of the eigenvalues of $S_p$ and, hence, oscillations occur only in those specific directions.


We also remark that our result generalizes the recent theoretical findings of~\cite{chen2023edge}, where they proved the existence of a certain class of scalar functions \( f(x) \) for which GD does not diverge even when operating beyond the stability threshold.
%\( \eta > \frac{2}{f^{''}(\hat{x})} \), where \( \hat{x} \) is a local minimum of \( f(x) \). Specifically, they showed that for a function dependent $\epsilon > 0$, there exists a range \( \eta \in \left( \frac{2}{f^{''}(\hat{x})}, \frac{2}{f^{''}(\hat{x})}(1+\epsilon) \right) \), 
They demonstrated that there exists a range in which the loss oscillates around the local minima with a certain periodicity. These oscillations gradually progress into higher periodic orbits (e.g., 2, 4, 8 periods), transition into chaotic behavior, and ultimately result in divergence. In our work, we prove that this oscillatory behavior beyond the stability threshold also occurs in DLNs.

\begin{comment}
    

\paragraph{Discussion on Strict Balancedness at EOS.}

For ease of analysis, we utilized the results from Proposition~\ref{prop:balancing} to assume strict balancing throughout our proofs. This raises the question of whether periodic oscillations can occur around other local minima and what the learning rate might be in those scenarios to induce such oscillations.
Empirically, we actually observe that the two-period orbits \emph{only} occurs around the balanced solution.
To illustrate this claim, in Figure~\ref{fig:contour}, we provide a plot of the GD trajectory for minimizing a two-layer scalar loss $f(\sigma_1, \sigma_2) = \frac{1}{2}(\sigma_2 \cdot \sigma_1 - 5)^2$, starting from an unbalanced initial point $(\sigma_1, \sigma_2) = (1.5, 2.25)$. Notice that, by Lemma~\ref{lemma:hessian_eigvals}, the sharpness around the balanced minimum is $L\sigma_{\star,1}^{2-2/L} = 10$, and thus the necessary learning rate to enter the EOS regime is $\eta = 0.2$.
We plot the GD trajectory under two cases for the learning rate: (i) slightly below the EOS learning rate, $\eta = 0.1999$, and (ii) slightly above it, $\eta = 0.2010$. When $\eta = 0.2010$, GD first arrives at an unbalanced solution, then oscillates until it reaches the balanced minimum, where it sustains a two-period orbit around the balanced solution. The other minima are too narrow to sustain the oscillations induced by the large learning rate, causing the GD iterates to bounce out of those minima and settle at the flattest, balanced solution.
When the learning rate is slightly below $\eta = 0.2$, it remains large enough to induce oscillations around other unbalanced minima, but ultimately GD converges to the balanced solution, where it exhibits no oscillations, as predicted by Theorem~\ref{thm:align_thm}. This empirical observation suggests that two-period oscillations and balancing occur simultaneously, which we leave for future investigation.

\end{comment}