\documentclass{ar2rc}
\usepackage{graphicx}
\usepackage{subfigure}
%% ready for submission %%%
\input{ICLR/math_commands.tex}
\input{ICLR/commands}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{tgpagella}
\usepackage{tcolorbox}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{cite}
\usepackage{subcaption}
\usepackage{svg}

\usepackage{enumitem}
\usepackage{authblk}
\usepackage{natbib}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    citecolor=magenta,      
    urlcolor=cyan,
    pdfpagemode=FullScreen,
}
\usepackage{url}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{lipsum}  
\usepackage{cleveref}
\usepackage{xcolor}  
\usepackage{wrapfig}% colors
%\usepackage{natbib}
\usepackage{multirow}
% \bibliographystyle{achemso}

\newcommand{\conv}{\circledast}
\newcommand{\cconv}{  \boxasterisk }
\newcommand{\mb}{\mathbf}
\newcommand{\mc}{\mathcal}
\newcommand{\mf}{\mathfrak}
\newcommand{\md}{\mathds}
\newcommand{\bb}{\mathbb}
\newcommand{\magnitude}[1]{ \left| #1 \right| }
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\condset}[2]{ \left\{ #1 \;\middle|\; #2 \right\} }
\newcommand{\tbW}{\widetilde{\bm{W}}}
\newcommand{\bWcomp}{\bm{W}^{\textrm{comp}}}
\newcommand{\DS}[1]{{\color{magenta} (DS: #1)}}
\newcommand{\ZK}[1]{{\color{cyan} [{\em Zekai:} #1]}}
\newcommand{\SMK}[1]{{\color{red} [{\em SMK:} #1]}}

\newcommand{\qq}[1]{\textcolor{blue}{\bf [{\em Qing:} #1]}}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
%%% to compile a preprint version, e.g., for submission to arXiv, add the [preprint] option %%%
%\usepackage[preprint]{cpal_2024}

%%% to compile a camera-ready version, add the [final] option %%%
%\usepackage[final]{cpal_2024}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}


%add packages
\usepackage{url}
\title{ICLR rebuttal for deep matrix factorization EOS}

\begin{document}




\maketitle
\author{}
\section{Reviewer GbsE}
We thank the reviewer for the helpful and in-depth comments regarding our work and providing valuable feedback. Here we adddress each of the weakness points one by one as follows:

\subsection{Overclaiming difference between linear and non-linear networks}

\RC While it is useful to understand learning dynamics of deep linear networks from a theoretical perspective*, I'm not sure the results shed light on dynamics in deep nonlinear networks. In fact, the paper misinterprets the result of [1], which show that only for shallow networks are linear and nonlinear networks (2-layer bias-free ReLU networks) similar---for deep networks there is a strict separation in function approximation. [1] reports similarities between deep linear and nonlinear networks under very strong assumptions on data distribution and structure, which this manuscript does not consider. This submission misses this nuance (in abstract and lines 80-83), and overpromises results for deep nonlinear networks. In fact, the manuscript comments about the differences in loss landscapes in Line 511---nonlinear networks have highly irregular landscapes, and when trained with (minibatch) SGD could lead to catapults across regions not just in the vicinity. This is a major weakness in my view.

\AR 
Thank you for highlighting this point and we agree with the reviewer. Based on this comment we modified the paper to focus that linear networks, although non-convex, has simplified landscape basin compared to what we encounter in practice with non-linear networks. We added a figure in section-, to highlight EOS behaviours based on two landscapes: 1) we plot a two variable depth-4 dynamics at EOS on a 3D plot which highlights how the loss landscape is amenable to 2-period oscillation for different l.r.s. 2) We plot GD optimization trajectory for a Holder Table landscape (which is highly non-convex) for increasing learning rates. Here, the sharpness closely follows the limit 2/l.r supporting the EOS observations made in Cohen et al. This phenomenon has a close resemblance to landscapes occuring in neural networks. 
   Although far from perfect, we believe this experiment and figure justifies why we observe sustained periodic oscillations in linear networks and not on practical neural networks, which you also have kindly pointed out. 

We referenced [1] to highlight some particular cases where behaviours between linear and non-linear networks might be similar. Specifically, we believe EOS behaviours with depth are quite similar for linear networks and non-linear ones. Furthermore, our experiments with bias-free RELU suggests that there are  basins which support sustained oscillations over some local neighborhood till catapulting happens as Figure-13 in appendix suggests. However, we agree that analyzing the landscape for bias-free RELU needs further rigour and is currently beyond the scope of this work. We thank the reviewer for pointing out the setting of this paper and based on his/her comment we removed this reference on this particular context. 
\subsection{Initialization practicality}

\RC The 
alpha-scaled initialization scheme is not used in practice, especially for LoRA training where one of the matrix (A) is initialized at 0 and the other (B) with gaussian entries. The paper argues that the results work when initialization leads to convergence to the "singular vector stationary set" (line 148). Why is this not a strong and impractical assumption on how initialization is done in practice? Does random initialization satisfy this condition?

\AR Firstly, we would like to remark that the $\alpha$-scaled initialization for deep linear networks is a very common setting to study, as it corresponds to what is often referred to as the ``rich'' training regime. 

\RC As highlighted in Section 5 by the authors, the assumptions on singular values' "strict balanced state" and initialization's "stationary set condition" are strong. It's not clear if and when they can be met.


\AR Thanks, we agree that this is a current weakness. But in appendix, we provide several experiments to show that strict balancedness indeed holds when the initialization scale is smaller than a threshold. We update/correct the proof to show that strict balancing occurs with the current initialization scale \textcolor{red}{(Specifically, we modify the proof here...update it later)}. Furthermore, we provide two experiments in Section B.1 where we have an orthogonal matrix initialization and in Figure-14 where we have unbalanced random Gaussian initialization, in both of which we observe that singular vectors converges to the SVS set and strict balancing occurs.

% For future work: we believe it can be proven that the initialization with first layer zero and rest of them with random initialization, concentrates very close to the Singular vector stationary set in higher dimensions. However, we think it's hard to come up with a formal proof in the limited rebuttal time. \textcolor{red}{ask Rongrong on this proof sketch}

\RC The authors need not motivate deep linear networks by (wrongly) arguing their similarity to deep nonlinear networks; linear networks have been studied in the ML optimization research for a while now.

\AR We agree with the reviewer on this point. We made changes in the introduction and section 4.3 to now focus on the landscape in linear models. 

\RC Moreover, the "catapult" observation in LoRA experiments is on very shaky ground, since stochasticity affects the training loss and makes it oscillate. The paper mentions this in Line 431, so I'm not convinced that the oscillations are, in fact, "catapults".


\AR

\RC A natural experiment to try: In Line 188, step size is suggested to be chosen based on the depth 
 of the network (as 
 is larger, smaller values of step size will lead to oscillcations). This is curious to verify.

 
 \AR We provide the experiment in Appendix (\textcolor{red}{TODO}). Since the EOS limit for $\eta$ is $\frac{2}{L\sigma_{1}^{2-\frac{2}{L}}}$, a smaller $\eta$ for large $L$ ensures EOS. 
 

\RC In the introduction, 3 observations are described from [2, 3], but it is not clear to me how or which ones the paper addresses. It is also not clear to me how sharpening and catapults from the two papers are related to each other.

\AR Probably this confusion arose since [3] examines catapults in the top eigenvector space of the NTK matrix, whereas in our paper we study the catapult in the top eigenvector space of the Hessian. The NTK and the Hessian behaves almost similarly near the end of training since the residual goes to zero for MSE loss, which the authors in [3] address in Appendix A.1. In our paper, we study sharpening (or progressive sharpening) in DLNs for the max eigenvalue of the Hessian like in Cohen et al and then prove the oscillatory phenomenon in the top eigenvector subspace of the Hessian based on the l.r. The latter observation validates experiments in [3] since they observe catapults in the top eigenvector space of the NTK (or equivalently the Hessian). We will make this clear in the introduction.

\RC Definition 2 is introduced abruptly without need or context. There is discussion in Lines 191-195, which can be better placed before/after Def. 2.

\AR Thanks, our current manuscript update removes this abruptness and addresses the importance of strict balancing before defining it. 

\RC The paper is missing a related work section. It is included as Appendix A.1, but I'd suggest cutting down on the main sections to include related work within 10 pages.

\AR Thank you for this suggestion (\textcolor{red}{smirks}). We decided to include the related section and move the LORA section to the appendix stating it as a future work (instead of a contribution). We further remove the contribution of studying LORA's catapulting phenomenon in this paper. We believe this is an important future work given the connection our paper shows between low rank subspace and oscillations. 



\RC Theorem 1. Why is this called 1-d subspace oscillation?

In Theorem-1, the oscillations in subspace is a rank-1 outerproduct. So, we changed the oscilaltion to a rank-1 oscillation instead of 1-d subspace oscillation. 

\RC Figure 2. There are 2 kinks in the EOS regime in the left plot but only 1 is visible in the right plot. Do the authors know why?

Thanks for pointing this out. This is an artifact of plotting in the logarithm scale. Essentially in DLN's near small initialization, the sharpness is negative, since this region is a saddle point with a negative curvature. So, the logarithm (along with some irregular quantization) made the negative sharpness look like a kink. We updated the figure. 


\RC Section 4.2. The original LoRA paper [1] only applies adaptation to the attention weight matrices. Any reason the authors adapt all weights?

\RC Line 450. Pearson correlation between what two quantities? If this is the metric from the STS-B task, then how would this kind of experiment extend to other tasks?

\RC Line 474. Why do smooth, low-frequency images correspond to low task complexity? For the CIFAR dataset, number of samples 
 is used as a proxy task complexity, why? Both datasets are image datasets. It seems to me that I can pick and choose any proxy/hyperparameter from the experiment setup to argue that sharpness does not rise to EOS regime in low complexity tasks.

\AR Smooth/low frequency images are much easier to fit for convolutional neural networks due to \textit{spectral bias} (https://arxiv.org/abs/1806.08734), where the networks incrementally learns each Fourier Frequency (low to high). Similarly, fitting simple datasets such as MNIST or even datasets with less images are low complexity tasks since the intrinsic update dimension in parameter space is much smaller (https://arxiv.org/abs/1804.08838,https://arxiv.org/abs/2104.08894). However, the exact relation between the intrinsic dimension of parameter update and it's sharpness is unknown, it is a common observation that sharpness do not rise much when the network easily/quickly fits the underlying function (appendix in Cohen et al). Although currently the relation between complexity of the tasks in linear and non-linear network are far from perfect, the top singular value of the target matrix $\mbf{M}$ is a proxy for complexity. We updated the manuscript to reflect this. 


 \RC Figure 1. What is the pearson correlation in the last plot? A pointer to the relevant section would help.

 \RC  What is "sharpening"? First time this term is mentioned.

\AR Apologies for the hindsight. We defined "progressive sharpening" at the start of section-3 and in figure-2 which is after line-71. We will include it before 71. 

 \RC Lines 249-254. Lost here, what are 
, 
, and the line? I think more text is needed to explain how the analysis is done for 
-dimensional oscillations. A figure would really help the reader.

\AR We added figure which highlights the statement of this theorem. For example the X-axis and Y-axis refers to the two perpendicular lines in which oscillation occurs. In higher dimensions, these orthogonal directions are denoted by $\Delta_{i}$, (which are the eigenvector directions of the Flattened Hessian) which passes through the global minima (we referred to as $x$ in paper). So this line is denoted as $y=x+t\Delta_{i}$. 
\section{Reviewer CF1F}
 
\RC Although deep linear network shares some similarities with deep nonlinear networks, the claim that ``Our analysis explains two key phenomena in deep nonlinear networks" in abstract seems overstated and exaggerated.

\AR Thank you for highlighting this point. We updated our manuscript stating that we highlight the similarities and difference deep linear networks has with those of their non-linear counterparts. In Figure-, we plot the landscape of two variable depth-4 linear scalar network
and GD dynamics at EOS. Along with it we also plot GD optimization trajectory for a Holder Table landscape (which is highly non-convex) for increasing learning rates. Here, the sharpness closely follows the
limit 2/l.r supporting the EOS observations made in Cohen et al. This phenomenon has a close resemblance to landscapes occuring in neural networks. Instead of claiming that we explain the phenomenons, we state that we highlight the simmilarities and differences in these two networks at EOS. 

\RC In the analysis, the initialization (eqn (3)) is very specific, and it is just a member in the singular vector stationary set. It is not clear if all statements work only based on the initialization (eqn (3)) or for any initialization in the singular vector stationary set. For example, will Lemma 1 and Lemma 2 work with general singular vector stationary initial data?

\AR All the statements work for any initialization in the singular vector stationary set (SVS). The only change will appear in the closed form rank-1 updates in the Theorem-1. For example, now if the initialization is made to the general SVS set, 
\begin{align*}
    \begin{cases}
    (\mbf{U}_L, \mbf{V}_L) &= (\mbf{U}_\star, \mbf{Q}_L), \\
    (\mbf{U}_\ell, \mbf{V}_\ell) &= (\mbf{Q}_{\ell+1}, \mbf{Q}_\ell), \quad\forall \ell \in [2, L-1], \\
    (\mbf{U}_1, \mbf{V}_1) &= (\mbf{Q}_2, \mbf{V}_\star),
\end{cases}
\end{align*}
, then the rank-1 oscillation updates are given as 
\begin{align*}
      \mbf{W}_L(t) &= \underbrace{ \rho_i(t)\cdot \mbf{u}_{\star, 1}\mbf{q_{L}}_{\star, 1}^{\top} }_{\text{oscillation subspace}}+ \underbrace{\sum_{j=2}^r \sigma_{\star, j}\mbf{u}_{\star, j}\mbf{q_{L}}_{\star, j}^{\top}}_{\text{stationary subspace}}, \quad i = 1, 2,
\end{align*}
\begin{align*}
     \mbf{W}_\ell(t) &=\underbrace{ \rho_i(t) \cdot\mbf{q_{\ell+1}}_{\star, 1}\mbf{q_{\ell}}_{\star, 1}^{\top}}_{\text{oscillation subspace}} + \underbrace{\sum_{j=2 }^r \sigma_{\star, j}\mbf{q_{\ell+1}}_{\star, j}\mbf{q_{\ell}}_{\star, j}^{\top}}_{\text{stationary subspace}}, \quad i = 1, 2,
\end{align*}

Furthermore, in Lemma, we show that the Hessian eigenvalues are same and given by the ones in Lemma-1 for all solutions in the SVS initial set. This consequence is reached by finding that the roots of the characteristic polynomial of the Hessian for any balanced solution at SVS set are independent of the singular vectors. So Lemma-1 holds for any SVS initial data.
Lemma-2 concerns only the singular value dynamics so it automatically holds for all initial points in SVS set. We will highlight this point in the manuscript.


\RC Why is the period of the fixed orbit is always 2? It seems that it comes from the two-step GD update. How about other periodicities?

\AR This is a great question. Empricially it is observed that just beyond the EOS limit, oscillations start occuring in 2 period orbits. But as learning rate is increased the periodicitiy of oscillations double, i.e, it goes through oscillations of period 2,4,8 then chaos and eventually divergence when l.r is kept increasing. We plot this in a bifurcation diagram in page-2 where both the singular value of the end to end product matrix as well as the eigenvalues of the Hessian undergo period doubling oscillations as l.r is increased beyond EOS. Our paper only analyzes this 2-period orbiticity by two step GD update, since analyzing higher periods are complicated.  


\RC (15)-(16) on page 19-20 seem to have typos. 
 on the left side should be 
?

\AR Thank you for pointing out this typo. It will indeed be $\mbf{U}_{l}(t)$ and we corrected it. 

\section{Reviewer n4c2}

\RC My main concern is that Lemma 2 does not ensure convergence to strict balancing.


\AR Thank you for highlighting this important aspect. Indeed, if a sequence $\{a(t)\}_{t}$ lower bounded by $0$ obeys $a(t+1)<a(t)$, then the sequence is not guaranteed to converge to $0$. However, if there exists a $0<c<1$ such that $a(t+1)<c\cdot a(t)$, then the sequence is guaranteed to converge to $0$ as $t \rightarrow \infty$ (see Lemma 9). However, this is not exactly what happens with GD and the balancing goes to a value infinitesimally small, which is why we used the strict balancing assumption. Notice that we also added in the balanced initialization, and hence the conclusions made in our paper do not change.

Furthermore, to make a stronger argument, we made an attempt to modify Lemma 2 to show strict balancing with $b(t+1) < c\cdot b(t)$. To do this, we analyze 2 cases: (i) $\pi(t)< \sigma_{*}$, and (ii) $\pi(t)> \sigma_{*}$, and showed that there exists a constant $0<c<1$ such that it holds.
However, when $\pi(t)> \sigma_{*}$, we can only prove for $c=1$, i.e, $b(t+1)< b(t)$. Hence, the balancing gap cannot be proven to go to exactly zero but something (very) small as our experiments suggest. For the final manuscript, we can make these points even more rigorous by performing an error analysis -- we will prove our results with an error term $e(\alpha)$, where $\alpha$ is the initialization scale, which arises when balancing \emph{only approximately} holds. Then, we can re-state our results in the order of $e(\alpha)$. However, due the time limitations, we leave it for the final manuscript but will update the manuscript if completed during the discussion period. 


% indeed there still exists a gap in the proof. However, DLN dynamics mostly stay in case-1 (small initialization), and our experiments suggest that at EOS, it is mostly the case that balancing reaches 0. We made this drawback clear in the main text and considered several experiments that include large initialization where strict balancing fails to hold as predicted by Lemma-2.

\RC The model fails to capture (non-monotonic) decreasing training loss in the Edge of Stability.

\AR We would like to point out that the linear model also decreases the loss non-monotonically where oscillations occur while incremental rank learning takes place for example in Figure-2. Here, oscillations occur after the first and second saddle jump takes place. However, when the iterates reaches the global minima, models can't decrease the training loss further since there is only a single loss basin and sustained oscillations take place near the global minima. We discussed this point of difference and included a loss landscape figure comparing linear scalar deep networks with highly non-convex landscapes like the Holder Table. Furthermore, deep linear networks exhibit oscillatory behaviours beyond the stability regime instead of divergence, which Theorem-1 shows. Since, the optimization community has mostly focused on optimization in the stable regime, we believe our work steps the founding stone to study deep models that operate beyond stability, which neural networks typically operate on. 



\RC The claims on LoRA dynamics in Section 4.2 lack sufficient evidence. 

\AR

\RC Wrong citation: "Ahn et al. (2022) established the phenomenon in two-layer networks..." is citing the paper [Ahn et al., 2022], but it should be citing another paper [Ahn et al., 2023].

\AR Thank you for pointing this out. We changed it to the proper citation. 

\RC Could the authors please include the following references in the related works section? Prior to [Cohen et al., 2021], [Jastrzebski et al., 2019] and [Jastrzebski et al., 2020] demonstrated that step size influences sharpness along optimization trajectories. Additionally, [Ahn et al., 2023], [Song et al., 2023], and [Karla et al., 2023] provide rigorous analyses of learning dynamics at the Edge of Stability in simplified settings, such as two-layer linear networks. [Zhu et al., 2024] and [Chen et al., 2024] study gradient descent dynamics for quadratic models in large learning rate regimes where catapults occur


\AR We agree and added the citations in the proper context, 


\RC When initialization is outside the singular vector invariant set, how do loss and sharpness behave at the Edge of Stability? Specifically, does the weight converge to a period-2 orbit, or does the loss oscillate while decreasing non-monotonically? Similarly, if oscillations begin before strict balance is achieved, how are the loss trajectory and learning dynamics affected? It would be insightful if the authors could provide additional experimental results exploring these scenarios.


\AR When initialization is outside the SVS set, the loss still oscillates within a 2-period orbit and the weights indeed converge to period-2 orbit. Near the global minima, the loss can't decrease non-monotonically since the oscillations are contained in a \textit{single loss basin}. We consider three initializations outside the SVS set 1) our intialization scheme, 2) all matrices initialized with random Gaussian weights and 3) matrices with orthogonal initialization and provide them in experiment-\textcolor{red}{Soo-Min refer it}. In the last two cases, the initialization do not belong to the SVS set but even at EOS, they converge to the SVS set. The existing theory needs to be modified to study the dynamics in the singular vectors at large learning rate to tackle these conditions. 
In Figure-, we consider an initialization larger than the one in Theorem-1, where strict balancing fails to hold. Here, although oscillations start before strict balancing holds, the loss and singular values of weights exhibits 2-period oscillations.  
\section{Reviewer nnRE}

\RC Lemma 2 does not tell us that the singular values become balanced as 
 (L191-193).  The strictly decreasing balancing gap and the difference being lower bounded by zero does not lead to the convergence to zero (L307-309). Think about the sequence. Can you provide a proof to show the convergence to zero?
 
\AR \AR \AR Thank you for pointing out this important part. Indeed if a sequence $\{a(t)\}_{t}$ lower bounded by $0$ obeys $a(t+1)<a(t)$, then the sequence is not guaranteed to converge to $0$. But if there exists a $0<c<1$ where $a(t+1)<c.a(t)$, then the sequence is guaranteed to converge to $0$ as $t \rightarrow \infty$ [refer lemma]. However, this is not exactly what happens with GD and the balancing does not exactly go to zero but something very small which does not change the conclusion of our paper. We made an attempt to modify lemma-2 to show strict balancing $b(t+1) < c. b(t)$. Based on our proof, we analyze 2 cases and if there exists a $0<c<1$ in each of these cases: 1) $\pi(t)< \sigma_{*}$, and 2) $\pi(t)> \sigma_{*}$. We prove that for the initialization limit in Lemma-1, we indeed have $0<c<1$. However, when $\pi(t)> \sigma_{*}$, we can only prove for $c=1$, i.e, $a(t+1)< a(t)$. So, the balancing gap can't be proven to go to exactly zero but something quite small (probably only significant in the machine precision scale) as our experiments suggest. A more rigorous way to prove would be to perform an error analysis, i.e, suppose the error in approximate balancing is say $e(\alpha)$, where $\alpha$ is the initialization. Then in what orders of $e(\alpha)$ is there an error in the eigenvalue calculations that assume strict balance. However, we believe this would require more rigorous analysis that does not substantially change the contribution or observations made in the paper. If time allows, we will try to come up with an error analysis during the discussion period. 


\RC "When the learning rate is large enough to induce catapults, we observe that the training loss decreases rapidly ..." The statement does not seem right. Large lr may lead to fast decreasing of the training loss because of its large stepsize not because of the catapults. Same for the statement "when the learning rate is small, convergence takes much longer, as the model seems to bounce around within the same local basin before reaching a low training loss". Can you provide an experiment to show that "the catapult implies faster optimization"?

\AR

\RC It is hard to say "in DLNs, self-stabilization does not occur". Actually, also in the original EOS paper, the sharpness oscillates above 
 (they say "it hovers above 
"). It is true that DLNs do not satisfy the condition of the proposition in the self-stabilization paper, but self-stabilization is a broader concept and the threshold may not be necessarily 
. The statement should be written carefully.

\AR We modified this statement. In DLNs, sharpness oscillates periodically about the limit $\frac{2}{\eta}$ and this comes from the property that in each eigenvector direction, it satisfies the stable oscillation condition. This is in contrast to the self-stabilization effect explored in Damian et al where sharpness decreases below a specified threshold and is a result of a closed loop sharpness reduction feedback. The difference between sustained oscillations and damped oscillations/catapults are classically studied in control system by locating the poles of transfer function of a closed loop system. We believe a similar study can be made to study these two cases but it is currently beyond the scope of our work. 


\RC In Thm 1, what is 
 exactly? It only says that 
. Does 
 depend on 
? As it says that the matrix oscillates, it is likely that 
 changes with 
. Is it 
 if 
 is odd (even) and 
 if 
 is even (odd)? If the readers have to guess the meaning, then it is not well-written.

\AR $\rho_{1}(t)$ is the rank-1 oscillation magnitude that takes only two values $\rho_{1}$ and $\rho_{2}$ since it is a 2-period orbit. We changed the notation without any $t$-dependence to make the theorem clearer. 

 \RC What do you mean by that "To show oscillations in two or more subspace, we can easily extend Theorem 1"? Why do we need to set 
? I don't think this is a trivial result from Thm 1. Can you elaborate more on the paragraph L241-248? My understanding is that 
 in the first term of 
 in Thm 1 corresponds to the oscillation, is it right? I didn't fully understand the 
-subspace oscillation part.

\AR We apologise for the incorrect phrasing. Theorem-1 is actually the direct consequence of the stable subspace oscillation theorem in Theorem-2. This theorem states that whenever the step-size $\eta$ is such that it exceeds the stability condition $\eta>\frac{2}{\lambda_{i}}$ in each eigenvector direction $\Delta_{i}$, then it stably oscillates about the eigenvector direction $\Delta_{i}$. So by ensuring that $\eta$ lying in $ \lambda_{3}<\frac{2}{\eta}< \lambda_{2} < \lambda_{1} $, oscillations only occur along $\Delta_{1}$ and $\Delta_{2}$ and not along $\Delta_{3}$. The choice of learning rate allows us to control the dimension of the subspace for oscillation. 

For example when $\lambda_{2}<\frac{2}{\eta}< \lambda_{1}$, oscillations occur only along the top eigenvector $\Delta_{1}$. Since, $\Delta_{1}= \mathrm{vec}\left(\frac{1}{\sqrt{L}}\mbf{u}_1 \mbf{v}_1^\top, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top, \ldots, \frac{1}{\sqrt{L}}\mbf{v}_1 \mbf{v}_1^\top \right)$ (eigenvector of the flattened Hessian), the oscillations for each weight matrix occur about this eigenvector. For example, $\mbf{W}_{L}$ oscillates in the rank-1 outer product $\mbf{u}_1 \mbf{v}_1^\top$ and $\mbf{W}_{l}$ oscillates around $\mbf{v}_1 \mbf{v}_1^\top$. 


\RC For the second term 
, isn't it 
 with 
, not 
 with 
 
\AR Thank you for pointing out this typo. We fixed it in the revision. 

\RC Is there any reason that we can view the adaptations as individual low-rank matrix factorization problems? Can you elaborate more on this? I hope some "math" may help the reader to understand this.




\RC "Notably, for ranks 
 and 
, there are catapults ... do not occur for 
 or 
". What catapults are you talking about? In Fig 7, it is hard to see the catapult (or loss oscillation) and compare it with other ranks (
). I don't fully understand Section 4.2.


\RC Can you somehow compute or estimate the 
 for Fig 10 (a) and for each image in Fig 10 (b)? It would be better to have a quantitative understanding of the "low-complexity learning"

\AR Fitting low frequency images with convolutional neural networks can be considered a low complexity task because of spectral bias (https://arxiv.org/abs/1806.08734) of such networks. Smooth/low frequency images are much easier to fit for convolutional neural networks due to spectral bias where the networks incrementally learns each Fourier Frequency (low to high). So, a measure of complexity of each image can be the Fourier bandwidth of the image. Furthermore, while fitting low frequency images or simple datasets like the MNIST, intrinsic update dimension in the parameter space of networks tend to be much smaller (https://arxiv.org/abs/1804.08838,https://arxiv.org/abs/2104.08894). However, the exact relation between the intrinsic dimension of parameter update and it's sharpness is unknown and calls for further in-depth study. We revised the manuscript stating that we highlight the difference between the low complexity learning in CNN's vs linear nets in our setting. 

. 




 




\section{TODOs}

\begin{itemize}
    \item Modify the proof to change the balancing to $c<1$. Consult the proof strategy. (moderate-hard)
    \item Proof sketch to address how we tackle non-identitiy initialization. 
    \item Do the depth experiment. For large depth, lr needed to enter EOS is smaller. (easy)
    \item Experiments at large initialization, for which oscillations starts occuring before balancing. (easy)
    \item 
\end{itemize}



{\small 
\bibliography{arXiv/iclr2025_conference}
\bibliographystyle{ICLR/iclr2025_conference}
}
\end{document}