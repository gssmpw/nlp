\section{Introduction}

Understanding generalization in deep neural networks requires an understanding of the optimization process in gradient descent (GD).
In the literature, it has been empirically observed that the learning rate $\eta$ plays a key role in driving generalization~\citep{hayou2024lora,lewkowycz2020large}. The ``descent lemma'' from classical optimization theory says that for a $\beta$-smooth loss $\mathcal{L}(\mathbf{\Theta})$ parameterized by $\mathbf{\Theta}$, GD iterates satisfy
\begin{align*}
    \mathcal{L}(\mathbf{\Theta}(t+1)) \leq \mathcal{L}(\mathbf{\Theta}(t)) - \frac{\eta(2-\eta \beta)}{2} \|  \nabla \mathcal{L}(\mathbf{\Theta}(t)) \|_{2}^2,
\end{align*}
and so if the learning rate is such that $\eta < 2/\beta$, then the loss monotonically decreases.
%However, classical optimization theory on GD restricts itself to small infinitesimal learning rate and unrealistic convergence assumptions which barely holds for training deep networks in practice. Traditional analysis of GD show that for $L$-smooth loss, only if the step-size $\eta$ is selected to be less than $\frac{2}{L}$, the training is stable and the loss decreases monotonically.
However, many recent works have shown that the training loss decreases even for $\eta > 2/\beta$, albeit non-monotonically.
Surprisingly, it has been observed that learning rate beyond the stability threshold often provides better generalization over smaller ones that lie within the stability threshold. This observation has led to a series of works analyzing the behavior of GD within a regime dubbed ``the edge of stability'' (EOS). By letting $\mbf{\Theta}$ parameterize a deep network, we formally define EOS as follows:
%Consider GD is run with learning rate $\eta$ to minimize a loss $l(\mbf{\theta})$ with a deep network. \cite{cohen2021gradient} observed a surprising and a commonly occuring phenomenon in deep networks coined "Edge of Stability" which is formally defined:
\begin{phenomenon}[Edge of Stability~\citep{cohen2021gradient}]
During training, the sharpness of the loss, defined as $S(\mbf{\Theta}):= \| \nabla^2 \mathcal{L}(\mbf{\Theta})\|_{2}$, continues to grow until it reaches $2/\eta$ (progressive sharpening), after which it stabilizes around $2/\eta$. During this process, the training loss behaves non-monotonically over short timescales but consistently decreases over long timescales.
\end{phenomenon}


\begin{figure}[t!]
    \centering
     \begin{subfigure}[t!]{0.49\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/sval_bifur.pdf}
     \caption*{Singular Values of Weights}
     \end{subfigure}
     \begin{subfigure}[t!]{0.49\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/hess_bifur.pdf}
         \caption*{Eigenvalues of Hessian}
     \end{subfigure}
    \caption{Bifurcation plot of the oscillations in the singular values (left) and the eigenvalues of the Hessian (right) of a 3-layer end-to-end DLN. The bifurcation plots indicate the existence of a period-doubling route to chaos in DLNs, which we analyze by examining the two-period orbit. %\edit{Here, $\beta =L \sigma^{2-\frac{2}{L}}_{1}(\mbf{M}_\star) $ and $\eta > 2/\beta$ corresponds to the EOS regime where $\sigma_{1}(\mbf{M}_\star)$ denotes the largest singular value of target matrix $\mbf{M}_\star$.}
    Here, $\eta > 2/\beta$ corresponds to the EOS regime, where $\beta = L\sigma_{\star, 1}^{2 - 2/L}$ is the sharpness at the minima, $L$ is the depth of the network and $\sigma_{\star, 1}$ is the first singular value of the target matrix $\mbf{M}_\star$.
    }
    
    \label{fig:bifurcation}
\end{figure}


Using a large learning rate to operate at the EOS is hypothesized to give better generalization performance by inducing ``catapults'' in the training loss~\citep{zhu2023catapults}. Intuitively, whenever the sharpness $S(\mbf{\Theta})$ exceeds the local stability limit $2/\eta$, the GD iterates momentarily diverge (or catapults) out of a sharp region and self-stabilizes~\citep{damian2023selfstabilization} to settle for a flatter region where the sharpness is below $2/\eta$. This self-stabilization mechanism enables GD to auto-regularize and find flatter solution which has shown to correlate with better generalization~\citep{keskar2017on, izmailov2019averaging, petzka2021relative, foret2021sharpnessaware, gatmiry2023inductive}. Of course, the dynamics within EOS differ based on the loss landscape. 
%For example, as illustrated in Figure~\ref{fig:catapults_vs_osc}, 
When the loss landscape is highly non-convex with many local valleys, catapults may occur, whereas sustained oscillations may exist for benign landscapes. 
When sustained oscillations occur, the sharpness hovers about the local stability limit $2/\eta$ rather than settling to a sharpness below $2/\eta$. We refer to this region as ``beyond the EOS'' following existing work~\citep{wang2023good,minimal_eos}.
It is of great interest to understand these behaviors within different architectures to further our understanding of EOS.
%and how these behaviors can lead to better generalization.

From a theoretical perspective, there have been many recent efforts to understand EOS. These works generally focus on analyzing ``simple'' functions, examples including scalar losses \citep{minimal_eos,wang2023good,kreisler2023gradient}, quadratic regression models \citep{agarwala2022second}, diagonal linear networks \citep{even2024s} and two-layer matrix factorization \citep{chen2023edge}. 
However, the simplicity of these functions cannot fully capture the behaviors of deep neural networks within the EOS regime. Specifically, the following observations remain unexplained by existing analyses: (i) mild (or no) sharpening occurs when either networks are shallow or ``simple'' datasets are used for training~(Caveat 2 from~\citep{cohen2021gradient}); and (ii) the oscillations and catapults in the training loss occur in the span of the top eigenvectors of the NTK~\citep{zhu2023catapults}.
%(i) mild (or no) sharpening occurs when deep networks are trained using ``simple'' datasets~(Caveat 2 from~\citep{cohen2021gradient}); (ii) the oscillations and catapults in the weights occur within a low-rank subspace or within the top singular values of each weight matrix~\citep{zhu2024catapultssgdspikestraining}; and (iii) the oscillations are stronger within weight subspaces that generate more prominent features in the output fitting the data. For example, \cite{zhu2024catapultssgdspikestraining} observed that large catapults occurred within the top eigenspace of the NTK, followed by mild (or no) catapults in the complementary space, and \cite{rosenfeld2024outliers} showed that network inputs with large-magnitude features and strong opposing signals oscillate with larger spikes for those particular inputs.%
%\qq{need to discuss the limitation of previous results, especially for the last part} \smk{actually the limitation is that they cannot explain the phenomena}


\begin{comment}
    
These works developed our understanding of EOS but are far from capturing EOS dynamics occuring in deep neural networks. The formulation of deep neural networks is very close to deep matrix factorization problem which only exempts the use of non-linear activations and with a practical assumption that the underlying data is low rank. However, existing literature in deep matrix factorization focused on gradient flow dynamics \cite{arora2019implicit,saxe2014exact} or GD operating in the stable regime \cite{chou2024gradient,gidel} where training loss decreases monotoncally. We enlist important observations made while training deep network in the EOS regime which are currently unexplained by existing analysis in the literature:

\begin{enumerate}
\item  (Obs-1:) \textbf{Mild-sharpening} or no sharpening occurs 1) when deep networks are trained to fit \textit{simple} datasets (Caveat-2 \cite{cohen2021gradient}) (refer Figure) or 2) when the depth of the network is small.  % 
\item (Obs-2:) \textbf{Subspace Oscillation}: Oscillation/catapaults in the weight space occurs in a low rank subspace or within the top few sinuglar values of weight matrix in each layer \cite{zhu2024catapultssgdspikestraining}, hence preserving \textit{structure amidst chaos}.%
\item (Obs-3:) \textbf{Stronger oscillations} occur along weight subspace that generates prominent features in the output fitting the data. \cite{zhu2024catapultssgdspikestraining} observed that large catapaults occured in the top eigenspace of the NTK followed by mild/no catapaults in complementary space. \cite{rosenfeld2024outliers} showed that network inputs with large magnitude features and strong opposing signals oscillate with larger spikes for that particular input. %
\end{enumerate}

Our analysis of EOS on deep matrix factorization aims at explaining these 3 key observations and thereby attempting to bridge the gap between theory and practice. But before understanding EOS dynamics, it is important to understand gradient flow dynamics in deep matrix factorization. 

\begin{figure}[t!]
    \centering
     \begin{subfigure}[b]{0.2475\textwidth}
         \centering
        \includegraphics[width=\textwidth]{figures/intro/dln_train_loss.pdf}
        \caption{Linear Networks}
     \end{subfigure}
     \begin{subfigure}[b]{0.2475\textwidth}
         \centering
         \includegraphics[width=\textwidth]{figures/intro/mlp_train_loss.pdf}
         \caption{ReLU Networks}
     \end{subfigure}
        \begin{subfigure}[b]{0.475\textwidth}
         \centering
         \includegraphics[width=0.95\textwidth]{figures/intro/lora_osc.pdf}
         \caption{BERT using LoRA}
     \end{subfigure}
     \vspace{-0.1in}
    \caption{Illustration of instability in the training loss for different network architectures. When the loss landscape is more complex with many local valleys (b, c), catapults occur, whereas free/sustained oscillation occur in (a). Parts with oscillations/catapults are zoomed in.}
    
    \label{fig:catapults_vs_osc}
\end{figure}

\end{comment}



%In this work, we attempt to bridge the gap between theory and practice by providing a fine-grained analysis of the learning dynamics of deep linear networks (DLNs) within the EOS regime. DLNs share many characteristics with their nonlinear counterparts, making them a useful surrogate for analyzing behaviors in deep neural networks~\citep{wang2024understandingdeeprepresentationlearning, zhang2024when}. For example,~\cite{zhang2024when} has provably shown that bias-free ReLU networks behave similarly to linear networks.
%Generally, existing works on DLNs focus on analyzing the effect of depth and the initialization scale, and how they implicitly bias the trajectory of gradient flow towards low-rank solutions when the learning rate is chosen to be stable~\citep{saxe2014exact, arora2018optimization, implicit_dmf, zhang2024structure, pesme2023saddle, jacot2022saddletosaddle}.

In this work, we present a fine-grained analysis of the learning dynamics of deep linear networks (DLNs) beyond the EOS regime, demonstrating that these phenomena can be partially replicated and effectively explained using DLNs. Generally, there are two lines of work for DLNs: (i) those that analyze the effects of depth and initialization scale, and how they implicitly bias the trajectory of gradient flow towards low-rank solutions when the learning rate is chosen to be stable~\citep{saxe2014exact, arora2018optimization, implicit_dmf, pesme2023saddle, jacot2022saddletosaddle}, and (ii) those that analyze the similarities in behavior between linear and nonlinear networks~\citep{wang2024understandingdeeprepresentationlearning, zhang2024when,yaras2023law}.
Our analysis builds upon these works to show that DLNs exhibit intricate and interesting behaviors outside the stability regime and to demonstrate how factors such as depth and initialization scale contribute to the EOS regime. Our main results can be summarized as follows: 
%\qq{the bolded sentences below do not capture the key contributions. It needs to be rephrased.}
%particularly periodic oscillations within a small subspace, where the subspace dimension is precisely characterized by the step size. 
%\qq{need a short title for each bullet point}
\begin{itemize}

\item \textbf{Walk Towards the Flattest Global Minima beyond EOS.} 
 Similar to the observations made by~\cite{chen2023edge}, we adopt the proof techniques of~\cite{kreisler2023gradient} to show that the layers (or weights) of the DLN become increasingly balanced under mild assumptions at EOS. Specifically, we characterize how small the balancing gap at initialization must be for GD to reduce the imbalance over iterations. We further show that balanced minima correspond to the flattest minima in DLNs and our result captures an implicit regularization effect that drives the network toward the flattest minima at learning rates beyond EOS regime.


\item \textbf{Sharpness Scales with the Network Depth.} We identify all eigenvalues of the Hessian at the balanced minimum, demonstrating that the sharpness (i.e., the largest eigenvalue) scales with network depth. This rigorously validates the observations made by~\cite{cohen2021gradient} and shows that the learning rate required to enter the EOS regime is depth-dependent, further highlighting its significance in deep networks.

\item \textbf{Oscillations in Low-Dimensional Subspaces.} Once the network goes beyond the EOS regime, we prove that the network undergoes periodic oscillations within $r$-dimensional subspaces in DLNs, where $r$ is precisely characterized by the learning rate. For DLNs, a period-doubling route to chaos~\citep{Ott_2002} exists in both the singular values of the DLN and the eigenvalues of the Hessian, as shown in Figure~\ref{fig:bifurcation}. We characterize the case of the two-period orbit, aiming to contribute to explaining the empirical observations by~\cite{zhu2023catapults} and~\cite{cohen2021gradient}. 

%We also prove that the learning rate needed to enter EOS is a function of the network depth, further revealing its role in deep networks.
%\item \textbf{EOS Dynamics for DLNs and Diagonal Linear Networks.} 
%\edit{We prove that while DLNs and diagonal linear networks have different curvatures about the global minimum, they exhibit similar behaviors within the EOS regime.
%We show that the additional eigendirections present in DLNs are not explored during training, making the behavior of the two networks synonymous.}
\end{itemize}

\begin{comment}
    
\subsection{Implicit regularization in deep linear models with gradient flow}
Over the past few years, several works have analyzed the effect of depth and initialization in deep linear networks \cite{saxe2014exact, arora2018optimization, implicit_dmf, zhang2024structure}. It has been observed that small initialization and larger depth make the trajectory of gradient flow implicitly biased towards low-rank solutions. Similar observations have been made in diagonal linear networks \cite{woodworth2020kernel, pesme2021implicit}, where the scale of initialization determines whether the dynamics of the iterates are in the rich regime or the kernel regime. Smaller initialization promotes rich regime dynamics, where features are learned incrementally through saddle-to-saddle dynamics \cite{pesme2023saddle, jacot2022saddletosaddle}. Larger depth increases the duration of these saddle points and has a stronger implicit bias towards exact low-rank solutions by suppressing the residual singular values through the depth factor \cite{arora2018optimization}. In deep matrix factorization gradient flow aligns the singular vectors of successive layer factors \cite{du2018algorithmic}. 

Due to this alignment, the flow dynamics in deep matrix factorization are the same as those in deep linear networks, but act on the singular values of the matrices.

Our analysis and experiments reveal certain surprising (yet explainable) phenoemons that is critical to understand Edge of Stability:

\begin{itemize}
    \item  The dynamics in deep matrix factorization and diagonal linear networks (or even tensor factorization) differ in the EOS regime, unlike in gradient flow (GF). These dynamics depend on the dimensionality of the layers (whether vector in diagonal linear networks, matrix in deep matrix factorization, or tensor in tensor factorization), which is not the case in GF.
    \item  We prove that in deep linear networks, only the depth and the largest singular value of the target matrix determine whether the network will operate in the EOS regime.
    \item The condition to enter the EOS regime is independent of the initialization scale and the dimensionality of the layers, dimension the rank of the target matrix. 
\end{itemize}


\ag{I am unsure whether it is better to state it after notations otherwise it will be a reptitition. Probably we should state this in the contribution section?}


\end{comment}





%\paragraph{Related Works.} We briefly survey a few related works to highlight their differences, and provide a detailed discussion in \Cref{app:related}. 
%    DLNs are often used as prototypes to study the behaviors of nonlinear networks~\citep{benign, saxe2014exact, wang2024understanding, nc}. The most relevant literature on DLNs are those by Yaras et al.~\citep{yaras2023law, yaras2024compressible} and Kwon et al.~\citep{kwon}, who reveal that the weight updates of deep networks occur within an invariant subspace. Our work differs from that of Yaras et al.~\citep{yaras2023law} in that we fully capture the learning dynamics of DLNs throughout the entire GD process. While Kwon et al.~\citep{kwon} observe invariant weight updates, they use this observation for model compression and do not study the learning dynamics with large learning rates. Regarding the edge of stability, the most relevant works are those that analyze scalar functions to demonstrate that the edge of stability occurs on such functions, which have a non-zero third-order derivative and satisfy certain regularity conditions~\citep{chen2023edge,wang2023good,minimal_eos}. However, as mentioned previously, these works do not capture the more complicated models that we consider in this work.


\section{Notation and Problem Setup}

%\qq{we need to be consistent, some place we call it deep linear networks, other places we call it deep matrix factorization. Let us call it "deep linear network", and training loss for DLN}

\paragraph{Notation.}
We denote vectors with bold lower-case letters (e.g., $\mbf{x}$)
and matrices with bold upper-case letters (e.g., $\mbf{X}$).
%Given any $n \in \N$,
We use $\mbf{I}_n$ to denote an identity matrix of size $n \in \N$.
We use $[L]$ to denote the set $\{1, 2, \ldots, L\}$. 
We use the notation $\sigma_i(\mbf{A})$ to denote the $i$-th singular value of the matrix $\mbf{A}$. We also use the notation $\sigma_{\ell, i}$ to denote the $i$-th singular value of the matrix $\mbf{W}_\ell$.

\begin{comment}
\paragraph{Deep linear model}
Consider the problem of least squares fitting problem with multidimensional output. We want to learn a mapping $f(.):\mbb{R}^{d}\rightarrow \mbb{R}^{d}  $ given $d$ number of input vectors $\mbf{x}_{i} \in \mbb{R}^{d}$ to same number of target vector $\mbf{y}_{i} \in \mbb{R}^{d}$. Considering $f(.)$ to be deep linear model with set of parameters  $\mbf{\Theta} = \left(\mbf{W}_1, \mbf{W}_2, \ldots, \mbf{W}_L \right)$, then the fitting regression problem can be formulated as minimizing the loss wrt $\frac{1}{2} \|\mbf{Y} - \mbf{X} \mbf{W}_L \cdot \ldots \cdot \mbf{W}_1\|^2_{F} $ where $\mbf{X}$ and $\mbf{Y}$ are column stacking of the input  $\left\{\mathbf{x}_{i}\right\}_{i=1}^d$ and target $ \left\{\mathbf{y}_{i}\right\}_{i=1}^d$. Assuming the data matrix $\mbf{X}$ to be whitened matrix with dominant features such that $\mbf{X}^T \mbf{Y} = \mbf{M}$ (say) is low rank, then the optimization can be formulated as a low rank recovery problem. Throughout the paper, we will restrict ourselves to analyze the deep matrix factorization problem, which we introduce next.
\end{comment}


 \paragraph{Deep Matrix Factorization Loss.} 
 %\qq{needs to explain why we choose to study based upon deep matrix factorization. We also need to be consistent when we refer to deep matrix factorization, or DLNs.}
The objective in deep matrix factorization is to model a low-rank matrix $\mbf{M}_\star \in \mbb{R}^{d\times d}$ with $\rank(\mbf{M}_\star) = r$ via a DLN parameterized by a set of parameters $\mbf{\Theta} = \left(\mbf{W}_1, \mbf{W}_2, \ldots, \mbf{W}_L \right)$, which can be estimated by solving
\begin{align}\label{eqn:deep_mf}
    \underset{\mbf{\Theta}}{\rm{arg min}} \, f(\mbf{\Theta}) \coloneqq \frac{1}{2}\|\underbrace{\mbf{W}_L \cdot \ldots \cdot \mbf{W}_1}_{\eqqcolon \mbf{W}_{L:1}} - \mbf{M}_\star\|^2_{\mathsf{F}},
\end{align}
where we adopt the abbreviation $\mbf{W}_{j:i} = \mbf{W}_{j}\cdot \ldots \cdot \mbf{W}_i$ to denote the end-to-end DLN and is identity when $j < i$. We assume that each weight matrix has dimensions $\mbf{W}_\ell \in \mbb{R}^{d\times d}$ to observe the effects of overparameterization. We also assume that the singular values of $\mbf{M}_\star$ are distinct.
%such that $\sigma_{\star, 1} > \ldots > \sigma_{\star, r}$.





\paragraph{Optimization.}
Each weight matrix $\mbf W_\ell \in \mbb{R}^{d\times d}$ is updated using GD with iterations given by
\begin{align}\label{eqn:gd}
    \mbf{W}_\ell(t) = \mbf{W}_\ell(t-1) - \eta\cdot \nabla_{\mbf{W}_\ell}f(\mbf{\Theta}(t-1)), \quad \forall \ell \in [L],
\end{align}
where $\eta > 0$ is the learning rate and $\nabla_{\mbf{W}_\ell}f(\mbf{\Theta}(t))$ is the gradient of $f(\mbf{\Theta})$ with respect to the $\ell$-th weight matrix at the $t$-th GD iterate. 




\paragraph{Initialization.}

In this work, we consider both balanced and unbalanced initializations, respectively:
\begin{align}
\label{eqn:balanced_init}
 &\mbf{W}_\ell(0) = \alpha \mbf{I}_d, \quad \forall \ell \in [L], \\
 \label{eqn:unbalanced_init}
 &\mbf{W}_L(0) = \mbf{0},  \quad \mbf{W}_\ell(0) = \alpha \mbf{I}_d, \quad \forall \ell \in [L-1],
\end{align}
where $\alpha > 0$ is a small constant. We assume $\alpha$ is chosen small enough such that $\alpha \in (0, \sigma_{\star, r})$, where $\sigma_{\star, r}$ is the $r$-th singular value of $\mbf{M}_\star$. Generally, many existing works on both shallow and deep linear networks assume a zero-balanced initialization (i.e., $\mbf{W}_i^\top(0)\mbf{W}_i(0) = \mbf{W}_j(0)\mbf{W}^\top_j(0)$ for $i \neq j$). This introduces the invariant $\mbf{W}_i^\top(t)\mbf{W}_i(t) = \mbf{W}_j(t)\mbf{W}^\top_j(t)$ for all $t > 0$, ensuring two (degenerate) conditions throughout the training trajectory: (i) the intermediate singular vectors of each of the layers remain aligned and (ii) the singular values stay balanced.
For the unbalanced initialization, the zero weight layer can be viewed as the limiting case of initializing the weights with a (very) small constant $\alpha' \ll \alpha$, and has been similarly explored by~\cite{two_layer_bias} and \cite{xu2024provable}, albeit for two-layer networks. 
The zero weight layer relieves the balancing condition of the singular values.
Rather than staying balanced, we show that the singular values become increasingly balanced (see Proposition~\ref{prop:balancing}). This allows us to jointly analyze the singular values of the weights for either case.

Nevertheless, we also show that our analysis is not limited to either initialization but applies to \emph{any initialization} that converges to a set we call the singular vector stationary set (see Proposition~\ref{prop:svs_set}). To the best of our knowledge, it is common to assume that the singular vectors remain aligned, as many existing works make the same assumption~\citep{two_layer_bias, implicit_dmf, saxe2014exact, gidel,chou2024gradient, kwon}. Hence, throughout the rest of this paper, we refer to the balancing gap as the difference in singular values across layers and clarify where necessary.



%In the following section (as well as Appendix~\ref{sec:additional_exp}), we provide examples of such initializations, which we adopt for our analyses as well.



%\smk{I am pretty adamant that we should have this section like this; it has clear motivation.} \ag{Slightly debatable, but let's hear opinions of others. }
%\ag{@Qing: would it be better if we specify the general class of initialization instead of the zero-one initilaization in the main text?}\smk{Note that we never prove that any initialization belongs in the SVS set! And we need to then change all of our proofs to be $\mbf{Q}$ orthogonal matrices rather than the target matrix $\mbf{V}$ like we have now.}
 % For deep linear networks, it is common to use an orthogonal-type initialization rather than a random one~\citep{kwon, kwon2024on, yaras2023law}, as it has been provably shown that random initialization slows down convergence~\citep{Hu2020Provable}.