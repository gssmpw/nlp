The goal of our experimental evaluation is to study the following questions:

\begin{enumerate}
    \item Can \ourslong outperform Diffusion Policy by delivering more responsive control? (\Cref{sec:main})?
    \item Can \ourslong function as a superior acceleration method compared to commonly used alternatives on simpler tasks that do not require responsive control (\Cref{sec:exp_speed_claim})?
    \item What are the effects of the components introduced by \ourslong (\Cref{sec:ablation})?
\end{enumerate}

\subsection{Experimental Setup}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/task_visualization.pdf}
    \caption{\textbf{Task Visualization in Simulation.} ManiSkill tasks including PushT, RollBall, PushChair, StackCube, TurnFaucet; Adroit tasks including Relocate, Door, Pen, Hammer. See \Cref{sup:task_descriptions} for more properties about each task.}
    \label{fig:task_vis}
\end{figure}

To validate the responsiveness and efficiency of \ourslong, our experimental setup incorporates \textit{variations in the following dimensions}:

\begin{itemize}
    \item \textbf{Task Types:} Stationary robot arm manipulation, mobile manipulation, dual-arm coordination, dexterous hand manipulation, articulated object manipulation, and high-precision tasks.
    \item \textbf{Demo Sources:} Teleoperation, Task and Motion Planning, RL, and Model Predictive Control.
    \item \textbf{Observation Modalities:} State observation (low-dim) and visual observation (high-dim).
\end{itemize} 

\subsubsection{Task Descriptions}

Our experiments are conducted on 9 tasks from 3 benchmarks: ManiSkill2 (robotic manipulation; 3 tasks), ManiSkill3 (robotic manipulation; 2 tasks) and Adroit (dexterous manipulation; 4 tasks). These tasks are separated into 2 groups to validate the responsiveness and efficiency of \ourslong.

\textbf{Response-sensitive Group: Tasks Involving Dynamic Object Manipulation Requiring Responsive Control} 
We consider 5 challenging tasks involving contact-rich, dynamic object manipulation to assess the responsiveness of RNR-DP. PushT requires using a stick to push a T-shaped block to a target location and orientation. RollBall involves pushing and rolling a ball to a randomized goal region. PushChair tests bimanual manipulation of articulated objects with variations. Adroit Relocate involves picking up a ball and moving it to a goal position, while Adroit Door requires unlocking and opening a door. See \Cref{fig:task_vis} for task visualizations, and more details are included in \Cref{sup:task_descriptions}.

\textbf{Regular Group: Simpler Tasks that Do Not Require Responsive Control}
We consider the rest 4 simpler tasks that do not require responsive control to validate the efficiency of RNR-DP. StackCube requires picking up a cube and stack it onto another cube. TurnFaucet uses a stationary arm to turn on faucets of various geometries and topology. Adroit Pen repositions the blue pen to match the orientation of the green target. Adroit Hammer picks up a hammer and drives a nail into a board. More details are included in \Cref{sup:task_descriptions}.

\subsection{Baselines}

We compare our \ourslong against a set of strong baselines related to Diffusion Policy.

\textbf{Diffusion Policy (DDPM)} is the original setting from \citet{chi2023diffusion}, which utilizes a conditional Denoising Diffusion Probalistic Model with discrete time scheduling to generate actions. We use 100 DDPM denoising steps in our experiments.

\textbf{Diffusion Policy (EDM)} is introduced in \citet{prasad2024consistency} to train a teacher model and employs the EDM \citep{karras2022elucidating} framework with continuous-time scheduling. This model takes the current position \(x_t\), time \(t\), and conditioning \(o\) as inputs along a PFODE and is used to estimate the derivative of the PFODE's trajectory. We utilize the EDM model with Heun's second-order solver taking 80 denoising steps, which requires two neural network evaluations per discretized step in the ODE, resulting in a total of 159 neural function evaluations (\textbf{NFEs}).

\textbf{Diffusion Policy (DDIM)} is used in \citet{chi2023diffusion} to accelerate Diffusion Policy with Diffusion Denoising Implicit Model framework \citep{song2020denoising}. We test 1, 2, 4, and 8 DDIM denoising steps.

\textbf{Consistency Policy (CP)} is introduced in \citet{prasad2024consistency}, which employs the Consistency Trajectory Model \citep{kim2023consistency} to distill the knowledge from a Teacher Model (EDM). We evaluate both the 1-step Consistency Policy and the 8-step chaining Consistency Policy.

\textbf{Streaming Diffusion Policy (SDP)} is a recent advancement over Diffusion Policy that stays close to our approach. See \Cref{sup:sdp} for detailed discussion.

\subsection{Results \& Analysis on Responsive Control}
\label{sec:main}

First, we provide comprehensive evaluation and validate the responsiveness of RNR-DP on 5 tasks (Response-sensitive Group) involving contact-rich dynamic object manipulation, a primary focus of our policy. \Cref{fig:bars} shows an overview of performance improvement. As shown in \Cref{table:exp_dynamics_claim_state} and \Cref{table:exp_dynamics_claim_visual}, RNR-DP consistently outperforms Diffusion Policy across all tested scenarios in both state and visual experiments. Specifically, in state experiments, RNR-DP achieves a \( 15.1\% \) improvement over Diffusion Policy, while in visual experiments, it demonstrates a \( 24.9\% \) improvement. Overall, RNR-DP surpasses Diffusion Policy by \( 18.0\% \). Notably, in the Relocate task, RNR-DP achieves a significant performance boost of \( 38.6\% \) over Diffusion Policy.

We attribute these significant performance gains to the design of our noise-relaying buffer and sequential denoising mechanism, which offer two key advantages. The noise-relaying buffer ensures that the denoising of each action remains consistent, allowing actions to follow the same mode. This effectively eliminates frequent mode bouncing, enabling RNR-DP to support single-action rollouts (\( T_a = 1 \)). Furthermore, the single-action rollout in RNR-DP ensures that all actions are conditioned on the latest observations, resulting in significantly more responsive control to environmental changes compared to Diffusion Policy.

\input{tables/exp_dynamics_claim_state}
\input{tables/exp_dynamics_claim_visual}

\subsection{Results \& Analysis on Efficient Control}

In this section, we conduct extended experiments on 4 simpler tasks (Regular Group) that do not require responsive control to evaluate the efficiency of RNR-DP. An overview of the performance improvements is provided in \Cref{fig:bars}. \Cref{sec:nfe_a} introduces a metric designed for fair efficiency comparisons, while \Cref{sec:empirical_speed} provides a detailed analysis of the empirical results compared to commonly used acceleration methods.

\label{sec:exp_speed_claim}

\subsubsection{Neural Function Evaluations per Action (NFEs/a)}
\label{sec:nfe_a}

Adopting the settings from \cite{chi2023diffusion}, Diffusion Policy and related methods utilize a relatively large action horizon (\( T_a \)) to achieve better performance, whereas RNR-DP employs a single-action rollout (\( T_a = 1 \)) at each inference. To facilitate a fair comparison of efficiency, we introduce a new metric, \textbf{Neural Function Evaluations per Action (NFEs/a)}, as defined in \Cref{eq:nfe_per_action_definition}.

\begin{equation}
    \label{eq:nfe_per_action_definition}
    \mathrm{NFEs/a} = \frac{\mathrm{NFEs}}{T_a}
\end{equation}

\subsubsection{Empirical Comparison with Commonly Used Acceleration Methods}
\label{sec:empirical_speed}

To evaluate the efficiency of Responsive Noise-Relaying Diffusion Policy (RNR-DP), we compare it against common acceleration methods, including DDIM \citep{chi2023diffusion} and Consistency Policy \citep{prasad2024consistency}, on simpler tasks that do not require responsive control (Regular Group). As shown in \Cref{table:exp_speed_claim_state} and \Cref{sup:more_speed_claim_results}, RNR-DP consistently achieves the highest overall performance among all DDIM and Consistency Policy variants. For a fair comparison, we specifically evaluate RNR-DP against 8-step DDIM (NFEs/a = 1) and 8-step-chaining Consistency Policy (NFEs/a = 1). In state-based experiments, RNR-DP outperforms 8-step DDIM by 5.7\% and 8-step-chaining Consistency Policy by 66.2\%. In vision-based experiments, it surpasses 8-step DDIM by 8.5\% and 8-step-chaining Consistency Policy by 3.4\%, yielding an overall improvement of 6.9\% over 8-step DDIM and 32.0\% over 8-step-chaining Consistency Policy. Additionally, both DDIM and Consistency Policy struggle in certain tasks. For instance, 8-step DDIM achieves a $0\%$ success rate on Adroit Hammer, while Consistency Policy performs poorly on StackCube and TurnFaucet. In contrast, RNR-DP demonstrates consistent and robust performance across all tasks. Notably, RNR-DP achieves an average success rate comparable to Diffusion Policy across all state and visual experiments while being 12.5 times faster. These results confirm that even on simpler tasks, RNR-DP serves as a superior acceleration method, enabling efficient control.


We attribute this robust performance to the design of the noise-relaying buffer and sequential denoising mechanism. By reusing denoising steps from previous outputs, RNR-DP requires only one denoising step to generate a single action, while ensuring all actions undergo sufficient denoising to maintain high action quality. In contrast, DDIM and Consistency Policy significantly reduce the number of denoising steps per action, leading to pronounced performance drops.

\input{tables/exp_speed_claim_state}

\subsection{Ablation Study}
\label{sec:ablation}

We conduct various ablations to provide further insights on the effects of components of RNR-DP.

\textbf{Noise Scheduling Scheme}
We conduct a comprehensive study to evaluate how different noise scheduling schemes impact the performance of the Responsive Noise-Relaying Diffusion Policy. As shown in \Cref{table:ablate_noise_scheme}, the results demonstrate that relying solely on either a \emph{linear schedule} or a \emph{random schedule} significantly reduces task success rates. This highlights the importance of integrating \emph{mixture scheduling} to fully exploit the potential of the noise-relaying buffer design. By combining the two schedules, our model effectively utilizes different noise levels, enhancing the robustness and adaptability of action generation.

\input{tables/ablate_2_noise_scheme}


\textbf{Noise-Relaying Buffer Initialization}
We conduct an ablation study to evaluate the effectiveness of our initialization scheme. \emph{Pure noise} directly uses a fully noisy buffer as the initialization for subsequent operations. As shown in \Cref{table:ablate_buffer_init}, \emph{laddering initialization} achieves significant performance gains compared to \emph{pure noise}. This result highlights the critical role of our initialization scheme in enabling strong and robust performance.

\input{tables/ablate_1_buffer_init}

\begin{wrapfigure}{r}{0.34\textwidth}
    \centering
    \vspace{-\baselineskip}
    \includegraphics[width=0.32\textwidth]{figures/ablate_buffer_capacity.pdf}
    \vspace{-13pt}
    \caption{Noise-relaying buffer capacity v.s. average success rate.}
    \label{fig:ablate_buffer_capacity}
\end{wrapfigure}
\textbf{Noise-Relaying Buffer Capacity}
We also evaluate the impact of the noise-relaying buffer capacity to the performance, the only tuned hyperparameter in our approach. As shown in \Cref{fig:ablate_buffer_capacity}, on the Adroit Relocate task, RNR-DP achieves peak performance of \(58.5\%\) at a buffer capacity of 84 and maintains strong performance within the range of 56 to 92, demonstrating a wide tolerance. Performance declines when the buffer capacity is too small (e.g., 48) or too large (e.g., 100). Notably, Diffusion Policy achieves only \(42.2\%\), while RNR-DP with buffer capacities between 48 and 100 consistently outperforms it. This highlights that for a single task, the noise-relaying buffer offers robust performance over a broad range and is not difficult to tune. Empirically, starting with a value between 56 and 84 and making adjustments suffices most cases.


\vspace{1em}
\textbf{Model Prediction Type}
In addition, we investigate the impact of the model's prediction type. As shown in \Cref{table:ablate_prediction_type}, models predicting the added noise component outperform those directly predicting the action sequence. This finding aligns with the common practice of using noise prediction in diffusion models within the vision domain \citep{ho2020denoising}.

\input{tables/ablate_3_prediction_type}
