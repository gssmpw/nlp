\textbf{Diffusion Model Acceleration Techniques}  Diffusion models \citep{ho2020denoising} have garnered significant attention for their capacity to model complex distributions. However, their iterative sampling processes can be computationally expensive due to the large number of diffusion steps required. One approach to address the low inference speed of diffusion models is by reducing the number of denoising steps needed, as seen in works like \citep{song2020denoising} and \citep{karras2022elucidating}. Another line of research employs distillation-based techniques, which begin with a pretrained teacher model and train a new student model to take larger steps over the ODE trajectories that the teacher has already learned to map, as demonstrated in \citep{song2023consistency}, \citep{liu2023instaflow}, and \citep{salimans2022progressive}. Among the most commonly used acceleration techniques in the robotics community are DDIM \citep{song2020denoising} and Consistency Models \citep{song2023consistency}, which are particularly effective for speeding up the diffusion policy process.

\textbf{Diffusion Model for Motion Synthesis} Diffusion Model, renowned for its strong representational capabilities, has been widely applied to motion synthesis tasks \citep{shafir2023human}. Building on this foundation, \cite{zhang2024tedi} proposed an innovative framework that incorporates temporally varying denoising and maintains a motion buffer comprising progressively noised poses, enabling long-term motion synthesis. Inspired by TEDi, we develop a noise-relaying buffer with incrementally increasing noise levels and implement a sequential denoising mechanism to enhance Diffusion Policy, ensuring more efficient and responsive control.

\textbf{Diffusion Model as Policy} With the success of diffusion models in image synthesis and video generation \citep{ho2020denoising}, they have become a popular choice as policy backbones in the robotics community. These models are utilized in two main ways: 1) As policies in reinforcement learning (RL) methods, including offline RL \citep{wang2022diffusion,hansen2023idql,mao2024diffusion}, offline-to-online RL \citep{ding2023consistency}, and online RL \citep{yang2023policy}; 2) As policies in imitation learning \citep{chi2023diffusion,reuss2023goal}. Diffusion Policy belongs to the second category and has demonstrated state-of-the-art performance compared to other imitation learning methods \citep{shafiullah2022behavior,florence2022implicit,abramson1970aloha}. Furthermore, it exhibits significant potential for future research and practical applications \citep{yuan2024policy}.
