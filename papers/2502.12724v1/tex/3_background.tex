\textbf{Diffusion Policy} \citep{chi2023diffusion} models control policies using Denoising Diffusion Probabilistic Models (DDPMs) \citep{ho2020denoising}, which have shown strong performance in generative modeling. In control, Diffusion Policy predicts the future action sequence \(\At\) using a noise prediction network \(\varepsilon_\theta({\mathbf{A}}_t^{(k)}; \Ot, k)\), where \({\mathbf{A}}_t^{(k)} = {\mathbf{A}}_t + \epsilon^{k}\) is a perturbed version of the clean action sequence \({\mathbf{A}}_t\) with added Gaussian noise \(\epsilon\) at noise level \(k\). The model learns to estimate and remove noise by minimizing the mean squared error (MSE) loss:

\[
\mathcal{L} = \|\varepsilon_\theta({\mathbf{A}}_t^{(k)}; \Ob, k) - \varepsilon^k\|^2.
\]

During inference, given an observation \(\Ot\), the trained network iteratively refines the action sequence over \(K\) denoising steps following:

\[
{\mathbf{A}}_t^{(k-1)} = \alpha \left({\mathbf{A}}_t^{(k)} - \gamma \varepsilon_\theta({\mathbf{A}}_t^{(k)}; \Ot, k) + \epsilon \right),
\]

where the initial action sequence \({\Ab}_t\) is sampled from \(\mathcal{N}(0,1)\), and \(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\) represents Gaussian noise. The predefined noise schedule functions \(\alpha, \beta,\) and \(\sigma\) are part of the DDPM scheduler \citep{ho2020denoising}. Once denoised to \({\Ab}_t^{(0)}\), the agent executes the first \(n = T_a\) future steps after time \(t\).
