Imitation learning is a powerful approach for training robots to perform complex tasks, including grasping \citep{johns2021coarse, xie2020deep, stepputtis2020language}, legged locomotion \citep{ratliff2007imitation, al2023locomujoco, yang2023generalized}, dexterous manipulation \citep{qin2022dexmv, radosavovic2021state}, and mobile manipulation \citep{wong2022error, du2022bayesian}. Advances in computer vision and natural language processing have led to increasingly sophisticated imitation learning frameworks, achieving impressive success across diverse tasks \citep{chen2021decision, abramson1970aloha, florence2022implicit, shafiullah2022behavior}. A notable breakthrough, Diffusion Policy \citep{chi2023diffusion}, models robot action sequences through a conditional denoising diffusion process, setting new benchmarks over traditional imitation learning techniques. Consequently, Diffusion Policy has rapidly gained traction and is now widely adopted as a foundational framework in both research and real-world applications.

\begin{wrapfigure}{r}{0.35\textwidth}
    \centering
    \vspace{-\baselineskip}
    \includegraphics[width=0.35\textwidth]{figures/bars.pdf}
    \vspace{-20pt}
    \caption{Our RNR-DP consistently delivers responsive and efficient control.}
    \label{fig:bars}
\end{wrapfigure}

However, Diffusion Policy faces significant limitations. As shown in \cite{chi2023diffusion}, its performance heavily depends on having a relatively large action horizon, \( T_a \), which corresponds to executing multiple actions in the environment. The policy widely achieves optimal performance at \( T_a = 8 \) but suffers substantial degradation when \( T_a = 1 \), where only a single action is executed per inference. We attribute this to the nature of modeling multi-modal data: each inference independently samples actions aligned with a specific mode. Consequently, a larger action horizon is essential to ensure a sequence of actions adheres to the same mode, maintaining consistency. Conversely, using \( T_a = 1 \) leads to severe mode bouncing, causing significant performance drops. However, employing a large action horizon (e.g., \( T_a = 8 \)) also introduces drawbacks, as most actions are not conditioned on the latest observations, thereby reducing responsiveness and adaptability to environmental changes. Empirically, we observe that Diffusion Policy struggles particularly with tasks requiring responsive control, such as handling dynamic objects.

To address these issues, we propose the Responsive Noise-Relaying Diffusion Policy (RNR-DP). This approach fundamentally relies on a noise-relaying buffer that contains increasing noise levels and implements a sequential denoising mechanism. After each denoising step, conditioned on the most recent observations, the model executes an immediate noise-free action at the buffer's head and appends fully noisy actions at the buffer's tail. The noise-relaying buffer, combined with the sequential denoising mechanism, not only reuses denoising steps from previous outputs—allowing one denoising step to generate one action—but also ensures active control based on the most recent observations and prevents frequent mode bouncing, maintaining action consistency throughout the entire process. We name it \textbf{Responsive Noise-Relaying Diffusion Policy} because it employs a \textbf{noise-relaying buffer} at its core and has the ability to \textbf{respond quickly and actively to the environment}.

We evaluate our approach across a range of benchmarks, focusing on 9 tasks from three well-established datasets: ManiSkill2 \citep{gu2023maniskill2}, ManiSkill3 \citep{tao2024maniskill3}, and Adroit \citep{rajeswaran2017learning}. Our primary evaluation targets 5 tasks involving dynamic object manipulation that demand responsive control. Empirical results show that RNR-DP significantly outperforms Diffusion Policy, delivering much more responsive control. Additionally, we extend our evaluation to tasks that do not require responsive control. The results indicate that, even on these simpler tasks, RNR-DP functions as a superior acceleration method compared to popular alternatives such as DDIM \citep{song2020denoising} and Consistency Policy \citep{song2023consistency, prasad2024consistency}. Overall, our evaluations systematically demonstrate that RNR-DP provides both highly responsive and efficient control.

To summarize, our contributions are as follows:

\begin{itemize}
  \item We identify a key limitation of Diffusion Policy: its reliance on a relatively large action horizon \( T_a \), which compromises its responsiveness and adaptability to environmental changes.
  \item We propose Noise-Relaying Diffusion Policy (RNR-DP) which maintains a noise-relaying buffer with progressively increasing noise levels and employs a sequential denoising mechanism.
  \item We conduct extensive experiments on 5 tasks involving dynamic object manipulation and 4 simpler tasks that do not demand responsive control. The results consistently demonstrate that RNR-DP delivers both highly responsive and efficient control.
\end{itemize}
