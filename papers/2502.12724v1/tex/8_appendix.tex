\section{Further Details on the Experimental Setup}
\label{sup:more_exp_setup}

\subsection{Task Descriptions}
\label{sup:task_descriptions}
We consider a total of 9 continuous control tasks from 3 benchmarks: ManiSkill2 \citep{gu2023maniskill2}, ManiSkill3 \citep{tao2024maniskill3}, and Adroit \citep{rajeswaran2017learning}. This section provides detailed task descriptions on overall information, task difficulty, object sets, state space, and action space. Some task details are listed in \Cref{table:tasks}.

\begin{table}[!ht]
\caption{We consider 9 continuous tasks from 3 benchmarks. We list important task details below.}
\label{table:tasks}
\setlength{\tabcolsep}{3.5pt}
\begin{center}
{{
\begin{tabular}{lcccc}
\toprule[1pt]
\textbf{Task}
& \textbf{State Obs Dim $C_{\mathrm{state}}$}
& \textbf{Act Dim $C_a$}
& \textbf{Max Episode Step}
\\
\midrule
ManiSkill3: PushT      & 31 & 7  & 100 \\
ManiSkill3: RollBall   & 44 & 4  & 80 \\
ManiSkill2: StackCube  & 55    & 4  & 200 \\
ManiSkill2: TurnFaucet & 43    & 7  & 200 \\
ManiSkill2: PushChair  & 131   & 20 & 200 \\
Adroit: Door       & 39    & 28 & 300 \\
Adroit: Pen        & 46    & 24 & 200 \\
Adroit: Hammer     & 46    & 26 & 400 \\
Adroit: Relocate   & 39    & 30 & 400 \\
\bottomrule[1pt]
\end{tabular}
}}
\end{center}
\vspace{-12pt}
\end{table}

\subsubsection{ManiSkill2 Tasks}

\textbf{StackCube}
\begin{itemize}
    \item Overall Description: Pick up a red cube and place it onto a green one. See \Cref{fig:task_stackcube} for episode visualization.
    \item Task Difficulty: This task requires precise control. The gripper needs to firmly grasp the red cube and accurately place it onto the green one.
    \item Object Variations: No object variations
    \item Action Space: Delta position of the end-effector and joint positions of the gripper.
    \item State Observation Space: Proprioceptive robot state information, such as joint angles and velocities of the robot arm, and task-specific goal information
    \item Visual Observation Space: one 64x64 RGBD image from a base camera and one 64x64 RGBD image from a hand camera.
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_stackcube.jpg}
    \caption{StackCube Episode Visualization.}
    \label{fig:task_stackcube}
\end{figure}

\textbf{TurnFaucet}
\begin{itemize}
    \item Overall Description: Turn on a faucet by rotating its handle.
    \item Task Difficulty: This task needs to handle object variations. See \Cref{fig:task_turnfaucet} for episode visualization.
    \item Object Variations: We have a source environment containing 10 faucets, and the dataset is collected in the source environment. w/o g means the agent directly interacts with the source environment online; w/ g means the agent interacts with the target environment online, which contains 4 novel faucets.
    \item Action Space: Delta pose of the end-effector and joint positions of the gripper.
    \item State Observation Space: Proprioceptive robot state information, such as joint angles and velocities of the robot arm, the mobile base, and task-specific goal information.
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_turnfaucet.jpg}
    \caption{TurnFaucet Episode Visualization.}
    \label{fig:task_turnfaucet}
\end{figure}

\textbf{PushChair}
\begin{itemize}
    \item Overall Description: A dual-arm mobile robot needs to push a swivel chair to a target location on the ground (indicated by a red hemisphere) and prevent it from falling over. The friction and damping parameters for the chair joints are randomized. See \Cref{fig:task_pushchair} for episode visualization.
    \item Task Difficulty: This task needs to handle object variations.
    \item Object Variations: We have a source environment containing 5 chairs, and the dataset is collected in the source environment. w/o g means the agent directly interacts with the source environment online; w/ g means the agent interacts with the target environment online, which contains 3 novel faucets.
    \item State Observation Space: Proprioceptive robot state information, such as joint angles and velocities of the robot arm, task-specific goal information.
    \item Visual Observation Space: three 50x125 RGBD images from three cameras $120^\circ$ apart from each other mounted on the robot.
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_pushchair.jpg}
    \caption{PushChair Episode Visualization.}
    \label{fig:task_pushchair}
\end{figure}

\subsubsection{ManiSkill3 Tasks}

\textbf{PushT}
\begin{itemize}
    \item Overall Description: It is a simulated version of the real-world push-T task from Diffusion Policy: \href{https://diffusion-policy.cs.columbia.edu/}{\color{red}{https://diffusion-policy.cs.columbia.edu/}}. In this task, the robot needs to precisely push the T-shaped block into the target region, and move the end-effector to the end-zone which terminates the episodes. The success condition is that the T block covers 90\% of the 2D goal T's area. See \Cref{fig:task_pusht} for episode visualization.
    \item Task Difficulty: The task involves manipulating a dynamic T-shaped object, which introduces non-linear dynamics, friction, and contact forces.
    \item Object Variations: No object variations.
    \item Action Space: Delta pose of the end-effector and joint positions of the gripper.
    \item State Observation Space: Proprioceptive robot state information, such as joint angles and velocities of the robot arm, and task-specific goal information.
    \item Visual Observation Space:
    one 64x64 RGBD image from a base camera.
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_pusht.jpg}
    \caption{PushT Episode Visualization. The T-Block is pushed from sampled initial configuration to the goal area.}
    \label{fig:task_pusht}
\end{figure}

\textbf{RollBall}
\begin{itemize}
    \item Overall Description: A task where the objective is to push and roll a ball to a goal region at the other end of the table. The success condition is that The ball’s xy position is within goal radius (default 0.1) of the target’s xy position by euclidean distance. See \Cref{fig:task_rollball} for episode visualization.
    \item Task Difficulty: The task involves manipulating a dynamic ball, which introduces non-linear dynamics, friction, and contact forces.
    \item Object Variations: No object variations.
    \item Action Space: Delta position of the end-effector and joint positions of the gripper.
    \item State Observation Space: Proprioceptive robot state information, such as joint angles and velocities of the robot arm, and task-specific goal information.
    \item Visual Observation Space: one 64x64 RGBD image from a base camera.
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_rollball.jpg}
    \caption{Rollball Episode Visualization. The blue ball is pushed and rolled from sampled initial configuration to the target red circle.}
    \label{fig:task_rollball}
\end{figure}

\subsubsection{Adroit Tasks}

\textbf{Adroit Door}
\begin{itemize}
    \item Overall Description: The environment is based on the Adroit manipulation platform, a 28 degree of freedom system which consists of a 24 degrees of freedom ShadowHand and a 4 degree of freedom arm. The task to be completed consists on undoing the latch and swing the door open. See \Cref{fig:task_door} for episode visualization.
    \item Task Difficulty: The latch has significant dry friction and a biass torque that forces the door to stay closed. No information about the latcch is explicitly provided. The position of the door is randomized.
    \item Object Variations: No object variations.
    \item Action Space: Absolute angular positions of the Adoit hand joints.
    \item State Observation Space: The angular position of the finger joints, the pose of the palm of the hand, as well as state of the latch and door.
    \item Visual Observation Space: one 128x128 RGB image from a third-person view camera.
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_door.jpg}
    \caption{Door Episode Visualization.}
    \label{fig:task_door}
\end{figure}

\textbf{Adroit Pen}
\begin{itemize}
    \item Overall Description: The environment is based on the Adroit manipulation platform, a 28 degree of freedom system which consists of a 24 degrees of freedom ShadowHand and a 4 degree of freedom arm. The task to be completed consists on repositioning the blue pen to match the orientation of the green target. See \Cref{fig:task_pen} for episode visualization.
    \item Task Difficulty: The target is also randomized to cover all configurations.
    \item Object Variations: No object variations.
    \item Action Space: Absolute angular positions of the Adroit hand joints.
    \item State Observation Space: The angular position of the finger joints, the pose of the palm of the hand, as well as the pose of the real pen and target goal.
    \item Visual Observation Space: one 128x128 RGB image from a third-person view camera. 
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_pen.jpg}
    \caption{Pen Episode Visualization.}
    \label{fig:task_pen}
\end{figure}

\textbf{Adroit Hammer}
\begin{itemize}
    \item Overall Description: The environment is based on the Adroit manipulation platform, a 28 degree of freedom ShadowHand and a 4 degree of freedom arm. The task to be completed consists on picking up a hammer with and drive a nail into a board. See \Cref{fig:task_hammer} for episode visualization.
    \item Task Difficulty: The nail position is randomized and has dry friction capable of absorbing up to 15N force.
    \item Object Variations: No object variation.
    \item Action Space: Absolute angular positions of the Adroit hand joints.
    \item State Observation Space: The angular position of the finger joints, the pose of the palm of the hand, the pose of the hammer and nail, and external forces on the nail.
    \item Visual Observation Space: one 128x128 RGB image from a third-person view camera. 
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_hammer.jpg}
    \caption{Hammer Episode Visualization.}
    \label{fig:task_hammer}
\end{figure}

\textbf{Adroit Relocate}
\begin{itemize}
    \item Overall Description: The environment is based on the Adroit manipulation platform, a 30 degree of freedom system which consists of a 24 degrees of freedom ShadowHand and a 6 degree of freedom arm. The task to be completed consists on moving the blue ball to the green target. See \Cref{fig:task_relocate} for episode visualization.
    \item Task Difficulty: The positions of the ball and target are randomized over the entire workspace.
    \item Object Variations: No object variations.
    \item Action Space: Absolute angular positions of the Adroit hand joints.
    \item State Observation Space: The angular position of the finger joints, the pose of the palm of the hand, as well as kinematic information about the ball and target.
    \item Visual Observation Space: one 128x128 RGB image from a third-person view camera.
\end{itemize}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/task_relocate.jpg}
    \caption{Relocate Episode Visualization.}
    \label{fig:task_relocate}
\end{figure}

\subsection{Demonstrations}
This subsection provides the details of demonstrations used in our experiments. See \Cref{table:demonstrations}. ManiSkill2 and ManiSkill3 demonstrations are provided in \cite{gu2023maniskill2} and \cite{tao2024maniskill3}, and Adroit demonstrations are provided in \cite{rajeswaran2017learning}.

\begin{table}[!ht]
\caption{Demonstration sources, numbers and generation methods.}
\label{table:demonstrations}
\setlength{\tabcolsep}{3.5pt}
\begin{center}
{{
\begin{tabular}{lccc}
\toprule[1pt]
\textbf{Task}
& \textbf{Traj Num for Training}
& \textbf{Generation Method}
\\
\midrule
ManiSkill3: PushT      & 1000 & Reinforcement Learning \\
ManiSkill3: RollBall   & 1000 & Reinforcement Learning \\
ManiSkill2: StackCube  & 1000 & Task \& Motion Planning \\
ManiSkill2: TurnFaucet & 1000 & Model Predictive Control \\
ManiSkill2: PushChair  & 1000 & Reinforcement Learning \\
Adroit: Door       & 25   & Human Demonstration \\
Adroit: Pen        & 25   & Human Demonstration \\
Adroit: Hammer     & 25   & Human Demonstration \\
Adroit: Relocate   & 25   & Human Demonstration \\
\bottomrule[1pt]
\end{tabular}
}}
\end{center}
\vspace{-12pt}
\end{table}

\section{Implementation Details}
\label{sup:implementation_details}
\subsection{Noise-Relaying Diffusion Policy Inference}
\label{sup:algo}
We summarize the inference pseudo-code of our \ours in \Cref{alg:inference}.
\input{algos/alg_inference}
\subsection{Noise-Relaying Diffusion Policy Training}
We summarize the training pseudo-code of our \ours in \Cref{alg:training}.
\input{algos/alg_training}

\subsection{Policy Architecture}
\label{sup:policy_architecture}
We build our \ours on top of the UNet-based architecture of Diffusion Policy \citep{chi2023diffusion}. The model includes 2 downsampling modules and 2 upsampling modules with each module containing 2 residual blocks. The residual block consists of 1D temporal convolutions (Conv1d), group normalizations (GN), and Mish activation layers. The encoded noise-aware conditioning data (\Cref{sec:key_design_choices}) is fused into each residual block through the FiLM transformation \citep{perez2018film}. The raw conditioning data is of shape $R^{f \times (C_{\mathrm{emb}} + C_{\mathrm{state}})}$ for state policies and of shape $R^{f \times (C_{\mathrm{emb}} + C_{\mathrm{visual}} + C_{\mathrm{state}})}$ for visual policies. See \Cref{fig:policy_architecture} for the visualization of a visual policy. We follow the UNet denoiser design for the observation that transformer-based policies are more sensitive to hyperparameters and often require more tuning \citep{chi2023diffusion}. The choice of policy architecture is orthogonal to our method and we believe our design would also improve this policy class.

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.85\linewidth]{figures/policy_architecture.pdf}
    \caption{The detailed policy architecture for our \ours. We only extract visual features for visual policies.
    }  
    \label{fig:policy_architecture}
\end{figure}

\subsection{Important Hyperparameters}

\subsubsection{Key Hyperparameters of RNR-DP}
We summarize the key hyperparameters of RNR-DP in \Cref{table:rnr_dp_training_hyperparameters}.
The observation horizon $T_o$ and noise-relaying buffer capacity $f$ for each task is listed in \Cref{table:rnr_dp_buffer_capacity}.
The number of trainable parameters for each task is listed in \Cref{table:rnr_dp_trainable_parameters}.


\begin{table}[!ht]
\caption{We list the key hyperparameters of RNR-DP used in our experiments.}
\label{table:rnr_dp_training_hyperparameters}
\setlength{\tabcolsep}{3.5pt}
\begin{center}
{{
\begin{tabular}{lc}
\toprule[1pt]
\textbf{Hyperparameter}
& \textbf{Value}
\\
\midrule
\ours Noise Scheduling Scheme & Mixture Sampling($p_{\mathrm{linear}}$) $p_{\mathrm{linear}}=0.4$ \\
\ours Model Prediction Type & Noise \\
Diffusion Step Embedding Dimension & 64 \\
UNet Downsampling Dimensions & [64, 128, 256] \\
Optimizer & AdamW \\
Weight Decay & 1e-6 \\
Learning Rate & 1e-4 \\ 
Learning Rate Scheduler & Cosine \\
EMA Model Update & 0.9999 \\
Online Evaluation Episodes & 1000 \\

\bottomrule[1pt]
\end{tabular}
}}
\end{center}
\vspace{-12pt}
\end{table}

\begin{table}[!ht]
\caption{The observation horizon and noise-relaying buffer capacity of our \ours for each task.}
\label{table:rnr_dp_buffer_capacity}
\setlength{\tabcolsep}{3.5pt}
\begin{center}
{{
\begin{tabular}{l c c}
\toprule[1pt]
\textbf{Task}
& \textbf{Obs $T_o$}
& \textbf{Capacity $f$}
\\
\midrule
ManiSkill3: PushT (Visual) & 2 & 48 \\
ManiSkill3: RollBall (Visual) & 2 & 64 \\
ManiSkill2: StackCube (Visual) & 2 & 84 \\
Adroit: Pen (Visual) & 2 & 4 \\
Adroit: Hammer (Visual) & 2 & 64 \\
Adroit: Door (Visual) & 2 & 56 \\
& \\
ManiSkill3: PushT (State) & 2 & 32 \\
ManiSkill3: RollBall (State) & 2 & 4  \\
ManiSkill2: StackCube (State) & 2 & 84 \\
ManiSkill2: TurnFaucet w/g (State) & 2 & 64 \\
ManiSkill2: TurnFaucet w/o g (State) & 2 & 72 \\
ManiSkill2: PushChair w/g (State) & 2 & 56\\
ManiSkill2: PushChair w/o g (State) & 2 & 48\\
Adroit: Door (State) & 2 & 74 \\
Adroit: Pen (State) & 2 & 4 \\
Adroit: Hammer (State) & 2 & 32 \\
Adroit: Relocate (State) & 2 & 84 \\
\bottomrule[1pt]
\end{tabular}
}}
\end{center}
\vspace{-12pt}
\end{table}

\begin{table}[!ht]
\caption{The number of our \ours trainable parameters for each task. Noise-relaying buffer size doesn't affect the number of trainable parameters for each task.}
\label{table:rnr_dp_trainable_parameters}
\setlength{\tabcolsep}{3.5pt}
\begin{center}
{{
\begin{tabular}{lc}
\toprule[1pt]
\textbf{Task}
& \textbf{Trainable Params}
\\
\midrule
ManiSkill3: PushT (Visual) & 11.42M \\
ManiSkill3: RollBall (Visual) & 11.59M \\
ManiSkill2: StackCube (Visual) & 10.85M \\
Adroit: Pen (Visual) & 14.67M \\
Adroit: Hammer (Visual) & 14.72M \\
Adroit: Door (Visual) & 14.41M \\
& \\
ManiSkill3: PushT (State) & 4.53M \\
ManiSkill3: RollBall (State) & 4.73M \\
ManiSkill2: StackCube (State) & 4.91M \\
ManiSkill2: TurnFaucet (State) & 4.71M \\
Adroit: Door (State) & 4.66M \\
Adroit: Pen (State) & 4.75M \\
Adroit: Hammer (State) & 4.77M \\
Adroit: Relocate (State) & 4.66M \\
\bottomrule[1pt]
\end{tabular}
}}
\end{center}
\vspace{-12pt}
\end{table}


\subsubsection{Key Hyperparameters of Diffusion Policy}
We summarize the key hyperparameters of Diffusion Policy in \Cref{table:dp_training_hyperparameters}.
The observation horizon $T_o$, action executation horizon $T_a$ and action prediction horizon $T_p$ for each task are listed in \Cref{table:dp_To_Ta_Tp}.

\begin{table}[!ht]
\caption{We list the key hyperparameters of Diffusion Policy baseline used in our experiments.}
\label{table:dp_training_hyperparameters}
\setlength{\tabcolsep}{3.5pt}
\begin{center}
{{
\begin{tabular}{lc}
\toprule[1pt]
\textbf{Hyperparameter}
& \textbf{Value}
\\
\midrule
Diffusion Step Embedding Dimension & 64 \\
UNet Downsampling Dimensions & [64, 128, 256] \\
Optimizer & AdamW \\
Weight Decay & 1e-6 \\
Learning Rate & 1e-4 \\ 
Learning Rate Scheduler & Cosine \\
EMA Model Update & 0.9999 \\
Online Evaluation Episodes & 1000 \\

\bottomrule[1pt]
\end{tabular}
}}
\end{center}
\vspace{-12pt}
\end{table}

\begin{table}[!ht]
\caption{We list the observation horizon, action executation horizon and action prediction horizon of Diffusion Policy baseline for each task.}
\label{table:dp_To_Ta_Tp}
\setlength{\tabcolsep}{3.5pt}
\begin{center}
{{
\begin{tabular}{l c c c}
\toprule[1pt]
\textbf{Task}
& \textbf{Obs $T_o$}
& \textbf{Act Exec $T_a$}
& \textbf{Act Pred $T_p$}
\\
\midrule
ManiSkill3: PushT (Visual) & 2 & 2 & 16 \\
ManiSkill3: RollBall (Visual) & 2 & 4 & 16 \\
ManiSkill2: StackCube (Visual) & 2 & 8 & 16 \\
Adroit: Pen (Visual) & 2 & 8 & 16 \\
Adroit: Hammer (Visual) & 2 & 8 & 16 \\
Adroit: Door (Visual) & 2 & 8 & 16 \\
& \\
ManiSkill3: PushT (State) & 2 & 1 & 16 \\
ManiSkill3: RollBall (State) & 2 & 4 & 16 \\
ManiSkill2: StackCube (State) & 2 & 8 & 16 \\
ManiSkill2: TurnFaucet (State) & 2 & 8 & 16 \\
Adroit: Door (State) & 2 & 8 & 16 \\
Adroit: Pen (State) & 2 & 8 & 16 \\
Adroit: Hammer (State) & 2 & 8 & 16 \\
Adroit: Relocate (State) & 2 & 8 & 16 \\
\bottomrule[1pt]
\end{tabular}
}}
\end{center}
\vspace{-12pt}
\end{table}

\subsection{Training Details}
We train our models and baselines with cluster assigned GPUs (NVIDIA 2080Ti \& A10). We use AdamW optimizer with an initial learning rate of 1e-4, applying 500 warmup steps followed by cosine decay. We use batch size of 1024 for state policies and 256 for visual policies for both ManiSkill and Adroit benchmarks. We evaluate DP, CP and \ours model checkpoints using EMA weights every 10K training iterations for ManiSkill tasks and every 5K for Adroit tasks. DDIMs are evaluated using the best checkpoints of DDPMs in an offline manner. CPs are trained using the best checkpoints of EDMs.

\section{Additional Results}
\label{sup:additional_results}

\subsection{Empirical Comparison with Acceleration Methods on Visual Observations}
\label{sup:more_speed_claim_results}
We summarize the results of vision-based experiments in \Cref{table:exp_speed_claim_visual}. As shown in \Cref{table:exp_speed_claim_visual}, our \ours ourperforms all DDIM variations and CP variations and particularly has an overall improvement over 8-step DDIM by $6.9\%$, over 8-step-chaining CP by $3.4\%$.
\input{tables/exp_speed_claim_visual}

\subsection{Comparsion with Streaming Diffusion Policy (SDP)}
\label{sup:sdp}

Streaming Diffusion Policy (SDP) \citep{høeg2024streamingdiffusionpolicyfast} is a recent advancement over Diffusion Policy that stays close to our approach. In this section, we compare our method with SDP in terms of motivation (\Cref{sup:sdp_motivation}), methodology (\Cref{sup:sdp_method}), and empirical results (\Cref{sup:sdp_empirical}).

\subsubsection{Motivation Comparison}
\label{sup:sdp_motivation}

SDP accelerates Diffusion Policy inference by reducing the number of denoising steps required to generate an action sequence. While improving diffusion inference speed is a relevant research topic, its impact in robotics is less compelling, as DDIM and Consistency Policy already provide reasonable speedups with strong performance. In contrast, our method addresses a fundamental limitation of Diffusion Policy—its lack of responsiveness—which significantly hinders performance in rapidly changing environments (e.g., contact-rich dynamic object manipulation). This challenge is far more critical to advancing robotic control. Although our approach also serves as an effective acceleration method, we view this as a secondary benefit compared to its primary advantage of enabling more responsive control.

\subsubsection{Method Comparison}
\label{sup:sdp_method}

\textbf{Rollout Method} SDP also employs an action buffer structure but partitions the prediction horizon $T_p$ into multiple action chunks, ensuring that (1) each chunk maintains the same noise level and (2) noise levels increase across chunks. This chunk-wise design focuses solely on reducing denoising steps and accelerating inference. However, it does not address responsiveness and thus retains the limitations of Diffusion Policy. In contrast, our sequential denoising scheme conditions all actions on the latest observations, enabling responsive control while leveraging the noise-relaying buffer to maintain efficiency.

\textbf{Policy Architecture}
SDP fuses all time embeddings along the temporal dimension into a single embedding. In contrast, our architecture retains multiple time embeddings, ensuring that noisy actions within the noise-relaying buffer can perceive time step changes based on the latest observation features. This design preserves temporal dynamics, allowing each action to adapt to varying time steps, thereby improving responsiveness and consistency in action generation.

\subsubsection{Empirical Results Comparison}
\label{sup:sdp_empirical}

To demonstrate that SDP inherits the same limitations as Diffusion Policy and lacks responsive control, we conduct experiments on the Adroit Relocate task (see \Cref{table:compare_sdp}). As the empirical results indicate, Streaming Diffusion Policy performs similarly to Diffusion Policy on the Adroit Relocate task, whereas our method achieves significantly more responsive control than both.


\begin{table}[!ht]
\caption{
    We compare Streaming Diffusion Policy with Diffusion Policy and our method on Adroit Relocate task.
}
\label{table:compare_sdp}
\setlength{\tabcolsep}{3.5pt}
\begin{center}
    {
        {
\begin{tabular}{l c c c}
\toprule[1pt]
& \textbf{DP}
& \textbf{SDP}
& \textbf{RNR-DP}
\\
\textbf{Task}
&
&
&
\\
\midrule
Relocate (Adroit)
& 0.422
& 0.436
& \cellcolor{oursBlue}{\textbf{0.585}}
\\
\bottomrule[1pt]
\end{tabular}
        }
    }
\end{center}
\vspace{-12pt}
\end{table}
