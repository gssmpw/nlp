This section explores the key limitations of Diffusion Policy in detail. In summary, Diffusion Policy relies on a relatively large action horizon, \( T_a \), to ensure a sequence of actions adheres to the same mode and avoid frequent mode bouncing. However, using a large action horizon results in most actions being unconditioned on the latest observations, thereby limiting the policy's responsiveness to environmental changes. Section \ref{sec:why_large_action_horizon} delves into why Diffusion Policy requires a large action horizon, while Section \ref{sec:how_limit_responsiveness} examines how this impacts responsiveness.

\subsection{Why Diffusion Policy Needs A Large Action Horizon?}
\label{sec:why_large_action_horizon}

As shown in \cite{chi2023diffusion}, Diffusion Policy performs best with a relatively large action horizon $T_a$ (e.g., $T_a = 8$), while $T_a = 1$ significantly degrades performance. This is because each action sequence is independently denoised from noise, and in multi-modal settings, $T_a = 1$ allows actions to switch modes, causing inconsistencies. Executing multiple actions within the same mode is crucial for stable performance. To validate this, we train Diffusion Policy on 500 single-modal demonstrations from an RL agent in the ManiSkill2 StackCube task. Results in \Cref{table:exp_dp_demo_types} show no performance drop with $T_a = 1$, even slightly improving over $T_a = 8$, confirming that mode bouncing in multi-modal settings is the main reason for requiring a large action horizon.

\subsection{How A Large Action Horizon Limits Responsiveness?}
\label{sec:how_limit_responsiveness}

As discussed above, a large action horizon \( T_a \) is crucial for Diffusion Policy, particularly when modeling multi-modal data. However, an excessively large \( T_a \) can hinder responsiveness to rapid environmental changes. Consider a dexterous robotic hand tasked with picking up a ball from a surface and transporting it to a goal position. This task demands continuous fine-grained control and rapid adaptation to unforeseen disturbances, such as the ball slipping or shifting unpredictably within the fingers. If the policy commits to executing \( T_a = 8 \) future actions, it may struggle to react promptly to subtle variations in grip force or contact dynamics. For example, if the ball begins to slip, a long action sequence could delay corrective actions, making recovery difficult. In contrast, a short action horizon (\( T_a = 1 \)) allows the policy to continuously refine its grip based on real-time feedback, ensuring stable and controlled relocation. This example highlights that in contact-rich object manipulation tasks, a large action horizon forces the policy to predict complex interactions prematurely, making precise control more challenging. Empirically, we observe that Diffusion Policy struggles with such dynamic, contact-rich manipulation tasks, where real-time adaptability is essential, further underscoring the dilemma between long-horizon consistency and responsiveness.

\input{tables/exp_dp_demo_types}
