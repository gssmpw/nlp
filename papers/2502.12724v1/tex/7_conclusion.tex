In this paper, we identify the key limitation of Diffusion Policy: its reliance on a relatively large action horizon $T_a$ compromises its responsiveness and adaptability to environment changes. To address these issues, we propose Responsive Noise-Relaying Diffusion Policy which maintains a noise-relaying buffer with progressively increasing noise levels and employs a sequential denoising mechanism. Our method provides more responsive control than Diffusion Policy on 5 tasks involving dynamic object manipulation, and delivers more efficient control than commonly used acceleration methods on 4 simpler tasks.

\textbf{Limitations}. A limitation of this work is the lack of additional experiments to demonstrate whether our method preserves behavioral diversity compared to Diffusion Policy. This remains an area for future research.
