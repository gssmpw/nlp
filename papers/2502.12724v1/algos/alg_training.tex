\begin{algorithm}
\setstretch{1.35}
\caption{\ourslong Training}
\label{alg:training}
\begin{algorithmic}[1]
\State
\textbf{Require}: demonstration dataset, $\mathcal{D} = \{(\textbf{O}_i, \textbf{A}_i)\}_{i=1}^N$; denoising model, $\varepsilon_\theta$; number of diffusion steps, $f$
\Repeat
    \State Sample $(\textbf{O}, \textbf{A}) \sim \mathcal{D}$
    \State Sample $p \sim \mathrm{Unif}(0, 1)$; Sample noise $\bm{\epsilon} \sim \mathcal{N}(0, \mathbf{I})$ and reshape to $\mathbb{R}^{C_a \times f}$
    \If{$p \leq p_{\mathrm{linear}}$}
        \State $\mathbf{k} = \{k_1=1, \cdots, k_f=f\}$
            \Comment{linear schedule}
    \Else
        \State $\mathbf{k} = \{k_1 \sim \mathrm{Unif}(\{1, \cdots, f\}), \cdots, k_f \sim \mathrm{Unif}(\{1, \cdots, f\})\}$
            \Comment{random schedule}
    \EndIf
    \ForAll {$\mathbf{a}_j \in \mathbf{A}$ indexed by frame index $j$}
        \State $\hat{\mathbf{a}}_j = \sqrt{\bar{\alpha}_{t_j}} \mathbf{a}_j + \sqrt{1 - \bar{\alpha}_{t_j}} \bm{\epsilon}_j$
            \Comment{perturbe each $\mathbf{a}_j$ independently}
    \EndFor
    \State $\hat{\mathbf{A}} = \{\hat{\mathbf{a}}_0, \cdots, \hat{\mathbf{a}}_{f-1}\}$
    \State Take gradient descent step to update $\theta$ on
    \State     $\quad \ \ \nabla_\theta \| \bm{\epsilon} - \varepsilon_{\theta}(\hat{\mathbf{A}}; \color{darkblue}{\mathbf{O}, \mathbf{k}}\color{black}{) \|}$
        \Comment{\textcolor{darkblue}{noise-aware conditioning}}
\Until{converged}
\end{algorithmic}
\end{algorithm}
