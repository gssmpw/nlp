\begin{algorithm}
    \setstretch{1.35}
    \caption{Noise-relaying Diffusion Policy Inference} \label{alg:inference}
    \begin{algorithmic}[1]
    \State
    \textbf{Require}:
    denoising model, $\varepsilon_\theta$;
    observation, $\mathbf{O}_t$;
    noise-relaying buffer, $\mathbf{\tilde{Q}}_t$;
    buffer capacity $f$;
    \vspace{1.35mm}
    \While{task execution}
        \State \makebox[0.6cm][l]{$\mathbf{Q}_t$} $\gets \varepsilon_\theta(\mathbf{\tilde{Q}}_t; \mathbf{O}_t, \{1, \cdots, f\})$
            \Comment{$\varepsilon_\theta$ is trained using $f$ noise levels} \label{line:decode}

        \State \makebox[0.6cm][l]{$\mathbf{a}_{t}^{(0)}$} $\gets \mathbf{Q}_t \mathrm{.pop}(0)$
            \Comment{$\mathbf{a}_{t}^{(0)}$ is a clean action (fully denoised)}

        \State \makebox[0.6cm][l]{$\mathbf{\tilde{Q}}_t$} $\gets \mathbf{Q}_t \mathrm{.push}(\mathbf{z})$
            \Comment{$\mathbf{z}$ is a random noisy action sampled from $\mathcal{N}(\bm{0}, \mathbf{I})$}

        \State \makebox[0.6cm][l]{$\mathbf{O}_t$} $\gets \mathrm{env.step}(\mathbf{a}_{t}^{(0)})$
            \Comment{executes $\mathbf{a}_{t}^{(0)}$ and envrionment updates observation}

        \State \makebox[0.6cm][l]{$\hphantom{1}t$} $\gets t+1$
            \Comment{update timestep for the next iteration}
    \EndWhile
    \end{algorithmic}
\end{algorithm}
