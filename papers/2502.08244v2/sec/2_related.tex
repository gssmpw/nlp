\section{Related Work}
\label{sec:related}

% \subsection{Camera-Controllable Video Synthesis}
\paragraph{Camera-controllable video synthesis.}
Following the tremendous success of video diffusion models, numerous efforts have been made to integrate camera controllability into the video generation process.
MovieGen~\cite{polyak2024movie} uses text descriptions that describe camera motions to control camera motion.
MCDiff~\cite{chen2023motion}, DragNUWA~\cite{yin2023dragnuwa}, and MotionI2V~\cite{shi2024motion} enable camera control through user-provided strokes, manipulating background motion to adjust camera movement.
AnimateDiff~\cite{guo2023animatediff} and Direct-a-video~\cite{yang2024direct} enable camera control by training models on augmented video datasets that contain simple camera movements, such as translation.
While these methods offer basic camera control with high-level instructions such as zoom and pan, they lack the detailed control capability for user-defined specific camera motions. 


Recent methods have demonstrated detailed control of camera movement by using desired camera parameters as conditional input.
To this end, these approaches train models on datasets that provide ground-truth camera parameters for every video frame.
MotionCtrl~\cite{wang2024motionctrl} directly projects the camera extrinsic parameters onto the intermediate features of a diffusion model, while CameraCtrl~\cite{he2024cameractrl} leverages the Pl\"{u}cker embedding scheme~\cite{sitzmann2021light} to encode the camera origin and ray directions as conditioning input.
CamCo~\cite{xu2024camco} and CamI2V~\cite{zheng2024cami2v} enhance camera control through an epipolar attention mechanism across video frames.
VD3D~\cite{bahmani2024vd3d} enables camera motion control within the video synthesis process of transformer-based video diffusion models.

While these methods support detailed camera control, they are primarily trained on restricted datasets~\cite{zhou2018stereo} due to the requirement for camera parameters during training.
This limitation degrades the generalization capability and leads to unnatural object motion synthesis.
In contrast, our model can be trained on arbitrary videos by leveraging optical flow maps as input, which can be robustly estimated using recent optical flow estimation models~\cite{teed2020raft}.


\begin{figure*}[t!]
\centering
\includegraphics[width=0.97\linewidth]{fig/framework_overview.pdf}
\vspace{-3mm}
\caption{
Overview of \MethodName{}.
Given an image and camera parameters, our framework synthesizes video frames following the input camera trajectory.
To this end, we synthesize two sets of optical flow maps that represent camera and object motions. Then, two optical flow maps are integrated and fed into the flow-conditioned video synthesis model, enabling camera-controllable video generation.
}
\vspace{-3mm}
\label{fig:framework_overview}
\end{figure*}


\paragraph{Flow-based two-stage video synthesis.}
Recently, several studies have introduced optical-flow-based two-stage pipelines for video synthesis~\cite{li2024generative, holynski2021animating, endo2019animating, mahapatra2022controllable, zhao2022thin, ni2023conditional, liang2024movideo}.
These approaches, similar to ours, utilize two distinct models: one to generate optical flow maps and another to produce video frames based on the generated optical flow.
However, these approaches aim to improve video synthesis quality in terms of temporal coherence, and do not address camera-controlled video synthesis.
Furthermore, they do not distinguish between camera and object motions, thus extending them to incorporate camera control is not straightforward.
