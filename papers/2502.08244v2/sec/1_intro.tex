\vspace{-4.5mm}

\section{Introduction}
\label{sec:intro}





Video diffusion models have made significant strides in generating high-quality videos by leveraging large-scale datasets \cite{ho2022video,blattmann2023align,he2022latent,singer2022make,yang2024cogvideox,videoworldsimulators2024,blattmann2023stable,wang2024microcinema,polyak2024movie}. However, they often lack the ability to incorporate user-defined controls, particularly in terms of camera movement and perspective. This limitation restricts the practical applications of video diffusion models, where precise control over camera parameters is crucial for various tasks such as film production, virtual reality, and interactive simulations.

Recently, several approaches have introduced camera controllability to video diffusion models. One line of methods uses either text descriptions or user-drawn strokes that describe background motion as conditional inputs to represent camera motion~\cite{chen2023motion, yin2023dragnuwa, shi2024motion, wang2024motionctrl, polyak2024movie}. However, these methods support only limited camera controllability, such as zoom and pan, during video generation.





More sophisticated camera control has been achieved by directly using camera parameters as inputs~\cite{wang2024motionctrl, he2024cameractrl, xu2024camco, zheng2024cami2v, bahmani2024vd3d, yang2024direct, kuang2024collaborative,zhang2024recapture,xu2024cavia,wang2024humanvid}. 
In particular, recent methods embed input camera parameters using the Pl\"ucker embedding scheme~\cite{sitzmann2021light}, which involves embedding ray origins and directions, and feed them into video diffusion models~\cite{he2024cameractrl, xu2024camco, zheng2024cami2v, bahmani2024vd3d}.
While these approaches offer more detailed control, they require a training dataset that includes ground-truth camera parameters for every video frame. Acquiring such datasets is challenging, leading to the use of restricted datasets that primarily consist of static scenes, such as RealEstate10K~\cite{zhou2018stereo}. 
Consequently, they suffer from limited generalization capability, producing videos with unnatural object motions and inaccurate camera control (\cref{fig:teaser}).


To enable natural object motion synthesis and accurate camera control,
our key idea is to use \emph{optical flow} as conditional input to a video diffusion model.
This approach provides two key benefits.
First, since optical flow can be directly estimated from videos, our method eliminates the need for using datasets with ground-truth camera parameters.
This flexibility enables the utilization of arbitrary training videos.
Second, since background optical flow encodes 3D correlations across different viewpoints, our method enables detailed camera control by leveraging the background motion.
As a result, our approach facilitates natural object motion synthesis and precise camera control (\cref{fig:teaser}).


Based on the key idea, this paper presents \emph{\MethodName{}}, a novel camera-controllable video generation framework that leverages optical flow.
Given a single image and camera parameters, \MethodName{} synthesizes future frames with a desired camera trajectory.
To this end, our framework employs a two-stage video synthesis pipeline: optical flow generation and flow-conditioned video synthesis.
We generate optical flow maps representing the motions of the camera and moving objects from the input image and camera parameters.
These flow maps are then fed into a flow-conditioned video synthesis model to generate the final output video.


To synthesize natural object motion while supporting detailed camera control, \MethodName{} divides the optical flow generation stage into two sub-problems: camera flow generation and object flow generation.
First, we convert input camera parameters into optical flow of the background motion by using 3D structure estimated from the input image.
Next, an object motion synthesis model is introduced to generate the optical flow for object motions based on the input image.
We obtain the final optical flow maps by combining the background and object motion flows.

% By combining both the background and object motion flows, we obtain the final optical flow maps.


Our contributions are summarized as follows:
% \vspace{-1mm}
\begin{itemize}
    \item We present a novel camera-controllable video generation framework that leverages optical flow, allowing our method to utilize arbitrary training videos without ground-truth camera parameters.
    \item To achieve detailed camera control and high-quality video synthesis, we adopt a two-stage video synthesis pipeline, flow generation and flow-conditioned video synthesis.
    \item Extensive evaluation demonstrates the effectiveness of our method, showcasing its ability to produce high-quality videos with 
    accurate camera control and natural object motion.
\end{itemize}