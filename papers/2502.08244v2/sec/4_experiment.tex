







\section{Experiments}
\label{sec:experiment}

\subsection{Implementation Details}
\label{sec:implementation_detail}
\MethodName{} synthesizes 14 video frames at once, following Stable Video Diffusion~\cite{blattmann2023stable}.
We use a resolution of $320 \times 576$ for both video frames and optical flow maps.
FVSM is trained for 50K iterations with 16 video clips and their optical flow maps per training batch.
OMSM is trained on the full dataset for 100K iterations and then fine-tuned using the curated dataset for 50K iterations, with 8 optical flow maps per training batch.
Inspired by T2I-Adapter~\cite{mou2024t2i}, we use a quadratic timestep sampling strategy (QTS) in training FVSM for better camera controllability~(Tab.~S2 in the supplemental document).
For stable training and inference of \MethodName{}, we adaptively normalize optical flow maps based on statistics computed from the training dataset, following Li et al.~\cite{li2024generative}.
Refer to the supplemental document for more implementation details.




\begin{figure*}[!t]
\centering
\begin{tabular}{@{}c}
\includegraphics[width=0.95\linewidth]{fig/qual_cam_control.pdf} \\
\end{tabular}
\vspace{-4mm}
\caption{
Qualitative comparison of camera control using the RealEstate10K test dataset~\cite{zhou2018stereo}.
MotionCtrl~\cite{wang2024motionctrl} often fails to follow the input camera parameters.
Notably, our method shows accurate camera control results despite not using camera parameters in training.
}
\label{fig:qual_cam_control}
\vspace{-4.5mm}
\end{figure*}


\subsection{Evaluation Protocol}
\label{sec:evaluation_protocol}


%\vspace{-2mm}
\paragraph{Camera controllability.}
We evaluate the camera controllability following previous methods~\cite{he2024cameractrl,zheng2024cami2v}.
For an input image and camera parameters, we first synthesize a video.
We then estimate camera parameters from the synthesized video using GLOMAP~\cite{pan2024global}, and compare them against the input camera parameters to evaluate how faithfully the synthesized video follows the input parameters.
For the evaluation, we sampled 1,000 video clips and their associated camera parameters from the test set of RealEstate10K~\cite{zhou2018stereo}.


We employ the evaluation protocol of previous methods~\cite{he2024cameractrl,zheng2024cami2v} for the camera controllability. Specifically, for an input image and camera parameters, we first synthesize a video. We then estimate camera parameters from the synthesized video using GLOMAP~\cite{pan2024global}, and compare the estimated camera parameters against the input parameters to evaluate how faithfully the synthesized video follows the input camera parameters. For the evaluation dataset, we sampled 1,000 video clips and their associated camera parameters from the test set of RealEstate10K~\cite{zhou2018stereo}.


To evaluate estimated camera parameters against input ones,
we measure the mean rotation error (mRotErr), mean translation error (mTransErr), and mean error in camera extrinsic matrices (mCamMC), which are defined as:
\begin{equation}
\vspace{-2.5mm}
\begin{aligned}
\textnormal{mRotErr}&=\frac{1}{T} \sum\limits_{t=1}^{T} \cos^{-1}\frac{\textrm{tr}(\hat{R_t}R_t^{T}) - 1}{2}, \\
\vspace{-2.3mm}
\textnormal{mTransErr}&=\frac{1}{T} \sum\limits_{t=1}^{T} \lVert \hat{\tau}_t - \tau_{t} \rVert,~~~~~~\textrm{and} \\
\vspace{-2.3mm}
\textnormal{mCamMC}&=\frac{1}{T} \sum\limits_{t=1}^{T} \lVert [\hat{R}_t|\hat{\tau}_t] - [R_t|\tau_{t}] \rVert_{2}, \\
\end{aligned}
\end{equation}
where $T$ is the number of video frames.
$\hat{R}_t$ and $\hat{\tau}_t$ are the camera rotation matrix and translation vector estimated from the $t$-th synthesized video frame, and $R_t$ are $\tau_t$ are their corresponding input rotation matrix and translation vector, respectively.





\vspace{-3mm}
\paragraph{Video synthesis quality.}
We evaluate the video synthesis quality in terms of (1) sample quality and (2) object motion synthesis quality.
For sample quality, we first construct a benchmark dataset using 1,500 videos randomly sampled from the Pexels dataset~\cite{pexels_dataset} (Pexels-random).
For the model's capability of diverse object motion synthesis, we construct three benchmark video datasets with small, medium, and large object motions, each containing 500 video clips with minimal camera motions to avoid potential bias caused by camera motion.
The datasets are categorized based on the average magnitudes of the optical flow vectors of moving objects: smaller than 20 pixels (Pexels-small), between 20 and 40 pixels (Pexels-medium), and more than 40 pixels (Pexels-large).
More details on the benchmark datasets can be found in the supplemental document.


To evaluate the video synthesis quality, we synthesize videos using the first frames of videos in the aforementioned  benchmark datasets and compare these synthesized videos with the datasets.
While our method's video synthesis quality is minimally affected by these parameters, previous methods that synthesize video frames directly from them might be more influenced. 
To account for this, we utilize seven types of camera trajectories during video synthesis: translation to the left, right, up, and down, as well as zoom-in, zoom-out, and no camera motion (`stop').
Consequently, for all models, we generate seven videos for each video included in the benchmark datasets.
Finally, we evaluate the video synthesis performance of a given method through the Frechet Video Distance (FVD)~\cite{unterthiner2018towards}, Frechet Image Distance (FID)~\cite{heusel2017gans}, and Inception Score (IS)~\cite{salimans2016improved}.




\begin{table}[!t]
\centering
\resizebox{\columnwidth}{!}{
    % \setlength\tabcolsep{1.5pt}
    \begin{tabular}{l|c|ccc}
    \toprule[1pt]
                    & Training Data     & mRotErr ($^\circ$) & mTransErr        & mCamMC     \\ \hline
    MotionCtrl      & RE10K             & 5.90              & 0.1610            & 0.1719    \\
    CameraCtrl      & RE10K             & 1.44              & 0.0927            & 0.0945    \\ 
    Ours            & RE10K             & \textbf{1.43}     & \textbf{0.0869}   & \textbf{0.0887}    \\
    Ours            & Internal          & 1.79              & 0.0994            & 0.1018   \\
    Ours w/ OMSM    & RE10K             & 1.52              & 0.0971            & 0.0989    \\
    Ours w/ OMSM    & Internal          & 1.88              & 0.1042            & 0.1066    \\
         
    \bottomrule[1pt]
    \end{tabular}
}
\vspace{-3.0mm}
\caption{
Quantitative evaluation of camera controllability using the RealEstate10K test dataset~\cite{zhou2018stereo}. 
% For a comprehensive comparison, we evaluate for versions of our method and compare them with previous methods~\cite{he2024cameractrl, wang2024motionctrl}.
Our method shows superior camera control performance against previous methods~\cite{he2024cameractrl, wang2024motionctrl}, even without using camera parameters in training.
}
\vspace{-4.0mm}
\label{table:cam_control}
\end{table}





\subsection{Comparison}


We compare our method with recent camera-controllable video synthesis methods, MotionCtrl~\cite{wang2024motionctrl} and CameraCtrl~\cite{he2024cameractrl}, both of which support detailed camera control by taking camera parameters as input.
Additional comparisons with other methods that support basic camera movements can be found in the supplemental document.







\begin{figure*}[!t]
\centering
\begin{tabular}{@{}c}
\includegraphics[width=0.95\linewidth]{fig/qual_obj_motion.pdf} \\
\end{tabular}
\vspace{-4mm}
\caption{
Qualitative comparison of video synthesis quality.
Video frames are synthesized with 'stop' camera motion.
X-t slice reveals how pixel value changes over time along the horizontal red line.
MotionCtrl~\cite{wang2024motionctrl} often fails to follow input camera trajectory and synthesizes video frames with artifacts, due to the lack of generalization capability.
CameraCtrl~\cite{he2024cameractrl} frequently synthesizes motionless object in generated videos.
Our method synthesizes video frames with natural object motion while supporting precise camera control.
}
\label{fig:qual_obj_motion}
\vspace{-3mm}
\end{figure*}







\vspace{-4mm}
\paragraph{Camera controllability.}
We first compare the camera controllability of our method against MotionCtrl~\cite{wang2024motionctrl} and CameraCtrl~\cite{he2024cameractrl}.
Both MotionCtrl and CameraCtrl were trained on RealEstate10K~\cite{zhou2018stereo}, which provides no object motions but a wider range of camera motions than our full dataset.
For a comprehensive comparison, we evaluate four versions of our model.
Specifically, we train FVSM on either our internal dataset or RealEstate10K, but without utilizing the ground-truth camera parameters available in RealEstate10K.
We also include variants of our model with and without OMSM, as RealEstate10K contains only static scenes without moving objects.
In this evaluation, OMSM is trained using our internal dataset.



As shown in \cref{fig:qual_cam_control}, 
MotionCtrl~\cite{wang2024motionctrl} produces video frames that do not accurately follow the input camera trajectories due to its suboptimal camera parameter embedding scheme.
On the other hand, both CameraCtrl~\cite{he2024cameractrl} and ours accurately reflect the input camera parameters, and produce video frames that closely resemble the ground-truth frames.

As reported in \cref{table:cam_control}, our model trained on RealEstate10K~\cite{zhou2018stereo} outperforms both MotionCtrl and CameraCtrl across all metrics.
Moreover, our other models show comparable performances to CameraCtrl, while using the internal dataset and incorporating OMSM slightly increase errors due to domain differences and object motions.
These results prove the effectiveness of our camera control scheme based on optical flow.









\vspace{-4.5mm}
\paragraph{Video synthesis quality.}
We also compare the video synthesis quality of our method with previous ones~\cite{he2024cameractrl,wang2024motionctrl}.
\cref{fig:qual_obj_motion} shows a qualitative comparison, including X-t slices to visualize pixel value changes over time, computed from the positions marked by the red lines.
In this comparison, we synthesize videos using camera parameters without any motion, mainly to compare the video synthesis quality.
CameraCtrl~\cite{he2024cameractrl} produces results with no object motions, as shown in its X-t slices.
MotionCtrl~\cite{wang2024motionctrl} produces artifacts with inconsistent foreground and background regions, as marked by blue arrows.
These artifacts result from the limited generalization capability, since MotionCtrl updates certain pre-trained parameters of the video diffusion model during training.
Unlike these methods, our method produces high-quality videos with natural object motions.

The superior performance of our method is also evidenced by the quantitative comparison in \cref{table:sample_quality}.
For the Pexels-random dataset, our method reports better sample quality against the previous methods~\cite{he2024cameractrl,wang2024motionctrl}.
These results prove that our method does not harm the video synthesis quality of the pre-trained video diffusion model, compared to the previous ones.

Our method also achieves better performances for the benchmark datasets of object motion synthesis quality (Pexels-small, Pexels-medium, and Pexels-large), as reported in \cref{table:sample_quality}.
While CameraCtrl exhibits significantly degraded quality for large object motions (Pexels-large), our method achieves substantially better results for all three benchmark datasets.
MotionCtrl often fails to follow input camera parameters, synthesizing videos where the viewpoint remains close to the input image. This may lead to good FVD scores, as the synthesized videos align well with the minimal camera movement present in most benchmark videos.
However, as shown in \cref{fig:qual_obj_motion}, MotionCtrl often produces visual artifacts in the synthesized videos.
These artifacts are also evidenced by the degraded FID scores for MotionCtrl.
More visual examples of the visual artifacts can be found in the supplemental document.
In addition, by employing a timestep sampling strategy from the EDM framework~\cite{karras2022elucidating}, our method outperforms previous methods across all metrics (Tab.~S1 in the supplemental document).


\begin{table}[!t]
\centering
\resizebox{\linewidth}{!}{
    \setlength\tabcolsep{1.5pt}
    \begin{tabular}{l|ccc|ccc|ccc|ccc}
    \toprule[1pt]
                    & \multicolumn{3}{c|}{Pexels-random} & \multicolumn{3}{c|}{Pexels-small ($<20$)} & \multicolumn{3}{c|}{Pexels-med. ($<40$)} & \multicolumn{3}{c}{Pexels-large ($\ge40$)}  \\    
                    %& \multicolumn{3}{c|}{Randomly Sampled} & \multicolumn{3}{c|}{Set 1 (0-20)} & \multicolumn{3}{c|}{Set 2 (20-40)} & \multicolumn{3}{c}{Set 3 (40-inf)}  \\
                    & FVD        & FID        & IS         & FVD             & FID         & IS           & FVD          & FID         & IS         & FVD             & FID        & IS        \\ \hline
    MotionCtrl      & 93.54          & 36.06      & 11.19  & 235.39      & 28.94      & 10.53      & 214.47          & 25.76      & 10.74       & \cellcolor{yellow!100}\textbf{188.89} & 25.84   & 11.71           \\
    CameraCtrl      & 151.06         & 40.69      & 11.23  & 226.40       & 25.57      & 10.63      & 328.71          & 24.54      & 10.76       & 340.36       & 25.53      & 11.68           \\ 
    Ours            & \cellcolor{yellow!100}\textbf{91.55} & \cellcolor{yellow!100}\textbf{35.66} & \cellcolor{yellow!100}\textbf{11.63}   & \cellcolor{yellow!100}\textbf{220.65}& \cellcolor{yellow!100}\textbf{22.49}  & \cellcolor{yellow!100}\textbf{11.58} & \cellcolor{yellow!100}\textbf{183.14}  & \cellcolor{yellow!100}\textbf{20.71}    & \cellcolor{yellow!100}\textbf{11.68}     & 207.39     & \cellcolor{yellow!100}\textbf{21.12} & \cellcolor{yellow!100}\textbf{12.95}  \\       
    \bottomrule[1pt]
    \end{tabular}
}
\vspace{-3mm}
\caption{
Quantitative evaluation of video synthesis quality using the Pexels dataset~\cite{pexels_dataset}.
Our method shows superior video synthesis performance against previous methods~\cite{wang2024motionctrl,he2024cameractrl}.
}
\vspace{-3mm}
\label{table:sample_quality}
\end{table}










\subsection{Analysis}
\label{sec:ablation_study}
In the following, we provide an ablation study of our main components and a detailed mechanism of FVSM. 
Refer to Sec.~5 in the supplemental document for further analysis.






\vspace{-3mm}
\paragraph{Ablation study.}
We conduct an ablation study to verify the effect of our main components: OMSM, and training with a wide range of real-world videos, both of which are made possible by our optical-flow-based framework.
We utilize our models trained with two different timestep sampling strategies for in-depth analysis under different settings.
In \cref{fig:ablation_main}, the baseline model indicates a variant of our model that has no OMSM (i.e., only FVSM) and is trained on RealEstate10K~\cite{zhou2018stereo}.
`+~OMSM' indicates a variant model with OMSM, while its FVSM is still trained on RealEstate10K.
OMSM is trained using our full and curated datasets.
`+~large-scale data' is our final model where both OMSM and FVSM are trained using our datasets.

As shown in \cref{fig:ablation_main}(a), the baseline model does not synthesize noticeable object motions.
Introducing OMSM enables our framework to generate object motion, but it also occasionally produces artifacts for moving objects as shown in \cref{fig:ablation_main}(b).
Our final model produces natural-looking object motions without noticeable artifacts (\cref{fig:ablation_main}(c)). \cref{table:ablation_object} also shows similar trends that introducing each component to our framework consistently improves evaluation metrics for Pexels-random.
Additional quantitative ablation study can be found in Tab.~S1 of the supplemental document.

\begin{figure}[!t]
\centering
\includegraphics[width=0.93\linewidth]{fig/ablation_main_small.pdf}
\vspace{-3.0mm}
\caption{
Qualitative ablation with 'zoom-out' camera motion. X-t slice reveals pixel value changes along the horizontal red line. 
}
\vspace{-3.5mm}
\label{fig:ablation_main}
\end{figure}

\begin{table}[!t]
\centering
\resizebox{0.85\linewidth}{!}{
    % \setlength\tabcolsep{2.0pt}
    \begin{tabular}{l|c|ccc}
    \toprule[1pt]
                    & Training  & \multicolumn{3}{c}{Pexels-random}  \\
                    & Data      & FVD ($\downarrow$)    & FID ($\downarrow$)     & IS ($\uparrow$)            \\ \hline
    Baseline    & RE10K  & 157.99   & 39.61  & 11.19       \\
    \hspace{1mm} + OMSM & RE10K & 104.92 & 36.33 & 11.51   \\       
    \hspace{1mm} + large-scale data  & Internal  & \textbf{91.55}  & \textbf{35.66} & \textbf{11.63}   \\ 
    \bottomrule[1pt]
    \end{tabular}
}
\vspace{-3mm}
\caption{
Ablation study of our main components with the evaluation of video synthesis quality using the Pexels-random dataset.
}
\vspace{-1mm}
\label{table:ablation_object}
\end{table}


\vspace{-3.5mm}
\paragraph{Flow-conditioned video synthesis.}
Our method generates camera flow maps using the 3D structure estimated from an input image and feeds them to FVSM. To better understand our framework, \cref{fig:explicit_control} presents visualizations of warped images using the estimated 3D structure and camera parameters, alongside their associated video synthesis results produced by FVSM.
As shown in the figure, 3D-based image warping may introduce distortions and holes, yet still provides realistic-looking images. This result indicates that leveraging the 3D structure can serve as a powerful hint for camera-controllable video synthesis. \cref{fig:explicit_control} also shows that our flow-conditioned video synthesis successfully produces realistic-looking results that closely resemble the warping results, but without artifacts such as distortions and holes.






\subsection{Applications}
\vspace{-1mm}
\paragraph{Temporally-consistent video editing.}
Our framework using FVSM enables temporally-consistent video editing at no extra cost.
Specifically, we first obtain optical flow maps from the input video and edit the first frame of the video.
We then synthesize a video by using FVSM with the edited first frame and flow maps as inputs, producing temporally-consistent video editing results~(\cref{fig:video_editing}).

% \paragraph{Temporally-consistent video editing.}
% Our framework using FVSM enables temporally-consistent video editing at no extra cost.
% A video can be edited as follows.
% First, we obtain optical flow maps from the video by using an off-the-shelf optical flow estimator~\cite{teed2020raft}.
% We then edit the first frame of the input video using an off-the-shelf image editing tool, e.g., InfEdit~\cite{xu2023inversion}.
% We synthesize a video by using FVSM with the edited first frame and estimated optical flow maps as inputs.
% As shown in \cref{fig:video_editing}, this simple strategy using our framework can produce temporally-consistent video editing results, thanks to our optical-flow-based approach.




\vspace{-4mm}
\paragraph{Cinematic camera control.}
Thanks to its 3D awareness, our framework supports advanced camera controls such as the dolly zoom, which moves the camera forward or backward while simultaneously adjusting the zoom in the opposite direction.
%To achieve this effect, we synthesize videos using forward-moving camera parameters while adjusting the zoom in the opposite direction.
\cref{fig:teaser}(Left) shows a synthesized video with the dolly zoom effect, where the subject remains a similar size while the background appears to converge inward.
Notably, our framework accomplishes this without requiring training on video datasets with varying camera intrinsic parameters across frames.








\begin{figure}[!t]
\centering
\includegraphics[width=\columnwidth]{fig/explicit_control_small_ver2.pdf}
\vspace{-6mm}
\caption{
Explicit camera control. Our model can follow the warped frames while handling artifacts such as holes, which are caused by imperfect 3D structure estimation.
}
\vspace{-2mm}
\label{fig:explicit_control}
\end{figure}


