
\section{\MethodName{} Framework}
\label{sec:method}
\cref{fig:framework_overview} presents an overview of \MethodName{}.
Our framework takes an image $I_s$ and camera parameters $\mathcal{C}=\{C_t\}_{t=1}^T$ as input where $t$ is a video frame index, and $T$ is the number of video frames.
$C_t$ is defined as a set of extrinsic and intrinsic camera parameters.
Given the input conditions, our framework synthesizes a video $\mathcal{I}=\{I_t\}^{T}_{t=1}$ that starts from $I_s$ as the first frame and follows the input camera trajectory, where $I_t$ is the $t$-th video frame.


\MethodName{} consists of two stages. First, the flow generation stage synthesizes two sets of optical-flow maps that represent camera and object motions using 3D warping and an object motion synthesis model (OMSM), respectively. We refer to these optical flow maps as camera flow maps and object flow maps.
These flow maps are integrated to form a single set of optical flow maps, which we refer to as camera-object flow maps. 
In the subsequent stage, a flow-conditioned video synthesis model (FVSM) synthesizes a video using the input image $I_s$ and the camera-object flow maps. In the following, we describe each stage in detail.


\subsection{Flow Generation}
\label{sec:flow_generation}
\paragraph{Camera flow generation.}
In the flow generation stage, we first generate camera flow maps $\mathcal{F}^c=\{f^c_t\}_{t=1}^{T}$ where $f^c_t$ is an optical flow map from the first frame to the $t$-th frame.
To generate camera flow maps reflecting the 3D structure in the input image $I_s$, we estimate a depth map $d_s$ from $I_s$ using an off-the-shelf single-image 3D estimation network~\cite{yang2024depth}.
Using the estimated depth map, we unproject each pixel coordinate $x_s$ in the input image $I_s$ into the 3D space.
Then, for each $t$, we warp the unprojected coordinates and project them back to the 2D plane using $C_t$ to obtain the warped coordinate $x_t$.
Finally, we construct the camera flow map $f_t^c$ by computing displacement vectors from $x_s$ to $x_t$.



\begin{figure}[t!]
\centering
\includegraphics[width=0.89\columnwidth]{fig/diffusion_models.pdf}
\vspace{-3mm}
\caption{
Network architectures of OMSM and FVSM.
}
\vspace{-4mm}
\label{fig:diffusion_models}
\end{figure}



\paragraph{Object flow synthesis.}
In this stage, we also generate object flow maps $\mathcal{F}^o=\{f_t^o\}_{t=1}^T$ that represent object motions independent of background motions.
To this end, we develop OMSM based on the latent video diffusion model~\cite{blattmann2023stable}.
Specifically, as shown in \cref{fig:diffusion_models}(a), OMSM consists of a denoising U-Net, and an encoder and decoder of the latent video diffusion model's variational autoencoder (VAE).
In OMSM, the input image $I_s$ is first encoded by the VAE encoder.
Then, the denoising U-Net takes a concatenation of the encoded input image and a noisy latent feature volume as input, and iteratively denoises the latent feature volume to synthesize latent object motion flow maps.
Finally, the VAE decoder decodes the synthesized result and produces object flow maps $\mathcal{F}^o$. 

Inspired by Marigold \cite{ke2024repurposing}, we utilize the VAE decoder of the latent video diffusion model, which is trained on RGB images, for decoding object flow maps without any architectural changes or fine-tuning. Specifically, from the three output channels of the VAE decoder corresponding to RGB, we use only the first two channels for the $x$ and $y$ components of an object flow map.
Our training process also involves the VAE encoder.
% , although it is not used during inference. 
Like the VAE decoder, we use the VAE encoder of the latent video diffusion model without any architectural changes or fine-tuning. For the three input channels of the VAE encoder, we feed the $x$ and $y$ components of an object flow map, along with their average $(x+y)/2$.
We verified that the optical flow map can be reconstructed from the encoded latent feature with a negligible error without any modification of the VAE.


\paragraph{Flow integration.}
Once the camera and object flow maps, $\mathcal{F}^c$ and $\mathcal{F}^o$, are generated, we obtain camera-object flow maps $\mathcal{F}=\{f_t\}_{t=1}^T$ by combining them.
The integration is performed as follows.

First, we estimate a binary mask $M^{obj}$ from the input image $I_s$ using an off-the-shelf segmentation model \cite{ravi2024sam}, which indicates pixels corresponding to moving objects. We use a single binary mask $M^{obj}$ for $t$, as all flow maps are forward-directional optical flow maps from the first frame to the $t$-th frame. Based on $M^{obj}$, we combine $f_t^c$ and $f_t^o$.
Specifically, for each pixel $x$ specified by $M^{obj}$, we compute its displaced position $x'$ using the object motion in $f_t^o$ as $x' = x + f_{t,x}^o$, where $f_{t,x}^o$ is the optical flow vector in $f_t^o$ at pixel $x$. Next, we transform $x'$ using the camera parameter $C_t$ and the depth map $d_s$, obtaining $x'_t$, which represents the displaced position of $x$ at the $t$-th frame due to both camera and object motions. We then compute the flow vector $f'_{t,x} = x'_t - x$. 
Finally, we derive the camera-object flow map $f_t$ as:
% Final camera-object flow map $f_t$ is defined as:
\begin{equation}
f_{t,x} = (1 - M^{obj}_x) \cdot f_{t,x}^c + M^{obj}_x \cdot f'_{t,x},
\label{eq:flow_integration}
\end{equation}
where $M^{obj}_x$ is the binary value of $M^{obj}$ at pixel $x$.


It is important to note that physically valid integration of camera and object flows requires object motion information along the $z$-axis (orthogonal to the image plane), which is not captured in the object flow maps.
Thus, our integration process does not produce physically accurate camera-motion flow maps.
However, we experimentally found that our framework can still synthesize videos with natural object motions.
This is made possible by the flow-conditioned video synthesis model (FVSM), which is trained on natural-looking videos, ensuring realistic object motions, even for input noisy camera-motion flow maps.

Our OMSM is trained to generate non-zero flow vectors only for dynamic objects. Therefore, we do not necessarily need to use the mask $M^{obj}$ in \cref{eq:flow_integration}; instead, we can transform the entire object motion flow maps using the camera parameters. However, we empirically found that using the mask $M^{obj}$ improves video synthesis quality by removing incorrectly synthesized flow vectors in static regions of $f_t^o$.




\subsection{Flow-Conditioned Video Synthesis}
\label{sec:video_synthesis_model}

The flow-conditioned video synthesis stage synthesizes a video $\mathcal{I}$ using the input image $I_s$ and the camera-object flow maps $\mathcal{F}$ as conditions.
To achieve this, our framework utilizes FVSM, which extends the latent video diffusion model~\cite{blattmann2023stable} by incorporating an additional flow encoder inspired by the T2I-Adapter architecture~\cite{mou2024t2i}.
Specifically, as shown in \cref{fig:diffusion_models}(b), our model consists of a flow encoder $E^f$, a denoising U-Net, and a VAE encoder and decoder.

The flow encoder takes the input camera-object flow maps $\mathcal{F}$, and computes multi-level flow embeddings:
\begin{equation}
% \vspace{-2mm}
\begin{aligned}
\{\xi_{(t,l)}\}^{T,L}_{t=1,l=1}=E^f(\mathcal{F}),
\end{aligned}
\end{equation}
where $\xi_{(t,l)}$ is a flow embedding of the $t$-th frame $I_t$ at level $l$. Each flow embedding has the resolution of its corresponding layer's latent feature in the denoising U-Net.
The denoising U-Net takes a concatenation of the encoded input image and a noisy latent feature volume as input, and iteratively denoises the latent feature volume of the video.
Additionally, the denoising U-Net also takes the multi-level flow embeddings by adding each of them to the feature at each layer of the denoising U-Net.
Finally, the synthesized video frames are obtained by decoding the denoised latent feature volume using the VAE decoder.
More details on the network architecture can be found in the supplemental document.

\section{\MethodName{} Training}
\label{sec:Training}






\MethodName{} utilizes two diffusion models: OMSM and FVSM, which are trained separately. As discussed in \cref{sec:intro}, both models can be effectively trained using a wide range of videos with dynamic object motions without requiring ground-truth camera parameters, thanks to our optical-flow-based representation.
In the following, we explain our datasets and training strategies for our models.
% Additional training details on the training of our models are provided in the supplemental document.

\subsection{Training Datasets}
For training these diffusion models, we primarily use an internal dataset containing 500K video clips, and its subset of video clips without camera motions. We refer to these as the full dataset and the curated dataset, respectively. The full dataset contains scenes similar to those in the Pexels dataset~\cite{pexels_dataset}.
The curated dataset contains around 100K video clips.
For training OMSM, we use both datasets, while for training FVSM, we use only the full dataset.

Training the diffusion models in our framework requires optical flow maps for each video clip.
We estimate the optical flow maps using an off-the-shelf estimator~\cite{teed2020raft}, and use them as the ground-truth object flow maps for OMSM, and the camera-object flow maps for FVSM. 


The curated dataset is generated through the following process. For each video clip in the full dataset, we first detect the static background region from the first frame using an off-the-shelf semantic segmentation model~\cite{ravi2024sam}. Next, we compute the average magnitude of the optical flow vectors for all video frames within the background region. If this average magnitude is smaller than a specified threshold, we consider the video clip to have no camera motion and include it in the curated dataset.


\subsection{Training Object Motion Synthesis}
\label{sec:Training_MSM}
OMSM is trained in two stages.
The first stage initializes the model with the parameters of a pre-trained video diffusion model, and trains the model on the full dataset.
% including camera motions.
The second stage fine-tunes the model using the curated dataset without camera motions.
During training, we only update the parameters of the denoising U-Net, while fixing the parameters of the VAE encoder and decoder.
We train the model via denoising score matching~\cite{karras2022elucidating}.

The two-stage approach helps overcome the domain difference between the video synthesis task of the pretrained model and the object motion synthesis task, allowing for effective learning of object motion synthesis from the small-scale curated dataset.
\cref{fig:Data_Curation} shows an example of synthesized motions after the first and second training stages.
After the first stage, OMSM is effectively trained to synthesize object motion flow maps, but they exhibit camera motions in the background.
After the second stage, the model can successfully synthesize object motion flow maps with minimal camera motions.


\begin{figure}[t!]
\centering
\includegraphics[width=\columnwidth]{fig/Data_Curation.pdf}
\vspace{-6.3mm}
\caption{
Object flow maps synthesized by OMSM, which is trained on the full dataset (left) and the curated dataset (right), respectively.
White indicates optical flow vectors with no motion.
}
\vspace{-5mm}
\label{fig:Data_Curation}
\end{figure}




\subsection{Training Flow-Conditioned Video Synthesis}
\label{sec:Training_VSM}
We initialize FVSM using the parameters of a pretrained video diffusion model~\cite{blattmann2023stable}.
Then, we train only the flow encoder while fixing the other components.
Similar to OMSM, we train FVSM via denoising score matching~\cite{karras2022elucidating}.
While the optical flow maps are directly estimated from video datasets in the training time, the camera-object flow maps used in the inference time are synthesized through 3D warping and OMSM. Nevertheless, both optical flow maps contain camera and object motions in the form of flow vectors, enabling the FVSM to produce natural videos with the desired camera motion effectively.

