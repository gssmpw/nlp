\begin{abstract}

We present \MethodName{}, a novel video diffusion model for camera-controllable video generation.
\MethodName{} leverages optical flow to represent the motions of the camera and moving objects.
This approach offers two key benefits.
Since optical flow can be directly estimated from videos, our approach allows for the use of arbitrary training videos without ground-truth camera parameters.
Moreover, as background optical flow encodes 3D correlation across different viewpoints, our method enables detailed camera control by leveraging the background motion.
To synthesize natural object motion while supporting detailed camera control, our framework adopts a two-stage video synthesis pipeline consisting of optical flow generation and flow-conditioned video synthesis.
Extensive experiments demonstrate the superiority of our method over previous approaches in terms of accurate camera control and natural object motion synthesis.



\end{abstract}