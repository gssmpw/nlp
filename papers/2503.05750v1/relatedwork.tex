\section{Related Work}
\label{sec_rl}

In recent years, deep learning has significantly improved neural abstractive summarization tasks in NLP \cite{5rush2015neural, 6chopra2016abstractive, 7tan2017abstractive, 8zhou2017selective, 9li2017deep}. Traditional models are primarily trained on general datasets like CNN/Daily Mail \cite{10nallapati2016abstractive} and Gigaword Corpus \cite{11sutskever2014sequence}. The attention-based seq2seq model by \citet{5rush2015neural} and its extension by \citet{6chopra2016abstractive} laid the groundwork for these advancements. Recently, large-scale pre-trained language models have shown impressive summarization results \cite{12liu2019reading, 13karn2021few, 14raffel2020exploring, 15kieuvongngam2020automatic, 16chang2021jointly}. \citet{15kieuvongngam2020automatic} employed BERT and GPT for summarizing COVID-19 research. However, unique word distributions in radiology corpora limit the applicability of these techniques \cite{1Cai2023}.

 \citet{20zhang2018learning} first explored automatic radiology Impression generation, followed by \citet{21macavaney2019ontology}, who introduced an ontology-aware pointer-generator model to enhance summarization quality. A background-augmented pointer-generator network with copy and background-guided decoding was proposed \cite{added1zhang-etal-2020-optimizing}, and a word graph captured critical words and relations \cite{added2hu-etal-2021-word}. Anatomies were extracted, radiographs encoded, and fused with anatomy-enhanced co-attention \cite{added3hu-etal-2023-improving-radiology}. \cite{added4sotudeh-gharebagh-etal-2020-attend} enhanced clinical summarization by augmenting ontological terms, and \cite{added5hu-etal-2022-graph} integrated findings by a unified framework with knowledge via text and graph encoders. \cite{added6karn-etal-2022-differentiable} proposed a two-step extractive-abstractive method using a Differentiable Multi-Agent Actor-Critic framework.

A hybrid retrieval-generation agent \cite{22li2018hybrid} integrated human knowledge with neural networks for medical report generation. Models like OpenAI GPT \cite{16chang2021jointly}, BERT \cite{25devlin2019bert}, ELMo \cite{26peters2018deep}, and XLNet \cite{27yang2019xlnet} have improved various NLP tasks through extensive external knowledge. BioBERT \cite{28lee2020biobert} captures semantic features in the biomedical field, pre-trained on large corpora like PubMed abstracts and PMC articles. Recently, \citet{1Cai2023} proposed a pre-trained language model, ChestXrayBert, specifically designed for summarizing chest radiology reports. However, these studies may struggle with effective Impression generation methodologies.


\begin{table}[t!]
\resizebox{\columnwidth}{!}{%
\begin{threeparttable}
\begin{tabular}{@{}llllllll@{}}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{\# Reports} & \multicolumn{3}{l}{Findings (Avg.)} & \multicolumn{3}{l}{Impressions (Avg.)} \\ \cmidrule(l){3-8} 
 &  & \# S & \# W/S & \# Source W & \# S & \# W/S & \# Source W \\ \midrule
MIMIC-CXR & 121,975 & 5.47 & 10.08 & 55.09 & 1.94 & 8.50 & 16.46 \\
Open-I & 3,312 & 4.62 & 8.03 & 37.06 & 1.81 & 5.52 & 9.98 \\ \bottomrule
\end{tabular}%
\begin{tablenotes}
    \item S: Sentence, W: Words
\end{tablenotes}
\end{threeparttable}}
\caption{Summary of dataset statistics of Findings and Impression}
\label{tab:dataset_summary}
\end{table}