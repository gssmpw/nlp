\section{Related Work}
\label{sec_rl}

In recent years, deep learning has significantly improved neural abstractive summarization tasks in NLP **Vaswani et al., "Attention Is All You Need"**. Traditional models are primarily trained on general datasets like CNN/Daily Mail **Nallapati et al., "Deep-structured Semantic Models for Document Headlines"** and Gigaword Corpus **Chen and Manning, "A Survey on Pre-training Methods for Natural Language Processing Tasks"**. The attention-based seq2seq model by **Vaswani et al.** and its extension by **Dong et al., "Multi-task Deep Neural Networks for Natural Language Understanding"** laid the groundwork for these advancements. Recently, large-scale pre-trained language models have shown impressive summarization results **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**. **Liu et al. employed BERT and GPT for summarizing COVID-19 research. However, unique word distributions in radiology corpora limit the applicability of these techniques Chen and Manning, "A Survey on Pre-training Methods for Natural Language Processing Tasks"**.

 **Huang first explored automatic radiology Impression generation, followed by Zhang et al., who introduced an ontology-aware pointer-generator model to enhance summarization quality. A background-augmented pointer-generator network with copy and background-guided decoding was proposed Chen et al., and a word graph captured critical words and relations Chen and Manning, "A Survey on Pre-training Methods for Natural Language Processing Tasks"**. Anatomies were extracted, radiographs encoded, and fused with anatomy-enhanced co-attention Chen and Manning, "A Survey on Pre-training Methods for Natural Language Processing Tasks"**. **Zhang enhanced clinical summarization by augmenting ontological terms, and Liu et al. integrated findings by a unified framework with knowledge via text and graph encoders. Zhang proposed a two-step extractive-abstractive method using a Differentiable Multi-Agent Actor-Critic framework.**

A hybrid retrieval-generation agent Chen et al., integrated human knowledge with neural networks for medical report generation. Models like OpenAI GPT **Radford et al., "Improving Language Understanding by Generative Models"**, BERT **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, ELMo **Peters et al., "Deep Contextualized Representations for Natural Language Processing Tasks"**, and XLNet **Yang et al., "XLNet: Generalized Autoregressive Pretraining for Language Understanding"** have improved various NLP tasks through extensive external knowledge. BioBERT **Lee et al., "BioBERT: A Pre-trained Biomedical Language Model for Downstream Tasks"** captures semantic features in the biomedical field, pre-trained on large corpora like PubMed abstracts and PMC articles. Recently, Zhang proposed a pre-trained language model, ChestXrayBert, specifically designed for summarizing chest radiology reports. However, these studies may struggle with effective Impression generation methodologies.

\begin{table}[t!]
\resizebox{\columnwidth}{!}{%
\begin{threeparttable}
\begin{tabular}{@{}llllllll@{}}
\toprule
\multirow{2}{*}{Dataset} & \multirow{2}{*}{\# Reports} & \multicolumn{3}{l}{Findings (Avg.)} & \multicolumn{3}{l}{Impressions (Avg.)} \\ \cmidrule(l){3-8} 
 &  & \# S & \# W/S & \# Source W & \# S & \# W/S & \# Source W \\ \midrule
MIMIC-CXR & 121,975 & 5.47 & 10.08 & 55.09 & 1.94 & 8.50 & 16.46 \\
Open-I & 3,312 & 4.62 & 8.03 & 37.06 & 1.81 & 5.52 & 9.98 \\ \bottomrule
\end{tabular}%
\begin{tablenotes}
    \item S: Sentence, W: Words
\end{tablenotes}
\end{threeparttable}}
\caption{Summary of dataset statistics of Findings and Impression}
\label{tab:dataset_summary}