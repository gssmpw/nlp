\newpage
\clearpage
\section{Implementation Details}\label{appendix:implementation_details}

\subsection{GPT4o Prompts}
We include our prompt for zero-shot factual consistency evaluation in Table \ref{tab:gpt4o_prompt}.
\begin{table*}
\scriptsize
    \begin{center}
        \begin{tabular}{l}
        \toprule
             Determine whether the provided claims are consistent with the corresponding document. Consistency in this context\\
             implies that
    all information presented in the claim is substantiated by the document.  If not, it should be \\
    considered inconsistent.\\
    \\
    Document: [DOCUMENT]\\
    Claims: [CLAIMS] \\
    Please assess the claimâ€™s consistency with the document by responding with either "yes" or "no".\\
    The CLAIMs are ordered in the format of a dictionary, with \{ index: CLAIM \}. You will need to return the result in JSON format.\\
    For instance, for a CLAIMs list of 4 items, you should return \{0:yes/no, 1:yes/no, ...., 3:yes/no\}.\\
\\
    ANSWER: \\
            \bottomrule
        \end{tabular}
        \caption{Zero-shot factual consistency evaluation prompt for GPT4o.}\label{tab:gpt4o_prompt}
    \end{center}
\end{table*}

\subsection{Baselines}
\paragraph{AlignScore} (model size 355M) \cite{zha-etal-2023-alignscore} is an entailment-based model that has been trained on data from a wide range of tasks such as NLI, QA, and fact verification tasks. It divides the source document into a set of sequential chunks at sentence boundaries. For a multi-sentence summary, it predicts the max scoring value of all combinations of source chunk and target sentence, then returns the unweighted average of all sentences as the summary prediction. We follow the original setting by setting chunk size at 350 tokens and use the default model alingsocre\_large ckpt. The model outputs a score between 0 and 1. We conduct experiments on top of their released codebase \url{https://github.com/yuh-zha/AlignScore}.
\paragraph{MiniCheck-FT5} (model size 770M) \cite{tang2024minicheck} is an entailment-based fact checker built on flan-t5-large. It has been further fine-tuned on 21K datapoints from the ANLI dataset \cite{nie2019adversarial} and 35k synthesized data points generated in \cite{tang2024minicheck} on the tasks to predict whether a given claim is supported by a document. We follow the authors's setting and set the chunk size to 500 tokens using white space splitting. The output score is between 0 and 1. We use the released code repo from \url{https://github.com/Liyan06/MiniCheck}.

\paragraph{LongDocFactScore} \cite{bishop-etal-2024-longdocfactscore-evaluating} is a reference-free framework for assessing factual consistency. It splits source documents and the generated summary into sentences, then computes the pair-wise similarities by computing the cosine similarities of sentences (they use the sentence-transformers library initialized with the bert-base-nmli-mean-tokens model). Afterward, for each individual summary sentence,  K most similar source sentences are picked.  The method extracts the neighboring source document sentences of the selected sentences as context, then applies a metric  BARTScore to evaluate the score between source context and summary sentences. The overall summary score is an unweighted average of all sentences. We follow the authors' parameters setting and utilize their released code repo from \url{https://github.com/jbshp/LongDocFACTScore}.

\paragraph{InfUSE} (model size 60M) \citet{zhang-etal-2024-fine} uses a variable premise size and breaks the summary into sentences or shorter hypotheses. Instead of fixing the source context, it retrieves the best possible context to assess the faithfulness of an individual summary sentence by applying an NLI model to successive expansions of the document sentences. Similar to prior approaches, it outputs an entailment score for each summary sentence, and the summary-level score is the unweighted average. We follow their settings on \textsc{InfUsE} with summary sentences instead of \textsc{InfUsE\textsubscript{SUB}} as the authors only released the code for the former model. \textsc{InfUsE} outputs scores in the range 0-1. We use the author's released codebase from \url{https://github.com/HJZnlp/Infuse}.
\paragraph{GPT4o}
 We used the version of gpt-4o-2024-05-13; we set max\_tokens 100, sampling temperature at 0.7, and  top\_p as 1.0. We call the OpenAI API from \url{https://openai.com/api}. Given the lengthy summary, we prompted the LLM to assign a binary label (yes/no) to assess individual summary sentences' consistency with the original article. Then, we reported the percentile of ``yes'' answers as the summary-level rating. 

 \paragraph{BeSpoke-MC-7B}
 We harnessed the SOTA Llama-3.1-Bespoke-MiniCheck-7B (BeSpoke-MC-7B) released by Bespoke Labs. The model is fine-tuned from ``internlm/internlm2\_5-7b-chat'' \cite{cai2024internlm2} on the combination of 35K data points following the approach in MiniCheck \cite{tang2024minicheck}. We use the suggested code repo from \url{https://huggingface.co/bespokelabs/Bespoke-MiniCheck-7B}. To calculate the AUC score, we employed the raw probabilities returned by the code to determine sentence-level ratings, and we calculated the summary-level score as the unweighted average across all sentences.



\subsection{Machine Configuration for Models}\label{appendix:machine_config}
We use up to 4 NVIDIA RTX 5000 GPUs, each equipped with 16 GB VRAM,  for model inferences on our hardware. According to Lambda\footnote{\url{https://lambdalabs.com/service/gpu-cloud}} (RTX5000 is depreciated), a single NVIDIA Quadro RTX 6000 (the closest to our setting) GPU costs \$0.5 per hour and has 24 GB VRAM. Additionally, we loaded the Bespoke-MC-7B model on a single NVIDIA L40S GPU with 48 GB of VRAM, provided by the Pitt CRC computing cluster.

\section{Experimental Results}
\subsection{Discussion on Performance Compared to Strong Baselines}\label{appendix:more_discussion}
Our primary analysis focuses on discussing how the proposed approach can improve different baselines (we utilized three backbone baselines: rows 5, 11, and 17 with their improved versions) in Table \ref{tab:aggrefact_diversum_res}. We observe several baselines obtained the best performance on certain tasks and provide a more careful justification below: 

\paragraph{}
While the improvements may appear marginal in some baseline models, they are statistically significant and consistent across multiple datasets. The capabilities of baseline models and the characteristics of testbeds can also affect performance. For instance, as noted in Section \ref{sec:result}, dialogue-based inputs in QMS limit the effectiveness of discourse parsing (RQ2), while short summaries like XSUM minimize the impact of reweighting (RQ1). On longer datasets like AXV and CSM, gains are more substantial, with improvements of up to 7 points (row 14 vs. row 11 in AXV). This is comparable to, or even more significant than, prior work \cite{zhang-etal-2024-fine}, and it is common to observe varying levels of performance gains across different tasks \cite{tang-etal-2023-understanding,tang2024minicheck}.

\paragraph{LongDocFactScore} (LDFS) introduced the LongSciVerify (LSV) dataset (PUB and AXV), using a different annotation method by subsampling three sentences with human annotations for factuality. We conjecture this may lead to less accurate summary-level labels, favoring their metric, which utilizes the top-k sentence-level scores. Meanwhile, LDFS underperformed compared to most other baselines on AggreFact, QMS, AXV (from \textsc{DiverSumm}), and LongEval-PUB. In contrast, our approach outperformed LongDocFactScore on most other benchmarks (e.g., 86.32 vs. 65.36 on AXV), suggesting our approach is more robust and capable of handling different long document summarization datasets. While each baseline may excel in specific tasks, a more robust benchmarking dataset could better ensure fair comparisons for future research.

\paragraph{GPT-4o} GPT4o is utilized as a comparison between the SOTA LLMs (GPT4o models have unknown sizes but could be greater than known open-sourced LLMs with up to 405B) and our lightweight model (770M), which in the usual case, the LLMs can outperform baselines by noticeable margins \cite{tang2024minicheck}). In Table \ref{tab:aggrefact_diversum_res}, regarding the long document summarization datasets (from GOV in DiverSumm to LegalSumm), our models (rows 12, 13) outperformed GPT4o in 5 out of 6 test sets (the only exception is LongEval PUB). This confirmed that the discourse-inspired approaches are beneficial.

\paragraph{BeSpoke-MC-7B} is claimed to be the best fact-checking model publicly available on the LLM-AggreFact benchmark, which outperformed many other LLMs with bigger sizes. Compared to our proposed models, it performed better on QMS, XSM\textsubscript{AG}, LSV-AXV, and had the best performance on LongEval-PUB (similar pattern to GPT-4o). However, on other benchmarks, our discourse approaches still demonstrate their benefits (i.e., on \textsc{LegalSumm}, AlignScore + reweighting obtained 76.57 while BeSpoke-MC-7B only scored 55.81). 


\subsection{Ablation Study}\label{appendix:ablation_study}
Table \ref{tab:larger_ablation} presents the ablation results of different discourse features on our baselines. We cover the long document summarization tasks starting from QMS in Table \ref{tab:aggrefact_diversum_res}.

\begin{table*}[ht!]
\small
\centering
% \setlength{\tabcolsep}{1pt}
\begin{NiceTabular}{lccccccc}
 \toprule
\multirow{1}{*}{\shortstack{\textbf{Model} }}
 &  \textbf{QMS} &  \textbf{GOV}  & \textbf{AXV}  & \textbf{CSM} & \textbf{LSV-PUB} & \textbf{LSV-AXV} & \textbf{LE-PUB}\\
% avg src. len &  4612.4 & 1138.7 & 4407.0 & 2008.2 & 669.2 \\
\midrule


{MC-FT5 (SENT)} &  60.66 & 83.24 & 78.66 & 59.74 & 55.7 & 52.7 & 30.2 \\

{\hspace{3mm}\textit{+ subtree height}} & 60.21 & 84.55 & 79.09 & 60.55& 53.6 & 55.1 & 30.4\\
{\hspace{3mm}\textit{+ depth score}} & 60.51 & 83.65 & 78.90 & 59.90 & 55.7 & 53.8 & 33.3  \\
 re-weighting  & 60.36 & 84.75 & 79.38 & 60.06& 52.8 & 55.1 & 31.4 \\
\midrule 
\midrule
{AlignScore} & 56.48 & 87.02 & 77.46 & 61.03 & 54.9 & 53.9 & 36.9 \\

{\hspace{3mm}\textit{+ subtree height}} &52.91 & 87.29 & 81.15 &  60.47 & 51.7 & 55.4 & 34.1\\
{\hspace{3mm}\textit{+ depth score}} & 56.63 & 87.29 & 77.66 & 60.30 & 54.3 & 52.4 & 36.6  \\
 re-weighting  & 53.95 & 87.29 & 81.15 & 60.55 & 53.0 & 54.3 & 34.8 \\

\bottomrule
\end{NiceTabular}
\caption{Ablation results on long document datasets from \textsc{DiverSumm}, \textsc{LongSciVerify} and \textsc{LongEval}.}\label{tab:larger_ablation}
\end{table*}


% \section{LegalSumm Annotation Guideline}
