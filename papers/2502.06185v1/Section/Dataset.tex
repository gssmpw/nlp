
\begin{table*}[th!]
% \renewcommand{\arraystretch}{0.9}
\scriptsize
    \centering
       \setlength\columnsep{1pt}
    \begin{NiceTabular}{l|crrrrr}
    \toprule
      \textbf{Dataset}  & \textbf{Sum.Task} &  \textbf{Size}  & \textbf{Doc.Word} & \textbf{Doc.Sent} & \textbf{Sum.Sent} & \textbf{Sum.Word}  \\
      \midrule
       \multirow{2}{*}{\textsc{AggreFact-FtSOTA}} &XSum \cite{tang-etal-2023-understanding} & 558 & 360.54 & 16.09 & 1.01 & 20.09\\
         & CNNDM \cite{tang-etal-2023-understanding} & 559 & 518.85 & 23.31& 2.72 & 52.21 \\
    \midrule
         \multirow{5}{*}{\textsc{DiverSumm}} & Multi-news  \cite{fabbri-etal-2019-multi} &  90& 669.20 & 27.2 & 6.81 & 152.20 \\
         & QMSUM \cite{zhong-etal-2021-qmsum}  & 90 & 1138.72 &72.80 & 3.04 & 65.22\\
         & Government \cite{huang-etal-2021-efficient} & 147 & 2008.16 & 71.35 & 15.1 & 391.22\\
           & ArXiv \cite{cohan-etal-2018-discourse} & 146& 4406.99 & 195.18 & 6.18 & 149.70 \\
           & ChemSumm \cite{adams-etal-2023-desired}  & 90 & 4612.40 & 188.80 & 7.36& 172.79\\
         
          
          
    \midrule
    \multirow{2}{*}{\textsc{LongSciVerify}} & PubMed \cite{cohan-etal-2018-discourse} & 45 & 3776.80 & 125.00 & 8.60& 225.60 \\
    & ArXiv \cite{cohan-etal-2018-discourse} & 45 & 6236.40 & 282.93 & 7.28 & 210.93\\
    

    \midrule
    \textsc{LongEval} & PubMed \cite{krishna-etal-2023-longeval} & 40 & 3158.35 & 110.00 & 10.38 & 193.55 \\
    \midrule
    \textsc{LegalSumm} & Legal Opinions \cite{elaraby-etal-2023-towards} & 50 & 2873.87 & 115.64 & 8.36 & 208.28 \\
    \bottomrule
    \end{NiceTabular}
    \caption{Summary-level task statistics on \textsc{AggreFact-FtSOTA}, \textsc{DiverSumm}, \textsc{LongSciVerify}, \textsc{LongEval} and \textsc{LegalSUMM}. We report the number of annotated doc-summary pairs of the test split (Size), document length in the average
number of words (Doc.Word) and the average number of sentences (Doc.Sent), summary length in the average number of sentences (Sum.Sent), and words (Sum.Word). }\label{tab:detection_dataset_analyses}

\end{table*}

\section{Datasets}
This section describes the datasets used to explore our research questions. We begin with the discourse analysis dataset, which includes sentence-level fine-grained labels of errors introduced in \citet{pagnoni-etal-2021-understanding}, enabling systematic analysis of the relationships between different features and their labels. We then discuss the benchmark datasets, which provide summary-level labels in either binary or continuous scores, and evaluate our approach and baselines on them.

\paragraph{Discourse Analysis Dataset}\label{token_level_dataset}
Our discourse analysis harnessed the subsets of \textsc{ArXiv} and \textsc{GovReport} from \textsc{DiverSumm} \cite{zhang-etal-2024-fine}, which come with annotated sentence-level errors labels. Following \citet{zhang-etal-2024-fine}, we denote it as \textsc{DiverSumm-sent}.  It covers 293 document-summary pairs of which 3138 summary sentences have sentence-level annotations.\footnote{We include analysis of the short document summarization datasets in Appendix \ref{appendix:short_analysis}.}

\paragraph{Summary-level Factuality Detection Datasets}
We test on the \textsc{AggreFact-FtSOTA} split \cite{tang-etal-2023-understanding},
%which similar work has done as well \cite{scire-etal-2024-fenice, yang2024fizz, zhang-etal-2024-fine}, 
\textsc{DiverSUMM} \cite{zhang-etal-2024-fine}, \textsc{LongSciVerify} and \textsc{LongEval} from \citet{bishop-etal-2024-longdocfactscore-evaluating}. We additionally collect \textsc{LegalSumm}, a legal summarization dataset, which covers model-generated summaries from the CanLII (Canadian Legal Information
Institute) dataset \cite{xu2021, elaraby-etal-2023-towards} with document-level factuality labels annotated by legal experts.\footnote{We provide details of the dataset in Appendix \ref{appendix:legalsumm_detail}.} Table \ref{tab:detection_dataset_analyses} presents a careful comparison of datasets from different perspectives. We conduct analysis on the document's structure in \S\ref{sec:document_structure} using these datasets. Except for \textsc{AggreFact}, all remaining datasets are focused on long documents and summary pairs. 


