\vspace{-2mm}
\section{Discourse Analysis}\label{sec:discourse_analysis}

\begin{figure*}[t!]
    \centering
    \includegraphics[width=0.99\textwidth]{Figs/ARR_Oct_image.001.jpeg}
    \caption{Our proposed approach to faithfulness inconsistency detection utilizes findings from discourse analysis. We first conduct discourse analysis on parsed summary sentences (\S \ref{sec:summary_error_analysis}) and exploit the source document's discourse structure (\S \ref{sec:document_structure}). Motivated by the findings, our proposed approach is introduced in \S  \ref{sec:source_segment} and \S \ref{sec:reweight_algorithm}.  }
    \label{fig:discourse_inspired_detection}
\end{figure*}
% \vspace{-6mm}
\paragraph{Preliminaries}

Discourse analysis with Rhetorical Structure Theory (RST) is helpful for different downstream tasks, such as argument mining \cite{peldszus_stede_2016a, hewett-etal-2019-utility}, text simplification \cite{Zhong_Jiang_Xu_Li_2020},  AI-generated text detection \cite{kim2024threads}, and summarization \cite{Marcu1998tobuild, xu-etal-2020-discourse}. \textbf{RST} predicts tree structures on the grounds of underlying coherence relations
that are primarily defined in speaker intentions \cite{MANNTHOMPSON+1988}. 
The discourse tree comprises lower-level Elementary Discourse Units (EDUs), each corresponding to a phrase within a sentence. These units are then integrated into more complex structures, such as sentences and paragraphs, to form the full discourse tree. Discourse labels (i.e., elaboration, contrast, condition, etc.) are assigned as the relation between nodes. Additionally, a nuclearity attribute is assigned to every node of the discourse tree, aiming to encode the relative importance between the pairs of sub-trees (nucleus roughly implying primary importance and a satellite means supplemental).
%\footnote{We provide the complete list of discourse relations in Appendix \ref{appendix:discourse_relations}.} 

% \begin{figure}
%     \centering
%     \includegraphics[width=0.48\textwidth]{Figs/diversumm_error_loc.jpg}
%     \caption{Relative location of sentences that contain faithful errors within the summary in \textsc{DiverSumm} (This includes the GOV and AXV split). Sentences located in the latter half are more likely to contain errors.}
%     \label{fig:diversumm_location}
% \end{figure}



We first parse the summaries from the datasets as mentioned earlier in Section \ref{token_level_dataset} with an open-sourced DMRST model \cite{liu-etal-2021-dmrst}, following similar work which utilizes the same model for discourse parsing \cite{adams-etal-2023-generating,pu-etal-2023-incorporating, kim2024threads}. In the following paragraphs, we propose and verify multiple hypotheses that inspired our discourse-structure-aware factual inconsistency detection approach. Figure \ref{fig:discourse_inspired_detection} summarizes our findings in \S \ref{sec:summary_error_analysis} and \S \ref{sec:document_structure}.


\subsection{Discourse Analysis on Summary Errors}\label{sec:summary_error_analysis}
% \paragraph{Finding 1: Errors are more likely near the documents' end.}

% % \begin{figure}
% %     \centering
% %     \includegraphics[width=0.48\textwidth]{Figs/diversumm_error_depth.jpg}
% %     \caption{Relative depth of sentences in the RST tree with faithful errors in DiverSUMM.}
% %     \label{fig:diversumm_depth}
% % \end{figure}
% Figure \ref{fig:diversumm_location} illustrates the distribution of sentence locations of different types of errors within the summary from the annotated subset of \textsc{DiverSumm} (\textsc{DiverSumm-Sent)}. We can observe that factual inconsistency errors are more likely to appear in the latter part of the article, with grammatical errors appearing most often near the end. 
\begin{table}[]
\renewcommand{\arraystretch}{0.6}
\scriptsize

    \centering
    \setlength\columnsep{3pt}
    \begin{tabular}{c|c|c|c}
    
    \toprule
    \textbf{Error} & \multicolumn{3}{c}{\textbf{Discourse Subtree Depth}}\\
    \cmidrule{2-4}
    & -1 & 0  &  >= 1 \\
 &  (split link) &  (1 edu) & shallow/deep trees \\
 \midrule 
 GramE &  6\% & 28\% & 66\% \\
 \midrule  
  LinkE & 14\% & 23\% & 63\%\\
 \midrule  
  OutE & 15\% & 13\% & 72\%\\
  \midrule
   EntE & 11\% & 10\% & 79\%\\
 \midrule  
  PreE & 20\% & 13\% & 67\%\\
 \midrule  
  CorefE & 11\% & 0\% & 89\%\\
  \midrule 
 CircE &  8\% & 8\% & 84\% \\
 \midrule
 NoE & 8\% & 23\% & 69\% \\
 
 \bottomrule
    \end{tabular}
    \caption{The distribution depths of discourse subtrees of a sentence that are not factually consistent (depth of sub-tree) in \textsc{DiverSumm-Sent}. ``-1'' means the original sentence belongs to two sub-trees. Appendix \ref{sec:appendix_discourse_test} includes details of error types. }
    \label{tab: subtree_struct}
\end{table}

\paragraph{Finding 1:  Errors are located in sentences with dense  discourse tree (more EDUs)}
RST can capture the salience of a
sentence with respect to its role in the larger context. Prior work finds that the salience of a unit or sentence does not strictly follow the linear order of appearance in the document but is more indicative through its depth in the tree \cite{Zhong_Jiang_Xu_Li_2020}. We consider the depth of the current sentence in
the RST tree of the document (viewing each sentence as a
discourse unit). We also noted that, at times, the original summaries' sentences are broken into parts and span two discourse subtrees (i.e., a sentence covers EDUs 24-28, while the parsing tree's subtrees are ``22-25''', ``26-28''). In this case, we approximate the depth of the sentence by computing the square root of the absolute distance of min and max EDUs, i.e., in the above case, the depth is computed as $\sqrt{(28-24)}=2$.\footnote{We assume that the discourse tree is nearly binary, with each node having two children.}

We additionally studied the distribution of the tree structure of sentences with errors. The hypothesis is that several errors will likely appear in sentences with complex structures (more EDU units and dense trees). As shown in Table \ref{tab: subtree_struct}, sentences containing factual inconsistency errors are generally more complicated and cover multiple discourse units. It is worth noting that the case of ``-1'' means the sentence is deeply intervened with its neighboring sentences, and the discourse parser fails to segment it independently. One example is illustrated in the summary of Figure \ref{fig:discourse_inspired_detection}, where Sentence 3 (S3) contains three EDU segments, making it more complex than the other two sentences.

\paragraph{Finding 2: Errors are associated with the nuclearity and related discourse features}
We further analyze the distribution of nuclearity and different discourse features of sentences containing errors from the \textsc{DiverSumm-Sent} dataset. We observe that a greater number serve as satellites within the discourse relation (62\%) for sentences comprising a single Elementary Discourse Unit (EDU). 

\begin{table}[]
    \centering
    \small
    \begin{NiceTabular}{l|c|l}
    \toprule
    \textbf{RST features}     &  \textbf{t-stat} & \textbf{p-value}\\
    \midrule
      Ono penalty \cite{ono-etal-1994-abstract}    & 1.606 & 0.1089 \\
      Depth score \cite{Marcu1998tobuild} & \textbf{-9.084} &0.0000\\
    Promotion score \cite{Marcu1998tobuild} & -0.828 &0.4083\\
    \midrule
    %\multicolumn{3}{l}{\textit{Introduced in \cite{louis-etal-2010-discourse}}}\\
    % \midrule
    Normalized Ono penalty  & \textbf{2.160} &0.0314\\
      Normalized depth score& \textbf{-8.919} &0.0000\\
        Normalized promotion score & -0.303 & 0.7617\\
        
    \bottomrule
    \end{NiceTabular}
    \caption{Two-sided t-test of significant RST-based features comparing sentences with factual inconsistency errors to consistent ones in \textsc{DiverSumm-Sent}. We report the test statistics and significance levels. The original and normalized depth scores and the normalized penalty scores are significant (p-value <= 0.05). Fine-grained per error-type results are in Table \ref{tab:rst_result_all_errors} of Appendix \ref{sec:appendix_discourse_test}.}
    \label{tab:rst_feature_diversumm}
\end{table}

We calculated several discourse feature scores: the penalty score (Ono penalty) as defined in \citet{ono-etal-1994-abstract}, the maximum depth score (Depth score) \cite{Marcu1998tobuild}, and the promotion score \cite{Marcu1998tobuild}.\footnote{Details of feature scores are in Appendix \ref{appendix:disocurse_feature_details}.} The penalty score accounts for
the number of satellite nodes found on the path from
the tree's root to that EDU. The depth score is determined by the proximity of an EDU's highest promotion to the tree's root. The highest promotion refers to the closest node to the root, including the EDU within its promotion set. The promotion score quantifies the salience of an EDU based on how many levels it has been promoted through within the tree structure. We compute both unnormalized and normalized versions (with the max tree depth) for the above three scores. As shown in Table \ref{tab:rst_feature_diversumm}, we find significant differences in the distributions of depth score. We normalize the Ono penalty and depth score between factually consistent and inconsistent sentences and will include them in our proposed approach.

%We will include details on the score computation in Appendix \ref{appendix:ff}.



\subsection{Document Structure}\label{sec:document_structure}
 We further analyze the structure of parsed discourse trees for both documents and summaries of different datasets. We assume that the linguistic structure of discourse can change depending on factors such as the writing style, domain, and depth of reasoning of texts. To check whether the
structures are evenly branched or follow a more sequential pattern, we measure a document graph's average shortest path length \cite{kim2024threads}. The intuition is that linear or chain-like graphs tend to have shorter average shortest path lengths (ASPL), reflecting the linear pattern. Meanwhile, branched structures would have a longer ASPL, given the spread nature of nodes. As shown in Fig \ref{fig:aspl_fig}, for long document datasets (the last seven datasets), the source documents' ASPL is longer than the news articles such as CNN/DM and XSUM.\footnote{We exclude Multi-news and \textsc{LegalSumm}, as the former dataset's source text is composed of multiple news articles and the latter comes with split section structures, making the ASPL reporting less accurate.} In the meantime, longer summaries also carry evenly branched complex structures compared to short news summaries. While mainstream research segments long source texts into continuous chunks with limited window size, we argue that this disrupts the original structure of texts, leading to information loss.\footnote{See Appendix \ref{appendix:segmentation_examples} for examples.} We propose utilizing the tree structure and constructing the segments based on level traversals to preserve the high-level segmentation. 
%of the discourse tree
%We will discuss this further in later sections. 

\begin{figure}
    \centering
    \includegraphics[width=0.97\linewidth]{Figs/aspl.001.jpeg}
    \caption{Average shortest path length per dataset for document and summary discourse trees. We sort the dataset by the average length of the document, finding that longer document-summary (DOC, SUMM) pairs would be more branched, and their summaries are also complicated. AG, DS, LSV, and LE refer to \textsc{AggreFact FtSOTA}, \textsc{DiverSumm}, \textsc{LongSciVerify} and \textsc{LongEval} respectively.}
    \label{fig:aspl_fig}
\end{figure}
