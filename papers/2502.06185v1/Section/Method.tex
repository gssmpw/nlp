\section{StructScore}
In this section, we describe the \textsc{StructScore} framework. The lower right part of Figure \ref{fig:discourse_inspired_detection} presents motivations for each module.
\subsection{Tree-structure Inspired Weighting Algorithm}\label{sec:reweight_algorithm}
Prior work \cite{zha-etal-2023-alignscore, scire-etal-2024-fenice} computes the aggregated summary-level prediction on factual consistency score by picking the minimum sentence-level score or selecting the average. However, as indicated in Section \ref{sec:summary_error_analysis}, EDUs with different discourse relations and structures can be weighted differently. We thus propose to re-weigh the sentences based on the features of the discourse. 

First, we examine the sentence's nuclearity and the associated discourse features within the discourse tree. As found in Table \ref{tab:rst_feature_diversumm}, the normalized depth score, which utilizes the given node's nuclearity and the tree structure, is significantly different given the existence of factual inconsistency errors (p-value < 0.00001), where inconsistent sentences have a lower normalized depth score (Finding 2 in \S \ref{sec:summary_error_analysis}).\footnote{Among the three significant features, we use the normalized depth score to ensure consistent scaling. Our preliminary results also indicate that the normalized Ono penalty score did not enhance the dev set performance as much. } Based on this finding, we decided to increase the weight of the alignment score for sentences with lower depth scores within their parsed tree. Since NLI methods generate scores within a 0-1 range, we apply an exponent to appropriately scale these scores.
Let \( x_i \) be the computed normalized depth score of a summary sentence, \(s_i\) the original computed aligning score,  and \( \overline{x}_{1:j} \) the mean of all depth scores from \( x_1 \) to \( x_j \) in the summary with length j. The function to re-weight the aligning score \( f(s_i) \) can be defined as follows:
\[
f(s_i) = 
s_i^{1+ (\overline{x}_{1:j} - x_i)} 
\]
Secondly, observing that sentences that contain connective EDUs or have complicated discourse structures with more EDUs are more likely to contain errors (Finding 1 in \S \ref{sec:summary_error_analysis}), we propose scaling the score by selecting an appropriate exponent, given that the original score falls within the range of 0 to 1. We apply a tuning factor $\alpha$ on the discourse sub-tree height for the summary sentence $sent_i$: 
\[s_i^{*} = f(s_i)^{1+({height-subtree}(sent_i)*\alpha)}\] We conduct ablation studies on these two components in \S \ref{sec:abalation_diversumm}. We search for the best parameters on a held-out dev set of \textsc{DiverSumm} and keep the same across other datasets.




\subsection{Source Document Segmentation}\label{sec:source_segment}

We parse the original article with the RST parser and break the long documents into linear segments. This approach differs from prior work, which either applies a fixed window or selects a few context sentences surrounding a given source sentence. Motivated by findings from \S \ref{sec:document_structure}, we follow the below approach:
(1) If the parser fails, we will use the document structure (paragraph/sentence hierarchies) to group by the neighboring sentences. We then follow the naive chunking approach in \textsc{AlignScore} (window size 350) to prepare the input. 
(2) If the parsing is successful, we will extract the segmentation from the discourse tree up to level N. For instance, in the top-right of Figure \ref{fig:discourse_inspired_detection}, an original article has EDU segments (1-688), and the root of the RST tree is split into 1-648 and 649-688; we will adopt this segmentation. We apply the chunking approach outlined previously for segments that exceed the \textsc{AlignScore} model's context capacity. On the second level, we break (1-648) into (1-325) and (326-648), while the remainder are also broken into smaller chunks. Since the RST parser could break long sentences into multiple EDUs, we have additional post-processing to map the EDUs back to the source sentences.

