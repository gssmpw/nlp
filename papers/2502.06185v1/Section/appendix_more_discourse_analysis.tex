
\newpage
\clearpage
\section{Discourse Analyses}
\subsection{Short Summary Analysis}\label{appendix:short_analysis}
\begin{table}[h!]
\scriptsize
\setlength\columnsep{1pt}
\begin{NiceTabular}{l|ccc}

\toprule
\textbf{Dataset} & \textbf{Size} &  \textbf{Gran} & \textbf{Error Tag} \\
\midrule

\textsc{AGU}\_\textsc{Cliff} &  300 & word & intrin./extrin./other/wld. knowl.\\
\textsc{AGU}\_Goyal'22 & 150   & span & intrins./extrin./other \\
\bottomrule
\end{NiceTabular}
\caption{Statistics of Sent/Span-level factual inconsistency datasets \textsc{AggreFact-Unified} (AGU) \cite{tang-etal-2023-understanding}. We report the size of doc-summary pairs (Size), the granularity of annotation (Gran), and the error labels (Error Tag).  }\label{tab:token_label_short}
\end{table}
We also conduct a discourse analysis on \textsc{AggreFac-United} \cite{tang-etal-2023-understanding}, as shown in Table \ref{tab:token_label_short}. This dataset includes BART and Pegasus summaries from \textsc{Cliff} \cite{cao-wang-2021-cliff} and Goyal'21 \cite{goyal-durrett-2021-annotating}.\footnote{\textsc{AggreFact-Unified} (\textsc{AGU\_Cliff}) includes additional error types such as \textit{comments}, \textit{other errors: noise, grammar} and \textit{world knowledge} (wld. knowl.)}  In the Goyal22 split of AGGREFACT-UNITED,
a total of 61 errors were detected. Intrinsic errors
are found to appear more often in satellite EDUs (18/31) with the attribution relation. Regarding extrinsic errors, the nucleus EDUs take the majority. We further analyzed the CLIFF dataset \cite{cao-wang-2021-cliff}, where span-level annotations of faithful errors are available. Out of 600 sentences, the parser failed to parse 131 summaries, likely due to their short lengths and simplistic structures. Therefore, our analysis focused on the 469 summaries that were successfully parsed. We observed that Elementary Discourse Units (EDUs) containing errors are more likely to appear at the bottom of the discourse tree.  These findings are similar to the long summary analysis in \S \ref{sec:discourse_analysis}. %pecifically, the relations labeled as "Background" and "Elaboration" appear more frequently in instances of intrinsic errors, while "Attribution," "Background," and "Elaboration" are predominant in summaries with extrinsic and worldknowledge errors.




% \subsection{Discourse Relations in RST}\label{appendix:discourse_relations}
% We include the complete list of coarse-grained and fine-grained relation classes in the RST Discourse Treebank in Table \ref{tab:discourse_relations}, as summarized in \cite{feng_2015}.

\subsection{Discourse Features}
\label{appendix:disocurse_feature_details}

Following prior work \cite{louis-etal-2010-discourse}, we analyze the nucleus-satellite penalty score (Ono penalty) \cite{ono-etal-1994-abstract}, the maximum depth (Depth score) \cite{Marcu1998tobuild}, and the promotion-based score \cite{Marcu1998tobuild} for sentence level. The penalty/score for
a sentence is computed as the maximum of the
penalties/scores of its constituent EDUs.
For the normalized version, instead of following \citet{louis-etal-2010-discourse}, who normalized them by the number of words in the
document, we opt to divide the scores by the maximum depth of the discourse tree, which similarly alleviates the scores' dependencies on document length. Below, we provide one example demonstrating the computation of each score (borrowed from \citet{louis-etal-2010-discourse}) and will release our code for reproduction purposes.

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.9\linewidth]{Figs/discourse_example.jpg}
    \caption{RST for the example sentence, and the salient units (promotion
set) of each text span are shown above the horizontal line, which represents the span.The example is taken from \citet{louis-etal-2010-discourse}.}
    \label{fig:discourse_example}
\end{figure}

\subsubsection{Example}
Here, we re-utilize the example from \citet{louis-etal-2010-discourse}, which is part of the RSTDT \cite{carlson2002rst} in Figure \ref{fig:discourse_example}, which contains four EDUs.

\textit{1. [Mr. Watkins said] 2. [volume on Interprovincial’s system is down about 2\% since January] 3. [and is expected to
fall further,] 4. [making expansion unnecessary until perhaps
the mid-1990s.]} 
\paragraph{Nucleaus-Satellite Penalty (Ono Penalty)}\cite{ono-etal-1994-abstract}: The spans of individual EDUs are represented
at the leaves of the tree. At the root of the tree, the
span covers the entire text. The path from EDU 1
to the root contains one satellite node. It is, therefore, assigned a penalty of 1. Paths to the root from
all other EDUs involve only nucleus nodes; subsequently, these EDUs do not incur any penalty. Thus, the Ono Penalty scores for EDU 1 to 4 are [1, 0, 0, 0].

\paragraph{Maximum Depth Score}
Below we cite the original texts from \cite{louis-etal-2010-discourse}.
\begin{quote}


\citet{Marcu1998tobuild} proposed the method to utilize the nucleus-satellite distinction, rewarding nucleus status instead of penalizing the satellite. He introduced the notion of \textit{promotion set}, consisting of
salient/important units of a text span. The nucleus is denoted as the more salient unit in the full span of
a mono-nuclear relation (i.e., in Elaboration, the satellite unit is to elaborate on the key information of the nucleus. Thus, the latter is more salient). In a multinuclear relation,
all the nuclei are salient units of the larger span.

For example, in Figure \ref{fig:discourse_example}, EDUs 2 and 3 participate in a multinuclear (List) relation. As a result,
both EDUs 2 and 3 appear in the promotion set of
their combined span (2-3). The salient units (promotion
set) of each text span are shown above the horizontal line which represents the span. At the leaves,
salient units are the EDUs themselves.


For the purpose of identifying important content, units in the promotion sets of nodes close to
the root are hypothesized to be more important
than those at lower levels. The highest promotion of an EDU occurs at the node closest to the
root, which contains that EDU in its promotion set.
The depth of the tree from the highest promotion
is assigned as the score for that EDU. Hence, the
closer to the root an EDU is promoted, the better
its score. Since EDUs 2, 3 and 4 are promoted all
the way up to the root of the tree, the score assigned to them is equal to 4, the total depth of the
tree. EDU 1 receives a depth score of 3.
\end{quote}

Thus, the final maximum depth score based on the promotion set for EDUs 1-4 are [3, 4, 4, 4]. 

\paragraph{Promotion Score}
In the same example, while EDUs 2, 3, and 4 all have a depth score of 4, EDUs 2 and 3 are promoted to the root from a greater depth than EDU 4. To account for the difference, \citet{Marcu1998tobuild} further introduced the promotion score, which is a measure of the number
of levels over which an EDU is promoted. For instance, EDU 2 is promoted by three levels, while EDU 4 is promoted by two levels. Thus,
EDUs 2 and 3 receive a promotion score of 3, while the score of EDU 4 is only 2. EDU 1, given that it is never promoted received scores of 0. 


\paragraph{Discourse Tree Computation}
In Section \ref{sec:discourse_analysis} Table \ref{tab: subtree_struct}, we compute the tree depth as follows.
We use a string-matching system to construct a dictionary that aligns annotated sentences with EDU segments. For instance, in Figure \ref{fig:discourse_example}, the sentence is mapped to EDUs 1-4. We then compute the maximum depth of the discourse tree from the root node to the lowest leaf node, which would be 3 in this case. However, there may be cases where sentences are segmented into EDUs that are not gathered into a single node in the parsed discourse tree. In such instances, we employ the methods described in Section \ref{sec:discourse_analysis} to approximate the depth.

\section{LegalSumm Dataset}\label{appendix:legalsumm_detail}
We utilized a subset of the \textbf{CanLII Dataset} \cite{xu2021}, which consisted of 1,049 legal opinion documents with expert-written summaries.\footnote{Data obtained through an agreement with CanLII
\url{https://www.canlii.org/en/}}. We followed the setting from \citet{elaraby-etal-2024-adding}, where we consider the output of three different abstractive models in our annotation process: (1) \textbf{Finetuned LED-base} \cite{elaraby-litman-2022-arglegalsumm} which finetuned
the pre-trained longformer-encoder-decoder \cite{beltagy2020longformer} (LED) on the CanLII cases without additional information about the argument structure
of the document (2) \textbf{arg-LED-base}, which utilizes the LED model but includes the information about the argument units (Issues, Reasons, and Conclusions) in its training phase, and (3) \textbf{arg-aug-LED-base}, a model introduced in \citet{elaraby-etal-2023-towards} that can select a summary from multiple augmented versions of generated summaries based on its overlap with the input case’s predicted argument roles. 

\paragraph{Annotation Details}
We conducted evaluations with two voluntary legal experts from the research group, all of whom hold a J.D. degree and
possess at least four years of experience
in providing professional legal services. For each summary, the annotators are asked to select from four choices justifying the factual consistency of the model-generated summary with the reference summary and source article. They are also encouraged to provide free-text rationales justifying their selections.

To guarantee the quality of annotation, we conducted multiple
sessions with annotators to refine the guidelines and continuously monitor the agreements. Ultimately, the first author and the two annotators held in-person sessions to resolve label inconsistencies. The labels remained unresolved in two cases as the annotators identified differing yet reasonable interpretations of the instructions.  We thus retain the average scores as is. To distinguish summaries with severe or moderate factual inconsistencies from those without, we computed the average of the two annotators' ratings and rounded based on a threshold of 2.
The annotation guideline is included in Figure \ref{fig:annotation_guideline}.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{Figs/annotation.001.jpeg}
    \caption{The annotation interface for LegalSumm. The left panel displays the instructions and the content to be annotated. Annotators are then prompted to select one of four options, as shown in the right panel.}
    \label{fig:annotation_guideline}
\end{figure*}

\section{Discourse Analysis on Fine-grained Error Types}\label{sec:appendix_discourse_test}
\begin{table*}[h!]
    \centering
    \scriptsize
    \begin{NiceTabular}{l|c|c|c|c|c|c|c|c}
    \toprule
    \textbf{RST features}     & \textbf{GramE} & \textbf{LinkE} & \textbf{OutE} & \textbf{EntE} & \textbf{PredE} & \textbf{CorefE} & \textbf{CircE} & \textbf{ALL Errors}\\
    Count & (83) & (35) & (48) & (117) & (15) & (9) & (13) & (320)\\
    
    \midrule
      Ono penalty   &  -1.166\textsuperscript{}& 1.855\textsuperscript{}& 0.621\textsuperscript{}& 1.647\textsuperscript{}& 0.730\textsuperscript{}& 0.215\textsuperscript{}& 1.627\textsuperscript{}& 1.606 (0.1089)\\
      Depth score& -5.218\textsuperscript{**}& -7.381\textsuperscript{**}& -4.628\textsuperscript{**}& -3.252\textsuperscript{**}& -2.002\textsuperscript{}& 0.214\textsuperscript{}& -0.565\textsuperscript{}& -8.249 (0.0000)\\
    Promotion score  & -6.519\textsuperscript{**}& -0.971\textsuperscript{}& -0.440\textsuperscript{}& 1.734\textsuperscript{}& -0.195\textsuperscript{}& 2.613\textsuperscript{*}& 0.629\textsuperscript{}& -0.828 (0.4083)\\
    \midrule
    Normalized penalty  &  -1.742\textsuperscript{}& 3.051\textsuperscript{**}& 0.695\textsuperscript{}& 1.990\textsuperscript{*}& 0.673\textsuperscript{}& -0.002\textsuperscript{}& 0.493\textsuperscript{}& 2.160 (0.0314)\\
      Normalized depth score & -6.689\textsuperscript{**}& -6.043\textsuperscript{**}& -4.823\textsuperscript{**}& -3.307\textsuperscript{**}& -1.731\textsuperscript{}& -0.153\textsuperscript{}& -1.986\textsuperscript{}& -9.084 (0.0000)\\
        Normalized promotion score &  -5.754\textsuperscript{**}& 0.487\textsuperscript{}& -0.322\textsuperscript{}& 1.796\textsuperscript{}& -0.087\textsuperscript{}& 2.206\textsuperscript{}& -0.218\textsuperscript{}& -0.303 (0.7617)  \\
    \bottomrule
      
    \end{NiceTabular}
    \caption{Two-sided t-test statistic of significant RST-based
features comparing unfaithful sentences to faithful
ones in \textsc{DiverSumm} annotated split. We report the
test statistics and significance levels. For fine-grained errors, we report the significant level in * (0.01 <= p-value <=0.05) and ** (p-value <=0.01). For All errors, we report the p-value in parenthesis.}
    \label{tab:rst_result_all_errors}
\end{table*}



\paragraph{Error Types} Relation
Error (PreE) is when the predicate in a summary sentence is inconsistent with respect to the document. Entity Error (EntE) is when the primary
arguments of the predicate are incorrect. Circumstance Error (CircE) is when the predicate’s circumstantial information (i.e., name or time) is wrong.
Co-reference error (CorefE) is when there is a pronoun or reference with an incorrect or non-existing
antecedent. Discourse Link Error (LinkE) is when
multiple sentences are incorrectly linked. Out of
Article Error (OutE) is when the piece of summary
contains information not present in the document.
Grammatical Error (GramE) indicates the existence
of unreadable sentences due to grammatical errors.


% \paragraph{Error Examples}

\paragraph{Fine-grained Error Analysis} In Table \ref{tab:rst_result_all_errors}, we demonstrate the breakdowns of fine-grained error types and report the t-test results on different discourse features. 


    



\section{Example of Segmentation Failures}\label{appendix:segmentation_examples}
This section includes one example of the \textsc{AlignScore}'s chunking method that failed to preserve the document structure, while our discourse-inspired chunk addresses it.

For example, as shown in Figure \ref{fig:sub1}, the original document contains two consecutive sentences: "To determine the extent ..." and "To develop the SMS" (highlighted in the orange box). These sentences are meant to be read together and should not be separated.  However, the default chunking approach in \textsc{AlignScore} and \textsc{MiniCheck} breaks this continuity by placing them in two separate chunks, given the former chunk is large enough. On the contrary, our approach maintains the structural integrity of the documents, keeping the sentences connected as intended. Similarly, in Figure \ref{fig:sub2}, the conclusion section is separated into two chunks by the default chunking approach, while our method maintains them in a single chunk.

\begin{figure*}[]
\centering
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figs/segmentation_example.001.jpeg}
        \caption{Example from GovReport of \textsc{DiverSumm}.}\label{fig:sub1}
    \end{subfigure}%
    \caption{Example of segmentation failures, left is the output of chunking method used in \textsc{AlignScore} and \textsc{MiniCheck}, right is the segments produced by our segmentation method.}\label{fig:segmentation_fault1}
\end{figure*}

\begin{figure*}[]
    
    \begin{subfigure}[b]{1\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{Figs/segmentation_example2.001.jpeg}
        \caption{Example from ArXiv of \textsc{DiverSumm}.}\label{fig:sub2}
    \end{subfigure}
    \caption{Example of segmentation failures, left is the output of chunking method used in \textsc{AlignScore} and \textsc{MiniCheck}, right is the segments produced by our segmentation method.}\label{fig:segmentation_fault2}
\end{figure*}



