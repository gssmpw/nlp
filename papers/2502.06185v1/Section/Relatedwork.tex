\section{Related Work}

\paragraph{Factual Inconsistency Detection in Long Document Summarization}

%Despite the numerous datasets released in the news domain \cite{kryscinski-etal-2020-evaluating, cao-wang-2021-cliff, goyal-durrett-2021-annotating, laban2022summac, tang-etal-2023-understanding}, 
Research on automatic factual inconsistency evaluation metrics and resources for long document summarization is limited.
Recently, \citet{Koh2022AnES} surveyed the progress of long document summarization evaluation and called for better metrics and corpora to evaluate long document summaries. \citet{koh-etal-2022-far} released annotated model-generated summaries assessing factual consistency at the
\textbf{sentence} and \textbf{summary} levels for GovReport \cite{huang-etal-2021-efficient} and arXiv \cite{cohan-etal-2018-discourse}. Furthermore, \citet{bishop-etal-2024-longdocfactscore-evaluating} and \citet{zhang-etal-2024-fine} introduced benchmarks of \textsc{LongSciVerify} and \textsc{DiverSumm} that cover diverse domains respectively, and further proposed different frameworks to utilize the context of source sentences for evaluating the factual consistency of generated summaries. However, their approaches relied on extracting context through computing similarities with the summary sentence. The summary-level score is a simple average of all sentence-level predictions. \textit{Our work analyzed a subset of \textsc{DiverSumm} and \textsc{AggreFact} \cite{tang-etal-2023-understanding} that have sentence-level factual inconsistency types and introduced a generalizable approach to better detect such inconsistency errors across domains.}


% \paragraph{Inconsistency Detection with Source Selection and Atomic Claim Extraction} 



\paragraph{Aggregation of Sentence-level Evaluations}
Text summaries are usually composed of multiple sentences. Most factual inconsistency evaluation metrics first compute the sentence-level scores for individual summaries, then aggregate them by either \textbf{soft aggregation} in computing the 
\textbf{unweighted-average} \cite{ zha-etal-2023-alignscore, glover-etal-2022-revisiting,scire-etal-2024-fenice,zhang-etal-2024-fine} or  \textbf{hard aggregation} with the minimum score \cite{ schuster-etal-2022-stretching,yang2024fizz}. ver, these approaches were primarily validated on older benchmarks, consisting of shorter texts (a few hundred input words and summaries of 2-3 sentences). There lacks a systematic study in the context of long document summarization. \textit{Our work dives into the discourse structure of system-generated summaries with span/sentence-level factuality annotations. We introduce a discourse-inspired re-weighting algorithm to calibrate the scores.}
\vspace{-1mm}

\paragraph{Discourse-assisted Text Summarization} Discourse factors have been known to play an important role in the summarization task \cite{ono-etal-1994-abstract, Marcu1998tobuild, kikuchi-etal-2014-single, xu-etal-2020-discourse, hewett-stede-2022-extractive, pu-etal-2023-incorporating}. \citet{louis-etal-2010-discourse} conducted comprehensive experiments to examine the power of different discourse features for context selection. We carry a similar analysis but focus on summary sentences that contain factual inconsistency errors. On adjusting the weight of EDUs, \citet{huber-etal-2021-w} proposed a weighted RST style discourse framework that derives the discourse units' continuous weights from auxiliary summarization task \cite{xiao-etal-2021-predicting}. Differently, our re-weighting algorithm is built on top of the trained parser's parsed discourse tree and applies to the final aggregation of scores. \textit{To the best of our knowledge, our work is the first that studies the connections between RST discourse structure and the factual consistency of model-generated summaries.}
