\section{Experimental Details}
\newcommand{\dashrule}[1][black]{%
  \color{#1}\rule[\dimexpr.5ex-.2pt]{1pt}{.4pt}\xleaders\hbox{\rule{1pt}{0pt}\rule[\dimexpr.5ex-.2pt]{3pt}{.4pt}}\hfill\kern0pt%
}

We adapt mainstream evaluation setups for each benchmark. For \textsc{DiverSumm}, we apply an 80/20 test/dev split by stratifying the labels for each subtask. For \textsc{AggreFact}, we use their released val/test split. For \textsc{LongSciVerify}, \textsc{LongEval} and \textsc{LegalSumm}, we use them as test sets.

\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\newcolumntype{?}{!{\vrule width 1pt}}
\begin{table*}[t!]
% \small
\scriptsize
\centering
\setlength{\tabcolsep}{3pt}
% \setlength{\tabcolsep}{6pt}

\begin{tabular}{llll?llllc|l?ll?c|c}
 \toprule
\multirow{2}{*}{\shortstack{\textbf{ID} }} & \multirow{2}{*}{\shortstack{\textbf{Evaluation Model} }} & \multicolumn{2}{c}{\textsc{\textbf{AggreFact}}} & \multicolumn{6}{c}{\textsc{\textbf{DiverSumm}}} &  \multicolumn{2}{c}{\textsc{\textbf{LSV}}} & \textsc{\textbf{LongEval}} & \textsc{\textbf{LegalS}}\\
\cmidrule(lr){3-4}\cmidrule{5-10} \cmidrule{11-14} 

&& XSM\textsubscript{AG} & CND\textsubscript{AG} &MNW  & QMS  & GOV 
 & AXV    & CSM & \textit{Macro-} & PUB & AXV & PUB &  \\
 & evaluation metric & \multicolumn{2}{c}{\textit{AUC}}  & \multicolumn{5}{c}{\textit{AUC}} & \textit{AVG} & \multicolumn{2}{c}{\textit{Kendal's $\tau$}} & \textit{Kendal's $\tau$} & AUC\\
& avg src. len & 360.54 & 518.85 & 669.20 & 1138.72   & 2008.16 & 4406.99 & 4612.40 & -- & 3776.80 & 6236.40 & 3158.35 & 2873.87  \\
% \midrule



\midrule

\multicolumn{12}{l}{\textit{\textbf{Baselines}}}\\
\midrule
% \RowStyle{\color{gray}}
\textbf{1} & \textbf{\textsc{LongDocFactScore}} & 50.47 &   65.27& \cellcolor{green!10}{61.20} & 40.69 & 83.52 & 65.36 & 60.06 & 62.17 &\cellcolor{green!10}{61.0}  & \cellcolor{green!10}{61.0} & 29.0 & 60.19 \\


\textbf{2} & \textbf{\textsc{MiniCheck-FT5}} & 75.04 &  72.62 & 48.68 & 45.31 & 70.26 & 61.77 & 52.93 & 55.79& 26.5 & 38.1 & 17.4 & 61.33\\


\textbf{3} &\textbf{GPT4o} & 75.36 & 70.47 &51.11 &\cellcolor{green!10}{70.22} & {86.81} & 67.78 & 61.53 & 67.49& 54.7 & 51.8 & \underline{51.2} & 67.71\\

\textbf{4} &\textbf{BeSpoke-MC-7B} & \cellcolor{green!10}{83.56} & 71.38  & 55.38 & \underline{65.42} &  82.83 & 75.07 & 63.43 & \underline{68.42} & 55.1 & \underline{57.9} & \cellcolor{green!10}{58.1} & 55.81 \\ 
\midrule

\multicolumn{12}{l}{\textit{{Apply our approach with different \textbf{baselines}(\textit{$\uparrow$} means improved the performance compared to the baseline with significance.)}}}\\

\midrule
% \RowStyle{\color{gray}}
\textbf{5} & \textbf{\textsc{AlignScore}} & {75.66} & 69.50 & 46.74 & 56.48 & {87.02} & 77.46 & 61.03 & 65.75 & 54.9 & 53.9 & 36.9 & {73.52}\\

 6 &   \hspace{3mm}+ re-weighting  & 75.67 & 69.20 &45.33 & 53.95 & 87.29$\uparrow$ & 81.15$\uparrow$ & 60.55 & 65.65 & 53.0 &  54.3$\uparrow$ & 34.8 & \cellcolor{green!10}{76.57}$\uparrow$\\
 \cmidrule{2-14}
 7 &\hspace{3mm}\textsc{+ Lv1 segment} & 76.23$\uparrow$ & 69.25\textsuperscript{$\dagger$} & 45.86\textsuperscript{$\dagger$}  & {61.25}$\uparrow$   & {86.74}\textsuperscript{$\dagger$} & 79.47$\uparrow$ & \underline{64.15}$\uparrow$ & 67.49$\uparrow$  & 51.9 & 52.8 & 43.6$\uparrow$ & 59.43\\
8& \hspace{3mm}\textsc{StructS-Lv1} & 76.20$\uparrow$ & 69.03 & 46.21\textsuperscript{$\dagger$}  & 60.06$\uparrow$ & 86.04  & {82.78}$\uparrow$   & \cellcolor{green!10}{64.47}$\uparrow$  & 67.91$\uparrow$  &  50.4 & 53.9\textsuperscript{$\dagger$} & 43.4$\uparrow$ & 59.81 \\
\cmidrule{2-14}

9& \hspace{3mm}\textsc{+ Lv2 segment}  & 74.27 & 70.30$\uparrow$  & 46.03\textsuperscript{$\dagger$}  & 55.74  & 85.10  & 76.79  & 63.11$\uparrow$  & 65.35 &  58.1$\uparrow$ & 51.1 & {43.9}$\uparrow$ & 67.05\\
10& \hspace{3mm}\textsc{StructS-Lv2}  &  74.28 & 69.85$\uparrow$&  45.33 & 51.86 & 85.65 & 80.00$\uparrow$ & 63.59$\uparrow$ & 65.29& 55.3$\uparrow$ & 54.1$\uparrow$ &  43.7$\uparrow$  & 64.00\\
 
\midrule
\midrule
% \textsc{InFusE}

% \RowStyle{\color{gray}}
\textbf{11}& \textbf{\textsc{MC-FT5 (sent)}} &  {79.62} & 70.95 & \underline{57.67} & 60.66&  83.24 & 78.66 & 59.74 & 67.99 & 55.7 & 52.7 & 30.2 & 61.14\\
12&\hspace{3mm}+ re-weighting & \underline{79.73} & 70.76\textsuperscript{$\dagger$} & 56.79 & 60.36\textsuperscript{$\dagger$}  & 84.75$\uparrow$ & 79.38$\uparrow$ & 60.06$\uparrow$ & {68.27}$\uparrow$  & 52.8 & 55.1$\uparrow$ & 31.4$\uparrow$  & 59.81\\
\cmidrule{2-14}
13&\hspace{3mm}\textsc{+ Lv1 segment} & 77.84 & \cellcolor{green!10}{73.48$\uparrow$} & 44.80 & 61.10$\uparrow$ & \underline{87.50}$\uparrow$ &\underline{85.22}$\uparrow$ & 63.59$\uparrow$ & \cellcolor{green!10}68.44$\uparrow$   & 57.5$\uparrow$ & 51.4 &  33.0$\uparrow$ & 68.95$\uparrow$ \\
14&\hspace{3mm}\textsc{StructS-Lv1} & 76.75 & \cellcolor{green!10}{73.40$\uparrow$}& 38.45 & 60.66\textsuperscript{$\dagger$}  & \cellcolor{green!10}88.05$\uparrow$ & \cellcolor{green!10}{86.32}$\uparrow$ & 63.11$\uparrow$ & 67.31 &  56.2$\uparrow$ & 53.8$\uparrow$ & 30.7$\uparrow$ & 72.57$\uparrow$\\
\cmidrule{2-14}
15&\hspace{3mm}\textsc{+ Lv2 segment} & 73.70&  72.30$\uparrow$ & 47.80 & 57.53 & 86.26$\uparrow$& 83.73$\uparrow$ & 62.07$\uparrow$ & 67.48 & 56.0$\uparrow$ & 52.9$\uparrow$&  35.6$\uparrow$ & 72.57$\uparrow$\\
16&\hspace{3mm}\textsc{StructS-Lv2} & 71.31 & 72.30$\uparrow$ & 41.27 & 59.02 & 87.16$\uparrow$ & 84.78$\uparrow$ & 61.75$\uparrow$ & 66.80 & 53.4 & 54.2$\uparrow$ & 33.0$\uparrow$ & \underline{73.71}$\uparrow$\\
\midrule 
\midrule
% \RowStyle{\color{gray}}
\textbf{17}&\textbf{\textsc{InFusE}} &68.48 & {72.52} & 54.14 & 39.64 & 84.41 & 68.13 & 57.82 & 60.83 &  \underline{59.4} & 55.9 & 36.9 & 63.43\\
18&+ re-weighting& 67.30 & {72.37} & 53.44 & 40.54$\uparrow$ & 84.68$\uparrow$ & 74.31$\uparrow$ & 59.82$\uparrow$ & 62.56$\uparrow$ &  58.3 & {56.3}$\uparrow$ &  34.6 & 66.29$\uparrow$\\

\bottomrule

\end{tabular}
\caption{Results for all summarization tasks in \textsc{AggreFact-FtSOTA} (\textsc{AggreFact}), \textsc{DiverSumm}, \textsc{LongSciVerify} (LSV), \textsc{LongEval} and \textsc{LegalSumm} (LegalS).  In \textsc{DiverSumm},  CSM, MNW, QMS, AXV, and GOV refer to ChemSum, MultiNews, QMSUM, ArXiv, and
GovReport. We also report the macro-average of \textsc{DiverSumm} AUC. We highlight the \colorbox{green!10}{best} performed approach where multiple greens indicate systems indistinguishable from the best
according to a paired bootstrap test with p-value < 0.05, and the \underline{second-best} system for each column. The seven baseline models are \textbf{bolded}. Cells with \textsuperscript{$\dagger$} mean the result is {indistinguishable} from the raw baseline according to the bootstrap test. We report the average of 3 runs for GPT4o, given the randomness in LLM inference. 
%We highlight highest scores and scores significantly different from FULLDOC, SUMMACvariants and SENTLI approaches (at p < .05). 
%For \textsc{AggreFact}, we report the overall ROCAUC on XSum and CNN/DM, respectively.
}\label{tab:aggrefact_diversum_res}
\end{table*}

\paragraph{Baselines}
One of our baselines is \textbf{\textsc{AlignScore}} \cite{zha-etal-2023-alignscore}, an NLI-based metric that computes the aggregated inference score between a source article and generated summaries. We included \textbf{\textsc{Infuse}} \cite{zhang-etal-2024-fine}, which sets the SOTA on \textsc{DiverSumm},  \textbf{\textsc{MiniCheck FT5}} (MiniCheck-FlanT5 checkpoints) \cite{tang2024minicheck} that is a best-performing non-LLM fact-checker over multiple benchmarks, and \textbf{\textsc{LongDocFactScore}} \cite{bishop-etal-2024-longdocfactscore-evaluating} which claimed to work well on factuality validation of lengthy scientific article summaries. Our experiment notes that \textsc{MiniCheck} did not work well over long summaries due to its design objectives of short-statement fact-checking. We thus introduce \textbf{\textsc{MC-FT5 (SENT)}}, which computes the individual summary sentences' scores using \textsc{MiniCheck} and reports their average as the final summary score. We additionally include the \textbf{GPT4o} \cite{openai2024gpt4technicalreport} as the LLM fact-checker, using a prompt adopted from \citet{tang2024minicheck} (see Table \ref{tab:gpt4o_prompt} in Appendix \ref{appendix:implementation_details}). Lastly, we include \textbf{Llama-3.1-BeSpoke-MiniCheck-7B (BeSpoke-MC-7B)}\footnote{\url{https://huggingface.co/bespokelabs/Bespoke-MiniCheck-7B}}, the SOTA fact-checking model on the LLM-AggreFact benchmark \cite{tang2024minicheck}. Unless otherwise noted, we reran the baseline models on our datasets using the original authors' released code and checkpoints. Implementation details are provided in Appendix \ref{appendix:implementation_details}.
%\vspace{-3mm}
\paragraph{Our Approach}

We re-utilized  baseline models to compute the scores between context chunks and summary sentences, including \textsc{AlignScore} \cite{zha-etal-2023-alignscore}, \textsc{MiniCheck-FT5} (SENT) and \textsc{InfUsE} \cite{zhang-etal-2024-fine}, and experimented with below settings to apply our proposed approaches:
\setlist{nolistsep}
\begin{itemize}
\itemsep0em 
    \item + re-weighting: we apply the discourse-inspired re-weighting algorithm to adjust the sentence-level scores. We tune the factor $\alpha$ on height-subtree weighting as 1 over the validation set of \textsc{DiverSumm} and apply it to other benchmark datasets.
    \item + LvN \textsc{Segment}: Instead of using the default chunking approach, we segmented the source documents with the algorithms introduced in Sec. \ref{sec:source_segment} with different levels of granularity. 
    \item \textsc{StructS}-LvN: Combining top two methods.
\end{itemize}

The reweighting and segmentation can not be applied to \textsc{LongDocFactScore}, as it produced negative scores on all enumeration of source-target sentence pairs, which does not utilize the structural information. \textsc{InfUsE} utilizes the ranked list of entailment scores for all document sentences associated with each summary sentence. Thus, the segmentation approach does not affect.  
\vspace{-2.5mm}
\paragraph{Evaluation Metrics}

For experiments with {\textsc{AggreFact-FtSOTA}, \textsc{DiverSumm}} and \textsc{LegalSumm}, following \citet{laban2022summac, zhang-etal-2024-fine}, we adopt ROCAUC which measures classification
performance with varied thresholds as our evaluation metric.
On \textsc{LongSciVerify} and \textsc{LongEval}, we report Kendall's Tau $\tau$, following \citet{bishop-etal-2024-longdocfactscore-evaluating}.

\section{Results}\label{sec:result}
% \subsection{Factual Inconsistency Evaluation}\label{sec:main_result}
\textbf{Overall Performance} Table \ref{tab:aggrefact_diversum_res} presents our main results with detailed setups. Overall,  our proposed approach (with different combinations of re-weighting and segmentation settings) achieves the best or second best across \textsc{AggreFact}, most of \textsc{DiverSumm} and \textsc{LegalSumm} (\textsc{LegalS}).
Compared to top-performed LLM-based models (rows 3,4), our approach outperforms in 7 out of 11 datasets, with significant improvements on GOV, AXV, CSM, and \textsc{LegalSumm}.\footnote{More discussions on strong baselines in Appendix \ref{appendix:more_discussion}.}
The rest of the section addresses the following research questions: \textbf{RQ1:} Can the re-weighting algorithm help improve the models' performance?  \textbf{RQ2}: How does source document segmentation impact factual inconsistency detection? \textbf{RQ3}: How does combining both in \textsc{StructScore} perform?
\vspace{-1mm}
\paragraph{RQ1.}\label{sec:abalation_diversumm} \textit{We observe that the re-weighting algorithm improves prediction performance on different baselines (rows 5-6, 11-12, 17-18).} For long source documents, the re-weighting approach consistently improves or closely matches GOV, AXV, CSM splits in \textsc{DiverSumm} and the AXV split in \textsc{LongSciVerify} (LSV-AXV) and \textsc{LegalS} performance. Noticeably, \textsc{AlignScore} with reweighting scored the best on LegalS. On the other hand,  for both XSM and CND in \textsc{AggreFact-FtSOTA}, the re-weighting algorithm does not help much. We posit that the short summary length (1-3 sentences) has minimally structured information, so the scores will not change much. For MNW and QMS, the short summaries in QMS (averaging 3 sentences) reduce the effectiveness of the re-weighting algorithm. Moreover, MNW's non-factual sentences often receive high prediction scores, which our re-weighting approach tends to amplify, leading to a drop in performance. We also observe a slight performance drop on LSV-PUB and \textsc{LongEval-PUB} for \textsc{AlignScore} and \textsc{InfUsE}, potentially due to the different document structure of scientific articles from the medical domain. These observations also suggest potential future work for a dynamic weighting algorithm based on the document structure and domain knowledge.
In Table \ref{tab:diversumm_ablation}, we ablate the two discourse factors from the re-weighting algorithm with our best baseline MC-FT5 (SENT) on a subset of long datasets, noticing both features are helpful, and the improvement in adding subtree height is greater.\footnote{We include a more complete table in Appendix \ref{appendix:ablation_study}.}
\begin{table}[ht!]
\scriptsize
\centering
% \setlength{\tabcolsep}{1pt}
\begin{NiceTabular}{lcccc}
 \toprule
\multirow{1}{*}{\shortstack{\textbf{Model} }}
 &   \textbf{GOV}  & \textbf{AXV}  & \textbf{CSM} & \textbf{LSV-AXV} \\
% avg src. len &  4612.4 & 1138.7 & 4407.0 & 2008.2 & 669.2 \\
\midrule


{MC-FT5 (SENT)} & 83.24 & 78.66 & 59.74 & 52.73 \\

{\hspace{3mm}\textit{+ subtree height}} & 84.55 & 79.09 & 60.55 & 55.08 \\
{\hspace{3mm}\textit{+ depth score}} & 83.65 & 78.90 & 59.90 & 53.80 \\
 re-weighting  &  84.75 & 79.38 & 60.06& 55.08 \\


\bottomrule
\end{NiceTabular}
\caption{Ablation results on a subset of datasets from \textsc{DiverSumm} and \textsc{LongSciVerify}, the top and bottom rows are rows 11 and 12 in Table \ref{tab:aggrefact_diversum_res}.}\label{tab:diversumm_ablation}
\end{table}

\vspace{-1mm}
\paragraph{RQ2.} \textit{Applying document and discourse-structure-inspired approaches enhances performance across different baselines on long document summarization tasks.} We start by applying the level-1 and level-2 segmentation to preserve the document structures while segmenting at higher levels. For example, MC-FT5 (SENT) with \textsc{Lv1 Segment} (row 13) obtains the highest macro-average AUC on \textsc{DiverSumm}, a trend also observed with \textsc{ALignScore}. Specifically, comparing row 11 and row 13, the Lv1 \textsc{Segment} improved the model's performance on 7 of 8 long datasets from QMS to \textsc{LegalS} (i.e. 78.66 -> 85.22  and 83.24 -> 87.50  on AXV and GOV). However, the effect of fine-grained segmentation can vary depending on the document's length and structure. For instance, \textsc{AlignScore} in row 9 with Lv2 segment obtained better performance than Lv1 on LSV-PUB but was worse on QMS.
%We also observe that \textsc{Lv2 Segment} can lead to slight performance declines on multiple long document summarization datasets compared to \textsc{Lv1 Segment}, such as GOV, AXV and LSV-PUB for MC-FT5 (SENT) (row 14 vs. row 12),  as well as shorter datasets like XSM in \textsc{AggreFact-FtSOTA}) (74.27 in row 8 vs. 75.66 in row 4 and 73.70 vs. 79.62 in rows 14 and 10, respectively) for both models. 
\vspace{-1mm}
\paragraph{RQ3.} \textit{Combining both approaches is not universally beneficial across all scenarios}. When both individual approaches contribute positively, the combined \textsc{StructS} generally achieves better performance, as seen in row 8 on AXV, CSM, and row 14 on AXV. However, when one component causes a performance drop, combining both often leads to weaker overall performance than the stronger component alone. For instance, on GOV, row 8 performs worse than row 5, likely due to the segmentation in row 7, making the model less accurate. Similarly, row 14 performs slightly better than row 11 on LSV-PUB, but row 13's improvement does not translate into better performance gains when combined with row 12.  Differences in evaluation metrics (AUC vs. correlation) and dataset sizes may also have influenced these outcomes (i.e., row 14 does not improve much on \textsc{LongEval}-PUB while rows 12 and 13 have larger gains).

% We find that combining both re-weighting and source segmentation can further enhance the performance when neither approach hurts the performance much. For example,  \textsc{StructS-Lv1} on \textsc{AlignScore} obtains the best macro-average performance on \textsc{DiverSumm} compared to all its variants (rows 4-9), particularly on AXV and CSM tasks. This improvement persisted in rows 10-13. However, we observed that \textsc{StructS} occasionally underperformed compared to re-weighting and segmentation-only approaches on \textsc{LSV} and \textsc{LongEval}, as well as on GOV and \textsc{AggreFact}. In some cases, low performance in one component was amplified, leading to worse overall results when combined (e.g., on QMS, rows 5, 6, and 7 are lower than row 4). Differences in evaluation metrics (AUC vs. correlation) and dataset sizes may also have influenced these outcomes.
%We attribute this to our re-weighting algorithm being fine-tuned on \textsc{DiverSumm}’s development split, which has a different structure.





% \begin{table}[]
% \scriptsize
% \centering
% % \setlength{\tabcolsep}{1pt}
% \begin{NiceTabular}{l|ccc}
%  \toprule
% \textbf{Evaluation Model} & \textbf{LSV} & \textbf{LSV} & \textbf{LE} \\
% & -PUB & -AXV & -PUB \\
% avg src. len & 6515 & 3209 & 3193\\
% \midrule
% \multicolumn{4}{l}{\textit{Baselines}}\\
% \midrule
% LongDocFACTScore & \colorbox{green!10}{0.61} & \colorbox{green!10}{0.61} & 0.29 \\
% \textsc{InfUsE} & \underline{0.594} &0.559  & 0.369\\ 

% \textsc{MiniCheck-FT5}   & 0.174&0.381& 0.265 \\

% \textsc{MC-FT5 (sent)} & 0.557 & 0.527 & 0.302 \\
% \textsc{AlignScore} &  0.549 &  0.539 & 0.369\\
% GPT4o & 0.547 & 0.518 & \colorbox{green!10}{0.512}\\
% \midrule
% \multicolumn{4}{l}{\textit{Apply Discourse-inspired Re-weighting}}\\
% \midrule
% \textsc{InfUsE}-weighted & 0.587& \underline{0.574}& 0.343\\ 
% \textsc{AlignScore}-weighted & & 0.543 &   \\ 
% \textsc{MC-FT5 (sexnt)}-weighted &0.556 & 0.543 & 0.304 \\
% \midrule 
% \multicolumn{4}{l}{\textit{Ours with AlignScore as backbone}}\\
% \midrule 
% % \RowStyle{\color{gray}}

%  \multicolumn{4}{@{}c@{}}{\makebox[\linewidth]{\dashrule[black]}} \\
%   \multicolumn{4}{l}{\textit{{w/ Source Segmentation}}}\\
  
% \textsc{Segment}-LV.1 & 0.519 & {0.528} & {0.436} \\
% \textsc{Segment}-LV.2 & {0.581} & 0.511 &  {0.439} \\
% \textsc{Segment}-LV.3 & 0.549 &  0.513 & 0.423 \\
% \midrule
% \multicolumn{3}{l}{\textit{{w/ Source Segmentation + Discourse-inspired Re-weighting}}}\\
% StructS-LV.1 & 0.515 & {0.543} & {0.439} \\
% StructS-LV.2 & 0.568 & 0.526 &  \underline{0.444} \\
% StructS-LV.3 & 0.547 &  0.520  & 0.434 \\



% % \midrule

% \bottomrule
% \end{NiceTabular}
% \caption{Kendall’s Tau ($\tau$) correlations between the human factual consistency annotations and automatic
% metrics for LongSciVerify datasets (\textbf{LSV-PubMed/ArXiv}) and LongEval PubMed (LE PUB) dataset. We highlight the \colorbox{green!10}{best}
% performance for each dataset the second-best is \underline{underlined}.}\label{tab:longdocfactscore}
% \end{table}







\colorlet{green30}{green!20}
\colorlet{green50}{green!30}
\colorlet{green70}{green!40}
\colorlet{red30}{red!20}
\colorlet{red50}{red!30}
\colorlet{red70}{red!40}




% \subsection{Computational Cost Comparison}
% Following \citet{tang2024minicheck}, We compare the computational cost of different approaches and LLMs on our test split of \textsc{DiverSumm}. For most models, we
% use our hardware and convert the prediction
% time on our GPUs to the equivalent cost of using
% cloud computing services (see Appendix \ref{appendix:machine_config} for
% details). For GPT4o, we compute the costs of corresponding API calls. Results
% are shown in Table \ref{tab:computation_cost}. We see that non-LLM approaches have much lower inference costs while maintaining better or comparable performances on long document summarizations. 

% \begin{table}[]
%     \centering
%     \scriptsize
%     \begin{tabular}{l|c}
%     \toprule
%     \textbf{Model} & \textbf{Cost (\$)} \\
%     \midrule
%         \textsc{AlignScore} &  \$0.3\\
%         \textsc{StructScore} & \$0.3-\$0.6 \\
%         \textsc{LongDocFactScore} & \$0.3\\
%         \textsc{MC-FT5 (SENT)} &  \$3.2\\
%         \midrule
%          GPT4o&  \$15\\
%          \bottomrule
%     \end{tabular}
%     \caption{Comparison of models' inference cost (around \$0.5/GPU-hr) to the API model GPT4o on our test split of \textsc{DiverSumm}.}
%     \label{tab:computation_cost}
% \end{table}
% % \begin{table*}[th]
% % \footnotesize
% % \centering
% % \setlength{\tabcolsep}{2pt}
% % \begin{NiceTabular}{l|cccccccccc|c}
% %  \toprule
% % \textbf{Model} & Podcast& BillSum &SAMSum &News& Sales C& Sales E &Shkspr &SciTLDR &QMSum &ECTSum &Avg. (↓) \\
% % avg src len &1075.9 & 1202.5 & 121.0 & 506.6 & 403.9 & 371.5 &898.5 & 163.8 & 624.8 & 154.6 \\
% % \midrule 
% % \RowStyle{\color{gray}}
% % DAE &54.9 &55.1 &59.5 &61.7 &50.8 &55.0 &54.5 &55.2& 52.0 &58.6 &55.7 \\
% % \RowStyle{\color{gray}}
% % SummaC &58.5 &55.7 &54.7 &62.1& 59.0& 57.7& 59.3 &59.7 &56.6 &64.4 &58.8 \\
% % \RowStyle{\color{gray}}
% % QAFactEval& 64.0& 54.4 &66.3& 74.6& 68.5& 64.2 &61.9& 67.5& 62.4& 72.9 &65.7 \\

% % \RowStyle{\color{gray}}
% % PaLM2-bison &66.0& 62.0 &69.0& 68.4& 74.5 &68.1 &61.6& 78.1 &70.2 &72.3 &69.0 \\
% % \RowStyle{\color{gray}}
% % Dav003 &65.7 &59.9& 67.5& 71.2 &78.8 &69.4& 69.6& 74.4 &72.2 &77.9 &70.7 \\
% % \RowStyle{\color{gray}}
% % GPT3.5-turbo &68.4 &63.6 &69.1 &74.5 &79.7 &65.5 &68.1& 75.6 &69.2& 78.9 &71.3\\
% % \RowStyle{\color{gray}}
% % GPT4 &83.3& 71.1& 82.9 &83.3& 87.6& 80.1& 84.6& 82.4 &80.4 &88.0 &82.4 \\
% % \midrule 
% % \multicolumn{12}{c}{\textit{Our Scores}}\\
% % \midrule
% % Alignscore &  75.9  & 59.3  & 78.4  & 75.1  & 86.0  & 79.0  & 67.2  & 77.4  & 75.4  & 85.6  & 75.9 \\
% % STRUCTSCORE\_lvl1 & 75.0  & 58.8  & 73.0  & 70.8  & 85.6  & 77.8  & 66.8  & 75.0  & 71.3  & 76.3  & 73.0 \\
% % STRUCTSCORE\_lvl2 & 69.2  & 58.3  & 66.7  & 72.6  & 85.2  & 77.9  & 65.0  & 74.9  & 69.2  & 76.0  & 71.5 \\
% % STRUCTSCORE\_lvl3 & 69.0  & 59.1  & 66.3  & 71.7  & 81.4  & 75.1  & 64.9  & 62.4  & 72.1  & 66.2  & 68.8 \\
% % \midrule
% % \multicolumn{12}{c}{\textit{Upper Bound}}\\
% % \midrule

% % \RowStyle{\color{gray}}
% % GPT4 Oracle &90.2 &85.5& 86.3 &88.3& 91.1 &83.5 &96.6& 86.3 &89.9 &91.7& 88.9\\
% % \RowStyle{\color{gray}}
% % Human Perf.& 90.8 &87.5 &89.4 &90.0& 91.8 &87.4& 96.9& 89.3 &90.7 &95.4 &90.9 \\
% % \bottomrule

% % \end{NiceTabular}
% %  \caption{Results for all summarization tasks in the \textsc{SummEdits} dataset. We report the balanced accuracy score.}
% %  \end{table*}

 

 
% % \begin{table*}[t]
% % \small
% % \centering
% % \setlength{\tabcolsep}{4pt}
% % \begin{NiceTabular}{llcccc|cccc}
% %  \toprule
% % \multirow{4}{*}{{\parbox{1cm}{\textbf{Model Type}}}}  & \multirow{4}{*}{\parbox{1.5cm}{\textbf{Evaluation Models
% % }}} & \multicolumn{4}{c}{Sentence-Level (BAcc $\uparrow$)} & \multicolumn{4}{c}{Summary-Level (BAcc $\uparrow$)}\\
% % \cmidrule(lr){3-6} \cmidrule(lr){7-10}
% % && \multicolumn{2}{c}{MediaSum} & \multicolumn{2}{c}{MeetingBank} & \multicolumn{2}{c}{MediaSum} & \multicolumn{2}{c}{MeetingBank}\\
% % \cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}\cmidrule(lr){9-10}
% % &  & Main & Marginal & Main & Marginal& Main & Marginal& Main & Marginal\\
% % \midrule 
% % - &Baseline & 50.0 &50.0& 50.0 &50.0 &50.0 &50.0 &50.0 &50.0 \\
% % \midrule
% % \multirow{4}{*}{\parbox{0.8cm}{\textbf{Non-LLM}}} & 
% % \RowStyle{\color{gray}}
% % \textsc{SummaC-ZS} & 66.1&  73.9&  63.9&  80.6 & 62.7&  64.1 & 58.1 & 72.4 \\
% % \RowStyle{\color{gray}}
% % & \textsc{SummaC-CV} &67.6 &73.0& 62.6 &77.3 &61.2 &66.5 &52.4 &72.9 \\
% % \RowStyle{\color{gray}}
% % & \textsc{QAFactEval} &53.9 &74.0 & 58.0 &75.8 & 61.4 &74.2 &55.1 &68.2 \\
% % \RowStyle{\color{gray}}
% % & \textsc{AlignScore} & 69.2 & 76.2 & 61.2  &78.6  &65.5 & 72.1 & 63.4 & 71.8 \\
% % \cmidrule(lr){2-10}
% % & \textsc{AlignScore}* & 68.1 & 75.5 & 60.4  &78.8  &62.8 & 71.6 & 64.1 & 72.1 \\
% % \midrule
% % \multirow{4}{*}{\parbox{0.8cm}{\textbf{Open
% % Source
% % LLM}}}&
% % \RowStyle{\color{gray}}
% % Vicuna-13B &54.0 &54.8 &49.6 &61.9 &55.6 &59.1 &51.2 &59.2 \\
% % \RowStyle{\color{gray}}
% % &Vicuna-33B &51.0 &51.1 &53.6& 48.4 &52.5 &53.4 &53.2 &51.0 \\
% % \RowStyle{\color{gray}}
% % &WizardLM-13B &59.8 &53.5 &58.8 &56.6 &57.0 &54.5 &54.6& 58.0 \\
% % \RowStyle{\color{gray}}
% % &WizardLM-30B &54.5 &53.9 &53.5 &53.4 &53.3& 54.4 &53.0 &53.2 \\
% % \midrule 
% % \multirow{2}{*}{\parbox{0.8cm}{\textbf{Prop.
% % LLM}}} & 
% % \RowStyle{\color{gray}}
% % { GPT-3.5-Turbo} &  61.6 & 68.9 & 56.0&  65.0 & 59.6 & 65.8&  63.2&  65.7\\
% % \RowStyle{\color{gray}}
% % & GPT-4 & 64.9 & 80.2 & 67.5 & 90.3 & 63.7 & 78.9 & 74.7 & 83.1 \\
% % \midrule 
% % \multirow{2}{*}{\parbox{1.5cm}{\textbf{Ours}}} & \textsc{AlignScore-utter}  & 64.6 & 73.3 & 58.2 & 79.3 & 61.1 & 71.6 & 56.6 & 66.7\\
% % &
% % \\ 
% % \\ 
% % \midrule
% % \end{NiceTabular}
% % \caption{\textbf{Sentence-level and summary-level balanced accuracy (BAcc) for factual consistency evaluators on
% % the test set of \textsc{TofuEval}}. Most scores are taken from \citet{Tang2024TofuEvalEH}. * means a reproduced score evaluation. For LLM-based methods, summary-level labels are aggregated sentence-level
% % labels, as it achieves better performance than directly predicting consistency labels on whole summaries. Note that a baseline method that always predicts inconsistent or consistent
% % achieves 50\% balanced accuracy}
% % \end{table*}



