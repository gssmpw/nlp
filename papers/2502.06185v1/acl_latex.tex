% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[raggedrightboxes]{ragged2e}
\usepackage{nicematrix}
% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor,colortbl}
\usepackage{transparent}  
\usepackage{nicematrix,tikz}
\usepackage[raggedrightboxes]{ragged2e}
\usepackage{enumitem}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Discourse-Driven Evaluation: Unveiling Factual Inconsistency in Long Document Summarization}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Yang Zhong \\
  Department of Computer Science \\
 University of Pittsburgh \\
  \texttt{yaz118@pitt.edu} \\\And
  Diane Litman \\
  Department of Computer Science and LRDC \\
 University of Pittsburgh \\
  \texttt{dlitman@pitt.edu} \\}

\begin{document}
\maketitle
\begin{abstract}
Detecting factual inconsistency for long document summarization remains challenging, given the complex structure of the source article and long summary length. In this work, we study factual inconsistency errors and connect them with a line of discourse analysis. We find that errors are more common in complex sentences and are associated with several discourse features. We propose a framework that decomposes long texts into discourse-inspired chunks and utilizes discourse information to better aggregate sentence-level scores predicted by natural language inference models. Our approach shows improved performance on top of different model baselines over several evaluation benchmarks, covering rich domains of texts, focusing on long document summarization. This underscores the significance of
incorporating discourse features
in developing models for scoring summaries for long document factual inconsistency.
\end{abstract}

\section{Introduction}

Current state-of-the-art summarization systems can generate fluent summaries; however, their ability to produce factually consistent summaries that adhere to the source content or world knowledge remains questionable. This phenomenon is known as \textbf{factual inconsistency}, one type of ``hallucination'' problem \cite{maynez-etal-2020-faithfulness, zhang2023languagemodelhallucinationssnowball, cao-wang-2021-cliff,kryscinski-etal-2020-evaluating, goyal-durrett-2021-annotating, cao-etal-2022-hallucinated}.  A rigorous line of research approaches this problem by developing models to detect unfaithful summary content, including utilizing pre-trained models such as natural language inference (NLI)  \cite{ kryscinski-etal-2020-evaluating, laban2022summac, zha-etal-2023-alignscore} and question answering (QA) \cite{scialom-etal-2021-questeval, fabbri-etal-2022-qafacteval} models. Such approaches are tested on rich benchmark datasets, such as \textsc{True} \cite{honovich-etal-2022-true}, \textsc{Summac} \cite{laban2022summac}, and \textsc{AggreFact} \cite{tang-etal-2023-understanding}, etc. 

However, such benchmark datasets only include short documents (< 1000 words) and summaries with a few sentences. While the methods mentioned above perform well with short texts, they struggle with longer documents \cite{schuster-etal-2022-stretching}. Recent NLI work addresses this by selecting the input and breaking down the summary. Lengthy summaries are split into individual sentences or more minor atomic claims, while small chunks of the source document are extracted as premises. This approach reduces the task to multiple short evaluations, which are then aggregated to provide a summary-level label \cite{zha-etal-2023-alignscore, zhang-etal-2024-fine, scire-etal-2024-fenice, yang2024fizz}. 

Out of the existing NLI-based methods, \textsc{AlignScore} demonstrated superior performance on multiple benchmarks. It breaks the input document into continuous chunks of text to tackle the input restriction. However, this exhaustive approach may break the structure of the context (section and paragraph split), thus reducing the chances that the summary sentence can be correctly verified with its factual consistency.  On the other hand, most factuality evaluation metrics aggregate the sentence-level aligning scores through averaging or selecting the minimum, disregarding that sentences are not equally important \cite{krishna-etal-2023-longeval}. For instance, people can remember the big picture more easily but struggle to retain low-level details when retelling a story. The natural questions would be: do system-generated summaries carry a similar pattern? If so, how can we utilize the text organization information to help detect the inconsistencies between the summary and the source document? 

In this work, we study the factual inconsistency problem through the lens of discourse analysis. By analyzing the structure (here we use Rhetorical Structure Theory (RST) \cite{MANNTHOMPSON+1988}) of the original articles and the summaries, we uncover the importance of preserving the article structure and studying the connections between discourse structure and the factual consistency of model-generated summaries. Our analysis shows that complex sentences built by multiple elementary discourse units (EDUs, the basic units used in the discourse theory) have a higher chance of containing errors, and we also find several discourse features connected to the factual consistency of summary sentences.

Motivated by the analyses mentioned above, we propose a new evaluation method, \textsc{StructScore}, based on the NLI-based approaches to better detect factual inconsistency. Our algorithm includes two steps:  (1) leveraging the discourse information when aggregating the sentence-level alignment scores of the target summary and (2) decomposing the long input article into multiple discourse-inspired chunks. We tested our proposed approach on multiple document summarization benchmarks, including \textsc{AggreFact-FtSOTA} split, \textsc{DiverSumm}, \textsc{LongSciVerify}, \textsc{LongEval}, and a non-scientific domain dataset \textsc{LegalSumm} with a focus on long document summarization. Our proposed approach obtained a performance gain on multiple tasks.\footnote{Our models and model outputs are publicly available at \url{https://github.com/cs329yangzhong/discourse-driven-summary-factuality-evaluation}} 

To sum up, two research questions are addressed:
1. How and what discourse features are connected to the factual inconsistency evaluation?
2. Can our discourse-inspired approach improve the detection performance on long document summarization?




\input{Section/Relatedwork}



\input{Section/Dataset}

% \input{Section/Motivations}
\input{Section/Analysis}
\input{Section/Method}
\input{Section/Experiments_Results}
% \input{Section/Experimental_Analysis}
\input{Section/Conclusion}
\input{Section/Limitations}
% \section*{Acknowledgements}


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

% \section{Example Appendix}
% \label{sec:appendix}
\newpage

\input{Section/appendix_more_discourse_analysis}
% \input{Section/appendix_prompt}
\input{Section/appendix_dataset_performance}

\end{document}
