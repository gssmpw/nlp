\documentclass[table]{ai2style/ai2}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{lineno}
\usepackage{enumitem}
\usepackage{listings} %
\usepackage{svg}

\definecolor{ darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}



\usepackage{amssymb}
\usepackage{multirow}
\usepackage{bigdelim}
\usepackage{todonotes}
\usepackage{longtable}
\usepackage{tabularray}
\usepackage{wrapfig}
\usepackage[most]{tcolorbox} 
\usepackage{url}
\usepackage{xspace}
\usepackage{svg}
\usepackage[absolute]{textpos} %

\usepackage{fdsymbol}   %




\usepackage[utf8]{inputenc} %
\usepackage[T1]{fontenc}    %
\usepackage{url}            %
\usepackage{booktabs}       %
\usepackage{amsfonts}       %
\usepackage{nicefrac}       %
\usepackage{microtype}      %
\usepackage[table]{xcolor}         %
\usepackage{amsmath}
\usepackage[most]{tcolorbox}
\usepackage{csquotes}

\usepackage{siunitx}
\usepackage{graphicx}
\usepackage{arydshln}
\usepackage{wrapfig}
\usepackage{enumitem}
\usepackage{soul} %

\usepackage{multirow}
\usepackage{xspace}
\usepackage{adjustbox}
\usepackage{pifont}
\usepackage{caption}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{bold-extra}
\usepackage{url}

\usepackage{pgf-pie}

\usepackage{hyperref}
\definecolor{linkcolor}{RGB}{0, 0, 128}
\hypersetup{
     colorlinks   = true,
     citecolor    = linkcolor,
     linkcolor    = linkcolor,
     urlcolor     = linkcolor,
}
\usepackage{pifont}%
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage{listings}

\setlist[itemize]{leftmargin=*,itemsep=0em,parsep=0.3em,topsep=0.3em}


\DeclareUnicodeCharacter{2212}{\ensuremath{-}}

\addtolength{\extrarowheight}{\belowrulesep}
\aboverulesep=0pt
\belowrulesep=0pt

\definecolor{maroon}{HTML}{F26035}
\definecolor{yellow}{HTML}{FDBC42}
\definecolor{lavender}{HTML}{734f96}
\definecolor{darkergrey}{HTML}{444444}
\definecolor{midgrey}{HTML}{e6eded}
\definecolor{ai2pink}{HTML}{f0529c}%
\definecolor{ai2midpink}{HTML}{fad3e5}
\definecolor{ai2lightpink}{HTML}{fbecf3}
\definecolor{ai2midwhite}{HTML}{f2e5d9}
\definecolor{ai2offwhite}{HTML}{fbf4ee}
\definecolor{ai2green}{HTML}{0fcb8c}
\definecolor{ai2lightgreen}{HTML}{e7f9f3}
\definecolor{ai2darkgreen}{HTML}{105257}
\definecolor{ai2purple}{HTML}{B932EB}
\definecolor{ai2lightpurple}{HTML}{f7e8fc}
\definecolor{neutralEight}{HTML}{343434}
\definecolor{neutralFive}{HTML}{838383}
\definecolor{neutralThree}{HTML}{bebebe}
\definecolor{neutralOne}{HTML}{dedede}
\definecolor{lightgrey}{HTML}{fafcfc}

\usepackage{tikz}
\newcommand{\cblock}[3]{
  \hspace{-1.5mm}
  \begin{tikzpicture}
    [
    node/.style={square, minimum size=10mm, thick, line width=0pt},
    ]
    \node[fill={rgb,255:red,#1;green,#2;blue,#3}] () [] {};
  \end{tikzpicture}%
}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\definecolor{maroon}{HTML}{F26035}
\definecolor{yellow}{HTML}{FDBC42}
\definecolor{darkred}{RGB}{156, 39, 33}
\definecolor{darkblue}{RGB}{31, 90, 153}
\definecolor{forestgreen}{rgb}{0.13, 0.55, 0.13}
\definecolor{olmoDarkBlue}{HTML}{012e59}
\definecolor{olmoBlue}{HTML}{265ed4}
\definecolor{olmoLightBlue}{HTML}{012e59}
\definecolor{olmoTeal}{HTML}{00d5ff}
\definecolor{olmoYellow}{HTML}{ffbb00}
\definecolor{olmoOrange}{HTML}{ff9100}


\newcommand{\nol}[1]{{\color{purple} [nol]: #1}}


\usepackage{setspace}

\usepackage{nicematrix}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{P}[1]{>{\centering\let\newline\\\arraybackslash\columncolor{ai2lightpink}}m{#1}}
\newcolumntype{W}[1]{>{\columncolor{white}}c}  %
\addtolength{\extrarowheight}{\belowrulesep}
\aboverulesep=0pt
\belowrulesep=0pt

\newcommand{\aitwo}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{logos/ai2_logo.png}}\xspace}


\newcommand{\olmOCRLogoWithText}{\raisebox{-.7em}{\rlap{\raisebox{.6em}{\hspace{3.3em}\scriptsize olmOCR}}\includegraphics[height=2.3em]{logos/logo-with-ai2-bg.pdf}}\hspace{.1em}\xspace}



\title{\pipeline: Unlocking Trillions of Tokens in PDFs with Vision Language Models}


\newcommand{\kyle}[1]{{\color{red} [Kyle]: #1}}
\newcommand{\jake}[1]{{\color{green} [Jake]: #1}}
\newcommand{\luca}[1]{{\color{purple} [Luca]: #1}}

\newcommand{\pipeline}{\textsc{olmOCR}\xspace}
\newcommand{\olmocr}{\pipeline{}\xspace}
\newcommand{\model}{\texttt{olmOCR-7B-0225-preview}\xspace}
\newcommand{\train}{\texttt{olmOCR-mix-0225}\xspace}
\newcommand{\method}{\textsc{document-anchoring}\xspace}
\newcommand{\GptFourO}{GPT-4o\xspace}

\newcommand{\marker}{\textsc{Marker}\xspace}
\newcommand{\gotocr}{\textsc{GOTOCR}\xspace}
\newcommand{\mineru}{\textsc{MinerU}\xspace}



\newcommand{\huggingface}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{logos/hf.pdf}}\xspace}
\newcommand{\coreContrib}{\raisebox{.28em}{\hspace{.05em}\includegraphics[height=.45em]{logos/core.pdf}}\xspace}
\newcommand{\emailLogo}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{logos/email.pdf}}\xspace}
\newcommand{\hfdataset}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{logos/db.pdf}}\xspace}
\newcommand{\github}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{logos/github.pdf}}\xspace}
\newcommand{\aitoo}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{logos/ai2.pdf}}\xspace}
\newcommand{\wandb}{\raisebox{-1.5pt}{\includegraphics[height=1.05em]{logos/wandb-logo.pdf}}\xspace}





\authorOne[]{Jake Poznanski\coreContrib}

\authorTwo[]{Jon Borchardt}
\authorTwo[]{Jason Dunkelberger}
\authorTwo[]{Regan Huff}
\authorTwo[]{Daniel Lin}
\authorTwo[]{Aman Rangapur}
\authorTwo[]{Christopher Wilhelm}

\authorThree[]{Kyle Lo\coreContrib}
\authorThree[]{Luca Soldaini\coreContrib}



\contribution[]{Allen Institute for AI, Seattle, USA \quad \texttt{\{jakep|kylel|lucas\}@allenai.org} \quad \coreContrib{ indicates core contributors.}}


\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}


\setmainfigure{

\begin{figure}[!h]
    \centering
    \vspace*{\fill}  %
    \includegraphics[width=0.9\linewidth]{figures/olmocr-v3-light.pdf}
    \label{fig:figure1}
\end{figure}

}


\abstract{
PDF documents have the potential to provide trillions of novel, high-quality tokens for training language models.
However, these documents come in a diversity of types with differing formats and visual layouts that pose a challenge when attempting to extract and faithfully represent the underlying content for language model use. 
We present \pipeline{}, an open-source Python toolkit for processing PDFs into clean, linearized plain text in natural reading order while preserving structured content like sections, tables, lists, equations, and more.
Our toolkit runs a fine-tuned 7B vision language model (VLM) trained on a sample of 260,000 pages from over 100,000 crawled PDFs with diverse properties, including graphics, handwritten text and poor quality scans.
\pipeline{} is optimized for large-scale batch processing, able to scale flexibly to different hardware setups and convert a million PDF pages for only \$190 USD.
We release all components of \pipeline{} including VLM weights, data and training code, as well as inference code built on serving frameworks including vLLM and SGLang.
}

\setmaintable{
\begin{table}[!h]
    \centering
    \begin{tabular}{l l}
        \github~{\sans{Code}} & \href{https://github.com/allenai/olmocr}{\texttt{allenai/olmocr}} \\[0.0em]
        \huggingface~{\sans Weights \& Data} & \href{https://huggingface.co/collections/allenai/olmocr-67af8630b0062a25bf1b54a1}{\texttt{allenai/olmocr}} \\ [0.0em]
        \aitwo~{\sans{Demo}}&\href{https://olmocr.allenai.org/}{\texttt{olmocr.allenai.org}}
    \end{tabular}
\end{table}
}






\begin{document}


\maketitle











\section{Introduction}
\label{sec:intro}












Access to clean, coherent textual data is a crucial component in the life cycle of modern language models (LMs).
During model development, LMs require training on trillions of tokens derived from billions of documents~\citep{soldaini2024dolma,penedo2024finewebdatasetsdecantingweb,Li2024-zy}; errors from noisy or low fidelity content extraction and representation can result in training instabilities or even worse downstream performance~\citep{Penedo2023TheRD,Li2024-zy,OLMo2}.
During inference, LMs are often prompted with plain text representations of relevant document context to ground user prompts; for example, consider information extraction~\citep{Kim2021OCRFreeDU} or AI reading assistance~\citep{semantic-reader} over a user-provided document and cascading downstream errors due to low quality representation of the source document.

While the internet remains a valuable source of textual content for language models, large amounts of content are not readily available through web pages.
Electronic documents (\textit{e.g.}, PDF, PS, DjVu formats) and word processing files (\textit{e.g.}, DOC, ODT, RTF) are widely-used formats to store textual content. 
However, these formats present a unique challenge: unlike modern web standards, they encode content to facilitate rendering on fixed-size physical pages, at the expense of preserving logical text structure. 
For example, consider the PDF format, which originated as a means to specify how digital documents should be printed onto physical paper.
As seen in Figure~\ref{fig:pdf-chars}, PDFs store not units of text---headings, paragraphs, or other meaningful prose elements---but single characters alongside their spacing, placement, and any metadata used for visual rendering on a page.
As more and more documents became digital, users have relied this file format to create trillions of documents \citep{pdfa2015};
yet, these documents remain difficult to leverage in LM pipelines because PDFs lack basic structure necessary for coherent prose, such as ground truth reading order.

\begin{figure}[!h]
    \centering
    \begin{tcolorbox}[
        colback=ai2offwhite,     %
        colframe=ai2darkgreen,         %
        coltitle=ai2offwhite,         %
        fonttitle=\bfseries,      %
        boxrule=1pt,              %
        arc=3mm,                  %
        boxsep=5pt                %
    ]
    \begin{verbatim}
    Character: 'o'
    Transform Matrix: (1.02, 0.0, 0, 1, 70.866, 709.481)
    Font: JURTWD+Manrope-Bold, Size: 24.78710000000001
    ------------------------------
    Character: 'l'
    Transform Matrix: (1.02, 0.0, 0, 1, 86.490796356, 709.481)
    Font: JURTWD+Manrope-Bold, Size: 24.78710000000001
    ------------------------------
    Character: 'm'
    Transform Matrix: (1.02, 0.0, 0, 1, 93.56999211600001, 709.481)
    Font: JURTWD+Manrope-Bold, Size: 24.78710000000001
    ------------------------------
    Character: 'O'
    Transform Matrix: (1.02, 0.0, 0, 1, 116.299267074, 709.481)
    Font: JURTWD+Manrope-Bold, Size: 24.78710000000001
    ------------------------------
    Character: 'C'
    Transform Matrix: (1.02, 0.0, 0, 1, 135.236115732, 709.481)
    Font: JURTWD+Manrope-Bold, Size: 24.78710000000001
    ------------------------------
    Character: 'R'
    Transform Matrix: (1.02, 0.0, 0, 1, 153.894853128, 709.481)
    Font: JURTWD+Manrope-Bold, Size: 24.78710000000001\end{verbatim}
    \end{tcolorbox}
    \caption{Example of how PDFs represent textual content, such as this paper title, as individual glyphs with metadata.}
    \label{fig:pdf-chars}
\end{figure}


Faithful content extraction and representation of digitized print documents has long been of interest, with early research efforts in the 1950s, and first commercial optical character recognition (OCR) tools debuting in the late 1970s~\citep{Mori1992-qy}.
The release of Tesseract in 2006 represented a significant milestone, as the first high-quality, open-source OCR toolkit~\citep{Smith2013-pp}.
The current landscape of PDF extraction toolkits can be partitioned in pipeline-based systems and end-to-end models.
Pipeline-based systems (MinerU, \citealt{mineru}; Marker, \citealt{marker})  are comprised of multiple ML components (\textit{e.g.}, section segmentation, table parsing) chained together; 
some, such as Grobid~\citep{GROBID}, VILA~\citep{shen-etal-2022-vila}, and PaperMage~\citep{lopapermage}, are tailored to scientific papers.
On the other hand, end-to-end models parse a document with a single model.
For example, Nougat~\citep{nougat} and GOT Theory 2.0~\citep{gottheory} take images of PDF pages as input, and return plain text.
Notably, while pipeline-based systems have historically focused on simply faithful extraction, end-to-end-systems have also made strides to enable \emph{linearization} of this content---prescribing a flattening of this content to adhere to logical reading order---which can be quite challenging for layout-rich documents with many floating elements (e.g. multi-column documents with floating diagrams, headers, footnotes, and more).
Recently, rapid advances in the proprietary LMs have led to significant improvements in end-to-end text extraction capabilities~\citep{Bai2025-ub,geminiPDF}.
However, this capability comes at a steep price: for example, converting a million pages using GPT-4o can cost over \$6,200 USD.\footnote{With batch pricing, at \$1.25 USD (input) and \$5.00 USD (output) per million tokens in Feb 2025.}

We introduce {\bf\olmocr}, a general-purpose context extraction and linearization toolkit to convert PDFs or images of documents into clean plain text:
\begin{itemize}
    \item \pipeline{} is capable of processing a diversity of document types, covering different domains as well as visual layouts. It uses Markdown~\citep{markdown2004} to represent structured content, such as sections, lists, equations and tables.
    
    \item Unlike other end-to-end models, \olmocr uses \textit{both} text and visual information to obtain an accurate text representation of a documents. 
    We develop \method, a technique to extract text and layout information from born-digital PDF documents. 
    \method can be used to prompt VLMs alongside images of document pages to significantly improve extraction. 
    
    \item To build \olmocr, we curate \train, a dataset of nearly 260,000 PDF pages from a diverse set of PDFs crawled from the web and public domain books. 
    This corpus is used to fine-tune \model from Qwen2-VL-7B-Instruct \citep{qwen2vl}.
    We release \train to facilitate further research in document extraction, and open source model weights and code as part of our toolkit.

    \item \olmocr is a fully optimized pipeline compatible with both SGLang~\citep{zheng2024sglang} and vLLM~\citep{kwon2023efficient} inference engines.
    In our tests, we have been able to scale it efficiently from one to hundreds of GPUs.
    It achieves an amortized cost of less than \$190 per million pages converted, or $1/32^{nd}$ of the price of calling GPT-4o APIs.
    Furthermore, \olmocr is resilient: it includes several heuristics to handle common parsing failures and metadata errors.
    
\end{itemize}









































\section{Methodology}
\label{sec:method}






\paragraph{Approach}
Many end-to-end OCR models, such as GOT Theory 2.0~\citep{gottheory} and Nougat~\citep{nougat}, exclusively rely on rasterized pages to convert documents to plain text;
that is, they process images of the document pages as input to autoregressively decode text tokens. 
This approach, while offering great compatibility with image-only digitization pipelines, misses the fact that most PDFs are born-digital documents, thus already contain either digitized text or other metadata that would help in correctly linearizing the content.

\begin{figure}[t]  %
    \centering
    \includegraphics[width=0.8\linewidth]{figures/olmocr-anchoring.drawio.pdf} 
    \caption{Example of how \method{} works for a typical page. Relevant image locations and text blocks get extracted, concatenated, and inserted into the model prompt.
    When prompting a VLM for a plain text version of the document, the anchored text is used in conjunction with the rasterized image of a page.}
    \label{fig:anchoring-example}
\end{figure}

In contrast, the \olmocr pipeline leverages document text and metadata. 
We call this approach {\bf\method}. 
Figure~\ref{fig:anchoring-example} provides an overview of our method; 
\method extracts coordinates of salient elements in each page (\textit{e.g.}, text blocks and images) and injects them alongside raw text extracted from the PDF binary file.
Crucially, the anchored text is provide as input to any VLM \textit{alongside} a rasterized image of the page. 

Our approach increases the quality of our content extraction.
We apply \method{} when prompting \GptFourO to collect silver training samples,
when fine-tuning \model, 
and when performing inference with the \olmocr toolkit.


\paragraph{Implementation}
\method{} processes PDF document pages via the \texttt{pypdf}~\citep{pypdf} library to extract a representation of the page's structure from the underlying PDF.
All of the text blocks and images in the page are extracted, including position information.
Starting with the most relevant text blocks and images\footnote{We prioritize text blocks and images which are located at the start and end of the document.}, these are sampled and added to the prompt of the VLM, up to a defined maximum character limit\footnote{We use a character limit for convenience and speed, but during training or inference, if a page's prompt exceeds the model's token limit, we just regenerate it with exponentially decreasing character limits until it is suitable.}. This extra information is then available to the model when processing the document.

Overall, we find that using prompts constructed using \method results in significantly fewer hallucinations.
Prompting with just the page image was prone to models completing unfinished sentences, or to invent larger texts when the image data was ambiguous.
Finally, while \method{} helps with quality on born-digital documents, our pipeline maintains high performance on documents that do not have any digital metadata encoded in them. 
In these cases, the model will not have the benefit of seeing the internal structure of the PDF document, 
instead relying on just the rasterized image of a page to process the underlying document. 


\section{Fine-tuning Models for \olmocr}
\label{sec:model}

While the \method method presented in Section~\S\ref{sec:method} can be used to prompt any language model, we find that fine-tuning a smaller VLM on this linearization task yields models that are as accurate as larger, general-purpose models and are much more efficient at inference time. 
In this section, we summarize the procedure we followed to distill the output of a larger VLM; 
Section~\S\ref{sec:model:dataset} details how we collect the training data, while Section~\S\ref{sec:model:training} provides an overview of fine-tuning.
 
\subsection{Dataset}
\label{sec:model:dataset}

\paragraph{Choice of teacher model}
At the time of construction (October 2024), we evaluated several open-weights and API models to label our training data. 
Our analysis relied on qualitative assessment of linearization performance on a small collection of PDF pages with moderately complicated layout. 
At this stage, we evaluated model outputs visually across a set of several PDF documents that were known to cause problems for traditional PDF content extraction tools, and contained varied elements such as equations, tables, multi-column layouts, and born-analog documents captured in poor lighting conditions.

Among possible options, we found GPT-4o, GPT-4o mini, Gemini 1.5, and Claude Sonnet 3.5 to have acceptable performance.
Gemini was discarded because a high proportion of prompts returned \texttt{RECITATION} errors, meaning that the output was too similar 
to Gemini's own training data\footnote{We tried Gemini 1.5 Flash again at time of release (Feb 2025) and the proportion of RECITATION errors was below 0.3\%.}. 
Ultimately, we chose \texttt{gpt-4o-2024-08-06} due to its high performance and relatively low cost in batch mode. 
GPT-4o mini produced too many hallucinations, and Claude Sonnet 3.5 was found to be cost prohibitive. 

\paragraph{Choice of PDF tools}
\olmocr leverages two tools for PDF rasterization and metadata manipulation: \citet{poppler} transforms pages in a PDF to images; \citet{pypdf} extracts text blocks, images, and their positions as part of \method{}. 

\paragraph{Prompting strategy}
We prompt GPT-4o with the image of a PDF page.
We render each page using Poppler's \texttt{pdftoppm} tool,
at a resolution such that its longest edge is 2048 pixels, 
the largest supported by the GPT-4o model at the time. 
We report the full prompt in Appendix~\ref{sec:silver_prompt}, as augmented by our \method{} technique. 
As mentioned in Section~\S\ref{sec:method}, we found that this significantly reduced hallucinations. 
In contrast, prompting with just the page image was prone to complete unfinished sentences, or to produce unfaithful output when the image data was ambiguous. 


Finally, we instruct GPT-4o to respond with structured output to our requests. 
We report the full JSON schema in Appendix~\ref{sec:json_schema}.
It forces the model to first extract page metadata, such as language, page orientation, and presence of tables, before moving onto outputting the text of the page in a natural reading order. 
We believe the order of the fields in the structured schema is helpful in improving output quality, because it forces the model to analyze the page as a whole first before moving on to extracting the text. 
The output schema encourages the model to return no text at all, if appropriate: this aspect is necessary to prevent GPT-4o from visually describing the content of a PDF in cases where no text is found on the page (\textit{e.g.}, the page contains just a photographic image or non-textual diagram). 


\paragraph{Data acquisition and page sampling}

\begin{table}[!h]
    \centering
    \begin{minipage}{0.45\textwidth}
        \begin{center}
        \begin{tabular}{lrr}
            \toprule
            \textbf{Source} & \begin{tabular}[c]{@{}r@{}}\textbf{Unique}\\[-0.5em]\textbf{docs}\end{tabular} & \begin{tabular}[c]{@{}r@{}}\textbf{Total}\\[-.5em]\textbf{pages}\end{tabular} \\
            \midrule
            Web crawled PDFs & 99,903 & 249,332 \\
            Internet Archive books & 5,601 & 16,803 \\
            \midrule
            {\emph{Total}} & {\emph{105,504}} & {\emph{266,135}} \\         
            \bottomrule
        \end{tabular}
        \caption{Training set composition by source. Web crawled PDFs are sampled from a set of over 240 million documents crawled from public websites. Books in the Internet Archive set are in the public domain.}
        \label{tab:training_source}
        \end{center}
    \end{minipage}\hfill
    \begin{minipage}{0.45\textwidth}
        \centering
        \begin{tabular}{lr}
            \toprule
            \textbf{Document type} & \textbf{Fraction} \\
            \midrule
            Academic & 60\% \\
            Brochure & 12\% \\
            Legal & 11\% \\
            Table & 6\% \\
            Diagram & 5\% \\
            Slideshow & 2\% \\
            Other & 4\% \\
            \bottomrule
        \end{tabular}
        \caption{Web PDFs breakdown by document type. Distribution is estimating by sampling 707 pages, which are classified using \texttt{gpt-4o-2024-11-20}. Details of the prompt used in Appendix~\ref{sec:classification_prompt}.}
        \label{tab:pdf_types}
    \end{minipage}
\end{table}

To generate the primary training dataset, we sample 100,000 PDFs from an internal dataset of 240 million PDFs crawled from public internet sites. 
Using the Lingua package~\citep{lingua-py}, we identify and filter out documents that were not in English.
Further, we remove any document that failed to be parsed by pypdf, contains spam keywords, is a fillable form, or whose text is too short\footnote{An implementation of these heuristics is available on GitHub:~\href{https://github.com/allenai/olmocr/blob/cc1f476b3e6018978ed1c6ca0b20b41ee2586604/olmocr/filter/filter.py\#L14-L112}{\path{olmocr/filter/filter.py#L14-L112}}.}. 
We then sampled at most 3 pages uniformly at random from each PDF. This resulted in approximately 249,332 PDF pages.
We also sampled 5,601 PDFs from a dataset public domain scanned books from the Internet Archive, and processed it similarly. 
Unlike the web crawled set, PDFs in this collection consist of image scans of book pages, as opposed to born-digital documents.
We summarize the data distribution in Tables~\ref{tab:training_source} and~\ref{tab:pdf_types}.



\subsection{Model Training}
\label{sec:model:training}

\paragraph{Fine-tuning}
\model is fine-tuned from a Qwen2-VL-7B-Instruct checkpoint. 
Training is implemented using Hugging Face's \texttt{transformers} library~\citep{wolf-etal-2020-transformers}.
Hyperparameters are set as follows: we use an effective batch size of 4 (batch size 1 with 4 gradient accumulation steps), a learning rate of $1e-6$, AdamW optimizer, and a cosine annealing schedule for 10,000 steps (roughly 1.2 epochs). 
We use single node with 8 NVIDIA H100 GPUs.
We experimented with both full fine-tuning as well as  LoRA~\citep{hu2021loralowrankadaptationlarge}.

\paragraph{Dataset formatting}
During fine-tuning, we slightly alter the prompt used for dataset labeling in Section~\S\ref{sec:model:dataset}. 
Namely, we remove some of the additional instructions from the prompt, and shrink the image size so that PDF pages
get rendered to a maximum dimension of 1024 pixels on the longest edge. 
The simplified text prompt is listed in Appendix~\ref{sec:fine_tune_prompt}.
The prompt is capped to 6,000 characters, so a typical prompt uses about 1,000 tokens to encode a page image, 1,800 tokens for the anchor text, for about 3,000 total input tokens.

We keep the same structured JSON output that was present in the outputs of \train{}. Each training example was truncated to 8,192 tokens to cover cases when the prompt was unusually large, and the loss was masked so only the final response tokens participated in the loss calculation.



\begin{figure}[]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/s2pdf_lora_vs_full_loss.pdf}
        \caption{Validation Loss - Web PDFs}
        \label{fig:s2pdf-loss}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/iabooks_lora_vs_full_loss.pdf}
        \caption{Validation Loss - Internet Archive Books}
        \label{fig:iabooks-loss}
    \end{minipage}
\end{figure}


\paragraph{Evaluation}
We track validation loss against a development subset of \train{} during fine-tuning;
Figure~\ref{fig:s2pdf-loss} and Figure~\ref{fig:iabooks-loss}, show the loss curves for both the web PDFs and the Internet Archive books subsets.
LoRA resulted in higher loss values compared to full fine-tuning, which we use for the final model.
 

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/eval-template.png}
    \caption{Example of side-by-side evaluation tool used during development. 
    The software used to create these comparisons is released as open-source software as part of \pipeline{}.}
    \label{fig:eval-template}
\end{figure}

To set hyperparameters and make other decisions during development, we relied on manual side-by-side evaluation as shown in Figure~\ref{fig:eval-template}. A random selection of 20 to 50 documents
were processed using two different methods, and were displayed side by side along with the render of the document page. We also open source our evaluation tool to support qualitative inspection of this visually-rich data.


\section{Deploying \pipeline}

\begin{table}
    \label{tab:inference_cost}

    \centering
    \begin{tabular}{ccccc}
        \toprule
        \textbf{Model} & \textbf{Hardware} & \textbf{Tokens/sec} & \textbf{Pages/USD} & \textbf{Cost per million pages}\\
        \midrule

        GPT-4o & API & - & 80 & \$12,480   \\
        GPT-4o & Batch & - & 160 & \$6,240 \\
        marker & API & - & 800 &\$1,250  \\
        MinerU & L40s & 238 & 1678 & \$596 \\
        \midrule
        
        \pipeline & A100 80GB & 1,487 & {\bf{3,700}} & \$270\\
        \pipeline & L40S & 906 & {\bf{5,200}} & {\bf{\$190}} \\
        \pipeline & H100 80GB  & 3,050 & {\bf{5,200}} & {\bf{\$190}}\\
        \bottomrule
    \end{tabular}
    \caption{Inference cost comparison against other OCR methods. A100 80GB Estimated at \$1.89 per hour, L40S Estimated at \$0.79 per hour, H100 80GB Estimated at \$2.69 per hour. Assuming 20\% retries.}
    \label{tab:inference_cost}
\end{table}


\subsection{Inference Pipeline}
To efficiently convert millions of documents, we develop the \olmocr pipeline using SGLang~\citep{zheng2024sglang} as the inference engine. 
The pipeline batches documents into work items of around 500 pages each.
Each work item is then queued to run on a worker with access to a GPU for inference.
Optionally, workers can coordinate using a shared cloud bucket\footnote{We use Amazon Simple Storage Service (S3), but other cloud providers or network storage solutions could be easily used.}, allowing for batch jobs that scale from single nodes to hundreds of nodes without the need for complicated queue management.

To balance maintaining high GPU utilization while also ensuring work items are completed
quickly, each worker queues up inference for all PDF pages in a work item simultaneously, and then
waits until the SGLang server has no more pending requests before proceeding to another work item in the queue.

We summarize our efforts by comparing operational costs of \olmocr against other API and local models in Table~\ref{tab:inference_cost}.
Overall, we find \olmocr to be significantly more efficient than other pipelines.
It is over 32 times cheaper than GPT-4o in batch mode; 
compared to other purposed-built pipelines and models, \olmocr is over 6 times cheaper than MinerU, and $1/3^{rd}$ of the cost of marker. 

\subsection{Increasing Robustness}

We implement several heuristics to improve reliability of \olmocr without compromising its throughput.


\paragraph{Prompt format}

During inference, we use the same abbreviated prompt described in Section~\S\ref{sec:model:training}.
This keeps the test time examples looking the same as what the model was trained on.
If the additional tokens generated by \method{} cause the overall prompt to exceed 8,192 tokens,
then we continue regenerating the \method{} tokens with exponentially lower character limits
until the overall prompt is of acceptable length.



\paragraph{Retries}
Unlike when we created \train{}, we do not enforce a specific JSON schema during inference on our fine-tuned model. 
This is for two reasons:
first, we find that open source tools designed to force decode a sequence into a particular schema are unreliable, and that enforcing a schema which is even slightly off from what the model expects can cause generations to go out-of-domain or collapse into repetitions. 
Second, and most importantly, we note that, since the model was extensively fine-tuned on the structured output,
it reliably adheres to the required schema without constraints. 
For the rare cases when JSON parsing fails, we simply retry generating from the same input sequence. 


\paragraph{Rotations}

The output JSON schema includes fields for \texttt{is\_rotation\_valid}
and \texttt{rotation\_correction}. 
During inference, \pipeline{} pipeline reads
these two fields and if \texttt{is\_rotation\_valid} is set to \texttt{true} it will
rotate the page by the amount specified in \texttt{rotation\_correction} and reprocess the page.


\paragraph{Decoding}

In developing \olmocr, the most common failure we experience is outputs degenerating into endless repetitions of the same token, line, or paragraph. 
This failure is caught automatically when the model's output either exceeds the maximum context length, or does not validate against our JSON schema. 
We find that increasing generation temperature from $\tau=0.1$ up to $\tau=0.8$ reduces the likelihood of repetitions occurring. 
Further, we modify \olmocr to reprocess failed pages up to N times, falling back to a plain text-based PDF extraction if the pipeline repeatedly fails.  
This last mitigation is aided by the fact that \method{} randomly samples which anchors to include in the prompt;
thus, resampling can sometimes help the page process correctly by removing potentially problematic meta tokens. 

We note that one one limitation of this approach is that, if retries occur often, the total generation throughput could be significantly reduced. 
Further, letting generations repeat up to maximum sequence length uses significant memory within SGLang.  
In future work, we plan to detect repeated generations sooner than at the maximum context length limit, and abort promptly.





\section{Evaluating \olmocr}
\label{sec:results}

We conduct three evaluations for \olmocr.
First, we study how faithful \olmocr is to its teacher model (Section~\S\ref{sec:result:alignment}).
Then, we compare \olmocr to other PDF text extraction systems through pairwise comparisons of their outputs (Section~\S\ref{sec:elorating}). 
Finally, we quantify the usefulness \olmocr for language modeling by continued pretraining on an OLMo 2 checkpoint~\citep{OLMo2} on content extracted and linearized with our toolkit (Section~\S\ref{sec:results:annealing}). 


\subsection{Alignment with Teacher Model}
\label{sec:result:alignment}




To compare the output of \model{} to the GPT-4o silver data in \train{}, we build a document similarity 
metric which splits a document into words, uses Hirschberg's algorithm to align those words,  and counts what proportion match.


We report alignment scores in Table~\ref{tab:eval_alignment}.
Overall, we find that \model{} has good alignment, 0.875 on average, with its teacher model. 
To calibrate this result, we also report GPT-4o self-alignment score of 0.954, which is simply from calling the model again; imperfect alignment here is due to resampling differences.
In fact, we find that our model actually better mimics the content extraction and linearization of GPT-4o than its smaller counterpart GPT-4o mini.

When partitioning scores in low, medium, and high alignment buckets (Table~\ref{tab:eval_matching}), we find that most documents parsed with \olmocr have medium to high alignment with GPT-4o.
Increasing temperature unsurprisingly leads to a wider distribution of alignment scores, as noted by the increase of low matches for $\tau = 0.8$.


\begin{table}[!h]
    \vskip 0.15in
    \centering
    \begin{tabular}{cccc}
        \toprule
        \textbf{Model} & \textbf{Temperature}~$\tau$ & \textbf{Alignment}\\
        \midrule
        \textit{GPT-4o (self-alignment)} & \textit{0.1} & \textit{0.954}  \\
        GPT-4o mini & 0.1 & 0.833 \\
        \pipeline{} & 0.8 & 0.859 \\
        \pipeline{} & 0.1 & {\bf{0.875}} \\
        \bottomrule
    \end{tabular}   
    \caption{Page-weighted alignment between GPT-4o, GPT-4o mini, and our fine-tuned model. We find that \model is more consistent with respect to its teacher than GPT-4o mini. Note that GPT-4o does not achieves a perfect alignment against itself due to the probabilistic nature of autoregressive decoding.}
    \label{tab:eval_alignment}
\end{table}


\begin{table}[!h]
    \centering
    \begin{tabular}{lrrr}
    \toprule
    \textbf{Name} & \textbf{Low match} & \textbf{Medium match} & \textbf{High match} \\
    \midrule
    \textit{GPT-4o (self alignment)} & \textit{38} & \textit{218} & \textit{965} \\
    GPT-4o mini & 214 & 478 & 529 \\
    \pipeline{} ($\tau=0.1$) & 158 & 363 & 700 \\
    \pipeline{} ($\tau=0.8$) & 195 & 390 & 636 \\
    \bottomrule
    \end{tabular}
    \caption{Match-up between olmOCR and different models compared to the \protect\train{} dataset. Low match indicates < 70\% alignment, Medium match is 70-95\% alignment, High match is >95\% alignment.}
    \label{tab:eval_matching}
\end{table}


\subsection{Intrinsic Human Evaluation}
\label{sec:elorating}
\paragraph{Experimental setup} To compare \pipeline{} against other common OCR methods, we collected pairwise human judgments of plain text produced by the three top ML-based PDF linearization tools---Marker, MinerU, and GOT-OCR 2.0---and calculating ELO ratings.

To create our evaluation set, we sample 2,017 new PDFs from the same distribution as used to create \train{} and run each PDF through \pipeline{} and the linearization tools mentioned above. 
All other linearization tools were installed from either PyPI or Github according to their publicly available instructions as of January 14th, 2025. GOT-OCR 2.0 was configured in ``format'' mode, but otherwise all comparisons were done against default settings.

We then sampled 2,000 comparison pairs (same PDF, different tool).
We asked 11 data researchers and engineers at Ai2 to assess which output was the higher quality representation of the original PDF, focusing on reading order, comprehensiveness of content and representation of structured information. The user interface used is similar to that in Figure~\ref{fig:eval-template}. Exact participant instructions are listed in Appendix~\ref{sec:eloappendix}.


\paragraph{Evaluation results}

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.45\linewidth]{figures/boxplots.png}
    \caption{ELO ranking of \pipeline{} vs other popular PDF content extraction tools.}
    \label{fig:eloranking}
\end{figure}

We collected a total of 452 judgments where a participant expressed a preference between two models (the remaining 1,548 pairs were either skipped for being too similar, or marked as invalid).
On average, this is 75 judgments per pair of tools.
We calculate ELO ratings starting from a base of 1500 and report the average of 100 simulations to avoid ordering effects in ELO calculations; for 95\% confidence intervals, we use bootstrapping with 5000 resamples.

We visualize our results in Figure~\ref{fig:eloranking}. \pipeline{} achieves an ELO score over 1800, far exceeding all other PDF linearization tools.



\subsection{Downstream Evaluation}
\label{sec:results:annealing}


\paragraph{Experimental setup}
To assess the impact of improved PDF linearization, we experiment using the pre-mid-training checkpoint of \texttt{OLMo-2-1124-7B} and perform mid-training (that is, continued pretraining while linearly decaying learning rate to zero) using content extracted from the same PDFs but with different linearization tools. 
This procedure for assessing the quality of new data sources was introduced as domain upsampling in \cite{blakeney2024doesdatasparkjoy}, annealing in \cite{dubey2024llama}, and micro-annealing in \cite{OLMo2}.

For our baseline, we use PDF extracted tokens from \texttt{peS2o}~\citep{peS2o}, a collection of over 58B tokens extracted from academic PDFs which were derived using Grobid in S2ORC~\citep{lo-etal-2020-s2orc} and further cleaned with heuristics for language modeling.
To represent \pipeline{}, we identify the same documents used in peS2o, acquire their source PDFs from the upstream S2ORC pipeline, and reprocess  them using \pipeline{}.
For both of these, we perform 50B tokens worth of mid-training.



\begin{table}[!h]
\begin{center}
\begin{tabular}{lcccccccc}
\toprule
\textbf{PeS2o version} & \textbf{Average} & \textbf{MMLU} & \textbf{$\textbf{ARC}_\text{\scriptsize\bf\sans{C}}$} & \textbf{DROP} & \textbf{HSwag} & \textbf{NQ} & \textbf{WinoG} \\
\midrule
$\text{Grobid}+\text{rules}$~\href{https://huggingface.co/datasets/allenai/peS2o}{\citep{peS2o}}
& 53.9 & {\bf{61.1}} & 75.0 & 42.3 & 57.4 & {\bf{29.4}} & {\bf{58.3}} \\
\olmocr & {\bf{55.2}} & {\bf{61.1}} & {\bf{76.4}} & {\bf{43.7}} & {\bf{62.6}} & 29.1 & 58.0 \\
\bottomrule
\end{tabular}
\caption{Comparison on \cite{OLMo2} downstream evaluation tasks of \texttt{OLMo-2-7B-1124} on 50B of original peS2o tokens vs 50B tokens from the same source PDFs but processed with \pipeline{}.}
\label{tab:downstream}
\end{center}
\end{table}

 

\paragraph{Evaluation results}

We use the same downstream evaluation setup as in \cite{OLMo2}; results are in Table~\ref{tab:downstream}.
Overall, we see an improvement in +1.3 percentage points on average across a number of core benchmark tasks, which most performance improvements in ARC Challenge and DROP.















\section{Conclusion}

We introduce \pipeline{}, an open-source toolkit for converting PDF documents into clean plain text.
Our approach combines \method{}, a novel prompting technique that leverages available metadata in born-digital PDFs, with a fine-tuned 7B parameter vision language model to achieve results competitive with closed commercial solutions at a fraction of the cost.
Our efficient inference pipeline which ships as part of this release contains everything needed to
start converting anything from single documents to million-page archives of PDFs.
We hope \pipeline{}'s ability to efficiently process millions of documents will help unlock new sources of training data for language models, particularly from high-quality PDF documents that are currently underrepresented in existing datasets that rely heavily solely on crawled web pages. 


\section*{Acknowledgments}
\label{sec:acks}

This work would not be possible without the support of our colleagues at Ai2.  We thank:
\begin{itemize}
    \item Jon Borchardt, Byron Bischoff, Aaron Sarnat, Huy Tran, Sam Skjonsberg, Eric Marsh, and Chris Newell for help setting up the live demo,
    \item Taira Anderson, Sruthi Sreeram for program management support,
    \item Will Smith and Crystal Nam for legal guidance,
    \item and Michael Schmitz, Caitlin Wittlif and Carissa Schoenick for various indirect support.
\end{itemize}

We also thank Benjamin Charles Germain Lee for helpful feedback and suggestions around evaluation and potential use cases.


\bibliographystyle{ai2style/plainnat}
\bibliography{references}

\clearpage
\appendix
\section{Appendix}

\subsection*{\train{} construction prompt for gpt-4o}
\label{sec:silver_prompt}

The prompt below was used to create the silver dataset that was then used to fine-tune our model.
The \texttt{\{base\_text\}} is replaced with the output of \method{}.


\begin{lstlisting}[breaklines=true, frame=single, basicstyle=\ttfamily\footnotesize, keywordstyle=\bfseries\color{blue}, showstringspaces=false]
Below is the image of one page of a PDF document, as well as some raw textual content that was previously extracted for it that includes position information for each image and block of text (The origin [0x0] of the coordinates is in the lower left corner of the image). 
Just return the plain text representation of this document as if you were reading it naturally.
Turn equations into a LaTeX representation, and tables into markdown format. Remove the headers and footers, but keep references and footnotes.
Read any natural handwriting.
This is likely one page out of several in the document, so be sure to preserve any sentences that come from the previous page, or continue onto the next page, exactly as they are.
If there is no text at all that you think you should read, you can output null.
Do not hallucinate.
RAW_TEXT_START
{base_text}
RAW_TEXT_END
\end{lstlisting}

\subsection*{JSON Schema}
\label{sec:json_schema}



\begin{lstlisting}[breaklines=true, frame=single, basicstyle=\ttfamily\footnotesize, keywordstyle=\bfseries\color{blue}, showstringspaces=false]
"json_schema": {
            "name": "page_response",
            "schema": {
                "type": "object",
                "properties": {
                    "primary_language": {
                        "type": ["string", "null"],
                        "description": "The primary language of the text using two-letter codes or null if there is no text at all that you think you should read.",
                    },
                    "is_rotation_valid": {
                        "type": "boolean",
                        "description": "Is this page oriented correctly for reading? Answer only considering the textual content, do not factor in the rotation of any charts, tables, drawings, or figures.",
                    },
                    "rotation_correction": {
                        "type": "integer",
                        "description": "Indicates the degree of clockwise rotation needed if the page is not oriented correctly.",
                        "enum": [0, 90, 180, 270],
                        "default": 0,
                    },
                    "is_table": {
                        "type": "boolean",
                        "description": "Indicates if the majority of the page content is in tabular format.",
                    },
                    "is_diagram": {
                        "type": "boolean",
                        "description": "Indicates if the majority of the page content is a visual diagram.",
                    },
                    "natural_text": {
                        "type": ["string", "null"],
                        "description": "The natural text content extracted from the page.",
                    },
                },
                "additionalProperties": False,
                "required": [
                    "primary_language",
                    "is_rotation_valid",
                    "rotation_correction",
                    "is_table",
                    "is_diagram",
                    "natural_text",
                ],
            },
            "strict": True,
        },
\end{lstlisting}


\subsection*{\model{} prompt}
\label{sec:fine_tune_prompt}

The prompt below is used to prompt the fine-tuned model, using the same rule 
to replace \texttt{\{base\_text\}} with the output of \method{}.

\begin{lstlisting}[breaklines=true, frame=single, basicstyle=\ttfamily\footnotesize, keywordstyle=\bfseries\color{blue}, showstringspaces=false]
Below is the image of one page of a document, as well as some raw textual content that was previously extracted for it.
Just return the plain text representation of this document as if you were reading it naturally.
Do not hallucinate.
RAW_TEXT_START
{base_text}
RAW_TEXT_END
\end{lstlisting}

\subsection*{\train{} Classification Prompt}
\label{sec:classification_prompt}

The prompt and structured schema below was used to classify a sample of documents from \train{} as reported
in Table~\ref{tab:pdf_types}.

\begin{lstlisting}[breaklines=true, frame=single, basicstyle=\ttfamily\footnotesize, keywordstyle=\bfseries\color{blue}, showstringspaces=false]
This is an image of a document page, please classify it into one of the following categories that best overall summarizes its nature: academic, legal, brochure, slideshow, table, diagram, or other. Also determine the primary language of the document and your confidence in the classification (0-1).
\end{lstlisting}

\begin{lstlisting}[breaklines=true, frame=single, basicstyle=\ttfamily\footnotesize, keywordstyle=\bfseries\color{blue}, showstringspaces=false]
class DocumentCategory(str, Enum):
    ACADEMIC = "academic"
    LEGAL = "legal"
    BROCHURE = "brochure"
    SLIDESHOW = "slideshow"
    TABLE = "table"
    DIAGRAM = "diagram"
    OTHER = "other"

class DocumentClassification(BaseModel):
    category: DocumentCategory
    language: str
    confidence: float
\end{lstlisting}

\section{ELO Evaluation Instructions}
\label{sec:eloappendix}
In Section~\ref{sec:elorating}, we asked participants to compare the output of various common OCR tools
against \pipeline{}. Participants were given the instructions below, and presented with a document page, and the output of two random tools. They could then select which output was better, or select
"Both Good", "Both Bad", or "Invalid PDF" any of which would not count the comparison in the ELO ranking.

\subsection*{Instructions to participants}

\begin{lstlisting}[breaklines=true, frame=single, basicstyle=\ttfamily\footnotesize, keywordstyle=\bfseries\color{blue}, showstringspaces=false]
Compare the text in the two fields, and select which one better represents the contents of the document.

REMINDER: This is not about "the most faithful OCR", but "this OCR output seems really useful for training LMs"

- Does the text capture all of the meaningful content in the document in a natural order?
- Are the words correct (no weird incorrect words or split words)
- Is the whitepsace sensical?
- Is the useless header/footer content removed?
- Do the tables/equations look okay?

There is not a strict preference between Markdown and LaTeX, most importantly you should evaluate it on the text content, not which method was used to format it.

If you are not sure, or the document is in a language other than english, you can skip that entry, or mark "both good" "both bad", "invalid pdf".
\end{lstlisting}

\subsection*{ELO data}

\begin{table}[htbp]
    \caption{Pairwise Win/Loss Statistics Between Models}
    \label{tab:pairwise-stats}

    \centering
    \begin{tabular}{lcc}
        \toprule
        Model Pair & Wins & Win Rate (\%) \\
        \midrule
        \pipeline{} vs. \marker{} & 49/31 & \textbf{61.3} \\
        \pipeline{} vs. \gotocr{} & 41/29 & \textbf{58.6} \\
        \pipeline{} vs. \mineru{} & 55/22 & \textbf{71.4} \\
        
        \marker{} vs. \mineru{} & 53/26 & 67.1 \\
        \marker{} vs. \gotocr{} & 45/26 & 63.4 \\
        \gotocr{} vs. \mineru{} & 38/37 & 50.7 \\

        \midrule
        Total & 452 & \\
        \bottomrule
    \end{tabular}
  
\end{table}

\newpage
\section{Example output}

Below are some sample outputs on particularly challenging data. \pipeline{}, MinerU, GOT-OCR 2.0 and Marker run with default settings.


\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{p{3.6cm}p{3.6cm}p{3.6cm}p{3.6cm}}
    \multicolumn{4}{c}{
      \begin{tabular}{@{}l@{}}
        \includegraphics[height=8cm]{samples/rawoldpage.png}
      \end{tabular}
    } \\
    \textbf{\pipeline} & \textbf{MinerU} & \textbf{GOT-OCR 2.0} & \textbf{Marker} \\
    \midrule
        \tiny
        Christians behaving themselves like Mahomedans.

        4. The natives soon had reason to suspect the viceroy's sincerity in his expressions of regret at the proceedings of which they complained. For about this time the Dominican friars, under pretence of building a convent, erected a fortress on the island of Solor, which, as soon as finished, the viceroy garrisoned with a strong force. The natives very naturally felt indignant at this additional encroachment, and took every opportunity to attack the garrison. The monks, forgetful of their peaceable profession, took an active part in these skirmishes, and many of them fell sword in hand.

        The Mahomedan faith has been appropriately entitled, The religion of the sword; and with equal propriety may we so designate the religion of these belligerent friars. The Portuguese writers give an account of one of their missionaries, Fernando Vinagre, who was as prompt in the field of battle as at the baptismal font. This man, though a secular priest, undertook the command of a squadron that was sent to the assistance of the rajah of Tidore, on which occasion he is said to have acted in the twofold capacity of a great commander, and a great apostle, at one time appearing in armour, at another in a surplice; and even occasionally, baptizing the converts of his sword without putting off his armour, but covering it with his ecclesiastical vest. In this crusade he had two
    &
  \tiny
      ININDIASY BOOKU
Christians bchaving.themselves like Mahome dans.3

4.The natives soon had reason to suspect ihe viceroy's sincerity in his expressions of regret at the proceedings of which they complained. For about this time the Dominican friars,under pretenceof building a convent,erected a for tress on the island of Solorwhich,as soon as finishedthe viceroy garrisoned with a strong force. The natives very naturally felt indig nant at this additional encroachment, and took every pportunity to attack the garrison.The monks,forgetful of their peaceable profession took an activa part in these skirmishes, and many of tbein feil sword in hand.

TheMahornedan faithhas been appropriately ntitled.The religion of the swordand with equal propriety may we so designate the region of these belligerent friars.The Portugueswriters give an account of one of their missionarzes,femando Vinagre,who was as prompt in the field of battle as at the baptismal font. This man, though a secular priest, undertook the command of a squadron that was sent to the assistance of the rajah of Tidore,4 on which occasion he is said to have acted in the twofold capacity of a great commander, and a great apostle, at one time appearing in armour, at another in a surplice;and even occasionally baptizing the converts of his sword without put ting off his armour, but covering it with his ecclesiastical vest.In this crusadehe had two
   &
\tiny
      IN INDIA:  BOOK U 269 Christians behaving themselves like Mahome-  1670.  4. The natives son had reason to suspect the Viceroy' s vice roy' s sincerity in his expressions of regret in s in e eri ty at the proceedings of which they complained.  fl it ars.  For about this time the Dominican f mars, under pre ten ce of building a convent, erected a for-  tress on the island of Sol or, which, as soon as finished, the vice roy garrisoned with a strong force. The natives very naturally felt indig-  nant at this additional encroachment, and took every opportunity to attack the garrison. The monks, forgetful of their peaceable profession,  took an active part in these skirmishes, and many of the n fell sword in hand.  The Mh on med an faith has been appropriately entitled. The religion of the sword; and with e ral Tropriety may we so designate the re-  gian of these belligerent friars. The Port u-  gue s writers give an account of one of their mission are s, Fer endo Vina gre, who was as prompt in the fe ld of battle as at the baptismal font. This man, though a secular priest, un-  der took the command of a squadron that was sent to the assistance of the rajah of Tidore, on which occasion he is said to have acted in the twofold capacity of a great commander, and a great apostle, at one time appearing in armour,  at another in a surplice; and even occasionally,  baptizing the converts of his sword without put-  ting off his armour, but covering it with his ecclesiastical vest. In this crusade he had two 3 Ged des History, \& c. , pp. 24-27.  P ude th aec opp rob ria nobis Vel die ipo tui sse.  Called Tadur u or Daco, an island in the Indian Ocean,  one of the Mol ucc as These a laDra goon conversions. Ged des History, p. 27.
    &
    \tiny
\#\# **IN INDIA *** BOOK TI. S69
Christians behaving themselves like Ma borne- a. dans.3 .\textquotedblleft5/0-
*t>.*

The natives soon had reason to suspect the viceroy, viceroy's sincerity in his expressions of regret at the proceedings of which they complained. \textquotedblleft n.\textquotedblleft' For about this time the Dominican friars, under pretence of building a. convent, erected a fortress on the island of Sol or, which, as soon as finished, the viceroy garrisoned with a strong force. The natives' very naturally felt indig-S nant at this additional encroachment, and took every opportunity to attack the garrison. The monks, forgetful/ of their peaceable profession, took an active part in these skirmishes, and many of tbg.tr fell sword in hand.

The i'lfinomedan faith has been appropriately entitled., \textquoteleft The religion of the sword\textquoteright,; and with equal propriety may we so designate the re- .\ i'gv.m of these belligerent friars. The Portugu writers give an account of one of their \textquoteleft missionaries,\textquoteright\ Fernando Vinagre, who was as prompt in the field of battle as at the baptismal font. This man, though a secular priest, undertook the command of a squadron that was I sent to the assistance of the rajah of Tidore,4 on which occasion he is said to have acted in the twofold capacity of a great commander, and a great apostle, at one time appearing in armour, ; at another in a surplice; and even occasionally, baptizing the converts of his sword without putting off his armour, but covering it with his ecclesiastical vest. In this crusade5 he had two
\> 3 Geddes History, \&c., pp. 24---27. Pudet h\ae c opprobria nobis Vel dici potuisse.
\> 4 Called \textquoteleft T a d u ra\textquoteright\ or \textquoteleft D a c o,\textquoteright\ an island in the Indian Ocean, one of the Moluccas
\> 5 \textquoteleft These \textquoteleft a la D ra g o o n\textquoteright\ conversions.\textquoteright\ Geddes' History, p. 27.
  \\
    \bottomrule
  \end{tabular}
\end{table}

\newpage


\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{p{3.6cm}p{3.6cm}p{3.6cm}p{3.6cm}}
    \multicolumn{4}{c}{
      \begin{tabular}{@{}l@{}}
        \includegraphics[height=7cm]{samples/openstax-calculus-vol1-273-277_preview.png}
      \end{tabular}
    } \\
    \textbf{\pipeline} & \textbf{MinerU} & \textbf{GOT-OCR 2.0} & \textbf{Marker} \\
    \midrule
        \tiny
3.4 EXERCISES

For the following exercises, the given functions represent the position of a particle traveling along a horizontal line.

a. Find the velocity and acceleration functions.

b. Determine the time intervals when the object is slowing down or speeding up.

150. \( s(t) = 2t^3 - 3t^2 - 12t + 8 \)

151. \( s(t) = 2t^3 - 15t^2 + 36t - 10 \)

152. \( s(t) = \frac{t}{1 + t^2} \)

153. A rocket is fired vertically upward from the ground. The distance \( s \) in feet that the rocket travels from the ground after \( t \) seconds is given by \( s(t) = -16t^2 + 560t \).

a. Find the velocity of the rocket 3 seconds after being fired.

b. Find the acceleration of the rocket 3 seconds after being fired.

154. A ball is thrown downward with a speed of 8 ft/s from the top of a 64-foot-tall building. After \( t \) seconds, its height above the ground is given by \( s(t) = -16t^2 - 8t + 64 \).

a. Determine how long it takes for the ball to hit the ground.

b. Determine the velocity of the ball when it hits the ground.

155. The position function \( s(t) = t^2 - 3t - 4 \) represents the position of the back of a car backing out of a driveway and then driving in a straight line, where \( s \) is in feet and \( t \) is in seconds. In this case, \( s(t) = 0 \) represents the time at which the back of the car is at the garage door, so \( s(0) = -4 \) is the starting position of the car, 4 feet inside the garage.

a. Determine the velocity of the car when \( s(t) = 0 \).

b. Determine the velocity of the car when \( s(t) = 14 \).

156. The position of a hummingbird flying along a straight line in \( t \) seconds is given by \( s(t) = 3t^3 - 7t \) meters.

a. Determine the velocity of the bird at \( t = 1 \) sec.

b. Determine the acceleration of the bird at \( t = 1 \) sec.

c. Determine the acceleration of the bird when the velocity equals 0.

157. A potato is launched vertically upward with an initial velocity of 100 ft/s from a potato gun at the top of an 85-foot-tall building. The distance in feet that the potato travels from the ground after \( t \) seconds is given by \( s(t) = -16t^2 + 100t + 85 \).
\ldots
    &
  \tiny
\# 3.4 EXERCISES  

For the following exercises, the given functions represent the position of a particle traveling along a horizontal line.  

a. Find the velocity and acceleration functions. b. Determine the time intervals when the object is slowing down or speeding up.  

150. $s(t)=2t^{3}-3t^{2}-12t+8$   
151. $s(t)=2t^{3}-15t^{2}+36t-10$   
152. $s(t)=\frac{t}{1+t^{2}}$  

153. A rocket is fired vertically upward from the ground. The distance $s$ in feet that the rocket travels from the ground after $t$ seconds is given by $s(t)=-16t^{2}+560t,$ .  

a. Find the velocity of the rocket 3 seconds after being fired.   
b. Find the acceleration of the rocket 3 seconds after being fired.  

154. A ball is thrown downward with a speed of 8 ft/ s from the top of a 64-foot-tall building. After $t$ seconds, its height above the ground is given by $s(t)=-16t^{2}-8t+64.$ .  

a. Determine how long it takes for the ball to hit the ground.   
b. Determine the velocity of the ball when it hits the ground.  

155. The position function $s(t)=t^{2}-3t-4$ represents the position of the back of a car backing out of a driveway and then driving in a straight line, where $s$ is in feet and $t$ is in seconds. In this case, $s(t)=0$ represents the time at which the back of the car is at the garage door, so $s(0)=-4$ is the starting position of the car, 4 feet inside the garage.  

a. Determine the velocity of the car when $s(t)=0$ .   
b. Determine the velocity of the car when $s(t)=14$ .  

156. The position of a hummingbird flying along a straight line in $t$ seconds is given by $s(t)=3t^{3}-7t$ meters.  

a. Determine the velocity of the bird at $t=1$ sec. b. Determine the acceleration of the bird at $t=1$ sec. c. Determine the acceleration of the bird when the velocity equals 0.  

157. A potato is launched vertically upward with an initial velocity of 100 ft/s from a potato gun at the top of an 85-foot-tall building. The distance in feet that the potato travels from the ground after $t$ seconds is given by $s(t)=-16t^{2}+100t+85.$ .  

\ldots
   &
\tiny
Chapter 3 | Derivatives
273
3.4 EXERCISES
For the following exercises, the given functions represent
the position of a particle traveling along a horizontal line.
a.
Find the velocity and acceleration functions.
b.
Determine the time intervals when the object is
slowing down or speeding up.
150.
s(t) = 2t3 −3t2 −12t + 8
151.
s(t) = 2t3 −15t2 + 36t −10
152.
s(t) =
t
1 + t2
153.
A rocket is ﬁred vertically upward from the ground.
The distance
s
in
feet
that
the
rocket
travels
from
the
ground after
t seconds is given by
s(t) = −16t2 + 560t.
a.
Find the velocity of the rocket 3 seconds after being
ﬁred.
b.
Find the acceleration of the rocket 3 seconds after
being ﬁred.
154.
A ball is thrown downward with a speed of 8 ft/
s from the top of a 64-foot-tall building. After t seconds,
its
height
above
the
ground
is
given
by
s(t) = −16t2 −8t + 64.
a.
Determine how long it takes for the ball to hit the
ground.
b.
Determine the velocity of the ball when it hits the
ground.
155.
The position function
s(t) = t2 −3t −4 represents
the position of the back of a car backing out of a driveway
and then driving in a straight line, where
s
is in feet and
t is in seconds. In this case, s(t) = 0 represents the time
at which the back of the car is at the garage door, so
s(0) = −4 is the starting position of the car, 4 feet inside
the garage.
a.
Determine the velocity of the car when
s(t) = 0.
b.
Determine the velocity of the car when
s(t) = 14.
156.
The position of a hummingbird ﬂying along a straight
line in
t seconds is given by
s(t) = 3t3 −7t
2
2
2
2
2
2
2
2
2
2
3
3
3
3
3
3
3
3
3
3
4
4
4
4
4
4
4
4
4
4
5
5
5
5
5
5
5
5
5
5
4
4
4
4
4
4
4
4
4
3
3
3
3
3
3
3
3
3
1
1
1
1
1
1
1
1
1
1
3
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
2
2
2
2
2
2
2
2
2
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
0
3
3
3
3
3
3
3
3
3
2
2
2
2
2
2
2
2
2
4
4
4
4
4
4
4
4
4
2
2
2
2
2
2
2
2
2
0
1
1
1
1
1
1
1
1
3
4
4
4
4
4
4
4
4
3
4
4
4
4
4
4
4
4
2
3
3
3
3
3
3
3
3
0
1
1
1
1
1
1
1
1
0
1
1
1
1
1
1
1
1
2
1
1
1
1
1
1
1
1
0
2
2
2
2
2
2
2
2
2
5
5
5
5
5
5
5
5
5
1
1
1
1
1
1
1
1
1
5
5
5
5
5
5
5
5
5
0
0
0
0
0
0
0
0
0
5
5
5
5
5
5
5
5
5
3
3
3
3
3
3
3
3
3
5
5
5
5
5
5
5
5
5
2
2
2
2
2
2
2
2
2
a.
Use the graph of the position function to determine
the time intervals when the velocity is positive,
negative, or zero.
b.
Sketch the graph of the velocity function.
c.
Use the graph of the velocity function to determine
the time intervals when the acceleration is positive,
negative, or zero.
d.
Determine the time intervals when the object is
speeding up or slowing down.
\ldots
    &
    \tiny
\#\# **3.4 EXERCISES**

For the following exercises, the given functions represent the position of a particle traveling along a horizontal line.

- a. Find the velocity and acceleration functions.
- b. Determine the time intervals when the object is slowing down or speeding up.

$$150. \quad s(t) = 2t^3 - 3t^2 - 12t + 8$$

$$151. \quad s(t) = 2t^3 - 15t^2 + 36t - 10t$$

$$152. \quad s(t) = \frac{t}{1+t^2}$$

153. A rocket is fired vertically upward from the ground. The distance *s* in feet that the rocket travels from the ground after *t* seconds is given by *s*(*t*) = −16*t* 2 + 560*t*.

- a. Find the velocity of the rocket 3 seconds after being fired.
- b. Find the acceleration of the rocket 3 seconds after being fired.

154. A ball is thrown downward with a speed of 8 ft/ s from the top of a 64-foot-tall building. After *t* seconds, its height above the ground is given by *s*(*t*) = −16*t* 2 − 8*t* + 64.

- a. Determine how long it takes for the ball to hit the ground.
- b. Determine the velocity of the ball when it hits the ground.

155. The position function *s*(*t*) = *t* 2 − 3*t* − 4 represents the position of the back of a car backing out of a driveway and then driving in a straight line, where *s* is in feet and *t* is in seconds. In this case, *s*(*t*) = 0 represents the time at which the back of the car is at the garage door, so *s*(0) = −4 is the starting position of the car, 4 feet inside the garage.

- a. Determine the velocity of the car when *s*(*t*) = 0.
- b. Determine the velocity of the car when *s*(*t*) = 14.

156. The position of a hummingbird flying along a straight line in *t* seconds is given by *s*(*t*) = 3*t* 3 − 7*t* meters.

- a. Determine the velocity of the bird at *t* = 1 sec.
- b. Determine the acceleration of the bird at *t* = 1 sec.
- c. Determine the acceleration of the bird when the velocity equals 0.

\ldots
  \\
    \bottomrule
  \end{tabular}
\end{table}


\newpage

\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{p{3.6cm}p{3.6cm}p{3.6cm}p{3.6cm}}
    \multicolumn{4}{c}{
      \begin{tabular}{@{}l@{}}
        \includegraphics[height=8cm]{samples/lincoln_letter_preview.jpg}
      \end{tabular}
    } \\
    \textbf{\pipeline} & \textbf{MinerU} & \textbf{GOT-OCR 2.0} & \textbf{Marker} \\
    \midrule
        \small
Executive Mansion,

Washington City,

January 15th, 1864

Major General Hitchcock, Commissioner of Exchanges, is authorized and directed to offer Brigadier General Trimble, now a prisoner of war in Fort McHenry, in exchange for Major White, who is held as a prisoner at Richmond. He is also directed to send forward the offer of exchange by Henry M. Warfield, Esq. of Baltimore, under a flag of truce, and give him a pass to City Point.

Abraham Lincoln
    &
  \small
\textit{No text produced.}
   &
\small
43571
Bachington City
January 10th 1864.
Major General Architect, Commissioner of aivachangera
is authorized and directed by ffeed Bngader General Trelmble,
new a firemen of war in Fert nchery in exchange for
Mayor White, who held a a firemen at Hillmannd.
He is aker conducted by end forward the offer of exchange
by Henry in. Warfield, Lag. of Balthmore, under a flag
of three, and five him afaies to City Bink.
Abraham Lincoln
    &
    \small
necuhve Mansion Vastington amany layor Seneral Hitchcocks Commissioner of Cachanges, is anthonged and directed to offer Bingadier General Trin prisoner of war in Fort Inctienny, in exchange now w Major White, who is held as a preises at Richmond Ite is also directed to vand forwards the offer of exchange by Stenny in. Warfield, Eag. of Baltimore, under aflag 11 mice, and give him apass to tity Point. Abrakan Sincolus 
  \\
    \bottomrule
  \end{tabular}
\end{table}

\end{document}
