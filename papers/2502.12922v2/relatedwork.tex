\section{Related Work}
\label{sec:related_work}

Locus~\cite{Wen2016} is the first work that proposed to localise the bug at the
software change level. It takes a bug report as an input query and locates the
relevant change hunk based on the token similarities. IR-based techniques, such
as Locus, and \name can complement each other depending on circumstances. When
the failure cannot be reproduced from the bug report, IR-based techniques can
be used instead of \name. However, if the coverage of the failing and passing
tests are available, we can apply \name with SBFL to more precisely rank the
commits without relying on IR.
ChangeLocator~\cite{Wu2017} aims to find a BIC for crashes using the call stack
information. It is a learning-based approach that requires data from fixed
crashes. Unlike ChangeLocator, \name is not limited to crashes and can be
applied to general failures. Orca~\cite{Bhagwan2018} takes symptoms of bugs,
such as an exception message or customer complaints, as an input query and
outputs a ranked list of commits ordered by their relevance to the query. It
uses the TF-IQF~\cite{Yang2008} to compute the relevance scores of files, and
aggregate them to a commit level. Subsequently, it uses machine learning to
predict the risk of candidate commits for breaking ties.
Bug2Commit~\cite{Murali2021} uses multiple features extracted from bug reports
and commits, and aggregates all features by taking the average of their vector
representations. Although Bug2Commit uses an unsupervised learning approach, it
needs the historical data of project-specific bug reports and commits to train
the word embedding model. FBL-BERT~\cite{Ciborowska2022} retrieves the relevant
changeset for the input bug report using a fine-tuned BERT model that can
capture the semantics in the text. It proposes fine-grained changeset encoding methods and accelerates the retrieval by offline indexing~\cite{johnson2019billion}. The major difference between \name and the techniques
mentioned above is that \name does not require any training. Further, \name can
be combined with any code-level FL technique, without being coupled to
specific sources of information, as long as the coverage of failing executions
is available.

The weighted bisection algorithm we propose is similar to FACF (Flaky Aware Culprit Finding)~\cite{Henderson2023}, which formulates the \emph{flake-aware} bisection problem as a Bayesian inference, in that both guide the bisection process based on the probability of commits being a source of test failure. The difference between the two algorithms is that ours uses commit scores from \name to establish the initial probability distribution, while  FACF updates the probability based on the test results during the search taking into account the potential for flakiness. The original work notes that FACF can take into account any prior information about the bug inducing change in the form of an initial probability distribution. Hence, we believe that the commit scores generated by \name can be used as an effective prior distribution for the FACF framework.

There exist studies that are highly relevant to \name despite not being specifically about the BIC identification domain. FaultLocator~\cite{zhang2011localizing} is similar to \name as both use
code-level FL scores to identify suspicious changes. FaultLocator combines spectrum information with the change impact
analysis to precisely identify the failure-inducing \emph{atomic} edits out
of all edits between two versions, whereas \name aims to pinpoint BICs in the
commit history. WhoseFault~\cite{Servant2012} is a method that utilises code-level FL scores and commit history to determine the developer responsible for a bug. While it provides insights into the assignment of bugs, it does not specifically target BIC identification. As a result, it cannot be directly compared with \name in our evaluation, nor can it be integrated with our bisection algorithm. Our belief is that accurately identifying the BIC can also be used to find the developer responsible for fixing the bug, based on the authorship of the changes, in addition to helping developers understand the context in which the failure occurred.



% To use FaultLocator, we need to identify the earliest version that a test starts to fail. In that sense, FaultLocator and \name can be used together.

% An et al.~\cite{An2021} proposed to reduce the search space of BIC only using the coverage of failing executions without requiring any input that needs human effort, such as bug report or fixing commit. They only included the commits which either introduced or modified the program elements covered by failing executions in the BIC search space. The evaluation using Defects4J benchmark showed that the substantial amount of commits, 87.6\% on average, can be removed from the BIC search space. However, as they also pointed out, if the failing executions have broad coverage, there could be still a large number of commits in the reduced search space so that it is hard to accurately pinpoint the BIC.


% Locus and FBL-BERT assumes that there is a bug report written in natural language.
% While ChangeLocator uses the crash call stack trace, which can be automatically obtained, it can be applied only to the crash errors.
% Orca and Bug2Commit needs training step which requires a large amount of historical data. For example, Orca uses 92M commits to train its commit risk evaluation model. Bug2Commit need a global pool of commits and bug reports (and their features) and trains word weights for the vectorizer. For example, in their evaluation, the word embedding is trained using lots of crash reports (or regression reports) and more than XXX commits. However, it limits it applicability to where such historical is not available.


% - Change Locator, Orca: need supervised learning
% - Locus, FBL-BERT: need human-written natural language bug report
% - Bug2Commit: good baseline! Since the implementation of Bug2Commit is not publicly available, we make our own prototype of Bug2Commit. As it was developed for the use in Facebook, As a feature of bug report, we use the name of the failing tests, crash message and crash call stack (only when crash), and the error message (regression error). As a feature of commit, we use the commit message, changed hunk (with the context), and .... but you know what? Bug2commit also requires lots of training data to learn the word embedding

% Bug2Commit is the state-of-the-art IR techniques developed by Facebook.
% if it appears in a short feature as opposed to a lengthy feature.
% For this purpose, we use a BM25 [27] based vectorizer rather
% than tf-idf. BM25 is a popular scoring function used by search
% engines such as Lucene [23], an


% \begin{itemize}
%     \item Locus~\cite{Wen2016}: \textbf{Unlike our work, this can be only applied when human-written bug reports are availabe.}
%     \item ChangeLocator~\cite{Wu2017}: \textbf{while it only targets the crashing error,} \name can be applied any general faults as it use the coverage information instead of the crash call stack. \textbf{it requires training step. A supervised learning-based approach}
%     \item Orca~\cite{Bhagwan2018} \textbf{Orca also uses a learning
%     approach to break ties in ranking}

%     \refi{- Input query: A symptom of the bug (the name of the anomalous probe, an exception message, a stack-trace, the words of a customer complaint (NL))}

%     \refi{- Documents: commit (Names of the files changed, commit and review comments (NL), modified code, modified configuration parameters)}

%     \refi{- Output: a ranked list of commits}

%     \refi{- Method: Use the \textbf{machine-learning based} model of commit risk prediction trained on \textbf{two-years of history}, for each candidate commit, for each file (differnce set) and token in the symtom, calculate the td-idf score of the tuple. IDF is calculated \textbf{by analysing Orion's logs}. The commit score will be the maximum value of the TF-IDF of the token in symtoms}

%     \refi{- Limitation: \textbf{requires significant amount of historical data to train the machine-learning model and to maintain the idf value of tokens}}
%     - Differential Code Analaysis for search-space pruning: Performing a syntactic analysis on the differences that extracts the added, modified, or deleted source code by a commit.

%     - The Build Provenance Graph for search-space explosion: The graph captures dependencies between various builds.

%     - Input: A search query that describes symtoms of the problem, consistien of probe names, exception texts, log messages, etc

%     - Algorithm: find all builds related to the symptomatic build, enumerate all commits that created the builds, calculate the relevance score between the commits and symtom.
%     \item Bug2Commit~\cite{Murali2021} \refi{In the training phase it continously gathers a global pool of commits and bug reports, and trains word weights for the vectorizer.} \textbf{NEED TRAINING ALTHOUGH IT IS UNSUPERVISED LEARNING} what is there is no such corpus of bug rerports?

% \end{itemize}

% % by sm
% https://ieeexplore.ieee.org/abstract/document/9426017