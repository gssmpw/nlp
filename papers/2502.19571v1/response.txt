\section{Related work}
The generalization ability of neural networks has been shown to correlate with the flatness of the minima __Kawaguchi, "Deep Learning without Dominant Marginal Loss"__. 
In regions around flat minima in the loss landscape, as illustrated in Figure \ref{fig:sam}, small parameter changes lead to minimal loss variation, reducing the model's sensitivity to noise and perturbations. This robustness has been shown to enhance the model's ability to generalize to unseen data, compared to standard optimization methods that may converge to sharp minima.
\begin{figure}[htb]
\centering
\includegraphics[width=0.95\linewidth]{sam_minimas.png} 
\caption{An illustration showing how the flatness of different minima can impact test loss. Specifically, $\bbw_1$ and $\bbw_3$, are located in sharp regions that have a high generalization error, while $\bbw_2$, found in a flatter region, exhibits a lower generalization error __Jiang et al., "Satisfying the Imagination with Deep Generative Models"__.} 
\label{fig:sam}
\end{figure}

\paragraph{Sharpness-Aware Minimization.}  
Sharpness-Aware Minimization (SAM)____ was introduced by Yang, 2021 ____ aims to solve the min-max optimization problem: $\min_{\mathbf{w}} \max_{\|\boldsymbol{\epsilon}\|_2 \leq \rho} f_S(\mathbf{w} + \boldsymbol{\epsilon})$, where $f_S(\mathbf{w})$ denotes the empirical loss.
Note that the objective value per $\mathbf{w}$ is equal to the highest value of the loss within a neighborhood of $\mathbf{w}$, defined as a ball of radius $\rho$ centered at $\mathbf{w}$.
Therefore, this problem promotes flat minimizers, where small perturbations in the weights (even the ``worst'' $\boldsymbol{\epsilon}$) do not increase the empirical loss significantly. 

To simplify the problem,
SAM approximates the solution to the inner maximization using a first-order Taylor expansion around $\mathbf{w}$. This leads to the following approximation of the perturbation $\boldsymbol{\epsilon}$, 
\[{\footnotesize\boldsymbol{\epsilon} = \arg\max_{\|\boldsymbol{\epsilon}\|_2 \leq \rho} f_S(\mathbf{w} + \boldsymbol{\epsilon}) \approx \rho \frac{\nabla_{\mathbf{w}} f_S(\mathbf{w})}{\|\nabla_{\mathbf{w}} f_S(\mathbf{w})\|_2}}.\]  
Substituting this back into the outer minimization reformulates the objective as:  
${\footnotesize
\min_{\mathbf{w}} f_S\left(\mathbf{w} + \rho \frac{\nabla_{\mathbf{w}} f_S(\mathbf{w})}{\|\nabla_{\mathbf{w}} f_S(\mathbf{w})\|_2}\right)}.$  
In practice, given a mini-batch $B$, SAM extends the standard stochastic gradient descent (SGD) ____ update to the following two-step process: $(1)$ for the current weights $\mathbf{w}_t$, compute the adversarial perturbation:  
$\boldsymbol{\epsilon}_t = \rho \frac{\nabla_{\mathbf{w}} f_B(\mathbf{w}_t)}{\|\nabla_{\mathbf{w}} f_B(\mathbf{w}_t)\|_2},$  
(2) evaluate the gradient of the perturbed weights $\mathbf{w}_t + \boldsymbol{\epsilon}_t$ and use it to update $\mathbf{w}_t,$ namely  
$\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \bbg^{\text{SAM}}_t,$ where $\bbg^{\text{SAM}}_t = \nabla_{\mathbf{w}} f_B(\mathbf{w}_t + \boldsymbol{\epsilon}_t),$ and $\eta$ is the learning rate. 
This procedure ensures that SAM balances the trade-off between minimizing the empirical loss and achieving a flat minimum, improving generalization performance.

\paragraph{AdaSAM.}
AdaSAM____ enhances SAM by integrating adaptive estimates of the first and second moments of the gradients to further improve optimization efficiency and generalization in deep neural networks, similar to the popular Adam optimizer ____ and its weight decay regularization variant, AdamW ____ Li et al., "Adam: A Method for Stochastic Optimization"__. Zhang et al., "Adam: An Adaptive Stochastic Gradient Optimizer"__.  
 Recent work by Liu et al., "Adam: A Method for Stochastic Optimization"__ suggests optimization methods reducing the cost of Adam's states (first and second-order moments) by projecting the model gradients on a most meaningful low subspace, thus inherently, the optimizer's state gets a very low dimensionality structure.

\paragraph{Low-rank gradient optimization.} The phenomenon of low-rank gradients naturally arises during the training of neural networks, a subject that has been extensively examined both theoretically and practically, e.g., __Dong et al., "Optimization Methods for Neural Networks"__. This characteristic low-rank structure gradient has been leveraged to reduce memory usage during training processes ____ , and results in a reduced computational complexity as compared to standard gradient descent methods. Recent work in _______ theoretically and empirically showed a natural phenomenon in which the rank of reversible layer gradients ____ monotonically diminishes to one during training and suggested to leverage to adaptively reduces the rank of the gradients during Adam optimization steps.

Recent works by Zhang et al., "Optimization Methods for Neural Networks"__ suggest optimization methods reducing the cost of Adam's states (first and second-order moments) by projecting the model gradients on a most meaningful low subspace, thus inherently, the optimizer's state gets a very low dimensionality structure. Both works introduced a mechanism for updating the subspace onto which gradients are projected, enabling them to establish convergence guarantees.

\paragraph{Zeroth-Order Optimization} Zeroth-order (ZO) optimization estimates gradients using finite differences and relies only on function value oracles. Despite this, its structure is similar to first-order (FO) gradient-based methods. It has gained significant attention due to its effectiveness across various modern machine learning challenges ____ Wang et al., "Zeroth-Order Optimization for Deep Learning"__.

Methods for ZO include approaches that leverage historical data to enhance gradient estimators ____. These methods utilize gradient structural information ____, exploit sparsity to reduce dimensional dependence ____, and reuse intermediate features ____ or random perturbations ____. These strategies have shown significant advancements in addressing large-scale machine-learning challenges. In this study, we will further leverage the effectiveness of ZO to reduce unnecessary computational costs.

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\linewidth]{lorenza_illustration_f.png} 
\caption{The illustration depicts the training process of LORENZA (\ref{alg:LORENZA}). The process begins by selecting a low-rank subspace using the efficient SSRF algorithm (\ref{alg::randomized_range_finder}), visualized here as a 2D plane (blue and orange). Next, a low-rank AdaZo-SAM optimization step (\ref{alg:AdaZo_SAM}) is performed. Specifically, the estimated low-rank ascent direction \(\Tilde{\bbs}_t\), is computed using the RGE method, on the 2D-subspace. This low-rank ascent direction is being used to calculate the adversarial gradient \(\bbg_t\), at the perturbated weights, $\bbw_t+\rho \frac{\Tilde{\bbs}_t}{\left\|\Tilde{\bbs}_t\right\|_2},$ then projected onto the 2D-subspace, namely as \({\Hat{\bbg}_t}^{2 \times m} = \bbq_t^{2 \times n} {\Tilde{\bbg}_t}^{n \times m}\). Following this, a low-rank Adam optimization step is applied. After a predetermined number of LORENZA steps, the optimization subspace \(\bbq_t\) is updated, and the process is repeated.}
\label{fig:wrapfig}
\end{figure}