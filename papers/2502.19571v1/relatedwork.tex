\section{Related work}
The generalization ability of neural networks has been shown to correlate with the flatness of the minima \citep{hochreiter1997flat,keskar2017largebatchtrainingdeeplearning,sun2023adasamboostingsharpnessawareminimization,si2023,yue2024sharpnessawareminimizationrevisitedweighted}. 
In regions around flat minima in the loss landscape, as illustrated in Figure \ref{fig:sam}, small parameter changes lead to minimal loss variation, reducing the model's sensitivity to noise and perturbations. This robustness has been shown to enhance the model's ability to generalize to unseen data, compared to standard optimization methods that may converge to sharp minima.
\begin{figure}[htb]
\centering
\includegraphics[width=0.95\linewidth]{sam_minimas.png} 
\caption{An illustration showing how the flatness of different minima can impact test loss. Specifically, $\bbw_1$ and $\bbw_3$, are located in sharp regions that have a high generalization error, while $\bbw_2$, found in a flatter region, exhibits a lower generalization error \citep{rs16162877}.} 
\label{fig:sam}
\end{figure}

\paragraph{Sharpness-Aware Minimization.}  
Sharpness-Aware Minimization (SAM)~\cite{foret2021sam} aims to solve the min-max optimization problem: $\min_{\mathbf{w}} \max_{\|\boldsymbol{\epsilon}\|_2 \leq \rho} f_S(\mathbf{w} + \boldsymbol{\epsilon})$, where $f_S(\mathbf{w})$ denotes the empirical loss.
Note that the objective value per $\mathbf{w}$ is equal to the highest value of the loss within a neighborhood of $\mathbf{w}$, defined as a ball of radius $\rho$ centered at $\mathbf{w}$.
Therefore, this problem promotes flat minimizers, where small perturbations in the weights (even the ``worst'' $\boldsymbol{\epsilon}$) do not increase the empirical loss significantly. 

To simplify the problem,
SAM approximates the solution to the inner maximization using a first-order Taylor expansion around $\mathbf{w}$. This leads to the following approximation of the perturbation $\boldsymbol{\epsilon}$, 
\[{\footnotesize\boldsymbol{\epsilon} = \arg\max_{\|\boldsymbol{\epsilon}\|_2 \leq \rho} f_S(\mathbf{w} + \boldsymbol{\epsilon}) \approx \rho \frac{\nabla_{\mathbf{w}} f_S(\mathbf{w})}{\|\nabla_{\mathbf{w}} f_S(\mathbf{w})\|_2}}.\]  
Substituting this back into the outer minimization reformulates the objective as:  
${\footnotesize
\min_{\mathbf{w}} f_S\left(\mathbf{w} + \rho \frac{\nabla_{\mathbf{w}} f_S(\mathbf{w})}{\|\nabla_{\mathbf{w}} f_S(\mathbf{w})\|_2}\right)}.$  
In practice, given a mini-batch $B$, SAM extends the standard stochastic gradient descent (SGD) \cite{battash2024revisiting} update to the following two-step process: $(1)$ for the current weights $\mathbf{w}_t$, compute the adversarial perturbation:  
$\boldsymbol{\epsilon}_t = \rho \frac{\nabla_{\mathbf{w}} f_B(\mathbf{w}_t)}{\|\nabla_{\mathbf{w}} f_B(\mathbf{w}_t)\|_2},$  
(2) evaluate the gradient of the perturbed weights $\mathbf{w}_t + \boldsymbol{\epsilon}_t$ and use it to update $\mathbf{w}_t,$ namely  
$\mathbf{w}_{t+1} = \mathbf{w}_t - \eta \bbg^{\text{SAM}}_t,$ where $\bbg^{\text{SAM}}_t = \nabla_{\mathbf{w}} f_B(\mathbf{w}_t + \boldsymbol{\epsilon}_t),$ and $\eta$ is the learning rate. 
This procedure ensures that SAM balances the trade-off between minimizing the empirical loss and achieving a flat minimum, improving generalization performance.

\paragraph{AdaSAM.}
AdaSAM~\cite{sun2023adasamboostingsharpnessawareminimization} enhances SAM by integrating adaptive estimates of the first and second moments of the gradients to further improve optimization efficiency and generalization in deep neural networks, similar to the popular Adam optimizer~\citep{kingma2017adammethodstochasticoptimization} and its weight decay regularization variant, AdamW~\citep{loshchilov2019decoupledweightdecayregularization}. Specifically, the algorithm alternates between calculating a perturbation and updating parameters using the Adam optimization rule. Formally, AdaSAM modifies the SAM optimization process by incorporating the notion of AdaM~\cite{tan2019convergence}, introducing a momentum term,
$\bbm_t = \beta_1 \bbm_{t-1} + (1 - \beta_1)\bbg^{\text{SAM}}_t,$
which is weighted by a momentum factor \(\beta_1\). Additionally, it tracks a second-moment estimate using a smoothing parameter \(\beta_2\), namely
$\bbv_t = \beta_2 \bbv_{t-1} + (1 - \beta_2)\left[\bbg^{\text{SAM}}_t\right]^2.$ This allows it to dynamically adjust using historical gradient information.

AdaSAM achieves a convergence rate of $\mathcal{O}(1 / \sqrt{bT})$, where $b$ is the batch size, providing a linear speedup with increased batch sizes, making it suitable for large-scale training scenarios.

This adaptive variant of SAM requires the storage of both $\bbm_t$ and $\bbv_t$ at each time step, resulting in a memory cost of $2mn$. Additionally, the perturbation introduces an extra memory usage of $mn$, bringing the total memory access for AdaSAM optimization to $4mn$. It is important to note that this inefficiency not only leads to high memory requirements but also increases computational time. Compared to gradient-based optimizers, SAM and its variants involve two gradients, which means two backpropagation procedures are performed during a single update step.

\paragraph{Surrogate Gap Guided Sharpness-Aware Minimization (GSAM).}  
GSAM \cite{zhuang2022surrogate} extends SAM by jointly minimizing the perturbed loss \( f_p(\mathbf{w}) \) and the surrogate gap \( h(\mathbf{w}) = f_p(\mathbf{w}) - f(\mathbf{w}) \), which measures sharpness. The algorithm first computes the perturbed weight \( \mathbf{w}_{\text{adv}} = \mathbf{w} + \rho \frac{\nabla f(\mathbf{w})}{\|\nabla f(\mathbf{w})\|_2} \), where \( \rho \) defines the neighborhood radius, and evaluates the gradient \( \nabla f_p(\mathbf{w}) = \nabla f(\mathbf{w}_{\text{adv}}) \). To reduce the surrogate gap without affecting the minimization of the perturbed loss, the gradient \( \nabla f(\mathbf{w}) \) is decomposed into parallel and orthogonal components with respect to \( \nabla f_p(\mathbf{w}) \), expressed as \( \nabla f(\mathbf{w}) = \nabla_\parallel f(\mathbf{w}) + \nabla_\perp f(\mathbf{w}) \), and only $\nabla_\perp f(\mathbf{w})$ is utilized for minimizing $h(\mathbf{w})$. Specifically, the final update adjusts the weights to \( \mathbf{w}_{t+1} = \mathbf{w}_t - \eta \left( \nabla f_p(\mathbf{w}_t) - \alpha \nabla_\perp f(\mathbf{w}_t) \right) \), where \( \alpha \) controls the term that promotes minimization of $h(\mathbf{w})$. This approach ensures the model converges to a flat minimum with better generalization by maintaining low \( f_p(\mathbf{w}) \) while explicitly reducing a measure of sharpness.

\paragraph{Memory efficient optimizers.}
Recently, several works have focused on developing memory-efficient optimization techniques. Multiple studies have aimed to reduce the memory requirements of gradient statistics in adaptive optimization algorithms \citep{shazeerAdafactorAdaptiveLearning,anilMemoryEfficientAdaptive}. One common approach is quantization, which helps decrease the memory footprint of optimizer states \citep{liMemoryEfficientOptimizers2023}. Additionally, recent advancements have suggested reducing the memory used by weight gradients by integrating the backward operation with the optimizer update \citep{lvAdaLomoLowmemoryOptimization2023,lvFullParameterFinetuning2023}. This characteristic has been leveraged to reduce memory usage during training processes \citep{gooneratneLowrankGradientApproximation2020, huangLowRankGradientDescent2023, modoranuErrorFeedbackCan2024}.
Efforts to reduce SAM's memory demands have been reported as well. They all appear to focus solely on the overhead caused by the perturbation (ascent step) computation. FSAM \cite{Zhong2022ImprovingSM} and SSAM \cite{zhao2023randomizedsharpnessawaretrainingboosting} leverage Fisher information to selectively perturb a parameter subset, achieving 50\%-90\% memory savings from the overhead at the cost of increased computation. Recent work on $\nu$-SAM \cite{anonymous2024sam} employs nuclear norm constraints during the ascent step for greater memory efficiency. Similarly, SAM-ON \cite{mueller2023} focuses perturbations solely on normalization layers. However, these approaches do not address the memory complexity of the baseline optimizer performing the descent step.
Furthermore, they often trade memory savings related to the ascent step with increased computational complexity 
or struggle to generalize across diverse fine-tuning tasks. 
Our method bridges this gap by introducing a low-rank gradient optimization framework that is applied in both ascent and descent directions. We also estimate randomized ascent direction (gradient perturbation), leading to both memory efficiency and computational simplicity while enabling robust generalization in pre-training and fine-tuning of LLMs.

\paragraph{Low-rank gradient optimization.} The phenomenon of low-rank gradients naturally arises during the training of neural networks, a subject that has been extensively examined both theoretically and practically, e.g., \cite{zhaoZerOInitializationInitializing2022, cossonLowRankGradientDescent2023, yang2023spectral}. This characteristic low-rank structure gradient has been leveraged to reduce memory usage during training processes \cite{gooneratneLowrankGradientApproximation2020, huangLowRankGradientDescent2023, modoranuErrorFeedbackCan2024}, and results in a reduced computational complexity as compared to standard gradient descent methods. Recent work in \cite{refael2025adarankgrad} theoretically and empirically showed a natural phenomenon in which the rank of reversible layer gradients \citep{tian2021} monotonically diminishes to one during training and suggested to leverage to adaptively reduces the rank of the gradients during Adam optimization steps.

Recent works by \cite{zhao2024galore,refael2025adarankgrad} suggest optimization methods reducing the cost of Adam's states (first and second-order moments) by projecting the model gradients on a most meaningful low subspace, thus inherently, the optimizer's state gets a very low dimensionality structure. Both works introduced a mechanism for updating the subspace onto which gradients are projected, enabling them to establish convergence guarantees. While these methods effectively reduce memory consumption and perform well on relatively homogeneous tasks, they struggle to maintain accuracy on more complex challenges, such as reasoning or coding, when compared to FFT. Additionally, they do not generalize effectively to out-of-sample scenarios, domain shifts, or zero-shot tasks.

\paragraph{Zeroth-Order Optimization} Zeroth-order (ZO) optimization estimates gradients using finite differences and relies only on function value oracles. Despite this, its structure is similar to first-order (FO) gradient-based methods. It has gained significant attention due to its effectiveness across various modern machine learning challenges \citep{liu2020primer}.

Methods for ZO include approaches that leverage historical data to enhance gradient estimators \citep{meier2019improving, cheng2021convergence}. These methods utilize gradient structural information \citep{singhal2023guess}, exploit sparsity to reduce dimensional dependence \citep{cai2021zeroth, cai2022zeroth, chen2023deepzero}, and reuse intermediate features \citep{chen2023deepzero} or random perturbations \citep{malladi2024finetuning}. These strategies have shown significant advancements in addressing large-scale machine-learning challenges. In this study, we will further leverage the effectiveness of ZO to reduce unnecessary computational costs.

\begin{figure}[htb]
\centering
\includegraphics[width=0.8\linewidth]{lorenza_illustration_f.png} 
\caption{The illustration depicts the training process of LORENZA (\ref{alg:LORENZA}). The process begins by selecting a low-rank subspace using the efficient SSRF algorithm (\ref{alg::randomized_range_finder}), visualized here as a 2D plane (blue and orange). Next, a low-rank AdaZo-SAM optimization step (\ref{alg:AdaZo_SAM}) is performed. Specifically, the estimated low-rank ascent direction \(\Tilde{\bbs}_t\), is computed using the RGE method, on the 2D-subspace. This low-rank ascent direction is being used to calculate the adversarial gradient \(\bbg_t\), at the perturbated weights, $\bbw_t+\rho \frac{\Tilde{\bbs}_t}{\left\|\Tilde{\bbs}_t\right\|_2},$ then projected onto the 2D-subspace, namely as \({\Hat{\bbg}_t}^{2 \times m} = \bbq_t^{2 \times n} {\Tilde{\bbg}_t}^{n \times m}\). Following this, a low-rank Adam optimization step is applied. After a predetermined number of LORENZA steps, the optimization subspace \(\bbq_t\) is updated, and the process is repeated.}
\label{fig:wrapfig}
\end{figure}