@article{Zhong2022ImprovingSM,
  title={Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models},
  author={Qihuang Zhong and Liang Ding and Li Shen and Peng Mi and Juhua Liu and Bo Du and Dacheng Tao},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.05497}
}

@article{anilMemoryEfficientAdaptive,
  author    = {Rohan Anil and Vineet Gupta and Tomer Koren and Yoram Singer},
  title     = {Memory efficient adaptive optimization},
  journal   = {Advances in Neural Information Processing Systems},
  year      = {2019}
}

@inproceedings{battash2024revisiting,
  title={Revisiting the noise model of stochastic gradient descent},
  author={Battash, Barak and Wolf, Lior and Lindenbaum, Ofir},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4780--4788},
  year={2024},
  organization={PMLR}
}

@article{cai2021zeroth,
  title={A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization},
  author={Cai, HanQin and Lou, Yuchen and McKenzie, Daniel and Yin, Wotao},
  journal={arXiv preprint arXiv:2102.10707},
  year={2021}
}

@article{cai2022zeroth,
  title={Zeroth-order regularized optimization (zoro): Approximately sparse gradients and adaptive sampling},
  author={Cai, HanQin and Mckenzie, Daniel and Yin, Wotao and Zhang, Zhenliang},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={2},
  pages={687--714},
  year={2022},
  publisher={SIAM}
}

@article{chen2023deepzero,
  title={DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training},
  author={Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia},
  journal={ICLR},
  year={2024}
}

@article{cheng2021convergence,
  title={On the convergence of prior-guided zeroth-order optimization algorithms},
  author={Cheng, Shuyu and Wu, Guoqiang and Zhu, Jun},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={14620--14631},
  year={2021}
}

@article{cossonLowRankGradientDescent2023,
  author    = {Romain Cosson and Ali Jadbabaie and Anuran Makur and Armin Reisizadeh and Devavrat Shah},
  title     = {Low-{Rank Gradient Descent}},
  journal   = {IEEE Open Journal of Control Systems},
  year      = {2023}
}

@misc{foret2021sam,
      title={Sharpness-Aware Minimization for Efficiently Improving Generalization}, 
      author={Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur},
      year={2021},
      eprint={2010.01412},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{gooneratneLowrankGradientApproximation2020,
  author    = {M. Gooneratne and K. C. Sim and P. Zadrazil and A. Kabel and F. Beaufays and G. Motta},
  title     = {Low-rank gradient approximation for memory-efficient on-device training of deep neural network},
  booktitle = {2020 {IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP} 2020, Barcelona, Spain, May 4-8, 2020},
  publisher = {IEEE},
  year      = {2020}
}

@article{hochreiter1997flat,
  title={Flat minima},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={1},
  pages={1--42},
  year={1997},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@article{huangLowRankGradientDescent2023,
  author    = {Shuang Huang and Brian D. Hoskins and Michael W. Daniels and Matthew D. Stiles and George C. Adam},
  title     = {Low-{Rank Gradient Descent} for {Memory-Efficient Training} of {Deep In-Memory Arrays}},
  journal   = {ACM Journal on Emerging Technologies in Computing Systems},
  year      = {2023}
}

@misc{keskar2017largebatchtrainingdeeplearning,
      title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima}, 
      author={Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang},
      year={2017},
      eprint={1609.04836},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{kingma2017adammethodstochasticoptimization,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{liMemoryEfficientOptimizers2023,
  author    = {Bin Li and Jie Chen and Jun Zhu},
  title     = {Memory efficient optimizers with 4-bit states},
  journal   = {Advances in Neural Information Processing Systems},
  year      = {2024}
}

@inproceedings{liu2020primer,
  title={A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning: Principals, Recent Advances, and Applications},
  author={Liu, Sijia and Chen, Pin-Yu and Kailkhura, Bhavya and Zhang, Gaoyuan and Hero III, Alfred O and Varshney, Pramod K},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={43--54},
  year={2020},
  publisher={IEEE}
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{lvAdaLomoLowmemoryOptimization2023,
  author    = {Ke Lv and Haoyi Yan and Qibin Guo and Hao Lv and Xipeng Qiu},
  title     = {{AdaLomo}: {Low-memory Optimization} with {Adaptive Learning Rate}},
  journal   = {ArXiv preprint arXiv:2310.10195},
  year      = {2023},
  note      = {arXiv:2310.10195}
}

@article{lvFullParameterFinetuning2023,
  author    = {Ke Lv and Yongwei Yang and Tianyu Liu and Qingyu Gao and Qibin Guo and Xipeng Qiu},
  title     = {Full {Parameter Fine-tuning} for {Large Language Models} with {Limited Resources}},
  journal   = {ArXiv preprint arXiv:2306.09782},
  year      = {2023},
  note      = {arXiv:2306.09782}
}

@article{malladi2024finetuning,
  title={Fine-Tuning Language Models with Just Forward Passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2305.17333},
  year={2023}
}

@article{meier2019improving,
  title={Improving gradient estimation in evolutionary strategies with past descent directions},
  author={Meier, Florian and Mujika, Asier and Gauy, Marcelo Matheus and Steger, Angelika},
  journal={arXiv preprint arXiv:1910.05268},
  year={2019}
}

@article{modoranuErrorFeedbackCan2024,
  author    = {Ionut-Vlad Modoranu and Alexander Kalinov and Ermin Kurtic and Erwin Frantar and Dan Alistarh},
  title     = {Error {Feedback Can Accurately Compress Preconditioners}},
  journal   = {ArXiv preprint arXiv:2306.06098},
  year      = {2023},
  note      = {arXiv:2306.06098}
}

@misc{mueller2023,
      title={Normalization Layers Are All That Sharpness-Aware Minimization Needs}, 
      author={Maximilian Mueller and Tiffany Vlaar and David Rolnick and Matthias Hein},
      year={2023},
      eprint={2306.04226},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@Article{rs16162877,
AUTHOR = {Dong, Mingrong and Yang, Yixuan and Zeng, Kai and Wang, Qingwang and Shen, Tao},
TITLE = {Implicit Sharpness-Aware Minimization for Domain Generalization},
JOURNAL = {Remote Sensing},
VOLUME = {16},
YEAR = {2024},
NUMBER = {16},
ARTICLE-NUMBER = {2877},
ISSN = {2072-4292},
DOI = {10.3390/rs16162877}
}

@article{shazeerAdafactorAdaptiveLearning,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  journal={Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018, Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018},
  publisher={PMLR},
  year={2018}
}

@misc{si2023,
      title={Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima}, 
      author={Dongkuk Si and Chulhee Yun},
      year={2023},
      eprint={2306.09850},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{singhal2023guess,
  title={How to guess a gradient},
  author={Singhal, Utkarsh and Cheung, Brian and Chandra, Kartik and Ragan-Kelley, Jonathan and Tenenbaum, Joshua B and Poggio, Tomaso A and Yu, Stella X},
  journal={arXiv preprint arXiv:2312.04709},
  year={2023}
}

@misc{sun2023adasamboostingsharpnessawareminimization,
      title={AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks}, 
      author={Hao Sun and Li Shen and Qihuang Zhong and Liang Ding and Shixiang Chen and Jingwei Sun and Jing Li and Guangzhong Sun and Dacheng Tao},
      year={2023},
      eprint={2303.00565},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@inproceedings{tan2019convergence,
  title={On the convergence speed of amsgrad and beyond},
  author={Tan, Tao and Yin, Shiqun and Liu, Kunling and Wan, Man},
  booktitle={2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI)},
  pages={464--470},
  year={2019},
  organization={IEEE}
}

@misc{tian2021,
      title={Understanding Self-supervised Learning with Dual Deep Networks}, 
      author={Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli},
      year={2021},
      eprint={2010.00578},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{yang2023spectral,
  author    = {Greg Yang and Jacob B. Simon and Jeremy Bernstein},
  title     = {A spectral condition for feature learning},
  journal   = {arXiv preprint arXiv:2310.17813},
  year      = {2023},
  note      = {arXiv:2310.17813}
}

@misc{yue2024sharpnessawareminimizationrevisitedweighted,
      title={Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term}, 
      author={Yun Yue and Jiadi Jiang and Zhiling Ye and Ning Gao and Yongchao Liu and Ke Zhang},
      year={2024},
      eprint={2305.15817},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{zhao2023randomizedsharpnessawaretrainingboosting,
      title={Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning}, 
      author={Yang Zhao and Hao Zhang and Xiuyuan Hu},
      year={2023},
      eprint={2203.09962},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@misc{zhao2024galore,
      title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection}, 
      author={Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
      year={2024},
      eprint={2403.03507},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
}

@article{zhaoZerOInitializationInitializing2022,
  author    = {Jingzhao Zhao and Frederik T. Schaefer and Anima Anandkumar},
  title     = {Zero initialization: Initializing neural networks with only zeros and ones},
  journal   = {Transactions on Machine Learning Research},
  year      = {2022}
}

