[
  {
    "index": 0,
    "papers": [
      {
        "key": "hochreiter1997flat",
        "author": "Hochreiter, Sepp and Schmidhuber, J{\\\"u}rgen",
        "title": "Flat minima"
      },
      {
        "key": "keskar2017largebatchtrainingdeeplearning",
        "author": "Nitish Shirish Keskar and Dheevatsa Mudigere and Jorge Nocedal and Mikhail Smelyanskiy and Ping Tak Peter Tang",
        "title": "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"
      },
      {
        "key": "sun2023adasamboostingsharpnessawareminimization",
        "author": "Hao Sun and Li Shen and Qihuang Zhong and Liang Ding and Shixiang Chen and Jingwei Sun and Jing Li and Guangzhong Sun and Dacheng Tao",
        "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks"
      },
      {
        "key": "si2023",
        "author": "Dongkuk Si and Chulhee Yun",
        "title": "Practical Sharpness-Aware Minimization Cannot Converge All the Way to Optima"
      },
      {
        "key": "yue2024sharpnessawareminimizationrevisitedweighted",
        "author": "Yun Yue and Jiadi Jiang and Zhiling Ye and Ning Gao and Yongchao Liu and Ke Zhang",
        "title": "Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "rs16162877",
        "author": "Dong, Mingrong and Yang, Yixuan and Zeng, Kai and Wang, Qingwang and Shen, Tao",
        "title": "Implicit Sharpness-Aware Minimization for Domain Generalization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "foret2021sam",
        "author": "Pierre Foret and Ariel Kleiner and Hossein Mobahi and Behnam Neyshabur",
        "title": "Sharpness-Aware Minimization for Efficiently Improving Generalization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "battash2024revisiting",
        "author": "Battash, Barak and Wolf, Lior and Lindenbaum, Ofir",
        "title": "Revisiting the noise model of stochastic gradient descent"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "sun2023adasamboostingsharpnessawareminimization",
        "author": "Hao Sun and Li Shen and Qihuang Zhong and Liang Ding and Shixiang Chen and Jingwei Sun and Jing Li and Guangzhong Sun and Dacheng Tao",
        "title": "AdaSAM: Boosting Sharpness-Aware Minimization with Adaptive Learning Rate and Momentum for Training Deep Neural Networks"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "kingma2017adammethodstochasticoptimization",
        "author": "Diederik P. Kingma and Jimmy Ba",
        "title": "Adam: A Method for Stochastic Optimization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "loshchilov2019decoupledweightdecayregularization",
        "author": "Ilya Loshchilov and Frank Hutter",
        "title": "Decoupled Weight Decay Regularization"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "tan2019convergence",
        "author": "Tan, Tao and Yin, Shiqun and Liu, Kunling and Wan, Man",
        "title": "On the convergence speed of amsgrad and beyond"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "zhuang2022surrogate",
        "author": "Juntang Zhuang and Boqing Gong and Liangzhe Yuan and Yin Cui and Hartwig Adam and Nicha C Dvornek and sekhar tatikonda and James s Duncan and Ting Liu",
        "title": "Surrogate Gap Minimization Improves Sharpness-Aware Training"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "shazeerAdafactorAdaptiveLearning",
        "author": "Shazeer, Noam and Stern, Mitchell",
        "title": "Adafactor: Adaptive learning rates with sublinear memory cost"
      },
      {
        "key": "anilMemoryEfficientAdaptive",
        "author": "Rohan Anil and Vineet Gupta and Tomer Koren and Yoram Singer",
        "title": "Memory efficient adaptive optimization"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "liMemoryEfficientOptimizers2023",
        "author": "Bin Li and Jie Chen and Jun Zhu",
        "title": "Memory efficient optimizers with 4-bit states"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "lvAdaLomoLowmemoryOptimization2023",
        "author": "Ke Lv and Haoyi Yan and Qibin Guo and Hao Lv and Xipeng Qiu",
        "title": "{AdaLomo}: {Low-memory Optimization} with {Adaptive Learning Rate}"
      },
      {
        "key": "lvFullParameterFinetuning2023",
        "author": "Ke Lv and Yongwei Yang and Tianyu Liu and Qingyu Gao and Qibin Guo and Xipeng Qiu",
        "title": "Full {Parameter Fine-tuning} for {Large Language Models} with {Limited Resources}"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "gooneratneLowrankGradientApproximation2020",
        "author": "M. Gooneratne and K. C. Sim and P. Zadrazil and A. Kabel and F. Beaufays and G. Motta",
        "title": "Low-rank gradient approximation for memory-efficient on-device training of deep neural network"
      },
      {
        "key": "huangLowRankGradientDescent2023",
        "author": "Shuang Huang and Brian D. Hoskins and Michael W. Daniels and Matthew D. Stiles and George C. Adam",
        "title": "Low-{Rank Gradient Descent} for {Memory-Efficient Training} of {Deep In-Memory Arrays}"
      },
      {
        "key": "modoranuErrorFeedbackCan2024",
        "author": "Ionut-Vlad Modoranu and Alexander Kalinov and Ermin Kurtic and Erwin Frantar and Dan Alistarh",
        "title": "Error {Feedback Can Accurately Compress Preconditioners}"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "Zhong2022ImprovingSM",
        "author": "Qihuang Zhong and Liang Ding and Li Shen and Peng Mi and Juhua Liu and Bo Du and Dacheng Tao",
        "title": "Improving Sharpness-Aware Minimization with Fisher Mask for Better Generalization on Language Models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhao2023randomizedsharpnessawaretrainingboosting",
        "author": "Yang Zhao and Hao Zhang and Xiuyuan Hu",
        "title": "Randomized Sharpness-Aware Training for Boosting Computational Efficiency in Deep Learning"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "anonymous2024sam",
        "author": "Anonymous",
        "title": "\\ensuremath{\\nu}{SAM}: Memory-Efficient Sharpness-Aware Minimization via Nuclear Norm Constraints"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "mueller2023",
        "author": "Maximilian Mueller and Tiffany Vlaar and David Rolnick and Matthias Hein",
        "title": "Normalization Layers Are All That Sharpness-Aware Minimization Needs"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zhaoZerOInitializationInitializing2022",
        "author": "Jingzhao Zhao and Frederik T. Schaefer and Anima Anandkumar",
        "title": "Zero initialization: Initializing neural networks with only zeros and ones"
      },
      {
        "key": "cossonLowRankGradientDescent2023",
        "author": "Romain Cosson and Ali Jadbabaie and Anuran Makur and Armin Reisizadeh and Devavrat Shah",
        "title": "Low-{Rank Gradient Descent}"
      },
      {
        "key": "yang2023spectral",
        "author": "Greg Yang and Jacob B. Simon and Jeremy Bernstein",
        "title": "A spectral condition for feature learning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "gooneratneLowrankGradientApproximation2020",
        "author": "M. Gooneratne and K. C. Sim and P. Zadrazil and A. Kabel and F. Beaufays and G. Motta",
        "title": "Low-rank gradient approximation for memory-efficient on-device training of deep neural network"
      },
      {
        "key": "huangLowRankGradientDescent2023",
        "author": "Shuang Huang and Brian D. Hoskins and Michael W. Daniels and Matthew D. Stiles and George C. Adam",
        "title": "Low-{Rank Gradient Descent} for {Memory-Efficient Training} of {Deep In-Memory Arrays}"
      },
      {
        "key": "modoranuErrorFeedbackCan2024",
        "author": "Ionut-Vlad Modoranu and Alexander Kalinov and Ermin Kurtic and Erwin Frantar and Dan Alistarh",
        "title": "Error {Feedback Can Accurately Compress Preconditioners}"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "refael2025adarankgrad",
        "author": "Yehonathan Refael and Jonathan Svirsky and Boris Shustin and Wasim Huleihel and Ofir Lindenbaum",
        "title": "AdaRankGrad: Adaptive Gradient Rank and Moments for Memory-Efficient {LLM}s Training and Fine-Tuning"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "tian2021",
        "author": "Yuandong Tian and Lantao Yu and Xinlei Chen and Surya Ganguli",
        "title": "Understanding Self-supervised Learning with Dual Deep Networks"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian",
        "title": "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection"
      },
      {
        "key": "refael2025adarankgrad",
        "author": "Yehonathan Refael and Jonathan Svirsky and Boris Shustin and Wasim Huleihel and Ofir Lindenbaum",
        "title": "AdaRankGrad: Adaptive Gradient Rank and Moments for Memory-Efficient {LLM}s Training and Fine-Tuning"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "liu2020primer",
        "author": "Liu, Sijia and Chen, Pin-Yu and Kailkhura, Bhavya and Zhang, Gaoyuan and Hero III, Alfred O and Varshney, Pramod K",
        "title": "A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning: Principals, Recent Advances, and Applications"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "meier2019improving",
        "author": "Meier, Florian and Mujika, Asier and Gauy, Marcelo Matheus and Steger, Angelika",
        "title": "Improving gradient estimation in evolutionary strategies with past descent directions"
      },
      {
        "key": "cheng2021convergence",
        "author": "Cheng, Shuyu and Wu, Guoqiang and Zhu, Jun",
        "title": "On the convergence of prior-guided zeroth-order optimization algorithms"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "singhal2023guess",
        "author": "Singhal, Utkarsh and Cheung, Brian and Chandra, Kartik and Ragan-Kelley, Jonathan and Tenenbaum, Joshua B and Poggio, Tomaso A and Yu, Stella X",
        "title": "How to guess a gradient"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "cai2021zeroth",
        "author": "Cai, HanQin and Lou, Yuchen and McKenzie, Daniel and Yin, Wotao",
        "title": "A Zeroth-Order Block Coordinate Descent Algorithm for Huge-Scale Black-Box Optimization"
      },
      {
        "key": "cai2022zeroth",
        "author": "Cai, HanQin and Mckenzie, Daniel and Yin, Wotao and Zhang, Zhenliang",
        "title": "Zeroth-order regularized optimization (zoro): Approximately sparse gradients and adaptive sampling"
      },
      {
        "key": "chen2023deepzero",
        "author": "Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia",
        "title": "DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "chen2023deepzero",
        "author": "Chen, Aochuan and Zhang, Yimeng and Jia, Jinghan and Diffenderfer, James and Liu, Jiancheng and Parasyris, Konstantinos and Zhang, Yihua and Zhang, Zheng and Kailkhura, Bhavya and Liu, Sijia",
        "title": "DeepZero: Scaling up Zeroth-Order Optimization for Deep Model Training"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "malladi2024finetuning",
        "author": "Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev",
        "title": "Fine-Tuning Language Models with Just Forward Passes"
      }
    ]
  }
]