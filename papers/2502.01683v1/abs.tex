\renewcommand{\thefootnote}{\arabic{footnote}}
\begin{abstract}
% 大语言模型的快速发展催生了massive LLM supply and application needs, where 需要定制化的benchmark作为桥梁进行匹配
The rapid advancement of large language models (LLMs) has led to a surge in both model supply and application demands. To facilitate effective matching between them, reliable, generic and efficient benchmark generators are widely needed.
% 然而，人类标注者受限制于低效性，而当前的LLM-driven benchmark generators在缺乏通用性的同时，也因为缺乏comprehensive的evaluation framework使得reliability难以得到验证和优化。
However, human annotators are constrained by inefficiency, and current LLM benchmark generators not only lack generalizability but also struggle with limited reliability, as they lack a comprehensive evaluation framework for validation and optimization.
To fill this gap, 
% 我们首先从四个维度提出了具有10个criteria的自动化的、无偏的evaluation framework for benchmark generators。Under this framework, 我们carefully analyze了the pros and cons of directly prompting LLM as generic benchmark generators。To 提升它的reliability，我们提出了一系列methods来解决发现的weaknesses，并集成为\textsc{BenchMaker}。
we first propose an automated and unbiased evaluation framework, structured around four dimensions and ten criteria. Under this framework, we carefully analyze the advantages and weaknesses of directly prompting LLMs as generic benchmark generators. To enhance the reliability, we introduce a series of methods to address the identified weaknesses and integrate them as \textsc{BenchMaker}. 
% 全面的实验证明\textsc{BenchMaker}在所有指标上都取得了优于或与human-annotated benchmarks comparable的表现。More importantly, they yield highly consistent evaluation results across 12 LLMs (0.967 Pearson correlation with MMLU-Pro), with \textsc{BenchMaker} taking only $0.005 and 0.38 minutes per sample.
Experiments across multiple LLMs and tasks confirm that \textsc{BenchMaker} achieves superior or comparable performance to human-annotated benchmarks on all metrics, highlighting its generalizability and reliability. More importantly, it delivers highly consistent evaluation results across 12 LLMs (0.967 Pearson correlation against MMLU-Pro), while taking only \$0.005 and 0.38 minutes per sample. See our codes in \url{https://github.com/ypw0102/BenchMaker}.
\end{abstract}