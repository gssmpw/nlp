\section{Derivation of the Probability for \( E[a] > E[b] \)}
\label{sec:prof}
In this appendix, we present a detailed derivation of the probability that the expected accuracy \( E[a] \) of model A is greater than the expected accuracy \( E[b] \) of model B in a noiseless benchmark, given the noisy benchmark observations. We will use the following notation throughout:

\begin{itemize}
    \item \( N \): The total number of samples in the benchmark.
    \item \( K \): The proportion of samples with incorrect labels, i.e., the noise ratio, where \( K \in [0,1] \).
    \item \( \bar{a} \): The observed accuracy of model A on the noisy benchmark.
    \item \( \bar{b} \): The observed accuracy of model B on the noisy benchmark.
    \item \( p \): The probability that both models predict the incorrect label correctly on a noisy sample. This probability is assumed to be identical for both models on incorrect labels.
    \item \( E[a] \): The expected accuracy of model A on the noiseless benchmark.
    \item \( E[b] \): The expected accuracy of model B on the noiseless benchmark.
\end{itemize}

Given these notations, our goal is to determine the probability that \( E[a] > E[b] \) based on the noisy observed accuracies \( \bar{a} \) and \( \bar{b} \).

\subsection*{Step 1: Relating \( \bar{a} \) and \( \bar{b} \) to \( E[a] \) and \( E[b] \)}

From the given setup, we know that the observed accuracies \( \bar{a} \) and \( \bar{b} \) can be written as a weighted average of the expected accuracies \( E[a] \) and \( E[b] \) on the correct samples, and the probability \( p \) on the incorrect samples. Specifically, the formulas for \( \bar{a} \) and \( \bar{b} \) are:

\[
\bar{a} = (1 - K) \cdot E[a] + K \cdot p
\]
\[
\bar{b} = (1 - K) \cdot E[b] + K \cdot p
\]

These equations express the observed accuracy of each model as the weighted average of the correct label samples and the noisy (incorrect label) samples. The weight \( (1 - K) \) represents the proportion of correct labels, and \( K \) represents the proportion of incorrect labels.

\subsection*{Step 2: Solving for \( E[a] \) and \( E[b] \)}

To isolate \( E[a] \) and \( E[b] \), we rearrange the above equations:

\[
E[a] = \frac{\bar{a} - K \cdot p}{1 - K}
\]
\[
E[b] = \frac{\bar{b} - K \cdot p}{1 - K}
\]

Thus, \( E[a] \) and \( E[b] \) are directly related to the observed accuracies \( \bar{a} \) and \( \bar{b} \), and the noise ratio \( K \).

\subsection*{Step 3: Comparing \( E[a] \) and \( E[b] \)}

To determine the probability that \( E[a] > E[b] \), we first compute the difference between \( E[a] \) and \( E[b] \):

\[
E[a] - E[b] = \frac{\bar{a} - K \cdot p}{1 - K} - \frac{\bar{b} - K \cdot p}{1 - K}
\]

Simplifying this expression:

\[
E[a] - E[b] = \frac{\bar{a} - \bar{b}}{1 - K}
\]

Thus, \( E[a] > E[b] \) if and only if \( \bar{a} - \bar{b} > 0 \), which indicates that the observed accuracy of model A must be greater than that of model B in the noisy benchmark for \( E[a] \) to exceed \( E[b] \) in the noiseless benchmark.

\subsection*{Step 4: Statistical Hypothesis Testing}

To quantify the probability of \( E[a] > E[b] \), we perform a hypothesis test on \( \bar{a} - \bar{b} \), assuming that both \( \bar{a} \) and \( \bar{b} \) are derived from binomial distributions (since they represent the correct classification probabilities on the noisy benchmark). We assume that the difference \( \bar{a} - \bar{b} \) follows a normal distribution under certain conditions (via the Central Limit Theorem). Thus, the expected value of \( \bar{a} - \bar{b} \) is:

\[
\mathbb{E}[\bar{a} - \bar{b}] = \frac{\bar{a} - \bar{b}}{1 - K}
\]

The variance of \( \bar{a} - \bar{b} \), assuming independent samples, is given by:

\begin{equation}
\begin{split}
\text{Var}[\bar{a} - \bar{b}] =& \frac{\text{Var}[\bar{a}] + \text{Var}[\bar{b}]}{(1 - K)^2} \\
=&\frac{\bar{a}(1-\bar{a})/N +\bar{b}(1-\bar{b})/N}{(1 - K)^2}
\end{split}
\end{equation}

where \( \text{Var}[\bar{a}] \) and \( \text{Var}[\bar{b}] \) are the variances of the observed accuracies of models A and B, respectively. These variances can be computed from the binomial distributions underlying \( \bar{a} \) and \( \bar{b} \).

Now, we define the \( z \)-score for the observed difference \( \bar{a} - \bar{b} \) as follows:

\begin{equation}
\begin{split}
z =& \frac{\mathbb{E}[\bar{a} - \bar{b}]}{\sqrt{\text{Var}[\bar{a} - \bar{b}]}}\\
=&\frac{(\bar{a} - \bar{b})/(1-K)}{\sqrt{(\bar{a}(1-\bar{a}) + \bar{b}(1-\bar{b}))/(N(1-K)^2)}} \\
=&\frac{(\bar{a} - \bar{b})\sqrt{N}}{\sqrt{\bar{a}(1-\bar{a}) + \bar{b}(1-\bar{b})}}
\end{split}
\end{equation}

This \( z \)-score follows a standard normal distribution. The probability that \( E[a] > E[b] \) is the probability that \( \bar{a} - \bar{b} > 0 \), which is equivalent to:

\[
P(\bar{a} - \bar{b} > 0) = P(z > 0) = 1 - \Phi(z)
\]

where \( \Phi(z) \) is the cumulative distribution function (CDF) of the standard normal distribution. Thus, the \( p \)-value is given by:

\[
p\text{-value} = 1 - \Phi(z)
\]

\subsection*{Step 5: Conclusion}

To summarize, the probability that \( E[a] > E[b] \) in the noiseless benchmark, given the noisy benchmark observations, is determined by the observed accuracy difference \( \bar{a} - \bar{b} \) and the noise ratio \( K \). The probability is computed using a hypothesis test on \( \bar{a} - \bar{b} \), assuming it follows a normal distribution. The final formula for this probability is:

\[
P(E[a] > E[b]) = 1 - \Phi(z)
\]

where \( \Phi(z) \) is the CDF of the standard normal distribution and \( z \) is the computed \( z \)-score. This result allows us to assess the likelihood that model A has a higher expected accuracy than model B in the noiseless benchmark based on noisy observations.

\section{Unsuccessful Attempts for Optimizing Benchmark Generator}
\label{sec:unsuccess}
\subsection{Faithfulness}
% 我们尝试了使用被广泛研究的self- correction策略来试图提升benchmark的faithfulness。这涉及到，对于每个生成的sample，模型先进行judge，再对其认为faithfulness不足的sample进行优化。我们的初步实验结果显示，该方案在数学任务上能带来微小的提升，但对于MMLU-Pro等几乎没有帮助，反而会产生额外的开销。
We explored the widely studied self-correction strategy to improve the faithfulness of benchmarks. Specifically, for each generated sample, the model first acts as a judge and then refines samples it deems insufficiently faithful. However, our preliminary results indicate that while this approach yields minor improvements in mathematical tasks, it provides little benefit for tasks such as MMLU-Pro and instead introduces additional computational overhead.

\subsection{Difficulty Controllability}
% 如前所述，我们尝试过让模型生成指定难度等级的sample，但这种方式所产生的sample难度区分度很低。我们进一步尝试了让模型对于生成的sample的难度进行判断，但该策略仅在MATH上面有较好的表现。
As previously mentioned, we attempted to have the model generate samples with specified difficulty levels, but the resulting samples exhibited low difficulty differentiation. To address this, we further explored having the model assess the difficulty of its generated samples. However, this strategy yielded promising results only on the MATH task.

\subsection{Difficulty Diffusion Mechanism}
% 先前的研究尝试过增加选项数量的方式来提升题目难度。我们在尝试后发现scaling up候选数量会比较快地陷入饱和，我们猜测这是因为模型难以生成大量具有迷惑度的候选项。
Previous studies \citep{mmlupro} have attempted to increase question difficulty by expanding the number of answer choices. However, our experiments show that scaling up the number of candidates quickly reaches a saturation point. We hypothesize that this is due to the model’s difficulty in generating a large number of sufficiently deceptive distractors.

\subsection{Diversity}
% 为了增加samples的多样性，除了AttrPromt，我们还尝试过赋予模型不同的persona并命令模型依据自身的persona来生成具有特点的samples。然而，我们发现这对于MATH任务并不十分奏效。
To enhance sample diversity, in addition to AttrPrompt, we experimented with assigning different personas \citep{persona} to the model and instructing it to generate characteristic samples based on its assigned persona. However, we found that this approach was not particularly effective for the MATH task, especially in semantic diversity.


\section{Data from Huggingface}
% 我们从 Huggingface API \url{https://huggingface.co/docs} 获取了有关于开源模型release和开源模型下载量的信息。其中，由于开源模型的release数量远多于闭源模型，因此我们用前者来表示the Number of Language Model Releases。由于huggingface无法提供每个模型每个月的下载量，我们用某个统计区间发布的模型对应的历史下载总量，作为该区间的下载总量。代码如下所示。
\label{sec:huggingface}
We obtained information on open-source model releases and download counts from the Hugging Face API (\texttt{from huggingface\_hub import HfApi}). Since the number of open-source model releases far exceeds that of closed-source models, we use the former to represent the "Number of Language Model Releases." Additionally, as Hugging Face does not provide monthly download counts for each model, we use the historical total downloads of models released within a given statistical period as the total downloads for that period. The corresponding code is shown below.


\section{Difficulty Levels}
\label{sec:dif_level}
\begin{itemize}
    \item \textbf{Level 1}: The simplest, equivalent to lower-grade elementary school
\item \textbf{Level 2}: Relatively simple, equivalent to upper-grade elementary school
\item \textbf{Level 3}: Simple, equivalent to middle school
\item \textbf{Level 4}: Average, equivalent to high school
\item \textbf{Level 5}: Slightly difficult, equivalent to university student
\item \textbf{Level 6}: Difficult, equivalent to Master's
\item \textbf{Level 7}: Quite difficult, equivalent to PhD student
\item \textbf{Level 8}: Very difficult, equivalent to professor
\item \textbf{Level 9}: Extremely difficult, equivalent to field expert
\item \textbf{Level 10}: Most difficult, equivalent to top human level or beyond human level
\end{itemize}

\section{Benchmarking Model List}
\label{sec:bench_model_list}
\begin{itemize}
\item \textbf{phoenix-inst-chat-7b}: \url{https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b}
\item \textbf{vicuna-7b-v1.3}: \url{https://huggingface.co/lmsys/vicuna-7b-v1.3}
\item \textbf{Qwen2.5-3B}: \url{https://huggingface.co/Qwen/Qwen2.5-3B}
\item \textbf{phi-2}: \url{https://huggingface.co/microsoft/phi-2}
\item \textbf{Phi-3.5-mini-instruct}: \url{https://huggingface.co/microsoft/Phi-3.5-mini-instruct}
\item \textbf{Yi-1.5-6B-Chat}: \url{https://huggingface.co/01-ai/Yi-1.5-6B-Chat}
\item \textbf{Qwen2.5-7B}: \url{https://huggingface.co/Qwen/Qwen2.5-7B}
\item \textbf{vicuna-7b-v1.5}: \url{https://huggingface.co/lmsys/vicuna-7b-v1.5}
\item \textbf{Qwen2-1.5B-Instruct}: \url{https://huggingface.co/Qwen/Qwen2-1.5B-Instruct}
\item \textbf{phoenix-inst-chat-7b-v1.1}: \url{https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b-v1.1}
\item \textbf{Qwen-Plus}: \url{https://huggingface.co/Qwen} 
\item \textbf{GPT-3.5 turbo}: \url{https://openai.com/index/gpt-3-5-turbo-fine-tuning-and-api-updates/}
\end{itemize}

\section{Details of Difficulty Diffusion Mechanism}
\label{sec:ddd}
Given that the LLM has a certain level of difficulty perception, we iteratively select the more challenging samples according to $\beta$ from the generated ones as difficulty references, and instruct the LLM to generate a more difficult sample. 
Specifically,
% 为了防止作为reference的samples固定导致生成的samples同质化，我们：（1）记录每个sample $x_i$ 作为 reference sample的次数$t_i$,并计算：$\text{Calibrate_Difficulty} = \text{Difficulty_Label} * pow(0.9,t_i/\text{Reference_Number})$，作为修正后的难度标签进行排序。（2）每次选择$2*\text{Reference_Number}$个具有最大$\text{Calibrate_Difficulty}$的samples作为候选，然后再从中随机选出打乱顺序的$\text{Reference_Number}$个samples作为reference samples。我们的前期实验发现题目难度正相关于$\text{Reference_Number}$。我们在实验中设定$\text{Reference_Number}$为8。
To prevent reference samples from becoming overly fixed, which may lead to homogenization in generated samples, we adopt the following strategy:
\begin{enumerate}
    \item We track the number of times each sample \( x_i \) has been used as a reference sample, denoted as \( t_i \), and compute a calibrated difficulty label:
    \begin{equation}
        \text{Calibrate\_Difficulty} = \text{Difficulty\_Label} \times 0.9^{t_i / \text{Reference\_Number}}
    \end{equation}
    The samples are then sorted based on this adjusted difficulty.

    \item Each time, we select \( 2 \times \text{Reference\_Number} \) samples with the highest \( \text{Calibrate\_Difficulty} \) as candidates. From this pool, we randomly sample \( \text{Reference\_Number} \) as reference samples and shuffle their order.
\end{enumerate}
Our preliminary experiments indicate a positive correlation between problem difficulty and \( \text{Reference\_Number} \). In our experiments, we set \( \text{Reference\_Number} \) to 8.
This allows the sample difficulty to rise continuously through diffusion.

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{figs/chain_4omini.pdf}
\vspace{-10pt}
\caption{{Pearson correlations among key factors of benchmark evaluation and LLM (GPT-4o mini) judge scores (faithfulness and alignment). The most relevant path of each subject is highlighted in red to show the possible causal chain.}}
\vspace{-7pt}
\label{fig:chain_4omini}
\end{figure*}

\section{Converting Benchmark Sample Format}
\paragraph{MCQ to OTG Format.}
\label{sec:convert}
By removing the options from the samples and using the solution and answer corresponding to the correct option as the ground truth, we can easily transform the MCQ-style benchmark generated based on MATH assessment demands into an open-ended text generation (OTG) benchmark. Comparing these two benchmark formats, we find that the OTG format makes the questions more challenging (error rate: $0.865 > 0.768$) and results in lower knowledge diversity (hamming distance: $0.365 < 0.403$). We attribute this to the model's inability to rely on option cues to answer certain questions, which leads to a large portion of the knowledge vector being zero, thereby reducing knowledge diversity. Additionally, we observe a decline in the benchmark’s effectiveness (pearson: $0.915 < 0.935$), which we hypothesize is indirectly caused by the drop in knowledge diversity.

\paragraph{OTG to MCQ Format.}
\label{sec:MATH_convert}
%为了将MATH benchmark转化为MCQ format，我们选择了GPT-4o，GPT-4o mini,GPT-3.5 turbo,Claude-3.5 Haiku分别回答MATH，并以温度1各采样十次。我们统计频次最高的3个错误答案作为候选项，并保留正确答案的rationale。如果错误答案数量不足，我们则让GPT-4o mini进行补充。由此，我们实现了将MATH benchmark由OTG to MCQ Format的转化。
To convert the MATH benchmark into a MCQ format, we employed GPT-4o, GPT-4o mini, GPT-3.5 turbo, and Claude-3.5 Haiku to answer MATH problems, sampling 10 responses per model with a temperature of 1. We identified the three most frequently occurring incorrect answers as distractors while retaining the correct answer's rationale. If the number of incorrect answers was insufficient, we supplemented them using GPT-4o mini. Through this process, we successfully transformed the MATH benchmark from an OTG format to an MCQ format.


\section{Examples of the Generated Samples}
\label{sec:example}
\textbf{MATH:}
\begin{lstlisting}
Example 1:
A researcher is studying the distribution of three specific proteins in a cell. 
There are 4 locations within the cell where each protein can be present. 
However, due to experimental conditions, at least one protein must be present 
in each location. In how many different ways can the proteins be distributed 
in the cell, considering overlap in presence is allowed?
A. 2187
B. 2401
C. 4096
D. 2048

Label:B


Example 2:
Find the smallest positive integer \\( n \\) such that \\( n \\) is divisible
by 6, 10, and 15, and \\( n \\equiv 2 \\pmod{4} \\).
A. 120
B. 20
C. 90
D. 30

Label:D
\end{lstlisting}

\textbf{MMLU-Pro:}
\begin{lstlisting}
Example 1:
A 45-year-old woman with type 2 diabetes decides to improve her health by
adopting a low-carbohydrate, high-protein diet, starting a daily
30-minute brisk walk routine, and taking a new medication that
increases insulin sensitivity. She also begins consuming a herbal
supplement believed to enhance energy levels. After two months, she
notices an increase in fatigue, frequent headaches, and unexplained
weight gain. What is the most likely reason for her symptoms?
A. Low-carbohydrate diet leading to nutritional deficiencies
B. Brisk walk routine causing excessive physical exertion
C. Medication side effects causing insulin fluctuations
D. Herbal supplement causing hormonal imbalance
E. Increased protein intake causing kidney strain
F. Inadequate hydration from dietary changes
G. Overconsumption of high-protein foods leading to weight gain
H. Lack of fiber intake affecting metabolism
I. Decrease in carbohydrate intake causing energy depletion
J. Stress from lifestyle changes impacting health

Label:C


Example 2:
An architect is designing a complex apartment building, which features a 
series of irregularly shaped balconies. The layout of one of the building's
wings is depicted in the accompanying diagram. Each balcony's area is defined
by the function \( f(x) = 3x^2 + 2x + 1 \) over the interval [0, 2] meters,
representing a horizontal cross-section. The total length of the wing is
10 meters, and each balcony occurs at every meter along this length, 
aligned perpendicularly. To meet safety regulations, the 
architect needs to ensure that the probability of a randomly selected 
balcony having an area greater than 8 square meters is at least 0.5. 
Calculate the probability that a randomly selected balcony from this
wing has an area greater than 8 square meters, using integration
to determine the areas and probabilities involved. Consider potential
pitfalls like incorrect integral setup or probability interpretation.
A. 0.1
B. 0.2
C. 0.3
D. 0.4
E. 0.5
F. 0.6
G. 0.7
H. 0.8
I. 0.9
J. 1.0

Label:J
\end{lstlisting}

\textbf{HellaSwag:}
\begin{lstlisting}
During a family reunion, Mark is honored with the 'Outstanding Contributor'
award for his recent volunteer efforts in the community. As he stands
in front of his relatives, he expresses heartfelt gratitude towards everyone
who supported him but fails to mention his younger sister, Lily, who 
organized the charity event that helped him earn this recognition. 
After the ceremony, Lily watches Mark celebrate with others, her face
a mix of pride and disappointment. When Mark approaches her, excitedly
asking, 'Did you see me win? I couldn't have done it without your help!'
Given Lily's conflicting feelings about being overlooked, how is she
most likely to respond?
A. 'Congratulations, Mark! I'm really proud of you! But I can't help 
feeling a bit overshadowed since I organized the event.'  
B. 'Wow, Mark! You totally deserve this! Yet, it's tough for me to 
celebrate when my efforts went unnoticed.'  
C. 'That was an amazing award, Mark! I'm happy for you! However, it 
stings that my contribution was overlooked.'  
D. 'I'm so thrilled for you, Mark! Your achievement is incredible! 
But it feels a little unfair that I didn't get a shoutout.'

Label:C
\end{lstlisting}

\section{Prompt List}
\textbf{LLM as Faithfulness Judge:}
\label{sec:prompt_judge}
\begin{lstlisting}
You are an expert who excels at analyzing whether a given response 
correctly answers a provided question.

**Question:**
{{question}}

**Response to be Checked:**
{{response}}

Please note that the given question may be unsolvable, have a unique solution, multiple solutions,
etc. Therefore, you should carefully analyze the correctness of
the response to be checked based on the given question.

Here are the rules to strictly follow when analyzing the
correctness of a response:
1. **Step-by-Step Analysis**: Analyze the response step by step, reviewing
the reasoning and correctness of each step. For every step, first **restate
and summarize** the reasoning logic and conclusion presented in the response,
then analyze the correctness of that specific step.
2. **Focus on Evaluation**: Remember that your primary mission is to determine
whether the reasoning process is correct. Avoid attempting to solve the
problem yourself. Instead, focus strictly on analyzing the correctness
of the response's reasoning process, one step at a time.
3. **Avoid Premature Judgments**: Do not rush to make judgments (such as
claiming the response is flawed or completely correct) at the beginning.
Ensure your evaluation is based on a thorough step-by-step analysis before
arriving at a conclusion.
4. **Reverse Validation**: After completing the step-by-step analysis,
substitute the answer back into the original problem and perform
reverse validation of the parameters to cross-verify the correctness
of the response.
After completing your analysis, please provide your judgment on the correctness
of the response, as well as your confidence level in that judgment.

Your output should follow the template and example below:
Analyses:{Your detailed analyses}
Judgement:{0: You think both the final answer of the response is wrong;
0.5: You think the reasoning path has some mistakes, but the final
answer of the response is correct; 1: You agree with the reasoning path
and the final answer of the response}

##Example##
Analyses:{Your detailed analyses}
Judgement:1
##Example End##

Now begin with "Analyses:"
\end{lstlisting}

\textbf{LLM as Comparison-based Faithfulness Judge:}
\label{sec:prompt_cmp_judge}
\begin{lstlisting}
You are a knowledgeable expert with the task of analyzing the quality
of a given question and its candidate answers.

###Question
{{question}}

###Candidate 1:
{{can1}}

###Candidate 2:
{{can2}}

###Your task: Correctness Analysis
1. Analyze whether the question is correct, reasonable, and clearly stated.
2. For the given question, analyze whether the provided ###Candidate 1 and
###Candidate 2 are correct step by step sequentially.
(Do not favor a candidate just because it is long; evaluate candidates strictly
based on correctness.)
3. Based on the above analysis, output your judgment of the question
quality according to the following scale:
    0 point indicate an incorrect question with ambiguities and no uniquely 
    suitable answer among the options.
    0.5 point indicates a minor error in the question, but there is 
    still a uniquely suitable answer among the options.
    1 point indicate no errors in the question, with one uniquely correct
    answer among the options.
4. Please also output your chosen correct option
You should follow the template
below to output:
"##Faithfulness:{{score}}##, ##Label:{{}}##" (e.g., ##Faithfulness:2##,##Label:B##).
Please note that if you believe there is no correct option or there are multiple
correct options, output ##Faithfulness:0##, ##Label:None##.

You should begin your response with "Correctness Analysis".

\end{lstlisting}

\textbf{LLM as Relevance Judge:}
\label{sec:prompt_relevance_judge}
\begin{lstlisting}
You are an expert who excels at analyzing whether a given question can be used to assess a specific ability.

**Question:**
{{question}}

**Ability:**
{{ability}}

You should first carefully analyze what abilities the given question can be used to test.
Based on this analysis, compare it with the given abilities.
After completing your analysis, please provide your judgment on whether the given question can be used to test the given ability, as well as your confidence in that judgment.

Your output should follow the template below:
Analyses:{Your detailed analyses}
Judgement:
{output 0 if: You believe the given question is completely unable to test the given ability;
output 0.5 if: You believe the given question is primarily meant to test other abilities, but can also test the given ability to some extent;
output 1 if: You believe the given question primarily tests the given ability.}

Now begin with "Analyses:"
\end{lstlisting}

\paragraph{Directly Prompting LLM as Generic Benchmark Generator:}
\label{sec:prompt_direct}
%值得一提的是，在我们让LLM正式生成benchmark之前，会先让其依据assessment demands生成对于sample每一部分的描述，包括：Task Description,Query Description, Option Description.这可以帮助模型更好地理解assessment demands并与之对齐。
Notably, before allowing the LLM to formally generate the benchmark, we first require it to produce descriptions for each part of the sample based on the assessment demands, including Task Description, Query Description, and Option Description. This helps the model better understand and align with the assessment demands, ensuring higher-quality and more consistent benchmark generation.
\begin{lstlisting}
You are a knowledgeable benchmark creator.
Your task is to generate a creative questions based on the provided Task Description, Query Description, Option Description, Generation Guidelines, and Output Description to help build a benchmark that assesses the given task.

### Task Description:
{{task define}}

### Query Description:
{{query define}}

### Option Description:
{{option define}}

### Generation Guidelines:
1. Analyze the given task and think step-by-step about the content needed to construct the question, begin with "Analyses:".
2. Generate the question content, begin with "Question:".
3. Generate 10 candidates, with only one as the right option. Begin with "Candidates:".
4. Generate the index of the right option, begin with "Right Option:".

### Output Description:
Strictly follow the template below to generate your sample.
**Template**
##Analyses:## {{You analyze the provided attributes and outline the process for constructing the question to be generated.}}
##Question:## {{Your generated question content}}
##Candidates:##
{{Your generated Candidates}}
##Right Option:##{{Index of the right option, e.g., B}}
**Template End**

Attention: You need to **strictly follow the template** and don't generate any other contents. Begin your response with "##Analyses:## "
\end{lstlisting}


\section{Examples of the Generated Difficulty Strategies}
\label{sec:strategies}
\textbf{MATH:}
\begin{lstlisting}
Strategy 1:
Complexity of Biological Concept is Basic
Complexity of Biological Concept is Intermediate
Complexity of Biological Concept is Advanced

Strategy 2:
Required Reasoning Steps set as Single-step
Required Reasoning Steps set as Multi-step (2-3 steps)
Required Reasoning Steps set as Multi-step (4-6 steps)
Required Reasoning Steps set as More than 6 steps

Strategy 3:
Familiarity with the Topic is Common
Familiarity with the Topic is Uncommon
Familiarity with the Topic is Rare

Strategy 4:
Type of Biological Data Analysis is Qualitative
Type of Biological Data Analysis is Quantitative
Type of Biological Data Analysis is Advanced Data Interpretation

Strategy 5:
Application of Concepts is Direct
Application of Concepts is Modified
Application of Concepts is Novel

Strategy 6:
Integration Across Biological Disciplines is Single-discipline
Integration Across Biological Disciplines is Cross-disciplinary
Integration Across Biological Disciplines is Interdisciplinary

Strategy 7:
Depth of Required Knowledge is Surface-level
Depth of Required Knowledge is In-depth
Depth of Required Knowledge is Comprehensive
\end{lstlisting}


\textbf{Prompt of \textsc{BenchMaker}:}
\label{sec:prompt_benchmaker}
\begin{lstlisting}
You are a knowledgeable benchmark creator.
Your task is to generate a creative question based on the provided Task Description, Query Description, Option Description, General Attributes Descriptions, Difficulty Strategies Description, Generation Guidelines, and Output Description to help build a benchmark that assesses the given task.

### Overall Task Description:
{{original task}}


### Detailed Task Description:
{{task define}}


### Query Description:
{{query define}}


### Option Description:
{{option define}}


### General Attributes Description:
You can refer to the following attributes and their corresponding values to construct questions, which means the questions you generate should ideally align with some of these attributes.
Please note, if you find any conflicting or confusing parts among the attributes listed, you may disregard them.
{{attribute define}}


### Difficulty Strategies Description:
Your generated questions should meet the following difficulty attribute requirements. If you find conflicts among these requirements, you may choose to selectively ignore them.
{{difficulty attribute define}}


### Difficulty Description:
The following are some samples (0 or several).
Please ensure that the difficulty level of the samples you generate is harder than these examples.
The samples you generate should aim to assess different knowledge and skills compared to the given samples.
The format of given samples are not what you should follow.
**Please ensure that the sample you create differ substantially from the following samples, so as to maintain diversity in the resulting benchmark.**
{{demonstrations}}


### Generation Guidelines:
**Stage 1: Analyze**
In this stage, you should analyze following the steps below and begin with "##Analyses:##". **You need to clearly articulate the analysis content for each step**, which means after completing Stage 1, you should have already produced a question that meets the requirements along with a correct and unique answer.
1-1. Analyze the general attributes, difficulty attributes and difficulty description, and think step-by-step about the content needed to construct the question. **Please use your imagination and avoid any obvious overlap with the given samples, either in the specific knowledge points being tested or in the format.**
1-2. Start by drafting your question. If you discover any issues with the question or any overlapping parts between the generated question and the given samples during this process, feel free to revise it.
1-3. Think through what the correct answer should be. If you discover any issues during this process, repeat the entire Stage 1 process from the beginning.
1-4. Identify the plausible and potentially misleading incorrect options that could serve as distractors (at least nine). If you discover any issues during this process, repeat the entire Stage 1 process from the beginning.
1-5. Reevaluate your proposed question, answer and options to ensure that: the question meet the given attributes and Difficulty Description (you should compare the generated samples and given samples to verify this); the answer is both correct and unique. If it does not meet these criteria or you are not sure about this, repeat the entire Stage 1 process from the beginning.

**Stage 2: Generate Sample**
In this stage, you should give your generated sample in the right template based on the analyses above.
2-1. Generate the question content, begin with "##Question:##".
2-2. Generate a step-by-step reasoning process and the corresponding correct answer. Begin with "##Reasoning Path:##". If you find an issue with the question, return to Step 2-1 to regenerate the question.
2-3. Generate {{OptionNum}} candidates, with only one as the right option. Begin with "##Candidates:##".
2-4. Generate the index of the right option, begin with "##Right Option:##".

### Output Description:
Strictly follow the template below to generate your sample.
**Template**
##Analyses:## {{You analyze the provided attributes and outline the process for constructing the question to be generated.}}
##Question:## {{Your generated question content}}
##Reasoning Path:## {{Your step-by-step reasoning process}}
##Candidates:##
{{CandidatesDemo}}
##Right Option:##{{Index of the right option, e.g., B}}
**Template End**


Attention: You need to **strictly follow the template** and don't generate any other contents. Begin your response with "##Analyses:##\n1-1. "
\end{lstlisting}


\section{Assessment Demands List}
\label{sec:demands}
\textbf{MATH:}
\label{sec:demand_math}
\begin{lstlisting}
Subset Name: Prealgebra
Assessment Demands:Prealgebra

Subset Name: Algebra
Assessment Demands:Algebra

Subset Name: Number Theory
Assessment Demands:Number Theory

Subset Name: Counting & Probability
Assessment Demands:Counting & Probability

Subset Name: Geometry
Assessment Demands:Geometry

Subset Name: Intermediate Algebra
Assessment Demands:Intermediate Algebra

Subset Name: Precalculus
Assessment Demands:Precalculus
\end{lstlisting}
\textbf{MMLU-Pro:}
\label{sec:demand_mmlupro}
\begin{lstlisting}
Subset Name: psychology
Assessment Demands:This benchmark is designed to assess **psychology** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: philosophy
Assessment Demands:This benchmark is designed to assess **philosophy** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: health
Assessment Demands:This benchmark is designed to assess **health** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: history
Assessment Demands:This benchmark is designed to assess **history** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: business
Assessment Demands:This benchmark is designed to assess **business** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: physics
Assessment Demands:This benchmark is designed to assess **physics** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: engineering
Assessment Demands:This benchmark is designed to assess **engineering** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: chemistry
Assessment Demands:This benchmark is designed to assess **chemistry** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: math
Assessment Demands:This benchmark is designed to assess **math** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: computer science
Assessment Demands:This benchmark is designed to assess **computer science** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: biology
Assessment Demands:This benchmark is designed to assess **biology** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: economics
Assessment Demands:This benchmark is designed to assess **economics** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format

Subset Name: law
Assessment Demands:This benchmark is designed to assess **law** abilities while simultaneously evaluating knowledge understanding and complex reasoning skills, using **ten multiple-choice questions** as the evaluation format
\end{lstlisting}

\textbf{HellaSwag:}
\label{sec:demand_hella}
\begin{lstlisting}
Subset Name: NLI
Assessment Demands:The task is to evaluate the model's commonsense natural language inference ability. Specifically, each question should present a concrete scenario, and the model should select the most likely event from the options based on a series of inferences.
\end{lstlisting}