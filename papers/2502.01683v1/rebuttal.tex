
% Reviewer1
我们十分感谢您细致的审稿以及提供的有益建议，我们细致阅读了它们，并进行了深思熟虑的讨论（见下文）。我们相信这将进一步帮助提升论文的质量。

- The paper is heavily packed with different experiments. Personally, I would prefer less experiments but more discussion about the obtained results with perhaps some qualitative analysis.

我们在广泛的文本评测任务上进行实验来保证所得到结论的soundness。我们也认同围绕实验结果进行更多的定性分析能帮助我们更全面地认知BatchEval。

* Is there any analysis of performance differences between evaluation criteria? e.g., why understanding in TopicalChat is more difficult for BatchEval to predict rather than other criteria? Did you make any further analysis of that?

我们认为BatchEval的performance differences between evaluation criteria来自于两方面原因：人类annotation的质量与依赖推理能力的程度。首先，在understanding creterion上较低的inter-annotator agreement说明人类的annotations可能存在较大的误差、不够精准。与不够精准的人类annotation之间的一致性注定难以较高。此外，understanding criterion ("Is the response understandable given the previous context?" [1]) 需要evaluator具有较好的推理能力。而目前LLM的推理能力仍有较大的提升空间，特别是相比于其流畅的语言能力和对应的在Naturalness、Coherence上所体现的良好表现而言。


* Are there any performance differences between the criteria that need to be evaluated with respect to a source document (e.g., factuality for summarisation) and the criteria that evaluate the text on its own (e.g., naturalness)?

对于Topical-Chat benchmark而言，我们确实发现BatchEval在criteria (understanding, coherence) that need to be evaluated with respect to a source document上的表现略差于在criteria (engaging, naturalness) that evaluate the text on its own上的表现. 我们认为前者需要更多地结合source document与text进行推理，而后者更加考察LLM的语言建模能力。这从另一个方面验证了LLM在依赖推理的评测任务上表现次优。

* The coherence criterion is present in 3 evaluated benchmarks. Were the results of BatchEval consistent wrt this criterion?

是的，在3 evaluated benchmarks中BatchEval在coherence criterion上的表现都位居所有criteria中的前列，尽管因为不同类型文本导致的任务难度不同产生了相关性系数数值大小的差异。


- Section 4.1: What were human scales in the tested datasets? Likert-type, continuous? Did you use averaged scores for humans when there're several annotators per item? Was there any score normalisation?

以下是我们在使用中所涉及的数据集的详细统计,对于每个数据集我们均采用对应原文中的setting,如表中所示：
| Dataset      |  type | annotator number | post processing |
| :--: | :--:| :--: | :--: |
| Topical-Chat  | Likert-type | 3.0 | calculate the mean |
| FED     |   Likert-type |  5.0  | calculate the mean after removing outliers ｜
| HANNA      |    Likert-type | 3.0  | calculate the mean |
| QAGS      |    Likert-type | 3.0  | majority voting |


- I'd welcome a discussion why inter-annotator agreement for datasets is worse than BatchEval. What does that say about LLM-based evaluation?

我们感谢您讨论的意愿！以下是我们关于您所提出论题的一些理解：在文本评测集构建的相关工作中，较高的inter-annotator agreement意味着标注者具有更好的标注能力。例如，[2]中表明专家标注员往往能比非专家标注员获得更好的inter-annotator agreement。因此，在部分任务上BatchEval-annotator agreement 高于inter-annotator agreement的现象能够支持BatchEval（with GPT-4）的文本评测能力强于human-annotator的论点。我们认为这是比人类平均水平更具有世界知识的GPT-4在修正了不合理的评测方式（sample-wise），转而采用类似于人类的评测方式（batch-wise）后的自然结果。

- Was it always possible to parse correctly the responses of GPTs?
是的，我们在所有实验中都没有观察到解析失败的现象发生。

再次感谢您对帮助我们提升论文质量所付出的努力。

[1] Mehri, Shikib, and Maxine Eskenazi. USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation.
[2] Pustu-Iren, Kader, et al. Investigating Correlations of Inter-Coder Agreement and Machine Annotation Performance for Historical Video Data.



We sincerely appreciate your thorough review and the constructive suggestions you have provided. We have carefully read them and engaged in thoughtful discussions (detailed below). We believe this will further contribute to enhancing the quality of our manuscript.

- The paper is heavily packed with different experiments. Personally, I would prefer less experiments but more discussion about the obtained results with perhaps some qualitative analysis.

We conducted experiments across a wide range of text evaluation tasks to ensure the soundness of our conclusions. We also agree that conducting more qualitative analysis around the experimental results would help us gain a more comprehensive understanding of BatchEval. Here is our discussion in response to the insightful questions you raised:

- Is there any analysis of performance differences between evaluation criteria? e.g., why understanding in TopicalChat is more difficult for BatchEval to predict rather than other criteria? Did you make any further analysis of that?

We think the performance differences between evaluation criteria in BatchEval stem from two main reasons: the quality of human annotations and the degree of reliance on reasoning capabilities. Firstly, the lower inter-annotator agreement in the understanding criterion suggests that human annotations may contain more errors and lack precision. Consistency with less precise human annotations is inherently challenging to achieve at a high level. Furthermore, the understanding criterion ("Is the response understandable given the previous context?" from [1]) requires evaluators to possess strong reasoning abilities. The current reasoning capabilities of LLMs still have considerable room for improvement, especially when compared to their fluent language abilities and the corresponding favorable performance demonstrated in Naturalness and Coherence.

- Are there any performance differences between the criteria that need to be evaluated with respect to a source document (e.g., factuality for summarisation) and the criteria that evaluate the text on its own (e.g., naturalness)?

For the Topical-Chat benchmark, we indeed observed that BatchEval performs slightly less well on criteria (such as understanding and coherence) that need to be evaluated with respect to a source document, compared to criteria (such as engaging and naturalness) that evaluate the text on its own. We believe the former requires more reasoning involving both the source document and the text, whereas the latter more directly relies on the language modeling capabilities of LLMs. This further supports the conjecture that LLMs exhibit suboptimal performance on evaluation tasks that rely heavily on reasoning.

- The coherence criterion is present in 3 evaluated benchmarks. Were the results of BatchEval consistent wrt this criterion?

Yes, across the three evaluated benchmarks, BatchEval's performance on the coherence criterion consistently ranks among the top across all criteria, despite the variability in correlation coefficient values due to the differing levels of task difficulty presented by various types of texts.

- Section 4.1: What were human scales in the tested datasets? Likert-type, continuous? Did you use averaged scores for humans when there're several annotators per item? Was there any score normalisation?

Here are the detailed statistics of the datasets we used in our study, with each dataset being utilized according to the settings specified in their respective original publications, as shown in the table below:

| Dataset      |  scale type | annotator number | post processing |
| :--: | :--:| :--: | :--: |
| Topical-Chat  | Likert-type | 3.0 | calculate the mean |
| FED     |   Likert-type |  5.0  | calculate the mean after removing outliers ｜
| HANNA      |    Likert-type | 3.0  | calculate the mean |
| QAGS      |    Likert-type | 3.0  | majority voting |

- I'd welcome a discussion why inter-annotator agreement for datasets is worse than BatchEval. What does that say about LLM-based evaluation?

We appreciate your willingness to engage in discussion! Here are some insights regarding the topic you raised: In the context of constructing text evaluation benchmarks, a higher inter-annotator agreement signifies better annotation quality. For instance, it is demonstrated in [2] that expert annotators tend to achieve better inter-annotator agreement than non-experts. Therefore, the phenomenon where annotator-BatchEval agreement exceeds inter-annotator agreement in some tasks supports the argument that BatchEval has superior text evaluation capabilities compared to human annotators. We believe this is a natural outcome when LLM, which possesses a broader knowledge of the world and better language modeling capabilities than the average level of human, corrects unreasonable evaluation methods (sample-wise) and adopts an evaluation approach more akin to humans (batch-wise).

[1] Mehri, Shikib, and Maxine Eskenazi. USR: An Unsupervised and Reference Free Evaluation Metric for Dialog Generation.
[2] Pustu-Iren, Kader, et al. Investigating Correlations of Inter-Coder Agreement and Machine Annotation Performance for Historical Video Data.


% Reviewer2
我们十分感谢您细致的审稿以及提供的有益建议，我们将依据它们完善论文。
对于您所提到的，
“might require tweaking for summarization tasks with longer input and outputs, in order to fit into LLM context window.”
我们十分认同，同时对此保持乐观的态度，因为当前LLM已经拥有的context window已经提升到了比这篇论文写作时数倍的水平（e.g., 1M for Claude 3）,并且这一趋势仍然在持续。
再次感谢您对我们手稿的高度评价以及为进一步提升其质量所做的努力。

We are deeply grateful for your meticulous review and valuable suggestions, which we will follow to enhance our paper.

Regarding your comment that

- might require tweaking for summarization tasks with longer inputs and outputs, in order to fit into the LLM context window.

we fully agree and remain optimistic, as the current context windows available in LLMs have already increased to levels several times higher than those at the time of writing this paper (e.g., 1M for Claude 3), and this trend is continuing.

Thank you once again for your high appraisal of our manuscript and for your efforts to further enhance its quality.


% Reviewer3
我们十分感谢您细致的审稿以及提供的有益建议，我们将在最终版本中依据您的建议修改line 50中的表述，使之更准确。
我们同样非常感谢您对于BatchEval的认可和推荐。我们深受鼓励，并将在未来为推动文本自动评测领域的进步、解放人类标注员的愿景做出持续的努力。


We are deeply thankful for your thorough review and the constructive suggestions provided. In the final version of our paper, we will revise the statement in line 50 according to your recommendations to make it more appropriate.

We also deeply appreciate your recognition and recommendation of BatchEval. Your encouragement is highly motivating, and we are committed to continuing our efforts towards advancing the field of automatic text evaluation and realizing our vision of liberating human annotators from their burdensome tasks.

