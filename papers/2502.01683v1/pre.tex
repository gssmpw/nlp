\section{Benchmarking Benchmark Generator}
% \section{Evaluation Framework for Benchmark Generator}
\label{sec:pre}

\begin{table*}[t]
    \renewcommand\arraystretch{1.3}
    \small
    \centering
    \caption{Criteria taxonomy and definition of the proposed evaluation framework for the benchmark generator. Criteria marked with * indicate optimization objectives that are distinctive to benchmark synthesis compared to training data synthesis.}
    \setlength{\tabcolsep}{0.3em} % 设置列之间的间距
    \begin{tabular}{lll}
    \toprule
    \textbf{Taxonomy}&\textbf{Criterion} & \textbf{Definition}  \\
    \midrule
    \multirow{2}{*}{Credibility}&faithfulness&The sample is well-defined, with the ground truth answer being correct.\\
    \multirow{2}{*}{}&alignment *&The abilities evaluated by the sample align well with the given assessment demands.\\
    \hline
    \multirow{3}{*}{Diversity}&lexical&The samples exhibit sufficient lexical diversity.\\
    \multirow{3}{*}{}&semantic&The samples exhibit sufficient semantic richness.\\
    \multirow{3}{*}{}&knowledge *&The knowledge and skills assessed by different samples should not be redundant.\\
    \hline
    \multirow{2}{*}{Difficulty}&controllability *&The samples have correct difficulty labels to form subsets with varying difficulties.\\
    \multirow{2}{*}{}&boundary *&The hardest subset is difficult enough to explore the boundaries of advanced models.\\
    \hline
    \multirow{3}{*}{Benchmark-Level}&effectiveness *&The benchmarking results align with human benchmark under the same assessment demands.\\
    \multirow{3}{*}{}&robustness *&The benchmarking results of generated benchmarks under similar assessment demands align.\\
    \multirow{3}{*}{}&efficiency&The time and cost of generating a benchmark are low enough.\\
    \bottomrule
    \end{tabular}
    \vspace{-3pt}
    \label{tab: criteria}
\end{table*}

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{figs/chain_.pdf}
\vspace{-10pt}
\caption{{Pearson correlations among key factors of benchmark evaluation and LLM (Qwen-Plus) judge scores (faithfulness and alignment). The most relevant path of each subject is highlighted in red to show the possible causal chain.}}
\vspace{-7pt}
\label{fig:chain}
\end{figure*}

% 与训练数据合成重点关注faithfulness和diversity有所区别的是，对合成benchmark的评估需要更加全面来确保benchmark的测评结果足够可信。为此，我们建立如表格1所示的benchmark generator评价体系。
While training data synthesis focuses on faithfulness, diversity and the final performance of the trained models \citep{attr,datasurvey2}, the evaluation of synthetic benchmark should be more comprehensive to ensure the reliability of its benchmarking results. Thus, we carefully establish an evaluation framework for benchmark generator with ten criteria, as illustrated in Table~\ref{tab: criteria}.
% 
\subsection{Credibility}
\label{sec:credibility}
% 保障benchmark可信度的两个基本的要素是faithfulness和alignment。
% 其中，faithfulness要求生成的sample不能存在歧义，且提供的答案正确。
% 而alignment要求所生成的题目要严格与给定的assessment demands对齐，这意味着不能生成数学题来测试模型的代码能力.
Two key criteria for ensuring the credibility of a benchmark are \textbf{faithfulness} and \textbf{alignment}. Faithfulness indicates that the generated sample be free of ambiguity with a correct answer. Alignment requires the generated samples to strictly adhere to the specified assessment demands $X$, especially in abilities to be assessed. 
% 对于这种难以通过rule-based方法衡量的criteria，先前的方法通常采用人类或LLM-as-a-judge来评估。但是前者难以自动化，后者容易受到bias的影响。
% 我们考虑找到并消除LLM-as-a-judge在我们框架下潜在的bias
For these criteria, previous approaches rely on human evaluation \citep{unigen,dyval2} or LLM-as-a-judge \citep{llmasjudge}. However, the former lacks automation, and the latter is susceptible to biases \citep{judgingjudge}. 
% 首先，我们认为潜在的bias因素包括sample难度、sample长度和LLM judge时的推理长度。为此，我们在MATH benchmark上进行实验，which被广泛使用且被认为具有完美的faithfulness。


To this end, we seek to detect and mitigate any biases of LLM-as-a-judge that may exist within the framework. 
We choose Qwen-Plus \citep{qwen} as the judge with scoring range as $[0,1]$ (See prompt in Appendix~\ref{sec:prompt_judge}). Experiments are conducted on the high-quality MATH benchmark \citep{math}, for which we assign score 1 to both faithfulness and alignment for every sample. 
% Ideally，judge的分数不应该与任何属性呈现一致性。但是如图所示，我们发现both faithfulness and alignment都与sample难度、sample长度、judge的rationale长度呈现显著的相关性(p-value>0.05)
Ideally, the scores assigned by the judge should not exhibit any consistency with specific factors. However, as shown in Figure~\ref{fig:chain}-(a), both faithfulness and alignment are significantly correlated (p-value $<$ 0.05) with sample difficulty, sample length, and the length of the judge’s rationale. 
% 对于每个主体，我们找到对其影响最大的attribute并将对应连线画成红色。我们可以由此看到一条清晰且易于理解的因果链路：题目更难往往意味着有更长的sample长度；judge需要进行更长的分析来对长题目进行判断；更长的分析过程更容易导致judge出错，导致较低的rating.
For each factor, we highlight its weightiest path in red, revealing a clear causal chain: harder questions lead to longer samples, requiring judges to conduct lengthier analyses. For faithfulness, longer analyses increases the likelihood of judge errors, resulting in lower faithfulness ratings. While for alignment, longer analyses increases the probability of task-relevant words appearing and results in higher alignment ratings.
% 为了验证上述的猜想，我们控制judge length，再计算sample difficulty, sample length分别与faithfulness and alignment的部分相关系数。
To validate the above hypothesis, we control the judge length and respectively calculate the partial correlations \citep{pengouin} of sample difficulty and sample length with faithfulness and alignment. 
% 结果如图所示，在隔离了judge长度的影响后，sample difficulty, sample length的影响都不再显著(p-value $<$ 0.05). 
As shown in Figure~\ref{fig:chain}-(b), after isolating the influence of judge length, the effects of other factors are no longer significant (p-value $>$ 0.05). Similar conclusions also hold true when GPT-4o mini \citep{4o} serves as the judge (Figure~\ref{fig:chain_4omini} in Appendix).





% 基于以上分析，我们LLM judge潜在的biases都以judge length作为中介变量，因此可以得到以下公式。我们考虑以下方案去除该框架下LLM judge潜在的biases. 记所有待评测的benchmark generators as $M_{i\in[1,N]}$，我们的核心是
Based on the analysis above, the potential biases of the LLM judge in this scenario are all mediated by judgment length.
% 因此，对于待评测的benchmark generators $\mathcal{M}_{1:N}$, 我们如下获取对他们的无偏judge结果.我们以judge score为因变量，以generator类别为哑变量，以judge length长度为协变量，并选定一个base generator (e.g., $\mathcal{M}_1$)，来构造Ordinary Least Squares Regression 模型：
Therefore, for benchmark generators $\mathcal{G}_{1:|\mathcal{G}|}$ under evaluation, we derive their unbiased judge results with a Multiple Regression model. Specifically, we set the judge score as the dependent variable, the generator categories as dummy variables, and judge length as the covariate:
\begin{equation}
\small
f(i) = \beta_i + \beta_{len} \cdot \text{judge\_length}+\epsilon
\label{equation: bias}
\end{equation}
where $f(i)$ denotes the average judge score of $\mathcal{G}_{i}$ and $\beta_i$ reflects the debiased score of $\mathcal{G}_{i}$, which we select as our metrics for faithfulness and alignment.



\subsection{Diversity}
\label{sec:diversity}
% 在credibility得到保证的前提下，benchmark的多样性决定了评测结果是否能全面地反映模型在整个待评测domain上的真实能力。
% 我们全面地考察benchmark的lexical，semantic和knowledge\&skill层面的多样性。
With credibility ensured, the diversity of the benchmark determines the extent to which evaluation results can reflect the true model capability across the assessed domain. 
Apart from the widely tested lexical and semantic diversity, our framework also examines the knowledge diversity to make the evaluation more comprehensive.

\paragraph{Lexical Diversity}reflects vocabulary richness in benchmarks. Traditional metrics like vocabulary size and self-BLEU \citep{selfbleu} used in \citet{unigen} and \citep{attr} are biased by sample length \citep{diverbias}. We use unbiased word frequency entropy \citep{entropy} as the metric to evaluate lexical diversity.
% \begin{equation}
%     \small
%     H = -\sum_{i=1}^{\text{vocabulary\_size}} \text{frequency}(\text{word}_i) \log \text{frequency}(\text{word}_i)
% \label{eq:entropy}
% \end{equation}

% lexical diversity体现在生成的benchmark中所包含的词汇的丰富性。
% 先前的方法统计benchmark的词表大小和self-bleu来衡量该criteria，但这些metrics对sample 长度存在bias。
% 因此我们考虑采用无偏的词表entropy来衡量lexical diversity：

\paragraph{Semantic Diversity}quantifies a benchmark's semantic comprehensiveness. We calculate the average Euclidean distance between semantic embeddings of samples as the metric. Specifically, we use powerful text-embedding-ada-002 \citep{text-embedding-ada-002} as the embedding model.
% Semantic Diversity反映了benchmark在语义层面的全面性。我们用ada模型提取sample的语义embedding并计算embedding的平均欧氏距离作为metric。

\paragraph{Knowledge Diversity}evaluates whether the samples evaluate different sub-abilities within the assessment demands. 
% Given that the primary goal of a benchmark is to thoroughly assess a model's capability of specific task, this criterion is essential. 
When samples test the same sub-ability, the model is likely to exhibit similar correctness patterns. Therefore, we use the correctness of a set of models (denoted as $\mathcal{M}_{1:|\mathcal{M}|}$, see Appendix~\ref{sec:bench_model_list} for detailed list) to represent the knowledge embedding for each sample. If the embeddings of two samples are highly similar, it reflects a strong alignment in the sub-abilities they assess. The average pairwise Hamming distance \citep{hamming}, suitable for discrete embeddings, is employed as the metric for this criterion.
% Knowledge\&Skill Diversity 衡量benchmark中不同sample是否尽量考察了不同的sub-ability。考虑到benchmark的核心目的就是评测模型在特定task下的知识与技能的掌握程度，对该criterion的考察至关重要。
% 考察相同sub-ability的samples很可能会引起模型同样的正确性。因此，对每个sample，我们用一组模型的正确性作为该题目的knowledge\&skill embedding。当两个sample的embedding高度相似时说明其考察的sub-ability也高度一致。在此基础上，我们用平均 pairwise hamming距离（适用于这类离散embedding）作为对应的metric。

\subsection{Difficulty}
\label{sec:difficulty}
% 如果benchmark的多样性满足需求，我们应进一步考虑benchmark的difficulty属性，这在如今模型能力分化愈发显著的时代显得尤为重要。
When diversity meets requirements, we should further consider the difficulty attribute, which is particularly significant in an era of increasingly divergent model capabilities.
\paragraph{Difficulty Controllability}refers to assigning differentiated difficulty labels to the samples (e.g., MATH \citep{math}). These labels enable the benchmark to be divided into subsets for more targeted evaluation of models with varying capabilities. For each sample, we use the average error rate of $\mathcal{M}_{1:|\mathcal{M}|}$ as the ground truth for difficulty label. Based on this, we compute the Spearman correlation between the difficulty labels provided by the benchmark and the ground truth as the metric.
% Controllability指benchmark中的samples有有区分度的难度标签（e.g., MATH）,据此可以将benchmark分成不同的subset来更有针对性地测试具有不同能力的模型。对于每个sample，我们用$\mathcal{M}_{1:|\mathcal{M}|}$的平均错误率作为difficulty的ground truth。在此基础上，我们计算benchmark提供的difficulty label和ground truth之间的斯皮尔曼相关性作为衡量difficulty controllability的metric.
\paragraph{Difficulty Boundary} denotes the difficulty of the hardest subset of a benchmark.
With the growing strength of LLMs, their performance on simpler benchmarks has reached saturation \cite{mmlu}, making it difficult to differentiate their capabilities.
Consequently, more challenging benchmarks \citep{mmlupro} are continuously introduced to evaluate the latest LLMs.
Thus, we propose assessing the average error rate of $\mathcal{M}_{1:|\mathcal{M}|}$ on the hardest subset of benchmark to measure its difficulty boundary.
% Difficulty Boundary denotes benchmark中最难的subset的难度。
% 随着模型能力越发强大，其在简单的benchmark上的表现逐渐饱和而不再能得到有效区分。
% 更难的benchmark被不断推出来评测最新的强力LLMs。
% 因此，我们考虑通过评测$\mathcal{M}_{1:|\mathcal{M}|}$在最难的subset上的平均准确率来衡量benchmark的difficulty boundary.


\subsection{Benchmark-Level}
\label{sec:benchmarklevel}
% 最后，我们考察benchmark generator的一些high-level的性质。
Lastly, we introduce high-level metrics for assessing benchmark generators.
\paragraph{Effectiveness.}
% 尽管先前介绍的criteria在各个层面检验了benchmark的质量，但我们需要一个统一的指标确定benchmark的有效性。
% 将高质量的人类标注的benchmark作为ground truth，我们检验在相同的assessment demands下generator生成的benchmark是否能起到相同的评测效果。
% 具体来说，我们分别测试$\mathcal{M}_{1:|\mathcal{M}|}$在生成的benchmark和human benchmark上的accuracy，并计算二者之间的皮尔逊相关性作为effectiveness 的metric。
While the earlier criteria assess benchmark quality from various aspects, a unified metric is required to measure benchmark effectiveness.
Taking high-quality human-annotated benchmark as the ground truth, we examine whether generated benchmark under identical assessment demands can deliver equivalent evaluation results.
To this end, we calculate the accuracy of $\mathcal{M}_{1:|\mathcal{M}|}$ on both generated and human benchmarks and use the Pearson correlation between them as the effectiveness metric.

\paragraph{Robustness.}
% 在输入类似时，一个鲁棒的系统应该产生类似的输出。相同地，我们期望一个鲁棒的benchmark generator也能对于类似的assessment demands产生相同评测效力的benchmarks。为此，我们calculate the accuracy of $\mathcal{M}_{1:|\mathcal{M}|}$ on由类似的assessment demands(原始的和由GPT-4o改写的)产生的benchmarks and use the Pearson correlation between them as the robustness metric.
Under similar inputs, a robust system should produce comparable outputs. Similarly, we expect a robust benchmark generator to produce benchmarks with equivalent evaluation efficacy for similar assessment demands. Thus, we calculate the accuracy of $\mathcal{M}_{1:|\mathcal{M}|}$ on benchmarks generated under similar assessment demands (the original and that rewritten by GPT-4o) and calculate the Pearson correlation between them as the robustness metric.

\paragraph{Efficiency.} 
% 高质量的human-annotated benchmark因其生产的低效性而阻碍了其广泛应用。我们衡量benchmark generator在生成一定size的benchmark时所消耗的时间和金钱开销作为衡量其efficiency的metric。
High-quality human-annotated benchmarks are constrained due to inefficiencies in their construction. We evaluate the efficiency of a benchmark generator by measuring the time and monetary costs associated with generating benchmarks of a certain size.

By establishing this comprehensive evaluation framework, the strengths and weaknesses of benchmark generators can be thoroughly assessed, and the reliability of the proposed method can be validated.
% 通过建立这个完备的evaluation framework，the pros and cons of benchmark generators can be thoroughly examined. 


