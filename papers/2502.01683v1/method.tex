\section{Development of BenchMaker}
\label{submission}

\begin{figure*}[h]
\centering
\includegraphics[width=0.95\textwidth]{figs/method.pdf}
\caption{Overview of \textsc{BenchMaker}.}
\label{fig:method}
\end{figure*}

% 由于先前的研究都没有实现真正的generic benchmark generator (输入只有assessment demands),因此我们将首先探究directly prompting LLM as generic benchmark generator的pros and cons in section 1。
% 在本节，在所提出的evaluation framework下，我们将首先探究directly prompting LLM as generic benchmark generator的pros and cons in section 1. 在此基础上，我们将对其
% 在本节，我们首先讨论我们主要研究的benchmark题目format.
In this section, we first discuss the primary sample format we studied in \S\ref{sec:sampleformat}.
Afterwards, since previous studies have yet to realize generic benchmark generators (with assessment demands $X$ as the sole input), we analyze the pros and cons of directly prompting the LLM as such generator in \S\ref{sec:proscons}. Building on the experimental results, we refine its weaknesses in the following sections, leading to the development of \textsc{BenchMaker}.

% 在benchmark的题目类型上，我们基于以下考量重点研究以选择题形式评测模型：（1）选择题形式具有较好的通用性，大部分ability都能通过选择题进行评测；（2）选择题能够最大程度上避免由于输出格式差异导致的误判问题；（3）无需LLM-as-a-Judge之类的外部模块辅助，具有高效性。（4）我们在生成题目时会同时生成解题思路，因此选择题benchmark可以便捷地转换为text generation的题目形式，which我们在附录进行了展示。
% 我们同样选择MATH benchmark进行实验。
\subsection{Sample Format Selection}
\label{sec:sampleformat}
Following previous studies \citep{perteval,dyval2}, we have chosen multiple-choice questions (MCQs) as the primary sample format for benchmark generation based on the following reasons: (1) Versatility: MCQ serves as a universal format for evaluating most capabilities; (2) Accuracy: Misjudgment can be effectively prevented caused by variations in output formats \citep{format}; (3) Efficiency: MCQs do not depend on external modules such as LLM-as-a-Judge, ensuring a streamlined evaluation process; (4) Transformability: Each generated sample includes a rationale, enabling easy conversion into other formats, such as the text generation format presented in the Appendix~\ref{sec:convert}.

\subsection{Pros and Cons of Directly Prompting}
\label{sec:proscons}
% 我们选择MATH 作为高质量参照benchmark。It 包含的subsets对应七个数学子学科，which we use as assessment demands. 我们以Appendix中展示的prompt来引导LLM生成Credibible、diverse的samples，且等比例调整prompt中的难度需求。我们以GPT-4o mini作为generator，并将采样温度设置为1来增加多样性。实验结果如表所示，
% We choose human-annotated MATH\footnote{We convert it into MCQ format, see details in Appendix.} \citep{math} (mathematical reasoning) and MMLU-Pro \citep{mmlupro} (language understanding) as high-quality benchmarks $\mathcal{D}_{human}$ for comparison. Based on the descriptions in respective papers, we write simple assessment demands $X$ for each subset (See details in Appendix 2). We adopt the prompt in the appendix to guide $\mathcal{M}$: GPT-4o mini in generating credible and diverse samples $s_{1:|\mathcal{D}_{human}|}$, with sampling temperature set as 1:
We choose MATH \citep{math}, MMLU-Pro \citep{mmlupro} and HellaSwag \citep{hellaswag} as high-quality benchmarks $\mathcal{D}_{human}$ for comparison. We adopt the prompt in Appendix~\ref{sec:prompt_direct} to guide $\mathcal{M}$: GPT-4o mini in generating credible and diverse samples $s_{1:|\mathcal{D}_{human}|}$:
\begin{equation}
    s_i = \{q_i,r_i,o_i,a_i\} = \mathcal{M}(\text{prompt}_{base},l,X)
\end{equation}
% As shown in Table~\ref{tab:math}, compared with human-annotated benchmark, directly prompting LLM as benchmark generator展现了较差的faithfulness，较低的lexical和semantic多样性，较弱的难度controllability和较低的难度挑战性
where $q_i,r_i,o_i,a_i$ denote question, rationale, options, and label, respectively. We proportionally adjust the difficulty level $l$ from 1 to 10 in the prompt (see descriptions in Appendix~\ref{sec:dif_level}), and select samples with top 20\% difficulty level to form the hardest subset. 
The assessment demands $X$ are shown in Appendix~\ref{sec:demands}.
As shown in Table~\ref{tab:main}, compared to $\mathcal{D}_{human}$, directly prompting LLM as generic benchmark generator demonstrates poorer faithfulness, lower lexical and semantic diversity, weaker difficulty controllability, and less challenging subset. 
% Meanwhile, we也观察到了较好的alignment（我们将Human Benchmark对应值设定为1，并对其余benchmark的相应值进行修正，因此可能得到大于1的结果）、较好的knowledge\&skill diversity，以及更优的efficiency。
Meanwhile, we also observe its advantages in better alignment\footnote{We set the debiased LLM-as-a-judge score of the human benchmark to 1, adjusting scores of generated benchmarks accordingly, which may result in scores exceeding 1.}, greater knowledge diversity, and improved efficiency. 

\subsection{Faithfulness Optimization}
\label{sec:fat_opt}
% 为了提升Faithfulness，先前的工作尝试过self-critique和利用外部工具等方法。我们认为前者具有较好地通用性，因此设计了以下两种适配BenchMaker的技术来优化faithfulness。
To enhance faithfulness, previous studies have explored methods such as self-correction \citep{SelfIns,SelfReflect} and the use of external tools \citep{solver,rag}. As self-correction offers greater versatility, we propose the following two BenchMaker-compatible techniques to optimize faithfulness.
\paragraph{Stepwise Self-correction.}Since errors might occur at any step during the generation of $\{q_i,r_i,o_i,a_i\}$, we instruct the model to validate the content at each step. If an error is detected, the model will return to the beginning. Compared to full-sample self-checking, step-wise critique boosts error detection with less decoding cost (See Appendix~\ref{sec:unsuccess}).
\paragraph{Conflict Guided Contrastive Discrimination.} 
% 研究发现有些时候模型struggle to amend their prior responses，特别是对于较难的题目。
\citet{cannotselfcritic} finds that LLMs struggle to correctly judge their prior answers on challenging questions. 
% 因此，除了让LLM act as judge in Stepwise Self-correction，我们还让LLM作为做题者来帮助找到潜在的错误题目。具体来说，我们让LLM独立回答$q_i$ T次，并进行多数投票获取LLM的self-consistency结果$\Bar{a}_i$. 如果$\Bar{a}_i \neq a_i$，说明模型在出题和做题的视角对$q_i%有不同的rationale。考虑到\citet{judge}指出comparing-based judge相比于 item-wise judge更加精准，我们让LLM对$r_i$和%\Bar{r}_i%进行contrastive discrimination. 我们以最终的judge结果作为s_i最终的rationale和label.
Therefore, we extend Stepwise Self-correction by having the LLM not only act as a judge but also as a test-taker to identify potential errors. Let the LLM answers $q_i$ $T$ times to attain $\Bar{a}_i^{1:T}$, we get the self-consistency \citep{sc} result $\Dot{\Bar{a}}_i$ through majority voting. If $\Dot{\Bar{a}}_i \neq a_i$, the conflict suggests differing $r_i$ and $\Dot{\Bar{r}}_i$. As \citet{llmasjudge} finds that comparison-based judges are more accurate than item-wise judges, we have the LLM conduct a contrastive discrimination between $r_i$ and $\Dot{\Bar{r}}_i$ to determine the final rationale and label for $s_i$.

\subsection{Difficulty Optimization}
\label{sec:diff_bound_opt}
% 根据\S\ref{sec:proscons}中的实验结果，我们发现LLM的难度可控sample生成能力较差。特别是在language understanding task(\ref{tab})上，所生成sample的实际难度和期望难度之间的Spearman只有0.021. 这也是导致LLM难以生成较难的subset的原因。为此，我们首先考察LLM的难度感知能力：让LLM对所生成的sample的难度进行打分并据此作为difficulty label，我们发现Spearman仅仅从0.021涨到了0.089。这说明LLM存在一定的难度感知能力，但较差。我们再次尝试切换LLM的角色，从做题者的视角尝试感知题目难度：我们计算$\Bar{a}_i^{1:T}$与$a_i$的一致率作为difficulty label，发现此时相关性大幅提升至0.415. 
\paragraph{Difficulty Controllability.} From \S\ref{sec:proscons}, we know that the LLM's ability to control the difficulty of generated samples is limited. In particular, for the language understanding task (MMLU-Pro), the Spearman correlation between the actual and expected difficulty of the samples is only 0.021. To further explore this, we examine LLM’s difficulty perception by asking it to score the difficulty label of the generated samples. However, the correlation only increases to 0.089, suggesting that while LLM has some capacity to perceive difficulty, it is still weak. We then switch the role of LLM and assess the difficulty from the perspective of test-taker:
\begin{equation}
    \beta_i = \frac{1}{T} \sum_{j=1}^T \textbf{1}_{\Bar{a}_i^j\neq a_i}
\end{equation}
By taking the inconsistency between $\Bar{a}_i^{1:T}$ and $a_i$ as difficulty label, the correlation increases to 0.415, suggesting that $\beta$ is a reliable metric for difficulty controllability.
% On this basis, we propose the following techniques to extend the sample difficulty boundary.


\paragraph{Difficulty Diffusion Mechanism.} 
% 考虑到模型有一定的难度感知能力，我们随机选取当前已经生成的较难的samples作为输入，让模型以此为基准生成更难的sample。具体的算法见
Given that the LLM has a certain level of difficulty perception, we iteratively select the more challenging samples according to $\beta$ from the generated ones as difficulty references, and instruct the LLM to generate a more difficult sample. This allows the sample difficulty to rise continuously through diffusion. The detailed algorithm is described in Appendix~\ref{sec:ddd}. 
\paragraph{Difficulty Strategy Guidance.}
% 我们进一步考虑为LLM提供任务相关的生成困难sample的策略。为此我们让LLM先对给定的$X$，生成不同难度等级sample所对应的策略，如越难的考察推理能力的sample应有越多的推理step。配合Difficulty Diffusion Mechanism，随着sample index的增加，我们为LLM输入更难sample的策略。
We further consider providing the LLM with task-specific difficulty-control strategies. Specifically, we first require the LLM to give varying strategies for generating samples of specific difficulty levels based on the given $X$ (see examples in Appendix~\ref{sec:strategies}). For example, difficult samples those assessing reasoning ability generally require more reasoning steps. With the Difficulty Diffusion Mechanism, we progressively introduce more difficult sample generation strategies to the LLM to further extend the difficulty boundary.


\subsection{Diversity Optimization}
\label{sec:div_opt}
% 优化合成数据的多样性已经被广泛研究，我们对它们进行了广泛的测试，并选择了最有效的以下方法。
The optimization of synthetic data diversity has been widely studied \citep{datasurvey1}. We conduct extensive tests and select the most generic and effective \textbf{AttrPrompt} \citep{attr} technique for \textsc{BenchMaker}. 
%AttrPrompt 通过为每个sample随机分配预先生成的(attribute,value) pairs作为input的一部分来显示地提升benchmark的lexical和semantic多样性。
AttrPrompt explicitly enhances the lexical and semantic diversity of benchmarks by randomly assigning pre-generated (attribute, value) pairs as part of the input for each sample.
%此外，我们注意到把生成的samples作为难度reference可能造成sample同质化问题。因此我们设计了in-batch redundancy filtering方法，让LLM每次生成$K$个候选sample，只保留与输入的reference samples word frequency entropy最大的one。
Furthermore, we notice that the introduction of treating the generated samples as difficulty references might cause sample homogeneity. To mitigate this, we propose an \textbf{In-batch Diversity Boosting} method, where LLM generates $L$ (We set $L$ as 5 for our default setting) candidate samples and selects the one with the greatest word frequency entropy difference from the input reference samples.

% \citet{attr}提出让LLM针对给定的task生成