\section{Experiments and Analyses}
\label{sec:exp}
\begin{table*}[t]
\renewcommand\arraystretch{1.2}
\centering
\small
\caption{Overall experimental results under the proposed evaluation framework. For each setting, we run three times and report the average results. We take GPT-4o mini as default generator. Values in bold denote the best results between \textsc{BenchMaker} and Human Benchmark.}
\setlength{\tabcolsep}{0.32em} 
\begin{tabular}{ccccccccccc}
\toprule
\multirow{3}{*}{\textbf{Methods}} & \textbf{Faithful} & \textbf{Alignment} & \textbf{Lexical} & \textbf{Semantic} & \textbf{Knowledge} & \textbf{Control} & \textbf{Boundary} & \textbf{Effective} & \textbf{Robust} & \textbf{Efficiency}\\
\cdashline{2-11}
&  Unbias & Unbias  & \multirow{2}{*}{Entropy\textbf{$\uparrow$}} & Euclidean  & Hamming & Spea-  & Error&Pear- &Pear- &\$/item,\\
&Score\textbf{$\uparrow$} &Score\textbf{$\uparrow$}&&Distance\textbf{$\uparrow$}&Distance\textbf{$\uparrow$}&rman\textbf{$\uparrow$}&Rate\textbf{$\uparrow$}&son\textbf{$\uparrow$}&son\textbf{$\uparrow$}&min/item\textbf{$\downarrow$}\\
\midrule
\multicolumn{11}{c}{MATH \citep{math}} \\
% \rowcolor[HTML]{FFCCCC}
Human Benchmark& \textbf{1.000} & 1.000 & 8.054 & 0.665 & 0.349 & 0.143 & 0.752&-&-&high \\
% \rowcolor[HTML]{FFFFCC}
Direct Prompt& 0.665 & 1.166 & 7.091 & 0.618 & 0.365 & 0.109 & 0.635 &0.687 & 0.991 & 0.002, 0.17 \\
% \rowcolor[HTML]{E5CCFF}
+AttrPrompt& 0.611 & 1.138 & 8.265 & 0.675 & 0.360 & 0.124 & 0.659 &0.759 & 0.983 & 0.002, 0.19 \\
% \rowcolor[HTML]{E5CCFF}
+InBatchDivBoost& 0.623 & 1.142 & 8.652 & 0.677 & 0.366 & 0.115 & 0.628 &0.778 & 0.985 & 0.003, 0.20 \\
% \rowcolor[HTML]{E5CCFF}
+StepSelfCorrect& 0.924 & 1.152 & 8.674 & 0.675 & 0.369 & 0.162 & 0.557 &0.803 & 0.979 & 0.003, 0.23 \\
% \rowcolor[HTML]{E5CCFF}
+ConflictConDisc& 1.019 & 1.151 & 8.668 & 0.678 & 0.357 & 0.175 & 0.515 &0.838 & 0.992 & 0.004, 0.35 \\
% \rowcolor[HTML]{E5CCFF}
+DiffControl& 1.019 & 1.151 & 8.668 & 0.678 & 0.357 & 0.403 & 0.515 &0.838 & 0.992 & 0.004, 0.35 \\
% \rowcolor[HTML]{E5CCFF}
+DiffDiffusion& 0.994 & 1.166 & 8.705 & 0.680 & 0.387 & 0.451 & 0.683 &0.882 & 0.990 & 0.005, 0.39 \\
% \rowcolor[HTML]{CCFFFF}
BenchMaker& 0.930 & 1.200 & \textbf{8.976} & \textbf{0.681} & 0.403 & \textbf{0.434} & 0.768&0.935&0.986&\textbf{0.005, 0.42} \\
% \rowcolor[HTML]{CCFFFF}
BenchMaker$_{\text{4o}}$& 0.918 & \textbf{1.223} & 8.835 & 0.675 & 0.385 & 0.432 & \textbf{0.779}&\textbf{0.941}&\textbf{0.988}&0.084, 1.12 \\
% \rowcolor[HTML]{CCFFFF}
BenchMaker$_{\text{haiku}}$& 0.902 & 1.116 & 8.878 & 0.676 & \textbf{0.410} & 0.401 & 0.775&0.912&0.979&0.026, 0.57 \\
\cdashline{1-11}
\multicolumn{11}{c}{MMLU-Pro \citep{mmlupro}} \\
% \rowcolor[HTML]{FFCCCC}
Human Benchmark& 1.000 & 1.000 & \textbf{10.404} & \textbf{0.731} & 0.307 & 0.000 & 0.751&-&-&high \\
% \rowcolor[HTML]{FFFFCC}
Direct Prompt& 0.894 & 1.218 & 9.608 & 0.726 &0.391  & 0.021 & 0.587&0.850&0.989&0.002, 0.16 \\
% \rowcolor[HTML]{CCFFFF}
BenchMaker& \textbf{1.020} & \textbf{1.245} & 10.166 & 0.728 & \textbf{0.395} & \textbf{0.477} & \textbf{0.759} & 0.967&0.982 & \textbf{0.005, 0.38} \\
\cdashline{1-11}
\multicolumn{11}{c}{HellaSwag \citep{hellaswag}} \\
% \rowcolor[HTML]{FFCCCC}
Human Benchmark& 1.000 & 1.000 & \textbf{9.167} & 0.655 & 0.384 & 0.000 & 0.569&-&-&high \\
% \rowcolor[HTML]{FFFFCC}
Direct Prompt& 0.862 & 1.107 & 8.165 & 0.660 &0.396  & 0.047 & 0.626&0.821&0.979&0.002, 0.17 \\
% \rowcolor[HTML]{CCFFFF}
BenchMaker& \textbf{1.032} & \textbf{1.130} & 9.052 & \textbf{0.663} & \textbf{0.421} & \textbf{0.439} & \textbf{0.708} & 0.958&0.984 & \textbf{0.005, 0.40} \\
% \rowcolor[HTML]{CCFFCC} % 淡绿色
% \rowcolor[HTML]{E5CCFF} % 淡紫色
\bottomrule
\end{tabular}
\label{tab:main}
\vspace{-0.1cm}
\end{table*}
% 在本节，我们对所提出的\textsc{BenchMaker}进行全面的检验。
% 在所提出的evaluation framework下，我们首先横向比较了\textsc{BenchMaker}与高质量人类标注benchmark。在此基础上，我们
We conduct comprehensive experiments to validate \textsc{BenchMaker} under the proposed framework in this section.
\paragraph{Settings.}
% 我们选择human-annotated的 MATH\footnote{We convert it into MCQ format, see details in Appendix.}（考察mathematical reasoning）和MMLU-Pro（考察multi-task language understanding）作为我们的基准benchmarks。
% 对于MATH 包含的7个子集和MMLU-Pro包含的13个子集\footnote{Type 'other' is excluded.}，我们分别write simple assessment demands (See details in Appendix 2) 作为generator的输入，进行实验。
% 每组实验我们进行3次，并汇报平均结果。
% 我们以GPT-4o mini作为main generator，并额外探究了GPT-4o的表现。
% 解码温度设为1。
We select the widely used human-annotated MATH\footnote{Converted into a MCQ format, see details in Appendix~\ref{sec:MATH_convert}.} \citep{math} (mathematical reasoning), MMLU-Pro (multi-task language understanding) \citep{mmlupro} and HellaSwag (commonsense reasoning) \citep{hellaswag} as high-quality baseline benchmarks.
For the 7 subsets of MATH, the 13 subsets of MMLU-Pro\footnote{Excluding the type 'other'.} and HellaSwag, we write simple assessment demands respectively (see details in Appendix~\ref{sec:demands}) as inputs for the benchmark generator. 
% 对于每个demands我们让模型生成500条samples，并随机采样human-annotated benchmark与所生成的数据量对齐。
For each demand, we generate 500 samples and randomly downsample the human-annotated benchmark to match the number of generated samples for fair comparison.
Each experiment is repeated three times, and the average results are reported.
We use GPT-4o mini \citep{4o} as the default generator and also explore the performance of GPT-4o and Claude 3.5 Haiku \citep{claude}. The decoding temperature is set to 1.
To mitigate the self-enhancement bias \citep{llmasjudge} associated with LLM-as-a-judge, we substitute the generators with Qwen-Plus \citep{qwen} as the judge.

\subsection{Comparison with Human-annotated Benchmark}
% 如表格2，3，4所示，整体来说，\textsc{BenchMaker} 在faithfulness，lexical and semantic diversity上取得了与human-annotated benchmark相当的性能，而在其他指标上取得了更优的效果
As shown in Table~\ref{tab:main}, overall, \textsc{BenchMaker} achieves comparable performance to human-annotated benchmarks in terms of faithfulness and lexical\&semantic diversity. Meanwhile, \textsc{BenchMaker} outperforms them in all other metrics, especially in alignment, knowledge diversity, difficulty controllability and efficiency. 
The exceptional results achieved in these metrics comprehensively validate the reliability of the generated samples by \textsc{BenchMaker}.
%在这些指标上的卓越表现使得所生成的samples的reliability从各个方面都得到了验证与强化。
\paragraph{Effectiveness.} 
% benchmark的核心目的就是为待评测的模型给予适当的评分，从而进行能力区分。\textsc{BenchMaker}的benchmarking结果与human-annotated benchmarks高度对齐，在线性相关性方面达到了平均0.953的皮尔逊相关性，在秩序相关性上更是达到了平均0.966的相关性，展现了卓越的effectiveness。
The primary goal of benchmarking is to assign accurate scores to models under evaluation, facilitating capability differentiation. The benchmarking results of \textsc{BenchMaker} align closely with human-annotated benchmarks, with an average of 0.953 linear correlation (Pearson) and a remarkable 0.966 for rank-order correlation (Spearman), highlighting its outstanding effectiveness.
\paragraph{Robustness.}
% 让模型$\mathcal{M}$根据demands $X$ 生成benchmark的过程可以认为是从模型的特定解码空间$f(\mathcal{M},X)$中进行采样。实验结果说明，
% 在evaluation demands 语义相同而语言风格有区别的情况下，所得到的benchmarks的评测efficacy几乎一致（平均0.984的Pearson correlation）。这验证了BenchMaker对于输入的鲁棒性，也保障了具有不同语言习惯的人在使用BenchMaker时能得到无冲突的结果。
Under evaluation demands where semantic equivalence is maintained but linguistic styles vary, the benchmarks exhibit nearly identical assessment efficacy, with an average Pearson correlation of 0.984. This demonstrates the robustness of \textsc{BenchMaker} to diverse inputs and ensures that users with different linguistic preferences can obtain consistent evaluation results.
\paragraph{Efficiency.}
% human-annotated benchmark的最大限制就是较低的构建效率。然而，\textsc{BenchMaker}能够以平均\$0.005的开销用0.40minute生成一条samples。而且随着未来技术与硬件的持续发展，效率还将进一步提升。\textsc{BenchMaker}的Efficiency使得我们能够以极小的成本为特定的下游任务选择合适的模型。
The primary limitation of human-annotated benchmarks lies in their low construction efficiency. However, \textsc{BenchMaker} can generate a sample at an average cost of \$0.005 within 0.40 minutes. Furthermore, its efficiency is expected to continuously improve with the development of technology and hardware.

\paragraph{Generalizability.}
% 实验结果显示，\textsc{BenchMaker}在对于不同的任务类型以及不同的generator都具有良好的泛化性。特别地，我们注意到，更强力的模型并不一定在各个指标上都带来更优异的性能。相比GPT-4o，GPT-4o mini是性价比更高的benchmark generator.
Experimental results demonstrate that \textsc{BenchMaker} exhibits strong generalization across different task types and generators. Notably, a more powerful model does not necessarily yield superior performance across all metrics. Compared to GPT-4o, GPT-4o mini proves to be a more cost-effective benchmark generator.


\subsection{Ablation Studies}
% 我们在MATH benchmark上，通过在Direct Prompt上sequentially添加techniques的方式，对它们的有效性分别进行了验证。
We validate the effectiveness of different techniques by sequentially integrating them to the Direct Prompt baseline on the MATH benchmark, as shown in Table~\ref{tab:main}.
\paragraph{Diversity.}
% 相比于Direct Prompt，AttrPrompt 和 In-batch Diversity Boosting均有效提升了lexical和semantic diversity。我们也注意到knowledge diversity并没有得到提升,这说明表面的多样化与对于skills的考察范围的广泛性并不等价。
Compared to Direct Prompt, both AttrPrompt and In-batch Diversity Boosting effectively enhance lexical and semantic diversity. Noticeably, we observe that knowledge diversity remains unchanged, indicating that surface-level diversification does not necessarily equate to a broader assessment of knowledge and skills. Meanwhile, the diversity improvement leads to a slight drop in faithfulness, possibly because of the attributes constraints.
\paragraph{Faithfulness.} 
% 在应用Stepwise Self-correction和Conflict Guided Contrastive Discrimination后，我们注意到faithfulness得到了持续的显著提升。与此同时，我们注意到最困难子集的难度下降了（accuracy从0.341提升至0.443）。我们猜测这是因为在faithfulness得不到保障的时候，label较高的错误率导致了对于模型性能的低估。因此在label得到修正后，模型的accuracy才能真正反映benchmark的难度。
After applying Stepwise Self-correction and Conflict Guided Contrastive Discrimination, we observe a sustained and significant improvement in faithfulness. At the same time, we notice a reduction in the difficulty of the hardest subset, with the error rate decreasing from 0.659 to 0.557. We hypothesize that this may be due to the high error rate in labels when faithfulness is not ensured, which leads to an underestimation of model performance. Consequently, once the labels are corrected, the accuracy can better reflect the actual difficulty of the benchmark.
\paragraph{Difficulty Controllability.}
% By 将generator作为test-taker and taking the error rate of test-taker as difficulty label，我们对于题目难度的控制更加精确了（0.403斯皮尔曼相关性）。考虑到先前发现LLM的难度感知能力较差，我们猜测提升来自于角色的转变使得模型需要进行确切的推理，且采用了预测与label间的不一致性这一客观指标作为metric。
By treating the generator as the test-taker and using its error rate as the difficulty label, we achieve more precise control over sample difficulty (Spearman correlation of 0.403). Considering the previously observed weak difficulty perception of LLMs, we hypothesize that this improvement stems from the role shift, which requires the model to engage in explicit reasoning, along with the adoption of prediction-label inconsistency as an objective metric.

\begin{figure}[t]
\centering
\includegraphics[width=0.47\textwidth]{figs/index.pdf}
\caption{Trends of real and labeled difficulty over the index.}
\label{fig:index}
\vspace{-5pt}
\end{figure}

\paragraph{Difficulty Boundary.}
% With our proposed Difficulty Diffusion Mechanism and Difficulty Strategy Guidance, the difficulty boundary is significantly extended (Error rate increases from 0.515 to 0.768), 这验证了它们的有效性。我们额外统计了题目的实际难度和难度标签随题目生成order的变化。如图所示，题目的难度标签和实际难度都不断地提升。这既表明了Difficulty Diffusion Mechanism如我们预期的那般进行，也直观地展示了难度标签与实际难度的一致性
With our proposed Difficulty Diffusion Mechanism and Difficulty Strategy Guidance, the difficulty boundary is significantly extended, as evidenced by an increase in error rate from 0.515 to 0.768, validating their effectiveness. Additionally, we analyze how the actual difficulty and difficulty labels evolve with the order of generated samples. As illustrated in Figure~\ref{fig:index}, both the difficulty label and actual difficulty exhibit a continuous upward trend. This not only confirms that Difficulty Diffusion Mechanism operates as intended but also visually demonstrates the strong consistency between difficulty label and actual difficulty.

\begin{figure}[!htb]
    \centering
    \subfigure[MMLU-Pro]{\includegraphics[width=0.48\hsize, height=0.27\hsize]{figs/wordfig_mmlu_ori.pdf}\label{fig: sub_figure1}} 
    \vspace{-3mm}
    \subfigure[\textsc{BenchMaker}]{\includegraphics[width=0.48\hsize, height=0.27\hsize]{figs/wordfig_mmlu_gen.pdf}\label{fig: sub_figure2}} 
    % \setlength{\abovecaptionskip}{-5mm}
    % \setlength{\belowcaptionskip}{0pt}
    % \includegraphics[width=0.98\textwidth]{score.pdf}
    \caption{Word cloud of MMLU-Pro and the benchmark generated by \textsc{BenchMaker} under similar assessment demands.}
    \label{fig:word}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.47\textwidth]{figs/tsne_mmlupro.pdf}
\caption{T-SNE results on the text embeddings of benchmarks.}
\label{fig:tsne}
\end{figure}

\subsection{A Closer Look at the Generated Benchmark}
% 我们以MMLU-Pro assessment demands为input所生成的benchmark为例，对\textsc{BenchMaker}进行更细致的检查。
After metric analysis, we perform a more thorough examination of \textsc{BenchMaker}. 
% 我们在附录展示一些生成的samples
Some of the generated samples are shown in Appendix~\ref{sec:example}.
\paragraph{Lexical and Semantic.}
% 首先，虽然the generated benchmark在与MMLU-Pro的word 分布有着较大的区别，但其也紧密围绕MMLU-Pro所包含的各个domain，展现了较强的task alignment。
First, despite the obvious differences in word distribution between the generated benchmark and MMLU-Pro (Figure~\ref{fig:word}), it remains closely aligned with the domains covered by MMLU-Pro, demonstrating strong task alignment.
Meanwhile, 
% 在语义上二者的对齐性更加显著。需要注意的是，输入的demands没有提到MMLU-Pro相关的信息，这避免了模型通过记忆并复写MMLU-Pro中的samples实现较高对齐度的可能性。
The semantic alignment between the two is more pronounced (Figure~\ref{fig:tsne}). Notably, the input demands (Appendix~\ref{sec:demand_mmlupro}) do not mention any information related to MMLU-Pro, effectively preventing the model from achieving a high degree of alignment by memorizing and replicating samples from MMLU-Pro.
\paragraph{Actual Error Rate.}
% 尽管LLM-as-a-judge已经无偏地对benchmark的faithfulness进行了估计，我们额外随机选择了80条samples并进行了人工check。其中，我们发现有3条samples存在label不正确问题，3条存在没有正确候选项问题，整体错误率为7.5\%。同时，LLM-as-a-judge认为这80条中有5条存在问题，其中3条与human check的结果相同。这些结果可以说明：（1）\textsc{BenchMaker}的faithfulness确实有提升空间（2）LLM-as-a-judge能够一定程度上作为人类判断的代理。
Although LLM-as-a-judge has provided an unbiased estimation of the benchmark’s faithfulness, we additionally conduct a manual check on 80 randomly selected samples. Our findings indicate that 3 samples have incorrect labels, 3 samples lack a correct candidate, resulting in an overall error rate of 7.5\%. Meanwhile, LLM-as-a-judge identifies 5 problematic samples, with 3 overlapping with human judgment. These results suggest that: (1) \textsc{BenchMaker} still has room for improvement in faithfulness; (2) LLM-as-a-judge can serve as a partial proxy for human evaluation.
\subsection{Reliability Estimation.}
% 考虑到当前生成的benchmark的faithfulness仍有缺陷，我们进行以下分析：
Since faithfulness of the generated benchmark cannot be totally ensured, we are curious about the effects of incorrect samples:
Let $\Bar{a}$ and $\Bar{b}$ be the observed accuracies of two models $A$ and $B$ on the generated benchmark of size $N$, where a fraction $K$ of the samples are incorrect (which can be estimated by the LLM-as-a-judge). 
Suppose that $\bar{a}>\bar{b}$, we aim to estimate the probability that the observed ability rank is correct (the true accuracies satisfy $E[a] > E[b]$). See detailed derivation in Appendix~\ref{sec:prof}. Suppose $A$ and $B$ have the same accuracy $p$ on incorrect samples, we get:
\begin{equation}
E[a] = \frac{\bar{a} - K \cdot p}{1 - K}, \quad E[b] = \frac{\bar{b} - K \cdot p}{1 - K}
\end{equation}
and
\begin{equation}
E[a] - E[b] = \frac{\bar{a} - \bar{b}}{1 - K}
\end{equation}
Next, we perform hypothesis testing to assess the probability of \( E[a] > E[b] \). We assume that \( \bar{a} - \bar{b} \) follows a normal distribution. The \( z \)-score for the difference is:
\begin{equation}
% \small
\begin{split}
z =& \frac{(\bar{a} - \bar{b})/(1-K)}{\sqrt{(\bar{a}(1-\bar{a}) + \bar{b}(1-\bar{b}))/(N(1-K)^2)}} \\
=&\frac{(\bar{a} - \bar{b})\sqrt{N}}{\sqrt{\bar{a}(1-\bar{a}) + \bar{b}(1-\bar{b})}}
\end{split}
\end{equation}
where \( \Phi(z) \) is the cumulative distribution function of the standard normal distribution. 
The probability \( P(E[a] > E[b]) \) is given by the right-tail probability of the normal distribution:
\begin{equation}
P(E[a] > E[b]) = 1 - \Phi(z)
\label{eq:bound}
\end{equation}
where \( \Phi(z) \) is the cumulative distribution function of the standard normal distribution. 
We can assess the reliability of \textsc{BenchMaker} evaluation results using ~\eqref{eq:bound}.
Also, we notice that $K$ has the same scaling effect on both the numerator and denominator of the test statistic, thus does not alter the \( z \)-score. Consequently, as long as there is no bias, a certain proportion of noise in the benchmark will not affect the statistical significance of ability ranking.
% 我们可以以eq 5来判断\textsc{BenchMaker}评测结果的reliability.



% \begin{table*}[ht]
% \renewcommand\arraystretch{1.2}
% \centering
% \small
% \setlength{\tabcolsep}{0.32em} 
% \begin{tabular}{ccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Methods}} & \textbf{Faithful} & \textbf{Alignment} & \textbf{Lexical} & \textbf{Semantic} & \textbf{Knowledge} & \textbf{Control} & \textbf{Boundary} & \textbf{Effective} & \textbf{Robust} & \textbf{Efficiency}\\
% \cdashline{2-11}
% &  Unbias & Unbias  & \multirow{2}{*}{Entropy\textbf{$\uparrow$}} & Euclidean  & Hamming & Spea-  & \multirow{2}{*}{Accuracy\textbf{$\downarrow$}}&Pear- &Pear- &\$/item,\\
% &Score\textbf{$\uparrow$} &Score\textbf{$\uparrow$}&&Distance\textbf{$\uparrow$}&Distance\textbf{$\uparrow$}&rman\textbf{$\uparrow$}&&son\textbf{$\uparrow$}&son\textbf{$\uparrow$}&min/item\textbf{$\downarrow$}\\
% \midrule
% \rowcolor[HTML]{FFCCCC}
% Human Benchmark& 1.000 & 1.000 & 10.404 & 0.731 & 0.307 & - & 0.249&-&-&high \\
% \rowcolor[HTML]{FFFFCC}
% Direct Prompt& 0.894 & 1.218 & 9.608 & 0.726 &0.391  & 0.021 & 0.413&0.850&0.989&0.002, 0.16 \\
% \rowcolor[HTML]{CCFFFF}
% BenchMaker& 1.020 & 1.245 & 10.166 & 0.728 & 0.395 & 0.477 & 0.241 & 0.967&0.982 & 0.005, 0.38 \\
% % \rowcolor[HTML]{CCFFCC} % 淡绿色
% % \rowcolor[HTML]{E5CCFF} % 淡紫色
% \bottomrule
% \end{tabular}
% \caption{MMLUPro}
% \label{tab:mmlupro}
% \vspace{-0.1cm}
% \end{table*}

% \begin{table*}[ht]
% \renewcommand\arraystretch{1.2}
% \centering
% \small
% \setlength{\tabcolsep}{0.32em} 
% \begin{tabular}{ccccccccccc}
% \toprule
% \multirow{3}{*}{\textbf{Methods}} & \textbf{Faithful} & \textbf{Alignment} & \textbf{Lexical} & \textbf{Semantic} & \textbf{Knowledge} & \textbf{Control} & \textbf{Boundary} & \textbf{Effective} & \textbf{Robust} & \textbf{Efficiency}\\
% \cdashline{2-11}
% &  Unbias & Unbias  & \multirow{2}{*}{Entropy\textbf{$\uparrow$}} & Euclidean  & Hamming & Spea-  & \multirow{2}{*}{Accuracy\textbf{$\downarrow$}}&Pear- &Pear- &\$/item,\\
% &Score\textbf{$\uparrow$} &Score\textbf{$\uparrow$}&&Distance\textbf{$\uparrow$}&Distance\textbf{$\uparrow$}&rman\textbf{$\uparrow$}&&son\textbf{$\uparrow$}&son\textbf{$\uparrow$}&min/item\textbf{$\downarrow$}\\
% \midrule
% \rowcolor[HTML]{FFCCCC}
% Human Benchmark& 1.000 & 1.000 & 9.167 & 0.655 & 0.384 & - & 0.431&-&-&high \\
% \rowcolor[HTML]{FFFFCC}
% Direct Prompt& 0.862 & 1.107 & 8.165 & 0.660 &0.396  & 0.047 & 0.374&0.821&0.979&0.002, 0.17 \\
% \rowcolor[HTML]{CCFFFF}
% BenchMaker& 1.032 & 1.130 & 9.052 & 0.663 & 0.421 & 0.439 & 0.292 & 0.958&0.984 & 0.005, 0.40 \\
% % \rowcolor[HTML]{CCFFCC} % 淡绿色
% % \rowcolor[HTML]{E5CCFF} % 淡紫色
% \bottomrule
% \end{tabular}
% \caption{HellaSwag}
% \label{tab:hellaswag}
% \vspace{-0.1cm}
% \end{table*}