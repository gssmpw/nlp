\section*{Limitations}
From an objective perspective , we think there are two main limitations of this paper:
\begin{enumerate}

\item \textsc{BatchEval} requires LLMs to have a certain capability to handle longer contexts. From Appendix~\ref{sec:hyper}, we found that as the batchsize increases, LLMs struggle to handle too many samples, leading to a performance decline. We also attempted to test \textsc{BatchEval}'s performance on Llama-2-13b-chat-hf and found that the batchsize must be set to 2 or 3 to see any benefits. Therefore, when setting the batchsize, we cannot exceed the limit of how many samples an LLM can process in a single context. Fortunately, we discovered that a batchsize of 10 is suitable for current mainstream LLMs. Additionally, as LLMs continue to advance, they can handle increasingly larger contexts. Thus, from this perspective, \textsc{BatchEval} is a scalable method that improves alongside the capabilities of LLMs (increasing the batchsize within the capabilities of the LLM can enhance the evaluation effectiveness of the LLM).

\item We only explored a limited number of schemes of \textsc{BatchEval}. We leave exploring possible schemes of \textsc{BatchEval} for future research.


\end{enumerate}
% 对上下文长度有依赖 -- 上下文不断扩大
% 还有更多的潜在方法、推广到其他任务中 -- promising，留给未来探究
