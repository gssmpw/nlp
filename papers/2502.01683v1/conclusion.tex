\section*{Conclusions}
\label{sec:con}
% In this work，针对当前对于generic benchmark generator的紧迫需求，我们：（1）提出了一个综合、自动化、无偏的evaluation framework来验证和优化generator的reliability。（2）develop了一个reliable, Generic, and Efficient \textsc{BenchMaker}。Experiments across multiple tasks and LLMs验证了\textsc{BenchMaker}具有与human-annotated benchmarks对齐的质量，和更优的效率与泛化性。
The rapid advancement of large language models has driven an urgent demand for a generic benchmark generator. To this end, we first propose a comprehensive, automated, and unbiased evaluation framework to validate and optimize the reliability of benchmark generators.
Based on this, we develop the \textsc{BenchMaker} method for reliable, generic, and efficient benchmark generation.
Comprehensive experiments across multiple tasks and LLMs demonstrate that \textsc{BenchMaker} achieves human-aligned benchmark quality, with superior efficiency and generalization. 
% 我们further提供了


\section*{Impact Statement}
This paper presents \textsc{BenchMaker}, an LLM-driven reliable , generic and efficient benchmark generator. There
are many potential societal consequences of our work, none
which we feel must be specifically highlighted here.
\paragraph{Ethics Statement.}
All of the datasets used in this study were publicly available. 
% 多个作者共同完成了Actual Error Rate部分的manual check。
Multiple authors jointly conducted the manual check for the Actual Error Rate section, and no extra annotators were employed for our data collection.
We confirm that the datasets we used did not contain any harmful content and was consistent with their intended use (research). We have cited the datasets and relevant works used in this study.