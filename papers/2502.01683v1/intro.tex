\section{Introduction}
\label{sec:intro}
% 第一段整体考虑：从供（LLM）需（下游任务）增长角度，阐述定制化benchmark的需求随之增长。在related work章节说明我们工作还对解决哪些问题有帮助（数据泄露、benchmark饱和、训练数据构建）

% 近两年，随着持续在多个维度对语言模型进行scaling up，我们见证了如图一所示的两个趋势：
With the ongoing scaling up of large language models (LLMs) in multiple dimensions over the past few years, two key trends have emerged (Figure~\ref{fig:abs}): 
% （1）不同训练轨迹导向性能具有特异性的模型被release的速度在经历近两年的加速后目前维持在~30k per season.
% （2）语言模型能力的增长催生了更多的应用需求，这可以从当前54m per season的开源语言模型下载量体现。
(1) The LLM release process has accelerated and now exceeds 30k per season; (2) The growth in LLM capabilities has spurred application demand, reflected in over 50M downloads of open-source models per season.
% 作为连接海量供给（LLMs）与需求（Applications）的桥梁，对定制化的benchmark的需求也相应激增，which帮助特定下游任务选择最合适的LLM
Serving as a bridge between massive LLM supply and various application needs, the demand for customized benchmarks is rapidly growing, helping downstream tasks identify the most suitable LLM. 

% 然而，当前的benchmark构建方法大多完全或部分依赖人类提供评测信号，which 周期长、开销大。为解决这一问题，利用LLM当前接近人类的理解和生成能力，LLM-driven的benchmark生成方法开始被探索。但是，这些方法通常依赖对已有的类似benchmark进行数据增强，且缺乏任务泛化性。一个能够接受任意定制化任务约束，并产生高质量样本的customizable benchmark generator目前被迫切需要。
However, current benchmark construction processes largely rely on human-provided signals \citep{evalsurvey,mmlupro}, leading to long cycles and high costs. To this end, efficient LLM-driven methods have recently been explored. 
% they generally rely on augmenting existing benchmarks
Unfortunately, they generally rely on the existence of seed benchmarks for data augmentation \citep{dyval2,unigen,perteval,databench} and task specific designs \citep{dyval,s3eval}, lacking generalization across tasks and domains. 
% Meanwhile, current absence of a comprehensive evaluation framework hinders the assessment of the reliability of benchmark generators.
Meanwhile, the current absence of a comprehensive evaluation framework hinders the assessment and optimization of benchmark generators, weakening our confidence in their reliability for real applications.
% 此外，也缺少一个全面的evaluation框架来validate 这些方法的可信性。
Hence, an automatic and comprehensive evaluation framework and a generic and reliable benchmark generator that can handle any assessment demands and efficiently generate high-quality samples are urgently needed.

\begin{figure}[t]
\centering
\includegraphics[width=0.48\textwidth]{figs/first.pdf}
\caption{The trends of LLMs released and open-source LLMs downloads per season since the debut of ChatGPT. We obtain the data via the Huggingface API. See details in Appendix~\ref{sec:huggingface}.}
\vspace{-11pt}
\label{fig:abs}
\end{figure}
% 为了打造\textsc{CustomGen}，我们通过前期的调研和实验总结了三个核心的挑战：
% 1. 当前对于benchmark generator的优化目标设定缺乏一个全面、可泛化的标准，导致对于generator缺陷可能的遗漏。
% 
% 2. 对于LLM as benchmark generator的优缺点缺乏系统的认知。
% (自身多样性有限；
% 难度感知生成弱；
% 相关性良好；
% 直接生成可信度有限；
% 性价比高)
% 3. 对于一些优化目标缺乏评测任务之间可泛化的通用方法。

% To build \textsc{CustomGen}, we have identified three core challenges based on preliminary analyses:
% (1) Current absence of a comprehensive evaluation framework for benchmark generators may result in overlooking their potential defects;
% (2) The pros and cons of directly prompting LLM as benchmark generator have not been thoroughly examined;
% (3) Some objectives of benchmark generator lack optimizing methods that can be applied across assessment scenarios.



% 为此，我们首先构建了一个包含十个criteria的全面的、自动化的evaluation framework for benchmark generators。特别地，我们通过因果学习技术识别并去除了LLM-as-judge在多个criteria下存在的bias，来确保framework的可信性。
% 我们进一步通过该evaluation framework细致地检查了directly using LLM as \textsc{CustomGen}。结果显示所生成的benchmark存在有限的多样性、较差的难度可控性、较低的题目faithfulness等问题，也有着题目相关度高、高效性的优势。
% 在此基础上，我们尝试应用现有方法、设计新技术来优化所识别的问题，并集成为我们的\textsc{CustomGen}。具体来说，其通过应用general attribute引导和in-batch冗余过滤机制提升了多样性；通过difficulty attribute引导和难度扩散机制提升了难度可控性；通过可回溯式生成技术和基于对比的后验判别方法提升了题目的正确性。
To this end, we first construct an automatic evaluation framework with ten criteria for benchmark generators. Notably, we utilize causal learning \citep{caulearn} techniques to identify and remove biases of LLM-as-a-judge \citep{geval} across various criteria, ensuring the reliability of the framework. 
On this basis, we examine the strengths and weaknesses of directly prompting LLM as generic benchmark generator through this evaluation framework. The results reveal that the generated benchmark exhibits limited lexical and semantic diversity, poor controllability over difficulty, and low sample faithfulness, while showing advantages in high task alignment and knowledge diversity.
Bearing this in mind, we develop a generic benchmark generator \textsc{BenchMaker} by integrating existing techniques with newly designed approaches to address the identified issues.
Specifically, \textsc{BenchMaker}: strengthens sample faithfulness using stepwise self-correction generation and conflict guided contrastive discrimination; extends difficulty boundary with difficulty strategy guidance and difficulty diffusion mechanism; enhances diversity through AttrPrompt \citep{attr} and in-batch redundancy filtering. We also discuss some unsuccessful attempts in Appendix~\ref{sec:unsuccess} to provide more insights for future research.

% 我们用具有不同能力的LLMs作为generator在不同的task （assessment demands）上利用所提出的evaluation framework全面地validate \textsc{CustomGen}。
% 实验结果显示，相比于MMLU-Pro、MATH等高质量人类标注benchmark，\textsc{CustomGen}所生成的benchmark能取得更好的task relevance、更好的难度可控性、更难的题目难度，以及comparable的sample faithfulness、多样性和高度一致的测评结果（e.g., 0.986 spearman correlation across 12 models）,而后者仅花费几个小时、几十美元。
% 我们进一步进行了详尽的实验验证了\textsc{CustomGen}的良好鲁棒性、卓越泛化性和每个部件的有效性。
% 我们已经在https://www.overleaf发布了项目代码


% such as MMLU-Pro \citep{mmlupro} and MATH \citep{math}
We conduct comprehensive experiments to validate \textsc{BenchMaker}  under the proposed evaluation framework.
Compared to high-quality human-annotated benchmarks, the benchmarks generated by \textsc{BenchMaker} exhibit superior task alignment, better difficulty controllability, more challenging question difficulties, and comparable sample faithfulness and diversity.
More importantly, they yield highly consistent evaluation results across 12 LLMs (0.967 Pearson correlation with MMLU-Pro), with \textsc{BenchMaker} taking only \$0.005 and 0.38 minutes per sample.
We further perform detailed experiments to validate the outstanding generalization and robustness across tasks and LLMs, and the effectiveness of each component of \textsc{BenchMaker}.
% 最后我们推导了faithfulness无法得到完全满足的条件下，检验benchmarking结果置信度的公式，进一步提高了\textsc{BenchMaker}的实用性。
Finally, we derive a formula for evaluating the confidence of benchmarking results under conditions where faithfulness cannot be fully satisfied, further enhancing the practicality and reliability of \textsc{BenchMaker}.



