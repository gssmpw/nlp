% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\usepackage{amsmath}
\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib} 
\begin{document}
%
\title{Optimize Cardinality Estimation Model Pretraining by Simplifying the Training Datasets}
%
%\titlerunning{Abbreviated paper title}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Boyang Fang\inst{1}\orcidID{} }
% \and Tiezheng Nie\inst{1}\orcidID{} 
% \and Derong Shen\inst{1}\orcidID{}}
%
% \authorrunning{F. Author et al.}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
% \institute{Northeast University, Shenyang, China \and
% \email{lncs@springer.com}\\
% \url{http://www.springer.com/gp/computer-science/lncs} \and
% }
% %
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
The cardinality estimation is a key aspect of query optimization research, and its performance has significantly improved with the integration of machine learning. To overcome the "cold start" problem or the lack of model transferability in learned cardinality estimators, some pre-training cardinality estimation models have been proposed that use learning across multiple datasets and corresponding workloads. These models typically train on a dataset created by uniformly sampling from many datasets, but this approach may not be optimal. By applying the Group Distributionally Robust Optimization (Group DRO) algorithm to training datasets, we find that some specific training datasets contribute more significantly to model performance than others. Based on this observation, we conduct extensive experiments to delve deeper into pre-training cardinality estimators. Our results show how the performance of these models can be influenced by the datasets, corresponding workloads. Finally, we introduce a simplified training dataset, which has been reduced to a fraction of the size of existing pretraining datasets. Sufficient experimental results demonstrate that the pre-trained cardinality estimator based on this simplified dataset can still achieve comparable performance to existing models in zero-shot setups.

\keywords{AI4DB  \and cardinality estimation \and query optimization.}
\end{abstract}
%
%
%
\section{INTRODUCTION}
How can a database system obtain accurate query results in the fastest manner when faced with a given SQL query? The query optimizer within a database system is specifically designed to solve this problem. The paper\cite{leis2015good} titled "How Good Are Query Optimizers, Really" introduces the JOB dataset and argues that within an optimizer, the cardinality estimator, cost model, and plan enumeration are the core components for generating the optimal query plan, with the cardinality estimator having the greatest impact on performance. The quest for more accurate cardinality estimators has been a hot topic in database research, with numerous papers over the decades introducing methods to improve accuracy. In recent years, given the demonstrated capabilities of machine learning models, researchers have also attempted to address the cardinality estimation problem using machine learning models.

Existing research\cite{hilprecht2019deepdb,yang2019deep,yang2020neurocard,kipf2018learned} on cardinality estimation, after introducing machine learning methods, can be divided into traditional methods and learning-based methods. The latter can be further categorized into query-driven and data-driven approaches based on the learning objectives. Most learning-based methods establish models based on a single database instance or a workload applicable to that instance. Under conditions where the data distribution or workload remains unchanged, learning-based methods represent the current state-of-the-art (SOTA). However, such constraints prevent their practical application in real-world databases, and there is a need for models with stronger generalization capabilities. Consequently, there is a strong demand for pre-trained cardinality estimators, which are expected to be effective under zero-shot conditions regarding the instance or workload and, after fine-tuning, can achieve SOTA performance.

Existing work\cite{chronis2024cardbench, PRICE} on pre-trained cardinality estimators lacks unified conclusions and sufficient testing regarding model performance. Factors such as the pre-training dataset and the type and structure of the initial model can all impact the pre-trained cardinality estimator. We investigate the impact of pre-training datasets on the performance of pre-trained models. Using the DoReMi algorithm, we analyze the weights of 26 different datasets on model performance and find that certain datasets exhibit absolute advantages. In other words, using specific datasets can significantly reduce the data required for training pre-trained models.

We introduce a simplified training set. Through extensive experiments, we demonstrate that the pre-trained cardinality estimators obtained from this training set are effective, generalizable, and stable. Furthermore, the pre-trained cardinality estimators trained using this simplified training set can achieve the same, if not better, performance while reducing overhead.

Lastly, we compare and analyze the impact of different training set compositions on the performance of pre-trained cardinality estimators. Experimental results show that the choice of training set and the structure of the model are equally important for the final model performance. These findings offer new perspectives on the generalization of pre-trained cardinality estimators.

\section{SETUP}
Consider a database instance composed of a set of tables $T=\left\{{T_{1},T_{2},..., T_{n}}\right\}$, where each table $T_{i}$ contains $n$ attributes $A_{i}$
, thus can be represented as $T_{i}=\left\{{A_{1},A_{2},...,A_{m}}\right\}$
. Generally, we restrict the data type of each attribute to either numerical or continuous. Therefore, the domain of the attribute $A_{i}$ can be represented as 
 $Dom(A_{i})=\left\{{c_{1},c_{2},...,A_{0}}\right\}$ or $Dom(A_{i})=[min_{i}, max_{i} ]$.

We denote the workload corresponding to a dataset instance as $X_{i}$. In general learning-based cardinality estimation methods, $X_{i}$ serves as a training set. In the context of pre-trained cardinality estimation models, the training set of the model is composed of workloads corresponding to multiple dataset instances, thus represented as 
$x=\left\{{X_{1},X_{2},...,X_{k}}\right\}$.

In a database instance, the workload $X_{i}$ contains many SQL queries $q_{i}$. In this paper, we primarily focus on select-project-join SQL queries with conjunctive predicates. These queries are typically composed of a subset of tables $T_{q}$ from Table set $T$, join conditions $J$, and filter predicates $F$, represented as:
\begin{equation}
 SELECT \  COUNT(*) \  FROM \  T_{q} \  WHERE \  J \  AND \  F.
\end{equation}
The join conditions $J$ are all primary-key to foreign-key (PK-FK) equijoins of the form $T_{i}.A_{b}=T_{j}.A_{c}$. The filter predicates $F$ consist of three parts: the attribute $T_{i}.A_{b}$, the predicate $pred_{i} \in \{=, >, <, \geq, \leq\}$, and the predicted value $v_{i} \in Dom(A_{j})$. Such an SQL query and its corresponding true cardinality form a tuple $x_{i}$, hence $X_{i}=\{x_{1},x_{2},...,x_{l}\}$.
For ease of description, we refer to the pre-trained model derived from the paper \cite{PRICE} as PRICE.

\section{OVERVIEW}
Existing work\cite{chronis2024cardbench, PRICE} on pre-trained cardinality estimators generally needs to address two key challenges: one is to identify transferable features that represent the structure of the dataset and the set of SQL queries executed on this dataset; the other is the collection of training data. The datasets used in current work are typically composed of various datasets from different domains and their corresponding workloads.

\subsubsection{Transferable features.} Encoding transferable features is a necessary step for applying pre-trained models to real-world problems. Zero-shot models, like query-driven learning models, require executing a certain number of SQL workloads and collecting training data before training. The main difference is that pre-trained models are intended for previously unseen datasets, where these queries are out-of-distribution (OOD). The features used by query-driven methods come solely from the training SQLs, typically including tables, joins, and predicates information, which limits them to a single database instance and its workload. Consequently, their performance significantly degrades when faced with OOD SQLs.For example, the commentcount column in the STATS dataset may be encoded as $(0,0,1,0)$ using one-hot encoding during the encoding phase, but the same encoding $(0,0,1,0)$ might represent the $production_year$ column in the IMDB dataset. These columns have completely different data distributions and types, making the model likely to lose effectiveness; hence, such encoded features lack transferability. Although data-driven learning models do not rely on encoding information about tables, joins, and predicates, they model the fundamental distribution of each dataset, leading to non-transferable models between different datasets.

Pre-trained models require a suitable feature encoding method to represent query information in a unified format across different databases, ensuring sufficient expressiveness for accurate estimation. Current solutions to this problem can be broadly categorized into two approaches: the linear model cascading low-level features method used in PRICE\cite{PRICE}, and the graph-structured data method used in CardBench\cite{chronis2024cardbench}. Both approaches include various statistical information of each attribute to learn the data distribution. However, the multi-stage method of cardinality estimation in PRICE\cite{PRICE} offers more flexibility when dealing with joins involving different numbers of tables compared to CardBench\cite{chronis2024cardbench}.

\subsubsection{Collection of Training Data.}Rich training data is essential for pre-training cardinality estimation models. Existing work is based on dozens of multi-table datasets of different types, including synthetic datasets or real-world datasets. They are all datasets with multi-table joins. The collected training data includes randomly generated select-project-join type queries that meet certain requirements for each dataset, as well as partial information and final true cardinalities and query plans after execution. Some studies have shown that increasing the size of the training set helps improve the performance of pre-trained models in zero-shot scenarios. However, the optimal quantity and type of datasets and corresponding workloads needed remain undetermined.

Current work generally uses workloads in the same or similar proportions for training, and some studies evaluate through the use of holdout sets. However, this approach only demonstrates generalization ability under the current data distribution and does not determine which datasets are truly necessary. Many existing studies have shown that the composition of the pre-training data significantly affects model performance. Given the fact that each training set $X_{i}$ in current pre-trained cardinality estimation methods holds generally equal status, the challenge lies in identifying which datasets and corresponding workloads are necessary, and how to rank the contribution of these datasets' workloads to the overall model performance.

\section{Simplifying the Training Datasets by distributionally robust optimization}
Good training data for models should be Learnable, Worth Learning, and Not Yet Learnt\cite{mindermann2022prioritized}. The more such data a training set $X_{i}$ contains, the greater its contribution to the model. As we know, tuples in the form of SQL queries and corresponding cardinalities from the dataset constitute a training set $X_{i}$, and they are completely independent of each other. Therefore, it can be assumed that before training, each tuple in the training set contributes equally to the model. However, the input vectors of the pre-trained cardinality estimation model are encoded by combining training data and the corresponding statistical information of the dataset. In other words, these training data are actually grouped by the training set. At this point, the contribution $\alpha_{i}$ of each training set to the model $M$ is equivalent to the number of tuples.

During the training process of the pre-trained model $M$, some data in certain training sets may not be Learnable or Worth Learning compared to data in other training sets. Removing these data from the training set will not affect the final model performance but will only reduce the weight $\alpha_{i}$ of the corresponding training set. We expect that a model $M'$ trained on the dataset $X$ with these useless data removed will have performance similar to the pre-trained model $M$, i.e., the loss gap between model $M'$ and model $M$ on all training sets $X_{i}$ should be as small as possible.

This training objective can be addressed using Group Distributionally Robust Optimization (DRO)\cite{sagawa2019distributionally}, which aims to optimize the worst-case loss over potential test distributions. To obtain the Simplified weight $\alpha'_{i}$ for each dataset $X'_{i}$, we draw inspiration from the domain weight update steps in the DoReMi algorithm\cite{xie2024doremi}, but with differences: (1) The proxy model $M_{\theta}$ used for training is of the same scale as the pre-trained model $M$; (2) Our minimax objective optimization becomes:
\begin{equation}
 \min_{\theta} \max_{\alpha} L(\theta, \alpha) := \sum_{i=1}^{k} \alpha_i \cdot \left[ \sum_{x \in X_i} \ell_{\theta}(x) - \ell(x) \right] 
\end{equation}
where the losses $\ell_{\theta}(x)$ and $\ell(x)$ are the mean squared error losses between the predicted cardinalities $card(Q)$ of the proxy model $M_{\theta}$ and the pre-trained model $M$ with the true cardinalities, respectively. Therefore, compared to the original DoReMi algorithm, the optimization objective is at the example level, not the token level.

\subsection{Application.} We choose the pre-trained model provided in PRICE\cite{PRICE} as the proxy model because it supports more join patterns and has a smaller q-error in zero-shot scenarios. For the proxy model used in PRICE, we set hyperparameters according to the original configuration, using all 26 training datasets, with the corresponding workload containing 50,000 training data points. Because the amount of training data is the same for each training set, the weights of each baseline are the same which is calculated as $\alpha_{i} = 1 / 26 \approx 0.038462$. After training, the updated dataset weights $\alpha'_{i}$ obtained through the Group DRO method are shown in Table~\ref{tab1}. Due to hardware constraints, we reduced the batch size to 2000 during training.

\begin{table}
\caption{Weights of the 26 Training Sets in PRICE\cite{PRICE}. Baseline training set weights are calculated based on the number of SQL queries in the corresponding workload. Simplified represents the optimized training set weights.}\label{tab1}
\centering 
\begin{minipage}{0.495\textwidth} 
\centering
\begin{tabular}{|l|l|l|}
\hline
Training Sets &  Baseline & Simplified \\
\hline
accidents & 0.038462 & 1.1361e-04 \\
airline & 0.038462 & 1.2603e-04 \\
baseball & 0.038462 & 9.8129e-01 \\
basketball & 0.038462 & 3.1450e-03 \\
carcinogenesis & 0.038462 & 8.1916e-04 \\
ccs & 0.038462 & 3.1671e-05 \\
chembl & 0.038462 & 3.9577e-04 \\
consumer & 0.038462 & 1.2322e-05 \\
credit & 0.038462 & 4.4172e-05 \\
employee & 0.038462 & 3.0345e-04 \\
financial & 0.038462 & 4.8792e-05 \\
fnhk & 0.038462 & 9.0708e-05 \\
grants & 0.038462 & 1.0821e-04 \\
\hline
\end{tabular}
\end{minipage} 
\hfill 
\begin{minipage}{0.495\textwidth} 
\centering
\begin{tabular}{|l|l|l|}
\hline
Training Sets &  Baseline & Simplified \\
\hline
hepatitis & 0.038462 & 6.8115e-05 \\
hockey & 0.038462 & 2.8076e-03 \\
legalacts & 0.038462 & 6.9069e-05 \\
movielens & 0.038462 & 3.0999e-04 \\
sakila & 0.038462 & 2.1328e-05 \\
sap & 0.038462 & 4.7435e-05 \\
seznam & 0.038462 & 1.1786e-04 \\
ssb & 0.038462 & 1.4508e-05 \\
talkingdata & 0.038462 & 8.6677e-04 \\
telstra & 0.038462 & 2.0335e-05 \\
tournament & 0.038462 & 8.9861e-03 \\
tpc\_h & 0.038462 & 2.3872e-05 \\
tubepricing & 0.038462 & 1.2185e-04 \\
\hline
\end{tabular}
\end{minipage}
\end{table}

We sampled training data from the baseline training sets based on the proportions of the simplified training set weights to form a new training set. To ensure fairness and avoid introducing additional influencing factors, we randomly sampled the most training data, 49,065 samples, from the workload of the training set with the largest weight 
$\alpha'_{i}$, which is the baseball workload. For training sets with very small weights $\alpha'_{i}$, such as ssb and tpch, since $\alpha'_{i}*5*10^4<1$, we randomly sampled only one instance. Thus, the simplified training set (Simplified) comprised a total of 50,015 training data points. Using this as the basis, we applied the method described in PRICE\cite{PRICE} to obtain the pre-trained cardinality estimator, referred to as Simple PRICE.

\section{Experiment}
\subsection{Experiment Setup}
\subsubsection{Dataset and Workload.}To ensure fairness, the test set uses the workloads of four unseen real-world datasets: IMDB\cite{IMDB}, STATS\cite{STATS}, ErgastF1\cite{ErgastF1}, and Visual Genome\cite{Genome}, as in PRICE\cite{PRICE}. The IMDB dataset is a classic benchmark widely used in the database field. The STATS dataset is an anonymized dump of user-contributed content on the STATS Stack Exchange network. ErgastF1 is a dataset about Formula 1 race information. Visual Genome is a dataset, a knowledge base, and an ongoing effort to connect structured image concepts to language. The workloads corresponding to the first two datasets are JOB-light and STATS-CEB, which contain 70 queries and 146 queries for testing, respectively. The workloads corresponding to the latter two datasets are provided in PRICE\cite{PRICE} for testing, containing 148 and 186 queries, respectively.

\subsubsection{Evaluation Metrics.} We chose q-error and p-error as the primary metrics for evaluating model performance.

\paragraph{q-error}:The contribution should contain no more than four levels of
headings. Cardinality estimation problems often use q-error as an accuracy metric, which is calculated according to the formula:
\begin{equation}
 \text{q-error}=\max\left(\frac{\widehat{\text{Card}(Q)}}{\text{Card}(Q)}, \frac{\text{Card}(Q)}{\widehat{\text{Card}(Q)}}\right)
\end{equation}
where $\widehat{\text{Card}(Q)}$ represents the estimated cardinality and \text{Card}(Q) represents the true cardinality.Therefore, the range of a q-error is between $\left[1,\infty \right)$.However, this metric does not differentiate whether the relationship between the estimated cardinality and the true cardinality is an overestimate or an underestimate. When generating an execution plan, the impact of an overestimate or underestimate of the cardinality on the final end-to-end time is agnostic.

\paragraph{p-error}: Although the q-error metric is often used to measure the accuracy of estimated cardinality, the q-error metric cannot serve as a good indicator for query execution performance. This is because both overestimates and underestimates of cardinality can yield the same q-error but result in completely different query plans, leading to significantly different query times. While the best way to evaluate the quality of cardinality estimation is by directly comparing the actual execution times under the same benchmark, this entails a substantial time cost. Therefore, the p-error metric was proposed in \cite{han2021cardinality}.

To generate an execution plan for a query $\text{q}$, it is first necessary to estimate the corresponding subqueries. The true cardinality set and the estimated cardinality set of the subqueries are denoted as $C_{T}$ and $C_{E}$, respectively. In calculating p-error, PostgreSQL\cite{PostgreSQL} is used to output the corresponding query plans $P(C_{T})$ and $P(C_{E})$ based on $C_{T}$ and $C_{E}$. The cost model of the PostgreSQL query optimizer takes the query plan $\text{P}$ and the required cardinality set $\text{C}$as inputs, and its output estimated execution cost is denoted as $\text{PPC}(P(C), C)$. Thus, p-error is defined as:
\begin{equation}
 \text{p-error} = \frac{PPC(P(C_E), C_T)}{PPC(P(C_T), C_T)}
\end{equation}
This metric can measure the differences at the query plan level between the estimated cardinality and the true cardinality and can serve as a proxy for the end-to-end time metric to a certain extent.

Since the inference time and size of the model are related to the model's parameter size, Simple PRICE and PRICE only differ in the number of training sets. Therefore, these two evaluation metrics are essentially the same and are thus omitted.

\paragraph{Experimental Environment}:Our Linux server is equipped with an Intel Xeon Gold 5218R CPU, 128GB of memory, and an NVIDIA A5000 GPU.

\subsection{Performance Difference Between Simple PRICE and PRICE}
We compared the performance of Simple PRICE and PRICE in zero-shot scenarios on these four test sets. We do not list the performance of other learning cardinality estimates here because the main purpose of our experiment is to explore the performance difference between Simple PRICE and PRICE, independent of other models. As shown in Table~\ref{tab2}, we have the following observations:

1). Compared to PRICE, Simple PRICE shows a noticeable drop in q-error on the STATS and ErgastF1 test sets only at the 95th and 99th percentiles. However, even at the 99th percentile, it only increases by 2.256 times and 1.104 times, respectively, which is still lower than other machine learning-based cardinality estimation methods. Interestingly, on the IMDB and Visual Genome test sets, we observe an improvement in the q-error metric, with only 76.5\% and 74.7\% of the original values at the 99th percentile, respectively. For the p-error metric, a decline is seen only at the 99th percentile on the STATS dataset, but this merely indicates a significant difference in the query plans of certain subqueries and does not necessarily imply a large discrepancy in the final actual end-to-end (E2E) time.

This suggests that, compared to the complete training set, the simplified training set largely maintains the performance of the pre-trained cardinality estimation model without any substantial loss.

2). The simplified training set significantly reduces the training time of the PRICE model to only 3.44\% of that required by the complete training set. For query-driven methods, the training time is proportional to the size of the training set. Compared to PRICE, Simple PRICE requires very few training resources, thereby lowering the difficulty of training. The cold start problem is a major weakness of query-driven cardinality estimation models; obtaining the training set requires significant overhead to execute thousands of SQL queries to obtain the corresponding true cardinalities. The issue is exacerbated when pre-trained cardinality estimators need to be trained with workloads from multiple datasets. However, the simplified training set alleviates this phenomenon. Since the pre-trained model only requires a single model to be used for all zero-shot scenarios, the training time for Simple PRICE and PRICE remains unchanged across the four test sets.

\begin{table}[ht]
\caption{the performance of Simple PRICE and PRICE zero shot pretraining model.}\label{tab2}
\centering 
\resizebox{\textwidth}{!}{
\begin{tabular}{|c|c|ccccc|ccccc|c|} 
\hline 
\textbf{DATASETS} & \textbf{METHOD} & \multicolumn{5}{c|}{\textbf{Q-ERROR}} & \multicolumn{5}{c|}{\textbf{P-ERROR}} & \textbf{PRETRAINING TIME(MIN)} \\ 
& & \textbf{50\%} & \textbf{80\%} & \textbf{90\%} & \textbf{95\%} & \textbf{99\%} & \textbf{50\%} & \textbf{80\%} & \textbf{90\%} & \textbf{95\%} & \textbf{99\%} \\
\hline
IMDB & Simple PRICE & 2.0029 & 4.1047 & 7.6879 & 19.0663 & 54.2403 & 1.0 & 1.0537 & 1.0939 & 1.2081 & 1.4636 & 43 \\
& PRICE (Pretrained) & 1.7771 & 4.0716 & 8.3952 & 15.4516 & 70.889 & 1.0 & 1.0544 & 1.1649 &  1.2947 & 1.6517 & 1249 \\
STATS & Simple PRICE & 2.494 & 9.3853 & 24.0765 & 62.5588 & 1307.7956 & 1.1762 & 2.7326 & 6.8562 & 25.4018 & 1288.4794 & 43 \\
& PRICE (Pretrained) & 1.8709 & 5.4934 & 12.4606 & 35.5513 & 579.6716 & 1.0 & 1.3615 &  1.7248 & 2.557 & 7.8393 & 1249 \\
ErgastF1 & Simple PRICE & 2.0029 & 4.1047 & 7.6879 & 19.0663 & 54.2403 & 1.0 & 1.0537 & 1.0939 & 1.2081 & 1.4636 & 43 \\
& PRICE (Pretrained) & 1.7771 & 4.0716 & 8.3952 & 15.4516 & 70.889 & 1.0 & 1.0544 & 1.1649 &  1.2947 & 1.6517 & 1249 \\
Genome & Simple PRICE & 1.8303 & 4.6351 & 11.2894 & 15.8277 & 85.4872 & 1.0 & 1.0907 & 1.29 & 1.8453 & 2.6369 & 43 \\
& PRICE (Pretrained) & 1.6545 & 3.5951 & 5.1724 & 15.6697 & 114.4227 & 1.0 & 1.0 & 1.0091 & 1.4184 & 2.6369 & 1249 \\
\hline
\end{tabular}}
\end{table}

In summary, the Group DRO method can partially address the question we raised earlier: how to rank the training sets based on their contribution to the model. The more significant the contribution, the more necessary the training set is, and thus, the higher its proportion in the Simplified training set.

\subsection{Ablations and Analysis}
In the previous chapters, we found that the model obtained using the Simplified training set performed similarly to the original PRICE model, and the Simplified training set was randomly sampled from the original datasets in certain proportions. In this chapter, we delve into the composition of the Simplified training set and further analyze the impact of these training sets on the performance of the pre-trained cardinality estimation model.

\paragraph{Baseball as the Base of These Training Sets}.Revisiting the proportions of different training sets in the Simplified training set, we can see that the weight of the baseball dataset $\alpha_{baseball}$ is as high as 98.1\%. What happens if we train the PRICE model using only the baseball training set? Conversely, what would be the performance if we train the PRICE model without including any data from the baseball training set, using the remaining 25 training sets instead?

We refer to the PRICE model trained using only the baseball dataset as baseball PRICE, the model trained without any data from the baseball dataset but using all remaining 25 training sets as anti-baseball PRICE, and the model trained using only the hockey dataset as hockey PRICE. The test sets remain the workloads corresponding to IMDB, STATS, ErgastF1, and Visual Genome.

Fig.~\ref{fig1} shows the q-error of the original PRICE, baseball PRICE, and anti-baseball PRICE on the four test sets. Based on the observations from Figure 1, we draw the following conclusions:

1). Baseball PRICE has performance close to the original PRICE, while anti-baseball PRICE performs significantly worse than baseball PRICE. This indicates that during the training process of PRICE, the training set based on the baseball dataset contributes the most, and its role cannot be replaced by other training sets.

2). The schema complexity of the hockey dataset is relatively similar to that of the baseball dataset among the remaining 25 training sets, with even more join relations. However, the performance of hockey PRICE on the four unseen test sets is not as good as that of baseball PRICE. This experiment result suggests that although the pre-trained performance of PRICE is strong, it still requires sufficiently high-quality training sets to support it. The factors that determine whether a training set is good enough need further investigation. It is likely not determined by obvious features such as the number of tables or join relations. Discovering these features is our future research objective.

\begin{figure}
\includegraphics[width=\textwidth]{q-errors_of_PRICE,baseball_PRICE,_anti-baseball_PRICE,and_hockey_PRICE.png}
\caption{q-errors of PRICE, baseball PRICE, anti-baseball PRICE, and hockey PRICE.} \label{fig1}
\end{figure}

\paragraph{More experiments significantly bolster the credibility of our findings}.In the 30 datasets provided by PRICE\cite{PRICE} (26 datasets for training and 4 datasets for testing), since both Simple PRICE and PRICE are trained on all 26 training sets, to prevent data contamination, only the remaining 4 unseen datasets can be used as test sets. However, the baseball PRICE, obtained using only the baseball training set, does not have such concerns and can use all datasets except the baseball dataset as test sets. Here, we provide a generalization test for baseball PRICE. For non-pre-trained learning-based cardinality estimators, testing on different workloads means training multiple new models, a training cost we cannot afford. Therefore, in Fig~\ref{fig2}, we compare only the q-error metrics of baseball PRICE and PostgreSQL.

As shown in Fig~\ref{fig2}, the accuracy of estimates based on baseball PRICE is significantly higher than that achieved using PostgreSQL directly. This improvement only requires training a single baseball PRICE model, which takes less than one hour on our hardware. The 25 test sets exhibit substantial differences, and such performance demonstrates the stability and generalization capabilities of baseball PRICE.

\begin{figure}
\includegraphics[width=\textwidth]{q-errors_of_baseball_PRICE_and_PostgreSQL.png}
\caption{the performance of Simple PRICE and PRICE zero shot pretraining model.} \label{fig2}
\end{figure}

\section{RELATED WORK}
\paragraph{Learning-Based Cardinality Estimators.}Learning-based cardinality estimators are typically divided into query-driven and data-driven models based on the form of inputs required during training. Query-driven methods\cite{kipf2018learned,dutt2019selectivity,zhao2022lightweight} aim to establish a regression model between the data range and the true cardinality, indirectly fitting the data distribution under the schema. Data-driven methods\cite{hilprecht2019deepdb,wang2021face,wu2012bayescard,wu2023factorjoin,yang2019deep,yang2020neurocard,} directly model the data distribution under the schema by combining the principles of probabilistic graphical models in machine learning. The learning objectives of these two types of methods are not contradictory; hence, some research combines these two approaches into hybrid-driven methods. Some hybrid methods \cite{dutt2019selectivity,wu2021unified} use queries and true cardinalities as penalty items for the learning objective to enhance the performance of data-driven methods and introduce several strengths of query-driven methods.

\paragraph{Distributionally Robust Optimization (DRO).}DRO methods are typically used in the deep learning field\cite{oren2019distributionally,sagawa2019distributionally,xie2024doremi} to enhance the generalization ability of models to obtain robust models by defining the uncertainty set. Group DRO\cite{sagawa2019distributionally}, emerging on this basis, reduces training difficulty at the cost of fewer degrees of freedom. Our method follows DoReMi\cite{xie2024doremi}, using a small-scale proxy model to optimize the data for training large language models more efficiently. We use group DRO to calculate the contribution of each training set to the model's performance.

\section{CONCLUSION}
In this paper, after analyzing the weights of 26 different datasets on the performance of pre-trained cardinality estimation models using the DoReMi algorithm, we proposed a simplified training set that significantly optimizes training overhead while essentially ensuring performance. Furthermore, our experiments demonstrate that certain special training sets are necessary to maintain model performance, further clarifying the role of training sets in pre-trained cardinality estimators.

However, we are currently unable to identify which data characteristics determine the uniqueness of these training sets. We hope to design such synthetic datasets in future work to advance the development of pre-trained cardinality estimators.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
% \bibliographystyle{splncs04}
% \bibliography{references}
%


% \begin{thebibliography}{8}

% \bibitem{ErgastF1}
% ErgastF1 Dataset.https://relational-data.org/dataset/ErgastF1

% \bibitem{IMDB}
% IMDB Dataset.http://homepages.cwi.nl/~boncz/job/imdb.tgz

% \bibitem{STATS}
% STATS Dataset.https://relational.fit.cvut.cz/dataset/Stats

% \bibitem{Genome}
% Visual Genome Dataset.https://relational-data.org/dataset/VisualGenome


% \bibitem{ref_url1}
% LNCS Homepage, \url{http://www.springer.com/lncs}, last accessed 2023/10/25
% \end{thebibliography}

\printbibliography

\end{document}
