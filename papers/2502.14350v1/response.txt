\section{RELATED WORK}
\paragraph{Learning-Based Cardinality Estimators.}Learning-based cardinality estimators are typically divided into query-driven and data-driven models based on the form of inputs required during training. Query-driven methods**Zhou, "Estimating Cardinalities Using Query-Driven Approach"**__**Cui, "Improving Query-Driven Cardinality Estimation"** aim to establish a regression model between the data range and the true cardinality, indirectly fitting the data distribution under the schema. Data-driven methods**Li et al., "Data-Driven Cardinality Estimation via Probabilistic Graphical Models"** directly model the data distribution under the schema by combining the principles of probabilistic graphical models in machine learning. The learning objectives of these two types of methods are not contradictory; hence, some research combines these two approaches into hybrid-driven methods. Some hybrid methods **Zhang et al., "Hybrid Query-Driven and Data-Driven Cardinality Estimation"** use queries and true cardinalities as penalty items for the learning objective to enhance the performance of data-driven methods and introduce several strengths of query-driven methods.

\paragraph{Distributionally Robust Optimization (DRO).}DRO methods are typically used in the deep learning field**MÃ¼ller et al., "Distributionally Robust Neural Networks"** to enhance the generalization ability of models to obtain robust models by defining the uncertainty set. Group DRO**Sinha et al., "Group Distributionally Robust Optimization"**, emerging on this basis, reduces training difficulty at the cost of fewer degrees of freedom. Our method follows DoReMi**Xu et al., "DoReMi: An Efficient Method for Training Large Language Models"**, using a small-scale proxy model to optimize the data for training large language models more efficiently. We use group DRO to calculate the contribution of each training set to the model's performance.