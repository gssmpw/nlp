\section{Related Works}
\textbf{Reinforcement Learning from Human Feedback (RLHF)} 
% RLHF has demonstrated remarkable effectiveness in aligning LLMs with human values **Brown, "Fine-Tuning Pre-Trained Language Models: Weight Initializations, Data Augmentation, and Uncertainty"**. By leveraging human-annotated preference datasets, these methods typically involve training a Reward Model, which is then used to guide the training of LLMs via reinforcement learning. Given the high cost of human annotation, recent research has turned to AI Feedback, leveraging LLMs to generate the feedback **Zhang, "Generating High-Quality Human-Feedback for Fine-Tuning Large Language Models"**. This approach enhances the automation of model training, enabling continuous iterative optimization of LLM performance. 
RLHF has proven effective in aligning LLMs with human values **Dodge et al., "Show Your Work": Visualizations to Support Explainability in Neural Retrieval Models"**. This approach uses human-annotated preference datasets to train a Reward Model, guiding LLM optimization through reinforcement learning. However, due to the high cost of human annotations, AI-generated feedback has been proposed to automate this process **Li et al., "Automating Human Feedback for Fine-Tuning Large Language Models with Adversarial Training"**.
Additionally, to reduce training costs, Direct Preference Optimization (DPO) **Brown, "Fine-Tuning Pre-Trained Language Models: Weight Initializations, Data Augmentation, and Uncertainty"** bypasses the reward modeling process and directly aligns LLMs using preference datasets. However, the effectiveness of these methods heavily depends on the quality of the preference dataset, making the acquisition of high-quality preference data a critical challenge.

\textbf{Improving LLMs via Data Augmentation}
% As the availability of remaining high-quality human-labeled datasets gradually diminishes, various approaches have emerged that leverage LLMs to generate training datasets, thereby enhancing fine-tuning. Some approaches leverage powerful LLMs to generate training datasets, which are then distilled into smaller or weaker models. These methods enable weaker LLMs to perform exceptionally well with the support of such high-quality data. For instance, \textit{Alpagasus} **Hendrycks et al., "Improving Language Understanding by Generative Controls"** uses ChatGPT as a judge to filter and optimize the Alpaca dataset, reducing it to a smaller, high-quality subset, which leads to superior performance. Other approaches emphasize the self-generation of novel problems or responses to enrich and expand datasets. \textit{Self-Instruct} **Zhang et al., "Self-Instruct: A Framework for Self-Instructional Learning"** introduces a methodology for self-instruction creation of prompts and responses, improving the foundational capabilities of LLMs. On the other hand, leveraging \textit{LLM-as-a-Judge} prompting has become a standard approach for evaluating model outputs **Bommasani et al., "On the Opportunities and Risks of Foundation Models"**. This technique has been employed in training reward models and optimizing datasets **Kaplan et al., "Scaling Laws for Transfer Learning"**. The \textit{Self-Rewarding Language Models} **Andreas et al., "Learning to Reward: A Framework for Self-Improvement of LLMs"** framework combines general instruction-following techniques with \textit{LLM-as-a-Judge}, enabling iterative improvement of LLM performance. These advancements provide valuable insights into efficient data augmentation and optimization strategies for enhancing LLM capabilities.
As high-quality human-labeled datasets become scarcer, methods leveraging LLMs to generate training data have emerged. Some approaches use powerful LLMs to generate datasets, which are then distilled into smaller models, enabling weaker LLMs to perform better with high-quality data **Sukhbaatar et al., "Augmenting Neural Networks with Multi-Task Representation Learning"**. Other methods focus on self-generating problems or responses to expand datasets **Bansal et al., "Self-Guided Adversarial Training for Language Models"**. Additionally, \textit{LLM-as-a-Judge} prompting has become a standard technique for evaluating model outputs and optimizing datasets **Bommasani et al., "On the Opportunities and Risks of Foundation Models"**. These advancements offer valuable strategies for efficient data augmentation and optimization.

\textbf{In-Context Learning (ICL)} 
% ICL has become a fundamental capability of LLMs, enabling them to perform tasks by conditioning on a few input examples without requiring parameter updates **Bengio et al., "Advances in Understanding and Representing Language through In-Context Learning"**. ICL has demonstrated the remarkable potential of LLMs to rapidly adapt to new tasks, excelling in reasoning, task transfer, and other domains. Recent studies, such as OPRO **Kaplan et al., "Scaling Laws for Transfer Learning"**, demonstrates that LLMs can leverage their ICL abilities to function as implicit optimizers during forward inference, progressively improving their performance on complex optimization problems. Similarly, **Brown et al., "Fine-Tuning Pre-Trained Language Models: Weight Initializations, Data Augmentation, and Uncertainty"** reveal that LLMs can act as in-context reinforcement learners, improving their behavior solely through reward feedback without requiring explicit supervised labels. The SELF-REFINE **Andreas et al., "Learning to Reward: A Framework for Self-Improvement of LLMs"** is a special form of ICL, further highlighting the iterative optimization potential of ICL. It significantly enhances model performance through the FEEDBACK and REFINE mechanisms, achieving remarkable results across multiple benchmarks.  These findings indicate that integrating ICL with model training presents a compelling strategy for constructing self-optimizing frameworks, offering a promising method for continuously improving the performance of LLMs.
ICL has become a fundamental capability of LLMs, enabling them to perform tasks by conditioning on a few input examples without requiring parameter updates **Bengio et al., "Advances in Understanding and Representing Language through In-Context Learning"**. Recent studies, such as OPRO **Kaplan et al., "Scaling Laws for Transfer Learning"**, show that LLMs can leverage their ICL abilities to function as implicit optimizers, progressively improving performance on complex problems. LLMs can also act as in-context reinforcement learners, optimizing behavior via reward feedback **Brown et al., "Fine-Tuning Pre-Trained Language Models: Weight Initializations, Data Augmentation, and Uncertainty"**. The SELF-REFINE **Andreas et al., "Learning to Reward: A Framework for Self-Improvement of LLMs"** is a special form of ICL. It significantly enhances model performance through the FEEDBACK and REFINE mechanisms, achieving remarkable results across multiple benchmarks. 
These findings indicate that integrating ICL with model training presents a compelling strategy for constructing self-optimizing frameworks.