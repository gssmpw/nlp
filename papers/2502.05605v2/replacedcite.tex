\section{Related Works}
\textbf{Reinforcement Learning from Human Feedback (RLHF)} 
% RLHF has demonstrated remarkable effectiveness in aligning LLMs with human values ____. By leveraging human-annotated preference datasets, these methods typically involve training a Reward Model, which is then used to guide the training of LLMs via reinforcement learning. Given the high cost of human annotation, recent research has turned to AI Feedback, leveraging LLMs to generate the feedback ____. This approach enhances the automation of model training, enabling continuous iterative optimization of LLM performance. 
RLHF has proven effective in aligning LLMs with human values ____. This approach uses human-annotated preference datasets to train a Reward Model, guiding LLM optimization through reinforcement learning. However, due to the high cost of human annotations, AI-generated feedback has been proposed to automate this process ____.
Additionally, to reduce training costs, Direct Preference Optimization (DPO) ____ bypasses the reward modeling process and directly aligns LLMs using preference datasets. However, the effectiveness of these methods heavily depends on the quality of the preference dataset, making the acquisition of high-quality preference data a critical challenge.

\textbf{Improving LLMs via Data Augmentation}
% As the availability of remaining high-quality human-labeled datasets gradually diminishes, various approaches have emerged that leverage LLMs to generate training datasets, thereby enhancing fine-tuning. Some approaches leverage powerful LLMs to generate training datasets, which are then distilled into smaller or weaker models. These methods enable weaker LLMs to perform exceptionally well with the support of such high-quality data. For instance, \textit{Alpagasus} ____ uses ChatGPT as a judge to filter and optimize the Alpaca dataset, reducing it to a smaller, high-quality subset, which leads to superior performance. Other approaches emphasize the self-generation of novel problems or responses to enrich and expand datasets. \textit{Self-Instruct} ____ introduces a methodology for self-instruction creation of prompts and responses, improving the foundational capabilities of LLMs. On the other hand, leveraging \textit{LLM-as-a-Judge} prompting has become a standard approach for evaluating model outputs ____. This technique has been employed in training reward models and optimizing datasets ____. The \textit{Self-Rewarding Language Models} ____ framework combines general instruction-following techniques with \textit{LLM-as-a-Judge}, enabling iterative improvement of LLM performance. These advancements provide valuable insights into efficient data augmentation and optimization strategies for enhancing LLM capabilities.
As high-quality human-labeled datasets become scarcer, methods leveraging LLMs to generate training data have emerged. Some approaches use powerful LLMs to generate datasets, which are then distilled into smaller models, enabling weaker LLMs to perform better with high-quality data ____. Other methods focus on self-generating problems or responses to expand datasets ____. Additionally, \textit{LLM-as-a-Judge} prompting has become a standard technique for evaluating model outputs and optimizing datasets ____. These advancements offer valuable strategies for efficient data augmentation and optimization.

\textbf{In-Context Learning (ICL)} 
% ICL has become a fundamental capability of LLMs, enabling them to perform tasks by conditioning on a few input examples without requiring parameter updates ____. ICL has demonstrated the remarkable potential of LLMs to rapidly adapt to new tasks, excelling in reasoning, task transfer, and other domains. Recent studies, such as OPRO ____, demonstrates that LLMs can leverage their ICL abilities to function as implicit optimizers during forward inference, progressively improving their performance on complex optimization problems. Similarly, ____ reveal that LLMs can act as in-context reinforcement learners, improving their behavior solely through reward feedback without requiring explicit supervised labels. The SELF-REFINE ____ is a special form of ICL, further highlighting the iterative optimization potential of ICL. It significantly enhances model performance through the FEEDBACK and REFINE mechanisms, achieving remarkable results across multiple benchmarks.  These findings indicate that integrating ICL with model training presents a compelling strategy for constructing self-optimizing frameworks, offering a promising method for continuously improving the performance of LLMs.
ICL has become a fundamental capability of LLMs, enabling them to perform tasks by conditioning on a few input examples without requiring parameter updates ____. Recent studies, such as OPRO ____, show that LLMs can leverage their ICL abilities to function as implicit optimizers, progressively improving performance on complex problems. LLMs can also act as in-context reinforcement learners, optimizing behavior via reward feedback ____. The SELF-REFINE ____ is a special form of ICL. It significantly enhances model performance through the FEEDBACK and REFINE mechanisms, achieving remarkable results across multiple benchmarks. 
These findings indicate that integrating ICL with model training presents a compelling strategy for constructing self-optimizing frameworks.