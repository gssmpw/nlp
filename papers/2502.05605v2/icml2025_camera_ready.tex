%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{xurl}
\usepackage{wrapfig}
\usepackage{longtable}
\usepackage{listings}
% \usepackage{minted}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}  % 可选，如果想使用自定义颜色

\lstset{
  basicstyle=\ttfamily,       % 设置字体为等宽字体（类似 Verbatim）
  keywordstyle=\color{blue},  % 设置关键字颜色（可选）
  stringstyle=\color{red},    % 设置字符串的颜色（可选）
  showstringspaces=false,     % 不显示字符串中的空格
  breaklines=true,            % 启用自动换行
  frame=single,               % 添加框架（可以去掉，保持更简洁）
  breakatwhitespace=true,     % 仅在空格处换行
  postbreak=\mbox{$\hookrightarrow$}, % 设置行尾符号（可选）
  numbers=none,               % 不显示行号（可以启用）
  backgroundcolor=\color{white}, % 背景色（可以更改）
  xleftmargin=0pt,            % 左边距
  xrightmargin=0pt,           % 右边距
  aboveskip=5pt,              % 上边距
  belowskip=5pt,              % 下边距
}
\usepackage[absolute,overlay]{textpos}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{caption}   % For customizing captions
\usepackage{adjustbox}
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\newtcolorbox{myminted}[2][]{colframe=black!60!black, colback=black!5!white, coltitle=white, title=#2,#1, fonttitle=\large, fontupper=\small}

% \renewcommand{\lstlistingname}{Something}
% \renewcommand{\listingscaption}{Some}
% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{pythonhighlight}
\usepackage{bm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{amsthm}

\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{multirow}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{thmtools,thm-restate}
\usepackage{booktabs} % for professional tables

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

\makeatletter
\renewcommand{\maketag@@@}[1]{\hbox{\m@th\normalsize\normalfont#1}}%
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\addornot}[1]{\textcolor{teal}{[Add or not: #1]}}
\newcommand{\analyse}[1]{\textcolor{red}{[Analyse: #1]}}

\newcommand{\xxx}[1]{{\bf \color{blue} [[xxx: #1]]}}

% \newcommand{\methodname}{ARIES: Stimulating Self-Refinement of Large Language Models with and for Iterative Preference Optimization}
\newcommand{\methodname}{ARIES: Stimulating Self-Refinement of Large Language Models by Iterative Preference Optimization}
\newcommand{\methodabb}{ARIES}

% ARIES: Stimulating Self-Refinement in Large Language Models Yield a Superior Iterative Preference Optimizer

% PRAISE: Preference-based Refinement Accelerates Iterative Self-Enhancement

% Preference-based self-Refinement framework for Accelerating Iterative Substantial Enhancement(PRAISE)

% Enhancing Self-Refinement in Large Language Models for Iterative Preference Training

% Self-refined language models are iterative preference optimizers

% Enhancing Self-Refinement in Large Language Models for Iterative Preference Training


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\methodname}

\begin{document}

\twocolumn[
\icmltitle{\methodname}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yongcheng Zeng}{CASIA,CAS}
\icmlauthor{Xinyu Cui}{CASIA,CAS}
\icmlauthor{Xuanfa Jin}{CASIA,CAS}
\icmlauthor{Guoqing Liu}{ustc}
\icmlauthor{Zexu Sun}{ruc}
\icmlauthor{Quan He}{huawei}
\icmlauthor{Dong Li}{huawei}
\icmlauthor{Ning Yang}{CASIA}
\icmlauthor{ Jianye Hao}{huawei}
\icmlauthor{Haifeng Zhang}{CASIA}
\icmlauthor{Jun Wang}{UCL}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{CASIA}{Institute of Automation, Chinese Academy of Sciences}
\icmlaffiliation{CAS}{School of Artificial Intelligence, University of Chinese Academy of Sciences}
\icmlaffiliation{ustc}{University of Science and Technology of China}
\icmlaffiliation{ruc}{Gaoling School of Artificial Intelligence, Renmin University of China}
\icmlaffiliation{huawei}{Huawei Noah's Ark Lab, China}
\icmlaffiliation{UCL}{University College London}

\icmlcorrespondingauthor{Jun Wang}{jun.wang@cs.ucl.ac.uk}

\icmlcorrespondingauthor{Haifeng Zhang}{haifeng.zhang@ia.ac.cn}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% A truly intelligent large language model should possess the ability to evaluate and correct errors in its responses through external interactions. However, even the most advanced models struggle to refine their own responses to achieve a satisfied response.
A truly intelligent Large Language Model (LLM) should be capable of correcting errors in its responses through external interactions. However, even the most advanced models often face challenges in improving their outputs.
In this paper, we explore how to 
% equip large language models with the ability of self-refinement through
cultivate LLMs with the self-refinement capability through iterative preference training, and 
how this ability can be leveraged to improve model performance during inference.
% how to leverage this capability to further improve the model's performance. 
To this end, we introduce a novel post-training
% training 
and inference framework, called \textbf{ARIES}: Adaptive Refinement and Iterative Enhancement Structure. This method iteratively performs preference training and self-refinement-based data collection. 
% During training, we design an innovative algorithm that activates the self-refinement ability of language models. 
% During training, ARIES integrates an innovative algorithm to 
During training, ARIES strengthen the model's direct question-answering capability while simultaneously unlocking its self-refinement potential.
During inference, ARIES harnesses this self-refinement capability to
% for multiple sequential inference, 
% collecting
generate a series of progressively refined responses, which are then filtered using 
% the Reward Model Scoring mechanism or a simple yet \textcolor{blue}{effective} Rule-Based Selection mechanism, 
either the Reward Model Scoring or a simple yet effective Rule-Based Selection mechanism, specifically tailored to our approach, to construct a dataset for the next round of preference training. 
% This process creates a synergistic loop that enhances both training and inference performance. 
Experimental results demonstrate the remarkable performance of ARIES. 
When applied to the Llama-3.1-8B model and under the self-refinement setting, ARIES surpasses powerful models such as GPT-4o, achieving 62.3\% length-controlled (LC) and a 63.3\% raw win rates on AlpacaEval 2, outperforming Iterative DPO by 27.8\% and 35.5\% respectively, as well as a 50.3\% win rate on Arena-Hard, surpassing Iterative DPO by 26.6\%.
Furthermore, ARIES consistently enhances performance on mathematical reasoning tasks like GSM8K and MATH.
\end{abstract}

\section{Introduction}
Reinforcement Learning from Human Feedback (RLHF) has been demonstrated as an effective pathway to enhance Large Language Models' performance across downstream tasks \citep{ouyang2022training,bai2022training}.
The classic RLHF approach utilizes the PPO \citep{schulman2017proximal} algorithm to train Large Language Models (LLMs) for alignment, but its major drawback is the substantial resource overhead. As an alternative, a more efficient and simple method directly optimizes the LLM itself \citep{rafailov2023direct,azar2024general,ethayarajh2024kto,zeng2024token,meng2024simpo,hong2024orpo}. 
However, both approaches critically depend on access to high-quality, human-annotated preference datasets. As these datasets become increasingly scarce, the challenge of identifying novel strategies to further enhance LLM capabilities becomes increasingly urgent.

% Although RLHF has been demonstrated to significantly improve the alignment of LLMs with human preferences, the efficient collection and construction of high-quality training datasets have emerged as critical challenges impeding this progress. Research on SELF-REFINE has shown that advanced models like GPT-4 can refine their output quality through self-refinement mechanisms, inspiring us to consider leveraging the self-refine capabilities of large models to autonomously generate data with deep reasoning, thereby facilitating iterative model improvement.

% However, in preliminary experiments, we observed that the self-refine capabilities of smaller language models are relatively weak and, in some cases, may even lead to performance degradation, as shown in \ref{dpo_rm_result}. Addressing this phenomenon, we propose an innovative solution: the introduction of a Plug-in algorithm, integrated with existing DPO-like methods, aimed at first enhancing the language model's self-optimization capabilities. Our objective is to utilize this improved self-refinement mechanism to enable the model to produce higher-quality data, thereby providing valuable resources for subsequent iterative training. This approach not only holds the potential to address the current scarcity of high-quality data but also to further propel LLMs towards more intelligent and human-like development.

% Aligning large language models (LLMs) with human preferences using preference datasets has been demonstrated to significantly enhance their performance across downstream tasks \citep{ouyang2022training,bai2022training}.  Reinforcement Learning from Human Feedback (RLHF) has become a commonly used method for training LLMs to align with human values. Current RLHF-based methods can be broadly categorized into two approaches. The first approach involves training a reward model, which is then frozen and used to train the LLM using RL such as PPO \citep{schulman2017proximal}. The second approach bypasses the reward model training altogether, directly aligning the LLM to human preferences \citep{rafailov2023direct,azar2024general,ethayarajh2024kto,zeng2024token,meng2024simpo,hong2024orpo}. However, both approaches critically depend on access to high-quality, human-annotated preference datasets. As these datasets become increasingly scarce, the challenge of identifying novel strategies to further enhance LLM capabilities becomes increasingly urgent.

\begin{figure*}[t]
% s\vskip -0.35in
\begin{center}
\begin{minipage}{0.48\textwidth}
    % \vspace{-7pt}  % 微调表格的垂直位置
    \centering
    \includegraphics[width=\linewidth]{figs/alpacaeval.pdf}
    \caption{Length-controlled win rate on AlpacaEval 2 improves with ARIES iterations, surpassing GPT-4 level for the base versions of Llama-3.1-8B when utilizing the self-refinement strategy.}
    \label{figs:alpacaeval}
\end{minipage}\hfill
\begin{minipage}{0.48\textwidth}
    % \vspace{-3pt}  % 微调表格的垂直位置
    \centering
    \renewcommand{\arraystretch}{1.15}  % 调整表格行距，值越大行距越大
    \resizebox{\linewidth}{!}{
        \begin{tabular}{lccc}
            \toprule
            \textbf{Model} & \textbf{Size} & \textbf{LC}(\%) & \textbf{WR}(\%) \\
            \midrule
            \textbf{Llama-3.1-8B-Base-ARIES} ($iter2$, SR) & 8B & 62.3 & 63.3 \\
            GPT-4o (05/13) & - & 57.5 & 51.3 \\
            GPT-4-turbo (04/09) & - & 55.0 & 46.1 \\
            GPT-4o-mini (07/18) & - & 50.7 & 44.7 \\
            \textbf{Llama-3.1-8B-Base-ARIES} ($iter1$, SR) & 8B & 50.2 & 49.9 \\
            GPT-4\_1106\_preview & - & 50.0 & 50.0 \\
            \textbf{Llama-3.1-8B-Base-ARIES} ($iter2$, Direct) & 8B & 45.0 & 46.8 \\
            Claude 3 Opus (02/29) & - & 40.5 & 29.1 \\
            Llama-3.1-405B-Instruct-Turbo & 405B & 39.3 & 39.1 \\
            Qwen2-72B-Instruct & 72B & 38.1 & 29.9 \\
            Llama-3-70B-Instruct & 70B & 34.4 & 33.2 \\
            \textbf{Llama-3.1-8B-Base-ARIES} ($iter1$, Direct) & 8B & 32.7 & 33.5 \\
            Mistral Large (24/02) & 123B & 32.7 & 21.4 \\
            Gemini Pro & - & 24.4 & 18.2 \\
            Llama-3.1-8B-Instruct & 8B & 20.9 & 21.8 \\
            \bottomrule
        \end{tabular}
    }
   \makeatletter\def\@captype{table}\makeatother \caption{Results on AlpacaEval 2 leaderboard. LC and WR represent length-controlled and raw win rate, respectively. "Direct" refers to the direct response generation strategy, while "SR" denotes the self-refinement generation strategy.}
    \label{tab:alpacaeval}
\end{minipage}
\end{center}
% \vskip -0.3in
\end{figure*}

Recent research highlights that LLMs themselves can serve as a substitute for human feedback, reducing the reliance on human annotations while significantly increasing the automation of the training process \citep{leerlaif,wang2022self,yuan2024self,dubois2024alpacafarm,li2023alpacaeval}. To further improve LLM performance, the multiple parallel sampling strategy
% advanced model-driven generation techniques, such as Best of N (BoN) sampling and beam search, 
is integrated with self-evaluation mechanisms to curate high-quality datasets for downstream tasks \citep{yuan2024self,wu2024meta}. 
% For example, the \textit{Self-Rewarding Language Model} \citep{yuan2024self} utilizes BoN sampling to generate candidate datasets by producing multiple responses for each prompt. These responses are then evaluated and scored using the \textit{LLM-as-a-Judge} paradigm, forming a preference dataset that is subsequently used for iterative fine-tuning via DPO \citep{rafailov2023direct}. While this approach shows promise, it faces a critical limitation: preference datasets constructed through BoN sampling often lack sufficient differentiation. Even with extensive sampling, the gap between the chosen and rejected responses in terms of reasoning depth or answer quality remains narrow, limiting the informational gain required for further model improvement.

In this work, we propose an alternative approach to dataset generation inspired by the human problem-solving process, which starts with an initial draft and is refined iteratively.
% In this work, we explore an additional approach to dataset generation. Considering humans tackle complex problems, the process often begins with an initial draft answer, which is then refined and corrected iteratively until a more comprehensive solution emerges. 
In this iterative process, self-refinement plays a critical role in progressively enhancing the quality of the answer. 
% In the field of LLMs, SELF-REFINE \citep{madaan2024self} has demonstrated the effectiveness of self-refinement in enhancing advanced models such as GPT-4 \citep{achiam2023gpt}. 
This prompts us to consider whether the self-refinement capability of LLMs can be harnessed specifically to generate higher-quality datasets. 
% Compared to approaches that employ multiple parallel sampling strategies for data collection, this sequential generation method has the advantage that the model can leverage previous answers as priors when generating revised responses, 
Unlike methods that rely on parallel sampling, sequential generation allows models to build on previous responses, fostering deeper reflection and resulting in datasets with greater intellectual depth \citep{snell2024scaling,qu2024recursive,kumar2024training}.
% by enabling the models to engage in iterative reasoning and deep reflection on the same problem. 
% Such datasets would not only better capture the model's nuanced understanding of complex tasks but also serve as a more robust foundation for further fine-tuning and iterative training. 

However, preliminary results reveal that smaller models often exhibit weak self-refinement, which can even degrade performance. This suggests that models aligned exclusively through RLHF like DPO, struggle with this generation strategy, as shown in \cref{performance}. To address this, we introduce a novel algorithm that gradually instills the self-refinement capability in the model. Afterward, we leverage the model's activated self-refinement ability to collect higher-quality datasets, thereby enabling such iterative training.
% Our objective is to utilize this improved self-refinement mechanism to enable the model to produce higher-quality data, thereby providing valuable resources for subsequent iterative training. This approach not only holds the potential to address the current scarcity of high-quality data but also to further propel LLMs towards more intelligent and human-like development.

% After obtaining high-quality datasets, the next critical challenge is determining how to effectively utilize them for model training. Our experiments reveal that models aligned exclusively through RLHF like DPO often experience a reduction in self-refinement capability when applied to downstream tasks. To address this limitation and sustain the effectiveness of self-refinement, we explored strategies to enhance this ability. Our approach not only boosts the models’ generative performance but also reinforces their capacity for iterative reasoning and deep reflection on complex tasks, providing a solid foundation for advancing iterative model training methodologies.

In conclusion, we present the Adaptive Refinement and Iterative Enhancement Structure (ARIES), a novel framework for iterative preference training. \methodabb{} introduces a plug-in algorithm that complements existing DPO-like methods, fostering the development of self-refinement capability in LLMs. Building upon this, we leverage the self-refinement ability of LLMs to autonomously generate high-quality preference datasets, enabling an iterative training process that harmonizes both training and inference. Through rigorous empirical evaluations, \methodabb{} demonstrates remarkable efficacy, achieving state-of-the-art performance across a variety of benchmark datasets, including AlpacaEval 2 \citep{li2023alpacaeval}, Arena-Hard \citep{li2024live}, and MT-Bench \citep{zheng2023judging}. 
As shown in \cref{tab:alpacaeval}, ARIES employs the self-refinement generation strategy to achieve a length-controlled win rate of 62.3\% and a raw win rate of 63.3\% on AlpacaEval 2, alongside a win rate of 50.3\% on Arena-Hard, underscoring the significant impact of our approach in boosting the performance of LLMs.
 
% In conclusion, we propose a novel Adaptive Refinement and Iterative Enhancement Structure
%  (\methodabb) for preference training. The key contributions of our work are summarized as follows:
% \begin{itemize}
%     \item[1. ] \textbf{An innovative iterative training framework}: We propose a novel framework that leverages the Self-Refinement capability of LLMs to generate high-quality preference datasets, which are then used to iteratively train the model. This framework offers a scalable alternative to address the diminishing availability of high-quality human-annotated datasets.
%     \item[2. ] \textbf{Integration of Self-Refinement with RLHF}: We organically combine Self-Refinement with RLHF, enhancing the model’s foundational ability to tackle complex tasks while significantly strengthening its iterative optimization capabilities. This creates a coherent, self-consistent training loop.
%     \item[3. ] \textbf{Comprehensive empirical validation}: Through extensive quantitative and qualitative experiments, we demonstrate the effectiveness of our approach. Results show significant improvements in generation quality, dataset differentiation, and training efficiency compared to existing methods, paving the way for further advancements in automated optimization of LLMs.
% \end{itemize}

% By systematically addressing the limitations of existing methods and proposing a cohesive framework, our work provides a novel perspective on optimizing LLMs through self-supervised alignment, contributing to the ongoing evolution of automated LLM training methodologies.

% \begin{figure}[th]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=0.85\columnwidth]{figs/apacaeval.pdf}}
% \caption{Length-controlled win rate on AlpacaEval 2 improves with ARIES iterations, surpassing GPT-4 level for the base versions of Llama-3.1-8B.}
% \label{figs:tdpo}
% \end{center}
% \vskip -0.1in
% \end{figure}

% \begin{table}[t]
%     \centering
%     \caption{Results on AlpacaEval 2 leaderboard. LC and WR represent length-controlled and raw
% win rate, respectively.}
%     \begin{tabular}{lccc}
%         \toprule
%         \textbf{Model} & 
%         \textbf{Size} & 
%         \textbf{LC}(\%) & \textbf{WR}(\%) \\
%         \midrule
%         \textbf{Llama-3.1-8B-Base-ARIES} ($iter2$, SR) & 8B & 62.3 & 63.3 \\
%         GPT-4o (05/13) & - & 57.5 & 51.3 \\
%         GPT-4-turbo (04/09) & - & 55.0 & 46.1 \\
%         GPT-4o-mini (07/18) & - & 50.7 & 44.7 \\
%         \textbf{Llama-3.1-8B-Base-ARIES} ($iter1$, SR) & 8B & 50.2 & 49.9 \\
%         GPT-4\_1106\_preview & - & 50.0 & 50.0 \\
%         \textbf{Llama-3.1-8B-Base-ARIES} ($iter2$, Base) & 8B & 45.0 & 46.8 \\
%         Claude 3 Opus (02/29) & - & 40.5 & 29.1 \\
%         Llama-3.1-405B-Instruct-Turbo & 405B & 39.3 & 39.1 \\
%         Llama-3-70B-Instruct & 70B & 34.4 &     33.2 \\
%         \textbf{Llama-3.1-8B-Base-ARIES} ($iter1$, Base) & 8B & 32.7 & 33.5 \\
%         Mistral Large (24/02) & 123B & 32.7 & 21.4 \\
%         Gemini Pro & - & 24.4 & 18.2 \\
%         Llama-3.1-8B-Instruct & 8B & 20.9 & 21.8 \\
        
%         \bottomrule
%     \end{tabular}
%     \label{tab:performance}
% \end{table}


\section{Related Works}
\textbf{Reinforcement Learning from Human Feedback (RLHF)} 
% RLHF has demonstrated remarkable effectiveness in aligning LLMs with human values \cite{christiano2017deep, ouyang2022training, bai2022training, song2023preference, touvron2023llama}. By leveraging human-annotated preference datasets, these methods typically involve training a Reward Model, which is then used to guide the training of LLMs via reinforcement learning. Given the high cost of human annotation, recent research has turned to AI Feedback, leveraging LLMs to generate the feedback \citep{bai2022constitutional,lee2023rlaif}. This approach enhances the automation of model training, enabling continuous iterative optimization of LLM performance. 
RLHF has proven effective in aligning LLMs with human values \cite{christiano2017deep, ouyang2022training, bai2022training, song2023preference, touvron2023llama}. This approach uses human-annotated preference datasets to train a Reward Model, guiding LLM optimization through reinforcement learning. However, due to the high cost of human annotations, AI-generated feedback has been proposed to automate this process \citep{bai2022constitutional, lee2023rlaif}.
Additionally, to reduce training costs, Direct Preference Optimization (DPO) \cite{rafailov2023direct} bypasses the reward modeling process and directly aligns LLMs using preference datasets. However, the effectiveness of these methods heavily depends on the quality of the preference dataset, making the acquisition of high-quality preference data a critical challenge.

\textbf{Improving LLMs via Data Augmentation}
% As the availability of remaining high-quality human-labeled datasets gradually diminishes, various approaches have emerged that leverage LLMs to generate training datasets, thereby enhancing fine-tuning. Some approaches leverage powerful LLMs to generate training datasets, which are then distilled into smaller or weaker models. These methods enable weaker LLMs to perform exceptionally well with the support of such high-quality data. For instance, \textit{Alpagasus} \citep{chen2023alpagasus} uses ChatGPT as a judge to filter and optimize the Alpaca dataset, reducing it to a smaller, high-quality subset, which leads to superior performance. Other approaches emphasize the self-generation of novel problems or responses to enrich and expand datasets. \textit{Self-Instruct} \citep{wang2022self} introduces a methodology for self-instruction creation of prompts and responses, improving the foundational capabilities of LLMs. On the other hand, leveraging \textit{LLM-as-a-Judge} prompting has become a standard approach for evaluating model outputs \citep{dubois2024alpacafarm,bai2024benchmarking,saha2023branch}. This technique has been employed in training reward models and optimizing datasets \citep{lee2023rlaif,li2023self}. The \textit{Self-Rewarding Language Models} \citep{yuan2024self} framework combines general instruction-following techniques with \textit{LLM-as-a-Judge}, enabling iterative improvement of LLM performance. These advancements provide valuable insights into efficient data augmentation and optimization strategies for enhancing LLM capabilities.
As high-quality human-labeled datasets become scarcer, methods leveraging LLMs to generate training data have emerged. Some approaches use powerful LLMs to generate datasets, which are then distilled into smaller models, enabling weaker LLMs to perform better with high-quality data \citep{chen2023alpagasus}. Other methods focus on self-generating problems or responses to expand datasets \citep{wang2022self}. Additionally, \textit{LLM-as-a-Judge} prompting has become a standard technique for evaluating model outputs and optimizing datasets \citep{dubois2024alpacafarm,bai2024benchmarking,saha2023branch,yuan2024self}. These advancements offer valuable strategies for efficient data augmentation and optimization.

\textbf{In-Context Learning (ICL)} 
% ICL has become a fundamental capability of LLMs, enabling them to perform tasks by conditioning on a few input examples without requiring parameter updates \citep{brown2020language}. ICL has demonstrated the remarkable potential of LLMs to rapidly adapt to new tasks, excelling in reasoning, task transfer, and other domains. Recent studies, such as OPRO \citep{yang2024largelanguagemodelsoptimizers}, demonstrates that LLMs can leverage their ICL abilities to function as implicit optimizers during forward inference, progressively improving their performance on complex optimization problems. Similarly, \citet{monea2024llms} reveal that LLMs can act as in-context reinforcement learners, improving their behavior solely through reward feedback without requiring explicit supervised labels. The SELF-REFINE \citep{madaan2024self} is a special form of ICL, further highlighting the iterative optimization potential of ICL. It significantly enhances model performance through the FEEDBACK and REFINE mechanisms, achieving remarkable results across multiple benchmarks.  These findings indicate that integrating ICL with model training presents a compelling strategy for constructing self-optimizing frameworks, offering a promising method for continuously improving the performance of LLMs.
ICL has become a fundamental capability of LLMs, enabling them to perform tasks by conditioning on a few input examples without requiring parameter updates \citep{brown2020language}. Recent studies, such as OPRO \citep{yang2024largelanguagemodelsoptimizers}, show that LLMs can leverage their ICL abilities to function as implicit optimizers, progressively improving performance on complex problems. LLMs can also act as in-context reinforcement learners, optimizing behavior via reward feedback \citep{monea2024llms}. The SELF-REFINE \citep{madaan2024self} is a special form of ICL. It significantly enhances model performance through the FEEDBACK and REFINE mechanisms, achieving remarkable results across multiple benchmarks. 
These findings indicate that integrating ICL with model training presents a compelling strategy for constructing self-optimizing frameworks.

\section{Preliminaries}
%  \label{DPO} 
% %\jun{remove this to have just one section without subsections. }In this section, we primarily introduce some common notations used in text generation (\textsection \ref{Nota}) and provide an overview of DPO (\textsection \ref{DPO}).
% For language generation, a language model (LM) is prompted with prompt (question) ${x}$ to generate a response (answer) ${y}$, where both ${x}$ and ${y}$ consist of a sequence of tokens. Direct Preference Optimization (DPO) \cite{rafailov2023direct}
% %\jun{starts this without previous section so do add defintion of x and y etc.}
%  commences with the RL objective from the RLHF:
% \begin{equation}
% \begin{split}
% \max_{\pi_\theta}\ \mathbb{E}_{{x}\sim\mathcal{D},{y}\sim\pi_\theta(\cdot|{x})}&\big[r({x},{y})\\
% -\beta D_{\mathrm{KL}}&\big(\pi_\theta(\cdot\mid {x})\big\|\pi_{\mathrm{ref}}(\cdot\mid {x})\big)\big],
% \end{split}\label{rlhfob}
% \end{equation}
% where $\mathcal{D}$ represents the human preference dataset, $r({x},{y})$ denotes the reward function, $\pi_{\mathrm{ref}}(\cdot|{x})$ serves as a reference model, typically chosen the language model after supervised fine-tuning, $\pi_{\theta}$ represents the model undergoing RL fine-tuning, initialized with $\pi_{\theta}=\pi_{\mathrm{ref}}$, and $\beta$ is the coefficient for the reverse KL divergence penalty.

% By directly deriving from Eq.~\ref{rlhfob},
% DPO establishes a mapping between the reward model and the optimal policy under the reverse KL divergence, obtaining a representation of the reward function concerning the policy:
% % \begin{equation}
% % r({x}, \cdot)=\beta\log\frac{\pi_{\theta}(\cdot|{x})}{\pi_{\mathrm{ref}}(\cdot|{x})}+\beta\log Z({x})
% % \label{dpo_mapping}
% % \end{equation}
% \begin{equation}
% r({x}, {y})=\beta\log\frac{\pi_{\theta}({y}|{x})}{\pi_{\mathrm{ref}}({y}|{x})}+\beta\log Z({x}).
% \label{dpo_mapping}
% \end{equation}
% Here, $Z({x})$ is the partition function. 

% To align with human preference, DPO uses the Bradley-Terry model for pairwise comparisons:
% % \begin{equation}
% %     \begin{aligned}
% %         P_{\mathrm{BT}}({y}_1\succ {y}_2 | {x})&=\frac{\exp(r({x}, {y}_1))}{\exp(r({x}, {y}_1))+\exp(r({x}, {y}_2))}\\
% %         &=\sigma(r({x}, {y}_1) - r({x}, {y}_2))
% %     \end{aligned}
% %     \label{BT_model}
% % \end{equation}
% \begin{align}
%     P_{\mathrm{BT}}({y}_1\succ {y}_2 | {x})&=\frac{\exp(r({x}, {y}_1))}{\exp(r({x}, {y}_1))+\exp(r({x}, {y}_2))}.
%     % \\
%     % &=\sigma(r({x}, {y}_1) - r({x}, {y}_2)) 
%     \label{BT_model}
% \end{align}
% % where $\sigma(x)=1/(1+exp(-x))$ is the logistic sigmoid function.

% By substituting Eq.~\ref{dpo_mapping} into Eq.~\ref{BT_model} 
% % PDO establishes a direct relationship between preference optimization and policy.
% and leveraging the negative log-likelihood loss, DPO derives the objective function:
% % \begin{equation}
% \begin{align}
%     u({x}, {y}_w, {y}_l)=\beta\log\frac{\pi_\theta({y}_w\mid {x})}{\pi_{\mathrm{ref}}({y}_w\mid {x})}-\beta\log\frac{\pi_\theta({y}_l\mid {x})}{\pi_{\mathrm{ref}}({y}_l\mid {x})},\nonumber\\
%     \mathcal{L}_{\mathrm{DPO}}(\pi_\theta;\pi_{\mathrm{ref}})=-\mathbb{E}_{({x},{y}_w,{y}_l)\sim\mathcal{D}}\left[\log\sigma\left(u({x}, {y}_w, {y}_l)\right)\right],
% \end{align}
% and the derivative is given as follows:
% \begin{equation}
% \nabla_\theta\mathcal{L}_{\mathrm{DPO}}(\pi_\theta;\pi_{\mathrm{ref}})=-\mathbb{E}_{({x},{y}_w,{y}_l)\sim\mathcal{D}}
%     \left[\sigma\left(-u\right)
%     % \mathcal{G}({x}, {y}_w, {y}_l;\theta)
%     \nabla_\theta u\right],
% \end{equation}
% where $u$ is the abbreviation of $u({x}, {y}_w, {y}_l)$, $y_w$ and $y_l$ denotes the preferred and dispreferred completion.

% \textbf{DPO}
\paragraph{DPO}
The standard DPO algorithm usually involves two stages: (1) Supervised Fine-Tuning (SFT) and (2) DPO training.

In the SFT stage, the DPO algorithm fine-tunes a pre-trained language model $\pi_\theta$ with the loss function defined as:
\begin{align}
\mathcal{L}_{\mathrm{SFT}}(\pi_{\theta};\pi_{\mathrm{ref}}) = -\underset{(x,y)\sim\mathcal{D}}{\mathbb{E}}[\log \pi_{\theta}({y}|{x})],
\end{align}
which yields a fine-tuned model, denoted as $\pi_{\mathrm{ref}} = \pi_{\mathrm{SFT}}$, to be used as the reference model in subsequent stages.
If given a preference dataset $\mathcal{D} = \{x^{(i)}, y_w^{(i)}, y_l^{(i)}\}_{i=1}^N$ for fine-tuning, the chosen response $y_w$ is typically selected as the target for SFT. In this case, the loss becomes:
\begin{align}
\mathcal{L}_{\mathrm{SFT}}(\pi_{\theta}; \pi_{\mathrm{ref}}) = -\underset{(x,y_w,y_l)\sim\mathcal{D}}{\mathbb{E}} [\log \pi_{\theta}({y_w}|{x})].
\end{align}
In the DPO preference training stage, DPO uses the fine-tuned model as initialization, i.e., $\pi_\theta = \pi_{\mathrm{SFT}}$, and directly optimizes the policy model using the following negative log-likelihood loss:
% \begin{align}
% u({x}, {y}_w, {y}_l)=\beta\log\frac{\pi_\theta({y}_w\mid {x})}{\pi_{\mathrm{ref}}({y}_w\mid {x})}-\beta\log\frac{\pi_\theta({y}_l\mid {x})}{\pi_{\mathrm{ref}}({y}_l\mid {x})},\nonumber\\
%     \mathcal{L}_{\mathrm{DPO}}(\pi_\theta;\pi_{\mathrm{ref}})=-\mathbb{E}_{({x},{y}_w,{y}_l)\sim\mathcal{D}}\left[\log\sigma\left(u({x}, {y}_w, {y}_l)\right)\right].
% \end{align}
\begin{small}
% \hspace{-5pt}
% \hskip -1in
\begin{align}
\hspace{-4pt} u({x}, {y}_1, {y}_2;\pi)=\beta\log\frac{\pi({y}_2\mid {x})}{\pi_{\mathrm{ref}}({y}_2\mid {x})}-\beta\log\frac{\pi({y}_1\mid {x})}{\pi_{\mathrm{ref}}({y}_1\mid {x})},\nonumber\\
    \hspace{-4pt} \mathcal{L}_{\mathrm{DPO}}(\pi_\theta;\pi_{\mathrm{ref}})=-\underset{(x,y_w,y_l)\sim\mathcal{D}}{\mathbb{E}}\left[\log\sigma\left(u({x}, {y}_l, {y}_w;\pi_\theta)\right)\right].
\end{align}
\end{small}
% \begin{equation}
%     \mathcal{L}_{\mathrm{DPO}}(\pi_\theta;\pi_{\mathrm{ref}})=-\underset{(x,y_{w},y_{l})\sim\mathcal{D}}{\mathbb{E}}\left[\log\sigma\left(\beta_{\mathrm{DPO}}\log\frac{\pi_\theta({y}_w\mid {x})}{\pi_{\mathrm{ref}}({y}_w\mid {x})}-\beta_\mathrm{DPO}\log\frac{\pi_\theta({y}_l\mid {x})}{\pi_{\mathrm{ref}}({y}_l\mid {x})}\right)\right].
% \end{equation}
% \begin{equation}
% \begin{aligned}
%     &\mathcal{L}_{\mathrm{DPO}}(\pi_\theta;\pi_{\mathrm{ref}})=\\
%     &-\underset{(x,y_{w},y_{l})\sim\mathcal{D}}{\mathbb{E}}\left[\log\sigma\left(\beta_{\mathrm{DPO}}\log\frac{\pi_\theta({y}_w\mid {x})}{\pi_{\mathrm{ref}}({y}_w\mid {x})}-\beta_\mathrm{DPO}\log\frac{\pi_\theta({y}_l\mid {x})}{\pi_{\mathrm{ref}}({y}_l\mid {x})}\right)\right].
% \end{aligned}
% \end{equation}
% \textbf{Self-Refinement}

\paragraph{Self-Refinement}
In language generation, given a question input \( x \), the model typically generates a response \( y \sim \pi(\cdot | x) \). We refer to this approach as \textit{direct response generation} in this paper. Building on this, our work advances further by assuming that we are given both a question input \( x \) and an existing response \( y_1 \). Our goal is to generate a better response \( y_2 \sim \pi(\cdot | x, y_1) \), where this generation method is termed as \textit{Self-Refinement} in our work.


% \end{equation}
% However, under the above optimization objective of the DPO algorithm, the gap of the KL divergences between the preferred response subset and the dispreferred response subset tends to widen gradually, affecting the generation diversity of the LM, as illustrated in \cref{fig1}. For theoretical analysis to aid comprehension, we introduce some simplifications and formalize this imbalance based on the Total Variation (TV) divergence in the Lemma \ref{lemma3}. We will also demonstrate this phenomenon in the experimental section.
%  \begin{restatable}{lemma}{mylemmathree}\label{lemma3}
%     For a human preference dataset $\mathcal{D}$ and a pair-wise comparison $({x},{y}_w,{y}_l)\sim \mathcal{D}$, we decompose the derivative of the DPO loss function into two parts, removing the common factors between them, as follows: 
%     \begin{itemize}
%         \item[1. ] $\mathcal{G}({y}_w;\theta)=-\nabla_{\theta}\log\pi_{\theta}({y}_w|{x})$;
%         \item[2. ]$\mathcal{G}({y}_l;\theta)=\nabla_{\theta}\log\pi_{\theta}({y}_l|{x})$.
%     \end{itemize}
%     We assume $\pi_{\theta}({y}_w|{x}) > \pi_{\theta}({y}_l|{x})$ and $\nabla_{\theta}\pi_{\theta}({y}_w|{x}) = \nabla_{\theta}\pi_{\theta}({y}_l|{x})$. As the iterative optimization progresses, the gap of the sequential TV divergence between the preferred response subset and the dispreferred response subset gradually widens.
% \end{restatable}
% We prove \cref{lemma3} in \cref{A_3}. To address this issue in DPO, we introduce our algorithm below.

\section{Methodology}
% Existing works \citep{monea2024llms,yang2024largelanguagemodelsoptimizers,madaan2024self} have demonstrated the in-context learning and self-refinement capability of large models. However, when these methods are applied to weaker open-source models, their performance is ultimately limited by the inherent capabilities of the base models. Despite leveraging in-context learning and self-refinement, the final performance of large models is still constrained. Recently, RLHF (Reinforcement Learning with Human Feedback) has emerged as a highly effective method for improving the capabilities of base models. With this in mind, we propose a novel approach: leveraging the in-context learning and self-refinement ability of large models to collect preference datasets, and then using RLHF to strengthen the foundational capabilities of existing open-source models based on the collected preference datasets. This process can be iteratively improved. Finally, during inference, we utilize the enhanced large model's in-context learning and self-refinement ability to generate highly impressive and accurate responses. This paper addresses the development and evaluation of this approach.

How to enable large models to correct their responses when provided with additional information is both an intriguing and valuable challenge. However, existing research indicates that even the most advanced models struggle with self-improvement, often failing to refine prior outputs effectively \citep{huang2023large}. 
% often failing to enhance prior outputs. 
In some cases, 
% excessive correction demands can even reduce the models' confidence 
repeated correction attempts can even diminish the models' confidence
in their responses, resulting in progressively worse revisions, as shown in \cref{performance}.  In this section, we investigate how to stimulate the model's self-refinement capability within the RLHF framework, and how leveraging the self-refinement capability can, in turn, enhance overall model training performance. 
We will elaborate on our approach, \methodabb{}, covering both the training methods and the inference framework. Finally, we will summarize the complete workflow of the proposed framework.
% We will elaborate on our approach \methodabb{} from three aspects: training methods, inference framework, and data selection. Finally, we will summarize the complete workflow of our proposed framework.

\begin{figure*}[t]
% \vskip -0.05in
\begin{center}
% \centerline{\includegraphics[width=0.8\columnwidth]{figs/hh/hh.pdf}}
\centerline{\includegraphics[width=\textwidth]{figs/Method3.pdf}}
% \vskip -0.1in
\caption{\textbf{ARIES: Adaptive Refinement and Iterative Enhancement Structure.} Our method iteratively alternates between inference and training processes. In the inference phase, we utilize the model $M_t$ from the previous training round to generate a series of self-refined responses to the prompt dataset through the self-refinement strategy. These responses are then filtered using either Rule-Based Selection or Reward Model Scoring mechanism to construct a preference dataset for training. In the training phase, we apply a novel preference-based training algorithm to train the model $M_t$ using the collected preference dataset. This algorithm primarily focuses on activating and strengthening the model's self-refinement ability to assist in generation during the inference phase. 
The top-left template is our self-refinement template, used in both the training and inference stages.}
\label{method}
\end{center}
% \vskip -0.4in
\end{figure*}


% \subsection{\methodabb{} Training Method: Plug-in Extension for DPO Framework}

\subsection{Training: Plug-in Extension for DPO Framework}

Considering that existing models generally struggle to achieve self-improvement, the primary challenge we aim to address is how to enable models to undergo effective self-refinement. Conventional RLHF algorithms, such as DPO, do not inherently provide this capability. Therefore, we extend the DPO framework, incorporating both Supervised Fine-Tuning (SFT) and preference training, to empower models with the self-refinement ability.

% Now that we have constructed a preference dataset through self-refinement and the \textit{LLM-as-a-Judge} approach, the next challenge is determining how to leverage this dataset to enhance the model's foundational capabilities. A conventional choice involves employing existing RLHF algorithms, such as DPO , for training. However, our experiments reveal that training exclusively with DPO on the preference dataset leads to a decline in the model's ICL ability. In other words, the self-refinement performance of the model deteriorates, yielding outputs that are worse than its direct generation, as demonstrated in Figure \ref{dpo_rm_result}.

% \textbf{Supervised Fine-Tuning (SFT)}
\paragraph{Supervised Fine-Tuning (SFT)}
During the SFT stage, in addition to the standard Negative Log-Likelihood Loss, we seek to enhance the model's ability to refine suboptimal responses. Specifically, we aim to improve the model's ability to assign a higher probability to a good response \( y_w \) given the input \( x \), the existing suboptimal response \( y_l \) and a special refinement template \(z\). Thus, the loss function at the SFT stage is defined as follows:
% \begin{align}
%     &\mathcal{L}_{\mathrm{\methodabb{}}-\mathrm{SFT}}(\pi_{\theta};\pi_{\mathrm{ref}}) =  -\mathbb{E}_{(x,y_l,y_w)\sim\mathcal{D}}[\log\pi_{{\theta}}({y_w}|p_{\mathrm{gen}}\|{x}) + \log\pi_{{\theta}}({y_w}|p_{\mathrm{refine}}\|{x}\|y_l)].\label{sfft}
% \end{align}
\begin{equation}
   \begin{aligned}
    &\mathcal{L}_{\mathrm{\methodabb{}}-\mathrm{SFT}}(\pi_{\theta};\pi_{\mathrm{ref}}) = \\
    & \ -\underset{(x,y_w,y_l)\sim\mathcal{D}}{\mathbb{E}}
    [\log\pi_{{\theta}}({y_w}|{x}) + \log\pi_{{\theta}}({y_w}|{x}, y_l,z)].
\end{aligned} 
\label{aries-sft}
\end{equation}
% \begin{figure}[ht]
% % \vskip 0.2in
% \begin{center}
% % \centerline{\includegraphics[width=0.8\columnwidth]{figs/hh/hh.pdf}}
% \centerline{\includegraphics[width=\textwidth]{figs/dpo_rm_result.png}}
% \caption{Change in the model's ICL performance after DPO training during inference. The x-axis represents the number of self-refinement iterations, and the y-axis shows the Reward Model scores.}
% \label{dpo_rm_result}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \textbf{Preference Training}
\paragraph{Preference Training}
In the preference training stage, we begin by presenting the following scenario: Given a prompt \(x\), assume we have already sampled an initial response \(y_1\). 
How can we utilize the prompt \(x\) and the response \(y_1\) to generate a more refined response \(y_2\) guided by a refinement template $z$? To achieve this, we define the following objective function:
% Guided by a refinement template $z$, our task is to identify how to utilize the prompt \(x\) and the response \(y_1\) to generate a more refined response \(y_2\). To achieve this, we define the following objective function:
% \begin{align}
%     \max_{\pi}\ \mathbb{E}_{\substack{y_{2}\sim\pi(\cdot|p_{\mathrm{refine}}\|x\|y_{1})
%     % \\y_{1}\sim \mu(x)
%     }}\bigg[p(y_{2}\succ y_{1}|x)-\beta D_{\mathrm{KL}}(\pi||\pi_{\mathrm{ref}}|p_{\mathrm{refine}}\|x\|y_{1})\bigg]
% \end{align}
\begin{small}
\begin{align}
    \hspace{-2pt} \max_{\pi}\ \underset{y_{2}\sim\pi(\cdot|x, y_{1},z)}{\mathbb{E}}\bigg[p(y_{2}\succ y_{1}|x)-\beta D_{\mathrm{KL}}(\pi||\pi_{\mathrm{ref}}|x,y_{1},z)\bigg].\label{eq:objective}
\end{align}
\end{small}
Here, \(p(y_{2} \succ y_{1} | x)\) represents the human preference probability, indicating the likelihood that \(y_2\) is preferred over \(y_1\) given \(x\):
\begin{equation}
\small
    p(y_2 \succ y_1 \mid x) = \mathbb{E}_h[\mathbb{I}\{h \text{ prefers } y_2 \text{ over } y_1 \text{ given } x\}], \label{human_preference}
\end{equation}
where the expectation is taken over the distribution of humans \(h\).
% The term \(\pi_{\mathrm{ref}}\) denotes the reference model, and \(D_{\mathrm{KL}}(\pi || \pi_{\mathrm{ref}} | p_{\mathrm{refine}} \| x \| y_{1})\) represents the KL divergence between the policy \(\pi\) and the reference model \(\pi_{\mathrm{ref}}\), conditioned on the input \(p_{\mathrm{refine}} \| x \| y_{1}\).
The optimization process of the objective function in Eq. \ref{eq:objective} follows a similar approach to SRPO \citep{choi2024self}. 
Solving Eq. \ref{eq:objective} yields the following equality:
\begin{small} 
\begin{align}
v({x}, {y}_1, {y}_2, z;\pi)&=\beta
        \log\left(\frac{\pi(y_2|x, y_{1}, z)\pi_{\mathrm{ref}}(y_{1}|x, y_{1}, z)}{\pi_{\mathrm{ref}}(y_2|x, y_{1}, z)\pi(y_{1}|x, y_{1}, z)}\right) ,\nonumber \\
    p(y_{2} & \succ y_1|x) - \frac{1}{2} =v({x}, {y}_1, {y}_2, z;\pi^*). \label{eq:preference_1_2}
\end{align}
\end{small}
The derivation is presented in \cref{A_1}. According to Eq. \ref{eq:preference_1_2}, we adopt the mean squared error as the loss function and parametrize the policy model as $\pi_{\theta}$, iterating over all prompts $x$ and responses $y_1$, $y_2$, which leads to:
\begin{equation}
\small
\begin{aligned}
\mathcal{L}(&\pi_{\theta};\pi_{\mathrm{ref}}) = \\
    &\underset{\substack{(x, y_{1}, y_{2})\sim \rho}}{\mathbb{E}} 
    \bigg[p(y_{2}\succ y_1|x) - 
        \frac{1}{2} - v({x}, {y}_1, {y}_2,z;\pi_\theta)
    \bigg]^2,
\end{aligned}
\label{eq:origin_loss}
\end{equation}
where $\rho$ represents the true distribution.
% Consider the preference function \(p(y_{2} \succ y_{1} \mid x)\), which represents the probability that humans prefer $y_2$ to $y_1$ given the input $x$. It can be expressed as \(p(y_2 \succ y_1 \mid x) = \mathbb{E}_h[\mathbb{I}\{\text{h prefers } y_2 \text{ to } y_1 \text{ given } x\}]\), where the expectation is taken over the distribution of humans \(h\). With a sufficiently high-quality preference dataset \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}\), we consider the data to reflect absolute preferences, such that \(p(y_{w} \succ y_{l} \mid x) = 1\) and \(p(y_{l} \succ y_{w} \mid x) = 0\). Based on this, we define our final loss function as:
% Note that \( p(y_2 \succ y_1 \mid x) \) denotes the probability that humans prefer \( y_2 \) over \( y_1 \) given input \( x \):
% \[
% p(y_2 \succ y_1 \mid x) = \mathbb{E}_h[\mathbb{I}\{h \text{ prefers } y_2 \text{ over } y_1 \text{ given } x\}],
% \]where the expectation is taken over the distribution of humans \(h\), 
We substitute Eq. \ref{human_preference} into Eq. \ref{eq:origin_loss}. For a well-curated preference dataset \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N\), we obtain the following loss function: 
% With a sufficiently high-quality preference dataset \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}\), we assume that the data reflects absolute preferences. Specifically, this implies \(p(y_w \succ y_l \mid x) = 1\) and \(p(y_l \succ y_w \mid x) = 0\). Based on this assumption, we obtain our final loss function as follows:
% \begin{equation}
% \begin{aligned}
%     \mathcal{L}_{\mathrm{refine}}(\pi_{\theta};\pi_{\mathrm{ref}})& =\underset{(x,y_{w},y_{l})\sim\mathcal{D}}{\mathbb{E}}\left[\frac{1}{2}-\beta\left[\log\left(\frac{\pi_{\theta}(y_{w}|x, y_{l})}{\pi_{\mathrm{ref}}(y_{w}|x, y_{l}))}\right)-\log\left(\frac{\pi_{\theta}(y_{l}|x, y_{l})}{\pi_{\mathrm{ref}}(y_{l}|x, y_{l})}\right)\right]\right]^{2}  \\
% &+\underset{(x,y_{w},y_{l})\sim\mathcal{D}}{\mathbb{E}}\left[\frac{1}{2}-\beta\left[\log\left(\frac{\pi_{\theta}(y_{w}| x, y_{w})}{\pi_{\mathrm{ref}}(y_{w}|x, y_{w}))}\right)-\log\left(\frac{\pi_{\theta}(y_{l}|x, y_{w})}{\pi_{\mathrm{ref}}(y_{l}|x, y_{w})}\right)\right]\right]^{2} 
% \end{aligned}
% \label{loss}
% \end{equation}
\begin{equation}
\small
\begin{aligned}
    \hspace{-4pt}\mathcal{L}_{\mathrm{refine}}(\pi_{\theta};\pi_{\mathrm{ref}})& =\underset{(x,y_{w},y_{l})\sim\mathcal{D}}{\mathbb{E}}\left[\frac{1}{2}-v(x, y_l, y_w,z;\pi_\theta)\right]^{2}  \\
\hspace{-4pt}&+\underset{(x,y_{w},y_{l})\sim\mathcal{D}}{\mathbb{E}}\left[\frac{1}{2}+v(x,y_w,y_l,z;\pi_\theta)\right]^{2}. 
\end{aligned}
\label{loss:refine}
\end{equation}
% \analyse{ Objective Function
% \begin{align}
%     \max_{\pi}\ \mathbb{E}_{\substack{y_{2}\sim\pi(\cdot|y_{1},x)
%     % \\y_{1}\sim \mu(x)
%     }}\bigg[p(y_{2}\succ y_{1}|x)-\beta_1 D_{\mathrm{KL}}(\pi||\pi_{\mathrm{ref}}|y_{1},x)-\beta_2 D_{\mathrm{KL}}(\pi||\pi_{\mathrm{old}}|y_{1},x)\bigg]
% \end{align}
% This can be solved similarly.}
Finally, we combine the DPO loss and the self-refinement loss to derive our final loss function:
\begin{equation}
\begin{aligned}
    \hspace{-4pt}\mathcal{L}_{\mathrm{\methodabb{}}}&(\pi_{\theta};\pi_{\mathrm{ref}}) =\\
    \hspace{-4pt}&(1- \alpha) \mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}}) + \alpha\mathcal{L}_{\mathrm{refine}}(\pi_{\theta};\pi_{\mathrm{ref}}). 
\end{aligned}
\label{loss}
\end{equation}
Through this optimization, we not only enhance the direct conversational capabilities of LLMs but also strengthen the models' self-refinement ability.
Notably, our approach does not impose any restrictions on the choice of RLHF algorithm. In other words, any RLHF algorithm's loss function can be substituted into Eq. \ref{loss} to replace the DPO loss term. 

\textbf{It is essential to emphasize that the effectiveness of our approach, \methodabb{}, does not stem from a particular refinement algorithm, such as Eq. \ref{loss:refine}, but rather from the fundamental refinement principle it encapsulates.} In \cref{appendix:loss}, we introduce a new refinement loss function derived from the perspective of the Bradley-Terry model, which we designate as BT\_\methodabb{}. Experimental results show that both BT\_\methodabb{} and \methodabb{} yield comparable effectiveness, driving substantial improvements in model performance. For further details, please refer to \cref{appendix:loss}.

% For instance, if we opt to use the DPO algorithm for training, the overall loss function becomes:
% \begin{align}
%     &\mathcal{L}(\pi_{\theta};\pi_{\mathrm{ref}}) = (1- \alpha) \mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}}) + \alpha\mathcal{L}_{\mathrm{refine}}(\pi_{\theta};\pi_{\mathrm{ref}}) \nonumber
% \end{align}

% Recent studies \citep{yang2024largelanguagemodelsoptimizers, monea2024llms, madaan2024self} have highlighted the impressive ICL abilities of LLMs. Nevertheless, their performance remains limited by their inherent foundational strengths. Even with the integration of ICL and self-refinement, these models still face an upper bound on achievable performance.  In this work, we aim to enhance LLMs by integrating their ICL abilities with RLHF, employing iterative training to further improve performance. Additionally, during inference, we leverage the ICL capabilities of LLMs to enhance their understanding of complex tasks and refine their problem-solving capacity. In the following section, we detail our framework from three key aspects: inference, data selection, and training. Finally, we will summarize the complete workflow of our proposed algorithm.

% \subsection{\methodabb{} Inference Framework: Data Collection Through Self-Refinement}
\subsection{Inference: Self-Refinement for Data Collection}
% During the inference phase, we leverage the self-refinement capability instilled in the model during training to gather a set of iteratively refined responses of the new prompt dataset, as illustrated in \cref{method}. Through this iterative process, the model repeatedly revisits the same question, progressively enhancing its understanding and improving the logical depth, coherence, and robustness of its response, much like the process of human problem-solving. Specifically, we explore the following iterative response refinement strategies:

% \textbf{Response Refinement}
\paragraph{Response Refinement}
During the inference phase, we exploit the self-refinement capability instilled in the model during training to generate a sequence of progressively refined responses to the new prompt dataset, as illustrated in \cref{method}. This iterative process mirrors human-like problem-solving, where the model revisits the same question multiple times, refining its understanding and improving the quality of its responses. By using the response from the previous iteration as a prior, the model incrementally enhances its performance, ensuring that each successive turn builds upon the insights of the last.
% Through each turn, the model refines its last response, aiming to improve key attributes such as logical depth, coherence, and robustness. This process leads to responses that exhibit greater alignment with the prompt’s underlying intent and demonstrate increasingly nuanced insights. Specifically, we investigate the following iterative response refinement strategies, which are aimed at improving both the quality and reliability of the generated outputs:

% Specifically, given a problem \( x \), we first use the model to directly generate an answer \( y_1 \). Then, based on the prior knowledge of \( x \) and \( y_1 \), the model generates a refined response \( y_2 \), i.e.,
% $y_2 \sim \pi(\cdot | x, y_1)$. Similarly, the model generates \( y_3 \) based on \( x \) and \( y_2 \). This process continues for \( N \) iterations, where at each round, the answer \( y_n \) is generated based on the given problem \( x \) and the previous round’s answer \( y_{n-1} \):
Specifically, given a problem \( x \), we first use the model to directly generate an answer \( y_1 \). Next, we apply a fixed refinement template $z$ that concatenates the problem \( x \) and the answer \( y_1 \), with the objective of improving the answer's quality. The model is then provided with two options for refinement:
\begin{itemize}
    \item[1. ]If the model believes that the existing answer is sufficient, it only needs to modify the given answer to further enhance its quality.
    \item[2. ]If the model finds the existing answer lacking in clarity or relevance to the problem, it disregards the previous answer and directly generates a more effective new response.
\end{itemize}
This approach grants the model some degree of backtracking, preventing it from repeatedly getting stuck in a dead-end that could lead to performance degradation. The refinement template is illustrated in \cref{method}. By applying the refinement template iteratively, we generate \( y_2 \) from \( x \) and \( y_1 \), then \( y_3 \) from \( x \) and \( y_2 \). This process continues for \( N \) rounds, where each answer \( y_n \) is generated based on the given problem \( x \) and the previous round's answer \( y_{n-1} \):
\begin{align}
y_n \sim \pi(\cdot | x, y_{n-1})
\end{align}
Through this iterative generation process, we obtain a progressively refined set of responses for the problem \( x \), denoted as \( \{ x, y_1, y_2, \dots, y_N \} \). 
% It is worth noting that we also experimented with other data generation strategies, such as few-shot prompting and creating new responses based on previous answers and their evaluations (utilizing the question, response, and evaluation). However, due to the model's limited self-evaluation ability, we did not observe substantial improvements. The approach described above is considered the most efficient and effective. For further details, please refer to Appendix B.


% \textbf{Data Selection}
\paragraph{Data Selection}
Given a new prompt dataset \( \mathcal{D} = \{x^{(i)}\}_{i=1}^N \), through the iterative self-refinement process described above, we can obtain a self-refined prompt-responses dataset \( \mathcal{D} = \{(x^{(i)}, y_1^{(i)}, y_2^{(i)}, \dots, y_N^{(i)})\}_{i=1}^N \).
However, not every round of model generation improves the response quality. Therefore, we must implement a selection mechanism to filter the data, enhancing the robustness of the final preference dataset. 
We primarily explore two data filtering methods: 
% \textit{LLM-as-a-Judge} \citep{yuan2024self} and Reward Model scoring. Additionally, we investigate a novel data selection approach that relies purely on the model’s self-refinement capability, without any external supervision. 
\textbf{Rule-Based Selection} and \textbf{Reward Model Scoring}. The Rule-Based Selection mechanism is an intriguing approach that relies purely on the model's self-refinement capability without any external supervision. 
%, as an ablation experiment to demonstrate the effectiveness of our proposed methods
Specifically, we directly select the responses from round 0 (i.e., the initial answers to the questions) as rejected responses, while empirically designating the responses from round $N$ as chosen responses to construct the preference dataset. This mechanism plays a crucial role in validating the effectiveness of our proposed technique, highlighting the remarkable efficacy of \methodabb{}.
% Recent advancements, such as the \textit{LLM-as-a-Judge} approach \citep{yuan2024self}, suggest that the model itself can act as an evaluator. Based on this insight, we propose two strategies for dataset selection:
% \begin{itemize}
%     \item[1. ] \textbf{Empirical Selection}: Select responses from the 0th round (without ICL or self-refinement) and a specific \(n\)-th round (empirically identified as producing the best results) to construct the preference dataset.  
%     \item[2. ] \textbf{SFT Model as a Judge}: Utilize a supervised fine-tuned (SFT) model to evaluate responses across the 0th, 1st, ..., \(N\)-th rounds. The SFT model identifies the best and worst responses, which are then used to build the preference dataset.  
% \end{itemize}

\subsection{Workflow: Stimulating Model Self-Refinement}
% \begin{wrapfigure}[16]{r}{0.33\textwidth} % Adjust the number of lines for wrapping and width of the figure
%     \vskip -0.3in
%     \centering
%     \includegraphics[width=0.33\textwidth]{figs/offline_necessity.pdf}
%     \caption{Performance demonstration of the self-refinement capability after \methodabb{}-SFT and offline preference training. Following preference training, the model's self-refinement ability shows further improvement compared to the \methodabb{}-SFT model.}
%     \label{fig:offline_necessity}
% \end{wrapfigure}

The goal of our work is to activate the model's self-refinement ability and leverage this capability to assist the model in achieving improvement during the iterative training. Since current open-source models face challenges in enhancing their own responses, our first task is to trigger the model's self-refinement ability using our proposed algorithm. Starting from a base model, we perform two main steps in this phase:
\begin{itemize}
    \item[1. ]Apply the \methodabb{}-SFT loss function, as defined in Eq. \ref{aries-sft}, to Supervise Fine-Tuning the base model, resulting in the fine-tuned model;
    \item[2. ]Perform preference training on the fine-tuned model with an offline preference dataset and the loss function Eq. \ref{loss}, obtaining the \methodabb{} \textit{offline} model.
\end{itemize}

The reason for conducting preference training on the \methodabb{}-SFT model
with the offline preference dataset is that we observe this process further activates the model's self-refinement capability, as shown in 
% \cref{fig:offline_necessity}. In \cref{fig:offline_necessity}, we use the reward model \href{https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}{Skywork/Skywork-Reward-Llama-3.1-8B-v0.2} \citep{liu2024skywork} for scoring, which is a relatively small model but demonstrates strong performance on the \href{https://huggingface.co/spaces/allenai/reward-bench}{RewardBench leaderboard} \citep{lambert2024rewardbench}. The experimental details refer to \cref{experiments}.
\cref{performance}. 

% \begin{figure}[t]
% % \vskip 0.2in
% \begin{center}
% % \centerline{\includegraphics[width=0.8\columnwidth]{figs/hh/hh.pdf}}
% \centerline{\includegraphics[width=\textwidth]{figs/offline_necessity.pdf}}
% \caption{method}
% \label{method}
% \end{center}
% \vskip -0.2in
% \end{figure}

In subsequent phases, we iteratively perform data collection and preference training to continuously improve the model's performance. Specifically, we employ our inference framework and data filtering mechanism to assemble a high-quality preference dataset, which is then utilized for the next round of preference training. In each iteration, the model not only enhances its direct problem-solving capabilities but also strengthens its self-refinement ability. The synergistic evolution of both aspects ensures the efficient operation of our inference framework, while progressively improving the quality of the collected datasets, leading to a continuous performance boost and ultimately achieving a significant breakthrough in model performance.
% In subsequent phases, our algorithm primarily relies on the model's self-refinement capability to collect new datasets and train a series of models. Each successive model \( M_t \) is trained on augmented training data generated by the previous model \( M_{t-1} \). We define $\mathrm{AISR}(M_t)$ as the AI Self-Refinement data generated by \( M_t \) through our data generation process. The overall training workflow is as follows:
% \begin{itemize}
%     \item $M_0$: The base pretrained LLM with no fine-tuning. 
%     \item $M_1$: Initialized with \( M_0 \), fine-tuned on a preference dataset if available via Eq. \ref{sfft}. 
%     \item $M_2$: Initialized with \( M_1 \), then trained with $ \mathrm{AISR}(M_1)$ data via Eq. \ref{loss}.  
%     \item $M_3$: Initialized with \( M_2 \), then trained with $ \mathrm{AISR}(M_2) $  via Eq. \ref{loss}. 
% \end{itemize} 

% In summary, we outline our framework as follows: For any given domain, if an offline preference dataset is available, we use the loss function Eq. \ref{sfft} to enhance the model's direct response generation ability as well as its self-refinement capability. After training, we leverage the model's self-refinement ability to iteratively optimize its responses. Finally, we use an LLM-as-a-Judge mechanism to select the best and worst responses to construct a new preference dataset, which forms the foundation for the next iteration of training.

% \vskip -0.2in
\section{Experiments}\label{experiments}
In this section, we will demonstrate the effectiveness of \methodabb{} in gradually instilling models to self-improve their responses over turns. Specifically, we will primarily investigate the following questions: 
(1) How significantly does \methodabb{} improve model performance compared to prior methods during the iterative process of data collection and training (\S\ref{exp:performance})? (2) What components contribute to the performance improvements of \methodabb{} (\S\ref{exp:ablation_study})?
(3) Can the self-refinement strategy induced by \methodabb{} generalize to problems out of the training domains (\S\ref{exp:Out-of-Domain})? By answering these questions, we aim to highlight the effectiveness, generality, and underlying mechanisms of \methodabb{}.

\paragraph{Models and Training Settings} 
% \textbf{Models and Training Settings} 
We primarily focus on preference optimization and capability analysis of \href{https://huggingface.co/meta-llama/Llama-3.1-8B}{Llama-3.1-8B Base} \citep{dubey2024llama}. During the SFT phase, we fine-tune \href{https://huggingface.co/meta-llama/Llama-3.1-8B}{Llama-3.1-8B Base} on the \href{https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-70b-preference-mixture}{llama-3.1-tulu-3-70b-preference-mixture} dataset \citep{lambert2024t} using Eq. \ref{aries-sft}, yielding the \methodabb{}-SFT model. Subsequently, we use the first 30K preference data from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback} dataset \citep{cui2023ultrafeedback} and apply preference training on top of the \methodabb{}-SFT model, guided by Eq. \ref{loss}, resulting in the \methodabb{} \textit{offline} model. Next, we extract a new 5K prompt dataset from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback} dataset and generate responses by iterating 4 times per prompt using the \methodabb{} \textit{offline} model and our inference framework, forming a dataset \( \mathcal{D} = \{x^{(i)}, y_1^{(i)}, y_2^{(i)}, y_3^{(i)}, y_4^{(i)}\} \). After applying a data filtering mechanism, we construct a new preference dataset, which is used for the next round of preference training to obtain the \methodabb{} \textit{iter}1 model. In the data filtering process, for the Rule-Based Selection mechanism, we directly select the response $y_1$ from round 0 as the rejected response, while empirically treating the response after 3 rounds of self-refinement, i.e., $y_4$, as the chosen response. For the Reward Model Scoring mechanism, we employ \href{https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}{Skywork/Skywork-Reward-Llama-3.1-8B-v0.2} \citep{liu2024skywork} to score the generated responses, selecting the highest-scoring response as the chosen response and the lowest-scoring response as the rejected response. Finally, we extract another 10K prompt dataset from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback} dataset, repeat the data generation and preference training process, producing the \methodabb{} \textit{iter}2 model. For convenience, we denote the results trained with the Rule-Based Selection dataset filtering mechanism as \textbf{\methodabb{}}, and the results trained with the Reward Model Scoring mechanism as \textbf{\methodabb{}+RM} in the following experiment.

% \textbf{Baselines}  
\paragraph{Baselines}  
% To evaluate the effectiveness of our proposed preference optimization approach, we compare it with other preference optimization methods. Specifically, we adopt Iterative DPO \citep{snorkel2024,xu2023some} as the baseline for model training. This method leverages the BoN sampling mechanism to iteratively generate data, followed by data filtering to construct a preference dataset, which is then used for next round of DPO training to achieve a stronger model. We reproduce two data filtering mechanisms: \textit{LLM-as-judge} and Reward Model Scoring.  
% For the \textit{LLM-as-judge} mechanism, we employ the \href{https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct}{Llama-3.1-70B-Instruct} model as the judge. For the Reward Model Scoring mechanism, we utilize \href{https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}{Skywork/Skywork-Reward-Llama-3.1-8B-v0.2} to score the generated responses and construct the preference dataset by selecting the highest- and lowest-scoring responses. For a fair comparison, we follow the same training procedure as our method. Specifically, we first perform supervised fine-tuning on the \href{https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-70b-preference-mixture}{llama-3.1-tulu-3-70b-preference-mixture} dataset, then use the first 30K samples from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback} dataset for offline training, and finally conduct two rounds of data collection and iterative training.
To evaluate the effectiveness of our proposed preference optimization approach, we compare it with other preference optimization methods. Specifically, we replicate Self-Rewarding \citep{yuan2024self} and Iterative DPO \citep{snorkel2024,xu2023some,xiong2023iterative,dong2024rlhf} as the baseline. These methods leverage the parallel sampling mechanism to iteratively generate data, followed by data filtering to construct a preference dataset, which is then used for the next round of training. Self-Rewarding employs an \textit{LLM-as-a-Judge} mechanism \citep{zheng2023judging} for data filtering, while Iterative DPO relies on reward model scoring for selection.
 Similarly to our approach, for the reward model scoring mechanism, we utilize \href{https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}{Skywork/Skywork-Reward-Llama-3.1-8B-v0.2} to score the generated responses and construct the preference dataset. For a fair comparison, we follow the same training procedure as our method. Both Self-Rewarding and Iterative DPO conduct two rounds of online preference training based on the DPO \textit{offline} model.
 % Specifically, we first perform supervised fine-tuning on the \href{https://huggingface.co/datasets/allenai/llama-3.1-tulu-3-70b-preference-mixture}{llama-3.1-tulu-3-70b-preference-mixture} dataset, then use the first 30K samples from the \href{https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback_binarized}{UltraFeedback} dataset for offline training, and finally conduct two rounds of data collection and iterative training.

% \textbf{Evaluation Benchmarks} 
\paragraph{Evaluation Benchmarks}
We evaluate our models using three most popular open-ended instruction-following benchmarks: AlpacaEval 2 \citep{li2023alpacaeval}, MT-Bench \citep{zheng2023judging}, and Arena-Hard \citep{li2024live}, along with two cross-domain mathematical reasoning tasks: GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring}.  For AlpacaEval 2, we provide the raw win rate (WR) and length-controlled win rate (LC) \citep{dubois2024length}. For MT-Bench, we report the average score using GPT-4 and GPT-4-Preview-1106 as judges. For Arena-Hard, we report the win rate relative to the baseline model. For GSM8K and MATH, we use the Step-DPO evaluation script.  Specifically, we evaluate the entire GSM8K test set, which contains 1319 math problems, and for MATH, we evaluate the first 1024 samples of the test set. Since MT-Bench is a multi-turn dialogue dataset, all evaluations, except for MT-Bench, assess both direct response generation (labeled as "\textbf{Direct}") and responses refined through 3 rounds of self-refinement using our inference framework (denoted as "\textbf{SR}").

% \subsection{Does \methodabb{} enhance performance over multiple turns compared to other approaches?}

% \subsection{Boost in Model Performance with \methodabb{} during Iterative Training}\label{exp:performance}


\subsection{Boost in Model Performance with \methodabb{}}\label{exp:performance}

\begin{figure*}[t]
% \vskip -0.1in
\centering
\subfigure[\label{performance:inference}]{\includegraphics[width=0.4\textwidth]{figs/inference_new.pdf}}
\subfigure[\label{performance:iterative_training_1}]{\includegraphics[width=0.4\textwidth]{figs/iterative_training_1_new.pdf}}
\subfigure[\label{performance:iterative_training_2}]{\includegraphics[width=0.4\textwidth]{figs/iterative_training_2_new.pdf}}
\subfigure[\label{performance:hist}]{\includegraphics[width=0.4\textwidth]{figs/inference_hist_new.pdf}}
\vspace{-8pt}
% \vskip -0.1in
\caption{Qualitative analysis of various methods on the UltraFeedback test set. We use the reward model \href{https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}{Skywork/Skywork-Reward-Llama-3.1-8B-v0.2} \citep{liu2024skywork} for scoring, which is a relatively small model but demonstrates strong performance on the \href{https://huggingface.co/spaces/allenai/reward-bench}{RewardBench leaderboard} \citep{lambert2024rewardbench}.  (a) shows the variation in model performance as the number of inference turns increases. (b) illustrates the performance gains of different methods during the iterative training process without external supervision signals. (c) depicts the performance improvement during the iterative training process with Reward Model Scoring. (d) demonstrates the performance improvement after 3 rounds of self-refinement.}
\label{performance}
% \vskip -0.1in
\end{figure*}

\begin{table*}[t]
    % \vskip -0.1in
    \centering
    \caption{Results on AlpacaEval 2 \citep{li2023alpacaeval}, Arena-Hard \citep{li2024live}, and MT-Bench \citep{zheng2023judging} under the \textbf{Direct} and \textbf{SR} settings. Here, \textbf{Direct} denotes direct response generation, while \textbf{SR} indicates three rounds of self-refinement on the responses. LC and WR represent length-controlled win rate and raw win rate, respectively.}
    \vskip 0.15in
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccccc}
        \toprule
        \multirow{3}{*}{\textbf{Method}} & 
        \multicolumn{5}{c}{\textbf{Direct}} & 
        \multicolumn{3}{c}{\textbf{Self-Refinement (SR)}} \\
        \cmidrule(lr){2-6} \cmidrule(lr){7-9}
        & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} & \multicolumn{2}{c}{\textbf{MT-Bench}} 
        & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} \\
        \cmidrule(lr){2-9}  
        & LC (\%) & WR (\%) & WR (\%) & GPT-4 Turbo & GPT-4   
        & LC (\%) & WR (\%) & WR (\%) \\
        \midrule
        SFT     & 15.9 & 12.7 & 12.7 & 6.4 & 6.9 & 13.8 & 8.1 & 8.0             \\
        DPO \textit{ offline}     & 17.9 & 16.7 & 16.5 & 6.9 & 7.4 & 18.3 & 12.6 & 12.6         \\
        Self-Rewarding \textit{ iter}1   & 19.3 & 17.2 & 14.2 & 6.9 & 7.5 & 19.3 & 12.6 & 11.6         \\
        Self-Rewarding \textit{ iter}2   & 18.2 & 14.2 & 15.9 & 6.7 & 7.4 & 19.0 & 11.2 & 11.0         \\
        Iterative DPO \textit{ iter}1    & 24.6 & 22.3 & 22.4 & 6.9 & 7.6 & 25.1 & 16.9 & 17.1         \\
        Iterative DPO \textit{ iter}2    & 34.1 & 33.5 & 29.6 & 7.1 & 7.8 & 34.5 & 27.8 & 23.7         \\
        \midrule
        \methodabb{}-SFT      & 15.9 & 15.5 & 16.5 & 6.4 & 7.0 & 20.0 & 18.6 & 18.0         \\
        \methodabb{} \textit{ offline}    & 19.1 & 18.6 & 17.4 & 7.0 & 7.6 & 28.8 & 27.1 & 23.5         \\
        \methodabb{} \textit{ iter}1    & 23.9 & 24.5 & 22.0 & 7.1 & 7.5 & 37.4 & 35.7 & 31.1         \\
        \methodabb{} \textit{ iter}2    & 28.4 & 29.7 & 24.9 & 7.1 & 7.7 & 41.3 & 39.5 & 32.0         \\
        \methodabb{}+RM \textit{ iter}1    & 32.7 & 33.5 & 31.9 & 7.3 & 7.7 & 50.2 & 49.9 & 37.5         \\
        \methodabb{}+RM \textit{ iter}2    & \textbf{45.0} & \textbf{46.8} & \textbf{38.0} & \textbf{7.7} & \textbf{8.1} & \textbf{62.3} & \textbf{63.3} & \textbf{50.3}         \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:performance}
    % \vskip -0.2in
\end{table*}

\begin{figure*}[t]
% \vskip -0.05in
\centering
\includegraphics[width=\textwidth]{figs/ablation_dpo.pdf}
% \vskip -0.2in
\caption{Impact analysis of DPO Loss on \methodabb{} Performance under the Reward Model Scoring mechanism.}
\label{ablation:dpo}
\vskip 0.15in
\includegraphics[width=\textwidth]{figs/ablation_ps.pdf}
% \vskip -0.2in
\caption{Effect of Parallel Sampling (PS) vs. Self-Refinement in preference dataset construction on \methodabb{} Performance under the Reward Model Scoring mechanism.}
\label{ablation:ps}
% \vskip -0.25in
\end{figure*}

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.9\textwidth]{figs/ablation_dpo.pdf}
% \vskip -0.1in
% \caption{Impact analysis of DPO Loss on \methodabb{} Performance under the Reward Model Scoring mechanism.}
% \label{ablation:dpo}
% \vskip -0.1in
% \end{figure*}

% \begin{figure*}[ht]
% \centering
% \includegraphics[width=0.9\textwidth]{figs/ablation_ms.pdf}
% \vskip -0.1in
% \caption{Effect of Parallel Sampling vs. Self-Refinement in preference dataset construction on \methodabb{} Performance under the Reward Model Scoring mechanism.}
% \label{ablation:ps}
% \vskip -0.1in
% \end{figure*}

We first evaluate the performance improvements on the UltraFeedback test set, focusing on both the inference phase and the iterative training process, as shown in \cref{performance}. In \cref{performance:inference}, we observe that the models trained with \methodabb{} consistently demonstrate gradual improvements during inference, while models trained by self-rewarding and iterative DPO struggle to enhance response quality. This highlights that \methodabb{} effectively stimulates the self-refinement capability of LLMs. In multi-round interactions, the models trained with \methodabb{} achieve incremental performance gains. \cref{performance:iterative_training_1} and \cref{performance:iterative_training_2} show that, compared to baseline methods, \methodabb{} significantly enhances the model's ability to directly answer questions and self-refine previous responses, even without external reward signals. This demonstrates the superior efficacy of \methodabb{} in improving overall model performance. Notably, while models trained with other methods exhibit a decline in self-refinement, we find that the models' self-refinement abilities positively correlate with their base performance in direct question answering. As iterative training progresses, models trained with alternative methods also show slow improvements in self-refinement. Finally, in \cref{performance:hist}, we find that under the Reward Model Scoring mechanism, \methodabb{}'s self-refinement gain improves progressively during the iterative training process, compared to direct response generation.  This indicates that \methodabb{} enables the models to gradually master this cognitive mode. In contrast, under the Rule-Based Selection mechanism, the self-refinement gains of \methodabb{} \textit{iter}2 are lower than those of \methodabb{} \textit{iter}1. This decline can be attributed to the limitations of rule-based data filtering, which inevitably introduces noise into the collected dataset, ultimately impacting the model's performance.
% we observe that the benefits of self-refinement increase steadily throughout the iterative training process. This indicates that our framework helps the model gradually master this cognitive mode, fostering a mutually reinforcing relationship between training and inference. 
% Ultimately, our approach enables the model to acquire self-refinement skills, which in turn enhance its overall performance.

Subsequently, we evaluate the model performance on AlpacaEval 2, Arena-Hard, and MT-Bench, as demonstrated in \cref{tab:performance}. Our method not only significantly improves model performance under the "\textbf{Direct}" setting but also achieves a qualitative leap in performance under the "\textbf{SR}" setting. In contrast, other baseline methods exhibit varying degrees of performance degradation after applying self-refinement. 
Under the "\textbf{Direct}" setting, \methodabb{}+RM \textit{iter}2 outperforms iterative DPO \textit{iter}2 by 10.9\% in LC win rate and 13.3\% in raw win rate on AlpacaEval 2, while achieving an 8.4\% improvement on Arena-Hard. In the "\textbf{SR}" setting, \methodabb{}+RM \textit{iter}2 shows a 27.8\% improvement in LC win rate and a 35.5\% improvement in raw win rate on AlpacaEval 2, with a 26.6\% gain on Arena-Hard. With self-refinement, \methodabb{}+RM \textit{iter}2 reaches performance levels comparable to GPT-4 on these benchmarks.

% \subsection{Components Driving Performance Improvements in \methodabb{}}
\subsection{Ablation Studies of \methodabb{}}\label{exp:ablation_study}

Here, we examine the impact of two key aspects on \methodabb{}: (1) whether the training algorithm includes DPO loss (i.e., $\alpha=0$), and (2) replacing the self-refinement sequential generation strategy with Parallel Sampling (PS) for data collection during the preference dataset construction process. These results are presented in \cref{ablation:dpo} and \cref{ablation:ps}.

In \cref{ablation:dpo}, we observe that omitting DPO loss progressively affects the performance of \methodabb{}. As iterative training advances, the absence of DPO loss leads to a slower rate of improvement in both "\textbf{Direct}" and "\textbf{SR}" results. For example, the performance of direct answers decreases by 6.7\% LC win rate, while self-refined answers drop by 7.4\% LC win rate in AlpacaEval 2. We attribute this decline to the critical role of DPO loss in improving the model's capacity to generate higher-quality answers directly. Without DPO loss, the performance of direct response generation deteriorates, which in turn negatively impacts the performance of self-refinement based on these suboptimal responses.

In \cref{ablation:ps}, we find that models trained on the dataset collected using the PS strategy exhibit lower overall performance compared to those trained on datasets collected with the self-refinement strategy. This suggests that, when coupled with the self-refinement strategy, \methodabb{} is more effective in collecting high-quality datasets. Further details can be found in \cref{appendix:examples}, where we demonstrate that self-refinement enhances the model's responses, improving logical coherence and resulting in clearer, more concise expressions.

\subsection{Generalization of \methodabb{} in Reasoning Tasks}\label{exp:Out-of-Domain}

\begin{table}[t]
    % \vskip -0.05in
    \centering
    \caption{Accuracy of different methods on GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring} tasks using \textbf{Direct} and \textbf{SR} generation strategies.}
    \vskip 0.15in
    \resizebox{0.45\textwidth}{!}{ % Resize the table to the width of the page
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & 
        \multicolumn{2}{c}{\textbf{GSM8K}(\%)} & 
        \multicolumn{2}{c}{\textbf{MATH}(\%)} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & {{\textbf{Direct}}} & {\textbf{SR}} & {{\textbf{Direct}}} & {\textbf{SR}} \\
        \midrule
        SFT     & 60.9  & 62.3 & 18.2 & 29.5           \\
        DPO \textit{ offline}     & 66.3 & 66.6 & 35.1 & 34.6          \\
        Self-Rewarding \textit{ iter}1   & 65.4 & 66.2 & 35.0 & 34.6          \\
        Self-Rewarding \textit{ iter}2   & 66.3 & 66.5 & 35.3 & 34.7         \\
        Iterative DPO \textit{ iter}1    & 68.5 & 67.9 & 36.6 & 35.6          \\
        Iterative DPO \textit{ iter}2    & 68.4 & 67.2 & 36.7 & 36.3          \\
        \midrule
        \methodabb{}-SFT      & 64.3 & 70.9 & 32.8 & 45.5          \\
        \methodabb{} \textit{ offline}    & 68.3 & 72.2 & 38.3 & 42.4         \\
        \methodabb{} \textit{ iter}1    & 68.2 & 71.2 & 39.9 & 45.3          \\
        \methodabb{} \textit{ iter}2    & 69.1 & 71.2 & 39.5 & 43.9          \\
        \methodabb{}+RM \textit{ iter}1    & 70.2 & 71.7 & 46.1 & 47.8          \\
        \methodabb{}+RM \textit{ iter}2    & \textbf{71.9} & \textbf{73.6} & \textbf{48.7} & \textbf{50.1}      \\
        \bottomrule
    \end{tabular}
    }
    \label{tab:math}
    \vskip -0.15in
\end{table}

We further access the generalization of various methods on the GSM8K and MATH, as shown in \cref{tab:math}. As iterative preference training progresses, although all methods gradually improve mathematical reasoning accuracy, \methodabb{} significantly outperforms the baseline approaches. By employing the self-refinement strategy, \methodabb{} achieves 73.6\% accuracy on GSM8K, a 1.7\% improvement over direct response generation, and 50.1\% on MATH, a 1.4\% improvement. These improvements stem from the self-refinement strategy, which enhances reasoning depth and maintains logical consistency during inference. 
% Notably, the improvements of both SFT and \methodabb{}-SFT models on the MATH task are particularly pronounced after applying the self-refinement generation strategy. We speculate that for relatively simpler problems in MATH, the models inherently possess the ability to solve them but make errors in the initial attempt. By leveraging the previous response as a prior, the models can clarify the problem-solving logic, resulting in a significant performance leap.

However, as the model's accuracy on the GSM8K and MATH tasks reaches a certain level, the effect of self-refinement begins to plateau. We attribute this to the lack of domain-specific training, preventing the model from mastering certain problem-solving strategies. This observation has prompted us to plan further domain-specific training for reasoning tasks,  to gain deeper insights into the enhancement of the model's reasoning capabilities.

% \begin{table}[h]
%     \centering
%     \caption{Accuracy of different methods on GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring} tasks using \textbf{Direct} and \textbf{SR} generation strategies.}
%     \begin{tabular}{lcccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & 
%         \multicolumn{2}{c}{\textbf{GSM8K}(\%)} & 
%         \multicolumn{2}{c}{\textbf{MATH}(\%)} \\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-5}
%         & {{Direct}} & {SR} & {{Direct}} & {SR} \\
%         \midrule
%         \methodabb{}+RM \textit{ iter}1 w/ MS    & 69.7 & 71.8 & 44.4 & 51.4          \\
%         \methodabb{}+RM \textit{ iter}2 w/ MS    & 70.9 & 72.0 & 48.5 & 51.2      \\
%         \methodabb{}+RM \textit{ iter}1    & 70.2 & 71.7 & 46.1 & 47.8          \\
%         \methodabb{}+RM \textit{ iter}2    & \textbf{71.9} & \textbf{73.6} & \textbf{48.7} & \textbf{50.1}      \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:performance}
% \end{table}

\section{Conclusion}
In this paper, we introduce \methodabb{}, an innovative framework for iterative preference training and self-refinement-based inference. During the training phase, \methodabb{} enhances the model's ability to directly answer questions while simultaneously stimulating and strengthening the self-refinement capability. In the inference phase, \methodabb{} leverages the self-refinement ability activated in the model to perform multi-step sequential inference, generating a series of self-refined outputs. These outputs are then filtered using either Rule-Based Selection or Reward Model Scoring mechanism to construct a preference dataset, which is used for the next round of preference training. The training and inference phases of \methodabb{} are mutually reinforcing, collectively enhancing the performance of LLMs. Experimental results demonstrate the exceptional performance of \methodabb{}, surpassing even GPT-4 on benchmarks such as AlpacaEval 2 and Arena-Hard. In the future, we will explore the application of our approach to reasoning tasks and continue to extend the existing framework.

% In summary, we summarize our framework as follows: For any given domain, if there is an existing offline preference dataset, we use our loss function, as shown in Eq. \ref{loss}, to enhance the model's direct expressive ability and corresponding self-refinement ability in that domain. After training, we leverage the model's self-refinement ability to iteratively optimize its responses. Finally, we use LLM-as-a-Judge to select the best and worst responses to form a new preference dataset, thereby starting a new iteration.

\newpage

% Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}


\section*{Impact Statement}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{reference}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{Mathematical Derivations}
\subsection{The optimal solution to the self-refinement objective in the preference training phase}\label{A_1}
In this Appendix, we aim to derive the loss function corresponding to the following objective:
\begin{align}
    \max_{\pi}\ \mathbb{E}_{\substack{y_{2}\sim\pi(\cdot|x, y_{1},z)
    % \\y_{1}\sim \mu(x)
    }}\bigg[p(y_{2}\succ y_{1}|x)-\beta D_{\mathrm{KL}}(\pi||\pi_{\mathrm{ref}}|x,y_{1},z)\bigg].
\end{align}
The derivation process follows a structure similar to SRPO \citep{choi2024self}. First, we can obtain the optimal solution of the objective:
\begin{align}
&\max_{\pi}\ \mathbb{E}_{\substack{y_{2}\sim\pi(\cdot|x, y_{1},z)
    % \\y_{1}\sim \mu(x)
    }}\bigg[p(y_{2}\succ y_{1}|x)-\beta D_{\mathrm{KL}}(\pi||\pi_{\mathrm{ref}}|x,y_{1},z)\bigg] \\
    = &\max_{\pi}\ \mathbb{E}_{\substack{y_{2}\sim\pi(\cdot|x, y_{1},z)
    }}\bigg[p(y_{2}\succ y_{1}|x)-\beta \log \frac{\pi(y_2|x, y_1,z)}{\pi_{\mathrm{ref}}(y_2|x, y_1,z)}\bigg] \\
    =&\max_{\pi}\ \beta\mathbb{E}_{\substack{y_{2}\sim\pi(\cdot|x, y_{1},z)
    }}\bigg[-\log \frac{\pi(y_2|x, y_1,z)}{\pi_{\mathrm{ref}}(y_2|x, y_1,z)\exp{\left(\frac{p(y_{2}\succ y_{1}|x)}{\beta}\right)}}\bigg] \\
    =&\max_{\pi}\ -\beta\mathbb{E}_{\substack{y_{2}\sim\pi(\cdot|x, y_{1},z)
    }}\bigg[\log \frac{\pi(y_2|x, y_1,z)Z(x, y_1,z)}{\pi_{\mathrm{ref}}(y_2|x, y_1,z)\exp{\left(\frac{p(y_{2}\succ y_{1}|x)}{\beta}\right)}}\bigg] + \beta\log Z(x, y_1,z) \\
    =&\max_{\pi}\ -\beta D_{\mathrm{KL}}\left(\pi(y_2|x, y_1,z)\bigg\|\frac{\pi_{\mathrm{ref}}(y_2|x, y_1,z)\exp{\left(\frac{p(y_{2}\succ y_{1}|x)}{\beta}\right)}}{Z(x, y_1,z)}\right) + \beta\log Z(x, y_1,z)
\end{align}
where $Z(x,y_1,z)$ is the partition function. Considering the non-negativity of the KL divergence, the optimal solution is:
\begin{align}
    \pi^*(y_2|x, y_{1},z)=\frac{\pi_{\mathrm{ref}}(y_2|x, y_{1},z)\exp\left(\frac{p(y_2\succ y_1|x)}{\beta}\right)}{Z(x,y_1,z)},
    \label{optimal_policy}
\end{align}

For convenience in subsequent derivations, we reorganize the representation of Eq. \ref{optimal_policy} as follows:
\begin{align}
    p(y_{2}\succ y_1|x) = \beta \log \frac{\pi^*(y_2| x, y_{1},z)}{\pi_\mathrm{ref}(y_2| x, y_{1},z)}  + \beta \log Z(x, y_1,z) \label{preference}
\end{align}
Noting that \(p(y_{1} \succ y_{1} | x) = 1/2\), we derive the following expression:
\begin{align}
    \frac{1}{2} = \beta \log \frac{\pi^*(y_1|x, y_{1},z)}{\pi_\mathrm{ref}(y_1|x, y_{1},z)}  + \beta \log Z(x, y_1,z) \label{1_2}
\end{align}
Subtracting Eq. \ref{1_2} from Eq. \ref{preference}, we obtain the following expression:
\begin{align}
    p(y_{2}\succ y_1|x) - \frac{1}{2} =\beta \log \frac{\pi^*(y_2|x, y_{1},z))}{\pi_\mathrm{ref}(y_2|x, y_{1},z))} - \beta \log \frac{\pi^*(y_1|x, y_{1}, z))}{\pi_\mathrm{ref}(y_1|x, y_{1}, z))} \label{preference_1_2}
\end{align}
According to Eq. \ref{preference_1_2}, we adopt the mean squared error as the loss function and parametrize the policy model as $\pi_{\theta}$, while iterating over all prompts $x$ and responses $y_1$, $y_2$, which leads to:
\begin{equation}
\begin{aligned}
    \mathcal{L}(\pi_{\theta};\pi_{\mathrm{ref}}) = \underset{\substack{(x, y_{1}, y_{2})\sim \rho}}{\mathbb{E}} 
    \bigg[p(y_{2}\succ y_1|x) - 
        \frac{1}{2} - \beta \bigg[
        \log\left(\frac{\pi_{\theta}(y_2|x, y_{1},z)}{\pi_{\mathrm{ref}}(y_2|x, y_{1},z)}\right) 
        - \log\left(\frac{\pi_{\theta}(y_{1}|x, y_{1},z)}{\pi_{\mathrm{ref}}(y_{1}|x, y_{1},z)}\right)
        \bigg]
    \bigg]^2,
\end{aligned}
\end{equation}
where $\rho$ represents the true distribution.

\clearpage
\section{Implementation Details}
\subsection{Self-Refinement Template}
The self-refinement template used in this paper is as follows:
% \begin{lstlisting}
% Below is a QUESTION from a user and an EXAMPLE RESPONSE.
% Please provide a more helpful RESPONSE, improving the EXAMPLE RESPONSE by making the content even clearer, more accurate, and concise. Focus on addressing the human's QUESTION without including irrelevant sentences.
% Your RESPONSE should not only be well-written, logical, and easy-to-follow, but also demonstrate expert-level insight, engaging the reader with the most relevant information.

% QUESTION:
% {Question}

% EXAMPLE RESPONSE:
% {Example_Response}

% Now, refine and improve the RESPONSE further. You can consider two approaches:
% 1. REFINEMENT: If the EXAMPLE RESPONSE is sufficient and addresses most of the QUESTION's concerns, enhance clarity, accuracy, or conciseness as needed.
% 2. NEW RESPONSE: If the EXAMPLE RESPONSE lacks clarity or relevance to the QUESTION, craft a more effective RESPONSE that thoroughly resolves the QUESTION.

% Do not include analysis—just give the improved RESPONSE.

% RESPONSE:

% \end{lstlisting}

\begin{myminted}{Self-Refinement Template}
Below is a QUESTION from a user and an EXAMPLE RESPONSE.

Please provide a more helpful RESPONSE, improving the EXAMPLE RESPONSE by making the content even clearer, more accurate, and concise. Focus on addressing the human's QUESTION without including irrelevant sentences.

Your RESPONSE should not only be well-written, logical, and easy-to-follow, but also demonstrate expert-level insight, engaging the reader with the most relevant information.\\

QUESTION:\\
\textcolor{red}{\{Question\}}\\

EXAMPLE RESPONSE:\\
\textcolor{red}{\{Example\_Response\}}\\

Now, refine and improve the RESPONSE further. You can consider two approaches:

1. REFINEMENT: If the EXAMPLE RESPONSE is sufficient and addresses most of the QUESTION's concerns, enhance clarity, accuracy, or conciseness as needed.

2. NEW RESPONSE: If the EXAMPLE RESPONSE lacks clarity or relevance to the QUESTION, craft a more effective RESPONSE that thoroughly resolves the QUESTION.\\

Do not include analysis-just give the improved RESPONSE.\\

RESPONSE:


\end{myminted}

\subsection{Training Details}
In the SFT phase, we set the learning rate to \(5 \times 10^{-6}\), with a batch size of 128 and a maximum sequence length of 1024. We employed a cosine learning rate schedule with 3\% warm-up steps for 1 epoch and used the AdamW optimizer.

In the preference training phase, the learning rate was reduced to \(1 \times 10^{-6}\). Additionally, we set \(\alpha = 0.8\) and \(\beta_{\mathrm{DPO}} = 0.1\). For training with the Reward Model Scoring dataset filtering mechanism, we set \(\beta_{\mathrm{refine}} = 0.01\), while for the Rule-Based Selection mechanism, \(\beta_{\mathrm{refine}} = 0.05\). The higher value of \(\beta_{\mathrm{refine}}\) in the Rule-Based Selection process is due to the presence of noisy data in the filtered preference dataset, which requires stronger regularization.

\subsection{Inference Details}
During the iterative training and dataset collection process, we employed a sampling decoding strategy with a temperature of 0.7 for direct response generation and self-refinement. For AlpacaEval 2, we set the temperature to 0.9 for generation, while for MT-Bench and Arena-Hard, we followed the official decoding configuration. For GSM8K and MATH, we utilized a greedy decoding strategy.

\subsection{Evaluation Benchmarks Details}
AlpacaEval 2 \citep{li2023alpacaeval} consists of 805 questions from 5 datasets, MT-Bench \citep{zheng2023judging} covers 80 questions across 8 categories in a multi-turn dialogue format, and Arena-Hard \citep{li2024live} is an enhanced version of MT-Bench with 500 well-defined technical questions. GSM8K \citep{cobbe2021training} includes elementary and middle school-level math problems, while MATH \citep{hendrycks2021measuring} contains more complex questions, spanning various mathematical branches such as algebra, counting and probability, geometry, number theory, and calculus. For each benchmark, we report scores according to their respective evaluation protocols.

\clearpage
\section{Alternative Refinement Loss}\label{appendix:loss}
\subsection{The Derivation of the Refinement Loss Function from the Bradley-Terry model Perspective}
Alternatively, we can enhance the model's self-refinement capability by leveraging insights from the Bradley-Terry (BT) model theory. We define the objective function as follows:
\begin{align}
    \max_{\pi}\ \mathbb{E}_{\substack{y_{2}\sim\pi(\cdot|x,y_{1},z)
    % \\y_{1}\sim \mu(x)
    }}\bigg[r(y_2|x,y_1,z)-\beta D_{\mathrm{KL}}(\pi||\pi_{\mathrm{ref}}|x,y_{1},z)\bigg]
\end{align}
The solution process is analogous to that of \cref{A_1}, allowing us to obtain the optimal solution:
\begin{align}
    \pi^*(y_2|x, y_1,z)=\frac{\pi_{\mathrm{ref}}(y_2|x,y_1,z)\exp\left(\frac{r(y_2 |x,y_1,z)}{\beta}\right)}{Z(x, y_1,z)}, \label{policy_reward}
\end{align}
where $Z(x,y_1,z)$ is the partition function. Reorganizing the above equation, we obtain:
\begin{align}
    r(y_{2}| x, y_1,z) & = \beta \log \frac{\pi(y_2|x, y_1,z)}{\pi_\mathrm{ref}(y_2|x,y_1,z)}  + \beta \log Z(x, y_1,z)
\end{align}
The standard expression of the BT model is:
\begin{align}
    p_{\mathrm{BT}}^*(y_2 \succ y_1 |x) =\sigma(r^*(y_2|x) - r^*(y_1|x))
\end{align}

Here, to enhance the self-refinement capability of the language model, we make a slight modification. Given the problem input \( x \) for the BT model, we also provide an arbitrary response \( y_{\mathrm{opt}} \) along with a refinement template $z$, which serves as guidance for the model to generate better responses:
\begin{align}
    p_{\mathrm{BT}}^*(y_2 \succ y_1 |x, y_{\mathrm{opt}},z) =\sigma(r^*(y_2|x, y_{\mathrm{opt}},z) - r^*(y_1|x, y_{\mathrm{opt}},z)).
\end{align}

Then we define the refinement preference function:
% \begin{align}
%     \log p_{\mathrm{refine}}^*(y_2 \succ y_1 |x)&=\log p_{\mathrm{BT}}(y_2 \succ y_1 |x, y_{1}) + \log p_{\mathrm{BT}}^*(y_2 \succ y_1 |x, y_{2}) \\
% &=\log \sigma(r^*(y_2|x,y_1) - r^*(y_1|x, y_1)) + \log \sigma(r^*(y_2|x, y_2) - r^*(y_1|x, y_2))\\
%     \begin{split}
%     &=\log \sigma\left(\beta \log \frac{\pi^*(y_2|x,y_1)}{\pi_\mathrm{ref}(y_2|x,y_1)} - \beta \log \frac{\pi^*(y_1|x,y_1)}{\pi_\mathrm{ref}(y_1|x,y_1)}\right) \\
%     &+ \log \sigma\left(\beta \log \frac{\pi^*(y_2|x, y_2)}{\pi_\mathrm{ref}(y_2|x, y_2)} - \beta \log \frac{\pi^*(y_1|x, y_2)}{\pi_\mathrm{ref}r(y_1|x, y_2)}\right)
%     \end{split}
% \end{align}
\begin{align}
    p_{\mathrm{{BT}\_refine}}^*(y_2 \succ y_1 |x,z)&= p_{\mathrm{BT}}^*(y_2 \succ y_1 |x, y_{1},z)p_{\mathrm{BT}}^*(y_2 \succ y_1 |x, y_{2},z) \\
&=\sigma(r^*(y_2|x,y_1,z) - r^*(y_1|x, y_1,z))\sigma(r^*(y_2|x, y_2,z) - r^*(y_1|x, y_2,z))\\
    \begin{split}
    &=\sigma\left(\beta \log \frac{\pi^*(y_2|x,y_1,z)}{\pi_\mathrm{ref}(y_2|x,y_1,z)} - \beta \log \frac{\pi^*(y_1|x,y_1,z)}{\pi_\mathrm{ref}(y_1|x,y_1,z)}\right) \\
    &\times \sigma\left(\beta \log \frac{\pi^*(y_2|x, y_2,z)}{\pi_\mathrm{ref}(y_2|x, y_2,z)} - \beta \log \frac{\pi^*(y_1|x, y_2,z)}{\pi_\mathrm{ref}(y_1|x, y_2,z)}\right)
    \end{split}
\end{align}

Assuming access to a well-curated preference dataset \(\mathcal{D} = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N\), 
% sampled from the refinement preference distribution, 
we aim to leverage this dataset to activate the self-refinement capability of language models, thereby gradually steering the models toward generating better responses during the inference phase. To achieve this, we parametrize the policy model $\pi_{\theta}$ and estimate its parameters through maximum likelihood estimation. By treating the problem as a binary classification task, we have the negative log-likelihood loss:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\mathrm{BT\_refine}}(\pi) &= -\underset{(x, y_{w},y_{l})\sim\mathcal{D}}{\mathbb{E}}\Bigg[\log \sigma\left(\beta \log \frac{\pi(y_w|x, y_l,z)}{\pi_\mathrm{ref}(y_w|x, y_l,z)} - \beta \log \frac{\pi(y_l|x, y_l,z)}{\pi_\mathrm{ref}(y_l|x, y_l,z)}\right) \Bigg]
    \\
    &- \underset{(x, y_{w},y_{l})\sim\mathcal{D}}{\mathbb{E}} \Bigg[\log \sigma\left(\beta \log \frac{\pi(y_l|x, y_w,z)}{\pi_\mathrm{ref}(y_l|x, y_w,z)} - \beta \log \frac{\pi(y_w|x, y_w,z)}{\pi_\mathrm{ref}(y_w|x, y_w,z)}\right) \Bigg]
\end{aligned}
\end{equation}

Finally, we integrate the DPO loss with the self-refinement loss derived from the BT model perspective to obtain the \methodabb{} loss function from the BT model viewpoint:
\begin{equation}
\begin{aligned}
    \mathcal{L}_{\mathrm{BT\_\methodabb{}}}(\pi_{\theta};\pi_{\mathrm{ref}}) =(1- \alpha) \mathcal{L}_{\mathrm{DPO}}(\pi_{\theta};\pi_{\mathrm{ref}}) + \alpha\mathcal{L}_{\mathrm{BT\_refine}}(\pi_{\theta};\pi_{\mathrm{ref}}) .
\end{aligned}
\label{BT:loss}
\end{equation}

\subsection{Training Details}
During the SFT phase, BT\_\methodabb{} follows the same procedure as \methodabb{}. The primary distinction between BT\_\methodabb{} and \methodabb{} arises in the preference training phase, where we set \(\beta_\mathrm{refine} = 0.05\) in BT\_\methodabb{}.

\subsection{Experiment Results}
We compared BT\_\methodabb{} and \methodabb{} across various benchmarks, with experimental results presented in \cref{tab:BT_ARIES}. Both BT\_\methodabb{} and \methodabb{} demonstrated nearly identical performance across all benchmarks, underscoring that the strength of our approach lies not in the algorithm itself, but in the foundational principles it embodies. Specifically, it is the concept of refinement that drives the effectiveness of our method and framework, enabling them to deliver impressive results.

\begin{table*}[h]
    % \vskip -0.1in
    \centering
    \caption{Comparison of experimental results between BT\_\methodabb{} and \methodabb{} on AlpacaEval 2 \citep{li2023alpacaeval}, Arena-Hard \citep{li2024live}, and MT-Bench \citep{zheng2023judging} under the \textbf{Direct} and \textbf{SR} settings. LC and WR represent length-controlled win rate and raw win rate, respectively.}
        \vskip 0.15in
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccccc}
        \toprule
        \multirow{3}{*}{\textbf{Method}} & 
        \multicolumn{5}{c}{\textbf{Direct}} & 
        \multicolumn{3}{c}{\textbf{Self-Refinement (SR)}} \\
        \cmidrule(lr){2-6} \cmidrule(lr){7-9}
        & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} & \multicolumn{2}{c}{\textbf{MT-Bench}} 
        & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} \\
        \cmidrule(lr){2-9}  
        & LC (\%) & WR (\%) & WR (\%) & GPT-4 Turbo & GPT-4   
        & LC (\%) & WR (\%) & WR (\%) \\
        \midrule
        BT\_\methodabb{} \textit{ offline}    & 19.8  & 19.3  & 20.0  & 7.0 & 7.5  & 27.8  & 25.7  & 24.8          \\
        BT\_\methodabb{}+RM \textit{ iter}1    & 31.9 & 34.5 & 31.1  & 7.1 & 7.5 & 50.6 & 51.8 & 41.0          \\
        BT\_\methodabb{}+RM \textit{ iter}2    & \textbf{45.2} & \textbf{47.7} & \textbf{39.5}  & 7.4 & 7.7 & \textbf{66.2} & \textbf{66.6} & 49.9         \\
        \midrule
        \methodabb{} \textit{ offline}    & 19.1 & 18.6 & 17.4 & 7.0 & 7.6 & 28.8 & 27.1 & 23.5         \\
        \methodabb{}+RM \textit{ iter}1    & 32.7 & 33.5 & 31.9 & 7.3 & 7.7 & 50.2 & 49.9 & 37.5         \\
        \methodabb{}+RM \textit{ iter}2    & 45.0 & 46.8 & 38.0 & \textbf{7.7} & \textbf{8.1} & 62.3 & 63.3 & \textbf{50.3}         \\
        \bottomrule
    \end{tabular}
    }
    % \vskip -0.2in
    \label{tab:BT_ARIES}
    % \vskip -0.2in
\end{table*}

\begin{table}[h]
    % \vskip -0.05in
    \centering
    \caption{Accuracy Comparison of BT\_\methodabb{} and \methodabb{} on GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring} tasks using \textbf{Direct} and \textbf{SR} generation strategies.}
    \vskip 0.15in
    % \resizebox{\textwidth}{!}{ % Resize the table to the width of the page
    \begin{tabular}{lcccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & 
        \multicolumn{2}{c}{\textbf{GSM8K}(\%)} & 
        \multicolumn{2}{c}{\textbf{MATH}(\%)} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        & {{Direct}} & {SR} & {{Direct}} & {SR} \\
        \midrule
        BT\_\methodabb{} \textit{ offline}    & 67.6 & 71.7 & 37.7 & 44.1         \\
        BT\_\methodabb{}+RM \textit{ iter}1    & 70.3 & 73.2 & 46.5 & 47.4         \\
        BT\_\methodabb{}+RM \textit{ iter}2    & 70.1  & 71.6 & \textbf{50.2} & \textbf{52.0}      \\
        \midrule
        \methodabb{} \textit{ offline}    & 68.3 & 72.2 & 38.3 & 42.4         \\
        \methodabb{}+RM \textit{ iter}1    & 70.2 & 71.7 & 46.1 & 47.8          \\
        \methodabb{}+RM \textit{ iter}2    & \textbf{71.9} & \textbf{73.6} & 48.7 & 50.1      \\
        \bottomrule
    \end{tabular}
    % }
    \label{tab:math_BT_ARIES}
    % \vskip -0.1in
\end{table}



\clearpage
\section{Details of Ablation Studies}
The specific experimental results for \cref{ablation:dpo} and \cref{ablation:ps} are presented in \cref{tab:ablation} below.
\begin{table}[h]
    % \vskip -0.1in
    \centering
    \caption{Ablation study on AlpacaEval 2 and Arena-Hard. LC and WR represent length-controlled win rate and raw win rate, respectively.}
        \vskip 0.15in
    % \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        \multirow{3}{*}{\textbf{Method}} & 
        \multicolumn{3}{c}{\textbf{Direct}} & 
        \multicolumn{3}{c}{\textbf{Self-Refinement (SR)}} \\
        \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard}
        & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} \\
        \cmidrule(lr){2-7}  
        & LC (\%) & WR (\%) & WR (\%) & LC (\%) & WR (\%) & WR (\%) \\
        \midrule
        \methodabb{}+RM \textit{ iter}1 using PS    & 28.7 & 29.7 & 29.8 & 41.0 & 42.5 & 35.5         \\
        \methodabb{}+RM \textit{ iter}2 using PS    & 40.7 & 42.7 & 37.1 & 59.4 & 60.6 & 49.6         \\ 
        \midrule
        \methodabb{} \textit{ offline} w/o DPO    & 19.3 & 19.2 & 18.3 & 25.9 & 24.8 & 25.0 \\
        \methodabb{}+RM \textit{ iter}1 w/o DPO    & 31.4 & 33.0 & 29.6 & 47.4 & 49.2 & 39.5         \\
        \methodabb{}+RM \textit{ iter}2 w/o DPO    & 38.3 & 41.3 & 36.0 & 54.9 & 56.3 & 48.6         \\
        \midrule
        \methodabb{} \textit{ offline}    & 19.1 & 18.6 & 17.4 & 28.8 & 27.1 & 23.5         \\  
        \methodabb{}+RM \textit{ iter}1    & 32.7 & 33.5 & 31.9 & 50.2 & 49.9 & 37.5         \\
        \methodabb{}+RM \textit{ iter}2    & \textbf{45.0} & \textbf{46.8} & \textbf{38.0} & \textbf{62.3} & \textbf{63.3} & \textbf{50.3}         \\
        \bottomrule
    \end{tabular}
    % }
    \label{tab:ablation}
\end{table}

% \begin{table}[h]
%     \centering
%     \caption{Ablation study on AlpacaEval 2 and Arena-Hard. LC and WR represent length-controlled win rate and raw win rate, respectively.}
%     % \resizebox{\textwidth}{!}{
%     \begin{tabular}{lcccccc}
%         \toprule
%         \multirow{3}{*}{\textbf{Method}} & 
%         \multicolumn{3}{c}{\textbf{Direct}} & 
%         \multicolumn{3}{c}{\textbf{Self-Refinement (SR)}} \\
%         \cmidrule(lr){2-4} \cmidrule(lr){5-7}
%         & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard}
%         & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} \\
%         \cmidrule(lr){2-7}  
%         & LC (\%) & WR (\%) & WR (\%) & LC (\%) & WR (\%) & WR (\%) \\
%         \midrule
%         \methodabb{}+RM \textit{ iter}1 w/ MS    & 28.7 & 29.7 & 29.8 & 41.0 & 42.5 & 35.5         \\
%         \methodabb{}+RM \textit{ iter}2 w/ MS    & 40.7 & 42.7 & 37.1 & 59.4 & 60.6 & 49.6         \\ 
%         \methodabb{}+RM \textit{ iter}1    & 32.7 & 33.5 & 31.9 & 50.2 & 49.9 & 37.5         \\
%         \methodabb{}+RM \textit{ iter}2    & \textbf{45.0} & \textbf{46.8} & \textbf{38.0} & \textbf{62.3} & \textbf{63.3} & \textbf{50.3}         \\
%         \bottomrule
%     \end{tabular}
%     % }
%     \label{tab:performance}
% \end{table}

% \begin{table}[h]
%     \centering
%     \caption{Accuracy of different methods on GSM8K \citep{cobbe2021training} and MATH \citep{hendrycks2021measuring} tasks using \textbf{Direct} and \textbf{SR} generation strategies.}
%     \begin{tabular}{lcccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & 
%         \multicolumn{2}{c}{\textbf{GSM8K}(\%)} & 
%         \multicolumn{2}{c}{\textbf{MATH}(\%)} \\
%         \cmidrule(lr){2-3} \cmidrule(lr){4-5}
%         & {{Direct}} & {SR} & {{Direct}} & {SR} \\
%         \midrule
%         \methodabb{}+RM \textit{ iter}1 using PS    & 69.7 & 71.8 & 44.4 & \textbf{51.4}          \\
%         \methodabb{}+RM \textit{ iter}2 using PS    & 70.9 & 72.0 & 48.5 & 51.2      \\
%         \midrule
%         \methodabb{} \textit{ offline} w/o DPO    & 68.2 & 72.8 & 36.4 & 44.7         \\
%         \methodabb{}+RM \textit{ iter}1 w/o DPO    & 68.5 & 72.8 & 46.8 & 49.1          \\
%         \methodabb{}+RM \textit{ iter}2 w/o DPO    & 69.5 & 71.9 & 48.5 & 49.7      \\
%         \midrule
%         \methodabb{} \textit{ offline}    & 68.3 & 72.2 & 38.3 & 42.4         \\
%         \methodabb{}+RM \textit{ iter}1    & 70.2 & 71.7 & 46.1 & 47.8          \\
%         \methodabb{}+RM \textit{ iter}2    & \textbf{71.9} & \textbf{73.6} & \textbf{48.7} & 50.1      \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:math_ablation}
% \end{table}

\clearpage
\section{Evaluation of Self-Refinement Capabilities in Open-Source Models}
In this section, we evaluate the self-refinement capabilities of several prominent open-source models. To provide a comprehensive assessment, we examine the performance of the Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, and Gemma-2-9B-Instruct models across three different refinement templates. The templates employed in this evaluation are as follows:
\begin{myminted}{Refinement Template 1}
Below is a QUESTION from a user and an EXAMPLE RESPONSE.

Please provide a more helpful RESPONSE, improving the EXAMPLE RESPONSE by making the content even clearer, more accurate, and concise. Focus on addressing the human's QUESTION without including irrelevant sentences.

Your RESPONSE should not only be well-written, logical, and easy-to-follow, but also demonstrate expert-level insight, engaging the reader with the most relevant information.\\

QUESTION:\\
\textcolor{red}{\{Question\}}\\

EXAMPLE RESPONSE:\\
\textcolor{red}{\{Example\_Response\}}\\

Now, refine and improve the RESPONSE further. You can consider two approaches:

1. REFINEMENT: If the EXAMPLE RESPONSE is sufficient and addresses most of the QUESTION's concerns, enhance clarity, accuracy, or conciseness as needed.

2. NEW RESPONSE: If the EXAMPLE RESPONSE lacks clarity or relevance to the QUESTION, craft a more effective RESPONSE that thoroughly resolves the QUESTION.\\

Do not include analysis-just give the improved RESPONSE.\\

RESPONSE:


\end{myminted}

\begin{myminted}{Refinement Template 2}
Below is a QUESTION from a user and an EXAMPLE RESPONSE.

Please provide a more helpful RESPONSE, improving the EXAMPLE RESPONSE by making the content even clearer, more accurate, and concise. Focus on addressing the human's QUESTION without including irrelevant sentences.

Your RESPONSE should not only be well-written, logical, and easy-to-follow, but also demonstrate expert-level insight, engaging the reader with the most relevant information.\\

QUESTION:\\
\textcolor{red}{\{Question\}}\\

EXAMPLE RESPONSE:\\
\textcolor{red}{\{Example\_Response\}}\\

Now, refine and improve the RESPONSE further. You can consider two approaches:

1. REFINEMENT: If the EXAMPLE RESPONSE is sufficient and addresses most of the QUESTION's concerns, enhance clarity, accuracy, or conciseness as needed.

2. NEW RESPONSE: If the EXAMPLE RESPONSE lacks clarity or relevance to the QUESTION, craft a more effective RESPONSE that thoroughly resolves the QUESTION.\\

Format your answer as follows:\\
ANALYSIS: <Analyze the strengths and shortcomings of the EXAMPLE RESPONSE>\\
RESPONSE: <Provide an improved response>

\end{myminted}

\begin{myminted}{Refinement Template 3}
Below is a QUESTION from a user and an EXAMPLE RESPONSE.\\
Please provide a better RESPONSE.\\

QUESTION:\\
\textcolor{red}{\{Question\}}\\

EXAMPLE RESPONSE:\\
\textcolor{red}{\{Example\_Response\}}\\

RESPONSE:

\end{myminted}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/opensource_inference.pdf}
    \vskip -0.1in
    \caption{Self-refinement Capability Evaluation of Llama-3.1-8B-Instruct, Qwen2.5-7B-Instruct, and Gemma-2-9B-Instruct across 3 kinds of refinement template. The score is assigned by the reward model \href{https://huggingface.co/Skywork/Skywork-Reward-Llama-3.1-8B-v0.2}{Skywork/Skywork-Reward-Llama-3.1-8B-v0.2} \citep{liu2024skywork}.}
    \label{fig:opensource}
\end{figure*}

The experimental results are presented in \cref{fig:opensource}. Our experiments reveal that, despite their widespread popularity, current open-source models often struggle to effectively refine their own responses, which can even lead to a degradation in performance.


% \clearpage
% \section{Generation strategies}

% \begin{itemize}
%     \item[1. ] \textbf{Zero-shot Prompting}: 
%    This strategy does not incorporate self-feedback. The model generates responses directly based on the given query. For a input \(x^{(i)}\) and the prompt template $p_{\mathrm{gen}}$, the model initially provides a response $y_1^{(i)}\sim \pi(p_{\mathrm{gen}}\|x^{(i)})$, where $\pi$ represents the LLM and $\|$ denotes concatenation. Subsequently, the model generates the response \(y_2^{(i)}\) conditioned on the prompt \(x^{(i)}\), response \(y_1^{(i)}\) and the refinement prompt template $p_{\mathrm{refine}}$, following $y_2^{(i)} \sim \pi(p_{\mathrm{refine}}\|x^{(i)}\|y_1^{(i)})$. Similarly, \(y_2^{(i)}\) is then used with \(x\) to generate \(y_3^{(i)}\), and this process continues for $N$ iterations. Ultimately, we obtain a set of progressively refined responses \(\{y_1^{(i)}, y_2^{(i)}, \cdots, y_N^{(i)}\}\).
%    \item[2. ] \textbf{Few-shot Prompting with Self-Feedback}:  
%    This strategy integrates a self-feedback mechanism, enabling the model to iteratively improve its responses with finer-grained insights. For a given input \(x^{(i)}\), the model initially generates a response $y_1^{(i)}\sim \pi(p_{\mathrm{gen}}\|x^{(i)})$. Based on \(x^{(i)}\) and \(y_1^{(i)}\), given the task-specific prompt template $p_{\mathrm{fd}}$, the model generates the feedback $fb_1^{(i)} \sim \pi(p_{\mathrm{fd}}\|x^{(i)}\|y_1^{(i)})$. Then the model starts to refine the response based on the input $x^{(i)}$, the response $y_1^{(i)}$, and the feedback $fb_1^{(i)}$, yielding the response \(y_2^{(i)}\):
%    $$
%    y_2^{(i)} \sim \pi(p_{\mathrm{refine}}\| x^{(i)} \| y_1^{(i)} \| fb_1^{(i)}),
%    $$
%     where $p_{\mathrm{refine}}$ is the refinement prompt template. This process is repeated for \(N\) iterations, with the response \(y_k^{(i)}\) at each step being conditioned on the input \(x^{(i)}\), the responses and feedback from previous steps (\(y_1^{(i)}, \cdots, y_{k-1}^{(i)}\), \(fb_1^{(i)}, \cdots, fb_{k-1}^{(i)}\)). The final response \(y_N^{(i)}\) is generated as follows: 
%     $$
%     y_N^{(i)} \sim \pi(p_{\mathrm{refine}}\| x^{(i)} \| y_1^{(i)} \| fb_1^{(i)}\|\cdots \|y_{N-1}^{(i)}\| fb_{N-1}^{(i)}).
%     $$
%     Ultimately, this iterative refinement produces a set of progressively improved responses \(\{y_1^{(i)}, y_2^{(i)}, \cdots, y_N^{(i)}\}\). 
% \end{itemize}

% \subsection{Other Experiment}

% \begin{table}[h]
%     \centering
%     \caption{Performance Comparison of Different Methods.}
%     \resizebox{\textwidth}{!}{
%     \begin{tabular}{lccccccccc}
%         \toprule
%         \multirow{3}{*}{\textbf{Method}} & 
%         \multicolumn{6}{c}{\textbf{Base}} & 
%         \multicolumn{3}{c}{\textbf{+Self-Refinement}} \\
%         \cmidrule(lr){2-7} \cmidrule(lr){8-10}
%         & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} & \multicolumn{3}{c}{\textbf{MT-Bench(GPT-4)}} 
%         & \multicolumn{2}{c}{\textbf{AlpacaEval 2}} & \textbf{Arena-Hard} \\
%         \cmidrule(lr){2-10}  
%         & LC (\%) & WR (\%) & WR (\%) & First turn & Second turn & Average   
%         & LC (\%) & WR (\%) & WR (\%) \\
%         \midrule
%         SFT     & 15.9 & 12.7 & 12.7 & 7.6 & 6.3 & 6.9 & 13.8 & 8.1 & 8.0             \\
%         DPO     & 17.9 & 16.7 & 16.5 & 8.2 & 6.7 & 7.4 & 18.3 & 12.6 & 12.6         \\
%         Self-Rewarding \textit{ iter}1   & 19.3 & 17.2 & 14.2 & 8.2 & 6.8 & 7.5 & 19.3 & 12.6 & 11.6         \\
%         Self-Rewarding \textit{ iter}2   & 18.2 & 14.2 & 15.9 & 7.9 & 6.9 & 7.4 & 19.0 & 11.2 & 11.0         \\
%         Iterative DPO \textit{ iter}1    & 21.4 & 19.7 & 21.9 & 8.1 & 6.6 & 7.4 & 20.9 & 13.6 & 14.3         \\
%         Iterative DPO \textit{ iter}2    & 27.5 & 25.5 & 27.0 & 8.0 & 6.7 & 7.4 & 24.8 & 15.9 & 19.3         \\
%         \midrule
%         SRFT      & 15.9 & 15.5 & 16.5 & 7.9 & 6.0 & 7.0 & 20.0 & 18.6 & 18.0         \\
%         \methodabb \textit{ offline}    & 19.1 & 18.6 & 17.4 & 8.5 & 6.8 & 7.6 & 28.8 & 27.1 & 23.5         \\
%         \methodabb \textit{ iter}1    & 31.6 & 33.2 & 30.9 & 8.3 & 6.6 & 7.5 & 47.8 & 42.9 & 41.6         \\
%         \methodabb \textit{ iter}2    & 42.3 & 40.5 & 32.5 & 8.0 & 7.0 & 7.5 & 52.4 & 48.4 & 43.1         \\
%         \methodabb+RM \textit{ iter}1    & 32.7 & 33.5 & 31.9 & 8.5 & 6.9 & 7.7 & 50.2 & 49.9 & 37.5         \\
%         \methodabb+RM \textit{ iter}2    & \textbf{45.0} & \textbf{46.8} & \textbf{38.0} & \textbf{8.5} & \textbf{7.6} & \textbf{8.1} & \textbf{62.3} & \textbf{63.3} & \textbf{50.3}         \\
%         \bottomrule
%     \end{tabular}
%     }
%     \label{tab:performance}
% \end{table}

% \begin{table}[h]
%     \centering
%     \caption{Performance Comparison of Different Methods.}
%     \begin{tabular}{lccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & 
%         \multicolumn{3}{c}{\textbf{MT-Bench(GPT-4 Turbo)}}\\
%         & First turn & Second turn & Average   
%        \\
%         \midrule
%         SFT     & 7.05 & 5.76 & 6.41           \\
%         DPO     & 7.55 & 6.30 & 6.93         \\
%         Self-Rewarding \textit{ iter}1   & 7.50 & 6.39 & 6.94         \\
%         Self-Rewarding \textit{ iter}2   & 7.18 & 6.16 & 6.67         \\
%         Iterative DPO \textit{ iter}1    & 7.30 & 6.48 & 6.89         \\
%         Iterative DPO \textit{ iter}2    & 7.35 & 6.14 & 6.74         \\
%         \midrule
%         SRFT      & 7.28 & 5.44 & 6.36         \\
%         \methodabb \textit{ offline}    & 7.93 & 6.15 & 7.04         \\
%         \methodabb \textit{ iter}1    & 7.95 & 6.46 & 7.21         \\
%         \methodabb \textit{ iter}2    & 7.43 & 6.80 & 7.11         \\
%         \methodabb+RM \textit{ iter}1    & 7.98 & 6.69 & 7.33         \\
%         \methodabb+RM \textit{ iter}2    & \textbf{8.16} & \textbf{7.31} & \textbf{7.74}         \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:performance}
% \end{table}

% \begin{table}[h]
%     \centering
%     \caption{Performance Comparison of Different Methods.}
%     \begin{tabular}{lccc}
%         \toprule
%         \multirow{2}{*}{\textbf{Method}} & 
%         \multicolumn{3}{c}{\textbf{MT-Bench(GPT-4 Turbo)}}\\
%         & First turn & Second turn & Average   
%        \\
%         \midrule
%         SFT     & 7.1 & 5.8 & 6.4           \\
%         DPO     & 7.6 & 6.3 & 6.9         \\
%         Self-Rewarding \textit{ iter}1   & 7.5 & 6.4 & 6.9         \\
%         Self-Rewarding \textit{ iter}2   & 7.2 & 6.2 & 6.7         \\
%         Iterative DPO \textit{ iter}1    & 7.3 & 6.5 & 6.9         \\
%         Iterative DPO \textit{ iter}2    & 7.4 & 6.1 & 6.7         \\
%         \midrule
%         SRFT      & 7.3 & 5.4 & 6.4         \\
%         \methodabb \textit{ offline}    & 7.9 & 6.2 & 7.0         \\
%         \methodabb \textit{ iter}1    & 8.0 & 6.5 & 7.2         \\
%         \methodabb \textit{ iter}2    & 7.4 & 6.8 & 7.1         \\
%         \methodabb+RM \textit{ iter}1    & 8.0 & 6.7 & 7.3         \\
%         \methodabb+RM \textit{ iter}2    & \textbf{8.2} & \textbf{7.3} & \textbf{7.7}         \\
%         \bottomrule
%     \end{tabular}
%     \label{tab:performance}
% \end{table}


\clearpage
\section{Qualitative Examples} \label{appendix:examples}
% \clearpage
% \vspace*{-20mm}
\begin{longtable}{p{\textwidth}}
    \toprule
    \small \textbf{Prompt:} \newline What type of soil is suitable for cactus? \\
    \midrule
    \endfirsthead

    \small \textbf{\methodabb{}+RM Direct Response:} \newline Cacti, being succulent plants adapted to arid environments, thrive in well-draining soils that replicate their natural desert conditions. The ideal soil for cacti should meet these characteristics:\newline
    1. \textbf{Well-Draining}: Cacti cannot tolerate waterlogged conditions. The soil should have excellent drainage to prevent root rot.\newline
    2. \textbf{Low Nutrient Content}: Cacti prefer soils low in organic matter and rich in minerals, especially calcium and magnesium. Excess nutrients can lead to overly lush growth, making the plant more susceptible to disease and rot.\newline
    3. \textbf{Sand-Like Texture}: The soil should be composed of a mix of coarse sand, perlite, or pumice, which helps with drainage and aeration.\newline
    4. \textbf{Coarse-Grained}: Fine, compacted soils can retain water, which is detrimental to cacti. A gritty, well-aerated soil is essential.\newline
    
    Here's a typical cactus soil mix:\newline\newline
    - \textbf{Coarse Sand}: About 40-50\% (to ensure excellent drainage)\newline- \textbf{Pumice or Perlite}: About 30-40\% (to improve aeration and drainage)\newline- \textbf{Compressed Coco Coir}: About 10-20\% (to hold some moisture and provide a gentle nutrient base)\newline
    - \textbf{Calcitic Lime}: Optional (to raise the pH and provide calcium, essential for healthy growth)\newline\newline
    You can purchase a pre-mixed cactus soil or create your own by combining these components. It's essential to avoid using heavy, clay-based soils, which can retain too much water and lead to rot. When planting, ensure the soil's surface is slightly elevated to allow for better drainage.\newline\newline
    Always ensure the soil remains slightly dry between waterings to mimic the natural desert environment. This will help promote healthy growth and prevent issues like root rot and fungal infections. Happy cactus growing!  \\
    \midrule
    
    \small\textbf{\methodabb{}+RM Self-Refinement Response:} \newline The \textbf{ideal soil type} for cacti is well-draining, nutrient-poor, and slightly acidic to neutral (pH 6.5-7.5). Here's a detailed breakdown:\newline\newline
    \textbf{1. Composition:}
    \begin{itemize}
        \item \textbf{Primary Ingredients:}
        \begin{itemize}
            \item \textbf{Coarse grit or sand (40-50\%)}: Provides excellent drainage, preventing waterlogging which can lead to root rot.
            \item \textbf{Perlite or pumice (20-30\%)}: Improves aeration, retains some moisture, and enhances drainage further.
            \item \textbf{Loam or vermiculite (10-20\%)}: Adds a bit of fertility and retains some moisture, though still minimal.
        \end{itemize}
        \item \textbf{Optional Additives:}
        \begin{itemize}
            \item \textbf{Calcined clay (1-5\%)}: Helps maintain soil structure and provide slight nutrient content.
            \item \textbf{Activated charcoal (1-2\%)}: Improves drainage, reduces odors, and filters out harmful substances.
        \end{itemize}
    \end{itemize}    
    \textbf{2. Characteristics:}
    \begin{itemize}
        \item \textbf{Well-draining:} Absolutely crucial. Cacti cannot tolerate waterlogged conditions.
        \item \textbf{Nutrient-poor:} Too much fertilizer can lead to excessive growth, making the cactus weak and susceptible to disease.
        \item \textbf{Acidic to slightly alkaline:} Most cacti prefer a neutral pH, but slightly acidic (around 6.5-7.0) is often ideal.
        \item \textbf{Loose and airy:} Allows roots to breathe and develop properly.
    \end{itemize}
    \textbf{Instead of creating your own mix, it's highly recommended to use a commercially available cactus-specific potting mix.}\newline\newline
    These blends are formulated with the perfect proportions of the above ingredients and are readily available at garden centers and nurseries. \newline\newline
    By using this well-draining, nutrient-poor soil, you'll create an optimal environment for your cacti to thrive, just like their natural desert home.  \\
    \bottomrule
    \caption{\small
A generated example on AlpacaEval 2 shows that both Direct Response Generation and Self-Refinement provide highly valuable responses for cactus planting.}
\end{longtable}

\clearpage
\begin{longtable}{p{\textwidth}}
    \toprule
    \textbf{Prompt:} \newline Mick pays his teacher \$800 for 40 lessons worth 2 hours each. If this will be all he is going to pay for his lessons, how much did he receive? \\
    \midrule
    \endfirsthead

    \textbf{\methodabb{}+RM Direct Response:} \newline Based on the information provided, Mick paid \$800 for 40 lessons, with each lesson lasting 2 hours. The calculation for the total time spent in lessons is as follows:\newline\newline
    \[ \text{Total hours} = \text{Number of lessons} \times \text{Duration per lesson} = 40 \times 2 = 80 \text{ hours} \]
    \newline\newline
    Since Mick is only paying \$800 for these lessons and there's no additional payment mentioned, the amount he "received" (which seems like a bit of a misinterpretation in this context, as he's paying for the lessons, not receiving a sum) is the same as what he paid, which is \$800.\newline\newline
    Therefore, Mick received (in the sense of what he paid) \$800. \newline\newline
    If the question were framed differently, ensuring clarity on what "received" exactly means (like "the total value of the lessons he paid for"), then the answer would still be \$800. But considering the literal receipt of cash, it aligns with \$800 he paid.  \\
    \midrule
    
    \textbf{\methodabb{}+RM Self-Refinement Response:} \newline Mick paid a total of \textbf{\$800} for his lessons. This question seems to be asking about the amount he paid, which is the same amount he received back in the form of the lessons he purchased. Therefore, Mick received \textbf{\$800}.\newline\newline
    The question mixes concepts of payment and "receiving back" (which is usually more about value received), but since all the money was paid upfront, it logically means Mick received the full value of what he paid for (40 lessons at \$20 each = \$800). Thus, the answer to the specific wording "how much did he receive" aligns with the \$800 he paid, considering the context that this is his sole payment for the lessons. \newline\newline
    To summarize, Mick received \textbf{\$800}.  \\
    \midrule
    \textbf{GPT4-1106-Preview Response:} \newline 
    To determine the amount Mick received per lesson, we need to divide the total amount paid by the number of lessons he received. \newline\newline
    Mick paid \$800 for 40 lessons. \newline\newline
    So, \$800 $\div$ 40 lessons = \$20 per lesson.\newline\newline
    Each lesson is 2 hours long, so to find out how much Mick received per hour, we divide the amount per lesson by the number of hours per lesson:\newline\newline
    \$20 per lesson $\div$ 2 hours per lesson = \$10 per hour.\newline\newline
    Thus, Mick received \$10 per hour for his lessons.\\
    \bottomrule
    \caption{A case study on AlpacaEval 2 demonstrates that our approach, \methodabb{}, allows the model to understand questions more deeply and comprehensively than the baseline GPT-4-1106-Preview. The comparison between "Direct Response" and "Self-Refinement Response" underscores that the Self-Refinement Response delivers answers that are more concise, logically structured, and well-organized.}
\end{longtable}

\clearpage

% \vspace*{-11mm}
\begin{longtable}{p{\textwidth}}
    \toprule
    \small \textbf{Prompt:} \newline if it takes 8 hours to fly to Singapore, stop over for an hour and a half and then fly 14 hours to London then what is the total travel time if the plane is delayed in Singapore \\
    \midrule
    \endfirsthead

    \small \textbf{\methodabb{}+RM Direct Response:} \newline The total travel time, \textbf{including the stopover}, is \textbf{20 hours}.\newline\newline
    Here's the breakdown:\newline\newline
    1. \textbf{Direct flight to Singapore:} 8 hours\newline
    2. \textbf{Stopover:} 1.5 hours\newline
    3. \textbf{Direct flight to London:} 14 hours\newline\newline
    \textbf{Total:} 8 hours + 1.5 hours + 14 hours = \textbf{23.5 hours}\newline\newline
    \textbf{However}, you mentioned a \textbf{delay} in Singapore. If the delay is only during the stopover and doesn't extend the overall travel time (e.g., the plane still reaches London within a reasonable timeframe), the total \textbf{affected} travel time remains \textbf{23.5 hours}. \newline\newline
    If the delay causes significant changes to the flight schedule and extends the overall travel time, you'd need more information about the specific delay duration to calculate the exact new total time.  \newline\newline
    Therefore, if the delay is considered minimal and doesn't affect the London arrival time, the \textbf{total travel time remains 23.5 hours}.  \\
    \midrule
    
    \small \textbf{\methodabb{}+RM Self-Refinement Response:} \newline 
    Here's how to calculate the total travel time, considering a delay in Singapore:\newline\newline
    1. \textbf{Calculate the base travel time without delay:}\newline
    \hspace*{2em}\textbullet\enspace Singapore to London: 8 hours (flight) + 1.5 hours (stopover) + 14 hours (second flight) = \textbf{23.5 hours}\newline\newline
    2. \textbf{Account for the delay in Singapore:}\newline
    \hspace*{2em}\textbullet\enspace Let the delay be represented as 'x' hours.\newline\newline
    3. \textbf{Calculate the new total travel time:}\newline
    \hspace*{2em}\textbullet\enspace New Singapore to London travel time: 8 hours (flight) + 1.5 hours (stopover) + x hours (delay) + 14 hours (second flight) = \textbf{23.5 + x hours}\newline\newline
    Therefore, the total travel time with the delay is \textbf{23.5 + x hours}.\newline\newline
    To get a specific number, you need to know the duration of the delay (x). For example, if the delay is 2 hours:\newline
    \hspace*{2em}\textbullet\enspace Total travel time: \textbf{23.5 + 2 = 25.5 hours}\\
    \midrule
    \small \textbf{GPT4-1106-Preview Response:} \newline 
    To calculate the total travel time, we need to add up all the segments of the journey along with any delays.\newline\newline
    Here's the breakdown:\newline\newline
    - Flight to Singapore: 8 hours\newline
    - Stopover in Singapore: 1.5 hours\newline
    - Flight to London: 14 hours\newline\newline
    Now, if the plane is delayed in Singapore, we have to add that delay to the stopover time. However, you haven't specified the duration of the delay. \newline\newline
    Let's assume the delay is D hours. The total travel time would then be:\newline\newline
    Total travel time = 8 hours (first flight) + 1.5 hours (scheduled stopover) + D hours (delay in Singapore) + 14 hours (second flight)\newline\newline
    If you provide the duration of the delay, I can give you the exact total travel time.\\
    \bottomrule
    \caption{\small 
    % A case study on AlpacaEval 2 shows that the Self-Refinement Response follows a more coherent problem-solving logic similar to GPT-4-1106-Preview, while the Direct Response displays more disorganized reasoning. This demonstrates the effectiveness of our Self-Refinement strategy in improving logical coherence and semantic consistency.
    A case study on AlpacaEval 2 reveals that the Self-Refinement Response exhibits a more coherent problem-solving logic, comparable to GPT-4-1106-Preview, whereas the Direct Response demonstrates more fragmented and disorganized reasoning. This highlights the effectiveness of our Self-Refinement strategy in enhancing logical coherence and semantic consistency.
    }
\end{longtable}


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
