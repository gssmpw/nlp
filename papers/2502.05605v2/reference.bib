@article{xiong2023iterative,
  title={Iterative preference learning from human feedback: Bridging theory and practice for rlhf under kl-constraint},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Wang, Ziqi and Zhong, Han and Ji, Heng and Jiang, Nan and Zhang, Tong},
  journal={arXiv preprint arXiv:2312.11456
        
        
        
        
        
        },
  year={2023}
}

@article{dong2024rlhf,
  title={Rlhf workflow: From reward modeling to online rlhf},
  author={Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong},
  journal={arXiv preprint arXiv:2405.07863},
  year={2024}
}

@article{wu2024meta,
  title={Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge},
  author={Wu, Tianhao and Yuan, Weizhe and Golovneva, Olga and Xu, Jing and Tian, Yuandong and Jiao, Jiantao and Weston, Jason and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2407.19594
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314
        
        
        
        
        
        },
  year={2024}
}

@article{qu2024recursive,
  title={Recursive introspection: Teaching language model agents how to self-improve},
  author={Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral},
  journal={arXiv preprint arXiv:2407.18219},
  year={2024}
}

@article{kumar2024training,
  title={Training language models to self-correct via reinforcement learning},
  author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
  journal={arXiv preprint arXiv:2409.12917
        
        },
  year={2024}
}

@article{chen2023alpagasus,
  title={Alpagasus: Training a better alpaca with fewer data},
  author={Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others},
  journal={arXiv preprint arXiv:2307.08701
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@misc{snorkel2024,
  author       = {Snorkel},
  title        = {New Benchmark Results Demonstrate Value of Snorkel AI Approach to LLM Alignment},
  year         = {2024},
  url          = {https://snorkel.ai/new-benchmark-results-demonstrate-valueof-snorkel-ai-approach-to-llm-alignment}
}

@article{zheng2023judging,
  title={Judging llm-as-a-judge with mt-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46595--46623},
  year={2023}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{dubois2024length,
  title={Length-controlled alpacaeval: A simple way to debias automatic evaluators},
  author={Dubois, Yann and Galambosi, Bal{\'a}zs and Liang, Percy and Hashimoto, Tatsunori B},
  journal={arXiv preprint arXiv:2404.04475
        
        
        
        
        
        },
  year={2024}
}

@misc{li2024live,
  title={From live data to high-quality benchmarks: The arena-hard pipeline},
  author={Li, Tianle and Chiang, Wei-Lin and Frick, Evan and Dunlap, Lisa and Zhu, Banghua and Gonzalez, Joseph E and Stoica, Ion},
  year={2024},
  publisher={April}
}

@article{xu2023some,
  title={Some things are more cringe than others: Preference optimization with the pairwise cringe loss},
  author={Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2312.16682
        
        
        
        
        
        },
  year={2023}
}

@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}

@article{chen_fan_girshick_he_2020, title={moco v2}, journal={arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition}, author={Chen, Xinlei and Fan, Haoqi and Girshick, Ross and He, Kaiming}, year={2020}, month={Mar} }

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}

@article{lee2022multi,
  title={Multi-game decision transformers},
  author={Lee, Kuang-Huei and Nachum, Ofir and Yang, Mengjiao Sherry and Lee, Lisa and Freeman, Daniel and Guadarrama, Sergio and Fischer, Ian and Xu, Winnie and Jang, Eric and Michalewski, Henryk and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27921--27936},
  year={2022}
}

@article{reed2022generalist,
  title={A generalist agent},
  author={Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and others},
  journal={arXiv preprint arXiv:2205.06175
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{fedus_zoph_shazeer_2021, title={Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.}, journal={arXiv: Learning,arXiv: Learning}, author={Fedus, William and Zoph, Barret and Shazeer, Noam}, year={2021}, month={Jan} }

@article{gupta_mendonca_liu_abbeel_levine_2018, title={Meta-Reinforcement Learning of Structured Exploration Strategies}, journal={arXiv: Learning,arXiv: Learning}, author={Gupta, Abhishek and Mendonca, Russell and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey}, year={2018}, month={Feb} }

@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@inproceedings{he_fan_wu_xie_girshick_2020, title={moco}, url={http://dx.doi.org/10.1109/cvpr42600.2020.00975}, DOI={10.1109/cvpr42600.2020.00975}, booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross}, year={2020}, month={Jun} }

@article{janner_li_levine_2021, title={Offline Reinforcement Learning as One Big Sequence Modeling Problem}, journal={Neural Information Processing Systems,Neural Information Processing Systems}, author={Janner, Michael and Li, Qiyang and Levine, Sergey}, year={2021}, month={Dec} }

@article{kirsch_harrison_sohl-dickstein_metz_2022, title={General-Purpose In-Context Learning by Meta-Learning Transformers}, author={Kirsch, Louis and Harrison, James and Sohl-Dickstein, Jascha and Metz, Luke}, year={2022}, month={Dec} }

@article{laskin2022context,
  title={In-context reinforcement learning with algorithm distillation},
  author={Laskin, Michael and Wang, Luyu and Oh, Junhyuk and Parisotto, Emilio and Spencer, Stephen and Steigerwald, Richie and Strouse, DJ and Hansen, Steven and Filos, Angelos and Brooks, Ethan and others},
  journal={arXiv preprint arXiv:2210.14215
        
        
        
        
        
        },
  year={2022}
}

@article{lee_xie_pacchiano_chandak_finn_nachum_brunskill_university_research_deepmind_0, title={Supervised Pretraining Can Learn In-Context Reinforcement Learning}, author={Lee, JonathanN and Xie, Annie and Pacchiano, Aldo and Chandak, Yash and Finn, Chelsea and Nachum, Ofir and Brunskill, Emma and University, Stanford and Research, Microsoft and Deepmind, Google}, year={0} }

@article{li_huang_luo_2021, title={Provably Improved Context-Based Offline Meta-RL with Attention and Contrastive Learning}, journal={arXiv: Learning,arXiv: Learning}, author={Li, Lanqing and Huang, Yuanhao and Luo, Dijun}, year={2021}, month={Feb} }

@article{li_yang_luo_2020, title={Efficient Fully-Offline Meta-Reinforcement Learning via Distance Metric Learning and Behavior Regularization}, journal={Learning,Learning}, author={Li, Lanqing and Yang, Rui and Luo, Dijun}, year={2020}, month={Oct} }

@article{li_ildiz_papailiopoulos_oymak_2023, title={Transformers as Algorithms: Generalization and Stability in In-context Learning}, author={Li, Yingcong and Ildiz, M.Emrullah and Papailiopoulos, Dimitris and Oymak, Samet}, year={2023}, month={Jan} }

@article{lin_liu_sengupta_0, title={Switch Trajectory Transformer with Distributional Value Approximation for Multi-Task Reinforcement Learning}, author={Lin, Qinjie and Liu, Han and Sengupta, Biswa}, year={0} }

@article{lu_schroecker_gu_parisotto_foerster_singh_behbahani_2023, title={Structured State Space Models for In-Context Reinforcement Learning}, author={Lu, Chris and Schroecker, Yannick and Gu, Albert and Parisotto, Emilio and Foerster, Jakob and Singh, Satinder and Behbahani, Feryal}, year={2023}, month={Mar} }

@article{rakelly_zhou_quillen_finn_levine_2019, title={Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables}, journal={Cornell University - arXiv,Cornell University - arXiv}, author={Rakelly, Kate and Zhou, Aurick and Quillen, Deirdre and Finn, Chelsea and Levine, Sergey}, year={2019}, month={Mar} }},
  year={2022}
}

@article{sourati_ilievski_sandlin_mermoud_2023, title={Case-Based Reasoning with Language Models for Classification of Logical Fallacies}, author={Sourati, Zhivar and Ilievski, Filip and Sandlin, H\^ong-\^An and Mermoud, Alain}, year={2023}, month={Jan} }

@article{sun_ma_madaan_bonatti_huang_kapoor_2023, title={SMART: Self-supervised Multi-task pretrAining with contRol Transformers}, author={Sun, Yanchao and Ma, Shuang and Madaan, Ratnesh and Bonatti, Rogerio and Huang, Furong and Kapoor, Ashish}, year={2023}, month={Jan} }

@article{wang_zhao_luo_ren_zhang_li_2022, title={Bootstrapped Transformer for Offline Reinforcement Learning}, author={Wang, Kerong and Zhao, Hanye and Luo, Xufang and Ren, Kan and Zhang, Weinan and Li, Dongsheng}, year={2022}, month={Jun} }

@article{yuan_lu_2022, title={Robust Task Representations for Offline Meta-Reinforcement Learning via Contrastive Learning}, author={Yuan, Haoqi and Lu, Zongqing}, year={2022}, month={Jun} }

@article{zhou_cong_yu_0, title={Task Inference for Offline Meta Reinforcement Learning via Latent Shared Knowledge}, author={Zhou, Ying and Cong, Shan and Yu, Chao}, year={0} }

@article{watson1994case,
  title={Case-based reasoning: A review},
  author={Watson, Ian and Marir, Farhi},
  journal={The knowledge engineering review},
  volume={9},
  number={4},
  pages={327--354},
  year={1994},
  publisher={Cambridge University Press}
}

@article{lee2023rlaif,
  title={Rlaif: Scaling reinforcement learning from human feedback with ai feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others},
  journal={arXiv preprint arXiv:2309.00267
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{lee2023supervised,
  title={Supervised Pretraining Can Learn In-Context Reinforcement Learning},
  author={Lee, Jonathan N and Xie, Annie and Pacchiano, Aldo and Chandak, Yash and Finn, Chelsea and Nachum, Ofir and Brunskill, Emma},
  journal={arXiv preprint arXiv:2306.14892
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@inproceedings{xu2022prompting,
  title={Prompting decision transformer for few-shot policy generalization},
  author={Xu, Mengdi and Shen, Yikang and Zhang, Shun and Lu, Yuchen and Zhao, Ding and Tenenbaum, Joshua and Gan, Chuang},
  booktitle={international conference on machine learning},
  pages={24631--24645},
  year={2022},
  organization={PMLR}
}

@article{xu2023hyper,
  title={Hyper-decision transformer for efficient online policy adaptation},
  author={Xu, Mengdi and Lu, Yuchen and Shen, Yikang and Zhang, Shun and Zhao, Ding and Gan, Chuang},
  journal={arXiv preprint arXiv:2304.08487
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{huang2023large,
  title={Large language models cannot self-correct reasoning yet},
  author={Huang, Jie and Chen, Xinyun and Mishra, Swaroop and Zheng, Huaixiu Steven and Yu, Adams Wei and Song, Xinying and Zhou, Denny},
  journal={arXiv preprint arXiv:2310.01798
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{lambert2024rewardbench,
  title={Rewardbench: Evaluating reward models for language modeling},
  author={Lambert, Nathan and Pyatkin, Valentina and Morrison, Jacob and Miranda, LJ and Lin, Bill Yuchen and Chandu, Khyathi and Dziri, Nouha and Kumar, Sachin and Zick, Tom and Choi, Yejin and others},
  journal={arXiv preprint arXiv:2403.13787
        
        
        
        },
  year={2024}
}

@article{liu2024skywork,
  title={Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs},
  author={Liu, Chris Yuhao and Zeng, Liang and Liu, Jiacai and Yan, Rui and He, Jujie and Wang, Chaojie and Yan, Shuicheng and Liu, Yang and Zhou, Yahui},
  journal={arXiv preprint arXiv:2410.18451
        
        
        
        
        
        
        
        },
  year={2024}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2020}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@misc{stiennon2022learning,
      title={Learning to summarize from human feedback}, 
      author={Nisan Stiennon and Long Ouyang and Jeff Wu and Daniel M. Ziegler and Ryan Lowe and Chelsea Voss and Alec Radford and Dario Amodei and Paul Christiano},
      year={2022},
      eprint={2009.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Koh_2022,
   title={An Empirical Survey on Long Document Summarization: Datasets, Models, and Metrics},
   volume={55},
   ISSN={1557-7341},
   url={http://dx.doi.org/10.1145/3545176},
   DOI={10.1145/3545176},
   number={8},
   journal={ACM Computing Surveys},
   publisher={Association for Computing Machinery (ACM)},
   author={Koh, Huan Yee and Ju, Jiaxin and Liu, Ming and Pan, Shirui},
   year={2022},
   month=dec, pages={1–35} }

@misc{gao2023pal,
      title={PAL: Program-aided Language Models}, 
      author={Luyu Gao and Aman Madaan and Shuyan Zhou and Uri Alon and Pengfei Liu and Yiming Yang and Jamie Callan and Graham Neubig},
      year={2023},
      eprint={2211.10435},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chen2021evaluating,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{christian2023amazing,
  title={Amazing “jailbreak” bypasses ChatGPT’s ethics safeguards},
  author={Christian, Jon},
  journal={Futurism, February},
  volume={4},
  pages={2023},
  year={2023}
}

@article{hendrycks2023overview,
  title={An Overview of Catastrophic AI Risks},
  author={Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
  journal={arXiv preprint arXiv:2306.12001
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{shevlane2023model,
  title={Model evaluation for extreme risks},
  author={Shevlane, Toby and Farquhar, Sebastian and Garfinkel, Ben and Phuong, Mary and Whittlestone, Jess and Leung, Jade and Kokotajlo, Daniel and Marchal, Nahema and Anderljung, Markus and Kolt, Noam and others},
  journal={arXiv preprint arXiv:2305.15324
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{urbina2022dual,
  title={Dual use of artificial-intelligence-powered drug discovery},
  author={Urbina, Fabio and Lentzos, Filippa and Invernizzi, C{\'e}dric and Ekins, Sean},
  journal={Nature Machine Intelligence},
  volume={4},
  number={3},
  pages={189--191},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and O’Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347
        
        
        
        },
  year={2017}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Trębacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@inproceedings{gao2023scaling,
  title={Scaling laws for reward model overoptimization},
  author={Gao, Leo and Schulman, John and Hilton, Jacob},
  booktitle={International Conference on Machine Learning},
  pages={10835--10866},
  year={2023},
  organization={PMLR}
}


@article{dong2023raft,
  title={Raft: Reward ranked finetuning for generative foundation model alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  journal={arXiv preprint arXiv:2304.06767
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}
@article{song2023preference,
  title={Preference ranking optimization for human alignment},
  author={Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng},
  journal={arXiv preprint arXiv:2306.17492
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}
@article{yuan2023rrhf,
  title={Rrhf: Rank responses to align language models with human feedback without tears},
  author={Yuan, Zheng and Yuan, Hongyi and Tan, Chuanqi and Wang, Wei and Huang, Songfang and Huang, Fei},
  journal={arXiv preprint arXiv:2304.05302
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{zeng2024token,
  title={Token-level Direct Preference Optimization},
  author={Zeng, Yongcheng and Liu, Guoqing and Ma, Weiyu and Yang, Ning and Zhang, Haifeng and Wang, Jun},
  journal={arXiv preprint arXiv:2404.11999
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@misc{li2023alpacaeval,
  title={Alpacaeval: An automatic evaluator of instruction-following models},
  author={Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  year={2023}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560
        
        
        
        
        
        },
  year={2022}
}

@article{choi2024self,
  title={Self-Improving Robust Preference Optimization},
  author={Choi, Eugene and Ahmadian, Arash and Geist, Matthieu and Pietquin, Oilvier and Azar, Mohammad Gheshlaghi},
  journal={arXiv preprint arXiv:2406.01660
        
        
        
        
        
        },
  year={2024}
}

@inproceedings{leerlaif,
  title={RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback},
  author={Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and Mesnard, Thomas and Ferret, Johan and Lu, Kellie Ren and Bishop, Colton and Hall, Ethan and Carbune, Victor and Rastogi, Abhinav and others},
  booktitle={Forty-first International Conference on Machine Learning}
}

@inproceedings{havrilla2023trlx,
  title={trlX: A Framework for Large Scale Reinforcement Learning from Human Feedback},
  author={Havrilla, Alexander and Zhuravinskyi, Maksym and Phung, Duy and Tiwari, Aman and Tow, Jonathan and Biderman, Stella and Anthony, Quentin and Castricato, Louis},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={8578--8595},
  year={2023}
}

@article{monea2024llms,
  title={LLMs Are In-Context Reinforcement Learners},
  author={Monea, Giovanni and Bosselut, Antoine and Brantley, Kiant{\'e} and Artzi, Yoav},
  journal={arXiv preprint arXiv:2410.05362
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}

@misc{yang2024largelanguagemodelsoptimizers,
      title={Large Language Models as Optimizers}, 
      author={Chengrun Yang and Xuezhi Wang and Yifeng Lu and Hanxiao Liu and Quoc V. Le and Denny Zhou and Xinyun Chen},
      year={2024},
      eprint={2309.03409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2309.03409}, 
}

@article{nie2024importance,
  title={The Importance of Directional Feedback for LLM-based Optimizers},
  author={Nie, Allen and Cheng, Ching-An and Kolobov, Andrey and Swaminathan, Adith},
  journal={arXiv preprint arXiv:2405.16434
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}

@article{li2015diversity,
  title={A diversity-promoting objective function for neural conversation models},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  journal={arXiv preprint arXiv:1510.03055
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2015}
}

@inproceedings{zhu2018texygen,
  title={Texygen: A benchmarking platform for text generation models},
  author={Zhu, Yaoming and Lu, Sidi and Zheng, Lei and Guo, Jiaxian and Zhang, Weinan and Wang, Jun and Yu, Yong},
  booktitle={The 41st international ACM SIGIR conference on research \& development in information retrieval},
  pages={1097--1100},
  year={2018}
}
@article{liu2023statistical,
  title={Statistical rejection sampling improves preference optimization},
  author={Liu, Tianqi and Zhao, Yao and Joshi, Rishabh and Khalman, Misha and Saleh, Mohammad and Liu, Peter J and Liu, Jialu},
  journal={arXiv preprint arXiv:2309.06657
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{wiher2022decoding,
  title={On decoding strategies for neural text generators},
  author={Wiher, Gian and Meister, Clara and Cotterell, Ryan},
  journal={Transactions of the Association for Computational Linguistics},
  volume={10},
  pages={997--1012},
  year={2022},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@article{khalifa2020distributional,
  title={A distributional approach to controlled text generation},
  author={Khalifa, Muhammad and Elsahar, Hady and Dymetman, Marc},
  journal={arXiv preprint arXiv:2012.11635
        
        
        
        
        
        
        
        
        
        },
  year={2020}
}

@article{perez2202red,
  title={Red teaming language models with language models, 2022},
  author={Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  journal={URL https://arxiv. org/abs/2202.03286}
}

@article{santurkar2023whose,
  title={Whose opinions do language models reflect?},
  author={Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2303.17548
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{wang2023beyond,
  title={Beyond Reverse KL: Generalizing Direct Preference Optimization with Diverse Divergence Constraints},
  author={Wang, Chaoqi and Jiang, Yibo and Yang, Chenghao and Liu, Han and Chen, Yuxin},
  journal={arXiv preprint arXiv:2309.16240
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{knox2022models,
  title={Models of human preference for learning reward functions},
  author={Knox, W Bradley and Hatgis-Kessell, Stephane and Booth, Serena and Niekum, Scott and Stone, Peter and Allievi, Alessandro},
  journal={arXiv preprint arXiv:2206.02231
        
        
        
        
        
        },
  year={2022}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{bradley1952rank,
  title={Rank analysis of incomplete block designs: I. The method of paired comparisons},
  author={Bradley, Ralph Allan and Terry, Milton E},
  journal={Biometrika},
  volume={39},
  number={3/4},
  pages={324--345},
  year={1952},
  publisher={JSTOR}
}

@article{knox2023learning,
  title={Learning Optimal Advantage from Preferences and Mistaking it for Reward},
  author={Knox, W Bradley and Hatgis-Kessell, Stephane and Adalgeirsson, Sigurdur Orn and Booth, Serena and Dragan, Anca and Stone, Peter and Niekum, Scott},
  journal={arXiv preprint arXiv:2310.02456
        
        },
  year={2023}
}

@misc{jiang2023mistral,
      title={Mistral 7B}, 
      author={Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
      year={2023},
      eprint={2310.06825},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{workshop2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Workshop, BigScience and Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and others},
  journal={arXiv preprint arXiv:2211.05100
        
        
        
        },
  year={2022}
}
@article{taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}

@misc{vu2023koala,
      title={Koala: An Index for Quantifying Overlaps with Pre-training Corpora}, 
      author={Thuy-Trang Vu and Xuanli He and Gholamreza Haffari and Ehsan Shareghi},
      year={2023},
      eprint={2303.14770},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}
@article{lambert2024t,
  title={T$\backslash$" ULU 3: Pushing Frontiers in Open Language Model Post-Training},
  author={Lambert, Nathan and Morrison, Jacob and Pyatkin, Valentina and Huang, Shengyi and Ivison, Hamish and Brahman, Faeze and Miranda, Lester James V and Liu, Alisa and Dziri, Nouha and Lyu, Shane and others},
  journal={arXiv preprint arXiv:2411.15124
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}

@misc{cui2023ultrafeedback,
      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, 
      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2310.01377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{weidinger2021ethical,
  title={Ethical and social risks of harm from language models},
  author={Weidinger, Laura and Mellor, John and Rauh, Maribeth and Griffin, Conor and Uesato, Jonathan and Huang, Po-Sen and Cheng, Myra and Glaese, Mia and Balle, Borja and Kasirzadeh, Atoosa and others},
  journal={arXiv preprint arXiv:2112.04359
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2021}
}

@misc{rauh2022characteristics,
      title={Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models}, 
      author={Maribeth Rauh and John Mellor and Jonathan Uesato and Po-Sen Huang and Johannes Welbl and Laura Weidinger and Sumanth Dathathri and Amelia Glaese and Geoffrey Irving and Iason Gabriel and William Isaac and Lisa Anne Hendricks},
      year={2022},
      eprint={2206.08325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{ousidhoum2021probing,
  title={Probing toxic content in large pre-trained language models},
  author={Ousidhoum, Nedjma and Zhao, Xinran and Fang, Tianqing and Song, Yangqiu and Yeung, Dit-Yan},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={4262--4274},
  year={2021}
}

@article{huang2023survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={arXiv preprint arXiv:2311.05232
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@inproceedings{azar2024general,
  title={A general theoretical paradigm to understand learning from human preferences},
  author={Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4447--4455},
  year={2024},
  organization={PMLR}
}


@article{ethayarajh2024kto,
  title={Kto: Model alignment as prospect theoretic optimization},
  author={Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe},
  journal={arXiv preprint arXiv:2402.01306
        
        },
  year={2024}
}

@inproceedings{hong2024orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={11170--11189},
  year={2024}
}

@article{meng2024simpo,
  title={Simpo: Simple preference optimization with a reference-free reward},
  author={Meng, Yu and Xia, Mengzhou and Chen, Danqi},
  journal={arXiv preprint arXiv:2405.14734
        
        },
  year={2024}
}

@article{deshpande2023toxicity,
  title={Toxicity in chatgpt: Analyzing persona-assigned language models},
  author={Deshpande, Ameet and Murahari, Vishvak and Rajpurohit, Tanmay and Kalyan, Ashwin and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2304.05335
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{sheng2021societal,
  title={Societal biases in language generation: Progress and challenges},
  author={Sheng, Emily and Chang, Kai-Wei and Natarajan, Premkumar and Peng, Nanyun},
  journal={arXiv preprint arXiv:2105.04054
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2021}
}

@article{wu2023fine,
  title={Fine-Grained Human Feedback Gives Better Rewards for Language Model Training},
  author={Wu, Zeqiu and Hu, Yushi and Shi, Weijia and Dziri, Nouha and Suhr, Alane and Ammanabrolu, Prithviraj and Smith, Noah A and Ostendorf, Mari and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2306.01693
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{saha2023branch,
  title={Branch-solve-merge improves large language model evaluation and generation},
  author={Saha, Swarnadeep and Levy, Omer and Celikyilmaz, Asli and Bansal, Mohit and Weston, Jason and Li, Xian},
  journal={arXiv preprint arXiv:2310.15123
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{bai2024benchmarking,
  title={Benchmarking foundation models with language-model-as-an-examiner},
  author={Bai, Yushi and Ying, Jiahao and Cao, Yixin and Lv, Xin and He, Yuze and Wang, Xiaozhi and Yu, Jifan and Zeng, Kaisheng and Xiao, Yijia and Lyu, Haozhe and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{dubois2024alpacafarm,
  title={Alpacafarm: A simulation framework for methods that learn from human feedback},
  author={Dubois, Yann and Li, Chen Xuechen and Taori, Rohan and Zhang, Tianyi and Gulrajani, Ishaan and Ba, Jimmy and Guestrin, Carlos and Liang, Percy S and Hashimoto, Tatsunori B},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gulcehre2023reinforced,
  title={Reinforced self-training (rest) for language modeling},
  author={Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal={arXiv preprint arXiv:2308.08998
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{reid2022learning,
  title={Learning to model editing processes},
  author={Reid, Machel and Neubig, Graham},
  journal={arXiv preprint arXiv:2205.12374
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{schick2022peer,
  title={Peer: A collaborative language model},
  author={Schick, Timo and Dwivedi-Yu, Jane and Jiang, Zhengbao and Petroni, Fabio and Lewis, Patrick and Izacard, Gautier and You, Qingfei and Nalmpantis, Christoforos and Grave, Edouard and Riedel, Sebastian},
  journal={arXiv preprint arXiv:2208.11663
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{welleck2022generating,
  title={Generating sequences by learning to self-correct},
  author={Welleck, Sean and Lu, Ximing and West, Peter and Brahman, Faeze and Shen, Tianxiao and Khashabi, Daniel and Choi, Yejin},
  journal={arXiv preprint arXiv:2211.00053
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2022}
}

@article{yuan2024self,
  title={Self-rewarding language models},
  author={Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  journal={arXiv preprint arXiv:2401.10020
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2024}
}

@article{madaan2024self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{kim2023prometheus,
  title={Prometheus: Inducing fine-grained evaluation capability in language models},
  author={Kim, Seungone and Shin, Jamin and Cho, Yejin and Jang, Joel and Longpre, Shayne and Lee, Hwaran and Yun, Sangdoo and Shin, Seongjin and Kim, Sungdong and Thorne, James and others},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{li2023self,
  title={Self-alignment with instruction backtranslation},
  author={Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Levy, Omer and Zettlemoyer, Luke and Weston, Jason and Lewis, Mike},
  journal={arXiv preprint arXiv:2308.06259
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}

@article{tunstall2023zephyr,
  title={Zephyr: Direct distillation of lm alignment},
  author={Tunstall, Lewis and Beeching, Edward and Lambert, Nathan and Rajani, Nazneen and Rasul, Kashif and Belkada, Younes and Huang, Shengyi and von Werra, Leandro and Fourrier, Cl{\'e}mentine and Habib, Nathan and others},
  journal={arXiv preprint arXiv:2310.16944
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        
        },
  year={2023}
}
