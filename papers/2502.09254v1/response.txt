\section{Related Work}
\subsection{Graph Anomaly Detection} 
Existing GAD methods can be generally classified into unsupervised and supervised methods **Zhu, "Unsupervised Graph Anomaly Detection"**, depending on the assumption regarding the presence of normal and anomaly labels.
The fully supervised approaches have recently achieved remarkable progress by leveraging both labeled normal and abnormal nodes **Fei et al., "Supervised Graph Anomaly Detection via Heterophilic Edge Reduction"**. These methods are primarily designed to improve message aggregation in graph neural networks (GNNs) by reducing heterophilic edges from both spectral and spatial perspectives, effectively mitigating the over-smoothing problem in GAD **Zhou et al., "Over-Smoothing Problem in Graph Neural Networks"**. Fully supervised methods are highly effective in GAD as they treat the task as an imbalanced classification problem. However, the requirement for both normal and abnormal labels significantly hinders their applications to scenarios where labeled nodes are difficult to obtain.

Unsupervised methods, which typically assume the absence of both normal and abnormal labels, have garnered significant attention due to their more practical setting assumption on data labels. They generally incorporate some conventional techniques such as reconstruction **Vincent et al., "Extracting and Composing Robust Features with Denoising Autoencoders"**,**Maaten et al., "Visualizing Data using t-SNE"**, one-class classification **Tax, "One-class Classification using Support Vector Machines"**,**Campbell, "Learning Distance Functions with Simple Neural Networks and a Dimensionality Reduction Preprocessing"**, contrastive learning **Hadsell et al., "Dimensionality Reduction by Learning an Invariant Mapping"**,**Chopra et al., "Learning to Count Objects in Images"**, and adversarial learning **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** into graph learning to capture the normal patterns within the graph and then assign an anomaly score for each node based on its deviation from the normal patterns. However, these methods still follow the paradigm of training and inference on the same graph, making them struggle to generalize to new/unseen graphs due to the distribution shift between training and testing set.


% Unsupervised methods can operate on data without labels, but their constraints are often too strict for real-world applications. 
%  Due to the strict constraints of the unsupervised methods and the fact that normal labels are generally easier to obtain than abnormal labels in real-world applications **Fei et al., "Supervised Graph Anomaly Detection via Heterophilic Edge Reduction"**, semi-supervised methods that consider partially known normal labels have been gradually introduced. These methods are more aligned with practical scenarios. Some unsupervised methods can be adapted to semi-supervised settings by directly applying normal pattern extraction techniques to the clean normal samples**Zhu et al., "Semi-Supervised Graph Anomaly Detection using Normal Pattern Extraction"**. However, these methods still follow the paradigm of training and inference on the same graph making them struggle to generalize to the new graph from different domains.

% GGAD **Jin et al., "Graph Anomaly Detection via Generative Adversarial Networks"** is the first semi-supervised method to address the scenario where only a subset of normal labels is available. It generates pseudo-anomalous nodes from partially labeled normal nodes to provide effective negative samples for training a discriminative one-class classifier. 



\subsection{Foundation Models for GAD}
Generalist models have recently achieved significant progress on non-graph data by leveraging large pre-trained models to facilitate generalized pattern recognition in diverse downstream tasks, such as generalist anomaly detection on images **Hendrycks et al., "Deep Anomaly Detection with Outlier Exposure"**. However, applying these methods to graph data remains highly challenging due to the absence of such pre-trained models **Kipf et al., "Variational Graph Autoencoder"** on graph data.
% Despite significant advancements in foundation models across various domains, the development of GFMs is still in its early stages. Recent studies have shown initial successes in specialized areas like knowledge graphs **Bordes et al., "Translating Embeddings for Modeling Multi-relational Data"** and molecular graphs 
The main challenge in designing a GFM is capturing invariance across diverse graphs while mapping them into a shared representation space to enable positive transfer between training and inference **Dai et al., "Graph Contrastive Learning with Augmentation"**. Currently, most GFM models use prompt learning to enable the knowledge transfer across graphs for general graph tasks **Yang et al., "Graph Prompt Engineering"**.
% For example, GraphPrompt **Zhang et al., "GraphPrompt: A Pre-training and Prompting Framework for Graph-Level Tasks"** builds a pre-training and prompting framework designed for both graph-level and node-level classification tasks. It learns a prompt that helps downstream tasks identify and leverage the most relevant knowledge from the pre-trained model in a task-specific way.
% While GFMs have made significant progress in general graph tasks, 
However,
they still struggle to generalize to the GAD task due to the inherently infrequent, irregular, and heterogeneous
abnormality patterns in GAD datasets **Jin et al., "Graph Anomaly Detection via Generative Adversarial Networks"**. Therefore, 
% some generalist methods that versatilely support zero-shot inference and sample-efficient tuning have been proposed for graph anomaly detection very recently by training on source graphs and evaluating on test graphs
some recent efforts attempt to devise the GFMs for GAD **Dai et al., "Graph Contrastive Learning with Augmentation"**.
ARC **Wu et al., "Fine-Tuning Graph Neural Networks via Anomaly-Resilient Contrastive Learning"** is one of such methods, a fine-tuning GFMs for GAD, enabling a ``one-for-all‚Äù GAD model through a fine-tuning mechanism. 
% It introduces an ego-neighbor residual graph encoder that learns node embeddings sensitive to abnormalities. During inference, a cross-attentive in-context anomaly scoring module is used to predict node abnormality by leveraging a few-shot set of normal samples.
UNPrompt **Zhang et al., "Zero-Shot Generalist Graph Anomaly Detection via Unsupervised Prompt Learning"** is the first zero-shot generalist GAD method that learns generalized neighborhood prompts, allowing latent node attribute predictability to serve as a generalized anomaly measure.
% across diverse datasets without requiring fine-tuning on the target dataset.
Different from ARC and UNPrompt, which address the GAD-oriented GFMs partially under few-shot or zero-shot settings, AnomalyGFM can effectively capture abnormalities across graphs from different domains while supporting both zero-shot inference and few-shot prompt fine-tuning/inference. This offers more versatile abilities for a GFM-based GAD approach.