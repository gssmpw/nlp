\section{Related Work}
\subsection{Graph Anomaly Detection} 
Existing GAD methods can be generally classified into unsupervised and supervised methods \cite{ma2021comprehensive, qiao2024deep}, depending on the assumption regarding the presence of normal and anomaly labels.
The fully supervised approaches have recently achieved remarkable progress by leveraging both labeled normal and abnormal nodes \cite{tang2022rethinking, liu2021pick, gao2023addressing, wang2023open}. These methods are primarily designed to improve message aggregation in graph neural networks (GNNs) by reducing heterophilic edges from both spectral and spatial perspectives, effectively mitigating the over-smoothing problem in GAD \cite{tang2023gadbench, qiao2024deep}. Fully supervised methods are highly effective in GAD as they treat the task as an imbalanced classification problem. However, the requirement for both normal and abnormal labels significantly hinders their applications to scenarios where labeled nodes are difficult to obtain.

Unsupervised methods, which typically assume the absence of both normal and abnormal labels, have garnered significant attention due to their more practical setting assumption on data labels. They generally incorporate some conventional techniques such as reconstruction \cite{ding2019deep}, one-class classification \cite{wang2021one, zhou2021subtractive,qiao2024truncated}, contrastive learning \cite{liu2021anomaly, pan2023prem}, and adversarial learning \cite{ding2021inductive} into graph learning to capture the normal patterns within the graph and then assign an anomaly score for each node based on its deviation from the normal patterns. However, these methods still follow the paradigm of training and inference on the same graph, making them struggle to generalize to new/unseen graphs due to the distribution shift between training and testing set.


% Unsupervised methods can operate on data without labels, but their constraints are often too strict for real-world applications. 
%  Due to the strict constraints of the unsupervised methods and the fact that normal labels are generally easier to obtain than abnormal labels in real-world applications \cite{qiao2024generative}, semi-supervised methods that consider partially known normal labels have been gradually introduced. These methods are more aligned with practical scenarios. Some unsupervised methods can be adapted to semi-supervised settings by directly applying normal pattern extraction techniques to the clean normal samples~\cite{ding2019deep, wang2021one, liu2021anomaly}. However, these methods still follow the paradigm of training and inference on the same graph making them struggle to generalize to the new graph from different domains.

% GGAD \cite{qiao2024generative} is the first semi-supervised method to address the scenario where only a subset of normal labels is available. It generates pseudo-anomalous nodes from partially labeled normal nodes to provide effective negative samples for training a discriminative one-class classifier. 



\subsection{Foundation Models for GAD}
Generalist models have recently achieved significant progress on non-graph data by leveraging large pre-trained models to facilitate generalized pattern recognition in diverse downstream tasks, such as generalist anomaly detection on images \cite{zhou2023anomalyclip,zhu2024toward}. However, applying these methods to graph data remains highly challenging due to the absence of such pre-trained models \cite{sun2023all, liu2023graphprompt, wen2023voucher} on graph data.
% Despite significant advancements in foundation models across various domains, the development of GFMs is still in its early stages. Recent studies have shown initial successes in specialized areas like knowledge graphs \cite{} and molecular graphs 
The main challenge in designing a GFM is capturing invariance across diverse graphs while mapping them into a shared representation space to enable positive transfer between training and inference \cite{maoposition,beaini2023towards,galkin2023towards}. Currently, most GFM models use prompt learning to enable the knowledge transfer across graphs for general graph tasks \cite{liu2023graphprompt, fang2024universal}.
% For example, GraphPrompt \cite{liu2023graphprompt} builds a pre-training and prompting framework designed for both graph-level and node-level classification tasks. It learns a prompt that helps downstream tasks identify and leverage the most relevant knowledge from the pre-trained model in a task-specific way.
% While GFMs have made significant progress in general graph tasks, 
However,
they still struggle to generalize to the GAD task due to the inherently infrequent, irregular, and heterogeneous
abnormality patterns in GAD datasets \cite{liu2024arc, niu2024zero, qiao2024deep}. Therefore, 
% some generalist methods that versatilely support zero-shot inference and sample-efficient tuning have been proposed for graph anomaly detection very recently by training on source graphs and evaluating on test graphs
some recent efforts attempt to devise the GFMs for GAD \cite{niu2024zero, liu2024arc}.
ARC \cite{liu2024arc} is one of such methods, a fine-tuning GFMs for GAD, enabling a ``one-for-all‚Äù GAD model through a fine-tuning mechanism. 
% It introduces an ego-neighbor residual graph encoder that learns node embeddings sensitive to abnormalities. During inference, a cross-attentive in-context anomaly scoring module is used to predict node abnormality by leveraging a few-shot set of normal samples.
UNPrompt \cite{niu2024zero} is the first zero-shot generalist GAD method that learns generalized neighborhood prompts, allowing latent node attribute predictability to serve as a generalized anomaly measure.
% across diverse datasets without requiring fine-tuning on the target dataset.
Different from ARC and UNPrompt, which address the GAD-oriented GFMs partially under few-shot or zero-shot settings, AnomalyGFM can effectively capture abnormalities across graphs from different domains while supporting both zero-shot inference and few-shot prompt fine-tuning/inference. This offers more versatile abilities for a GFM-based GAD approach.