\newpage
\appendix
\onecolumn
\section{Implementation Details}\label{app:imp_detail}  All our models are trained on 4 NVIDIA A100 GPUs. For the training instance generation, the filtering threshold, $tau$, is set as $0.85$. 878 text prompts were generated by prompting GPT-3.5-Turbo. We split 50 prompts from generated prompts to build the valid set. Then, we inferred our base models to sample five videos for each text prompt to build training text-video instances. 
For the training of the video patch construction, we set $h_n=3$ and $w_n=3$. For the patch reward construction, since we aim to post-train the VideoScore-v1.1, which already has strong video evaluation abilities, we prompt GPT-4o to evaluate video patches from 492 text-video pairs (sampled from training data) to decrease API cost. It contains 4,428 labeled video patches. 
We split them by the ratio 6:1:3 into training, validation, and test datasets to continually train VideoScore-v1.1, building our patch reward model. The batch size is 64, the learning rate is 1e-6, and the epoch is 10. 
Then we evaluate all generated text-video instances by our reward models to build training pairs for Gran-DPO. To ensure effective and efficient post-training, we used the LoRA technique~\cite{lora} to conduct the post-training. Specifically, the LoRA rank was set by 64, the learning rate was set by 1e-4, and the training step was 8k. Following the setting of base models, the inference diffusion steps of T2V-Turbo-v1 and v2 are 8, and CogVideoX-2B is 50. 

\section{Case Study of Patch Scores}\label{app:case-patch-score}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figure/app_cases.pdf}
    \caption{Cases to show the evaluated patch rewards.}
    \label{fig:app-case}
\end{figure*}
\begin{table}[th]
    \centering
    \caption{The cases to show patch rewards from human annotations and GPT-4o. ``0'', ``1'', and ``2'' denote the indexes of patch columns and rows.}
    \begin{tabular}{>{\centering\arraybackslash}p{0.07\textwidth}|>{\centering\arraybackslash}p{0.12\textwidth}>
    {\centering\arraybackslash}p{0.12\textwidth}>
    {\centering\arraybackslash}p{0.12\textwidth}|>
    {\centering\arraybackslash}p{0.1\textwidth}>
    {\centering\arraybackslash}p{0.1\textwidth}>
    {\centering\arraybackslash}p{0.1\textwidth}}%{cccc|ccc}
        \toprule
        - & \multicolumn{3}{c|}{\makecell{at night 2:00am a boy a name virat who crossing\\ the river and also he enjoys the changing seasons \\amidst the slow waves of the river.}} & \multicolumn{3}{c}{\makecell{A cute dog  but getting startled by a loud \\noise and jumping into a bowl of popcorn}} \\
        \midrule
        - & 0 & 1 & 2 & 0 & 1 & 2  \\
        \midrule
        0 & 1.8/1.2 & 2.2/1.2 & 2.2/1.6 & 1.8/1.0 & 1.8/2.3 & 1.8/2.3 \\
        \midrule
        1 & 1.8/1.0 & 1.8/1.4 & 2.2/1.2 & 1.6/1.9 & 1.8/2.2 & 1.6/1.4 \\
        \midrule
        2 & 1.8/1.1 & 1.8/1.4 & 1.8/1.0 & 1.6/1.7 & 1.6/1.5 & 1.6/1.5 \\
        \bottomrule
    \end{tabular}
    \label{tab:app-case}
\end{table}

We also demonstrate some cases to show the positive correlations between GPT-4o patch scores and human annotations. For the shown AI-generated videos, which are collected by VideoScore, we present the video contents in Figure~\ref{fig:app-case} and the patch reward scores in Table~\ref{tab:app-case}. Note that the complete prompt of the first case is ``at night 2:00am a boy a name virat who crossing the river and also he enjoys the changing seasons amidst the slow waves of the river.''. 

In the first case, there is a sudden ``white back'' in the left regions. Consequently, the patch scores of these regions, such as ``(0,0)'',``(1,0)'',``(2,0)'', and ``(2,1)'' are relatively low in both human annotation and GPT-4o scores. In the second case, some patches of the ``double-walled bowl'' \eg, ``(2,1)'' and ``(2,2)'', are in contrast to the real world, thus their patch scores are relatively low in both reward methods. The results also demonstrate a certain positive correlation between GPT-4o and human annotations, confirming its reliability in serving as our teacher model.

\section{Details of Used Instructions}\label{app:instructions}
\begin{figure*}[h]
    \begin{tcolorbox}[title={Instructions for Human Annotators to Label Video Patch Scores.}] 
- Task Brief: judging and evaluating the quality of AI-generated videos (generated by providing a text prompt to AI models).\newline
- Description: \newline
    - For an AI-generated video, we divide 9 video patches along the height and width of the video according to a 3*3 grid. I will provide you with the **text prompt**, the **generated video**, and its **video patches** with coordinates (the coordinate of the left-upper patch is (0,0)). \newline
    - Please give patch-level fine-grained scores for all video patches to represent the generation quality of this video patch. \newline
    - Please consider the following 5 dimensions to evaluate the video patch:\newline
        (1) visual quality: the quality of the video patch in terms of clearness, resolution, brightness, and color.\newline
        (2) temporal consistency: the consistency of objects or humans or background in video patch.\newline
        (3) dynamic degree: the degree of dynamic changes.\newline
        (4) text-to-video alignment: the alignment between the text prompt and the video patch content. Note that please only evaluate the semantic consistency between the **video patch** and **the text part related to this patch**, instead of the whole text (The semantic consistency between a video patch and the whole input text should always be very low).\newline
        (5) factual consistency: the consistency of the video patch content with the common-sense and factual knowledge.\newline
    - Annotation Format: For each dimension, output a number from [1,2,3,4],
        in which '1' means 'Bad', '2' means 'Average', '3' means 'Good', 
        '4' means 'Real' or 'Perfect' (the video is like a real video)\newline
    - Note: \newline
        1. I will provide the evaluation scores of these 5 dimensions of the original video, which can provide some reference for the evaluation of the video patch.\newline
        2. Please write the evaluation result in the file named "evaluation\_results.xlsx".\newline
    \end{tcolorbox}
\caption{Instructions for human annotators to label video patch scores.}
\label{box:human-annotation-instruction}
\end{figure*}


\begin{figure*}[h]
    \begin{tcolorbox}[title={Instruction for GPT-3.5-Turbo to Generate Training Text Prompts}] 
    \#\# Background\newline
    You are an expert in generating user queries. I will provide you with an evaluation dimension, and your task is to generate user queries based on this dimension. The generated queries will be used as input for video generation models, allowing me to evaluate the quality of the generated videos based on the specified dimension. Your goal is to generate high-quality and diverse text queries that align with the provided evaluation dimension.\newline
    
    \#\#  Input Format\newline
    I will provide you with an evaluation dimension and some good examples in the following JSON format:\newline
    \{\{\newline
    \quad"dimension": a string indicating the evaluation dimension,\newline
    \quad"examples": a list of JSON-formatted data pieces, each representing a good example for the given dimension\newline
    \}\}\newline
    \newline
    \#\#  Output Format\newline
    You should generate a list of high-quality and diverse query examples that are closely related to the provided dimension and examples. The generated examples should follow the writing style and tone of the provided examples. The output should be in the format of a JSON list, with the same data format as the examples in the input.\newline
    \newline
    \#\#\# Notions\newline
    1. Closely follow the writing style and tone of the provided examples when generating new queries.\newline
    2. Output only the JSON-formatted list of generated queries, without any additional words or characters.\newline
    \end{tcolorbox}
\caption{Instruction for GPT-4o to Generate Evaluation Scores for the Input Video Patch.}
\label{box:text-generation-instruction}
\end{figure*}

\begin{figure*}[h]
    \begin{tcolorbox}[title={Instruction of GPT-4o to generate evaluation scores for the input video patch.}] 
    \#\# Background \newline
    Suppose you are an expert in judging and evaluating the quality of AI-generated videos. For a generated video, we divide 9 video patches along the height and width of the video according to a 3*3 grid. I will provide you with the text prompt, the generated video, one of its video patches, and the coordinate of the video patch (the coordinate of the left-upper patch is (0,0), the first number means the index of rows, and the second number means the index of columns). Please watch the following frames of a given video and one of its split patches, and see the text prompt for generating the video. Then, you should give patch-level fine-grained scores for this video patch to represent the generation quality of this video patch. The evaluation of the video patch\'s quality should cover the following dimensions: \newline
        \{\{ \newline
            "visual quality": "the quality of the video in terms of clearness, resolution, brightness, and color.", \newline
            "temporal consistency": "the consistency of objects or humans in video.", \newline
            "dynamic degree": "the degree of dynamic changes.", \newline
            "text-to-video alignment": "the alignment between the text prompt and the video content.", \newline
            "factual consistency": "the consistency of the video content with the common-sense and factual knowledge." \newline
        \}\}
     \newline
    \#\#  Evaluation Requirments \newline
    1. You should do **fine-grained and patch-leval** evaluation for **assessing the quality of the video patch, instead of the whole video**. The role of the entire video is to provide the context of the video patch to understand it better.   \newline
    2. You should detail view the patch of the input video to give a reliable and rigorous evaluation score of this patch, both considering the patch\'s local quality and its correlation with the whole video. \newline
    3. You should carefully evaluate the reality of the generated video patch to provide reliable evaluation results. \newline
     \newline
    \#\#  Output Requirments \newline
    1. For each dimension, output a number ranging from 0 to 10, in which ’0’ means the worst and '10' means the best.  \newline
    2. The output should be in JSON format. \newline
    3. Only output the JSON-format result without any other words. \newline
    4. Here is an output example: \newline
        \{\{ \newline
            "visual quality": An int value ranging from 0 to 10, representing the visual quality score of the evaluated video, \newline
            "temporal consistency": An int value ranging from 0 to 10, representing the temporal consistency score of the evaluated video, \newline
            "dynamic degree": An int value ranging from 0 to 10, representing the dynamic degree score of the evaluated video, \newline
            "text-to-video alignment": An int value ranging from 0 to 10, representing the text-to-video alignment score of the evaluated video, \newline
            "factual consistency": An int value ranging from 0 to 10, representing the factual consistency score of the evaluated video \newline
        \}\}
     \newline
    \#\#  Input information \newline
    - Text prompt: "\{text\_prompt\}". \newline
    - Coordinate of the patch: (\{m\}, \{n\}). \newline
    - All the frames of the video and its evaluated patch are as follows: \newline
    \end{tcolorbox}
\caption{Instruction for GPT-4o to generate evaluation scores for the input video patch.}
\label{box:gpt4-patch-eval}
\end{figure*}

% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
