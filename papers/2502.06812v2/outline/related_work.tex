\section{Related Work}\label{{sec:related}}
\paragraph{Diffusion-based VGMs.}
Recently, with the superiority of diffusion models in image generation tasks~\cite{DM1,DM2,DDIM,SDXL,LatentDM}, many studies also proposed DM-based text-to-video generation models~\cite{VDM,LAVIE,Text2Video-Zero,VC2,CogVideoX,ModelScope}. 
The structures of DMs typically have two main branches. One is UNet-based models~\cite{UNet,VC2,ModelScope}, such as VideoCrafter1~\cite{VC1}, which used 3D-UNet~\cite{3DUNet} to effectively model temporal information. Another is transformer-based models~\cite{DiT,EasyAnimate-DiT,CogVideo,CogVideoX}, such as CogVideo~\cite{CogVideo,CogVideoX}, which stacked expert transformer-blocks to implement diffusion denoising. 

Due to the limited amount and quality of text-to-video datasets~\cite{webvid-1m,OpenVid-1M,UCF101} compared to text-to-image datasets~\cite{LAION-5B}, some works used pre-trained image generation models to build VGMs~\cite{show-1,AlignLatent,Text2Video-Zero}, or combined image and video datasets to optimize VGMs~\cite{VC1,VC2}, leading to better foundational ability of VGMs.

\paragraph{Alignment of generative AI models.}
Fine-tuning pre-trained generative models with reward models has proven effective in enhancing their overall generation quality. 
It has been successfully applied in various fields \eg, text generation~\cite{LMFI,GPT-3.5,GPT-4,richrag} and visual generation~\cite{AlignDM,DRaFT,DPOK,DMDPO,RLDMDiverse,t2v-turbo,t2v-turbo-v2,InstructVideo}. 

Recent studies in text generation~\cite{TLCR,Step-DPO} have gone beyond rewarding entire generated responses. They introduced local feedback at the step or token level to provide fine-grained signals, enhancing LLMs' reasoning and generation capabilities. 
For text-to-video generation, advanced studies also leveraged reward models to evaluate the global quality of generated videos. For example, some studies~\cite{DRaFT,DDPO,DPOK,t2v-turbo,InstructVideo} used HPSv-2~\cite{HPSv2} or PickScore~\cite{PickScore} to measure the alignment between text and images. Recently, VideoScore~\cite{VideoScore} proposed to evaluate video quality by fine-tuning visual LLMs with human-annotated labels. 
Despite these advancements, there is still room for exploring the role of local feedback in videos to enhance the abilities of VGMs. In this study, we propose a new post-training framework for VGMs that incorporates patch rewards, enabling a better quality of the generated videos.