
\section{Experiment Settings}\label{sec:experimetn}

\begin{table*}[h]
    % \small
    \centering
    \caption{Overall experimental results under the evaluation of VBench and VideoScore. The best results are indicated in bold. Results that are better than the base models are highlighted with underscores.
    }
    \resizebox{0.99\linewidth}{!}{\begin{tabular}{lcccccccccccc}
        \toprule
        \multirow{2}{*}{Models} & \multicolumn{6}{c}{VBench} & \multicolumn{6}{c}{VideoScore} \\
        \cmidrule(lr){2-7} \cmidrule(lr){8-13}
        & \makecell{Imaging\\Quality} & \makecell{Multiple\\Objects} & \makecell{Human\\Action} & \makecell{Spatial\\Relationship} & \makecell{Scene} & Avg. & \makecell{Visual\\Quality} & \makecell{Temporal\\Consistency} & \makecell{Dynamic\\Degree} & \makecell{Text-to-video\\Alignment} & \makecell{Factual\\Consistency} & Avg. \\
        \midrule
        Lavie & 61.90 & 33.32 & 96.80 & 34.09 & 52.69 & 55.76 & 2.5655 & 2.2523 & 2.8385 & 2.4456 & 2.1970 & 2.4598 \\
        ModelScope & 58.57 & 38.98 & 92.40 & 33.68 & 39.26 & 52.58 & 2.0777 & 2.1142 & 2.7028 & 2.0670 & 1.8980 & 2.1719 \\
        VideoCrafter2 & 67.22 & 40.66 & 95.00 & 55.29 & 25.13 & 56.66 & 2.6238 & 2.3486 & 2.7142 & 2.4670 & 2.2096 & 2.4726 \\
         \midrule
        T2V-Turbo-v1 & 72.09 & 51.30 & 95.00 & 40.35 &  \textbf{56.35} & 63.02 & 2.6388 & 2.4419 & 2.8022 & 2.5744 & 2.2944 & 2.5503 \\
        \makecell{\quad + HALO} & 72.07 & \underline{54.97} & \underline{95.00} & \underline{41.10} & 54.72 & \underline{63.57} & \underline{2.6430} & \underline{2.4453} & \underline{2.8093} & \underline{2.5775} &  2.2880 & \underline{2.5526} \\
        \midrule
        T2V-Turbo-v2 & \textbf{72.50} & 59.16 & 96.40 & 35.58  & 53.60 & 63.45  & 2.5076 & 2.2961 & 2.8507 & 2.5132 & 2.2219 & 2.4779  \\
        \makecell{\quad + HALO} & 69.11 & \underline{67.50} & \underline{97.60} & \underline{53.19} & \underline{55.07} & \underline{68.49}& \underline{2.5508} & \underline{2.3502} & 2.8417 & \underline{2.5694} & \underline{2.2252} & \underline{2.5074} \\
        \midrule
        CogvideoX-2B       & 60.88 & 68.72 & 97.80 & 65.23 & 51.09 & 68.74 & 2.8128 & 2.6666 & 2.8571 & 2.7875 & 2.5045 & 2.7257 \\
        \makecell{\quad + HALO} & \underline{61.90} & \textbf{\underline{72.91}} & \textbf{\underline{98.00}} & \textbf{\underline{65.24}} & \underline{51.16} & \textbf{\underline{69.84}} & \textbf{\underline{2.8510}} & \textbf{\underline{2.6887}} & \textbf{\underline{2.9160}} & \textbf{\underline{2.8396}} & \textbf{\underline{2.5211}} & \textbf{\underline{2.7633}} \\
        \bottomrule
    \end{tabular}}
    \label{tab:overall}
\end{table*}


\paragraph{Evaluation prompts and methods.} 
In this section, we demonstrate our experimental results and further analyses to sufficiently validate the effectiveness of our proposed method, \ours{}. We conduct our experiments on VBench~\cite{vbench} prompts and use VBench and VideoScore~\cite{VideoScore} as our evaluation methods. VBench is a widely used benchmark to assess the capabilities of VGMs from 16 predefined dimensions. We choose five dimensions: image quality, multiple objects, human action, spatial relationship, and scene, which we focus on and have optimized space, to evaluate our method.\footnote{\href{https://github.com/Vchitect/VBench/tree/master}{https://github.com/Vchitect/VBench/tree/master}}  VideoScore~\cite{VideoScore} is a video quality evaluator fine-tuned from a visual LLM, Mantis-8B-Idefics2~\cite{mantis}. Specifically, \href{https://huggingface.co/TIGER-Lab/VideoScore-v1.1}{VideoScore-v1.1} serves as our evaluator. It assesses the video quality from five dimensions: visual quality, temporal consistency, dynamic degree, text-to-video alignment, and factual consistency.

\paragraph{Baselines.} To implement our method, we select three VGMs as our base models: CogVideoX-2B~\cite{CogVideoX}, which utilizes the transformer-based diffusion module; T2V-Turbo-v1 and T2V-Turbo-v2~\cite{t2v-turbo,t2v-turbo-v2}, which leverage video RMs and consistency models~\cite{CM} to post-train VideoCrafter-2.0~\cite{VC2}. Notably, all training videos in our study were generated by base models, rather than using real videos collected from the web, which is common practice in existing post-training of VGMs with video RMs~\cite{t2v-turbo,t2v-turbo-v2}. 
This approach allows us to explore the upper limits of the base models through themselves. Therefore, we evaluate our method as a plug-in across various base models. Additionally, we assess the performance of several popular VGMs, \ie, Lavie~\cite{LAVIE}, ModelScope~\cite{ModelScope}, and VideoCrafter-2.0, to highlight the superior capabilities of our base models. 

\paragraph{Implementation details.} For training data generation, 878 text prompts were produced from GPT-3.5-Turbo. Then, we enable our base models to sample five videos for each prompt. To build video patches, we set $h_n=3$ and $w_n=3$. We first prompt GPT-4o to evaluate video patches from 492 text-video pairs (sampled from training data). It contains 4,428 labeled video patches. For continuously training VideoScore-v1.1, the batch size is 64, the learning rate is 1e-6, and the epoch is 10. Then, we use our reward models to assess all text-video instances, hence building training pairs for Gran-DPO. For efficient and effective training, we use the LoRA technique~\cite{lora} to fine-tune base models. The LoRA rank was set by 64, the learning rate was set by 1e-4, and the training step was 8k. 
Following the setting of base models, the inference diffusion steps of T2V-Turbo-v1 and v2 are 8, and CogVideoX-2B is 50. 
Due to limited space, more details are demonstrated in Appendix~\ref{app:imp_detail}.

\begin{table}[]
    \centering
    \caption{Comparison results of ablation experiments. 
    }
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{lcccc}
    \toprule
        \multirow{2}{*}{Models} & \multicolumn{2}{c}{VBench} & \multicolumn{2}{c}{VideoScore} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
        & Average & Difference &  Average & Difference \\
    \midrule
        \ours{}        & 68.49 & - & 2.5074 & - \\
    \midrule
        \quad w/o PatchDPO & 63.25 & -5.24 & 2.4602 & -0.0472 \\ 
        \quad w/o VideoDPO & 63.27 & -5.22 & 2.4791 & -0.0283\\
        \quad w/o PairWeight  & 62.86 & -5.63 & 2.4891 & -0.0183\\
        \quad w/ PickScore  & 64.27 & -4.22 & 2.4981 & -0.0093\\
        \quad w/ HPSv2 & 64.63 & -3.86 & 2.4833 & -0.0241\\
    \bottomrule
    \end{tabular}
    }
    \label{tab:ablation}
\end{table}

\begin{table}[!h]
    \centering
    \small
    \caption{Consistency evaluation of patch reward models.
    % \cx{Is GPT-4o patch or global?}\stw{patch}
    }
    \resizebox{0.99\linewidth}{!}{
    \begin{tabular}{p{0.15\linewidth}p{0.35\linewidth}>{\centering\arraybackslash}p{0.4\linewidth}}
    \toprule
        Model A &  Model B & Spearman Correlations \\
    \midrule
        Human & GPT-4o & 0.4023 \\
        GPT-4o & Patch Reward Model & 0.6062 \\
        Human & Patch Reward Model & 0.3684 \\
    \bottomrule
    \end{tabular}
    }
    \label{tab:eval_patch_rw}
\end{table}
\section{Experimental Results}
In this section, we demonstrate our experimental results and further analyses to validate the effectiveness of \ours{}.
\subsection{Overall Results}\label{sec:overall}
We provide overall experimental results in Table~\ref{tab:overall}, and present case comparisons between the base models and those post-trained by \ours{} in Figure~\ref{fig:case}. 

The quantitative comparison indicates that our proposed method significantly enhances the performance of base VGMs across both evaluation methods. This finding underscores its effectiveness and generalization to various evaluation metrics. 
Notably, our model not only improves the pre-trained VGM (CogVideoX-2B), but also further boosts the performance of VGMs that are post-trained by video rewards, \ie, T2V-Turbo-v1 and v2. This supports our assumption that merely introducing video rewards still overlooks localized patch defects, thereby limiting model capabilities. By introducing our fine-grained patch rewards, we could alleviate this issue and further enhance the overall quality of generated videos.

We indicate the unsatisfying generated video patches by red boxes. The visual comparison distinctly illustrates that though our base models could generate videos from prompts that appear good overall, they occasionally exhibit local errors. For example, in the first video generated by the base model, the child has three hands, which defies natural laws. However, our generated video presents a more reasonable video with a bright and colorful background. In the second case, the robot DJ in the upper video fails to play the turntable, whereas our generated video aligns better with the text prompt. Lastly, our generated video successfully supplements the bowl missed by the base model.


\begin{figure*}[h]
    \centering
    \includegraphics[width=1\textwidth]{figure/case_study_grid_indicate.pdf}
    \caption{The visualized comparison between our proposed model \ours{} and baselines.
    % \stw{add grid.}
    }
    \label{fig:case}
\end{figure*}
\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{figure/all_preference_consist_train_amount_reward_trends_6.pdf}
    \caption{Further analyses about our reward models and training process.
    }
    \label{fig:three-analyses}
\end{figure*}
\subsection{Results of Ablation Experiments}\label{sec:ablation}
We conduct several ablation experiments to validate the impact of our key modules. We examine three model variants by removing components: patch DPO loss (w/o PatchDPO), video DPO loss (w/o VideoDPO), and DPO pair weights (w/o PairWeight). Additionally, we replace our video reward model with two commonly used alternatives, PickScore~\cite{PickScore} and HPSv2~\cite{HPSv2}, resulting in two variants, w/ PickScore and w/ HPSv2. T2V-Turbo-v2 serves as a representative model for experiments due to its fast inference speed. The results are shown in Table~\ref{tab:ablation}. 

The comparison reveals that relying solely on either patch or video DPO results in underperformance compared to our complete model. This phenomenon proves that using local and global rewards collaboratively could complement each other, leading to better generation quality. Meanwhile, the decreased performance of w/o PairWeight compared to \ours{} also reveals that it is necessary to consider the different importance of DPO training pairs. It guides model optimization to focus more on challenging samples, resulting in better generation performance. 
We noticed that changing our video RM also decreases the model performance. This may be attributed to these reward models independently evaluating the video frames, potentially neglecting the temporal evaluation. Additionally, the scales and distributions of these reward models differ from our patch reward model, which may introduce inconsistencies during optimization.  

\subsection{Evaluation of Patch Reward Model}\label{sec:human-eval}
To ensure the effectiveness and reliability of our patch reward model, we perform human annotations to assess the reward quality of GPT-4o and our distilled patch RM. Following VideoScore~\cite{VideoScore}, we define five evaluation dimensions for video patches with scores ranging from 1 to 4. The annotated videos are sampled from VideoScore-collected AI-generated videos and prompts. Finally, valid videos are validly annotated with 360 labeled video patches. The annotation guidelines are presented in Appendix~\ref{app:instructions}. 

Concretely, we calculate Spearman's rank correlation coefficient between two patch reward models to assess their correlations. We demonstrate the evaluation results in Table~\ref{tab:eval_patch_rw}. The evidently positive correlation between the GPT-4o patch RM and human annotations confirms the validity of using GPT-4o as the teacher model to train our patch reward model. Furthermore, our patch reward model exhibits a significantly positive correlation with the GPT-4o patch RM, underscoring the effectiveness of our distillation. Lastly, the positive correlation between the patch reward model and human annotations further validates the reliability of our patch reward model. We also provide cases to show the patch score correlations in Appendix~\ref{app:case-patch-score}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figure/patch_reward_distri_two.pdf}
    \caption{Two types of patch reward distributions. 
    }
    \vspace{-5pt}
    \label{fig:patch-distribution}
\end{figure}
\subsection{Visualization of Reward Distributions}
\paragraph{Consistency distribution of reward preference.}
To quantitatively analyze the importance of considering patch rewards, we conduct visualization experiments on our reward distributions. 
Preliminarily, we compare the preference consistency between video and patch rewards. For a generated video, we obtain its video and patch rewards from our two specialized reward models. We average the patch rewards from nine patches to generate a reward scalar from patch reward models. Then, for five generated videos from the same text prompt, we perform pairwise comparisons of the preference consistency between their video rewards and patch rewards. If the preferences from the video and patch rewards are the same, we label the pair as ``Consistent'', if one of the rewards has no preferences between the two videos (reward values are identical), we label the pair as ``None'', if the preferences differ, they are labeled ``Inverse''. We conduct this analysis on all our training prompts by the results from T2V-Turbo-v2. The distribution of preference consistency is visualized in Figure~\ref{fig:three-analyses} (a). 

From the comparison result, we observe some consistency between video and patch rewards, but they are not exactly the same. This phenomenon further validates the following two key points: (1) The reliability of our patch reward models, which appears to be of a certain correlation with the video rewards. (2) Patch rewards may bring more fine-grained information beyond what is offered by video rewards. Therefore, they do not demonstrate exactly consistent preferences. This phenomenon also reinforces the value of introducing patch rewards to optimize VGMs.

\paragraph{Distribution of patch reward variances.}
Then, we define and visualize two types of patch reward distributions to further analyze our model. The first is the \textit{inner video distribution}, which measures the variance of patch rewards within a video, quantitatively reflecting the patch reward discrepancies inner a video. The second is the \textit{sorted distribution}, which demonstrates the reward distributions from the perspective of sorted patch values. Specifically, for each video with nine patch rewards, we first sort these reward values to obtain nine levels of rewards. ``L0'' means the lowest reward and ``L8'' denotes the highest. Then, we analyze the nine-level rewards across all videos and visualize the distribution of each reward level, leading to nine reward distributions. To reflect the effectiveness of our method, we visualize the sorted distributions both \textit{before} and \textit{after} fine-tuning with  \ours{}. The visualization results of these two distributions are shown in Figure~\ref{fig:patch-distribution}.

From Figure~\ref{fig:patch-distribution}~(a), we observe significant reward variance among different patches within the same video, which confirms that different patches have different generation difficulty and quality. Therefore, it is necessary to consider and model the discrepancies among different patches to provide fine-grained feedback for the optimization of VGMs. According to Figure~\ref{fig:patch-distribution}~(b), 
we notice that distributions of patch values have significant discrepancies among different levels. This phenomenon proves our previous assumption and observation that VGMs usually generate videos containing good overall quality but sometimes have localized errors. After fine-tuning the VGM by our method, the distributions of all levels shift to the right. These results validate that our proposed method not only could improve the overall quality of generated videos but also could enhance the localized and fine-grained quality of AI-generated videos. As a result, it could generate more realistic and reliable videos.

\subsection{Analysis of Training Settings}
We further analyze the detailed performance of the model during training and the impact of different training configurations on the model performance. 
First, we visualize the DPO reward trendlines of our method during the training of our model (the T2V-Turbo-V2 as the representation). Specifically, we demonstrate the reward trendlines of winners and losers in inner DPO pairs. The results are presented in Figure~\ref{fig:three-analyses} (b). 
It is evident that during the training process, the rewards of winners become greater and the loser rewards gradually drop down. This phenomenon proves the effectiveness of our training algorithm and the quality of our constructed DPO pairs that bring valuable information to optimize VGM to focus on fine-grained video generation quality, leading to better model performance.  

Then, we validate the influence of the amount of training data on the model performance. Concretely, we scaled the number of DPO pairs training from 0 to 1.2k with T2V-Turbo-v2 as a representative experiment model. The trends in model performance with the amount of training data are provided in Figure~\ref{fig:three-analyses} (c). Obviously, the average scores of VideoScore and VBench generally increase with increasing amounts of training data. This fits with our intuition because more training data often introduces more valuable information to optimize text-to-video generation models. Due to limited resources and generated training instances, we do not scale our training amount further and leave the expansion experiment to our future work. 