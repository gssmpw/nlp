\begin{abstract}
The emergence of diffusion models (DMs) has significantly improved the quality of text-to-video generation models (VGMs). However, current VGM optimization primarily emphasizes the global quality of videos, overlooking localized errors, which leads to suboptimal generation capabilities. 
To address this issue, we propose a post-training strategy for VGMs, \ours{}, which explicitly incorporates local feedback from a patch reward model, providing detailed and comprehensive training signals with the video reward model for advanced VGM optimization. 
To develop an effective patch reward model, we distill GPT-4o to continuously train our video reward model, which enhances training efficiency and ensures consistency between video and patch reward distributions. Furthermore, to harmoniously integrate patch rewards into VGM optimization, we introduce a granular DPO (Gran-DPO) algorithm for DMs, allowing collaborative use of both patch and video rewards during the optimization process.
Experimental results indicate that our patch reward model aligns well with human annotations and \ours{} substantially outperforms the baselines across two evaluation methods. Further experiments quantitatively prove the existence of patch defects, and our proposed method could effectively alleviate this issue.
\end{abstract}

\section{Introduction}\label{sec:intro}
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figure/intro_case.pdf}
    \caption{A generated video with local flaws (red boxed). 
    }
    \vspace{-0.5cm}
    \label{fig:patch-defect-case}
\end{figure}

The advent of diffusion models has significantly elevated the quality of AI-generated videos, enabling the creation of high-resolution and visually appealing AI videos. 
This advancement holds great promise for various industrial applications, such as video editing and artistic creation~\cite{Pictory,runwayml,synthesia}.

Typically, the pre-training of VGMs relies on extensive text-to-video datasets~\cite{webvid-1m}, leading to strong foundational generation abilities. However, the varied quality of these datasets still limits the potential of VGMs. 
Inspired by the alignment of large language models (LLMs)~\cite{rlhf}, recent studies~\cite{t2v-turbo} introduced reward models to assess the video generation quality. This approach facilitates the alignment of VGMs during the post-training stages and improves the overall generation quality. 

Although current VGMs can generally yield visually appealing videos, they still exhibit spatially localized errors in certain video patches. 
Unfortunately, these localized errors manifest significant hallucinations that diverge from reality, resulting in poor user experience and awkward video generation. 
For example, as illustrated in Figure~\ref{fig:patch-defect-case}, when given the input prompt ``a dog wagging its tail happily'', an advanced VGM (T2V-Turbo-v1~\cite{t2v-turbo}) generates a video that \textit{appears commendable at first glance}. However, in the video patches indicated by red boxes, there are \textit{two tails} for the dog, which contradicts real-world expectations. 

In this paper, we propose a post-training framework, \ours{}, that explicitly considers patch-level feedback for generated videos to enhance the alignment of VGMs. 
Specifically, our framework \textbf{HA}rnesses \textbf{LO}cal feedback from a specialized patch reward model to identify localized and fine-grained defects in generated videos. 
We integrate it with a video reward model that assesses the global quality of generated videos, thereby ensuring that models do not prioritize local details at the expense of the overall quality of videos.

To implement our approach, we first distill GPT-4o~\cite{GPT-4} to continuously optimize the video reward model, resulting in our patch reward model. This strategy leverages the video reward model's foundational ability to evaluate the video quality, thereby reducing the burden of data labeling and training resource allocation. Furthermore, originating from the same model enables patch and reward models to generate consistent reward distributions and scales, minimizing potential conflicts and providing uniform guidance during the optimization of VGMs. 
Next, to integrate patch rewards into VGM optimization, we develop a novel Gran-DPO algorithm. This algorithm first tailors a patch DPO from the classic Diffusion-DPO~\cite{DMDPO}, which serves as the video DPO. Then, the patch and video DPO losses are weightily combined to achieve a harmonious optimization of both local and global generation quality.

We conduct experiments using the VBench~\cite{vbench} prompts and evaluate models by VBench and VideoScore~\cite{VideoScore}. Experimental results confirm that \ours{} significantly outperforms all baselines. Further analyses reveal several key findings: \textit{First}, the patch reward model demonstrates positive correlations with human annotations, validating the efficacy of our optimization signals. \textit{Next}, patch reward preferences exhibit certain differences from video reward preferences, suggesting that patch rewards could introduce unique values beyond video rewards. 
\textit{Finally}, \ours{} effectively improves patch reward values, indicating its abilities in dealing with local defects.

The main contributions of our study are threefold:

(1) We introduce a patch reward model to produce localized feedback for generated videos, facilitating a fine-grained VGM optimization.

(2) We develop a Gran-DPO algorithm to leverage local and global rewards collaboratively, hence optimizing VGMs in a harmonious manner.

(3) Experiments prove the consistency of our patch reward model with human annotations and the capability of our method to address the problem of local defects.