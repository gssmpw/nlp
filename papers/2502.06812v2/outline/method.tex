\section{\ours{} Method}\label{sec:method}

In this section, we detail the techniques of our proposed method, \ours{}. 
Firstly, we provide essential definitions and functions of diffusion models and their DPO algorithms in Section~\ref{sec:preliminary}, serving as the background of our study.
Next, we introduce the construction of our reward model in Section~\ref{sec:rw}. 
Following that, Section~\ref{sec:dpo} illustrates the details of our Gran-DPO algorithm for diffusion models. Finally, the acquisition of training prompts is presented in Section~\ref{sec:prompt}. The framework of \ours{} is visualized in Figure~\ref{fig:model}.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=1\linewidth]{figure/method.pdf}
    \caption{The framework of our proposed method, \ours{}.
    }
    \label{fig:model}
\end{figure*}

\subsection{Preliminaries}\label{sec:preliminary}

\paragraph{Text-to-video diffusion models.}
Generally, a text-to-video generation model comprises three key modules: (1) A text encoder $\mathcal{C}$, which embeds the text prompt into a latent representation $\mathbf{c}$, serving as the conditional embedding for the diffusion process. (2) A video autoencoder $\mathcal{E}$-$\mathcal{D}$. It encodes a video $\mathbf{v}$ into (or decodes from) the latent space by 
$\mathbf{x}=\mathcal{E}(\mathbf{v}), \mathbf{v}=\mathcal{D}(\mathbf{x})$, $\mathbf{x}\in\mathbb{R}^{f\times h\times w\times c},\mathbf{v}\in\mathbb{R}^{F\times H\times W\times 3}$. $f$,$h$, and $w$ are the dimensions compressed from $F$,$H$, and $W$, representing the frame number, height, and width of videos, respectively. $c$ is the output channel number of $\mathcal{E}$. 
(3) A denoising module $\epsilon_\theta$, which conducts the forward and reverse diffusion processes to implement the training and inference of diffusion models. 

For training the diffusion module, the input text and video are firstly embedded into latent representations $(\mathbf{c}, \mathbf{x})$ ($\mathbf{x}$ also termed as $\mathbf{x}_0$). By a predefined scheduler $\{\alpha_t\}_{t=1}^T$, the forward diffusion adds random noise to $\mathbf{x}_0$ to build $\mathbf{x}_t$: 
\begin{equation}%\small
     \mathbf{x}_t = \sqrt{\overline{\alpha}_t}\mathbf{x}_0 + \sqrt{1-\overline{\alpha}_t}\epsilon, \epsilon\sim\mathcal{N}(\mathbf{0},\mathbf{I}),t\sim\mathcal{U}(1,T).
\end{equation}
Then, the denoising module performs the reverse diffusion to model the noise at the $t$-th step conditioned on the text embedding. The loss function~\cite{DM2} is:
\begin{equation}%\small
    \mathcal{L}(\theta) = \mathbb{E}_{\mathbf{x}, \mathbf{c}, \epsilon, t}\left[\|\epsilon - \epsilon_\theta(\mathbf{x}_t, \mathbf{c}, t) \|^2_2\right].
\end{equation}
The inference of the diffusion process starts by normally sampling the initial latent $\mathbf{x}_T\sim\mathcal{N}(\mathbf{0},\mathbf{I})$. Then it generates the video latent $\mathbf{x}_0$ via a stepwise reverse diffusion process: 
\begin{align}
% \small
    \mathbf{x}_{t-1} &= \frac{1}{\sqrt{\alpha_t}}(\mathbf{x}_t-\frac{1-\alpha_t}{\sqrt{1-\overline{\alpha_t}}}\epsilon_\theta(\mathbf{x}_t, t))+\sigma_t\mathbf{z}, \nonumber\\
    \mathbf{z}&\sim\mathcal{N}(\mathbf{0},\mathbf{1}), \;\;\text{if}\;\; t>1, \;\;\text{else}\;\; \mathbf{z}=\mathbf{0}.
\end{align}
To accelerate the inference, DDIM sampling~\cite{DDIM} is widely used to generate videos by fewer sampling steps than training.

\paragraph{DPO for diffusion models.}
The DPO algorithm has been widely adopted in the fine-tuning stage of LLMs, benefiting from its easy implementation and stable optimization. Recently, \citet{DPODM} also tailored DPO to diffusion models. The derived loss function is presented as follows:

\begin{align}
\small
    \mathcal{L}_{\text{DM-DPO}}(\theta) = -\mathbb{E}_{(\mathbf{x}^w,\mathbf{x}^l),t,(\mathbf{x}_t^w,\mathbf{x}_t^l)}\left[
    \log \sigma \left(
    -\beta T\left( \nonumber \right.\right.\right.\\ 
    \|\epsilon^w-\epsilon_\theta(\mathbf{x}^w_t ,\mathbf{c},t) \|^2_2-
    \|\epsilon^w-\epsilon_{\text{ref}}(\mathbf{x}^w_t,\mathbf{c},t) \|^2_2-\nonumber \\ \left.\left.\left.
    \left(
    \|\epsilon^l-\epsilon_\theta(\mathbf{x}^l_t ,\mathbf{c},t) \|^2_2-
    \|\epsilon^l-\epsilon_{\text{ref}}(\mathbf{x}^l_t,\mathbf{c},t) \|^2_2
    \right)
    \right) 
    \right) 
    \right].
\label{eq:diffusion_dpo}
\end{align}

$\mathbf{x}^w$ and $\mathbf{x}^l$ are the winner and the loser within a video pair, $\epsilon_{\text{ref}}$ is the reference model, $\sigma()$ denotes the sigmoid function, and $\beta$ is a hyperparameter to control regularization. The objective of this optimization is to enhance the diffusion module to optimize more on the winner $\mathbf{x}^w$ than the loser $\mathbf{x}^l$ while ensuring the optimized policy model $\epsilon_\theta$ remains close to the reference model $\epsilon_{\text{ref}}$. Following~\cite{DM2}, we simplify optimization by removing the weighting function.

\subsection{Construction of Patch Reward Model}\label{sec:rw}
We use VideoScore~\cite{VideoScore} as our video reward model, which evaluates the video quality by a fine-tuned visual LLM, Mantis-Idefics2-8B~\cite{mantis}. The fine-tuning process relies on human-annotated scores for AI-generated videos. VideoScore defines five evaluation dimensions, such as visual quality, text-to-video alignment, etc. For an input text-video pair, it predicts five scores on a scale of 1 to 4. We treat the average of five evaluation scores to derive the final video reward score, $v$:
\begin{equation}
    v = \frac{1}{|\mathcal{D}^{VS}|}\sum\nolimits_{d\in\mathcal{D}^{VS}}v^d.
    \label{eq:rw_avg}
\end{equation}
$\mathcal{D}^{VS}$ is the set of evaluation dimensions and $v^d$ denotes the reward score of the video for the specific dimension $d$.

Considering the lack of publicly available patch reward models, we propose fine-tuning our video reward model to create the patch reward model. This operation allows us to preserve similar distributions and scales between patch and video reward scores, while also simplifying the training task by leveraging the foundational evaluation capabilities of the video reward model. To mitigate the time and economic costs associated with human annotation, we employ GPT-4o~\cite{GPT-4} to label the quality of video patches and then distill it to build our patch reward model.  

\paragraph{GPT-4o labeling.} We divide a video into a $h_n\times w_n$ grid of video patch alongside height and width, where $h_n$ and $w_n$ are the number of divisions in height and width. Note that we do not segment videos along the time axis, as current AI-generated videos are relatively short (usually only a few seconds), resulting in a low information density across timestamps. We leave the task of temporal segmentation for future work. For segmented video patches, we define their indexes by referring to the upper left corner as $(0,0)$.

To ensure complete semantics during the evaluation, we also inject the entire video into the input, thereby contextualizing the evaluated video patch. Ultimately, we treat the task instruction, the text prompt, the video, a certain video patch, and its index as input for evaluation. Following VideoScore, we prompt GPT-4o to generate five-dimensional evaluation scores, which serve as training labels. The instruction for GPT-4o is presented in Appendix~\ref{app:instructions}.\footnote{We instruct GPT-4o to generate evaluation scores from 0 to 10 to mitigate the issue of identical scores. Then, these scores are normalized to a scale of 1 to 4 to align with VideoScore.} 

\paragraph{Distill GPT-4o to build the patch RM.} With the labeled data from GPT-4o, we fine-tune the video reward model by regression loss following VideoScore, as expressed below,
\begin{equation}\small
    \mathcal{L}_{\text{reg}}(\mathbf{v}) =\frac{1}{h_nw_n|\mathcal{D}^{VS}|}\sum_{i=1}^{h_n}\sum_{j=1}^{w_n}\sum_{d\in\mathcal{D}^{VS}} \| p^{d}_{ij} - g^{d}_{ij} \|_2^2.
\end{equation}
$p^{d}_{ij}$ denotes the reward score predicted by the patch reward model for the video patch in the index of $(i,j)$, and $g^{d}_{ij}$ is the GPT-4o-labeled score. The final patch reward $p_{ij}$ is also the average score of all dimensions similar to Eq.~(\ref{eq:rw_avg}).

\subsection{Granular DPO for VGMs}\label{sec:dpo}

\paragraph{Building pairwise training data for Gran-DPO.} 
Given the set of training prompts (the creation is introduced in Section~\ref{sec:prompt}), we first input every prompt into our base text-to-video generation models and sample several generated videos, thereby constructing the training text-to-video instances. Next, we employ our two specialized reward models to compute patch and video rewards for each instance. Finally, we develop a data builder to create the pairwise training instances for the Gran-DPO algorithm. Specifically, for a text prompt and its generated videos, we perform a pairwise comparison of their rewards and retain data pairs that satisfy one of the following criteria: 
(1) The video reward margin between two videos exceeds the statistical median of all pairwise video reward margins $m_V$.
(2) One of the patch reward margins surpasses the statistical median of all pairwise patch reward margins $m_P$.

\paragraph{Gran-DPO algorithm.} 

To collaboratively align VGMs from both local and global perspectives, we adapt the Diffusion-DPO algorithm to create the Gran-DPO algorithm. This approach integrates patch DPO and video DPO losses to formulate the final optimization object.

Specifically, given a pair of text-to-video instances, their initial and $t$-th latents are $\mathbf{x}^w, \mathbf{x}^l, \mathbf{x}^w_t, \mathbf{x}^l_t\in\mathbb{R}^{f\times h\times w\times c}$. The ``winner'' and ``loser'' are determined by comparing their video rewards. Following Eq.~(\ref{eq:diffusion_dpo}), the video DPO loss for this pair can be represented as below:
\begin{align}\small
     l(\mathbf{x}^w,\mathbf{x}^l) &= -\log \sigma(
    -\beta T( \nonumber\\
    \|\epsilon^w  -\epsilon_\theta&(\mathbf{x}^w_t,\mathbf{c},t) \|^2_2-
    \|\epsilon^w-\epsilon_{\text{ref}}(\mathbf{x}^w_t,\mathbf{c},t) \|^2_2-\nonumber\\
    (
    \|\epsilon^l\; -\epsilon_\theta&(\mathbf{x}^l_t\;,\mathbf{c},t) \|^2_2-
    \|\epsilon^l\;-\epsilon_{\text{ref}}(\mathbf{x}^l_t\;,\mathbf{c},t) \|^2_2
    )
    ) 
    ). 
    \label{eq:dpo-pair}
\end{align}
To calculate patch DPO loss, we first split latents along the height and width into a $h_n\times w_n$ grid of patch latents. The shape of a patch latent, $\mathbf{x}_{ij}$, is $f\times\lfloor \frac{h}{h_n}\rfloor\times\lfloor \frac{w}{w_n}\rfloor\times c$.\footnote{The height (width) of patches in the last row(column) is $h-(h_n-1)\lfloor\frac{h}{h_n}\rfloor$ ($w-(w_n-1)\lfloor\frac{w}{w_n}\rfloor$).} Therefore, for a patch pair within the given video pair, we construct their patch DPO loss by adapting Eq.~(\ref{eq:dpo-pair}):
\begin{align}\small
    l(\mathbf{x}^{w}_{ij},\mathbf{x}^{l}_{ij}) &= -\log \sigma(
    -\beta T \cdot \mathbb{F}(p^{w}_{ij},p^{l}_{ij})( \nonumber\\
    \|\epsilon^{w}_{ij}  -\epsilon_\theta&(\mathbf{x}^w_t,\mathbf{c},t)_{ij} \|^2_2-
    \|\epsilon^{w}_{ij}-\epsilon_{\text{ref}}(\mathbf{x}^w_t,\mathbf{c},t)_{ij} \|^2_2-\nonumber\\
    (
    \|\epsilon^l_{ij}\; -\epsilon_\theta&(\mathbf{x}^l_t\;,\mathbf{c},t)_{ij} \|^2_2-
    \|\epsilon^l_{ij}\;-\epsilon_{\text{ref}}(\mathbf{x}^l_t\;,\mathbf{c},t)_{ij} \|^2_2
    )
    ) 
    ), \nonumber\\
    \mathbb{F}(p^{w}_{ij},p^{l}_{ij}) &= \left\{
        \begin{aligned}
            1, & \;\;\text{if}\;\; p^{w}_{ij} > p^{l}_{ij},\\
            0, & \;\;\text{if}\;\; p^{w}_{ij} = p^{l}_{ij},\\
           -1, & \;\;\text{if}\;\; p^{w}_{ij} < p^{l}_{ij}.
        \end{aligned}
        \right.
\end{align}
Note that the patch pair reward preference of the patch pair is not always consistent with the video pair. Thereby, we introduce a triple indication function $\mathbb{F}()$ to identify the actual winner and loser of the patch pair. 

Furthermore, considering that a larger reward margin indicates a more distinct quality discrepancy between patch (or video) pairs, we project the reward margin as the pair weight to make the VGM focus on important data pairs. Finally, the loss function of Gran-DPO is:
\begin{align}\small
    \mathcal{L}_{\text{Gran-DPO}}(\theta) &= -\mathbb{E}_{(\mathbf{x}_0^w,\mathbf{x}_0^l),t,(\mathbf{x}_t^w,\mathbf{x}_t^l)}
    [
    \omega(v^w,v^l)l(\mathbf{x}^w,\mathbf{x}^l)+ \nonumber\\ 
    &\quad\quad\sum\nolimits_{i,j}\omega(p^{w}_{ij},p^{l}_{ij})l(\mathbf{x}^{w}_{ij},\mathbf{x}^{l}_{ij})
    ], \nonumber\\
    \omega(v^{w},v^{l}) &= \max\left(\min\left(|v^{w}-v^{l}|/m_P, 1\right), 0\right).
\end{align}
Recall that $v^w$ and $v^l$ represent the video rewards of the winner and loser, which are calculated by Eq.~(\ref{eq:rw_avg}).

\subsection{Acquisition of Training Prompts}\label{sec:prompt}
\paragraph{Generation of text-to-video training pairs.}
To avoid overlapping with the evaluation text prompts and to ensure controllability over the number of training instances, following self-instruct~\cite{self-instruct}, we utilize GPT-3.5-Turbo~\cite{GPT-3.5} to generate text prompts, which serve as the training text prompts for our method.

Adhering to the VBench prompt format~\cite{vbench}, we generate our training prompts based on 16 evaluation dimensions \eg, image quality and multiple objects, which also improves the diversity of our training data. For each dimension, we first sample some VBench prompts to create demonstrations for GPT-3.5-Turbo. As a result, we regard the instruction, demonstrations, and dimension descriptions as the input for generating our training prompts. 
The instruction content is presented in Appendix~\ref{app:instructions}.

We further establish a filtering process to eliminate generated prompts that exhibit high similarity with existing ones, thereby avoiding data leakage and repetition issues:
\begin{align}
\mathcal{T}^T &= \{t\;|\;t\in\mathcal{T}^G \land S(t)=1\}, \\
S(t)&= \mathbb{I}\left(\max\left(\{\mathrm{sim}\left(t,\tilde{t}\right)\;|\;\tilde{t}\in\mathcal{T}^E\cup\mathcal{T}^G_{<t} \}\right) < \tau\right), \nonumber
\end{align}
where $t$ denotes a generated prompt, $\mathcal{T}^T$, $\mathcal{T}^G$, and $\mathcal{T}^E$ denote the training, generated, and evaluation prompt sets separately. $\tau$ is a hyperparameter. These generated prompts are then input to VGMs to build text-to-video instances, supporting the optimization of VGMs.


