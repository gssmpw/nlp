\section{Conclusion}
In this paper, we propose explicitly considering and minimizing localized errors in AI-generated videos, hence developing a post-training framework for text-to-video generation models, \ours{}. 
This framework introduces a patch reward model that delivers fine-grained training signals alongside video rewards to enhance the advanced optimization of VGMs. To efficiently and effectively create the patch reward model, we continuously train our video reward model by distilling GPT-4o-annotated labels. This strategy simplifies the optimization task and yields consistent distributions between patch and video rewards. Furthermore, we develop a Gran-DPO algorithm to post-train diffusion models by collaboratively leveraging patch and video DPO. Our comprehensive experiments demonstrate a positive correlation between our patch reward and human annotations, as well as the superior video generation capabilities of our proposed method compared to baseline models. 
