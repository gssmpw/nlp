\section{Observed Salience}
\subsection{RQ1: What notion of salience have LLMs learned in different domains?}
\label{sec:results-salience}
To understand how LLMs prioritize different information, we consider average question answerability as a proxy for salience. We show the results for the \emph{RCT} dataset as a representative example in \cref{fig:salience-pubmed}, and include other datasets in \cref{sec:appendix-salience}.

\textbf{Models prioritize information hierarchically.}
We observe a clear hierarchy in how information is prioritized across summary lengths.
For example, fundamental aspects such as the focus of a study (\emph{Q1}), and the condition being treated (\emph{Q3}) consistently achieve higher scores, even at 10-word summaries.
In contrast, more specific and technical information like the study design (\emph{Q10}) and the statistical significance of results (\emph{Q12}) are primarily discussed in longer summaries ($\ge 100$ words).

\textbf{Information frequency is not in itself predictive of salience.}
When we consider how frequently a question is answered by documents in the corpus (leftmost column of \cref{fig:salience-pubmed}), we find that even relatively rare questions such as biological markers and adverse effects (\emph{Q7}/\emph{11}, prevalence 40\%/26\%) maintain a consistent representation in summaries.
This suggests that LLMs do not simply prioritize information based on its frequency in a genre.

\textbf{Summaries progressively get more detailed, and information density differs across models.}
As expected, longer summaries consistently include more information as shown by the higher average answerability (bottom row in \cref{fig:salience-pubmed}).
However, the absolute scores differ across models.
GPT-4o has a notably higher answerability score than Llama 3.1, particularly at longer summaries (0.81 vs. 0.71 at the 200-word length).
Given that both models generate summaries of similar lengths (cf. \cref{fig:length-deviation}), this suggests that GPT-4o conveys information more efficiently.

\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/agg-pubmed-cl}
\caption{
    Do LLMs share a similar notion of salience?
    Heatmaps show agreement of content-selection at the atomic-claim level (Krippendorff's $\alpha$).
    Dashed bounding boxes indicate models of the same family.
    The diagonal shows self-agreement over multiple generations. Top row: \emph{RCT}, Bottom row: \emph{CL}.
}
\label{fig:agg-pubmed-cl}
\end{figure*}

\subsection{RQ2: Do LLMs of different families and sizes have a similar notion of salience?}
\label{sec:results-model-model-similarity}
We want to understand to what extent different models (e.g., families, scales) have a shared notion of information salience in a given domain.
We define a fine-grained similarity metric that compares models' content-selection decisions.
Intuitively, two models are more similar if their summaries include the same answer claims.
More formally, for each summary length $l$, we compile all atomic claims derived from question-answers along with their entailment labels (cf. \cref{sec:method-questions}). These form a binary vector $\mathbf{v}_{M,l}$ indicating which claims model $M$ includes in its summaries.
We then measure agreement between two models using Krippendorff's alpha: $\alpha(\mathbf{v}_{M_1,l}, \mathbf{v}_{M_2,l})$.
This claim-level agreement metric is stricter than comparing aggregate answerability scores, as it requires models to consistently include or exclude the same claims at each summary length.\footnote{In contrast, similar answerability scores can result from selecting a similar \emph{number} of claims.}
\cref{fig:agg-pubmed-cl} shows the model-model agreement for the \emph{RCT} and \emph{CL} datasets.

\textbf{High agreement across multiple runs suggests models apply salience notion consistently.}
The diagonal in~\cref{fig:agg-pubmed-cl} shows the average pairwise agreement across 5 model runs.
Overall, self-agreement is the highest for \emph{RCT} ($\approx .80$), while it is slightly lower for \emph{CL}, \emph{Astro} and \emph{QMSum} ($\approx .75$).
We observe a slight decline in self-agreement as the summary length increases.
We hypothesize that each document has a tail of medium- to low-salient topics which may or may not be included as the length budget gives more ``freedom'' to the models.

\textbf{Models of the same family or size do \emph{not consistently} have a higher agreement than any other model.}
We next inspect the off-diagonal agreements, comparing one model family with another model family.
Overall, we find that within-family agreement is not consistently higher than cross-family agreement.
While there are isolated cases with a higher within-family agreement (e.g., Llama 3.1 and GPT-4o on \emph{RCT}), this trend cannot be confirmed for all families and datasets.

\textbf{Agreement by summary length and with GPT-4o-mini.}
We observe that certain summary-lengths have higher agreement than others, though the peak is different for each dataset (e.g., agreement on \emph{RCT} is highest for 50 word summaries, whereas on \emph{CL} it peaks at 100 words).
There could be a ``natural'' summary length for each dataset where model more easily agree.
Lastly, we find that more recent and bigger models agree better with GPT-4o-mini which suggests a clear scaling effect and that open-weights models are getting closer in capabilities to large proprietary models (\cref{fig:agg-gpt4}).

\begin{figure}[t]
\includegraphics[width=\linewidth]{figures/agg-gpt4}
\caption{Agreement with GPT-4o-mini, averaged over all datasets and summary lengths.}
\label{fig:agg-gpt4}
\end{figure}

\section{Perceived Salience and Alignment}
In addition to the \emph{observational salience} analysis,
we elicit \emph{perceived salience} by having humans and models directly rate the salience of each question.
This study has two purposes: (1) to understand whether model behavior aligns with human expectations, and (2) to see if the summarization behavior of LLMs can be approximated by direct prompting.

\subsection{Setup}
\paragraph{Human salience annotation.}
We recruited 18 experts across the four domains through our network (3 for \emph{RCT}, and 5 each for \emph{Astro, CL, QMSum}).\footnote{
    Trained physicians (\emph{RCT}), graduate students/faculty (\emph{Astro}), and graduate students (\emph{CL, QMSum}) based in US/Europe.
}
Experts rated the relative salience of each question on a 5-point Likert scale (ranging from 1: least important, to 5: most important).
Annotators were asked to motivate their rating through a brief rationale to encourage thoughtful judgments and to allow post-hoc analysis of their decision-making process.
To establish a shared understanding between annotators of what content a question may elicit, each question is accompanied by an example answer from a randomly drawn document in the domain.
To ensure high annotation quality, we conducted two pilot rounds with four annotators to refine our annotation guidelines (see \cref{sec:appendix-annotation-guidelines}).

Importantly, the human annotations cannot be regarded as a gold standard for salience. The ratings represent how humans \emph{perceive} question salience, which may not be reflective of how humans actually write summaries.

\paragraph{Model-based salience ratings (LLM-perceived).}
We prompt LLMs to directly rate question salience.
The prompt includes the question list for a given domain and instructions that closely mirror the human annotation guidelines to allow for direct comparison (i.e., 5-point Likert scale and rationales).
Each model is prompted 5 times with a shuffled question list to mitigate position bias and to quantify consistency.
See \cref{sec:appendix-prompts} for the full prompt.

\paragraph{Analysis method.}
We use Spearman's rank correlation coefficient ($\rho$) to quantify alignment between three measures: human-perceived salience, LLM-perceived salience (both 5-point Likert scale) and LLM-observed salience (continuous $[0,1]$).\footnote{We take observed salience scores at the 200-words summary length which correlated on average most strongly with human salience. Other scores are explored in~\cref{sec:appendix-salience-score-ablation}.}
For groups with multiple ratings, we report averaged pairwise correlation.

\paragraph{Human correlation.}
We observe that inter-human correlation varies across domains, with meeting summarization (QMSum, $\rho = 0.60$) and RCT abstracts ($\rho = 0.46$) showing a moderate to strong correlation (\cref{tab:annotator-agreement}).
These domains presumably have established conventions about summary content.
In contrast, correlation is weak on summarization of related work sections (CL, $\rho = 0.26$) and discussion sections (Astro, $\rho = 0.16$).
Documents in these domains may vary significantly in the type of content they present (i.e., certain questions may be more relevant to theoretical vs. empirical papers).
While our annotation protocol aims to control for this aspect through the example answers by question, there remains annotator subjectivity related to their personal interests.

\input{51-tab-human-agreement}

\subsection{Results}
\label{sec:results-introspection}
To understand if LLMs can reliably rate question salience, we study three conditions.
First, as a reference point, we measure consistency of the observational and perceived salience measures estimated over 5 model runs (LLM-observed, LLM-perceived).
Second, we study the correlation of LLM-perceived and LLM-observed to measure if models' explicit ratings align with their summarization behavior (RQ3).
Third we correlate LLM-derived salience in human perceived salience (RQ4).
We report results for the three conditions in \cref{tab:results-rater-agreement} and provide qualitative examples in \cref{tab:results-examples}.

\input{52-tab-salience-correlation-avg-only}
\input{53-tab-examples}
\paragraph{RQ3: When models introspect, does their perceived notion of salience align with their summarization behavior?}
LLMs have strong and consistent \emph{implicit} notions of salience, but
they are unreliable when explicating these preferences in rating tasks. We detail these observations below.

\textbf{Observational salience is highly stable.}
We find that observational question salience leads to highly stable scores for all models ($\rho \ge 0.96$).
This suggests that LLMs' underlying summarization process is highly deterministic despite the stochastic nature of language models.
Also, it suggests that our proposed approach is a reliable tool for analyzing model behavior.

\textbf{Models fail to have consistent perceived salience.}
We find that the consistency of direct salience ratings varies significantly for all models and datasets.
Generally, strong instruction-following models have more consistent perceived salience than weaker models (avg. $\rho$ ranges from 0.20 for OLMo to 0.76 for GPT-4o).
This finding mirrors recent results in the LLM-as-a-judge literature which demonstrated instability in ratings due to various factors including position bias~\cite{Wang:2024:ACL,Stureborg:2024:arXiv}.

\textbf{Perceived $\neq$ observed salience.}
Lastly, we find only a weak to moderate correlation between perceived and observed salience (highest: avg. $\rho = 0.56$ for GPT-4o-mini, lowest: $\rho = 0.12$ for OLMo).
Again, stronger instruction-following models show higher correlations, indicating a clear scaling effect.
This gap echoes broader findings where generative abilities may not reflect an underlying understanding in models~\cite{West:2024:ICLR}.

\paragraph{RQ4: To what extent does model salience align with human perceived salience?}
\label{sec:results-human-alignment}
We find that both LLM-salience estimates only show a weak to moderate correlation with human salience perception.
Direct rating for question salience correlates more than observed salience (highest LLM-perceived: avg. $\rho = 0.53$ for GPT-4o, highest LLM-observed: avg. $\rho = 0.35$ for Llama 3.1 70B).
Weak correlation between models and humans holds for all dataset, also those where humans agree more strongly among themselves (\cref{tab:results-rater-agreement-full}).

Users of LLMs should carefully consider if the models are appropriate for their summarization task, or provide explicit signals about content priority through prompts or during model training.
