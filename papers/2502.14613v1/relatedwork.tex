\section{Related Work}
\textbf{Evaluating and Interpreting Summarization.}
Recent work suggests that LLMs match or surpass human performance in news summarization~\cite{Zhang:2024:TACL}.
However, traditional evaluation protocols remain unreliable especially for LLM-generated summaries~\cite{Fabbri:2021:TACL,Goyal:2023:arXiv}.
This spurred interest in analyzing summarization model behavior.
Studies found biases towards content near the beginning/end of documents~\cite{Ravaut:2024:ACL,Laban:2024:EMNLP}.
Others analyze training dynamics of summarization models to identify when skills like content selection are learned~\cite{Goyal:2022:ACL}.
Extract-then-abstract pipelines~\cite{Gehrmann:2018:EMNLP,Li:2021:ws} aim for interpretable text summarization but this interpretability is limited to the document-level~\cite{Dhaini:2024:INLG}.
Our research complements prior work by providing a \emph{global interpretation} of what topics LLMs consider important through the lens of text summarization.

\textbf{Explainable Topic Modeling.}
Our analysis method draws inspiration from the interpretable topic modeling literature.
While classical topic models such as LDA~\cite{Blei:2003:JMLR} have long been used to explain latent themes in text corpora, they are often difficult to interpret~\cite{Chang:2009:NeurIPS}.
Recent work showed that LLMs can effectively be used to generate natural language descriptions of latent themes in text mining, clustering and concept induction workflows~\cite{Pham:2024:NAACL,Zhong:2024:NeurIPS,Wang:2023:EMNLP,Lam:2024:CHI}.
Our framework uses LLMs to describe salient summary content in form of information-seeking QUDs.
The use of QUDs as a representation of information units was shown successful in a wide range of tasks~\cite{Newman:2023:EMNLP,Laban:2022:Findings,Trienes:2024:ACL,Wu:2023b:EMNLP}.