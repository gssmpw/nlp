\section*{Limitations}
We only consider zero-shot prompting with temperature-based decoding to generate summaries. While these settings are common defaults for LLM users, it is conceivable that different prompting styles (e.g., chain-of-density) or decoding methods influence salience patterns. Future work should explore how these techniques affect salience, particularly in adjacent information-seeking tasks such as query-based summarization.

While our experiments cover diverse disciplines (medicine, astrophysics, computational linguistics, and meetings) and discourse types (structured writing, academic discourse, and dialogue), the texts are primarily technical. Since our framework is designed to be domain-agnostic, we believe it is an exciting direction for future work to explore less technical genres such as fiction~\cite{Kim:2024:COLM}.

Our user study assumed a uniform background and interests among participants, which is a simplification of practical applications. Additionally, the specialized nature of two tasks (i.e., summarizing related work and discussion sections) may have contributed to variability in responses, as even domain experts may not have strong priors on how these texts should be summarized. Future work could explore how differences in expertise and prior knowledge shape perceptions of salience.
