\section{Length-instruction Following}
\label{sec:appendix-length-analysis}
We analyze to what extent length-controlled summarization is a consistent probe for content selection.
Ideally, we expect the following behavior of summarization models: (1) the generated summary length matches approximately the target length, and (2) as we increase the length budget, summaries should provide all content of the shorter version in addition to expanding on it.
We define two measures for these desiderata.

\paragraph{Target length ratio (TLR).}
We quantify the length deviation of a generated summary ($s_{l}$) from the target word count ($l$) as follows:
%
\begin{equation}
\text{\textbf{TLR}}(s_{l}) = \frac{|s_{d,l}|}{l}.
\end{equation}
%
Where $|\cdot|$ is the summary length (whitespace separated tokens). A value of 1 indicates perfect length match, while values greater or smaller than 1 indicate over- or under-generation, respectively.

\paragraph{Incremental consistency (IC).}
Longer summaries should contain a proper superset of claims found in the adjacent shorter version.
Formally, for each document $d$ and topic $t$ recall that we have a set of atomic claims $A_t$ (\cref{sec:method-questions}).
We first identify the set of claims that are entailed at least once across any summary length:
%
\begin{equation*}
    A_{\text{entailed}}(d,t) = \{a \in A_t \mid \exists l \in L, e(a,s_{d,l}) = 1\},
\end{equation*}
%
where $e$ is an NLI model indicating whether claim $a$ is entailed by summary $s_{d,l}$ of length $l$.
Next, we determine if a claim is included consistently across increasing summary lengths (monotonicity condition).
%
\begin{equation*}
    e(a,s_{d,l_1}) \leq e(a,s_{d,l_2}) \; \forall l_1 < l_2
\end{equation*}
%
We then define the set of consistent claims where this condition holds:
%
\begin{equation*}
\begin{aligned}
    A_{\text{consistent}}(d,t) =\,&\{a \in A_{\text{entailed}}(d,t) \\
    \quad& \mid \text{\small monotonicity holds} \, \forall l \in L \}.
\end{aligned}
\end{equation*}
%
Finally, the overall incremental consistency for summaries of a corpus $D$ is given as the fraction of consistent claims:
%
\begin{equation}
\text{\textbf{IC}}(D) = \frac{
        \sum_{d \in D} \sum_{t \in T} |A_{\text{consistent}}(d,t)|
    }{
        \sum_{d \in D} \sum_{t \in T} |A_{\text{entailed}}(d,t)|
    }.
\end{equation}
%
This metric ranges from 0 to 1, where 1 indicates perfect monotonicity (longer summaries always include all information found in shorter ones).

\begin{figure}[t]
\includegraphics[width=\linewidth]{figures/tlr}
\caption{Distribution of target length ratios over all generated summaries (aggregating lengths and datasets).}
\label{fig:length-deviation}
\end{figure}

\noindent\textbf{Do models meet the target length?}
We find that all models generally undershoot the length target (\cref{fig:length-deviation}).
However, more recent models match the target length more closely and consistently, showing a clear scaling effect.
The best performing models are Llama 3.1 and GPT-4o, while OLMo is unable to follow length-instructions, presumably because this was not part of the instruction tuning data.
Surprisingly, we do not find substantial differences across datasets.
This suggests that the ability of models to follow length-instructions is mostly invariant to the input document length, even if they are considerably long (e.g., meeting transcripts).
See \cref{fig:tlr-stratified} for an analysis of summary length stratified by dataset and target length.

\noindent\textbf{How incrementally consistent are summaries?}
We report the average incremental consistency by dataset and model in \cref{fig:incremental-consistency}.
We observe that all models are substantially more consistent than the random summarization baseline.
Furthermore, incremental consistency decreases with more difficult datasets, likely because there is more freedom on what content to include in a summary.
Similar to the ability of following length instructions, we observe a scaling effect where stronger models have a higher incremental consistency.

\begin{figure}[t]
\includegraphics[width=\linewidth]{figures/ic}
\caption{Incremental consistency by model and dataset.}
\label{fig:incremental-consistency}
\end{figure}

\noindent\textbf{Influence of temperature sampling.}
The main results in this paper are obtained with a temperature of $\tau = 0.3$.
To assess how temperature affects summary length and incremental consistency, we perform a temperature sweep on the RCT dataset for all open-weights models (20 settings in $[0,1]$).
Surprisingly, higher temperatures do not affect the \emph{average} summary length on a dataset-level, but lead to greater variance at the document level (up to 10\% length difference between generations, \cref{fig:temperature-length}).
Furthermore, higher temperatures lead to a slight decline in incremental consistency for all models that adequately follow length instructions (a drop of 1\% to 9\%, \cref{fig:temperature-ic}).

\noindent\textbf{Summary.}
Overall, we find that strong models are able to follow length-instructions and that they consistently expand the summary content with increasing length budgets.
As our salience analysis assumes this behavior of models, it may be less reliable for weaker models (OLMo, Mistral, Llama~2).

\begin{figure}[t]
\includegraphics[width=\linewidth]{figures/temperature-ic}
\caption{Influence of temperature on incremental consistency.}
\label{fig:temperature-ic}
\end{figure}

\section{Salience Score Ablation}
\label{sec:appendix-salience-score-ablation}
We analyze how different salience scores derived from the CSM correlate with human salience.
Recall that the $\text{CSM}(D)_{t,l}$ tracks the average answerability of question $t \in T$ at summary length $l \in L = \{10,20,50,100,200\}$.
We take raw salience scores at each summary length in addition to question-wise aggregations.
Intuitively, questions which are more answerable at shorter summaries score higher under the aggregated scheme.
Formally, we aggregate scores as follows:
%
\begin{equation*}
\text{CSM}_{\text{agg}}(D)_{t} = \frac{\sum_{l \in L} w_l \cdot \text{CSM}(D)_{t,l}}{\sum_{l \in L} w_l},
\end{equation*}
%
where $w_l$ is a weighting term. We experiment with three weighting functions: uniform ($w_l = 1$), reciprocal length ($w_l = 1/l$), and logarithmic decay ($w_l = 1/\log(1 + l)$).
\cref{fig:salience-score-correlation} shows the Spearman rank correlation coefficient ($\rho$) with human salience for each salience score.
Overall, on \emph{RCT} and \emph{Astro} we find that all salience scores correlate similarly with human salience ratings, while on \emph{CL} and \emph{QMSum} the 200 words salience score correlates most strongly.

\begin{figure}[t]
\includegraphics[width=\linewidth]{figures/salience-score-correlation}
\caption{Correlation of different salience scores with human salience. Here we aggregate over all LLMs which showed similar trends.}
\label{fig:salience-score-correlation}
\end{figure}

\section{Salience Analysis}
\label{sec:appendix-salience}
The corpus-level salience analysis for PubMed, Astro, CL, and QMSum is given in \cref{fig:salience-pubmed}, \cref{fig:salience-astro-ph}, \cref{fig:salience-cs-cl}, and \cref{fig:salience-qmsum}, respectively. We also provide a fully-worked example of the content salience analysis in \cref{fig:worked-example}.

\section{Responsible NLP Considerations}
\paragraph{Compute Requirements.} Experiments were conducted on NVIDIA A100 80GB GPUs, requiring approximately 20 GPU hours per dataset, and an additional 360 GPU hours for the temperature sweep on the RCT dataset, totaling 440 GPU hours.
We ran inference using \textsc{vllm}.\unfootnote{All URLs accessed 2025-02-15.}\footnote{\rurl{docs.vllm.ai}}
GPT-4o models were accessed through the OpenAI API with inference costs $\leq 100\$$.

\paragraph{Salience Annotation Study.} Participants joined on a volunteer basis, gave informed consent and agreed that their annotations will be shared in anonymized form in the paper repository. According to our institutional policies, this study did not require institutional review board (IRB) approval.

\paragraph{Data Licensing.}
We obtain RCT abstracts in accordance with fair use principles through the PubMed Entrez API.\footnote{\rurl{www.ncbi.nlm.nih.gov/home/develop/api/}}
Related work sections of CL and Astro papers were collected via the arXiv API.\footnote{\rurl{info.arxiv.org/help/api/index.html}} While the majority of papers on arXiv is published under the arXiv license\footnote{\rurl{arxiv.org/licenses/nonexclusive-distrib/1.0/license.html}} retaining \emph{copyright} with the original author(s), the \emph{use} of paper contents for research is explicitly granted and encouraged in the arXiv API terms \& conditions.\footnote{\rurl{info.arxiv.org/help/api/tou.html}}
We reused meeting transcripts from QMSum~\cite{Zhong:2021:NAACL}.\footnote{\rurl{github.com/Yale-LILY/QMSum}} All meeting transcripts are under an open use license, such as CC BY 4.0\footnote{\rurl{creativecommons.org/licenses/by/4.0/legalcode}} (academic meetings\footnote{\rurl{groups.inf.ed.ac.uk/ami/icsi/license.shtml}} and product meetings\footnote{\rurl{groups.inf.ed.ac.uk/ami/corpus/license.shtml}}) or Open Government Licence\footnote{\rurl{www.nationalarchives.gov.uk/doc/open-government-licence/version/3/}} (Parliament Commitee meetings).


\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/temperature-length}
\caption{Influence of temperature on generated summary length. \textbf{Left:} target length-ratio. \textbf{Center:} ``within-document length variance'' calculated as the mean deviation from the average summary length of 5 summaries for the same document (MAD). MAD is normalized to be comparable across length targets. \textbf{Right:} zoomed version.}
\label{fig:temperature-length}
\end{figure*}

\input{52-tab-salience-correlation-full}

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{figures/tlr-by-dataset}
\caption{Distribution of target length ratios over all generated summaries stratified by dataset.}
\label{fig:tlr-dataset}
\end{subfigure}
\vspace{0.5cm}
\begin{subfigure}[b]{\textwidth}
\includegraphics[width=\textwidth]{figures/tlr-by-length}
\caption{Distribution of target length ratios over all generated summaries stratified by target summary length.}
\label{fig:tlr-length}
\end{subfigure}

\caption{Analysis of length-instruction following. The target length ration (TLR) indicates to what extent models match the provided length. A value of 1 indicates perfect length match, while values greater or smaller than 1 indicate over- or under-generation, respectively.}
\label{fig:tlr-stratified}
\end{figure*}


\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/salience-astro-ph}
\caption{Corpus-level content salience map for \emph{Astro} summaries by four methods.}
\label{fig:salience-astro-ph}
\end{figure*}

\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/salience-cs-cl}
\caption{Corpus-level content salience map for \emph{CL} summaries by four methods.}
\label{fig:salience-cs-cl}
\end{figure*}

\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/salience-qmsum}
\caption{Corpus-level content salience map for \emph{QMSum} summaries by four methods.}
\label{fig:salience-qmsum}
\end{figure*}

\begin{figure*}[t]
\centering

    \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{figures/agg-astro}
    \caption{Model similarity for \emph{Astro}.}
    \label{fig:agg-astro}
    \end{subfigure}

    \vspace{0.5cm}

    \begin{subfigure}[b]{\textwidth}
    \includegraphics[width=\textwidth]{figures/agg-qmsum.pdf}
    \caption{Model similarity for \emph{QMSum}.}
    \label{fig:agg-qmsum}
    \end{subfigure}

\caption{Do models share a similar notion of salience? Heatmaps show agreement on topic inclusion by summary length (Krippendorff's alpha calculated column-wise for the CSM pairs).}
\label{fig:agg-astro-qmsum}
\end{figure*}

\begin{table*}[t]
    \fontsize{8.5pt}{8.5pt}\selectfont
    \begin{tabular}{@{}p{0.17\textwidth}rp{0.23\textwidth}p{0.45\textwidth}@{}}
    \toprule
        \textbf{Question} &
        $L_o$/$L_p$/$H$ &
        \faRobot\ \textbf{Rationale LLM-perceived} &
        \faUsers\ \textbf{Rationales Human}\\
    \midrule
    \multicolumn{4}{c}{\emph{continued from \cref{tab:results-examples}}} \\
    \textbf{(\#4, RCT \faUserMd)}
        What were the comparative results between intervention and control groups? & 3.2/4.6/5.0 &
        Comparative results are key to understanding the effectiveness of the intervention. &
        (P\textsubscript{\{1,2,3\}} = 5) Brief comparison between groups allows for readers to quickly assess statistical significance; Comparative results are one of the primary outputs reflecting effectiveness; essential for understanding the findings. \\

    \textbf{(\#5, RCT \faUserMd)}
        What implications or future recommendations did the study suggest based on its findings? & 4.6/3.6/2.0 &
        Implications and recommendations are important for understanding the study's impact and future research directions. &
        (P\textsubscript{1} = 1) This is a job for the reader to decide for himself or herself; (P\textsubscript{2} = 3) helps the reader understand the paper. Useful for at the end of a summary. (P\textsubscript{3} = 2) While helpful, they are not necessarily always based on the opinion of the authors; would have the reader get to that conclusion themselves.\\

    \textbf{(\#6, RCT \faUserMd)}
        What is the main outcome or effect observed? & 4.6/5.0/5.0 &
        The main outcome is critical to understanding the study's primary conclusion. &
        (P\textsubscript{\{1,2,3\}} = 5) determines whether a paper can be used to influence decision making; primary information when reading summary \\

    \textbf{(\#7, RCT \faUserMd)}
        How were the participants or subjects of the study selected and divided? & 3.7/3.2/2.3 &
        Participant selection and division are important for assessing the study's validity. &
        (P\textsubscript{1} = 2) This helps a reader determine external validity of a study;
        (P\textsubscript{2} = 1) Not necessary as fairly standardized;
        (P\textsubscript{3} = 4) Crucial to evaluate the study's validity and relevance \\

    \textbf{(\#8, Astro \faStar)}
        What limitations or considerations are noted by the study? & 1.1/3.6/2.3 &
        Limitations are crucial for interpreting the study's reliability and potential biases. &
        (P\textsubscript{1} = 3) important but best reserved for readers who choose to critically examine the study in its totality; (P\textsubscript{2} = 1) Typically addressed in the discussion section of a paper, but is not necessary for a summary; (P\textsubscript{3} = 3) would note if I had the space, however, I would also implore the readers to think about limitations themselves. \\

    \textbf{(\#9, Astro \faStar)}
        How do the findings relate to existing models or theories? & 1.6/4.0/3.8 &
        This is important for understanding the study's impact on current scientific understanding. &
        (P\textsubscript{1} = 4) I think this is important to describe the finding in context of the research question, initial hypothesis, or a theory.  (P\textsubscript{2} = 3) This is important for the broader context of the study, however it is more important to present the study results in a clear way. Only after that is clear I'd compare to other studies.(P\textsubscript{3} = 5) This forms the main body of a discussion section. (P\textsubscript{4} = 3) Situating the findings within the literature by relating to existing models or theories is important.(P\textsubscript{3} = 4) Comparing with existing literature is very important for the discussion section.\\

    \textbf{(\#10, Astro \faStar)}
        What are the main findings of the study? & 3.5/5.0/4.8 &
        The main findings are the core of the study and must be included in any summary. &
        (P\textsubscript{1} = 5) key takeaway from the paper and should be included regardless of what the paper is about (P\textsubscript{2} = 5) most important information of the summary (P\textsubscript{3} = 4) The main findings should be briefly addressed in the summary of a discussion for the reader's quick follow-up (P\textsubscript{4} = 5) The main findings of the study, along with the main focus, form the two most important elements of an article summary. (P\textsubscript{5} = 5) Important to state the main findings and then discuss them in details. \\

    \textbf{(\#11, Astro \faStar)}
        What specific challenges or limitations does the study address or identify? & 1.6/3.2/2.6 &
        Understanding the challenges or limitations provides context for the study's reliability and areas for improvement. &
        (P\textsubscript{1} = 1) I most likely do not include challenges and limitations. These examples focused on the future needs not an existing open question. The focus will be on the findings in the context of a hypothesis, conjecture, or a theory. (P\textsubscript{2} = 1) Level of detail that a reader would need only if interested in full paper. Some challenges can be identified if the methods and scope of the paper are summarized clearly. (P\textsubscript{3} = 5) This forms the main body of a discussion section. (P\textsubscript{4} = 2) depends upon the significance of those challenges or limitations (P\textsubscript{5} = 4) Identify the limitations and challenges of the study is very important \\
    \bottomrule
    \end{tabular}
    \caption{Example questions, salience scores by LLM-observed ($L_o$, rescaled to 1-5), LLM-perceived ($L_p$), humans ($H$) and summarized rationales.}
    \label{tab:results-examples-part2}
\end{table*}

\clearpage
\onecolumn
\section{LLM Prompts}
\label{sec:appendix-prompts}
This section provides all prompts used throughout the experiments. Summarization (\cref{lst:summarization,lst:summarization-meetings}), question generation (\cref{lst:qg}), question answering (\cref{lst:qa}), answer claim  splitting (\cref{lst:claim-split}), and introspection (\cref{lst:introspection}).

\codeboxinput[label=lst:summarization]{Summarization prompt}{prompts/summarization.txt}
\codeboxinput[label=lst:summarization-meetings]{Summarization prompt for meeting transcripts}{prompts/summarization-meetings.txt}
\codeboxinput[label=lst:qg]{Question generation prompt}{prompts/qg.txt}
\codeboxinput[label=lst:qa]{Question answering prompt}{prompts/qa.txt}
\codeboxinput[label=lst:claim-split]{Claim splitting prompt}{prompts/claim-splitting.txt}
\codeboxinput[label=lst:introspection]{Introspection prompt}{prompts/introspection.txt}

\input{91-annotation-guidelines.tex}

\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/worked-example}
\caption{Fully worked example of the question-based content analysis. Two documents in a fictional domain are each summarized at three lengths. Afterwards Steps 1 -- 4 are analogous to \cref{sec:method-questions}. Summary claims are colorcoded.}
\label{fig:worked-example}
\end{figure*}
