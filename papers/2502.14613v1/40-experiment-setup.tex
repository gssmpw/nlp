\section{Experimental Settings}
\label{sec:experimental-settings}

\begin{table}[t]
\small
\centering
\setlength{\tabcolsep}{4pt}
\begin{tabular}{lrrrr}
\toprule
\textbf{Statistic} & \textbf{RCT} & \textbf{CL} & \textbf{Astro} & \textbf{QMSum} \\
\midrule
Documents & 200 & 185 & 106 & 90 \\
Words/doc & 290 & 459 & 703 & 10,837 \\
\midrule
Questions & 21 & 14 & 13 & 10 \\
Answered/doc & 84.1\% & 86.2\% & 96.5\% & 91.9\% \\
Words/answer & 30.9 & 53.0 & 70.3 & 161.5 \\
Claims/answer & 6.5 & 11.4 & 12.4 & 29.6 \\
Claims (total) & 23,124 & 25,353 & 16,430 & 24,459 \\
\bottomrule
\end{tabular}

\caption{Dataset overview. Number of words is calculated as whitespace-separated tokens.}
\label{tab:dataset-statistics}

\end{table}

\paragraph{Datasets.}
We analyze LLM salience across several technical and scientific domains using four datasets (\cref{tab:dataset-statistics}).
We designed slightly unconventional summarization tasks because of their limited ``oracle'' summaries in common LLM training datasets.
This allows us to analyze how LLMs handle texts without strong priors, and how salience judgments vary across genres and discourse types (structured technical writing, academic discourse, and dialogue).

\paragraph{(1) Randomized Controlled Trials (RCT).}
We draw a random sample of 200 abstracts of RCTs published Jan--Apr 2024 from PubMed.
These documents follow established conventions to describe the conduct and outcomes of clinical studies.
The task is to further summarize the abstracts.


\paragraph{(2) Computation and Language (CL).} The second task is to summarize the \emph{related work} sections of NLP/CL papers published on arXiv.
Although CL paper summarization is common, summarizing the related work section itself is not.
We convert raw LaTeX sources to Markdown and only consider documents up to 2,000 tokens to fit the context window of smaller models.
A random sample of 185 documents published in October 2024 is drawn.

\paragraph{(3) Astrophysics (Astro).} The third dataset contains \emph{discussion} sections of astrophysics papers published on arXiv.
These documents interpret key results of theoretical and empirical astrophysics research.
Similar to the CL portion, summarizing only the discussion sections is uncommon.
A random sample of 106 documents is drawn, with pre-processing analogous to CL.

\paragraph{(4) Meetings (QMSum).} Lastly, we consider meeting transcript summarization.
We randomly sample 90 documents balanced across three domains from QMSum~\cite{Zhong:2021:NAACL}: product design, research and political discussions.
We format transcripts as \texttt{\small [Speaker]: [Utterance]} turns, separated by newlines.
We only experiment with long-context models ($\ge 32\text{k}$ tokens) on this dataset.

\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/salience-pubmed-sample}
\caption{Corpus-level content salience map for \emph{RCT} summaries by four methods.}
\label{fig:salience-pubmed}
\end{figure*}

\paragraph{Summarization Models.}
We experiment with 13 LLMs of different scales:
\textbf{OLMo}~(7B; 02/24, 07/24; \citealp{Groeneveld:2024:ACL}), \textbf{Mistral}~(7B; v0.3; \citealp{Jiang:2023:arXiv}), \textbf{Mixtral}~(8x7B; v0.1, \citealp{Jiang:2024:arXiv}), \textbf{Llama 2}~(7B, 13B, 70B; \citealp{Touvron:2023:arXiv}), \textbf{Llama~3}~(8B, 70B), and \textbf{Llama~3.1}~(8B, 70B; \citealp{Grattafiori:2024:arXiv}). For API-based models, we use \textbf{\gptmini}~(07/24) and \textbf{\gpt}~(08/24; \citealp{OpenAI:2024:arXiv}).
We also include 3 baselines to contextualize results: \textbf{Lead-N}, \textbf{Random} and \textbf{TextRank}~\cite{Mihalcea:2004:EMNLP}, all adjusted to meet summary length budgets. To assess consistency across multiple rounds of decoding, we generate 5 summaries per document and target length with temperature $\tau = 0.3$.
We use a zero-shot summarization prompt (\cref{sec:appendix-prompts}).

Before analyzing salience in these models, we validate two key assumptions: \emph{(i)} generated summaries should approximately meet the target length, and \emph{(ii)} longer summaries should expand on shorter ones (``incremental consistency''). Additionally, we analyze how greater $\tau$ affect those criteria.
Our analysis confirms that models largely meet above criteria, with newer and bigger models showing better length control.
Higher $\tau$ results in stable \emph{average} summary length at the corpus level, but greater length variance at the document level (up to 10\% difference), along with a slight decline in incremental consistency (details in \cref{sec:appendix-length-analysis}).
