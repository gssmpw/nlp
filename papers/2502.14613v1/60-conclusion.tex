\section{Conclusion}
We propose an interpretable framework to systematically derive and analyze LLMs' notion of information salience, a previously latent concept.
Our work builds on two key ideas: using length-controlled summarization as a behavioral probe for content selection, and describing what is salient as the answerability of questions.
We found that LLMs have a highly consistent notion of salience which is largely compatible across models.
We further found that LLMs cannot directly rate the salience of questions, and that model salience weakly aligns with human perceptions.
Our work opens new directions to study how LLM salience emerges during training, and for diagnosing content selection challenges in text synthesis tasks.
