\section{Introduction}
Large Language Models (LLMs) significantly advanced text synthesis tasks, including text summarization, which they perform well even under zero-shot conditions~\cite{Goyal:2023:arXiv,Zhang:2024:TACL}.
The nature of the summarization task requires models to do content selection: picking the most salient pieces of information for inclusion in a summary \cite{Mani:1999:advances}.
However, it remains unclear what underlying notion of salience the models have internalized.

Prior work investigated information salience from several angles.
Theories of discourse structure have been used to induce content salience \cite{Marcu:1999:advances,Louis:2010:SIGDIAL}, and a large body of summarization research uses word distribution or centrality as the main signal for content selection \cite{Nenkova:2012:survey,Nazari:2019:survey}. \citet{Peyrard:2019:ACL} laid out a theoretical perspective for content salience in summarization, though the exact notion remains largely latent and aloof; rather, pre-LLM summarization work uses human summaries as supervision signals to learn what to include \cite[][\emph{inter alia}]{Gehrmann:2018:EMNLP,Chen:2018:ACL,Liu:2019:EMNLP}.
Yet, none of these accounts explains why LLM zero-shot summarization works so well on the one hand, while missing key elements on the other~\cite{Kim:2024:COLM,Trienes:2024:ACL,Huang:2024:NAACL}.

To begin to make sense of this behavior, we need to understand how models internalize salience: whether it is a \emph{consistent} notion within and across models, \emph{how} they prioritize information, and whether LLMs' notion of salience \emph{aligns} with prior theories or human intuitions.

In this paper, we present a novel explainable framework to systematically derive and investigate LLMs' grasp of information salience through their summarization behavior.
Our method combines \textbf{two key ideas}. First, we can use length-constrained summarization~\cite{Fan:2018:NGT,He:2022:EMNLP} as a behavioral probe into the content selection process of LLMs.
Intuitively, when there is a limited length budget for a summary, we posit that the least important information is dropped first.

\begin{figure*}[t]
\includegraphics[width=\textwidth]{figures/overview}
\caption{
    Framework overview, conceptualizing content salience as question answerability.
    \textbf{Left:} Given a corpus, we derive questions that are typically answered in summaries. Length-controlled summarization acts as a probe into the content-selection process of LLMs.
    Question paraphrases are clustered by semantic intent.
    \textbf{Middle:} Answerability is calculated as the fraction of document-answer claims entailed by the summary.
    \textbf{Right:} The content salience map tracks answerability at each summary length. More salient questions remain answerable even in shorter summaries.
}
\label{fig:overview}
\end{figure*}

Second, we can describe what is salient as the answerability of domain-relevant Questions Under Discussion (QUDs;~\citealp{Van:1995:discourse,Benz:2017:questions,Wu:2023:EMNLP}). QUDs can be thought of as representations of a coherent unit of information in the form of information-seeking questions, e.g., \emph{Who are the participants of this study?}
We use such questions --- and hence their answers extracted from documents, according to alternative semantics \cite{Hamblin:1973:Questions,Karttunen:1977:syntax,Groenendijk:1984:studies} --- as the primary unit of analysis, making our framework interpretable and customizable.

Taken together, by gradually decreasing the length budget available for a summary and by systematically tracing question answerability throughout, we can derive a \emph{proxy} for how models prioritize information.
See~\cref{fig:overview} for an overview.

Using this framework (\cref{sec:method}), we empirically study LLMs' content selection \emph{behavior}, and its alignment with \emph{perceived} notions of salience. Through experiments on 13 models and four datasets (\cref{sec:experimental-settings}), we aim to answer the following research questions:
%
\begin{itemize}[noitemsep,leftmargin=27pt]
    \item[\textbf{RQ1}] What notion of salience have LLMs learned in different domains?
    \item[\textbf{RQ2}] Do LLMs of different families/sizes have a similar notion of salience?
    \item[\textbf{RQ3}] When models introspect, does their perceived notion of salience align with their summarization behavior?
    \item[\textbf{RQ4}] To what extent does model salience align with human perceived salience?
\end{itemize}
%
We find that LLMs have a nuanced notion of salience prioritizing information hierarchically across summary lengths (\cref{sec:results-salience}).
Also the notion of salience is generally compatible between models even of different families and sizes, though more recent/bigger LLMs correlate more strongly with GPT-4o (\cref{sec:results-model-model-similarity}).
Furthermore, models show highly consistent behavior and hence notions of salience, but it cannot be elicited through introspection (i.e., directly prompting for salience of topics; \cref{sec:results-introspection}). Lastly, we find that model behavior only weakly aligns with human perceptions of salience (\cref{sec:results-human-alignment}).
