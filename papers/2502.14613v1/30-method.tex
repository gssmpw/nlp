\section{Method: Analyzing Content Salience}
\label{sec:method}
%
To analyze content salience we need a way to both \emph{observe} what content models consider important (\cref{sec:method-probe}), and to \emph{describe} it in an interpretable manner (\cref{sec:method-questions}).
\cref{fig:overview} illustrates the framework.
%

\subsection{Length-constrained Summarization as a Content Salience Probe}
\label{sec:method-probe}
To elicit content-selection decisions from models, we use length-constrained summarization as a probe. Our key intuition is that, under a limited length budget, well-behaving models will drop the least important information first, while preserving the most salient content.

\paragraph{Summary Generation.}
Given a corpus $D$, and a set of target lengths $L$ specified in words, we generate summaries $S = \{s_{d,l} \mid d \in D, l \in L \}$ for all documents and length targets.
We consider $L = \{10, 20, 50, 100, 200\}$ to capture a range of typical summary lengths.

\paragraph{Tracing Content-selection Decisions.}
To understand how summary content changes with varying length budgets, we introduce \textit{Content Salience Maps (CSMs)} as a structured representation to systematically track the inclusion and exclusion of topics.
Formally, let $T$ be a set of topics of interest, and let $f : T \times S \rightarrow [0,1]$ be a function that measures to what extent topic $t$ is present in summary $s$.
For a document $d$, $\text{CSM}(d)$ is a $|T|\times|L|$ matrix, where each entry is defined as:
%
\begin{equation}
    \text{CSM}(d)_{t,l} = f(t, s_{d,l}).
\end{equation}
%
We define the corpus-level $\text{CSM}(D)$ as the average of document-level measurements:
%
\begin{equation}\label{eqn:corpus-level-csm}
    \text{CSM}(D)_{t,l} = \frac{1}{|D^t|} \sum\limits_{d \in D^t}f(t,s_{d,l}),
\end{equation}
%
where $D^t$ is the set of documents that contain topic $t$.
We also define \emph{topic prevalence} as $|D^t|/|D|$, representing the fraction of documents in the corpus that contain topic $t$.

Below, we describe a concrete instantiation of this framework, where the set of topics $T$ is represented as QUDs, and the inclusion measure $f$ is defined as question answerability.
However, we note that the framework is highly customizable
in terms of the definitions of $T$ and $f$.

\subsection{Question-based Content Analysis}
\label{sec:method-questions}
We represent the topics $t \in T$ as Questions Under Discussion (QUD), a linguistic representation for topics in discourse \cite{Van:1995:discourse}.
In our setup, each QUD represents a possible \emph{answer space} across different documents in the same genre. This aligns with alternative semantics, where questions are viewed as the set of possible answers \cite{Hamblin:1973:Questions,Karttunen:1977:syntax,Groenendijk:1984:studies}.
In addition to the interpretability provided by natural language questions,
we can also quantify content salience through \emph{question answerability}:
questions which remain answerable even with shorter summaries are more salient than questions which can only be answered with longer summaries.
Below (also \cref{alg:csm-derivation}), we describe a four-step pipeline to implement this approach.

\input{31-algorithm}

\paragraph{Step 1: Question Generation.}
We design a question-generation prompt inspired by \cite{Laban:2022:Findings}.
Given summaries of varying lengths from a random sample of documents, we prompt an
LLM to generate $n$ questions which each summary answers in a unique way.
The prompt specifies two requirements: (1) the questions should be answerable by most documents in the given genre, and (2) they should highlight meaningful differences between summaries of different lengths (full prompt in \cref{sec:appendix-prompts}).
For example, in movie reviews, most summaries will answer questions such as \emph{``What is the main plot of the movie?''}, but naturally the answers will be different for each review.
We repeat this process for all documents and associated summaries in the corpus.\footnote{
    We ran question generation over GPT-4o, Llama 3.1 (8B), and Mistral summaries. As the resulting questions were highly similar, we did not include additional models.
}

\paragraph{Step 2: Clustering.}
We then cluster questions with the same semantic intent.
For instance, \emph{``Is the soundtrack effective?''} and \emph{``How does the music contribute to the film's atmosphere?''} are considered equivalent, as they ask for the same information. We select the question closest to the mean embedding of each cluster as its representative. These questions form the topics $T$.

\paragraph{Step 3: Question-Answering and Claim Decomposition.}
For each \texttt{\small (original document, question)} pair, we first obtain a \emph{reference answer} using a QA-model.
We then decompose each answer into a set of atomic claims $A_t$ (see~\cref{fig:overview} for an example).
These claims support the answerability calculation (described next), and a fine-grained analysis of summary similarity and consistency through claim entailment patterns (\cref{sec:results-model-model-similarity}).

\paragraph{Step 4: Answerability Estimation.}
To quantify how well a summary answers a question, we measure what fraction of reference answer claims are entailed by the generated summary.
This approach naturally accounts for questions that are only partly answerable with a given summary.
Formally, let $A_t$ be the set of answer claims for a given question.
The answerability score is then calculated as:
%
\begin{equation}\label{eqn:answerability}
f(t,s) = \frac{1}{|A_t|} \sum\limits_{a \in A_t} e(a, s),
\end{equation}
%
where $e: A \times S \rightarrow \{0,1\}$ is a natural language inference (NLI) model that determines if claim $a$ is entailed (1) or not entailed (0) by summary $s$.
This practice of claim-entailment is commonly used in similar settings such as fact checking~\cite{Kamoi:2023:EMNLP,Min:2023:EMNLP,Stacey:2024:EMNLP}.

\paragraph{Implementation.}
For question generation, we found it necessary to use a strong model (i.e., \gpt).
For clustering, following~\citet{Lam:2024:CHI}, we represent questions using sentence embeddings~\cite{Reimers:2019:EMNLP}, followed by a dimensionality reduction and density-based clustering with \textsc{hdbscan}~\cite{McInnes:2017:JOSS} which requires minimal parameter tuning and does not presuppose a fixed number of clusters.\footnote{
    We use \texttt{all-mpnet-base-v2} for sentence embeddings, \textsc{umap} for dimensionality reduction, and cluster with \textsc{hdbscan} (leaf clustering, min size = 15, defaults: $\epsilon=0$, $\alpha=1$).
}
After an initial round of clustering, we found several overlapping clusters which were merged manually.
For question-answering and answer-claim splitting, we use Llama 3.1 8B (see \cref{sec:appendix-prompts} for the prompts).
For claim entailment, we use the efficient \texttt{\small MiniCheck}~\cite{Tang:2024:EMNLP}.
