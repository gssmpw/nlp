Accelerating inference in Large Language Models (LLMs) is critical for real-time interactions, as LLMs have been widely incorporated into real-world services.  Speculative decoding, a fully algorithmic solution, has gained attention for improving inference speed by drafting and verifying tokens, thereby generating multiple tokens in a single forward pass. However, current drafting strategies usually require significant fine-tuning or have inconsistent performance across tasks.
To address these challenges, we propose \textbf{Hierarchy Drafting} (HD)\footnote{\scriptsize \url{https://github.com/zomss/Hierarchy_Drafting}}, a novel lossless drafting approach that organizes various token sources into multiple databases in a hierarchical framework based on temporal locality. 
In the drafting step, HD sequentially accesses multiple databases to obtain draft tokens from the highest to the lowest locality, ensuring consistent acceleration across diverse tasks and minimizing drafting latency.
Our experiments on Spec-Bench using LLMs with 7B and 13B parameters demonstrate that HD outperforms existing lossless drafting methods, achieving robust inference speedups across model sizes, tasks, and temperatures.