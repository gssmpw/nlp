With the growing demand for accelerating Large Language Model (LLM) inference to enable efficient real-time human-LLM interactions, Speculative Decoding~\cite{BlockWise, SpecDecoding, SpecSampling} has gained attention for providing a fully algorithmic solution with minimal drawbacks.
While autoregressive decoding generates token by token, the decoding step in this method is divided into two substeps: \textit{drafting}, where likely tokens are sampled externally from a less complex model, and \textit{verifying}, where the sampled tokens are accepted or rejected by comparing with the LLM’s actual output.
By allowing the LLM to generate multiple accepted tokens in the verification phase, speculative decoding improves both the throughput and the latency of the LLM inference. 
Crucially, the efficiency of this approach depends on how draft tokens are generated, as performance gains hinge on the acceptance rate of these tokens~\cite{SpecSampling}.
Therefore, subsequent approaches to speculative decoding have focused on developing drafting strategies that sample tokens closely aligned with the target model.

\input{figure/motivation}

Recent efforts in speculative decoding have focused on developing effective drafting methods, using LM-based approaches, such as using smaller models than LLM~\cite{DistilSpec, SpecInfer} or incorporating specialized branches within the LLM architecture~\cite{MEDUSA, EAGLE2}.
However, their applicability in real-world scenarios is limited by the significant overhead associated with fine-tuning for optimization.
First, smaller models for drafting must be fine-tuned, such as by distillation, to generate tokens similar to LLMs to achieve optimal performance regardless of the given tasks~\cite{DistilSpec, multilingual}.
In addition, current LLM families~\cite{Llama2, vicuna} do not offer models of an appropriate size for drafting, often necessitating training from scratch.
In branch-based drafting, which modifies its original LLM architecture, the computational cost for training such branches within LLM is significant due to gradient calculations across the entire model, even though most parameters remain frozen~\cite{MEDUSA, EAGLE2, EAGLE}.
For example, EAGLE~\cite{EAGLE}, one of the leading methods, needs 1-2 days of training on 2-4 billion tokens using 4 A100 GPUs to train the 70B model.

To address these limitations, this paper explores a lightweight, lossless drafting strategy: \textit{Database Drafting}, eliminating the need for parameter updates~\cite{PLD, LAD, REST}. 
Database drafting constructs databases from various token sources and fetches draft tokens from the database using previous tokens.
However, as previous work relies on a single database from a single source, the coverage of draft tokens is restricted, leading to inconsistent acceleration across different tasks, as depicted in the left side of Figure~\ref{fig:motivation}. 
For example, PLD~\cite{PLD}, which uses previous tokens as its source, shows strengths in the summarization, highly repeating the tokens in the earlier texts, yet it achieves only marginal speedups in QA, where fewer promising tokens are included in the prior text. 
A straightforward solution to improve coverage is incorporating diverse sources into a single database. 
However, increasing the database scale leads to higher drafting latency, resulting in additional overhead.
As shown in the right side of Figure~\ref{fig:motivation}, REST~\cite{REST}, which uses the largest database, accurately predicts future tokens but suffers from significant latency, negating its high acceptance ratio benefits. 
Therefore, this paper proposes a solution to these limitations: \textit{Utilize diverse token sources simultaneously for robust performance and minimal overhead.}


With this objective in mind, we propose a simple yet effective solution: \textbf{Hierarchy Drafting} (HD), which integrates diverse token sources into a hierarchical framework. 
Our proposed method is inspired by the memory hierarchy system, which prioritizes data with high \textit{temporal locality} in the memory access for performance optimization~\cite{hierarchy}.
Therefore, HD groups draft tokens from diverse sources based on their temporal locality---the tendency for some tokens to reappear within or across generation processes. 
For example, when an LLM solves a math problem like, ‘\textit{The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3). What is the area of the triangle?}’, the coordinates frequently repeat within only a generation process for a given query but not across other generation processes.
In a related sense, phrases commonly generated by LLMs, such as ‘\textit{as an AI assistant}’, or frequent grammatical patterns exhibit relatively moderate locality, often appearing across different generation processes. 

Based on their temporal locality, the multiple databases of HD organize them into \textit{context-dependent database}, which stores tokens with high temporal locality for a given context; \textit{model-dependent database}, which captures frequently repeated phrases by LLMs across generations; and \textit{statistics-dependent database}, which contains statistically common phrases with slightly lower locality across processes than those in the model-dependent database.
During inference, HD accesses the databases in order of temporal locality, prioritizing tokens with high locality by starting with context-dependent, then model-dependent, and finally statistics-dependent databases until a sufficient number of draft tokens are obtained to convey to the LLM for verification.


This strategy has two benefits: firstly, increasing drafting accuracy by leveraging temporal locality and
secondly, reducing the overhead from drafting latency, as the scale of the databases is inversely correlated with the degree of locality—tokens with high locality are rarer. Thus, starting with the smaller context-dependent database for drafting tokens is more accurate and faster than using the larger statistics-dependent database alone.
Also, our hierarchical framework can encompass other database drafting methods owing to its \textit{plug-and-play} nature, making it easy to integrate diverse drafting sources based on their temporal locality.

We evaluate HD and other database drafting methods using widely adopted LLMs, Llama-2~\cite{Llama2} and Vicuna~\cite{vicuna}, on Spec-Bench~\cite{Spec_Survey}, a benchmark designed to assess effectiveness across diverse tasks.
Our proposed method, HD, outperforms other methods in our experiment and consistently achieves significant inference speedup across various settings, including model size, temperature, and tasks.
We also analyze how the hierarchical framework adaptively selects the appropriate database for each task while minimizing draft latency, aligning with our design goals.


Our contributions in this paper are threefold:
\vspace{-0.1in}
\begin{itemize}[itemsep=0.3mm, parsep=1pt, leftmargin=*]
    \item We identify the limitations of existing speculative decoding methods, which require additional fine-tuning or deliver inconsistent acceleration gains.
    \item We introduce a novel database drafting method, Hierarchy Drafting (HD), incorporating diverse token sources into the hierarchical framework for robust performance with minimizing overhead.
    \item We demonstrate that HD consistently achieves significant acceleration gains across various scenarios compared to other lossless methods.
\end{itemize}

