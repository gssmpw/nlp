We now introduce speculative decoding and lossless drafting strategies based on the database.

\paragraph{Speculative Decoding} 
Speculative decoding is a novel approach that accelerates LLM inference by minimizing the number of forward passes required, thereby reducing total latency~\cite{BlockWise, SpecDecoding, SpecSampling}. The core concept is that tokens, such as frequent phrases, can be predicted with high confidence using simpler models, enabling the generation of multiple tokens at once. \citet{BlockWise} introduced the \textit{Draft-then-Verify} paradigm, dividing each decoding step into two sub-steps: drafting multiple tokens from draft models and verifying them against LLM outputs in parallel. This concept has been expanded to accurately speculate the future tokens along with supporting sampling strategy~\cite{seq2seq, SpecDecoding, SpecSampling}. 

\paragraph{Types of Drafting Method}
The straightforward approach for the drafting strategy of speculative decoding involves using an additional language model (LM) specialized for drafting~\cite{SpecDecoding, SpecSampling, SpecInfer, DistilSpec}. 
To ensure effective drafting, such LMs must follow the target model's generation pattern and be smaller to minimize additional latency costs.
LMs with parameter sizes under a billion are typically preferred for drafting, but currently, widely used LLM families do not usually have appropriate models. 
For example, the smallest officially available Llama-2 model~\cite{Llama2}, with 7 billion parameters, is too large and inefficient for drafting purposes.
Therefore, such methodologies often require training overhead to get the suitable LM for the targeted LLM, such as the distilled models from the target models~\cite{SpecInfer} or lightweight models trained for mobile devices~\cite{TinyLlama}.

Instead of using a separate language model for drafting, some approaches enhance the drafting capabilities of the target model itself~\cite{MEDUSA, EAGLE2, EAGLE, Hydra}. 
In this line of work, the additional layer or branch in the target model is integrated into the target model to predict several subsequent tokens more than the very next token based on the last hidden states of given inputs.
Following \citet{BlockWise}, which exploits multiple heads for parallel decoding, Medusa~\cite{MEDUSA} first integrates additional decoding heads into the target model.
Subsequently, the branch-based drafting methodologies~\cite{EAGLE2, EAGLE, Hydra} show remarkable effectiveness in sampling appropriate future tokens with achieving state-of-the-art results.
However, integrating these layers or branches still requires significant training overhead. 
To sum up, branch-based drafting methods achieve remarkable speedup gains yet require additional computational costs, which are not trivial and are a new type of overhead for implementing speculative decoding.

\paragraph{Database Drafting}  
Database drafting eliminates training costs by retrieving draft tokens for previous inputs from a database rather than relying on smaller LMs or additional architectural branches. 
The database stores token pairs, with prefix tokens as keys and subsequent tokens as values.
The sources of these databases vary across different methods, with each method relying on its own unique database source. Some approaches utilize input prompt tokens as draft sources, which is particularly effective for tasks like summarization or retrieval-augmented generation, where input tokens are frequently repeated during generation~\cite{PLD, InfwRef}. Another method retrieves draft tokens from large text corpora by leveraging language patterns~\cite{REST}. 
Although retrieval from large corpora introduces some latency overhead, the acceleration gained from accurate drafting typically outweighs this, resulting in faster inference overall.
Additionally, LLMs can serve as sources for database drafting by generating tokens stored in the database, either through parallel decoding~\cite{ParallelDecoding, LAD} or token recycling~\cite{trashintotreasure}, where tokens are relevant to the current generation process. 
Finally, the previously generated texts by LLMs can be served as draft token sources because LLMs frequently reuse specific phrases or words~\cite{StagedSpec}.
Each source offers distinct strengths in predicting future tokens in certain scenarios, yet these strengths can become weaknesses in others. Therefore, it is worth noting that reliance on a single source may lead to limitations.

\input{table/related_work}

Table~\ref{tab:related_work} shows the experimental results of current database drafting methods, which construct their databases from a single source. 
Specifically, PLD~\cite{PLD} exhibits the highest speedup compared to other approaches but also shows a significant standard deviation in speedup gains. This variability is attributed to the limited and uneven sizes of the databases, leading to inconsistent acceleration across the generation process.
In contrast, LADE~\cite{LAD} achieves an impressively low drafting latencyâ€”less than 0.01 ms. However, this remarkable value does not translate to significant acceleration due to its small database size, akin to PLD. 
However, increasing database size alone, as demonstrated by REST~\cite{REST}, does not provide a viable solution for improving the effectiveness of database drafting. While a larger database scale can improve the accuracy of the drafting step, it also leads to higher latency since retrieving tokens from a larger database introduces additional processing overhead.

Therefore, to address the limitations of current lossless drafting methods relying on a single source, we propose integrating diverse sources into a hierarchical framework, aiming to harness each source's strengths more effectively with minimal overhead.

\input{figure/overview}


% In this work, we highlight that previous methods often overlook the distinct characteristics of databases, which can limit task coverage or reduce the benefits of accepted tokens. To this end, we explore integrating diverse sources into a hierarchical system to fully leverage the strengths of each source.
