
We begin by formally defining speculative decoding and database drafting and present our proposed method, Hierarchy Drafting (HD), which addresses the limitations of database drafting methods.

\subsection{Preliminary}

\paragraph{Speculative Decoding} 
At each step of speculative decoding, multiple tokens \(\tilde{\bm{x}}_{1:m}\) (i.e., draft token sequence) are drafted from an approximate model \(\mathcal{M}_q\) to predict future tokens of LLM \(\mathcal{M}_p\) (i.e., target model) for previous text tokens \(\bm{x}_{\leq t}\):
\begin{align}
    \tilde{\bm{x}}_{1:m} &\sim_m \mathcal{M}_q(\bm{x}_{\leq t}).
\end{align}

All draft token sequence \(\tilde{\bm{x}}_{1:m}\) are verified against the actual output of \(\mathcal{M}_p\). For example, in the greedy decoding, the tokens \(\bm{x}'_{t+1:t+m}\) are obtained for a given \(\tilde{\bm{x}}_{1:m}\) and \(\bm{x}_{\leq t}\) by solving the following equations in parallel:
\begin{align}
\begin{cases}
    x'_{t+1} &= \argmax P_{\mathcal{M}_p}(x | \bm{x}_{\leq t}), \\
    x'_{t+2} &= \argmax P_{\mathcal{M}_p}(x | \tilde{x}_{1}, \bm{x}_{\leq t}),\\
    &\dots \\
    x'_{t+m} &= \argmax P_{\mathcal{M}_p}(x | \tilde{\bm{x}}_{1:m}, \bm{x}_{\leq t}).
\end{cases}
\end{align}
Each token \(x'_{t+i}\) is verified against the corresponding draft token \(\tilde{x}_{t+i}\), starting from \(i = 0\) until the verification fails or \(i = m\) is reached.
To enhance the likelihood of acceptance, multiple draft token sequences \(\bm{\tilde{X}} = \{\tilde{\bm{x}}^i\}_{i=1}^N\) (i.e., draft set) are verified in parallel.
The specialized attention mask implements the parallel verification of the draft set, not causal attention mask~\cite{LAD, SpecInfer}.
In the sampling strategy, speculative sampling~\cite{SpecSampling} is commonly used to accept more tokens while maintaining identical output distributions of the target model.
In summary, the generation step is divided into two sub-steps with a single forward pass of the target model. The multiple accepted tokens are generated simultaneously, compressing the overall decoding process.
% In summary, the generation step consists of two sub-steps: a single forward pass of the target model, followed by simultaneous generation of multiple tokens accepted through verification, compressing the overall decoding process.


\paragraph{Database Drafting}
As shown on the left side of Figure~\ref{fig:overview}, the methods included in database drafting exploit the database \(\mathcal{D}\), having the prefix tokens as the key and the subsequent tokens as the value. Per each step of the generation process, the draft token sequence \(\bm{\tilde{x}}_{1:m}\) is retrieved from database \(\mathcal{D}\) for given previous tokens \(\bm{x}_{t-l:t}\):
\begin{align}
    \bm{\tilde{x}}_{1:m} \in \bm{\tilde{X}} &= \texttt{Ret}(\bm{x}_{t-l:t};\mathcal{D}),
\end{align}
where \(l\) and \(m\) are the length of previous tokens and draft token sequence. Subsequently, the verifying step is the same as other methods.

\subsection{Hierarchy Drafting}

We introduce Hierarchy Drafting (HD), which organizes tokens from diverse sources into three databases based on temporal locality and accesses them in order from the smallest to the largest scale. The overview and decoding process are depicted on the right side of Figure~\ref{fig:overview}.

\input{figure/generation}

\paragraph{Observation: Temporal Locality}
The main idea behind database drafting is that some tokens are easy to retrieve from the database because they exhibit temporal locality—meaning they tend to be repeated within or across the generation processes. 
However, note that not all draft token sequences share the same level of temporal locality during generation. 
We analyze the pattern of unique 4-grams during 100 text generations on Spec-Bench~\cite{Spec_Survey}, as shown in Figure~\ref{fig:generation}. The results reveal that certain 4-grams are frequently repeated and exhibit varying locality levels.
Specifically, the blue dots and the right small plot in Figure~\ref{fig:generation} illustrate local redundancy, where the same 4-gram appears multiple times within a single generation step. This reflects high temporal locality within a single generation rather than across multiple generations. In contrast, the red dots in Figure~\ref{fig:generation} highlight a pattern where the model repeatedly generates the same 4-grams at different stages of the generation process, illustrating its tendency to reuse familiar sequences over time.
Additionally, the lower plot of Figure~\ref{fig:generation} presents the frequency study of sampled red and blue dots, demonstrating that some tokens exhibit high temporal locality within a specific context, while others maintain consistent locality across generation processes.
Therefore, given the varying temporal locality of tokens throughout the generation process, drafting steps should prioritize tokens with higher temporal locality over others.

% For example, when an LLM solves a math problem like, “The vertices of a triangle are at points (0, 0), (-1, 1), and (3, 3). What is the area of the triangle?”, the coordinates are frequently repeated.
% Next, frequently generated phrases by LLMs, such as “as an AI assistant,” show moderate locality, as they often appear across various generation processes for LLM-generated texts. 
% Finally, grammatical patterns and universal phrases commonly used by humans have the lowest locality, as they are statistically frequent across all types of texts yet do not constantly occur in each generation process.

\paragraph{Database Design}  
 Based on the temporal locality of draft token candidates, we design three types of databases to categorize them. 
\textbf{1) Context-dependent DB} (\(\mathcal{D}_c\)) contains tokens highly relevant to the specific context of the generation process, such as the blue dots in the Figure~\ref{fig:generation}. 
This includes tokens from the input prompt, tokens generated through parallel decoding, tokens discarded during the generation process, and others that are highly relevant to a given context.  
\(\mathcal{D}_c\) is lookup table with the prefix tokens, \(\bm{x}_{1:l}\), as the key and the subsequent tokens, \(\bm{x}_{l:l+m}\), as the value.
Also, \(\mathcal{D}_c\) is consistently updated during each forward step and initialized when the following generation process is started. 
The database follows the Least Recently Used (LRU) policy for draft sequence updates.
\textbf{2) Model-dependent DB} (\(\mathcal{D}_m\)) stores tokens frequently generated by LLM regardless of context, as represented by the red dots in Figure~\ref{fig:generation}.
Top-$k$ frequently generated token sequences, $\bm{x}_{1:l+m}$, are sampled from the model-generated texts, with $\bm{x}_{1:l}$ as the key and $\bm{x}_{l+1:l+m}$ as the value.
For \(\mathcal{D}_c\) and \(\mathcal{D}_m\), the maximum size of values for a single key is the same as the maximum draft set size \(N\). 
\textbf{3) Statistics-dependent DB} (\(\mathcal{D}_s\)) draws its tokens from large text corpora to capture universal phrases commonly used in the language. 
Although these tokens are frequent, they occur less consistently across processes than those in \(\mathcal{D}_m\).
To efficiently retrieve the sequence from a large corpus, we utilize a suffix array~\cite{suffix_array} following the implementation of~\citet{REST}.
Implementation details are in \S\ref{sec:experiement}.


Our database design yields three distinct advantages. First, it integrates diverse sources into multiple databases, enabling us to leverage each source’s strengths for robust acceleration across various tasks. Then, each database’s size decreases as the tokens’ temporal locality increases since tokens with higher locality are rarer, providing an opportunity to optimize drafting latency. Finally, the design is \textit{plug-and-play}, easily integrating additional token sources by assigning them to the appropriate database based on their temporal locality.

\input{alg/HD_process}

\paragraph{Hierarchical Access}
Using the three databases designed with the temporal locality in mind, we retrieve draft token sequence \(\bm{\tilde{x}}_{1:m}\) for the given previous input \(\bm{x}_{t-l:t}\).
Database access order is based on the degree of temporal locality within the current generation process; thereby, the access starts with \(\mathcal{D}_c\).
Access then proceeds to \(\mathcal{D}_m\), which has high locality across generations, and finally \(\mathcal{D}_s\), with moderate locality across generations, until draft set \(\bm{\tilde{X}}\) accumulates a sufficient number of candidates as pre-defined hyperparameter \(N\).
These accesses leverage the locality of the draft token sequence to enhance drafting accuracy and minimize latency overhead, preserving the benefits of drafting.

\paragraph{Decoding Process}
We introduce the inference process of speculative decoding with our proposed method, HD. 
% First, for a given previous input \(\bm{x}_{t-l, t}\), the set of draft token \(\bm{\tilde{X}}\) are acquired from the three databases with hierarchical access. 
First, for a given previous input \(\bm{x}_{t-l, t}\), we acquire the set of draft token \(\bm{\tilde{X}}\) from the three databases with hierarchical access. 
% Then, the target LLM \(\mathcal{M}_p\) verifies the draft token sequences simultaneously generating the additional tokens \(\bm{\hat{x}}\) for updating context-dependent DB either through parallel decoding~\cite{ParallelDecoding, LAD} or by recycling wasted tokens~\cite{trashintotreasure}. 
Then, the target LLM \(\mathcal{M}_p\) verifies the draft token sequences while simultaneously generating the additional tokens \(\bm{\hat{x}}\). 
These tokens are used to update the context-dependent DB either through parallel decoding~\cite{ParallelDecoding, LAD} or by recycling wasted tokens~\cite{trashintotreasure}. 
These processes are repeated iteratively until either the \texttt{[EOS]} token is generated or the sequence reaches the pre-defined maximum length \(T\).
Details of the decoding are depicted in Algorithm~\ref{alg:HD_process}.

