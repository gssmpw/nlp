\section*{Limitation}

One limitation of this paper is the limited use of LLMs with more than 13B parameters. While our evaluation focused on models like Llama-2 and Vicuna with up to 13B parameters, the performance of HD on larger models remains unexplored. However, we expect that the larger models will be much more appropriate for our approach, considering the high acceptance ratio of our proposed method, HD, across diverse scenarios and decreased sensitivity to draft latency as generation latency increases.
Also, we plan to extend our experiments to larger models in future work. 

While this paper leverages multiple databases to maximize their strengths with minimal overhead, the sources of these databases are not entirely new. Rather than focusing on the novelty of each source, we emphasize that our approach is \textit{plug-and-play}, making it easy to integrate future methods by simply adding tokens from new sources into the appropriate database. For instance, although we omitted token recycling~\cite{trashintotreasure} in our experiments, recycled tokens could be added to the context-dependent database, given their temporal locality.

\section*{Ethics Statements}

This work proposes a lossless drafting strategy in speculative decoding for optimal and general acceleration gains. However, our method may generate violent or biased responses, which is beyond the scope of this paper. We strongly believe that future research on large language models will address these issues and help mitigate such concerns.