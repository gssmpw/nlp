\label{sec:experiement}
We introduce the details of the experiment setups conducted to evaluate the effectiveness of HD.
% covering the datasets, models, baseline methods, evaluation metrics, and implementation details.

\paragraph{Dataset} 
We exploit Spec-Bench~\cite{Spec_Survey}, a comprehensive benchmark to evaluate speculative decoding across various tasks. Specifically. the collected datasets are MT-bench~\cite{vicuna} for Multi-turn Conversation, WMT14 DE-EN~\cite{translation} for Translation, CNN/Daily Mail~\cite{summarization} for Summarization, Natural Question~\cite{QA} for Question Answering, GSM8K~\cite{math} for Math Reasoning, DPR~\cite{RAG} for RAG. Each task has 80 instances, making a total of 480 generations.
% multi-turn conversation~\cite{vicuna}, translation~\cite{translation}, summarization~\cite{summarization}, question answering (QA)~\cite{QA}, mathematical reasoning~\cite{math}, and retrieval-augmented generation (RAG)~\cite{RAG}

\paragraph{Model}
We utilize two LLM families: \textbf{Vicuna-v1.3-\{7,13,33\}B}~\cite{vicuna} and \textbf{Llama-2-chat-\{7,13\}B}~\cite{Llama2} to demonstrate the effectiveness of the proposed method. 
% Additionally, 7B and 13B models are exploited to assess the effectiveness across different model scales.

\input{table/main_table}

\paragraph{Baseline Method}
% We compare HD with autoregressive decoding and various database drafting methods. Specifically, \textbf{1) Autoregressive decoding (AR)} serves as an indicator for measuring acceleration gains. Additionally, we include \textbf{2) PLD}~\cite{PLD}, which uses previous input prompts as a database, \textbf{3) LADE}~\cite{LAD}, which employs parallel decoding via a Jacobian iteration method, and \textbf{4) REST}~\cite{REST}, which retrieves draft tokens from a large text corpus. Finally, our proposed method, \textbf{5) HD}, is included to validate its effectiveness compared to others.
We compare our proposed method, \textbf{HD}, with autoregressive decoding and various database drafting methods to validate its effectiveness. Specifically, \textbf{1) Autoregressive decoding (AR)} serves as an indicator for measuring acceleration gains. We also include \textbf{2) PLD}\footnote{PLD is included only in the greedy setting due to its official repository's lack of temperature sampling support.}~\cite{PLD}, utilizing previous input prompts as a database, \textbf{3) LADE}~\cite{LAD}, employing parallel decoding via a Jacobian iteration method, and \textbf{4) REST}~\cite{REST}, which retrieves draft tokens from a large text corpus. 
% Finally, our proposed method, \textbf{5) HD}, is included to validate its effectiveness compared to others.

% \input{table/DB_details}

\paragraph{Evaluation Metric} 
We utilize a variety of metrics to evaluate drafting overhead, drafting accuracy, and acceleration gain. To measure drafting overhead, we use \textbf{1) Drafting Latency}, which refers to the time taken to fetch draft tokens. Following~\citet{DistilSpec}, the drafting accuracy is assessed using \textbf{2) Acceptance Ratio (\(\alpha\))} and \textbf{3) Mean Accepted Tokens (\(\tau\))}. The acceptance ratio (\(\alpha\)) represents the ratio of accepted tokens to total tokens, while the mean accepted tokens (\(\tau\)) denotes the expected number of accepted tokens per step. Finally, acceleration gain is measured using the \textbf{4) Speedup Ratio}, which compares \#tokens/sec of each method from autoregressive decoding.


\paragraph{Implementation Detail}
The proposed method, HD, is configured with the hyperparameters \(l\), \(m\), \(N\), and \(T\) set to 2, 4, 7, and 1024, respectively. 
Specifically, \(l\) denotes the length of the previous tokens used as the database key, and \(m\) represents the length of the draft sequence used as the database value. Finally, \(N\) specifies the size of the draft set passed to the LLM for verification. 
% the structure of the draft tokens is based on 4-grams, 
% thereby leaving applying tree-based verification as valuable future work.
To adopt a sampling strategy, we exploit speculative sampling ~\cite{SpecSampling} by setting draft probability as 1.0.
For the context-dependent database (\(\mathcal{D}_c\)), the previous input tokens and the tokens generated via parallel decoding are included. 
For parallel decoding, we follow the implementation proposed by LADE~\cite{LAD}, which allows simultaneous processing of the parallel decoding and verification branches.
Therefore, following the implementation of LADE, the verifying step is based on \textit{n}-gram.
For the model-dependent database (\(\mathcal{D}_m\)), we collect LLM-generated texts from the English portion of the OASST training set~\cite{OASST}, using a 7B model from the targeted LLM family.
A total of 39,283 texts were generated, from which we sampled the 100k most frequent token sequences.
Lastly, for the statistics-dependent database (\(\mathcal{D}_s\)), we adopt the setting of REST~\cite{REST}, utilizing data sourced from UltraChat~\cite{UltraChat}, with the database size being approximately 12GB.
More details are in Appendix~\ref{app:impl}.

\input{figure/temperature_and_task}


\paragraph{Experimental Setup}
All experiments are conducted on a machine equipped with a single A100-40GB-PCIe GPU for 7B and 13B models and A100-80GB-PCIe GPU for 33B model, using float16 precision for the models. To ensure a fair comparison, we follow the implementations of other database drafting methods and the evaluation scripts provided by~\citet{Spec_Survey}\footnote{\scriptsize \url{https://github.com/hemingkx/Spec-Bench}}. Our experimental results are based on a single run, though we observed only marginal differences between runs.