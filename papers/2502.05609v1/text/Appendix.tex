\section*{Appendix}

In the Appendix, we introduce more related work and details on how the experiments were implemented, along with supplementary results and analysis not included in the main text.

\section{More Related Work}

\paragraph{LLM Inference Acceleration}
Large Language Models (LLMs)~\cite{GPT3, Llama2} have significantly advanced Natural Language Processing (NLP) and are widely used in real-world applications via APIs, highlighting the importance of real-time human-LLM interactions. However, the slow inference speed of LLMs presents the primary bottleneck in deploying these models as practical services. This issue is predominantly memory-bound, with latency arising from the autoregressive decoding process~\cite{latencylagsbandwidth, ScalingTransformerInference, MemoryWall}. During the inference process, each token generation requires transferring model parameters and key-value caches from global memory to the accelerator’s cache, consuming substantial overhead. Given the limited progress in memory bandwidth, several algorithmic approaches have been developed to mitigate these challenges, including model parameter quantization~\cite{ZeroQuant, GPTQ, SqueezeLLM}, memory allocation optimization~\cite{MQAttn, H2O, PagedAttn}, and Speculative Decoding~\cite{BlockWise, SpecDecoding, SpecSampling}.
While other approaches often require hardware modifications or lead to side effects like performance degradation, speculative decoding has gained particular attention for offering a fully algorithmic solution with minimal drawbacks.


\section{More Implementation Details}\label{app:impl}

\paragraph{Implementation Details of HD}  First, we introduce more details of our multiple databases. For \(\mathcal{D}_c\) and \(\mathcal{D}_m\), the prefix token length used as the key is 1, while the given previous token length is \(l\). This mismatch is because increasing the prefix length often leads to draft misses (i.e., keys not found in the database) due to the limited scale of token sources constructing \(\mathcal{D}_c\) and \(\mathcal{D}_m\). Also, for \(\mathcal{D}_s\), the retrieval process is repeated by reducing the previous token length until draft token sequences are found or the token length reaches 0, following implementation of REST~\cite{REST}.
As mentioned in the main text, \(\mathcal{D}_c\) and \(\mathcal{D}_m\) are lookup tables implemented by the Python Dictionary class. In contrast, \(\mathcal{D}_s\) is implemented by DraftRetriever\footnote{\scriptsize{\url{https://github.com/FasterDecoding/REST/tree/main/DraftRetriever}}}, a Python Library based on suffix arrays, proposed by REST for handling a large text corpus with minimal overhead.
The average number of values (i.e., draft token sequences) stored in the database is 1K, 100K, and 200M for \(\mathcal{D}_c\), \(\mathcal{D}_m\), and \(\mathcal{D}_s\), respectively.

We now explain the detailed implementation of the decoding process with HD. Our method is primarily based on the implementation of LADE\footnote{\scriptsize \url{https://github.com/hao-ai-lab/LookaheadDecoding}}~\cite{LAD}, as LADE employs a similar database to $\mathcal{D}_c$ and updates tokens generated through parallel decoding. We extend this by incorporating a hierarchical framework; thereby, the verification step in HD follows LADE’s $n$-gram verification process. However, we emphasize that HD can be extended to other database drafting methods by integrating a hierarchical drafting framework with multiple databases into their implementation.

\paragraph{Prompting Format}
We use the chat format for text input, provided by the Python library FastChat\footnote{\scriptsize \url{https://github.com/lm-sys/FastChat}}, which consists of system, user, and model components. Additionally, we adopted the system message for Vicuna from FastChat and for Llama-2 from the official repository\footnote{\scriptsize \url{https://github.com/meta-llama/llama/blob/main/example_chat_completion.py}}.

\input{figure/tokens}

\paragraph{Details of Dataset}

We exploit Spec-Bench~\cite{Spec_Survey}, collecting data from representative datasets for each task.
Specifically. the collected datasets are MT-bench~\cite{vicuna} for Multi-turn Conversation, WMT14 DE-EN~\cite{translation} for Translation, CNN/Daily Mail~\cite{summarization} for Summarization, Natural Question~\cite{QA} for Question Answering, GSM8K~\cite{math} for Math Reasoning, DPR~\cite{RAG} for Retrieval Augmented Generation.

\section{Additional Results}\label{app:additional_result}

\subsection{Impact of Generated Token Length} 

Since speculative decoding shortens generation steps and reduces the correlation between generated token length and elapsed latency, we explore this relationship to showcase the effectiveness of HD.
As depicted in Figure~\ref{fig:tokens}, all database drafting methods successfully lower the slope compared to autoregressive decoding. 
Among them, HD demonstrates the shallowest slope, highlighting its effectiveness even for long text generation. 
Furthermore, despite variations in drafting latency and accuracy among other methods, as shown in Table~\ref{tab:main}, they exhibit similar slopes, underscoring the importance of balancing latency and drafting accuracy to achieve optimal acceleration for database drafting methods.


\subsection{Impact of Temperature}
\input{figure/temperature}
As temperature sampling is commonly used to increase the diversity of text generation, we analyze its impact on the database drafting methods, as shown on the left of Figure~\ref{fig:temperature}.  
Other drafting methods maintain higher speeds than autoregressive decoding across all temperatures. LADE slightly decreases as temperature increases, whereas REST remains consistent.
% As updating \(\mathcal{D}_c\) with sampled outputs, similar to LADE, leads to token mismatches after applying higher temperatures, HD shows a slight reduction in speed at higher temperatures.
However, updating \(\mathcal{D}_c\)  with sampled outputs, similar to LADE, results in token mismatches at higher temperatures, causing HD to show a slight reduction in speed.
Nevertheless, our proposed method, HD, outperforms all others across the entire temperature range, maintaining a high decoding speed of over 70 tokens per second.
% Note that our proposed method, HD, outperforms all others across the entire temperature range, maintaining the highest decoding speed despite a slight reduction in speed at higher temperatures.
% The slight decrease is due to sampling outputs for updating $\mathcal{D}_c$, similar to LADE, which leads to token mismatches after applying higher temperatures.
These results demonstrate that HD remains a suitable solution even with a sampling strategy, enhancing its potential in real-world scenarios.
% \input{figure/temperature}

\subsection{Database Access Statistics}
\input{figure/access_pattern}


We conducted a breakdown study of database access patterns, as shown in Figure~\ref{fig:access_pattern}, focusing on draft \& verify success and its relationship to the temporal locality of draft tokens. The context-dependent database (\(\mathcal{D}_c\)) achieves the highest verify success rate (32.3\%), highlighting its strong alignment with recent input tokens due to its ability to capture local context, though its limited scale results in higher draft failure rates. The model-dependent database (\(\mathcal{D}_m\)) has a lower verify success rate (15.5\%), yet its broader scope allows for more frequent draft success, albeit less aligned with the immediate context. Finally, the statistics-dependent database (\(\mathcal{D}_s\)) exhibits the lowest verify success (6.5\%) but benefits from its vast coverage, which makes it less sensitive to temporal locality. These patterns suggest that \(\mathcal{D}_c\) is critical for capturing temporally localized tokens, while \(\mathcal{D}_m\) and \(\mathcal{D}_s\) complement it by providing a more general, though not as closely aligned to the LLM generation.

\subsection{Ablation Study}

\input{table/ablation}
% \input{figure/ablation}

We conducted an ablation study on the databases used in HD, as shown on the left side of Table~\ref{tab:ablation}. Using only a single database results in an acceptance ratio below 60\%, with a significant increase in draft failures, notably when $\mathcal{D}_s$ is excluded. Incorporating two databases improves the acceptance ratio and reduces draft failures, but it still underperforms compared to all three databases. These findings highlight the importance of combining multiple databases to improve token acceptance, leading to more robust and efficient performance.

Additionally, we observe the need to balance both the acceptance ratio and drafting latency for better speedup. For instance, using the largest database $\mathcal{D}_c$ alone increases the acceptance ratio but significantly raises drafting latency, resulting in the worst speedup—even slower than autoregressive decoding. Although the acceptance ratio with ($\mathcal{D}_c, \mathcal{D}_m$) is 5\% lower than with ($\mathcal{D}_c, \mathcal{D}_m, \mathcal{D}_s$), it achieves higher speedup due to trivial drafting latency under 1ms. However, the negative impact of drafting latency may be mitigated when applying HD to larger models. Since the longer generation latency of larger LLMs makes drafting latency negligible, a high acceptance ratio becomes crucial for acceleration. From these insights, our future research will focus on reducing drafting latency to be more effective regardless of model scale while maintaining an optimal acceptance ratio.

\subsection{Impact of Token Quality}

\input{table/db_ablation}

To further investigate the impact of token quality, we conducted experiments using alternative token sources, as presented in Table~\ref{tab:db_ablation}. For Vicuna-7b, we utilized Llama-7b responses as model-dependent databases and the code generation corpus, The Stack (924MB)~\cite{Stack}, as a statistics-dependent database. Also, we exploited a small version of the general generation corpus, ShareGPT (465MB)\footnote{\url{https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered/blob/main/ShareGPT_2023.05.04v0_Wasteland_Edition.json}}. While token sources' quality influences acceleration, our proposed method demonstrates significant acceleration gains even when the token sources are not well-aligned with the target LLM or task. In addition, as discussed in the main text, drafting latency is a critical consideration, with smaller statistics-dependent databases demonstrating better acceleration despite a lower accepted length.



\subsection{Token Coverage}

\input{figure/token_coverage}

Beyond the ablation study, we performed an in-depth analysis to verify the unique token distributions of each database by examining the accepted tokens when using only a single database, as shown on the right side of Figure~\ref{fig:token_coverage}. A significant portion of unique draft tokens was found, indicating that no single database can handle all tokens for drafting. Specifically, the context-dependent database (\(\mathcal{D}_c\)) contains 16\% unique accepted tokens not found in other databases, though its limited scale often leads to draft failures. These findings confirm that our designed databases complement each other by compensating for individual weaknesses in token distribution.


\subsection{Case Study}

Table~\ref{tab:case_study} presents a case study of text generation in question answering (QA) and retrieval-augmented generation (RAG) tasks using Llama-2-7b. The texts highlighted in green, red, and yellow are retrieved from \(\mathcal{D}_c\), \(\mathcal{D}_m\), and \(\mathcal{D}_s\), respectively. Red-highlighted texts are usually found in the middle of the input and are contextually relevant, often including numerical data or named entities. Green-highlighted texts typically appear at the beginning or end, repeated across generation processes; for example, the phrase ‘\textit{Thank you for your question!}’ is consistently retrieved from \(\mathcal{D}_m\). Yellow-highlighted texts are rarer and often capture grammatical patterns, such as articles or prepositions. Also, notable differences exist between tasks: green texts appear more frequently in the QA task, while red texts dominate in the RAG task.


\input{table/case_study}
