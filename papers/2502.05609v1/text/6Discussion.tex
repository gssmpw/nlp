\input{figure/order}


Although our proposed method achieves significant performance gains over other database drafting methods, recent approaches based on model retraining~\cite{MEDUSA, EAGLE2, Hydra} have demonstrated substantially higher acceleration.
However, it is essential to note that the training costs associated with these methods are non-trivial, particularly in dynamic or resource-intensive settings. 
For instance, retraining-based approaches necessitate additional training steps, which pose practical challenges in real-world applications like multi-model serving~\cite{SLoRA, OptCall} or resource-limited environments~\cite{Edge}. 
Specifically, deploying multiple LLMs for diverse domain-specific tasks using numerous LoRA adapters~\cite{SLoRA} or employing model routing strategies for efficient serving~\cite{OptCall} can significantly increase computational overhead when such methods must be applied to all LLMs. 
As a result, the retraining requirement can complicate deployment, particularly in real-world serving scenarios.

Given these constraints, we position database drafting methods as a practical alternative to model retraining by leveraging readily available data resources in serving scenarios rather than asserting the best performance. 
Database drafting methods can effectively address serving challenges in real-world applications by achieving fully lossless speculative decoding without requiring parameter updates. 
Among database drafting methods, our proposed method, HD, further enhances the practicality of database drafting by incorporating diverse data resources into a hierarchical framework for accurately and efficiently drafting future tokens of LLMs. Thus, HD narrows the performance gap with state-of-the-art speculative decoding methods, demonstrating the potential of database drafting to accelerate inference significantly without fine-tuning models.