\documentclass[manuscript,nonacm]{acmart}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\setcopyright{none}

\usepackage{amsmath}
\usepackage[inline]{enumitem}
\usepackage{algorithmic}
\usepackage{alltt}
\usepackage{array}
\usepackage{booktabs}
\usepackage{boxedminipage}
\usepackage[capitalise, nameinlink, noabbrev]{cleveref}
\usepackage{caption}
\usepackage{colortbl}
\usepackage{color}
\usepackage{enumitem}
\usepackage{fp}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{lineno}
\usepackage{listings}
\usepackage{xltabular}
\usepackage{MnSymbol}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{orcidlink}
\usepackage{pifont}
\usepackage{relsize}
\usepackage{subcaption}
\usepackage{syntax}
\usepackage{tabularx}
\usepackage[textsize=tiny]{todonotes}
\usepackage{tcolorbox}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage{url}
\usepackage{wasysym}
\usepackage{wrapfig}
\usepackage{xspace}

\usetikzlibrary{tikzmark}
\usetikzlibrary{arrows.meta}

\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1pt,fill=orange,line width=0.5pt] (char) {#1};}}

\newcommand*\circledext[1]{\tikz[baseline=(char.base)]{
	\node[shape=circle,draw,inner sep=1pt,fill=violet,line width=0.5pt] (char) {\textcolor{white}{#1}};}}

\newtheorem{definition}{Definition}[section]

% Abbreviations

\def\ACCURACY{\emph{Accuracy}\xspace}
\def\AE{{\smaller{}AE}\xspace}
\def\AFL{{\scshape{}afl}\xspace}
\def\AFLFAST{{\scshape{}aflfast}\xspace}
\def\AFLPP{{\scshape{}afl++}\xspace}
\def\ALHAZEN{{\scshape{}alhazen}\xspace}
\def\AMPLE{{\smaller{}AMPLE}\xspace}
\def\ANTLR{{\scshape{}antlr}\xspace}
\def\ASTOR{{\smaller{}ASTOR}\xspace}
\def\ATLAS{{\scshape{}ATLAS}\xspace}
\def\AVICENNA{{\scshape{}Avicenna}\xspace}
\def\BASHIRI{{\scshape{}bashiri}\xspace} % Swahili word for "predict", "consult a fortune teller".
\def\BEARS{{\scshape{}Bears}\xspace}
\def\BUG{{\scshape{}bug}\xspace}
\def\BUGSINPY{{\scshape{}BugsInPy}\xspace}
\def\BUGSJAR{{\scshape{}Bugs.jar}\xspace}
\def\BUGSJS{{\scshape{}BugsJS}\xspace}
\def\BUGSWARM{{\scshape{}BugSwarm}\xspace}
\def\C{{\scshape{}C}\xspace}
\def\CALCULATOR{{calculator}\xspace}
\def\CARDUMEN{{\smaller{}CARDUMEN}\xspace}
\def\CODEFLAWS{{\scshape{}Codeflaws}\xspace}
\def\COOKIECUTTER{{\scshape{}cookiecutter}\xspace}
\def\COOKIECUTTERFOUR{{\scshape{}cookiecutter.4}\xspace}
\def\COOKIECUTTERTHREE{{\scshape{}cookiecutter.3}\xspace}
\def\COOKIECUTTERTWO{{\scshape{}cookiecutter.2}\xspace}
\def\DSTAR{{\smaller{}D*}\xspace}
\def\DEEPREPAIR{{\smaller{}DEEPREPAIR}\xspace}
\def\DEFECTS4J{{\scshape{}Defects4J}\xspace}
\def\EFDD{{\scshape{}EFDD}\xspace}
\def\ENTBUG{{\scshape{}entbug}\xspace}
\def\EXAM{{\smaller{}EXAM}\xspace}
\def\EXPRESSION{{expression}\xspace}
\def\FASTAPI{{\scshape{}FastAPI}\xspace}
\def\FASTAPIONE{{\scshape{}FastAPI.1}\xspace}
\def\FASTAPITWO{{\scshape{}FastAPI.2}\xspace}
\def\FASIRI{{\smaller{}FASIRI}\xspace}
\def\FIXKIT{{FixKit}\xspace}
\def\FONESCORE{\emph{F1 Score}\xspace}
\def\FUZZBENCH{{\scshape{}FuzzBench}\xspace}
\def\GASSERT{{\scshape{}GAssert}\xspace}
\def\GENPROG{{\smaller{}GENPROG}\xspace}
\def\GP{{\smaller{}GP}\xspace}
\def\GPOT{{\smaller{}GP13}\xspace}
\def\HTTPIE{{HTTPie}\xspace}
\def\IDISCOVERY{{\scshape{}iDiscovery}\xspace}
\def\ISLa{{\scshape{}ISLa}\xspace}
\def\ISLearn{\scshape{}ISLearn\xspace}
\def\JACCARD{{\scshape{}jaccard}\xspace}
\def\JAVA{{\scshape{}Java}\xspace}
\def\JSON{{\scshape{}json}\xspace}
\def\KALI{{\smaller{}KALI}\xspace}
\def\LEARNTWOFIX{{\scshape{}learn2fix}\xspace}
\def\MACRO{{macro}\xspace}
\def\MARKUP{{markup}\xspace}
\def\MIDDLE{{middle}\xspace}
\def\MIDDLEONE{{\scshape{}middle.1}\xspace}
\def\MUTREPAIR{{\smaller{}MUTREPAIR}\xspace}
\def\NAISHT{{\smaller{}NAISH2}\xspace}
\def\NOBUG{{\scshape{}no-bug}\xspace}
\def\OCHIAI{{\smaller{}OCHIAI}\xspace}
\def\PRECISION{\emph{Precision}\xspace}
\def\PYENV{{\scshape{}pyenv}\xspace}
\def\PYSNOOPER{{\scshape{}PySnooper}\xspace}
\def\PYSNOOPERTHREE{{\scshape{}PySnooper.3}\xspace}
\def\PYSNOOPERTWO{{\scshape{}PySnooper.2}\xspace}
\def\PYTEST{{\scshape{}pytest}\xspace}
\def\PYTHON{{\scshape{}Python}\xspace}
\def\RECALL{\emph{Recall}\xspace}
\def\REFACTORY{{\scshape{}refactory}\xspace}
\def\RESULTREF{{89\%}\xspace}
\def\SANIC{{Sanic}\xspace}
\def\SCIKITLEARN{{\scshape{}scikit-learn}\xspace}
\def\SEER{{\scshape{}SEER}\xspace}
\def\SFLKIT{{\scshape{}SFLKit}\xspace}
\def\SHAP{{\scshape{}shap}\xspace}
\def\SPR{{\smaller{}SPR}\xspace}
\def\SWAMI{{\scshape{}Swami}\xspace}
\def\TARANTULA{{\smaller{}TARANTULA}\xspace}
\def\TESTS4PY{{\scshape{}Tests4Py}\xspace}
\def\THEFUCK{{The Fuck}\xspace}
\def\TOGA{{\scshape{}TOGA}\xspace}
\def\TORADOCU{{\scshape{}Toradocu}\xspace}
\def\WEIGHTEDAVERAGE{{weighted average}\xspace}
\def\WORD2VEC{{\smaller{}WORD2VEC}\xspace}
\def\YOUTUBEDL{{youtube-dl}\xspace}


% Links
\definecolor{rltred}{rgb}{0.5,0,0}
\definecolor{rltgreen}{rgb}{0,0.5,0}
\definecolor{rltblue}{rgb}{0,0,0.5}
\hypersetup{
colorlinks=true,
urlcolor=rltblue,
linkcolor=rltred,
citecolor=rltgreen,
}

% Dealing with identifiers
\newcommand{\mathid}[1]{\textit{#1}}
\newcommand{\codeid}[1]{\texttt{#1}}
% \let\codeid=\mathid
\def\|#1|{\mathid{#1}}
\def\<#1>{\codeid{#1}}

% Use Python style for highlighting
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}
\definecolor{deeporange}{rgb}{0.8,0.45,0}
\lstset{
language=Python,
basicstyle=\tt,
morekeywords={self, def, if, elif, return, else},              % Add keywords here
keywordstyle=\tt\bfseries\color{deeporange},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\tt\bfseries\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false
}

% Colors

% Colors for statistical debugging
\definecolor{Grey}{rgb}{0.5,0.5,0.5}
\definecolor{LightGrey}{rgb}{0.9,0.9,0.9}
\definecolor{Green}{rgb}{0.0,0.6,0.0}
\definecolor{Red}{rgb}{0.6,0.0,0.0}
\definecolor{Blue}{rgb}{0.0,0.0,0.6}

\definecolor{DarkBlue}{rgb}{0.0859, 0.308, 0.523}
\definecolor{DarkOrange}{rgb}{0.8, 0.4, 0.0}
\definecolor{DarkGreen}{rgb}{0.00,0.40,0.00}
\definecolor{ScarletRed}{rgb}{0.60,0.00,0.00}
\definecolor{AlmostWhite}{rgb}{0.80,0.80,0.80}
\definecolor{Gray}{gray}{0.85}

\def\rowstrut{\rule{0pt}{1.2em}}
\definecolor{row}{rgb}{0.8627, 0.9098, 0.9804}

% \definecolor{Cornsilk}{rgb}{1.0, 0.97, 0.86}
\definecolor{Cornsilk}{rgb}{0.98, 0.94, 0.9}

% Symbols for pass and failure
\newcommand{\PASS}{\text{\color{Green}\ding{52}}\xspace}
\newcommand{\FAIL}{\text{\color{Red}\ding{56}}\xspace}

\newcolumntype{a}{>{\columncolor{TableHeader}}l}

% Usage: \confusion[Title]{TP}{FN}{FP}{TN}
\newcommand\confusion[5][]{
	\FPeval{\tpfn}{clip(#2 + #3)}
	\FPeval{\fptn}{clip(#4 + #5)}
	\FPeval{\tpfp}{clip(#2 + #4)}
	\FPeval{\fntn}{clip(#3 + #5)}
	\FPeval{\tpfpfntn}{clip(#2 + #3 + #4 + #5)}
	\FPeval{\precision}{round(#2 / (#2 + #4), 2)}
	\FPeval{\recall}{round(#2 / (#2 + #3), 2)}
	\FPeval{\accuracy}{round((#2 + #5) * 100 / (#2 + #3 + #4 + #5), 0)}
	\FPeval{\specificity}{round(#5 / (#4 + #5), 2)}
	\FPeval{\fonescore}{round(2 * #2 / (2 * #2 + #4 + #3), 2)}
	\begin{tabular}{ar@{ }rr@{ }rr}
		\toprule
		\rowcolor{TableHeader}
		\multicolumn{1}{l}{\textbf{#1}}        & \multicolumn{4}{c}{\textbf{Classified as}} &  \\
 		\rowcolor{TableHeader}
 		& \multicolumn{2}{c}{\makebox[1.5cm][c]{\textbf{\BUG{}}}} & \multicolumn{2}{c@{}}{\makebox[1.5cm][c]{\textbf{\NOBUG{}}}} & \makebox[1.5cm][c]{\textbf{Total}} \\
 		\BUG{}  & \cellcolor{LightGreen}{TP =\ } & \cellcolor{LightGreen}{#2} & \cellcolor{LightRed}{FN =\ } & \cellcolor{LightRed}{#3} & \tpfn \\
 		\NOBUG{} & \cellcolor{LightRed}{FP =\ } & \cellcolor{LightRed}{#4} & \cellcolor{LightGreen}{TN =\ } & \cellcolor{LightGreen}{#5} & \fptn \\
 		Total   &      & \tpfp &      & \fntn & \tpfpfntn \\\bottomrule
        \multicolumn{6}{@{}l@{}}{\smaller \emph{Accuracy} \accuracy\%, \emph{Precision} \precision,  \emph{Recall} \recall, \emph{Specificity} \specificity, \emph{F1 Score} \fonescore} \\
	\end{tabular}
}

\newcounter{confusions}
\setcounter{confusions}{0}
\def\confusionsubject#1{%
\addtocounter{confusions}{1}
\emph{\theconfusions\) #1} \\}


% Shortcuts for identifiers
% To introduce after the fact, replace \\nonterm\{([^}]*)\} by \\<$1>
\let\backslashpipe=\|
\let\backslashstar=\*  % in case we ever need \*
\let\backslashangle=\<
\def\*#1*{\codeid{#1}}  % Code

% TODOs
\newcommand{\maketodo}[2]{\expandafter\newcommand\csname #1\endcsname[1]{\todo[bordercolor=#2!80!black,color=#2]{\textbf{\MakeUppercase #1:} ##1}\xspace}}

\presetkeys{todonotes}{fancyline}{}
\definecolor{todored}{rgb}{1, 0.6, 0.6}
\definecolor{todoorange}{rgb}{1, 0.8, 0.4}
\definecolor{todoblue}{rgb}{0.4, 0.8, 1.0}
\definecolor{todogreen}{rgb}{0.8, 1.0, 0.4}
\definecolor{todopurple}{RGB}{255, 100, 127}

\maketodo{NOTE}{todogreen}
\maketodo{TODO}{todored}
\maketodo{CHECK}{todoblue}
\newcommand{\DONE}[1]{} % Older arguments, now addressed
\newcommand{\WONTFIX}[1]{} % Other arguments, won't be addressed
\newcommand{\ALSO}[1]{} % Additional arguments

% Result boxes
\newenvironment{result}%
{\medskip
\noindent
\let\emph=\textbf
\begin{boxedminipage}{\linewidth}\begin{center}\em}%
{\end{center}\end{boxedminipage}%
\medskip
}

% Lists and enumerations
\newlist{questions}{enumerate}{1}
\setlist[questions,1]{label=\bfseries RQ\arabic*:,ref=RQ\arabic*,leftmargin=3\parindent}
\crefname{question}{}{}
\Crefname{question}{}{}

\newenvironment{quickenum}
{\begin{enumerate*}[before=\unskip{: }, itemjoin={{; }}, itemjoin*={{; and }}, after={.}]}
{\end{enumerate*}}

\setlength{\intextsep}{0pt}%

\title{How Execution Features Relate to Failures}
\subtitle{An~Empirical~Study and Diagnosis~Approach}

\author{Marius Smytzek}
\orcid{0000-0002-4899-9031}
\affiliation{%
 \institution{CISPA Helmholtz Center for Information Security}
 \city{Saarbr{\"u}cken}
 \country{Germany}
}
\email{marius.smytzek@cispa.de}

\author{Martin Eberlein}
\orcid{0000-0003-4268-7632}
\affiliation{%
 \institution{Humboldt-Universit{\"a}t zu Berlin}
 \city{Berlin}
 \country{Germany}
}
\email{martin.eberlein@hu-berlin.de}

\author{Lars Grunske}
\orcid{0000-0002-8747-3745}
\affiliation{%
 \institution{Humboldt-Universit{\"a}t zu Berlin}
 \city{Berlin}
 \country{Germany}
}
\email{grunske@hu-berlin.de}

\author{Andreas Zeller}
\orcid{0000-0003-4719-8803}
\affiliation{%
 \institution{CISPA Helmholtz Center for Information Security}
 \city{Saarbr{\"u}cken}
 \country{Germany}
}
\email{zeller@cispa.de}

\begin{CCSXML}
    <ccs2012>
        <concept>
            <concept_id>10011007.10011074.10011099.10011102.10011103</concept_id>
            <concept_desc>Software and its engineering~Software testing and debugging</concept_desc>
            <concept_significance>500</concept_significance>
        </concept>
        <concept>
            <concept_id>10011007.10011006.10011073</concept_id>
            <concept_desc>Software and its engineering~Software maintenance tools</concept_desc>
            <concept_significance>300</concept_significance>
        </concept>
        <concept>
            <concept_id>10002944.10011123.10010912</concept_id>
            <concept_desc>General and reference~Empirical studies</concept_desc>
            <concept_significance>500</concept_significance>
        </concept>
        <concept>
            <concept_id>10002944.10011123.10011124</concept_id>
            <concept_desc>General and reference~Metrics</concept_desc>
            <concept_significance>500</concept_significance>
        </concept>
        <concept>
            <concept_id>10002944.10011123.10010916</concept_id>
            <concept_desc>General and reference~Measurement</concept_desc>
            <concept_significance>500</concept_significance>
        </concept>
        <concept>
            <concept_id>10003752.10003777.10003782</concept_id>
            <concept_desc>Theory of computation~Oracles and decision trees</concept_desc>
            <concept_significance>500</concept_significance>
        </concept>
        <concept>
            <concept_id>10003752.10010070.10010071.10010286</concept_id>
            <concept_desc>Theory of computation~Active learning</concept_desc>
            <concept_significance>300</concept_significance>
        </concept>
     </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Software testing and debugging}
\ccsdesc[300]{Software and its engineering~Software maintenance tools}
\ccsdesc[500]{General and reference~Empirical studies}
\ccsdesc[500]{General and reference~Metrics}
\ccsdesc[500]{General and reference~Measurement}
\ccsdesc[500]{Theory of computation~Oracles and decision trees}
\ccsdesc[300]{Theory of computation~Active learning}


\keywords{fault localization, statistical debugging, execution features, empirical study, automated debugging}

\begin{abstract}
    Fault localization is a fundamental aspect of debugging, aiming to identify code regions likely responsible for failures. Traditional techniques primarily correlate statement execution with failures, yet program behavior is influenced by diverse execution features—such as variable values, branch conditions, and definition-use pairs—that can provide richer diagnostic insights.

    In an empirical study of 310~bugs across 20~projects, we analyzed 17 execution features and assessed their correlation with failure outcomes. Our findings suggest that fault localization benefits from a broader range of execution features:
       \begin{enumerate*}[label=(\arabic*)]
           \item \emph{Scalar pairs} exhibit the strongest correlation with failures;
           \item Beyond line executions, \emph{def-use pairs} and \emph{functions executed} are key indicators for fault localization; and
           \item Combining \emph{multiple features} enhances effectiveness compared to relying solely on individual features.
       \end{enumerate*}
   
    Building on these insights, we introduce a debugging approach to diagnose failure circumstances. 
    The approach extracts fine-grained execution features and trains a decision tree to differentiate passing and failing runs.
    From this model, we derive a diagnosis that pinpoints faulty locations and explains the underlying causes of the failure. 
   
    Our evaluation demonstrates that the generated diagnoses achieve high predictive accuracy, reinforcing their reliability. 
    These interpretable diagnoses empower developers to efficiently debug software by providing deeper insights into failure causes.
\end{abstract}

\begin{document}

\maketitle

\section{Introduction}%
\label{sec:introduction}

Debugging is the process of identifying and fixing faults in a program.
Be it automated or manual, a central task in debugging is \emph{fault localization}---that is, identifying the locations in the code that are likely to contain the root cause of a failure.
Fault localization techniques typically rely on the analysis of program executions to identify the locations that are most relevant for the failure.

As a classic example, consider \Cref{fig:middle-example}, used by its authors to demonstrate the \TARANTULA{} localization~\cite{jones2002tarantula}.
The \texttt{middle()} function returns neither the minimum nor the maximum of its arguments \texttt{x}, \texttt{y}, and~\texttt{z}.
However, \texttt{middle()} is faulty, as \texttt{middle(2, 1, 3)} returns \texttt{1} rather than \texttt{2}.

\TARANTULA{} runs a suite of tests on the program, correlating the failures with the execution of code lines.
Given the four test cases from~\cite{jones2002tarantula}, Line~6 is executed only in the one failing test, which strongly correlates with the failure.
(Line~6 also \emph{is} the faulty line.)
However, the effectiveness of line-based fault localization depends heavily on the test cases:
Adding a test \texttt{middle(1, 1, 3)} breaks the strong correlation---Line~6 is executed, but the test returns \texttt{1}, passing.

The advantage of line coverage is that it is universal—pretty much any programming language provides a means to measure it.
The coverage of a line, however, is just one of many features of a program execution.
Besides line coverage, other features could also correlate with failures---such as variable values, definition-use pairs (i.e., pairs of locations in which values are defined and later used), branch conditions, and more.

Moreover, debugging the locations alone may not be sufficient to understand the root cause of a failure from a developer's perspective.
A developer relies on questions that can not be answered by a single line of code, such as:
\begin{enumerate*}[label=(\arabic*)]
    \item Why did the failure occur? 
    \item What led to the failure? 
    \item What contributed to the failure?
\end{enumerate*}

\begin{wrapfigure}{l}{0.5\textwidth}
	\def\*{{\color{Blue}$\blacksquare$}}
	\def\+{{\color{Blue}$\blacksquare$}}
	\def\-{{\color{LightGrey}$\Box$}}
	\def\ind{\qquad}
	\scriptsize
    \centering
	\begin{tabular}{@{}>{\tiny}r>{\tt}l@{\quad}l@{\ \ }r@{\ \ }r@{\ \ }r@{\ \  }r@{\ \ }r@{\ \ }r@{\ \ }>{\tiny}r}
	\\
	& \textsf{\color{black}\*: covered line} 
	     & \color{Blue}\texttt{x}      & 3   & 1   & 3   & 5   & 5   & 2  \\
	   & & \color{DarkBlue}\texttt{y}  & 3   & 2   & 2   & 5   & 3   & 1  \\
	   & &  \color{DarkBlue}\texttt{z} & 5   & 3   & 1   & 1   & 4   & 3  \\
	   \\ \cline{4-9}
	1  & \textbf{\color{DarkOrange}def} {\color{Blue}middle}({\color{DarkBlue}x}, {\color{DarkBlue}y}, {\color{Blue}z}):  & & & & & & & & 1 \\
	2  & \ind  \textbf{\color{DarkOrange}if} {\color{DarkBlue}y} < {\color{DarkBlue}z}:         
	& & \*  & \*  & \*  & \*  & \*  & \*  & 2 \\ %\cline{3-8}
	3  & \ind \ind  \textbf{\color{DarkOrange}if} {\color{DarkBlue}x} < {\color{DarkBlue}y}:       
	& & \*  & \*  & \-  & \-  & \*  & \*  & 3 \\ %\cline{3-8}
	4  & \ind \ind \ind  \textbf{\color{DarkOrange}return} {\color{DarkBlue}y}      
	& & \-  & \*  & \-  & \-  & \-  & \-  & 4 \\ %\cline{3-8}
	5  & \ind \ind  \textbf{\color{DarkOrange}elif} {\color{DarkBlue}x} < {\color{DarkBlue}z}:  
	& & \*  & \-  & \-  & \-  & \*  & \*  & 5 \\ %\cline{3-8}
	6  & \ind \ind \ind  \textbf{\color{DarkOrange}return} {\color{DarkBlue}y}      
	& & \*  & \-  & \-  & \-  & \-  & \*  & 6 \\ %\cline{3-8}
	7  & \ind  \textbf{\color{DarkOrange}else}:            
	& & \-  & \-  & \*  & \*  & \-  & \-  & 7 \\ %\cline{3-8}
	8 & \ind \ind  \textbf{\color{DarkOrange}if} {\color{DarkBlue}x} > {\color{DarkBlue}y}:       
	& & \-  & \-  & \*  & \*  & \-  & \-  & 8 \\ %\cline{3-8}
	9 & \ind \ind \ind  \textbf{\color{DarkOrange}return} {\color{DarkBlue}y}      
	& & \-  & \-  & \*  & \-  & \-  & \-  & 9 \\ %\cline{3-8}
	10 & \ind \ind  \textbf{\color{DarkOrange}elif} {\color{DarkBlue}x} > {\color{DarkBlue}z}:  
	& & \-  & \-  & \-  & \*  & \-  & \-  & 10 \\ %\cline{3-8}
	11 & \ind \ind \ind  \textbf{\color{DarkOrange}return} {\color{DarkBlue}x}      
	& & \-  & \-  & \-  & \*  & \-  & \-  & 11 \\ %\cline{3-8}
	12 & \ind  \textbf{\color{DarkOrange}return} {\color{DarkBlue}z}             
	& & \-  & \-  & \-  & \-  & \*  & \-  & 12 \\ %\cline{3-8}
	   &                           
	& & \PASS{} & \PASS{} & \PASS{} & \PASS{} & \PASS{} & \FAIL{}\\
	\end{tabular}
	\caption{Statistical fault localization~\cite{jones2002visualization}. The \texttt{middle()} function takes three values and returns the one that is neither the single smallest nor the single largest one; execution of Line~6 correlates most with the failure.}
	\label{fig:middle-example}
\end{wrapfigure}

Recent toolkits allow retrieving all of these features from a program's execution, thus providing a much more ideal choice of execution features to use for fault localization.
The \SFLKIT{}~\cite{smytzek2022sflkit} toolkit for Python programs, among many other execution features, allows extracting \emph{scalar pairs} that occurred during execution---that is, relationships between two variables.
Aided by \SFLKIT{} on the \texttt{middle()} example, we could extract a feature that the test fails if $\texttt{y} < \texttt{x}$ hold.
However, this feature is only one of many that could be extracted from the execution.

Do various execution features work better than lines to characterize failures or locate faults? Moreover, which execution features would these be?
To answer these questions, we conduct an empirical study to determine \emph{which execution features are most relevant to failures.}
During program execution, we collect 17~different execution features inspired by test coverage criteria---from branches covered to definition-use pairs and many more---and assess how effective these features are in detecting and localizing faults.
Our study encompasses 310~faults from 20~open-source projects from the \TESTS4PY{} dataset~\cite{smytzek2024tests4py}, which is based on the faults in \BUGSINPY{}~\cite{widyasari2020bugsinpy}.

To the best of our knowledge, this is the first study to analyze and compare the interplay of a large set of execution features with software failures.
Our findings suggest that fault localization \emph{should rely on a larger and more diverse range of execution features:}
\begin{enumerate}[topsep=5pt]
    \item Features that capture data \emph{and} control flow, such as scalar pairs, showed the highest correlation with failures;
    \item Besides lines executed, \emph{def-use pairs} and \emph{functions executed} are the best features for localizing faults; and
    \item Considering \emph{several features} yields better fault localization than executed lines alone.
\end{enumerate}
Insights such as these can
\begin{enumerate*}[label=(\arabic*)]
 \item help in identifying root causes of program failures more accurately,
 \item provide better hints for automated code repairs, and
 \item open up new possibilities for debugging techniques and research.
\end{enumerate*}

Building on these insights, we introduce a novel debugging approach that learns relevant execution features and generates diagnoses to explain the underlying causes of failures.
We leverage the investigated execution features to train a \emph{classifier} for predicting failures based on the collected features.
In the case of \texttt{middle()} and the executions in \Cref{fig:middle-example}, for instance, the model determined by our execution-feature-driven debugging (or short \EFDD{}) approach predicts a failure if two properties hold:
\begin{enumerate*}
    \item The \texttt{return y} in Line~6 is executed, and
    \item $y$ is less than $x$.
\end{enumerate*}
These two conditions form an accurate \emph{diagnosis}---the failure occurs precisely under these conditions that a developer can understand and act upon, i.e., the fix needs to be in Line~6, which returns $y$, but $y$ is less than $x$, and from the previous Line~5 we know that $x$ is less than $z$, hence $x$ should be returned.

Note that our debugging approach does not need a specification of correct behavior (which, even for a seemingly simple function like \texttt{middle()}, is not straightforward).
Nor does it assume a \emph{generic} failure such as a crash (for which the execution feature that most correlates with failure is obvious.\footnote{It is the crash.}).
Moreover, our approach is not limited to specific kinds of tests, such as unit tests, but works with any test suite that provides labeled outcomes, such as inputs for which we know correct and incorrect behavior.
To the best of our knowledge, this is the first approach that learns relevant features based on the execution and generates fault diagnoses that pinpoint faulty locations and explain the underlying causes of failures.

Overall, we make the following contributions:
\begin{description}[leftmargin=5pt,noitemsep,partopsep=0pt,topsep=0pt,parsep=0pt]
 \item[An \emph{empirical study} assessing how execution features relate to failures.] We conduct an \emph{empirical study} to investigate the effectiveness of the various execution features in detecting and localizing faults, which guides future research in this area and can inspire novel debugging ideas.
 \item[An investigation on \emph{combinations of features}.] We investigate how collecting and leveraging \emph{several features} from program executions can improve fault localization.
 This way, we can identify the \emph{features that are most relevant to failures} instead of relying on one feature (say, line coverage) only.
 \item[A comprehensive \emph{dataset}.] We provide a dataset of faults and executions, i.e., all events of a corresponding execution, to enable further research.
 \item[A debugging approach] We present an execution-feature-driven-debugging approach to produce failure diagnoses that relate failures to \emph{fine-grained} execution features such as individual lines and variable values, which can \emph{precisely pinpoint failure conditions}.
 The diagnoses produced by our debugging approach come as \emph{explicit conditions} that \emph{explain} the failure and can be assessed by humans.
\end{description}

The remainder of this paper is organized as follows.
\Cref{sec:execution-features} describes the execution features analyzed in our study.
Additionally, this section introduces the concept of using multiple execution features for fault localization.
\Cref{sec:study} details our study's design for evaluating these features' effectiveness in fault detection and localization.
This section concludes with a presentation of our study's results and implications.
After introducing execution features and their value for debugging, we present a debugging approach that leverages these features to generate fault diagnoses in \Cref{sec:debugging}.
\Cref{sec:evaluation} evaluates the effectiveness of our debugging approach in generating fault diagnoses.
We address potential threats to validity in \Cref{sec:threats-to-validity} and review related work in \Cref{sec:related-work}.
Finally, \Cref{sec:conclusion} concludes the paper and suggests avenues for future research.
All data and tools developed for this study are available as open source (\Cref{sec:tool}).


\section{Execution Features}%
\label{sec:execution-features}

Our study analyzes program executions to identify features most pertinent to failures, aiming to capture behavioral nuances of the program during execution.
We collect various data points—such as executed lines, variable states, method calls, and condition outcomes—recorded as \emph{events} across different execution instances.
These events encapsulate control and data flow, providing insight into the program's operational context.

From these events, we derive \emph{features}, representing whether specific behaviors occurred during execution or if conditions relevant to failure were met.
By statistically analyzing these features, we identify those most strongly associated with failure, extending beyond the typical scope of statement coverage used in traditional fault localization.

\subsection{Statistical Fault Localization}%
\label{sub:statistical-fault-localization}

Statistical fault localization techniques aim to pinpoint code locations most associated with failures by correlating program execution data with the occurrence of failures.
Many of these techniques rely on \emph{spectra}-based information, typically as the line or block coverage, to identify suspect areas.
For instance, the \TARANTULA{} tool~\cite{jones2002tarantula} calculates the \emph{suspiciousness} of each line based on its coverage across failing and passing test cases, with lines more frequently executed by failing tests flagged as potential fault sites.
Beyond line coverage, some techniques expand to other execution details. Liblit et al.~\cite{liblit2005sd} focus on \emph{branches}, identifying branches that appear disproportionately in failing test cases.
Similarly, Santelices et al.~\cite{santelices2009defuse} introduce \emph{definition-use pairs} as a data flow feature for fault localization, highlighting variable definitions and uses that correlate with failure.
Although these methods improve diagnostic granularity, they typically lack direct mappings to specific fault locations in the code, limiting their practical diagnostic utility.
Traditional methods also rely on specific metrics, such as \TARANTULA{}~\cite{jones2002tarantula}, \OCHIAI{}~\cite{abreu2006ochiai}, and \JACCARD{}~\cite{chen2002jaccard}, which calculate line suspiciousness by comparing the presence of features across failing and passing executions.
While these metrics have shown utility, they primarily operate on executed lines or specific statements, potentially overlooking richer data sources within the program execution.

In our study, we expand statistical fault localization by applying suspiciousness metrics to a diverse set of \emph{execution features} beyond line and branch coverage alone.
By integrating features like data flow events, variable values, and condition outcomes, we aim to capture a more comprehensive view of program behavior that could more accurately reflect fault locations.
These broader investigation targets of our study enable us to assess whether a feature-centric perspective, rather than a line-centric one, can yield improved fault localization outcomes.

\subsection{Events}%
\label{sub:events}

During execution, we capture events that reflect the program's behavior.
These events are categorized using standard test coverage metrics and further enriched to support broader fault analysis.
This collection process involves instrumenting a faulty program, allowing us to obtain a detailed execution trace during any program run.
The primary types of captured events are outlined below:

\begin{description}[leftmargin=5pt,noitemsep,partopsep=0pt,topsep=0pt,parsep=0pt]
    \item[Lines.]
 We monitor the execution of each program line, recording every executed line as an event in the trace.
 This fine-grained detail helps pinpoint fault locations and understand execution flow.
    
    \item[Branches.]
 Whenever a conditional branch is executed, it is logged in the trace.
 This event provides insights into the program's decision-making process and helps detect logical errors.
    
    \item[Functions.]
 All function calls, including their names and passed parameters, are logged.
 These calls offer a detailed view of the program's modular structure and function interactions.
    
    \item[Variable Definitions.]
 An event is generated whenever a variable is defined, capturing its name and value. 
This event helps to understand the program's state evolution and identify issues related to variable initialization and scope.
    
    \item[Variable Uses.]
 Events are recorded when variables are referenced or manipulated, allowing for better analysis of variable utilization and detection of incorrect variable usage.
    
    \item[Return Values.]
 Function return values are logged to enhance the trace, enabling a deeper understanding of function outcomes and their influence on execution.
 This tracking is crucial for diagnosing faults related to incorrect returns.
    
    \item[Loops.]
 We track the frequency of loop executions, capturing the number of iterations and controlling conditions.
 This frequency helps identify infinite loops and incorrect loop conditions.
    
    \item[Conditions.]
 The evaluation results of branch conditions, including nested and compound conditions, are recorded.
 This event aids in analyzing complex decision-making processes and detecting logical errors.
\end{description}

This comprehensive instrumentation ensures that each program execution—whether from a unit test or an arbitrary input—generates a detailed trace of these defined events.
After instrumentation, we iteratively execute the provided test cases to assemble an event trace for each.
Given that test cases include labels indicating their ``pass'' or ``fail'' status, we can differentiate between passing and failing traces.

By capturing a wide range of events, we gain a holistic view of the program's execution, which enhances fault localization and diagnosis.
This detailed event trace forms the foundation of our subsequent analysis, particularly in our debugging approach, where we leverage machine learning to identify patterns and correlations indicative of faults.
Understanding the exact sequence of events leading to a fault enables developers to pinpoint root causes and implement appropriate fixes efficiently.

\subsection{Features}%
\label{sub:features}

We derive \emph{features} from the event traces that capture meaningful execution behaviors.
These features indicate the program's behavior under both normal and failing conditions.
Inspired by statistical fault localization research~\cite{jones2002tarantula,jones2005tarantula,liblit2005sd,santelices2009defuse}, we construct a total of 17~features, which can be categorized into four key groups: coverage information, value properties, condition outcomes, and triggered exceptions.

\begin{description}[leftmargin=5pt,noitemsep,partopsep=0pt,topsep=0pt,parsep=0pt]
 \item[Coverage.]
 We focus on line execution, as suggested in spectrum-based fault localization~\cite{jones2002tarantula}, and also consider specific branches taken during execution~\cite{liblit2005sd}.
 Additionally, we capture definition-use pairs as proposed by Santelices et al.~\cite{santelices2009defuse}.
 Loop coverage is assessed by tracking whether a loop was skipped, entered once, or executed multiple times.

 \item[Values.]
 We extract features based on logged \emph{values} in events, such as variable assignments and function return values.
 Examples include detecting whether a number is zero, a variable is \emph{NULL}, or a return value contains non-ASCII characters.
 We also compare values to other variables to create \emph{scalar pairs}, an approach influenced by Liblit et al.~\cite{liblit2005sd}.
 Furthermore, for variables with lengths, such as strings or lists, we classify their length as zero, one, or more than one.

 \item[Conditions.]
 Since \emph{branch conditions} are already recorded in our events, they are directly incorporated as features.

 \item[Exceptions.]
 \emph{Exceptions} play a critical role in program control flow and can hint at fault occurrences.
 Hence, we include a feature that records whether a function exited normally or with an exception.
\end{description}

A feature is considered \emph{satisfied} for a specific execution if the related event was triggered and the feature's defined condition was met during execution.
For instance, a feature monitoring the execution of a particular line is satisfied if that line was executed within the program run.
Similarly, a feature based on a variable's value is satisfied if the variable was defined during execution and met a specified condition, such as an integer value less than zero.
Once derived from events, these features are analyzed using statistical fault localization techniques to assess their relevance to failures.
We identify features most strongly associated with faults by correlating feature satisfaction across individual test cases with failure occurrences.
This correlation helps pinpoint execution characteristics that indicate likely fault locations.

Our feature selection is inspired by the extensive set of events, spectra, and predicates available in \SFLKIT{}~\cite{smytzek2022sflkit}, providing a robust foundation for constructing and evaluating feature conditions for fault localization.

As an example, consider the \texttt{middle()} function from \Cref{fig:middle-example}.
\Cref{fig:features-example} illustrates the features derived from executing this function.
To maintain conciseness, we collect executed lines, branches, definition-use pairs, scalar pairs, and conditions instead of all possible features.
For instance, a scalar pair feature is collected when a variable is defined and constructed by comparing the variable to another variable, e.g., \texttt{y < x}, for all compatible types.
This feature holds if the comparison evaluates to \texttt{True} and does not hold if it evaluates to \texttt{False}.
If a feature is not recorded for an execution, we assign a value indicating its absence.
Similarly, a definition-use pair feature is collected when a variable is used, incorporating its definition and usage, e.g., \texttt{y} defined in line~1 and used in line~3.
More straightforward features, such as line execution, branch outcomes, or condition evaluations, are collected whenever the corresponding event occurs, e.g., execution of line~2 or evaluation of \texttt{y < z}.
Again, absent features are assigned values indicating their non-occurrence.

Later in our debugging approach, we transform these features into binary and tertiary representations.
A binary feature either holds ($1$) or does not hold ($0$) during a run, while a tertiary feature can hold ($1$), not hold ($0$), or be unobserved ($-1$).
For example, line execution is a binary feature, as it can only be executed or not executed. In contrast, condition evaluation is tertiary, as it can evaluate to \texttt{true}, \texttt{false}, or remain unobserved.

Since not all runs include every feature, we assign default values during feature vector construction to normalize the dataset.
The default value for binary features is \texttt{false}, while for features with three states, it is \texttt{not observed}.
This standardization ensures that feature vectors maintain consistent size and structure across all runs.

\begin{figure}
    \def\ind{\qquad}
    \centering
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{@{}>{\scriptsize}r>{\tt\color{Blue}}l@{\quad}l@{\ \ }>{\raggedright}p{.3\columnwidth}@{\ \ }>{\raggedright}p{.3\columnwidth}@{\ \ }>{\raggedright}p{.3\columnwidth}@{\ \ }>{\scriptsize}r}
      & &\color{Blue}\texttt{x}& \multicolumn{1}{c}{1} &  \multicolumn{1}{c}{3} &  \multicolumn{1}{c}{2} & \\
      & &\color{Blue}\texttt{y}& \multicolumn{1}{c}{2} &  \multicolumn{1}{c}{1} &  \multicolumn{1}{c}{1} & \\
      & &\color{Blue}\texttt{z}& \multicolumn{1}{c}{3} &  \multicolumn{1}{c}{2} &  \multicolumn{1}{c}{3} & \\
    1 & \textbf{\color{deeporange}def} middle(x, y, z):     & 
    & \multicolumn{3}{@{}>{\raggedright}p{\columnwidth}}{\scriptsize ScalarPair(y<x), ScalarPair(y>x), ScalarPair(y==x), ScalarPair(y<=x), ScalarPair(y>=x), ScalarPair(y!=x), 
    ScalarPair(z<x), ScalarPair(z>x), ScalarPair(z==x), ScalarPair(z<=x), ScalarPair(z>=x), ScalarPair(z!=x),
    ScalarPair(z<y), ScalarPair(z>y), ScalarPair(z==y), ScalarPair(z<=y), ScalarPair(z>=y), ScalarPair(z!=y)} 
    & 1 \\ %\cline{4-8}
    2 & \ind  \textbf{\color{deeporange}if} y < z:          & 
    & \scriptsize Line(2), DefUse(x,1,2), DefUse(z,1,2), Condition(y<z)
    & \scriptsize Line(2), DefUse(x,1,2), DefUse(z,1,2), Condition(y<z)
    & \scriptsize Line(2), DefUse(x,1,2), DefUse(z,1,2), Condition(y<z)
    & 2 \\ %\cline{3-8}
    3 & \ind \ind  \textbf{\color{deeporange}if} x < y:     & 
    & \scriptsize Line(3), Branch(1), DefUse(x,1,3), DefUse(y,1,3), Condition(x<y)
    & \scriptsize Line(3), Branch(1), DefUse(x,1,3), DefUse(y,1,3), Condition(x<y)
    & \scriptsize Line(3), Branch(1), DefUse(x,1,3), DefUse(y,1,3), Condition(x<y)
    & 3 \\ %\cline{3-8}
    4 & \ind \ind \ind  \textbf{\color{deeporange}return} y & 
    & \scriptsize Line(4), Branch(3), DefUse(y,1,4) 
    & \scriptsize 
    & \scriptsize 
    & 4 \\ %\cline{3-8}
    5 & \ind \ind  \textbf{\color{deeporange}elif} x < z:   & 
    & \scriptsize 
    & \scriptsize Line(5), Branch(4), DefUse(x,1,5), DefUse(y,1,5), Condition(x<z)
    & \scriptsize Line(5), Branch(4), DefUse(x,1,5), DefUse(y,1,5), Condition(x<z)
    & 5 \\ %\cline{3-8}
    6 & \ind \ind \ind  \textbf{\color{deeporange}return} y & 
    & \scriptsize 
    & \scriptsize 
    & \scriptsize Line(6), Branch(5), DefUse(y,1,6) 
    & 6 \\ %\cline{3-8}
    7 & \ind  \textbf{\color{deeporange}elif} x > y:        & & \scriptsize & \scriptsize & \scriptsize & 7 \\ %\cline{3-8}
    8 & \ind \ind  \textbf{\color{deeporange}return} y      & & \scriptsize & \scriptsize & \scriptsize & 8 \\ %\cline{3-8}
    9 & \ind  \textbf{\color{deeporange}elif} x > z:        & & \scriptsize & \scriptsize & \scriptsize & 9 \\ %\cline{3-8}
    10 & \ind \ind  \textbf{\color{deeporange}return} x     & & \scriptsize & \scriptsize & \scriptsize & 10 \\ %\cline{3-8}
    11 & \ind  \textbf{\color{deeporange}return} z          & 
    & \scriptsize 
    & \scriptsize Line(11), Branch(6), DefUse(z,1,11) 
    & \scriptsize 
    & 11 \\ %\cline{3-8}
    & & & \multicolumn{1}{c}{\PASS} & \multicolumn{1}{c}{\PASS} & \multicolumn{1}{c}{\FAIL} & \\
    \end{tabular}%
    }%
    \caption{Collecting of Features. 
    For the \texttt{middle()} function, we derive features from the execution. For example, we show the collection of executed lines, branches, definition-use pairs, scalar pairs, and conditions.
    We show when a feature is collected by assigning it to the corresponding line in the code.
    The values in parentheses indicate the parameters of a feature; for lines, the line number; for branches, the branch ID; for definition-use pairs, the variable and the lines for the definition and use; for scalar pairs, the two variables, and the applied comparison; for conditions, the actual condition.
    }%
    \label{fig:features-example}
\end{figure}

\subsection{Soundness and Completeness of Features}%
\label{sub:soundness-completeness}

\subsubsection*{Soundness}
Our feature selection is designed to be sufficiently detailed to represent executions accurately.
By ensuring that each set of features is unique to its corresponding execution or closely similar, we avoid incorrect outcomes, thereby affirming the soundness of our features in distinguishing between passing and failing runs.
Soundness is crucial because it guarantees that the features we select are reliable indicators of the execution's behavior.
This reliability is essential for debugging and optimizing the system. 
It ensures that the features accurately represent the underlying processes and reduces false positives and negatives, leading to more accurate insights.

\vspace{-.5\baselineskip}
\subsubsection*{Completeness.}
While we strive for completeness by covering a broad range of general scenarios, there are instances where our features may fall short.
For example, consider a fault arising from a function that should have been executed.
Our current features might not detect such faults because they focus on observable effects rather than potential omissions.
Although we may pick up indirect signs of this fault, these indicators are not always definitive but may be sufficient to identify the fault.
However, we cannot guarantee the latter scenario.
Enhancing our feature set to capture such scenarios is possible, but some unique situations will inevitably still challenge our feature coverage's completeness.
Completeness is vital because it ensures that all relevant aspects of the execution are considered, providing a holistic view of the system's behavior.
This comprehensive coverage is necessary to identify and address all potential issues, improving the system's robustness and reliability.
To achieve greater completeness, we can incorporate additional features that capture a more comprehensive range of execution behaviors, including those that are less common or more subtle.
However, it is essential to balance the need for completeness with the practical constraints of feature selection, such as computational overhead and data availability.

\subsection{Why Using a Single Feature?}%
\label{sub:multi-feature-technique}

While each feature could be leveraged to localize faults, combining these features that highlight their benefits while mitigating their drawbacks could provide a more comprehensive fault localization.
Hence, we propose to leverage \emph{multi-feature techniques}, similar to Santelices et al.~\cite{santelices2009defuse}, that leverages these derived features and combines the strengths of various features considering the different information sources and their relation to failures to localize faults more accurately and precisely.
These techniques aim to verify whether combining multiple features can provide a more comprehensive fault localization than relying on a single feature alone.

We have implemented such an approach that evaluates all features simultaneously, calculating the correlation of each based on a provided coefficient, e.g., \TARANTULA{}, which considers the number of passed and failed test cases that satisfy the feature.
We then order the features based on their suspiciousness scores, identifying the most relevant features for the failure to occur.
Each feature corresponds to a specific code location, such as a single line or a block of lines, allowing us to assign a suspiciousness score to each line.
A challenge arises when multiple features correspond to the same line, necessitating a method for combining their suspiciousness scores.
We provide an interface in our technique to utilize various metrics to integrate the suspiciousness of multiple features mapping to the same line.
Intuitively, we can select among metrics such as maximum, minimum, median, or average suspiciousness to determine the final score for each line in the program, where we consider mainly the maximum suspiciousness as the metric of choice.
After calculating the suspiciousness scores for each line, we can rank them based on their scores, identifying the lines most likely to contain the root cause of the failure as traditional fault localization techniques do.


%%%%
%%%%
%%%%

\section{Empirical Study}%
\label{sec:study}

\subsection{Research Questions}%
\label{sub:research-questions}

In our empirical study, we aim to investigate the effectiveness of the presented execution features in detecting and localizing faults.
Hence, we formulate the following research questions to guide our study:

\begin{questions}[topsep=2pt]
 \item\label[question]{rq1} \textbf{Correlation.} To what degree does each execution feature correlate with the presence of failures?
 \item\label[question]{rq2} \textbf{Localization.} How well does each execution feature localize the faults in the program?
 \item\label[question]{rq3} \textbf{Multi-Features.} Can we combine execution features to provide a more comprehensive fault localization?
\end{questions}

In the traditional evaluation of fault localization, the faulty lines are leveraged to assess the effectiveness of a technique, which comes from a fixed version of the program that may not be the only possible fix for the fault, which may lead to a bias in the evaluation.
To address this issue, we introduced \Cref{rq1} that shifts the focus to the correlation of features with faults as an initial assessment of the features' effectiveness in identifying the faults' presence.

\subsection{Subjects}%
\label{sub:subjects}

Our study utilizes open-source projects from the \TESTS4PY{} dataset~\cite{smytzek2024tests4py}, a curated collection of Python projects with documented faults and corresponding test cases.
Each project in this dataset includes a faulty and a fixed version, where the faults have been addressed.
To ensure fault validity, we first executed the provided test cases on the faulty versions, confirming that the expected failures occurred and that each fault had at least one passing test case.
This passing test case is essential for calculating meaningful correlations in fault localization.
We then verified the fixed versions by rerunning the test cases, confirming that all tests passed and the faults were correctly resolved.
Through this process, we successfully reproduced failures and validated test case accuracy for 310~subjects across 20~projects in the dataset.
These 310~subjects were selected for our empirical analysis as they provide a reliable and reproducible basis for evaluating fault localization techniques.

\subsection{Collecting the Data}%
\label{sub:data-collection}

For each project, we collected the trace of events during the execution of the test cases for the faulty version of the program.
\TESTS4PY{} provides a set of test cases for each project relevant to the failure.
This set includes test cases that trigger the failure and those that explore related functionality without triggering the bug, such as executing the same function with different parameters.
We ensured that all test case executions were correctly categorized, confirming that the test cases expected to fail did indeed fail.

\subsection{Analysis}%
\label{sub:analysis}

To address \Cref{rq1}, we conducted a statistical analysis of the collected execution features.
We computed the correlation of each feature with the presence of failures using \TARANTULA{}~\cite{jones2002tarantula}, \OCHIAI{}~\cite{abreu2006ochiai}, \DSTAR{}~\cite{wong2012dstar}, \NAISHT{}~\cite{naish2011sbfl}, and \GPOT{}~\cite{xie2013gp} coefficients across all feature types (e.g., lines, branches).
For each feature type, we calculated four metrics:
\begin{enumerate*}[label=(\arabic*)]
    \item the highest correlation among features,
    \item the mean correlation of all features,
    \item the median correlation of all features, and
    \item the lowest correlation among features.
\end{enumerate*}
These metrics provide a comprehensive view of the suspiciousness of how likely each feature type correlates with the presence of failures.
For instance, we have line coverage features for two subjects.
For the first subject, Lines 1, 2, 3, and 4 have correlations of 0.8, 0.6, 0.4, and 0.2, respectively.
For the second subject, Lines 1, 2, and 3 have correlations of 0.7, 0.5, and 0.3.
To determine the average correlation metrics for the line feature class:
the highest average correlation is $(0.8 + 0.7) / 2 = 0.75$;
the mean correlation across all features is $((0.8 + 0.6 + 0.4 + 0.2) / 4 + (0.7 + 0.5 + 0.3) / 3) / 2 = 0.5$;
the median correlation is $(0.5 + 0.5) / 2 = 0.5$; and the lowest average correlation is $(0.2 + 0.3) / 2 = 0.25$.

Additionally, we calculate the actual correlation between the features and the presence of failures by relating the number of failing runs for which a feature is satisfied to the total number of runs for which the feature is satisfied. This relation will show how well the feature correlates with the presence of failures, i.e., if a feature is primarily satisfied if the run is failing, which we will evaluate using Spearman's rank correlation coefficient.
Similar to the similarity coefficients, we calculate the overall correlation (i.e., considering all subjects at once) for each feature type along with the highest, mean, median, and lowest correlation when considering each subject individually.

We evaluated fault localization for \Cref{rq2} by applying the same coefficients to each feature and ranking suggested lines based on faulty lines extracted from patch files in the \TESTS4PY{} dataset.
While these patch files may not exclusively contain lines that directly address the fault (since fixes may involve code outside the directly faulty lines), we chose this approach to remain consistent with established methods in prior work~\cite{jones2002tarantula,jones2005tarantula,abreu2006ochiai,widyasari2022sfl,pearson2017sfl,wong2007wong,landsberg2015sfl}. 
Moreover, the analysis of \Cref{rq1} mitigates the potential bias introduced using the patch files for fault localization.

Following Widyasari et al.~\cite{widyasari2022sfl} and Pearson et al.~\cite{pearson2017sfl}, we assessed localization performance using the top-1, top-5, top-10, and top-200 lines suggested by each coefficient~\cite{le2015topk}.
We calculated the \EXAM{} score~\cite{wong2008crosstab} and \emph{wasted effort}~\cite{xuan2014test} for each feature type, guided by evaluation methods used in previous studies~\cite{just2018sfl,long2016search}.
The \EXAM{} score for a statement $s$ is defined as:
\begin{equation}
 \text{\EXAM{}}_s = \frac{\text{rank}(s)}{\text{Total number of statements}}
\end{equation}
\begin{wraptable}{l}{.61\textwidth}
    \caption{Suspiciousness and Correlation. 
    The calculated suspiciousness for each metric \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{} and the correlation. Highlighted values represent the best results when compared to the other features. Underlined values for the correlation represent statistical significance ($p < 0.05$).}%
    \label{tab:results-correlation-1}
    \centering
    \setlength\extrarowheight{-3pt}
    \resizebox{.6\textwidth}{!}{%
    \input{data/correlation-1.tex}
    }
\end{wraptable}
where $\text{rank}(s)$ is the rank of the statement $s$ in the list of suggested lines ordered by the suspiciousness of each line.
If multiple lines have the same suspiciousness, we consider the average rank of these lines, calculated as ${\frac{n}{2} + (k - 1)}$~(\cite{widyasari2022sfl,pearson2017sfl,steimann2013threats}), where $n$ is the number of lines with the same suspiciousness as $s$ and $k$ is the position of the first line with the same suspiciousness, i.e., the highest possible rank.
The wasted effort for a line $s$ is defined as the number of lines that need to be inspected to find the target line, starting from the top of the list of suggested lines and including the target line itself and the lines with the same suspiciousness.
We calculated the average over all subjects for each feature type and metric.
Similar to Widyasari et al.~\cite{widyasari2022sfl} and Pearson et al.~\cite{pearson2017sfl} we included three debugging scenarios,
\begin{enumerate*}[label=(\arabic*)]
    \item finding one,
    \item half, and
    \item all faulty lines
\end{enumerate*}
named
\begin{enumerate*}[label=(\arabic*)]
    \item \textit{best-case},
    \item \textit{average-case}, and
    \item \textit{worst-case}
\end{enumerate*}
debugging respectively.

We considered the same similarity coefficients for the localization to answer \Cref{rq3}, which. Still, we calculated the suspiciousness for each line based on the features that map to the line.
We considered two cases for the multi-feature technique: one in which we consider the maximum suspiciousness of a line when considering each feature and one in which we consider the mean suspiciousness of the features that map to the line.

\subsection{Implementation}%
\label{sub:implementation}

We implemented the empirical study using the fault localization tool \SFLKIT{}~\cite{smytzek2022sflkit}, a versatile framework supporting a range of statistical fault localization techniques, including \TARANTULA{}~\cite{jones2002tarantula}, \OCHIAI{}~\cite{abreu2006ochiai}, \DSTAR{}~\cite{wong2012dstar}, \NAISHT{}~\cite{naish2011sbfl}, and \GPOT{}~\cite{xie2013gp}.
\SFLKIT{} provides an interface that allows us to integrate custom execution features into its pipeline, instrumenting the program to capture events during execution and offering the necessary infrastructure to analyze these events and derive features.
We have constructed a workflow that instruments the program, extracts the events, assembles the features, and analyzes their effectiveness for all three research questions while providing intermediate results after each step.
With its intermediate results, this process is ideal for reproducing all our findings and verifying that all steps work as intended.

\begin{wraptable}{r}{.61\textwidth}
    \caption{Suspiciousness and Correlation.
    Continuation of \Cref{tab:results-correlation-1}}%
    \label{tab:results-correlation-2}
    \centering
    \setlength\extrarowheight{-3pt}
    \resizebox{.6\textwidth}{!}{%
    \input{data/correlation-2.tex}
    }
\end{wraptable}

\subsection{Results}%
\label{sub:study-results}

All summarized results of our empirical study are presented in \Cref{tab:results-correlation-1,tab:results-correlation-2,tab:results-localization-1,tab:results-localization-2,tab:results-localization-multiple}.
\Cref{tab:results-correlation-1,tab:results-correlation-2} show the correlation of the execution features with the presence of failures for the metrics \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, \GPOT{}, and the corresponding Spearman's $\rho$ values for a feature.
Moreover, \Cref{tab:results-localization-1,tab:results-localization-2} show the localization of the faults for the metrics \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{} according to top-1, top-5, top-10, top-200, \EXAM{} score, and wasted effort for all features.
Finally, \Cref{tab:results-localization-multiple} shows the results when considering a näive multi-feature technique from \Cref{sub:multi-feature-technique}.


\subsubsection{\bfseries\Cref{rq1}: Correlation with Failures}%
\label{sub:eval-correlation}

To address \Cref{rq1}, we conducted a detailed analysis of the correlation between execution features and the presence of failures, using the \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{} coefficients along with Spearman's $\rho$ values.
Generally, a high suspiciousness score indicates a likely causality between a feature and the presence of failures, while high correlation coefficients suggest a strong relationship.
In other words, a feature is satisfied when a failure occurs, and the failure occurs when the feature is present.
If both values are high, the feature is likely to indicate the presence of failures.
In contrast, if the suspiciousness score is low, the feature is not commonly satisfied when a failure occurs. 
If the correlation coefficient is low, the feature is often satisfied when a failure does not occur.

Our findings reveal that features capturing data flow and data relationships, such as def-use pairs, scalar pairs, variable values, and null values, exhibit exceptionally high correlations with failures.
Additionally, specific features that verify the contents of strings show an increased correlation based on the overall correlation coefficient, while they do not show a general high suspiciousness score.
This discrepancy suggests that these features indicate failures when they occur but are not standard for most failures.

Features representing control flow, including branches and conditions, also show strong correlations, though they appear slightly less indicative than data flow features.
Interestingly, loop-related features correlate with failures only when they capture variations in loop frequency, such as the number of iterations. 

These findings suggest that while control flow is essential, the data flow and relationships within the program data may be more critical in understanding failure correlations.

\begin{result}
 Features that capture data and control flow, significantly \emph{scalar pairs}, are highly correlated with failures.
\end{result}

Additionally, \emph{Lines} show a high correlation with the presence of failures, particularly when considering the best feature of each class, but also on the mean and median of all features, compared with the other feature classes.
While \emph{functions} have slightly lower correlations than lines, they still show significant correlations for the mean and median of all features.
However, these features cover almost the entire program, with exceptionally executed functions corresponding to an entire code block. 
Because of this information overload, they might not be specific enough to pinpoint the causes of a fault.
Note that the fault's cause may differ from the faulty location.
This finding is supported by Spearman's $\rho$ values, which show that lines and functions are less correlated with failures.

\begin{result}
    \emph{Lines} and \emph{functions} show a high suspicion for the occurrence of failures but are not specific enough to accurately detect a fault's causes based on Spearman's $\rho$ values.
\end{result}

While \emph{return values}, \emph{variables values}, \emph{null values}, and \emph{lengths} provide a high suspiciousness, other value-related features---for instance, if a string is empty or consists of digits.
However, \emph{return values} do not show a high correlation with the presence of failures, suggesting that they might not be as relevant to failure occurrences as the variables themselves and their properties, e.g., properties corresponding to well-known failure patterns, for instance, null pointers or index-out-of-bounds.
Specifically to highlight are \emph{special strings}, i.e., strings that contain characters other than digits or letters, which show a higher suspiciousness and an extremely high correlation with the presence of failures.

\begin{result}
    \emph{Variables values}, \emph{null values}, \emph{lengths}, and if a \emph{string contains special characters} highly correlate with the occurrence of failures.
\end{result}

Counterintuitively, features that capture if a function terminates with exceptions do \emph{not} highly correlate with failure occurrence, suggesting they might not be as relevant.

\begin{figure}
    \centering
    \begin{subfigure}[h]{.32\textwidth}
        \includegraphics[width=\textwidth]{figs/suspiciousness-Tarantula-best.pdf}
        \caption{\TARANTULA{}}%
        \label{fig:sus-tarantula}
    \end{subfigure}
    \begin{subfigure}[h]{.32\textwidth}
        \includegraphics[width=\textwidth]{figs/suspiciousness-Ochiai-best.pdf}
        \caption{\OCHIAI{}}%
        \label{fig:sus-ochiai}
    \end{subfigure}
    \begin{subfigure}[h]{.32\textwidth}
        \includegraphics[width=\textwidth]{figs/suspiciousness-DStar-best.pdf}
        \caption{\DSTAR{}}%
        \label{fig:sus-dstar}
    \end{subfigure}
    \begin{subfigure}[h]{.32\textwidth}
        \includegraphics[width=\textwidth]{figs/suspiciousness-Naish2-best.pdf}
        \caption{\NAISHT{}}%
        \label{fig:sus-naish}
    \end{subfigure}
    \begin{subfigure}[h]{.32\textwidth}
        \includegraphics[width=\textwidth]{figs/suspiciousness-GP13-best.pdf}
        \caption{\GPOT{}}%
        \label{fig:sus-gp}
    \end{subfigure}
    \caption{Suspiciousness. Results of the suspiciousness that a feature correlates with the presence of failures for the metrics \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{}. Each feature is evaluated according to the best feature of each class for each subject.}%
    \label{fig:suspiciousness}
\end{figure}

\Cref{fig:suspiciousness} illustrates the suspiciousness results for the five best-performing execution features leveraging \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{} for each subject.
These results support our findings that the features that capture data flow and the relation between a program's data, lines, functions, and return values are more relevant to failures.

\subsubsection{\bfseries\Cref{rq2}: Localizing Faults}%
\label{sub:eval-localization}

\begin{table}
    \caption{Localization. Results for the localization of faults leveraging \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{}. Each feature is evaluated according to Top-1, Top-5, Top-10, Top-200, \EXAM{} score and wasted effort for three debugging scenarios.}%
    \label{tab:results-localization-1}
    \setlength\extrarowheight{-3pt}
    \centering
    \resizebox{\textwidth}{!}{%
    \input{data/localization-1.tex}
    }
\end{table}

\begin{table}
    \caption{Localization. Continuation of \Cref{tab:results-localization-1}.}%
    \label{tab:results-localization-2}
    \setlength\extrarowheight{-3pt}
    \centering
    \resizebox{\textwidth}{!}{%
    \input{data/localization-2.tex}
    }
\end{table}

Concerning the fault localization and addressing \Cref{rq2}, we can see that \emph{lines} outperform all other feature classes in localizing faulty lines, especially in the top-5 best-case debugging scenario. Lines also perform well in average and worst-case scenarios, only being surpassed by functions for \DSTAR{}.

When considering top-10 and top-200, we see a shift in the performance of the best localization to functions and def-use pairs, where \emph{def-use} pairs slightly outperform functions for the best-case and average-case debugging scenario.
For the \EXAM{} scores, we see that \emph{functions} show the lowest score. 
However, lines, def-use pairs, and function errors also show a comparable low \EXAM{} score, indicating that these features are more likely to suggest the faulty lines early in the suggested lines.
The wasted effort reinforces the results for the \EXAM{} score, where lines, def-use pairs, functions, and function errors show the lowest score.

\begin{result}
    \emph{Lines}, \emph{def-use pairs}, and \emph{functions} are the best features for localizing faults.
\end{result}

However, some features by design are inherently better suited for localizing faults.
For instance, lines and functions covering almost the entire program are likelier to assign reasonable suspiciousness scores to the faulty lines and rank them higher.
Other features can only partially cover the code and may not assign a reasonable suspiciousness score to the actual faulty lines. For instance, loops only cover a small portion of the code and might not be executed in the presence of a failure.
This argument is supported by the high wasted effort of these features, for instance, return values, and the generally poor performance of the particular features, e.g., if a string contains digits or is empty.

\begin{result}
 By design, some features are better suited for localizing faults than others.
\end{result}

\subsubsection{\bfseries\Cref{rq3}: Multi-Feature Localization}%
\label{sub:eval-multi-feature}

\begin{table}
    \caption{Localization of Multiple Features. 
    Results for the localization of faults leveraging \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{} for considering multiple features at once. Each feature is evaluated according to Top-1, Top-5, Top-10, Top-200, \EXAM{} score and wasted effort for three debugging scenarios. The bolted values show that multiple features achieve a better result than any individual one.}%
    \label{tab:results-localization-multiple}
    \centering
    \setlength\extrarowheight{-3pt}
    \resizebox{\textwidth}{!}{%
    \input{data/unified-localization.tex}
    }
\end{table}

After evaluating the näive multi-feature technique for \Cref{rq3}, we found it generally performs well but does not outperform the best individual feature classes for localizing faults for most top-k metrics and debugging scenarios, which is reflected in \Cref{tab:results-localization-1,tab:results-localization-2}.
However, the approach eliminates the flaws of the individual features.
For instance, when compared with lines, the multi-feature technique, considering the maximum suspiciousness, does not reach the results of lines for the top-5. Still, with increasing k, the multi-feature technique outperforms lines.
The same holds for the approach of leveraging mean suspiciousness.
Vice versa, this observation applies to functions where the multi-feature technique does not reach the results of functions for the top-200, but with decreasing k, it outperforms functions.

Moreover, the multi-feature technique achieves significantly lower \EXAM{} scores and wasted effort than the individual features, indicating that the multi-feature technique is more likely to suggest the faulty lines early.
These findings are consistent across all debugging scenarios, indicating that the multi-feature technique is generally beneficial for localizing faults.
These benefits are compelling, considering a developer must still inspect the suggested lines to find the fault.

\begin{result}
 Leveraging multiple execution features considerably reduces the effort to find the faulty lines.
\end{result}

Finally, we can conclude that the maximum suspiciousness performs better than the mean for the multi-feature technique.

\section{Learning Diagnoses from Execution Features}%
\label{sec:debugging}

Now that we have established the effectiveness of execution features in identifying failure-inducing properties in executions, can we make them actionable for developers?
In software debugging, developers often face the challenge of diagnosing faults based on available test cases.
\EFDD{} (\textbf{E}xecution-\textbf{F}eature-\textbf{D}riven \textbf{D}ebugging) addresses this by systematically transforming raw execution data into actionable insights, helping developers localize faults with greater precision and interpretability.

We focus on scenarios where developers are equipped with a faulty program and a set of labeled test cases—comprising both passing and failing cases—that effectively expose the fault.
The primary objective of \EFDD{} is to utilize these labeled test executions to infer an interpretable diagnosis that explains why specific test cases trigger failures while others succeed.
At its core, \EFDD{} leverages the contrast between passing and failing execution features to extract meaningful diagnoses.
Analyzing execution traces identifies correlations between execution features and observed outcomes, enabling an execution-driven approach to fault diagnosis.

\Cref{fig:overview-efdd} outlines the architecture of \EFDD{}, which is composed of two primary phases: the Execution Phase and the Learning Phase.
The process unfolds through the following sequential steps:

\begin{enumerate*}[label=(\arabic*)]
    \item (\emph{Program Instrumentation}) \EFDD{} begins by instrumenting the program under test. This instrumentation embeds probes within the code to capture execution events, allowing us to understand the program's dynamic behavior.
    \item (\emph{Test Execution}) The instrumented program is then executed using the provided test cases. Both passing and failing tests are run to gather diverse event execution traces. Each trace consists of a chronological sequence of events triggered during the program's execution, offering a fine-grained view of the program's behavior under different inputs.
    \item (\emph{Feature Extraction}) From the collected execution traces, \EFDD{} constructs feature vectors that abstract relevant characteristics of each run. These features capture key execution properties—such as the frequency of certain events, the activation of specific code paths, or data dependencies—that may correlate with faults. The feature engineering process ensures that the resulting vectors are informative and amenable to machine learning.
    \item (\emph{Model Training}) With the labeled feature vectors, \EFDD{} trains a decision tree classifier. This model learns to discriminate between passing and failing executions by identifying patterns in the extracted features. The choice of a decision tree ensures that the resulting diagnosis is transparent and easily understandable by developers, as the tree structure naturally maps to logical conditions that can be traced back to the program's behavior.
    \item (\emph{Diagnosis Generation}) Finally, \EFDD{} analyzes the trained decision tree to produce a diagnosis that pinpoints the program behaviors most strongly associated with failures. The diagnosis highlights critical decision points and feature thresholds that differentiate failing runs from passing ones, providing developers with clear insights into the root cause of the fault.
\end{enumerate*}

\begin{figure}
    \includegraphics[width=\textwidth]{figs/efdd-overview.pdf}
    \caption{\EFDD{} at a glance. \EFDD{} takes a program and a set of labeled test cases as input, instruments the program, executes the test cases, and captures an execution trace. From this trace, it constructs execution features to train a decision tree. The resulting model offers an interpretable diagnosis for the observed fault.}
    \label{fig:overview-efdd}
\end{figure}

For any subsequent input or test, \EFDD{} would streamline this process.
It executes the input, captures the events, formulates the features, and then classifies the run based on the initially trained model.


\subsection{Training a Decision Tree Model}%
\label{sub:classifying}
The concluding step of our approach involves training a machine learning classifier using the feature vectors we have constructed.
Inspired by \ALHAZEN{}~\cite{kampmann2020alhazen}, we opt for decision trees as our classifier of choice for several reasons.
First, our features predominantly consist of discrete values, making them well-suited for decision tree-based classification.
These features enable us to distinguish effectively between failing and passing runs through straightforward combinations of feature values.
Second, decision trees' explainable nature allows us to identify relevant features that contribute to fault detection quickly.
Finally, this model enables classifying an unseen test input's behavior by correlating extracted execution features with program failures.

\begin{wrapfigure}{l}{0.45\textwidth}
    \centering
    \input{data/decision-tree.tex}
    \caption{The decision tree generated by \EFDD{} for the \texttt{middle()} example. Each node represents a decision, leading to a classification of either \PASS{} (Pass) or \FAIL{} (Fail).}%
    \label{fig:tree}
\end{wrapfigure}

Our preliminary experiments with various classifiers, including other naive classifiers, neural networks, and large language models, consistently revealed that decision trees either outperformed or matched the performance of the best classifiers in all cases, making them ideal candidates for our classification models.
Additionally, imbalanced sample sizes were not an issue, as most experiments were, as in the real world, with fewer failing than passing examples.

We are committed to maintaining impartiality in our classification process and do not favor detecting faults over identifying correct executions.
We consider passing and failing executions equally valuable for learning an adequate diagnosis.
Biasing our classification towards a particular outcome could lead to issues in any downstream applications of our approach.

\subsection{Deriving Diagnoses}%
\label{sub:diagnoses}

A central design goal of \EFDD{} is to ensure the \textbf{interpretability} of its machine learning model, enabling developers to gain clear insights into the underlying causes of software faults.
Using decision trees as the core diagnostic model was a deliberate choice, as they inherently provide a transparent and logical decision-making flow that developers can easily follow.
By framing the diagnosis as a decision tree, \EFDD{} transforms complex execution traces into intuitive, rule-based explanations.
Each node in the tree represents a conditional check derived from the program's execution features, guiding developers through the logical paths that lead to passing or failing outcomes.
This approach localizes faults and offers a deeper understanding of the execution contexts that trigger failures.

We illustrate this interpretability through the \texttt{middle()} example introduced in \Cref{sec:introduction}.
Applying \EFDD{} to this case, using the initial set of labeled test cases, yields the decision tree shown in \Cref{fig:tree}. 

The decision tree reveals two critical features associated with the fault:
\begin{enumerate*}
    \item \textbf{Execution of Line 6:} Whether the statement \texttt{return y} at Line~6 was executed.
    \item \textbf{Comparison of Variables (\texttt{y} $\ge$ \texttt{x}):} Whether the value of \texttt{y} is greater than or equal to \texttt{x} when entering the function.
\end{enumerate*}
The fault occurs under a specific condition: when Line~6 is executed and \texttt{y} is not greater than or equal to \texttt{x}.
This precise insight directs developers to the faulty behavior without combing through the entire execution trace.
From the decision tree, we can infer the following:
\begin{enumerate*}
    \item If Line~6 is \textbf{not} executed, the test case passes—irrespective of other conditions.
    \item If Line~6 \textbf{is} executed and \texttt{y} $\ge$ \texttt{x}, the execution still passes.
    \item However, if Line~6 is executed \textbf{and} \texttt{y} $<$ \texttt{x}, the failure is triggered.
\end{enumerate*}
This diagnosis immediately highlights both the location (\textbf{Line~6}) and the condition leading to the fault (\texttt{y} $<$ \texttt{x}), offering actionable information to the developer.
The comprehensible nature of the decision tree makes it an ideal candidate for diagnosis generation.
Since the model identifies the faulty line and the conditions leading to failure, developers can systematically target these conditions to design a correct fix.

\subsection{Implementation}%
\label{sub:implementation-efdd}

Our implementation of \EFDD{} is built upon \SFLKIT{}~\cite{smytzek2022sflkit}.
\SFLKIT{} provides the means to instrument a \PYTHON{} program under test and collect a trace of execution events.
In our implementation, we utilize \SFLKIT{}'s built-in iteration over the event trace to gather features.
Specifically, while \SFLKIT{} processes the events to extract various predicates and spectra (such as coverage information), we inject our feature collector into this loop.
This feature collector constructs the features based on the extracted data.
Once the feature vectors are constructed, we convert them into data frames suitable for feeding directly into a machine-learning classifier.
We chose \SCIKITLEARN{}'s decision tree learner as the backbone of our diagnosis.
Although \SCIKITLEARN{} offers a variety of classifiers, we restricted ourselves to decision trees for several reasons:
\begin{enumerate*}[label=(\alph*)]
\item The inherent explainability of decision trees, as detailed in \Cref{sub:classifying}.
\item The focus of this work is on the overarching approach, not on classifier comparisons.
\end{enumerate*}

\section{Evaluation}%
\label{sec:evaluation}

To assess the effectiveness of \EFDD{}, we designed our evaluation around the following key research question:

\begin{questions}[topsep=2pt]
    \setcounter{questionsi}{3}
    \item\label[question]{rq4} \textbf{Diagnosis' Quality}. How accurate are the diagnoses generated by \EFDD{} from \Cref{sec:debugging}?
\end{questions}

\subsection{Experimental Setup}%
\label{sec:setup}

The evaluation focuses on measuring \EFDD{}'s capability to generate accurate and interpretable diagnoses by assessing how effectively the generated diagnosis can classify the presence of faults based on program executions.
Specifically, we evaluate the diagnosis as a classifier that distinguishes between failing and passing runs, aligning with established practices in this research domain~\cite{kampmann2020alhazen,eberlein2023avicenna}.
Unlike prior approaches such as \ALHAZEN{} and \AVICENNA{}, which evaluate both the generative and predictive capabilities of their models, our evaluation exclusively focuses on the predictive accuracy of the diagnosis.
This distinction stems from the fact that \EFDD{} does not generate new program inputs—a limitation discussed further in \Cref{sub:related-execution-features}—but instead builds diagnoses based on existing execution data.

To ensure a rigorous and meaningful evaluation, we selected subject programs based on the following criteria:
\begin{description}[leftmargin=5pt,noitemsep,partopsep=0pt,topsep=0pt,parsep=0pt]
    \item[Fault Impact on Output.] Each subject must include a fault that affects the program's state and, consequently, its output, ensuring that the fault is observable through test executions and making diagnosis feasible.
    
    \item[Emphasis on Functional Bugs.] Most subjects focus on functional bugs that cause incorrect outputs rather than bugs that lead to exceptions or crashes. Functional bugs often require deeper analysis, aligning with \EFDD{}'s core strength. While exceptions can signal faults explicitly, functional bugs pose a greater diagnostic challenge.

    \item[Realistic and Isolated Defects.] Subjects are chosen to reflect real-world defects. Each subject contains exactly one bug to isolate the diagnostic process. Although \EFDD{} is currently designed to analyze one fault at a time, it can handle multiple bugs in a single program if tests clearly distinguish between them, allowing separate diagnoses for each.

    \item[Availability of Ground Truth.] Each subject includes a known ground truth, enabling a direct comparison between the diagnosed fault and the actual defect, which is critical for quantitatively assessing the accuracy of \EFDD{}'s diagnoses.

    \item[Labeled Test Cases.] Each subject provides at least one set of pre-labeled test cases, including failing and passing tests. This initial labeled set is a foundation for training and evaluating \EFDD{} and ensures the fault is triggered and avoided across different test executions.
\end{description}

Based on our benchmark requirements, we selected \REFACTORY{}~\cite{hu2019refactory} for our evaluation.
\REFACTORY{} is a comprehensive benchmark of student submissions for five distinct programming tasks.
It provides initial input-output pairs and a correct reference implementation for each task, making it an ideal candidate for evaluating fault localization and diagnosis tools like \EFDD{}.
For each subject in the benchmark, we label the generated inputs as either \textit{passing} or \textit{failing} by comparing the subject's actual output against the expected output from the correct implementation.
A test is labeled passing if the program's output matches the expected output and labeled failing if the program's output deviates from the expected result.
We then use these labeled examples to diagnose each subject based on the initial seed inputs.
However, we apply a filtering step to exclude uninformative subjects that either fail or pass on all the initial seeds.

\begin{wraptable}{l}{.4\textwidth}
	\caption{Quality of the generated diagnoses by \EFDD{}.}%
	\label{tab:results-quality}
    \centering
    \input{data/results-refactory.tex}
\end{wraptable}

To assess the accuracy of the diagnoses generated by \EFDD{}, we generated 400~labeled evaluation input examples for each subject. We evaluated the model's ability to classify passing and failing runs correctly.
We compute the achieved accuracy, precision, recall, and F1 score based on the evaluation input.
To allow an efficient analysis, we evaluate all considered metrics (i.e., accuracy, precision, recall, F1 score, and their macro counterparts) of all subjects not by averaging but by considering all correctly labeled and mislabeled predictions.

\subsection{\Cref{rq4}: Diagnosis Quality}%
\label{sec:results-diagnosis}

For the quality of the diagnosis, we consider the 1777 subjects that pass our requirements from the \REFACTORY{} benchmark.
\Cref{tab:results-quality} comprises all our results over all subjects for each benchmark.

Our approach generated diagnoses that can distinguish passing from failing executions with an overall accuracy of $89.36\%$ and a macro F1 score of $0.89$, showing the predictive power of these diagnoses.
On 884 subjects from \REFACTORY{}, we could infer diagnoses that could distinguish all runs from our evaluation set, which means that in 50\% of the cases, the diagnoses were as sound as they could be in our evaluation setup.
Overall, all our evaluation metrics show stable values without significant outliers, demonstrating again how powerful and balanced, in terms of not favoring non-buggy features over buggy ones since they are more represented in the training sets, the diagnoses generated by \EFDD{} can be.
In addition, an average execution time of \EFDD{} of 3.9 seconds on our evaluation machines is a manageable workload with a highly beneficial outcome.

\begin{result}
    From a small set of given labeled tests, \EFDD{} can generate diagnoses that can differentiate unseen passing and failing program runs with a high predictive power, implying the high accuracy of the diagnoses.
\end{result}

\section{Threats to Validity}%
\label{sec:threats-to-validity}

\subsection{Execution Features Study}%
\label{sub:threats-study}

\subsubsection*{Internal Validity}
One potential threat to internal validity is the accuracy of the event collection process.
Any inaccuracies in capturing execution features could lead to incorrect correlations between features and failures.
To mitigate this, we ensured that the instrumentation and data collection processes were consistent across all experiments.
Another concern is the potential for biases in the test cases the \TESTS4PY{} dataset provides, which we tackled by verifying that the test cases failed and passed as they should in the buggy and fixed versions.

\vspace*{-.5\baselineskip}
\subsubsection*{External Validity}
Selecting projects from the \TESTS4PY{} dataset primarily threatens our study's external validity.
While this dataset offers a diverse set of Python projects, the results may not generalize to projects written in other programming languages or those with different characteristics, such as size or complexity.
Additionally, the dataset's focus on open-source projects may only partially represent the range of proprietary or industrial software failures.

\vspace*{-.5\baselineskip}
\subsubsection*{Construct Validity}
Construct validity concerns arise from the metrics used to evaluate correlation and fault localization.
To mitigate this risk we included multiple well-established coefficients (\TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{}) and evaluation metrics (Top-1, Top-5, Top-10, Top-200, \EXAM{} score, and wasted effort).
However, these metrics may only partially align with practical debugging experiences~\cite{parnin2011automated}, but we attempted to address this by providing a comprehensive analysis across multiple metrics and cross-verified our results with these.

\vspace*{-.5\baselineskip}
\subsubsection*{Conclusion Validity}
Conclusion validity could be impacted by statistical methods correlating execution features with failures. These methods should be corrected to avoid incorrect conclusions about the relevance of certain features. We mitigated this threat by employing established statistical fault localization techniques and cross-verifying our results with multiple metrics. Additionally, using a single dataset limits our empirical analysis's diversity, potentially affecting our conclusions' robustness. Future work could involve replicating the study on additional datasets to strengthen the generalizability of our findings.

\subsection{Execution Features Driven Debugging}%
\label{sub:threats-debugging}

\subsubsection*{Internal Validity}
Concerning implementing our diagnosis approach, we cannot verify that it realizes the exact approach presented in \Cref{sec:debugging}.
However, the results of our experiments from \Cref{tab:results-quality} should eliminate the risk of any significant flaws in our implementation.
Moreover, the manual inspection of the diagnostic power of our approach in \Cref{sub:diagnoses} reinforces this claim.
Regarding our experiments for \Cref{rq1} we leveraged our implemented test generation for \REFACTORY{} that may be
\begin{enumerate*}[label=(\alph*)]
    \item incomplete, i.e., there are possible tests that will not be generated,
    \item easy to distinguish by simple features,
    \item or produce inputs that the program was not designed to handle.
\end{enumerate*}
For the test generation for \REFACTORY{}, we tried to stay true to the tests provided by this benchmark and introduce randomness to cover as much of the input space as possible.
Moreover, all generated inputs produced a result when executed on the correct implementation for each question, showing that they are at least accepted.
However, when considering the test generation and our implementation, another threat could be that we learn diagnoses based on the execution features but instead overfit to the generators, i.e., learn only to distinguish passing and failing runs that the generator produces.
We tried to eliminate this risk by ensuring that the generators were as general as possible (considering that they should still trigger the fault) and assuming an adequate number of unseen inputs for our evaluation sets.

\vspace*{-.5\baselineskip}
\subsubsection*{External Validity}
The selection of our subjects might not be sufficient to show the applicability of our approach to an unseen real-world program.
This threat might be heavy, especially considering the subjects in \REFACTORY{} student submissions to relatively small tasks.
Since the critical part of our approach is the underlying \SFLKIT{}~\cite{smytzek2022sflkit} as its base, our approach should be applicable whenever \SFLKIT{} is, which was already verified and tested on the entire \BUGSINPY{}~\cite{widyasari2020bugsinpy} benchmark.
Because of this reason, we would consider that \EFDD{} applies to actual programs, and our evaluation's results are generalizable and can transfer to an unseen fault.

Since our evaluation is done in \PYTHON{}, we cannot mitigate the risk that \EFDD{} does not apply to other programming languages.
However, arguing that our execution features are general and could also be extracted from programs implemented in other programming languages, like \C{} or \JAVA{}, we are convinced that our technique is general.

\vspace*{-.5\baselineskip}
\subsubsection*{Construct Validity}
Another concern is the metrics we have chosen to evaluate the results for \Cref{rq1} that might be insufficient to show our diagnosis quality.
We leveraged several established metrics (precision, recall, F1 score, and accuracy) to counter this threat for all our measurements.

\vspace*{-.5\baselineskip}
\subsubsection*{Conclusion Validity}
The statistical methods used to evaluate the quality of the diagnoses could affect the conclusion's validity.
To mitigate this risk, we cross-verified our results with multiple metrics.


\section{Related Work}%
\label{sec:related-work}

\subsection{Fault Localization}%
\label{sub:related-fault-localization}

Fault localization is a critical area in software debugging.
It aims to pinpoint the exact locations in the code responsible for failures.
Several traditional techniques have been extensively studied, particularly statistical methods that introduce similarity coefficients such as \TARANTULA{}~\cite{jones2005tarantula}, \OCHIAI{}~\cite{abreu2006ochiai}, \DSTAR{}~\cite{wong2012dstar}, \GP{}~\cite{xie2013gp}, \AMPLE{}~\cite{dallmeier2005ample}, \JACCARD{}~\cite{chen2002jaccard}, and many others.
Spectrum-based fault localization (SBFL) techniques, including those mentioned, are especially notable for using execution traces to estimate the likelihood that specific code elements are faulty statistically.
In recent years, these techniques have gained traction in automated debugging, particularly within automatic program repair frameworks, where they significantly contribute to identifying code regions for repair~\cite{qi2013apr,lutellier2020coconut}.

Other research in this area has shifted the focus from using lines as the unit for localization to other coverage-based information.
Zhang et al.~\cite{zhang2009capturing} examined basic blocks, Le et al.~\cite{le2010path} investigated paths, Jiang et al.~\cite{jiang2023variable} leveraged variables and decision trees, and Vancsics et al.~\cite{vancsics2021calls} considered call frequency.
Ribeiro et al.~\cite{ribeiro2019dataflow} used data flow for localizing faults.
The work by Yu et al.~\cite{yu2011models} leverages data and control dependency models to calculate suspiciousness scores.
In contrast, Yan et al.~\cite{yan2023context} employ traditional SBFL but modify the scores afterward based on the context in which the fault propagates.
Other approaches do not consider coverage of the traditional SBFL.
Soremekun et al.~\cite{soremekun2021slicing} present an approach that builds on program slices for localization, while Papadakis et al.~\cite{papadakis2014mutation,papadakis2015mutation} leverage mutation analysis.

The extensive research in this area has led to numerous metrics designed to assess the correlation between code locations and failures~\cite{daniel2013sbfl,landsberg2015sfl,naish2011sbfl}.
Parnin et al.~\cite{parnin2011automated} critically evaluate these techniques and question their practical applicability for developers.
They conclude that while these approaches can reduce the search space, they often overwhelm developers with numerous potential fault locations, making pinpointing the exact faulty lines challenging.
Another comprehensive survey by Soremekun et al.~\cite{soremekun2023evaluating} investigates general assumptions of fault localization, finding that most developers would prefer a diagnosis to locations for debugging.
Other studies, such as those by Abreu et al.~\cite{abreu2009practical}, Pearson et al.~\cite{pearson2017sfl}, Heiden et al.~\cite{heidengkkhfl19}, and Widyasari et al.~\cite{widyasari2022sfl}, have evaluated fault localization techniques across different benchmarks to assess their effectiveness and limitations.
In contrast to our work, they did not consider multiple features in their evaluation but concentrated on lines as the unit of measure for localization.

Beyond traditional fault localization, recent research has expanded to leverage these techniques in novel ways.
For instance, Le et al.~\cite{lelgg16} use a learning-to-rank-based approach that integrates knowledge from mined likely invariants~\cite{ernstpgmptx07} to prioritize functions that are more likely to be the cause of faults.
Additionally, approaches like \ENTBUG{}~\cite{camposafd13} utilize fault localization to drive test generation, incorporating localization metrics into the fitness of a genetic test generator.

Considering how statistical fault localization needs to execute the entire test suite to provide relevant code locations, Jiang et al.~\cite{jiangzctc12} explore how test case prioritization could improve the process.
Yoo et al.~\cite{yoo2013priorization} and Gonzales-Sanchez et al.~\cite{gonzalez-sanchezagg11} build an approach that applies prioritization not only to reduce execution time but also to enhance fault localization results.
Gong et al.~\cite{gongwsm13} directly reduce the test suite to achieve similar improvements.
In contrast, Xuan et al.~\cite{xuanm14b} purify test cases by splitting them into smaller parts, providing more granular insights, and leveraging these purified tests to improve statistical fault localization.
Moreover, test generation can also enhance fault localization results.
For example, Artzi et al.~\cite{artzidtp10b} show that generating directed test cases maximizing the similarity between path constraints of generated tests and those of faulty executions outperforms undirected generation.
While our approach seeks to maximize execution differences, the concept aligns with test generation methodologies.

With the rise of machine learning techniques, researchers have explored using various learning techniques for fault localization~\cite{li2019deepfl,li2021covrepresentation,widyasari2022xai4fl,wang2024mtltransfer,yang2024multilingual,li2022cc}.
Notably, feature-based fault localization approaches by Lei et al.~\cite{lei2022featurefl} abstract program behaviors as features, while Meng et al.~\cite{meng2022transfer} leverage semantic features.
Additionally, research has investigated the impact of large language models on fault localization~\cite{kang2024llm}.

This body of work highlights both the progress made in fault localization and the challenges that remain, particularly in enhancing the practical utility and precision of these techniques for developers.


\subsection{Feature-based Debugging}%
\label{sub:related-execution-features}

Recent research also uses \emph{features} for deriving debugging diagnoses.
\ALHAZEN{}~\cite{kampmann2020alhazen} was one of the first approaches that leveraged features for debugging.
The approach learns a decision tree from a set of \emph{input features} and iteratively refines the tree by generating new inputs that aim to trigger the failure.
From the decision tree, \ALHAZEN{} derives a diagnosis.
Similar to \ALHAZEN{}, \AVICENNA{}~\cite{eberlein2023avicenna} leverages \emph{input features} for debugging.
It follows the same refinement loop but leverages a sophisticated constraint learner to derive a diagnosis and generates new inputs by solving these constraints.

In contrast, our work opens the field for \emph{execution-feature-driven debugging} that can generate diagnoses explaining the actual program behavior that leads to a failure, providing a more comprehensive understanding of the failure while \ALHAZEN{} and \AVICENNA{} focus on the input space that is not as useful for debugging as the execution space.
However, our approach can, at this point, not effortlessly generate new inputs to trigger the failure since we cannot directly map the execution features to the input space, but this is mitigated by the fact that \ALHAZEN{} and \AVICENNA{} require a specification of the input space. In contrast, our approach runs out-of-the-box on any program.
We believe that \ALHAZEN{} or \AVICENNA{} would complement our approach.

\section{Conclusion and Future Work}%
\label{sec:conclusion}

First, we conducted an empirical study that explored alternative execution features that could enhance fault localization in software programs.
By leveraging the \TESTS4PY{} dataset, we analyzed the correlation between these features and the presence of failures using various statistical fault localization techniques, including \TARANTULA{}, \OCHIAI{}, \DSTAR{}, \NAISHT{}, and \GPOT{}.
Our findings indicate that incorporating a diverse set of execution features can improve the accuracy and precision of fault localization.

Second, we introduced a novel technique to generate accurate diagnoses that refer to the features investigated in our study, such as lines executed or variable values, making them easy to read and assess.

Our future work will focus on the following topics:

\begin{description}[leftmargin=5pt,noitemsep,partopsep=0pt,topsep=0pt,parsep=0pt]
     \item[More Programming Languages.] To further validate our findings, we will build on our findings by extending the analysis to a broader range of programming languages.
    \item[Test Generation.] We aim to map execution features to the input space, allowing us to directly generate new inputs to trigger the fault based on the generated diagnosis.
    \item[Automated Repair.] In automated repair, the diagnosis inferred by our approach helps to generate patches that fix the fault.
 For instance, the diagnosis of the \texttt{middle()} example from \Cref{fig:tree} could directly suggest to replace the variable \texttt{y} with \texttt{x} in the \texttt{return} in Line~6 for a program repair.
    \item[More Execution Features.] Despite our capability to produce precise diagnoses, our approach's performance depends on its set of execution features: If a failure does not depend on any of the features collected, it will be hard to produce an accurate diagnosis.
 This limitation can be countered by adding more features, notably \emph{derived} features such as string or arithmetic properties---but if a failure occurs, say, whenever $n$ is a perfect number, and $d$ is a day with a full moon, it will still be hard to detect these features.
    \item[Program Synthesis.] By selecting relevant features, we effectively synthesize a \emph{predicate}. Such predicates can also be synthesized through symbolic means~\cite{gulwani2017synthesis} and possibly yield better results while still being explainable.
    \item[Deep Learning Models.] If one can live without explainability, many machine learning models are available that may all result in applicable diagnoses with even higher accuracy.
 Nevertheless, the decision tree classifier outperformed all other models investigated during our preliminary experiments.
 However, given the vast number of available models and the frequency with which novel models are introduced, it is worth investigating this area further, e.g., by conducting a large-scale study.
    \item[Testing Oracle.] Based on our diagnoses and an adequate mapping approach that allows us to relate features of a program version to an altered version of the same program, we plan on investigating the possibility of generating a testing oracle that can differentiate between passing and failing tests.
    \item[User Study.] To evaluate the usability of our diagnosis approach, we plan to conduct a user study with developers to assess the effectiveness of the diagnoses generated by our approach. 
\end{description}

\section*{Data and Tool Availability}%
\label{sec:tool}

Our evaluation data, all scripts, and our \EFDD{} artifact are all open source.
The current versions of evaluation scripts and \EFDD{} can be downloaded from
\vspace{-.2\baselineskip}
\begin{center}
    \url{https://github.com/smythi93/efdd}%
\end{center}
\vspace{-.2\baselineskip}
The dataset containing the collected events, features, and all intermediate results is available at
\vspace{-.2\baselineskip}
\begin{center}
    \url{https://doi.org/10.5281/zenodo.14909966}%
\end{center}

\begin{acks}
    This research was partially funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) --- ZE 509/7--2 and GR 3634/4--2 Emperor (261444241) and by the European Union (ERC S3, 101093186). Views and opinions expressed are, however, those of the authors only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them. 
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
