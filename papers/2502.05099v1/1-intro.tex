\section{Introduction}
\label{sec:intro}

The use of automated employment decision tools (AEDTs) has rapidly increased in hiring contexts, especially for computing jobs. This includes the use of software in reading resumes, scoring coding assessments, and even conducting interviews to assess job applicants among others~\cite{bogen2018help, sanchez2020does}. Job applicants in this changing hiring landscape have developed strategies to navigate these new tools, such as using online coding assessment practice tools or putting their resumes through resume scanners~\cite{armstrong2023navigating}. The automated systems with which job seekers must contend are typically built by third-party companies for employers, and their development largely ignores the perceptions and experiences of the applicants interacting with them. While prior work has identified a lack of auditing, accountability, and transparency in this space~\cite{ajunwa2019auditing, sloane2022silicon, sanchez2020does}, things are beginning to change as policy-makers start to introduce auditing laws for AEDTs, such as New York City's Local Law 144, which mandates employers publish some auditing results for any AEDTs they use in hiring~\cite{locallaw144}. However, meta-audits of the hiring audits published in compliance with Local Law 144 have found significant limitations, including considerable employer discretion about whether a system constitutes an AEDT and is therefore covered by the law, what data is used, what results are actually published in the audits, and where the findings are publicized (making them largely inaccessible to job seekers)~\cite{wright2024null}. As AEDTs and related law and policy proliferate, it is imperative to understand job applicants' own perceptions of AEDTs, the strategies they use to navigate this environment, and how strategies relate to their job market outcomes.

Understanding applicants' beliefs about and practices surrounding the use of automation in hiring is important for understanding the impact of these tools, improving such technologies when they are used, and restricting their use when inappropriate. Investigating human perceptions is imperative to building trustworthy, fair, and explainable AI in a range of domains~\cite{woodruff2018qualitative, lee2018understanding, glikson2020human}. Prior work has identified that one factor contributing to people's negative perceptions of algorithmic decision-making is a feeling of dehumanization caused by being algorithmically evaluated~\cite{lee2018understanding}. Another is distrust in the system, which can be caused by the perception that the system is not fair~\cite{woodruff2018qualitative, lee2018understanding}. Using a procedural fairness framework, which investigates fairness through the processes by which decisions are made as opposed to the outcomes~\cite{lee2019procedural}, we build on on prior work in this space to investigate how applicants perceive and navigate different forms of AI-based automated hiring technologies, as well as the impact, if any, of such strategies on hiring outcomes. 

While many automated hiring companies claim to reduce bias in hiring, prior work has shown that many automated hiring systems can be as biased as human-decision makers~\cite{caliskan2017semantics}, and that some even violate U.S. and U.K. anti-discrimination laws~\cite{ajunwa2019paradox, sanchez2020does}. For computing students, who are often navigating the job market for the first time, biases introduced by automated hiring methods may be especially impactful, affecting their applications' success, but also their sense of belonging in the field, overall anxieties, and even their ability to enter the workforce at all~\cite{behroozi2019hiring, kasinidou2021agree}. Some initial work on applicant awareness of AEDTs has found that applicants have experience with a variety of AEDTs, even if they do not know they are interacting with an AEDT, and report using strategies to navigate automated hiring, such as getting referrals, modifying resumes for automated scanners, and using online coding assessment practice materials in response to them~\cite{armstrong2023navigating}. While previous work explores bias in automated hiring or applicant perceptions of automation, this is a still-growing area, with more research needed on the intersection of applicants' perceptions of various forms of automation, experience with automated hiring strategies, and job outcomes. 

To better understand young job seekers' perceptions and experiences with automated hiring, we pose the following research questions: 

% RQs:
\begin{enumerate} 
    \item[\textbf{RQ1}] How do young job seekers' perceptions of AEDTs' procedural fairness and their own willingness to be evaluated by different AEDTs change as the level of automation and type of evaluation change?
    \item[\textbf{RQ2}] Do perceptions of AEDTs' procedural fairness and willingness to be evaluated by AEDTs change based on applicants' use of strategies to navigate automation in the hiring process?
    \item[\textbf{RQ3}] How do awareness about AEDTs, strategies to navigate automation, and demographic attributes (like gender, race, and income) relate to job market outcomes?
\end{enumerate}

% Method: 
To answer these questions, college-aged computer science students (relevant for their status as young, current job seekers) answered a two-part survey. In the first, they were given descriptions of different scenarios describing a hypothetical hiring process, each of which included an \textit{evaluation type} (coding assessment, resume review, or interview) and \textit{automation level} (ranging from human-only to AI-only review). For each scenario, they were asked to assess the fairness of each hiring process \textit{(procedural fairness measure)} and whether they themselves would want to be evaluated that way \textit{(willingness measure)}; this data was analyzed to evaluate their perceptions of and attitudes towards automated hiring algorithms. In the second part, participants provided information on whether they had experienced or heard about AEDTs (coding assessments, resume scanners, etc.) and what strategies they used relating to these practices (coding assessment practice, adding keyword to their resumes, etc.). We analyzed this data to evaluate whether participants' perceptions of automated hiring processes were related to their strategy use, and whether those strategies had an impact on job market success (whether they reported receiving job offers).

We found that participants across the board rated the use of AI as unfair compared to human-only evaluation, and that as the level of automation increased, procedural fairness and willingness ratings decreased. Additionally, participants found the use of AI more inappropriate in less technical evaluations. For example, fairness and willingness ratings for the use of AI in coding assessments were higher than for behavioral interviews. Notably, examining the relationship between participants' strategy use and demographics with job outcomes, we found that only two attributes were significantly positively associated with job market success: the degree to which participants used referrals and their family's household income. 

Overall, our findings point to widespread dislike among young job seekers towards AEDTs, as well as the continued role (not `fixed' by automation) of social and socioeconomic privilege in job seeking. Our work makes contributions to CSCW by providing new insight into young job-seekers' perspectives on AEDTs and the strategies they use to succeed in an increasingly automated hiring market. We conclude by discussing implications for auditing, policy, and use of AEDTs by employers in the future.