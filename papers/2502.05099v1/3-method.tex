\section{Method}
\label{sec:methods}

We conducted a survey of computer science students to investigate their perceptions of various automated hiring technologies and how their strategy use in this landscape impacts these perceptions as well as job success.

\subsection{Participants \& Recruitment}
After an initial power analysis for small to moderate effect size ($0.25-0.30$) suggested we should recruit at least 170-250 participants, we surveyed 525 computer science undergraduate and graduate students in two phases. Across both phases of recruitment, the recruitment message described the research team as being interested in understanding fairness in automated hiring and hoping to share what they learn to inform better automated hiring practices and career service support. Participants were informed that the survey would take 15 minutes and ask them to respond to scenarios about different hiring practices and describe their recruitment experiences. The University of Pennsylvania's Institutional Review Board approved our initial study, along with modifications when recruitment was expanded to other schools. Additionally, we pre-registered our hypotheses and analyses on Open Science Framework~\cite{armstrong2023computing}.

\subsubsection{Recruitment Phase 1. }In the first phase, 294 participants were recruited from the University of Pennsylvania, a private university in Philadelphia. These students were enrolled in an upper-level artificial intelligence course, and were given extra course credit for completing our survey. This was initially intended to be the full scope of the study, but we ultimately elected to delay analysis and instead diversify our participant sample outside of the University of Pennsylvania, expanding it to two additional universities in the same city. 

\subsubsection{Recruitment Phase 2. }Lacking connection to a specific course with an extra credit research incentive (as we were able to do in the first phase of recruitment), we instead reached out to professors at two other institutions who distributed our survey to their students with a monetary incentive. We offered to enter participants into a gift card raffle with limited success. In pursuit of obtaining enough participants at the other universities to balance the nearly-300 from the University of Pennsylvania, we used several additional strategies. These included recruiting through flyers on those campuses, emailing computing clubs' leadership and faculty colleagues at the two institutions, posting on computing employment-related social media forums, contacting administrators related to school-wide surveys and job boards, and even tabling on those campuses near their computer science buildings. We also emailed all previous participants encouraging them to refer a friend with an incentive of another entry in the gift card raffle. We found more success when we reached out again to professors to distribute the survey to their students with another non-monetary incentive, individualized feedback on students' resume. All strategies considered, we were able to get an additional 231 responses. 

\subsubsection{Data Cleaning. }Our 525 survey responses were reduced to 448 after removing unfinished surveys ($n=31$), respondents who were not computing students at the University of Pennsylvania, Temple University, or Drexel University ($n=16$), those who failed attention checks ($n=28$), and duplicate email addresses ($n=2$). After data cleaning, we analyzed 284 responses from the University of Pennsylvania, 153 from Temple University and 11 from Drexel University. Note that in the following analyses (Section \ref{sec:results}) we do not analyze for differences by university. Due to the different recruiting strategies used, students at the University of Pennsylvania were predominantly upper-level, while those at Temple University and Drexel University were more junior, so we found it inappropriate to compare by this attribute. We also collected information about whether students were applying for jobs and whether their job-searching process was complete or still underway.

\begin{table}[t]
\begin{tabular}{lllll}
\cline{1-2} \cline{4-5}
\textbf{Gender}               & \textbf{Count} &  &\textbf{ Race \& Ethnicity}               & \textbf{Count} \\ \cline{1-2} \cline{4-5} 
Agender              & 3     &  & American Indian or Alaskan Native      & 1     \\
Genderqueer          & 2     &  & Asian American or Asian         & 251   \\
Man                  & 238   &  & Black or African-American       & 46    \\
Non-binary           & 6     &  & Hispanic or Latinx              & 18    \\
Woman                & 167   &  & Middle Eastern or North African & 8     \\
Prefer Not to Answer & 36    &  & Pacific Islander                & 1    \\ \cline{1-2}
                     &       &  & White or Caucasian              & 82    \\
                     &       &  & Prefer Not to Answer            & 40    \\ \cline{4-5} 
\end{tabular}
\vspace*{3mm}
\caption{\label{tab:gender/race} Participant demographics for gender and race \& ethnicity}
\end{table}

\subsubsection{Demographics. }At the end of the survey, participants were asked to describe their gender and racial/ethnic identities (summarized in Table ~\ref{tab:gender/race}). Demographic information questions were created based on inclusive survey guidance~\cite{fernandez2016more} and allowed participants to optionally choose zero, one, or multiple categories that best described them. 250 participants opted to share their family's annual household income, ranging from \$0 to \$1.5 million with a median of \$100,000 (see Figure ~\ref{fig:income}). 

\begin{figure}[t]
    \centering
    \includegraphics[width = 0.7\textwidth]{figures/income.png}
    \caption{Participant self-reported household income from \$0 to \$1,500,000, with a median of \$100,000.}
    \label{fig:income}
    \Description[Histogram of participant self-reported household]{Histogram of participant self-reported household income from \$0 to \$1,500,000, with a median of \$100,000.}
\end{figure}

\subsection{Survey}
The survey contained two main parts: participant evaluations of 15 hiring scenarios (Part 1), and questions about participants' own experience with automated hiring processes (Part 2), in addition to demographic questions. For the full survey, see Appendix \ref{app:survey}.

\subsubsection{Perceptions}
The first part of the survey assessed participants' perceptions of procedural fairness in and willingness to be evaluated by 15 different hiring scenarios. These were made up of three \textbf{evaluation types} (technical coding assessment, resume reviewing, and behavioral interviews) and five different \textbf{automation levels} (human-only reviewing, human-AI reviewing equally, human reviewers for AI rejections, human reviewers for AI acceptances, AI-only reviewing). 

For example, the five scenarios pertaining to an \textit{online coding assessment} evaluation type were: 

\begin{enumerate}
  \item \textit{Human-only reviewers:} An applicant submits a sample of code, which is reviewed by a recruitment team who determines whether the applicant advances to the next phase.
  \item \textit{Human-AI reviewing equally:} An applicant is given an online coding assessment, which is evaluated by an algorithm. If the applicant reaches a certain score on the autograder, the applicant advances to the next phase. All algorithmic decisions are reviewed by a recruitment team.
  \item \textit{Human reviewers for AI rejections:} An applicant is given an online coding assessment, which is evaluated by an algorithm. If the algorithm rejects the applicant, the decision is reviewed by a recruitment team.
  \item \textit{Human reviewers for AI acceptances:} An applicant is given an online coding assessment, which is evaluated by an algorithm. If the algorithm advances the applicant, the decision is reviewed by a recruitment team.
  \item \textit{AI-only reviewing:} An applicant is given an online coding assessment, which is evaluated by an algorithm that determines whether an applicant advances to the next phase.
\end{enumerate}

% True positives are when the organization hires a candidate who is a job match. True negatives are when the organization rejects a candidate who is not a match. False positives are when the algorithm accepts the candidate, but the candidate wasnâ€™t a good fit. Lastly, false positives are when the algorithm rejects a candidate, but the candidate was qualified. 

The evaluation types were chosen based on prior work interviewing computing students about the types of automation they encountered in the hiring process~\cite{armstrong2023navigating} and on the most prevalent automated employment decision tools (AEDTs) for the screening of candidates~\cite{sanchez2020does, bogen2018help}. We chose to focus on technical coding assessments, resume reviewing, and behavioral interviews where applicants receive limited transparency and feedback. Prior work has shown that young job seekers, especially in computing, must navigate automated hiring process more frequently in earlier hiring stages before progressing to human interviews and that some are even hired through entirely automated processes~\cite{armstrong2023navigating, sanchez2020does, bogen2018help}. The different automation levels were chosen based on prior work on perceptions of automated decision-making~\cite{zhang2022examining, lee2018understanding,binns2018s}. We chose to include scenarios for human review of both AI rejections and acceptances, since previous work suggests that both conditions are important to applicants~\cite{friedrich1993primary, roulin2014interviewers, morse2021ends, de2021use}. 

Participants responded to each scenario on a five-point Likert scale, rating both \textbf{procedural fairness}:``this hiring process seems fair'' (1: Strongly disagree -- 5: Strongly agree) and \textbf{willingness}: ``I want to be evaluated this way'' (1: Strongly disagree -- 5: Strongly agree). These options were determined based on best practices with Likert scale use in learning about perceptions ~\cite{jebb2021review, joshi2015likert} and understanding fairness perceptions in automated systems~\cite{lee2019procedural, schoeffer2021appropriate, morse2021ends}. This practice of soliciting and aggregating responses to understand procedural fairness perceptions is consistent with prior CSCW and related work on perceptions of AI systems~\cite{lee2019procedural, lee2018understanding, zhang2022examining, gonzalez2022allying}.

\subsubsection{Awareness of and Experiences with Automated Hiring}
The second part of the survey asked participants to answer questions about their own awareness of AI systems in hiring. They were asked about whether they had experienced or heard about several specific AEDTs (online coding assessments, automated resume readers, and automated video interviews) along with descriptions of those tools. Participants chose from four possible responses (had experienced it, had heard of it but not experienced it, had not heard of or experienced it, or did not know). They also answered whether ``I know how my data was used in the hiring process'' and ``I received feedback from automated hiring algorithms'' on a five-point Likert scale (1: Strongly disagree --- 5: Strongly agree).

They were also asked yes/no questions about their own strategies for navigating the job market, including whether they had modified resumes with keywords, used tools like LeetCode to practice for online coding assessments, gotten referrals, or had family and friends who worked at companies. These strategies were chosen based on prior work interviewing students about the strategies they used to navigate AEDTs~\cite{armstrong2023navigating} as well as past work that resume scanners rely on parsing with natural language processing~\cite{sanyal2017resume, harsha2022automated, bharadwaj2022resume} and keywords in job descriptions~\cite{dhende2018candidate, rhea2022resume, satheesh2020resume} to assess candidate matches. We also asked whether they were currently applying to jobs (300 of our 448 final participants were), how many companies they had applied to, and whether they had received any job offers. Applicants who reported they were not applying for jobs were excluded from analysis about strategy use and job outcomes.

At the end of the survey, participants optionally provided demographic information about their gender, race, and annual household income, factors we hypothesized could play a role in their job market success based on prior work on perceptions of automated decision-making~\cite{lee2018understanding, wang2020factors}. 