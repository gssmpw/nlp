\section{Dataset Construction}\label{sec:benchmark-construction}
\input{figs/workflow}

To build \benchname, we design a pipeline implementing an automated workflow in \Cref{fig:workflow} to collect user-submitted solutions on LeetCode and distill pseudocode solutions from them using a recent reasoning model DeepSeek-R1. The pipeline may also be adapted to refurbish other existing code generation benchmarks.

\smalltitle{Data Source}
To lessen the data leakage threat, we select user-submitted solutions based on the problems most recently collected by \livecb~\cite{livecb}. These are the latest programming problems released after the training cut-off dates of popular LLMs.
In other words, we select the most recent subset of problems indexed by \livecb at LeetCode. We further collect the 
corresponding user-submitted solutions from LeetCode.
For each problem, we manually collect the most popularly voted solutions in Python, C++, and Rust, respectively.


\smalltitle{Task Cleaning}
To ensure the correctness of the collected user-submitted solutions,
we run each solution via the LeetCode online judge to ensure the solution passes all mandated tests.
If the most popularly voted solutions fail (usually due to the update of problems/tests), we collect another solution that passes the updated tests.
The study of our research questions requires evaluating the correctness of many generated codes. Submitting all of them to the LeetCode online judge for correctness validation is inappropriate. Therefore, we collect the published tests deduced by \livecb and use them to evaluate the correctness of the generated codes in our study. However, these \livecb tests are deduced by LLMs and subject to noises. %
We consider a deduced \livecb test noisy if it fails the collected solutions. %
In total, we find 16 noisy instances and exclude them from our study. 
After cleaning, we collect 365 solutions in C++ and Python and 351 solutions in Rust.

\smalltitle{Code to Pseudocode}
Each pseudocode used to evaluate the coding capability of LLMs is generated by the reasoning model \dsr~\cite{ds-r1} given a solution code and a detailed list of rules (\Cref{sec:prompts}) that the output pseudocode needs to satisfy, i.e., the criteria in \Cref{subsec:criteria}.
For example, the pseudocode should not contain explicit types like 32-bit or 64-bit integers and language-specific operations like \codef{yield} in Python.

We choose a reasoning model over a chat model like GPT-4o. Our pilot experiments find that chat models often fail to obey the rules in a long context or just write the pseudocode line by line without undergoing a substantial thinking process.
The prompt we use consists of only the user query without a system message or few-shot examples, as suggested by the DeepSeek team~\cite{ds-r1}.
We also follow their experiment setting (\codef{temperature=0.6}, \codef{top\_p=0.95}).
One pseudocode sample is obtained for each selected user-submitted solution due to the limited access to the R1 service and the incurred time latency.

\smalltitle{Pseudocode Quality Assessment}
{To remove incorrect R1-generated pseudocode, we use LLMs to generate code from the R1-generated pseudocode using our study setup and remove the tasks where \emph{NO LLMs} can pass the task with ten attempts.
Finally, we remove 22 subjects where R1 hallucinates a pseudocode with incorrect logic (e.g., adding an incorrect condition), and keep 1,059 subjects.
Besides, we compare the lengths and effectiveness of pseudocode annotated by R1 and humans for randomly sampled subjects in RQ4. The results also suggest good quality of the retained pseudocode.} %
