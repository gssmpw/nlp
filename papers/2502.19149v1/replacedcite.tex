\section{Related Work}
\label{sec:related-work}


\smalltitle{Benchmarking End-to-End Code Generation}
Various benchmarks have been developed to assess LLMs in end-to-end code generation -- %
some benchmarks {\textit{broader the programming languages}} to evaluate. Classical benchmarks focus on Python programming ____; later, benchmarks considering other programming languages, e.g., Java ____ and even multilingual ____, emerge. %
Some studies evaluate LLM programming {\textit{across different contexts}}, %
such as class-level ____, project-level ____, and repository-level ____, pushing the boundaries of LLM capabilities in real-world scenarios.
The performance of generating \textit{{code in different domains}} also attracts studies ____. %
Several recent studies explored LLMs' code-generation capabilities {\textit{incorporating external techniques}}, for example, using RAG to retrieve codes ____ and documents ____, allowing LLMs to code with external resources.

Though these studies assess LLM's performance in various scenarios, they reveal relatively limited information about LLM's ability at steps within the end-to-end pipeline, e.g., coding a solution logic.

\smalltitle{Benchmarking Code Generation Using Pseudocode}
Only a few works have studied translating pseudocode into code. ____ propose a conceptual framework that breaks down pseudocode into XML elements. ____ explore potential mappings of pseudocode and C++ code using test cases. The SPoC dataset with 18K line-to-line mappings is built in the work. However, the fairly trivial line-by-line pseudocode may not accurately reflect the human-written pseudocode typically appearing in real-world software development. SPoC was later utilized by ____ to train two basic deep-learning models for pseudocode-to-code translation.
These studies worked on relatively small and trivial pseudocode snippets. They also barely compared the performance of code generators (in particular the advanced LLMs) or discussed the detailed abilities. %