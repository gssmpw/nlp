\section{Related Work}\label{sec:related-work}


\smalltitle{Benchmarking End-to-End Code Generation}
Various benchmarks have been developed to assess LLMs in end-to-end code generation -- %
some benchmarks {\textit{broader the programming languages}} to evaluate. Classical benchmarks focus on Python programming \cite{humaneval, mbpp}; later, benchmarks considering other programming languages, e.g., Java \cite{JavaBench} and even multilingual \cite{humanevalx}, emerge. %
Some studies evaluate LLM programming {\textit{across different contexts}}, %
such as class-level \cite{classeval}, project-level \cite{deveval}, and repository-level \cite{repocoder, repoeval}, pushing the boundaries of LLM capabilities in real-world scenarios.
The performance of generating \textit{{code in different domains}} also attracts studies \cite{domaineval}. %
Several recent studies explored LLMs' code-generation capabilities {\textit{incorporating external techniques}}, for example, using RAG to retrieve codes \cite{ase24retrieval_repo, codesearchallyouneed_hu} and documents \cite{docragarxiv, docragccwan}, allowing LLMs to code with external resources.

Though these studies assess LLM's performance in various scenarios, they reveal relatively limited information about LLM's ability at steps within the end-to-end pipeline, e.g., coding a solution logic.

\smalltitle{Benchmarking Code Generation Using Pseudocode}
Only a few works have studied translating pseudocode into code. \citet{Dir17} propose a conceptual framework that breaks down pseudocode into XML elements. \citet{Kul19} explore potential mappings of pseudocode and C++ code using test cases. The SPoC dataset with 18K line-to-line mappings is built in the work. However, the fairly trivial line-by-line pseudocode may not accurately reflect the human-written pseudocode typically appearing in real-world software development. SPoC was later utilized by \citet{Ach22} to train two basic deep-learning models for pseudocode-to-code translation.
These studies worked on relatively small and trivial pseudocode snippets. They also barely compared the performance of code generators (in particular the advanced LLMs) or discussed the detailed abilities. %




