\section{Experiment}\label{sec:experiment}



\smalltitle{Data}
To facilitate the comparison across programming languages, 999 (333 $\times$ 3) experiment subjects are drawn from the intersected programming tasks for C++ (355), Python (357), and Rust (348).

\smalltitle{Metrics}
The correctness of the generated programs is calculated by their \passk rates on the tests published by \livecb. %
The conciseness of pseudocode is measured by their lengths regarding the number of Byte-Pair Encoding (BPE) tokens and lines of codes.

\input{tables/overall}

\smalltitle{Studied LLMs}
We study the code generation performance of ten diverse popular LLMs, including Qwen-series (Qwen-2.5-Coder 7B, 14B, 32B, 32B-Int(q)4) \cite{qwen2.5-coder}, Gemma-series (Gemma-2-9b) \cite{google-gemma-2}, Llama-series (Llama-3.1-8B and -3.2-3B) \cite{meta-llama-3-2}, Phi-series (Phi-4-14B and -3.5-4B) \cite{microsoft-phi-4}, and GPT-series (GPT-4o-mini) \cite{gpt-4o-mini-report}. Relatively more LLMs evaluated are under 15B parameters. This is to understand the {language-coding ability} of lighter, more deployable models.%

\smalltitle{Parameters}
We follow the setting suggested by LiveCodeBench to sample ten times of generations for each problem using a temperature of $0.2$ and top\_p of $0.95$.
We compare one-shot and zero-shot prompts for pseudocode-based code generation.

\smalltitle{Experiment Environment}
The experiments are conducted on a Linux server with two NVIDIA RTX 6000Ada GPUs. The commercial GPT-4o-mini and the primary DeepSeek-R1 are accessed via API calls. Other open-weight LLMs are deployed locally on the server with the vLLM engine.


\subsection{RQ1: Overall Performance}
\input{tables/compare-lang}



To understand the {language-coding capability} of LLMs, we analyze the quality of the codes that LLMs generated from pseudocode. 
Each column in \Cref{tab:overall-result} presents the Pass@1 rate of LLMs in generating programs of a specified programming language based on the pseudocode derived from the solution codes in the same language.
We also list LLMs' Pass@1 rates when directly generating codes from problem descriptions using the prompt adopted by LiveCodeBench as a reference.


\smalltitle{Effect of Pseudocode} 
All ten LLMs achieve significantly higher Pass@1 rates on all programming languages when generating programs from pseudocode than from problem descriptions. 
Specifically, the overall Pass@1 rates on all difficulties increase from 0.38, 0.35, 0.28 to 0.75, 0.66, 0.46 on Python, C++, and Rust, respectively.
The results suggest that the solutions encoded in pseudocode help LLMs generate more correct programs.
As such, we consider that \textit{problem-solving ability is a key bottleneck common to LLMs}.
Regarding programming languages, all LLMs exhibit the largest performance gain in Python programming (+99\% on average), followed by C++ (+87\% on average). Performance improvement on Rust is the least (67\% on average) yet is still significant. As Rust coding is lower in resource availability \cite{humanevalx,cao2025should}, the result suggests \textit{the correlation between language-coding ability and the prevalence of the language in corpus}.

\smalltitle{Language-Coding Capability} 
LLMs' language-coding capability varies across programming languages. 
Given pseudocode, most LLMs can generate correct implementations in Python; while they still cannot generate correct Rust implementations for many tasks. For example, given pseudocode, the Python, C++, and Rust Pass@1 rates are 0.89, 0.82, 0.63 for GPT-4o-mini and 0.85, 0.74, 0.55 for Phi-4, respectively.
It suggests that \textit{the bottleneck of LLMs in code generation is problem-solving, while as to Rust and C++, they are struggling relatively more in language coding}. 

\smalltitle{Difficulty-Wise} LLMs show the most improvement in Pass@1 rates on the hard tasks (573\%$\uparrow$, 334\%$\uparrow$, and 370\%$\uparrow$ in Python, C++, and Rust, respectively), followed by the medium tasks (160\%$\uparrow$, 139\%$\uparrow$, and 107\%$\uparrow$) and then the easy tasks (31\%$\uparrow$, 34\%$\uparrow$, and 29\%$\uparrow$). Since hard tasks depend more on problem-solving ability, the result echos our conclusion that problem-solving is a key bottleneck.

\smalltitle{Model-Wise} 
Given pseudocode, the Python Pass@1 rates of the best-performing studied LLM, QWen32B (and its quantified variant QWen32Bq4), on easy, medium, and hard tasks significantly increase from around 0.90, 0.56, 0.20 to 0.93, 0.94, 0.79, respectively, followed by QWen14B (0.80, 0.51, 0.14 $\rightarrow$ 0.97, 0.90, 0.70) and GPT-4o-mini (0.82, 0.32, 0.07 $\rightarrow$ 0.95, 0.88, 0.76). Similar trends are observed in C++ and Rust.
This indicates such powerful LLMs likely have mature language-coding capabilities, particularly in Python.
Most of their bottleneck in solving LiveCodeBench tasks may reside in the problem-solving procedure. 
In comparison, although the smaller models show improvement in Pass@1 rates given pseudocode, their generations based on pseudocode are still error-prone.
For example, almost all Pass@1 rates on Llama-3.1-8B, Llama-3.2-3B, and Phi-3.5-4B for medium and hard tasks are still below 0.50. 
This suggests that \textit{regarding the ability to implement a given programming logic, smaller LLMs are much inferior to the larger LLMs}. 








\smalltitle{Worsening Cases}
We noticed a few cases where providing pseudocode degrades the models' performance. 
For example, when writing Python code in 16 problems, %
GPT-4o-mini shows a lower Pass@1 when referring to pseudocode than solving problems directly.
Our analysis of the failure cases suggests that in some cases (e.g., \Cref{subsec:worsening}), LLM fails to understand expressions like ``cumulative sums'' of an array/list indicated in the pseudocode, which is expected to be implemented with \codef{accumulate()}. Such expressions may conversely mislead LLMs who were able to reason a correct solution from problem descriptions by themselves, particularly for easy problems.
In practice, ambiguous natural language expressions are inevitable in pseudocode or instructions, despite the semi-structured format of pseudocode. It is an interesting future work to detect and fix such noise.

In general, these interesting findings help us understand the language-coding ability of LLMs. They echo our motivation to gain a clearer picture of LLMs' coding capabilities by isolating the evaluation of their problem-solving and language-coding abilities by introducing pseudocode.






\subsection{RQ2: Cross-Programming Language}

Recalling that pseudocode abstracts language-specific details of solutions (\Cref{subsec:criteria}), we further investigate if pseudocode manifests generalizable effectiveness such that the pseudocode derived from solution code in a programming language can benefit LLMs in generating codes in another language.
\Cref{tab:compare-lang} presents the comparison. 

\smalltitle{Language-Wise} The pseudocode derived from solution codes written in any programming language ($P_{lang}$) effectively help LLMs gain much higher Pass@1 rates in comparison to the performance of generating from problems listed in \Cref{tab:overall-result} (e.g., 0.75{\textasciitilde}0.76 \textit{v.s.} 0.38 Pass@1 in Python generation). The result suggests that \textit{pseudocode can serve as a language-agnostic representation to hint LLMs about solution logic and guide LLMs generating programs in various programming languages}, which may shed light on cross-language tasks such as code translation and code search. 
Furthermore, we surprisingly found from the comparison among $P_{lang}$ that \textit{the pseudocode derived from C++ solutions help LLMs gain the highest Pass@1 in generating not only C++ but also Python and Rust programs on average}; meanwhile, pseudocode of Python solutions show inferior effectiveness for C++ and Rust generation.
Our manual analysis suggests the reason may be that Python codes often implement logic with various libraries, and thereby, the detailed idea to implement some features cannot be extracted into pseudocode. As a result, when there is no available corresponding library to use in C++ and Rust, the LLMs cannot correctly implement the logic. The analysis of the lengths of pseudocode derived from different programming languages also shows the trend as indicated in the 2nd{\textasciitilde}4th columns in \Cref{tab:loc-tokens}.%

\smalltitle{Model-Wise} The studied LLMs consistently gain improvement in Pass@1 rates with the help of pseudocode derived from any-language solution codes. 
Meanwhile, the most helpful programming language varies across LLMs. For example, Qwen14B and QWen32B work best when referring to the pseudocode derived from the solution code written in the target programming language, GPT-4o-mini prefers pseudocode of C++ or Rust solutions, and the others prefer C++.
This may indicate distinct LLMs need unique logic information implied in solution codes of specific programming languages.







\subsection{RQ3: Effects of Inference Strategies}
\input{figs/compare_passk}

We investigate if two typical configurations, i.e., whether using in-context learning (zero-shot $\rightarrow$ one-shot) and increasing the attempts (1$\rightarrow$5$\rightarrow$10), help improve the performance. 
\Cref{fig:passk} presents the Pass@\{1, 5, 10\} rates of LLMs when using zero-/one-shot prompts on generating C++ codes based on the pseudocode derived from C++ solutions. The results of other languages show consistent conclusions and are available at \Cref{sec:add-exp-results}.

\smalltitle{Zero-shot \textit{v.s.} One-shot}%
One-shot prompting benefits most LLMs but may disturb poorer LLMs like Phi-3.5-4B and Llama-3.1-3B which may not effectively handle long contexts. The result suggests using one-shot prompting to guide most LLMs better while driving smaller LLMs with more concise prompts. For consistency, in RQ1 and RQ2, we use one-shot prompts as a general setup for all LLMs.

\smalltitle{Pass@\{1, 5, 10\}} Increasing attempts also brings more chances of generating correct codes in the setup of pseudocode-based code generation, in particular for smaller LLMs like Llama-3.2-3B. For larger LLMs, 5 attempts may be appropriate considering cost-effectiveness.

\smalltitle{\textit{v.s.} Generating from Problem} (\textit{abbr.} direct) It is clear that Pass@1 rates of all LLMs when generating from pseudocode have already surpassed the effort of 10 attempts when generating from problem descriptions. The finding again echos our conclusion that \textit{problem-solving is the key bottleneck of current LLMs in code generation}. Besides, with pseudocode to hint the solution logic, \textit{one} attempt enables all LLMs except Phi-3.5 and Llama3B to outperform the Pass@{10} rates achieved by the commercial GPT-4o-mini generating from problems.

\subsection{RQ4: Automatically-generated v.s. Manually-written pseudocode}\label{subsec:resrq4}

\input{tables/loc-tokens}


\noindent We compare the pseudocode annotated by humans and DeepSeek-R1 on 55 sampled programming tasks. The manual annotation involved six developers with over five years of C++ coding experience; one pseudocode is annotated by an annotator and validated with another two. We mainly compare the simplicity and effectiveness of the two sets of pseudocode, which are essential qualities clear to measure based on lengths and pass rates.


\smalltitle{Simplicity} \Cref{tab:loc-tokens} lists the lengths of pseudocode annotated manually and automatically in the number of BPE tokens \cite{tiktoken} and lines of codes ignoring blank lines and comments. It is found that pseudocode, particularly the ones annotated by humans and DeepSeek-R1 (\textit{abbr.} R1), are much shorter than the source codes. This suggests that \textit{pseudocode is a simplified format to express the solution of programs}.
Besides, interestingly, although the manually annotated pseudocode include fewer lines than R1-generated ones, the manual ones are found to include more tokens than R1's. The reason is R1 tends to describe the logic more concisely.


\input{tables/compare-manual-r1-acc}


\smalltitle{Effectiveness} We compare how effectively the automatically and manually annotated pseudocode guide LLMs in generating correct codes. As presented in \Cref{tab:passk-manual-r1}, the manual pseudocode help LLMs generate more compilable codes on the validation tasks. Meanwhile, surprisingly, \textit{the pseudocode generated by R1 contribute to higher Pass@k rates than the human-written ones}. The cause may be the gap between the expression style and knowledge preferences of humans and LLMs as reported by existing studies \cite{ase24-llmpreference}. %





Based on these detailed clues, we consider current SOTA reasoning LLMs like DeepSeek-R1 an effective helper to abstract reference codes into concise pseudocode with high accuracy.
They may offer feasible automation to abstract existing valuable code resources in GitHub or code generation benchmarks and facilitate studies on pseudocode.







