\section{Introduction}\label{sec:introduction}


\input{figs/motivating_example}

Large Language Models (LLMs) have exhibited impressive proficiency in aiding software development, particularly in the realm of code generation. Existing code generation benchmarks, such as HumanEval~\cite{humaneval}, typically present a natural language description (e.g., ``return a list with elements incremented by 1'') and require LLMs to generate code that fulfills the described functionality. On the HumanEval leaderboard, various LLMs have achieved scores close to perfection (at most 99.4\%~\footnote{Result on Feb 13,2024, from \url{https://paperswithcode.com/sota/code-generation-on-humaneval}}). However, on another benchmark known for minimal contamination, LiveCodeBench~\cite{livecb}, the highest score recorded is 76.5\%~\footnote{Result on Feb 13, 2024, from \url{https://livecodebench.github.io/leaderboard.html}}. The highest score drops to 52.2\% for problems in the hard category.

However, what do these scores truly imply? When scores approach perfection, does it genuinely imply that the LLMs have nearly attained the capability to replace Python developers? The answer seems to be \textit{no}. Numerous studies have revealed significant shortcomings in LLMs' code generation capabilities, such as producing code with syntactic errors, code that does not meet the intended requirements, or code with low-level implementation mistakes. Yet, merely summarizing these phenomena as ``hallucinations''~\cite{li2023halueval,xu2024hallucination,zhang2023siren,dhuliawala2023chain, issta25llmhal} is an oversimplification. We seek to understand \textbf{\textit{what the bottleneck of code generation is}} -- Is it due to a lack of \textbf{\textit{problem-solving capability}} or \textbf{\textit{language-coding capability}}, or both?



To facilitate this study, we constructed a multi-lingual code generation benchmark, \name, with 1,060 subjects with not only problem-solution pairs but also intermediate solutions represented as pseudocode, which serve to isolate the problem-solving capability from the language-coding capability.
Take Figure~\ref{fig:motivating-example} for example. Given a problem description (upper-left corner), the existing end-to-end code generation benchmarks typically examine whether the generated code (lower-left corner) is implemented correctly and report a binary result (pass or fail) as the evaluation output.
However, the binary result gives little hint of the bottleneck, i.e., it is still unclear \textbf{\textit{whether LLMs are incapable of coming up with solutions for this problem}} or \textbf{\textit{\textbf{suffering from language-specific implementation}}} such as writing syntactic- or semantic-correct code in certain programming languages such as C++ or Rust. With \name, the assessment would yield clearer results -- by breaking the end-to-end task down into two steps.
One could observe when providing the solution (Pseudocode in the middle of Figure~\ref{fig:motivating-example}), LLMs can successfully code it in three languages (Python, C++, and Rust),
while all experimental LLMs failed to solve this easy-tagged problem without the provided solution,
\textbf{\textit{indicating the bottleneck for this problem is more on the problem-solving than language-coding capability}}. Furthermore, to expand the usefulness of \name, we explore four research questions (RQs). 

\textbf{\textit{RQ1. To what extent can the provided pseudocode improve the correctness of code generation?}} This RQ provides an overall profiling of the performance from the question description and the pseudocode. Understanding performance differences with and without pseudocode across different LLMs/programming languages/difficulties of the questions helps identify \textit{the bottleneck of code generation in different programming languages}. 

\textit{\textbf{RQ2. To what extent can the solution from one programming language benefit the code generation in another programming language?}} This RQ extends the study from monolingual to multilingual observation. It explores whether the pseudocode derived from codes in one programming language could benefit the code generation in another programming language. The results could \textit{give hints of the possibility of transferring problem-solving capability across programming languages.}

\textit{\textbf{RQ3. Can different inference strategies yield significantly different observations?}} Different promptings and attempts may yield different performances. This RQ explores whether the observation of the bottleneck (problem-solving or implementation) would significantly vary under different inference strategies. 


\textbf{\textit{RQ4. What is the difference between human-written pseudocode and auto-extracted pseudocode?}} The pseudocode in \name is automatically extracted from the solution code. However, no prior study has been made to examine the quality of the pseudocode quantitatively. In this RQ, we compare the difference between the human-written pseudocode and the auto-extracted pseudocode in terms of token lengths, lines of code, and the LLMs' performance with pseudocode generated in both ways. The study provides more evidence to demonstrate the quality of the pseudocode in \name, and once assured, the auto-extraction we proposed could facilitate the extension to existing benchmarks. 




Our study yields interesting observations. First, \textbf{\textit{the bottleneck}} in Python code generation is problem-solving, while C++ and Rust struggle relatively more in language-coding. Second, most solutions are \textbf{\textit{language-agnostic}}, indicating it may be enough for LLMs to learn problem-solving skills in certain programming languages and put more effort into the coding capability in programming languages. Third, the auto-generated pseudocode is \textbf{\textit{comparable or even better}} quality than human-written ones. Thus, it is feasible to extend the existing benchmarks with pseudocode with our pipeline. The contribution of this paper includes:

{\footnotesize \ding{108}} \textbf{\textit{Problem Decomposition}}: We break down the end-to-end code generation (from problem description to implementation) into a two-step evaluation (from problem description in natural language or from solutions in pseudocode). By doing so, the bottleneck of code generation in various programming languages could be isolated and identified.

{\footnotesize \ding{108}} \textit{\textbf{Benchmark \name}}: We constructed a multi-lingual (Python, C++, and Rust) code generation benchmark with 1,060 subjects comprising not only problem description in natural language and corresponding tests, but also the intermediate solutions in the form of pseudocode. The benchmark enables exploration of the bottleneck in code generation, provides clear criteria for pseudocode construction, and makes available a pipeline that implements a workflow automating the construction process. With it, one could refurbish existing code generation benchmarks easily. 

{\footnotesize \ding{108}} \textit{\textbf{Insight}}: We isolate LLMs' capabilities for code generation into problem-solving and language-coding. Our study finds that the bottleneck of generating code in different programming languages is different. Our study further suggests that problem-solving capability may transfer across programming languages while the coding capability for programming languages beyond the most popular ones remains to be improved.


