\section{Insights from Study Results}


\indent\indent \ding{182} Code generation bottleneck differs across programming languages (PLs). %
One can improve end-to-end LLM programming performance for popular PLs like Python by boosting problem-solving abilities, whereas for less-trained languages like Rust, enhancing language-coding skills is crucial.

\ding{183} %
Problem-solving ability may transfer across PLs, which may allow LLMs' coding performance to be improved in a unified manner across PLs.

\ding{184} %
Reasoning models can effectively handle the code-to-pseudocode transformation. This enables easy creation of up-to-date benchmarks focusing on problem-solving capability, which may help relieve the current bottleneck and support cross-PL tasks.


These insights may shed light on enhancing LLMs in code generation and other cross-PL tasks, as well as guide human-LLM collaboration in the era of AI-driven low/zero-code development.


\section{Conclusion}\label{sec:conclusion}

To understand the bottlenecks in end-to-end code generation for LLMs, we introduce \name, a multilingual code generation benchmark incorporating pseudocode as input,
isolating the evaluation of language-coding from problem-solving capabilities. Empirical study results with \name reveal key insights about the bottlenecks identified for different programming languages, broad applicability of pseudocode across programming languages, and exceptional quality of automatically derived pseudocode. %

\clearpage
