\section{Limitations}
\smalltitle{Pseudocode Samples}
Due to the limited access to DeepSeek-R1, the latency of response of reasoning models, and the costs of the subsequence inference, this study only sample one pseudocode for each problem.
As revealed in \Cref{subsec:resrq4}, a small portion of the generated pseudocode could be not semantic preserving and is filtered out from the final benchmark.
The thorough study on whether sampling multiple pseudocode or using a majority vote mechanism can further improve the pseudocode quality is left as future work.


\smalltitle{Problem Domain}
The current \name selects subjects from LiveCodeBench and their solutions on LeetCode, which are mainly algorithmic code for programming puzzles.
Although this meets the purpose of using pseudocode to present algorithms in practice, the daily software development scenarios such as implementing business logic are not covered.
It is unclear whether the performance gap between problem-to-code generation and pseudocode-to-code generation is also significant in such scenarios.
The future work to understanding this problem can be extending the workflow of \name to code generation benchmarks in different scenarios.

\smalltitle{Involved Programming Languages}
The programming languages studied in this paper are Python, C++, and Rust.
They represent three popular imperative programming languages, with a major difference in the type system.
Python is dynamic, C++ is static but weakly typed, and Rust is known for having a rigorous type checking mechanism. 
The results in RQ2 may shed light on similar languages such as Java, but may not apply to functional languages such as Haskell or low-resourced languages such as domain-specific languages.

\newpage



