\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[final]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}
\usepackage{pifont}
\usepackage{graphicx}

%\usepackage{minted}
\usepackage{listings}
\usepackage{markdown}
\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mymauve}{rgb}{0.58,0,0.82}
\lstset{
backgroundcolor=\color{white},
basicstyle=\footnotesize\ttfamily,
columns=fullflexible,
breaklines=true,
postbreak=\mbox{$\hookrightarrow$\space},
columns=fullflexible,
captionpos=b,
tabsize=4,
commentstyle=\color{mygreen},
escapeinside={\%*}{*)},
keywordstyle=\color{blue},
stringstyle=\color{mymauve}\ttfamily,
frame=single,
rulesepcolor=\color{red!20!green!20!blue!20},
keywordstyle=\color{blue!70},
commentstyle=\color{red!50!green!50!blue!50},
numbers=none, 
rulesepcolor=\color{red!20!green!20!blue!20},
xleftmargin=2em,
xrightmargin=2em
}

\input{custom-commands}
\newcommand{\todoc}[2]{\textcolor{#1}{[#2]}} %
\newcommand{\scc}[1]{\todoc{violet}{SC: #1}}
\newcommand{\tian}[1]{\todoc{blue}{Tian: #1}}
\newcommand{\jr}[1]{\todoc{orange}{jr: #1}}
\newcommand{\csq}[1]{\todoc{cyan}{csq: #1}}
\newcommand{\bella}[1]{\todoc{teal}{bella: #1}}

\newcommand{\name}{\textsc{PseudoEval}\xspace}

\title{Isolating Language-Coding from Problem-Solving: Benchmarking LLMs with PseudoEval}


\author{Jiarong Wu\textsuperscript{1} \\  
  \texttt{jwubf@connect.ust.hk} \\\And
  Songqiang Chen\textsuperscript{1} \\
  \texttt{i9s.chen@connect.ust.hk} \\\And
  Jialun Cao\textsuperscript{1,*} \\
  \texttt{jcaoap@cse.ust.hk} \\
  \AND
  {\bf Hau Ching Lo\textsuperscript{1}} \\
  \texttt{hcloaf@connect.ust.hk} \\\And
  {\bf Shing-Chi Cheung\textsuperscript{1,*}} \\
  \texttt{scc@cse.ust.hk} \\
  \AND
  \textnormal{
  \text{\textsuperscript{1}The Hong Kong University of Science and Technology,
  \textsuperscript{*}Corresponding Authors}}
  }


\begin{document}
\maketitle
\begin{abstract}
Existing code generation benchmarks for Large Language Models (LLMs) such as HumanEval and MBPP are designed to study LLMs' end-to-end performance, where the benchmarks feed a problem description in nature language as input and examine the generated code in specific programming languages. However, the evaluation scores revealed in this way provide a little hint as to the bottleneck of the code generation -- whether LLMs are struggling with their problem-solving capability or language-coding capability.
To answer this question, we construct \name, a multilingual code generation benchmark that provides a solution written in pseudocode as input.
By doing so, the bottleneck of code generation in various programming languages could be isolated and identified. Our study yields several interesting findings. For example, we identify that the bottleneck of LLMs in Python programming is problem-solving, while Rust is struggling relatively more in language-coding.
Also, our study indicates that problem-solving capability may transfer across programming languages, while language-coding needs more language-specific effort, especially for undertrained programming languages.
Finally, we release the pipeline of constructing \name to facilitate the extension to existing benchmarks. \name is available at: \url{https://anonymous.4open.science/r/PseudocodeACL25-7B74/}.
\end{abstract}


\input{1-intro}
\input{2-prelim}
\input{3-construction}
\input{4-experiment}
\input{5-relatedwork}
\input{6-ending}
\input{7-after-conclusion}

\bibliography{custom}

\appendix
\input{8-appendix}


\end{document}
