% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[]{acl}
% \usepackage{acl}



\usepackage{arabtex}
\usepackage{utf8}
\setcode{utf8}
\usepackage{subcaption}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}
\usepackage[normalem]{ulem} % For robust underlining

% This is not strictly necessary and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}


% This is also not strictly necessary and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
% \usepackage{inconsolata}
\usepackage{inconsolata}
% \usepackage{zi4}
\usepackage{textcomp}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{import}
\usepackage{layout}
\usepackage{tabularx, makecell}
\usepackage{booktabs}
\usepackage{ragged2e} 
\usepackage{mathrsfs}
\usepackage{amssymb} 
\usepackage{url}
\usepackage{graphicx}
\usepackage{xspace,paralist}
\usepackage{times,latexsym}
\usepackage{amsmath}
\usepackage{appendix}
\usepackage{comment} 
\usepackage{enumitem}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage{cleveref}
\usepackage{todonotes}
\usepackage{longtable,supertabular}
\usepackage[T2A,T1]{fontenc}
\usepackage[russian,english]{babel}
\usepackage{array}
\usepackage{CJKutf8}
\usepackage{float}
\usepackage{adjustbox}

\newcolumntype{H}{>{\setbox0=\hbox\bgroup}c<{\egroup}@{}}

\newcommand{\eqnref}[1]{Eq~\eqref{#1}\xspace}
\newcommand{\tabref}[2][]{Table#1~\ref{#2}\xspace}
\newcommand{\figref}[1]{Figure~\ref{#1}\xspace}
\newcommand{\secref}[1]{Section~\ref{#1}\xspace}
\newcommand{\appref}[1]{Appendix~\ref{#1}\xspace}

\newcommand{\dataset}[1]{\text{#1}\xspace}
\newcommand{\arxiv}{\dataset{arXiv}}
\newcommand{\wikipedia}{\dataset{Wikipedia}}
\newcommand{\wikihow}{\dataset{WikiHow}}
\newcommand{\peerread}{\dataset{PeerRead}}
\newcommand{\reddit}{\dataset{Reddit}}

\newcommand{\model}[1]{\text{#1}}
% \newcommand{\model}[1]{\text{#1}\xspace}
\newcommand{\mcdropout}{\model{MC-Dropout}}
\newcommand{\chatgpt}{\model{ChatGPT}}
\newcommand{\gptfour}{\model{GPT-4}}
\newcommand{\instructgpt}{\model{InstructGPT}}
\newcommand{\gptthree}{\model{GPT-3}}
\newcommand{\davinci}{\model{davinci-003}}
\newcommand{\bard}{\model{Bard}}
\newcommand{\dolly}{\model{Dolly-v2}}
\newcommand{\bloomz}{\model{BLOOMz}}
\newcommand{\llama}{\model{Llama}}
\newcommand{\llamatwo}{\model{Llama-2}}
\newcommand{\chatglm}{\model{ChatGLM2}}
% \newcommand{\claude}{\model{Claude}}
\newcommand{\alpaca}{\model{Alpaca}}
\newcommand{\vicuna}{\model{Vicuna}}
\newcommand{\longformer}{\model{Longformer}}
\newcommand{\bert}{\model{BERT}}


\newcommand{\claude}{\model{Claude}}
\newcommand{\gptfouro}{\model{GPT-4o}}
\newcommand{\yandexgpt}{\model{YandexGPT}}
\newcommand{\kazllmseventy}{\model{KazLLM-1.0-70B}}
\newcommand{\llamaeight}{\model{Llama-3.1-8B}}
\newcommand{\sherkala}{\model{Sherkala-Chat (8B)}}
\newcommand{\llamaseventy}{\model{Llama-3.1-70B}}
\newcommand{\qwen}{\model{Qwen-2.5-7B}}
\newcommand{\falcon}{\model{Falcon3-10B}}
\newcommand{\kazllmeight}{\model{KazLLM-1.0-8B}}
\newcommand{\aya}{\model{Aya101}}
\newcommand{\vikhr}{\model{Vikhr-Nemo}}

\newcommand{\ex}[1]{\textit{#1}\xspace}







\newcommand{\multirowcellii}[1]{\begin{tabular}[c]{@{}c@{}}#1\end{tabular}}
\usepackage[most]{tcolorbox}
\usepackage[utf8]{inputenc}
\usepackage{titlesec}
\usepackage[T2A,T1]{fontenc}
\AtBeginDocument{%
  \DeclareFontFamilySubstitution{T2A}{\familydefault}{Tempora-TLF}%
}

\title{Qorǵau: Evaluating LLM Safety in Kazakh-Russian Bilingual Contexts}

\author{Maiya Goloburda$^1$\thanks{\hspace{0.2cm} These authors contributed equally.} \quad Nurkhan Laiyk$^{1*}$ \quad  Diana Turmakhan$^{1*}$  \quad \textbf{Yuxia Wang}$^{1*}$ \\   \textbf{Mukhammed Togmanov}$^1$ \quad    \textbf{Jonibek Mansurov}$^1$ \quad   \textbf{Askhat Sametov}$^1$ \\   \textbf{Nurdaulet Mukhituly}$^1$  \quad   \textbf{Minghan Wang}$^2$  \quad  \textbf{Daniil Orel}$^1$ \quad \textbf{Zain Muhammad Mujahid}$^1$  \\   \textbf{Fajri Koto}$^1$  \quad  \textbf{Timothy Baldwin}$^{1,3,4}$ \quad  \textbf{Preslav Nakov}$^1$ \\
$^1$Department of Natural Language Processing, MBZUAI \\
$^2$Monash University  \quad $^3$The University of Melbourne \quad $^4$LibrAI \\
	\texttt{\small \{maiya.goloburda,nurkhan.laiyk,diana.turmakhan,yuxia.wang\}@mbzuai.ac.ae 
	} 
}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Yuxia Wang\textsuperscript{1,2} \quad \textbf{Zenan Zhai}\textsuperscript{1} \quad Haonan Li\textsuperscript{1,2} \quad  Xudong Han\textsuperscript{1,2} \quad
% \textbf{Lizhi Lin}\textsuperscript{4,5} \\ \textbf{Zhenxuan Zhang}\textsuperscript{1} \quad  \textbf{Jingru Zhao}\textsuperscript{5} \quad 
%  \textbf{Preslav Nakov}\textsuperscript{2} \quad \textbf{Timothy Baldwin}\textsuperscript{1,2,3} \\
% \textsuperscript{1}LibrAI \qquad \textsuperscript{2}MBZUAI \qquad  \textsuperscript{3}The University of Melbourne \\
% \textsuperscript{4}Tsinghua University \qquad \textsuperscript{5}MiraclePlus \\
%   \texttt{\{yuxia.wang,zenan.zhai,haonan.li,xudong.han\}@librai.tech}
% }
% preslav.nakov, timothy.baldwin
\begin{document}
\maketitle

\begin{abstract}
    Large Language Models (LLMs) are known to have the potential to generate harmful content, posing risks to users. While major progress has been made in developing taxonomies for LLM risks and safety evaluation prompts, most studies have focused on monolingual contexts, primarily in English. However, language- and region-specific risks in bilingual contexts are often overlooked, and core findings can diverge from such in monolingual settings. In this paper, we introduce Qorǵau (meaning `\emph{to protect}' in Kazakh), a novel dataset specifically designed for safety evaluation in Kazakh and Russian, reflecting the unique bilingual context in Kazakhstan, where both Kazakh (a low-resource language) and Russian (a high-resource language) are spoken. Experiments with both multilingual and language-specific LLMs reveal notable differences in safety performance, emphasizing the need for tailored, region-specific datasets to ensure the responsible and safe deployment of LLMs in countries such as Kazakhstan. \textcolor{red}{Warning: this paper contains example data that may be offensive, harmful, or biased.}

\end{list}
\end{abstract}

\nocite{wang2024chinesedatasetevaluatingsafeguards}

\input{section/1_introduction}
\input{section/2_related_work}
\input{tables/dataset_description}

\input{section/3_dataset}
\input{section/4_evaluation}
\input{section/5_experiments}
\input{section/6_conclusion}
\input{section/limitations}
\bibliographystyle{acl_natbib}
\bibliography{ref}

\onecolumn 
\input{section/appendix}


\end{document}