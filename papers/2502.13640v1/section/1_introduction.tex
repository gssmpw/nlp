\section{Introduction}
Large Language Models (LLMs) have demonstrated impressive performance across various tasks, but they can also generate harmful or unsafe outputs, raising concerns about their responsible use. While there has been a lot of research on LLM safety, it has primarily focused on English or on monolingual region-specific environments \citep{wang2023not, wang2024chinesedatasetevaluatingsafeguards, ashraf2024arabicdatasetllmsafeguard}, and limited attention has been paid to multilingual contexts. This gap is significant because models often behave differently across languages due to variations in their training data, linguistic structure, and cultural nuances. For instance, LLMs trained predominantly on English or other high-resource languages may misinterpret regional contexts, overlook important cultural sensitivities, or fail to address the needs of users in low-resource language communities \citep{song2024multilingualblendingllmsafety}. In bilingual settings, such as in Kazakhstan, where Kazakh and Russian are both widely used, additional complexities arise as users tend to switch between Kazakh and Russian within the same sentence.
% users navigate between a low-resource language (Kazakh) and a high-resource language (Russian). 
These challenges underscore the importance of evaluating LLMs in diverse linguistic and cultural settings in order to ensure that they function reliably and safely for all users.

% Large Language Models (LLMs) have shown impressive performance across a variety of tasks but are also capable of producing harmful or unsafe outputs, raising concerns about their usage. While safety research has primarily focused on English, a few studies have started to explore specific regions. This regional focus is essential because models often behave differently across languages due to variations in training data, cultural nuances, and linguistic structures. For instance, LLMs trained predominantly on English or other high-resource languages may misinterpret regional contexts, overlook important cultural sensitivities, or fail to address the needs of users in low-resource language communities. These challenges underscore the importance of evaluating LLMs in diverse linguistic and cultural settings to ensure they function reliably and safely for all users.

In this study, we aim to bridge this gap by introducing a new dataset specifically curated for evaluating the safety of LLMs in the bilingual context of Kazakh and Russian. 
% Two languages have distinct characteristics: Kazakh is a low-resource and Russian is a high-resource language. To this end,
% We introduce a dataset curated to evaluate the safeguards of LLMs in these two languages. This dataset includes 
The dataset includes region-specific and culturally relevant prompts tailored for Kazakhstan, allowing us to assess how LLMs handle risks that are unique to this country.
% 
% We provide a comprehensive evaluation by collecting responses of 12 LLMs from both language-specific and multilingual LLMs and perform fine-grained annotations to categorize them as safe or unsafe. Our analysis highlights notable differences in the behavior of LLMs across Kazakh and Russian, emphasizing the importance of addressing safety concerns in bilingual regions like Kazakhstan, where linguistic and cultural dynamics are intertwined.
%We evaluated responses of 12 LLMs, including both multilingual LLMs (GPT-4o, Claude and Llama) and language-specific LLMs (YandexGPT, Vikhr-Nemo, Aya101).
We evaluated the responses of twelve LLMs, including both multilingual (\gptfouro, \claude, and \llama) and language-specific ones (\yandexgpt, \vikhr, \aya).
%We find Claude is the safest model for Kazakh, while YandexGPT holds the top for Russian. % In contrast, 
%Aya101 ranks the least for Kazakh, and KazLLM-1.0-8B for Russian.
We found that \claude\ was the safest model for Kazakh, while \yandexgpt\ was the safest for Russian; % In contrast, 
\aya\  and \kazllmeight\ rank lowest for Kazakh and Russian, respectively.

% Further analysis reveals that while in Kazakh multilingual models generate slightly fewer unsafe responses overall, those responses are more evenly distributed across risk types and question categories, whereas in Russian, despite producing slightly more unsafe answers, models tend to generate them for tricky or region-specific questions.
% 
% YX: Add which one is the safest? which one is least under Kazakh and Russian respectively?
We further perform fine-grained analysis of how LLMs respond to risky questions. %, categorizing their responses into safe and unsafe patterns, 
Safe responses take the form of a rejection to answer, or providing well-rounded statements or disclaimers, while unsafe responses may include harmful or misleading content, or privacy violations. The results show that for both languages, safe responses primarily offer generic information. Safe responses in Russian are more evenly distributed across six fine-grained categories, indicating that when prompting in Russian, the models we tested exhibit greater nuance than when prompted in Kazakh. 
A similar trend was observed for unsafe responses, where models prompted in Russian exhibit more diverse types of harmful content. 
These highlight notable differences of LLM behavior across Kazakh and Russian, emphasizing the importance of addressing safety concerns in bilingual regions.
%
% YX: add main findings for fine-grained classification.
% perform fine-grained annotations to categorize them as safe or unsafe. 
% Our analysis 
%, where linguistic and cultural dynamics are intertwined.
% 
% Contributions can be summarized into three folds:
This paper makes three major contributions:
\begin{compactitem}
    \item We introduce a dataset curated for LLM safety evaluation in the bilingual context of Kazakhstan using both Kazakh and Russian, with an emphasis on region-specific and culturally-relevant prompts. We also explore model behaviors when prompted by code-switched Kazakh questions, where the two languages are mixed in a single sentence.
    % a language-switched version in Kazakh to explore further the challenges posed by bilingualism and the interplay between Kazakh (a low-resource language) and a Russian (high-resource language) in model behavior.
    %, reflecting the bilingual context of Kazakhstan.
    % \item We evaluated a code-switched version in Kazakh to explore further the challenges posed by bilingualism and the interplay between Kazakh (a low-resource language) and a Russian (high-resource language) in model behavior.
    \item We collect responses from 12 LLMs and evaluate them from two perspectives: (i) safe vs. unsafe to identify the safety level;
    and (ii) fine-grained response patterns, to understand the nuanced model behavior in different languages. The automatic evaluation based on our proposed region-specific evaluation criteria showed high correlation with human annotations.
    % We also conducted a human evaluation to ensure a thorough assessment, facilitating a nuanced understanding of LLM behavior in different linguistic and cultural settings.
    % distinguishing between safe and unsafe answers. We also conducted a human evaluation to ensure a thorough assessment, facilitating a nuanced understanding of LLM behavior in different linguistic and cultural settings.
    \item We analyze the differences in LLM performance between Kazakh and Russian, demonstrating the critical need for region- and language-specific evaluations to ensure responsible deployment in region-specific contexts. Our data, code, and human annotations are publicly available at \texttt{URL\_withheld}.
\end{compactitem}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../ARR_2025"
%%% End:
