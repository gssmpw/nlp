\section{Dataset}
% \input{tables/dataset_description}
To evaluate LLM safety in Kazakh and Russian bilingual contexts, we developed a region-specific dataset based on the ``Chinese Do-Not-Answer'' dataset~(CDNA: \citet{wang2024chinesedatasetevaluatingsafeguards}). The process involved: (i) translating and localizing CDNA questions; (ii) creating \textit{region-specific prompts} tailored to the unique safety concerns of Kazakhstan and Russia; and (iii) collecting code-switched prompts. The dataset covers six risk areas (17 harm types), ensuring comprehensive coverage of potential harms. 
% Below, we detail the methods used in its creation.

\subsection{Translation and Localization}
% To create the Kazakh and Russian datasets, 
We first manually translated and localized 3042 questions in CDNA. % following the approach described in CDNA. 
Given a question, we preserved the original intent of probing LLM vulnerability on a specific risk, but adjusted names, events, conventions, and other nuances to align with cultural, linguistic, and societal characteristics of the target country. The translation and localization process was achieved with three key stages.

First, the original Chinese question was translated into English by a Chinese native speaker proficient in English. This step was crucial for accurately interpreting the meanings and subtleties of the question, creating a reliable intermediate dataset for further translation.

% all of whom are fluent in English and are graduate students specializing in NLP and ML at English-speaking institutions. 
Next, to ensure quality, eight Kazakh and Russian native speakers who are fluent in English (Master's and PhD students at English-speaking institutions) translated the intermediate English dataset into their respective mother languages. This process emphasized preserving grammatical accuracy and naturalness to ensure that the questions aligned with the linguistic norms of both languages.

Finally, we localized Kazakh and Russian questions to align with the cultural and historical contexts of Kazakhstan. This process involved replacing foreign elements, such as names, terms, organizations, and places, with culturally relevant equivalents. For example, \ex{Daisy} and \ex{John} were substituted with \ex{Aizhan} and \ex{Nurlan} (see \Cref{localizing_examples} for more examples). Additionally, context-specific questions related to Chinese history or societal structures were rephrased to reflect Kazakhstan and Russia. Questions about Chinese dynasties were replaced with topics on Kazakh history or societal traditions, ensuring cultural relevance and sensitivity. 

% It must be noted, that for non-region specific 

% , and localized Russian questions to the context of Russia, instead of Kazakhstan. This decision considers two reasons. Firstly, Russia is a close neighbor to Kazakhstan and Russian media is widely consumed in Kazakhstan, Russia-specific questions also influences Kazakhstan. Secondly, there is few Russia-specific safety evaluation datasets reflecting local culture and traditions. Our collected dataset can bridge this gap.

% This process involved replacing foreign elements, such as names, terms, organizations, and places, with culturally relevant equivalents. For example, "Daisy" and "John" were substituted with "Aizhan" and "Nurlan" (see more examples in Appendix~\ref{localizing_examples}). Additionally, context-specific questions related to Chinese history or societal structures were rephrased to reflect Kazakhstan and Russia. Questions about Chinese dynasties were replaced with topics on Kazakh history or societal traditions, ensuring cultural relevance and sensitivity. 

This resulted in a total of 5,448 questions, with 2,724 questions in each language spanning five risk areas: information hazards, malicious uses, discrimination and toxicity, misinformation harms, and human-chatbot interaction harms. 

% (see Table \ref{tab:localizing-kz-ru-ex})

% \begin{itemize}
%     \item \textbf{Replacing Foreign Elements:} Common foreign names, terms, organizations, and places were substituted with culturally relevant equivalents to align with local naming conventions and regional familiarity. For example, "Daisy" and "John" were replaced with "Aizhan" and "Nurlan" (see Table \ref{tab:localizing-kz-ru-ex}). These straightforward replacements involved categories such as names, places, and organizations.
    
%     \item \textbf{Adapting Context-Specific Questions:} Questions related to Chinese-specific contexts, such as Chinese history or societal structures, were rephrased or substituted with counterparts relevant to Kazakhstan and Russia. For instance, questions about Chinese dynasties were replaced with those focusing on Kazakhstan history or societal traditions to ensure cultural relevance and sensitivity (see Table \ref{fig:localizing-kz-ru-ex}). These more nuanced adjustments often required rephrasing and included categories such as cultural references, local terms, and significant events.
% \end{itemize}



\subsection{Region-Specific Questions}
We additionally created 1,062 Kazakhstan-specific and 597 Russia-specific questions to ensure comprehensive coverage of safety concerns specific to these two countries. We include Russia-specific prompts since Russia is a close neighbor to Kazakhstan and Russian media is widely consumed in Kazakhstan, meaning Russia-specific questions also influences Kazakhstan.
These tasks were performed by six native Russian and Kazakh speakers.
For each Kazakhstan-specific and Russia-specific direct attack question (``Ori''), we created corresponding indirect attacks (reworkings of the prompt which appear benign but are intended to elicit unsafe content: ``FN'') and over-sensitivity assessments (reworkings of the prompt which appear harmful, but are not intended to elicit unsafe content: ``FP''), following the approach of the CDNA dataset. Detailed instructions for annotators regarding each question type creation are provided in Appendix \ref{annotator_guidelines}. % for reference.

Then, we manually translated Kazakhstan-specific questions to Russian without any localization to Russia, and included them into Russian dataset. %, so that Russian dataset can reflect sensitive topics in both Kazakhstan and Russia. Since Russian is spoken in both countries, it should reflect two countries' sensitivities. 
Given that Kazakh is not spoken in Russia, the Kazakh dataset does not reflect Russia-specific safety regulations.
This resulted in a total of 3,786 and 4,383 examples in Kazakh and Russian, respectively.
\Cref{tab:kazakh-russian-data} provides a detailed statistical distribution of the three types of questions across six risk areas and 17 harm types.

% To further enhance the dataset, new region-specific Kazakh questions were translated into Russian, recognizing that Russian is widely spoken in Kazakhstan. These translations were human-verified to ensure grammatical accuracy and natural phrasing but were not further localized, as the topics in the Kazakhstani dataset were already relevant for Russian-speaking users due to the bilingual nature of the population. 
% Conversely, Kazakh-specific prompts were not translated into Russian, as topics unique to Kazakhstan were not deemed contextually significant for users in Russia. 



\subsection{Kaz-Ru Code-switching Questions}

The rich bilingual environment in Kazakhstan makes it natural for
% the prevalence of bilingualism is a defining characteristic of its linguistic landscape, with 
many individuals to seamlessly mix Kazakh and Russian in daily communication~\cite{Zharkynbekova2022}. There is currently no large-scale safety evaluation dataset containing code switching, so the impact of this phenomenon on LLM safeguarding is unknown.
To this end, we randomly sampled 500 Kazakh--Russian direct attack question pairs from the dataset, ensuring that each pair conveys the same meaning.
% we sampled 500 direct attack questions in both Kazakh and Russian.
% YX: what does similar quries mean? what's the sampling criteria?
Based on these, we created a mixed prompt that naturally mixes Kazakh and Russian, reflecting how people communicate in Kazakhstan.
This subset enables us to evaluate the safety performance of LLMs in a code-switching context, to ensure safety robustness.


% navigate bilingual interactions. To address this, we sampled 500 questions from both Kazakh and Russian question sets, ensuring they represent similar queries but with culturally nuanced differences in meaning. 
% Building on these, we developed a mixed version that naturally integrates Kazakh and Russian, reflecting how people in Kazakhstan communicate.

% In the Russian subset, there are 1,447 questions for direct attack, 1,449 for indirect attack, and 1,447 general questions with sensitive words. For the Kazakh subset, there are 1,248 questions for direct attack, 1,290 for indirect attack, and 1,248 general questions with sensitive words. This results in a combined total of 8,169 entries. Table xxxw provides a detailed breakdown of the dataset, illustrating the distribution of these three question types across six risk areas and seventeen harm types.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../ARR_2025"
%%% End:
