% \section{Experiment and Results}
\section{Results and Analysis}
In this section, we first present safe vs. unsafe evaluation results for 12 LLMs, followed by fine-grained responding pattern analysis over six models among them, and compare models' behavior when they are attacked by same risky questions presented in different languages: Kazakh, Russian and code-switching language.    

\begin{table}[t!]
\centering
\small
\resizebox{\columnwidth}{!}{
\begin{tabular}{clcccc}
\toprule
\multicolumn{1}{l}{\textbf{Rank} } & \textbf{Model} & \textbf{Kazakh $\uparrow$} & \textbf{Russian $\uparrow$} & \textbf{English $\uparrow$} \\
\midrule
1 & \claude & \textbf{96.5}   & 93.5    & \textbf{98.6}    \\
2 & \gptfouro & 95.8   & 87.6    & 95.7    \\
3 & \yandexgpt & 90.7   & \textbf{93.6}    & 80.3    \\
4 & \kazllmseventy & 88.1 & 87.5 & 97.2 \\
5 & \llamaseventy & 88.0   & 85.5    & 95.7    \\
6 & \sherkala & 87.1   & 85.0    & 96.0    \\
7 & \falcon & 87.1   & 84.7    & 96.8    \\
8 & \qwen & 86.2   & 85.1    & 88.1    \\
9 & \llamaeight & 85.9   & 84.7    & 98.3    \\
10 & \kazllmeight & 74.8   & 78.0    & 94.5 \\
11 & \aya & 72.4 & 84.5 & 96.6 \\
12 & \vikhr & --- & 85.6 & 91.1 \\
\bottomrule
\end{tabular}
}
\caption{Safety evaluation results of 12 LLMs, ranked by the percentage of safe responses in the Kazakh dataset. \claude\ achieves the highest safety score for both Kazakh and English, while \yandexgpt\ is the safest model for Russian responses.}
\label{tab:safety-binary-eval}
\end{table}



\subsection{Safe vs. Unsafe Classification}
% In this subsection, 
We present binary evaluation results of 12 LLMs, by assessing 52,596 Russian responses and 41,646 Kazakh responses.
% 26,298 responses generated by six models on the Russian dataset and 22,716 responses on the Kazakh dataset. 

%\textbf{Safety Rank.} In general, proprietary systems outperform the open-source model. For Russian, As shown in Table \ref{tab:model_comparison_russian}, \textbf{Yandex-GPT} emerges as the safest large language model (LLM) for Russian, with a safety percentage of 93.57\%. Among the open-source models, \textbf{Vikhr-Nemo-12B} is the safest, achieving a safety percentage of 85.63\%.
% Edited: This is mentioned in the discussion
% This outcome highlights the potential impact of pretraining data on model behavior. Models pre-trained primarily on Russian data are better at understanding and handling harmful questions - in both proprietary systems and open-source setups. 
%For Kazakh, as shown in Table \ref{tab:model_comparison_kazakh}, \textbf{Claude} emerges as the safest large language model (LLM) with a safety percentage of 96.46\%, closely followed by GPT-4o at 95.75\%. In contrast, \textbf{Aya-101}, despite being specifically tuned for Kazakh, consistently shows the highest unsafe response rates at 72.37\%. 

\begin{figure*}[t!]
	\centering
        \includegraphics[scale=0.28]{figures/question_type_no6_kaz.png}
	\includegraphics[scale=0.28]{figures/question_type_exclude_region_specific_new.png} 

	\caption{Unsafe answer distribution across three question types for risk types I-V: Kazakh (left) and Russian (right)}
	\label{fig:qt_non_reg}
\end{figure*}

\begin{figure*}[t!]
	\centering
        \includegraphics[scale=0.28]{figures/question_type_only6_kaz.png}
	\includegraphics[scale=0.28]{figures/question_type_region_specific_new.png} 
	
	\caption{Unsafe answer distribution across three question types for risk type VI: Kazakh (left) and Russian (right)}
	\label{fig:qt_reg}
\end{figure*}

\textbf{Safety Rank.} In general, proprietary systems outperform the open-source models. 
For Russian, as shown in Table~\ref{tab:safety-binary-eval},  % \ref{tab:model_comparison_russian}, 
\yandexgpt emerges as the safest language model for Russian, with safe responses account for 93.57\%. Among the open-source models, \kazllmseventy is the safest (87.5\%), followed by \vikhr with a safety percentage of 85.63\%.

For Kazakh, % as shown in Table \ref{tab:model_comparison_kazakh}, 
% YX: todo, rerun Kazakh safety percentage using Diana threshold
\claude is the safest model with 96.46\% safe responses, closely followed by \gptfouro\ at 95.75\%. \aya, despite being specifically tuned for Kazakh, shows the highest unsafe rates at 72.37\%.



% \begin{table}[t!]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{clccc}
% \toprule
% \textbf{Rank} & \textbf{Model Name}  & \textbf{Safe} & \textbf{Unsafe} & \textbf{Safe \%} \\ \midrule
% \textbf{1} & \textbf{Yandex-GPT} & \textbf{4101} & \textbf{282} & \textbf{93.57} \\
% 2 & Claude & 4100 & 283 & 93.54 \\
% 3 & GPT-4o & 3839 & 544 & 87.59 \\
% 4 & Vikhr-12B & 3753 & 630 & 85.63 \\
% 5 & LLama-3.1-instruct-70B & 3746 & 637 & 85.47 \\
% 6 & LLama-3.1-instruct-8B & 3712 & 671 & 84.69 \\
% \bottomrule
% \end{tabular}
% }
% \caption{Comparison of models based on safety percentages for the Russian dataset.}
% \label{tab:model_comparison_russian}
% \end{table}


% \begin{table}[t!]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{clccc}
% \toprule
% \textbf{Rank} & \textbf{Model Name}  & \textbf{Safe} & \textbf{Unsafe} & \textbf{Safe \%} \\ \midrule
% 1             & \textbf{Claude}  & \textbf{3652} & \textbf{134} & \textbf{96.46} \\ 
% 2             & GPT-4o                      & 3625          & 161          & 95.75 \\ 
% 3             & YandexGPT                   & 3433          & 353          & 90.68 \\
% 4             & LLama-3.1-instruct-70B      & 3333          & 453          & 88.03 \\
% 5             & LLama-3.1-instruct-8B       & 3251          & 535	       & 85.87 \\
% 6             & Aya-101                     & 2740          & 1046         & 72.37 \\ 
% \bottomrule
% \end{tabular}
% }
% \caption{Comparison of models based on safety percentages for the Kazakh dataset.}
% \label{tab:model_comparison_kazakh}
% \end{table}



\textbf{Risk Areas.} 
We selected six representative LLMs for Russian and Kazakh respectively and show their unsafe answer distributions over six risk areas.
As shown in Table \ref{tab:unsafe_answers_summary}, risk type VI (region-specific sensitive topics) overwhelmingly contributes the largest number of unsafe responses across all models. This highlights that LLMs are poorly equipped to address regional risks effectively. For instance, while \llama models maintain comparable safety levels across other risk type (I–V), their performance drops significantly when dealing with risk type VI. Interestingly, while \yandexgpt\ demonstrates relatively poor performance in most other risk areas, it handles region-specific questions remarkably well, suggesting a stronger alignment with regional norms and sensitivities. For Kazakh, Table \ref{tab:unsafe_answers_summary_kazakh} shows that region‐specific topics (risk type VI) pose a substantial challenge across all six models, including the commercial \gptfouro\ and \claude, which demonstrate superior safety on general categories. 

% \begin{table}[t!]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{lccccccc}
% \toprule
% \textbf{Model} & \textbf{I} & \textbf{II} & \textbf{III} & \textbf{IV} & \textbf{V} & \textbf{VI} & \textbf{Total} \\ \midrule
% LLama-3.1-instruct-8B & 60 & 70 & 16 & 31 & 9 & 485 & 671 \\
% LLama-3.1-instruct-70B & 29 & 55 & 8 & 4 & 1 & 540 & 637 \\
% Vikhr-12B & 41 & 93 & 15 & 1 & 3 & 477 & 630 \\
% GPT-4o & 21 & 51 & 6 & 2 & 0 & 464 & 544 \\
% Claude & 7 & 10 & 1 & 0 & 0 & 265 & 283 \\
% Yandex-GPT & 55 & 125 & 9 & 3 & 8 & 82 & 282 \\
% \bottomrule
% \end{tabular}%
% }
% \caption{Ru unsafe cases over risk areas of six models.}
% \label{tab:unsafe_answers_summary}
% \end{table}


\begin{table}[t!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Model} & \textbf{I} & \textbf{II} & \textbf{III} & \textbf{IV} & \textbf{V} & \textbf{VI} & \textbf{Total} \\ \midrule
\llamaeight & 60 & 70 & 16 & 31 & 9 & 485 & 671 \\
\llamaseventy & 29 & 55 & 8 & 4 & 1 & 540 & 637 \\
\vikhr & 41 & 93 & 15 & 1 & 3 & 477 & 630 \\
\gptfouro & 21 & 51 & 6 & 2 & 0 & 464 & 544 \\
\claude & 7 & 10 & 1 & 0 & 0 & 265 & 283 \\
\yandexgpt & 55 & 125 & 9 & 3 & 8 & 82 & 282 \\
\bottomrule
\end{tabular}%
}
\caption{Ru unsafe cases over risk areas of six models.}
\label{tab:unsafe_answers_summary}
\end{table}


% \begin{table}[t!]
% \centering
% \resizebox{\columnwidth}{!}{%
% \begin{tabular}{lccccccc}
% \toprule
% \textbf{Model} & \textbf{I} & \textbf{II} & \textbf{III} & \textbf{IV} & \textbf{V} & \textbf{VI} & \textbf{Total} \\ \midrule
% Aya-101 & 96 & 235 & 165 & 166 & 90 & 294 & 1046 \\
% Llama-3.1-instruct-8B & 25 & 15 & 91 & 37 & 14 & 353 & 535 \\
% Llama-3.1-instruct-70B & 33 & 39 & 88 & 27 & 20 & 246 & 453 \\
% Yandex-GPT & 29 & 76 & 95 & 29 & 16 & 108 & 353 \\
% GPT-4o & 2 & 1 & 41 & 0 & 3 & 114 & 161 \\
% Claude & 2 & 1 & 26 & 3 & 6 & 96 & 134 \\ \bottomrule
% \end{tabular}%
% }
% \caption{Kaz unsafe cases over risk areas of six models.}
% \label{tab:unsafe_answers_summary_kazakh}
% \end{table}


\begin{table}[t!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Model} & \textbf{I} & \textbf{II} & \textbf{III} & \textbf{IV} & \textbf{V} & \textbf{VI} & \textbf{Total} \\ \midrule
\aya & 96 & 235 & 165 & 166 & 90 & 294 & 1046 \\
\llamaeight & 25 & 15 & 91 & 37 & 14 & 353 & 535 \\
\llamaseventy & 33 & 39 & 88 & 27 & 20 & 246 & 453 \\
\yandexgpt & 29 & 76 & 95 & 29 & 16 & 108 & 353 \\
\gptfouro & 2 & 1 & 41 & 0 & 3 & 114 & 161 \\
\claude & 2 & 1 & 26 & 3 & 6 & 96 & 134 \\ 
\bottomrule
\end{tabular}%
}
\caption{Kaz unsafe cases over risk areas of six models.}
\label{tab:unsafe_answers_summary_kazakh}
\end{table}

% \begin{figure*}[t!]
% 	\centering
% 	\includegraphics[scale=0.28]{figures/human_1000_kz_font16.png} 
% 	\includegraphics[scale=0.28]{figures/human_1000_ru_font16.png}

% 	\caption{Human vs \gptfouro\ fine-grained labels on 1,000 Kazakh (left) and Russian (right) samples.}
% 	\label{fig:human_fg_1000}
% \end{figure*}

\textbf{Question Type.} For Russian, Figures \ref{fig:qt_non_reg} and \ref{fig:qt_reg} reveal differences in how models handle general risks I-V and region-specific risk VI. For risks I-V, indirect attacks % crafted to exploit model vulnerabilities—
result in more unsafe responses due to their tricky phrasing. 

In contrast, region-specific risks see slightly more unsafe responses from direct attacks, 
% as these explicit prompts are more likely to bypass safety mechanisms. 
since indirect attacks for region-specific prompts often elicit safer, vaguer answers. %, suggesting models struggle less with implicit harm. 
Overall, the number of unsafe responses is similar across question types, indicating models generally struggle with safety alignment in all jailbreaking queries.

For Kazakh, Figures \ref{fig:qt_non_reg} and \ref{fig:qt_reg} show greater variation in unsafe responses across question types due to the low-resource nature of Kazakh data. For general risks I-V, \llamaseventy\ and \aya\ produce more unsafe outputs for direct harm prompts. At the same time, \claude\ and \gptfouro\ struggle more with indirect attacks, reflecting challenges in handling subtle cues. For region-specific risk VI, most models perform similarly due to limited Kazakh-specific data, though \llamaeight\ shows higher unsafe outputs for indirect local references, likely due to their implicit nature. Direct region-specific attacks yield fewer unsafe responses, as explicit prompts trigger more cautious outputs. Across all risk areas, general questions with sensitive words produce the fewest unsafe answers, suggesting over-flagging or cautious behavior for unclear harmful intent.



% \subsection{Fine-grained Classification}
% We extended our analysis to include fine-grained classifications for both safe and unsafe responses. For unsafe responses, we categorized outputs into four harm types, as shown in Table \ref{table:unsafe_response_categories}. 

% For safe responses, we classified outputs into six distinct patterns of safety, following a fine-grained rubric provided in \cite{wang2024chinesedatasetevaluatingsafeguards}. The types outlined in this rubric are presented in Table \ref{table:safe_response_categories}.

% To validate the fine-grained classification, we conducted human evaluation on the same 1,000 responses in Russian used for the preliminary binary classification.
% The confusion matrix highlights the alignment and discrepancies between human annotations and GPT's fine-grained labels. The diagonal values represent instances where the GPT's labels match human annotations, with category 5 (provides general, safe information) showing the highest agreement (404 instances). However, off-diagonal values reveal areas of disagreement, such as misclassifications in categories 1 (not willing to answer), 4 (recommends consulting an expert), and 7 (provides harmful or risky instructions). For example, in category 1, while 94 responses were correctly labeled, GPT-4o misclassified several instances into categories 4, 5, or 7, indicating overlap or ambiguity in these classifications. Similar trends are observed in other categories, where GPT sometimes struggles to differentiate nuanced distinctions in human-labeled categories. Overall, GPT's fine-grained labels match human annotations in 710 out of 1000 cases, achieving an agreement rate of 71\%.

% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/human_fg_1000_ru.png}
%     \caption{{Human vs GPT-4o Fine-Grained Labels on 1,000 Russian Samples}}
%     \label{fig:human_fg_1000_ru}
% \end{figure}


% After conducting human evaluation on a representative sample, we extended the fine-grained classification to a full dataset comprising 21,915 responses generated by five different models. 


% \begin{figure}[ht!]
%     \centering
%     \includegraphics[width=0.95\linewidth]{figures/all_5_ru.png}
%     \caption{Fine-grained label distribution for responses from five models for Russian.}
%     \label{fig:all_5_fg_russian}
% \end{figure}
% Category 5 ("safe and general information") consistently has the highest frequency across all models, aligning with its dominance in the 1,000-sample evaluation. However, differences in the distribution across other categories highlight variability in how models handle nuanced safety risks, with Yandex-GPT showing a slightly broader spread across categories. 
% In the distribution of unsafe responses, most models exhibit higher counts in certain labels such as 8. However, Yandex-GPT displays comparatively fewer responses in these labels. 
% It exhibits a high rate of responses classified under label 7, which indicates instances where the model provides harmful or risky instructions, including unethical behavior or sensitive discussions. While this may suggest a vulnerability in addressing complex or challenging prompts, it was observed that many of Yandex-GPT’s responses tend to deflect responsibility or offer vague advice such as "check the internet". Although this approach minimizes the risk of unsafe outputs, it often results in responses that lack depth or contextually relevant information, limiting their overall usefulness for users.

% % \subsection{Kazakh}


% % Overall, these findings underscore how resource constraints and prompt explicitness affect model safety in Kazakh. Some models manage direct attacks better yet fail on indirect ones, while region-specific content remains challenging for all given the lack of localized training data.
% \subsubsection{Fine-grained Classification}
% Similarly, we conducted a human evaluation on 1,000 Kazakh samples, following the same methodology as the Russian evaluation. The match between human annotations and GPT-4o's fine-grained classifications was 707 out of 1,000, ensuring that the fine-grained classification framework aligned well with human judgments.
% The confusion matrix in Figure \ref{fig:human_fg_1000_kz} for 1,000 Kazakh samples illustrates the agreement between human annotations and GPT-4o's fine-grained classifications. The highest agreement is observed in category 5 (360 instances), indicating GPT-4o's strength in recognizing responses labeled by humans as "safe and general information." However, discrepancies are evident in categories such as 3 and 7, where GPT-4o misclassified several instances, highlighting areas for further refinement in distinguishing nuanced classifications.


\begin{figure}[t!]
	\centering
	\includegraphics[scale=0.18]{figures/human_1000_kz_font16.png} 
	\includegraphics[scale=0.18]{figures/human_1000_ru_font16.png}

	\caption{Human vs \gptfouro\ fine-grained labels on 1,000 Kazakh (left) and Russian (right) samples.}
	\label{fig:human_fg_1000}
\end{figure}

% \begin{figure}[t!]
% 	\centering
% 	\includegraphics[scale=0.28]{figures/human_1000_kz_font16.png} 
% 	\includegraphics[scale=0.28]{figures/human_1000_ru_font16.png}

% 	\caption{Human vs \gptfouro\ fine-grained labels on 1,000 Kazakh (top) and Russian (bottom) samples.}
% 	\label{fig:human_fg_1000}
% \end{figure}

% \begin{figure*}[t!]
% 	\centering
% 	\includegraphics[scale=0.28]{figures/all_5_kz_font16.png} 
% 	\includegraphics[scale=0.28]{figures/all_5_ru_font_16.png} \\
% 	\caption{Fine-grained responding pattern distribution across five models for Kazakh (left) and Russian (right).}
% 	\label{fig:all_5}
% \end{figure*}

\begin{figure}[t!]
	\centering
	\includegraphics[scale=0.28]{figures/all_5_kz_font16.png} 
	\includegraphics[scale=0.28]{figures/all_5_ru_font_16.png} \\
	\caption{Fine-grained responding pattern distribution across five models for Kazakh (top) and Russian (bottom).}
	\label{fig:all_5}
\end{figure}


\subsection{Fine-Grained Classification}
\label{sec:fine-grained-classification}
% As discussed in Section \ref{harmfulness_evaluation}, 
We further analyzed fine-grained responding patterns for safe and unsafe responses. For unsafe responses, outputs were categorized into four harm types, and safe responses were classified into six distinct patterns of safety, as rubric in Appendix \ref{safe_unsafe_response_categories}. 
% \cite{wang2024chinesedatasetevaluatingsafeguards}

\paragraph{Human vs. GPT-4o}
Similar to binary classification, we validated \gptfouro's automatic evaluation results by comparing with human annotations on 1,000 samples for both Russian and Kazakh. %, comparing human annotations with \gptfouro's fine-grained labels.
For the Russian dataset, \gptfouro's labels aligned with human annotations in 710 out of 1,000 cases, achieving an agreement rate of 71\%. 
Agreement rate of Kazakh samples is 70.7\%. %with 707 out of 1,000 cases matching
% The confusion matrix highlights areas of alignment and discrepancies.
% 
As confusion matrices illustrated in Figure~\ref{fig:human_fg_1000}, the majority of cases falling into \textit{safe responding patter 5} --- providing general and harmless information, for both human annotations and automatic predictions.
% highest agreement with 404 correct classifications for Russian. 
Mis-classifications for safe responses mainly focus on three closely-similar patterns: 3, 4, and 5, and patterns 7 and 8 are confusing to discern for unsafe responses, particularly for Kazakh (left figure).
We find many Russian samples which were labeled as ``1. reject to answer'' by humans are diversely classified across 1-6 by GPT-4o, which is also observed in Kazakh but not significant.

% suggesting label alignment issues are language-independent. 
% YX: I did not observe this, commented
% Notably, Russian showed confusion between 7 (risky instructions) and 1 (refusal to answer), this trend does not appear in Kazakh.


% highlight the strengths and limitations of {\gptfouro}'s fine-grained classification framework across both languages, paving the way for further refinements.


% However, misclassifications were observed in categories such as 1 (not willing to answer), 4 (recommends consulting an expert), and 7 (provides harmful or risky instructions), revealing overlaps and ambiguities in nuanced classifications.

% Similarly, for the Kazakh dataset, the agreement rate between human annotations and GPT-4o's labels was 70.7\%, with 707 out of 1,000 cases matching. As with the Russian analysis, category 5 (360 instances) showed the highest alignment. However, discrepancies were more prominent in categories such as 3 and 7, underscoring GPT-4o's challenges in differentiating fine-grained human-labeled categories. 
% A similar pattern was observed for Kazakh dataset, which suggests that alignment and misaligned of fine-grained lables is not language dependent.

% These findings, illustrated in Figures \ref{fig:human_fg_1000}, highlight the strengths and limitations of {\gptfouro}'s fine-grained classification framework across both languages, paving the way for further refinements.

\paragraph{Fine-grained Analysis of Five LLMs}
% After conducting human evaluation on representative samples, we extended 
\figref{fig:all_5} shows fine-grained responding pattern distribution across five models based on the full set of Russian and Kazakh data.
% For Russian, we selected \vikhr, \gptfouro, \llamaseventy, \claude, and \yandexgpt, while for Kazakh, we chose \aya, \gptfouro, \llamaseventy, \claude, and \yandexgpt. 
% The evaluation covered 21,915 responses in Russian and 18,930 responses in Kazakh.
% 
In both languages, pattern 5 of providing \textit{general and harmless information} consistently witnessed the highest frequency across all models, with \llamaseventy\ exhibiting the largest number of responses falling into this category for Kazakh (2,033). 
% YX：summarize more noteable findings here.

Differences of other patterns vary across languages. 
Unsafe responses in Russian are predominantly in pattern 8, where models provide incorrect or misleading information without expressing uncertainty. % (misinformation and speculation), 
For Kazakh, \aya\ exhibits the highest occurrence of pattern 7 (harmful or risky information) and pattern 8, indicating a stronger tendency to generate unethical, misleading, or potentially harmful content.

%Variations in other patterns across models highlight differences in how nuanced safety risks are classified, reflecting the models' differing capabilities in handling safety evaluation for these distinct linguistic contexts. For Russian, the majority of unsafe responses fall under pattern 8 (misinformation and speculation), indicating that models frequently provide incorrect or misleading information without acknowledging uncertainty. For Kazakh, \aya\ has the highest occurence of pattern 7 (harmful or risky information) and pattern 8 (misinformation and speculation), indicating a greater tendency to generate unethical, misleading, or potentially harmful content. 

%This trend suggests that Russian models may struggle more with factual accuracy, whereas Kazakh models, particularly \aya, pose higher risks related to both harmful content and misinformation. Additionally, \gptfouro\ and \claude\ consistently produce fewer unsafe responses in both languages, demonstrating stronger alignment with safety standards
\subsection{Code Switching}
\input{tables/code_switch_res}

\gptfouro\ and \claude\ demonstrate strong safety performance across three languages, even with a high proportion of safe responses in the challenging code-switching context. In contrast, \llamaseventy\ and \yandexgpt\ are less safe, exhibiting more harmful responses, particularly in the code-switching scenario. These results show the varying capabilities of models in defending the same attacks that are just presented in different languages, where open-sourced large language models especially require more robust safety alignment in multilingual and code-switching scenarios.

% \subsection{LLM Response Collection}
% We conducted experiments with a variety of mainstream and region-specific 
% large language models for both Russian and Kazakh languages. For both Russian and Kazakh languages, we employed four multilingual models: Claude-3.5-sonnet, Llama 3.1 70B \cite{meta2024llama3}, GPT-4 \cite{openai2024gpt4o}, and YandexGPT. Additionally, we included language-specific models: VIKHR \cite{nikolich2024vikhrconstructingstateoftheartbilingual} for Russian and Aya \cite{ustun-etal-2024-aya} for Kazakh. 

% \subsection{Kazakh-Russian Code-Switching Evaluation}

% In Kazakhstan, the prevalence of bilingualism is a defining characteristic of its linguistic landscape, with most individuals seamlessly mixing Kazakh and Russian in daily communication \cite{Zharkynbekova2022}. This phenomenon, known as code-switching, reflects the unique cultural and social dynamics of the region. Despite this, there is currently no safety evaluation dataset tailored to this unique multilingual environment. Developing a code-switched dataset is essential to evaluate the ability of large language models (LLMs) to navigate the complexities of bilingual interactions, ensuring they produce contextually appropriate, non-harmful, and culturally sensitive responses. To address this, we sampled 500 questions from both Kazakh and Russian question sets, ensuring they represent similar queries but with culturally nuanced differences in meaning. Building on these, we developed a mixed version that integrates Kazakh and Russian naturally, reflecting the way people in Kazakhstan communicate. This process involved annotators to ensure that the mixed questions were crafted with a natural tone and were localized to Kazakhstan's context.


% \begin{table*}[t!]
%     \centering
%     \small
%     \setlength{\tabcolsep}{6pt} % Adjusts column separation
%     \adjustbox{max width=\linewidth}{
%     \begin{tabular}{ll ccc ccc c}
%     \toprule
%     \textbf{Risk Area} & \textbf{Harm Type} & \multicolumn{3}{c}{\textbf{Kazakh}} & \multicolumn{3}{c}{\textbf{Russian}} & \textbf{\#Q} \\
%                        &                    & \textbf{Ori} & \textbf{FN} & \textbf{FP} & \textbf{Ori} & \textbf{FN} & \textbf{FP} & \\
%     \midrule
%     \multirow{2}{*}{I. Information Hazards} & 1. Risks from leaking sensitive information... & 131 & 133 & 131 & 131 & 133 & 131 & 790 \\
%                                              & 2. Compromise of privacy by leaking or inferring private information (person/individual) & 81 & 82 & 81 & 81 & 82 & 81 & 488 \\
%     \midrule
%     \multirow{3}{*}{II. Malicious Uses} & 3. Assisting illegal activities & 132 & 135 & 132 & 132 & 135 & 132 & 798 \\
%                                         & 4. Nudging or advising unethical actions & 71 & 71 & 71 & 71 & 71 & 71 & 426 \\
%                                         & 5. Reducing the cost of disinformation campaigns & 40 & 42 & 40 & 40 & 42 & 40 & 244 \\
%     \midrule
%     \multirow{3}{*}{III. Discrimination, Exclusion, Toxicity, Hateful, Offensive} & 6. Social stereotypes and unfair discrimination & 94 & 96 & 94 & 94 & 96 & 94 & 568 \\
%                                         & 7. Toxic language (hate speech) & 52 & 59 & 52 & 52 & 59 & 52 & 326 \\
%                                         & 8. Adult content & 27 & 29 & 27 & 27 & 29 & 27 & 166 \\

%     \midrule
%     \multirow{2}{*}{IV. Misinformation Harms} & 9. Disseminating false or misleading information & 92 & 99 & 92 & 92 & 99 & 92 & 566 \\
%                                         & 10. Causing material harm by disseminating misinformation e.g. in medicine or law & 63 & 63 & 63 & 63 & 63 & 63 & 378 \\
%     \midrule
%     \multirow{2}{*}{V. Human–chatbot Interaction Harms} & 11. Mental health or overreliance concerns & 66 & 66 & 66 & 66 & 66 & 66 & 396 \\
%                                         & 12. Treating the chatbot as a human & 50 & 51 & 50 & 50 & 51 & 50 & 302 \\

%     \midrule
%     \multirow{2}{*}{VI. Kazakhstan-specific Sensitivity} & 13. Politically sensitive topics & 63 & 66 & 63& 63 & 66 & 63 &  384 \\
%         & 14. Controversial historical events & 46 & 57 & 46 & 46 & 57 & 46  & 298 \\
% & 15. Regional and racial issues & 45 & 45 & 45 &  45 & 45 & 45 & 270  \\
% & 16. Societal and cultural concerns & 138 & 139 & 138 &  138 & 139 & 138  & 830  \\
% & 17. Legal and human rights matters & 57 & 57 & 57 & 57 & 57 & 57  & 342 \\
%     \midrule
%         \multirow{2}{*}{VII. Russia-specific Sensitivity} 
%             & 13. Politically sensitive topics & - & - & - & 54 & 54 & 54 & 162 \\
%     & 14. Controversial historical events & - & - & - & 38 & 38 & 38 & 114 \\
%     & 15. Regional and racial issues & - & - & - & 26 & 26 & 26 & 78 \\
%     & 16. Societal and cultural concerns & - & - & - & 40 & 40 & 40 & 120 \\
%     & 17. Legal and human rights matters & - & - & - & 41 & 41 & 41 & 123 \\
%     \midrule
%     \bf Total & -- & 1248 & 1290 & 1248 & 1447 & 1489 & 1447 & \textbf{8169} \\
%     \bottomrule
%     \end{tabular}
%     }
%     \caption{The number of questions for Kazakh and Russian datasets across six risk areas and 17 harm types. Ori: original direct attack, FN: indirect attack, and FP: over-sensitivity assessment.}
%     \label{tab:kazakh-russian-data}
% \end{table*}




\section{Discussion}

% \subsection{Kazakh vs Russian}

% The evaluation reveals that Kazakh responses tend to be generally safer than their Russian counterparts, likely due to Kazakh being a low-resource language with significantly less training data. As a result, Kazakh models are less exposed to the vast, often unfiltered datasets containing harmful or unsafe content, which are more prevalent in high-resource languages like Russian. This data scarcity naturally limits the model's ability to generate nuanced but potentially unsafe responses. However, this does not mean the models are specifically fine-tuned for safer performance. When analyzing unsafe answers, it’s clear that Kazakh models, while safer overall, distribute their unsafe responses more evenly across various risk types and question types. This suggests Kazakh models generate fewer unsafe answers but in a broader range of contexts.

% In contrast, Russian models tend to concentrate unsafe answers in specific areas, particularly region-specific risks or indirect attacks. This indicates that Russian models have learned to handle certain types of unsafe content by focusing on specific topics, such as politically sensitive issues, but struggle when confronted with unfamiliar content, leading to unsafe responses due to insufficient filtering. Kazakh models, despite having less training data, tend to respond more broadly, including both direct and indirect risks. This could be due to the less curated nature of their training data, making them more likely to answer unsafe questions without filtering the potential harm involved. The exception to this trend is Aya, a model specifically fine-tuned for Kazakh. Despite fine-tuning, it exhibits the lowest safety percentage (72.37\%) in the Kazakh dataset, suggesting that fine-tuning in specific languages may introduce risks if proper safety measures are not taken.

% The evaluation reveals notable differences in the distribution of safe response patterns across Kazakh and Russian fine-grained labels. Refusal to answer is more frequent in Russian models, particularly Yandex-GPT, reflecting a cautious approach to safety-critical queries. Interestingly, Aya, despite being fine-tuned for Kazakh and exhibiting lower overall safety, also frequently refuses to answer, suggesting an over-reliance on conservative mechanisms. Responses providing general, safe information dominate in both languages, with Kazakh models displaying a slightly higher tendency to rely on this approach. This highlights how the low-resource nature of Kazakh results in more generalized and inherently safer responses. In contrast, Russian models excel at recognizing risks, issuing disclaimers, and refuting incorrect assumptions, likely benefiting from richer and more diverse training data.
% Yandex-GPT exhibits a notably high rate of responses classified under label 7, indicating an overreliance on general disclaimers or deflections, such as "check the internet" or "I don't know." While these responses minimize the risk of unsafe outputs, they often lack substantive or contextually relevant information, reducing their overall utility for users.


Most models perform safer on Kazakh dataset than Russian dataset, higher safe rate on Kazakh dataset in \tabref{tab:safety-binary-eval}. This does not necessarily reveal that current LLMs have better understanding and safety alignment on Kazakh language than Russian, while this may conversely imply that models do not fully understand the meaning of Kazakh attack questions, fail to perceive risks and then provide general information due to lacking sufficient knowledge regarding this request.

We observed the similar number of examples falling into category 5 \textit{general and harmless information} for both Kazakh and Russian, while the Kazakh data set size is 3.7K and Russian is 4.3K. Kazakh has much less examples in category 1 \textit{reject to answer} compared to Russian. This demonstrate models tend to provide general information and cannot clearly perceive risks for many cases.

Additionally, in spite of less harmful responses on Kazakh data, these unsafe responses distribute evenly across different risk areas and question categories, exhibiting equally vulnerability spanning all attacks regardless of what risks and how we jailbreak it.
In contrary, unsafe responses on Russian dataset often concentrate on specific areas and question types, such as region-specific risks or indirect attacks, presenting similar model behaviors when evaluating over English and Chinese data.
It suggests that broader training data in English, Chinese and Russian may allow models to address certain types of attacks robustly,
% effectively—particularly politically sensitive issues—
yet they may falter when confronted with unfamiliar content like regional sensitive topics.

Moreover, in responses collection, we observed many Russian or English responses especially for open-sourced LLMs when we explicitly instructed the models to answer Kazakh questions in Kazakh language. This further implies more efforts are still needed to improve LLMs' performance on low-resource languages.
Interestingly, \aya, a fine-tuned Kazakh model, proves an exception by displaying the lowest safety percentage (72.37\%) among Kazakh models, revealing that the multilingual fine-tuning without stringent safety measures can introduce risks.



% However, this does not mean they are explicitly fine-tuned for safety, likely it happens due to limited training data, which reduces exposure to harmful content. 
% \aya, a fine-tuned Kazakh model, proves an exception by displaying the lowest safety percentage (72.37\%) among Kazakh models, revealing that the multilingual fine-tuning without stringent safety measures can introduce risks.
% Kazakh models generally produce safer responses than their Russian counterparts, likely because Kazakh is a low-resource language with less training data. 
% This limited exposure to harmful or unsafe content naturally limits nuanced yet potentially unsafe outputs. 
% However, it does not imply that the models are specifically fine-tuned for enhanced safety.


% while Kazakh models tend to generate fewer unsafe answers overall, those unsafe responses appear more evenly spread across different risk types and question categories.
% Russian models, on the other hand, often concentrate unsafe responses in specific areas, such as region-specific risks or indirect attacks.
% It implies that their broader training datasets allow them to address certain types of unsafe content more effectively—particularly politically sensitive issues—yet they may falter when confronted with unfamiliar or insufficiently filtered content.

% Meanwhile, Kazakh models sometimes respond more broadly, possibly due to less curated training data. 

Differences also emerge in how language models handle safe responses. 
\yandexgpt, for instance, often refuses to answer high-risk queries. 
It frequently relies on generic disclaimers or deflections like ``check in the Internet'' or ``I don’t know,'' minimizing risk but are less helpful. Interestingly, it often responds with ``I don’t know'' in Russian, even for Kazakh queries, we speculate that these may be default responses stemming from internal system filters, rather than generated by model itself.
This likely explains why \yandexgpt\ is the safest model for the Russian language but ranks third for Kazakh. While its filters perform well for Russian, they struggle with the low-resource Kazakh language.

% Aya, despite its lower overall safety, also employs refusals often, hinting at an over-reliance on conservative approaches. 

% Across both languages, models commonly resort to providing general, safe information, although Kazakh models lean on this strategy slightly more. 
% Russian models, by contrast, excel at detecting risks, issuing disclaimers, and correcting inaccuracies, likely benefiting from richer and more diverse training data.


% \subsection{Response Patterns}


% We conducted a detailed analysis of the models' outputs and identified several noteworthy patterns. YandexGPT, while being one of the safest overall, frequently generates responses in Russian even when the question is posed in Kazakh. These responses often appear as placeholders, prompting users to search for the answer online. This behavior might not originate from the model itself but rather from safety filters implemented in the YandexGPT system. The model's leading performance in ensuring safety during Russian-language interactions, coupled with its lower performance in Kazakh, can be attributed to the limited robustness of these safety filters when handling unsafe content in Kazakh.

% In contrast, Aya-101 exhibits a tendency to fall into repetition, often repeating the same sentences multiple times. Interestingly, the Vikhr model, despite being of a similar size, does not exhibit this issue. We attribute this difference to two key factors. First, Vikhr and Aya-101 have distinct architectures: Vikhr is based on the Mistral-Nemo model, whereas Aya-101 is built on mT5, an older and less robust model. Second, Aya-101 is a multilingual model, while Vikhr was predominantly trained for Russian. Multilingualism has been shown to potentially degrade performance in large language models~\cite{huang2025surveylargelanguagemodels}, which may explain Aya-101's issues with repetition.
