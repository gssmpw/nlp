\section{Introduction}
Large language models (LLMs) have shown impressive performance across many tasks that require high-level language understanding. Meanwhile, as LLMs have increasingly been adopted for practical applications, there have been growing concerns about the safety and the trustworthiness of LLM-generated content, leading to a burgeoning body of work on AI safety. Despite many LLMs being multilingual, however, there are very few non-English datasets for evaluating the safety of LLMs, and also a lack of datasets that are challenging enough to match the speed of LLMs evolution. % of LLMs.

Recently, \citet{wang2023not} proposed a comprehensive taxonomy covering diverse potential harms of LLM responses, as well as the \emph{Do-not-answer} dataset. However, the questions in this dataset are too straightforward and 90\% of them are easily rejected by six mainstreaming LLMs. 
This limits the utility of the dataset for comparing the safety mechanisms across different LLMs.
Moreover, the dataset is for English only, and is limited to questions reflecting universal human values, with no region- or culture-specific questions.

Here we aim to bridge these gaps. We first translate and localize the dataset to Mandarin Chinese, and then we expand it with region-specific questions and align it with country-specific AI generation regulations, resulting in a total of 999 questions.
We further extend the dataset from two perspectives with: (\emph{i})~risky questions posed in an evasive way, aimed at evaluating LLM sensitivity to perceiving risks; and
(\emph{ii})~harmless questions containing seemingly risky words, e.g.,~\emph{fat bomb}, aimed at assessing whether the model is oversensitive, which can limit its helpfulness. This yields 3,042 Chinese questions for evaluating LLM safety.


% and 2.7k question in English to evaluate the safegards of LLMs


Our contributions in this paper are:
\begin{compactitem}
    \item We construct a Chinese LLM safety evaluation dataset from three attack perspectives, aimed to model risk perception and sensitivity to specific words and phrases.
    \item We propose new evaluation guidelines to assess the response harmfulness for both manual annotation and automatic evaluation, which can better identify why a given response is potentially dangerous.
    \item We evaluate five LLMs using our dataset and show that they are insensitive to three types of attacks, and the majority of the unsafe responses are concentrated on region-specific sensitive topics, which determine the final safety rank of these LLMs.
\end{compactitem}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../ARR_2024"
%%% End:
