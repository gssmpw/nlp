\section{Related Work}

%\todo{Preslav: It is best to have at the end of each paragraph some summary of how it relates to the present work.}


% Lizhi summarise recent papers regarding safety and jailbreak

% For example
% two RG papers: 
% A Chinese Prompt Attack Dataset for LLMs with Evil Content
% DeepInception: Hypnotize Large Language Model to Be Jailbreaker

% Find differences between ours and others, 
% Related work aims to motivate our work, why we still do it given there has been already other studies

\subsection{Assessing Particular Types of Risk}
% \todo{Preslav: "areas" -- do you mean "types"?}

Numerous studies have been dedicated to particular types of risk, including toxicity in language models \citep{hartvigsen-etal-2022-toxigen, roller-etal-2021-recipes}, misinformation \citep{van2022misinformation, wang2023factcheckgpt, wang-etal-2024-m4}, and bias \citep{han-etal-2021-diverse,dhamala2021bold}. Specific datasets have been created to evaluate LLMs regarding these risks, such as \emph{RealToxicityPrompts} for toxicity propensity \citep{gehman-etal-2020-realtoxicityprompts}, \emph{ToxiGen} for hate speech detection \citep{hartvigsen-etal-2022-toxigen}, \emph{BOLD} for bias detection \citep{dhamala2021bold}, and \emph{TruthfulQA} for assessing factuality against adversarial prompts \citep{lin-etal-2022-truthfulqa}. These datasets provide resources for developing safer LLMs via fine-tuning and evaluating the safety of existing LLMs.

With the popularization of fine-tuning, the robustness of alignment --- and its vulnerability to fine-tuning --- is of growing concern. \citet{Wolf2023FundamentalLO} and \citet{gade2023badllama} showed that fine-tuned LLaMA  is susceptible to prompts with malicious intent, and \citet{Qi2023FinetuningAL} demonstrated similar susceptibility for GPT-3.5 Turbo even when fine-tuned on benign datasets. These findings underscore the need to evaluate a model's safety capabilities after fine-tuning. Our efforts are a step in this direction: we build an open-source dataset with fine-grained labels covering a range of risks.
% \footnote{Considering the potential risks of misusing our dataset, researchers and practitioners can apply for the access to this dataset by email, followed by signing a non-disclosure agreement.}

\subsection{Prompt Engineering for Jailbreaking}

Prompt engineering to ``jailbreak'' aligned models has been a focus of recent research~\citep{lin2024against}. This includes hand-crafting complex scenarios, such as deeply nested structures \citep{li2023deepinception, ding2023wolf}, and carefully modulated personas \citep{shah2023scalable}. However, the focus has primarily been on prompting inappropriate behaviors, with less emphasis on the characterization of the involved safety risks. In contrast, in our work, we focus on characterizing region-specific safety risks and evaluating the robustness of existing LLMs to them. 

% \todo{Preslav: Is this something you do here? If so, say it.}

To identify jailbreaking strategies at a larger scale, researchers have turned to search and optimization algorithms. \citet{zou2023universal} applied greedy and gradient-based search techniques to find suffixes that induce transferable adversarial prompts across models, while \citet{Lapid2023OpenSU} used genetic algorithms for red-teaming prompt creation. With the large search space of prompts, it is not clear that such approaches are able to generate realistic and diverse red-teaming prompts.

LLMs have also been used as scalable tools for prompt generation. For instance, \citet{liu2023goaloriented} used seed topics and techniques to create sophisticated prompts using ChatGPT, and \citet{Mehrotra2023TreeOA} applied the tree-of-thought technique to evoke reasoning capabilities and generate complex jailbreaking prompts. Here, we adopt approaches similar to those of \citet{liu2023goaloriented} to augment our dataset, by generating prompts from seed topics. We further use \gptfour to perform fine-grained evaluation of both manually-crafted and automatically-generated questions in our new Chinese dataset.

% \todo{Preslav: Is this in your dataset or in the tested LLM outputs? This is unclear. Also, when talking about your new dataset, sometimes you say "question" and sometimes you say "prompt".}

% The practice of LLM 'red teaming' in the wild, where individuals volunteer to create jailbreaking prompts, has also been studied. \citet{schulhoff2023ignore} analyzes the outcomes from a competition aimed at generating such prompts. To understand the nature of these activities, \citet{shen2023do} categorizes a vast array of prompts, and \citet{Inie2023SummonAD} proposes a grounded theory to investigate the motivation behind these attacks.

\subsection{Multilingual Risk Evaluation of LLMs}

There has been considerably less work on evaluating safety risks in non-English languages, and studies show that prompts in lower-resource languages may more readily induce unsafe behavior \citep{deng2023multilingual, yong2023lowresource, Puttaparthi2023ComprehensiveEO}. To assess such risks, \citet{wang2023languages} developed the \emph{XSafety} dataset, which covers the ten most widely-used languages in Wikipedia. The prompts are relatively short and less likely to induce adversarial behavior in current LLMs. 

For work specifically on Chinese, \citet{liu2023goaloriented} introduced the  CPAD Chinese prompt attack dataset, but focused on jailbreaking risks. They emphasized how to improve the attack success rate by designing and optimizing the prompt based on a small number of seed prompts. Questions in our dataset actually serve as the seed prompts here. 
Different from the goal of attacking, we aim to evaluate LLM safety mechanisms. %, expecting to cover as many risk/harm types as possible, especially adding region-specific questions.
\citet{sun2023safety} also aimed to assess LLM safety, but only covering 8 scenarios.
In contrast, we use a more comprehensive and hierarchical risk taxonomy, with larger coverage and better organization. Additionally, we assess whether the current value-aligned models are over-sensitive to general questions with sensitive words, which is totally outside the consideration and concerns of jailbreaking studies, and under-explored in LLM safety evaluation.

There are also existing datasets for safety alignment for Chinese LLMs, such as \emph{Baichuan} \citep{yang2023baichuan} and \emph{Qwen} \citep{qwen}, but they are not publicly available. 
In contrast, our dataset is open-source. It to some extent mitigates this gap, and promotes the development of open-source LLM safety evaluation and alignment.
% is publicly available (via email request to the authors).

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../ARR_2024.tex"
%%% End:
