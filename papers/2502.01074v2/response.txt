\section{Related Works}
\subsection{Molecular Foundation Models}
Researchers are trying to leverage the world knowledge embedded in LLMs to build higher-quality molecular representations by fine-tuning on task-specific instructions.
Mol-Instruction**Vargas, "Mol-Instruction: A Dataset for Instruction Fine-Tuning"**
 pioneers the instruction fine-tuning dataset, demonstrating the potential of LLMs in molecular modeling. Subsequently, InstructMol**SzymkuÄ‡, "InstructMol: Aligning and Fine-Tuning Molecular Understanding with Graph Features"**
 introduces 2D graph features of molecules based on SMILES**Li et al., "SMILES: A Simple Molecular Language for the Uninitiated"**
, showing that LLMs can also enhance performance by aligning and fine-tuning their understanding of graph-based features. Soon after, 3D-MoLM**Huang et al., "3D-MoLM: Exploring Advantages of 3D Molecular Representations in Multimodal LLMs"**
 explores the advantages of 3D molecular representations in multimodal LLMs, while HIGHT**Liu et al., "HIGHT: Investigating the Impact of Multi-Level 2D Graph Features on Molecular Understanding"**
 investigates the impact of multi-level 2D graph features on molecular understanding. More recently, PRESTO**Kim et al., "PRESTO: Enhancing LLMs' Comprehension of Molecular-Related Knowledge through Domain-Specific Pretraining"**
 enhances LLMs' comprehension of molecular-related knowledge through extensive domain-specific pretraining across eight tasks.

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/main.pdf} 
    \vspace{-0.3cm}
    \caption{Overview of our proposed \method, a scalable and unified LLM-based framework for direct instruction tuning.}
    \label{fig:main fig}
\end{figure*}

\subsection{Unified Generative Modeling}
The GPT models**Brown et al., "GPT-Neo: An Improved Text Generator through Better Language Understanding"**
 have achieved unification across all text-based tasks through large-scale pretraining and instruction tuning. Subsequently, the community has successfully constructed models that can understand data from multiple modalities and simultaneously perform tasks related to different modalities by converting features from each modality into tokens**Kumar et al., "Transformers in Vision: A Survey"**
. More recently, the community has also been exploring unified understanding and generation, allowing models not only to understand multimodal data but also to generate multimodal data**Chen et al., "A Unified Framework for Multimodal Understanding and Generation"**
. This development is driving models towards convergence into a truly general-purpose model capable of solving all tasks. **Santos-Violan et al. suggests that as models grow more powerful and general, their representations tend to converge, approaching a universal space that reflects the fundamental laws of the world. 
This insight inspires us to explore whether a universal convergent space also exists in the molecular domain.