%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage[table,xcdraw]{xcolor}
% \usepackage[colorlinks=true]{hyperref}

\definecolor{MyGreen}{RGB}{0,128,0}

\usepackage[colorlinks=true]{hyperref}
\definecolor{darkgrey}{rgb}{0.25, 0.25, 0.25} % Define darkgrey color

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.

\usepackage{multirow}
\usepackage{minitoc}
\usepackage{xspace}
% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage[most]{tcolorbox}
\usepackage{enumitem}
% \usepackage{xcolor}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{\method: Exploring Universal Convergent Space for Omni-Molecular Tasks}

\usepackage[most]{tcolorbox}
\newcommand{\hypbox}[2]{%
\begin{tcolorbox}[colback=white!98!black,colframe=white!30!black,boxsep=1.1pt,top=6.75pt]%
\vspace{1.75pt}%
\textbf{#1}\\[-0.575em]
\noindent\makebox[\textwidth]{\rule{\textwidth}{0.4pt}}
\\[0.25em]
#2
\end{tcolorbox}
}

\newcommand{\breakable}{%
  \mathcode`[="8000 % Make [ an active character
  \mathcode`]="8000 % Make ] an active character
  {\catcode`[=\active
   \catcode`]=\active
   \gdef[{\discretionary{}{}{[}} % Allow line breaks before [
   \gdef]{\discretionary{}{}{]}} % Allow line breaks before ]
  }
}
\newcommand{\method}{Omni-Mol\xspace}
\newcommand{\haoli}[1]{{\color{yellow!70!black}{[HaoLi:#1]}}}

\begin{document}

\twocolumn[
\icmltitle{\method: Exploring Universal Convergent Space for Omni-Molecular Tasks}
% Large Language Models are Multitask Chemistry Learners
% \method: A Versatile Multimodal Large-scale Language Model for Molecular Community
% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Chengxin Hu$^*$}{yyy}
\icmlauthor{Hao Li$^*$}{ind}
\icmlauthor{Yihe Yuan$^*$}{yyy}
\icmlauthor{Zezheng Song}{sch}
\icmlauthor{Haixin Wang$^{\dagger}$}{ucla}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}


% \icmlaffiliation{NUS}{National University of Singapore, Singapore}
\icmlaffiliation{yyy}{National University of Singapore, Singapore}
\icmlaffiliation{ind}{Independent Researcher}
\icmlaffiliation{sch}{University of Maryland, College Park}
\icmlaffiliation{ucla}{University of California, Los Angeles}
\icmlcorrespondingauthor{Haixin Wang}{whx@cs.ucla.edu}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}
% 使用原生LaTeX命令避免环境冲突
\vspace*{10pt}  % 调整标题与Logo的间距
\par\centering
\smash{%
  % GitHub 部分
  \raisebox{-0.5ex}{\includegraphics[height=12pt]{figs/github_logo.pdf}} % GitHub logo
  \;\textbf{Code:}% 非超链接文本
  \;\raisebox{-0.01ex}{\href{https://anonymous.4open.science/r/Omni-Mol-8EDB}{Omni-Mol-Code}}% 超链接文本下移
  \hspace{0.5cm} % GitHub 和 Hugging Face 之间的水平间距
  % Hugging Face 部分
  \raisebox{-0.5ex}{\includegraphics[height=12pt]{figs/hf-logo.pdf}} % Hugging Face logo
  \;\textbf{Data:}% 非超链接文本
  \;\raisebox{-0.01ex}{\href{https://huggingface.co/datasets/CodeMagic/Omni-Mol-Data}{Omni-Mol-Data}}% 超链接文本下移
}
\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{$^*$Equal contribution, listing order is alphabetic.} % otherwise use the standard text.

\newcommand{\adjustedcolorbox}[2]{%
  \begingroup
  \setlength{\fboxsep}{0pt}% 
  \colorbox{#1}{\strut #2}%
  \endgroup
}

\begin{abstract}
\input{0_abstract}
\end{abstract}

\section{Introduction}

\input{1_intro}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.48\linewidth]{figs/grad_norm.pdf}
    \includegraphics[width=0.48\linewidth]{figs/task_scaling.pdf}
    \vspace{-0.4cm}
    \caption{\small (Left) Gradient norm of unified training on 15 tasks. Due to the conflicts of multiple tasks, the gradient norm of InstructMol competes and shows a significant increase, while the gradient norm of \method remains relatively stable. (Right) The scaling trend with task numbers on reagent prediction. As the number of tasks increases, \method is benefited and consistently achieves better performances averagely, while InstructMol fails to scale up.}
    \label{fig:grad norm}
    \vspace{-0.5cm}
\end{figure}

\section{Related Works}
\subsection{Molecular Foundation Models}
Researchers are trying to leverage the world knowledge embedded in LLMs to build higher-quality molecular representations by fine-tuning on task-specific instructions.
Mol-Instruction~\cite{fang2024molinstructions} pioneers the instruction fine-tuning dataset, demonstrating the potential of LLMs in molecular modeling. Subsequently, InstructMol~\cite{cao2023instructmol} introduces 2D graph features of molecules based on SMILES~\cite{weininger1988smiles}, showing that LLMs can also enhance performance by aligning and fine-tuning their understanding of graph-based features. Soon after, 3D-MoLM~\cite{li2024towards} explores the advantages of 3D molecular representations in multimodal LLMs, while HIGHT~\cite{chen2024hight} investigates the impact of multi-level 2D graph features on molecular understanding. More recently, PRESTO~\cite{cao-etal-2024-presto} enhances LLMs' comprehension of molecular-related knowledge through extensive domain-specific pretraining across eight tasks. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/main.pdf} 
    \vspace{-0.3cm}
    \caption{Overview of our proposed \method, a scalable and unified LLM-based framework for direct instruction tuning.}
    \label{fig:main fig}
\end{figure*}

\subsection{Unified Generative Modeling}
The GPT models~\cite{brown2020language, achiam2023gpt} have achieved unification across all text-based tasks through large-scale pretraining and instruction tuning. Subsequently, the community has successfully constructed models that can understand data from multiple modalities and simultaneously perform tasks related to different modalities by converting features from each modality into tokens~\cite{alayrac2022flamingo,li2022blip,li2023blip,dai2023instructblip,liu2024visual}. More recently, the community has also been exploring unified understanding and generation, allowing models not only to understand multimodal data but also to generate multimodal data~\cite{zhu2023minigpt4,zheng2023minigpt5,koh2024generating}. This development is driving models towards convergence into a truly general-purpose model capable of solving all tasks. \citet{huh2024platonic} suggests that as models grow more powerful and general, their representations tend to converge, approaching a universal space that reflects the fundamental laws of the world. 
This insight inspires us to explore whether a universal convergent space also exists in the molecular domain.

\section{Method}
\subsection{Overview}
\method is a multimodal LLM framework to handle $K$ diverse molecular tasks simultaneously. It comprises a language model, a graph encoder $f_\mathcal{G}$, and a projector $f_p$. The inputs include a text instruction $\mathbf{X}_I$, a SELFIES string $\mathbf{X}_S$, and the graph data $\mathbf{X}_G$ corresponding to the input molecules, where $\mathbf{X}_G$ is converted from $\mathbf{X}_S$ using RDKit. We model the response $\mathbf{Y}$ as the probability of the next token as: 
\begin{equation}
\begin{aligned}
    &P(\mathbf{Y}|\mathbf{X}_I, \mathbf{X}_S, \mathbf{H}_G) \\
    &= \prod _{i} P_\theta(\mathbf{Y}_i|\mathbf{X}_I, \mathbf{X}_S, \mathbf{H}_G, \mathbf{Y}_{<i})
\end{aligned}
\end{equation}
where $\mathbf{H}_G=f_p(f_\mathcal{G}(\mathbf{X}_G))$, and $\theta$ is the parameter of the LLM. The graph encoder encodes the molecule graph into its representation $\mathbf{h_g}\in \mathbb{R}^{n\times d_1}$, where $n$ is the length of the representation, the projector then projects its dimension to the LLM's hidden size and obtain $\mathbf{H}_G\in\mathbb{R}^{n\times d_2}$.


\subsection{Unified Encoding for Instruction Tuning}
\noindent\textbf{Unified Input format.}
We collect and format the data of any task into the following structure.
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Instruction} $\mathbf{X}_I$: A brief, clear, and distinguishable instruction that tells the model what task to perform. 

    \item \textbf{Input} $\mathbf{X}_S$: A sequence of molecules represented using SELFIES, with different molecules separated by a dot.

    \item \textbf{Output} $\mathbf{Y}$: The output corresponding to the task, which may be a number, a SELFIES representation, or a textual description, all in text string.
\end{itemize}

The data above will be processed according to a specific template, detailed information can be found in Appendix~\ref{sec:taskandprompt}.

\noindent\textbf{Unified Encoding.}
To unify tokens from diverse tasks and modalities, and to enable parallel training on samples of varying lengths, we apply uniform padding to the mini-batch samples. First, padding tokens are added to the right side of the text input, and the batch is then passed through an embedding layer to obtain text embeddings. Next, since the number of atoms in each molecule varies, the number of graph tokens per sample differs as well. After inserting the graph tokens, we continue to add padding token embeddings to the right end of the sequence, ensuring the mini-batch becomes a well-formed tensor. We then generate an attention mask based on the padded tensor and assign an `\textit{ignore\_index}' in the labels to prevent \method from learning to generate padding tokens.

\subsection{Active Learning-based Data Selection}
To facilitate effective unified tuning, we systematically screen data from multiple tasks to minimize redundancy and conflicts, while determining an optimal mixing ratio.
Actually, not all task-specific data is equally crucial. Inspired by~\citet{yu2024diversify}, we employ an iterative task-centric data filtering approach to actively screen multiple datasets, substantially reducing training costs.


Given \(K\) task collections \(\mathcal{T} = \bigcup_{q=1}^{K} \mathcal{T}_q\), along with a total budget \(\mathcal{B}\) and a maximum iteration count \(J\). For the \(j\)-th iteration \(\bigl(1 \le j \le J\bigr)\), we select a data portion of size $\alpha_{j} = \mathcal{B}/J$.
Assume the initial parameters of the model is $\theta_0$. For the \(K\) subsets, we first initialize the distribution
$\boldsymbol{\pi}^0 = \bigl(\pi_{1}^0,\,\pi_{2}^0,\,\ldots,\,\pi_{K}^0\bigr),$
such that $\pi_{q}^0 = 1/K$, Then, we sample each task dataset $\mathcal{T}_q$ to a subset $\mathcal{T}_q^{(1)}$ according to the distribution and create the sampled training set $\mathcal{T}^{(1)}$. We then finetune $\theta_0$ on \(\mathcal{T}^{(1)}\) for \(M\) epochs, yielding the updated parameter $\theta_1$.

After obtaining $\theta_{1}$, we perform inference on the sampled dataset $\mathcal{T}^{(1)}$, we collect sample pair $(u_{p,+},u_{p,-})$ indexed by $p$, where $u_{p,+}$ is the ground truth and $u_{p,-}$ is the answer generated by the model. For each sample pair, we calculate the score of the sample by:
\begin{equation}
    \nu(u_{p,+},u_{p,-})=\max\{\rho(\cdot,\cdot)\} - \rho(u_{p,+}.u_{p,-})
\end{equation}
where $\rho(\cdot,\cdot)$ is a normalized metric function for each respective task.
\noindent
For each sample pair $(u_{p,+},u_{p,-})$ in the task dataset \(\mathcal{T}_q\), we define the average rating as:
\begin{equation}
\begin{aligned}
\mu_{q}
&=
\frac{1}{\bigl|\mathcal{T}_q\bigr|}
\sum_{(u_{p,+},u_{p,-})\in\mathcal{T}_q}
\nu\bigl(u_{p,+},u_{p,-}\bigr).
\end{aligned}
\end{equation}
A larger \(\mu_{q}\) indicates higher complexity, implying that samples in this task are more instrumental for the model. At the end of the \(j\)-th iteration, we reweight the distribution via:
\begin{equation}
\pi_{q}'^{(j)}=\frac{\mu_q}{\sum_{r=1}^K \mu_r}\pi_q^{(j-1)},\quad
\pi_q^{(j)} = \frac{\pi_q'^{(j)}}{\sum_{r=1}^K \pi_r'^{(j)}}
\end{equation}
We then select an additional batch of size $\alpha_{j+1} = \mathcal{B}/J$
denoted \(\mathcal{T}^{(j+1)}\), from the \textit{unused portion} of the dataset. We concatenate it with the previously chosen data for further training. This \textit{sample--train--evaluate--reweight} routine continues until \(J\) total iterations are reached.




\subsection{Stable \& MoE Expanded Framework}
\label{sec:loramoesft}


\noindent\textbf{Adaptive Gradient Stabilization.}
\label{sec:AGS}
During unified training, each mini-batch consists of samples from several tasks. The resulting loss and gradients will be a combination of contributions from each task. Let $B_k$ denotes the number of samples for task $k$, and $T$ the sequence length. The total loss for the model can then be formulated as:
\begin{equation}
    \mathcal{L_{\text{total}}}= \frac{1}{B}\sum_{k=1}^K\sum_{b=1}^{B_k}\sum_{t=1}^{T_k}\mathcal{L}(m_{b,t}^{(k)},o_{b,t}^{(k)})
    \label{eq:mixed grad}
\end{equation}
where $m_{b,t}^{(k)}$ and $o_{b,t}^{(k)}$ are the predicted logits and label for batch element $b$ at time step $t$ in task $k$.

Actually, we observe a noticeable increase in gradient norms (shown in Figure~\ref{fig:grad norm}), which leads to training instability and hindered faster loss convergence. 
We attribute the main cause of these divergences to issues arising from the softmax operation when handling tasks drawn from different domains with substantially varied entropy, stemming from softmax’s translation invariance (\emph{i.e.}, $softmax \left( z \right) = softmax \left( z + c \right)$). 
Since all model parameters are shared among multiple tasks, each task competes by incrementally growing its norms. while this is not immediately detrimental, it leads to divergence once norms extend beyond the effective range of bf16. To mitigate this problem, we employ an adaptive coefficient $\gamma_{\theta} =  \frac{\alpha_{\theta}}{||r||_p} + \beta_{\theta}$ during the parameter-efficient fine-tuning with LoRA~\cite{hu2021lora}, where $\alpha_{\theta}$ and $\beta_{\theta}$ are learnable variables as scaling factors and $r$ represents the rank of LoRA. Through this mechanism, gradients can be adaptively stabilized on data $\mathcal{D}$:
\begin{equation}
\nabla_{\boldsymbol{\Delta{\mathcal{W}}}} = \frac{\partial \mathcal{L}_{total}\left( \mathcal{D};\boldsymbol{\mathcal{W}}_0 + \gamma_{\theta} \cdot \boldsymbol{\Delta{\mathcal{W}}} \right)}{\partial \boldsymbol{\Delta{\mathcal{W}}}}
\end{equation} 
where $\Delta\mathcal{W}$ is the updated parameters to the pre-trained $\mathcal{W}_0$. More details can be seen in Appendix \ref{sec:peft}.

 
\noindent\textbf{Anchor-and-Reconcile Experts Expansion.}
\method needs to learn a wide range of different tasks and handle multiple modalities, including graph features, text, and SELFIES. While SELFIES is treated as regular text input to the LLM, it inherently differs significantly from natural language semantics, requiring the model to separately learn how to understand and generate SELFIES expressions. 

We aim for the model to simultaneously learn general knowledge while also differentiating for different modalities and tasks. Hence, we borrow the idea of MoE~\cite{dai2024deepseekmoe} and perform upcycling~\cite{komatsuzaki2023sparse, lin2024moe}. We first construct $\mathcal{N}$ reconcile experts, each targeting specialized knowledge areas, and dynamically balances conflicting signals among these experts to effectively mitigate task-level conflicts. Besides, we introduce an additional anchor expert to learn the common knowledge that underpins fundamental understanding across tasks, by consistently capturing and aligning shared features to maintain a stable global representation.

To be specific, for a regular decoder layer $l=1\dots L$ from a pre-trained LLM,
\begin{equation}
\begin{aligned}
    &h'_l = h_{l-1} + \text{MHA}_{\phi}(\text{LN}(h_{l-1})) \\
    &h_l = h'_l + \text{FFN}_{\gamma}(\text{LN}(h'_l))
\end{aligned}
\end{equation}
where $\phi$ and $\gamma$ are parameters of the pre-trained LLM. We convert the decoder layer into:
\begin{equation}
\begin{aligned}
    &h'_l = h_{l-1} + \text{MHA}_{\adjustedcolorbox{green!30}{\scriptsize $\phi'$}}(\text{LN}(h_{l-1})) \\
    &h_l=\begin{cases}
    h'_l + \text{FFN}_{\adjustedcolorbox{green!30}{\scriptsize $\gamma'$}}(\text{LN}(h'_l)), \quad l=1\dots l_{\text{MoE}} \\
    h'_l + \text{MoE}_{\adjustedcolorbox{green!30}{\scriptsize $\gamma_i, \psi$}}(\text{LN}(h'_l)), \quad l=l_{\text{MoE}}\dots L 
    \end{cases}
\end{aligned}
\end{equation}
where $l_{\text{MoE}}$ represents the layer starts to utilize MoE. The converted $\phi' = \phi +\Delta \phi_{\text{LoRA}}^{\text{(MHA)}}$, $\gamma'=\gamma + \Delta \gamma_{\text{LoRA}}^{\text{(FFN)}}$. For MoE layer, we initialize $\mathcal{N}+1$ experts with the weight of the pre-trained FFN  $\gamma$. Here, it concludes $\mathcal{N}$ reconcile experts to learn specialized knowledge and $1$ anchor experts to learn the common knowledge. Let $E_{\gamma_i}$ denotes the $i$-th expert, $i=1\dots \mathcal{N}+1$. And $\gamma_i$ is the parameter of the $i$-th expert, at the beginning of the training, these experts have identical weights, \emph{i.e.}, $\gamma_1=\gamma_2=\cdots=\gamma$. Router $R_\psi$ is random initialized with Kaiming uniform~\cite{he2015delving}, where $\psi$ is the learnable parameter of the router. 

Let $s$ denote the output of the router logits for an input token, assume $\mathcal{E}$ experts are chosen, the output of the MoE layer can be written as:
\begin{equation}
    \mathbf{x_t'} = \sum_{i=1}^\mathcal{N} \text{TopK}(s, \mathcal{E}) E_{\phi_i}(\mathbf{x_t}) + E_{\phi_{\mathcal{N}+1}}(\mathbf{x_t})
\end{equation}

\subsection{Optimization}
\label{sec:optimization}
Training strategy of \method consists of two stages. 

\noindent\textbf{Stage 1:} We perform multimodal alignment on PubChem~\cite{PubChem}, learning to describe molecules through graph modality features. The input consists of instructions and graph data, excluding SELFIES. Only the multimodal projector $f_p$ is trainable.

\noindent\textbf{Stage 2:} We fine-tune \method by freezing the pre-trained parameters (wrapped by PEFT adapters), while the adapters, the MoE layers and the multimodal projector stay active throughout fine-tuning.

Training loss of both stages for language modeling is:
\begin{equation}
\mathcal{L}_{\text{LM}}=-\sum_i \log P_\theta (\mathbf{Y}_i|\mathbf{X}_I, \mathbf{X}_S, \mathbf{H}_G, \mathbf{Y}_{<i})
\end{equation}
For stage 2, we incorporate an additional auxiliary load balancing loss for the MoE layers, assume an input tensor $x\in \mathbb{R}^{B\times T\times d}$, and $\mathcal{E}$ experts out of $\mathcal{N}$ is selected, the load balancing loss is:  
$\mathcal{L}_{\text{aux}} = \frac{1}{B}\sum_{i=1}^B\sum_{j=1}^{\mathcal{N}} C_{ij}\cdot \bar s_{ij}$,
where
$C_{ij} = \frac{\mathcal{N}}{T\mathcal{E}}\sum_{t=1}^{T\mathcal{E}}\mathbf{1}\{\text{t'th token selects expert j}\}, \bar s_{ij} = \frac{1}{T}\sum_{t=1}^T s_{i,j,t}$
and $\mathbf{1}\{\cdot\}$ is an indicator function. This load balancing loss used in~\citet{liu2024deepseek} additionally considers the sequence-level information.

The total loss is a combination of $\mathcal{L}_{\text{LM}}$ and $\mathcal{L}_{\text{aux}}$ with a coefficient $\lambda$: $\mathcal{L} = \mathcal{L}_{\text{LM}} + \lambda \mathcal{L}_{\text{aux}}$.

\subsection{Theoretical Analysis}
\label{sec: diff rep}

In theory, the regularization effect of omni-molecular tasks training can enable \method to learn more general representations. However, demonstrating this phenomenon empirically remains challenging.
Our key insight is that as more tasks are learned together, the solution space of the problems becomes progressively smaller. 
Assume the hypothesis space $\mathcal{V}$ of the model, and the solution of a task $i$ is $\mathcal{F}_i$, where $\mathcal{F}_i\subseteq\mathcal{V}$, for omni-molecular tasks learning with $n$ different tasks, the solution space will be: 
$\mathcal{F}_{\text{general}}^{(n)} = \bigcap_{i=1}^n \mathcal{F}_i$.

\begin{theorem}
\label{lem:progressive caps}
For $m>n,m,n\in \mathbb{Z}^+$, we have $\mathcal{F}_{\text{general}}^{(m)} \subseteq \mathcal{F}_{\text{general}}^{(n)}$. If $\forall n\leq i,j\leq m, i\neq j$, we have $\mathcal{F}_i\neq \mathcal{F}_j$, then $\mathcal{F}_{\text{general}}^{(m)} \subsetneq \mathcal{F}_{\text{general}}^{(n)}$.
\end{theorem}

The proof of Theorem~\ref{lem:progressive caps} can be found in Appendix~\ref{sec:proof of lemma prog cap}

Therefore, as the number of tasks $n$ increases, the similarity between the representations learned from the solution space of $n-1$ tasks and the solution space of $n$ tasks will become increasingly higher. 
Let $R_n \in \mathcal{F}_{\text{general}}^{(n)}$ denotes the representation learned from a solution space, consider a series of learned representation $S_R = \{R_1, R_2,\dots,R_n\}$ learned from $S_\mathcal{F} = \{F_{\text{general}}^{(1)}, F_{\text{general}}^{(2)},\dots,F_{\text{general}}^{(n)}\}$. Assume function $\Gamma(\cdot,\cdot)$ measures the similarity between two representations. we expect that, $\exists N\in \mathbb{Z}^+$, such that $\forall n>N$, $\exists i<N$, we have $\Gamma(R_{n},R_{n+1})>\Gamma(R_i, R_{i+1})$.

In our experimental validation, we construct a series of mixed datasets comprising 1, 2, 4, and 8 tasks to form the solution space sequence \(S_\mathcal{F}\). We train the model on each of these datasets and subsequently extract representations for a specific task. Finally, we calculate the similarity between representations \(R_i\) and \(R_j\) for \(i,j = 0, \dots, 3\). See Appendix~\ref{sec:simi} for the calculation of similarity.


\begin{table*}[ht]
\label{tab:main result}
\centering
\scriptsize
\vspace{-0.01cm}
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{2mm}{
\begin{tabular}{lccccccccc}
\toprule
Model & Type & \#Param & Exact $\uparrow$ & BLEU $\uparrow$ & Levenshtein $\downarrow$ & RDK $\uparrow$ & MACCS $\uparrow$ & Morgan $\uparrow$ & Validity $\uparrow$ \\ \hline
\rowcolor[HTML]{c9ecff} 
\multicolumn{10}{l}{\cellcolor[HTML]{c9ecff}Forward Reaction Prediction Task} \\
Vicuna~\cite{zheng2023judging} & In-Context Learning & 6.7B & 0.000 & 0.057 & 41.690 & 0.007 & 0.016 & 0.006 & 0.059 \\
LLaMA2~\cite{touvron2023llama} & Specialist(PEFT) & 6.7B & 0.012 & 0.804 & 29.947 & 0.499 & 0.649 & 0.407 & 1.000 \\
Mol-Instruction~\cite{fang2024molinstructions} & Specialist(PEFT) & 6.7B & 0.045 & 0.654 & 27.262 & 0.313 & 0.509 & 0.262 & 1.000 \\
HIGHT~\cite{chen2024hight} & Specialist(PEFT) & 6.7B & 0.293 & 0.935 & 16.687 & 0.774 & 0.618 & 0.566 & 1.000 \\
InstructMol~\cite{cao2023instructmol} & Specialist(PEFT) & 6.7B & 0.536 & 0.967 & 10.851 & 0.776 & 0.878 & 0.741 & 1.000 \\
PRESTO$^*$~\cite{cao-etal-2024-presto} & Generalist & 3.2B & 0.691 & 0.976 & \textbf{6.525} & 0.871 & 0.931 & 0.841 & 1.000 \\
\rowcolor[HTML]{EFEFEF} 
\textbf{\method} & Generalist & \textbf{1.7B} & \textbf{0.718} & \textbf{0.981} & 6.528 & \textbf{0.878} & \textbf{0.934} & \textbf{0.854} & \textbf{1.000} \\ \hline
\rowcolor[HTML]{c9ecff} 
\multicolumn{10}{l}{\cellcolor[HTML]{c9ecff}Retrosynthesis Task} \\
Vicuna~\cite{zheng2023judging} & In-Context Learning & 6.7B & 0.000 & 0.057 & 46.877 & 0.025 & 0.030 & 0.021 & 0.017 \\
LLaMA2~\cite{touvron2023llama} & Specialist(PEFT) & 6.7B & 0.000 & 0.283 & 53.510 & 0.136 & 0.294 & 0.106 & 1.000 \\
Mol-Instruction~\cite{fang2024molinstructions} & Specialist(PEFT) & 6.7B & 0.009 & 0.705 & 31.227 & 0.283 & 0.487 & 0.230 & 1.000 \\
HIGHT~\cite{chen2024hight} & Specialist(PEFT) & 6.7B & 0.202 & 0.914 & 20.194 & 0.772 & 0.623 & 0.577 & 0.999 \\
InstructMol~\cite{cao2023instructmol} & Specialist(PEFT) & 6.7B & 0.407 & 0.941 & 13.967 & 0.753 & 0.852 & 0.714 & 1.000 \\
PRESTO$^*$~\cite{cao-etal-2024-presto} & Generalist & 3.2B & 0.531 & 0.958 & 10.298 & 0.823 & 0.887 & 0.790 & 1.000 \\
\rowcolor[HTML]{EFEFEF} 
\textbf{\method} & Generalist & \textbf{1.7B} &\textbf{0.559} & \textbf{0.961} & \textbf{9.263} & \textbf{0.840} & \textbf{0.900} & \textbf{0.809} & \textbf{1.000} \\ \hline
\rowcolor[HTML]{c9ecff} 
\multicolumn{10}{l}{\cellcolor[HTML]{c9ecff}Reagent Prediction Task} \\
Vicuna~\cite{zheng2023judging} & In-Context Learning & 6.7B & 0.000 & 0.010 & 27.948 & 0.038 & 0.002 & 0.001 & 0.007 \\
LLaMA2~\cite{touvron2023llama} & Specialist(PEFT) & 6.7B & 0.000 & 0.283 & 53.510 & 0.136 & 0.294 & 0.106 & 1.000 \\
Mol-Instruction~\cite{fang2024molinstructions} & Specialist(PEFT) & 6.7B & 0.044 & 0.224 & 23.167 & 0.237 & 0.364 & 0.213 & 1.000 \\
HIGHT~\cite{chen2024hight} & Specialist(PEFT) & 6.7B & 0.067 & 0.482 & 27.167 & 0.462 & 0.346 & 0.303 & 1.000 \\
InstructMol~\cite{cao2023instructmol} & Specialist(PEFT) & 6.7B & 0.129 & 0.610 & 19.664 & 0.444 & 0.539 & 0.400 & 1.000 \\
PRESTO$^*$~\cite{cao-etal-2024-presto} & Generalist & 3.2B & 0.212 & 0.712 & 16.313 & 0.544 & 0.607 & 0.479 & 1.000 \\
\rowcolor[HTML]{EFEFEF} 
\textbf{\method} & Generalist & \textbf{1.7B} & \textbf{0.257} & \textbf{0.763} & \textbf{13.558} & \textbf{0.601} & \textbf{0.660} & \textbf{0.556} & \textbf{1.000} \\ \bottomrule
\end{tabular}
}
\vspace{-0.3cm}
\caption{Comprehensive comparisons on three reaction tasks. PEFT is short for parameter-efficient fine-tuning. PRESTO$^*$ represents our re-implementation based on source codes.}
\vspace{-0.3cm}
\end{table*}



\section{Experiments}
We aim to address the following concerns: (1) Compared with existing baselines, can \method achieve the best performances on the comprehensive omni-molecular datasets with 15 tasks simultaneously? (2) Is \method a scalable framework with the capacity and potential to solve complex molecular tasks? (3) Are all key components of \method essential for solving conflict collapse? (4) How can we verify that \method converges reliably and progressively refines its representations toward a universal convergent space? We begin by describing the experimental setup, then answer all the questions in the subsequent sections.
\subsection{Setup and Baselines}
\label{sec:setup}

\noindent\textbf{Datasets.} 
To construct a general-purpose model, we select 15 tasks across 4 categories to cover as many diverse tasks as possible. Here, we briefly introduce the datasets.
\begin{itemize}[leftmargin=*,nosep]
    \item \textbf{Reaction Tasks}: Forward Prediction ($\sim$124k), Retrosynthesis ($\sim$128k), Reagent Prediction ($\sim$124k), Solvent Prediction ($\sim$67k), Catalyst Prediction ($\sim$10k).
    \item \textbf{Regression Tasks}: LogP Prediction ($\sim$10k), SCF Prediction ($\sim$623k), TPSA Prediction ($\sim$11k), Complecity Prediction ($\sim$11k), Molecular Weight Prediction ($\sim$11k), Yield Regression ($\sim$9k) from, Quantum Mechanics Property Prediction ($\sim$36k).
    \item \textbf{Description Tasks}: Molecular Caption (Molcap) ($\sim$29k) and Description QA ($\sim$56k).
    \item \textbf{Action Tasks}: Experiment Procedure Prediction ($\sim$241k).
\end{itemize}
Details of datasets can be found in Appendix~\ref{app:dataset detail}.


\noindent\textbf{Baselines.} 
To ensure a fair comparison, we first choose representative LLM-based models such as InstructMol and HIGHT, and also report several previous baselines, including Mol-Instruction, Llama, Vicuna, among others, some of which are derived through In-Context Learning. For datasets with fewer models, we re-implement PRESTO as baseline.

\begin{table}[t]
\centering
\scriptsize
\vspace{-0.01cm}
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{0.8mm}{
\begin{tabular}{lcccccccc}
\toprule
Model & Type & \#Param & B-2 & B-4 & R-1 & R-2 & R-L & M \\ \hline
\rowcolor[HTML]{c9ecff} 
\multicolumn{9}{l}{\cellcolor[HTML]{c9ecff}Molecular Captioning Task} \\
GPT-3.5-turbo$^1$ & Retrieval & - & 0.565 & 0.482 & 0.623 & 0.450 & 0.543 & 0.585 \\
GPT-4-0314$^1$ & Retrieval & - & 0.607 & 0.525 & 0.634 & 0.476 & 0.562 & 0.610 \\
BioMedGPT$^2$ & Generalist & 10B & 0.234 & 0.141 & 0.386 & 0.206 & 0.332 & 0.308 \\
Mol-Instruction & Specialist & 6.7B & 0.249 & 0.171 & 0.331 & 0.203 & 0.289 & 0.271 \\
HIGHT & Specialist & 6.7B & 0.498 & 0.397 & 0.582 & 0.414 & 0.518 & 0.525 \\
InstructMol & Specialist & 6.7B & 0.475 & 0.371 & 0.566 & 0.394 & 0.502 & 0.509 \\
\rowcolor[HTML]{EFEFEF} 
\textbf{\method} & Generalist & \textbf{1.7B} & \textbf{0.544} & \textbf{0.456} & \textbf{0.610} & \textbf{0.456} & \textbf{0.549} & \textbf{0.579} \\ \hline
\rowcolor[HTML]{c9ecff} 
\multicolumn{9}{l}{\cellcolor[HTML]{c9ecff}Description Q\&A Task} \\
Llama2$^3$ & Specialist & 6.7B & \multicolumn{1}{l}{0.282} & \multicolumn{1}{l}{0.232} & \multicolumn{1}{l}{0.351} & \multicolumn{1}{l}{0.221} & \multicolumn{1}{l}{0.304} & \multicolumn{1}{l}{0.469} \\
3D-MoLM(S)$^4$ & Specialist & 6.7B & \multicolumn{1}{l}{0.320} & \multicolumn{1}{l}{0.261} & \multicolumn{1}{l}{0.401} & \multicolumn{1}{l}{0.256} & \multicolumn{1}{l}{0.346} & \multicolumn{1}{l}{0.522} \\
3D-MoLM(G)$^4$ & Generalist & 6.7B & \multicolumn{1}{l}{0.318} & \multicolumn{1}{l}{0.261} & \multicolumn{1}{l}{0.401} & \multicolumn{1}{l}{0.259} & \multicolumn{1}{l}{0.350} & \multicolumn{1}{l}{0.519} \\
\rowcolor[HTML]{EFEFEF} 
\textbf{\method} & Generalist & \textbf{1.7B} & \textbf{0.516} & \textbf{0.440} & \textbf{0.529} & \textbf{0.382} & \textbf{0.492} & \textbf{0.580} \\ \bottomrule
\end{tabular}
}
\vspace{-0.2cm}
\caption{\small Main results of molecular captioning and description QA task, $^1$: 10-shot results from~\citet{li2024empowering}, $^{2,3}$: results from~\citet{luo2023biomedgpt, li2024towards}, $^4$: (S,G) means the specialist and generalist version of 3D-MoLM separately. B: BLEU, R: ROUGE, M: METEOR.}
\vspace{-0.7cm}
\end{table}


\noindent\textbf{Backbone.} 
We utilize LLaMA 3.2-1B~\cite{dubey2024llama} as the backbone, a single linear layer as the projector, and MoleculeSTM~\cite{mustafa2022multimodal} as the graph encoder for processing molecular graphs. For MoE expansion, we set $l_{\text{moe}}=1/4L$ and number of experts to 3. More details about model implementation can be found in Appendix~\ref{app:model imp}.

\noindent\textbf{Training Details.} We use PyTorch~\cite{paszke2019pytorch} with DeepSpeed ZeRO-2~\cite{rajbhandari2020zero} for more efficient parallel training. For unified tuning, we train 15 epochs with LoRA rank of 64. For separate tuning, \method is trained for 10 epochs with the same LoRA configuration. The learning rate is set to 8e-5 from grid search for all experiments. For experiment consistency, random seed is set to 0. More details can be found in Appendix~\ref{app: training detail}.



\subsection{Main Results}
\begin{figure*}[t]
\centering
\includegraphics[width=0.48\linewidth]{figs/data_scaling.pdf}
\includegraphics[width=0.48\linewidth]{figs/model_scaling_all.pdf}
\\[-7pt]
\vspace{-0.1cm}
\caption{Scaling trend of \method. (Left) The scaling trend respect to dataset proportion, metrics are averaged across tasks, (Right) The scaling trend respect to model size, the metrics of Quantum Mechanics Property Prediction task are normalized. we observed a clear log scaling behavior.}
\label{fig:scaling law}
\vspace{-0.4cm}
\end{figure*}


\begin{table}[t]
\centering
\scriptsize
\vspace{-0.01cm}
\renewcommand{\arraystretch}{1.1}
\setlength{\tabcolsep}{1.4mm}{
\begin{tabular}{lcccccc}
\toprule
Model & Type & \#Param & HOMO & LUMO & GAP & Avg. \\ \hline
\rowcolor[HTML]{c9ecff} 
\multicolumn{7}{l}{\cellcolor[HTML]{c9ecff}Quantum Mechanics Property Prediction Task} \\
Alpaca$^1$ & In-Context & 6.7B & - & - & - & 322.109 \\
LLaMA2$^2$ & In-Context & 6.7B & 0.7367 & 0.8641 & 0.5152 & 0.7510 \\
Vicuna$^2$ & In-Context & 13B & 0.7135 & 3.6807 & 1.5407 & 1.9783 \\
Mol-Instruction & Specialist & 6.7B & 0.0210 & 0.0210 & 0.0203 & 0.0210 \\
HIGHT & Specialist & 6.7B & 0.0056 & 0.0065 & 0.0077 & 0.0066 \\
InstructMol & Specialist & 6.7B & 0.0048 & \textbf{0.0050} & 0.0061 & \textbf{0.0050} \\
\rowcolor[HTML]{EFEFEF} 
\textbf{\method} & Generalist & \textbf{1.7B} & \textbf{0.0047} & 0.0056 & \textbf{0.0060} & 0.0052 \\ \bottomrule
\end{tabular}
}
\label{tab:my_label}
\vspace{-0.3cm}
\caption{Main results of Quantum Mechanics Property Prediction task, $^1$: In-Context Learning results from~\citet{fang2024molinstructions}, $^2$: 5-shot In-Context Learning results from~\citet{cao2023instructmol}.}
\vspace{-0.7cm}
\end{table}


Here, we obatin the answer that \textit{\method can achieve the best performance across almost all tasks.} As the results shown in Table~\ref{tab:main result}, we have the following observations. \method significantly outperforms all specialist baselines while utilizing only 25\% of the parameters. Furthermore, \method surpasses the corresponding state-of-the-art generalist baseline by an average of approximately 1\%, 4\%, 13\%, 15\%, and 40\% across forward prediction, retrosynthesis, reagent prediction, molcap, and Description Q\&A separately. That is to say, \method achieves superior performance with greater parameter efficiency, demonstrating its effectiveness in becoming a general AI chemist.
Due to the page limit, we only report 6 tasks, the remaining results can be found in Appendix~\ref{sec:more results}.


\subsection{Is \method a Scalable Framework?}

One critical property of LLMs is their scaling behavior in relation to both model and data size. In this study, we demonstrate that \textit{\method} is a scalable framework by conducting three distinct types of scaling experiments:
\textbf{(1)} We select three different sizes of LLMs from the LLaMA 3 series, 1B, 3B, and 8B, for language backbone scaling.
\textbf{(2)} We evaluate the impact of dataset size by down-sampling the original dataset to 20\%, 40\%, 60\%, and 100\% of its full size.
\textbf{(3)} To examine task scaling, we train \method on different numbers of tasks, specifically, 1, 2, 4, 8, and 15 tasks, and observe the performance of individual tasks within these multi-task settings.


\textbf{(1)} As shown in the left of Figure~\ref{fig:scaling law}, we observe a clear logarithmic scaling trend as the dataset proportion increases. The relationship between the model's average performance and the dataset proportion can be approximately expressed as $y=0.07\cdot\log(x)+0.41$, indicating that the model's performance improves as the amount of data increases. The overlaid radar charts further demonstrate that this trend holds true across all tasks.

\textbf{(2)}
As shown on the right side of Figure~\ref{fig:scaling law}, the performance of Omni-Mol across all tasks increases as the model size grows. We also observe a clear logarithmic scaling trend, where this relationship can be approximated as \( \text{perf} = 0.02 \cdot \log(x) + 0.32 \). However, as seen in the radar chart, the performance does not consistently improve across all tasks with the increase in model size.


\textbf{(3)} The average results are shown in Figure~\ref{fig:grad norm} based on reagent prediction. Although there is no clear elementary functional relationship between the model's average performance and the number of tasks, \method continues to benefit as the number of tasks increases. This indirectly supports our theory about the value of a high-quality, universal representation space. In contrast, InstructMol experiences a performance decline when the number of tasks exceeds eight, as conflicts between tasks hinder its ability to simultaneously learn all tasks effectively.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/active_learn2.pdf} \\[-7pt]
    \caption{Ablation study of active learning-based dataset selection. \method w. RS: we randomly sample all datasets and mix them, \method w. AD: we use all data samples.}
    \label{fig:active learn}
    \vspace{-0.7cm}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.32\linewidth]{figs/single_multi.pdf}
    \includegraphics[width=0.32\linewidth]{figs/lora_rslora.pdf}
    \includegraphics[width=0.32\linewidth]{figs/ffn_moe.pdf}
    \vspace{-0.3cm}
    \caption{Ablation studies: (Left) Ablation of unified training. The performance is averaged across all metrics. (Middle) Ablation of adaptive gradient stabilization. (Right) Ablation of MoE expansion.}
    \label{fig:ablation uit,lorarslora,moeffn}
    % \vspace{-0.5cm}
\end{figure*}




\subsection{Is Unified Instruction Tuning Essential?}
One key aspect of \method is its ability to leverage unified learning across omni-molecular tasks, enabling the convergence to more generalizable representations. To evaluate this, we compare the performance of separate tuning on individual tasks (\method w/o UT) against our unified tuning. As shown in Figure~\ref{fig:ablation uit,lorarslora,moeffn}, \method w/o UT performs significantly worse across five tasks compared to ours. This indicates that the representations learned through unified tuning are superior and benefit from shared knowledge. Interestingly, even tasks that are not directly related, such as the molcap task, which is distinct from both reaction and regression tasks, still show improvements with unified tuning. Additional ablation results are provided in Appendix~\ref{sec:more ablation}.



\subsection{Is Dataset Selection Essential?}

In Figure~\ref{fig:active learn}, we report the results compared with random sampling (\method w. RS) and all-data training (\method w. AD). The figure also includes the ratio of full data to down-sampled data. With 40\% of the data, the \method significantly outperforms (\method w. RS) and surpasses (\method w. AD) on the MolCap, Description QA, and Reagent tasks. This supports the hypothesis that redundant samples exist within the data, and selecting appropriate subsets from different datasets can lead to better performance. In the remaining two tasks, our results are also comparable, considering that we used only a small portion of the data.

\subsection{Can We Mitigate the Conflict Collapse?}

\noindent\textbf{How Do Adaptive Gradient Stabilization Helps?} 
We compare our \method with \method w/o ST, which replaces our adaptive module with the standard LoRA adapter. As shown in the middle of Figure~\ref{fig:ablation uit,lorarslora,moeffn}, \method w/o ST consistently exhibits lower performances than \method across all tasks. This consistent decline underscores the effectiveness of our adaptive module in enhancing performance by mitigating task conflicts. This mitigation of task conflicts ensures that \method can leverage shared knowledge without detrimental interference, thereby enhancing its ability to generalize across various tasks and modalities.


\noindent\textbf{Is MoE Expansion Essential?}
We conduct an ablation study by replacing our Anchor-and-Reconcile Experts Expansion with a single activated Feed-Forward Network (FFN). The comparison results are shown in the right of Figure~\ref{fig:ablation uit,lorarslora,moeffn}.
We observe that \method consistently outperforms the \method w. FFN across all tasks, including molcap, forward prediction, retrosynthesis, and reagent prediction. Additionally, for the homo-lumo, where lower values are preferable, Omni-Mol achieves a better score. This demonstrates that \method effectively enhances performance by leveraging specialized experts, as opposed to a single FFN. The most significant improvement is observed in forward prediction, where the diverse experts contribute to better generalization and representation learning. 

\subsection{Convergence Analysis via Mutual Similarity}
\begin{figure}[t]
\centering
\includegraphics[width=1\linewidth]{figs/align_heatmap.pdf}\\[-7pt]
\caption{Demonstration of similarity scores heatmap for methods trained on varying numbers of tasks. Surprisingly, \method shows rising similarity scores as the task count increases (direction of the black dashed line). Compared to the decreasing trend of InstructMol, this suggests that \method converges toward a more consistent representation space universally.}
\label{fig:alignment}
\vspace{-0.7cm}
\end{figure}

In this section, we aim to validate the theory in Section~\ref{sec: diff rep} and verify the convergence of \method, we compute the representation sequence with model trained on 1, 2, 4, and 8 tasks, we use \texttt{mutual\_knn}~\cite{huh2024platonic} as our similarity function $\Gamma(\cdot,\cdot)$, the results are shown in Figure~\ref{fig:alignment}.

Obviously, when the number of tasks increases, the similarity of the representations learned by \method also increases. This indicates that the model's representations are gradually converging. This outcome supports our earlier hypothesis that adding more tasks reduces the size of the model’s general solution space. As a result, the model is forced to learn representations within a smaller and more focused space, leading to the convergence of the learned representations. Ultimately, these representations converge to a universal form that can effectively solve all tasks.

Interestingly, in the mutual similarity analysis of InstructMol, we observe the opposite trend. As the number of tasks increases, the representations learned by InstructMol become progressively less similar to those learned previously. This suggests that with each added task, the changes in the solutions learned by InstructMol become larger, indicating that it is unable to converge to a universal representation space through unified training. In fact, the model may be moving further away from such a space.

\section{Conclusion}
We introduce \method, a model that unifies 15 tasks, resolves the conflict collapse problem, and learns generalizable representations. \method achieves this through unified SFT, MoE expansion, and active learning-based data selection. \method achieves sota performance across multiple tasks, and we demonstrated its scalability and ability to scale up performance as the number of tasks increases. Finally, we provide experimental evidence showing that \method achieves a more general convergent solution space, acquiring the general capability to solve diverse tasks.

\noindent\textbf{Limitation.}
We identified two limitations: (1) Due to the limited computational resources, we are unable to further scale up the model with higher computational resources, which prevents us from exploring the limits of model's performance. (2) \method's tasks are still primarily focused on molecular understanding, and it is not yet capable of designing new molecules. Future work should explore unifying understanding and generation. 

\clearpage
\section*{Acknowledgement}
The authors acknowledge the University of Maryland Institute for Advanced Computer Studies (UMIACS) for providing computational resources that contributed to this research.

\section*{Impact Statement}
This paper presents \method, which is the first scalable and unified molecular generalist model with outperforming results, enabling tasks such as molecule captioning, property prediction, and drug design. While these advancements provide powerful tools for molecular research, they also raise ethical concerns, such as the risk of misuse in designing harmful molecules. Transparency, responsible use, and interdisciplinary collaboration are essential to ensure these models serve the broader good, paving the way for impactful and responsible scientific innovation. 


\bibliography{example_paper}
\bibliographystyle{icml2025}

\input{7_appendix}

\end{document}