Large language models (LLMs), especially multimodal LLMs, have achieved significant breakthroughs in various scientific tasks due to their powerful representational capabilities and general reasoning abilities, spanning domains such as medicine~\cite{jee2024automated,zhou2024pre}, chemistry~\cite{boiko2023autonomous}, and biology~\cite{zhang2024multimodal}.
This cutting-edge technology has also sparked an increasing number of studies exploring how to align molecular representation spaces with textual representation spaces~\cite{cao2023instructmol,chen2024hight,fang2024molinstructions,cao-etal-2024-presto}. These works hold great promise to build powerful AI chemist for advancing molecule captioning, property/structure prediction, and text-conditioned de novo drug design.


The first step in creating an AI chemist is to develop a generalist model with universal capabilities, enabling it to understand diverse molecular structures and their interactions under multiple chemical domains.
Pioneering works, such as Text$+$Chem T5~\cite{christofidellis2023unifying}, introduces the first multi-domain, multi-task language model capable of unifying molecular and textual representations. Following this, PRESTO~\cite{cao-etal-2024-presto} further enhances performance by progressively improving multimodal LLMs through cross-modal alignment and multi-graph understanding. 
Similarly, in the field of general LLMs, the platonic representation~\cite{huh2024platonic} introduces the concept of the multitask scaling hypothesis, which suggests that as models are trained on an increasing number of tasks, they are driven to develop representations capable of addressing all tasks effectively. All of them highlight the potential of constructing the generalist model capable of handling a wide range of molecular tasks.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figs/platonic.pdf}
    \vspace{-0.3cm}
    \caption{\small (Left) Illustration of the universal convergent representation space for omni-molecule tasks. (Right) Conflict between tasks from different domains makes vanilla models fail to converge.}
    \vspace{-0.5cm}
    \label{fig:platonic conv}
\end{figure}

However, we have yet to observe a model that achieves outstanding performances across as many tasks as possible, nor have we seen a clear trend toward scalability in this direction. For instance, InstructMol~\cite{cao2023instructmol} attempts to scale up large language models but yields negligible gains, while PRESTO relies on a complex training strategy and requires extensive computational resources for pre-training.
We propose that the fundamental challenge is \textit{conflict collapse}, illustrated in Figure~\ref{fig:platonic conv}, which limits the emergence of truly generalist model in three key ways. \textbf{First}, potential conflicts may arise among various functional groups within a molecule and across the entire molecular structure, making it difficult to optimize the semantic relationships among different molecular representations.  
\textbf{Second}, data with conflicts from different domains often exhibit divergent distributions and interfere with each other, rendering it elusive to determine an ideal training data mixture.  
\textbf{Third}, the complexity of multi-task conflicts grows explosively as the volume of molecular data increases, requiring models with limited capacity to consume significantly greater resources in order to resolve these conflicts.

The recognition of the existing limitations naturally raises a pivotal question towards chemistry generalist model:
\begin{tcolorbox}[notitle, rounded corners, colframe=darkgrey, colback=white, boxrule=1.5pt, boxsep=0pt, left=0.15cm, right=0.17cm, enhanced, shadow={2.5pt}{-2.5pt}{1.5pt}{opacity=5},toprule=2pt, before skip=0.65em, after skip=0.75em 
  ]
\emph{
  {
    \centering 
  {
    \fontsize{8.5pt}{13.2pt}\selectfont 
    Is it possible to develop a generalist model that converges to a universal representation space for omni-molecular tasks?
  }
  \\
  }
  }
\end{tcolorbox}

This question drives us to develop \method, a scalable and unified LLM-based framework for direct instruction tuning. \method is the first generalist to effectively mitigate \textit{conflict collapse} in three respects:
\textbf{(1)} \method proposes a unified encoding mechanism applicable to any task input, leveraging the most comprehensive instruction-following omni-molecular dataset to date, which comprises 1.8M samples across 15 tasks.
\textbf{(2)} From the perspective of data, \method employs an active learning-based dynamic data selection after recognizing that not all tasks are equally important. We significantly reduced the dataset size to 40\% of its original volume while maintaining comparable performance with the full dataset.
\textbf{(3)} From the perspective of architecture, our novel designed adaptive gradient stabilization successfully mitigate the rapid gradients growth caused by task conflicts (shown in Figure \ref{fig:grad norm}). Besides, our anchor-and-reconcile mixture-of-expert (MoE) architecture reduces interference by dynamically routing tasks to reconcile experts and capturing common knowledge through anchor experts.

% siginifcant 换为具体的数字
Extensive experiments show that \method achieves significant improvements across most of the tasks, setting new state-of-the-art results among LLM-based models. Additionally, we observe that \method scales effectively with increases in data volume, model size, and the number of tasks, indicating the model's tremendous potential under larger computational budgets. Furthermore, by analyzing the representations of models trained with progressively more tasks, we discover that the representations become increasingly similar as the number of tasks grows. This provides robust evidence supporting our hypothesis of convergent space toward a universal molecular representation.

