\section{Related Works}
\subsection{Molecular Foundation Models}
Researchers are trying to leverage the world knowledge embedded in LLMs to build higher-quality molecular representations by fine-tuning on task-specific instructions.
Mol-Instruction~\cite{fang2024molinstructions} pioneers the instruction fine-tuning dataset, demonstrating the potential of LLMs in molecular modeling. Subsequently, InstructMol~\cite{cao2023instructmol} introduces 2D graph features of molecules based on SMILES~\cite{weininger1988smiles}, showing that LLMs can also enhance performance by aligning and fine-tuning their understanding of graph-based features. Soon after, 3D-MoLM~\cite{li2024towards} explores the advantages of 3D molecular representations in multimodal LLMs, while HIGHT~\cite{chen2024hight} investigates the impact of multi-level 2D graph features on molecular understanding. More recently, PRESTO~\cite{cao-etal-2024-presto} enhances LLMs' comprehension of molecular-related knowledge through extensive domain-specific pretraining across eight tasks. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figs/main.pdf} 
    \vspace{-0.3cm}
    \caption{Overview of our proposed \method, a scalable and unified LLM-based framework for direct instruction tuning.}
    \label{fig:main fig}
\end{figure*}

\subsection{Unified Generative Modeling}
The GPT models~\cite{brown2020language, achiam2023gpt} have achieved unification across all text-based tasks through large-scale pretraining and instruction tuning. Subsequently, the community has successfully constructed models that can understand data from multiple modalities and simultaneously perform tasks related to different modalities by converting features from each modality into tokens~\cite{alayrac2022flamingo,li2022blip,li2023blip,dai2023instructblip,liu2024visual}. More recently, the community has also been exploring unified understanding and generation, allowing models not only to understand multimodal data but also to generate multimodal data~\cite{zhu2023minigpt4,zheng2023minigpt5,koh2024generating}. This development is driving models towards convergence into a truly general-purpose model capable of solving all tasks. \citet{huh2024platonic} suggests that as models grow more powerful and general, their representations tend to converge, approaching a universal space that reflects the fundamental laws of the world. 
This insight inspires us to explore whether a universal convergent space also exists in the molecular domain.