@inproceedings{
fang2024molinstructions,
title={Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models},
author={Yin Fang and Xiaozhuan Liang and Ningyu Zhang and Kangwei Liu and Rui Huang and Zhuo Chen and Xiaohui Fan and Huajun Chen},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=Tlsdsb6l9n}
}

@inproceedings{cao-etal-2024-presto,
    title     = {{PRESTO}: Progressive Pretraining Enhances Synthetic Chemistry Outcomes},
    author    = {Cao, He and Shao, Yanjun and Liu, Zhiyuan and Liu, Zijing and Tang, Xiangru and Yao, Yuan and Li, Yu},
    editor    = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2024},
    month     = nov,
    year      = {2024},
    address   = {Miami, Florida, USA},
    publisher = {Association for Computational Linguistics},
    url       = {https://aclanthology.org/2024.findings-emnlp.597},
    pages     = {10197--10224}
}

@inproceedings{cao2023instructmol,
  title={Instructmol: Multi-modal integration for building a versatile and reliable molecular assistant in drug discovery},
  author={Cao, He and Liu, Zijing and Lu, Xingyu and Yao, Yuan and Li, Yu},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics (COLING 2025)},
  year={2025},
  address={Abu Dhabi, UAE},
  month={Jan}
}

@article{PubChem,
  title={PubChem 2023 update},
  author={Sunghwan Kim and Jie Chen and Tiejun Cheng and Asta Gindulyte and Jia He and Siqian He and Qingliang Li and Benjamin A. Shoemaker and Paul A. Thiessen and Bo Yu and Leonid Y. Zaslavsky and Jian Zhang and Evan E. Bolton},
  journal={Nucleic acids research},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:253182955}
}

@article{landrum2013rdkit,
  title={RDKit: A software suite for cheminformatics, computational chemistry, and predictive modeling},
  author={Landrum, Greg and others},
  journal={Greg Landrum},
  volume={8},
  number={31.10},
  pages={5281},
  year={2013}
}

@misc{PubChemPy,
  author       = {Matt Swain},
  title        = {PubChemPy: A Python wrapper for the PubChem PUG REST API},
  year         = {2024},
  howpublished = {\url{https://github.com/mcs07/PubChemPy}},
  note         = {Accessed: 2024-12-15}
}

@inproceedings{
li2024towards,
title={Towards 3D Molecule-Text Interpretation in Language Models},
author={Sihang Li and Zhiyuan Liu and Yanchen Luo and Xiang Wang and Xiangnan He and Kenji Kawaguchi and Tat-Seng Chua and Qi Tian},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=xI4yNlkaqh}
}

@inproceedings{liuvisual,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34892--34916},
 publisher = {Curran Associates, Inc.},
 title = {Visual Instruction Tuning},
 volume = {36},
 year = {2023}
}

@inproceedings{edwards2021text2mol,
  title={Text2mol: Cross-modal molecule retrieval with natural language queries},
  author={Edwards, Carl and Zhai, ChengXiang and Ji, Heng},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={595--607},
  year={2021}
}

@article{liu2024reactxt,
  title={ReactXT: Understanding Molecular" Reaction-ship" via Reaction-Contextualized Molecule-Text Pretraining},
  author={Liu, Zhiyuan and Shi, Yaorui and Zhang, An and Li, Sihang and Zhang, Enzhi and Wang, Xiang and Kawaguchi, Kenji and Chua, Tat-Seng},
  journal={arXiv preprint arXiv:2405.14225},
  year={2024}
}

@article{mustafa2022multimodal,
  title={Multimodal contrastive learning with limoe: the language-image mixture of experts},
  author={Mustafa, Basil and Riquelme, Carlos and Puigcerver, Joan and Jenatton, Rodolphe and Houlsby, Neil},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={9564--9576},
  year={2022}
}

@article{lin2024moe,
  title={Moe-llava: Mixture of experts for large vision-language models},
  author={Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
  journal={arXiv preprint arXiv:2401.15947},
  year={2024}
}

@inproceedings{
chen2024octavius,
title={Octavius: Mitigating Task Interference in {MLLM}s via Lo{RA}-MoE},
author={Zeren Chen and Ziqin Wang and Zhen Wang and Huayang Liu and Zhenfei Yin and Si Liu and Lu Sheng and Wanli Ouyang and Jing Shao},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=rTDyN8yajn}
}

@inproceedings{tian2024hydralora,
  title={HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning},
  author={Tian, Chunlin and Shi, Zhan and Guo, Zhijiang and Li, Li and Xu, Chengzhong},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2024}
}

@inproceedings{
zhao2024harmonizing,
title={Harmonizing Visual Text Comprehension and Generation},
author={Zhen Zhao and Jingqun Tang and Binghong Wu and Chunhui Lin and Shu Wei and Hao Liu and Xin Tan and zhizhong zhang and Can Huang and Yuan Xie},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=fqjeKsHOVR}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@article{lepikhin2020gshard,
  title={Gshard: Scaling giant models with conditional computation and automatic sharding},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020}
}

@inproceedings{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International conference on machine learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@inproceedings{
komatsuzaki2023sparse,
title={Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints},
author={Aran Komatsuzaki and Joan Puigcerver and James Lee-Thorp and Carlos Riquelme Ruiz and Basil Mustafa and Joshua Ainslie and Yi Tay and Mostafa Dehghani and Neil Houlsby},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=T5nUQDrM4u}
}

@inproceedings{
hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}

@inproceedings{
huh2024position,
title={Position: The Platonic Representation Hypothesis},
author={Minyoung Huh and Brian Cheung and Tongzhou Wang and Phillip Isola},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=BH8TYy0r6u}
}

@article{liu2024deepseek,
  title={Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model},
  author={Liu, Aixin and Feng, Bei and Wang, Bin and Wang, Bingxuan and Liu, Bo and Zhao, Chenggang and Dengr, Chengqi and Ruan, Chong and Dai, Damai and Guo, Daya and others},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{chen2024hight,
  title={HIGHT: Hierarchical Graph Tokenization for Graph-Language Alignment},
  author={Chen, Yongqiang and Yao, Quanming and Zhang, Juzheng and Cheng, James and Bian, Yatao},
  journal={arXiv preprint arXiv:2406.14021},
  year={2024}
}

@article{kalajdzievski2023rank,
  title={A rank stabilization scaling factor for fine-tuning with lora},
  author={Kalajdzievski, Damjan},
  journal={arXiv preprint arXiv:2312.03732},
  year={2023}
}

@article{yu2024diversify,
  title={Diversify and Conquer: Diversity-Centric Data Selection with Iterative Refinement},
  author={Yu, Simon and Chen, Liangyu and Ahmadian, Sara and Fadaee, Marzieh},
  journal={arXiv preprint arXiv:2409.11378},
  year={2024}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{liu2023multi,
  title={Multi-modal molecule structure--text model for text-based retrieval and editing},
  author={Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Animashree},
  journal={Nature Machine Intelligence},
  volume={5},
  number={12},
  pages={1447--1457},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{li2024empowering,
  title={Empowering molecule discovery for molecule-caption translation with large language models: A chatgpt perspective},
  author={Li, Jiatong and Liu, Yunqing and Fan, Wenqi and Wei, Xiao-Yong and Liu, Hui and Tang, Jiliang and Li, Qing},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@article{luo2023biomedgpt,
  title={Biomedgpt: Open multimodal generative pre-trained transformer for biomedicine},
  author={Luo, Yizhen and Zhang, Jiahuan and Fan, Siqi and Yang, Kai and Wu, Yushuai and Qiao, Mu and Nie, Zaiqing},
  journal={arXiv preprint arXiv:2308.09442},
  year={2023}
}

@article{jee2024automated,
  title={Automated real-world data integration improves cancer outcome prediction},
  author={Jee, Justin and Fong, Christopher and Pichotta, Karl and Tran, Thinh Ngoc and Luthra, Anisha and Waters, Michele and Fu, Chenlian and Altoe, Mirella and Liu, Si-Yang and Maron, Steven B and others},
  journal={Nature},
  pages={1--9},
  year={2024},
  publisher={Nature Publishing Group UK London}
}
@article{zhou2024pre,
  title={Pre-trained multimodal large language model enhances dermatological diagnosis using SkinGPT-4},
  author={Zhou, Juexiao and He, Xiaonan and Sun, Liyuan and Xu, Jiannan and Chen, Xiuying and Chu, Yuetan and Zhou, Longxi and Liao, Xingyu and Zhang, Bin and Afvari, Shawn and others},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={5649},
  year={2024},
  publisher={Nature Publishing Group UK London}
}
@article{zhang2024multimodal,
  title={Multimodal large language models for bioimage analysis},
  author={Zhang, Shanghang and Dai, Gaole and Huang, Tiejun and Chen, Jianxu},
  journal={nature methods},
  volume={21},
  number={8},
  pages={1390--1393},
  year={2024},
  publisher={Nature Publishing Group US New York}
}
@article{boiko2023autonomous,
  title={Autonomous chemical research with large language models},
  author={Boiko, Daniil A and MacKnight, Robert and Kline, Ben and Gomes, Gabe},
  journal={Nature},
  volume={624},
  number={7992},
  pages={570--578},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{huh2024platonic,
  title={The platonic representation hypothesis},
  author={Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  journal={arXiv preprint arXiv:2405.07987},
  year={2024}
}
@inproceedings{christofidellis2023unifying,
  title={Unifying molecular and textual representations via multi-task language modelling},
  author={Christofidellis, Dimitrios and Giannone, Giorgio and Born, Jannis and Winther, Ole and Laino, Teodoro and Manica, Matteo},
  booktitle={International Conference on Machine Learning},
  pages={6140--6157},
  year={2023},
  organization={PMLR}
}
@article{dai2024deepseekmoe,
  title={Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models},
  author={Dai, Damai and Deng, Chengqi and Zhao, Chenggang and Xu, RX and Gao, Huazuo and Chen, Deli and Li, Jiashi and Zeng, Wangding and Yu, Xingkai and Wu, Y and others},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}

@article{yu2024visual,
  title={Visual tuning},
  author={Yu, Bruce XB and Chang, Jianlong and Wang, Haixin and Liu, Lingbo and Wang, Shijie and Wang, Zhiyu and Lin, Junfan and Xie, Lingxi and Li, Haojie and Lin, Zhouchen and others},
  journal={ACM Computing Surveys},
  volume={56},
  number={12},
  pages={1--38},
  year={2024},
  publisher={ACM New York, NY}
}

@article{ding2023parameter,
  title={Parameter-efficient fine-tuning of large-scale pre-trained language models},
  author={Ding, Ning and Qin, Yujia and Yang, Guang and Wei, Fuchao and Yang, Zonghan and Su, Yusheng and Hu, Shengding and Chen, Yulin and Chan, Chi-Min and Chen, Weize and others},
  journal={Nature Machine Intelligence},
  pages={1--16},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{weininger1988smiles,
  title={SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules},
  author={Weininger, David},
  journal={Journal of chemical information and computer sciences},
  volume={28},
  number={1},
  pages={31--36},
  year={1988},
  publisher={ACS Publications}
}

@article{zhang2024graph,
  title={Graph Masked Autoencoder for Spatio-Temporal Graph Learning},
  author={Zhang, Qianru and Wang, Haixin and Yiu, Siu-Ming and Yin, Hongzhi},
  journal={arXiv preprint arXiv:2410.10915},
  year={2024}
}

@article{sun2024graph,
  title={Graph Fourier Neural ODEs: Bridging Spatial and Temporal Multiscales in Molecular Dynamics},
  author={Sun, Fang and Huang, Zijie and Wang, Haixin and Cao, Yadi and Luo, Xiao and Wang, Wei and Sun, Yizhou},
  journal={arXiv preprint arXiv:2411.01600},
  year={2024}
}

@article{sun2022does,
  title={Does gnn pretraining help molecular representation?},
  author={Sun, Ruoxi and Dai, Hanjun and Yu, Adams Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12096--12109},
  year={2022}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{dao2022flashattention,
  title={Flashattention: Fast and memory-efficient exact attention with io-awareness},
  author={Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={16344--16359},
  year={2022}
}

@article{zhang2025efficient,
  title={Efficient Traffic Prediction Through Spatio-Temporal Distillation},
  author={Zhang, Qianru and Gao, Xinyi and Wang, Haixin and Yiu, Siu-Ming and Yin, Hongzhi},
  journal={arXiv preprint arXiv:2501.10459},
  year={2025}
}

@article{ainslie2023gqa,
  title={Gqa: Training generalized multi-query transformer models from multi-head checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebr{\'o}n, Federico and Sanghai, Sumit},
  journal={arXiv preprint arXiv:2305.13245},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, A},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

@article{team2024chameleon,
  title={Chameleon: Mixed-modal early-fusion foundation models},
  author={Team, Chameleon},
  journal={arXiv preprint arXiv:2405.09818},
  year={2024}
}

@article{wang2023parameter,
  title={Parameter-efficient tuning of large-scale multimodal foundation model},
  author={Wang, Haixin and Yang, Xinlong and Chang, Jianlong and Jin, Dian and Sun, Jinan and Zhang, Shikun and Luo, Xiao and Tian, Qi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={15752--15774},
  year={2023}
}

@article{zhai2023parameter,
  title={When Parameter-efficient Tuning Meets General-purpose Vision-language Models},
  author={Zhai, Yihang and Wang, Haixin and Chang, Jianlong and Yang, Xinlong and Sun, Jinan and Zhang, Shikun and Tian, Qi},
  journal={arXiv preprint arXiv:2312.12458},
  year={2023}
}

@inproceedings{wang2024lion,
  title={Lion: Implicit vision prompt tuning},
  author={Wang, Haixin and Chang, Jianlong and Zhai, Yihang and Luo, Xiao and Sun, Jinan and Lin, Zhouchen and Tian, Qi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={6},
  pages={5372--5380},
  year={2024}
}


@article{krenn2020self,
  title={Self-referencing embedded strings (SELFIES): A 100\% robust molecular string representation},
  author={Krenn, Mario and H{\"a}se, Florian and Nigam, AkshatKumar and Friederich, Pascal and Aspuru-Guzik, Alan},
  journal={Machine Learning: Science and Technology},
  volume={1},
  number={4},
  pages={045024},
  year={2020},
  publisher={IOP Publishing}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{dai2023instructblip,
  title={Instructblip: Towards general-purpose vision-language models with instruction tuning. arxiv 2023},
  author={Dai, Wenliang and Li, Junnan and Li, D and Tiong, AMH and Zhao, J and Wang, W and Li, B and Fung, P and Hoi, S},
  journal={arXiv preprint arXiv:2305.06500},
  volume={2},
  year={2023}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{zhu2023minigpt4,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{zheng2023minigpt5,
  title={Minigpt-5: Interleaved vision-and-language generation via generative vokens},
  author={Zheng, Kaizhi and He, Xuehai and Wang, Xin Eric},
  journal={arXiv preprint arXiv:2310.02239},
  year={2023}
}

@article{koh2024generating,
  title={Generating images with multimodal language models},
  author={Koh, Jing Yu and Fried, Daniel and Salakhutdinov, Russ R},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{vacareanu2024words,
  title={From Words to Numbers: Your Large Language Model Is Secretly A Capable Regressor When Given In-Context Examples},
  author={Vacareanu, Robert and Negru, Vlad-Andrei and Suciu, Vasile and Surdeanu, Mihai},
  journal={arXiv preprint arXiv:2404.07544},
  year={2024}
}

@article{wu2018moleculenet,
  title={MoleculeNet: a benchmark for molecular machine learning},
  author={Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S and Leswing, Karl and Pande, Vijay},
  journal={Chemical science},
  volume={9},
  number={2},
  pages={513--530},
  year={2018},
  publisher={Royal Society of Chemistry}
}

@article{kim2021pubchem,
  title={PubChem in 2021: new data content and improved web interfaces},
  author={Kim, Sunghwan and Chen, Jie and Cheng, Tiejun and Gindulyte, Asta and He, Jia and He, Siqian and Li, Qingliang and Shoemaker, Benjamin A and Thiessen, Paul A and Yu, Bo and others},
  journal={Nucleic acids research},
  volume={49},
  number={D1},
  pages={D1388--D1395},
  year={2021},
  publisher={Oxford University Press}
}

@inproceedings{maho2015pubchemqc,
  title={The PubChemQC project: A large chemical database from the first principle calculations},
  author={Maho, Nakata},
  booktitle={AIP conference proceedings},
  volume={1702},
  year={2015},
  organization={AIP Publishing}
}

@article{kearnes2021open,
  title={The open reaction database},
  author={Kearnes, Steven M and Maser, Michael R and Wleklinski, Michael and Kast, Anton and Doyle, Abigail G and Dreher, Spencer D and Hawkins, Joel M and Jensen, Klavs F and Coley, Connor W},
  journal={Journal of the American Chemical Society},
  volume={143},
  number={45},
  pages={18820--18826},
  year={2021},
  publisher={ACS Publications}
}

@techreport{USPTO2020,
  author      = {USPTO},
  title       = {FY 2020 Performance and Accountability Report},
  institution = {USPTO},
  year        = {2020},
  type        = {Technical report},
  url         = {https://www.uspto.gov/sites/default/files/documents/USPTOFY20PAR.pdf}
}

@article{lu2022unified,
  title={Unified deep learning model for multitask reaction predictions with explanation},
  author={Lu, Jieyu and Zhang, Yingkai},
  journal={Journal of chemical information and modeling},
  volume={62},
  number={6},
  pages={1376--1387},
  year={2022},
  publisher={ACS Publications}
}

@misc{Lowe2017,
  author       = {Lowe, Daniel},
  year         = {2017},
  title        = {Chemical reactions from US patents (1976-Sep2016)},
  howpublished = {\url{https://figshare.com/articles/dataset/Chemical_reactions_from_US_patents_1976-Sep2016_/5104873}},
}

@article{hastings2016chebi,
  title={ChEBI in 2016: Improved services and an expanding collection of metabolites},
  author={Hastings, Janna and Owen, Gareth and Dekker, Adriano and Ennis, Marcus and Kale, Namrata and Muthukrishnan, Venkatesh and Turner, Steve and Swainston, Neil and Mendes, Pedro and Steinbeck, Christoph},
  journal={Nucleic acids research},
  volume={44},
  number={D1},
  pages={D1214--D1219},
  year={2016},
  publisher={Oxford University Press}
}

@article{ruddigkeit2012enumeration,
  title={Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17},
  author={Ruddigkeit, Lars and Van Deursen, Ruud and Blum, Lorenz C and Reymond, Jean-Louis},
  journal={Journal of chemical information and modeling},
  volume={52},
  number={11},
  pages={2864--2875},
  year={2012},
  publisher={ACS Publications}
}

@article{kohn1965self,
  title={Self-consistent equations including exchange and correlation effects},
  author={Kohn, Walter and Sham, Lu Jeu},
  journal={Physical review},
  volume={140},
  number={4A},
  pages={A1133},
  year={1965},
  publisher={APS}
}

@article{schwaller2021prediction,
  title={Prediction of chemical reaction yields using deep learning},
  author={Schwaller, Philippe and Vaucher, Alain C and Laino, Teodoro and Reymond, Jean-Louis},
  journal={Machine learning: science and technology},
  volume={2},
  number={1},
  pages={015016},
  year={2021},
  publisher={IOP Publishing}
}

@article{ahneman2018predicting,
  title={Predicting reaction performance in C--N cross-coupling using machine learning},
  author={Ahneman, Derek T and Estrada, Jes{\'u}s G and Lin, Shishi and Dreher, Spencer D and Doyle, Abigail G},
  journal={Science},
  volume={360},
  number={6385},
  pages={186--190},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@article{perera2018platform,
  title={A platform for automated nanomole-scale reaction screening and micromole-scale synthesis in flow},
  author={Perera, Damith and Tucker, Joseph W and Brahmbhatt, Shalini and Helal, Christopher J and Chong, Ashley and Farrell, William and Richardson, Paul and Sach, Neal W},
  journal={Science},
  volume={359},
  number={6374},
  pages={429--434},
  year={2018},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{liupre,
  title={Pre-training Molecular Graph Representation with 3D Geometry},
  author={Liu, Shengchao and Wang, Hanchen and Liu, Weiyang and Lasenby, Joan and Guo, Hongyu and Tang, Jian},
  booktitle={International Conference on Learning Representations},
  year={2022}
}