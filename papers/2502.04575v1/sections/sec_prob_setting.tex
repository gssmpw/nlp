\section{Problem Setting}
\label{sec:prob_setting}
Motivated by \cite{brosse2018normalizing,ge2020estimating}, given an accuracy threshold $\varepsilon\ll1$, our goal is to study the complexity (measured by the number of calls to the oracles $V$ and $\nabla V$) required to obtain an estimator $\Zh$ of $Z$ such that with $\Omega(1)$ probability, the relative error is within $\varepsilon$:
\begin{equation}
    \prob\ro{\abs{\frac{\Zh}{Z}-1}\le\varepsilon}\ge\frac{3}{4}.
    \label{eq:acc_whp}
\end{equation}

\begin{remark}
We make two remarks regarding this criterion. 
First, similar to how taking the mean of i.i.d. estimates reduces variance, we show in \cref{lem:med_trick} that the probability above can be boosted to any $1-\zeta$
using the \underline{median trick}: obtaining $O\ro{\log\frac{1}{\zeta}}$ i.i.d. estimates satisfying \cref{eq:acc_whp} and taking their median. Therefore, we focus on the task of obtaining a \underline{single} estimate satisfying \cref{eq:acc_whp} hereafter.
Second, \cref{eq:acc_whp} also allows us to quantify the complexity of estimating the free energy $F=-\log Z$, which is often of greater interest in statistical mechanics than the partition function $Z$. We show in \cref{app:guarantee} that estimating $Z$ with $O(\varepsilon)$ \underline{relative} error and estimating $F$ with $O(\varepsilon)$ \underline{absolute} error share the same complexity up to constants. 
Further discussion of this guarantee, including a literature review and the comparison with bias and variance, is deferred to \cref{app:guarantee}.
\label{rmk:guarantee}
\end{remark}

Recall that the rationale behind annealing involves a gradual transition from $\pi_0$, a simple distribution that is easy to sample from and estimate the normalizing constant, to $\pi_1=\pi$, the more complicated target distribution. Throughout this paper, we define a curve of probability measures 
$$\ro{\pi_\theta=\frac{1}{Z_\theta}\e^{-V_\theta}}_{\theta\in[0,1]},$$
where $V_1=V$ is the potential of $\pi$, and the normalizing constant $Z_1=Z$ is what we need to estimate. We do not specify the exact form of this curve now, but only introduce the following mild regularity assumption on the curve, as assumed in classical textbooks such as \cite{ambrosio2008gradient,ambrosio2021lectures,santambrogio2015optimal}:
\begin{assumption}
    The potential $[0,1]\times\R^d\ni(\theta,x)\mapsto V_\theta(x)\in\R$ is jointly $C^1$, and the curve $(\pi_\theta)_{\theta\in[0,1]}$ is AC with finite action $\cA:=\int_0^1\abs{\dot\pi}_\theta^2\d\theta$.
    \label{assu:AC}
\end{assumption}

For the purpose of non-asymptotic analysis, we further introduce the following mild assumption:
\begin{assumption}
    $V$ is $\beta$-smooth, and there exists $x_*$, with $\|x_*\|=:R\lesssim\frac{1}{\sqrt{\beta}}$ such that $\nabla V(x_*)=0$. Moreover, let $m:=\sqrt{\E_\pi\|\cdot\|^2}<\pif$.
    \label{assu:pi}
\end{assumption}
\begin{remark}
    Finding a global minimum of (possibly non-convex) $V$ is challenging, but it is always feasible to find some $x_+$ close to a stationary point $x_*$ using optimization algorithms (e.g., \cite{zhu2018neon2}) within negligible cost compared with the complexity for normalizing constant estimation. By considering the translated distribution $\pi(\cdot-x_+)$, we can assume the existence of a stationary point near $0$. The assumption $R\lesssim\frac{1}{\sqrt{\beta}}$ is for technical purposes in our proof.
\end{remark}

Equipped with this foundational setup, we now proceed to introduce the annealed LD and annealed LMC algorithms, and establish an analysis for JE and AIS.