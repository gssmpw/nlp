\section{Conclusion and Future Work}
\label{sec:conc}
In this paper, we analyzed the complexity of normalizing constant estimation using JE, AIS, and RDS, establishing non-asymptotic convergence guarantees based on insights from continuous-time analysis. Our analysis of JE (\cref{thm:jar_complexity}) applies to general interpolation curves without requiring explicit isoperimetric assumptions, which significantly extends prior work limited to log-concave distributions. While our main results (\cref{thm:jar_complexity,thm:ais_complexity}) provide upper complexity bounds, their tightness remains an open question. Deriving general lower bounds would further clarify whether curves with large action inherently require more oracle calls for both sampling and normalizing constant estimation, thereby rigorously validating the arguments in \cref{sec:revdif}. We also conjecture that our proof techniques can be further extended to samplers beyond overdamped LD (e.g., Hamiltonian or underdamped LD \citep{sohl2012hamiltonian}), and may be applied to estimating normalizing constants of compactly supported distributions on $\R^d$ (e.g., convex bodies volume estimation \citep{cousins2018gaussian}) and discrete distributions (e.g., Ising model and restricted Boltzmann machines \citep{huber2015approximation,krause2020algorithms}) via the Poisson stochastic integral framework \citep{ren2025how,ren2025fast}. We leave these directions for future research.