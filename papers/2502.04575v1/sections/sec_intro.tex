\section{Introduction}
\label{sec:intro}
We study the problem of estimating the normalizing constant $Z=\int_{\R^d}\e^{-V(x)}\d x$ of an unnormalized probability density function (p.d.f.) $\pi\propto\e^{-V}$ on $\R^d$, so that $\pi(x)=\frac{1}{Z}\e^{-V(x)}$. The normalizing constant appears in various fields: in Bayesian statistics, when $\e^{-V}$ is the product of likelihood and prior, $Z$ is also referred to as the marginal likelihood or evidence \citep{gelman2013bayesian}; in statistical mechanics, when $V$ is the Hamiltonian\footnote{Up to a multiplicative constant $\beta=\frac{1}{k_\mathrm{B}T}$ known as the thermodynamic beta, where $k_\mathrm{B}$ is the Boltzmann constant and $T$ is the temperature. When borrowing terminologies from physics, we ignore this quantity for simplicity.}, $Z$ is known as the partition function, and $F:=-\log Z$ is called the free energy \citep{chipot2007free,lelievre2010free,pohorille2010good}. The task of normalizing constant estimation has numerous applications, including computing log-likelihoods in probabilistic models \citep{sohl2012hamiltonian}, estimating free energy differences \citep{lelievre2010free}, and training energy-based models in generative modeling \citep{song2021how,carbone2023efficient,sander2025joint}. It is challenging in high dimensions or when $\pi$ is multimodal (i.e., $V$ has a complex landscape).

Conventional approaches based on importance sampling \citep{meng1996simulating} are widely adopted to tackle this problem, but they suffer from high variance due to the mismatch between target and proposal distributions when the target distribution is complicated \citep{chatterjee2018the}. To alleviate this issue, the technique of annealing tries constructing a sequence of intermediate distributions that bridge these two distributions, which motivates several popular methods including path sampling \citep{chen1997on,gelman1998simulating}, annealed importance sampling (AIS, \cite{neal2001annealed}), and sequential Monte Carlo (SMC, \cite{doucet2000sequential,delmoral2006sequential,syed2024optimised}) in statistics literature, as well as thermodynamic integration (TI, \cite{kirkwood1935statistical}) and Jarzynski equality (JE, \cite{jarzynski1997nonequilibrium,ge2008generalized,hartmann2019jarzynski}) in statistical mechanics literature. In particular, JE points out the connection between the free energy difference between two states and the work done over a series of trajectories linking these two states, while AIS constructs a sequence of intermediate distributions and estimates the normalizing constant by importance sampling over these distributions. These two methods are our primary focus in this paper.

Despite the empirical success of annealing-based methods \citep{ma2013estimating,krause2020algorithms,mazzanti2020efficient,yasuda2022free,chen2024ensemble,schonle2024sampling}, the theoretical understanding of their performance is still limited. Existing works for importance sampling mainly focus on the asymptotic bias and variance of the estimator \citep{meng1996simulating,gelman1998simulating}, while works on JE usually simplify the problem by assuming the work follows simple distributions (e.g., Gaussian or gamma) \citep{echeverria2012,arrar2019on}. Moreover, only analyses asymptotic in the number of particles derived from central limit theorem
exist \cite[Sec. 4.1]{lelievre2010free}. In this paper, we aim to establish a rigorous non-asymptotic analysis of estimators based on JE and AIS, while introducing minimal assumptions on the target distribution. Moreover, we also propose a new algorithm based on reverse diffusion samplers to tackle a potential shortcoming of AIS.

\paragraph{Contributions.} Our key technical contributions are summarized as follows.
\begin{itemize}[wide=0pt,itemsep=0pt, topsep=0pt,parsep=0pt,partopsep=0pt]
    \item We discover a novel strategy for analyzing the complexity of normalizing constant estimation,
    applicable to a wide range of target distributions (see \cref{assu:pi,assu:AC}) that may not satisfy isoperimetric conditions such as log-concavity.
    \item In \cref{sec:jar}, we study JE
    and prove an upper bound on the time required for running the annealed Langevin dynamics to estimate the normalizing constant within $\varepsilon$ relative error with high probability. The final bound depends on the action of the curve, specifically the integral of the squared metric derivative in Wasserstein-2 distance.
    \item Building on the insights from the analysis of the continuous dynamics, in \cref{sec:ais} we
    establish the first non-asymptotic oracle complexity bound for AIS, representing the first analysis of normalizing constant estimation algorithms without assuming a log-concave target distribution.
    \item Finally, in \cref{sec:revdif}, we point out a potential limitation of the geometric interpolation commonly used in annealing. To address this issue, we propose a novel algorithm based on reverse diffusion samplers and build up a framework for analyzing its oracle complexity.
\end{itemize}

\paragraph{Related works.} We briefly review some related works, and defer detailed discussion to \cref{app:rel_work}.
\begin{itemize}[wide=0pt,itemsep=0pt, topsep=0pt,parsep=0pt,partopsep=0pt]
    \item \underline{Methods for normalizing constant estimation.} We mainly discuss two classes of methods here. First, the \emph{equilibrium} methods, such as TI \citep{kirkwood1935statistical} and its variants \citep{brosse2018normalizing,ge2020estimating,chehab2023provable,kook2024sampling}, which involve sampling sequentially from a series of equilibrium Markov transition kernels. Second, the \emph{non-equilibrium} methods, such as AIS \citep{neal2001annealed}, which samples from a non-equilibrium SDE that gradually evolves from a prior distribution to the target distributions. In \cref{app:rel_work_ti}, we show that TI is a special case of AIS using the ``perfect'' transition kernels.4 Recent years have also witnessed the emergence of \textit{learning-based} non-equilibrium methods for normalizing constant estimation, which are typically byproducts of sampling algorithms \citep{zhang2022path,nusken2021solving,richter2024improved,sun2024dynamical,vargas2024transport,albergo2024nets,blessing2025underdamped,chen2025sequential}. Additionally, there are also several methods based on particle filtering (e.g., \citet{kostov2017algorithm,jasra2018multilevel,ruzayqat2022multilevel}).
    \item \underline{Variance reduction in JE and AIS.} Our poof methodology focuses on the discrepancy between the sampling path measure and the reference path measure, which is related to the variance reduction technique in applying JE and AIS. For example, \cite{vaikuntanathan2008escorted} introduced the idea of escorted simulation, \cite{hartmann2017variational} proposed a method for learning the optimal control protocol in JE through the variational characterization of free energy, and \cite{doucet2022score} leveraged score-based generative model to learn the optimal backward kernel. Quantifying the discrepancy between path measures is the core of our analysis.
    \item \underline{Complexity analysis for normalizing constant estimation.} \cite{chehab2023provable} studied the asymptotic statistical efficiency of the curve for TI measured by the asymptotic mean-squared error, and highlighted the advantage of the geometric interpolation. In terms of non-asymptotic analysis, existing works mainly rely on the isoperimetry of the target distribution. For instance, \cite{andrieu2016sampling}  derived bounds of bias and variance for TI under Poincar\'e inequality, \cite{brosse2018normalizing} provided complexity guarantees for TI under both strong and weak log-concavity conditions, while \cite{ge2020estimating} improved the complexity under strong log-concavity using multilevel Monte Carlo.
\end{itemize}