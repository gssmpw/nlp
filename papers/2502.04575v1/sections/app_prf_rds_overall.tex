\subsection{Discussion on the Overall Complexity of RDS}
\label{app:revdif_overall}
In RDS, an accurate score estimate $s_\cdot\approx\nabla\log\pib_\cdot$ is critical for the algorithmic efficiency. Existing methods estimate scores through different ways. Here, we review the existing methods and their complexity guarantees for sampling, and leverage \cref{thm:revdif} to derive the complexity of normalizing constant estimation. Throughout this section, we always assume that the target distribution $\pi\propto\e^{-V}$ satisfies $m^2:=\E_\pi\|\cdot\|^2<\infty$ and that $V$ is $\beta$-smooth.

\paragraph{(I) Reverse diffusion Monte Carlo.} The seminal work directly leveraged the following Tweedie's formula \citep{robbins1992an} to estimate the score:
\cite{huang2024reverse} 
\begin{equation}
    \nabla\log\pib_t(x)=\E_{\pib_{0|t}(x_0|x)}\frac{\e^{-t}x_0-x}{1-\e^{-2t}},    
    \label{eq:rds_score}
\end{equation}
where
\begin{equation}
    \pib_{0|t}(x_0|x)\propto_{x_0}\exp\ro{-V(x_0)-\frac{\|x_0-\e^tx\|^2}{2(\e^{2t}-1)}}
    \label{eq:rds_post}
\end{equation}
is the posterior distribution of $Y_0$ conditional on $Y_t=x$ in the OU process (\cref{eq:ou}). The paper proposed to sample from $\pib_{0|t}(\cdot|x)$ by LMC and estimate the score via empirical mean, which has a provably better LSI constant than the target distribution $\pi$ (see \citet[Lem. 2]{huang2024reverse}). They show that if the scores $\nabla\log\pib_t$ are uniformly $\beta$-Lipschitz, and assume that there exists some $c>0$ and $n>0$ such that for any $r>0$, $V+r\|\cdot\|^2$ is convex for $\|x\|\ge\frac{c}{r^n}$, then w.p. $\ge1-\zeta$, the overall complexity for guaranteeing $\kl(\Q\|\Qd)\lesssim\varepsilon^2$ is $$O\ro{\poly\ro{d,\frac1\zeta}\exp\ro{\frac{1}{\varepsilon}}^{O(n)}},$$
which is also the complexity of obtaining a $\Zh$ satisfying \cref{eq:acc_whp}.

\paragraph{(II) Recursive score diffusion-based Monte Carlo.} A second work \cite{huang2024faster} proposed to estimate the scores in a recursive scheme. Assuming the scores $\nabla\log\pib_t$ are uniformly $\beta$-Lipschitz, they established a complexity
$$\exp\ro{\beta^3\log^3\ro{\beta,d,m^2,\frac{1}{\zeta}}}$$
in order to guarantee $\kl(\Q\|\Qd)\lesssim\varepsilon^2$ w.p. $\ge1-\zeta$.

\paragraph{(III) Zeroth-order diffusion Monte Carlo.} The following work \cite{he2024zeroth} proposed a zeroth-order method that leverages rejection sampling to sample from $\pib_{0|t}(\cdot|x)$. When $V$ is $\beta$-smooth, they showed that with a small early stopping time $\delta$, the overall complexity for guaranteeing $\kl(\Q\|\Qd)\lesssim\varepsilon^2$ is $$\exp\ro{\Ot(d)\log\beta\log\frac{1}{\varepsilon}}.$$

\paragraph{(IV) Self-normalized estimator.} Finally, a recent work \cite{vacher2025polynomial} proposed to estimate the scores in a different approach:
$$\nabla\log\pib_t(x)=-\frac{1}{1-\e^{-2t}}\frac{\E[\xi\e^{-V(\e^t(x-\xi))}]}{\E[\e^{-V(\e^t(x-\xi))}]},\quad\text{where}~\xi\sim\n{0,(1-\e^{-2t})I}.$$
Assume that $V$ is $\beta$-smooth, and the distributions along the OU process starting from $\pi\propto\e^{-V}$ and $\pi'\propto\e^{-2V}$ have potentials whose Hessians are uniformly $\succeq\beta I$, then the complexity for guaranteeing $\E\kl(\Q\|\Qd)\lesssim\varepsilon^2$ is
$$O\ro{\ro{\frac{\beta(m^2\vee d)}{\varepsilon}}^{O(d)}}.$$