\subsection{Proof of the Second Part of \cref{lem:ais_est_z0}}
\label{app:rel_work_ti_prf}
Recall that our goal is to estimate $\pi_0$'s normalizing constant $Z_0=\int\e^{-V_0}\d x$, where $V_0$ is $\beta$-strongly convex and $3\beta$-smooth, with global minimizer $x'$ satisfying $\|x'\|\le R\lesssim\frac{1}{\sqrt{\beta}}$. The aim is to obtain an estimator $\Zh_0\approx Z_0$ such that
\begin{equation}
    \prob(\cF)\le\frac{1}{8},~\text{where}~\cF:=\cu{\abs{\frac{\Zh_0}{Z_0}-1}\ge\frac{\varepsilon}{8}}.
    \label{eq:ti_aim}
\end{equation}

Following the discussion above, the TI algorithm goes as follows. Consider a sequence of non-negative numbers $\lambda_0>\lambda_1>...>\lambda_K=0$, where there exists a common $\gamma_0>0$ such that $\lambda_k=\ro{1+\gamma_0}\lambda_{k+1}$, for all $k\in\sqd{0,K-2}$. Let $\rho_k:=\frac{1}{\zeta_k}\e^{-f_k}$, where $f_k:=V_0+\frac{\lambda_k}{2}\|\cdot\|^2$ is $(\beta+\lambda_k)$-strongly-convex and $(3\beta+\lambda_k)$-smooth. One can write
$$Z_0=\zeta_K=\zeta_0\prod_{k=0}^{K-1}\underbrace{\frac{\zeta_{k+1}}{\zeta_k}}_{=:G_k},~\text{where}~G_k=\E_{\rho_k}\underbrace{\exp\ro{\frac{\lambda_k-\lambda_{k+1}}{2}\|\cdot\|^2}}_{=:g_k},$$
and estimate each $G_k$ by
$$\Gh_k:=\frac{1}{N}\sum_{n=1}^Ng_k(\Xh_n^{(k)}),~\Xh_n^{(k)}\iid\rhoh_k\approx\rho_k,$$
so the final estimator is $\Zh_0:=\zetah_0\prod_{k=0}^{K-1}\Gh_k$, in which $\zetah_0\approx\zeta_0$. To proceed, we first prove the following lemma.

\begin{lemma}
    If
    \begin{enumerate}[wide=0pt,itemsep=0pt, topsep=0pt,parsep=0pt,partopsep=0pt]
        \item $\tv(\rhoh_k,\rho_k)\le\delta\asymp\frac{1}{NK}$, for all $k\in\sqd{0,K-1}$.
        \item The estimate $\zetah_0$ satisfies $\abs{\log\frac{\zetah_0}{\zeta_0}}\lesssim\varepsilon$.
        \item For all $k\in\sqd{0,K-1}$, the following equation holds:
              \begin{equation}
                  \frac{\E_{\rho_k}g_k^2}{\ro{\E_{\rho_k}g_k}^2}\le1+O(1).
                  \label{eq:ti_rel_var}
              \end{equation}
    \end{enumerate}
    Then with $N\asymp\frac{K}{\varepsilon^2}$, \cref{eq:ti_aim} holds.
    \label{lem:ti_main}
\end{lemma}

\begin{proof}
    By definition of TV distance, for each pair of $(n,k)$ one can construct a random variable $X_n^{(k)}\sim\rho_k$ that only depends on $\Xh_n^{(k)}$ and satisfies $\prob\ro{\Xh_n^{(k)}\ne X_n^{(k)}}\le\delta$. Define the event
    $$\cE=\cu{\Xh_n^{(k)}=X_n^{(k)}:\forall n\in\sqd{1,N},k\in\sqd{0,K-1}}.$$
   By independence, $\prob(\cE)\ge(1-\delta)^{NK}\ge1-\delta NK\gtrsim1$. If $\prob(\cF|\cE)\le\frac{1}{16}$ and $\prob(\cE^\complement)\le\frac{1}{16}$, then
    $$\prob(\cF)=\prob(\cF|\cE)\prob(\cE)+\prob(\cF|\cE^\complement)\prob(\cE^\complement)\le\prob(\cF|\cE)+\prob(\cE^\complement)\le\frac{1}{8},$$
    as desired.

    To obtain $\prob(\cF|\cE)\le\frac{1}{16}$, from now on we \emph{always} assume that $\cE$ happens, and omit the conditional notation $(\cdot|\cE)$ in probability and expectation for simplicity. Note that in this case, $\Gh_k=\frac{1}{N}\sum_{n=1}^Ng_k(X_n^{(k)})$, $X_n^{(k)}\iid\rho_k$, so $\E\Gh_k=G_k$. One can upper bound the probability of large relative error as follows, leveraging \cref{lem:logat1}:
    \begin{align*}
        \prob\ro{\abs{\frac{\Zh_0}{Z_0}-1}\gtrsim\varepsilon} & \le\prob\ro{\abs{\log\frac{\Zh_0}{Z_0}}\gtrsim\varepsilon}=\prob\ro{\abs{\log\frac{\zetah_0}{\zeta_0}+\log\prod_{k=0}^{K-1}\frac{\Gh_k}{G_k}}\gtrsim\varepsilon} \\
                                                           & \le\prob\ro{\abs{\log\frac{\zetah_0}{\zeta_0}+\log\prod_{k=0}^{K-1}\frac{\Gh_k}{G_k}}\gtrsim\varepsilon}                                                      \\
                                                           & \le\prob\ro{\abs{\log\prod_{k=0}^{K-1}\frac{\Gh_k}{G_k}}\gtrsim\varepsilon}\le\prob\ro{\abs{\prod_{k=0}^{K-1}\frac{\Gh_k}{G_k}-1}\gtrsim\varepsilon}             \\
                                                           & \lesssim\frac{1}{\varepsilon^2}\E\ro{\prod_{k=0}^{K-1}\frac{\Gh_k}{G_k}-1}^2=\frac{1}{\varepsilon^2}\ro{\prod_{k=0}^{K-1}\frac{\E\Gh_k^2}{G_k^2}-1},
    \end{align*}
    where the last line is due to Markov inequality. Choosing $N\asymp\frac{K}{\varepsilon^2}$ yields
    $$\frac{\E\Gh_k^2}{G_k^2}-1=\frac{\var\Gh_k^2}{G_k^2}=\frac{\E_{\rho_k}g_k^2-\ro{\E_{\rho_k}g_k}^2}{N\ro{\E_{\rho_k}g_k}^2}\lesssim\frac{1}{N},$$
    which implies
    \begin{align*}
        \prob\ro{\abs{\frac{\Zh_0}{Z_0}-1}\gtrsim\varepsilon} & \lesssim\frac{1}{\varepsilon^2}\ro{\prod_{k=0}^{K-1}\frac{\E\Gh_k^2}{G_k^2}-1}\le\frac{1}{\varepsilon^2}\ro{\ro{1+\frac{1}{N}}^K-1}\lesssim\frac{K}{N\varepsilon^2}\lesssim1.
    \end{align*}
\end{proof}

The following lemmas show how to accurately estimate $\zeta_0$ and how to satisfy \cref{eq:ti_rel_var}.

\begin{lemma}
    With $\lambda_0\asymp\frac{d\beta}{\varepsilon}$, $\zetah_0:=\exp\ro{-V_0(0)+\frac{\|\nabla V_0(0)\|^2}{2(3\beta+\lambda_0)}}\ro{\frac{2\pi}{3\beta+\lambda_0}}^\frac{d}{2}$ satisfies $\abs{\log\frac{\zetah_0}{\zeta_0}}\lesssim\varepsilon$.
    \label{lem:ti_est_zeta0}
\end{lemma}
\begin{proof}
    By assumption, $f_0$ is $(\beta+\lambda_0)$-strongly-convex and $(3\beta+\lambda_0)$-smooth. Using quadratic upper and lower bounds on $f_0$,
    \begin{align*}
        \exp\ro{-f_0(0)+\frac{\|\nabla f_0(0)\|^2}{2(3\beta+\lambda_0)}}\ro{\frac{2\pi}{3\beta+\lambda_0}}^\frac{d}{2}\le\zeta_0 \le\exp\ro{-f_0(0)+\frac{\|\nabla f_0(0)\|^2}{2(\beta+\lambda_0)}}\ro{\frac{2\pi}{\beta+\lambda_0}}^\frac{d}{2}.
    \end{align*}
    Since $f_0(0)=V_0(0)$, $\|\nabla f_0(0)\|=\|\nabla V_0(0)\|=\|\nabla V_0(0)-\nabla V_0(x')\|\le3\beta\|x'\|\lesssim\sqrt{\beta}$,
    \begin{align*}
        1\le\frac{\zeta_0}{\zetah_0}\le\exp\ro{\frac{\beta\|\nabla V_0(0)\|^2}{(\beta+\lambda_0)(3\beta+\lambda_0)}}\ro{1+\frac{2\beta}{\beta+\lambda_0}}^\frac{d}{2}\le\exp\ro{\frac{\beta^2}{(\beta+\lambda_0)(3\beta+\lambda_0)}+\frac{d\beta}{\beta+\lambda_0}}.
    \end{align*}
    So $\lambda_0\asymp\frac{d\beta}{\varepsilon}$ implies $\frac{\beta^2}{(\beta+\lambda_0)(3\beta+\lambda_0)}+\frac{d\beta}{\beta+\lambda_0}\lesssim\varepsilon$.
\end{proof}

\begin{lemma}
    For $k=K-1$, $\lambda_{k}\asymp\frac{\beta}{\sqrt{d}}$ implies \cref{eq:ti_rel_var}.
    \label{lem:ti_rel_var_min_noise}
\end{lemma}
\begin{proof}
    When $k=K-1$, $g_k=\exp\ro{\frac{\lambda_k}{2}\|\cdot\|^2}$. We have
    \begin{align*}
        \frac{\E_{\rho_k}g_k^2}{\ro{\E_{\rho_k}g_k}^2} & =\E_{\pi_0}\exp\ro{\frac{\lambda_k}{2}\|\cdot\|^2}\E_{\pi_0}\exp\ro{-\frac{\lambda_k}{2}\|\cdot\|^2}.
    \end{align*}
    Define
    $$h_1(\lambda):=\E_{\pi_0}\exp\ro{\lambda\|\cdot\|^2}\E_{\pi_0}\exp\ro{-\lambda\|\cdot\|^2},~\lambda\in\sq{0,\frac{\beta}{4}}.$$
    One can take derivative to obtain
    $$\de{}{\lambda}\log h_1(\lambda)=\int_{-\lambda}^{\lambda}\var_{\rhob_s}\|\cdot\|^2\d s,$$
    where $\rhob_s\propto\exp(-V_0+s\|\cdot\|^2)$ is $\frac{\beta}{2}$-strongly-log-concave and thus satisfies $\frac{2}{\beta}$-LSI. Hence,
    $$\var_{\rhob_s}\|\cdot\|^2\le\frac{8}{\beta}\E_{\rhob_s}\|\cdot\|^2.$$
    Let $x'_s$ be the global minimizer of $V_0-s\|\cdot\|^2$. By \cref{lem:glob_min}, $\|x'_s\|\le R$. Leveraging \cref{lem:2ordmomlogccv}, we have
    $$\var_{\rhob_s}\|\cdot\|^2\lesssim\frac{1}{\beta}\ro{\E_{\rhob_s}\|\cdot-x'_s\|^2+\|x'_s\|^2}\le\frac{1}{\beta}\ro{\frac{2d}{\beta}+R^2}\lesssim\frac{d}{\beta^2}.$$
    So $\de{}{\lambda}\log h_1(\lambda)\lesssim\frac{\lambda d}{\beta^2}$, and thus
    $$\frac{\E_{\rho_k}g_k^2}{\ro{\E_{\rho_k}g_k}^2}=h_1\ro{\frac{\lambda_k}{2}}=\exp\ro{O\ro{\frac{\lambda_k^2d}{\beta^2}}}=1+O\ro{\frac{\lambda_k^2d}{\beta^2}}=1+O(1).$$
\end{proof}

\begin{lemma}
    For $k\in\sqd{0,K-2}$, \cref{eq:ti_rel_var} holds with $\gamma_0=\frac{1}{\sqrt{d}}$.
    \label{lem:ti_rel_var_others}
\end{lemma}
\begin{proof}
    One can write $g_k=\exp\ro{\frac{\gamma_0\lambda_{k+1}}{2}\|\cdot\|^2}$. Simple calculation yields
    \begin{align*}
        \frac{\E_{\rho_k}g_k^2}{\ro{\E_{\rho_k}g_k}^2} & =\frac{\E_{\pi_0}\exp\ro{-\frac{(1+\gamma_0)\lambda_{k+1}}{2}\|\cdot\|^2}\E_{\pi_0}\exp\ro{-\frac{(1-\gamma_0)\lambda_{k+1}}{2}\|\cdot\|^2}}{{\E_{\pi_0}\exp\ro{-\frac{\lambda_{k+1}}{2}\|\cdot\|^2}}^2}.
    \end{align*}
    Define
    $$h_2(\gamma):=\E_{\pi_0}\exp\ro{-\frac{(1+\gamma)\lambda}{2}\|\cdot\|^2}\E_{\pi_0}\exp\ro{-\frac{(1-\gamma)\lambda}{2}\|\cdot\|^2},~\gamma\in\sq{0,\frac{1}{2}}.$$
    One can similarly show
    $$\de{}{\gamma}\log h_2(\gamma)=\frac{\lambda^2}{4}\int_{1-\gamma}^{1+\gamma}\var_{\rhot_t}\|\cdot\|^2\d t,$$
    where $\rhot_t\propto\exp\ro{-V_0-\frac{t\lambda}{2}\|\cdot\|^2}$ is $(\beta+t\lambda)$-strongly-log-concave and thus satisfies $\frac{1}{\beta+t\lambda}$-LSI. Hence, $\var_{\rhot_t}\|\cdot\|^2\le\frac{8}{\beta+t\lambda}\E_{\rhot_t}\|\cdot\|^2$.

    Let $x''_t$ be the global minimizer of $V_0+\frac{t\lambda}{2}\|\cdot\|^2$. By \cref{lem:glob_min}, $\|x''_t\|\le\frac{R}{1+\frac{t\lambda}{3\beta}}$. Therefore,
    $$\var_{\rhot_t}\|\cdot\|^2\lesssim\frac{1}{\beta+t\lambda}\ro{\E_{\rhot_t}\|\cdot-x''_t\|^2+\|x''_t\|^2}\lesssim\frac{1}{\beta+t\lambda}\ro{\frac{d}{\beta+t\lambda}+\frac{\beta^2R^2}{\ro{\beta+t\lambda}^2}}.$$
    As a result,
    \begin{align*}
        \de{}{\gamma}\log h_2(\gamma)            & \lesssim\lambda^2\int_{1-\gamma}^{1+\gamma}\frac{1}{\beta+t\lambda}\ro{\frac{d}{\beta+t\lambda}+\frac{\beta^2R^2}{\ro{\beta+t\lambda}^2}}\d t \\
                                                 & \le\lambda^2\int_{1-\gamma}^{1+\gamma}\frac{1}{t\lambda}\ro{\frac{d}{t\lambda}+\frac{\beta^2R^2}{t^2\lambda^2}}\d t                           \\
                                                 & \lesssim\lambda^2\gamma\cdot\frac{1}{\lambda}\ro{\frac{d}{\lambda}+\frac{\beta^2R^2}{\lambda^2}}=\gamma\ro{d+\frac{\beta^2R^2}{\lambda}}       \\
        \implies\log\frac{h_2(\gamma_0)}{h_2(0)} & \lesssim\gamma_0^2\ro{d+\frac{\beta^2R^2}{\lambda}}=1+\frac{\beta^2R^2}{d\lambda}.
    \end{align*}
    Since $\lambda_{k+1}\ge\lambda_{K-1}\asymp\frac{\beta}{\sqrt[]{d}}$ and $R\lesssim\frac{1}{\sqrt[]{\beta}}$, $\frac{\beta^2R^2}{d\lambda_{k+1}}\lesssim1$, so $\frac{\E_{\rho_k}g_k^2}{\ro{\E_{\rho_k}g_k}^2}\le1+O(1)$.
\end{proof}

Finally, one can compute the total complexity as follows. The choice $\lambda_0\asymp\frac{d\beta}{\varepsilon}$, $\lambda_{K-1}\asymp\frac{\beta}{\sqrt[]{d}}$, and $\lambda_k=\ro{1+\frac{1}{\sqrt[]{d}}}\lambda_{k+1}$ implies $K=\Thetat(\sqrt{d})$, and thus $N\asymp\frac{K}{\varepsilon^2}=\Thetat\ro{\frac{\sqrt{d}}{\varepsilon^2}}$. For each $k$, it is necessary to obtain $N$ i.i.d. approximate samples from $\rho_k$ that are $\delta\asymp\frac{1}{NK}=\Thetat\ro{\frac{\varepsilon^2}{d}}$-close in TV distance. Using proximal sampler \citep{fan2023improved}, the complexity for obtaining one sample is $\Ot(\sqrt{d})$ (note that the condition numbers of $f_k$'s are uniformly bounded by $3$), so the total oracle complexity is $NK\cdot\Ot(\sqrt{d})=\Ot\ro{\frac{d^\frac{3}{2}}{\varepsilon^2}}$. 
\hfill$\square$