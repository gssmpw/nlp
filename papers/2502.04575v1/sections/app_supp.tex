\section{Supplementary Lemmas}
\label{app:supp}
\begin{lemma}
    For $x>0$ and $\varepsilon\in\ro{0,\frac{1}{2}}$, define $x_0:=|\log x|$ and $x_1:=|x-1|$. Then $x_i\ge\varepsilon$ implies $x_{1-i}\ge\frac{\varepsilon}{2}$, and $x_i\le\varepsilon$ implies $x_{1-i}\le2\varepsilon$, for both $i=0,1$.
    \label{lem:logat1}
\end{lemma}
This follows from the standard calculus approximation $\log x\approx x-1$ when $x\approx1$. The proof is straightforward and is left as an exercise for the reader.

\begin{lemma}
    For any $0\le a\le b\le1$ and $r\ge1$, $b^r-a^r\le r(b-a)$.
    \label{lem:power_r_diff}
\end{lemma}
\begin{proof}
    This is immediate from the decreasing property of the function $\varphi(x):=x^r-rx$, $x\in[0,1]$, since $\varphi'(x)=r(x^{r-1}-1)\le0$.
\end{proof}

\begin{lemma}[The median trick \citep{jerrum1986random}]
    Let $\Zh_1,...,\Zh_N$ be $N(\ge3)$ i.i.d. random variables satisfying 
    $$\prob\ro{\abs{\frac{\Zh_n}{Z}-1}\le\varepsilon}\ge\frac{3}{4},~\forall n\in\sqd{1,N},$$
    and let $\Zh_*$ be the median of $\Zh_1,...,\Zh_N$. Then
    $$\prob\ro{\abs{\frac{\Zh_*}{Z}-1}\le\varepsilon}\ge1-\e^{-\frac{N}{72}}.$$
    In particular, for any $\zeta\in\ro{0,\frac14}$, choosing $N=\ceil{72\log\frac{1}{\zeta}}$, the probability is at least $1-\zeta$.
    \label{lem:med_trick}
\end{lemma}
\begin{proof}
    Let $A_n:=\cu{\abs{\frac{\Zh_n}{Z}-1}>\varepsilon}$, which are i.i.d. events happening w.p. $p\le\frac{1}{4}$. If $\abs{\frac{\Zh_*}{Z}-1}>\varepsilon$, then there are at least $\floor{\frac{N}{2}}$ $A_n$'s happening, i.e., $S_N:=\sum_{n=1}^N1_{A_n}\ge\floor{\frac{N}{2}}$. Then,
    \begin{align*}
    \prob\ro{\abs{\frac{\Zh_*}{Z}-1}>\varepsilon}&\le\prob\ro{S_N\ge\floor{\frac{N}{2}}}=\prob\ro{S_N-\E S_N\ge\floor{\frac{N}{2}}-pN}\\ 
    &\le\prob\ro{S_N-\E S_N\ge\frac{N}{12}}\le\e^{-\frac{N}{72}},
    \end{align*}
    where the first inequality on the second line follows from the fact that $\floor{\frac{N}{2}}\ge\frac{N-1}{2}\ge\frac{N}{3}$ for all $N\ge3$, and the last inequality is due to the Hoeffding's inequality.
\end{proof}

\begin{lemma}
    The update rule of AIS (\cref{eq:ais_ker_fh}) is:
    $$X_{T_\l}=\e^{-\varLambda(T_\l)}X_0-\ro{\int_0^{T_\l}\e^{-(\varLambda(T_\l)-\varLambda(t))}\d t}\nabla V(X_0)+\ro{2\int_0^{T_\l}\e^{-2(\varLambda(T_\l)-\varLambda(t))}\d t}^\frac{1}{2}\xi,$$
    where $\varLambda(t):=\int_0^t\lambda\ro{\theta_{\l-1}+\frac{\tau}{T_\l}(\theta_\l-\theta_{\l-1})}\d\tau$, and $\xi\sim\n{0,I}$ is independent of $X_0$.
    \label{lem:ais_ker_fh_update}
\end{lemma}

\begin{proof}
    By It\^o's formula, we have
    $$\d\ro{\e^{\varLambda(t)}X_t}=\e^{\varLambda(t)}\ro{\varLambda'(t)X_t\d t+\d X_t}=\e^{\varLambda(t)}\ro{-\nabla V(X_0)\d t+\sqrt{2}\d B_t}.$$

    Integrating over $t\in[0,T_\l]$, we obtain
    $$\e^{\varLambda(T_\l)}X_{T_\l}-X_0=-\ro{\int_{0}^{T_\l}\e^{\varLambda(t)}\d t}\nabla V(X_0)+\sqrt{2}\int_0^{T_\l}\e^{\varLambda(t)}\d B_t,$$
    $$\implies X_{T_\l}=\e^{-\varLambda(T_\l)}X_0-\ro{\int_0^{T_\l}\e^{-(\varLambda(T_\l)-\varLambda(t))}\d t}\nabla V(X_0)+\sqrt{2}\int_0^{T_\l}\e^{-(\varLambda(T_\l)-\varLambda(t))}\d B_t,$$
    and $\sqrt{2}\int_0^{T_\l}\e^{-(\varLambda(T_\l)-\varLambda(t))}\d B_t\sim\n{0,\ro{2\int_0^{T_\l}\e^{-2(\varLambda(T_\l)-\varLambda(t))}\d t}I}$ by It\^o isometry.
\end{proof}

\begin{lemma}
    The update rule of the RDS (\cref{eq:ou_rev_score}) is
    $$X_{t_{k+1}}=\e^{t_{k+1}-t_k}X_{t_k}+2(\e^{t_{k+1}-t_k}-1)s_{T-t_k}(X_{t_k})+\Xi_k,$$
    where 
    $$\Xi_k:=\int_{t_k}^{t_{k+1}}\sqrt{2}\e^{-(t-t_{k+1})}\d B_t\sim\n{0,(\e^{2(t_{k+1}-t_k)}-1)I},$$
    and the correlation matrix between $\Xi_k$ and $B_{t_{k+1}}-B_{t_k}$ is
    $$\corr(\Xi_k,B_{t_{k+1}}-B_{t_k})=\frac{\sqrt{2}(\e^{t_{k+1}-t_k}-1)}{\sqrt{(\e^{2(t_{k+1}-t_k)}-1)(t_{k+1}-t_k)}}I.$$
    \label{lem:rds_update}
\end{lemma}

\begin{proof}
    By applying It\^o's formula to \cref{eq:ou_rev_score} for $t\in[t_k,t_{k+1}]$, we have
    \begin{align*}
        \d(\e^{-t}X_t)&=\e^{-t}(-X_t\d t+\d X_t)=\e^{-t}(2s_{T-t_k}(X_{t_k})\d t+\sqrt[]{2}\d B_t)\\
        \implies\e^{-t_{k+1}}X_{t_{k+1}}-\e^{-t_k}X_{t_k}&=2(\e^{-t_k}-\e^{-t_{k+1}})s_{T-t_k}(X_{t_k})+\int_{t_k}^{t_{k+1}}\sqrt{2}\e^{-t}\d B_t.
    \end{align*}
    The covariance between two zero-mean Gaussian random variables $\Xi_k$ and $B_{t_{k+1}}-B_{t_k}$ is
    \begin{align*}
        \cov(\Xi_k,B_{t_{k+1}}-B_{t_k})&=\E\sq{\Xi_k(B_{t_{k+1}}-B_{t_k})\tp}\\
        &=\E\sq{\ro{\int_{t_k}^{t_{k+1}}\sqrt{2}\e^{-(t-t_{k+1})}\d B_t}\ro{\int_{t_k}^{t_{k+1}}\d B_t}\tp}\\
        &=\int_{t_k}^{t_{k+1}}\sqrt{2}\e^{-(t-t_{k+1})}\d t\cdot I=\sqrt{2}(\e^{t_{k+1}-t_k}-1)I.
    \end{align*}
    Finally, $\corr(u,v)=\diag(\cov u)^{-\frac12}\cov(u,v)\diag(\cov v)^{-\frac12}$ yields the correlation.
\end{proof}

\begin{lemma}[{\citet[Lemma 4.E.1]{chewi2022log}}]
    Consider a probability measure $\mu\propto\e^{-U}$ on $\R^d$. 
    \begin{enumerate}[wide=0pt,itemsep=0pt, topsep=0pt,parsep=0pt,partopsep=0pt]
        \item If $\nabla^2U\succeq \alpha I$ for some $\alpha>0$ and $x_\star$ is the global minimizer of $U$, then $\E_{\mu}{\|\cdot-x_\star\|^2}\le\frac{d}{\alpha}$.
        \item If $\nabla^2U\preceq \beta I$ for some $\beta>0$, then $\E_{\mu}{\|\nabla U\|^2}\le\beta d$.
    \end{enumerate}
    \label{lem:2ordmomlogccv}
\end{lemma}

\begin{lemma}
    Define $\pih_\lambda\propto\exp\ro{-V-\frac{\lambda}{2}\|\cdot\|^2}$, $\lambda\ge0$. Then under \cref{assu:pi}, $\E_{\pih_\lambda}{\|\cdot\|^2}\le m^2$ for all $\lambda\ge0$.%, and $\E_{\pih_\lambda}{\|\cdot\|^2}\lesssim\frac{d+\beta R^2}{\lambda}$ when $\lambda\ge2\beta$.
    \label{lem:2ordmom}
\end{lemma}

\begin{proof}
    Let $V_\lambda:=V+\frac{\lambda}{2}\|\cdot\|^2$, and $Z_\lambda=\int\e^{-V_\lambda}\d x$, so $\pih_\lambda=\e^{-V_\lambda-\log Z_\lambda}$. We have
    \begin{align*}
        \de{}{\lambda}\log Z_\lambda&=\frac{Z'_\lambda}{Z_\lambda}=-\frac{1}{Z_\lambda}\int\e^{-V_\lambda}V_\lambda'\d x=-\frac{1}{2}\E_{\pih_\lambda}\|\cdot\|^2,\\
        \implies\de{}{\lambda}\log\pih_\lambda&=-V'_\lambda-\de{}{\lambda}\log Z_\lambda=\frac{1}{2}\ro{\E_{\pih_\lambda}\|\cdot\|^2-\|\cdot\|^2},\\
        \implies\de{}{\lambda}\E_{\pih_\lambda}{\|\cdot\|^2}&=\int\|\cdot\|^2\ro{\de{}{\lambda}\log\pih_\lambda}\d\pih_\lambda=\frac{1}{2}\ro{\ro{\E_{\pih_\lambda}{\|\cdot\|^2}}^2-\E_{\pih_\lambda}{\|\cdot\|^4}}\le0.  
    \end{align*}
\end{proof}

\begin{lemma}
    If a function $U$ on $\R^d$ satisfies $0\prec\nabla^2U\preceq\beta I$ for some $\beta>0$, and for any $t\ge0$, let $x_t$ be the global minimizer of $U+\frac{t}{2}\|\cdot\|^2$. We have $\|x_t\|\le\frac{\|x_0\|}{1+\frac{t}{\beta}}$.
    \label{lem:glob_min}
\end{lemma}
\begin{proof}
    Since $\nabla U(x_t)+tx_t=0$, taking time derivative yields $\nabla^2U(x_t)\dot x_t+x_t+t\dot x_t=0$. Due to convexity, $\dot x_t=-(\nabla^2U(x_t)+tI)^{-1}x_t$. Therefore,
    $$\frac{1}{2}\de{}{t}\|x_t\|^2=x_t\tp\dot x_t=-x_t\tp(\nabla^2U(x_t)+tI)^{-1}x_t\le-\frac{\|x_t\|^2}{\beta+t},$$
    which implies $\de{}{t}\ro{\ro{1+\frac{t}{\beta}}^2\|x_t\|^2}\le0$, and thus the proof is complete.
\end{proof}