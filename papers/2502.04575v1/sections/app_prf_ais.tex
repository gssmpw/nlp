\section{Proof of \cref{thm:ais_complexity}}
\label{prf:thm:ais_complexity}
With the forward and backward path measures $\Pr$ and $\Pl$ defined in \cref{eq:ais_pr,eq:ais_pl}, we further define the reference path measure
\begin{equation}
    \P(x_{0:M})=\pi_0(x_0)\prod_{\l=1}^{M}F^*_\l(x_{\l-1},x_\l),
    \label{eq:ais_p}
\end{equation}
where $F^*_\l$ can be an arbitrary transition kernel transporting $\pi_{\theta_{\l-1}}$ to $\pi_{\theta_\l}$, i.e., it satisfies 
$$\pi_{\theta_\l}(y)=\int F^*_\l(x,y)\pi_{\theta_{\l-1}}(x)\d x,~\forall y\in\R^d\implies x_\l\sim\pi_{\theta_\l},~\forall\l\in\sqd{0,M}.$$
Define the backward transition kernel of $F^*_\l$ as
$$B^*_\l(x,x')=\frac{\pi_{\theta_{\l-1}}(x')}{\pi_{\theta_\l}(x)}F^*_\l(x',x),~\l\in\sqd{1,M},$$
which transports $\pi_{\theta_\l}$ to $\pi_{\theta_{\l-1}}$. Equivalently, we can write
$$\P(x_{0:M})=\pi_1(x_{M})\prod_{\l=1}^{M}B^*_\l(x_\l,x_{\l-1}).$$
Straightforward calculations yield
\begin{align}
    \kl(\P\|\Pr) & =\sum_{\l=1}^{M}\E_{\pi_{\theta_{\l-1}}(x_{\l-1})}{\kl(F^*_\l(x_{\l-1},\cdot)\|F_\l(x_{\l-1},\cdot))},\nonumber                          \\
    \kl(\P\|\Pl) & =\sum_{\l=1}^{M}\E_{\pi_{\theta_\l}(x_\l)}{\kl(B^*_\l(x_\l,\cdot)\|B_\l(x_\l,\cdot))}  \nonumber                                         \\
                 & =\sum_{\l=1}^{M}\kl(\pi_{\theta_{\l-1}}(x_{\l-1})F^*_\l(x_{\l-1},x_\l)\|\pi_{\theta_\l}(x_{\l-1})F_\l(x_{\l-1},x_\l))\label{eq:kl_p_pl} \\
                 & =\kl(\P\|\Pr)+\sum_{\l=1}^{M}\kl(\pi_{\theta_{\l-1}}\|\pi_{\theta_\l}).\label{eq:kl_p_pl_ge_kl_p_pr}
\end{align}

Also, recall that the sampling path measure $\Phr$ is defined in \cref{eq:ais_phr} starts at $\pih_0$, the distribution of an approximate sample of $\pi_0$. For convenience, we define the following path measure, which differs from $\Phr$ only from the initial distribution:
\begin{equation}
    \Pbr(x_{0:M})=\pi_0(x_0)\prod_{\l=1}^{M}\Fh_\l(x_{\l-1},x_\l).
    \label{eq:ais_ptr}
\end{equation}

Equipped with these definitions, we first prove a lemma about a necessary condition for the estimator $\Zh$ to satisfy the desired accuracy \cref{eq:acc_whp}.
\begin{lemma}
    \label{lem:disc_fram}
    Define the estimator $\Zh:=\Zh_0\e^{-W(x_{0:M})}$, where $x_{0:M}\sim\Phr$, and $\Zh_0$ is independent of $x_{0:M}$. To make $\Zh$ satisfy the criterion \cref{eq:acc_whp}, it suffices to meet the following four conditions:
    \begin{align}
         & \prob\ro{\abs{\frac{\Zh_0}{Z_0}-1}\ge\frac{\varepsilon}{8}}\le\frac{1}{8}\label{eq:ais_main_z0}, \\
         & \tv(\pih_0,\pi_0)\lesssim1,\label{eq:ais_main_pi0}                                           \\
         & \kl(\P\|\Pl)\lesssim\varepsilon^2\label{eq:ais_main_p_pl},                                    \\
         & \kl(\P\|\Pbr)\lesssim1.\label{eq:ais_main_p_ptr}
    \end{align}
\end{lemma}

\begin{proof}
    Recall that $Z=Z_0\e^{-\Delta F}$. Using \cref{lem:logat1}, we have
    \begin{align*}
        \prob\ro{\abs{\frac{\Zh}{Z}-1}\ge\varepsilon} & \le\prob\ro{\abs{\log\frac{\Zh}{Z}}\ge\frac{\varepsilon}{2}}=\prob_{x_{0:M}\sim\Phr}\ro{\abs{\log\frac{\Zh_0}{Z_0}+\log\frac{\e^{-W(x_{0:M})}}{\e^{-\Delta F}}}\ge\frac{\varepsilon}{2}} \\
                                                   & \le\prob\ro{\abs{\log\frac{\Zh_0}{Z_0}}\ge\frac{\varepsilon}{4}}+\Phr\ro{\abs{\log\frac{\e^{-W}}{\e^{-\Delta F}}}\ge\frac{\varepsilon}{4}}                   \\
                                                   & \le\prob\ro{\abs{\frac{\Zh_0}{Z_0}-1}\ge\frac{\varepsilon}{8}}+\Phr\ro{\abs{\frac{\e^{-W}}{\e^{-\Delta F}}-1}\ge\frac{\varepsilon}{8}}.
    \end{align*}
    The first term is $\le\frac{1}{8}$ if \cref{eq:ais_main_z0} holds. To bound the second term, using the definition of TV distance and the triangle inequality, we have
    \begin{align*}
        &\Phr\ro{\abs{\frac{\e^{-W}}{\e^{-\Delta F}}-1}\ge\frac{\varepsilon}{8}} \\
        & \le\tv(\Phr,\Pr)+\Pr\ro{\abs{\frac{\e^{-W}}{\e^{-\Delta F}}-1}\ge\frac{\varepsilon}{8}}                              \\
                                                                                    & \le\tv(\Phr,\Pbr)+\tv(\Pbr,\P)+\tv(\P,\Pr)+\Pr\ro{\abs{\de{\Pl}{\Pr}-1}\ge\frac{\varepsilon}{8}}.
    \end{align*}
    Recall that by definition (\cref{eq:ais_phr,eq:ais_ptr}), the distributions of $x_{1:M}$ conditional on $x_0$ are the same under $\Phr$ and $\Pbr$. Hence, $\tv(\Phr,\Pbr)=\tv(\pih_0,\pi_0)$. Applying Pinsker's inequality and leveraging \cref{eq:jar_acc_bound}, we have
    \begin{align*}
        & \Phr\ro{\abs{\frac{\e^{-W}}{\e^{-\Delta F}}-1}\ge\frac{\varepsilon}{8}} \\
                                                                                    & \lesssim\tv(\pih_0,\pi_0)+\sqrt{\kl(\P\|\Pbr)}+\sqrt{\kl(\P\|\Pr)}+\frac{\sqrt{\kl(\P\|\Pr)}+\sqrt{\kl(\P\|\Pl)}}{\varepsilon}.
    \end{align*}
    Note that from \cref{eq:kl_p_pl_ge_kl_p_pr} we know that $\kl(\P\|\Pr)\le\kl(\P\|\Pl)$, so if \cref{eq:ais_main_p_pl,eq:ais_main_p_ptr,eq:ais_main_pi0} hold up to some small enough absolute constants, we can achieve $\Phr\ro{\abs{\frac{\e^{-W}}{\e^{-\Delta F}}-1}\ge\frac{\varepsilon}{8}}\le\frac{1}{8}$, and therefore $\prob\ro{\abs{\frac{\Zh}{Z}-1}\ge\varepsilon}\le\frac{1}{4}$.
\end{proof}

In the next lemma, we show how to sample from $\pi_0$ and estimate $\Zh_0$ within the desired accuracy.
\begin{lemma}
\begin{enumerate}[wide=0pt,itemsep=0pt, topsep=0pt,parsep=0pt,partopsep=0pt]
    \item Using LMC initialized at $\mu_0=\n{0,\beta^{-1}I}$, the oracle complexity for obtaining a sample following a distribution $\pih_0$ that is $O(1)$-close in TV distance to $\pi_0$ is $\Ot(d)$.
    \item The oracle complexity for obtaining an estimator $\Zh_0$ of $Z_0$ such that \cref{eq:ais_main_z0} holds is $\Ot\ro{\frac{d^{\frac{3}{2}}}{\varepsilon^2}}$.
\end{enumerate}
\label{lem:ais_est_z0}
\end{lemma}

\begin{remark}
    Since $R\lesssim\frac{1}{\sqrt{\beta}}$, for both cases the dependence on $R$ is negligible.
\end{remark}

\begin{proof}
\begin{enumerate}[wide=0pt,itemsep=0pt, topsep=0pt,parsep=0pt,partopsep=0pt]
\item The bound comes from \citet[Theorem 2]{vempala2019rapid} (see also \citet[Theorem 4.2.5]{chewi2022log}). In particular, the bound there depends on $\log\kl(\mu_0\|\pi_0)$. We show that $\kl(\mu_0\|\pi_0)$ has a uniform upper bound over all $R\lesssim1$. The proof is as follows.

Note that $\pi_0$'s potential $V_0=V+\frac{2\beta}{2}\|\cdot\|^2$ is $\beta$-strongly-convex and $3\beta$-smooth. Let $x'$ be its global minimizer, which satisfies $\nabla V(x')+2\beta x'=0$. Recall from \cref{assu:pi} that $\nabla V(x_*)=0$, $\|x_*\|\le R$. So we have
$$2\beta\|x'\|=\|\nabla V(x')-\nabla V(x_*)\|\le\beta\|x'-x_*\|\le\beta(\|x'\|+R)\implies\|x'\|\le R.$$
Therefore,
\begin{align*}
    \kl(\mu_0\|\pi_0)&=\E_{\mu_0}\sq{\log\mu_0-\log\pi_0}\\
    &=\E_{\mu_0}\sq{-\frac{\beta}{2}\|\cdot\|^2+\frac{d}{2}\log\frac{\beta}{2\pi}+V_0+\log Z_0}\\
    &=-\frac{d}{2}+\frac{d}{2}\log\frac{d}{2\pi}+\E_{\mu_0}V_0+\log Z_0.
\end{align*}
By strong-convexity and smoothness, 
\begin{align*}
    \E_{\mu_0}V_0&\le\E_{\mu_0}\sq{V_0(x')+\frac{3\beta}{2}\|\cdot-x'\|^2}=V_0(x')+\frac{3\beta}{2}\ro{\frac{d}{\beta}+R^2};\\
    \log Z_0&=\log\int\e^{-V_0(x)}\d x\le\log\int\exp\ro{-V_0(x')-\frac{\beta}{2}\|x-x'\|^2}\d x\\
    &=-V_0(x')+\frac{d}{2}\log\frac{\beta}{2\pi},
\end{align*}
so we conclude that
$$\kl(\mu_0\|\pi_0)\le d+d\log\frac{\beta}{2\pi}+\frac{3\beta R^2}{2}.$$

\item The result is adapted from \citet[Section 3]{ge2020estimating}, with two key modifications. First, we relax their assumption that the global minimizer is at zero, requiring instead that the global minimizer $x'$ satisfies $\|x'\|\le R\lesssim\frac{1}{\sqrt{\beta}}$. Second, we use replace their Metropolis-Hasting adjusted Langevin algorithm (MALA) with the proximal sampler \citep{fan2023improved}, which achieves improved dimensional dependence. For completeness, we include a proof sketch in \cref{app:rel_work_ti_prf} and refer the readers to the original work for full technical details. Our analysis confirms that these relaxations have negligible impact on the final bounds.
\end{enumerate}
\end{proof}

Next, we study how to satisfy the conditions in \cref{eq:ais_main_p_pl,eq:ais_main_p_ptr} while minimizing oracle complexity. Given that we already have an approximate sample from $\pi_0$ and an accurate estimate of $Z_0$, we proceed to the next step of the AIS algorithm. Since each transition kernel requires one call to the oracle of $\nabla V$, and by plugging in $f_\theta\gets V+\frac{\lambda(\theta)}{2}\|\cdot\|^2$ in AIS (\cref{thm:ais}), the work function $W(x_{0:M})$ is independent of $V$, it follows that the remaining oracle complexity is $M$. The result is formalized in the following lemma.

\begin{lemma}
    To minimize the oracle complexity, it suffices to find the minimal $M$ such that there exists a sequence $0=\theta_0<\theta_1<...<\theta_M=1$ satisfying the following three constraints:
    \begin{align}
        \sum_{\l=1}^M\int_{\theta_{\l-1}}^{\theta_\l}(\lambda(\theta)-\lambda(\theta_\l))^2\d\theta & \lesssim\frac{\varepsilon^4}{m^2\cA},\label{eq:ais_cond_theta_a}        \\
        \sum_{\l=1}^{M}(\theta_\l-\theta_{\l-1})^2                                                                                                                                                                                                                      & \lesssim\frac{\varepsilon^4}{d\beta^2\cA^2},\label{eq:ais_cond_theta_b} \\
        \max_{\l\in\sqd{1,M}}\ro{\theta_\l-\theta_{\l-1}}                                                                                                                                                                                                               & \lesssim\frac{\varepsilon^2}{\beta\cA}.\label{eq:ais_cond_theta_c}
    \end{align}
\end{lemma}

\begin{proof}
    We break down the argument into two steps.
    
    \paragraph{Step 1.} We first consider \cref{eq:ais_main_p_pl}. 

    Note that when defining the reference path measure $\P$, the only requirement for the transition kernel $F_\l^*$ is that it should transport $\pi_{\theta_{\l-1}}$ to $\pi_{\theta_\l}$. Our aim is to find the ``optimal'' $F_\l^*$'s in order to minimize the sum of KL divergences, which can be viewed as a \emph{static Schr\"odinger bridge problem} \citep{leonard2014a,chen2016relation,chen2021stochastic}. By data-processing inequality,
    \begin{align*}
        C_\l:=\inf_{F_\l^*}\kl(\pi_{\theta_{\l-1}}(x_{\l-1})F^*_\l(x_{\l-1},x_\l)\|\pi_{\theta_\l}(x_{\l-1})F_\l(x_{\l-1},x_\l))\le & \inf_{\Pbf^\l}\kl(\Pbf^\l\|\Qbf^\l),
    \end{align*}
    where the infimum is taken among all path measures from $0$ to $T_\l$ with the marginal constraints $\Pbf^\l_0=\pi_{\theta_{\l-1}}$ and $\Pbf^\l_{T_\l}=\pi_{\theta_\l}$; $\Qbf^\l$ is the path measure of \cref{eq:ais_ker_f} (i.e., LD with target distribution $\pi_{\theta_\l}$) initialized at $X_0\sim\pi_{\theta_\l}$.

    For each $\l\in\sqd{1,M}$, define the following interpolation between $\pi_{\theta_{\l-1}}$ and $\pi_{\theta_\l}$:
    $$\mu^\l_t:=\pi_{\theta_{\l-1}+\frac{t}{T_\l}(\theta_\l-\theta_{\l-1})},~t\in[0,T_\l].$$

    Let $\Pbf^\l$ be the path measure of
    $$\d X_t=(\nabla\log\mu^\l_t+u^\l_t)(X_t)\d t+\sqrt{2}\d B_t,~t\in[0,T_\l];~X_0\sim\pi_{\theta_{\l-1}},$$
    where the vector field $(u^\l_t)_{t\in[0,T_\l]}$ is chosen such that $X_t\sim\mu^\l_t$ under $\Pbf^\l$, and in particular, the marginal distributions at $0$ and $T_\l$ are $\pi_{\theta_{\l-1}}$ and $\pi_{\theta_\l}$, respectively. By verifying the Fokker-Planck equation, the following PDE needs to be satisfied:
    $$\partial_t\mu^\l_t=-\nabla\cdot(\mu^\l_t(\nabla\log\mu^\l_t+u^\l_t))+\Delta\mu^\l_t=-\nabla\cdot(\mu^\l_tu^\l_t),~t\in[0,T_\l],$$
    meaning that $(u^\l_t)_{t\in[0,T_\l]}$ generates $(\mu^\l_t)_{t\in[0,T_\l]}$. Similar to the proof of JE (\cref{thm:jar_complexity}), using the relation between metric derivative and continuity equation (\cref{lem:metric}), among all vector fields generating $(\mu^\l_t)_{t\in[0,T_\l]}$, we choose $(u^\l_t)_{t\in[0,T_\l]}$ to be the a.s.-unique vector field that satisfies $\|u^\l_t\|_{L^2(\mu^\l_t)}=|\dot\mu^\l|_t$ for Lebesgue-a.e. $t\in[0,T_\l]$, which implies (using the chain rule)
    \begin{align*}
        &\int_0^{T_\l}\|u^\l_t\|_{L^2(\mu^\l_t)}^2\d t=\int_0^{T_\l}|\dot\mu^\l|_t^2\d t\\
        &=\int_0^{T_\l}\ro{\frac{\theta_\l-\theta_{\l-1}}{T_\l}|\dot\pi|_{\theta_{\l-1}+\frac{t}{T_\l}(\theta_\l-\theta_{\l-1})}}^2\d t=\frac{\theta_\l-\theta_{\l-1}}{T_\l}\int_{\theta_{\l-1}}^{\theta_\l}|\dot\pi|_\theta^2\d\theta.
    \end{align*}

    By \cref{lem:nelson}, we can equivalently write $\Pbf^\l$ as the path measure of the following backward SDE:
    $$\d X_t=(-\nabla\log\mu^\l_t+u^\l_t)(X_t)\d t+\sqrt{2}\d\Bl_t,~t\in[0,T_\l];~X_T\sim\pi_{\theta_{\l}}.$$

    Recall that $\Qbf^\l$ is the path measure of \cref{eq:ais_ker_f} initialized at $X_0\sim\pi_{\theta_\l}$, so $X_t\sim\pi_{\theta_\l}$ for all $t\in[0,T_\l]$. By Nelson's relation (\cref{lem:nelson}), we can equivalently write $\Qbf^\l$ as the path measure of
    $$\d X_t=-\nabla\log\pi_{\theta_\l}(X_t)\d t+\sqrt{2}\d\Bl_t,~t\in[0,T_\l];~X_{T_\l}\sim\pi_{\theta_\l}.$$

    The purpose of writing these two path measures in the way of backward SDEs is to avoid the extra term of the KL divergence between the initialization distributions $\pi_{\theta_{\l-1}}$ and $\pi_{\theta_\l}$ at time $0$ when calculating $\kl(\Pbf^\l\|\Qbf^\l)$. To see this, by Girsanov theorem (\cref{lem:rn_path_measure_contd}), the triangle inequality, and the change-of-variable formula, we have
    \begin{align*}
        C_\l\le\kl(\Pbf^\l\|\Qbf^\l) & =\frac{1}{4}\int_{0}^{T_\l}\norm{u^\l_t-\nabla\log\frac{\mu_t^\l}{\pi_{\theta_\l}}}^2_{L^2(\mu^\l_t)}\d t                                                                                                                                                 \\
                                     & \lesssim\int_{0}^{T_\l}\|u^\l_t\|^2_{L^2(\mu^\l_t)}\d t+\int_{0}^{T_\l}\norm{\nabla\log\frac{\mu_t^\l}{\pi_{\theta_\l}}}^2_{L^2(\mu^\l_t)}\d t                                                                                                        \\
                                     & =\frac{\theta_\l-\theta_{\l-1}}{T_\l}\int_{\theta_{\l-1}}^{\theta_\l}|\dot\pi|_\theta^2\d\theta+\frac{T_\l}{\theta_\l-\theta_{\l-1}}\int_{\theta_{\l-1}}^{\theta_\l}\norm{\nabla\log\frac{\pi_\theta}{\pi_{\theta_\l}}}_{L^2(\pi_\theta)}^2\d\theta.
    \end{align*}
    \begin{remark}
        Our bound above is based on a specific interpolation between $\pi_{\theta_{\l-1}}$ and $\pi_{\theta_\l}$ along the curve $(\pi_\theta)_{\theta\in[\theta_{\l-1},\theta_\l]}$. This approach is inspired by, yet slightly differs from, \citet[Theorem 1.6]{conforti2021a}, where the interpolation is based on the Wasserstein geodesic. As we will demonstrate shortly, our formulation simplifies the analysis of the second term (the Fisher divergence), making it more straightforward to bound.
    \end{remark}
    
    Now, summing over all $\l\in\sqd{1,M}$, we can see that in order to ensure $\kl(\P\|\Pl)\le\sum_{\l=1}^MC_\l\le\varepsilon^2$, we only need the following two conditions to hold:
    \begin{align}
        \sum_{\l=1}^M\frac{\theta_\l-\theta_{\l-1}}{T_\l}\int_{\theta_{\l-1}}^{\theta_\l}|\dot\pi|_\theta^2\d\theta                                                  & \lesssim\varepsilon^2,\label{eq:ais_klppl_conda} \\
        \sum_{\l=1}^M\frac{T_\l}{\theta_\l-\theta_{\l-1}}\int_{\theta_{\l-1}}^{\theta_\l}\norm{\nabla\log\frac{\pi_\theta}{\pi_{\theta_\l}}}_{L^2(\pi_\theta)}^2\d\theta & \lesssim\varepsilon^2.\label{eq:ais_klppl_condb}
    \end{align}

    Since $\sum_{\l=1}^M\int_{\theta_{\l-1}}^{\theta_\l}|\dot\pi|_\theta^2\d\theta=\cA$, it suffices to choose
    $$\frac{T_\l}{\theta_\l-\theta_{\l-1}}=:T\asymp\frac{\cA}{\varepsilon^2},~\forall\l\in\sqd{1,M}$$
    to make the l.h.s. of \cref{eq:ais_klppl_conda} $O(\varepsilon^2)$. Notably, $T$ is the summation over all $T_\l$'s, which has the same order as the total time $T$ for running JE (\cref{eq:jar_pr}) in the continuous scenario, in \cref{thm:jar}. Plugging this $T_\l$ into the second summation, and noticing that by \cref{eq:pi_theta} and \cref{lem:2ordmom},
    $$\norm{\nabla\log\frac{\pi_\theta}{\pi_{\theta'}}}_{L^2(\pi_\theta)}^2=\E_{x\sim\pi_\theta}{\|(\lambda(\theta)-\lambda(\theta'))x\|^2}\le(\lambda(\theta)-\lambda(\theta'))^2m^2,$$
    we conclude that \cref{eq:ais_cond_theta_a} implies \cref{eq:ais_klppl_condb}.

    \paragraph{Step 2.} Now consider the other constraint \cref{eq:ais_main_p_ptr}. By chain rule and data-processing inequality,
    $$\kl(\P\|\Pbr)=\sum_{\l=1}^{M}\kl(\pi_{\theta_{\l-1}}(x_{\l-1})F^*_\l(x_{\l-1},x_\l)\|\pi_{\theta_{\l-1}}(x_{\l-1})\Fh_\l(x_{\l-1},x_\l))\le\sum_{\l=1}^{M}\kl(\Pbf^\l\|\Qhbf^\l),$$
    where $\Pbf^\l$ is the previously defined path measure of the SDE
    \begin{align*}
        &\d X_t=(\nabla\log\mu^\l_t+u^\l_t)(X_t)\d t+\sqrt{2}\d B_t                                                                                                             \\
                & =\ro{-\nabla V(X_t)-\lambda\ro{\theta_{\l-1}+\frac{t}{T_\l}(\theta_\l-\theta_{\l-1})}X_t+u^\l_t(X_t)}\d t+\sqrt{2}\d B_t,~t\in[0,T_\l];~X_0\sim\pi_{\theta_{\l-1}},
    \end{align*}
    and $\Qhbf^\l$ is the path measure of \cref{eq:ais_ker_fh} initialized at $X_0\sim\pi_{\theta_{\l-1}}$, i.e.,
    \begin{align*}
        \d X_t & =\ro{-\nabla V(X_0)-\lambda\ro{\theta_{\l-1}+\frac{t}{T_\l}(\theta_\l-\theta_{\l-1})}X_t}\d t+\sqrt{2}\d B_t,~t\in[0,T_\l];~X_0\sim\pi_{\theta_{\l-1}}.
    \end{align*}

    By \cref{lem:rn_path_measure}, triangle inequality, and the smoothness of $V$, we have
    \begin{align*}
        \kl(\Pbf^\l\|\Qhbf^\l) & =\frac{1}{4}\int_0^{T_\l}\E_{\Pbf^\l}{\|\nabla V(X_t)-\nabla V(X_0)-u^\l_t(X_t)\|^2}\d t                \\
                               & \lesssim \int_0^{T_\l}\E_{\Pbf^\l}\sq{\|\nabla V(X_t)-\nabla V(X_0)\|^2+\|u^\l_t(X_t)\|^2}\d t       \\
                               & \le\beta^2\int_0^{T_\l}\E_{\Pbf^\l}{\|X_t-X_0\|^2}\d t+\int_0^{T_\l}\|u^\l_t\|_{L^2(\mu^\l_t)}^2\d t
    \end{align*}
    To bound the first part, note that under $\Pbf^\l$, we have
    $$X_t-X_0=\int_0^t(\nabla\log\mu^\l_\tau+u^\l_\tau)(X_\tau)\d\tau+\sqrt2B_t.$$
    Thanks to the fact that $X_t\sim\mu^\l_t$ under $\Pbf^\l$,
    \begin{align*}
        \E_{\Pbf^\l}{\|X_t-X_0\|^2} & \lesssim\E_{\Pbf^\l}{\norm{\int_0^t(\nabla\log\mu^\l_\tau+u^\l_\tau)(X_\tau)\d\tau}^2}+\E_{}{\|\sqrt2B_t\|^2}                                         \\
                                      & \lesssim t\int_0^t\E_{\Pbf^\l}{\|(\nabla\log\mu^\l_\tau+u^\l_\tau)(X_\tau)\|^2}\d\tau+dt                                                                \\
                                      & \lesssim t\int_0^t\ro{\|\nabla\log\mu^\l_\tau\|^2_{L^2(\mu^\l_\tau)}+\|u^\l_\tau\|_{L^2(\mu^\l_\tau)}^2}\d\tau+dt                                   \\
                                      & \lesssim T_\l\int_0^{T_\l}\ro{\|\nabla\log\mu^\l_\tau\|^2_{L^2(\mu^\l_\tau)}+\|u^\l_\tau\|_{L^2(\mu^\l_\tau)}^2}\d\tau+dT_\l,~\forall t\in[0,T_\l],
    \end{align*}
    where the second inequality follows from Jensen's inequality \cite[Sec. 4]{cheng2018underdamped}:
    $$\norm{\int_0^t f_\tau\d\tau}^2=t^2\|\E_{\tau\sim\un(0,t)}{f_\tau}\|^2\le t^2\E_{\tau\sim\un(0,t)}{\|f_\tau\|^2}=t\int_0^t\|f_\tau\|^2\d\tau.$$
    Therefore,
    \begin{align*}
        &\kl(\Pbf^\l\|\Qhbf^\l) \\
        &\le\beta^2\int_0^{T_\l}\E_{\Pbf^\l}{\|X_t-X_0\|^2}\d t+\int_0^{T_\l}\|u^\l_t\|_{L^2(\mu^\l_t)}^2\d t                                                                                                                                                                     \\
                               & \le\beta^2T_\l^2\int_0^{T_\l}\|\nabla\log\mu^\l_\tau\|^2_{L^2(\mu^\l_\tau)}\d\tau+(\beta^2T_\l^2+1)\int_0^{T_\l}\|u^\l_\tau\|_{L^2(\mu^\l_\tau)}^2\d\tau+d\beta^2T_\l^2                                                                                                 \\
                               & =\beta^2T_\l^2\frac{T_\l}{\theta_\l-\theta_{\l-1}}\int_{\theta_{\l-1}}^{\theta_\l}\|\nabla\log\pi_\theta\|^2_{L^2(\pi_\theta)}\d\theta+(\beta^2T_\l^2+1)\frac{\theta_\l-\theta_{\l-1}}{T_\l}\int_{\theta_{\l-1}}^{\theta_\l}|\dot\pi|_\theta^2\d\theta+d\beta^2T_\l^2.
    \end{align*}

    Recall that the potential of $\pi_\theta$ is $(\beta+\lambda(\theta))$-smooth. By \cref{lem:2ordmomlogccv} and the monotonicity of $\lambda(\cdot)$,
    $$\int_{\theta_{\l-1}}^{\theta_\l}\|\nabla\log\pi_\theta\|^2_{L^2(\pi_\theta)}\d\theta\le\int_{\theta_{\l-1}}^{\theta_\l}d(\beta+\lambda(\theta))\d\theta\le d(\theta_\l-\theta_{\l-1})(\beta+\lambda(\theta_{\l-1})).$$
    Thus,
    \begin{align*}
        \kl(\P\|\Pbr) & \le\sum_{\l=1}^{M}\ro{\beta^2T_\l^3d(\beta+\lambda(\theta_{\l-1}))+(\beta^2T_\l^2+1)\frac{\theta_\l-\theta_{\l-1}}{T_\l}\int_{\theta_{\l-1}}^{\theta_\l}|\dot\pi|_\theta^2\d\theta+d\beta^2T_\l^2} \\
                      & =\sum_{\l=1}^{M}\ro{\beta^2dT_\l^2\ro{T_\l(\beta+\lambda(\theta_{\l-1}))+1}+(\beta^2T_\l^2+1)\frac{\theta_\l-\theta_{\l-1}}{T_\l}\int_{\theta_{\l-1}}^{\theta_\l}|\dot\pi|_\theta^2\d\theta}       \\
    \end{align*}

    Assume $\max_{\l\in\sqd{1,M}}T_\l\lesssim\frac{1}{\beta}$, i.e., \cref{eq:ais_cond_theta_c}. so $\max_{\l\in\sqd{1,M}}T_\l(\beta+\lambda(\theta_{\l-1}))\lesssim1$, due to $\lambda(\cdot)\le2\beta$. We can further simplify the above expression to
    \begin{align*}
        \kl(\P\|\Pbr) & \le\sum_{\l=1}^{M}\ro{\beta^2dT_\l^2+\frac{\theta_\l-\theta_{\l-1}}{T_\l}\int_{\theta_{\l-1}}^{\theta_\l}|\dot\pi|_\theta^2\d\theta}\lesssim \beta^2d\ro{\sum_{\l=1}^{M}T_\l^2}+\varepsilon^2                                                                                 \\
                      & =\beta^2dT^2\sum_{\l=1}^{M}(\theta_\l-\theta_{\l-1})^2+\varepsilon^2\lesssim\beta^2d\frac{\cA^2}{\varepsilon^4}\sum_{\l=1}^{M}(\theta_\l-\theta_{\l-1})^2+\varepsilon^2.
    \end{align*}

    So \cref{eq:ais_cond_theta_c} implies that the r.h.s. of the above equation is $O(1)$.
\end{proof}

Finally, we have arrived at the last step of proving \cref{thm:ais_complexity}, that is to decide the schedule of $\theta_\l$'s.

Define $\vartheta_\l:=1-\theta_\l$, $\l\in\sqd{0,M}$. We consider the annealing schedule $\lambda(\theta)=2\beta(1-\theta)^r$ for some $1\le r\lesssim1$, and to emphasize the dependence on $r$, we use $\cA_r$ to represent the action of $(\pi_\theta)_{\theta\in[0,1]}$. The l.h.s. of \cref{eq:ais_cond_theta_a} is
\begin{align*}
    \sum_{\l=1}^M\int_{\theta_{\l-1}}^{\theta_\l}(\lambda(\theta)-\lambda(\theta_\l))^2\d\theta & \le\sum_{\l=1}^M(\theta_\l-\theta_{\l-1})(2\beta(1-\theta_{\l-1})^r-2\beta(1-\theta_\l)^r)^2                                                        \\
                                                                                                 & =\sum_{\l=1}^M(\vartheta_{\l-1}-\vartheta_\l)(2\beta\vartheta_{\l-1}^r-2\beta\vartheta_\l^r)^2                                                      \\
                                                                                                 & \lesssim\beta^2\sum_{\l=1}^M(\vartheta_{\l-1}-\vartheta_\l)(\vartheta_{\l-1}^r-\vartheta_\l^r)^2                                                    \\
                                                                                                 & \lesssim\beta^2\sum_{\l=1}^M(\vartheta_{\l-1}-\vartheta_\l)(\vartheta_{\l-1}-\vartheta_\l)^2=\beta^2\sum_{\l=1}^M(\vartheta_{\l-1}-\vartheta_\l)^3,
\end{align*}
where the last inequality comes from \cref{lem:power_r_diff}. So to satisfy \cref{eq:ais_cond_theta_a}, it suffices to ensure
$$\sum_{\l=1}^M(\vartheta_{\l-1}-\vartheta_\l)^3\lesssim\frac{\varepsilon^4}{m^2\beta^2\cA_r},$$
while \cref{eq:ais_cond_theta_b} and \cref{eq:ais_cond_theta_c} are equivalent to
$$\sum_{\l=1}^M(\vartheta_{\l-1}-\vartheta_\l)^2\lesssim\frac{\varepsilon^4}{d\beta^2\cA_r^2},\qquad\max_{\l\in\sqd{1,M}}(\vartheta_{\l-1}-\vartheta_\l)\lesssim\frac{\varepsilon^2}{\beta\cA_r}.$$
Since we are minimizing the total number of oracle calls $M$, the H\"older's inequality implies that the optimal schedule of $\vartheta_\l$'s is an arithmetic sequence, i.e., $\vartheta_\l=1-\frac{\l}{M}$. We need to ensure
$$\frac{1}{M^2}\lesssim\frac{\varepsilon^4}{m^2\beta^2\cA_r},\qquad\frac{1}{M}\lesssim\frac{\varepsilon^4}{d\beta^2\cA_r^2},\qquad\frac{1}{M}\lesssim\frac{\varepsilon^2}{\beta\cA_r}.$$
So it suffices to choose $\frac{1}{M}\asymp\frac{\varepsilon^2}{m\beta\cA_r^\frac{1}{2}}\wedge\frac{\varepsilon^4}{d\beta^2\cA_r^2}$, which implies the oracle complexity
$$M\asymp\frac{m\beta\cA_r^\frac{1}{2}}{\varepsilon^2}\vee\frac{d\beta^2\cA_r^2}{\varepsilon^4}.$$
\hfill$\square$