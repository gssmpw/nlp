\section{Preliminaries (Continued)}
\label{app:pre}
The theories of backward stochastic integral and the Girsanov theorem are adapted from \cite{vargas2024transport}. Here, we include relevant results and proofs to ensure a self-contained presentation.  

\begin{lemma}[Nelson's relation {\citep{nelson1967dynamical,anderson1982reversetime}}]
    \label{lem:nelson}
    Given a BM $(B_t)_{t\in[0,T]}$ and its time-reversal $(\Bl_t=B_{T-t})_{t\in[0,T]}$, the following two SDEs
    $$\d X_t=a_t(X_t)\d t+\sigma\d B_t,~X_0\sim p_0;~\d Y_t=b_t(Y_t)\d t+\sigma\d\Bl_t,~Y_T\sim q$$
    have the same path measure if and only if
    $$q=p_T,\quad\text{and}\quad b_t=a_t-\sigma^2\nabla\log p_t,~\forall t\in[0,T],$$
    where $p_t$ is the p.d.f. of $X_t$.
\end{lemma}

\begin{proof}
    The proof is by verifying the Fokker-Planck equation. For $X$, we have
    $$\partial_tp_t=-\nabla\cdot(a_tp_t)+\frac{\sigma^2}{2}\Delta p_t.$$
    Let $\star^\gets_t:=\star_{T-t}$. Then $\pl_t$ satisfies
    $$\partial_t\pl_t=\nabla\cdot(\al_t\pl_t)-\frac{\sigma^2}{2}\Delta\pl_t=-\nabla\cdot((-\al_t+\sigma^2\nabla\log\pl_t)\pl_t)+\frac{\sigma^2}{2}\Delta\pl_t,$$
    which means $(\Xl_t)_{t\in[0,T]}$ has the same path measure as the following SDE:
    $$\d Z_t=-(\al_t-\sigma^2\nabla\log\pl_t)(Z_t)\d t+\sigma\d B_t,~Z_t\sim\pl_t.$$
    On the other hand, by definition, $(\Yl_t)_{t\in[0,T]}$ satisfies the forward SDE
    $$\d\Yl_t=-\bl_t(\Yl_t)\d t+\sigma\d B_t,~Y_0\sim q,$$
    and thus the claim is evident.
\end{proof}

\begin{definition}[Backward stochastic integral]
    \label{def:bsi}
    For two continuous stochastic processes $X$ and $Y$ on $C([0,T];\R^d)$, the \textbf{backward stochastic integral} of $Y$ with respect to $X$ is defined as
    $$\int_0^T\inn{Y_t,*\d X_t}:=\prob\,{\text-}\lim_{\|\Pi\|\to0}\sum_{i=0}^{n-1}\inn{Y_{t_{i+1}},X_{t_{i+1}}-X_{t_i}},$$
    where $\Pi=\{0=t_0<t_1<...<t_n=T\}$ is a partition of $[0,T]$, $\|\Pi\|:=\max\limits_{i\in\sqd{1,n}}(t_{i+1}-t_i)$, and the convergence is in the probability sense. When both $X$ and $Y$ are continuous semi-martingales, one can equivalently define
    \begin{equation}
        \int_0^T\inn{Y_t,*\d X_t}:=\int_0^T\inn{Y_t,\d X_t}+[X,Y]_T,
        \label{eq:bsi_ito}
    \end{equation}
    where $[X,Y]_\cdot$ is the cross quadratic variation process\footnote{The notation used in \cite{karatzas1991brownian} is $\inn{\cdot,\cdot}_\cdot$. We use square brackets here to avoid conflict with the notation for inner product.} of the local martingale parts of $X$ and $Y$.
    \end{definition}

\begin{remark}
    Although rarely used in practice, the backward stochastic integral is sometimes referred to as the H\"anggi-Klimontovich integral in the literature. Recall that the It\^o integral is defined as the limit of Riemann sums when the leftmost point of each interval is used, while the Stratonovich integral is based on the midpoint and the backward integral uses the rightmost point. The equivalence in \cref{eq:bsi_ito} can be justified in \citet[Chap. 3.3]{karatzas1991brownian}. 
\end{remark}

\begin{lemma}[Continuation of \cref{lem:rn_path_measure}]
\begin{enumerate}[wide=0pt,itemsep=0pt, topsep=0pt,parsep=0pt,partopsep=0pt]
    \item If we replace the SDEs in \cref{lem:rn_path_measure} with
    $$\d X_t =a_t(X_t)\d t+\sigma\d\Bl_t,~X_T\sim\mu;\qquad\d Y_t =b_t(Y_t)\d t+\sigma\d\Bl_t,~Y_T\sim\nu,$$
    while keeping other assumptions and notations unchanged, then for any trajectory $\xi\in\Omega$, 
    \begin{align*}
        \log\de{\P^X}{\P^Y}(\xi) & =\log\de{\mu}{\nu}(\xi_T)+\frac{1}{\sigma^2}\int_0^T\inn{a_t(\xi_t)-b_t(\xi_t),*\d\xi_t}-\frac{1}{2\sigma^2}\int_0^T(\|a_t(\xi_t)\|^2-\|b_t(\xi_t)\|^2)\d t,
    \end{align*}
    and consequently, 
    $$\kl(\P^X\|\P^Y)=\kl(\mu\|\nu)+\frac{1}{2\sigma^2}\int_0^T\E_{\P^X}\|a_t(X_t)-b_t(X_t)\|^2\d t.$$

    \item Define the following two SDEs from $0$ to $T$:
    $$\d X_t =a_t(X_t)\d t+\sigma\d B_t,~X_0\sim\mu;\qquad\d Y_t =b_t(Y_t)\d t+\sigma\d\Bl_t,~Y_T\sim\nu.$$
    Denote the path measures of $X$ and $Y$ as $\P^X$ and $\P^Y$, respectively. Then for any trajectory $\xi\in\Omega$,
    \begin{align*}
        \log\de{\P^X}{\P^Y}(\xi) & =\log\frac{\mu(\xi_0)}{\nu(\xi_T)}+\frac{1}{\sigma^2}\int_0^T(\inn{a_t(\xi_t),\d\xi_t}-\inn{b_t(\xi_t),*\d\xi_t})-\frac{1}{2\sigma^2}\int_0^T(\|a_t(\xi_t)\|^2-\|b_t(\xi_t)\|^2)\d t.
    \end{align*}
\end{enumerate}
\label{lem:rn_path_measure_contd}
\end{lemma}

\begin{proof}
\begin{enumerate}[wide=0pt,itemsep=0pt, topsep=0pt,parsep=0pt,partopsep=0pt]
\item Let $\star^\gets_t:=\star_{T-t}$. We know that
$$\d\Xl_t =-\al_t(\Xl_t)\d t+\sigma\d B_t,~\Xl_0\sim\mu;\qquad\d\Yl_t =-\bl_t(\Yl_t)\d t+\sigma\d B_t,~\Yl_0\sim\nu.$$
Let $\P^{\Xl}$ and $\P^{\Yl}$ be the path measures of $\Xl$ and $\Yl$, respectively. From \cref{lem:rn_path_measure}, we know that
\begin{align*}
    \log\de{\P^{\Xl}}{\P^{\Yl}}(\xi) & =\log\de{\mu}{\nu}(\xi_0)-\frac{1}{\sigma^2}\int_0^T\inn{\al_t(\xi_t)-\bl_t(\xi_t),\d\xi_t}-\frac{1}{2\sigma^2}\int_0^T(\|\al_t(\xi_t)\|^2-\|\bl_t(\xi_t)\|^2)\d t.
\end{align*}
Since $\P^{\Xl}(\d\xi)=\prob(\Xl\in\d\xi)=\prob(X\in\d\xil)=\P^X(\d\xil)$, we obtain
\begin{align*}
    &\log\de{\P^X}{\P^Y}(\xi) =\log\de{\P^{\Xl}}{\P^{\Yl}}(\xil)\\
    &=\log\de{\mu}{\nu}(\xil_0)-\frac{1}{\sigma^2}\int_0^T\inn{\al_t(\xil_t)-\bl_t(\xil_t),\d\xil_t}-\frac{1}{2\sigma^2}\int_0^T(\|\al_t(\xil_t)\|^2-\|\bl_t(\xil_t)\|^2)\d t\\
    &=\log\de{\mu}{\nu}(\xi_T)+\frac{1}{\sigma^2}\int_0^T\inn{a_t(\xi_t)-b_t(\xi_t),*\d\xi_t}-\frac{1}{2\sigma^2}\int_0^T(\|a_t(\xi_t)\|^2-\|b_t(\xi_t)\|^2)\d t.
\end{align*}

To justify the last equality, if $\xi,\eta$ are two continuous stochastic processes, then by definition,
\begin{align}
    \int_0^T\inn{\xil_t,\d\etal_t}&=\prob\,{\text-}\lim_{\|\Pi\|\to0}\sum_{i=0}^{n-1}\inn{\xil_{t_{i-1}},\etal_{t_i}-\etal_{t_{i-1}}}\nonumber\\
    &=\prob\,{\text-}\lim_{\|\Pi\|\to0}\sum_{i=0}^{n-1}\inn{\xi_{T-t_{i-1}},\eta_{T-t_i}-\eta_{T-t_{i-1}}}\nonumber\\  
    &=\prob\,{\text-}\lim_{\|\Pi\|\to0}-\sum_{i=0}^{n-1}\inn{\xi_{T-t_{i-1}},\eta_{T-t_{i-1}}-\eta_{T-t_i}}\nonumber\\  
    &=-\int_0^T\inn{\xi_t,*\d\eta_t}.\label{eq:bwd_int}
\end{align}
On the other hand, 
$$\int_0^T\xil_t\d t=\int_0^T\xi_{T-t}\d t=\int_0^T\xi_t\d t.$$
Therefore, the equality of RN derivative holds. Plugging in $\xi\gets X$, we have
$$\log\de{\P^X}{\P^Y}(X) =\log\de{\mu}{\nu}(X_T)+\frac{1}{\sigma}\int_0^T\inn{a_t(X_t)-b_t(X_t),*\d\Bl_t}+\frac{1}{2\sigma^2}\int_0^T\|a_t(X_t)-b_t(X_t)\|^2\d t.$$
To obtain the KL divergence, it suffices to show the expectation of the second term is zero. Let 
$$M_t:=\int_t^T\inn{a_r(X_r)-b_r(X_r),*\d\Bl_r},~t\in[0,T].$$
By \cref{eq:bwd_int}, we have 
$$\Ml_t=-\int_0^t\inn{\al_r(\Xl_r)-\bl_r(\Xl_r),\d B_r}.$$
Since $\d\Xl_t =-\al_t(\Xl_t)\d t+\sigma\d B_t$, we conclude that $\Ml_t$ is a (forward) martingale, and thus $M$ is a \emph{backward} martingale with $\E M_t=\E\Ml_{T-t}=0$.

\item We present a formal proof by considering the process $\d Z_t=\sigma\d B_t$ and $Z_0\sim\lambda$, the Lebesgue measure. As a result, formally $Z_t\sim\lambda$ for all $t$, so it can also be written as $\d Z_t=\sigma\d\Bl_t$, $Z_T\sim\lambda$. The result follows by applying \cref{lem:rn_path_measure} to $X$ and $Z$ and \textbf{1.} to $Y$ and $Z$.
\end{enumerate}
\end{proof}

\begin{remark}
    The Girsanov theorem requires a technical condition ensuring that a local martingale is a true martingale, typically verified via the Novikov condition \cite[Chap. 3, Cor. 5.13]{karatzas1991brownian}, which can be challenging to establish. However, when only an upper bound of the KL divergence is needed, the approximation argument from \citet[App. B.2]{chen2023sampling} circumvents the verification of the Novikov condition. For additional context, see \citet[Sec. 3.2]{chewi2022log}. In this paper, we omit these technical details and \underline{always} assume that the Novikov condition holds.
\end{remark}

\begin{definition}[Isoperimetric inequalities]
    A probability measure $\pi$ on $\R^d$ satisfies a \textbf{Poincar\'e inequality (PI)} with constant $C$, or $C$-PI, if for all $f\in C_c^\infty(\R^d)$,
    $$\var_\pi f\le C\E_\pi\|\nabla f\|^2.$$
    It satisfies a \textbf{log-Sobolev inequality (LSI)} with constant $C$, or $C$-LSI, if for all $0\not\equiv f\in C_c^\infty(\R^d)$,
    $$\E_\pi f^2\log\frac{f^2}{\E_\pi f^2}\le2C\E_\pi\|\nabla f\|^2.$$
    Furthermore, $\alpha$-strong-log-concavity implies $\frac{1}{\alpha}$-LSI, and $C$-LSI implies $C$-PI \citep{bakry2014analysis}.
    \label{def:iso}
\end{definition}