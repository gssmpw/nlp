\subsection{Thermodynamic Integration}
\label{app:rel_work_ti}
\paragraph{(I) Review of TI.} We first briefly review the thermodynamic integration (TI) algorithm. Its essence is to write the free-energy difference as an integral of the derivative of free energy. Consider the general curve of probability measures $(\pi_\theta)_{\theta\in[0,1]}$ defined in \cref{eq:pi_theta}. Then, 
\begin{equation}
    \de{}{\theta}\log Z_\theta=-\frac{1}{Z_\theta}\int\e^{-V_\theta(x)}\partial_\theta V_\theta(x)\d x=-\E_{\pi_\theta}\partial_\theta V_\theta\implies\log\frac{Z}{Z_0}=-\int_0^1\E_{\pi_\theta}\partial_\theta V_\theta\d\theta.
    \label{eq:ti}    
\end{equation}
One may choose time points $0=\theta_0<...<\theta_M=1$ and approximate \cref{eq:ti} by a Riemann sum:
\begin{equation}
    \log\frac{Z}{Z_0}\approx-\sum_{\l=0}^{M-1}(\theta_{\l+1}-\theta_\l)\E_{\pi_{\theta_\l}}\partial_\theta|_{\theta=\theta_\l}V_{\theta},
    \label{eq:ti_approx}
\end{equation}
where the expectation under each $\pi_{\theta_\l}$ can be estimated by sampling from $\pi_{\theta_\l}$. Nevertheless, there is a way of writing the exact equality instead of the approximation in \cref{eq:ti_approx}: since 
\begin{align*}
    \log\frac{Z_{\theta_{\l+1}}}{Z_{\theta_\l}}&=\log\int\frac{1}{Z_{\theta_\l}}\e^{-V_{\theta_\l}(x)}\e^{-(V_{\theta_{\l+1}}(x)-V_{\theta_\l}(x))}\d x=\log\E_{\pi_{\theta_\l}}\e^{-(V_{\theta_{\l+1}}-V_{\theta_\l})},
\end{align*}
by summing over $\l=0,...,M-1$, we have
\begin{equation}
    \log\frac{Z}{Z_0}=\sum_{\l=0}^{M-1}\log\E_{\pi_{\theta_\l}}\e^{-(V_{\theta_{\l+1}}-V_{\theta_\l})},
    \label{eq:ti_exact}
\end{equation}
which constitutes the estimation framework used in \cite{brosse2018normalizing,ge2020estimating,chehab2023provable,kook2024sampling}. Hence, we also use TI to name this algorithm. 

\paragraph{(II) TI as a special case of AIS.} We follow the notations used in \cref{thm:ais} to demonstrate the following claim: \emph{TI (\cref{eq:ti_exact}) is a special case of AIS with every transition kernel $F_\l(x,\cdot)$ chosen as the perfect proposal $\pi_{\theta_\l}$}.

\begin{proof}
In AIS, with $F_\l(x,\cdot)=\pi_{\theta_\l}$ in the forward path $\Pr$, we have $\Pr(x_{0:M})=\prod_{\l=0}^{M}\pi_{\theta_\l}(x_\l)$. In this special case, 
$$W(x_{0:M})=\log\prod_{\l=0}^{M-1}\frac{\e^{-V_{\theta_\l}(x_\l)}}{\e^{-V_{\theta_{\l+1}}(x_\l)}},$$
and hence the AIS equality becomes the following identity, exactly the same as \cref{eq:ti}:
\begin{equation}
    \frac{Z}{Z_0}=\e^{-\Delta\cF}=\E_{\Pr}{\e^{-W}}=\prod_{\l=0}^{M-1}\E_{\pi_{\theta_\l}}\e^{-(V_{\theta_{\l+1}}-V_{\theta_\l})},    \label{eq:ais_fl_pil}
\end{equation}
\end{proof}

\paragraph{(III) Distinction between \textit{equilibrium} and \textit{non-equilibrium} methods.} In our AIS framework, the distinction lies in the choice of the transition kernels $F_\l(x,\cdot)$ within the AIS framework. 

In equilibrium methods, the transition kernels are ideally set to the perfect proposal $\pi_{\theta_\l}$. However, in practice, exact sampling from $\pi_{\theta_\l}$ is generally infeasible. Instead, one can apply multiple MCMC iterations targeting $\pi_{\theta_\l}$, leveraging the mixing properties of MCMC algorithms to gradually approach the desired distribution $\pi_{\theta_\l}$. Nonetheless, unless using exact sampling methods such as rejection sampling -- which is exponentially expensive in high dimensions -- the resulting sample distribution inevitably remains biased with a finite number of MCMC iterations. 

In contrast, non-equilibrium methods employ transition kernels specifically designed to transport $\pi_{\l-1}$ toward $\pi_{\l}$, often following a curve of probability measures. This distinguishes them as inherently non-equilibrium. A key advantage of this approach over the equilibrium one is its ability to provide unbiased estimates, as demonstrated in JE and AIS.