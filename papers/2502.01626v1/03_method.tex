\section{Method}
\label{sec:method}
\input{figs/figure3}


\subsection{Data preparation}\label{sec:data_preparation}
To build a custom dataset for the person-to-person task, we follow the methodology outlined in~\cite{zhang2024boow}. First, we randomly sample two paired instances from the garment-to-person dataset~\cite{choi2021viton}. Each instance consists of a person image and the corresponding garment, such as $\left(P_{mm},G_{m}\right)$ and $\left(P_{nn},G_{n}\right)$, where $P_{mm}\in\mathbb{R}^{3\times H\times W}$ represents the person $P_m$ wearing the original garment $G_m$. We then use an off-the-shelf VTON model~\cite{choi2024improving} to virtually swap garments between these two persons, generating new data pairs, $\left(P_{mn},G_{n}\right)$ and $\left(P_{nm},G_{m}\right)$, where $P_{mn}$ denotes person $P_m$ wearing the new garment $G_n$. As shown in the top of~\cref{fig:fig3_model}, this process results in four pseudo-data triplets: $\left(P_{nn}, P_{mm}, P_{mn}\right)$, $\left(P_{mm}, P_{nn}, P_{nm}\right)$, $\left(P_{nm}, P_{mn}, P_{mm}\right)$, and $\left(P_{mn}, P_{nm}, P_{nn}\right)$. Each triplet consists of the person wearing the reference garment, the target person to be fitted, and the target person wearing the reference garment, respectively.

\subsection{Mask-Free VTON with Diffusion Transformer}\label{sec:VTON}
We adopt FLUX.1-Fill-dev~\cite{flux} as the foundation model, which has the advanced inpainting capability to fill masked regions in an existing image based on text guidance. The model's input comprises Gaussian noise, a masked condition $cond\in\mathbb{R}^{3\times H\times W}$, and its corresponding mask $M\in\mathbb{R}^{1\times H\times W}$. Here, the condition represents the preserved portion of the image $I\in\mathbb{R}^{3\times H\times W}$ after masking, expressed as $cond=I\cdot\left(1-M\right)$. The output of the model maintains the same dimensions as the condition, with the unmask region remaining unchanged. We draw inspiration from In-Context-LoRA~\cite{huang2024context} by concatenating multiple images directly along the width dimension to form the image condition. To preserve the details of the target person, we avoid masking the garment regions of the target person, in contrast to the approach used in CatVTON-FLUX~\cite{catvton-flux}. For example, using $\left(P_{nn}, P_{mm}, P_{mn}\right)$ as a training example, the reference person $P_{nn}$ and the target person $P_{mm}$ are concatenated directly to create the condition. As shown in the bottom of~\cref{fig:fig3_model}, to align with the foundation model, we append an additional blank region $BLK\in\mathbb{R}^{3\times H\times W}$, which serves as the area for inpainting. Consequently, while the input condition $cond=Concat\left(P_{nn}, P_{mm}, BLK\right)$ is of size $3\times H\times 3W$ and the corresponding mask has the same spatial dimension, the actual inpainting is performed only on the final region of size $3\times H\times W$. This approach eliminates the need for a garment mask on the target person, resulting in a mask-free input condition.

\subsection{Focus Attention Loss}\label{sec:training_loss}
Since the model needs to focus on both the garment areas of the reference image and the regions outside the garment in the target image, guiding it with text alone is challenging. Therefore, we introduce the Focus Attention loss to directly supervise attention. Specifically, when calculating attention in the FLUX-Fill-dev~\cite{flux}, text embeddings $T\in\mathbb{R}^{b\times l_1\times c}$ and image latent $L=Concat\left(G, P, F\right)\in\mathbb{R}^{b\times 3l\times c}$ are concatenated along the spatial dimension, where $G$, $P$ and $F$ represent intermediate features of the reference person, target person and the fitting outcome, respectively. $l$ is the reshaped spatial dimension of $G$, $P$ or $F$ in latent space.
The query and key can be disassembled into $\left[T_x, G_x, P_x,F_x\right]$, where $x$ is in $\{q, k\}$. After computing the attention matrix, we get the components related to the fitting output $F$: $\left[F_qT_k, F_qG_k, F_qP_k, F_qF_k\right]$. Only the two sub-matrices of attention $F_qG_k\in\mathbb{R}^{n\times l}$ and $F_qP_k\in\mathbb{R}^{n\times l}$ are related to the image condition. Therefore, during the fine-tuning of the model, the garment mask $M_r\in\mathbb{R}^{1\times H\times W}$ for the reference image and the garment mask $M_t\in\mathbb{R}^{1\times H\times W}$ for the target image are utilized to compute the loss as follows:
\begin{equation}
\begin{array}{rcl}
\mathcal{L}_{\mathrm{FA}}=\frac{1}{n}\sum_{i=1}^{n}\mathrm{mean}(Attn_{FG\_i}\cdot (1-M_r) \\ +\mathrm{mean}(Attn_{FP\_i}\cdot M_t)), 
\end{array}
\end{equation}
where, attention map $Attn_{FG\_i}\in\mathbb{R}^{1\times l}$ and $Attn_{FP\_i}\in\mathbb{R}^{1\times l}$ represent components of $F_qG_k$ and $F_qP_k$, respectively. Additionally, $M_r$ and $M_t$ are resized and reshaped to match the dimensions of the attention map.



