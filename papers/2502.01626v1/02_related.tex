\section{Related Work}
\label{sec:related}

\subsection{Virtual Try-On}
Current virtual try-on methods can be categorized into garment-to-person~\cite{choi2021viton, morelli2022dress, kim2024stableviton, choi2024improving, xu2024ootdiffusion, zhang2024boow, catvton-flux} and person-to-person~\cite{xie2022pasta, cui2024street}. The methods corresponding to the former are mostly trained on VTON-HD~\cite{choi2021viton} and DressCode~\cite{morelli2022dress}, which contain high-resolution paired data of standard garments and person images. These methods mask the areas of garment to be generated, indicating where the new garment should be placed, but this leads to the loss of foreground information. In contrast, ~\cite{zhang2024boow} proposes a mask-free method that does not explicitly define the areas to be fitted, helping to preserve the original image details. However, these methods still rely on garments as references. To achieve person-to-person try-on, ~\cite{zeng2020TileGAN, velioglu2024tryoffdiff,xarchakos2024tryoffanyonetiledclothgeneration, tan2024ragdiffusion} suggests using an additional model to restore the standard garment from the person image. Several works~\cite{xie2022pasta,cui2024street} attempt to address person image-based try-on using unpaired datasets due to limitations in the datasets, but the fitting results remain suboptimal. In our work, we use a state-of-the-art VTON model~\cite{choi2024improving} to create a person-to-person dataset and design a mask-free framework based on FLUX-Fill-dev~\cite{flux} capable of handling both garment-to-person and person-to-person tasks.
