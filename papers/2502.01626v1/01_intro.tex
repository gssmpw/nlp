

\section{Introduction}
\label{sec:intro}


% To insert a figure: \input{figs/template}
% Or table: \input{tables/template}
% \input{figs/figure2} 

The initial VTON tasks~\cite{choi2021viton, morelli2022dress, kim2024stableviton, choi2024improving, xu2024ootdiffusion, zhang2024boow} focus on generating realistic fitting images of a person dressed in a reference garment, referred to as garment-to-person in this paper. This task requires the garment to be accurately transferred while ensuring that the person details remains unchanged, except in the area covered by the garment. With the advancement of generation models~\cite{goodfellow2014generative,isola2017image, ho2020denoising, song2020denoising, rombach2022high, esser2024scaling}, this task achieves highly realistic fitting outcomes and find applications in the e-commerce industry~\cite{zalando2024}. 

In the garment-to-person task, the garment is typically a product image, free from obstruction or distortion. However, it is more difficult to obtain compared to the garment already worn by a person. To achieve person-to-person VTON, three main approaches have been proposed. First, some users attempt to segment the garment from the reference image and then use existing state-of-the-art models for try-on. However, this garment misalignment between training and inference often introduces noticeable artifacts. To address this, some researchers~\cite{zeng2020TileGAN, velioglu2024tryoffdiff,xarchakos2024tryoffanyonetiledclothgeneration, tan2024ragdiffusion, shen2024igrimprovingdiffusionmodel} use an auxiliary model to recover the standard garment from the reference person image before applying VTON to generate fitting outcomes, yielding better fidelity than the previous method. However, this approach significantly increases computational load and introduces garment restoration errors. More directly, some researchers~\cite{xie2022pasta,cui2024street} concentrate on developing end-to-end methods for the person-to-person task. However, due to the absence of a suitable dataset, they resort to using unpaired data, which has resulted in limited progress and generated fitting images that still fall short of expectations.

The garment-to-person model has made significant progress in generating high-fidelity fitting images. In this paper, to address the lack of person-to-person datasets, we first use an off-the-shelf garment-to-person model~\cite{choi2024improving} to generate person-to-person paired data. Similar to CatVTON-FLUX~\cite{catvton-flux} (unless otherwise specified, we refer to CatVTON-FLUX simply as CatVTON throughout this paper), we employ Flux-Fill-dev~\cite{flux} as our foundation model. It is built upon diffusion transformers~\cite{peebles2023scalable, esser2024scaling} and introduces advanced inpainting and outpainting capabilities. We find that directly masking the garment region on the target person during generation results in pose inconsistencies between the target person and the final fitting output, as well as the loss of foreground details such as mobile phones held in hands. In the contrary, we directly concatenate the reference garment and the original target person as input condition to preserve as much original information as possible. To avoid compromising the performance of the pretrained model, we also concatenate a full-mask image for inpainting. Additionally, to better guide the model's fine-tuning, we introduce Focus Attention Loss, which helps the attention in transformer prioritize the garment on the reference person and the details outside the garment of the target person.

Our contributions are summarized as follows:
\begin{itemize}
    \item   We propose MFP-VTON, a mask-free VTON model that generates realistic fitting images with the reference garment from product image or worn by another person.
    \item	We prepare a custom dataset for person-to-person VTON, consisting of high-quality person images, and introduce Focus Attention loss to ensure that both reference garment and the details outside the garment of the target person receive full attention.
    \item	We evaluate the superior performance of our proposed method, showcasing its ability to outperform other state-of-the-art approaches in person-to-person task while achieving comparable results in garment-to-person task.
\end{itemize}




