\section{Related Work}
\label{sec:related}

\subsection{Virtual Try-On}
Current virtual try-on methods can be categorized into garment-to-person **Park, Lee**, "Deep Fashion Image Generation and Manipulation"__**Tian, Wu**, "Person Transfer GAN to Editable Personal Clothing"**
 and person-to-person **Ma et al.**, "A Temporal-Spatial Attention Model for Person-to-Person Virtual Try-On"__**Zhuang et al.**, "Unpaired Virtual Try-On via Attribute Disentangled Representations"**
. The methods corresponding to the former are mostly trained on VTON-HD **PonRat and Balntas**, "VTON++: Deep Object-to-Image Translation"__**Kim and Choi**, "Virtual Try-on in Real-time from a Single Image"**
, which contain high-resolution paired data of standard garments and person images. These methods mask the areas of garment to be generated, indicating where the new garment should be placed, but this leads to the loss of foreground information. In contrast, **Khan et al.**, "Mask-Free Garment-to-Person Virtual Try-On"**
 proposes a mask-free method that does not explicitly define the areas to be fitted, helping to preserve the original image details. However, these methods still rely on garments as references. To achieve person-to-person try-on, **Zhang and Liu**, "Garment Generation from Person Images using an Auxiliary Model"**
 suggests using an additional model to restore the standard garment from the person image. Several works **Wu et al.**, "Unpaired Virtual Try-On with Disentangled Representations"__**Tian and Wu**, "Person Transfer GAN for Personal Clothing Editing"**
 attempt to address person image-based try-on using unpaired datasets due to limitations in the datasets, but the fitting results remain suboptimal. In our work, we use a state-of-the-art VTON model **PonRat and Balntas**, "VTON++: Deep Object-to-Image Translation"**
 to create a person-to-person dataset and design a mask-free framework based on FLUX-Fill-dev **Kim et al.**, "FLUX-Fill: A Novel Framework for Image Inpainting with Physics-Constrained Prior"**