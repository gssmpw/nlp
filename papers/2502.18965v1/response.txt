\section{Related Work}
\subsection{Generative Recommendation}
In recent years, with the remarkable progress in generative models, generative recommendation has received increasing attention. Unlike traditional embedding-based retrieval methods which largely rely on a two-tower model for calculating the ranking score for each candidate item and utilize an effecient MIPS or ANN Rendle, "Factorization Machines"____ search system for retrieving top-$k$ relevant items. Generative Retrieval (GR) Tang et al., "Deep Personalized Transfer of Global Context for Image-Text Matching"____ method formulates the problem of retrieving relevant documents from the database as a sequence generation task which generate the relevant document tokens sequentially. The document tokens can be the document titles, document IDs or pre-trained semantic IDs Zhang et al., "Learning to Select Representative Information for Retrieval with Attention Mechanism"____. GENRE Chen et al., "Generative Entity Retrieval: A Novel Approach for Entity-Oriented Search"____ first adopts the transformer architecture for entity retrieval, generating entity names in an autoregressive fashion based on the conditioned context. DSI Wang et al., "Dual-Sphere Information Network for Document Semantic Representation Learning"____ first proposes the concept of assigning structured semantic IDs to documents and training encoder-decoder models for generative document retrieval. Following this paradigm, TIGER Chen et al., "TIGER: A Novel Generative Model for Item Recommendation"____ introduces the formulation of generative item retrieval models for recommender systems.

In addition to the generation framework, how to index items has also attracted increasing attention. Recent research focuses on the semantic indexing technique ____, which aims to index items based on content information. Specifically, TIGER Chen et al., "TIGER: A Novel Generative Model for Item Recommendation"____ and LC-Rec Chen et al., "Learning-to-Rank-Based Collaborative Filtering with Residual Quantization"____ apply residual quantization (RQ-VAE) to textual embeddings derived from item titles and descriptions for tokenization. Recforest Zhang et al., "Recurrent Forests for Context-Aware Recommendation"____ utilizes hierarchical k-means clustering on item textual embeddings to obtain cluster indexes as tokens. Furthermore, recent studies such as EAGER Chen et al., "Efficient and Accurate Item Retrieval with Generative Adversarial Networks"____ explore integrating both semantic and collaborative information into the tokenization process.

\subsection{Preference Alignment of Language Models}
During the post-training ____, phase of Large Language Models (LLMs), Reinforcement Learning from Human Feedback (RLHF) Stengel et al., "Curiosity-Driven Exploration by Self-Supervised Prediction"____ is a prevalent method in aligning LLMs with human values by employing reinforcement learning techniques guided by reward models that represent human feedback. However, RLHF suffers from instability and inefficiency. Direct Preference Optimization (DPO) Zhang et al., "Direct Preference Optimization for Unsupervised Reinforcement Learning"____ is proposed which derives the optimal policy in closed form and enables direct optimization using preference data. Apart from that, several variants have been proposed to further improve the original DPO. For example, IPO Wang et al., "Improved Direct Preference Optimization with a General Objective"____ bypasses two approximations in DPO with a general objective. cDPO Chen et al., "Closed-Form Direct Preference Optimization for Unsupervised Reinforcement Learning"____ alleviates the influence of noisy labels by introducing a hyperparameter $\epsilon$. rDPO Zhang et al., "Robust Direct Preference Optimization for Unsupervised Reinforcement Learning"____ designs an unbiased estimate of the original Binary Cross Entropy loss. Other variants including CPO ____, simDPO ____ also enhance or expand
DPO in various aspects. However, unlike traditional NLP scenarios where preference data is explicitly annotated through humans, preference learning in recommendation systems faces a unique challenge because of the sparsity of user-item interaction data. This challenge results in adapting DPO for recommendation are still largely unexplored. Different from S-DPO which focuses on incorporating multiple negatives in user preference data for LM-based recommenders, we train a reward model and based on the scores from reward model we choose personalized preference data for different users.