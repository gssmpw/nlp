\section{Related Work}
\subsection{Generative Recommendation}
In recent years, with the remarkable progress in generative models, generative recommendation has received increasing attention. Unlike traditional embedding-based retrieval methods which largely rely on a two-tower model for calculating the ranking score for each candidate item and utilize an effecient MIPS or ANN \cite{houle2014rank,muja2014scalable, shrivastava2014asymmetric, ge2013optimized,jegou2010product} search system for retrieving top-$k$ relevant items. Generative Retrieval (GR) \cite{tang2023recent} method formulates the problem of retrieving relevant documents from the database as a sequence generation task which generate the relevant document tokens sequentially. The document tokens can be the document titles, document IDs or pre-trained semantic IDs \cite{tay2022transformer}. GENRE \cite{de2020autoregressive} first adopts the transformer architecture for entity retrieval, generating entity names in an autoregressive fashion based on the conditioned context. DSI \cite{tay2022transformer} first proposes the concept of assigning structured semantic IDs to documents and training encoder-decoder models for generative document retrieval. Following this paradigm, TIGER \cite{rajput2023recommender} introduces the formulation of generative item retrieval models for recommender systems. 

In addition to the generation framework, how to index items has also attracted increasing attention. Recent research focuses on the semantic indexing technique \cite{rajput2023recommender,tay2022transformer, feng2022recommender}, which aims to index items based on content information. Specifically, TIGER \cite{rajput2023recommender} and LC-Rec \cite{zheng2024adapting} apply residual quantization (RQ-VAE) to textual embeddings derived from item titles and descriptions for tokenization. Recforest \cite{feng2022recommender} utilizes hierarchical k-means clustering on item textual embeddings to obtain cluster indexes as tokens. Furthermore, recent studies such as EAGER \cite{wang2024eager} explore integrating both semantic and collaborative information into the tokenization process. 

\subsection{Preference Alignment of Language Models}
During the post-training \cite{dubey2024llama} phase of Large Language Models (LLMs), Reinforcement Learning from Human Feedback (RLHF) \cite{stiennon2020learning, ouyang2022training} is a prevalent method in aligning LLMs with human values by employing reinforcement learning techniques guided by reward models that represent human feedback. However, RLHF suffers from instability and inefficiency. Direct Preference Optimization (DPO) \cite{rafailov2024direct} is proposed which derives the optimal policy in closed form and enables direct optimization using preference data. Apart from that, several variants have been proposed to further improve the original DPO. For example, IPO \cite{azar2024general} bypasses two approximations in DPO with a general objective. cDPO \cite{rafailov2024direct} alleviates the influence of noisy labels by introducing a hyperparameter $\epsilon$. rDPO \cite{chowdhury2024provably} designs an unbiased estimate of the original Binary Cross Entropy loss. Other variants including CPO \cite{xu2024contrastive}, simDPO \cite{chowdhury2024provably}, also enhance or expand
DPO in various aspects. However, unlike traditional NLP scenarios where preference data is explicitly annotated through humans, preference learning in recommendation systems faces a unique challenge because of the sparsity of user-item interaction data. This challenge results in adapting DPO for recommendation are still largely unexplored. Different from S-DPO which focuses on incorporating multiple negatives in user preference data for LM-based recommenders, we train a reward model and based on the scores from reward model we choose personalized preference data for different users.

\vspace{-0.4cm}