% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}








\usepackage[textsize=tiny]{todonotes}
\usepackage{xcolor}
\usepackage[most]{tcolorbox}
% AIbox is used to show prompts
% \tcbset{
%   aibox/.style={
%     width=474.18663pt,
%     top=15pt,
%     colback=white,
%     colframe=black,
%     colbacktitle=black,
%     fonttitle=\bfseries,
%     % enhanced,
%     center,
%     attach boxed title to top left={yshift=-0.1in,xshift=0.15in},
%     boxed title style={boxrule=0pt,colframe=white,},
%   }
% }
% \newtcolorbox{AIbox}[2][]{aibox,title=#2,#1}
\newtcolorbox[auto counter]{response}[1][]{
    enhanced,
  top=15pt,
  bottom=15pt,
  % width=210pt,
  left=25pt,
  right=25pt,
  % boxsep=20pt,
  % before skip=10pt,   % 让整个 box 在左侧空出 10pt
  left skip=30pt,
  right skip=30pt,
  colback=gray!5,
  colframe=black,
  fonttitle=\bfseries,
  coltitle=white,
  % tit
  title=Prompt~\thetcbcounter: #1
}
% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}

\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{booktabs}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{array}
\usepackage{tabularx}
\usepackage{makecell}
\usepackage{hyperref}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\usepackage{enumitem}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\usepackage{mathtools}
\usepackage{amsthm}

\title{FedMobileAgent: Training Mobile Agents Using Decentralized Self-Sourced Data from Diverse Users}


\author{
 \textbf{Wenhao Wang\textsuperscript{1,3,4}},
 \textbf{Zijie Yu\textsuperscript{2,4}},
 \textbf{William Liu\textsuperscript{1}},
 \textbf{Rui Ye\textsuperscript{2,4}}, 
 \textbf{Tian Jin\textsuperscript{4}} \\
 \textbf{Siheng Chen\textsuperscript{2,3,4 *}},
 \textbf{Yanfeng Wang\textsuperscript{2,3 *}},
\\
\textsuperscript{1}Zhejiang University, 
\textsuperscript{2}Shanghai Jiao Tong University, \\
\textsuperscript{3}Shanghai AI Laboratory, \\
\textsuperscript{4}Multi-Agent Governance \& Intelligence Crew (MAGIC)
\\
% \small{12321254@zju.edu.cn}
}


\begin{document}

% must for acl format
\maketitle

\renewcommand\thefootnote{$*$}
\footnotetext{Corresponding author.}
\renewcommand\thefootnote{}
\footnote{\\Preprint. Under review.}



\begin{abstract}
The advancement of mobile agents has opened new opportunities for automating tasks on mobile devices. 
Training these agents requires large-scale high-quality data, which is costly using human labor. 
% Given the vast number of mobile phone users, automated data collection presents a viable solution. 
% Given the vast number of phone users worldwide, if automated data collection from them is possible, the resulting data scale is unlimited.
Given the vast number of mobile phone users worldwide, if automated data collection from them is feasible, the resulting data volume and the subsequently trained mobile agents could reach unprecedented levels. 
% Given the vast number of mobile phone users worldwide, automatic data collection from users can result in unprecedented data volume and the subsequently trained mobile agents. 
% Given the vast number of mobile users worldwide, distributed automatic data collection can generate unprecedented data volumes and empower the training of advanced mobile agents.
Nevertheless, two major challenges arise: (1) extracting high-level and low-level user instructions without involving human and (2) utilizing distributed data from diverse users while preserving privacy.
% To address these challenges, we first propose Auto-Annotation, an efficient data collection method that enables the automatic collection of high-quality datasets during users' routine phone usage with minimal cost. 
% Our approach balances performance and efficiency through a step-wise instruction generation method. 
% To further leverage distributed user data for training mobile agents while ensuring privacy, we introduce FedMobileAgent, an enhanced federated learning framework with adapted global aggregation. 
To tackle these challenges, we propose FedMobileAgent, a collaborative framework that trains mobile agents using self-sourced data from diverse users.
Specifically, it includes two techniques.
First, we propose Auto-Annotation, which enables the automatic collection of high-quality datasets during users' routine phone usage with minimal cost. 
Second, we introduce adapted aggregation to improve federated training of mobile agents on non-IID user data, by incorporating both episode- and step-level distributions.
% FedMobileAgent mitigates heterogeneity issues by incorporating both episode and step distributions during global aggregation.
% and augmenting local data for clients with limited samples.
% Our contributions include (1) a novel data construction method for mobile agents, (2) an improved federated learning approach tailored for heterogeneous mobile environments, and (3) extensive experimental validation demonstrating the effectiveness of our proposed methods.
% Our extensive experiments validate the effectiveness of our approaches and demonstrate huge potential for real-world applications.
% Compared to human-annotation, our method achieves 1\% to 11\% performance improvement with only 0.3\% to 5.7\% cost. In distributed situation, our framework achieves comparable performance with less than 0.02\% cost of the centralized human-annotated data.
% Compared to human annotation, Auto-Annotation achieves a performance improvement of 1\% to 11\% at only 0.3\% to 5.7\% of the cost. In distributed settings, FedMobileAgent achieves comparable performance with less than 0.02\% of the cost of centralized human-annotated data.
% Compared to human annotation, Auto-Annotation improves performance by 1\%–11\% at just 0.3\%–5.7\% of the cost. 
% In distributed settings, FedMobileAgent matches centralized human-annotated performance at under 0.02\% of the cost, demonstrating the potential for real-world applications.
In distributed settings, FedMobileAgent achieves performance comparable to centralized human-annotated models at less than 0.02\% of the cost, highlighting its potential for real-world applications.



\end{abstract}

% \vspace{-4mm}
% Note use of \abovespace and \belowspace to get reasonable spacing
% above and below tabular lines.
\section{Introduction}

% Recent advances in Vision-Language Models (VLMs) have significantly propelled the evolution of Graphical User Interface (GUI) agents. 
% GUI agents on mobile phones, called Mobile Agents, can automate complex tasks, significantly reducing human workload.

% Recent advances in Vision-Language Models (VLMs) have significantly propelled the evolution of Mobile Agents \cite{baiDigiRLTrainingInTheWild2024, wangMobileAgentBenchEfficientUserFriendly2024}. 
Mobile agents \cite{baiDigiRLTrainingInTheWild2024, wangMobileAgentBenchEfficientUserFriendly2024} have experienced significant advancements, propelled by recent progress in Vision-Language Models (VLMs).
% Mobile agents are designed to simulate human mobile phone usage behavior based on visual comprehension of the core VLM. 
Mobile agents, which are designed to simulate human mobile phone usage behavior, can automate complex tasks on mobile phones, saving tremendous human labor and change everyday lives. 
Compared to non-agent solutions, mobile agents offer significantly better adaptability and generalizability, enabling them to effectively handle various mobile environments and operation scenarios \cite{zhangAppAgentMultimodalAgents2023}.

% Improving training of mobile agents relies heavily on large-scale high-quality datasets \cite{chaiAMEXAndroidMultiannotation2024, songAgentBankGeneralizedLLM2024, lu2024guiodysseycomprehensivedataset}. To build such datasets, current approaches require centralized data collection followed by human annotation, leading to limited scalability and high costs. 
% To efficiently obtain training data at a much larger scale, it is necessary to shift from \textbf{centralized} to \textbf{distributed} data collection, directly sourcing data from real user interactions. Moreover, replacing \textbf{human} annotation with \textbf{automatic} annotation is essential to efficiently process the vast amount of collected data.

The training of mobile agents heavily depends on large-scale, high-quality datasets \cite{chaiAMEXAndroidMultiannotation2024, songAgentBankGeneralizedLLM2024, lu2024guiodysseycomprehensivedataset}. To build such datasets, existing approaches rely on centralized data collection followed by human annotation, resulting in high costs and limited scalability.  
% To achieve large-scale data acquisition more efficiently, a paradigm shift from \textbf{centralized} to \textbf{distributed} data collection is necessary, enabling direct data sourcing from real user interactions. 
% Different from previous works, 
%
To achieve large-scale data acquisition more efficiently, a paradigm shift from \textbf{centralized} to \textbf{distributed} data collection is necessary, enabling diverse users to participate in data contribution.  
Additionally, replacing \textbf{human} annotation with \textbf{automatic} annotation is crucial for efficiently processing the vast amount of collected data, allowing direct data sourcing from real user interactions. 


% Improving training of mobile agents relies heavily on large-scale high-quality datasets \cite{chaiAMEXAndroidMultiannotation2024, songAgentBankGeneralizedLLM2024, lu2024guiodysseycomprehensivedataset}. To build such datasets, current approaches require substantial human annotation for each data sample, leading to high costs and inefficiencies in dataset construction. 
 % 
% Our insight is that the frequent phone usage by users worldwide naturally generates valuable supervisory information, which can serve as a rich data source for training mobile agents. 
% Building on this insight, 
In this work, we aim to automatically construct high-quality datasets from diverse users' routine phone usage in a decentralized manner.
% minimizing human involvement in the process.
However, two technical challenges remain:

\begin{enumerate}[nosep]
    \item Although the users' phone usage provides real-world screenshots and actions, it is difficult to extract the real intentions behind the actions in natural language; 
    % for the tasks they perform.
    \item Data collected from a single user is both limited in scale and privacy-sensitive. The challenge lies in how to utilize distributed data from diverse users to boost performance while protecting privacy.
\end{enumerate}


% Despite the value of user data, privacy concerns make it impossible to directly collect and utilize this data for training purposes.

% To address the first challenge, we propose a Auto-Annotation method that can automatically construct high-quality datasets during users' routine phone usage.


% Specifically, we develop two methods with different performance-efficiency trade-offs to accommodate various real-world needs.
% Initially, we propose an efficient instruction generation method that organizes all available information in a manner comprehensible to the VLM within a single inference.

% To address the first challenge, we propose Auto-Annotation, an automatic data collection method that utilizes locally deployed VLMs to annotate task instructions. 
% We start by converting the actions to make them comprehendible by the VLMs.
% We further improve performance by introducing a step-wise description technique. In this approach, we leverage the VLM to describe the current action and derive the low-level instruction based on the screenshot, step by step. Subsequently, we provide the low-level instructions and the concatenation of all screenshots to the VLM, and prompt it to summarize the high-level instruction of the user.

% To tackle these challenges, we propose FedMobileAgent, a collaborative learning framework which trains a federated mobile agent based on automatically collected user data from users' daily phone usage, while preserving privacy.
% Specifically, we introduce two novel techniques to confront the two challenges respectively.

To tackle these challenges, we propose \textbf{FedMobileAgent}, a collaborative learning framework that trains mobile agents using automatically collected user data from daily phone interactions while preserving users' privacy.  
Specifically, FedMobileAgent involves two novel techniques. 

First, we propose \textbf{Auto-Annotation}, an automated method for data collection and annotation that leverages locally deployed VLMs to annotate task instructions. 
The key technical innovation lies in combining step-wise low-level instruction breakdowns with episode-wise summarization, allowing even small local VLMs to better understand the user's intent.
The step-wise description decomposes complex user instructions into simpler steps, enabling the VLM to comprehend and extract information more accurately. Meanwhile, the episode-wise summarization provides a global perspective on the entire task, generating a more comprehensive caption of the user’s ultimate instruction.
Compared with human annotation, Auto-Annotation can generate data of comparable quality with minimal cost requirement. 

 % we introduce FedMobileAgent, an approach that utilizes data automatically collected from distributed users to train mobile agents while preserving privacy.

% Additionally, we aim to enable federated training of Mobile Agent while ensuring user privacy is rigorously protected, striking a balance between technical advancement and privacy preservation.

% Additionally, to utilize decentralized data from diverse users, we establish FedMobileAgent, a novel framework which enables Federated Learning (FL) \cite{kairouz2021advances} of Mobile Agents while ensuring user privacy is rigorously protected. 
% Second, to utilize decentralized data from diverse users, we incorporate Federated Learning (FL) \cite{kairouz2021advances} to enable collaborate training of mobile agents, while ensuring user privacy is rigorously protected.
% Beyond that, we introduce an adapted global aggregation method which adapts the global aggregation weight based on each clients different distributions.
% , to further enhance the performance of the trained mobile agents in non-IID situations. 

% differs from the traditional one-level distribution \cite{karimireddy2021breaking}. 

Second, to effectively utilize decentralized data from diverse users, we pioneer the integration of Federated Learning (FL) \cite{kairouz2021advances} to enable collaborative training of mobile agents, while ensuring rigorous user privacy protection. 
% We propose a novel aggregation method, termed \textbf{adapted global aggregation}, which incorporates both episode-level and step-level distributions, effectively addressing the limitations of traditional one-level aggregation methods \cite{karimireddy2021breaking} that fail to handle the two-level heterogeneity of users' mobile data.
We further propose a novel aggregation method, termed \textbf{adapted global aggregation}, which integrates both episode- and step-level distributions to handle the two-level heterogeneity in diverse users' data, overcoming the limitations of traditional one-level aggregation methods \cite{karimireddy2021breaking}.
% We propose a novel aggregation method, called \textbf{adapted global aggregation}, which accounts for both episode-level and step-level distributions, addressing the limitations of traditional one-level aggregation methods \cite{karimireddy2021breaking} which can not deal with the heterogeneity of users' mobile data. 
Adapted aggregation adjusts the global aggregation weights using a weighted sum of episode and step counts for each client, thereby enhancing the performance of mobile agents trained in non-IID scenarios.



% Directly applying traditional FL into mobile agents results in new heterogeneous issues, negatively impacts the performance of the trained agents. 
% Therefore, we first investigate the resulting new heterogeneous issues. We identify that these issues primarily arise from the two-level distributions, i.e. step-to-episode, episode-to-dataset distributions, which differs from the traditional step-to-dataset distribution \cite{karimireddy2021breaking}. Based on this insight, we introduce an adapted global aggregation method that accounts for both episode and step distributions to enhance the mobile agent trained on multiple users' data. 

% We also  local data for clients with few samples.

   % Federated training must address the heterogeneity across user data, which manifests in multiple dimensions, to ensure effective training and robust model performance:
   
   % - **Frequency (Data Volume) Heterogeneity**: The amount of data varies greatly among users. For instance, one user may have 100 records of shopping activity, while another has only 1.

% We aim to develop innovative solutions to address these challenges, laying a solid foundation for the federated training of Mobile Agent while ensuring privacy protection and user-centric design.


Extensive experiments on Android Control and Android in the Wild \cite{liEffectsDataScale2024, rawlesAndroidWildLargeScale2023} datasets
% to validate the effectiveness of Auto-Annotation. 
% Our experiments across various base ?models
% The results demonstrate that Auto-Annotation achieves superior performance compared to human-annotated data, with less than 1\% of the annotation cost. 
demonstrate that: (1) In FedMobileAgent, each participating user can obtain a mobile agent with strong performance at less than 0.01\% of the original cost,  
(2) Auto-Annotation surpasses human-annotated data in performance while reducing annotation costs by 99\%.
(3) Adapted aggregation achieves a 5\% relative improvement over FedAvg in non-IID scenarios.
% Furthermore, experiments with FedMobileAgent show that our method can effectively leverage distributed user data, even outperforming centralized human-annotated data when reaching a certain scale. 
% These promising results suggest that, if applied in real-world scenarios, our methods have immense potential.
These promising results highlight the immense potential of our methods for real-world applications.
To summarize, our contributions are as follows: 
\footnote{Our code is publicly available at: \href{https://anonymous.4open.science/r/FedMobileAgent-2816/}{https://anonymous.4open.science/r/FedMobileAgent-2816}.}
\begin{enumerate}[nosep]
    \item We formulate the problem of self-sourced data collection from distributed mobile phone users and propose Auto-Annotation, an automatic data collection method, which achieves data quality comparable to human-annotated data at a significantly lower cost.
    % and federated training of mobile agents on decentralized user data.
    % \item We formulate the problem of s
    \item  We introduce FedMobileAgent, a collaborate learning framework for training mobile agents on distributed user data. By incorporating an adapted aggregation method, we achieve superior performance when confronted with new heterogeneity.
    % \item We propose Auto-Annotation, an automatic data collection method based on users' phone usage, which achieves data quality comparable to human-annotated data at a significantly lower cost.
    \item We conduct extensive experiments, and the promising results demonstrate that our approach has substantial potential for real-world applications. 
    
\end{enumerate}


\vspace{-1mm}
\section{Formulating Mobile Agents Trained on Distributed Auto-Annotated User Data}
\label{pro:}
\begin{figure}[t]
    % \centering
    \includegraphics[width=1\linewidth]{illu-2.png}
    \vspace{-7mm}
    \caption{Illustration of traditional centralized data collection with human annotation versus our automatic distributed approach.
}
    \label{fig:problem}
    \vspace{-3mm}
\end{figure}

\subsection{Preliminaries}
% In this section, we will briefly elaborate on several key concepts, including the definition of mobile agents and the traditional data collection process for training them.
% \paragraph{Step-Wise User Phone Usage.}
% % Let \(K\) be the total number of participating mobile phone users. \(K\) can be very large as there are billions phone users worldwide. 
% Typically, the process of one user interacting with a mobile device is formulated as follows:  
% Initially, there is a screenshot of the interface, denoted as \( s_1 \). The user aims to complete a task, denoted as \( \mathcal{T} \) in natural language, which requires \( n \) steps. Given any screenshot \( s_i \), where \( i \in [1, n] \), the user performs an action \( a_i \), causing the interface to transition from \( s_i \) to \( s_{i+1} \):
% % \vspace{-5mm}
% \begin{equation}
%     \text{User: } s_i \xrightarrow{a_i} s_{i+1} \, .
% \end{equation}
% \vspace{-2mm}
% Once the last action \( a_n \) is performed, the interface reaches the final screenshot \( s_{n+1} \), finishing the task \( \mathcal{T} \) with \( n+1 \) screenshots and \( n \) actions in total.

% Users' phone usage is a step-wise process to complete a task, denoted as \( \mathcal{T} \) in natural language, which requires \( n \) steps.




% \paragraph{Functionality of Mobile Agents.}
% \paragraph{Training of Mobile Agents.}
% \paragraph{Dataset Composition for Mobile Agents.}
% \label{def:mobile_agent}
% The mobile agent, with the core being a VLM denoted as \( \mathcal{M}_m \), simulates human user in a step-wise process for task completion.
% When applied to tasks, it follows a step-wise process. 
% Given a task \( \mathcal{T} \) in natural language, which requires \( n \) steps, at each step \(i\), the primary function of the mobile agent is to predict the next action \( a_i^* \) required to complete \( \mathcal{T} \), based on the current screenshot \( s_i \) and contextual information; that is:
% The contextual information, such as historical actions, is omitted for simplicity.
% Specifically, \( \mathcal{M}_m \) calculates the probabilistic \( p(a_i \mid \langle \mathcal{T}, s_i \rangle) \):
% \vspace{-0.5mm}
% \begin{equation}
%     \text{Mobile Agent: } \langle \mathcal{T}, s_i \rangle  \xrightarrow{\mathcal{M}_m} a_i^* \, .
% \end{equation}
% \vspace{-4mm}

% The training dataset we aim to automatically construct is 
% We neglect \(s_{n+1}\) for formulation simplicity.
\vspace{-1mm}
\paragraph{Data Composition.}

% An episode of data for training the mobile agent, denoted as \(d\), consists of three components: the task instruction \( \mathcal{T} \), a series of screenshots, and the corresponding actions. 

The mobile agent, powered by a VLM, simulates human users and completes tasks in a step-wise process. 
To train the core VLM, one data episode, denoted as \( \mathcal{D} \), comprises multiple steps, each serving as a basic training unit.
A step consists of three components: a task instruction \( \mathcal{T} \), a screenshot, and a corresponding action.  
The data episode is defined as:  
\(
\mathcal{D} = \{\langle \mathcal{T}, a_i, s_i \rangle \mid i \in [1,n] \}  
\), 
where \( \langle \mathcal{T}, a_i, s_i \rangle \) represents the \( i \)-th step, with \( a_i \) and \( s_i \) denoting the action and screenshot respectively.

   




\vspace{-2mm}
% \paragraph{Human Collection Process.}
\paragraph{Traditional Approach.}
% Based on the above functionality, 

As automating mobile phones is challenging, the training data for mobile agents places a strong emphasis on accuracy, which, at present, are all annotated by humans.
% can only be achieved by humans.  
Therefore, to obtain the three components of training data, traditional methods \cite{liEffectsDataScale2024, liFerretUI2Mastering2024} rely on (1) human-annotated task instructions, followed by (2) centralized data collection and training.


% Because its difficulty, current agents can not accurately 

% which are given as:
% To obtain the three components of training dataset, traditional method 
% Human annotators are then tasked with performing these instructions step-by-step, collecting screenshots and actions in the process.  
% \vspace{-0.5mm}
% \begin{equation}
% \label{equ:human}
% \begin{aligned}
%     &\text{Human Data Annotation:} \hspace{2.05mm} 
%     human \xrightarrow{} \mathcal{T} \, ; \\
%     &\text{Centralized Collection:} \hspace{4.7mm} 
%     human \xrightarrow{} \langle a, s \rangle \, ; \\
%     &\text{Centralized Training:} \hspace{7mm} 
%     \langle \mathcal{T},a,s \rangle \xrightarrow{}\mathcal{M}_m \, ,
% \end{aligned}
% \end{equation}
% \vspace{-0.5mm}
% where \(\mathcal{M}_m\) denotes the mobile agent to be trained.


As illustrated in Figure \ref{fig:problem}, traditional methods employ human outsourcing to generate human-annotated instructions using predefined rules or heuristics to enhance quality and diversity.
These instructions are then executed in a centralized manner to collect corresponding screenshots and actions.  
To ensure accuracy, each task is carried out step by step by human annotators, incurring substantial costs.


% which can be significantly reduced by leveraging user data.

\vspace{-2mm}
\subsection{Primary Problem} 
% Different from traditional approach, in this work, we aim to harness decentralized user data, facilitating the paradigm shift from centralized to distributed collection and from human to automatic annotation. 
% Different from , 

To address the high cost associated with traditional approach, we propose a new problem for promoting mobile agents, that is: \textbf{How to harness distributed high-quality training data from diverse users without human annotation?}

As shown in Figure \ref{fig:problem}, we further decompose our primary problem into two subordinate problems:
(1) How to automatically collect data from individual users, while eliminating the need for expensive human annotation.
(2) How to utilize distributed data from diverse users and optimize the mobile agent, while adhering to users' privacy constraints.


% (1) Given the screenshots and actions collected from daily user interactions, how to automatically generate task instructions, 
% (2) Under privacy constraints, how to effectively conduct distributed training to optimize the mobile agent.
 % we decompose the primary problem into the following : automatic data annotation and distributed training.
 % To save the cost associated with centralized human collection
% 
% Our proposed approach is:
% \vspace{-1mm}
% \begin{equation}
% \label{equ:primary_objective}
% \begin{aligned}
%     &\text{Distributed User Interaction:} \hspace{2mm} user \, u_k \xrightarrow{} \langle a, s \rangle_k \, ; \\
%     &\text{Automatic Data Annotation:} \hspace{2mm} \langle a, s \rangle _k \xrightarrow{} \mathcal{T}_k\, ; \\
%     &\text{Distributed Training:} \hspace{12.5mm} \{\langle \mathcal{T},a,s \rangle_k\}_{k=1}^{K}  \xrightarrow{} \mathcal{M}_m \, ; \\
%     &\text{Privacy Constraint: } \hspace{14mm}  \langle \mathcal{T},a,s \rangle_k \, \text{stays with}\, u_k \, ,
% \end{aligned}
% \end{equation}
% where \( u_k \) denotes the \( k \)-th user among the total \( K \) participating users. \(\langle \mathcal{T},a,s \rangle_k\) represents tasks, actions and screenshots in the dataset of \( u_k \).
% and \(\mathcal{D}_k\) is the local dataset of user \(u_k\). \(\mathcal{M}_a\) is the local annotation model while \(\mathcal{M}_m\) is the global mobile agent we aim to optimize.


\vspace{-2mm}
\subsection{Automatic Data Annotation on User Side}
\label{pro:data}
% as this is primarily an engineering task. 
% However, obtaining the natural language description of the task \( \mathcal{T} \) is more challenging because (1) users are generally reluctant to invest effort in providing \( \mathcal{T} \), and (2) deriving \( \mathcal{T} \) in reverse is difficult.
During phone interactions, users spontaneously generate screenshots and actions, which are assumed to be easily collectible.  
However, users do not receive explicit natural language instructions and only act based on their underlying intentions, making task annotation challenging.
Since users are generally reluctant to articulate their intentions and such intentions are non-trivial to infer, the first subordinate problem is how to automatically derive user intentions without human intervention, thereby constructing the training dataset. 
The objective is to learn a function \( f(*) \) that predicts user intention \( \mathcal{T}^* \), an approximation of task instruction \( \mathcal{T} \), based on \( n \) steps of actions and screenshots \( \langle a_i, s_i \rangle \):  
\vspace{-0.5mm}
\begin{equation}
    \mathcal{T}^* = f(\{ \langle a_i, s_i \rangle \}_{i=1}^{n}) \,.
\end{equation}
% \vspace{2.5mm}

% where \(\mathcal{T}^*\) denotes the prediction of \(\mathcal{T}\), with . 
% \( \mathcal{M}_a\) is the locally deployed VLM we propose to utilize. 
% Typically, the annotation process relies on human annotators, as formulated in Equ. \ref{equ:human}, which is both costly and time-consuming. Moreover, in real-world applications, users are generally reluctant to invest effort in annotation.  
% To construct the dataset without user annotations, we must automatically derive the natural language description of task \( \mathcal{T} \) based on available information.
% Therefore, we propose Auto-Annotation, an approach that utilizes a locally deployed VLM, denoted as \( \mathcal{M}_a\), to automatically achieve the objective; that is: 



%  Given any screenshot \( s_i \), where \( i \in [1, n] \), the user performs an action \( a_i \), causing the interface to transition from \( s_i \) to \( s_{i+1} \):
% \begin{equation}
%     \text{User: } s_i \xrightarrow{a_i} s_{i+1} \, .
% \end{equation}
% Therefore, user's phone usage provides

% \vspace{-2mm}
% \paragraph{Data Collection Objective.}
% \paragraph{Automatic Instruction Annotation.}


\subsection{Distributed Training of Mobile Agents}
% \subsection{Federated Training of Mobile Agents on Distributed User Data}
\label{pro:fed}
The daily phone usage of an individual generates a limited dataset, constraining the performance of a mobile agent trained solely on it. Fortunately, with millions of users worldwide, there is immense potential to collaboratively train a mobile agent using their combined data, enabling virtually unlimited scalability.
% as suggested by the scaling law \cite{kaplan2020scaling}.  
Nevertheless, directly sharing user data poses significant privacy risks, necessitating its use in a distributed manner. 
Therefore the second subordinate problem is to leverage distributed data collected from diverse users and optimize the performance of collaboratively trained mobile agents, particularly in non-IID scenarios.
% \vspace{-2mm}
% \paragraph{Reasons for Distributed Training.}
% The duration of daily mobile phone usage is inherently limited for an individual, resulting in a relatively small dataset collected on a single user's device. This small-scale dataset constrains the performance of the mobile agent trained on it. Fortunately, with millions of mobile users worldwide, there exists a vast opportunity to incentivize users to collaborate and collectively train a mobile agent \( \mathcal{M} \), using their combined data. Following the scaling law \cite{kaplan2020scaling}, leveraging multiple users' data enables virtually unlimited scalability and yields promising results.
% However, directly sharing or merging data generated from users' daily phone usage poses significant privacy risks. So the local data can only be utilized in a distributed manner. 
% To address this challenge, we adopt federated learning and develop a collaborative training framework for mobile agents.


% To tackle this, we adopt federated learning and develop a collaborative training framework for mobile agents.


% To the best of our knowledge, we are the first to apply federated learning into the training of mobile agents.

% \vspace{-2mm}
% \paragraph{Optimization of Federated Mobile Agents.}

% and the global model they aim to train is a VLM represented by \( \mathcal{M}_{G} \).
% Given the local model \( \mathcal{M}_k \) and a data sample \((t,s,a)\) from \( \mathcal{D}_k \), the objective of FedMobileAgent is to optimize the global model \( \mathcal{M}_m \) based on these local datasets; that is:
% \vspace{-1.5mm}

% \begin{equation}
% \label{equ:optimization_objective}
%     \min_{\mathcal{M}} F(\mathcal{M}) := \frac{1}{K} \sum_{k=1}^{K} \mathbb{E}_{(t,s,a) \sim P_{TSA}^{(k)}} \big[\ell(\mathcal{M}_k; t,s,a)\big] \, .
% \end{equation}
% where \( \ell: \mathcal{T} \mathcal{S} \times \mathcal{A} \to \mathbb{R}_+ \) denotes the loss function, e.g. cross-entropy. \( P_{TSA}^{(k)} \) is the distribution over \( \mathcal{T} \mathcal{S} \times \mathcal{A} \). \( \mathcal{TS} \) represents the collective input space of screenshots and task instructions, while \( \mathcal{A} \) denotes the label space of actions.
% \begin{equation}
% \label{equ:optimization_objective}
%     \min_{\mathcal{M}_m} F(\mathcal{M}) := \frac{1}{K} \sum_{k=1}^{K} \mathbb{E}_{(t, s, a) \sim P_{\mathcal{T} \times \mathcal{S} \times \mathcal{A}}^{(k)}} \left[ \ell(\mathcal{M}_k; t, s, a) \right] \, .
% \end{equation}
% where \( \ell: \mathcal{T} \times \mathcal{S} \to \mathcal{A} \to \mathbb{R}_+ \) denotes the loss function, such as cross-entropy. 

% The global optimization objective is then formulated as:

% In traditional FL, all clients possess a unique data distribution \( P_{XY}^{(k)} \) over \( \mathcal{X} \times \mathcal{Y} \), where 
% In traditional FL, all clients possess a unique data distribution \( P_{XY}^{(k)} \) over \( \mathcal{X} \times \mathcal{Y} \), where \( \mathcal{X} \) represents the collective input space , while \( \mathcal{Y} \) denotes the label space of \( D_{k} \).
% We assume that the distributions \( P_{XY}^{(k)} \) differ across clients, which is a common scenario in FL. 


% where \( \mathcal{M} = (\mathcal{M}_1, \mathcal{M}_2, \dots, \mathcal{M}_K) \) represents the collection of all local VLMs.
% \paragraph{New Heterogeneity Introduced by Federated Mobile Agents.}
\vspace{-2mm}
\paragraph{New Heterogeneity Introduced.}
\label{hetero}

% Directly combining federated learning with mobile agents introduces new heterogeneity. Unlike traditional federated learning tasks, the structure of mobile datasets follows a temporal sequence and can be further divided into steps. During training, each step is treated as a data sample, while in real-world scenarios, the data are collected episode by episode. 



% This structure exhibits a two-level distribution.  which are overlooked by traditional FL. 

Conventional distributed learning models the distribution of individual steps while overlooking repetitive patterns within the same episode.  
However, the actual distribution exists at two levels: among steps within an episode and among episodes within the dataset, which we define as the two-level distribution.

Specifically, let the dataset for user \(u_k\) contain \( n_k^{epi} \) episodes. The \( j \)-th episode, where \( j \in [1, n_k^{epi}] \), consists of \( n_k^j \) steps under a fixed task instruction \( T_k^j \).  
Since \( T_k^j \) remains constant within an episode, its distribution simplifies to \( P_{SA}^{(j,k)} \). 
We define \( P_T^{(k)} \) as the marginal distribution of \( T_k^j \). 
\( \mathcal{T} \), \( \mathcal{S} \), and \( \mathcal{A} \) represent task, screenshot, and action spaces, respectively.
The two-level distribution \( P_{\mathcal{T} \times \mathcal{S} \times \mathcal{A}}^{(k)} \) is then given by:
% \vspace{-1.5mm}
\begin{equation}
    P_{\mathcal{T} \times \mathcal{S} \times \mathcal{A}}^{(k)} = \sum\nolimits_{j} P_{SA}^{(j,k)} \cdot P_T^{(k)}(T_k^j) \, ,
\end{equation}
% \vspace{-1.5mm}
We empirically observe that this two-level distribution gives rise to new heterogeneity issues that need to be addressed.


% Let the dataset contain \( n_k^{epi} \) episodes. For the \(j\)-th episode, where \( j \in [1, n_k^{epi}] \), it contains \( n_k^j \) steps and has a task instruction \(T_k^j\). Since \(T_k^j\) remains the same for all steps in the episode, the distribution within this episode simplifies to \( P_{SA}^{(j,k)} \). 
% Normally, for any two episodes \( j, j' \in [1, n_k^{epi}] \), \( P_{SA}^{(j,k)} \neq P_{SA}^{(j',k)} \).
% We define \( P_{T}^{(k)}(T_k^j) \) as the marginal distribution of \( T_k^j \), representing the probability of \( T_k^j \) occurring. The overall distribution can then be expressed as the sum of the product of these two distributions:
% \vspace{-5mm}
% In this work, we empirically find that the two-level distribution results in a newly discovered heterogeneity, which is more skewed compared to one-level heterogeneity in traditional FL. And we design an adapted aggregation technique to mitigate such issue.

% And we propose an adapted aggregation technique to address this absence.


% As pioneers in addressing this issue, we focus on the most straightforward feature: step length, simplifying the distribution \( P_{XY^j}^{(k)} \) to the count of steps within each episode. 
% Our experimental results in Section \ref{exp:non-iid} prove the existence of the two-level heterogeneity.
% Even if the client has an IID distribution based on episode numbers, i.e., \( P_{Y^jY}^{(k)} \), there still exists a non-IID distribution between different steps. For example, two clients may each have 10 episodes of shopping data, but one client's episodes are short and intensive, while the other's episodes are long and contain repeated steps. The models trained on these datasets will likely behave differently due to the heterogeneity of \( P_{XY^j}^{(k)} \). 

% On the other hand, even when the distribution \( P_{XY^j}^{(k)} \) is IID, one client may have many more episodes or different types of episodes compared to another client, still resulting in highly-skewed distribution. 


% In the setting of federated mobile agents, clients have a two-level distribution \( P_{XY}^{(k)} \) , where \( \mathcal{X} \) represents the input space for all steps \( \langle \mathcal{T}_j^k, s_{i,j}^k \rangle \), while  \( \mathcal{Y} \) denotes the label space corresponding to the actions \( a_{i,j} \) with \( i \in [1, n_k^j] \) in each episode \( e_k^j \), where \( i \in [1, n_k^j] \). Finally, \( \mathcal{Z} \) represents the label space for all actions in the dataset \( D_k \), which can be written as the set \( \{ a_{i,j} \mid i \in [1, n_k^j], j \in [1, n_k^{epi}] \} \).
% The key distinction behind the differences across app categories or app names is that the typical usage of these apps varies. The most apparent difference is in the step length, which is affected by the type of app.

% P_{XY}^{(k)}(x, y) = \sum_{j=1}^{n_k^{epi}} \sum_{i=1}^{n_k^j} P_{\mathcal{X}}(x_{i,j}) \cdot P_{\mathcal{Y}}(y_{i,j} \mid x_{i,j})


% So as the pioneer on this issue, we choose to focus on the most straightforward feature of step length, i.e. we simplify the distribution to the number counts.
% We desgin delicate experiments on this and Our experiment results in \ref{} prove the existence of such issue.
% We provide further analysis and results in \ref{}
% so we fix the traditional average aggregation of model weight and propose a two-level sample based aggregation method.

\section{Methodology}
\subsection{System Overview}
% FedMobileAgent consists of two levels. 
% As shown in the left part of Figure \ref{fig:main}, on the first level, we automatically collect and annotate data from one user through our proposed Auto-Annotation. 
% After data construction, each user trains a local mobile agent.
% Then move on to the second level of FedMobileAgent. As shown in the right part of Figure \ref{fig:main}, at this stage, we enable users to collaborate by utilizing federated learning and together train a global agent with strong capability.
The proposed FedMobileAgent is a collaborative learning framework for training mobile agents that automatically collects individual user data and achieves effective utilization of distributed data from diverse users.
% distributed training of mobile agents among diverse users.
It mainly involve two procedures: Auto-Annotation and federated training with adapted global aggregation.

First, as shown on the left of Figure \ref{fig:main}, to reduce the high cost produced by human annotation, we propose \textbf{Auto-Annotation} to automatically build dataset on each participating user. 
We utilize screenshots and actions from distributed users' daily phone usage, eliminating the need for traditional centralized human execution. 
As the actual user instruction is unknown, we then employ a locally deployed VLM to automatically derive user instructions, thereby reducing the annotation cost.
Inspired by human reasoning process, we decompose the entire task into multiple steps, allowing the VLM to better interpret user intention.

% For each screenshot and action pair, a locally deployed VLM, termed "Descriptor", is used to infer the user's intention and generate a low-level instruction of this step. These low-level instructions are then aggregated and passed to another local VLM, "Summarizer", which synthesizes them into the user's high-level task instruction.
 

Second, depicted in the right part of Figure \ref{fig:main}, 
% to utilize distributed data from diverse users while ensuring privacy protection, we incorporate federated learning to achieve collaborative training of a global mobile agent.
users with auto-annotated data collaborate via federated learning to jointly train the target mobile agent with enhanced capabilities. 
In FedMobileAgent, we leverage federated learning as it achieves effective collaboration while ensuring privacy protection. 
Moreover, as federated mobile agents encounter new heterogeneity, we introduce an \textbf{adapted aggregation method} that improves traditional approaches by considering both episode- and step-level distributions for each user.


% Each user trains a local model based on the dataset constructed by Auto-Annotation and uploads it to the server. Then the server aggregates all uploaded models using \textbf{adapted global aggregation} method. 

% After aggregation, the global mobile agent we aim to optimize is obtained.

% At the second level, depicted in the right part of Figure \ref{fig:main}, users collaborate via federated learning to jointly train a mobile agent with enhanced capabilities. Each user trains a local model and uploads it to the server, where the models are aggregated using our adapted global aggregation method. This method improves traditional aggregation by accounting for both episode and step distributions across users. After aggregation, the optimized global mobile agent is obtained.


% \begin{figure*}
%     \centering
%     \includegraphics[width=1\linewidth]{1-3.png}
%     \caption{Caption}
%     \label{fig:main}
% \end{figure*}

\begin{figure*}[t]
\vspace{-3mm}
    \centering
    \includegraphics[width=1\linewidth]{v2-3.png}
    \vspace{-7mm}
    \caption{System overview of FedMobileAgent. 
    During individual users' daily phone usage, Auto-Annotation automatically constructs training data through step-wise descriptions and episode-wide summarization. Each participating user then locally trains a agent and uploads it to the server. By applying adapted global aggregation, we obtain the target global mobile agent with enhanced capabilities.}
    \label{fig:main}
    \vspace{-3mm}
\end{figure*}


\subsection{Auto-Annotation: Automatic Mobile Data Collection from Daily Phone Usage}
% In this section, we introduce our data construction framework. We begin by formulate the data construction task on mobile devices.
% Considering the potentially limited computational resources on mobile devices, we propose two methods that address the trade-off between performance and efficiency to accommodate the diverse needs of real-world applications. 

Auto-Annotation functions by automatically building datasets from users' phone usage. To annotate user instructions, the idea is to employ a local VLM to interpret user intentions step by step.
Auto-Annotation is composed of two procedures: 
(1) During or after a user’s daily phone usage, it incrementally generates low-level instructions, step by step.  
(2) These low-level instructions are then consolidated into a high-level instruction for the entire episode.
Note: A low-level instruction is a specific, atomic directive that corresponds to an individual step in the task, whereas a high-level instruction represents the overall task objective.

% We also provide a simplified version of Auto-Annotation to accommodate the diverse needs of real-world applications in our experiments.
% \subsubsection{Efficient Method within Single Inference Time.}
\vspace{-2mm}
\paragraph{Step-Wise Instruction Description.}
% We introduce a multi-model, 

We aim to annotate low-level instructions through step-wise description, a novel technique that decomposes complex user tasks into multiple steps.
At each step, a local VLM is designed to automatically generate a low-level instruction in consideration of the current screenshot and converted action. 
This procedure enables the VLM to better interpret each step, leading to more accurate instructions.


As indicated by previous works \cite{zheng2024gpt4visiongeneralistwebagent}, some VLMs, such as GPT-4V \cite{2023GPT4VisionSC}, are unable to effectively identify the location of operations. Therefore, to make the original action interpretable to the VLM, also referred to as annotation model, we propose using a rule-based technique instead of using models \citet{wangMobileAgentAutonomousMultiModal2024} to transform the action (e.g., \texttt{click} \(x, y\)) into a natural language sentence.
Specifically, for \( \texttt{click} \) actions, we align the exact point of click with a specific unit in the interface, based on XML files. If the unit contains text or a function call, we use the corresponding text or function name to formulate the action description. 
For other actions, such as \( \texttt{navigate\_home} \) or \( \texttt{wait} \), we slightly adjust the phrasing to improve clarity and comprehension. After this conversion, we obtain the converted action \( A_i \).


% First, in order to make the original action interpretable to the VLM \(\mathcal{M}_a\), we transform the original action (e.g., "click at \(x, y\)") into a natural language sentence. 
% Specifically, using the technique similar to \citet{wangMobileAgentAutonomousMultiModal2024}, for $Click$ actions, we identify the exact point of click point based on XML files and align it with a specific unit in the interface. 

We subsequently employ the annotation model \(\mathcal{M}_a\), which we refer to as the "Descriptor", to generate low-level instructions. 
At each step \(i\), the Descriptor is prompted to describe the user's intention, which serves as an approximation of the actual low-level instruction \( \mathcal{T}_i^{\text{low}} \), based on the current screenshot \(s_i\) and the associate converted action \(A_i\):
\vspace{-1mm}
\begin{equation}
    \text{Descriptor} : \langle s_i, A_i \rangle \xrightarrow{\mathcal{M}_a} \mathcal{T}_i^{\text{low}} \, .
\end{equation}
% where \( \mathcal{T}_i^{\text{low}} \) is the prediction of user intention, serving as an approximation of the actual low-level instruction. 
% (subordinate instructions derived from user's high-level instructions). 
This step-wise process ensures that the information is more finely processed, allowing \(\mathcal{M}_a \) to provide a more accurate description of each step. As a result, the VLM can later perform improved summarization.
Details of our prompt templates can be found in Appendix \ref{app:template}.

\vspace{-2mm}
\paragraph{Episode-Wise Intention Summarization.}
Episode-wise intention summarization generates high-level instructions by summarizing the low-level instructions from all steps. 
The novelty is providing global context enriched with step-wise details, enabling the annotation model to effectively extract the user intention.


To provide global visual context for the annotation model \(\mathcal{M}_a\), referred to as "Summarizer" in this step, we first concatenate all relevant screenshots into a single image \( s_{\text{c}} \), arranged in chronological order.
% Note that, this approach (1) not only allows the Summarizer to develop a comprehensive understanding of the entire task sequence, (2) but also saves the need of multiple inferences by only inferencing once.
Note that this approach (1) allows Summarizer to develop a comprehensive understanding of the entire task sequence, and (2) eliminates the need for multiple inferences by performing inference only once.

Finally, we compile the concatenated screenshot \( s_{\text{c}} \) and the list of low-level instructions \( \{\mathcal{T}_i^{\text{low}}\}_{i=1}^{n} \) into a single prompt and feed it into \( \mathcal{M}_a \) to summarize the user’s overall intention  \(\mathcal{T}^{\text{high}}\):
\vspace{-1mm}
\begin{equation}
    \text{Summarizer} : \langle s_{\text{c}}, \{\mathcal{T}_i^{\text{low}}\}_{i=1}^{n} \rangle \xrightarrow{\mathcal{M}_a} \mathcal{T}^{\text{high}} \, .
\end{equation}
% \vspace{-1mm}
% where \(\mathcal{T}^{\text{high}}\) denotes the user's overall intention we extract. 
As users do not provide explicit instructions, \(\mathcal{T}^{\text{high}}\) serves as the high-level instruction, simulating what the user would convey if asking an agent to perform the same task. 

% Accordingly, we present the following formulation:
% During daily phone usage, users typically do not provide explicit instructions. The ultimate intention we extract simulates the high-level instruction the user would convey if asking an agent to perform the same task. 
% actions in a single inference step, significantly reducing computational costs.
% Finally, we organize the available information above and provide it to \(\mathcal{M}_a\). 
% % Once we have gathered all the low-level descriptions from the episode, 
% We compile the concatenated screenshot and the list of low-level instructions into one prompt and ask the Summarizer, to summarize the ultimate intention of the user. 


% Combined with step-wise description, this novel technique outputs high-quality instructions comparable to human annotation, while only using local deployed VLMs, saving tremendous cost.
Combined with step-wise description, episode-wise summarization produces high-quality instructions comparable to human annotation, all while exclusively using locally deployed VLMs, thereby substantially reducing cost.
% As formulated in Section \ref{pro:data}, the screenshots and actions are easy to collect, by automatic annotating user instructions, we can build the local training dataset \( \mathcal{D}_k \) for each user \(u_k\) participating in FedMobileAgent and eventually obtain a capable global mobile agent.
% By automatic annotating user instructions, we manage to build the local training dataset \( \mathcal{D}_k \) for each user \(u_k\) who later participates in the distributed training of FedMobileAgent.


% By automatically annotating user instructions, we construct the local training dataset \( \mathcal{D}_k \) for each user \( u_k \), who subsequently participates in the federated training of mobile agents.


% \vspace{-3mm}


% \label{met:single}
% For the first method, we focus on efficiency and simplicity. The most efficient manner to generate user instructions is gathering all the available information and provide them to the annotation model \(\mathcal{M}_a\). 
% However, two major challenges remain: (1) How to transform raw actions (e.g., click at coordinates \(x, y\)) into meaningful sentences in natural language that can be understood by \(\mathcal{M}_a\). (2) How to organize multiple converted actions and corresponding screenshots so that \(\mathcal{M}_a\) can gain a global understanding of the user's behavior.



% \subsubsection{Enhanced Method with Step-Wise Description}
% Building upon the efficient method presented in Section \ref{}, we further enhance the approach by introducing step-wise calibration and a multi-agent architecture.

% \paragraph{Screenshot Description}
% To better understand the information embedded in the screenshots, we do not merely glance at them in the final summarization step. Instead, we employ a VLM called the Interpreter to interpret the current screenshot \(s_i\) and extract key information from it. Specifically, the Interpreter is tasked with identifying the titles and names of the units within the screenshot.

% \begin{equation}
%     f_{\text{screen}} :  s_i \xrightarrow{\text{Interpreter}} \tau_{\text{low}} \, .
% \end{equation}

% \vspace{-2mm}
% Part 2: FedMobileAgentAgent
\subsection{Federated Training of Mobile Agents on Distributed User Data with Adapted Global Aggregation}
\label{met:fed}

% Due to constrained resources and phone usage time, the self-sourced data collected from a single user is limited in scale, thus hindering the development of the locally trained mobile agent. 
% % Continuing to improve the performance of mobile agents trained on self-sourced data necessitates 
% To facilitate training on distributed data from diverse users, we propose FedMobileAgent, a novel collaboration framework which incorporates federated learning to train a global mobile agent. 
% Our innovation lies in that we propose an adapted aggregation method to tentatively address the newly introduced heterogeneity formulated in Section \ref{hetero}.

Due to limited resources and phone usage time, the self-sourced data from a single user is restricted in scale, which in turn hinders the performance of locally trained mobile agents. 
To overcome this limitation, we introduce a novel collaborative framework within FedMobileAgent, that leverages federated learning to facilitate training on distributed data from diverse users. 
Our key innovation is the introduction of an adapted aggregation method, designed to address the heterogeneity discussed in Section \ref{hetero}. 
% FedMobileAgent consists of two primary procedures: local agent training and adapted global aggregation.


% to enable collaboration among multiple users without directly sharing privacy-concerned local data, by incorporating federated learning. 
% Our federated training framework for mobile agents, called , 

% We further propose a enhanced global aggregation method, called FedMobileAgent, to address the newly introduced heterogeneity which differs from traditional forms.
We consider a setup with \( K \) clients (users) and a central server, where all clients communicate with the server to collaboratively train a mobile agent without directly sharing private data. Let \( \mathcal{D}_k \) denote the private dataset for client \( u_k \), which is automatically constructed using Auto-Annotation.
\vspace{-2mm}
\paragraph{Local Agent Training.}
\label{met:local_training}
% We follow the the setup in Section \ref{pro:fed}.
% The users participating in the collaboration are denoted as \( u_k \).
% the server broadcast the global model to clients for local model initialization. 
For each communication round \(l\), the server broadcasts the global model \( \mathcal{M}^{(l)} \) to each available client \( u_k \in \mathcal{S}^l \), where $\mathcal{S}^l$ denotes the set of all participating clients at round \(l\). 
Then, each client \(u_k\) uses the global model to initialize its local model as:
% \vspace{-1mm}
\(
    \mathcal{M}_k^{(l,r+1)} := \mathcal{M}^{(l)} \, ,
\)
where \( \mathcal{M}_k^{(l,0)} \) denotes the local model at the \(l\)-th round and $0$-th training iteration.
Starting from the initial local model \( \mathcal{M}_k^{(l,0)} \), client \( u_k \) conducts multiple iterations of stochastic gradient descent (SGD) updates on its local dataset \( \mathcal{D}_k \). At each iteration \( r \), with learning rate \( \eta \), the local model is updated as follows:
\vspace{-0.5mm}
\begin{equation}
    \mathcal{M}_k^{(l,r+1)} = \mathcal{M}_k^{(l,r)} - \eta \nabla \ell(\mathcal{M}_k^{(l,\tau_k}); \mathcal{T},s,a) \, ,
\end{equation}
% \vspace{-0.5mm}
where \( \ell(\mathcal{M}_k^{(l,\tau_k}); t,s,a) \) represents the computed loss based on a data sample \(\langle \mathcal{T}, s, a \rangle \). 
% same as the one appearing in Equ. \ref{equ:optimization_objective}. 
% \( \eta \) is the learning rate. 
After \( \tau_k \) iterations, the final local model is denoted as \( \mathcal{M}_k^{(l,\tau_k)} \).
% In consideration of the high possibility that some users can only provide minimal data samples, if the proportion of such clients are large, the performance of model \(\mathcal{M}\) trained will be weaken.
% In light of the high likelihood that some users can provide only minimal data samples, a large proportion of such clients would weaken the performance of the trained model \( \mathcal{M} \).
% Therefore, we propose a novel augmentation technique. Specifically, we concatenate two screenshots \(s_i\) and \(s_{i+1}\) together, and connect the predicted actions to make a new training data sample apart from the existing two \(\langle \mathcal{T}, a_i, s_i \rangle\). The concatenation is denoted as a function \(C\).
% % \vspace{-2mm}
% \begin{equation}
%    \langle \mathcal{T}, a_{new}, s_{new} \rangle := \langle \mathcal{T}, C(a_i, a_{i+1}), C(s_i, s_{i+1}) \rangle \, .
% \end{equation}
When the local training finishes, each user \(u_k\) uploads \( \mathcal{M}_k^{(l,\tau_k)} \) to the server, completing the local training of FedMobileAgent.

\vspace{-2mm}
\paragraph{Adapted Global Aggregation.}
In this step, the server updates global model by aggregating local models, which is subsequently broadcast to available clients for the next round. 
Our innovation lies in adapting the aggregation strategy to accommodate the two-level structure of datasets used for training mobile agents, encompassing both step-level variations and episode-level distributions.


Traditional FL methods, such as FedAvg and FedProx \cite{fedprox}, use the sample number of client as the aggregation weight.
% % \vspace{-2mm}
% \begin{equation}
%     \omega_k = \frac{n_k}{\sum_{j\ins_i^t} n_k}
% \end{equation}
% \vspace{-2mm}
% \begin{equation}
%     \mathcal{M}^{(t+1)} := \sum_{k\ins_i^t} \frac{n_k}{\sum_{j\ins_i^t} n_k} \mathcal{M}_k^{(t)}.
% \end{equation}
This insight has been proven successful over the past several years \cite{li2019convergence, li2023revisiting}. However, previous aggregation methods \cite{fedavgm, fedopt}, which perform well on tasks such as image classification, overlook the two-level distribution discussed in Section \ref{hetero}. They treat all samples equally, regardless of whether they come from the same episode or not. 
This strategy fails in the case of heterogeneity.
% By treating all samples equally, regardless of whether they come from the same episode or not, these methods perform well on homogeneous datasets but fail when faced with heterogeneity.
% This works fine until met with a heterogeneous .


For instance, consider two clients, each having 100 data samples (steps). One client's data contains 25 short, intensive episodes, while the other has 5 long episodes with repeated steps. 
% Although both clients have the same sample counts, the nature of their data differs significantly.
The model trained on the first client’s data is expected to behave better due to exposure to a more diverse set of tasks.
% , while the model trained on the second client’s data, which has fewer episodes and repeated steps, might struggle. 
Yet, in traditional FL, these models are merged equally, ignoring the critical performance distinction.


To address this limitation, we propose a novel aggregation technique adapting to the new scenario in FedMobileAgent.
Within federated training of mobile agents, the data samples can be measured by both step count \( n_k \) and episode count \( n_k^{epi} \). \( n_k^{epi} \) is as well as, or even more important as it indicates how many tasks the agent has learned on.
% Therefore, we can ignore either. 
As \( n_k^{epi} \) and \( n_k \) are measured in different scales, we empirically set a hyper-parameter \( \lambda \) to align them, which is calculated around the average step length of all episodes. 
Then we redefine the sample count as \(n_k^*\) and reformulate the aggregation weight based on our adapted sample count \(n_k^*\); that is:
\vspace{-1mm}
\begin{equation}
\begin{aligned}
    n_k^* := \lambda n_k^{epi} + n_k \,, \hspace{4mm} \omega_k = \frac{n_k^*}{\sum_{k \in \mathcal{S}^l} n_k^*} \, ,
\end{aligned}
\end{equation}
% \vspace{-0.5mm}
where \( \omega_k \) denotes the weight for client \(u_k\) and \(\mathcal{S}^l\) is the sampled participating clients. This design smoothly improves upon traditional aggregation and inherits its convergence property.
When \(\lambda = 0\), it degrades to normal aggregation. Finally, the global model \( \mathcal{M}^{(l)}\) is adaptively aggregated as:
% \begin{equation}
%     \mathcal{M}^{(l+1)} := \sum_{k\in \mathcal{S}^l} \frac{n_k^*}{\sum_{j\in \mathcal{S}^l} n_j^*} \mathcal{M}_k^{(l)}.
% \end{equation}
% \vspace{-0.5mm}
\begin{equation}
    \mathcal{M}^{(l+1)} := \sum\nolimits_{k\in \mathcal{S}^l}  \omega_k \mathcal{M}_k^{(l)}.
\end{equation}
% \vspace{-0.5mm}
The adapted aggregation in FedMobileAgent balances both episode and step counts, achieving a better utilization of decentralized data from heterogeneous users.

\begin{table*}[t]
\vspace{-2mm}

\centering
% \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{5.5pt}
\begin{tabular}{lcccccccc}
\toprule
\multirow{2}{*}{\textbf{Methodology}} & \multirow{2}{*}{\textbf{Size}} & \multirow{2}{*}{\textbf{Clients}} & \textbf{High Level} & \multicolumn{2}{c}{\textbf{Low Level}} & \multicolumn{2}{c}{\textbf{Anno. Cost}}\\ 
~ &  ~ & ~ & Step Acc & Step Acc & Epi. Acc & Total & Per Client  \\ \midrule

 Zero-Shot & - & - & 27.69 & 54.61 & 56.04 & - & -  \\ \midrule

% \multirow{2}{*}{\makecell[{{p{2cm}}}]{\raggedright Human-\\Annotation}} 
Human-Centralized & 1000 & 1  & 54.31 & 76.89 & 25.5 & 10880 & 10880  \\ 
Human-FedAvg & 1000 & 10  & 39.21 & 73.76 & 23.5 & 10880 & 1088  \\ \midrule
% \multirow{6}{*}{\makecell[{{p{2cm}}}]{\raggedright Auto-\\Annotation}} & 500 & 10 & Local & 25.78 & 64.38 & 13.0 & 15.51  \\ 
\multirow{5}{*}{FedMobileAgent} & 500 & 10  & 35.07 & 67.28 & 16.5 & 15.51 & \multirow{5}{*}{\textbf{1.55}} \\ 
~ & 1000 & 20 &  39.36 & 69.34 & 20.0 & 31.01 &  \\ 
~ & 5000 & 100  & 48.51 & 74.75 & 23.5 & 155.06 &  \\ 
~ & 5000 & 100*  & \textbf{54.61} & 76.28 & 27.0 & 155.06 &  \\ 
~ & 7000 & 140  & 52.25 & \textbf{78.64} & \textbf{28.0} & 217.08 &  \\ 
\bottomrule
\end{tabular}
\caption{Performance and cost comparison of FedMobileAgent in IID settings. We assume each client has 50 episodes data. With more clients participating in the collaboration, the performance of the global mobile agents grows and even surpasses the centralized human-annotated data with less than 0.02\% cost per client. * denotes 60\% clients participation, while others are 30\%. }
\label{tab:fed}
% \vspace{-2mm}

\end{table*}


\begin{table*}[t]
% \vspace{-2mm}
\centering
\setlength\tabcolsep{6pt}
\begin{tabular}{lcccccc}
\toprule
% \multirow{2}{*}{\textbf{Fed-Alg}} & \multirow{2}{*}{\textbf{IID}} & \multicolumn{3}{c}{\textbf{Skew}} & \multicolumn{2}{c}{\textbf{Avg.}}\\
% ~ & ~ & \textbf{Step} & \textbf{Epi.} & \textbf{Both} & \textbf{All} & \textbf{Skew}  \\ 

\textbf{FL-Algorithm} & \textbf{IID} & \textbf{Step Skew} & \textbf{Epi. Skew} & \textbf{Both Skew} & \textbf{Avg. All} & \textbf{Avg. Skew}  \\
\midrule
Centralized Learning & \multicolumn{6}{c}{43.61}  \\

% Central Learning & 43.61 & 43.61 & 43.61 & 43.61 & 43.61 & 43.61 \\ 
Local Learning & 23.72 & 13.04 & 28.33 & 22.13 & 21.17 & 21.81  \\ 
\midrule
FedAvg w Step Aggregation & 34.52 & \textbf{35.05} & 26.88 & 29.64 & 31.52 & 30.52 \\ 
FedAvg w Epi. Aggregation & 34.52 & 32.81 & 27.14 & 30.57 & 31.26 & 30.17 \\ 
FedMobileAgent & 34.52 & 33.73 & \textbf{28.06} & \textbf{34.39} & \textbf{32.68} & \textbf{32.06} \\ 
\bottomrule
\end{tabular}
\caption{Evaluation of FedMobileAgent in non-IID settings. We evaluate four different distributions on WebShopping subset of AitW. FedMobileAgent with our adapted aggregation method achieves superior performance on average, compared to FedAvg.}
\label{tab:fed_skew}
\vspace{-4mm}
\end{table*}

\begin{table*}[t]
\vspace{-2mm}

\setlength\tabcolsep{6pt}
\centering
% \small
\begin{tabular}{llcccccc}
\toprule
\multirow{2}{*}{\textbf{Base Model}} & \multirow{2}{*}{\textbf{Methodology}} & \multirow{2}{*}{\textbf{Size}} & \textbf{High Level} & \multicolumn{2}{c}{\textbf{Low Level}} & \multicolumn{2}{c}{\textbf{Anno. Cost}}\\ 
% \cmidrule(l){4-8} 
&  &  & Step Acc & Step Acc & Epi. Acc & Single & Total \\ \midrule
\multirow{6}{*}{Qwen2-VL-7B} & Zero-Shot & - & 27.69 & 54.61 & 3.5 & - & -  \\ 
~ & Human-Annotation & 1000 & 54.31 & 76.89 & 25.5 & 10.88 & 10880  \\ 
~ & Auto-Annotation-S & 1000 & 52.48 & 66.36 & 16.5 & \textbf{0.0099} & \textbf{9.87} \\ 
~ & Auto-Annotation & 1000 & 52.94 & 70.52 & 23.0 & 0.0310 & 31.01  \\ 
~ & Auto-Annotation-S & 5000 & 55.91 & 67.58 & 20.5 & \textbf{0.0099} & 49.33  \\ 
~ & Auto-Annotation & 5000 & \textbf{56.06} & \textbf{81.92} & \textbf{33.5} & 0.0310 & 155.06 \\ 
\midrule
        
\multirow{6}{*}{InternVL2-2B} & Zero-Shot & - & 11.14 & 21.82 & 0.0 & - & -  \\ 
~ & Human-Annotation & 1000 & 40.50 & 71.06 & 24.0 & 10.88 & 10880  \\ 
~ & Auto-Annotation-S & 1000 & 37.60 & 47.29 & 4.5 & \textbf{0.0072} & \textbf{7.23}  \\ 
~ & Auto-Annotation & 1000 & 35.32 & 64.99 & 10.5 & 0.0269 & 26.94  \\ 
~ & Auto-Annotation-S & 5000 & \textbf{47.06} & 54.16 & 5.0 & \textbf{0.0072} & 36.13  \\ 
~ & Auto-Annotation & 5000 & 45.46 & \textbf{74.68} & 22.0 & 0.0269 & 134.71 \\ 
\midrule

\multirow{6}{*}{InternVL2-8B} & Zero-Shot & - & 29.52 & 31.35 & 1.0 & - & -  \\ 
~ & Human-Annotation & 1000 & 49.89 & 79.86 & 29.0 & 10.88 & 10880  \\ 
~ & Auto-Annotation-S & 1000 & 45.46 & 57.97 & 6.5 & \textbf{0.0232} & \textbf{23.18}  \\ 
~ & Auto-Annotation & 1000 & 49.66 & 65.45 & 17.5 & 0.1237 & 123.65  \\ 
~ & Auto-Annotation-S & 5000 & 51.64 & 55.42 & 11.5 & \textbf{0.0232} & 115.91  \\ 
~ & Auto-Annotation & 5000 & \textbf{59.73} & \textbf{82.00} & \textbf{35.5} & 0.1237 & 618.25 \\

% \multirow{6}{*}{Qwen2-VL-7B} & Zero-Shot & - & 27.69 \textcolor{green}{(49.01\%)} & 54.61 & 3.5 & - & -  \\ 
% ~ & Human-Annotation & 1000 & 54.31 \textcolor{green}{(0.00\%)} & 76.89 & 25.5 & 10.88 & 10880  \\ 
% ~ & Auto-Annotation & 1000 & 52.48 \textcolor{green}{(3.37\%)} & 66.36 & 16.5 & \textbf{0.0099} & \textbf{9.87} \textcolor{red}{(0.09\%)} \\ 
% ~ & Auto-Annotation-Step & 1000 & 52.94 \textcolor{green}{(2.52\%)} & 70.52 & 23.0 & 0.0310 & 31.01 \textcolor{red}{(0.29\%)}  \\ 
% ~ & Auto-Annotation & 5000 & 55.91 \textcolor{red}{(2.95\%)} & 67.58 & 20.5 & \textbf{0.0099} & 49.33 \textcolor{red}{(0.45\%)}  \\ 
% ~ & Auto-Annotation-Step & 5000 & \textbf{56.06} \textcolor{red}{(3.22\%)} & \textbf{81.92} & \textbf{33.5} & 0.0310 & 155.06 \textcolor{red}{(1.43\%)} \\ 
% \midrule
        
% \multirow{6}{*}{InternVL2-2B} & Zero-Shot & - & 11.14 \textcolor{green}{(72.49\%)} & 21.82 & 0.0 & - & -  \\ 
% ~ & Human-Annotation & 1000 & 40.50 \textcolor{green}{(0.00\%)} & 71.06 & 24.0 & 10.88 & 10880  \\ 
% ~ & Auto-Annotation & 1000 & 37.60 \textcolor{green}{(7.16\%)} & 47.29 & 4.5 & \textbf{0.0072} & \textbf{7.23} \textcolor{red}{(0.07\%)}  \\ 
% ~ & Auto-Annotation-Step & 1000 & 35.32 \textcolor{green}{(12.79\%)} & 64.99 & 10.5 & 0.0269 & 26.94 \textcolor{red}{(0.25\%)}  \\ 
% ~ & Auto-Annotation & 5000 & \textbf{47.06} \textcolor{red}{(16.20\%)} & 54.16 & 5.0 & \textbf{0.0072} & 36.13 \textcolor{red}{(0.33\%)}  \\ 
% ~ & Auto-Annotation-Step & 5000 & 45.46 \textcolor{red}{(12.25\%)} & \textbf{74.68} & 22.0 & 0.0269 & 134.71 \textcolor{red}{(1.24\%)} \\ 
% \midrule

% \multirow{6}{*}{InternVL2-8B} & Zero-Shot & - & 29.52 \textcolor{green}{(40.83\%)} & 31.35 & 1.0 & - & -  \\ 
% ~ & Human-Annotation & 1000 & 49.89 \textcolor{green}{(0.00\%)} & 79.86 & 29.0 & 10.88 & 10880  \\ 
% ~ & Auto-Annotation & 1000 & 45.46 \textcolor{green}{(8.88\%)} & 57.97 & 6.5 & \textbf{0.0232} & \textbf{23.18} \textcolor{red}{(0.21\%)}  \\ 
% ~ & Auto-Annotation-Step & 1000 & 49.66 \textcolor{green}{(0.46\%)} & 65.45 & 17.5 & 0.1237 & 123.65 \textcolor{red}{(1.14\%)}  \\ 
% ~ & Auto-Annotation & 5000 & 51.64 \textcolor{red}{(3.51\%)} & 55.42 & 11.5 & \textbf{0.0232} & 115.91 \textcolor{red}{(1.07\%)}  \\ 
% ~ & Auto-Annotation-Step & 5000 & \textbf{59.73} \textcolor{red}{(19.72\%)} & \textbf{82.00} & \textbf{35.5} & 0.1237 & 618.25 \textcolor{red}{(5.68\%)} \\
\bottomrule
\end{tabular}
\caption{Evaluation of Auto-Annotation on Android Control. We compare performance and cost across different models. Auto-Annotation achieves comparable or superior results to the model trained on human-annotated data with significant cost savings. 
Anno. Cost denotes annotation cost in terms of cents \textcent. Epi. is short for Episode. -S denotes a simplified version which removes the step-wise description.}
\label{tab:android_control}
\vspace{-4mm}
\end{table*}

\begin{table*}[t]
\vspace{-3mm}

\centering
% \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{6pt}
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Methodology} & \textbf{Size} & \textbf{General} & \textbf{Install} & \textbf{GoogleApps} & \textbf{Single} & \textbf{WebShopping} & \textbf{Overall} \\ \midrule
Zero-Shot & - & 15.90 & 5.20 & 15.08 & 28.38 & 11.41 & 15.19  \\ 
\midrule
Human-Annotation & 1000 & 35.04 & 54.50 & 46.65 & 55.46 & 39.82 & 46.29  \\ 
\midrule
Auto-Annotation-S & 1000 & 36.24 & 52.47 & 44.13 & 53.41 & 40.34 & 45.32  \\ 
Auto-Annotation & 1000 & 36.92 & 53.23 & 37.43 & 52.84 & 39.65 & 44.01  \\
Auto-Annotation-S & 5000 & 36.24 & \textbf{59.19} & 47.21 & 62.45 & 39.43 & 48.90  \\ 
Auto-Annotation & 5000 & \textbf{37.26} & 57.29 & \textbf{47.49} & \textbf{72.05} & \textbf{45.14} & \textbf{51.85} \\ \bottomrule
\end{tabular}
\caption{Evaluation of Auto-Annotation across different subsets of AitW dataset. Our methods achieve consistent superior performance compared to Human-Annotation method at a very low cost. -S denotes a simplified version which removes the step-wise description.}
\label{tab:aitw}
\vspace{-4mm}
\end{table*}
% \vspace{-mm}



\vspace{-2mm}
\section{Experiments}
\label{exp:}
% We conduct extensive experiments to evaluate both the automatic data construction method and the distributed training approach.
We conduct extensive experiments to evaluate FedMobileAgent and our results demonstrate its effectiveness.
More experimental results are attached in Appendix \ref{app:additional_experiments}.


\vspace{-2mm}
\subsection{Basic Setups}
\label{exp:basic_setup}

\paragraph{Models and Datasets.}  
The base model for most of our experiments is Qwen2-VL-Instruct-7B \cite{Qwen2VL} (referred to as Qwen2-VL-7B for simplicity in our tables). We also compare results with models from the InternVL2 family \cite{chen2024internvl} and API-based models, such as GPT-4o, as discussed in Section \ref{exp:android_control} and Appendix \ref{exp:ablation_model}.

As user data is both costly to collect and sensitive to reveal, we do not collect actual user data by ourselves. Instead, we utilize two popular datasets from Google \footnote{\href{https://github.com/google-research/google-research}{https://github.com/google-research/google-research}}: Android Control \cite{liEffectsDataScale2024} and Android in the Wild (AitW) \cite{rawlesAndroidWildLargeScale2023}. These datasets are collected by crowdsourcing and serve well as a simulation of real-world mobile data.
% We randomly sample $7,000$ episodes from AndroidControl and $1,000$ episodes from each subset of AitW to construct our training dataset pool.

\vspace{-2mm}
\paragraph{Training Framework.}  
We build upon the highly-starred VLM training framework, ms-swift \cite{zhao2024swiftascalablelightweightinfrastructure} \footnote{\href{https://github.com/modelscope/ms-swift}{https://github.com/modelscope/ms-swift}}, and extend it into a repository capable of training federated VLMs. Our extension follows the implementation of current federated training framework for Large Language Models (LLMs) \cite{ye2024openfedllm}. We apply Low-Rank Adaptation (LoRA) \cite{hu2021lora} to improve efficiency.

\vspace{-2mm}
\paragraph{Baselines.}  
We define two baselines: Zero-Shot, as the lower bound, and Human-Annotation, as the upper bound. Our methods are referred to as Auto-Annotation. Zero-Shot directly inferences the base model without training. Human-Annotation uses the original instructions from the datasets, while our methods generate new high-level and low-level instructions based on the given screenshots and actions.
For training algorithms, we consider three commonly used baselines in FL research: centralized training, FedAvg \cite{fedavg}, and local training.  
Standard FedAvg computes aggregation weights based on step counts, while Epi-Aggregation relies solely on episode counts.

% For the evaluation of Local, we only show the performance of the first local client.  
\vspace{-2mm}
\paragraph{Metrics.}
To evaluate the performance of trained mobile agents, following \citet{liEffectsDataScale2024}, we provide both step-wise and episode-wise accuracy (\%) for high-level and low-level training. Step-wise accuracy is calculated by comparing the predicted action for current step with the ground truth action. If the TF-IDF similarity between the predicted and the ground truth action exceeds 0.9, we consider the model's prediction to be accurate. When all steps in an episode are predicted correctly, this episode is considered correct, thereby yielding the episode-wise accuracy. 
% Due to column limitations and relatively small differences, we do not display high-level episode accuracy in our tables. 
% The accuracy numbers are shown as percent.

% We also provide a comparison of annotation costs between our methods and the baselines. In our experiments, we only utilize RTX 4090, which costs estimately 0.2857 \$ per hour. 
We also compare the annotation costs between our methods and the baselines. 
The cost of a single human-annotated sample is derived from a Refuel-AI technical report \footnote{\href{https://www.refuel.ai/blog-posts/llm-labeling-technical-report}{https://www.refuel.ai/blog-posts/llm-labeling-technical-report}}.
% our method's cost
We use multiple RTX 4090 GPUs, each costing approximately \$0.2857 per hour.
The costs for our methods are estimated by calculating the average GPU usage during generation as:
% The cost for one data sample is given by the following formula:
\vspace{-2mm}
\begin{equation}
\label{equ:cost}
    \text{Anno. Cost} = \left(\frac{\text{Price}}{3600}\right) \times \text{Time} \times \frac{\text{Memory}_{\text{Use}}}{\text{Memory}_{\text{Total}}} \, ,
\end{equation}
% \vspace{-1mm}
where \(\text{Price}\) is the GPU rent per hour. \(\text{Memory}_{\text{Use}}\) and \(\text{Memory}_{\text{Total}}\) represent the average occupied GPU memory and the total memory of the system, respectively. Time is the generation duration measured in seconds.
All cost numbers in our tables are presented in terms of cents (\textcent). 


% The factor of 1.5 accounts for less than 100\% GPU utilization.





\vspace{-2mm}
\subsection{FedMobileAgent in IID Settings}
\label{exp:fed}
\paragraph{Setups.}
All federated experiments are conducted on our generated data. Since the data produced by the step-wise method has superior quality, we use it as the representative of data constructed from real-world user phone usage.
During federated training, we randomly select \(30\%\) of clients each round to simulate real-world scenarios where users are occasionally offline \cite{dropout}. Models are evaluated at the same round, meaning the federated models undergo fewer iterations than the centralized model.
The cost per client is the fraction of the total cost by clients number. Since the scale of data for a single user is small, we assume each participating client has \(50\) episodes. Therefore, the cost per client remains fixed at a very low level. As more clients are incorporated, the total data volume increases.


\vspace{-2mm}
\paragraph{Results.}
As shown in Table \ref{tab:fed}, we can conclude that:
(1) With more clients participating and the data size increasing, the performance of the global model collaboratively trained grows correspondingly.
(2) Once the number of clients surpasses a certain threshold, users can obtain a highly capable mobile agent, comparable to or even surpassing centralized training with human annotation, while the per-client cost remains nearly zero.
(3) We also infer that data generated by Auto-Annotation is comparable in quality to Human-Annotation.
(4) The results validate our assumption in Section \ref{met:fed} and demonstrate the effectiveness of FedMobileAgent on distributed user data.


% \vspace{-2mm}
\subsection{FedMobileAgent in Non-IID Settings}
\label{exp:non-iid}
\paragraph{Setups.}
We conduct further experiments in non-IID settings to specifically investigate the issue formulated in Section \ref{pro:fed}. To eliminate the potential influence of Auto-Annotation, we use the original dataset for this section.
We choose AitW over Android Control, as AitW has a higher average step length per episode, making it easier to create non-IID settings based on varying episode distributions. We sample 1,000 episodes from the WebShopping subset with uniformly distributed step lengths and then create four different splits to form distinct scenarios.
Both the Step Skew and IID settings have 100 episodes per client. In the Step Skew scenario, clients share the same number of episodes \( n_k^{epi} \) but have skewed step counts \( n_k \), whereas in the Episode Skew scenario, the opposite holds. The Both Skew scenario involves clients with both skewed \(n_k^{epi}\) and \(n_k\).
% We report the average high-level performance for all settings, including IID, as well as for the three skewed scenarios.
\vspace{-2mm}
\paragraph{Results.}
As shown in Table \ref{tab:fed_skew}, the results demonstrate:
(1) On average, non-IID distributions negatively impact the performance of the trained global model. However, in certain cases, non-IID performs better, such as Step Skew in the table. We attribute this to Step Skew aligning well with traditional FedAvg, which focuses on the number of data samples (i.e., steps).  
(2)  Introduced adapted global aggregation method performs well in non-IID settings, achieving a 5\% relative improvement over FedAvg.
(3) Overall, the results confirm the existence of the heterogeneity highlighted in Section \ref{pro:fed}, posing a new challenge for the federated learning community. We hope our findings will motivate future research in this area.

% We also conduct some preliminary experiments in Section \ref{} to explore more non-iid situations such as clients with different app using habits. 


\vspace{-2mm}
\subsection{Auto-Annotation on Android Control}
\label{exp:android_control}
\paragraph{Setups.}
The validation set includes $200$ episodes randomly sampled from AndroidControl's test split. The training set with Size X includes the first X episodes from the 7,000 training data pool. 
The models are trained with 10 rounds, with each round sampling 1/10 total samples.
We show both the single annotation cost for one data sample and the total cost for the entire training set.
\vspace{-2mm}
\paragraph{Results.}
As shown in Table \ref{tab:android_control}, we conclude the following:
(1) Our methods achieve performance comparable to human annotation when the training data size is equal. Moreover, they outperform human annotation as the data size increases, demonstrating the utility of our generated data on mobile devices and its potential for real-world applications.
(2)  Our methods incur only a small fraction of the cost of human annotation. By leveraging the simplified approach, we achieve up to a \(99.9\%\) cost reduction with less than a \(2\%\) drop in high-level accuracy. Even as data size increases, the cost remains negligible compared to human labor.
% This represents a significant advantage of our approach.

% (3) Combining the above conclusions, we assert that, if applied in the daily use of mobile users, our methods can generate substantial amounts of usable data at minimal cost, effectively supporting the training of mobile agents.

\vspace{-2mm}
\subsection{Auto-Annotation on AitW Dataset}
\label{exp:aitw}
\paragraph{Setups.}
The AitW dataset consists of five subsets: General, Install, GoogleApps, Single, and WebShopping. For each subset of AitW, we sample 1,000 episodes for training and 100 for evaluation. The overall performance is the average of the five subsets.
For the validation metric, we omit validation samples that consist of click actions with no corresponding unit. These samples are too easy to predict in our setting and show no meaningful difference between different models.
We only present high-level accuracy due to the absence of low-level instructions in the original dataset.

\vspace{-2mm}
\paragraph{Results.}
As shown in Table \ref{tab:aitw}, we can conclude that:
(1) Apart from Android Control, our methods still achieve comparable results with Human-Annotation and outperform it by a large margin as the data size increases.
(2) Our method performs extremely well on the "Single" subset. We believe this is due to the short average step length for episodes in Single, which leads to more accurate reconstruction of the high-level instructions.

\vspace{-2mm}
\section{Related Work}
\vspace{-1mm}
\subsection{Development of Current Mobile Agents}
The advent of VLMs \cite{vlmsurver} has marked a significant shift in phone automation, enabling more dynamic, context-aware, and sophisticated interactions with mobile devices \cite{mobilesurvey}. Research on mobile agents has progressed through key milestones, with models becoming more proficient at interpreting multi-modal data, understanding user intent, and autonomously executing complex tasks. 
VLM-based mobile agents typically follow two approaches: (1) Prompt Engineering \cite{mobilegpt, lu2024omniparserpurevisionbased}, where pre-trained models are guided by carefully designed prompts, and (2) Training-Based Methods \cite{hongCogAgentVisualLanguage2023, chengSeeClickHarnessingGUI2024}, where VLMs are further optimized using large-scale mobile datasets. 
While training-based methods offer higher potential and generalizability by improving the VLM through fine-tuning, they require a large amount of training data, which can be very costly.

\vspace{-2mm}
\subsection{Efforts in Building Datasets for Mobile Agents}
% The trajectory data used to train mobile agents consists of screenshots (GUI information), corresponding actions, and instructions expressed in natural language. 
Acquiring training trajectories for mobile agents presents significant challenges. 
Existing approaches are often reliant on manual curation, making data collection both costly and inefficient. Some works have explored the possibility of automatically constructing datasets using VLMs or Application Programming Interfaces (APIs) \cite{wangScreen2WordsAutomaticMobile2021, lai2024autowebglm}. 
But these approaches either halfway to completing the datasets or depend on pre-defined tasks.

OS-Genesis \cite{sunOSGenesisAutomatingGUI2024}, the most advanced in this area, proposes reverse task synthesis to eliminate the need for pre-defined instructions. However, this method still requires an agent to execute synthetic tasks in a simulated mobile environment, to obtain the corresponding screenshots and actions. 
This process does not guarantee the accuracy of executed actions, while also incurs additional computational and resource costs.
In contrast, we propose collecting real-world data from mobile users. This approach offers both (1) unlimited data scale, given the billions of mobile users worldwide, and (2) ground truth accuracy, as the data is directly generated through human execution.

% None of them propose to collect data from real-world mobile phone users.



% \section{Discussions and Future Directions}


% \subsection{Potential Future Directions}
\vspace{-2mm}
\section{Conclusion}
\label{sec:conclusion}
To overcome the scalability and efficiency limitations of traditional data collection methods, we emphasize the necessity of transitioning from human to automatic annotation and from centralized to distributed data collection.  

We propose FedMobileAgent, a framework that collaboratively trains mobile agents using self-sourced data from diverse users.  
Specifically, we introduce Auto-Annotation, an efficient approach for generating high-quality datasets from routine phone usage at minimal cost. 
Additionally, we present a federated learning framework with adapted global aggregation to handle new data heterogeneity.
Extensive experiments validate FedMobileAgent's effectiveness.
% , demonstrating its potential for real-world applications.
% Experiments show that Auto-Annotation achieves up to 11\% performance improvement at less than 1\% of the cost of human annotation. FedMobileAgent further enables collaborative training across decentralized data, achieving comparable or superior performance to centralized methods with minimal cost. 
The results highlight the scalability and practicality of our approach, offering a privacy-preserving and cost-efficient solution for training large-scale mobile agent.



% \section*{Impact Statement}
% This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\bibliography{agent}
% \bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn





\section{Additional Experiments}
\label{app:additional_experiments}

\subsection{Performance \& Cost Trade-Off Comparison.}
We provide a comparison of performance and cost trade-off between FedMobileAgent and our baselines.
As shown in Figure \ref{fig:trade_off}, our approach achieves the best trade-off between performance and cost, outperforming traditional centralized data collection which relies on human annotation in terms of both efficiency and effectiveness. This demonstrates its significant potential for real-world applications, where resource constraints and scalability are key considerations.

\begin{figure}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.5\linewidth]{trade_off.png}
    \vspace{-3mm}
    \caption{Performance and cost comparison between FedMobileAgent and baselines. Our approach achieves the optimal balance. }
    \label{fig:trade_off}
    % \vspace{-2mm}
\end{figure}
\subsection{Quality \& Diversity of Constructed Data}
\label{exp:data_quality_diversity}

\begin{table}[t]
\centering
% \vspace{-2mm}

% \vskip 0.1in
\begin{tabular}{llccccc}
\toprule
\multirow{2}{*}{\textbf{Base Model}} & \multirow{2}{*}{\textbf{Methodology}} & \multicolumn{4}{c}{\textbf{Similarity / Quality}} & \multirow{2}{*}{\textbf{Diversity}} \\ 
~ & ~ & TF-IDF & Rouge & Bleu & Avg. & ~ \\ 
\midrule
- & Human-Annotation & 100.00 & 100.00 & 100.00 & 100.00 & 13.44 \\ 
\midrule
GPT-4o & Auto-Annotation & 29.81 & 38.87 & 11.72 & 26.8 & 23.82 \\ 
GPT-4o-Mini & Auto-Annotation & 32.41 & 39.27 & 12.38 & 28.02 & 24.42 \\ 
\midrule
\multirow{2}{*}{Qwen2-VL-7B} & Auto-Annotation-S & 28.24 & 28.14 & 10.44 & 22.27 & 12.78 \\ 
~ & Auto-Annotation & 26.90 & 37.51 & 10.60 & 25.00 & 16.55 \\
\midrule
\multirow{2}{*}{InternVL2-2B} & Auto-Annotation-S & 28.73 & 33.51 & 9.29 & 23.84 & 25.91 \\ 
~ & Auto-Annotation & 32.17 & 42.61 & 7.90 & 27.56 & 28.77 \\ 
\midrule
\multirow{2}{*}{InternVL2-8B} & Auto-Annotation-S & 23.74 & 28.60 & 9.56 & 20.63 & 16.85 \\ 
~ & Auto-Annotation & 30.98 & 37.46 & 11.47 & 26.64 & 22.40 \\ 
\bottomrule
\end{tabular}

\caption{Similarity comparison and diversity comparison across different base models. The results indicate that Auto-Annotation data is comparable to Human-Annotation data with an up to 40\% similarity. Additionally, the results indicate that model-generated data exhibits greater diversity.}
\vspace{-2mm}
\label{tab:similarity_diversity}
\end{table}

\paragraph{Setups.}

We measure data quality by calculating the similarity between our generated instructions and the ground-truth instructions from the original datasets. 
To evaluate similarity, we adopt three metrics: TF-IDF, BLEU score, and ROUGE. 
BLEU (Bilingual Evaluation Understudy) is a precision-based metric that evaluates text similarity by comparing n-grams between generated and reference texts \cite{papineni2002bleu}.
ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a recall-based metric which computes overlapping n-grams, word sequences, and longest common subsequences \cite{lin2004rouge}.
TF-IDF (Term Frequency-Inverse Document Frequency) is a statistical measure that evaluates word importance in a document relative to a corpus by balancing term frequency and inverse document frequency \cite{salton1988term}.

For diversity, following \citet{sunOSGenesisAutomatingGUI2024}, we use Sentence-BERT to embed each  use sentence-transformers\footnote{\href{https://huggingface.co/sentence-transformers}{https://huggingface.co/sentence-transformers}} library \cite{reimers-2019-sentence-bert} to embed instruction in both datasets. and compute the average cosine distance among the embeddings. 


\vspace{-2mm}
\paragraph{Results.}
As shown in Table \ref{tab:similarity_diversity}, the step-wise Auto-Annotation achieves higher quality compared to the simplified version, indicating superior performance.
Regarding diversity, our methods consistently exhibit greater diversity, even surpassing the original data. 
This is expected, as VLMs generate text in a manner distinct from human writing, often displaying a broader range of expressions and phrase organizations for the same concept.  
Additionally, for InternVL2-2B, the notably higher diversity suggests that the generated instructions may exhibit lower quality, as generating entirely random sentences can also result in high diversity.






\begin{table*}[t]
\centering
\vspace{-2mm}

\setlength\tabcolsep{6.5pt}
\begin{tabular}{lccccccc}
\toprule
\textbf{Methodology} & \textbf{Size} & \textbf{IID} & \textbf{App-Unseen} & \textbf{Task-Unseen} & \textbf{Category-Unseen} & \textbf{Overall}  \\ 
\midrule
Zero-Shot & - & 25.69 & 23.98 & 25.20 & 20.00 & 23.72 \\
\midrule
Human-Annotation & 1000 & 52.97 & 43.72 & 46.95 & 44.62 & 47.07 \\ 
\midrule
\multirow{3}{*}{\makecell[{{p{3cm}}}]{\raggedright Auto-Annotation}} & 1000 & 49.41 & 44.72 & 45.36 & 43.59 & 45.77  \\ 
~ & 3000 & 54.39 & \textbf{53.27} & 49.60 & 47.95 & 51.30  \\ 
~ & 5000 & \textbf{54.73} & 52.26 & \textbf{51.19} & \textbf{49.23} & \textbf{51.85}  \\ \bottomrule
\end{tabular}
\caption{Generalization analysis on Android Control. We compare Auto-Annotation with baselines on four evaluation subsets. Our method achieve consistent improvement over Human-Annotation with minimal annotation cost.}
\label{tab:generalization}
\end{table*}

\subsection{Generalization Analysis}
\paragraph{Setups.}
Based on the sub-splits of AndroidControl dataset. We randomly sample $60$ episodes from the four test splits, i.e. IID, App-Unseen, Task-Unseen and Category-Unseen. $60$ is to match the number of $200$ in Section \ref{exp:android_control} when combined with either the three unseen sub-splits or with the all four splits.

\vspace{-2mm}
\paragraph{Results.}

As shown in Table \ref{tab:generalization}, mobile agents trained on our automatically generated data exhibit strong generalizability across different situations.  
The results demonstrate the effectiveness of our method and further validate the utility of our auto-annotated data, which is derived solely from screenshots and actions.  
Additionally, we observed that the Category-Unseen subset has relatively lower accuracy compared to other evaluation subsets, indicating a higher level of difficulty.


\begin{figure}[t]
    \centering
    % \setlength{\belowcaptionskip}{-3cm}
    \includegraphics[width=0.6\columnwidth]
    {scaling_law.png}
    \vspace{-2mm}
    \caption{Scaling law analysis on Android Control dataset with different training strategies. All models show a growing tendency with increased data size.
    FedAvg with * denotes 60\% user participation while the other is 30\%.}
    
    \label{fig:scaling_law}
    \vspace{-2mm}
\end{figure}



\begin{table*}[t]
\centering
\vspace{-2mm}
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Base Model}} & \textbf{High level} & \multicolumn{2}{c}{\textbf{Cost}}  \\ 
~ & Step Acc & Anno. & Time  \\ 
\midrule
GPT-4o-Mini & 52.78 & 14.8 & 5061  \\ 
GPT-4o & 52.94 & 247.92 & 6858  \\ 
GPT-4-Vision & - & 2,841.43 & -  \\ 
Qwen2-VL-2B & 44.09 & 6.14 & 1180  \\ 
Qwen2-VL-7B & 52.48 & 16.77 & 1374  \\ 
InternVL2-1B & 34.94 & 2.58 & 2000  \\ 
InternVL2-2B & 37.60 & 7.23 & 1038  \\ 
InternVL2-8B & 45.46 & 23.18 & 2245 \\ 
\bottomrule
%     \end{tabular}
% \end{table}

% \begin{table*}[t]
% \renewcommand{\arraystretch}{1.1}
% \setlength\tabcolsep{6pt}
% \centering
% % \small
% \begin{tabular}{llcccccc}
% \toprule
% \multirow{2}{*}{\textbf{Model}} & \multirow{2}{*}{\textbf{Methodology}} & \multirow{2}{*}{\textbf{Size}} & \textbf{High Level} & \multicolumn{2}{c}{\textbf{Low Level}} & \multicolumn{2}{c}{\textbf{Anno. Cost}}\\ 
% % \cmidrule(l){4-8} 
% &  &  & Step Acc & Step Acc & Epi Acc & Single & Total \\ \midrule

\end{tabular}
\caption{Comparison of the performance and cost trade-off between various base models.}
% \vskip 0.1in
\label{tab:ablation_model}
\end{table*}

\begin{table}[t]
\centering
% \vspace{-2mm}

\begin{tabular}{lccc}
\toprule
\multirow{3}{*}{\textbf{Base Model}} & \multicolumn{3}{c}{\textbf{Anno. Cost across Backends}}    \\ 
~ & PyTorch & \makecell{vLLM or\\ LMDeploy} & API  \\ 
\midrule
Human & \multicolumn{3}{c}{10880}  \\ 
\midrule
GPT-4o-Mini & - & - & 14.8  \\ 
GPT-4o & - & - & 247.92  \\ 
GPT-4-Vision & - & - & 2841.43  \\ 
Qwen2-VL-2B & 6.14 & 8.42 & \textless21.15  \\ 
Qwen2-VL-7B & 16.77 & 9.87 & \textless21.15  \\ 
InternVL2-1B & 2.58 & 11.09 & Free  \\ 
InternVL2-2B & 16.04 & 7.23 & Free  \\ 
InternVL2-8B & 23.18 & - & Free \\
\bottomrule
\end{tabular}
\caption{Comparison of annotation costs using different inference backends across various base models. Our results indicate that employing efficient backends, such as vLLM and LMDeploy, can further reduce inference time and memory usage, ultimately lowering the annotation cost of our approach.}
\label{tab:backends}
\vspace{-2mm}
\end{table}


\begin{table}[t]
\centering
\vspace{-2mm}

% \vskip 0.1in
\setlength\tabcolsep{6pt}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Methodology}} & \multirow{2}{*}{\textbf{Size}} & \multirow{2}{*}{\makecell{\textbf{w} or\\ \textbf{w/o}}} & \multicolumn{2}{c}{\textbf{Low Level}} &\multicolumn{2}{c}{\textbf{Anno. Cost}} \\ 
~ & ~ & ~ & Step Acc & Epi. Acc & Single & Total \\ 
\midrule
Human-Annotation & 1000 & \makebox[0.09\textwidth][c]{w} & 76.89 & 25.5 & 10.88 & 10880 \\ 
\midrule
Human-Annotation & 1000 & w/o & 64.30 & 12.5 & 5.44 & 5440 \\ 
Auto-Annotation-S & 1000 & w/o & 66.36 & 16.5 & \textbf{0.01} & \textbf{9.87} \\ 
Auto-Annotation & 1000 & w & \textbf{69.41} & \textbf{23.0} & 0.03 & 31.01 \\ 
\bottomrule
\end{tabular}
\caption{Evaluating the improvement of our method on public datasets by supplementing generated low-level instructions which are often neglected in previous works such as AitW. If the users already have access to some public training sets, they can further refine trained mobile agents by training on both the public dataset and private dataset using Auto-Annotation. The notation "w" or "w/o" indicates whether the training dataset is attached with or without low-level instructions.
}
\label{tab:supply_low_level}
\end{table}

\begin{table}[t]
% \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{6pt}
\centering

% \small
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Methodology}} & \multirow{2}{*}{\textbf{Size}} & \textbf{High Level} & \multicolumn{2}{c}{\textbf{Low Level}} & \multicolumn{2}{c}{\textbf{Anno. Cost}}\\ 
% \cmidrule(l){4-8} 
&    & Step Acc & Step Acc & Epi Acc & Single & Total \\ \midrule
Human-Annotation & 2500 & \makebox[0.09\textwidth][c]{54.23} & 82.91 & 34.5 & 10.88 & 27200  \\ 
Auto-Annotation & 2500 & 53.24 & 78.95 & 30.0 & \textbf{0.03} & \textbf{77.53}  \\ 
Both & 5000 & \textbf{57.89} & \textbf{85.05} & \textbf{39.0} & 5.45 & 27277 \\ 
\bottomrule

\end{tabular}

\caption{Evaluating the improvement of our method on public datasets by supplementing data quantity. If the users already have access to some public training sets, they can further refine trained mobile agents by training on both the public dataset and private dataset using Auto-Annotation to increase the scalability.}
\label{tab:supply_quantity}
\vspace{-2mm}
\end{table}



\begin{table*}[t]
% \renewcommand{\arraystretch}{1.1}
\setlength\tabcolsep{6pt}
\centering

\begin{tabular}{llccccc}
\toprule
 \multirow{2}{*}{\textbf{Methodology}} & \textbf{High Level} & \multicolumn{2}{c}{\textbf{Low Level}} & \multicolumn{2}{c}{\textbf{Anno. Cost}} & \textbf{Time}\\ 
% \cmidrule(l){4-8} 
& Step Acc & Step Acc & Epi Acc & Single & Total & Single \\ \midrule
\multicolumn{7}{c}{\textbf{Size = 1000}} \\
Zero-Shot & 27.69 & 54.61 & 3.5 & - & - & - \\
Human-Annotation & 54.31 & 76.89 & 25.5 & 10.88 & 10880 & 56.3 \\
Auto-Annotation-S & 52.48 & 66.36 & 16.5 & 0.0099 & 9.87 & 1.37 \\
Auto-Annotation & 52.94 & 70.52 & 23.0 & 0.0310 & 31.01 & 4.36 \\
Auto-Annotation* & 52.78 & 69.41 & 23.5 & 0.0323 & 32.29 & 4.54 \\
\midrule
\multicolumn{7}{c}{\textbf{Size = 5000}} \\
Zero-Shot & 27.69 & 54.61 & 3.5 & - & - & - \\
Human-Annotation & 59.73 & 85.81 & 38.5 & 10.88 & 54440 & 56.3 \\
Auto-Annotation-S & 55.91 & 67.58 & 20.5 & 0.0099 & 49.33 & 1.37 \\
Auto-Annotation & 56.06 & 81.92 & 33.5 & 0.0310 & 155.06 & 4.36 \\
Auto-Annotation* & 57.13 & 82.45 & 35.0 & 0.0323 & 161.45 & 4.54 \\
\bottomrule


\end{tabular}
\caption{Performance and cost comparison of Auto-Annotation at different levels. -S denotes simplified version of Auto-Annotation without step-wise description. * denotes using screenshot concatenation in the summarization step.}
\label{tab:ablation_our}
\end{table*}



\subsection{Scaling Law Analysis}
\label{exp:scaling_law}

\paragraph{Setups.}
We conduct an ablation experiment on training data size to investigate whether the scaling law \cite{kaplan2020scaling} holds for our automatically generated data. Using the Android Control dataset, we train models that differ only in their training data size. For FedMobileAgent, we fix the number of clients at \(10\) and test different sample numbers, specifically 3 and 6. The resulting participation ratios are calculated to be 0.3 and 0.6, respectively.

\vspace{-2mm}
\paragraph{Results.}
As shown in Figure \ref{fig:scaling_law}, the performance of all tested models improves as the training data size increases, indicating that our generated data also follows the scaling law. We also observe a sharp performance increase between training with 100 and 1000 episodes. No saturation is observed in our experiments, but it can be inferred that the performance of all models grows more slowly once the data size reaches a certain threshold.


\subsection{Improvement on Existing Dataset}
\label{exp:improve_on}
\paragraph{Setups.}

In this section, we consider a scenario where a mobile user already has access to some public datasets for training mobile agents. We aim to determine whether, under such circumstances, the user can still benefit from our method. 

First, we assert that the composition of local datasets is orthogonal to the federated learning framework FedMobileAgent. In other words, regardless of the data on which the local model is trained, the user can still participate in collaboration.  

Second, we anticipate that using Auto-Annotation can enhance training on existing datasets from two aspects:  
(1) Auto-Annotation can further increase the quantity of training data, and  
(2) it can address the absence of low-level instructions in previous datasets, such as AitW \cite{rawlesAndroidWildLargeScale2023}.

We design corresponding experiments to verify our expectations.
We empirically estimate the human annotation cost without low-level instructions to be half of the total annotation cost.  
For training on human-annotated data without low-level instructions, we use the same prompt as shown in Figure \ref{fig:prompt_5}, but set the subordinate instruction to null to ensure a fair comparison.  
To evaluate our method, we retain the original high-level instructions from human annotations while replacing the low-level instructions with our generated ones to simulate the process of filling in missing information in already collected data.

\paragraph{Results.}

As shown in Table \ref{tab:supply_quantity}, utilizing additional data through Auto-Annotation significantly enhances model performance. This effect is particularly notable when the dataset size is small, i.e. 1,000, as the increased data quantity can substantially improve performance despite a slight drop in quality compared to human-annotated data.  

In the same vein, as shown in Table \ref{tab:supply_low_level}, models trained without low-level instructions experience considerable performance degradation when evaluated on low-level tasks.  
Although our method does not reach the upper bound of fully human-annotated data, it notably outperforms training without low-level instructions.


\subsection{Ablation on Base Models}
\label{exp:ablation_model}
\paragraph{Setups.}

We conduct ablation experiments to investigate the performance, annotation cost, and time requirements of different base models.  
Especially, we conduct experiments leveraging popular API-based models, including GPT-4 and GPT-4-mini \cite{gpt4}.
We exclude API-based models from our main experiments in Section \ref{exp:android_control} due to the additional privacy risks posed by API usage. Using APIs involves transmitting user screenshots directly to the API server, which raises concerns about privacy leakage and data abuse.

Among different inference backends, we select the one with the lowest cost. A more detailed comparison of the backends is provided in Section \ref{exp:backends}.  


\vspace{-2mm}
\paragraph{Results.}

As shown in Table \ref{tab:ablation_model}, different models present varying trade-offs between performance and cost.  
We conclude the following observations:  
(1) API-based models incur significantly higher annotation and time costs compared to locally deployed models.  
(2) Within a given VLM family, an increase in the number of parameters generally correlates with higher high-level performance and greater computational demand.  
(3) Models with similar parameter counts tend to have comparable GPU usage, leading to similar annotation costs.  

\subsection{Ablation on Our Methods}
\label{exp:ablation_ours}
\paragraph{Setups.}
We conduct an ablation to investigate the differences between our proposed Auto-Annotation methods. Methods denoted with * are different in that the last summarization step they merely rely on 
The asterisk (*) indicates that the final step of summarization does require image input.
\paragraph{Results.}
As shown in Table \ref{tab:ablation_our}, we can conclude that:  
(1) Simplifying Auto-Annotation weakens performance, particularly in low-level training, as it cannot generate low-level instructions. However, this simplification cuts annotation costs by approximately 60\%, which may be suitable for cases where efficiency is prioritized.  
(2) Comparing Auto-Annotation and Auto-Annotation*, enabling global visual language does not necessarily improve performance. This is because the textual information provided to the Summarizer is already sufficient.


\begin{figure}[t]
    \centering
    \vspace{-2mm}
    \includegraphics[width=0.5\linewidth]{actions.png}
    \vspace{-2mm}
    \caption{Accuracy across action types in the action space of Android Control.}
    \label{fig:action_type}
    \vspace{-2mm}

\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{case1.png}
    \caption{Episode example from Android Control dataset. The high-level task is \textit{Open the Zoho Meet app and view the scheduled meetings}. Instructions in grey indicate ground truth from the original dataset, while those in green are predictions generated by Auto-Annotation. Our generated data sample achieves quality comparable to human-annotated ground truth.
}
	\label{fig:case_study}
    \vspace{-3mm}
\end{figure}

\begin{table}[t]


% \vskip 0.1in
\centering
\begin{tabular}{lll}
\toprule
\textbf{Action Type} & \textbf{Target} & \textbf{Description}  \\ 
\midrule
click & text or (x,y) & Click at a specific point on the screen using the coordinates. \\ 
long press & text or (x,y) & Long press at a specific point on the screen using the coordinates. \\ 
scroll & direction & Scroll in a specific direction (one of 'up', 'down', 'left', or 'right'). \\ 
input\_text & text & Type the text in the current input field or search bar. \\ 
navigate\_home & - & Return to the home page. \\ 
navigate\_back & - & Return to the previous page or undo an action. \\ 
open\_app & app\_name & Open an app with the name. \\ 
wait & - & Pause for a moment before proceeding with the next action. \\ 
check\_status & status & Check whether the task is 'successful' or 'infeasible'. \\
\bottomrule
\end{tabular}
\caption{Action space for Android Control dataset.}
\label{tab:action_space_ac}
\end{table}

\begin{table}[t]
\centering

\begin{tabular}{lll}
\toprule
\textbf{Action Type} & \textbf{Target} & \textbf{Description}  \\ 
\midrule
click & text or (x,y) & Click at a specific point on the screen using the coordinates. \\ 
scroll & direction & Scroll in a specific direction (one of 'up', 'down', 'left', or 'right'). \\ 
input\_text & text & Type the text in the current input field or search bar. \\ 
navigate\_home & - & Return to the home page. \\ 
navigate\_back & - & Return to the previous page or undo an action. \\ 
press\_enter & - & Press on the button 'Enter'. \\ 
check\_status & status & Check whether the task is 'successful' or 'infeasible'. \\
\bottomrule
\end{tabular}
\caption{Action space for Android in the Wild dataset.}
\label{tab:action_space_aitw}
\vspace{-2mm}
\end{table}

% \subsection{Ablation on Client Numbers}
% \label{exp:ablation_client}
% \paragraph{Setups.}

% \paragraph{Results.}

\subsection{Comparison of Annotation Cost across Backends}
\label{exp:backends}
\paragraph{Setups.}
To further investigate whether the annotation cost using our method can be reduced and whether the memory requirements can be minimized with current efficient inference backends, such as vLLM \cite{vllm} and LMDeploy \cite{2023lmdeploy}, we conduct additional experiments to evaluate model costs on different backends.  
API-based costs are assessed using the OpenAI's library tiktoken \footnote{\href{https://github.com/openai/tiktoken}{https://github.com/openai/tiktoken}} to count input and output tokens via Auto-Annotation. The price per million tokens is also included.  
Moreover, we approximate the API cost for the Qwen2-VL family by using the pricing of Qwen-VL-Plus, as the server does not provide APIs for Qwen2-VL-7B or Qwen2-VL-2B. For the InternVL2 family, since the model server offers free trial access, we denote the cost as 'Free'.


\vspace{-2mm}
\paragraph{Results.}

As shown in Table \ref{tab:backends}, models exhibit explicitly different behaviors across backends.  
In general, most models reduce costs when using efficient backends. For example, InternVL2-2B saves annotation costs by more than half when leveraging LMDeploy.  
However, for smaller models, using an efficient backend does not necessarily lead to improvements. We attribute this to the fact that running vLLM on an RTX 4090 causes the model to occupy the entire GPU memory, which is 5 to 10 times the original memory usage of PyTorch. This increase in memory consumption does bring out improvement inference speed but fails to offset the additional memory demand. Since our annotation cost, as formulated in Equation \ref{equ:cost}, considers both time and memory usage, the overall cost does not necessarily decrease.  

Additionally, APIs remain a viable option since they eliminate the need for local deployment, while offering highly competitive pricing. However, using APIs comes at the sacrifice of privacy.

\subsection{Accuracy Comparison across Different Actions}
\paragraph{Setups.}
In accordance with the metric elaborated in Section \ref{exp:basic_setup}, we compute the accuracy for each action type in the Android Control action space, which is detailed in Table \ref{tab:action_space_ac}.


\vspace{-2mm}
\paragraph{Results.}
As shown in Figure \ref{fig:action_type}, the accuracies exhibit considerable variation across different action types. 

Notably, the accuracy is particularly high for \texttt{input\_text}. We attribute this to the fact that this action primarily relies on language comprehension rather than visual grounding. Currently, VLMs are better equipped to handle language tasks. 

In contrast, the accuracy for \texttt{navigate\_home} and \texttt{navigate\_back} is extremely low. We hypothesize that this is due to a scarcity of training samples, as these action types constitute only a small fraction of the total training data.


\section{Case Study}

To enhance understanding of our method, we present several data examples in this section and conduct a case study to further examine the task formulated in Section \ref{pro:data}.  
As shown in Figure \ref{fig:case_study}, our generated instructions (in green) closely resemble human-written instructions (in grey), demonstrating the effectiveness of the proposed Auto-Annotation.



% \begin{figure}[h]
% 	\centering
% 	\begin{minipage}{0.22\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{cases/00.png}}
% \begin{figure}
% 		    \centering
% 		    \includegraphics[width=1\linewidth]{CaseStudy1.png}
% 		    \caption{Enter Caption}
% 		    \label{fig:enter-label}
% 		\end{figure}
% 				\centerline{Screenshot 1}
% 	\end{minipage}
% 	\begin{minipage}{0.22\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{cases/01.png}}
% 		\centerline{Screenshot 2}
% 	\end{minipage}
% 	\begin{minipage}{0.22\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{cases/02.png}}
% 		\centerline{Screenshot 3}
% 	\end{minipage}
%     \begin{minipage}{0.22\linewidth}
% 		\vspace{3pt}
% 		\centerline{\includegraphics[width=\textwidth]{cases/03.png}}
% 		\centerline{Screenshot 4}
% 	\end{minipage}
 
% 	
% \end{figure}


\section{Additional Analysis.}
\subsection{Analysis of Resources on a Mobile Device}
To investigate the minimal resource requirements, we conduct additional experiments to determine whether small VLMs or models based on APIs can achieve similar effectiveness. The results in Section \ref{exp:ablation_model} show that even an 1B VLM can deliver competitive performance. Models based on APIs can also be used, though they come with the risk of privacy leakage, which we leave for future work.
\subsection{Real-World Applicability Analysis}
% \paragraph{Ques1: Why would users risk they privacy?}
% This is the biggest concern regarding the setup of our work. 
We will address the real-world applicability three-folds.
First, as shown in Section \ref{exp:fed}, each user only needs to provide a small amount of data, and not all of it is sensitive, resulting in minimal or no privacy risks.
Second, the benefits far outweigh the costs and risks. We assume that the server incentivizes participation by offering free use of the global agent in exchange for access to user data. Users can gain access to a highly capable mobile agent that saves them both time and efforts.
Finally, by incorporating federated learning, user data is processed locally, alleviating most privacy concerns.
% \vspace{-2mm}
% \paragraph{Ques2: Are the resources on mobile devices sufficient enough to construct data by our method?}


\section{Experimental Details}
\label{app:experimental_details}

\subsection{Action Space}
We define 9 action types for Android Control and 7 for the AitW dataset. The corresponding actions and their descriptions are provided in Tables \ref{tab:action_space_ac} and \ref{tab:action_space_aitw}, with any additional parameters indicated as \textit{Target}.

For click actions, we prioritize the button caption over coordinates when the location is associated with a button, as it is more comprehendible for the VLM to process. In AitW, we decompose the original press action into three distinct actions: \texttt{navigate\_home}, \texttt{navigate\_back}, and \texttt{press\_enter}, aligning it with the action space of Android Control. Additionally, we derive the \texttt{scroll} action from the original dual-point action.






\subsection{Training Parameters}
As shown in \cref{tab:training-params}, we include all key parameters for reproducibility.
For max-sequence-length, we choose 4096 for Qwen2-VL family and 2048 for InternVL2 family.

\begin{table}[t]
 
    \centering
    \small
    % \renewcommand{\arraystretch}{1.2} % 调整行距
    \begin{tabularx}{0.8\linewidth}{p{3.8cm}X | p{3.8cm}X}
        \toprule
        \textbf{Parameter} & \textbf{Value} & \textbf{Parameter} & \textbf{Value} \\
        \midrule
        \multicolumn{4}{l}{\textbf{Federated Learning}} \\
        number-of-rounds & 30 & number-of-clients & 10 \\
        number-of-clients-sampled & 3 & ratio \(\lambda\) & 3,5,7,9 \\
        \midrule
        \multicolumn{4}{l}{\textbf{LoRA Configuration}} \\
        lora-rank & 8 & lora-alpha & 32 \\
        lora-dropout & 0.05 & max-sequence-length & 4096, 2048 \\
        \midrule
        \multicolumn{4}{l}{\textbf{Optimization}} \\
        learning-rate & $5 \times 10^{-5}$ & batch-size & 1 \\
        optimizer & adamw\_torch & gradient-accumulation-steps & 4 \\
        weight-decay & 0.1 & adam-beta1 & 0.9 \\
        adam-beta2 & 0.95 & adam-epsilon & $1\times10^{-8}$ \\
        lr-scheduler & cosine & warmup-ratio & 0.03 \\
        \midrule
        \multicolumn{4}{l}{\textbf{Quantization Settings}} \\
        bnb-4bit-compute-dtype & torch.bfloat16 & bnb-4bit-quant-type & nf4 \\
        bnb-4bit-use-double-quant & true & load-in-4bit & false \\
        load-in-8bit & false & device-number & 2 \\
        \bottomrule
    \end{tabularx}
    \caption{Key training parameters regarding FL, LoRA, and quantization.}

    \label{tab:training-params}
\end{table}


% \begin{table}[h]
% \vspace{-2mm}
%     \caption{Key training parameters regarding FL, LoRA and quantization.}
%     \vskip 0.1in
%     \centering
%     \begin{tabular}{ll}
%         \toprule
%         \textbf{Parameter} & \textbf{Value} \\
%         \midrule
%         \multicolumn{2}{l}{\textbf{Federated Learning}} \\
%         % \midrule
%         number-of-rounds & 30 \\
%         number-of-clients & 10 \\
%         number-of-clients-sampled & 3 \\
%         ratio \(\lambda\) & 3,5,7,9 \\
%         \midrule
%         \multicolumn{2}{l}{\textbf{LoRA Configuration}} \\
%         lora-rank & 8 \\
%         lora-alpha & 32 \\
%         lora-dropout & 0.05 \\
%         \midrule
%         \multicolumn{2}{l}{\textbf{Model Configuration}} \\
%         max-sequence-length & 4096, 2048 \\
%         device-number & 2 \\
%         \midrule
%         \multicolumn{2}{l}{\textbf{Optimization}} \\
%         learning-rate & $5 \times 10^{-5}$ \\
%         batch-size & 1 \\
%         optimizer & adamw\_torch \\
%         gradient-accumulation-steps & 4 \\
%         weight-decay & 0.1 \\
%         adam-beta1 & 0.9 \\
%         adam-beta2 & 0.95 \\
%         adam-epsilon & $1\times10^{-8}$ \\
%         lr-scheduler & cosine \\
%         warmup-ratio & 0.03 \\
%         \midrule
%         \multicolumn{2}{l}{\textbf{Quantization Settings}} \\
%         bnb-4bit-compute-dtype & torch.bfloat16 \\
%         bnb-4bit-quant-type & nf4 \\
%         bnb-4bit-use-double-quant & true \\
%         load-in-4bit & false \\
%         load-in-8bit & false \\
%         \bottomrule
%     \end{tabular}

%     \label{tab:training-params}
% \end{table}

\subsection{Templates}
\label{app:template}
We provide all of our prompt templates used in generating instructions and training in Figures \ref{fig:prompt_1}, \ref{fig:prompt_2}, \ref{fig:prompt_3}, \ref{fig:prompt_4} and \ref{fig:prompt_5}.

\begin{figure}[t]
\vspace{-8mm}
% \setlength{\abovecaptionskip}{0pt}
\begin{response}[Step-Wise Description]
% \scriptsize
A user is performing a \textit{task} on a mobile phone, progressing through multiple steps to complete the task. 
Each step involves an interface shown in the provided screenshot, and a "User Action" performed to move on to the next step. \\
\\
Based on the current screenshot and the user’s action, describe the specific goal the user is trying to achieve in this step of the \textit{task}. \\
\\
\#\# User Action \\
\{ converted action \( A_i\) \} \\
\\
\#\# Answer Format \\
Keep your response concise and capture the important things, focusing on key details like the app name, email address, search terms, item name, and title. \\
\\
\#\# Your Answer\\
The user is trying to: 

\end{response}
\vspace{-2mm}
\caption{Prompt template for the Descriptor to generate low-level instruction \(\mathcal{T}_i^{low}\) based on the converted action \(A_i\) and screenshot \(s_i\) at the \(i\)-th step .}
\label{fig:prompt_1}
\vspace{-2mm}
\end{figure}


\begin{figure}[t]
\vspace{-2mm}
% \setlength{\abovecaptionskip}{0pt}
\begin{response}[Episode-Wise Summarization within Auto-Annotation]
A user is performing a \textit{task} on a mobile phone, progressing through multiple steps to complete the task. 
Each step involves an interface shown in the provided screenshot, and a "User Action" performed to move on to the next step. \\
The "User Action" is provided in the context \textit{Process}. \\
The \textit{task} is not known. Now based on the \textit{Process}, describe the mobile user's \textit{task} when completing these actions. \\
\\
\#\# Process \\
\{ low-level instruction \(\mathcal{T}_1^{low}\) \} \\
\{ low-level instruction \(\mathcal{T}_2^{low}\) \} \\
...\\
\{ low-level instruction \(\mathcal{T}_n^{low}\) \} \\
\\
\#\# Answer Format \\
Keep your answer concise and clear, as if the user were explaining the \textit{task} to someone else in one sentence. \\
Include key details like the app name, individual name, email address, search terms, item name, and title. \\
\\
\#\# Your Answer\\
The user is trying to:

\end{response}
\vspace{-2mm}
\caption{Prompt template for the Summarizer to generate high-level instruction \(\mathcal{T}^{high}\) based on the list of low-level instructions and the concatenated screenshot \(s_{c}\) .}
\label{fig:prompt_2}
\vspace{-2mm}
\end{figure}


\begin{figure}[t]
\vspace{-8mm}
% \setlength{\abovecaptionskip}{0pt}
\begin{response}[Episode-Wise Summarization within Auto-Annotation-S]
A user is performing a \textit{task} on a mobile phone, progressing through multiple steps to complete the task. \\
Each step involves an interface shown in the provided screenshot, and a "User Action" performed to move on to the next step.\\
The "User Action" is provided in the context \textit{Process}.\\
The \textit{task} is not known. Now based on the \textit{Process}, describe the mobile user's \textit{task} when completing these actions.\\
\\
\#\# Process\\
\{ converted action \( A_1\) \}\\
\{ converted action \( A_2\) \}\\
...\\
\{ converted action \( A_n\) \}\\
\\
\#\# Answer Format\\
Keep your answer concise and clear, as if the user were explaining the \textit{task} to someone else in one sentence.\\
Include key details like the app name, individual name, email address, search terms, item name, and title.\\
\\
\#\# Your Answer\\
The user is trying to:

\end{response}
\caption{Prompt template for the Summarizer to generate high-level instruction \(\mathcal{T}^{high}\) based on the list of converted actions and the concatenated screenshot \(s_{c}\) .}
\label{fig:prompt_3}
\end{figure}


\begin{figure}[t]
% \vspace{-8mm}
% \setlength{\abovecaptionskip}{0pt}
\begin{response}[High-Level Training for FedMobileAgent]
You are a smartphone assistant tasked with helping users complete actions by interacting with apps.
I will provide you with one screenshot, representing the UI state before an operation is performed.\\

For the screenshot, you need to identify and output a specific action required to complete the \textbf{User Instruction}.\\

\#\#\# User Instruction \#\#\#\\
\{ high-level instruction \(\mathcal{T}^{high}\) \}\\

\#\#\# Response Requirements \#\#\#\\
For each screenshot, you need to decide just one action on the current screenshot.\\
You must choose one of the actions below:\\

1. \textbf{Click on button with the text "\(\langle\)UI element text\(\rangle\)"}  \\
% If the button has no text related, just output "Click on button".\\
If the button has no text related, output "Click on the location \(\langle\)x,y\(\rangle\)". 

2. \textbf{Long press on button with the text "\(\langle\)UI element text\(\rangle\)"}  \\
If the button has no text related, output "Long press on the location \(\langle\)x,y\(\rangle\)". 

3. \textbf{Type text: "\(\langle\) input text \(\rangle\)"}  \\
Type the \textbf{\(\langle\)input text\(\rangle\)} in the current input field or search bar.

4. \textbf{Scroll \(\langle\)direction\(\rangle\)}  \\
Scroll the UI element by \(\langle\)direction\(\rangle\).\\
If the current UI includes scrollers but lacks the necessary elements for the task, try scrolling down to reveal elements below or scrolling up to uncover elements above.
Similarly, scroll right to reveal elements on the right or scroll left to uncover elements on the left.

5. \textbf{Return to the home page}  \\
Return to the home page. If you want to exit an app, use this action.

6. \textbf{Go back to the previous page} 

Go back to the previous page. If you need to return to the previous step or undo an action, use this action to navigate back.

7. \textbf{Open App: \(\langle\)app name\(\rangle\)}  \\
If you wish to open an app, use this action to open \(\langle\)app name\(\rangle\).

8. \textbf{Wait for response}  \\
Pause for a moment to allow any background processes to complete or for elements to load before proceeding with the next action.

9. \textbf{Check status: \(\langle\)successful/infeasible\(\rangle\)}  \\
If you think all the requirements of the user's instruction have been completed successfully and no further operation is required, you can choose "successful" to terminate the operation process.
If the task cannot be completed due to missing elements or any other issue, you can use "infeasible" to indicate that the action cannot be performed.\\

\#\#\# Your Response \#\#\#

\end{response}
\caption{Prompt template for the high-level training of federated mobile agents within FedMobileAgent.}
\label{fig:prompt_4}
\end{figure}


\begin{figure}[t]
% \vspace{-8mm}
% \setlength{\abovecaptionskip}{0pt}
\begin{response}[Low-Level Training for FedMobileAgent]
You are a smartphone assistant tasked with helping users complete actions by interacting with apps.
I will provide you with one screenshot, representing the UI state before an operation is performed.\\

For the screenshot, you need to identify and output a specific action required to complete the \textbf{User Instruction}.

The subordinate instruction for the current step is also provided as the \textbf{Subordinate Instruction}. 

\#\#\# User Instruction \#\#\#\\
\{ high-level instruction \(\mathcal{T}^{high}\) \}\\

\#\#\# Subordinate Instruction \#\#\#\\
\{ low-level instruction \(\mathcal{T}_i^{low}\) \}\\

\#\#\# Response Requirements \#\#\#\\
For each screenshot, you need to decide just one action on the current screenshot.\\
You must choose one of the actions below:\\

1. \textbf{Click on button with the text "\(\langle\)UI element text\(\rangle\)"}  \\
% If the button has no text related, just output "Click on button".\\
If the button has no text related, output "Click on the location \(\langle\)x,y\(\rangle\)". \\
2. \textbf{Long press on button with the text "\(\langle\)UI element text\(\rangle\)"}  \\
If the button has no text related, output "Long press on the location \(\langle\)x,y\(\rangle\)". 
3. \textbf{Type text: "\(\langle\) input text \(\rangle\)"}  \\
Type the \textbf{\(\langle\)input text\(\rangle\)} in the current input field or search bar.\\
4. \textbf{Scroll \(\langle\)direction\(\rangle\)}  \\
Scroll the UI element by \(\langle\)direction\(\rangle\).\\
If the current UI includes scrollers but lacks the necessary elements for the task, try scrolling down to reveal elements below or scrolling up to uncover elements above.
Similarly, scroll right to reveal elements on the right or scroll left to uncover elements on the left.

5. \textbf{Return to the home page}  \\
Return to the home page. If you want to exit an app, use this action.

6. \textbf{Go back to the previous page} 

Go back to the previous page. If you need to return to the previous step or undo an action, use this action to navigate back.

7. \textbf{Open App: \(\langle\)app name\(\rangle\)}  \\
If you wish to open an app, use this action to open \(\langle\)app name\(\rangle\).

8. \textbf{Wait for response}  \\
Pause for a moment to allow any background processes to complete or for elements to load before proceeding with the next action.

9. \textbf{Check status: \(\langle\)successful/infeasible\(\rangle\)}  \\
If you think all the requirements of the user's instruction have been completed successfully and no further operation is required, you can choose "successful" to terminate the operation process.
If the task cannot be completed due to missing elements or any other issue, you can use "infeasible" to indicate that the action cannot be performed.\\

\#\#\# Your Response \#\#\#

\end{response}
\caption{Prompt template for the low-level training of federated mobile agents within FedMobileAgent.}
\label{fig:prompt_5}
\end{figure}







\end{document}


% This document was modified from the file originally 


