\section{Feature Extraction}
When extracting features using datasets, we employed the following techniques to improve efficiency and accuracy:

\subsection{Extraction Modes}
\label{extraction_mode}
We tested the following three analysis modes during feature extraction on the artificially constructed dataset:


\textbf{Token-Frequency}: The total activation frequency of the basis vector across all tokens in the artificial dataset, sorted in descending order.

\textbf{Sentence-Frequency}: The percentage of sentences in the artificial dataset where the basis vector is activated, sorted in descending order.

\textbf{Sum-Activation}: The total activation value of the basis vector across all tokens in the artificial dataset, sorted in descending order.


We build a background dataset with diverse syntactic and semantic patterns. We extract basis vectors that frequently activate across all patterns; their meanings are irrelevant and treated as \textbf{background noise}. They can be removed in our experiments.

We test the analysis modes on the three linguistic features—simile, metaphor, and politeness—identified at layer 26. And we extract 42 background noise basis vectors at layer 26.

To evaluate the efficiency of the three analysis modes—\textit{token-frequency}, \textit{sentence-frequency}, and \textit{sum-activation}—we examine the ranking of target basis vectors in the extracted vectors under each analysis mode. We compare the results both with and without the removal of background noise. The results are presented in Table~\ref{tab:analysis_results_transposed}. Based on experimental results, performing frequency analysis at the sentence level and removing background noise is most efficient, as it minimizes the impact of irrelevant or outlier activations on the analysis outcomes.

\input{table/feature_analysis_results}

\subsection{“20-mix-2” Dataset for Extraction}
We first built a dataset composed entirely of sentences highly related to the target linguistic feature. Although directly analyzing sentence-level activation frequencies and sorting the results is feasible, we adopted a more efficient method by inserting 1–2 counterfactual sentences into the dataset. During frequency analysis, we focus on sentences where the counterfactuals show no activation. In such cases, the target feature typically ranks among the top five, greatly enhancing search efficiency.

\subsection{Multilingual Dataset}
A base vector with robust representational capacity should activate in response to relevant sentences in multiple languages. We observed that many features activate only in one language, which is undesirable. Therefore, we constructed the “20-mix-2” dataset with a 1:1 ratio of English to Chinese sentences, ensuring that the extracted features possess cross-linguistic representational ability.

\subsection{Weak Activations}
\label{weak_act}
In feature extraction, we observe that some features exhibit weak activation at “interference positions” where the target linguistic feature is not expected. These weak activations occur at positions that either match the cue words of the target feature, share similar morphology with the cue words, or belong to the same linguistic phenomenon category. The weak activations gradually diminish with increasing layer number until they vanish. This indicates that such weak activations are intermediate by-products of the model’s internal processing rather than being determined by the inherent connection strength between the base vector and the feature. Therefore, we exclude these weak activations (activations below one-fifth of the maximum activation value) when calculating the necessity probability of a base vector on the dataset. We present examples from morphology, syntax, and semantics as follows: 

For the agentive suffix feature (12L248164), consider the following sentences: \\
\textit{(a)} The journalist interviewed the mayor about the new policy. The “journalist” token has an activation value of 0.968. \\
\textit{(b)} The newspaper published an article about the new policy. The “newspaper” token has an activation value of 0.197.

For the intransitive verb feature (17L63597), consider the sentences: \\
\textit{(a)} She traveled to the supermarket. The “traveled” token has an activation value of 1.368. \\
\textit{(b)} She drove the car to the supermarket. The “drove” token has an activation value of 0.362.

For the simile feature (26L75327), consider the sentences: \\
\textit{(a)} Her eyes sparkled like stars in the night sky. The “like” token exhibits an activation value of 3.288. \\
\textit{(b)} He looks like his father. The “like” token has an activation value of 0.557.

In these examples, the activations in the (b) sentences are weak and are not considered during feature extraction and evaluation. The presence of weak activation suggests that the model initially activates a broad range of potential semantics and then, during deeper processing, emphasizes the correct, contextually appropriate semantics. This observation warrants further investigation.