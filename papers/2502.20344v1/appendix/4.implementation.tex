\section{Implementation Details}

We used 8 A100 GPUs with 80GB of memory for the experiments. While the exact GPU hours for each experiment were not precisely recorded, the total GPU usage did not exceed one hour. The system was set up with CUDA 12.4, Triton 3.0.0, and Ubuntu 22.04. For the Llama model, we employed the Hugging Face implementation of transformers, and for SAE model, we used the OpenSAE implementation\footnote{\url{https://github.com/THU-KEG/OpenSAE}} and set the hyperparameter $k$ to $128$ for TopK activation.