\section{Methodology}

\method consists of three key components.
(1) A hierarchical linguistic structure with supporting corpora for linguistic mechanism analysis;
(2) Linguistic feature analysis for interpreting SAE extracted features; and
(3) Linguistic feature intervention for causal analysis and LLM steering.


\begin{figure*}[tp]
    \centering
    \includegraphics[width=0.97\textwidth]{figure/methology.pdf}
    \vspace{-0.01in}
    \caption{
    The overall framework of \method.
    We propose a large-model linguistic mechanism framework encompassing six dimensions and select classical features from these dimensions for experimentation. 
    The experimental workflow is as follows: 
    (1) Construct minimal contrast and counterfactual datasets; 
    (2) Extract features and evaluate their relevance by analyzing the activation values of base vectors on the datasets; 
    (3) Intervene in the model output by modifying activation values and assess causality using an LLM as a judge.
    }\label{fig:method}
\end{figure*}


\subsection{Linguistic Structure}

\paragraph{Hierarchical Linguistic Structure.}
To systematically interpret the language capabilities of large models, we adopt a six level structure based on theoretical linguistics~\cite{fromkin2017introduction}: phonetics, phonology, morphology, syntax, semantics, and pragmatics.
The structure follows a logical progression from the external, physical realization of sound to the internal, contextual understanding of meaning. 
Each linguistic capability contains several concrete linguistic features, \textit{e.g.,} semantics level includes metaphor, simile, \textit{etc}.
We provide the exact definition for each linguistic capability in Appendix~\ref{app:ling}

% Our structure provides a comprehensive and modular way to explain how large language models achieve different levels of language ability. 
% By finding linguistic features at different levels in the SAE latent space of large models, we can more accurately reveal how these models represent and process natural language, thereby revealing the underlying mechanism of the models' language ability.
% This mechanism can also bring linguists a clearer understanding of how language knowledge is organized.

\paragraph{Dataset Construction.}
The sparse feature activation distribution of SAE is closely related to the conditions under which their corresponding linguistic features hold in linguistic knowledge.
To find the linguistic features and evaluate its dominance, we propose a method to construct the dataset and analyze feature activation frequencies.

For each linguistic feature, we first construct a set of sentences that significantly align with the desired feature. 
The feature activation representing this linguistic feature in SAE’s hidden space will be significantly activated on these sentences. 
However, this is not enough to accurately identify them, as there are some background noise vectors that are activated on all sentences in the dataset and interfere with our judgment. 
We need to include a control group without the feature in the constructed sentences. 

We introduce two types of control groups: minimal pairs and counterfactual sentences. Minimal pairs are constructed by changing only the part of a sentence that corresponds to a particular linguistic feature, while keeping all other parts unchanged. However, this approach often results in syntactically incorrect sentences.

To overcome this limitation, we also construct fully grammatically correct control groups, called counterfactual sentences, which differ from the original sentence only in terms of its linguistic features. Detailed dataset construction procedures are provided in Appendix~\ref{app:data_construction}.

\subsection{Feature Analysis}
We propose a causal probability approach to evaluate the relationship between extracted linguistic features and their activation on sentences containing those features. 

For a given feature \(x\), we define two key probabilities. The \emph{Probability of Necessity} (PN) quantifies how necessary the feature is for the activation of a corresponding base vector, while the \emph{Probability of Sufficiency} (PS) measures the likelihood that introducing the feature triggers activation. These probabilities are then combined into a \emph{Feature Representation Confidence} (FRC) score, which assesses both the representational capacity of the SAE latent space and the discriminative ability of the feature to identify the corresponding linguistic phenomenon. 

During feature analysis, we calculate the FRS on both the minimal contrast dataset and the counterfactual dataset, then average the results. This average more accurately reflects the ability of the base vectors to represent the linguistic features. Detailed definitions and calculation methods are provided in Appendix~\ref{app:frc}.



\subsection{Feature Intervention}
When we modify the values of SAE’s activation during forward propagation, we expect that such targeted interventions will influence the model’s behavior. 
However, our experiments show that altering only a small subset of features may not significantly impact the output—likely because linguistic phenomena are represented by multiple features across various layers. 
To assess the true impact of these interventions, we use a large language model as a judge. For each linguistic feature, we conduct both ablation and enhancement experiments. 
In the ablation experiment, we set the target feature’s activation to $0$, and in the enhancement experiment, we set it to $10$. 
In both cases, we also perform baseline experiments by randomly selecting 25 base vectors from the same layer.

For brevity, we denote the interventions as follows: let \(I_{abl}^{T}\) denote the targeted ablation intervention, \(I_{abl}^{B}\) the baseline ablation intervention, \(I_{enh}^{T}\) the targeted enhancement intervention, and \(I_{enh}^{B}\) the baseline enhancement intervention.

Let \(P_{abl}^{T}\) and \(P_{abl}^{B}\) denote the success probabilities (\textit{i.e.,} the probability that the intended change in the linguistic phenomenon is observed) for the targeted and baseline ablation experiments. The normalized ablation effect is then defined as
\[
\begin{aligned}
E_{abl} &= P_{abl}^{T} - P_{abl}^{B} \\
        &= \frac{P(Y=0 \mid I_{abl}^{T}) - P(Y=0 \mid I_{abl}^{B})}{P(Y=0 \mid I_{abl}^{T})}.
\end{aligned}
\]
Similarly, let \(P_{enh}^{T}\) and \(P_{enh}^{B}\) be the success probabilities for the targeted and baseline enhancement experiments, with \(Y=1\) indicating the presence of the phenomenon. The normalized enhancement effect is given by
\[
\begin{aligned}
E_{enh} &= P_{enh}^{T} - P_{enh}^{B} \\
        &= \frac{P(Y=1 \mid I_{enh}^{T}) - P(Y=1 \mid I_{enh}^{B})}{1 - P(Y=1 \mid I_{enh}^{B})}.
\end{aligned}
\]

Finally, we define the Feature Intervention Confidence (FIC) score as the harmonic mean of the normalized ablation and enhancement effects:
\[
\text{FIC} = \frac{2\, E_{abl}\, E_{enh}}{E_{abl} + E_{enh}}.
\]
When calculating FIC, if one or both of the $E$ values are negative, we incorporate a penalty coefficient $w$ to reflect the weakened or lost causality in such cases. 
This FIC score provides a balanced measure of how effectively targeted interventions, as opposed to random ones, influence the model’s output with respect to specific linguistic features.
The details for FIC are shown in Appendix~\ref{app:fic}.
% The detailed computation can be found in Appendix~\ref{app:fic}.