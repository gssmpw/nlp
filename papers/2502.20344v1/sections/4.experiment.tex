\section{Experiments}

\subsection{Experiment Setup}

\paragraph{Model.}
We conduct experiments on Llama-3.1-8B~\cite{grattafiori2024llama3}.
For SAEs, we use OpenSAE~\cite{opensae} and its released checkpoints on 32 layers of Llama-3.1-8B.
% and trained Sparse Auto-Encoders (SAEs) on each of its 32 layers. 
% The SAEs were trained at the post-norm position.

\paragraph{Dataset.}
% We train the SAEs on 22B tokens.
For feature analysis, we select $18$ canonical linguistic phenomena from the renowned textbook \textit{An Introduction to Language}~\cite{fromkin2017introduction} as experimental features. 
% For each feature, we construct $m$ minimal pair datasets and $18$ counterfactual sentence datasets.
The selected linguistic features include: \textit{Phonetics/Phonology}: vowels, fricatives, and stress; \textit{Morphology}: noun pluralization and past tense formation (inflectional), and agentive suffixes (derivational); \textit{Syntax}: verb valency (transitive/intransitive), inversion, subject-copula-predicate constructions, and object-complement structures; \textit{Semantics}: simile, metaphor, contrast, sequence, and causality; \textit{Pragmatics}: politeness expressions.

We construct sentences that significantly contain $18$ selected linguistic features, called \dataa.
Additionally, we create minimal pairs and counterfactual datasets for these sentences, called \datab and \datac.

\subsection{Main Results}

The main experiments to verify that \method finds linguistic features in the SAE space 
% and pave ways to steer LLMs in the feature space.
and intervening on these features is effective.

\subsubsection{Feature Extraction and Analysis}
\input{table/main_analysis}

We input the sentences from \dataa into Llama-3.1-8B and pass the neuron activation distributions after batch normalization through the corresponding SAE layers.
We then encode the activation distributions of base vectors at each token in each sentence. 
The ratio of sentences activated by each base vector is calculated. 
These base vectors are ranked by this ratio, and their activation is tested on \datab and \datac.
Base vectors that are significantly activated in \dataa but almost inactive in Datasets \datab and \datac are selected as potential base vectors for each linguistic feature. 
The detailed process mode of feature extraction can be found in Appendix~\ref{extraction_mode}.

For each linguistic feature’s potential base vectors, we compute their activation in \dataa, \datab, and \datac. 
Using the method described earlier, we calculate the sufficiency and necessity probabilities for each base vector in the minimal pairs and counterfactual datasets, and ultimately compute the FRC score for each feature.

Table~\ref{tab:main_analysis} shows the activation indices of a representative base vector for each linguistic feature. 
Due to the varying significance of linguistic features across different sentences, activations corresponding to the base vectors may be lost during the sparse forward propagation process of the SAE, particularly in the TopK activation function mode.
We consider a base vector to be strongly related to a linguistic feature if its sufficiency probability exceeds $0.9$.
% The necessity probability is more complex to consider. 
For necessity probability, as existing SAEs still exhibit slightly poly-semanticity issues~\cite{five-hurdles}, there are still a small amount of low activation for base vectors in positions that do not correspond to the linguistic features.
We assume these low activations do not affect the quality of the base vector’s representation of the linguistic feature, as we discussed in Appendix~\ref{weak_act}.
% For specific details and examples, please refer to Appendix~\ref{weak_act}.

Overall, the base vectors extracted for features across various linguistic levels show strong correlations. 
The high quality of phonetic and phonological features indicates that the model contains accurate IPA-related knowledge. 
The performance of the single feature for metaphor is suboptimal, suggesting that the representation and processing of metaphors may involve more complex mechanisms.
% See Experiments 4.6 and 2.7 for details. 
Furthermore, FRC values for features in other dimensions mainly exceed $0.9$, demonstrating that the selected typical linguistic features consistently yield highly correlated base vectors.

\subsubsection{Feature Intervention}
\input{table/main_intenvention}
We select 5 representative ones for intervention experiments. 
The intervention method involves modifying the activation values of specific base vectors (by index) on a designated SAE layer during forward propagation. 
We perform two types of intervention: feature enhancement and ablation. 
With identical input tokens, we set the activation value to $10$ for enhancement and $0$ for ablation. 
We then compare the generated outputs with those from the unmodified SAE model, focusing on the prominence of the target linguistic features.

We find that intervening on a single linguistic feature in one layer does not produce effects that are easily distinguishable by human evaluators. 
Therefore, we use an LLM (GPT-4o) as a judge~\cite{zheng2023judging} to assess the prominence of these features in the outputs. 
For each feature, we conduct $50$ experiments and calculate the probabilities of successful enhancement and ablation, \textit{i.e.,} increased and decreased feature prominence, respectively.

In addition, we randomly select 50 base vector indices from the intervention layer and conduct enhancement and ablation experiments under the same conditions as a control. 
The success rates in the control group are not around $0.5$; typically, the enhancement success rate is below $0.5$ while the ablation success rate is above $0.5$. This discrepancy may stem from the intervention affecting the model’s output quality, thereby interfering with the proxy LLM’s judgment.

We compute the efficacy of the selected base vectors in both experiments and calculate the FIC value and show the results in Table~\ref{tab:main_intervention}.
% The results are presented in Table\ref{tab:main_intervention}. 
Our results show that enhancement experiments yield significantly better effects than ablation experiments, with all features demonstrating marked enhancement effects. In the ablation experiments, the politeness feature shows relatively good performance, while other features are less affected; the simile feature does not yield the desired ablation effect. This may be because multiple features in the model control the same linguistic phenomenon. Enhancement interventions have a larger impact on the model, whereas ablation of a single feature may be compensated by other features, leading to suboptimal ablation outcomes.
Overall, all 5 features exhibit clear causality in the intervention experiments. 

% Based on the FIC scores, the causal strength ranking is: Politeness > Linking Verb > Causality > Past-Tense > Simile. This ranking does not fully match the FRC score order, indicating that feature representation ability and output control capability are independent and should be evaluated separately.


\subsection{Analysis}

We further conduct analytical experiments to explore the property of \method.

\subsubsection{Combined Intervention}

\begin{figure}[tp]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/combined-inter.pdf}
    \caption{Combined intervention results. Two figures separately present the enhancement and ablation experiment outcomes for the simile and politeness features at layer 26. In these experiments, multiple base vectors corresponding to each feature were jointly intervened.}
    \label{fig:cominter}
\end{figure}

We find that some layers contain multiple base vectors associated with the same linguistic feature. 
We can intervene on these base vectors simultaneously to achieve a stronger effect.

We select two linguistic features—simile and politeness—from layer 26. 
Each feature has four highly related base vectors in this layer. 
% In our combined intervention experiments, we intervene on one, two, three, or all four base vectors corresponding to the experimental feature. 
We increase the number of intervened features from one to four.
In each experiment, we randomly chose the specified number of base vectors from the four. 
We used GPT-4o to assess the prominence of the targeted linguistic feature in the generated outputs. For each feature, we conducted $200$ enhancement experiments and $200$ ablation experiments. 
We also perform control experiments in layer 26 by randomly selecting a set number of base vectors to intervene.

% The results of the enhancement and ablation experiments are shown in Figure\ref{fig:cominter}. 
Figure~\ref{fig:cominter} shows the results for combined intervention.
The results indicate that, as the number of intervened base vectors increases, both the directional intervention and the background control experiments exhibit the same trend: the success rate of enhancement experiments decreases, while that of ablation experiments increases. 
Increasing the number of interventions further affects the quality of the generated text, thereby leading to the observed trend. 
Moreover, the intervention effect of the feature does not change significantly with an increased number of intervened base vectors, indicating that, after excluding background influences, combined interventions on multiple features in the same layer yield only limited improvement in intervention efficacy.


\subsubsection{Deep Semantics Processing}

\begin{figure}[tp]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/deepsemantics.pdf}
    \caption{Activation value distributions of deep semantic corresponding features at layer 6 and 15 for reference ambiguity and metaphor example sentences.}
    \label{fig:deepsemantics}
\end{figure}

Deep semantics refers to the underlying meaning structures that extend beyond surface-level syntax and lexical definitions. 
It captures implicit relationships and conceptual associations within language. 
% Previous explanations of how LLMs process deep semantics have lacked fine-grained analysis. 
% With the help of SAE, we gain a deeper understanding of the internal mechanisms through which LLMs handle deep semantics.
We conduct experiments to show that SAE 

Reference and metaphor exemplify deep semantics by utilizing cognitive mappings and contextual dependencies to convey meaning beyond explicit expression. 
We conduct experiments on reference and metaphor at the sixth and fifteenth layers respectively.
From the results shown on Figure~\ref{fig:deepsemantics}, we observe the following:

\noindent\textbf{Reference.}
In the reference sentence, at the 6\textsuperscript{th} layer, pronouns do not activate the base vectors corresponding to their referents. At the 15\textsuperscript{th} layer, pronouns start to activate the correct base vectors (apple) for their referents, effectively resolving reference ambiguity in contexts where multiple possible referents exist. This indicates that as we move deeper into the layers, pronouns generate their deep semantics and disambiguate possible referents.

\noindent\textbf{Metaphor.}
In the metaphor sentence, our experimental statements contain only the vehicle (fire) and not the tenor (sun). 
In the 6\textsuperscript{th} layer, the base vector corresponding to the vehicle is activated, while the base vector for the tenor remains inactive. 
In the 15\textsuperscript{th} layer, the activation of the vehicle’s base vector decreases, while the base vector for the tenor becomes activated. 
This suggests that as the model moves to deeper layers, the vehicle maps to the target domain and generates the deep semantics of the tenor, even without the tenor in the context.

\subsubsection{Cross-layer Activations}

\begin{figure}[tp]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/cross-layer-compare.pdf}
    \caption{Activation value distributions of deep semantic corresponding features in the 6th and 15th layers for anaphoric and metaphor example sentences.}
    \label{fig:cross-compare}
\end{figure}


In different layers of the model, we identify distinct base vectors that are activated by the same linguistic feature. 
% These base vectors, corresponding to the same feature in different layers, exhibit sometimes similar and sometimes different activation patterns for the same input sentence. 
To validate this phenomenon, we select a standard base vector from a given layer for each linguistic feature and apply the corresponding SAE to other layers of Llama-3.1-8B. 
Encoding the hidden states of these layers with the cross-layer SAE, using the same dataset, reveals that in most layers the base vector with the same index as the standard one shows a similar activation pattern on the tokens.



This approach provides an effective tool for observing the cross-layer activation of linguistic features. 
Figure~\ref{fig:cross-compare} displays 8 linguistic features. 
For each feature, the average maximum activation curve and the reciprocal rank curve (used to assess the relative importance of the base vector on the dataset) of the standard base vector are shown for different layers. 
The distributions indicate that: 
(1) Linguistic features are widely activated across layers, with only some features failing to activate in the early or late layers; 
(2) Features that do not activate in the early layers are mostly semantic, while those that do not activate in the later layers are mainly syntactic, possibly reflecting the model’s functional division across layers; 
(3) Activation curves typically follow a “rise–plateau–fall” pattern, whereas the reciprocal rank curves often exhibit several sharp peaks. This suggests that each linguistic operation has its own active processing range within the model, although its relative importance may vary across layers.

\begin{figure}[tp]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/token.pdf}
    \caption{Distribution of average maximum activation values and reciprocal ranks (i.e., the reciprocal of the base vector ranking) for seven typical linguistic features across layers in the dataset.}
    \label{fig:cross-act}
\end{figure}

We further examine the cross-layer activation pattern at the sentence level using the metaphor feature as an example in Figure~\ref{fig:cross-act}. 
We select the standard metaphor feature from layer 26 and analyze its activation distribution in layers 2, 5, 10, 14, 19, 24, and 29. 
In both example sentences, layers 2 and 29 show no token activations, suggesting that the model’s initial and final layers may not process metaphors. Layers 5, 10, and 14 appear to be involved in preliminary metaphor processing: in the first sentence, the feature intermittently activated the token following the cue word ``like''; in the second sentence, the feature first activated a fixed structure (“as as”), then activated the second “as” and the subsequent token. 
Around layer 19, the activation extends to the cue word and the entire vehicle, while around layer 24, the activation recedes back to the cue word. 
These results indicate more complex internal mechanisms for metaphor processing and demonstrate that SAE-based feature extraction is a valuable tool for further exploring the model’s internal linguistic mechanisms.



\subsubsection{Case Study for Intervention}
\input{table/case}
% While using LLM as a judge, we also perform manual analysis on the generated results after intervening with the model. 
We conduct manual case study on the generated contents after intervening on one identified simile-related base vector.
% The case studies for the simile feature intervention experiments are shown in Table~\ref{tab:case}. 
We show cases in Table~\ref{tab:case}.


In Case \#1, the prompt is ``\textit{Generate a sentence describing winter}'', which does not explicitly include the target linguistic feature. 
We find that after enhancing the simile-related base vector, the LLM turns to use simile.
We can also find that the descriptive and imagistic quality of the default output is stronger than in the ablation results, which indicate that the simile-related base vector is also responsible for vividness.

Case \#2 uses the prompt ``\textit{Generate a sentence using a simile to describe love}'', with explicit requirement for using simile to generate the sentence.
When the simile-related base vector is ablated, the LLMs turn to use straightforward descriptions without using similes.
Meanwhile, when enhancing the simile-related base vector, the LLMs continue to generate sentences with simile.
We show more intervention cases in Appendix~\ref{case_appendix}.



% We find that in the enhancement experiment, similes are more frequent and prominent, while in the ablation experiment, no similes appear. 
% In the control experiment, no direct similes appear either, but the descriptive and imagistic quality is stronger than in the ablation results. 
% In Case \#2, the prompt explicitly includes the target linguistic feature. 
% Here, we observe that no similes appear in the ablation experiment. 
% Both the enhancement and control experiments show clear similes, but the similes in the enhancement experiment are more vivid and creative, with richer and more complex conceptual mappings. 
% In the control experiment, the simile is a commonly used idiomatic expression.


% From this case study, we can infer that when intervening with features such as similes, two types of intervention effects occur: (1) an increase in the frequency and prominence of the feature in the text, which is often observed when the feature is not present in the context, and (2) an improvement in the quality of the feature’s representation in the text, which is common when the feature is already present in the context. You can find 