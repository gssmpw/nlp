\section{Introduction}
Large language models (LLMs) exhibit excellent performance in solving tasks that require different levels of linguistic competence, such as dependency parsing~\cite{lin2022dependency,benchclamp}, reference disambiguation~\cite{disambiguation} and metaphor interpretation~\cite{gpt3-metaphor,eval-non-literal-intent,metaphor-reasoning}.
% While their linguistic capabilities are mostly credit to ability emergence from large-scale pre-training and model size~\cite{manning2020emergent,allen2023physics,mahowald2024dissociating}, the inner working mechanism of LLMs to process these linguistic structure still remains under-explored.
While their linguistic capabilities are largely attributed to the emergence of abilities from large-scale pre-training and model size~\cite{manning2020emergent,allen2023physics,mahowald2024dissociating}, the underlying mechanisms by which LLMs process these linguistic structures remain under-explored~\cite{saba2023stochastic}.
Thus, we aim to interpret the linguistic mechanisms of LLMs by addressing the following question: \textit{Can we identify minimal components within LLMs that are responsible for distinct linguistic processing capabilities?}
% Thus, we aim to interpret the linguistic mechanism of LLMs by answering the following question: \textit{Can we identify minimal components from LLMs for distinct linguistic processing capabilities?}


% Large-scale models possess powerful language capabilities, and understanding the mechanisms underlying these capabilities has been a topic of significant interest. 
% The mechanisms of language provide explanations for how models operate at the most fundamental and critical levels of linguistic behavior, while also demonstrating great potential for controlling model outputs at finer-grained structural levels. 
% However, in the face of skepticism from some researchers regarding the linguistic competence of language models, there remains a lack of systematic and comprehensive causal studies to demonstrate how language models KNOW linguistic knowledge.


\begin{figure}[tp]
    \centering
    \includegraphics[width=0.98\linewidth]{figure/intro.pdf}
    \caption{After a sentence is input into the model, its hidden states are encoded by the SAE into a sparse feature distribution. Across multiple layers, we can identify base vectors that are significantly activated and associated with the sentence’s linguistic features.}
    \label{fig:intro}
\end{figure}


Previous attempts at interpreting the linguistic mechanisms of LLMs usually involve instructing them with expert-designed prompts, aiming to explain how these models generate particular outputs.~\cite{yin2022interpreting}.
Nevertheless, such behavior-based methods do not provide model-structure-level mechanism interpretation.
Most recent works turn to establish the connection between specific linguistic capabilities of LLMs and their interior structure, such as hidden states~\cite{katz2023visit}, attention heads~\cite{wu2020structured}, and activated neurons~\cite{sajjad2022neuron,huang2023rigorously}. 
However, these approaches mostly suffer from the following challenges:
% coarse interpretation granularity and insufficient causal analysis.

% which potentially hinders the establishment of a detailed and comprehensive understanding of the models' underlying linguistic mechanisms and fails to provide compelling evidence for their linguistic capabilities.



% Previous research has made valuable attempts to interpret models at multiple levels—model behavior, hidden layers, and neurons. 
% At the model behavior level, studies have employed feature attribution, contrastive explanation, surrogate model explanation, and self-explanation approaches to elucidate the relationship between output behavior and input tokens. 
% Hidden layer interpretations focus on internal feature extraction and representation, primarily through probing tasks and matrix decomposition. 
% Neuron-level interpretations concentrate on the activation analysis of key circuits or neuron groups.



\textbf{Coarse Interpretation Granularity.}
Linguistic mechanism interpretation aims to find the \textit{atomic} linguistic structure in LLMs.
However, even neurons are the most fine-grained native components of LLMs, they are observed to be activated by multiple different conditions, a phenomena that are termed as poly-semanticity~\cite{yan2024encourage}.
Thus, it is necessary to extract more fine-grained structures from LLMs to interpret their linguistic mechanism.

% On one hand, large language models have a vast number of neurons, and each neuron often encodes multiple concepts. Consequently, a single neuron's activation may reflect several unrelated semantic features. This results in high uncertainty and ambiguity in fine-grained explanations. Similarly, probing or matrix decomposition methods applied to hidden layers can only reveal partial statistical associations. They struggle to precisely isolate specific features, leading to coarse-grained explanations. On the other hand, model behavior is directly observable, yet macroscopic explanations often disconnect from the underlying mechanisms. There is a significant disparity in granularity across methods, and no approach exists that both captures fine details and clearly reflects semantic content.

\textbf{Insufficient Causal Analysis.}
Current linguistic mechanism interpretations successfully identify relevant inner structure of LLMs, with activation patching~\cite{hanna2024faith,nanda2023progress} as the most typical methodology.
However, it is still challenging to verify causal relationships between linguistic abilities and corresponding internal structures, which is a prerequisite for effectively steering model behavior through interventions on corresponding mechanisms.


To address these challenges, we propose to utilize sparse auto-encoder (\textbf{SAE}) for interpreting \textbf{ling}uistic mechanisms of LLMs, a framework dubbed \method.
SAE learns a projection matrix which decomposes the hidden states of LLMs into an extremely high-dimensional feature space with sparse activation constraint, where each dimension is expected to represent a single meaning, as Figure~\ref{fig:intro} shows.
Building on this, \method comprises two components:
(1) \method designs sparse feature analysis for un-interpreted features that are extracted by SAE, which provides a fine-grained linguistic mechanism interpretation for LLMs;
(2) \method proposes to manipulate the LLMs via intervening on the features with desired interpretation, which verifies the causal relationship between the features and their interpretations.
It also potentially paves way to steer the linguistic behavior of LLMs.


% \method first establishes a hierarchical linguistic framework with annotated corpora for each level of linguistic ability that are observed in LLMs.
% Building on this, \method designs sparse feature analysis for un-interpreted features that are extracted by 



% To address issues of granularity and causality, we trained and open-sourced a set of Sparse AutoEncoders (SAEs) and analysis methods. The SAE is a weak dictionary learning method that effectively resolves the ambiguity problem of traditional approaches. Building on this, we propose a framework for analyzing language mechanisms based on theoretical linguistics and a method for extracting and evaluating linguistic features using causal analysis.


In particular, we first establishes a hierarchical linguistic framework with annotated corpora.
The framework classifies the linguistic features into six categories, including phonetics, phonology, morphology, syntax, semantics, and pragmatics.
These linguistic features are wildly observed linguistic abilities, thus guarantee the feasibility to interpret their mechanisms.
To interpret sparse features in SAEs, we construct minimal pairs and counterfactual sentences for each sentence in our dataset.
We also introduce a causal analysis method that intervenes on specific linguistic features via the SAE and uses an LLM as a judge to assess the intervention effect. 
Furthermore, we present two causality evaluation metrics: the Feature Representation Confidence (FRC) score and Feature Intervention Confidence (FIC) score, which measure a feature’s ability to identify the corresponding linguistic phenomenon in the input and its ability to regulate the model output to generate the phenomenon, respectively.
% FRC measures a feature’s ability to identify the corresponding linguistic phenomenon in the input, and FIC measures its ability to regulate the model output to generate the phenomenon. 
% Together, these metrics allow us to evaluate the quality and reliability of the extracted linguistic features.

% We selected dozens of classical linguistic features and used our SAE to extract their corresponding basis vector indices from the Llama-3.1-8B model. These features are systematically distributed across nearly all 32 layers. We classify the features into six categories: phonetics, phonology, morphology, syntax, semantics, and pragmatics. We demonstrate the method’s power by explaining complex language phenomena such as anaphora, metaphor, and simile.

% We propose a method for feature extraction and analysis using minimal pairs and counterfactual sentence datasets. We also introduce a causal analysis method that intervenes on specific linguistic features via the SAE and uses an LLM as a judge to assess the intervention effect. Furthermore, we present two causality evaluation metrics: the Feature Representation Confidence score (FRC) and Feature Intervention Confidence (FIC) score. FRC measures a feature’s ability to identify the corresponding linguistic phenomenon in the input, while FIC measures its ability to regulate the model output to generate the phenomenon. Together, these metrics allow us to evaluate the quality and reliability of the extracted linguistic features.


We conduct a series of experiments on Llama-3.1-8B~\cite{grattafiori2024llama3}.
Our experiment results show that \method effectively identifies key features for linguistic competence.
\method also provides a robust way to steer LLMs by intervening on the found linguistic features.
