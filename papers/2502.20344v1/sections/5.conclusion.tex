\section{Conclusion}
This work addresses two key challenges in interpreting linguistic mechanisms in LLMs: coarse interpretation granularity and insufficient causal analysis. We introduce \method, which uses sparse autoencoders (SAE) for fine-grained feature extraction, overcoming poly-semanticity in traditional methods. \method also verifies causal relationships by intervening on features, enabling more precise control over model behavior. Our approach reveals that LLMs encode structured linguistic knowledge and offers a robust framework for steering their outputs.

% \clearpage

\section{Limitations}
Our work has several limitations in terms of dataset size, number of features, intervention effects, and cross-layer analysis. 
Regarding \textbf{datasets}, each feature in our study is constructed with approximately 160 sentences. In the future, the dataset can be further expanded to serve as a benchmark for evaluating the language mechanism interpretability of the SAE framework. Concerning the \textbf{number of selected features}, we choose 18 representative linguistic features from various theoretical linguistic dimensions. This selection sufficiently demonstrates the effectiveness of our method across different linguistic levels; however, to construct a complete and comprehensive language mechanism system, our approach can be extended to extract a larger number of linguistic features. Achieving this extension will require further work or the development of automated feature extraction methods. In terms of \textbf{intervention effects}, our experiments show statistically significant effects for linguistic feature interventions, yet the effect and stability of each case are still inferior to fine-tuning methods. This issue calls for further research and refinement of SAE-based intervention methods. Finally, regarding \textbf{cross-layer analysis}, our experiments illustrate the cross-layer mechanism of linguistic features, revealing the potential of our method to explain how large language models process and transmit linguistic information across layers. However, we do not conduct large-scale experiments and inductive analyses in this area, which represents an extension of our method that remains to be explored in future work.

\section{Ethical Considerations}
This section discusses the ethical considerations and broader impact of this work:
\paragraph{Potential Risks:} There is a potential risk that understanding the linguistic mechanisms of the model could provide guidance for embedding malicious information into the modelâ€™s internal structure. To address this, we will fully open-source our method to enable the community to quickly develop countermeasures in the event of such attacks.
\paragraph{Intellectual Property:} The models used, Llama-3.1-8B, and the SAE framework OpenSAE, are both open-source and intended for scientific research use, in accordance with their respective open-source licenses.
\paragraph{Data Privacy:} All data used in this research has been manually reviewed to ensure it does not contain any personal or private information.
\paragraph{Intended Use:} \method is intended to be used as a method for analyzing the mechanisms of large language models.
\paragraph{Documentation of Artifacts:} The artifacts, including datasets and model implementations, are comprehensively documented with respect to their domains, languages, and linguistic phenomena to ensure transparency and reproducibility.
\paragraph{AI Assistants in Research or Writing:} We employ GitHub Copilot for code development assistance and use GPT-4 for refining and polishing the language in our writing.