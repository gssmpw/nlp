\section{Related Works}

Linguistic mechanism interpretation has been a ever-chasing goal since the emergence of LLMs.
We review linguistic capability evaluation for LLMs and corresponding mechanistic interpretation works.
We will also introduce the basic concepts for sparse auto-encoder.

\textbf{Linguistic Features in LLMs.}
LLMs are shown to be equipped with diverse linguistic features.
% At the phonetics and phonology level, tasks focus on basic syllabic and phonological processing. 
Morphological studies find inflectional and derivational phenomena along with word-formation processes in LLMs~\cite{rambelli2024nounnoun,weissweiler2023counting}. 
Syntactic evaluations include canonical constructions, \textit{e.g.,} genitives and object-complement structures~\citep{gauthier2020syntaxgym,zhang2023textembedding,arora2024causalgym}, and cross-linguistic tests~\cite{mueller2020crosslinguistic}. 
Semantic investigations address metaphor comprehension~\cite{metaphor-reasoning,gpt3-metaphor,stowe2021metaphor,he2022similes,liu2022figurative}, deep semantic analysis~\cite{chen2024emotionqueen}, and output consistency~\cite{raj2023semantic}. 
Pragmatic benchmarks examine the interpretation of contextual cues~\cite{sileo2022pragmatics,wu2024rethinking}.



\textbf{Linguistic Mechanism Interpretation.}
LLMs excel in most of the above tasks, which spurs growing interest in explaining their linguistic capabilities. 
At the behavioral explanation level, methods include feature attribution, contrastive explanation~\cite{yin2022interpreting}, surrogate model explanation, and self-explanation.
At the hidden-layer explanation level, approaches comprise analyses of attention heads~\cite{wu2020structured}, probing tasks~\cite{hahn2019tabula,arora2024causalgym,he2022similes}, and correlation studies~\cite{liu2024fantastic} of hidden-layer activation patterns. 
At the neuron explanation level, research has primarily focused on analyzing the activations of linguistically relevant neurons~\cite{linguistic-neuron}.


\textbf{Sparse Auto-encoder.} 
Recent work has employed sparse auto-encoders (SAEs) to interpret the hidden-layer activations of large language models by decomposing them into a large set of concept features~\cite{gao_sparse}. 
These concept features exhibit mono-semanticity and hold considerable interpretability potential~\cite{cunningham2023sparse}.
In particular, an SAE maps the hidden states $\mathbf{f} \in \mathbb{R}^{d}$ in LLMs into the feature space with sparse activations:
\begin{equation*}
    \mathbf{f} = \text{SparseConstraint} \left( \mathbf{W}_e \mathbf{h} + \mathbf{b}_e \right),
\end{equation*}
where the SAE is parameterized by $\mathbf{W}_e \in \mathbb{R}^{(r \times d) \times d}, \mathbf{b}_e \in \mathbb{R}^{(r \times d)}$.
$r$ is the expansion ratio, defined as the factor by which the hidden state dimension is expanded.
Commonly used sparse constraint include TopK~\cite{gao_sparse} and JupeReLU~\cite{rajamanoharan2024improving} functions.
As each dimension of the sparse activation in $\mathbf{f}$ corresponds to a base vector in $\mathbf{W}_e$, this paper uses base vector to denote features extracted by SAE.

% However, interventions based on SAE-extracted features have yielded suboptimal results, and existing studies suggest that these concept features may be distributed across multiple layers.
