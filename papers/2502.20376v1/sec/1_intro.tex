\section{Introduction}
\label{sec:intro}

\input{figures/teaser}

Text-to-image diffusion models have seen remarkable advancements in recent years \cite{ho2020denoisingdiffusionprobabilisticmodels, ramesh2022hierarchicaltextconditionalimagegeneration}. These models generate images through an iterative denoising process, where each step is conditioned on the input text prompt. 
This condition text prompt dictates the conditional distribution from which the generated image is sampled, and guides each step towards this distribution.


The ability of these models to produce high-quality and diverse images has sparked significant interest in their potential for editing \textit{real} images, which often fall outside the model's native distribution. 
To edit real images, inversion techniques are often employed to derive an initial noise that faithfully reconstructs the real image through the model's denoising process~\cite{dhariwal2021diffusionmodelsbeatgans, Mokady_2023_CVPR, hubermanspiegelglas2024editfriendlyddpmnoise}. Having the initial noise that reconstructs the image allows to steer the denoising process towards the target edit~\cite{hertz2022prompt, tumanyan2022plugandplaydiffusionfeaturestextdriven, hubermanspiegelglas2024editfriendlyddpmnoise, Parmar_2023, cao2023masactrltuningfreemutualselfattention, patashnik2023localizingobjectlevelshapevariations}.

Inverting a real image presents a significant challenge, as it requires balancing the tradeoff between accurately reconstructing the image and ensuring the editability of the resulting initial noise~\cite{tov2021designing}. DDIM inversion~\cite{song2022denoisingdiffusionimplicitmodels, dhariwal2021diffusionmodelsbeatgans} is a widely used approach and serves as the basis for many other inversion techniques~\cite{Mokady_2023_CVPR, garibi2024renoise, miyake2023negative, samuel2024lightningfastimageinversionediting}. This method reverses the sampling process by performing forward diffusion according to the reversed algorithm, utilizing the diffusion model at each step. When applying DDIM inversion in text-to-image diffusion models, the process also relies on setting an appropriate text prompt, which conditions the modelâ€™s prediction at every forward step.

In this work, we investigate the role of the specific condition used during the inversion process. Our findings reveal that conditioning the inversion on a text prompt that accurately describes the input image improves both the reconstruction quality and the editability of the inversion results. 
These findings are illustrated in Figure~\ref{fig:motivation-figure}. We present there reconstruction results of DDIM inversion using three levels of text prompt specificity.
As shown, closely aligning the condition with the source image effectively narrows and tightens the model's target distribution, shifting it from a broad range of images to those closely resembling the source image. 

Building on these insights, we propose an inversion approach that employs the ultimate condition: the source image itself. We call this method \textit{Tight Inversion}, as image conditions are inherently more precise than text conditions. This tight conditioning significantly improves inversion quality and enhances editing performance. We show that Tight Inversion integrates seamlessly with various inversion methods beyond DDIM inversion, consistently enhancing their performance.

To evaluate the effectiveness of our approach, we conduct extensive experiments, emphasizing both reconstruction accuracy and editability. While reconstruction ensures that the inverted noise reproduces the given image, it is not a meaningful goal on its own. The true purpose of inversion is to enable meaningful edits to the reconstructed image. Thus, our evaluation emphasizes how well the inversion facilitates edits while preserving fidelity to the original content. Our experiments primarily target the inversion of complex and challenging images, as this is where the strength of our method truly stands out. We demonstrate the effectiveness of our method using three types of models: a standard diffusion model, a few-step diffusion model, and a flow model.


\input{figures/motivation_figure}
