\section{Related Work}
\label{sec:related_work}







\paragraph{\textbf{Image Editing with Diffusion Models}}
In recent years, diffusion models \cite{ho2020denoisingdiffusionprobabilisticmodels, nichol2022glide, song2022denoisingdiffusionimplicitmodels, ramesh2022hierarchicaltextconditionalimagegeneration, saharia2022photorealistictexttoimagediffusionmodels, songscore} have shown rapid improvements in generating high-quality images from text prompts.
However, editing real images using textual prompts remains a challenge, as these models are not inherently designed to modify existing images. Image editing requires a careful balance between preserving key attributes of the original image (e.g., structure, semantics) and introducing controlled changes (e.g., style, pose, or specific objects).
To address this task, various approaches have been proposed. A notable line of work builds on the observation that images generated from the same initial noise tend to share semantic and structural similarities when conditioned on different signals. To further preserve original attributes, these methods manipulate the denoising process by injecting features from the source image into the edited output~\cite{hertz2022prompt, Parmar_2023, tumanyan2022plugandplaydiffusionfeaturestextdriven, cao2023masactrltuningfreemutualselfattention, alaluf2023crossimageattentionzeroshotappearance, patashnik2023localizingobjectlevelshapevariations, Mokady_2023_CVPR, ge2023expressive, lu2023tf, tokenflow2023, avrahami2024diffuhaul}. To apply these methods for real-image editing, an inversion technique is needed to predict the initial noise $z_T$ that reconstructs the image.

Other approaches for diffusion-based image editing include partially noising an input image followed by denoising with a different text condition~\cite{meng2022sdeditguidedimagesynthesis, hubermanspiegelglas2024editfriendlyddpmnoise, tsaban2023leditsrealimageediting, brack2024ledits}, fine-tuning the base model to accept an input image as a condition~\cite{Rombach_2022_CVPR, brooks2023instructpix2pixlearningfollowimage, Zhang2023MagicBrush, Avrahami_2023_CVPR, zhang2023addingconditionalcontroltexttoimage}, and utilizing masks to enable localized edits~\cite{Avrahami_2022, avrahami2023blendedlatent, couairondiffedit, mirzaei2025watch}.


\paragraph{\textbf{Diffusion Models Inversion}}
To edit an image $I$ using a diffusion model, many methods require obtaining an initial noise $z_T$ such that denoising $z_T$ reconstructs $I$. A common approach for this is DDIM inversion~\cite{song2022denoisingdiffusionimplicitmodels, dhariwal2021diffusionmodelsbeatgans}, which reverses the denoising process to approximate the initial noise. This inversion relies on solving an implicit equation by assuming that consecutive points in the denoising trajectory are close to each other. However, this assumption often does not hold during typical use with a practical number of denoising steps and introduces inaccuracies.
To address these inaccuracies, some methods~\cite{garibi2024renoise, samuel2024lightningfastimageinversionediting, Pan_2023} employ different algorithms to solve the implicit equation. Another limitation of DDIM inversion arises from the use of classifier-free guidance~\cite{ho2021classifierfree} during denoising~\cite{Mokady_2023_CVPR}. To address this, some methods optimize the null-text embedding~\cite{Mokady_2023_CVPR}, use empty prompts during inversion~\cite{cao2023masactrltuningfreemutualselfattention}, or use negative prompts~\cite{miyake2023negative, proxedit}.
As we demonstrate in this work, DDIM inversion is sensitive to the prompts used during the inversion process. Therefore, integrating DDIM inversion based methods with our approach can significantly improve both reconstruction and editability, particularly for challenging images.

Another line of work focuses on the non-deterministic DDPM denoising process \cite{ho2020denoisingdiffusionprobabilisticmodels}, inverting the image into the intermediate noise maps introduced throughout the stochastic process~\cite{hubermanspiegelglas2024editfriendlyddpmnoise, deutch2024turboedittextbasedimageediting, tsaban2023leditsrealimageediting, cyclediffusion}. While these methods ensure perfect reconstruction of the input image, they often struggle to preserve fidelity to the original image during editing, particularly for challenging cases. Our approach enhances the editability of these methods, achieving better preservation of the original image.






\paragraph{\textbf{Image Conditioned Diffusion Models}}
Some methods train encoders (or adapters) that take an image as input and produce a latent representation, which is then injected into a pretrained text-to-image model~\cite{ye2023ipadaptertextcompatibleimage, gal2024lcmlookahead, gal20232e4t, Wei_2023, parmar2025viscomposer, patashnik2025nested, guo2024pulid, zeng2024jedi, arar2023agnostic}. These approaches typically aim to personalize the text-to-image model, enabling it to generate a subject in new contexts and styles.
In our work, we utilize IP-Adapter~\cite{ye2023ipadaptertextcompatibleimage, xlabs-flux-ip-adapter} and PuLID~\cite{guo2024pulid} to condition the model on an image.
IP-Adapter was trained on a broad domain with the objective of reconstructing the input image. While it does not fully reconstruct the image in practice and instead produces semantic variations, it serves as an effective tool to transform text-conditioned models into models conditioned on both text and images. PuLID is trained on images containing faces with the goal of preserving identity in the generated image with minimal disruption to the original model's behavior.

\input{figures/method}
