
\section{Tight Inversion}


Given a real image $I$, the goal of our method is to predict a noise image $z_T$ such that denoising $z_T$ yields $I$ back. Importantly, it should be possible to edit $I$ when using a target text prompt during the denoising process. Our work builds on DDIM inversion~\cite{dhariwal2021diffusionmodelsbeatgans, song2022denoisingdiffusionimplicitmodels} and begins by analyzing it.

\paragraph{\textbf{Background and Motivation}}
To invert a real image $I$, DDIM inversion iteratively adds noise to the image, forming a trajectory from the real data distribution to the Gaussian distribution. 
Each point $z_t$ in the inversion trajectory is defined as:
\begin{equation} \label{eq:inversion}
    z_{t} = A_t z_{t-1} - B_t \epsilon_{\theta}(z_{t-1}, t, c),
\end{equation}
where $\epsilon_{\theta}$ is the pretrained diffusion model, $c$ is the text condition fed into the model, $A_t, B_t$ are constants defined by DDIM~\cite{song2022denoisingdiffusionimplicitmodels}, and $z_0=I$.
To reconstruct the image, the same condition $c$ as the one used during the inversion is used in the denoising process.

Previous work~\cite{songscore} has shown that a pretrained diffusion model can be viewed as a score function, resulting in
\begin{equation}
    \epsilon_{\theta}(z_t, t, c) \propto \nabla_{z_t} \log{p_{\theta}(z_t | c)}.
\end{equation} 
Therefore, a more detailed and precise condition $c$ should lead to a narrower conditional distribution $p_{\theta}(z_t | c)$, which in turn should improve the accuracy of $\epsilon_{\theta}(z_t, t, c)$. Since Equation~\ref{eq:inversion} relies on $\epsilon_{\theta}(z_t, t, c)$, we expect that its increased accuracy will result in a more accurate inversion process.
We verify this intuition through the following experiment. 

First, we generate a set of elaborated text prompts using an LLM, and sample a single image for each prompt. Then, we apply DDIM inversion~\cite{dhariwal2021diffusionmodelsbeatgans} on each image with three different text conditions: (i) the text prompt used to generate the image (full), (ii) a shortened version of this prompt (short), and (iii) an empty prompt. 
We re-generate the images from the inverted noises with the same condition used in the inversion, and do not use classifier-free guidance (CFG).
We measure $L_2$, PSNR, SSIM and LPIPS~\cite{zhang2018perceptual} between the sampled image and the reconstructed one and display the results in Table~\ref{tab:prompt_levels_experiment}. 
As observed from the results, across all the metrics using a short prompt results in a better reconstruction than using an empty prompt, and using a detailed prompt results in a better reconstruction than using a short prompt.

\input{tables/prompts_levels.tex}

\paragraph{\textbf{Toy Example}}
To further illustrate the motivation behind our method, we explore the role of the condition used during inversion through a toy example depicted in Figure~\ref{fig:method}. In this setup, we train a CNF (Flow Matching) model $\phi: \mathbb{R}^2 \rightarrow \mathbb{R}^2$ \cite{lipmanflow, chen2018neural}. The prior distribution, $\mathcal{N}(\textbf{0}, 1)$, is represented by the bottom Gaussian, while the posterior (target) distribution consists of five Gaussians $\{\mathcal{N}((5 \cdot c_i, 10), 1)\}_{i=1}^5$ with $c_i \in \{-2, -1, 0, 1, 2\}$, corresponds to the five Gaussians on the top (see Figure~\ref{fig:toy-flow}). The model $\phi$ is trained as a conditional model, where each sample from the posterior distribution is assigned a condition corresponding to the index of the Gaussian from which it was drawn. Additionally, in $50\%$ of the training iterations, a null condition is used.
Figure~\ref{fig:toy-flow} shows the denoising trajectories of (light blue) points sampled from the prior distribution when using the null condition.
In Figures~\ref{fig:toy-uncond}, \ref{fig:toy-cond}, \ref{fig:toy-incorrect-cond}, we sample points (shown in blue) from the posterior distribution of $c_5$ that were not seen during $\phi$'s training, then invert and reconstruct them. The inverted points are depicted in light blue, while the reconstructed points are shown in green. We show the inversion and reconstruction trajectories for a subset of the points to provide further insight. The inversion trajectory is depicted in blue while the reconstruction trajectory is depicted in light blue. For each timestep $t$, we connect the corresponding points on the inversion and reconstruction trajectories (see Figure~\ref{fig:toy-uncond}).


In Figure~\ref{fig:toy-uncond}, we inverted and reconstructed the points using the null condition. As shown, blue points located outside the dense regions of the posterior distribution tend to exhibit higher reconstruction errors. Additionally, the inverted points cluster within a small region of the prior distribution. 
Moreover, the inversion and reconstruction trajectories do not overlap, as illustrated by the lines connecting corresponding points on the inversion and reconstruction trajectories.
In Figure~\ref{fig:toy-cond}, we performed inversion using the correct condition for the blue points. This results in accurate reconstruction, with the inverted points distributed in better alignment with the prior distribution. Here, the inversion and reconstruction trajectories coincide, and therefore the lines connecting corresponding points on them are not seen. Finally, in Figure~\ref{fig:toy-incorrect-cond}, we inverted points sampled from the Gaussian matching $c_5$ but used the condition $c_4$ during inversion and reconstruction. Using an incorrect condition again leads to higher reconstruction errors. Furthermore, the inverted points are mapped to low-probability regions of the prior distribution, which suggests a reduction in the editability of these points.



\input{figures/recon_qualitative}

\paragraph{\textbf{Image-conditioned Inversion}}
Given a real image $I$, we opt to find a condition $c$ that best aligns with it. Unlike the synthetic samples from the previous experiments for which we know the conditions that were used to generate them, for real images we do not have such condition prompts.
A common approach is to use a VLM to generate such prompts. 
The key idea of our method is that the most descriptive condition for an image is the image itself. That is, the conditional distribution $p_{\theta}(z_t | c)$ where $c$ is set as $I$ is more narrow than any other condition we can potentially use. 
However, the conditioning mechanism of the text-to-image model was trained to take textual tokens as input rather than images. 

Notably, many recent methods~\cite{ye2023ipadaptertextcompatibleimage, xlabs-flux-ip-adapter} train image encoders or adapters, to condition the generation process on images. Specifically, we utilize IP-Adapter (Image Prompt Adapter)~\cite{ye2023ipadaptertextcompatibleimage} which adds cross-attention layers that operate in parallel to the existing cross-attention layers of the model, and take as input image tokens rather than textual tokens. Instead of using the original text-to-image model, $\epsilon_\theta$, in Tight Inversion we use the one that integrates with IP-Adapter, $\bar{\epsilon}_\theta$, during both inversion and denoising processes. In the last row of Table~\ref{tab:prompt_levels_experiment}, we show the reconstruction results obtained by utilizing the input image as a condition through IP-Adapter~\cite{ye2023ipadaptertextcompatibleimage}, where the condition text is set as an empty prompt. As observed by the table, using the input image as the model's condition results in superior inversion results.

We note that Tight Inversion can be easily integrated with previous inversion methods (e.g., Edit Friendly DDPM, ReNoise) by employing $\bar{\epsilon}_\theta$ instead of $\epsilon_\theta$. As we demonstrate in the next section, Tight Inversion consistently improves such methods in terms of both reconstruction and editability.


