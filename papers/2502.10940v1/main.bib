@inproceedings{lialin2023relora,
  title={Relora: High-rank training through low-rank updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{zhao2024galore,
  title={Galore: Memory-efficient llm training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}
@article{han2024sltrain,
  title={SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining},
  author={Han, Andi and Li, Jiaxiang and Huang, Wei and Hong, Mingyi and Takeda, Akiko and Jawanpuria, Pratik and Mishra, Bamdev},
  journal={arXiv preprint arXiv:2406.02214},
  year={2024}
}
@inproceedings{sainath2013low,
  title={Low-rank matrix factorization for deep neural network training with high-dimensional output targets},
  author={Sainath, Tara N and Kingsbury, Brian and Sindhwani, Vikas and Arisoy, Ebru and Ramabhadran, Bhuvana},
  booktitle={2013 IEEE international conference on acoustics, speech and signal processing},
  pages={6655--6659},
  year={2013},
  organization={IEEE}
}
@article{denil2013predicting,
  title={Predicting parameters in deep learning},
  author={Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={26},
  year={2013}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{hoffmann2022training,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}
@article{krajewski2024scaling,
  title={Scaling laws for fine-grained mixture of experts},
  author={Krajewski, Jakub and Ludziejewski, Jan and Adamczewski, Kamil and Pi{\'o}ro, Maciej and Krutul, Micha{\l} and Antoniak, Szymon and Ciebiera, Kamil and Kr{\'o}l, Krystian and Odrzyg{\'o}{\'z}d{\'z}, Tomasz and Sankowski, Piotr and others},
  journal={arXiv preprint arXiv:2402.07871},
  year={2024}
}
@article{kumar2024scaling,
  title={Scaling laws for precision},
  author={Kumar, Tanishq and Ankner, Zachary and Spector, Benjamin F and Bordelon, Blake and Muennighoff, Niklas and Paul, Mansheej and Pehlevan, Cengiz and R{\'e}, Christopher and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2411.04330},
  year={2024}
}
@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}
@article{chen2024fira,
  title={Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?},
  author={Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren},
  journal={arXiv preprint arXiv:2410.01623},
  year={2024}
}
@inproceedings{huang2024galore,
  title={Galore-mini: Low rank gradient learning with fewer learning rates},
  author={Huang, Weihao and Zhang, Zhenyu and Zhang, Yushun and Luo, Zhi-Quan and Sun, Ruoyu and Wang, Zhangyang},
  booktitle={NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability}
}
@article{liao2024galore,
  title={GaLore $+ $: Boosting Low-Rank Adaptation for LLMs with Cross-Head Projection},
  author={Liao, Xutao and Li, Shaohui and Xu, Yuhui and Li, Zhi and Liu, Yu and He, You},
  journal={arXiv preprint arXiv:2412.19820},
  year={2024}
}
@article{hao2024flora,
  title={Flora: Low-Rank Adapters Are Secretly Gradient Compressors},
  author={Hao, Yongchang and Cao, Yanshuai and Mou, Lili},
  journal={arXiv preprint arXiv:2402.03293},
  year={2024}
}
@article{zhu2024apollo,
  title={APOLLO: SGD-like Memory, AdamW-level Performance},
  author={Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon},
  journal={arXiv preprint arXiv:2412.05270},
  year={2024}
}
@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}
@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@article{eckart1936approximation,
  title={The approximation of one matrix by another of lower rank},
  author={Eckart, Carl and Young, Gale},
  journal={Psychometrika},
  volume={1},
  number={3},
  pages={211--218},
  year={1936},
  publisher={Springer}
}
@article{kamalakara2022exploring,
  title={Exploring low rank training of deep neural networks},
  author={Kamalakara, Siddhartha Rao and Locatelli, Acyr and Venkitesh, Bharat and Ba, Jimmy and Gal, Yarin and Gomez, Aidan N},
  journal={arXiv preprint arXiv:2209.13569},
  year={2022}
}
@article{huh2024training,
  title={Training neural networks from scratch with parallel low-rank adapters},
  author={Huh, Minyoung and Cheung, Brian and Bernstein, Jeremy and Isola, Phillip and Agrawal, Pulkit},
  journal={arXiv preprint arXiv:2402.16828},
  year={2024}
}
@article{yang2024comera,
  title={CoMERA: Computing-and Memory-Efficient Training via Rank-Adaptive Tensor Optimization},
  author={Yang, Zi and Liu, Ziyue and Choudhary, Samridhi and Xie, Xinfeng and Gao, Cao and Kunzmann, Siegfried and Zhang, Zheng},
  journal={arXiv preprint arXiv:2405.14377},
  year={2024}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}
@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}
@article{mozaffari2024slope,
  title={SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs},
  author={Mozaffari, Mohammad and Yazdanbakhsh, Amir and Zhang, Zhao and Dehnavi, Maryam Mehri},
  journal={arXiv preprint arXiv:2405.16325},
  year={2024}
}
@article{hu2024accelerating,
  title={Accelerating Transformer Pre-Training with 2: 4 Sparsity},
  author={Hu, Yuezhou and Zhao, Kang and Huang, Weiyu and Chen, Jianfei and Zhu, Jun},
  journal={arXiv preprint arXiv:2404.01847},
  year={2024}
}

@article{cui2020active,
  title={Active subspace of neural networks: Structural analysis and universal attacks},
  author={Cui, Chunfeng and Zhang, Kaiqi and Daulbaev, Talgat and Gusak, Julia and Oseledets, Ivan and Zhang, Zheng},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={2},
  number={4},
  pages={1096--1122},
  year={2020},
  publisher={SIAM}
}

@article{huh2021low,
  title={The low-rank simplicity bias in deep networks},
  author={Huh, Minyoung and Mobahi, Hossein and Zhang, Richard and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},
  journal={arXiv preprint arXiv:2103.10427},
  year={2021}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{dao2021pixelated,
  title={Pixelated butterfly: Simple and efficient sparse training for neural network models},
  author={Dao, Tri and Chen, Beidi and Liang, Kaizhao and Yang, Jiaming and Song, Zhao and Rudra, Atri and Re, Christopher},
  journal={arXiv preprint arXiv:2112.00029},
  year={2021}
}
@article{jaderberg2014speeding,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}
@article{lebedev2014speeding,
  title={Speeding-up convolutional neural networks using fine-tuned cp-decomposition},
  author={Lebedev, Vadim and Ganin, Yaroslav and Rakhuba, Maksim and Oseledets, Ivan and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1412.6553},
  year={2014}
}
@article{novikov2015tensorizing,
  title={Tensorizing neural networks},
  author={Novikov, Alexander and Podoprikhin, Dmitrii and Osokin, Anton and Vetrov, Dmitry P},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}
@inproceedings{tjandra2017compressing,
  title={Compressing recurrent neural network with tensor train},
  author={Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
  booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
  pages={4451--4458},
  year={2017},
  organization={IEEE}
}
@article{sui2024elrt,
  title={ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks},
  author={Sui, Yang and Yin, Miao and Gong, Yu and Xiao, Jinqi and Phan, Huy and Yuan, Bo},
  journal={arXiv preprint arXiv:2401.10341},
  year={2024}
}
@article{khodak2021initialization,
  title={Initialization and regularization of factorized neural layers},
  author={Khodak, Mikhail and Tenenholtz, Neil and Mackey, Lester and Fusi, Nicolo},
  journal={arXiv preprint arXiv:2105.01029},
  year={2021}
}
@article{chekalina2023efficient,
  title={Efficient gpt model pre-training using tensor train matrix representation},
  author={Chekalina, Viktoriia and Novikov, Georgii and Gusak, Julia and Oseledets, Ivan and Panchenko, Alexander},
  journal={arXiv preprint arXiv:2306.02697},
  year={2023}
}
@inproceedings{feng2021optimal,
  title={Optimal gradient checkpoint search for arbitrary computation graphs},
  author={Feng, Jianwei and Huang, Dong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11433--11442},
  year={2021}
}
@article{he2023transcending,
  title={Transcending runtime-memory tradeoffs in checkpointing by being fusion aware},
  author={He, Horace and Yu, Shangdi},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={414--427},
  year={2023}
}
@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}
@inproceedings{zhangsparse,
  title={How Sparse Can We Prune A Deep Network: A Fundamental Limit Perspective},
  author={Zhang, Qiaozhe and Zhang, Ruijie and Sun, Jun and Liu, Yingzhuang},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}