[
  {
    "index": 0,
    "papers": [
      {
        "key": "han2024sltrain",
        "author": "Han, Andi and Li, Jiaxiang and Huang, Wei and Hong, Mingyi and Takeda, Akiko and Jawanpuria, Pratik and Mishra, Bamdev",
        "title": "SLTrain: a sparse plus low-rank approach for parameter and memory efficient pretraining"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "hu2021lora",
        "author": "Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu",
        "title": "Lora: Low-rank adaptation of large language models"
      },
      {
        "key": "lialin2023relora",
        "author": "Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna",
        "title": "Relora: High-rank training through low-rank updates"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "yang2024comera",
        "author": "Yang, Zi and Liu, Ziyue and Choudhary, Samridhi and Xie, Xinfeng and Gao, Cao and Kunzmann, Siegfried and Zhang, Zheng",
        "title": "CoMERA: Computing-and Memory-Efficient Training via Rank-Adaptive Tensor Optimization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "hu2024accelerating",
        "author": "Hu, Yuezhou and Zhao, Kang and Huang, Weiyu and Chen, Jianfei and Zhu, Jun",
        "title": "Accelerating Transformer Pre-Training with 2: 4 Sparsity"
      },
      {
        "key": "mozaffari2024slope",
        "author": "Mozaffari, Mohammad and Yazdanbakhsh, Amir and Zhang, Zhao and Dehnavi, Maryam Mehri",
        "title": "SLoPe: Double-Pruned Sparse Plus Lazy Low-Rank Adapter Pretraining of LLMs"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhao2024galore",
        "author": "Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong",
        "title": "Galore: Memory-efficient llm training by gradient low-rank projection"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "loshchilov2017decoupled",
        "author": "Loshchilov, I",
        "title": "Decoupled weight decay regularization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "chen2024fira",
        "author": "Chen, Xi and Feng, Kaituo and Li, Changsheng and Lai, Xunhao and Yue, Xiangyu and Yuan, Ye and Wang, Guoren",
        "title": "Fira: Can We Achieve Full-rank Training of LLMs Under Low-rank Constraint?"
      },
      {
        "key": "huang2024galore",
        "author": "Huang, Weihao and Zhang, Zhenyu and Zhang, Yushun and Luo, Zhi-Quan and Sun, Ruoyu and Wang, Zhangyang",
        "title": "Galore-mini: Low rank gradient learning with fewer learning rates"
      },
      {
        "key": "liao2024galore",
        "author": "Liao, Xutao and Li, Shaohui and Xu, Yuhui and Li, Zhi and Liu, Yu and He, You",
        "title": "GaLore $+ $: Boosting Low-Rank Adaptation for LLMs with Cross-Head Projection"
      },
      {
        "key": "hao2024flora",
        "author": "Hao, Yongchang and Cao, Yanshuai and Mou, Lili",
        "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors"
      },
      {
        "key": "zhu2024apollo",
        "author": "Zhu, Hanqing and Zhang, Zhenyu and Cong, Wenyan and Liu, Xi and Park, Sem and Chandra, Vikas and Long, Bo and Pan, David Z and Wang, Zhangyang and Lee, Jinwon",
        "title": "APOLLO: SGD-like Memory, AdamW-level Performance"
      }
    ]
  }
]