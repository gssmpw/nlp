\section{Related Work}
\noindent {\bf Model Compression.} Recent research on efficient LLM pre-training primarily focuses on memory savings. To out best knowledge, SLTrain \cite{han2024sltrain} is the first method that reduces both trainable parameters and total parameters in LLM pre-training, without significantly hurting model performance. This also reduces memory usage for model, gradients, and optimizer states (see its smaller circle in Fig.~\ref{fig:main}). However, the existence of its unstructured sparse matrix $\mat{S}$ requires reconstructing $\tilde{\mat{W}} = \mat{BA} + \mat{S}$, otherwise it will incur dense-sparse multiplications that are still memory costly (Fig.~\ref{fig:arc}c). This causes additional computing than the full-rank baseline. LoRA/ReLoRA \cite{hu2021lora, lialin2023relora} reduces trainable parameters by freezing a full-rank $\mat{W}_0$ and training (at least in a later stage) only low-rank factors, potentially reducing memory needs. Yet, any compute savings are limited because the forward pass yields a larger compute than its full-rank counterpart, especially when the rank must stay relatively large in pre-training tasks. CoMERA~\cite{yang2024comera} achieves higher model compression and FLOP reduction, but its low-rank tensor operations are GPU unfriendly. Similar to matrix-compressed approaches, CoMERA cannot avoid a performance drop either. Some works investigate pure structured sparsity or combined with low-rank factors \cite{hu2024accelerating, mozaffari2024slope} to achieve speed up, but still show a significant performance drop during the pre-training stage.

\vspace{5pt}
\noindent {\bf Gradient Compression.} GaLore \cite{zhao2024galore} reduces memory by projecting gradients into a low-rank space, shrinking optimizer states below the typical $2\times$ AdamW overhead \cite{loshchilov2017decoupled}. However, it increases computation by adding up/down projections on top of already compute-heavy full-rank training. As shown in Fig.~\ref{fig:main}, its estimated FLOPs surpass full-rank training on the LLaMA-1B scale. Follow-up work \cite{chen2024fira, huang2024galore, liao2024galore, hao2024flora, zhu2024apollo} further explores low-rank gradient projection. While these methods are promising, they are mostly orthogonal to our focus. Crucially, these methods are still computing lower-bounded by the full-rank baseline. Our goal instead is to reduce computing cost to a fraction of full-rank training, thus lowering the demand of computing resources in LLM pre-training.
 




This paper presents an alternative approach that explores the low-rank property in model {\bf activations} from an architectural perspective. This is conceptually different from the above model compression methods despite the similarity in their formulations. Our approach is mostly orthogonal with gradient compression techniques, meaning that they can be combined to further boost efficiency.