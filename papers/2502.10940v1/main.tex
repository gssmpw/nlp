% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}
\usepackage{amssymb, amsmath, makecell, multirow, hyperref}
\usepackage[table]{xcolor}
\usepackage{booktabs}
\newcommand{\cmark}{\pmb{\checkmark}}
\newcommand{\xmark}{\(\pmb{\times}\)}
\definecolor{lightblue}{HTML}{DCE6F1}

\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newcommand{\ten}[1]{\mathbfcal{#1}}
\newcommand{\mat}[1]{\mathbf{#1}}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}
% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}
\usepackage{enumitem}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
\newcommand{\zz}[1]{{\color{red}[zz: #1]}}
\newcommand{\al}[1]{{\color{blue}[alvin: #1]}}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

% \title{CoLA: Revisiting Low-Rank Pre-Training of LLMs via \\ Low-Rank Economic Activations}

% Alternative choice
\title{CoLA: Compute-Efficient Pre-Training of LLMs via Low-Rank Activation}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
 \textbf{Ziyue Liu}\textsuperscript{*1}, 
 \textbf{Ruijie Zhang}\textsuperscript{*1}, 
 \textbf{Zhengyang Wang}\textsuperscript{*1}, 
 \textbf{Zi Yang}\textsuperscript{2}, 
 \textbf{Paul Hovland}\textsuperscript{3}, \\
 \textbf{Bogdan Nicolae}\textsuperscript{3}, 
 \textbf{Franck Cappello}\textsuperscript{3}, 
 \textbf{Zheng Zhang}\textsuperscript{1}
\\
\textsuperscript{1}University of California at Santa Barbara;
\textsuperscript{2}University at Albany, SUNY \\
\textsuperscript{3}Argonne National Laboratory
}


%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
% \textsuperscript{1}University of California, Santa Barbara}
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
\def\thefootnote{*}\footnotetext{Equal contribution}\def\thefootnote{\arabic{footnote}}
\begin{abstract}
Large language models (LLMs) are revolutionizing many science and engineering fields. However, their huge model sizes impose extremely demanding needs of computational resources in the pre-training stage. Although low-rank factorizations can reduce model parameters, their direct application in LLM pre-training often lead to non-negligible performance loss. To address this fundamental challenge, we introduce {\bf CoLA} and its memory-efficient implementation, {\bf CoLA-M}. We leverage the low-rank structure observed widely in model activations, enforcing non-linear transformations between factorized weight matrices to reduce model size, boost model capacity and training efficiency. Experiments on LLaMA models with 60 million to 7 billion parameters show that CoLA reduces the computing cost by $\bf 2\pmb{\times}$ and improves training throughput by $\bf 1.86\pmb{\times}$ while maintaining full-rank level performance. CoLA-M further squeezes memory cost without sacrificing throughput, offering a pre-training approach with collectively superior parameter, computing, and memory efficiency. The LLMs produced are also $\bf 2\pmb{\times}$ smaller, enabling faster inference with lower memory cost on resource-constrained platforms. \footnote{Code available \href{https://github.com/alvin-zyl/CoLA}{here}.}
\end{abstract}

\section{Introduction}
\begin{figure}
  \includegraphics[width=\columnwidth]{figures/cola-1b-flops.png}
  \caption{A joint comparison of validation perplexity, estimated compute FLOPs (per block, step) and model size between various pre-training methods on a LLaMA-1B model with a token batch size of 256. Among them, our proposed CoLA is the only one that reduces both compute FLOPs and model size while demonstrating on par validation perplexity with full-rank training.}
  \label{fig:main}
  \vspace{-10pt}
\end{figure}

Large foundation models have revolutionized the landscape of artificial intelligence, achieving unprecedented success in the language, vision, and scientific domains. In a quest to improve
accuracy and capability, foundation models have become huge. Several studies \cite{kaplan2020scaling, hoffmann2022training, krajewski2024scaling, kumar2024scaling} have highlighted a rapid increase in the size of the model and the number of training tokens. Models such as 175B GPT-3 \cite{brown2020language}, 405B LLaMA-3 \cite{dubey2024llama}, and 540B PaLM \cite{chowdhery2023palm} are just a few examples of this trend. 
Under such circumstances, a large number of GPUs are needed in order to provide the computational and high-bandwidth memory capacity needed to pre-train large fundation models over long periods of time (months). The staggering increase in cost results in an unsustainable trend, prompting the need to develop cost-efficient pre-training techniques that reduce the scale, FLOPs, and GPU memory cost. 

\textbf{Motivation:} At the core of increasing resource utilization and cost is the simple practice of scaling up full-size linear layers in decoder-only architectures, which has proven to be a viable and straightforward strategy. Thus, to break free from this unsustainable trend, it is imperative to improve architecture efficiency. This has been widely studied in the deep learning community, involving different levels of factorization of weight matrices: from simple matrix factorizations, i.e., a singular value decomposition (SVD), to higher-order tensor factorizations such as Canonical Polyadic, Tucker, and Tensor-Train (TT) format. Extensive studies have shown that such factorizations can effectively reduce the total number of parameters needed to achieve similar performance in numerous domains \cite{sainath2013low, jaderberg2014speeding, lebedev2014speeding, novikov2015tensorizing, tjandra2017compressing, dao2021pixelated, sui2024elrt, yang2024comera, zhangsparse}, especially when neural networks are overparameterized.

\textbf{Limitations of state-of-art:} The techniques mentioned above have been applied only to a limited degree to pre-training tasks, and their findings suggest that the pure low-rank or sparse structure often downgrades model performance \cite{khodak2021initialization, kamalakara2022exploring, chekalina2023efficient, zhao2024galore, hu2024accelerating, mozaffari2024slope}. This has pivoted most recent work of efficient pre-training into two directions: 1) Accumulating multiple low-rank updates \cite{huh2024training,lialin2023relora}; 2) Enforcing low-rank structures in gradients rather than parameters \cite{zhao2024galore, chen2024fira, huang2024galore, liao2024galore, hao2024flora, zhu2024apollo}. Both approaches have their limitations. 1) The accumulation of low-rank updates requires instantiating a full-rank matrix and a deeply customized training strategy that periodically merges and restarts the low-rank components. This creates computing overhead in practice and can only achieve (if only) marginal computing and memory reduction. 2) Enforcing low-rank gradients reduces only the optimizer memory and adds additional computation that downgrades training throughput. Furthermore, the memory saving caused by gradient compression becomes negligible as the training batch size increases, as activations dominate the total memory cost. A recent paper called SLTrain \cite{han2024sltrain} revisited the notion of parameter efficiency in foundation model pre-training, by having both low-rank factors and an unstructured sparse matrix. SLTrain effectively reduces the total number of parameters without significantly hurting model performance. However, it still introduces computing overhead on top of full-rank training due to the necessary reconstruction of low-rank factors. 
We note that none of the above works has achieved superior efficiency of {\bf parameter}, {\bf computing}, and {\bf memory} simultaneously {\bf without performance drop} in both {\bf training} and {\bf inference} for foundation model pre-training. 

\textbf{Contributions:} In this paper, we propose {\bf CoLA}: {\bf Co}mpute-Efficient Pre-Training of LLMs via {\bf L}ow-rank {\bf A}ctivation, and its memory efficient implementation {\bf CoLA-M}, to achieve all the desirable properties mentioned above. We summarize our contributions as follows:
\begin{itemize}[leftmargin=*]
\vspace{-5pt}
    \item We propose {\bf CoLA}, a novel architecture that enforces explicit low-rank activations by injecting non-linear operations between factorized weight matrices. CoLA can greatly reduce the computing FLOPS while maintaining the performance of full-rank pre-training.
    \vspace{-5pt}
    \item We provide a memory efficient implementation, namely {\bf CoLA-M}, to achieve superior memory reduction without sacrificing throughput.
    \vspace{-5pt}
    \item We extensively pre-train LLaMA with 60M up to 7B parameters. CoLA reduces model size and computing FLOPs by $\bf 2\pmb{\times}$, while maintaining on-par performance to its full-rank counterpart. At the system level, CoLA improves $\bf 1.86\pmb{\times}$ training and $\bf 1.64\pmb{\times}$ inference throughput.  CoLA-M reduces total pre-training memory by $\bf 2/3$, while still manages to improve $\bf 1.3\pmb{\times}$ training throughput over full-rank baselines.
\end{itemize}
A high-level comparison of CoLA/CoLA-M with main baselines is provided in Table~\ref{tab:summary}.

\begin{table}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c}
\toprule
\multicolumn{2}{c|}{} & \textbf{CoLA(-M)} & \textbf{SLTrain} & \textbf{GaLore} & \textbf{ReLoRA} \\
\midrule
\multicolumn{2}{c|}{\textbf{Parameter $\pmb{\downarrow}$}} & \cmark & \cmark & \xmark & \xmark \\
\midrule
\multirow{2}{*}{\textbf{Compute $\pmb{\downarrow}$}} & Training & \cmark & \xmark & \xmark & \cmark \\
& Inference & \cmark & \xmark & \xmark & \xmark \\
\midrule
\multirow{2}{*}{\textbf{Memory $\pmb{\downarrow}$}} & Training & \cmark & \cmark & \cmark & \cmark \\
& Inference & \cmark & \cmark & \xmark & \xmark \\
\midrule
\multirow{2}{*}{\textbf{Throughput $\pmb{\uparrow}$}} & Training & \cmark & \xmark & \xmark & \xmark \\
& Inference & \cmark & \xmark & \xmark & \xmark \\
\bottomrule
\end{tabular}%
}
\caption{Summary and comparison of different types of efficiency across various pre-training methods.}
\label{tab:summary}
\vspace{-10pt}
\end{table}

\section{Related Work}

\noindent {\bf Model Compression.} Recent research on efficient LLM pre-training primarily focuses on memory savings. To out best knowledge, SLTrain \cite{han2024sltrain} is the first method that reduces both trainable parameters and total parameters in LLM pre-training, without significantly hurting model performance. This also reduces memory usage for model, gradients, and optimizer states (see its smaller circle in Fig.~\ref{fig:main}). However, the existence of its unstructured sparse matrix $\mat{S}$ requires reconstructing $\tilde{\mat{W}} = \mat{BA} + \mat{S}$, otherwise it will incur dense-sparse multiplications that are still memory costly (Fig.~\ref{fig:arc}c). This causes additional computing than the full-rank baseline. LoRA/ReLoRA \cite{hu2021lora, lialin2023relora} reduces trainable parameters by freezing a full-rank $\mat{W}_0$ and training (at least in a later stage) only low-rank factors, potentially reducing memory needs. Yet, any compute savings are limited because the forward pass yields a larger compute than its full-rank counterpart, especially when the rank must stay relatively large in pre-training tasks. CoMERA~\cite{yang2024comera} achieves higher model compression and FLOP reduction, but its low-rank tensor operations are GPU unfriendly. Similar to matrix-compressed approaches, CoMERA cannot avoid a performance drop either. Some works investigate pure structured sparsity or combined with low-rank factors \cite{hu2024accelerating, mozaffari2024slope} to achieve speed up, but still show a significant performance drop during the pre-training stage.

\vspace{5pt}
\noindent {\bf Gradient Compression.} GaLore \cite{zhao2024galore} reduces memory by projecting gradients into a low-rank space, shrinking optimizer states below the typical $2\times$ AdamW overhead \cite{loshchilov2017decoupled}. However, it increases computation by adding up/down projections on top of already compute-heavy full-rank training. As shown in Fig.~\ref{fig:main}, its estimated FLOPs surpass full-rank training on the LLaMA-1B scale. Follow-up work \cite{chen2024fira, huang2024galore, liao2024galore, hao2024flora, zhu2024apollo} further explores low-rank gradient projection. While these methods are promising, they are mostly orthogonal to our focus. Crucially, these methods are still computing lower-bounded by the full-rank baseline. Our goal instead is to reduce computing cost to a fraction of full-rank training, thus lowering the demand of computing resources in LLM pre-training.
 




This paper presents an alternative approach that explores the low-rank property in model {\bf activations} from an architectural perspective. This is conceptually different from the above model compression methods despite the similarity in their formulations. Our approach is mostly orthogonal with gradient compression techniques, meaning that they can be combined to further boost efficiency.

\section{CoLA for Efficient LLM Pre-Training}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/mlp2_Spectrum.png}
    \caption{MLP Activation Spectrum of the pre-trained GPT-2 small \cite{radford2019language}. Model activations are evaluated on the WikiText2 dataset. a) The singular value decay across different decoder blocks. b) The full dimension vs. effective rank ($\alpha=0.95$) per block.}
    \label{fig:activation_spectrum}
    \vspace{-10pt}
\end{figure}

\begin{figure*}[t]
  \includegraphics[width=\linewidth]{figures/cola-arc.png}
  \caption{Comparison between CoLA (ours) and other efficient pre-training frameworks. a) LoRA/ReLoRA \cite{lialin2023relora} maintains a full-rank frozen weight; b) GaLore \cite{zhao2024galore} only reduces optimizer states by down and up projecting gradients; c) SLTrain \cite{han2024sltrain} requires necessary reconstruction of the low-rank and sparse matrices; d) CoLA (ours) is a pure low-rank method involves only rank $r$ weight matrices.}
  \label{fig:arc}
  \vspace{-10pt}
\end{figure*}

% For clarity, in this paper we refer (model) activations as intermediate results \zz{Avoid this statement. Activation has a rigorous definition in the ML and neuroscience community. }produced during the forward propagation, and refer the non-linear activation function (i.e., ReLU, SiLU, etc), as non-linear function/transformation/operation, or nonlinearity, interchangeably.

\subsection{A Motivating Example}

Many previous works have observed the low-rank structure of model activations in deep neural networks \cite{cui2020active,huh2021low}. We also observe this phenomenon in LLMs, i.e. the \textit{effective rank} of the activations is much smaller than their original dimensionality. To quantify this, we define the \textit{effective rank} \(r(\alpha)\) of activation as the minimal number of singular values needed to preserve an \(\alpha\)-fraction of the total spectral energy. Formally:
\begin{equation}
    r(\alpha) \;=\; \min \left\{ k \;\middle|\; \frac{\sum_{i=1}^k \sigma_i^2}{\sum_{i=1}^n \sigma_i^2} \;\ge\; \alpha \right\},
\end{equation}
where \(\sigma_1, \sigma_2, \ldots, \sigma_n\) are the singular values of the activation matrix, and \(0 < \alpha \le 1\) is the desired ratio of preserved information. As shown in our experiments, the rapid decay of singular values [Fig.~\ref{fig:activation_spectrum}a] leads to much smaller \(r(\alpha)\) compared to the full dimension [Fig.~\ref{fig:activation_spectrum}b]. This highlights the significant low-rank nature in the activations of pre-trained LLMs. More results showing the same pattern can be found in Appendix~\ref{sec:appendix-low-rank-activation}. 

\subsection{Low Rank Weights + Activations}
\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/cola-block-m.png}
  \caption{A decoder block in CoLA with LLaMA-like architecture (layer norms, rotary positional embeddings are omitted for simplicity). Every linear layer is replaced by low-rank factorization with a non-linear function in between. Modules painted in sketch are the re-computations during the backward step of CoLA-M (a memory efficient implementation of CoLA).}
  \label{fig:block}
  \vspace{-10pt}
\end{figure}

Motivated by the observed low-rank nature of LLM activations, we propose to enforce explicit low-rank activation via injecting non-linearity between factorized weight matrices.

Let $\mat{W} \in \mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$ be the weight matrix of an arbitrary linear layer followed by a nonlinear activation in the transformer architecture: 
\begin{equation}
    \mat{h} = \sigma\left(\mat{Wx}\right), \; \text{with} \; \mat{x}\in \mathbb{R}^{d_{\text{in}}}.
    \label{eq:full-rank-fwd}
\end{equation} 
We replace $\mat{W}$ by two low-rank matrices $\mat{A}\in \mathbb{R}^{r \times d_{\text{in}}} $ and $\mat{B}\in \mathbb{R}^{d_{\text{out}} \times r} $, where rank $ r<\min(d_{\text{in}, \text{out}}) $ is a hyper-parameter, and inject a non-linear activation $\sigma$ in the middle. This modification results in a transformation consisting of $(\text{linear} \circ \text{non-linear} \circ \text{linear})$ operations:
\begin{equation}
    \mat{h^{\prime}} = \mat{B} \, \sigma (\mat{A} \mat{x}).
    \label{eq:main-fwd}
\end{equation}
During the backward step, we simply apply the chain rule to compute gradients w.r.t each of the low rank factors as
\begin{equation}
\begin{aligned}
    & \nabla_{\mat{B}} = \nabla_{\mat{h}^{\prime}}\mat{z}^T, \nabla_{\mat{z}} = \mat{B}^T\nabla_{\mat{h}^{\prime}}, \nabla_{\mat{o}} = \nabla_{\mat{z}} \odot \sigma^{\prime}(\mat{o}), \\
    & \nabla_{\mat{A}} = \nabla_{\mat{o}}\mat{x}^T, \nabla_{\mat{x}} = \mat{A}^T \nabla_{\mat{o}},
\end{aligned}
\label{eq:main-bwd}
\end{equation}
where $\mat{o} = \mat{Ax}, \mat{z} = \sigma(\mat{o})$, $\odot$ denote the element-wise product. We empirically find that keeping the original nonlinearity on top of Eq.~\eqref{eq:main-fwd} does not harm the performance, nor necessarily brings benefits. However, applying Eq.~\eqref{eq:main-fwd} to all linear layers regardless of whether being followed by nonlinearity is crucial to boost model performance. We refer more details to the ablation study in Appendix~\ref{sec:appendix-ablation}.

Fig.~\ref{fig:block} shows the architecture of each transformer block when adopting CoLA into the LLaMA architecture. We highlight the fact that only the original linear layers and (if any) their follow-up non-linear transformation are modified to the CoLA formulation. Other computations such as the scaled-dot product of the self-attention, as well as residual connections and the element-wise product of LLaMA's feed-forward layers, remain unchanged.

\subsection{Computing Efficiency}
\label{sec:compute-eff}

\begin{table}[t]
\centering
\small
\begin{tabular}{c|c}
\toprule
\textbf{Operation} & \textbf{FLOPs} \\
\midrule
Attention: Q, K, V & $6nd^2$ \\
\midrule
Attention: SDP & $4n^2d$ \\
\midrule
Attention: Project & $2nd^2$ \\
\midrule
Feed-forward & $6ndd_{\text{ff}}$ \\
\midrule
Total Forward & $8nd^2 + 4n^2d +  6ndd_{\text{ff}} $ \\
\midrule
Total Backward & $16nd^2 + 8n^2d +  12ndd_{\text{ff}} $ \\
\bottomrule
\end{tabular}
\caption{Breakdown compute of a single LLaMA decoder layer in full-rank training. Lower-order terms such as bias, layer norm, activation are omitted.}
\label{tab:compute-breakdown-llama}
\vspace{-10pt}
\end{table}

\begin{table}[t]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c}
\toprule
\textbf{Methods} & \textbf{FLOPs} \\
\midrule
Full-Rank & \( C_{\text{Full-Rank}} = 24nd^2 + 12n^2d + 18ndd_{\text{ff}} \) \\
\midrule
CoLA & \( C_{\text{CoLA}} = 48ndr + 12n^2d + 18nr(d + d_{\text{ff}})\) \\
\midrule
(Re)LoRA & \( C_{\text{LoRA}} = C_{\text{CoLA}} + 16nd^2 + 12n^2d + 12ndd_{\text{ff}}\) \\
\midrule
SLTrain & \( C_{\text{SLTrain}} = C_{\text{Full-Rank}} + 24d^2r + 18dd_{\text{ff}}r \) \\
\midrule
GaLore & \( C_{\text{GaLore}} = C_{\text{Full-Rank}} + 16d^2r + 12dd_{\text{ff}}r \) \\
\bottomrule
\end{tabular}%
}
\caption{Estimated compute of a single LLaMA decoder layer for different pre-training methods. Results combine forward, backward and any additional compute occurred at optimizer step.}
\label{tab:compute-all-methods}
\vspace{-10pt}
\end{table}

We analyze and compare the computational complexity of CoLA with other efficient pre-training methods based on the LLaMA architecture. We adopt a similar notion from \cite{kaplan2020scaling}, where a general matrix multiply (GEMM) between an $M\times N$ matrix and an $N\times K$ matrix involves roughly $2MNK$ add-multiply operations. We denote the model inner width as $d$, and the inner width of the feed-forward layer as $d_{\text{ff}}$. For simplicity, we only show non-embedding calculations of a single sequence with token batch size of $n$ for each decoder layer. This is because the total computation scales only linearly with the number of layers $n_{\text{layer}}$ and the number of sequences $n_{\text{seq}}$. Furthermore, lower-order cheap operations of complexity $\mathcal{O}(nd)$ or $\mathcal{O}(nd_{\text{ff}})$ are omitted, such as bias, layer norm, non-linear function, residual connection, and element-wise product. %These lower-order operations are much cheaper than all other terms that are at least of $\mathcal{O}(nd^2)$ or $\mathcal{O}(n^2d)$.

We show the detailed cost of the full-rank training in Table.~\ref{tab:compute-breakdown-llama}. Notice that we apply the $2\times$ rule when calculating the backward cost. This is because for each forward GEMM that Eq.~\eqref{eq:full-rank-fwd} describes, two GEMMs are needed to compute gradients for both the weight matrix $\mat{W}$ and the input $\mat{x}$, and are of the same cost the forward GEMM, i.e.,
\begin{equation}
    \nabla_{\mat{x}} = \mat{W}^{T}\nabla_{\mat{h}}, \nabla_{\mat{W}} = \nabla_{\mat{h}}\mat{x}^T.
\end{equation}

We apply the same analysis to all the following pre-training methods: 
\begin{itemize}[leftmargin=*]
\vspace{-5pt}
    \item {\bf LoRA/ReLoRA} \cite{hu2021lora, lialin2023relora}: \(\mat{h}_{\text{LoRA}} = \mat{W}_0\mat{x} + \mat{BAx}\), with fixed $\mat{W}_0$.
    \vspace{-5pt}
    \item {\bf SLTrain} \cite{han2024sltrain}: \(\mat{h}_{\text{SLTrain}} = \mat{BAx} + \mat{Sx} = (\mat{BA} \oplus_{\mathcal{I}} \mathcal{V})\mat{x}\), where $\oplus$ denotes the scatter-add operator, $\mathcal{I}$ and $\mathcal{V}$ are the indices and values of non-zero elements in the sparse matrix $\mat{S}$.
    \vspace{-5pt}
    \item {\bf GaLore} \cite{zhao2024galore}: \(\mat{R}_t = \mat{P}_t^T \mat{G}_t,\)  \(\tilde{\mat{G}}_t = \mat{PN}_t\), where $\mat{P}_t$ projects the gradient $\mat{G}_t$ onto a low-rank space, and then projects it back when updating the full-rank weight $\mat{W}$.
\end{itemize}

We summarize the computational costs of these methods in Table~\ref{tab:compute-all-methods} and observe that the costs of SLTrain and GaLore are lower bounded by full-rank training, while (Re)LoRA is lower bounded by CoLA when choosing the same rank. In contrast, CoLA reduces the computation from full-rank training when $r < 0.62d$, assuming $d_{\text{ff}} \approx2.5d$ in LLaMA-like architecture. The default rank choice is set to $r = \frac{1}{4}d$, leading to a reduction in compute to about half of the full-rank training. We refer all details of compute analysis to Appendix~\ref{sec:appendix-compute-analysis}.

\section{CoLA-M: A Memory-Efficient Implementation}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/fullrank-memory-breakdown-withbz.png}
  \caption{Memory breakdown for LLaMA-1B using fairly large sequence batch sizes in pre-training. The activation memory is at dominant place.}
  \label{fig:memory-break}
  \vspace{-5pt}
\end{figure}

Although CoLA has more intermediate results from each low-rank projection and the following non-linear function (as shown in Fig.~\ref{fig:block}), we can choose strategically which ones to save in order to balance re-computations with memory overhead. In this section, we design and develop CoLA-M, a memory-efficient implementation to leverage CoLA's structural advantage to achieve superior memory saving without sacrificing throughput.

% From a pure conceptual perspective, our proposed method is not necessarily memory-efficient when implemented without practical considerations. This can be observed from the data-flow in Fig.~\ref{fig:block}. Specifically, CoLA has more intermediate results from each low-rank projection and the non-linear function right after it. However, this does NOT imply that CoLA cannot achieve memory efficiency. We will have an in-depth discussion on how much memory overhead CoLA imposes, and how this can be solved via system-level considerations when memory is of higher concerns. 


\subsection{Memory Breakdown in Pre-Training}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/bz-32-A100-memory-breakdown.png}
  \caption{Memory breakdown of pre-training LLaMA-1B on single GPU using different pre-training methods. Activation is at dominance for most methods. }
  \label{fig:gcp-memory-break}
  \vspace{-10pt}
\end{figure}

We assume a common notion that training modern transformers with Adam (or AdamW) involves four key memory components \cite{zhao2024galore, han2024sltrain}: model parameters, gradients, optimizer states, and activations (intermediate forward-pass results). Gradients consume \(1\times\) model size, Adam consumes \(2\times\), and activations typically consume \(1\sim 4\times\), depending on the batch size.

We focus on the scenario where the memory cost determined by the model size is not in the extreme limit of the GPU. We argue that this is rather realistic, since the model size and the minimum required tokens should scale up simultaneously during pre-training \cite{kaplan2020scaling, hoffmann2022training, krajewski2024scaling, kumar2024scaling}. A tiny batch size on a single GPU would be impractical. Therefore, we analyze memory usage on a 40-GB A100 or a 94-GB H100 GPU with a fairly large sequence batch size. Fig.~\ref{fig:memory-break} shows that activations dominate memory usage in this setup. Although our method overall consumes less memory than full-rank training (largely due to parameter efficiency), vanilla CoLA allocates more memory to activations. However, we argue that our unique architecture can significantly enhance existing memory-saving techniques and consume only a fraction of the original cost.

\subsection{CoLA Enables Efficient Checkpointing}
\label{sec:cola-m-efficiency}



\begin{table}[t]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c}
\toprule
\textbf{Methods} & \textbf{Memory} & \textbf{Re-Compute} \\
\midrule
Full-Rank & \( 20nd + 2n^2h \) & N/A \\
\midrule
Vanilla GCP & \( nd \) & \( 23nd^2 + 4n^2d \) \\
\midrule
CoLA & \( 17.5nd + 2n^2h + 14nr \) & N/A \\
\midrule
CoLA-M & \( 2nd + 7nr \) & \( 18.5ndr + 4n^2d \) \\
\bottomrule
\end{tabular}%
}
\caption{Memory and re-computation analysis of full-rank training with vanilla GCP vs. CoLA and CoLA-M.}
\label{tab:compute-memory-gcp}
\vspace{-5pt}
\end{table}

\begin{table}[t]
\centering
\small
\begin{tabular}{c|c|c}
\toprule
\textbf{Methods} & \textbf{Memory Saving} & \textbf{Re-Compute} \\ 
\midrule
\makecell{LLAMA-1B w/ \\ vanilla GCP} & 20.25 GB & $1\times$ \\
\midrule
CoLA-M-1B & 18.94 GB & $0.22\times$ \\
\bottomrule
\end{tabular}
\caption{Quantitative comparison of memory saving and re-computation between vanilla GCP and CoLA-M.}
\vspace{-10pt}
\label{tab:cola-m-vs-gcp}
\end{table}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/cola-m-vs-gcp.png}
  \caption{We show how memory reduction scales with the re-computation in full-rank training with GCP and compare with CoLA-M. With similar gains on memory efficiency, CoLA-M effectively reduces re-compute by $4.6\times$, enabling compute efficient checkpointing.}
  \label{fig:cola-m-vs-gcp}
  \vspace{-10pt}
\end{figure}

\begin{table*}[t]
\centering
\small
\caption{Comparison across various efficient pre-training methods of validation perplexity (PPL ($\downarrow$)), number of parameters in millions (Param), and the estimated memory usage (Mem) including model, gradient and optimizer states based on BF16 precision. We pre-train LLaMA models from 60M to 1B on the C4 dataset \cite{raffel2020exploring} following the same setup and compare results directly against those reported in \cite{zhao2024galore, han2024sltrain}.}
\label{tab:main-results}
\resizebox{\linewidth}{!}{%
\begin{tabular}{l|ccc|ccc|ccc|ccc}
\toprule
& \multicolumn{3}{c|}{60M} & \multicolumn{3}{c|}{130M} & \multicolumn{3}{c|}{350M} & \multicolumn{3}{c}{1B} \\
\midrule
\multicolumn{1}{l|}{\textit{r / d}} 
    & \multicolumn{3}{c|}{128 / 512}
    & \multicolumn{3}{c|}{256 / 768}
    & \multicolumn{3}{c|}{256 / 1024}
    & \multicolumn{3}{c}{512 / 2048} \\
\multicolumn{1}{l|}{\textit{Tokens}} 
    & \multicolumn{3}{c|}{1.1B}
    & \multicolumn{3}{c|}{2.2B}
    & \multicolumn{3}{c|}{6.4B}
    & \multicolumn{3}{c}{13.1B} \\
\midrule
& PPL & Param (M) & Mem (GB) & PPL & Param (M) & Mem (GB) & PPL & Param (M) & Mem (GB) & PPL & Param (M) & Mem (GB) \\
\midrule
Full-rank & 34.06 & 58 & 0.43 & \textbf{24.36} & 134 & 1.00 & \textbf{18.80} & 368 & 2.74 & 15.56 & 1339 & 9.98 \\
ReLoRA & 37.04 & 58 &  0.37 & 29.37 & 134 & 0.86 & 29.08 & 368 & 1.94 & 18.33 & 1339 & 6.79 \\
GaLore & 34.88 &  58 & 0.36 & 25.36 & 134 & 0.79 & 18.95 & 368 & 1.90 & 15.64 & 1339 & 6.60 \\
SLTrain & 34.15 & 44 & 0.32 & 26.04 & 97 & 0.72 & 19.42 & 194 & 1.45 & 16.14 & 646 & 4.81 \\
\midrule
CoLA  & {\bf 34.04} & {\bf 43} &  {\bf 0.32} & {24.48} & {\bf 94} &  {\bf 0.70}  & {19.40} & {\bf 185}  &  {\bf 1.38}  & {\bf 15.52} & {\bf 609}  & {\bf 4.54} \\
\bottomrule
\end{tabular}%
\vspace{-10pt}
}

\end{table*}

Gradient checkpointing (GCP) \cite{chen2016training} is a system-level technique that reduces memory usage by selectively storing (“checkpointing”) only a subset of intermediate activations during the forward pass. When the backward pass begins, the missing activations are recomputed on the fly instead of being stored in memory, thereby lowering the memory cost. A vanilla (also the most effective) implementation of GCP in LLM pre-training is to save merely the input and output of each transformer block, and re-compute everything within each block during the backward step. Some works have investigated the optimal selection of checkpoints through both empirical and compiler view \cite{feng2021optimal, he2023transcending}. Such techniques can also be developed for CoLA, and are beyond the scope of this paper.

Motivated by the bottleneck structure of CoLA, we implement CoLA-M as saving only the low-rank activations (red circles in Fig.~\ref{fig:block}), and re-compute the up projections, and (if applicable) the self-attention (painted in sketch in Fig.~\ref{fig:block}) during the backward pass. This reduces the re-computation cost to half of the CoLA forward. We analyze the memory and re-computation cost using the same notions as in Section~\ref{sec:compute-eff} and denote $h$ as the number of attention heads. We further simplify the analysis under LLaMA architecture by uniformly assuming $d_{\text{ff}} \approx 2.5d$. The memory and re-computation overhead are shown in Table~\ref{tab:compute-memory-gcp}. We refer the detailed analysis to Appendix~\ref{sec:appendix-mem-analysis}.

Although delicate optimizations of GCP is beyond our scope, we show in Table~\ref{tab:cola-m-vs-gcp} and in Fig.~\ref{fig:cola-m-vs-gcp} the quantitative results and scaling behavior of GCP on LLaMA-1B when applying a heuristic checkpointing strategy. We observe that CoLA-M greatly reduces re-computation cost while achieving similar memory saving as vanilla GCP.

\section{Experiments}

\begin{table}[t]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c|c|c}
\toprule
& Mem (GB) & 10k & 40k & 65k & 80k \\
\midrule
8-bit Adam & 72.59 & N/A & 18.09 & N/A & 15.47  \\
\midrule
8-bit GaLore & 65.16 & 26.87 & 17.94 & N/A & 15.39  \\
\midrule
SLTrain & 60.91 & 27.59 & \multicolumn{3}{c}{N/A} \\
\midrule
CoLA-M & {\bf 28.82} & {\bf 22.76} & {\bf 16.21} & {\bf 14.59} & {\bf 13.82} \\
\bottomrule
\end{tabular}%
}
\caption{Validation perplexity of LLaMA-7B pre-trained on C4 dataset. 8-bit Adam/GaLore are collected from \cite{zhao2024galore}. SLTrain is collected from \cite{han2024sltrain}. No results of BF16 Adam reported. We emphasize that CoLA outperforms other methods at 150k steps (14.61 \& 14.65) by using only 65k steps.}
\label{tab:result-7b}
\vspace{-10pt}
\end{table}

\subsection{Pre-Training LLaMA on C4}
We validate our proposed methods by extensively pre-training LLaMA-like LLMs from 60M to 7B scales. Experiments were performed on NVIDIA A100/H100 GPUs. We closely follow the experiment settings of \cite{zhao2024galore, han2024sltrain}, and directly compare CoLA with their reported results. We use the C4 dataset \cite{raffel2020exploring}, which is a colossal, cleaned version of Common Crawl's web crawl corpus. C4 dataset has been widely used for pre-training LLMs. Trainings were done without data repetition on a sufficiently large amount of tokens. We compare CoLA with baselines including {\bf full-rank} pre-training, {\bf ReLoRA} \cite{hu2021lora}, \textbf{GaLore} \cite{zhao2024galore}, and \textbf{SLTrain} \cite{han2024sltrain}, with a focus on methods that explore model efficiency.

We implement CoLA and its memory efficient variant CoLA-M by parameterizing all linear layers into the proposed linear-nonlinear-linear composition [i.e. Eq.~\eqref{eq:main-fwd}], and keep all other parameters and operations unchanged. We use AdamW optimizer and cosine annealing learning rate scheduler \cite{loshchilov2016sgdr} with warm-up. We remark that CoLA is NO more sensitive to optimizer-related hyper-parameters. We refer more details to Appendix~\ref{sec:appendix-hyper-param}.

Table~\ref{tab:main-results} compares our methods and other efficient pre-training techniques in terms of validation perplexity, parameter size, and estimated memory usage of model, gradients and optimizer states. CoLA has the smallest model size, thereby consumes the least memory, and demonstrates on-par validation perplexity compared to the full-rank baselines. Table~\ref{tab:result-7b} compares the validation perplexity on the 7B model. We highlight the fact that CoLA outperforms 8-bit Adam/GaLore at their 150k steps (14.61 \& 14.65) using only 65k steps. Meanwhile the total memory cost of CoLA-M is less than half of the other methods. We also emphasize our comparison with GaLore and SLTrain. Our proposed method has an overall better performance, whilst further reduces model size on top of the reduction of SLTrain. More importantly, we follow our discussion in Section~\ref{sec:compute-eff} and remark that CoLA has uniformly fewer computations than GaLore and SLTrain when applying the same rank. 
% \al{This is a placeholder, waiting for 7B results to add, so far the performance is beyond our expectation. Refer to Table.~\ref{tab:result-7b}.}

% This is because the full-rank training is the compute lower-bound of these two methods, regardless of their rank choices. On the other hand, CoLA reduces compute from full-rank training when $r$ is smaller than $\sim 0.62d$, which is satisfied in all these experiments as ranks are set to about one quarter of $d$. We will have a deeper view of CoLA's compute efficiency along with CoLA-M's memory efficiency via discussing their scaling behavior and system-level measurements.

\subsection{Scaling Behavior}

\begin{table}[t]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|cc|cc|cc}
\toprule
& \multicolumn{2}{c|}{60M} & \multicolumn{2}{c|}{130M} & 
\multicolumn{2}{c}{350M} \\
\cmidrule{2-7}
& PPL & FLOPs & PPL & FLOPs & PPL & FLOPs \\
\midrule
Full-Rank & 34.06 & $1\times$ & 24.36 & $1\times$ & 18.80 & $1\times$ \\
\midrule
Control & 37.73 & $0.4\times$ & 27.05 & $0.5\times$ & 20.53 & $ 0.4\times$  \\
\midrule
\multirow{2}{*}{CoLA} & 34.04 & $0.4\times$ & 24.48 & $0.5\times$ & 19.40 & $0.4\times$ \\
% \cmidrule{2-7}
& {\bf 31.52} & $0.7\times$ & {\bf 23.97} & $0.7\times$ & {\bf 18.32} & $0.7\times$ \\
\bottomrule
\end{tabular}%
}
\caption{Scaling behavior of CoLA and full-rank training. Control represents scaling down the full-rank training cost to be similar with CoLA in default, by reducing number of layers and/or size down model width.}
\label{tab:cola-scaling-up}
\vspace{-10pt}
\end{table}


We briefly discuss how CoLA might scale differently compared to full-rank training. A comprehensive investigation of this topic is beyond the scope of this work due to the huge computing cost. 

Table~\ref{tab:cola-scaling-up} shows a few experiments on how CoLA might be improved when computing is scaled up. The default rank choice reduces the compute cost to about a half of full-rank training, without significantly hurting the model performance. Meanwhile, if we relax the computing restriction and increase the rank to be greater than one quarter of $d$, then CoLA outperforms full-rank training in all three scales, while still being able to reduce the computing cost. One might argue that full-rank training can also be scaled down to a similar compute of CoLA and might perform similarly. We implement such baselines in Table~\ref{tab:cola-scaling-up} and refer this setup to ``Control". We typically reduce the number of layers or the model width of full-rank models to scale down their computing cost. We find empirically that they will tend to reduce performance quickly and dramatically underperform CoLA.


\subsection{System Measurements}

\begin{table}[t]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{c|ccc|ccc}
\toprule
% \textbf{Methods} & \textbf{Memory Saving} & \textbf{FLOPs} \\ 
% \midrule
& \multicolumn{3}{c|}{1B (BZ = 64)} & 
\multicolumn{3}{c}{7B (BZ = 16)} \\
\cmidrule{2-7}
& Mem (GB) & Throughput & FLOPs & Mem (GB) & Throughput & FLOPs\\
\midrule
Full-Rank & 69.84 & 12,365 & $1\times$ & 84.94 & 5,810 & $1\times$ \\
\midrule
Vanilla GCP & {\bf 14.89} & 8,799 & $1.68\times$ & 52.49 & 4,357 & $1.67\times$ \\
\midrule 
CoLA & 66.46 & {\bf 22,979} & $\bf 0.40\pmb{\times}$ & 55.52 & {\bf 9,638} & $\bf 0.40\pmb{\times}$ \\
\midrule
{CoLA-M} & 17.33 & 16,617 & $0.55\times$ & {\bf 26.82} & 7,026 &$0.54\times$ \\
\bottomrule
\end{tabular}%
}
\caption{Detailed measurements and comparison of CoLA and CoLA-M against full-rank and vanilla GCP on a 94 GB H100 GPU. CoLA-M consumes only one third of the memory while achieving higher throughput than full-rank training with only about half its compute.}
\label{tab:cola-gcp-mem-flops}
\vspace{-5pt}
\end{table}

\begin{figure}[t]
  \includegraphics[width=\columnwidth]{figures/bz-16-A100-throughput.png}
  \vspace{-10pt}
  \caption{Comparison of throughput measured when pre-training a LLaMA-1B on a 40 GB A100 GPU with sequence batch size of 16 for different methods. }
  \label{fig:throughput}
  \vspace{-10pt}
\end{figure}

We further investigate the efficiency of CoLA from a more practical perspective. It is often observed that a theoretically less expensive method can have worse system-level performance due to poorly designed hardware implementation or lack of system-aware optimizations. We show that this is NOT the case for CoLA, by illustrating that its out-of-the-box system performance already significantly outperforms the full-rank training and other efficient training methods. We focus on the actual memory usage and the pre-training throughput.

Fig.~\ref{fig:throughput} shows the measured throughput for pre-training the LLaMA/CoLA 1B model. The sequence batch size is set as 16, which fully utilizes the A100 GPU. Among all these methods, CoLA and CoLA-M are the only two that show higher GPU throughput than the full-rank baseline, while all other methods downgrade throughput due to their computing overhead. In particular, CoLA-M, the memory-efficient CoLA that significantly reduces overall GPU memory, still shows higher training throughput despite the re-computation overhead. Meanwhile, vanilla GCP, which uses a similar idea of trading compute for memory, reduces throughput from the full-rank baseline by 26\%. We show further the details of these measurements in Table~\ref{tab:cola-gcp-mem-flops}, and also compare their estimated FLOPs. At both 1B and 7B scale, CoLA-M manages to almost halve the computing cost and reduce the memory usage by two thirds, achieving a great balance between computing and memory efficiency. We refer more details of our system profiling to Appendix.~\ref{sec:appendix-detailed-experiment-setting}.

\subsection{Inference Performance}
\begin{table}[t]
\centering
\small
\resizebox{0.85\linewidth}{!}{%
\begin{tabular}{c|cc|cc}
\toprule
% \textbf{Methods} & \textbf{Memory Saving} & \textbf{FLOPs} \\ 
% \midrule
& \multicolumn{2}{c|}{1B (BZ=32)} & 
\multicolumn{2}{c}{7B (BZ=32)} \\
\cmidrule{2-5}
& Mem (GB) & Throughput & Mem (GB) & Throughput\\
\midrule
Full-rank & 5.74 & 21,109 & 18.15 & 11,086 \\
\midrule
SLTrain & 4.18 & 20,096 & 12.70 & 9,968 \\
\midrule
CoLA & {\bf 3.84} & {\bf 34,697} & {\bf 10.87} & {\bf 16,012} \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of memory (GB) and throughput (Token/sec) at inference time on an A100 GPU.}
\label{tab:inference}
\vspace{-10pt}
\end{table}

We highlight the fact that CoLA not only reduces pre-training resources but also speeds up inference and reduces its memory cost. Table~\ref{tab:inference} shows that CoLA improves inference throughput by up to $\bf 1.64\pmb{\times}$ while reducing memory cost by up to $\bf 1.67\pmb{\times}$.


% 1. Table.: memory decomposition → large bz, H100

% 2. Figure: fwd+bwd speedup → fixed large bz, A100/H100, SVD and full-rank: focus on training sys time

% 3. Figure: overall GPU hours → SVD and full-rank, maximum bz, hardware selection depends on performance: focus on throughput (tokens/sec)

\section{Conclusions}
We propose CoLA, and its memory efficient variant CoLA-M, to achieve collectively parameter, compute and memory efficiency at both pre-training and inference time for large foundation models. CoLA effectively reduces $2\times$ model size while preserving full-rank level performance. More importantly, we show the reduction does not come with additional compute. Instead, CoLA halves compute and almost doubles training throughput from its full-rank counterpart. When memory is of higher concerns, CoLA-M trades only minimum compute for state-of-the-art memory reduction during pre-training, meanwhile still reducing compute and improving throughput. We hope our work will inspire the community to further investigate the architecture efficiency that has been overlooked and under-discovered for large foundation models.

\section{Limitations}
This work limits the study of our proposed formulation under LLaMA-like architectures. The adaptation of CoLA to other architectures is conceptually trivial, but their performance is to be evaluated via real experiments. In this work, we only pre-train each model to be roughly compute-optimal (for original LLaMA models, not CoLA), while industry-produced LLMs (that are of similar scales) are often over-trained. It is worth investigating the performance of CoLA when significantly over-trained. We leave this computing-expensive research for future work.

\section*{Acknowledgments}

This material is based upon work supported by
the U.S.\ Department of Energy, Office of Science, Office of Advanced
Scientific Computing Research, Artificial Intelligence for Science program, under contracts DE-SC0025390 and DE-AC02-06CH11357.

This research used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility
supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231 using NERSC award ASCR-ERCAP0030039, as well as NERSC award ALCC-ERCAP0031379.


\bibliography{main}

\appendix

\section{Observation of Low-Rank Activation in Pre-Trained GPT2}
In this section, we further show the low-rank structure in model activations evaluated on a pre-trained GPT-2 \cite{radford2019language} small. The evaluation is conducted with sequence batch size of 64 and sequence length of 1024. We fix $\alpha = 0.95$ throughout this section. Similar patterns are observed from the attention layers (Fig.~\ref{fig:Q_Spectrum},~\ref{fig:K_Spectrum},~\ref{fig:V_Spectrum}). The low-rank nature of activations is evident across all the different components of the model. This suggests that despite the high-dimensional representations, the effective dimensionality of the activations remains constrained.
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/Q_Spectrum.png}
    \caption{Activation Spectrum of Attention Layer (Q)}
    \label{fig:Q_Spectrum}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/K_Spectrum.png}
    \caption{Activation Spectrum of Attention Layer (K)}
    \label{fig:K_Spectrum}
\end{figure}
\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{figures/V_Spectrum.png}
    \caption{Activation Spectrum of Attention Layer (V)}
    \label{fig:V_Spectrum}
\end{figure}
\label{sec:appendix-low-rank-activation}

\section{Detailed Compute Analysis}
\label{sec:appendix-compute-analysis}
According to Table.~\ref{tab:compute-breakdown-llama}, the total compute of full-rank training is simply combining forward and backward as 
\begin{equation}
    C_{\text{Full-Rank}} = 24nd^2 + 12n^2d + 18ndd_{\text{ff}}.
    \label{eq:full-rank-compute}
\end{equation}

In our proposed architecture, every single linear layer is replaced by low rank matrices $\mat{A}$,  $\mat{B}$, and an activation function sandwiched in between. The activation only introduces trivial compute thus can be omitted in the calculation. For each $d^2$ and $dd_{\text{ff}}$ in Eq.~\eqref{eq:full-rank-compute}, CoLA effectively converts them into $2dr$ and $r(d+d_{\text{ff}})$. Therefore the total compute of CoLA is
\begin{equation}
C_{\text{CoLA}} = 48ndr + 12n^2d + 18nr(d + d_{\text{ff}})
    \label{eq:cola-compute}.
\end{equation}
Plugging in an actual setting of LLaMA/CoLA-1B, in which $r = \frac{1}{4}d$ and $r \approx \frac{1}{10}d_{\text{ff}}$, we achieve a compute reduction from Eq.~\eqref{eq:full-rank-compute} to approximately
\begin{equation}
C_{\text{CoLA-1B}} = 16.5nd^2 + 12n^2d + 1.8ndd_{\text{ff}}.
\end{equation}

We now discuss and compare CoLA with other efficient pre-training methods in terms of their compute complexity. We start with LoRA \cite{hu2021lora} and ReLoRA \cite{lialin2023relora}. They share the same architecture that's shown in Fig.~\ref{fig:arc} a), in which low rank matrices $\mat{A}\in \mathbb{R}^{r \times d_{\text{in}}} $ and $\mat{B}\in \mathbb{R}^{d_{\text{out}} \times r} $ are adapted onto a full rank matrix $\mat{W}_0 \in \mathbb{R}^{d_{\text{out}}\times d_{\text{in}}}$. Hence modifies Eq.~\eqref{eq:full-rank-fwd} into
\begin{equation}
    \mat{h} = \mat{W}_0\mat{x} + \mat{BAx}.
    \label{eq:lora-fwd}
\end{equation}
This yields a consistently more expensive forward step than the full-rank training regardless the choice of $r$. During the backward step, since gradient does not flow into $\mat{W}_0$, only one GEMM that computes gradient w.r.t $\mat{x}$ is involved with the full-rank component $\mat{W}_0\mat{x}$. Combining together both full-rank and low-rank components in both forward and backward step, the total compute of LoRA is
\begin{multline}
    C_{\text{LoRA}} = 16nd^2 + 12n^2d + 12ndd_{\text{ff}} \\ + \underbrace{48ndr + 18nr(d + d_{\text{ff}})}_{C_{\text{CoLA}}}.
    \label{eq:lora-compute}
\end{multline}
When choosing the same $r$ for LoRA and CoLA, we have $C_{\text{LoRA}} > C_{\text{CoLA}} $ always true.

In ReLoRA \cite{lialin2023relora}, the hybrid strategy that warms up with the full-rank training arises more uncertainties in analyzing its complexity. And such strategy needs delicate tuning of hyper-parameters such as the full rank warm-up ratio, the restart frequency of optimizer, etc, and the choice of rank might also be affected by these strategy-level hyper-parameters. Therefore, we follow the same notion in \cite{zhao2024galore} that only consider the pure low-rank training of ReLoRA, which simplifies the compute analysis of ReLoRA to be the same as LoRA.

SLTrain \cite{han2024sltrain} proposes a low-rank + sparse parameterization  instead of having a fixed full-rank matrix $\mat{W}_0$. The architecture of SLTrain is shown in Fig.~\ref{fig:arc} c). We continue using the notation for the low-rank matrices, and denote the sparse matrix as $\mat{S}$, with the sparsity level as $\delta$. This modifies Eq.~\eqref{eq:full-rank-fwd} into
\begin{equation}
    \mat{h} = \mat{BAx} + \mat{Sx} = (\mat{BA} \oplus_{\mathcal{I}} \mathcal{V})\mat{x},
    \label{eq:sltrain-fwd}
\end{equation}
where $\oplus$ denotes the scatter-add operator, $\mathcal{I}$ and $\mathcal{V}$ denote the indices and values of non-zero elements in $\mat{S}$. This implementation avoids instantiating a full sized $\mat{S}$, instead keeping only the non-zero elements. However, this introduces non-trivial reconstruction cost of $\mat{BA}$ in every step. And if we further denote $\tilde{\mat{W}} = \mat{BA} \oplus_{\mathcal{I}} \mathcal{V}$, then the forward data-flow that starts from $\tilde{\mat{W}}$ is the same as in the full-rank training, as well as the backward data-flow that ends at $\tilde{\mat{W}}$. Therefore, the total compute of SLTrain should be $C_{\text{full-rank}}$ plus reconstructing $\tilde{\mat{W}}$, and its corresponding $2\times$ compute during backward, i.e.,
\begin{equation}
C_{\text{SLTrain}} = C_{\text{full-rank}} + 24d^2r + 18dd_{\text{ff}}r.
\end{equation}

For the last class of method to discuss, GaLore \cite{zhao2024galore} and it's follow-ups such as Fira \cite{chen2024fira} and APOLLO \cite{zhu2024apollo}, all investigate the memory efficiency associated with the AdamW optimizer. We only show the data-flow GaLore in Fig.~\ref{fig:arc} b), others are similar except some minor differences in how to manipulate gradients. The model architecture is kept unchanged in all these methods. Therefore, the complexity analysis is on the additional compute for projecting gradients into a low-rank space. GaLore proposes the following update rules:
\begin{equation}
\begin{aligned}
     \mat{R}_t &= \mat{P}_t^T \mat{G}_t, \tilde{\mat{G}}_t = \alpha\cdot \mat{PN}_t, \\
    \mat{W}_t &= \mat{W}_{t-1} + \eta \cdot \tilde{\mat{G}}_t,
\end{aligned}
\end{equation}
where the projector $\mat{P}_t\in \mathbb{R}^{d\times r}$ at time $t$ is computed by decomposing $\mat{G}_t\in \mathbb{R}^{d\times d}$ via singular value decomposition (SVD) and is updated periodically, $\mat{N}_t \in \mathbb{R}^{d\times r}$ is the low-rank optimizer states, $\alpha$ is a scaling factor and $\eta$ is the learning rate. Therefore, the total compute of GaLore is
\begin{equation}
    C_{\text{GaLore}} = C_{\text{full-rank}} + 16d^2r + 12dd_{\text{ff}}r.
\end{equation}

We remark that the compute analysis for the additional cost of SLTrain and GaLore (and its variants) is of limited scope and does not necessarily reflect their actual overhead. The actual cost will be dependent on other practical considerations on both algorithm and system level, such as the specific use case of these methods (e.g., pre-training, fine-tuning, etc), the actual number of the optimizer steps performed, the actual number of forward and backward steps performed when fixing total training tokens (i.e., if the hardware can afford larger batch sizes then the actual steps are fewer). It is almost impossible to give a unified notion while being fair when comparing between them. Hence we follow the similar setup used in \cite{zhao2024galore, han2024sltrain, chen2024fira, zhu2024apollo} when they analyze memory efficiency and measure system-level performance. However, it is rather safe to conclude that the overall cost introduced by GaLore and its variants will be diluted in real practices of pre-training due to the optimizer step is not frequent as forward and backward steps, hence are less expensive than SLTrain. Nonetheless, we highlight the fact that all the aforementioned methods are non-trivially more expensive than CoLA in terms of compute, and are all (except LoRA/ReLoRA) lower bounded by the full-rank training.

\section{Detailed Memory Analysis}
\label{sec:appendix-mem-analysis}

We continue using the notions defined in Section. \ref{sec:cola-m-efficiency} and start with the activation memory of full-rank training:
\begin{multline}
M_{\text{full-rank}} = \underbrace{3nd}_{\mat{Q}, \mat{K}, \mat{V}} + \underbrace{2n^2h + 2nd}_{\text{attention}} + \underbrace{11nd}_{\text{ffw}} \\
\underbrace{2nd}_{\text{residual connection}} + \underbrace{2nd}_{\text{layer norm}} = 20nd + 2n^2h.
\label{eq:full-rank-act-mem}
\end{multline}
When applying vanilla GCP, only the output of each block is saved, and all other activations are re-computed when needed. This dramatically reduces the total activation memory to only
\begin{equation}
    M_{\text{vanilla-GCP}} = nd.
\end{equation}
However, such benefit comes with a cost equal to almost an entire forward step. From Table. \ref{tab:compute-breakdown-llama}, we have the cost of vanilla-GCP as
\begin{equation}
    C_{\text{vanilla-GCP}} = C_{\text{full-rank}} +  23nd^2 + 4n^2d.
\end{equation}
Although we mentioned that delicate optimization of vanilla-GCP is beyond the scope of our discussion, we show a heuristic strategy when selecting checkpoints. Refer to Eq.~\eqref{eq:full-rank-act-mem}, activations that associated with minimal re-compute are: layer norm, residual connection, and non-linear function (included in the ffw term). Then intuitively these activations should always be re-computed when trying to save memory. In fact this can save a fair amount of memory. Note in this paper we analyze compute in pure theoretical notion that lower order terms does not bring noticeable effect hence are omitted. In practice, however, re-computation brings latency even for theoretically trivial operations, and will lower the overall GPU throughput. Other terms in Eq.~\eqref{eq:full-rank-act-mem} are all significant components when mapping to FLOPs change. One can gradually add more operations into the re-compute list and trade for more memory savings. We show the trend how they scale in Fig.~\ref{fig:cola-m-vs-gcp}.

Now we discuss CoLA and how it enables compute efficient checkpointing. We first evaluate how much memory overhead introduced by the low-rank activations. Compared to Eq.~\eqref{eq:full-rank-act-mem}, CoLA adds $2nr$ for each of the low-rank layers, i.e., $nr$ for $\mat{Ax}$, another $nr$ for $\sigma(\mat{Ax})$, thereby 
\begin{multline}
M_{\text{CoLA}} = M_{\text{full-rank}} + \underbrace{14nr}_{\text{low-rank }\sigma} - \underbrace{2.5nd}_{\text{remove original }\sigma}
\end{multline}
We notice that when model scales up, the original LLaMA activation no longer brings benefit to model performance, hence can be removed, which corresponds to $2.5nd$ less activations.

As shown in Figure. \ref{fig:block}, 
CoLA has multiple non-linear functions injected along the normal data-flow. This partitions the previously longer path, i.e., the whole block, to significantly shorter paths bounded by these low-rank activations. This provides a natural selection of checkpoints that are of $r$-dimensional instead of $d$. More importantly, these shorter paths halve the re-compute steps. We show in Figure. \ref{fig:block} that only the weights that are painted in sketch need re-computation during the backward step of CoLA-M. This reduces significantly the cost of implementing GCP in CoLA-like architecture, results in the cost of only
\begin{equation}
    C_{\text{CoLA-M}} = C_{\text{CoLA}} + 18.5ndr + 4n^2d.
\end{equation}
Meanwhile, the memory saving of CoLA-M is still significant. We have the activation memory of CoLA-M as
\begin{equation}
    M_{\text{CoLA-M}} = 2nd + 7nr.
\end{equation}

\section{Hyper-Parameters}
\label{sec:appendix-hyper-param}
For optimizer related hyper-parameters, we empirically found 0.003 is a balanced choice of learning rate for most of the models we trained, this is similar to the settings in \cite{han2024sltrain}. For CoLA-1B, this learning rate triggers a unstable loss curve, thereby is reduced to 0.002, and is further reduced to 0.001 for CoLA-7B as a conservative practice. For smaller models like CoLA-60M, an even larger learning rate such 0.006 can be adopted. For the warm-up ratio, weight decay and gradient clipping, we found the commonly adopted settings, 0.1, 0.01, 0.5, are proper choices for CoLA. Other than the standard optimizer parameters, one needs to pre-define a rank $r$ when initializing CoLA. A default choice is set to approximately one quarter of the model inner width, i.e., $r=\frac{1}{4}d$. 

\section{Ablation Study}
\begin{table}[t]
\centering
\small
% \resizebox{\linewidth}{!}{%
\begin{tabular}{c|c|c|c}
\toprule
& 60M & 130M & 350M \\
\midrule
CoLA w/ Both $\sigma$ & {\bf 34.04} & {\bf 24.48} & 19.56 \\
\midrule
CoLA w/ Only Low-Rank $\sigma$ & 34.35 & 25.20 & {\bf 19.40} \\
\midrule
\makecell[c]{CoLA w/ Only Low-Rank $\sigma$ \\ -- Reduced} & 35.41 & 25.90 & 20.50 \\
\midrule
CoLA w/ Only Full-Rank $\sigma$ & 36.26 & 26.85 & 21.18 \\
\bottomrule
\end{tabular}%
% }
\caption{Ablation study regarding where to place the low-rank non-linear functions.}
\label{tab:result-ablation}
\end{table}

\label{sec:appendix-ablation}
We empirically found that keeping the original LLaMA nonlinearity on top of our proposed formulation Eq.~\eqref{eq:main-fwd} helps improve the model performance at smaller scales, such as 60M and 130M. However, when scaling up to 350M we no longer observe such a benefit. Therefore, the default setting of pre-training CoLA-1B/7B is set to use only low-rank nonlinearity. We found also evident that applying low-rank nonlinearity (i.e., Eq.~\eqref{eq:main-fwd}) regardless of whether the original linear layer being followed by nonlinearity is crucial to boost model performance. Results are shown in Table.~\ref{tab:result-ablation}, in which "CoLA w/ Both $\sigma$" means keeping the original nonlinearity on top of proposed low-rank nonlinearity, "CoLA w/ Only Low-Rank $\sigma$" means applying Eq.~\eqref{eq:main-fwd} in an agnostic way to all linear layers, "CoLA w/ Only Low-Rank $\sigma$ -- Reduced" means only applying Eq.~\eqref{eq:main-fwd} to the linear layers that are originally followed by nonlinearity, "CoLA w/ Only Full-Rank $\sigma$" means keeping the low-rank factorization but does not apply low-rank nonlinearity.

\section{Detailed Profiling Setting}
\label{sec:appendix-detailed-experiment-setting}
This section provides a detailed explanation of the experimental setup for system-level measurements. For the memory breakdown in Fig.~\ref{fig:gcp-memory-break}, we use a sequence batch size of 32. For throughput measurement in Fig.~\ref{fig:throughput}, we use a sequence batch size of 16 because the full-rank model cannot fit into 40GB A100 when using a sequence batch size of 32. Throughput is measured incorporating one forward pass, one backward pass, and one optimizer step. This setup reflects a realistic training scenario, particularly in a multi-GPU environment, such as an 8x A100 cluster utilizing simple data parallelism. For a fair comparison, we set the update step in GaLore/APOLLO to 200, ensuring that the computationally expensive SVD/random projection is performed only once every 200 optimizer steps and is distributed across a single optimizer step. All experiments are conducted on a single GPU to isolate the effected of FLOP reduction on throughput improvement, without being influenced by multi-GPU framework settings or communication overhead. For Table.~\ref{tab:result-7b}, memory consumption is measured on a 94GB H100 with a sequence batch size of 16. For Table.~\ref{tab:inference}, inference is performed using the same configuration as pre-training, with a sequence batch size of 32.

\end{document}
