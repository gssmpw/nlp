We demonstrate that Masked Autoencoder and ViT approaches generalize directly to the domain of HAR without the need for domain-specific design choices. This is beneficial to take advantage of the findings resulting from advances in ViT in other domains, and is unlike what is often pursued in wearable and mobile computing which tend to propose ad-hoc architectures.

We establish the first known scaling laws for HAR Transformer models and validate their performance on 3 downstream HAR datasets (UCI HAR, WISDM Phone and WISDM Watch), with a large number of activity classes (18 in WISDM). We show that performance improves with a power-law relationship as data is increased, and that the parameters of the power-law depend directly on the diversity of the data being added. 
For example, we find that adding new users results in a power-law exponent that is 3x larger than adding more data from the same users. 

This contradicts the findings from  \cite{narayanswamy2024scalingwearablefoundationmodels} regarding increasing the diversity of data: our findings show this to be beneficial, while theirs suggest increasing volume without necessarily increasing diversity. 
This may be due to differences in their evaluations, and indicates the need for further investigation.
Indeed, we operated on raw sensor data (instead of their model operating on top of 10 statistical features extracted from raw data) and we evaluated performance on downstream task with a much larger number of activity classes (18 compared to 8 in their work).

Our evaluations differ from those in language and vision  by training to convergence in order to study the relationship between number of parameters and data under the unique conditions of HAR, where we are more constrained by data and inference compute than training compute.

We link model capacity directly to pre-training data by showing that larger models are required in order to take advantage of more pre-training data. In fact, even in the low data regime, we see evidence that it's often worth exploring over-parameterized Transformers. In short: when in doubt, increase the model capacity and train for longer, assuming sufficient resources.

We recommend revisiting previous works that may be under-parameterized. For example \cite{SSLUKbiobank} used the large UK Biobank dataset but fixed the encoder architecture with a 10M parameter ResNet. Masked Reconstruction \cite{HARmaskedreconstruction} was explored in \cite{assessingSSLHAR22} using the large Capture-24 \cite{capture24} dataset, but they fixed the Transformer encoder architecture at 1.5M parameters. We would expect these works to benefit from increasing the capacity to at least 30M parameters.

Even our largest model fits on a single TPUv2, and completes pre-training in under 12 TPU-days. Given this and our focus on public datasets, we believe our results are possible to replicate.
As we scale to larger models, it becomes less feasible to deploy these directly on-device. We recommend using large, pre-trained models as teachers that can be distilled and quantized to smaller, on-device student models.

Future work could verify further the existence of these scaling laws in other previously proposed models, such as \cite{Abedin20}, \cite{selfpab} and contrastive pre-training techniques such as \cite{simclr}. While our results were obtained from wearable motion sensor data, future work could also verify their existence when other sensor modalities are used, such as radar or WiFi which become increasingly more frequently explored in the field.
