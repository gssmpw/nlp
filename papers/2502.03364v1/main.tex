% This class has a lot of options, so please check deepmind.cls for more details.
% This is a minimal set for most needs.
\documentclass[11pt, a4paper, logo, twocolumn, copyright]{googledeepmind}

% Omit dates for reproducibility.
\pdfinfoomitdate 1
\pdftrailerid{redacted}

% This avoids duplicate hyperref bookmark entries when using \bibentry (e.g. via \citeas).
\makeatletter
\renewcommand\bibentry[1]{\nocite{#1}{\frenchspacing\@nameuse{BR@r@#1\@extra@b@citeb}}}
\makeatother

\usepackage{kantlipsum, lipsum}
\usepackage{dsfont}
\usepackage{gdm-colors}

\usepackage[]{mdframed}
\usepackage[capitalize,noabbrev]{cleveref}

% Sometimes you will get errors about pdflink ending up in diffrent position. Try this and
% comment it out again when you are done with your document.
%\hypersetup{draft}

% Set the bibliography options here.
\usepackage[authoryear, sort&compress, round]{natbib}

% Images will be looked for in this path, removes need for explicit path when including images.
\graphicspath{{figures/}}

% Important Information about your paper.
\title{Scaling laws in wearable human activity recognition}

% Can leave this option out if you do not wish to add a corresponding author.
\correspondingauthor{thoddes@google.com}

% Remove these if they are not needed
% \keywords{\LaTeX, Publications process, tools}
% \paperurl{arxiv.org/abs/123}

% Assign your own date to the report.
% Can comment out if not needed or leave blank if n/a.
\renewcommand{\today}{2025-01-31}

\author[1]{Tom Hoddes}
\author[*, 1]{Alex Bijamov}
\author[*, 1]{Saket Joshi}
\author[*,2,3]{Daniel Roggen}
\author[4,5]{Ali Etemad}
\author[2,6]{Robert Harle}
\author[1]{David Racz}

\affil[*]{Equal contributions}
\affil[1]{Google DeepMind}
\affil[2]{Google LLC}
\affil[3]{School of Engineering \& Informatics, University of Sussex, UK}
\affil[4]{Work done while at Google Research}
\affil[5]{Queen's University, Canada}
\affil[6]{Computer Laboratory, University of Cambridge, UK}

\begin{abstract}
Many  deep architectures and self-supervised pre-training techniques have been proposed for human activity recognition (HAR) from wearable multimodal sensors. 
Scaling laws have the potential to help move towards more principled design by linking model capacity with pre-training data volume.
Yet, scaling laws have not been established for HAR to the same extent as in language and vision. 
By conducting an exhaustive grid search on both amount of pre-training data and Transformer architectures, we establish the first known scaling laws for HAR. 
We show that pre-training loss scales with a power law relationship to amount of data and parameter count and that increasing the number of users in a dataset results in a steeper improvement in performance than increasing data per user, indicating that diversity of pre-training data is important, which contrasts to some previously reported findings in self-supervised HAR.
We show that these scaling laws translate to downstream performance improvements on three HAR benchmark datasets of postures, modes of locomotion and activities of daily living: UCI HAR and WISDM Phone and WISDM Watch. 
Finally, we suggest some previously published works should be revisited in light of these scaling laws with more adequate model capacities.
\end{abstract}

\begin{document}

\maketitle



% Incude paper content from external files
\section{Introduction}
\label{sec:introduction}
\input{introduction}





\section{Related Work}
\label{sec:related}
\input{related}

\section{Method}
\label{sec:method}


\subsection{Scaling Laws}
For our scaling laws, we take a different approach from that in language \cite{hoffmann2022chinchilla} \cite{kaplan2020scalinglawsneurallanguage} and vision \cite{zhai2022scalingvits}. 
In these domains, since data was abundant and compute was the primary constraint, they never completed a full epoch, and thus equated number of steps to amount of data. 
For HAR, however, data is the primary constraint, so we repeat data many times (over 100 epochs) until convergence. Since we are able to train even our largest models to convergence, we do not fix the amount of compute. 
Instead, we focus on the capacity of the models (number of parameters). 
This is more directly tied to inference cost than training, which aligns with the priorities of many HAR deployments.


\subsection{Encoder}
For the encoder backbone, we use a ViT \cite{vit} adapted for accelerometer and gyroscope motion sensors as shown in \cref{mae-architecture}. The input to the encoder consists of a time series window of 128 samples at 50Hz, where each sample has 6 channels (x, y, z for accelerometer and gyroscope). We break the window into ``patches" of 4 samples. We choose this patch size to be as small as possible while still fitting the attention matrix in memory. 
Each patch of shape (4, 6) is flattened and transformed linearly to an embedding of dimension size equal to 1/4 the width of the MLP.

We use a standard Transformer block with 8 attention heads. To determine the optimal encoder capacity (i.e. number of parameters) for a given data scale, we conduct a grid search of 3 different widths (512, 1024, 2048 hidden MLP units) and 3 different depths (5, 10, 20 blocks), resulting in 9 different models from 1M to 63M parameters.


\begin{figure}[htb]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=1\columnwidth]{HAR_Scaling_Paper_MAE_Diagram.png}}
\caption{Masked Autoencoder adapted for accelerometer and gyroscope. 
During pre-training, a random subset of accelerometer and gyroscope patches are masked out. Non-masked patches are passed to the encoder and the mask
tokens are re-introduced after the encoder. The encoded patches and mask tokens are then processed by a small decoder trained to reconstruct the original input sequence.
}
\label{mae-architecture}
\end{center}
% \vskip -0.2in
\end{figure}


\subsection{Pre-training}
Our pre-training approach follows Masked Autoencoder \cite{maskedautoencoder} closely, but adapted for accelerometer and gyroscope motion sensors as shown in \cref{mae-architecture}. This was chosen because it is simple to implement, scales well, and doesnâ€™t require negative examples or domain specific design choices such as augmentations (e.g. to prevent collapse in dual encoders).
We randomly mask whole patches rather than individual samples. 
We only encode unmasked patches and use a high masking ratio of 70\%. This saves considerably on compute costs.
We use a small decoder consisting of 2 Transformer blocks. We apply a linear projection between the encoder and decoder to decrease the width of the decoder by half.

\subsection{Evaluation}
To evaluate pre-trained encoders, we remove the decoder and use global average pooling to attach a linear classification head to the output of the encoder, which we keep frozen. We use linear evaluation as opposed to full fine-tuning to provide a clearer signal of the information extracted from pre-training alone.

\subsection{Datasets}
We use the Extrasensory dataset \cite{extrasensory2017} for pre-training. This dataset contains more than 300k examples of 20 seconds of sensor data from 60 users. Data has been collected while subjects were engaged in regular everyday behavior for several sensors including accelerometer, gyroscope and magnetometer across multiple phone and wearable devices. The dataset is pre-formatted into 5 folds split by user. Each fold is split into a training and a test set. After filtering for missing data we collected 286140 examples, which equates to approximately 1589 hours of data. 
We ignore the activity labels for pre-training. Within each fold, we vary the amount of data by sampling some percentage of examples. 
The USER sampling strategy takes all the data from a randomly selected percentage of the users in the fold.
The RANDOM sampling strategy puts the data of all users in the fold together and draws a random percentage of examples from that.
To control for non-uniform distribution between different sampling strategies and folds, we report results based on the total hours of pre-training data rather than by percent. We only use the phone data, since the watch data does not contain a gyroscope.

For downstream evaluation, we use popular benchmark datasets UCI HAR and WISDM Phone/Watch datasets. We use the full training dataset for all supervised training.
UCI HAR \cite{human_activity_recognition_using_smartphones_240} contains data from 30 volunteers aged 19-48 engaging in 6 modes of locomotion and postures: walking, walking upstairs, walking downstairs, sitting, standing, laying. The accelerometer and gyroscope data is recorded at 50Hz from a smartphone worn on the waist. 
We use the same random partitioning prescribed by the dataset authors (70\% training and 30\% test sets). We also keep the existing raw data preprocessing pipeline, involving noise filtering, 2.56sec sliding windows with 50\% overlap, resulting in 128 samples per window, and use the raw acceleration, not the low-pass filtered version also present in the dataset.

For WISDM we use the 2019 version of the dataset \cite{wisdm_smartphone_and_smartwatch_activity_and_biometrics_dataset__507} comprising 51 subjects performing 18 activities of daily living (postures, locomotion, house chores, nutrition, work-related activities and others) for 3 minutes each. 
We assign the first 2/3 of users to the training set (subjects 1600-1633), and use the remaining 1/3 for evaluation (subjects 1634-1650) similar to previous benchmarks. We split the WISDM dataset into Phone and Watch body positions and evaluate these separately.

We re-sample all datasets to 50Hz and normalize to the same units. None of the datasets contain a null class.

\subsection{Training schedule}
For every model, data combination, we fix the number of steps for pre-training to 500,000 with a batch size of 2048. This equates to over 100 epochs when using 100\% of the data. We use the Adam optimizer with three different learning rates (1e-3, 1e-4, 1e-5) for every model and take the best result. This ensures that each model has sufficient coverage of the parameter search space regardless of size. We apply dropout during pre-training with a rate of 0.1.

\subsection{Compute}
Our exhaustive grid search results in 1620 (3 learning rates * 6 data sizes * 2 sampling strategies * 5 folds * 9 encoder architectures) different hyperparameter combinations for pre-training. Each run takes between 3 and 35 hours to run on 4 TPUv2 chips with larger models running longer. Our total compute used for pre-training is about 62000 TPU-hours.





\begin{table}[htb]
\caption{Best F1 scores of models trained from scratch (FS) vs linear eval (LE) on pre-trained models for each dataset.} 
\label{f1-table}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Data set & FS & LE \\
\midrule
UCI HAR    & 95.1 & 97.9 \\
WISDM Phone    & 31.9 & 34.3 \\
WISDM Watch & 62.6 & 63.1 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
% \vskip -0.1in
\vspace{-1.5em}
\end{table}



\section{Results}

\subsection{Supervised training from scratch baseline}
For each dataset, we conduct a thorough capacity and hyperparameter search of from-scratch models to establish baselines for comparing with pre-trained models. The best results are listed in \cref{f1-table}. We also look at the effect of model capacity on from scratch training. We find that smaller models work better for UCI HAR, with our second smallest model of about 2M parameters performing best. The effect of capacity on WISDM is less clear. It also appears that deeper models perform better than wide models.

\begin{figure}[t]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{pretrain_eval_loss_data_law.png}}
\caption{Pre-training test loss vs data size (hours). We fit a power law to each fold and sampling strategy. Equations for each power law can be found in \cref{pretrain-data-law-table}.}
\label{pretrain-loss-data}
\end{center}
% \vskip -0.2in
\end{figure}

\begin{table}[t]
\caption{Power laws of pre-training test loss vs data size. Exponent values for the USER sampling strategy are roughly 3 times greater than RANDOM.}
\label{pretrain-data-law-table}
% \vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{lcccr}
\toprule
Fold & \multicolumn{2}{c}{Sampling Strategy} \\
& USER & RANDOM \\
\midrule
0    & $L = 0.058D^{-0.049}$ & $L = 0.046D^{-0.017}$ \\
1    & $L = 0.057D^{-0.045}$ & $L = 0.047D^{-0.015}$ \\
2    & $L = 0.052D^{-0.046}$ & $L = 0.043D^{-0.020}$ \\
3    & $L = 0.051D^{-0.052}$ & $L = 0.040D^{-0.019}$ \\
4    & $L = 0.043D^{-0.044}$ & $L = 0.035D^{-0.016}$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
% \vskip -0.1in
\end{table}





\begin{figure}[htb]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{pretrain_eval_loss_capacity_law.png}}
\caption{Pre-training test loss vs model capacity (number of parameters) and associated power law fit and equation.}
\label{pretrain-loss-capacity}
\end{center}
%\vskip -0.2in
\vspace{-1em}
\end{figure}




\begin{figure*}[htb]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\linewidth]{pretrain_f1_vs_data.png}}
\caption{Best linear F1 scores vs pre-training dataset size (hours). Each point represents the best F1 score corresponding to a pre-training fold and data size. The best score is chosen from 27 runs consisting of the 9 encoder architectures and 3 learning rates in our search space.}
\label{pretrain-f1-data}
\end{center}
%\vskip -0.2in
\vspace{-1em}
\end{figure*}




\begin{figure*}[htb]
% \vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{pretrain_capacity.png}}
\caption{Best linear {F1} scores vs model capacity (number of parameters). Each point represents the best F1 score corresponding to an encoder architecture (width and depth). The best score is chosen from all data sizes and learning rates. We indicate the width (mlp hidden dim) by color. At 5M or 20M parameters we have two models that are the same size, with one wider and shallower (5 blocks) and the other narrower and deeper (20 blocks).}
\label{pretrain-f1-capacity}
\end{center}
% \vskip -0.2in
\end{figure*}




\subsection{Scaling laws}
\label{sec:result_scaling}


In \cref{pretrain-loss-data} we establish scaling laws of the pre-training test loss vs hours of data. To calculate the loss, we use the full test set from each Extrasensory fold. 
This allows us to compare different training data amounts and distributions within a fold and fit power-law relationships for each fold. 
We observe roughly the same power-law exponent (or slope on the log-log plot) for a given fold and sampling strategy, giving confidence that this relationship was not due to random chance. 
Furthermore, in \cref{pretrain-data-law-table} we see that the exponent is roughly of 3x greater magnitude (or steeper slope) when data is increased by adding more users, as opposed to uniformly or per-user. This emphasizes that diversity of data is extremely important, and dictates the scaling law. Note that the offset is different for each fold, but that is to be expected, since the test sets are different. 
Similarly, in \cref{pretrain-loss-capacity} we fit a power law between pre-training test loss and model capacity in terms of number of parameters, further demonstrating the existence of a scaling law.



\subsection{Downstream performance}
\label{sec:downstream_performance}

We show that the scaling laws for pre-training translate to similar trends in improved downstream linear classification performance. For each pre-training dataset size, we plot the best F1 score from linear evaluation on downstream datasets UCI HAR and WISDM Phone/Watch. This can be seen in \cref{pretrain-f1-data}. Contrary to previous findings \cite{assessingSSLHAR22,howmuchdataSSLHAR23}, we see consistent improvement as we scale the data size. For UCI HAR, we reach 97.9\% F1 score with linear evaluation. 
To our knowledge, this is on par with the best reported result (98.6\% from \cite{uciharsota}) for this dataset. For all datasets, we surpass from scratch baseline results, with significant improvement for phone datasets UCI HAR (+2.8pp) and WISDM Phone (+2.4pp). 
For WISDM Watch, the improvement is smaller (+0.5pp). This is not surprising given that our pre-training dataset consists of only phone data. Still, the consistent increase in watch performance suggests that we are seeing positive transfer between body positions.


\begin{figure}[htb]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{capacity_vs_data.png}}
\caption{Optimal capacity for a given pre-training data size. Each plot shows the parameter count of the model resulting in the best performance for a given metric (pre-train test loss on the left, UCI HAR test F1 on the right).}
\label{capacity-vs-data}
\end{center}
%\vskip -0.2in
\vspace{-1em}
\end{figure}

In \cref{pretrain-f1-capacity} we study the effect the capacity of the encoder has on downstream performance. For each of the 9 encoder architectures (3 widths by 3 depths), we plot the best F1 score out of 180 runs (5 folds * 6 data sizes * 2 sampling strategies * 3 learning rates) from linear evaluation on downstream datasets UCI HAR and WISDM vs the number of parameters. We find that increasing the number of parameters is crucial to realizing performance improvements across all 3 tasks. The optimal capacity is reached at our biggest model which has about 63M parameters. This is in contrast to our from scratch baselines, where performance can peak at smaller models (e.g. about 2M parameters for UCI HAR).

\begin{figure*}[t]
%\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.8\textwidth]{augmentations_capacity.png}}
\caption{Best linear {F1} scores vs model capacity (number of parameters). Each point represents the best F1 score corresponding to an encoder architecture (width and depth) and whether augmentations were on or off. The best score is chosen from all data sizes and learning rates.}
\label{pretrain-augmentations}
\end{center}
%\vskip -0.2in
\vspace{-1em}
\end{figure*}



\subsection{Optimal capacity vs data size}
In \cref{capacity-vs-data} we study the optimal capacity for a given pre-training data size. For pre-training test loss, we find that optimal model size increases monotonically with more data. Downstream F1 performance tells a different story, with our largest models performing best even with minimal data. We hypothesize that this may be due to epoch-wise double descent \cite{double-descent} behaviour, which we have observed in some cases in this work.

\subsection{Augmentations}
\label{sec:augmentations}
We apply augmentations during pre-training, and study the effect on downstream model performance. In \cref{pretrain-augmentations} We separate results by encoder as in \cref{pretrain-f1-capacity}, but take the best score both with and without augmentations. We see that augmentations always improve performance, especially at larger scales. The optimal capacity without augmentations can be smaller, at either the 4th largest model (about 10M parameters) for UCI HAR or the 3rd largest model (20M parameters) for WISDM Phone. This is not surprising, since augmentations can be thought of as a strong regularizer, and/or an artificial expansion of the dataset.

\subsection{Width vs Depth}
Since we conducted a grid search on width and depth, we have results for a variety of width to depth ratios. There are also 2 model sizes (20M and 5M) for which we have a very deep model (20 blocks) and a very wide model (5 blocks) with the same number of parameters. This allows us to control for the total number of parameters. Looking at \cref{pretrain-f1-capacity}, we can see that increasing both width and depth improve performance, but wider models tend to perform better than deeper models for the same number of parameters.

\section{Conclusion}
\label{sec:conclusion}
\input{conclusion}

% Bibliography components
\bibliographystyle{abbrvnat}
\nobibliography*
\bibliography{main}

\end{document}
