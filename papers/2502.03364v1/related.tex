\subsection{Deep learning for wearable activity recognition}
%Prior to deep learning, HAR relied on a combination of signal processing, feature engineering and classical machine learning \cite{Bulling14}.
{DeepConvLSTM} was one of the first deep models to outperform classical machine learning pipelines on a benchmark datasets of activities of daily living. It consisted of 4 deep convolutional layers and 2 LSTM layers with a total 3.97M parameters \cite{Ordonez16a}.
As multimodality is important in HAR \cite{Munzner17} explored different fusion strategies based on convolutional architectures, with the largest model containing 7M parameters.
Since then, new models have been suggested with GRU units, attention mechanism, various normalization strategies, reflecting advances in other {ML} fields. 
One oft-cited architecture is Attend and Discriminate, which aims to leverage cross-modal sensor interaction using spatial attention mechanism on top of convolutional layers, and substitutes LSTM units by GRU \cite{Abedin20} (parameter count not reported).
More exhaustive reviews can be found in \cite{Chen21,Gu21}. 

Designing HAR models tends to follow engineering heuristic and design guidelines are still lacking.  Besides scaling laws mentioned in Sec. \ref{sec:introduction}, neural architecture search has been proposed to systematically explore the design space for a particular recognition problem \cite{Wang21}. \cite{Pellatt22} reports larger models than DeepConvLSTM, although the results are reported in FLOPS rather than parameter count (47.2M FLOPS compared to 5.3M for DeepConvLSTM).

Researchers in the field of mobile and wearable computing have tended to minimize compute cost rather than scaling up models in order to embed these models in battery-operated devices. 
TinyHAR performs multi-modal fusion with spatial and temporal Transformer blocks with the largest model having 165K parameters and also outperforming DeepConvLSTM \cite{Zhou22}. Similarly, a shallower version of DeepConvLSTM was proposed in \cite{Bock21} with 63\% less parameters and similar performance. 
Device constraints may seem to contradict the premises of this paper aiming to explore scaling laws and large models. However there are arguments for scaled up models when recognition performance is paramount: 1) pervasive network connectivity allows models to be running in the cloud; 2) scaled-up models can be distilled and quantized to smaller sizes customized for inference on a variety of devices.


\subsection{Self-supervised learning for HAR}

Annotation scarcity in activity recognition can be combated through self-supervised learning, where a pretext task is learnt on an unannotated dataset, with fine-tuning on a smaller annotated dataset.
%Pretext tasks include reconstructing masked data, identifying which data augmentation transformation has been applied, contrastive learning, and others.
An exhaustive review of methods can be found in \cite{assessingSSLHAR22,SSLHARsurvey}.
These reviews highlight that variations of masked reconstruction are commonly used.
\cite{HARmaskedreconstruction} proposes a Transformer encoder in the time domain, yielding a model with 1.5M parameters. They only mask 10\% of data and use an MLP decoder. 
SelfPAB \cite{selfpab} \cite{monoselfpab} and FreqMAE \cite{freqmae} use masked autoencoder pre-trained on the spectrograms of the input. 
SelfPAB is inspired by audio models, and does pre-training on the HUNT4 dataset with 100k hours of sensor signals.
FreqMAE is suggesting a specific Transformer block to account for spectral properties of the input signal. Our approach differs from these by faithfully applying masked autoencoder \cite{maskedautoencoder} with minimal change from the vision domain, demonstrating that this approach generalizes well to the HAR domain.


\subsection{Scaling laws in language, vision and HAR}

Scaling up the size of Transformers has led to significant improvements in performance in language and vision models.
We are particularly interested in the scaling {\em laws} that link model capacity, pre-training data volume and performance, as this contributes to more principled design.

In language foundation models, \cite{kaplan2020scalinglawsneurallanguage} demonstrated that performance improves as pre-training data and model capacity is increased with a power law relationship. \cite{hoffmann2022chinchilla} built on this and demonstrated that data and model capacity should be scaled equally.

Vision Transformers with masked pre-training \cite{vit} have been shown to perform increasingly better as the model size increased (from 86M to 632M parameters), later even up to 22B parameters \cite{Dehghani23a}, also verified in \cite{maskedautoencoder}.
A saturating power-law linking performance, data and compute was presented in \cite{zhai2022scalingvits}, similarly to language.

Work exploring scaling laws in HAR is less established but there is also evidence that more pre-training data is beneficial. 
\cite{SSLUKbiobank} exploited the 700k hour UK Biobank dataset and showed this on a ResNet encoder with 10M parameter.
These benefits are also reported in \cite{assessingSSLHAR22}, but only to a point. 
\cite{howmuchdataSSLHAR23} tried to identify what is the minimum amount of pre-training data which is required, after which a plateau is reached. The authors clarify their intent is not to identify scaling laws, but rather that identifying {\em ``minimal quantities can be of great importance as they can result in substantial savings during pre-training as well
as inform the data collection protocols''}.
None of these work draw explicit scaling laws.
SelfPAB \cite{selfpab} \cite{monoselfpab} varied data and model capacities and scale Transformers to a size of 60M parameters, similar to this present work. However, our work differs from theirs by establishing scaling laws that link model capacity, data, and performance.


Scaling laws have been explored in sensor foundation models \cite{narayanswamy2024scalingwearablefoundationmodels,Shapiro24}. 
The work in \cite{narayanswamy2024scalingwearablefoundationmodels} differs fundamentally from ours.
The authors create a foundational model not on raw motion sensor data but on a set of 10 engineered statistical features extracted from the motion sensors at a rate of one vector every minute, as well as physiological sensors. 
Although they introduce scaling laws, operating on engineered features makes it difficult to draw direct parallels to our work.
Furthermore, they rely on a proprietary dataset, and report scaling in terms of reconstruction performance (mean squared error) instead of downstream classification performance. 
Our work instead uses public datasets to help reproducibility, and our conclusions differ from theirs regarding data diversity (\cref{sec:conclusion}).
\cite{Shapiro24}  mention scaling up models as a future research direction and do not yet draw scaling laws from their experiments.

