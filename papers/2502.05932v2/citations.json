[
  {
    "index": 0,
    "papers": [
      {
        "key": "silver2017mastering",
        "author": "Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others",
        "title": "Mastering the game of go without human knowledge"
      },
      {
        "key": "andrychowicz2020learning",
        "author": "Andrychowicz, OpenAI: Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and others",
        "title": "Learning dexterous in-hand manipulation"
      },
      {
        "key": "berner2019dota",
        "author": "Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Debiak, Przemys{\\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others",
        "title": "Dota 2 with large scale deep reinforcement learning"
      },
      {
        "key": "vinyals2019grandmaster",
        "author": "Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\\\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others",
        "title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "agarwal2022reincarnating",
        "author": "Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc",
        "title": "Reincarnating reinforcement learning: Reusing prior computation to accelerate progress"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "peng2019mcp",
        "author": "Peng, Xue Bin and Chang, Michael and Zhang, Grace and Abbeel, Pieter and Levine, Sergey",
        "title": "Mcp: Learning composable hierarchical control with multiplicative compositional policies"
      },
      {
        "key": "qureshi2020composing",
        "author": "Qureshi, Ahmed H and Johnson, Jacob J and Qin, Yuzhe and Henderson, Taylor and Boots, Byron and Yip, Michael C",
        "title": "Composing Task-Agnostic Policies with Deep Reinforcement Learning"
      },
      {
        "key": "pertsch2021accelerating",
        "author": "Pertsch, Karl and Lee, Youngwoon and Lim, Joseph",
        "title": "Accelerating reinforcement learning with learned skill priors"
      },
      {
        "key": "merel2018neural",
        "author": "Josh Merel and Leonard Hasenclever and Alexandre Galashov and Arun Ahuja and Vu Pham and Greg Wayne and Yee Whye Teh and Nicolas Heess",
        "title": "Neural Probabilistic Motor Primitives for Humanoid Control"
      },
      {
        "key": "merel2020catch",
        "author": "Merel, Josh and Tunyasuvunakool, Saran and Ahuja, Arun and Tassa, Yuval and Hasenclever, Leonard and Pham, Vu and Erez, Tom and Wayne, Greg and Heess, Nicolas",
        "title": "Catch \\& carry: reusable neural controllers for vision-guided whole-body tasks"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "lecun2006tutorial",
        "author": "LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, M and Huang, Fujie and others",
        "title": "A tutorial on energy-based learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "janner2022planning",
        "author": "Janner, Michael and Du, Yilun and Tenenbaum, Joshua and Levine, Sergey",
        "title": "Planning with Diffusion for Flexible Behavior Synthesis"
      },
      {
        "key": "wang2024poco",
        "author": "Wang, Lirui and Zhao, Jialiang and Du, Yilun and Adelson, Edward H and Tedrake, Russ",
        "title": "Poco: Policy composition from and for heterogeneous robot learning"
      },
      {
        "key": "du2024position",
        "author": "Du, Yilun and Kaelbling, Leslie Pack",
        "title": "Position: Compositional Generative Modeling: A Single Model is Not All You Need"
      },
      {
        "key": "liu2022compositional",
        "author": "Liu, Nan and Li, Shuang and Du, Yilun and Torralba, Antonio and Tenenbaum, Joshua B",
        "title": "Compositional visual generation with composable diffusion models"
      },
      {
        "key": "luo2024potential",
        "author": "Luo, Yunhao and Sun, Chen and Tenenbaum, Joshua B and Du, Yilun",
        "title": "Potential Based Diffusion Motion Planning"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "smith2023continual",
        "author": "Smith, James Seale and Hsu, Yen-Chang and Zhang, Lingyu and Hua, Ting and Kira, Zsolt and Shen, Yilin and Jin, Hongxia",
        "title": "Continual diffusion: Continual customization of text-to-image diffusion with c-lora"
      },
      {
        "key": "wang2024sparse",
        "author": "Yixiao Wang and Yifei Zhang and Mingxiao Huo and Thomas Tian and Xiang Zhang and Yichen Xie and Chenfeng Xu and Pengliang Ji and Wei Zhan and Mingyu Ding and Masayoshi Tomizuka",
        "title": "Sparse Diffusion Policy:  A Sparse, Reusable, and Flexible Policy for Robot Learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "kessler2020unclear",
        "author": "Kessler, Samuel and Parker-Holder, Jack and Ball, Philip and Zohren, Stefan and Roberts, Stephen J",
        "title": "UNCLEAR: A straightforward method for continual reinforcement learning"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "liu2024continual",
        "author": "Liu, Jinmei and Li, Wenbin and Yue, Xiangyu and Zhang, Shilin and Chen, Chunlin and Wang, Zhi",
        "title": "Continual Offline Reinforcement Learning via Diffusion-based Dual Generative Replay"
      },
      {
        "key": "peng2023ideal",
        "author": "Peng, Liangzu and Giampouras, Paris and Vidal, Ren{\\'e}",
        "title": "The ideal continual learner: An agent that never forgets"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "liu2023tail",
        "author": "Liu, Zuxin and Zhang, Jesse and Asadi, Kavosh and Liu, Yao and Zhao, Ding and Sabach, Shoham and Fakoor, Rasool",
        "title": "Tail: Task-specific adapters for imitation learning with large pretrained models"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "nair2020awac",
        "author": "Nair, Ashvin and Gupta, Abhishek and Dalal, Murtaza and Levine, Sergey",
        "title": "Awac: Accelerating online reinforcement learning with offline datasets"
      },
      {
        "key": "lee2022offline",
        "author": "Lee, Seunghyun and Seo, Younggyo and Lee, Kimin and Abbeel, Pieter and Shin, Jinwoo",
        "title": "Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble"
      },
      {
        "key": "agarwal2022reincarnating",
        "author": "Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc",
        "title": "Reincarnating reinforcement learning: Reusing prior computation to accelerate progress"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "barreto2018transfer",
        "author": "Barreto, Andre and Borsa, Diana and Quan, John and Schaul, Tom and Silver, David and Hessel, Matteo and Mankowitz, Daniel and Zidek, Augustin and Munos, Remi",
        "title": "Transfer in deep reinforcement learning using successor features and generalised policy improvement"
      },
      {
        "key": "li2019hierarchical",
        "author": "Li, Siyuan and Wang, Rui and Tang, Minxue and Zhang, Chongjie",
        "title": "Hierarchical reinforcement learning with advantage-based auxiliary rewards"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "kim2024unsupervised",
        "author": "Kim, Junsu and Park, Seohong and Levine, Sergey",
        "title": "Unsupervised-to-Online Reinforcement Learning"
      },
      {
        "key": "wang2024train",
        "author": "Wang, Shenzhi and Yang, Qisen and Gao, Jiawei and Lin, Matthieu and Chen, Hao and Wu, Liwei and Jia, Ning and Song, Shiji and Huang, Gao",
        "title": "Train once, get a family: State-adaptive balances for offline-to-online reinforcement learning"
      },
      {
        "key": "barreto2018transfer",
        "author": "Barreto, Andre and Borsa, Diana and Quan, John and Schaul, Tom and Silver, David and Hessel, Matteo and Mankowitz, Daniel and Zidek, Augustin and Munos, Remi",
        "title": "Transfer in deep reinforcement learning using successor features and generalised policy improvement"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2023policy",
        "author": "Zhang, Haichao and Xu, Wei and Yu, Haonan",
        "title": "Policy Expansion for Bridging Offline-to-Online Reinforcement Learning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "li2019hierarchical",
        "author": "Li, Siyuan and Wang, Rui and Tang, Minxue and Zhang, Chongjie",
        "title": "Hierarchical reinforcement learning with advantage-based auxiliary rewards"
      }
    ]
  }
]