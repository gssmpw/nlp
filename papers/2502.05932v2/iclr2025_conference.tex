
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

% \usepackage{hyperref}
\usepackage{url}
\usepackage[colorlinks,
            linkcolor=blue,
            anchorcolor=blue,
            citecolor=brown
            ]{hyperref}      % hyperlinks
\usepackage{multirow}
\usepackage{graphicx} 
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\newtheorem{definition}{Definition}

\newcommand{\ljx}[1]{\textcolor{black}{\small{\bf [ljx: #1]}}}


\title{Skill Expansion and Composition in \\Parameter Space}
% Continual Policy Composition \\ for offline Reinforcement Learning}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Tenglong Liu$^{1}$\footnotemark[1]~~, Jianxiong Li$^{2}$\footnotemark[1]~~, Yinan Zheng$^2$, Haoyi Niu$^2$, \textbf{Yixing Lan$^{1}$}\footnotemark[2] \, ,
\textbf{Xin Xu$^{1}$}\footnotemark[2] \, , \\ \textbf{Xianyuan Zhan$^{2,3,4}$}\footnotemark[2] \\
$^1$ National University of Defense Technology, 
$^2$ Tsinghua University, \\
$^3$ Shanghai Artificial Intelligence Laboratory,
$^4$ Beijing Academy of Artificial Intelligence\\
\texttt{ltl@nudt.edu.cn,li-jx21@mails.tsinghua.edu.cn,} \\
\texttt{\{xinxu,lanyixing16\}@nudt.edu.cn,zhanxianyuan@air.tsinghua.edu.cn}\\
}


% \author{Tenglong Liu\thanks{} , Jianxiong Li\thanks{ Use footnote for providing further information} , Yinan Zheng, Haoyi Niu, Yixing Lan, Xin Xu, Xianyuan Zhan \\
% National University of Defense Technology\\
% Tsinghua University\\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% % \And
% % Ji Q. Ren \& Yevgeny LeNet \\
% % Department of Computational Neuroscience \\
% % University of the Witwatersrand \\
% % Joburg, South Africa \\
% % \texttt{\{robot,net\}@wits.ac.za} \\
% % \AND
% % Coauthor \\
% % Affiliation \\
% % Address \\
% % \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\vspace{-0pt}
\maketitle
\vspace{-7pt}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Equal contribution.}
\footnotetext[2]{Corresponding Authors.}
% \footnotetext[3]{Work done during a research visit at Tsinghua University.}

% \footnotetext[2]{Correspondence to Xianyuan Zhan and Xin Xu}
\renewcommand*{\thefootnote}


\maketitle
\vspace{-5pt}
\begin{abstract}
\vspace{-5pt}

Humans excel at reusing prior knowledge to address new challenges and developing skills while solving problems. This paradigm becomes increasingly popular in the development of autonomous agents, as it develops systems that can self-evolve in response to new challenges like human beings. However, previous methods suffer from limited training efficiency when expanding new skills and fail to fully leverage prior knowledge to facilitate new task learning. In this paper, we propose Parametric Skill Expansion and Composition (PSEC), a new framework designed to iteratively evolve the agents' capabilities and efficiently address new challenges by maintaining a manageable skill library. This library can progressively integrate skill primitives as plug-and-play Low-Rank Adaptation (LoRA) modules in parameter-efficient finetuning, facilitating efficient and flexible skill expansion. This structure also enables the direct skill compositions in parameter space by merging LoRA modules that encode different skills, leveraging shared information across skills to effectively program new skills. Based on this, we propose a context-aware module to dynamically activate different skills to collaboratively handle new tasks. Empowering diverse applications including multi-objective composition, dynamics shift, and continual policy shift, the results on D4RL, DSRL benchmarks, and the DeepMind Control Suite show that PSEC exhibits superior capacity to leverage prior knowledge to efficiently tackle new challenges, as well as expand its skill libraries to evolve the capabilities. Project website: \url{https://ltlhuuu.github.io/PSEC/}.
\end{abstract}
\vspace{-5pt}

\vspace{-8pt}
\section{Introduction}
\vspace{-7pt}
% 1. Given a task, humans' initial instinct is to assess whether the task is familiar. If so, they quickly apply previously learned skills to solve the current task efficiently. If not, they rapidly acquire new skills to address the challenge. This fundamental approach to problem-solving, leveraging both learned competencies and the ability to adapt, highlights a crucial aspect of human intelligence. Today's robotic policies often struggle to leverage the previously learned skills flexibly to improve the performance of the robotic policies and finetune for truly new tasks effectively combine the previously learned policies. 

% 2. However, this capability seems have not appeared in to autonomous agents where ...

% 3. To achieve this objective, several challenges remain: 1) how to identify whether the target task can be solved by leveraging previous skills; 2) How to efficiently compose existing policies to sufficiently solve the target task; 3) How to efficiently learn new skills given only a small number of data.


% 4. In this paper, we propose CPEC..., a general framework that jointly tackles the above challenges...


% 5. The experiments.


Humans excel at using existing skills and knowledge to tackle new tasks efficiently, while continually evolving their capabilities to rapidly adapt to new tasks.
% , enabling the monotonic evolution of their capabilities
~\citep{driscoll2024flexible, courellis2024abstract, eppe2022intelligent, eichenbaum2017prefrontal}. 
% For instance, a child can rapidly learn to 
% % walk
% recognize a tiger
% by integrating prior experiences of 
% recognizing a cat,
% and subsequently, combines this knowledge to adapt to recognizing a lion. 
This fundamental approach to problem-solving highlights a key aspect of human intelligence that is equally crucial for autonomous agents. However, most current decision-making algorithms adhere to a \textit{tabula rasa} paradigm, where they are trained from scratch without utilizing any prior knowledge or resources~\citep{akkaya2019solving, berner2019dota, silver2016mastering}, leading to severe sample inefficiency and elevated cost when the agent encounters new tasks~\citep{agarwal2022reincarnating, peng2019mcp, du2024position}. Therefore, in this paper, we aim to explore the capability of autonomous agents to leverage and expand upon their existing knowledge base in novel situations to enhance learning efficiency and adaptability.

% To emulate this human capability of leveraging prior knowledge and self evolution when encountering new tasks, two fundamental questions must be addressed: \textit{1) How to learn new skills in a monotonic evolution and efficient way?} Naively tackling new challenges 

% \textit{2) How can existing skills be efficiently combined to tackle unseen tasks?}. Successfully navigating these questions allows autonomous agents to progressively evolve their skills and address new challenges, paving the way for the development of general-purpose agents capable of self-evolution in open-world scenarios. 

While some existing studies, such as continual learning~\citep{liu2024continual, gai2024single}, compositional policies~\citep{peng2019mcp, janner2022planning, ajay2023conditional}, or finetuning-based methods~\citep{agarwal2022reincarnating}, aim to replicate this process, they jointly failed to tackle several key limitations. \textit{1) Catastrophic forgetting}: these approaches typically lack a fundamental mechanism to guarantee continuous improvement when acquiring new skills, making the autonomous agents very susceptible to overfitting on new tasks while forgetting previously learned skills without proper regularization~\citep{liu2023tail, liu2024continual, gai2024single}; \textit{2) Limited efficiency in learning new tasks}: Some methods avoid the catastrophic forgetting problem by adopting a parameter-isolation approach via encoding new skills in independent new parameters. However, they typically do not fully utilize prior knowledge from old skills to enhance training in current tasks, lacking an efficient way to learn new skills in terms of both parameters and training samples~\citep{peng2019mcp, zhang2023policy}, leading to tremendous costs as the number of skills progressively grows.  
% \textit{1) Limited problem scenario}: They often do not address the pivotal question of ``when to expand the skill sets", thereby may include many redundant skills into the skill sets to limit the efficiency~\citep{peng2019mcp, du2024position, li2023proto}; 

% \textit{1) Limited flexibility and expressiveness}: They are often constrained by the modeling capabilities of simple Gaussian primitive skills~\citep{peng2019mcp}, or lack the flexibility to adaptively compose skills in a context-aware manner to combine complex behaviors~\citep{janner2022planning, ajay2023conditional}; \textit{2) Inefficiency}: Most approaches overlook the inter-connections between tasks, lacking an efficient way to learn new skills in terms of both parameters and samples~\citep{peng2019mcp, du2024position}.

In order to deal with the above problems, we propose Parametric Skill Expansion and Composition~(PSEC), a framework that facilitates efficient self-evolution of autonomous agents by maintaining a skill library that progressively integrates new skills, facilitating rapid adaptation to evolving demands. The key mechanism of PSEC is to utilize the primitives in the skill library to tackle new challenges by exploiting the shared information across different skills within the parameter space.
% By leveraging these composable primitives, PSEC can efficiently tackle the new challenge, facilitating rapid and efficient learning.
% exploiting the shared information across different skills within the parameter space. 
% , a generic framework that strives to replicate the human-like process of skill acquisition and application. 
As shown in Figure~\ref{fig:intro} (a), we adopt the Low-Rank Adaptation (LoRA)~\citep{hu2021lora} approach, which encodes skills as trainable parameters injected into existing frozen layers. This parameter-isolation approach naturally resolves the catastrophic forgetting problem, and significantly reduces computational burden due to the low-rank decomposition structure. 
% avoid overfitting with limited adaptation
% data and
% significantly reducing computational and memory burden thanks to its low-rank decomposition structure. 
This efficient modular design allows for managing skills as plug-and-play modules, and thus can directly blend different abilities 
% can be blended directly
within the parameter space to interpolate new skills~\citep{clark2024directly}, as shown in Figure~\ref{fig:intro} (b). The proposed PSEC approach can leverage more shared or complementary structures across skills for optimal compositions. 
% Unlike conventional methods that combine different skills at later stages where policy distributions are already generated~\citep{peng2019mcp, janner2022planning, ajay2023conditional}, PSEC can fully utilize the complementary and shared information across different skills, 
% leading to enhanced compositions when addressing new challenges.
% enhancing the ability to collaboratively address new challenges effectively. 
Based on this insight, a context aware module is designed
% network 
to adaptively compose skills and
% , where 
each primitive is modeled by diffusion models
% , ensuring 
to ensure
both flexibility and expressiveness in composition. Through iterative expansion and composition, PSEC can continually 
% expands the sets to 
evolve 
% its capabilities 
and efficiently tackle new tasks,
% composes 
% existing
% skills to tackle emerging challenges, 
offering one promising pathway for developing human-level autonomous agents. 


% one complex skill can be composed by several orthogonal skills. Akin to the concept of Orthogonal Basis, these primitives cannot interpolate each other but can form complex behaviors together. This enables us to maintain a streamlined set of skills by solely absorbing orthogonal skills while dropping the redundant one that can be composed by other bases, significantly reducing computational overhead when encountering diverse tasks. We utilize a $\delta$-relative independence metric evaluated in the action space, effectively accessing the orthogonality between different skills. 
% If a novel orthogonal primitive is found, we integrate it within the skill sets adopting the Low-Rank Adaption approach~(LoRA)~\citep{hu2022lora}, facilitating the integration of knowledge across tasks and enabling efficient adaptations while minimizing computational overhead; If not, we directly adopt a 
% context-aware compositional network that activates various skills simultaneously in parameter space to collaboratively
% solve the task, with each primitives modeled by diffusion models to ensure flexibility and
% expressiveness in composition. Finally, PSEC forms an iterative framework that progressively expands the sets to  evolve its capabilities and composes skills to tackle emerging challenges. 

Empowering diverse settings including multi-objective composition, continual policy shift and dynamics shift, PSEC demonstrates its capacity to evolve and effectively solve new tasks by leveraging prior knowledge, evaluated on the D4RL~\citep{fu2020d4rl}, DSRL~\citep{liu2023datasets} and DeepMind Control Suite~\citep{tassa2018deepmind}, showcasing significant potential for real-world applications.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/intro.pdf}
    \vspace{-10pt}
    \caption{\small PSEC framework and its application in diverse scenarios. (a) We maintain a skill library that contains many skills primitives and can progressively expand by adding new LoRA modules. (b) Then we train a context-aware compositional network to adaptively compose different elements in the skill library to solve new tasks. (c) PSEC framework is versatile to diverse applications where reusing prior knowledge is crucial.
    }
    \label{fig:intro}
    \vspace{-13pt}
\end{figure}

% This structure enables us to maintain a streamlined set of skills, significantly reducing computational overhead and minimizing redundancy. When faced with a new task, PSEC first assesses whether it can be resolved using a combination of existing skills judging by a relative correlation metric~\cite{}. If the task is deemed unsolvable with the current skill set, it is identified as a novel orthogonal primitive that must be integrated into our skill repository. Meanwhile, the new tasks can be quickly learned by combining previously learned policies with the Low-Rank Adaption approach~(LoRA)~\citep{hu2022lora}. A pre-trained model can be leveraged to create multiple compact LoRA modules for different tasks, facilitating the integration of knowledge across tasks and enabling efficient adaptation to new tasks while minimizing memory usage and computational overhead. Concurrently, we introduce a context-aware compositional network that activates various skills simultaneously to collaboratively solve the task, with each primitive skill modeled through diffusion models to ensure flexibility and expressiveness in composition. Our iterative framework continuously expands and adaptively composes skill sets to tackle emerging challenges, thus enhancing decision-making across diverse scenarios. Evaluated on the traditional single-task and mutli-task settings in D4RL~\citep{fu2020d4rl}, DSRL~\citep{liu2023datasets} and DeepMind Control Suite~\citep{tassa2018deepmind}, PSEC has demonstrated its capacity to evolve and effectively solve new tasks by leveraging prior knowledge, showcasing significant potential for real-world applications.

% In this work, we propose an efficient Orthogonal Skill Expansion and Composition~(PSEC) method. Our method can effectively determine whether the current task is orthogonal to previous tasks based on the existing policies, which identify and leverage modular components with compositional potential for new tasks. Our method combines both the expressiveness and flexibility of the policies, ensuring they work together seamlessly to enhance performance. Meanwhile, the new tasks can be quickly learned by combining previously learned policies with the Low-Rank Adaption approach~(LoRA)~\citep{hu2022lora}. A pre-trained model can be leveraged to create multiple compact LoRA modules for different tasks, facilitating the integration of knowledge across tasks and enabling efficient adaptation to new tasks while minimizing memory usage and computational overhead.


% For example, in the robotics manipulation tasks, most long-horizon tasks can be temporally decomposed as a combination of some atomic skills including \texttt{pick \& place, open \& close, push, move};


% Our method can effectively determine whether the current task is orthogonal to previous tasks based on the existing policies, which identify and leverage modular components with compositional potential for new tasks. Meanwhile, our method balances the expressiveness and flexibility of the policies. 
% hey primarily focus on the first question, utilizing prior knowledge for new tasks, often assuming that skill sets are pre-prepared and that target tasks are always new; \textit{2) Limited flexibility or expressiveness}: Existing methods are often constrained by the modeling capabilities of simple Gaussian primitives or lack the flexibility to adaptively compose skills; \textit{3) Inefficiency}: Most methods overlook the interconnectedness between tasks, lacking an efficient approach to learning new skills in terms of both parameters and samples.


% To fully mimic the human's superior ability to leveraging and expanding the existing skill sets and knowledge when handling new tasks, one must answer two questions: \textit{How to efficiently combine existing skills to solve unseen tasks?} and \textit{When to expand their skill sets to enable efficient policy expansion and avoid redundancy?} Addressing these questions well, the autonomous agent can then gradually evolve their skills and solve new tasks, providing a fully autonomous and efficient way to develop general-purpose agents that can self-evolve in open-world scenarios. Some existing studies try to achieve this, but all exclusively focus on addressing the first question about how to leverage prior knowledge to address new tasks, such as continue learning~\citep{liu2024continual, gai2024single}, compositional policies~\citep{peng2019mcp, du2024position, janner2022planning, ajay2023is}, or finetune-based methods~\citep{agarwal2022reincarnating, li2023proto}.

%tenglong
% When humans encounter a new task, the brain first uses pattern recognition to assess whether the task resembles previous experiences. If certain features of the task match those from past learning, the brain may identify it as a problem that can be solved leveraging existing skills, enabling quicker decision-making and response~\citep{courellis2024abstract, eppe2022intelligent, eichenbaum2017prefrontal}.
% A complex task can be decomposed into some simple, basic tasks, then it can be solved by compositing the learned basic policies of the simple tasks~\citep{eppe2022intelligent}. 
% Humans and animals have the innate ability to recognize tasks, and then compose their basic skills flexibly into complex behaviors~\citep{driscoll2024flexible, rizzolatti2016mirror}.
% This fundamental approach to problem-solving, identifying new tasks intuitively and leveraging both learned skills and adaptive capabilities—highlights a key aspect of human intelligence. It is equally crucial for autonomous agents.
% However, current policy learning methods for autonomous agents ignore the correlation between the acquired knowledge and the new tasks. Their primary focus is on acquiring new behaviors rather than compositing and re-using the already acquired policies to solve novel tasks efficiently.

%tenglong
% Combinations of previously learned subtasks may be reconfigured to perform new tasks. This combination of subtasks is referred to as compositionality~\citep{driscoll2024flexible}. 
% There are some composition policy learning methods for autonomous agents.
% However, previous methods still have some problems.

%tenglong
% 1) The method lacks an identification module to recognize whether only leveraging the existing policies can finish the new tasks.
% They assume that the policy sets are prepared in advance for every task, only considering composite these existing policies. But in some cases, it can not finish the new tasks. In this way, how to quickly adapt to the new tasks by policy expansion? 
% They ignore considering what time we can composite to achieve the new tasks, and if compositing the existing policies can not finish the new tasks, what should we do?

%tenglong
% 2) Composition policies do not have both the necessary flexibility and expressiveness at the same time to handle more complex scenarios effectively. Recent composition methods use diffusion models with strong expressiveness to generate the composite policies~\citep{wang2023diffusion, kang2024efficient}. But they are rigid and inflexible. Due to their fixed composition weights, they can not adapt to the dynamic complex environment. Although previous composition methods consider adjusting the composition weights dynamically~\citep{qureshi2020composing, peng2019mcp}. However, since offline datasets are often collected by a mixture of policies, the true behavior policy may exhibit strong multimodality, skewness, or dependencies across action dimensions—patterns that diagonal Gaussian policies struggle to model effectively~\citep{shafiullah2022behavior}.

%tenglong
% 3) Basic policies are not both storage- and compute-efﬁcient~\citep{hu2021lora}. They use different models to solve different tasks. When there is a connection between tasks, the new model also contains as many parameters as in the original model. It makes training inefﬁcient and hinders quickly adapting to the new tasks.

%tenglong
% In this work, we propose an efficient Orthogonal Skill Expansion and Composition~(PSEC) method. Our method can effectively determine whether the current task is orthogonal to previous tasks based on the existing policies, which identify and leverage modular components with compositional potential for new tasks. Our method combines both the expressiveness and flexibility of the policies, ensuring they work together seamlessly to enhance performance. Meanwhile, the new tasks can be quickly learned by combining previously learned policies with the Low-Rank Adaption approach~(LoRA)~\citep{hu2022lora}. A pre-trained model can be leveraged to create multiple compact LoRA modules for different tasks, facilitating the integration of knowledge across tasks and enabling efficient adaptation to new tasks while minimizing memory usage and computational overhead.


%tenglong
% Composition diffusion policy methods can use a more expressive generative model than the Gaussian primitive model~\citep{ren2024diffusion}. It can effectively represent the sparsity and symmetry naturally found in nature~\citep{du2024position, urain2024deep}, such as PoCo~\citep{wang2024poco}, PDMP~\citep{luo2024potential}. However, they are over rigid and inflexible. Because they ignore the current state of the agent and hand-tuned composition weights are fixed. Meanwhile, many model parameters for training the basic policies are inefficient. This is not conducive to learning new policies quickly and efficiently to adapt to new tasks. The reliance on hand-tuned composition weights for each task reveals the lack of automation and adaptability for these methods. 

%tenglong
% In some cases, it is possible to improve the learned policy performance on a new task by combining some relevant task policies. Such as, babies learn to stand, then to walk, and finally to run as they grow up. Standing is the foundation of infant walking and running, and if infants don't stand well, the corresponding walking and running won't be done well either. 

%tenglong
% A key attribute of an autonomous agent is its capacity for continual learning and self-improvement from large-scale incremental data. Large-scale data has generated much of the success of the vision and language domains for training a large model.
% Similar methods have also been applied in decision-making and control applications. We would expect policy learning techniques to similarly leverage vast amounts of data to solve multiple real-world problems. The quality of the dataset influences policy learning. But, collecting high-quality datasets for robotics is expensive and time-consuming. It is crucial to continuously learn new policies from incremental diverse data and flexibly integrate previously learned policies to tackle new tasks. However, Adapting to new control tasks is more challenging than adapting to vision and language models due to (1) the scarcity of large-scale control data and (2) the significant variation in available actions across tasks, even when observation spaces are shared. Consequently, these large-scale decision-making models still depend on a close alignment between training and testing tasks. 


% How to leverage diverse data to strengthen the ability of an agent continually? 



% Reinforcement learning~(RL) has made agents more intelligent. Because RL 

%tenglong
% Existing offline datasets vary in different quality. Collecting large amounts of high-quality data is prohibitively expensive and difficult. Training a single policy on the general dataset is simple. However, the performance of a single policy can not finish the complex tasks. 
% Current methods consider compositing the single policy to finish the complex tasks. However, they are more rigid and inflexible. Because they ignore the current state the agent is in and hand-tuned composition weights are fixed. The reliance on hand-tuned composition weights for each task reveals the method’s lack of automation and adaptability.
% Can we flexibly composite these single policies to obtain a stronger policy to finish the complex tasks?
\vspace{-8pt}
\section{Related works}
\vspace{-7pt}
\label{rel_work}
% \ljx{revised till here}
% \textbf{Tabula Rasa}. 
% Tabula rasa learning is one popular paradigm for diverse existing decision-making applications, such as robotics and games~\citep{silver2017mastering, andrychowicz2020learning, berner2019dota, vinyals2019grandmaster}. It directly learns policies from scratch without the assistance of any prior knowledge. However, it suffers from notable drawbacks related to poor sample efficiency and constraints on the complexity of skills an agent can acquire~\citep{agarwal2022reincarnating}.

\textbf{Compositional Policies}.
% To tackle the problem of tabula rasa paradigm, 
Some previous methods try to leverage prior knowledge relying on pretrained primitive policies. More specifically, these methods used compositional networks in a hierarchical structure to adaptively compose primitives to form complex behaviors~\citep{peng2019mcp, qureshi2020composing, pertsch2021accelerating, merel2018neural, merel2020catch}. However, their expressiveness is limited by the expressiveness of simple Gaussian primitives. 
% The field of generative models for decision-making has seen significant progress in recent years. 
Recently, due to the strong expressiveness of the diffusion models and its inherent connection with Energy-Based Models~\citep{lecun2006tutorial}, many compositional policies have been approached by diffusion model. Diffusion models learn the gradient fields of an implicit energy function, which can be combined at inference time to generalize to new complex distribution readily~\citep{janner2022planning, wang2024poco, du2024position, liu2022compositional, luo2024potential}. However, these approaches rely on independently trained policies with fixed combination weights, which lack the flexibility to adapt to complex scenarios. Moreover, most previous methods can only combine skills after the policy distribution generation of each skill. Therefore, they fail to fully utilize the shared features of different skills to achieve optimal compositions. 
% Moreover, all previous methods assume a predefined policies libraries in advance and ignore policy expanding combined with existing policies to solve the unseen tasks, which is significantly important for the autonomous agent to self-evolution in given .
% Unlike these methods, w
We systematically investigate the advantages of skill composition 
% approach can adaptively compose skills 
within the parameter space, and compose skills in a context-aware manner with each skill modeled as a diffusion model. This ensures both flexibility and expressiveness in composing complex behaviors.
% \ljx  {revised till here}

\textbf{Continual Learning for Decision Making}.
Current continual learning methods for decision making, including continual reinforcement learning~(RL) and imitation learning (IL), primarily focus on mitigating catastrophic forgetting of prior knowledge when learning new tasks. They can be roughly classified into three categories: structure-based~\citep{smith2023continual, wang2024sparse},  regularization-based~\citep{kessler2020unclear}, and rehearsal-based methods~\citep{liu2024continual, peng2023ideal}. Different from previous continual RL and IL approaches, our study focuses on leveraging existing skills to facilitate efficient new task learning and enables the extension of skill sets. In addition, it naturally solves the catastrophic forgetting challenge due to the parameter isolation induced by the LoRA module~\citep{liu2023tail}, directly bypassing the key challenges of existing continual learning methods. 
% Therefore, our study can be considered as orthogonal to traditional continue learning methods.


% \textbf{Finetune-based Methods}. Some finetune-based methods aim to accelerate policy learning by leveraging prior knowledge. This knowledge may come from pretrained policy or offline data, such as 
% Offline-to-online RL~\citep{nair2020awac, lee2022offline, agarwal2022reincarnating} and transfer RL~\citep{barreto2018transfer, li2019hierarchical}. Some methods maintain a policy library that contains pretrained policies and adaptively selects one policy from this set to assist policy training~\citep{kim2024unsupervised, wang2024train, barreto2018transfer}. However, they are generally restricted to single-task scenarios where all policies serve the same task~\citep{zhang2023policy}, or only sequentially activate one policy in the pretrained sets, which greatly limits the expressiveness of the pretrained primitives~\citep{li2019hierarchical}. Our method, on the contrary, can both leverage multi-task knowledge to fulfill the new task, and can simultaneously activate all skills to compose more complex behaviors.

% \paragraph{Diffusion Composition methods} The field of generative models for decision-making has seen significant progress in recent years. 
% Recently, due to the strong expressiveness of the diffusion model for multimodality, there are many composition methods with the diffusion model. 

% \paragraph{Gaussian Composition methods}

% \paragraph{Continual Composition methods}
\vspace{-8pt}
\section{Methods}
\label{sec:methods}
\vspace{-7pt}
We propose PSEC, a generic framework that can efficiently reuse prior knowledge and self-evolve to address emerging new tasks. 
% In the following, 
Next, we will elaborate on our problem setup and technical details.

\vspace{-4pt}
\subsection{Preliminary}
\label{subsec:problem_setup}
\vspace{-4pt}
\textbf{Diffusion Model for Policy Modeling}. Recently, diffusion models have become popular for policy modeling because of their superior expressiveness to model complex distributions~\citep{wang2023diffusion, chen2022offline, lu2023contrastive, zheng2025diffusion}. 
% In diffusion models, the goal is to develop a generative model that accurately emulates the data distribution from the dataset. 
Considering a policy distribution $\pi(a|s)$ and a sample $(s,a)$ drawn from an empirical dataset $\mathcal{D}$ of $\pi(a|s)$, the diffusion process
% , modeled as a Markov chain
~\citep{ho2020denoising} progressively introduces Gaussian noise to the sample over $T$ steps, producing a sequence of noisy samples $a_0, a_1, ..., a_T$ with $a_0=a$ following the forward Gaussian kernel:
\begin{equation}
    q({a}_t|{a}_{t-1})=\mathcal{N}({a}_t;\sqrt{1-\beta_t}{a}_{t-1},\beta_t{I}), \quad q(a_t|a_0) = \mathcal{N}(a_t;\sqrt{\bar{\rho}_t}a_0, (1-\bar{\rho}_t) I),
    % ,\quad q(\boldsymbol{x}_{1:K}|\boldsymbol{x}_0)=\prod_{k=1}^Kq(\boldsymbol{x}_k|\boldsymbol{x}_{k-1}).
    \label{equ:forward_diffusion}
\end{equation}
where $\rho_t:=1-\beta_t, \bar{\rho}_t=\prod_{t=1}^t\rho_t$, and the noise is controlled by a variance schedule $\beta_1, ..., \beta_t$ to ensure $p({a}_T)=\mathcal{N}(0, I)$.  The denoise process aims to recover the sample from $p({a}_T)$ by learning
% A parameter policy can be expressed by the reverse process of a conditional diffusion model:
% \begin{equation}
%     \pi_\theta(\boldsymbol{a}|\boldsymbol{s})=p_\theta(\boldsymbol{a}_{0:K}|\boldsymbol{s})=p(\boldsymbol{a}_K)\prod_{k\boldsymbol{=}1}^Kp_\theta(\boldsymbol{a}_{k\boldsymbol{-}1}|\boldsymbol{a}_k,\boldsymbol{s}),
% \end{equation}
% Diffusion models learn 
a conditional distribution $p_\theta(a_{t-1}|a_t, s)$. The policy $\pi_\theta(a|s)$ is typically modeled as:
% and generative new samples by reversing the above process:
\begin{equation}
\begin{split}
    &\pi_\theta(a|s) = 
    % p_\theta({a}_{0:K}|s)=
    p({a}_t)\prod_{t=1}^Tp_\theta({a}_{t-1}|{a}_t,s); p_\theta({a}_{t-1}|{a}_t, s)=\mathcal{N}({a}_{t-1}; {\mu}_\theta({a}_t,t,s),{\Sigma}_\theta({a}_t,t, s)), 
    \label{equ:reverse}
\end{split}
\end{equation}
% where $a_K \sim \mathcal{N}(0,I)$ under the condition that $\prod_{k=1}^K(1-\beta_k)\approx0$.
% This reverse process can be trained via
where $\Sigma_\theta=\beta_t I$ is set as untrained time-dependent constants and $\mu_\theta(a_t,t,s)=\frac{1}{\sqrt{\rho_t}}(a_t-\frac{\beta_t}{\sqrt{1-\bar{\rho}_t}}\epsilon_\theta(a_t,t,s))$ is reparameterized by $\epsilon_\theta$. The trainable parameter $\theta$, modeled by deep networks, can be optimized via
% training is performed by 
% maximizing the evidence lower bound (ELBO) , which can be simplified to the
minimizing the following objective by predicting the noise:
\begin{equation}
\mathcal{L}_{\text{diff}}(\theta)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}}\left[w(s,a)\left\|{\epsilon}-{\epsilon}_\theta\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right].
\label{equ:diffusion_loss}
\end{equation}
where $\mathcal{U}$ is uniform distribution over the discrete set $\{1,..., T\}$. $w(s,a)$ is a flexible weight function that encodes human preference~\citep{zheng2024safe}. For example, $w(s,a)\propto {f}(A(s,a)), f\ge 0$ with $A(s,a)$ as the advantage function leads to weighted behavior cloning (BC) in offline reinforcement learning (RL)~\citep{zheng2024safe, kostrikov2021offline, xu2023offline}, and $w(s,a):=1$ degenerates to traditional BC~\citep{chenoffline}. After obtaining the approximated $\mu_\theta$ and $\Sigma_\theta$, we can substitute them into Eq.~(\ref{equ:reverse}) to iteratively denoise and obtain actions conditioned on the state.
% $\mathbb{E}_{a_0}[\log p_\theta(a_0)]\geq \mathbb{E}_q[\log \frac{p_\theta(a_{0:K}|s)}{q(x_{1:K}|x_0)}]$.
% Diffusion models learn a conditional distribution $p_\theta(x_{t-1}|x_t)$ and generative new samples by reversing the above process:
% \begin{equation}                
%     p_\theta(\boldsymbol{x}_{0:K})=p(\boldsymbol{x}_K)\prod_{k=1}^Kp_\theta(\boldsymbol{x}_{k-1}|\boldsymbol{x}_k),\quad p_\theta(\boldsymbol{x}_{k-1}|\boldsymbol{x}_k)=\mathcal{N}(\boldsymbol{x}_{k-1};\boldsymbol{\mu}_\theta(\boldsymbol{x}_k,k),\boldsymbol{\Sigma}_\theta(\boldsymbol{x}_k,k)),
% \end{equation}
% where $p(x_K)=\mathcal{N}(0,I)$ under the condition that $\prod_{k=1}^K(1-\beta_k)\approx0$. The training is performed by maximizing the evidence lower bound (ELBO): $\mathbb{E}_{x_0}[\log p_\theta(x_0)]\geq \mathbb{E}_q[\log \frac{p_\theta(x_{0:K}}{q(x_{1:K})|x_0}]$. 

\textbf{Problem Setups}. 
We consider a Markov Decision Process with $s\in\mathcal{S}$ and $a\in\mathcal{A}$ are state and action space, $\mathcal{P}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})$ is transition dynamics, and $r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ is reward function. We assume the state space $\mathcal{S}$ and action space $\mathcal{A}$ remain unchanged during training, which is a mild assumption in many relevant works~\citep{peng2019mcp, ajay2023conditional, nair2020awac, A2PR, dai2024mamba}.  
% with differences in the 
We consider an agent with $\pi_0$ as its initial policy and then
% \ljx{mention something about the pretrained $\pi_0$, @bear, DDL 9.19}
is progressively tasked with new tasks $\mathcal{T}_i, i=1,2,...$, 
with differences in the rewards $r$ or dynamics $\mathcal{P}$, to mirror real-world scenarios with non-stationary dynamics or new challenges continually emerge~\citep{luo2024ompo}. Each task is provided with several expert demonstrations $\mathcal{D}^{\mathcal{T}_i}_{e}:=\{(s, a)\}$ or a mixed-quality dataset with reward labels $\mathcal{D}^{\mathcal{T}_i}_{o}:=\{(s, a,r_i,s')\}$. So, we can use either offline RL or imitation learning (IL)~\citep{xudong2024iterative} to adapt to the new challenges~\citep{A2PR}.  Inspired by previous works~\citep{peng2019mcp, barreto2018transfer, zhang2023policy}, we maintain a policy library $\Pi$ to store the policies associated with different tasks and aim to utilize the prior knowledge to enable efficient policy learning and gradually expand it to incorporate new abilities across training.
% incorporate diverse primitive policies across the training, to efficiently store the prior knowledge and avoid training every new task from scratch. 
\begin{equation}
    \Pi=\{\pi_0, \pi_1, \pi_2, \pi_3, ...\}.
    \label{equ:skill_sets}
\end{equation}
% The agent is required to progressively address new tasks $\mathcal{T}_{i}, i=k+1,k+2,...$,
% In this work, we consider solely the settings that use IL to finetune the policy for its superior training stability. However, our framework can also adapt to RL finetuning.
% or a mixed-quality data with rewards $\mathcal{D}^{\mathcal{T}_i}_{o}:=\{(s,a,r,s')\}$.
% Therefore, we can utilize both imitation learning (IL) or offline reinforcement learning (RL) methods to learn the new tasks.
We aim to explore \textit{1) Efficient Expansion}: How to manage the skill library $\Pi$ to learn new skills in an efficient and manageable way, and
% , \textit{2)} how to learn new skills in an parameter- and sample-efficient way, 
\textit{2) Efficient Composition}: How to fully utilize the prior knowledge from primitives in the skill set $\Pi$ to tackle the emerging challenges. 

% \ljx{revised till here}
% we aim to investigate how to utilize the prior knowledge from the primitive policies in the skill set $\Pi$ to enhance the training efficiency, as well as how to maintain this skill library $\Pi$ in an efficient manner. Straightforwardly and naively, we can incorporate all policies $\pi_i$ for all tasks $\mathcal{C}_i$ into the library $\Pi$. However, careful readers may notice that this naive operation will inevitably introduce too much redundancies and drastically increase the computational overhead when the number of tasks gradually grows. Note that some tasks $\mathcal{C}$ might share a large similarity with each other, or in other words, can be potentially composed by others, which thus can be dropped as existing primitives can already express this policy. Therefore, we firstly introduce a $\delta$-relative correlation metric to measure the similarity and compositionality across different skills.
% \subsection{$\delta$-relative independence}

% For the first objective, a straightforward but naive approach would be to include all policies $\pi_i$ for each task $\mathcal{T}_i$ into the library $\Pi$. However, this method can lead to significant redundancies and dramatically increase computational overhead as the number of tasks grows. Given that some tasks $\mathcal{T}$ may exhibit considerable similarity or could potentially be composed by others, it becomes unnecessary to retain all individual policies when existing primitives can sufficiently represent these tasks. To address this, we introduce a new metric called $\delta$-relative independence metric to assess the similarity and compositionality across different skills, enabling more efficient library management.
% We introduce a new metric called \textit{$\delta$-relative correlation}, which measures policy correlation for autonomous agents to leverage and expand upon their existing knowledge base in novel situations.
% \begin{definition}
% [Overall numerical linear correlation] 
% Let $\epsilon$ be a given positive decimal, $0<\epsilon<1$, if $G^{\frac{1}{2}}[a_1, a_2,..., a_k]\leq \epsilon$, the policy set $\{a_1,a_2,...,a_k\}$ for different tasks is said to be $\epsilon$-linear correlation. If for some positive number $\delta$, $0<\delta\leq1$, there is $G^{\frac{1}{2}}[a_1, a_2,..., a_k]\geq \delta$, the vector set $\{a_1,a_2,...,a_k\}$ is said to be $\delta$-linear independence.
% \end{definition}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.99\linewidth]{test.jpg}
%     \caption{Illustrative figure for $\delta$-independence. \ljx{to be revised}}
%     \label{fig:delta_independent}
% \end{figure}

% \begin{definition}
% [$\delta$-relative independence] Consider a  policy set $\Pi=\{\pi_0, \pi_1,\pi_2,...,\pi_k\}$ for different task $\mathcal T_i, i=0, ..., k$. $G[a_0, a_1,a_2, ...,a_k]:={\rm det\ } A_k^TA_k$ is the Gram determinant, where $A_k:=\{a_0, a_1,a_2...,a_k\}$. Given a new policy $\pi_{k+1}$ for task $\mathcal{T}_{k+1}$. The policy relative numerical correlation indicators formula is expressed as follows,
% \begin{equation}\label{equ:correlation}
%     E[\pi_{k+1}|\pi_0, \pi_1,\pi_2,...,\pi_k] = \mathbb{E}_{a_i\sim\pi_i, i=0,...,k+1}\left[\frac{G[a_0, a_1, a_2,...,a_k,a_{k+1}]}{G[a_0, a_1,a_2,...,a_k]}\right]^{1/2}
% \end{equation}
% Let $\epsilon$ be a given positive decimal, $0<\epsilon<1$, if $E[\pi_{k+1}|\pi_0, \pi_1, \pi_2,..., \pi_k]\leq \epsilon$, the new policy $\pi_{k+1}$ is said to be $\epsilon$-relative correlation \textit{w.r.t} the policy set $\Pi=\{\pi_0, \pi_1,\pi_2,...,\pi_k\}$. If for some positive number $\delta$, there is $E[\pi_{k+1}|\pi_1, \pi_2,..., \pi_k]\geq \delta$, then, the new policy $\pi_{k+1}$ is said to be $\delta$-relative independence \textit{w.r.t} the policy set $\Pi=\{\pi_0,\pi_1,\pi_2,...,\pi_k\}$.
% \end{definition}

% In our paper, we utilize mini-batch samples to approximate the expectation term in Eq.~(\ref{equ:correlation}). Specifically, we random sample state $s$ from $\mathcal{D}_e^{\mathcal{T}_{k+1}}$ or $\mathcal{D}_o^{\mathcal{T}_{k+1}}$, and then infer actions $a_i\sim\pi_k(\cdot|s),i=0,...,k$ conditioned on the sampled states. 
% % The action sampled from new policy $\pi_{k+1}$ is approximated directly by the corresponding action \textit{w.r.t} the $s$ in $\mathcal{D}_e^{\pi_{k+1}}$. 
% This metric assesses the changes in relative correlation after incorporating the policy $\pi_{k+1}$ into the skill library $\Pi = \{\pi_0, \pi_1, \pi_2, ..., \pi_k\}$. Specifically, if the value $E[\pi_{k+1}|\pi_0, \pi_1,\pi_2,...,\pi_k]$ is less than 1, it suggests that the correlation among tasks strengthens, as $G[a_0, a_1, a_2,...,a_k,a_{k+1}]<G[a_0, a_1, a_2,...,a_k]$, which represents the actions generated by $\pi_{k+1}$ bring enhanced correlation to existing skills. Conversely, if $E[\pi_{k+1}|\pi_0, \pi_1,\pi_2,...,\pi_k] > 1$, it indicates that the correlation across different tasks weakens with the inclusion of $\pi_{k+1}$. (see Figure 1
% % ~\ref{fig:delta_independent} 
% for illustrations). In this case, the newly introduced policy $\pi_{k+1}$ does not redundantly overlap with the existing skill sets in $\Pi$ and, therefore, should be considered as one new ``orthogonal" skill and be added to the library to enhance the agent's capabilities.
% Intuitively, this metric measures the relative correlation changes after absorbing the policy $\pi_{k+1}$ into the skill library $\Pi=\{\pi_1,\pi_2...,\pi_k\}$. Specifically, if $E[\pi_{k+1}|\pi_1,\pi_2,...,\pi_k]>1$, this means the correlation across different tasks becomes weak after take $\pi_{k+1}$. On the other hand, it determines that the correlation becomes more large. This represents that the new introduced policy $\pi_{k+1}$ cannot be naively composed by existing skill sets $\Pi$, and thus this new skill should be integrated into the set to evolve the agent's capability.
\vspace{-4pt}
\subsection{Efficient Policy Expansion via Low-Rank Adaptation}
\vspace{-4pt}
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{0.41\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/lora.jpg}
        \caption{\small Learning new skills using LoRA modules.}
        \label{fig:lora}
    \end{subfigure}
    \hspace{1cm}
    \begin{subfigure}[b]{0.36\textwidth}
        \centering\includegraphics[width=\textwidth]{figs/compose_lora.jpg}
        \caption{\small Interpolation in LoRA modules.}
        \label{fig:merge_lora}
    \end{subfigure}
    \vspace{-5pt}
    \caption{\small (a) Each skill is encoded in separate LoRA modules respectively. (b) By adjusting the composing weights $\alpha_i$, different LoRA modules can merge together to interpolate new skills.}
    \label{fig:comparison}
    \vspace{-13pt}
\end{figure}

% Next, we demonstrate how to efficiently learn the identified new skill u
% \ljx{briefly highlight the drawback of existing methods}
For the first objective,
% about how to efficiently learn new skills, 
previous methods typically train each primitive
% skill
from scratch in a tabula rasa paradigm
~\citep{peng2019mcp,janner2022planning, lu2023contrastive}, failed to leverage the prior knowledge in $\Pi$ to 
% enhance training efficiency. 
efficiently obtain a good skill primitive. 
% Therefore, they typically require enormous data as well as training samples to obtain a good skill primitive,
This presents significant issues in terms of computational efficiency when the number of skills grows.
% , making the skill library hard to maintain in an efficient way.
% and can not deal with an unseen task with limited data. 
To mitigate these challenges,
we turn to Parameter-Efficient Fine-Tuning~(PEFT)~\citep{ ding2023parameter},
% Parameter-Efficient Fine-Tuning~(PEFT)~\citep{hu2022lora, ding2023parameter} comes to our attention,
which has proven highly effective in various
% significant success in various 
natural language processing and computer vision applications. 
One of the most popular PEFT implementations is LoRA~\citep{hu2022lora}.
% Among these techniques, LoRA~\citep{hu2022lora} is one of the most popular PEFT implementations, which 
It injects trainable low-rank decomposed matrices into the pretrained layer to avoid overfitting with limited adaptation data and significantly reduces computational and memory burden.
 % In this section, we focus on enabling the agent to learn new tasks. We use $\delta$-relative correlation as mentioned earlier to determine whether the agent is acquiring new skills while meeting new tasks. \ljx{motivate, why lora} 
 Inspired by this, we try to employ LoRA to efficiently learn new skills given solely limited data for the target skill. 

% Given a pre-trained model, let's consider one layer weight matrix in it, denoted as $\boldsymbol{\mathit{W}}\in\mathbb{R}^{d \times k}$. Its input and output hidden states are $h_{in} \in \mathbb{R}^d$ and $h_{out}\in \mathbb{R}^k$, respectively. Then we have $h_{out}=\boldsymbol{\mathit{W}}^{\top} h_{in}$. 

\textbf{Policy Expansion via Low-Rank Adaptation}. We consider a pretrained policy $\pi_0$ and denote \({\mathit{W}_0} \in \mathbb{R}^{d_{\rm in} \times d_{\rm out}}\) as its associated weight matrix. 
Directly finetuning $W_0$ to adapt to new skills might be extremely inefficient~\citep{liu2023tail}, instead, we introduce a tune-able LoRA module $\Delta W$ upon $W_0$, \textit{i.e.}, ${\mathit{W}_0} + \Delta {\mathit{W}} = {\mathit{W}_0} + BA$ to do the adaptation and keep ${\mathit{W}_0}$ frozen, where $B\in\mathbb{R}^{d_{\rm in}\times n}$, $A\in \mathbb{R}^{n\times d_{\rm out}}$ and $n\ll \min(d_{\rm in}, d_{\rm out})$. Specifically, the input feature of the linear layer is denoted as $h_{\rm in} \in \mathbb{R}^{d_{\rm in}}$, and the output feature of the linear layer is $h_{\rm out} \in \mathbb{R}^{d_{\rm out}}$,
the final output of a LoRA augmented layer can be calculated through the following forward process: 
\begin{equation}
    h_{\rm out} = (\mathit{W}_0 + \alpha \Delta \mathit{W})h_{\rm in} = (\mathit{W}_0 + \alpha BA)h_{\rm in} = \mathit{W}_0 h_{\rm in} + \alpha BAh_{\rm in},
    \label{equ:lora}
\end{equation}
% This relationship is defined by the following equation: $h_{out} = \boldsymbol{\mathit{W}}^{\top} h_{in}$. This structure is key to the layer’s operation in the model. 
% \begin{equation}
%     h_{out} = \boldsymbol{\mathit{W}}^{\top} h_{in}
% \end{equation}
% This shows how the input hidden state $h_{in}$ is transformed through the transposed weight matrix \(\boldsymbol{\mathit{W}}^{\top}\), producing the output hidden state $h_{out}$. This structure is key to the layer’s operation in the model.
% \ljx{add a figure here, and adhere the original A and B, rather than Wup and Wdown}
% This integration method, often associated with LoRA, introduces trainable low-rank matrices $\boldsymbol{\mathit{W}}_{down}\in \mathbb{R}^{d\times r}$ and $\boldsymbol{\mathit{W}}_{up}\in \mathbb{R}^{r\times k}$, $\Delta \boldsymbol{\mathit{W}}=\boldsymbol{\mathit{W}}^{\top}_{up}\boldsymbol{\mathit{W}}^{\top}_{down}$. Here $r$ represents the rank and is usually much smaller than the dimensions of the original matrix. These matrices are typically integrated in parallel with the original weight matrix $\boldsymbol{\mathit{W}}$ through addition, 
% \begin{equation}
%     h_{out} = \boldsymbol{\mathit{W}}^{\top}h_{in} + \alpha \Delta \boldsymbol{\mathit{W}}h_{in},
% \end{equation}
where $\alpha$ is a weight to balance the pre-trained model and LoRA modules. This operation naturally prevents catastrophic forgetting in a parameter isolation approach, and the low-rank decomposition structure of $A$ and $B$ significantly reduces the computational burden.
Benefiting from this lightweight characteristic, we can manage numerous LoRA modules $\{\Delta \mathit{W}_i = B_i A_i| i\in 1,2,...,k\}$ 
to encode different skill primitives $\pi_i$, respectively, as shown in Figure~\ref{fig:lora}. This flexible approach allows us to easily integrate new skills based on existing knowledge, while also facilitating library management by removing suboptimal primitives and retaining the effective ones. More importantly, by adjusting the value of $\alpha$, it holds the potential to interpolate the pretrained skill in $W_0$ and other primitives in $\Delta W_i$~\citep{clark2024directly} to generate novel skills, as shown in Eq.~(\ref{equ:lora_compose}) and Figure~\ref{fig:merge_lora}. 
\begin{equation}
    W = W_0 + \sum_{i=1}^{k}\alpha_i \Delta W_i = W_0 + \sum_{i=1}^{k} \alpha_i B_i A_i,
    \label{equ:lora_compose}
\end{equation}
where $\alpha_i$ is the weight to interpolate pre-trained weights and LoRA modules. This interpolation property has been explored in fields like text-to-image generation~\citep{clark2024directly} and language modeling~\citep{zhang2023composing}, but its application in decision-making scenarios remains highly underexplored, despite LoRA has proven efficacy in skill acquisition~\citep{liu2023tail}. Next, we will elaborate on how to effectively combine LoRA modules to adapt to decision-making applications.
% 
% Different LoRA modules can be merged to generalize to unseen skills by adjusting the composing weights.
% , as shown in Figure~\ref{fig:merge_lora}~\citep{clark2024directly}. 
% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.99\linewidth]{figs/ComLoRA.pdf}
%     \caption{Illustrative figure for LoRA modules. }
%     \label{fig:lora_modules}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.25\linewidth]{figs/lora2.pdf}
%     \caption{Illustrative figure for LoRA modules. }
%     \label{fig:lora}
% \end{figure}

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.25\linewidth]{figs/merge.pdf}
%     \caption{Illustrative figure for LoRA modules. }
%     \label{fig:merge_lora}
% \end{figure}


% \begin{figure}[ht]
%     \vskip 0.1in
%     \centering
%     \subfigure[]{
% 		\begin{minipage}[t]{0.3\linewidth}
% 			\centering
% 			\includegraphics[width=1.5in]{figs/lora2.pdf}
% 		\end{minipage}%
% 	}%
% 	\subfigure[]{
% 		\begin{minipage}[t]{0.3\linewidth}
% 			\centering
% 			\includegraphics[width=1.55in]{figs/merge.pdf}
% 		\end{minipage}%
% 	}%
%     \caption{The performance of different methods in the mixed policy datasets and the comprehensive ablation study of A2PR on halfcheetah-medium-v2 with different components.
%     }
%     \label{fig:mix_dataset}
%     % \vskip -0.2in
%     \vspace{-5mm}
%     \end{figure}
    
% We individually sample different actions $\{a_1, a_2, ..., a_k\}$ and $a_{new}$ on the same states from a policy set with different LoRA modulates and the training data of the new task. 

% Then we can obtain the $\delta$-relative correlation between the new task and the previous tasks by Equation (\ref{equ:correlation}),

% \begin{equation}\label{equ:correlation}
%     E[\pi_{new}|\pi_1,\pi_2,...,\pi_k] = \mathbb{E}_{\substack{a_i\sim\pi_i, i=1,...,k, \\ a_{new}\sim \pi_{new}}} \left[\frac{G[a_1, a_2,...,a_k,a_{new}]}{G[a_1,a_2,...,a_k]}\right]^{1/2}
% \end{equation}
% If $\delta$-relative correlation satisfies $\delta$-relative independence, we will add the policy with LoRA modulates of the new task to the policy set.

\vspace{-5pt}
\subsection{Context-aware Composition in Parameter Space}
\vspace{-4pt}

% During the context-aware composition, our goal is to optimize the coefficients $\alpha$ to leverage the diverse skills for boosting the policy's performance on the unseen task. 
% Now, we have different LoRA modules for diverse skills. 
Effectively combining skills encoded as different LoRA modules to solve new tasks is crucial. 
% Eq.~(\ref{equ:lora_compose}) shows that by adjusting the weight $\alpha_i$ for each skill respectively, we can form diverse new skills by interpolating and extrapolating different LoRA modules. 
Previous methods~\citep{du2024position, ajay2023conditional, janner2022planning} typically rely on fixed combinations of skills, resulting in limited compositional flexibility. This approach may be acceptable in static domains like language models, but it falls short in decision-making applications where dynamic skill composition is crucial. 
% Especially, agent can encounter varied scenarios when deployed in the environment, requiring the dynamical composition of different skills to best solve the current problems.
For example, in autonomous driving, the ability to dynamically prioritize skills of obstacle avoidance in potential collision scenarios, or acceleration when speeds are suboptimal, is essential. 
% locomotion task and a policy set $\Pi$ that contains a
% move forward skill $\pi_1$ and a stand skill $\pi_2$, when we compose these two skills to solve a run task, $\alpha_i(s;\theta)$ would assign higher weights for the stand skill $\pi_2$ when the agent is going to fall. 
Naively adopting a fixed set of $\alpha_i$ like previous approaches~\citep{du2024position, ajay2023conditional, janner2022planning, clark2024directly}, however, cannot adequately support such flexible deployment of skills based on real-time environmental demands.


%, or are suffered from weak model expressiveness.
\textbf{Context-aware Composition}. We propose a simple yet effective context-aware composition method that adaptively leverages pretrained knowledge to optimally address the encountering tasks according to the agent's current context. Specifically, we introduce a context-aware modular $\alpha(s;\theta)\in\mathbb{R}^k$ with $\alpha_i$ as its $i$-th dimension. The composition method can be expressed by Eq.~(\ref{equ:lora_adaptive_compose}):
\begin{equation}
\begin{split}
    W({\theta}) &= W_0 + \sum_{i=1}^{k}\alpha_i(s;\theta) \Delta W_i = W_0 + \sum_{i=1}^{k} \alpha_i(s;\theta) B_i A_i.\\   
\end{split}
\label{equ:lora_adaptive_compose}
\end{equation}
Here, $\alpha(s;\theta)$ adaptively adjusts output weights based on the agent's current situation $s$ with the parameter $\theta$ optimized via minimizing the diffusion loss in Eq.~(\ref{equ:diffusion_loss}).
Note that the trainable parameter $\theta$ lies solely in the composition network $\alpha_\theta$ with the pretrained weights $W_0$ and all LoRA modules $\Delta W_i$ being kept frozen, thus $\theta$ can be efficiently trained in terms of both samples and parameters. 


\begin{figure}[t]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/para_score_action.jpg}
    \vspace{-10pt}
    \caption{\small Comparison between parameter-, noise-, and action-level composition. Parameter-level composition offers more flexibility to leverage the shared or complementary structure across skills to compose new skills. Noise- and action-level composition, however, is too late to benefit from this information.}
    \label{fig:para_score_action}
    % \vspace{+2pt}
    % \includegraphics[width=0.95\textwidth]{test.jpg}
    % \vspace{-5pt}
    % \caption{\small t-SNE projection of different skills in parameter, noise, and action space. Different skills exhibit more overlaps in parameter space, making the knowledge sharing more easy than in noise and action space.}
    % \label{fig:tsne}
    \vspace{-10pt}
\end{figure}

\begin{figure}[t]
    \centering
    % \begin{subfigure}[b]{0.3\textwidth}
    %     \centering
    %     \includegraphics[width=\textwidth]{figs/parameter_com.jpg}
    %     \caption{t-SNE for parameter-level composition}
    %     % \label{fig:lora}
    % \end{subfigure}
    % \hspace{0.2cm}
    % \begin{subfigure}[b]{0.3\textwidth}
    %     \centering\includegraphics[width=\textwidth]{figs/score_com1.jpg}
    %     \caption{t-SNE for score-level composition}
    %     % \label{fig:merge_lora}
    % \end{subfigure}
    % \hspace{0.2cm}
    % \begin{subfigure}[b]{0.3\textwidth}
    %     \centering\includegraphics[width=\textwidth]{figs/action_com1.jpg}
    %     \caption{t-SNE for action-level composition}
    %     % \label{fig:merge_lora}
    % \end{subfigure}
    \includegraphics[width=0.92\textwidth]{figs/tsne.jpg}
    \vspace{-10pt}
    \caption{\small t-SNE projections of samples from different skills in parameter, noise, and action space. The parameter space exhibits a good structure for skill composition, where skills share common knowledge while retaining their unique features to avoid confusion. Noise and action spaces are either too noisy to clearly distinguish between skills or fail to capture the shared structure across them. See Appendix~\ref{subsec:tsne} for details.}
    % different preserving 
    % skills exhibit more overlaps in parameter space, making the knowledge sharing more easy than in noise and action space.}
    \label{fig:tsne}
    \vspace{-13pt}
\end{figure}

% \begin{figure}
%     \centering
    
%     \label{fig:lora-module}
% \end{figure}

 \textbf{Parameter-level \textit{v.s.} Action-level Composition}. Careful readers may notice that our context-aware composition is similar to previous works that adaptively compose Gaussian primitive skills to create complex behaviors~\citep{peng2019mcp, qureshi2020composing}, such as the one shown in Eq.~(\ref{equ:gaussian_compose})~\citep{peng2019mcp}:
% Previous methods rely on  to create complex behaviors, which can be expressed by the follow equation:
\vspace{-5pt}
\begin{equation}
\pi(a|s)=\frac1{Z(s)}\prod_{i=1}^k\pi_i(a|s)^{\alpha_i(s;\theta)},\quad\pi_i(a|s)=\mathcal{N}\left(\mu_i(s),\Sigma_i(s)\right),
\label{equ:gaussian_compose}
\end{equation}
where $\alpha(s;\theta)$ is optimized to combine the policy distributions $\pi_i, i=0,...,k$ to collaboratively build a new policy distribution $\pi$ to solve the new task.

However, these two methods differ fundamentally in their stages of composition, mirroring the advantages of \textit{early fusion} over \textit{late fusion} across various domains~\citep{gadzicki2020early, wang2024emiff}. PSEC employs a \textit{parameter-level composition}, where different skills are seamlessly integrated within the parameter space. By contrast, Eq.~(\ref{equ:gaussian_compose}) represents an \textit{action-level composition} that explicitly combines the output distributions of various skills. In comparison, \textit{parameter-level composition} will be more efficient, as it 
% directly composes different skills in the parameter spaces before generating the final policy distributions, which 
can leverage more shared or complementary information between different skills to enhance compositionality and overall performance before generating the final policy distribution~\citep{shazeer2016outrageously, wang2024sparse}.
% , as supported by recent advances in MoE~\citep{shazeer2016outrageously} for policy transfers~\citep{shazeer2016outrageously, wang2024sparse}.
Conversely, \textit{action-level composition} only merges skills after the action generation, which is too late to effectively leverage features across skills for optimal composition.
% In the middle between \textit{parameter-level} and \textit{action-level} composition, the recent progresses on composing different diffusion models by combining the scores can be regarded as a \textit{score-level composition} that compose the 
Besides, previous \textit{action-level} methods typically employ simple Gaussian primitives to construct their skill library, significantly limiting its expressiveness. 
% Moreover, Eq.~(\ref{equ:gaussian_compose}) can only introduce interpolation composition, but our approach in Eq.~(\ref{equ:lora_adaptive_compose}) can lead to both interpolated and extrapolated compositions, enjoying enhanced composition ability to form more complex distributions.
% by leveraging the property of LoRA module
% Our method allows an effective agent to adapt its previously learned skills and knowledge to different states, ensuring a more flexible and context-sensitive response. While this may seem to bear some resemblance to previous composition methods in the decision-making domain, our method is quite different in nature from previous composition methods. 
% , which is more efficient by leveraging the property of LoRA module, as when generating the action, the model already considering the composition in the model's parameter
% Previous methods~\citep{peng2019mcp, qureshi2020composing} rely on composing Gaussian primitive skills to create complex behaviors, which can be expressed by the follow equation:
% \begin{equation}
% \pi(a|s)=\frac1{Z(s)}\prod_{i=1}^k\pi_i(a|s)^{w_i(s)},\quad\pi_i(a|s)=\mathcal{N}\left(\mu_i(s),\Sigma_i(s)\right).
% \end{equation}
% Although these methods can compose primitives to address the combinatorial explosion by factoring the agent’s behavior without relying on time, they are constrained by their explicit task composition in the action space. Moreover, their expressiveness is limited due to their dependence on Gaussian-based primitive skills.

% \ljx{revised till here}
\textbf{Parameter-level \textit{v.s.} Noise-level Composition}. Some approaches use diffusion models for policy modeling and exhibit remarkable compositionality by identifying its connections to Energy-Based Models~\citep{du2024position, wang2024poco, janner2022planning, ajay2023conditional, lu2023contrastive}. Specifically, the noise predicted by diffusion models can be regarded as the gradient field of some energy functions. It thus can be directly merged to form new skills during sampling in a \textit{noise-level composition}, as shown in Eq.~(\ref{equ:score_compose}). This is equivalent to doing a logical operation on the energy functions to form complex behaviors~\citep{du2023reduce, liu2022compositional, lecun2006tutorial}.
% , effectively merging different skills 
\begin{equation}
    {\epsilon}(a_t, t, s) = \sum_{i=0}^k\alpha_i\epsilon_i(a_t, t, s).
    \label{equ:score_compose}
\end{equation}
% , combining diffusion policies at the score level of Energy-Based Models. 
 Here, $\epsilon_i$ represents the predicted noise derived from various skills, while ${\epsilon}$ is the aggregated noise resulting from their composition. Utilizing ${\epsilon}$ for denoising in Eq.~(\ref{equ:reverse}) allows for the generation of a joint distribution of skills, thereby facilitating the effective composition of these diverse capabilities~\citep{ajay2023conditional, ajay2024compositional, janner2022planning}. However, these methods employ fixed weights $\alpha_i$ for policy composition, limiting their flexibility and adaptability in dynamical scenarios where real-time adjustment on the compositional weights is required. In our paper, PSEC not only employs diffusion models to enhance the expressiveness of primitives, but also adaptively adjusts the context-aware compositional weights to enhance compositional flexibility. Additionally, this \textit{noise-level composition} also tends to be less effective than \textit{parameter-level composition}, as the latter integrates different skills at an earlier stage, leading to improved performance, as shown in Figure~\ref{fig:para_score_action}.

\textbf{Empirical Observations}. To evaluate the advantages of parameter-level composition over other levels of composition, we employ t-SNE~\citep{van2008visualizing} to project the output features of LoRA modules into a 2D space, alongside the noise and generated actions of various skills. Figure~\ref{fig:tsne} illustrates that in the parameter space, different skills not only share common knowledge, but also retain their unique features to avoid confusion. In contrast, noise and action spaces are either too noisy to clearly distinguish between skills or fail to capture the shared structure across them, making the compositions in noise and action space less effective than the parameter space.
% different skills exhibit significant overlaps, highlighting the potential to utilize this shared information across tasks to enhance compositions. In contrast, in the noise and action space, the overlaps among different skills gradually decrease, which complicates knowledge sharing between them.
% These methods are a composition of score levels. 




% \subsection{Theory}
\vspace{-8pt}
\section{Experiments}
\vspace{-7pt}
PSEC enjoys remarkable versatility across various scenarios since many problems can be resolved by reusing pre-trained policies and gradually evolving its capabilities during training. Thus, we present a comprehensive evaluation across diverse scenarios, including multi-objective composition, policy learning under policy shifts and dynamics shifts, to answer the following questions:

\begin{itemize}
    \item Can the context-aware modular effectively compose different skills?
    \item Can our parameter-level composition outperform noise- and action-level compositions?
    \item Can the introduction of LoRA modules enhance training and sample efficiency?
    \item Can PSEC framework iteratively evolve after incorporating more skills?
\end{itemize}
% First, we assess PSEC's performance on the D4RL benchmark using offline datasets to showcase its effectiveness in composing policies for individual tasks. Next, we evaluate its performance on the DSRL benchmark in a multi-task setting. Finally, we conduct continual learning experiments using the DeepMind control suite to illustrate PSEC's adaptability over new tasks.

% \ljx{add more discussions about why PSEC framework can apply to these diverse settings to let the reviewer to better understand the versatility of our method}

% * LoRA, separated LoRA can represent separated policies, whether the policy is the different level policy on the single tasks or the decomposed policy for the multi-tasks. Meanwhile, this structure natively avoids the catastrophic forgetting problem
% * parameter composition,  composing the policy on the parameter level, which can retain the advantage of different policies inherently. Therefore, PSEC can better leverage diverse knowledge to solve the tasks together.
% * 
% \ljx{we don't need this detailed discussion, just intuitively and briefly mention our versatility. For example, we can add a texbf paragraph to show our evaluation setups. This is experiment, we don't need to emphasize our advantage in technical perspective too much, just briefly mention them is ok.}

% This:
% 突然发现说中文也行。
% 可能简单地讲一下就好：类似于，PSEC可以高效利用pretrain的策略来学习新任务。那么，这个framework理论上可以赋能很多场景，比如说，当pretrain策略和target策略其实是一个任务的时候，我们的PSEC就应用到了经典的single-task finetuning的setting。当Pretrain策略和target任务关系不大的时候，就是是一个multi-task compostion。当target任务一直在增多的时候，就是一个continue learning的任务。因此，可以看出来PSEC的versatility非常好。所以，我们这篇paper从这几个经典任务场景出发，均展开了详尽的实验。

% action-level的组合，我们使用diffusion mlp学习策略还是使用lora学习策略呢，两种都可以实现。
% ok
% 这样：
% 我们主要的baseline:
% (Ours): diffusion + lora + 动态组合 + parameter-level
% (action-level): diffusion + lora + 动态组合 + action-level
% (score-level): diffusion + lora + 动态组合 + score-level
% (不利用pretrain策略的一系列baseline)

% ablation:
% (Ours w/o lora): diffusion + mlp + 动态组合 + score-level/action-level (因为不用lora就没法parameter组合了)
% (Ours w/o 动态组合)： diffusion + lora + 静态组合 + parameter-level

% For example: 
% \paragraph{Evaluation Setups} We can see the PSEC framework enjoys superior versatility and is applicable to diverse scenarios. PSEC can leverage pretrained policies and LoRA flexibly to facilitate finishing the new task efficiently. For instance, 1) When the pretrained policies and the target policies are the same task, PSEC can be used in the single-task finetuning setting. 2) When the pretrained policies are one of the decomposed policies of the target policies, PSEC can compose these pretrained policies to achieve multi-task setting. 3) When the target tasks appear incrementally, PSEC can leverage the previous policies to learn the new policies continually. 

% \textbf{Evaluation Setups.}  Specifically, 1) In \textit{multi-objective composition}, the pretrained policies can be regarded as different components of a target task, and we aim to effectively compose these elements to handle the new task to evaluate the compositionality of our context-aware composition in parameter space over other composition methods; 2) In \textit{dynamics shift} setting, the pretrained policy and current task undergoes different transition dynamics~\citep{luo2024ompo, niu2022trust}, we aim to leverage the transferable knowledge across prior dynamics to facilitate efficient new task training with limited samples. (3) In \textit{policy shift} setting, the agent is progressively tasked with new tasks with different rewards. We aim to continuously expanding the capabilities of agents as well as solve new tasks leveraging the prior knowledge. 

% single-task fine-tuning, PSEC can optimize performance when the pre-trained policies align directly with the target task. 2) In multi-task composition settings, when the pre-trained policies correspond to components of the target task, PSEC can effectively compose these to handle the new task. 3) For continual learning, where tasks emerge incrementally, PSEC can adaptively leverage previously learned policies to facilitate the learning of new ones. 

\vspace{-5pt}
\subsection{Multi-objective Composition}
\vspace{-5pt}
In many real-world applications, a complex task can be decomposed into simpler objectives, where collaboratively combining these atomic skills can tackle the complex task. In this setting, we aim to evaluate the advantages of parameter-level composition over other levels of composition in Figure~\ref{fig:para_score_action}, and the effectiveness of the context-aware modular. We consider one practical multi-objective composition scenario within the safe offline RL domain~\citep{zheng2024safe}. This setting requires solving a constrained MDP~\citep{altman2021constrained} to tackle a complex trilogy objective: avoiding distributional shift, maximizing rewards, and meanwhile minimizing costs. These objectives can conflict, thus requiring a nuanced composition to optimize performance effectively~\citep{zheng2024safe}. 

% PSEC effectively utilizes this decompose-compose approach. In safety reinforcement learning~(RL), the objective is to maximize expected cumulative rewards while adhering to cost constraints—a challenging problem framed as a Constrained Markov Decision Process~\citep{altman2021constrained}. Interestingly, this problem can be simplified into two basic objectives: maximizing rewards and minimizing costs.



% is trained for 1 million steps with 4 random seeds to generate the two basic policies, and only train 1K steps for composing the return policy and the cost policy. The experiments show the superiority of PSEC, achieving the state-of-art performance on reward and cost in all tasks.

\begin{table}[t]
\large
\renewcommand{\arraystretch}{1.2} 
\setlength{\tabcolsep}{0.1pt}
\centering
\caption{\small Normalized DSRL~\citep{liu2023datasets} benchmark results. Costs below 1 indicates safety.
$\uparrow$: the higher the better. $\downarrow$: the lower the better. 
Results are averaged over 20 evaluation episodes and 4 seeds. 
% {\color[HTML]{656565} Gray}: Unsafe agents.
\textbf{Bold}: Safe agents with costs below 1. 
{\color[HTML]{0000FF} \textbf{Blue}}: Safe agents achieving the highest reward.}
% \ljx{tune NSEC}} 
\label{tab:main-result}
\vspace{-5pt}
\resizebox{\textwidth}{!}{
\begin{tabular}{ccccccccccccccccc}
\hline
\toprule[1pt]
& \multicolumn{2}{c}{BC}  
& \multicolumn{2}{c}{CDT}  
% & \multicolumn{2}{c}{BCQ-Lag} 
& \multicolumn{2}{c}{CPQ} 
& \multicolumn{2}{c}{COptiDICE} 
% & \multicolumn{2}{c}{TREBI} 
& \multicolumn{2}{c}{FISOR} 
& \multicolumn{2}{c}{ASEC} 
& \multicolumn{2}{c}{NSEC} 
& \multicolumn{2}{c}{PSEC} 
\\ 
\cmidrule(r){2-17}
% \cmidrule(r){2-3} \cmidrule(r){4-5} \cmidrule(r){6-7} 
% \cmidrule(r){8-9} \cmidrule(r){10-11}
% \cmidrule(r){12-13} 
% \cmidrule(r){14-15}
\multirow{-2}{*}{Task} & reward $\uparrow$ & cost $\downarrow$ & reward $\uparrow$ & cost $\downarrow$ & reward $\uparrow$  & cost $\downarrow$  & reward $\uparrow$  & cost $\downarrow$  & reward $\uparrow$ & cost $\downarrow$
& reward $\uparrow$   & cost $\downarrow$  
& reward $\uparrow$  & cost $\downarrow$  
& reward $\uparrow$  & cost $\downarrow$  
% \\ \midrule
% % BC
% & {\color[HTML]{656565} 0.72}  & {\color[HTML]{656565} 10.58}
% % % CDT
% % & {\color[HTML]{656565} 0.63}  & {\color[HTML]{656565} 2.09}
% % % BCQ-Lag
% % & {\color[HTML]{656565} 0.71}  & {\color[HTML]{656565} 10.63}
% % % CPQ
% % & {\color[HTML]{656565} 0.32}  & {\color[HTML]{656565} 5.28}
% % % COptiDICE
% % & {\color[HTML]{656565} 0.55}  & {\color[HTML]{656565} 7.64}
% % % TREBI
% % & {\color[HTML]{656565} 0.56}  & {\color[HTML]{656565} 2.45}
% % ours
% & {\color[HTML]{656565} \textbf{0.39}}  & {\color[HTML]{656565} \textbf{0.10}}
\\ \midrule
easysparse
% BC
& {\color[HTML]{656565} 0.32}  & {\color[HTML]{656565} 4.73}
% CDT
& \textbf{0.05}  & \textbf{0.10}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.99}  & {\color[HTML]{656565} 14.00}
% CPQ
% & \textbf{-0.06}  & \textbf{0.24}
& \textbf{-0.06}  & \textbf{0.24}
% COptiDICE
& {\color[HTML]{656565} 0.94}  & {\color[HTML]{656565} 18.21}
% % TREBI
% & {\color[HTML]{656565} 0.26}  & {\color[HTML]{656565} 6.22}
% ours
& \textbf{{0.38}}  & \textbf{{0.53}}
& {\color[HTML]{656565} {0.95}}  & {\color[HTML]{656565} {5.8}}
& \textbf{{0.55}}  & \textbf{{0.08}}
& {\color[HTML]{0000FF} \textbf{0.55}}  & {\color[HTML]{0000FF} \textbf{0.02}}
\\
easymean
% BC
& {\color[HTML]{656565} 0.22}  & {\color[HTML]{656565} 2.68}
% CDT
& \textbf{0.27}  & \textbf{0.24}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.54}  & {\color[HTML]{656565} 10.35}
% CPQ
& \textbf{-0.06}  & \textbf{0.24}
% COptiDICE
& {\color[HTML]{656565} 0.74}  & {\color[HTML]{656565} 14.81}
% % TREBI
% & {\color[HTML]{656565} 0.19}  & {\color[HTML]{656565} 4.85}
% ours
& \textbf{{0.38}}  & \textbf{{0.25}}
& {\color[HTML]{0000FF} \textbf{0.63}}  & {\color[HTML]{0000FF} \textbf{0.75}}
& \textbf{{0.39}}  & \textbf{{0.54}}
& \textbf{{0.37}}  & \textbf{{0.00}}
\\
easydense 
% BC
& {\color[HTML]{656565} 0.20}  & {\color[HTML]{656565} 1.70}
% CDT
& {\color[HTML]{656565} 0.43}  & {\color[HTML]{656565} 2.31}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.40}  & {\color[HTML]{656565} 6.64}
% CPQ
& \textbf{-0.06}  & \textbf{0.29}
% COptiDICE
& {\color[HTML]{656565} 0.60}  & {\color[HTML]{656565} 11.27}
% % TREBI
% & {\color[HTML]{656565} 0.26}  & {\color[HTML]{656565} 5.81}
% ours
& \textbf{{0.36}}  & \textbf{{0.25}}
& {\color[HTML]{656565} {0.85}}  & {\color[HTML]{656565} {5.28}}
& {\color[HTML]{656565} {0.76}}  & {\color[HTML]{656565} {1.45}}
& {\color[HTML]{0000FF} \textbf{0.51}}  & {\color[HTML]{0000FF} \textbf{0.01}}
\\
mediumsparse
% BC
& {\color[HTML]{656565} 0.53}  & {\color[HTML]{656565} 1.74}
% CDT
& {\color[HTML]{656565} 0.26}  & {\color[HTML]{656565} 2.20}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.93}  & {\color[HTML]{656565} 7.48}
% CPQ
& \textbf{-0.08}  & \textbf{0.18}
% COptiDICE
& {\color[HTML]{656565} 0.64}  & {\color[HTML]{656565} 7.26}
% % TREBI
% & {\color[HTML]{656565} 0.06}  & {\color[HTML]{656565} 1.70}
% ours
& \textbf{{0.42}}  & \textbf{{0.22}}
& {\color[HTML]{656565} {0.93}}  & {\color[HTML]{656565} {2.52}}
& \textbf{{0.60}}  & \textbf{{0.08}}
& {\color[HTML]{0000FF} \textbf{0.76}}  & {\color[HTML]{0000FF} \textbf{0.03}}
\\
mediummean
% BC
& {\color[HTML]{656565} 0.66}  & {\color[HTML]{656565} 2.94}
% CDT
& {\color[HTML]{656565} 0.28}  & {\color[HTML]{656565} 2.13}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.60}  & {\color[HTML]{656565} 6.35}
% CPQ
& \textbf{-0.08}  & \textbf{0.28}
% COptiDICE
& {\color[HTML]{656565} 0.73}  & {\color[HTML]{656565} 8.35}
% % TREBI
% & {\color[HTML]{656565} 0.20}  & {\color[HTML]{656565} 1.90}
% ours
& \textbf{{0.39}}  & \textbf{{0.08}}
& {\color[HTML]{656565} {0.74}}  & {\color[HTML]{656565} {1.00}}
& {\color[HTML]{656565} {0.82}}  & {\color[HTML]{656565} {2.87}}
& {\color[HTML]{0000FF} \textbf{0.61}}  & {\color[HTML]{0000FF} \textbf{0.01}}
\\
mediumdense 
% BC
& {\color[HTML]{656565} 0.65}  & {\color[HTML]{656565} 3.79}
% CDT
& \textbf{0.29}  & \textbf{0.77}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.64}  & {\color[HTML]{656565} 3.78}
% CPQ
& \textbf{-0.08}  & \textbf{0.20}
% COptiDICE
& {\color[HTML]{656565} 0.91}  & {\color[HTML]{656565} 9.52}
% % TREBI
% & {\color[HTML]{656565} 0.03}  & {\color[HTML]{656565} 1.18}
% ours
& \textbf{{0.49}}  & \textbf{{0.44}}
& {\color[HTML]{0000FF}\textbf{0.81}}  & {\color[HTML]{0000FF}\textbf{0.52}}
& \textbf{{0.76}}  & \textbf{{0.27}}
&  \textbf{{0.66}}  & \textbf{{0.02}}
\\
hardsparse
% BC
& {\color[HTML]{656565} 0.28}  & {\color[HTML]{656565} 1.98}
% CDT
& \textbf{0.17}  & \textbf{0.47}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.48}  & {\color[HTML]{656565} 7.52}
% CPQ
& \textbf{-0.04}  & \textbf{0.28}
% COptiDICE
& {\color[HTML]{656565} 0.34}  & {\color[HTML]{656565} 7.34}
% % TREBI
% & {\color[HTML]{656565} 0.00}  & {\color[HTML]{656565} 0.82}
% ours
&  \textbf{0.30}  &  \textbf{0.01}
& \textbf{{0.30}}  & \textbf{{0.41}}
& {\color[HTML]{656565} {0.34}}  & {\color[HTML]{656565} {1.21}}
& {\color[HTML]{0000FF} \textbf{0.34}}  & {\color[HTML]{0000FF} \textbf{0.04}}
\\
hardmean
% BC
& {\color[HTML]{656565} 0.34}  & {\color[HTML]{656565} 3.76}
% CDT
& {\color[HTML]{656565} 0.28}  & {\color[HTML]{656565} 3.32}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.31}  & {\color[HTML]{656565} 6.06}
% CPQ
& \textbf{-0.05}  & \textbf{0.24}
% COptiDICE
& {\color[HTML]{656565} 0.36}  & {\color[HTML]{656565} 7.51}
% % TREBI
% & {\color[HTML]{656565} 0.16}  & {\color[HTML]{656565} 4.91}
% ours
& \textbf{{0.26}}  & \textbf{{0.09}}
& {\color[HTML]{656565} {0.46}}  & {\color[HTML]{656565} {1.05}}
& \textbf{{0.38}}  & \textbf{{0.32}}
& {\color[HTML]{0000FF} \textbf{0.39}}  & {\color[HTML]{0000FF} \textbf{0.07}}
\\
harddense 
% BC
& {\color[HTML]{656565} 0.40}  & {\color[HTML]{656565} 5.57}
% CDT
& {\color[HTML]{656565} 0.24}  & {\color[HTML]{656565} 1.49}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.39}  & {\color[HTML]{656565} 5.11}
% CPQ
& \textbf{-0.04}  & \textbf{0.24}
% COptiDICE
& {\color[HTML]{656565} 0.42}  & {\color[HTML]{656565} 8.11}
% % TREBI
% & {\color[HTML]{656565} 0.02}  & {\color[HTML]{656565} 1.21}
% ours
& \textbf{{0.30}}  & \textbf{{0.34}}
& {\color[HTML]{0000FF} \textbf{0.36}}  & {\color[HTML]{0000FF} \textbf{0.82}}
& {\textbf{0.19}}  & {\textbf{0.03}}
& \textbf{{0.34}}  & \textbf{{0.07}}
\\ \midrule
\begin{tabular}[c]{@{}c@{}}\textbf{MetaDrive}\\ \textbf{Average}\end{tabular}       
% BC
& {\color[HTML]{656565} 0.40}  & {\color[HTML]{656565} 3.21}
% CDT
& {\color[HTML]{656565} 0.25}  & {\color[HTML]{656565} 1.45}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.59}  & {\color[HTML]{656565} 7.48}
% CPQ
& \textbf{-0.06}  & \textbf{0.24}
% COptiDICE
& {\color[HTML]{656565} 0.63}  & {\color[HTML]{656565} 10.26}
% % TREBI
% & {\color[HTML]{656565} 0.13}  & {\color[HTML]{656565} 3.18}
% ours
& \textbf{{0.36}}  & \textbf{{0.25}}
& {\color[HTML]{656565} {0.67}}  & {\color[HTML]{656565} {2.02}}
& \textbf{{0.53}}  & \textbf{{0.76}}
& {\color[HTML]{0000FF} \textbf{0.50}}  & {\color[HTML]{0000FF} \textbf{0.03}}
\\ \midrule
AntRun
% BC
& {\color[HTML]{656565} 0.73}  & {\color[HTML]{656565} 11.73}
% CDT
& {\color[HTML]{656565} 0.70}  & {\color[HTML]{656565} 1.88}
% BCQ-Lag
% & {\color[HTML]{656565} 0.65}  & {\color[HTML]{656565} 3.30}
% CPQ
& \textbf{0.00}  & \textbf{0.00}
% COptiDICE
& {\color[HTML]{656565} 0.62}  & {\color[HTML]{656565} 3.64}
% % TREBI
% & {\color[HTML]{656565} 0.63}  & {\color[HTML]{656565} 5.43}
% ours
& \textbf{0.45}  & \textbf{0.03}
& {\color[HTML]{656565} {0.74}}  & {\color[HTML]{656565} {4.97}}
& {\color[HTML]{656565} {0.79}}  & {\color[HTML]{656565} {6.81}}
& {\color[HTML]{0000FF} \textbf{0.59}}  & {\color[HTML]{0000FF} \textbf{0.33}}
\\
BallRun
% BC
& {\color[HTML]{656565} 0.67}  & {\color[HTML]{656565} 11.38}
% CDT
% & \textbf{0.32}  & \textbf{0.45}
& {\color[HTML]{0000FF} \textbf{0.32}}  & {\color[HTML]{0000FF} \textbf{0.45}}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.43}  & {\color[HTML]{656565} 6.25}
% CPQ
& {\color[HTML]{656565} 0.85}  & {\color[HTML]{656565} 13.67}
% COptiDICE
& {\color[HTML]{656565} 0.55}  & {\color[HTML]{656565} 11.32}
% % TREBI
% & {\color[HTML]{656565} 0.29}  & {\color[HTML]{656565} 4.24}
% ours
& \textbf{0.18}  & \textbf{0.00}
& {\color[HTML]{656565} {0.35}}  & {\color[HTML]{656565} {4.35}}
& {\color[HTML]{656565} {0.58}}  & {\color[HTML]{656565} {7.46}}
& \textbf{{0.15}}  & \textbf{{0.95}}
\\
CarRun
% BC
& {\color[HTML]{656565} 0.96}  & {\color[HTML]{656565} 1.88}
% CDT
& {\color[HTML]{656565} 0.99}  & {\color[HTML]{656565} 1.10}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.84}  & {\color[HTML]{656565} 2.51}
% CPQ
& {\color[HTML]{656565} 1.06}  & {\color[HTML]{656565} 10.49}
% COptiDICE
& {\color[HTML]{0000FF} \textbf{0.92}}  & {\color[HTML]{0000FF} \textbf{0.00}}
% % TREBI
% & {\color[HTML]{656565} 0.97}  & {\color[HTML]{656565} 1.01}
% ours
% & \textbf{0.73}  & \textbf{0.14}
& \textbf{{0.73}}  & \textbf{{0.14}}
& \textbf{{0.93}}  & \textbf{{0.39}}
& \textbf{{0.93}}  & \textbf{{0.66}}
& {\textbf {0.83}}  & {\textbf{0.00}}
\\
DroneRun
% BC
& {\color[HTML]{656565} 0.55}  & {\color[HTML]{656565} 5.21}
% CDT
& {\color[HTML]{0000FF} \textbf{0.58}}  & {\color[HTML]{0000FF} \textbf{0.30}}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.80}  & {\color[HTML]{656565} 17.98}
% CPQ
& {\color[HTML]{656565} 0.02}  & {\color[HTML]{656565} 7.95}
% COptiDICE
& {\color[HTML]{656565} 0.72}  & {\color[HTML]{656565} 13.77}
% % TREBI
% & {\color[HTML]{656565} 0.59}  & {\color[HTML]{656565} 1.41}
% ours
& \textbf{{0.30}}  & \textbf{{0.55}}
& {\color[HTML]{656565} {0.57}}  & {\color[HTML]{656565} {2.29}}
& {\color[HTML]{656565} {0.62}}  & {\color[HTML]{656565} {7.3}}
& \textbf{{0.47}}  & \textbf{{0.87}}
\\
AntCircle
% BC
& {\color[HTML]{656565} 0.65}  & {\color[HTML]{656565} 19.45}
% CDT
& {\color[HTML]{656565} 0.48}  & {\color[HTML]{656565} 7.44}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.67}  & {\color[HTML]{656565} 19.13}
% CPQ
& \textbf{0.00}  & \textbf{0.00}
% COptiDICE
& {\color[HTML]{656565} 0.18}  & {\color[HTML]{656565} 13.41}
% % TREBI
% & {\color[HTML]{656565} 0.37}  & {\color[HTML]{656565} 2.50}
% ours
& \textbf{{0.20}}  & \textbf{{0.00}}
& {\color[HTML]{656565} {0.46}}  & {\color[HTML]{656565} {5.55}}
& {\color[HTML]{656565} {0.36}}  & {\color[HTML]{656565} {2.08}}
& {\color[HTML]{0000FF} \textbf{0.20}}  & {\color[HTML]{0000FF} \textbf{0.00}}
\\
BallCircle
% BC
& {\color[HTML]{656565} 0.72}  & {\color[HTML]{656565} 10.02}
% CDT
& {\color[HTML]{656565} 0.68}  & {\color[HTML]{656565} 2.10}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.67}  & {\color[HTML]{656565} 8.50}
% CPQ
& {\color[HTML]{656565} 0.40}  & {\color[HTML]{656565} 4.37}
% COptiDICE
& {\color[HTML]{656565} 0.70}  & {\color[HTML]{656565} 9.06}
% % TREBI
% & {\color[HTML]{656565} 0.63}  & {\color[HTML]{656565} 1.89}
% ours
& {\color[HTML]{0000FF} \textbf{0.34}}  & {\color[HTML]{0000FF} \textbf{0.00}}
& {\color[HTML]{656565} {0.54}}  & {\color[HTML]{656565} {1.58}}
& {\color[HTML]{656565} {0.58}}  & {\color[HTML]{656565} {2.08}}
& {\color[HTML]{0000FF} \textbf{0.34}}  & {\color[HTML]{0000FF} \textbf{0.22}}
\\
CarCircle
% BC
& {\color[HTML]{656565} 0.65}  & {\color[HTML]{656565} 11.16}
% CDT
& {\color[HTML]{656565} 0.71}  & {\color[HTML]{656565} 2.19}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.68}  & {\color[HTML]{656565} 8.84}
% CPQ
& {\color[HTML]{656565} 0.49}  & {\color[HTML]{656565} 4.48}
% COptiDICE
& {\color[HTML]{656565} 0.44}  & {\color[HTML]{656565} 7.73}
% % TREBI
% & {\color[HTML]{0000FF} \textbf{0.49}}  & {\color[HTML]{0000FF} \textbf{0.73}}
% ours
& {\color[HTML]{0000FF} \textbf{0.40}}  & {\color[HTML]{0000FF} \textbf{0.11}}
& {\color[HTML]{656565} {0.41}}  & {\color[HTML]{656565} {2.86}}
& {\color[HTML]{656565} {0.40}}  & {\color[HTML]{656565} {2.62}}
& \textbf{{0.36}}  & \textbf{{0.20}}
\\
DroneCircle
% BC
& {\color[HTML]{656565} 0.82}  & {\color[HTML]{656565} 13.78}
% CDT
& {\color[HTML]{656565} 0.55}  & {\color[HTML]{656565} 1.29}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.95}  & {\color[HTML]{656565} 18.56}
% CPQ
& {\color[HTML]{656565} -0.27}  & {\color[HTML]{656565} 1.29}
% COptiDICE
& {\color[HTML]{656565} 0.24}  & {\color[HTML]{656565} 2.19}
% % TREBI
% & {\color[HTML]{656565} 0.54}  & {\color[HTML]{656565} 2.36}
% ours
& {\color[HTML]{0000FF} \textbf{0.48}}  & {\color[HTML]{0000FF} \textbf{0.00}}
& {\color[HTML]{656565} {0.65}}  & {\color[HTML]{656565} {3.60}}
& {\color[HTML]{656565} {0.71}}  & {\color[HTML]{656565} {4.93}}
& \textbf{{0.33}}  & \textbf{{0.07}}
\\ \midrule
\begin{tabular}[c]{@{}c@{}}\textbf{BulletGym}\\ \textbf{Average}\end{tabular}       
% BC
& {\color[HTML]{656565} 0.72}  & {\color[HTML]{656565} 10.58}
% CDT
& {\color[HTML]{656565} 0.63}  & {\color[HTML]{656565} 2.09}
% % BCQ-Lag
% & {\color[HTML]{656565} 0.71}  & {\color[HTML]{656565} 10.63}
% CPQ
& {\color[HTML]{656565} 0.32}  & {\color[HTML]{656565} 5.28}
% COptiDICE
& {\color[HTML]{656565} 0.55}  & {\color[HTML]{656565} 7.64}
% % TREBI
% & {\color[HTML]{656565} 0.56}  & {\color[HTML]{656565} 2.45}
% ours
& {\textbf{0.39}}  & {\textbf{0.10}}
& {\color[HTML]{656565} {0.58}}  & {\color[HTML]{656565} {3.20}}
& {\color[HTML]{656565} {0.62}}  & {\color[HTML]{656565} {4.24}}
& {\color[HTML]{0000FF} \textbf{0.41}}  & {\color[HTML]{0000FF} \textbf{0.33}}
\\ \bottomrule[1pt]
\hline
\end{tabular}
}
\vspace{-10pt}
\end{table}


\textbf{Setup}.
 We evaluate on a popular safe offline RL benchmark, DSRL~\citep{liu2023datasets}. We set $w(s,a)=1$ in Eq.~(\ref{equ:diffusion_loss}) to train our initial policy $\pi_0$ as a behavior policy. Then, we set $w(s,a)={\rm exp}(A_r^*(s,a))$ and $w(s,a)={\rm exp}(-A_h^*(s,a))$ with $A_r^*(s,a)$ and $A_h^*(s,a)$ are the optimal reward and feasible value function learned by expectile regression~\citep{zheng2024safe} to train $\pi_1$ and $\pi_2$ that separately consider reward and safety performance respectively.
 % Evaluated by the $\delta$-relative independent metric, we observe that $\pi_{0,1,2}$ represent different skills and thus are included into our skill library $\Pi$. 
 During composition, we adopt a few filtered near-expert demonstrations that jointly consider the trilogy objective, which \textcolor{black}{is} too limited to imitate good policies. However, we can adopt these data to train a context-aware modular $\alpha(s;\theta)$ in Eq.~(\ref{equ:lora_adaptive_compose}) to adaptively compose $\pi_{0,1,2}$ to handle the conflicts in an efficient way.
 
 
 % We use normalized return and normalized cost as the evaluation metrics, where a normalized cost below 1 indicates safety. In the safe setting, we take safety as the primary criterion for evaluations and pursue higher rewards with low-cost constraints. Additional implementation details can be found in \textcolor{black}{Appendix ??}.

\textbf{Baselines.} To demonstrate the effectiveness of the composition in parameter space, we compare two other composition methods: \textit{noise-level} and \textit{action-level} composition. We denote them as NESC and ASEC respectively, where we control the only differences to PSEC being the composition stage as shown in Figure~\ref{fig:para_score_action} to ensure a fair comparison. We also compare recent state-of-the-art (SOTA) safe offline RL methods including FISOR~\citep{zheng2024safe}, CDT~\citep{liu2023constrained}, COptiDICE~\citep{lee2022coptidice}, CPQ~\citep{xu2022constraints} and BC. These traditional safe offline RL methods typically use human-tuned trade-offs to balance the trilogy objective, which is equivalent to using fixed composition weights compared to PSEC.
% can be regarded as utilizing fixed compositional weights to balance the trilogy objective.
% which however are well-tuned to ensure a good balance among different objectives.
% . Although without the assistance of expert demonstrations to train the compositional network, these traditional baselines are well-tuned by their authors to ensure a relatively good performance and balance among different objectives. 
All policies are trained on the full DSRL dataset to ensure a fair comparison (see {Appendix~\ref{subsec:exp_mmobjective_appendix}} for details).

% We conduct a comparative analysis of our method against several robust basslines, incorporating three state-of-art algorithms: FISOR~\citep{zheng2024safe}, CDT~\citep{liu2023constrained}. CDT is a Decision Transformer-based method that incorporates safety constraints by using cost as an additional input. FISOR is a diffusion-based method that considers hard constraint settings, achieving state-of-the-art safety performance on DSRL. CPQ~\citep{xu2022constraints} updates the Q-value function using only safe transitions and treats OOD actions as unsafe. COptiDICE~\citep{lee2022coptidice} is a DICE (Distribution Correction Estimation) based method that incorporates safety constraints.
% \vspace{-10pt}
\begin{figure}[t]
    \centering
        \centering
        \includegraphics[width=0.99\textwidth]{metadrive/metadrive.jpg}
        \label{fig:walk}
        \vspace{-10pt}
    \caption{\small Output weights of context-aware modular evaluated on the MetaDrive-easymean task. The network dynamically adjusts the weights to handle real-time demands: It prioritizes safety policies when the vehicle approaches obstacles or navigates a turn while avoiding boundary lines. When there are no obstacles and the task is simply to drive straight, the focus shifts to maximizing rewards and maintaining some safety insurance.}
    \label{fig:meta-context}
    \vspace{-15pt}
\end{figure}

\textbf{Main Results.}
% In this section, we present the results of PSEC and competing baselines on MetaDrive and BulletGym tasks of DSRL benchmark, as summarized in 
Table~\ref{tab:main-result} shows that PSEC achieves a good balance between high returns and satisfactory safety performance, and simultaneously mitigates distributional shift across all tasks, enjoying highly competitive performance. In contrast, NSEC and ASEC exhibit skewed learning behaviors, where both of them fail to discover an effective composition to ensure both good safety performance and high returns, resulting in relatively poor safety outcomes despite high rewards.
% Moreover, note that PSEC not only outperforms NSEC, but also NSEC surpasses ASEC. This hierarchy underscores the advantages of early composition over late stages. Early composition effectively leverages more shared or complementary information across different skills, leading to more optimal compositions.
PSEC also outperforms all traditional safe offline RL baselines, demonstrating the necessity of context-aware composition over fixed composition when the task requires intricate balance between different elements. To further support this, we visualize the outputs of our context-aware modular $\alpha(s;\theta)$ to illustrate its adaptive capabilities. Figure~\ref{fig:meta-context} demonstrates that the network dynamically adjusts the weightings to combine different skills, enabling a collaborative response to real-time environmental changes. This adaptive behavior highlights the importance of dynamically adjusting the compositional weights rather than relying on a fixed combination of different skills to jointly solve a new task like previous methods~\citep{ajay2023conditional, zheng2024safe, janner2022planning}.

% \textbf{Dynamical Behavior of Context-aware Composition Network}. To illustrate the adaptive capabilities of the context-aware compositional network, we visualize its outputs in Figure~\ref{fig:meta-context}. The results demonstrate that the network dynamically adjusts the weightings to effectively combine different skills, enabling a collaborative response to real-time environmental changes. Specifically, the network prioritizes safety policies when the vehicle approaches an obstacle or navigates a turn while avoiding boundary lines. Conversely, when no obstacles are present and the task is simply driving straight, it shifts focus toward maximizing rewards and maintain some safety insurance. This adaptive behavior highlights the crucial aspects to dynamically adjust the compositional weights rather than relying on a fixed combination of different skills to jointly solve a new task like previous methods~\citep{ajay2023conditional, zheng2024safe, janner2022planning}.



\vspace{-4pt}
\subsection{Continual Policy Shift Setting}
\vspace{-4pt}
% \ljx{till here}
We evaluate another practical scenario where the agent is progressively tasked with new tasks. We aim to continuously expand the skill libraries to test if the capabilities of agents to learn new skills can be gradually enhanced as prior knowledge grows and test the efficiency of LoRA.
% When the tasks emerge incrementally, an agent needs to have the ability to learn new tasks continually. To evaluate the ability of PSEC, we conduct experiments on DeepMind Control tasks. 

\textbf{Setup}. We conduct experiments on the DeepMind Control Suite (DMC)~\citep{tassa2018deepmind} environments, where an agent is progressively required to stand, walk, and run. We investigate whether PSEC can leverage the standing skill to rapidly learn to walk, and then effectively combine standing and walking skills to adapt to running. For this purpose, we pretrain $\pi_0$ to learn the basic standing skill by setting $w(s,a):=1$ in Eq.~(\ref{equ:diffusion_loss}) trained on a expert dataset $\mathcal{D}_e^{\mathcal{T}_0}$. Subsequently, we provide small expert datasets $\mathcal{D}_e^{\mathcal{T}_1}$ for walk and $\mathcal{D}_e^{\mathcal{T}_2}$ for run, while maintaining $w(s,a):=1$ to adapt to $\pi_1$ and $\pi_2$. After training $\pi_1$, we integrate it into the skill library ${\Pi}$ to assist $\pi_2$ training alongside $\pi_0$. See {Appendix~\ref{subsec:exp_conpolicy_shift_appendix}} for detailed experimental setups.
% The environment contains 2 domains with 4 tasks per domain, resulting in 8 tasks in total. 
% PSEC is a parameter-level composition method. We compare it with score-level composition methods and tabula rasa learning in a continual learning setting. Our experiments are conducted on the Walker tasks~(Stand, Walk, Run) and Quadruped tasks~(Jump, Roll-Fast, Walk, Run), which involve controlling a biped in a 2D vertical plane. Each task emphasizes different balancing and locomotion skills. The dataset consists of three types: medium, medium-replay, and expert, with 10, 30, 50, and 1000 episodes of interactions per dataset.

\textbf{Baselines}. \textit{1)} We compare NSEC and ASEC to further demonstrate the superiority of parameter- over noise- and action-level composition. \textit{2)} We evaluate training from scratch (denoted as Scratch), or replacing LoRA modules with multiplayer perceptions (MLP) to demonstrate the efficiency of compositions and LoRA module. \textit{3)} We evaluate different PSEC variants without context-aware modular (denoted as w/o CA) to further highlight the crucial role of dynamically combining skills.

\textbf{Training and sample efficiency}. To demonstrate the training and sample efficiency of PSEC, we conduct extensive evaluations across varying numbers of trajectories and different methods. Figure~\ref{fig:dmc_comparison}(a) shows that PSEC achieves superior sample efficiency across different training sample sizes, particularly when data is scarce (e.g., only 10 trajectories). Figure~\ref{fig:dmc_comparison}(b) shows that PSEC can quickly attain excellent performance even without composition, highlighting the effectiveness of the LoRA modules. Hence, we train less than 50k gradient steps for almost all tasks, while previous methods typically require millions of gradient steps and data to obtain reasonable results. 

% \vspace{-10pt}
\begin{wrapfigure}{r}{5.9cm}
\begin{minipage}[h]{5.6cm}
\captionof{table}{\small Results in policy shift setting. S, W, R denote stand, walk and run. 10 trajectories are provided for W and R tasks}
\centering
\scriptsize
\label{tab:policy_shift}
\setlength{\tabcolsep}{5.5pt}
\begin{tabular}{lccc}
\toprule
   ~ & S$\rightarrow$ W & S$\rightarrow$ R & S+W$\rightarrow$ R \\
\midrule
   
   % PSEC w/o CA  & & &\\
   Scratch & 58.9 & 25.5 & 25.5 \\
   % NSEC w/o CA & & & \\
   ASEC &65.7 &24.3 &30.8 \\
   NSEC &320.9 & 38.5 & 39.4\\
   PSEC (MLP) &424.1 &143.3 &194.5 \\
   PSEC  & \textbf{688} & \textbf{221} & \textbf{247}\\
   % ASEC w/o CA & & & \\
\bottomrule
\end{tabular}
\end{minipage}
\end{wrapfigure}

\textbf{Continual Evolution}. Table~\ref{tab:policy_shift} shows that PSEC effectively leverages prior knowledge to facilitate efficient policy learning given solely limited data. Notably, S+W$\rightarrow$R outperforms S$\rightarrow$R, demonstrating that the learning capability of PSEC gradually evolves as the skill library grows. In contrast, training from scratch or replacing the LoRA modules with MLP fails to learn new skills given limited data, highlighting the effectiveness of both utilizing prior knowledge and the introduction of LoRA to efficiently adapt to new skills and self-evolution. Moreover, note that even PSEC (MLP) outperforms NSEC and ASEC, further highlighting the advantages of parameter-level compositions.

% We present PSEC’s results on the DeepMind Control Suite, illustrated in Figure~\ref{fig:dmc_comparison}. The results demonstrate that PSEC significantly outperforms other baselines, particularly in learning with limited data. This improved performance is largely due to PSEC’s use of the LoRA module, which enables efficient updates with fewer parameters. By leveraging LoRA, PSEC achieves more effective policy learning while maintaining a low computational cost, leading to better outcomes in small data scenarios compared to the baseline method.

% \textbf{Training and sample efficiency}. To demonstrate the training and sample efficiency of PSEC, we conduct extensive evaluations across varying numbers of trajectories and different methods. Figure~\ref{fig:dmc_comparison}(a) shows that PSEC achieves superior sample efficiency across different training sample sizes, particularly when data is scarce (e.g., only 10 trajectories). Figure~\ref{fig:dmc_comparison}(b) shows that PSEC can quickly attain excellent performance even without composition, highlighting the effectiveness of the LoRA modules. Hence, we train less then 50k gradient steps for almost all tasks, while previous methods typically require millions of gradient steps and data to obtain reasonable results. 

\textbf{Context-aware Composition \textit{v.s.} Fixed Composition}. We carefully tune the fixed composition (w/o CA) of different skills during composition. However, Figure~\ref{fig:dmc_comparison}(c) shows that the context-aware modular can consistently outperform the fixed ones across different levels of compositions. This demonstrates the advantages of the context-aware composition network to fully leverage the prior knowledge in the skill library to enable efficient policy adaptations.

\begin{figure}[t]
% \begin{minipage}[h]{.30\textwidth}
% \captionof{table}{\small Results in policy shift setting. S, W, R denote stand, walk and run. 10 trajectories are provided for W and R tasks}
% \centering
% \scriptsize
% \label{tab:policy_shift}
% \setlength{\tabcolsep}{1.5pt}
% \begin{tabular}{lccc}
% \toprule
%    ~ & S$\rightarrow$ W & S$\rightarrow$ R & S+W$\rightarrow$ R \\
% \midrule
   
%    % PSEC w/o CA  & & &\\
%    Scratch & 58.9 & 25.5 & 25.5 \\
%    % NSEC w/o CA & & & \\
%    ASEC &65.7 &24.3 &30.8 \\
%    NSEC &320.9 & 38.5 & 39.4\\
%    PSEC (MLP) &424.1 &143.3 &194.5 \\
%    PSEC  & \textbf{688} & \textbf{221} & \textbf{247}\\
%    % ASEC w/o CA & & & \\
% \bottomrule
% \end{tabular}
% \end{minipage}
\begin{minipage}[h]{1.0\textwidth}
    \centering
    \begin{subfigure}[b]{0.335\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DMC/main_walk.pdf}
        \vspace{-20pt}
        \caption{\small Sample efficiency.}
        % \label{fig:sample_efficieny}
    \end{subfigure}
    \hspace{-0.18cm}
    \begin{subfigure}[b]{0.335\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DMC/lora-walk1.pdf}
        \vspace{-20pt}
        \caption{\small Training efficiency.}
        \label{fig:training_efficiency}
    \end{subfigure}
    \hspace{0.05cm}
    \begin{subfigure}[b]{0.305\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DMC/context-aware-ablation.pdf}
        \vspace{-15pt}
        \caption{\small Context-aware efficiency.}
        \label{fig:context_aware_compare}
    \end{subfigure}
    \vspace{-5pt}
    \caption{\small Comparisons on sample and training efficiency and the effectiveness of context-aware modular. S, W, R denote stand, walk and run, respectively. Each value is averaged over 10 episodes and 5 seeds.}
    \label{fig:dmc_comparison}
\end{minipage}
\vspace{-15pt}
\end{figure}



% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{DMC/lora-walk.pdf}
%         \caption{Performance Comparison for Walk}
%         \label{fig:walk-lora}
%     \end{subfigure}
%     % \hfill
%     % \begin{subfigure}[b]{0.48\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{DMC/run_perf3vs.pdf}
%     %     \caption{Performance Comparison for Run}
%     %     \label{fig:run}
%     % \end{subfigure}
%     \caption{\ljx{Learning curve to illustrate the efficiency of LoRA modules.}}
%     \label{fig:lora-walk}
% \end{figure}



\vspace{-4pt}
\subsection{Dynamics Shift Setting}
\vspace{-4pt}
% We evaluate on another practical setting to further validate the versatility of PSEC, where the dynamics $\mathcal{P}$ undergoes shifting, covering diverse scenarios like cross-embodiment~\citep{o2024open} and sim-to-real transfer~\citep{tobin2017domain}, and policy learning under non-stationary environments~\citep{xue2024state}.
We evaluate PSEC in another practical setting to further validate its versatility, where the dynamics \(\mathcal{P}\) shift to encompass diverse scenarios such as cross-embodiment~\citep{o2024open}, sim-to-real transfer~\citep{tobin2017domain}, and policy learning in non-stationary environments~\citep{xue2024state}.

% For instance, the friction condition of a robot can become varied with joint wearing. This setting requires the agent to effectively adapt to the dynamics changes. 

\textbf{Setup}. We evaluate on the D4RL environments~\citep{fu2020d4rl}
, where we modify the dynamics and morphology of locomotive robots to reflect the dynamic changes. Specifically, we first pretrain $\pi_0$ using a dataset $\mathcal{D}_o^{\mathcal{P}_0}$ collected from a modified dynamics $\mathcal{P}_0$ and then equip it with a new small dataset $\mathcal{D}_o^{\mathcal{P}_1}$ collected under the original D4RL dynamics $\mathcal{P}_1$. \textit{Friction}, \textit{Thigh Size} and \textit{Gravity} denote $\mathcal{P}_0$ modifies the friction condition, the thigh size of cheetah/walker, and the gravity respectively. Based on the new small dataset $\mathcal{D}_o^{\mathcal{P}_1}$, we set $w(s,a)={\rm exp}(A_r^*(s,a))$ with $A_r^*(s,a)$ as the advantage function trained by expectile regression on $\mathcal{D}_o^{\mathcal{P}_1}$~\citep{kostrikov2021offline} to obtain a new policy $\pi_1$ and then optimize the context-aware composition network $\alpha(s;\theta)$ to combine $\pi_{0,1}$ to collaboratively work under dynamics $\mathcal{P}_1$.
% leverage the transferable skill in $\pi_0$ to assist the dynamics transfer. 
See {Appendix~\ref{subsec:exp_dynamic_shift_appendix} for details.}

\textbf{Baselines}. One branch of baselines consists in training $\pi_1$ from scratch on the small dataset $\mathcal{D}_o^{\mathcal{P}_1}$, which may face data scarcity challenges, including BC, offline RL methods like CQL~\citep{kumar2020conservative}, IQL~\citep{kostrikov2021offline}, MOPO~\citep{yu2020mopo}. In addition, we evaluate some generalizable offline RL methods including DOGE~\citep{li2023when} and TSRL~\citep{cheng2023look} that are superior in the small sample regimes.
% We denote the policy $\pi_0$ in PSEC as No Transfer, 
Additionally, we evaluate the policy trained on the combination of $\mathcal{D}_o^{\mathcal{P}_0}$ and $\mathcal{D}_o^{\mathcal{P}_1}$, referred to as Joint train, to show the advantages of the PSEC framework over a brute-force method of combining all data to address dynamic gaps.

\textbf{Main Results}. Figure~\ref{fig:dynamic-result} demonstrates that PSEC effectively utilizes transferable knowledge from the pretrained policy $\pi_0$ to enhance performance under changed dynamics. In contrast, traditional offline RL methods perform poorly with limited data in new dynamic settings. Moreover, PSEC surpasses specialized sample-efficient offline RL methods like TSRL and DOGE, showcasing its superior ability to leverage prior knowledge for increased training efficiency.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.93\linewidth]{dynamics/dynamic_plot_with_spacing.pdf}
    \vspace{-5pt}
     \caption{\small Results in the dynamics shift setting over 10 episodes and 5 seeds. -m, -mr and -me refer to $\mathcal{D}_o^{\mathcal{P}_1}$ sampling from medium, medium-replay and medium-expert-v2 data in D4RL~\citep{fu2020d4rl}, respectively.}
    \label{fig:dynamic-result}
    \vspace{-12pt}
\end{figure}


\vspace{-4pt}
\subsection{Ablation Study}
\vspace{-4pt}
\begin{wrapfigure}{r}{5.5cm}
% \centering
     \vspace{-25pt}
    \includegraphics[width=5.7cm]{DMC/ablation_rank_bar.pdf}
    \vspace{-20pt}
    \captionsetup{justification=raggedleft, singlelinecheck=false}
    \caption{\small Ablations on LoRA ranks.}
     \label{fig:ablation}
     \vspace{-10pt}
\end{wrapfigure}
We primarily ablate on different LoRA ranks $n$ to assess the robustness of our methods in continual policy shift setting. Figure~\ref{fig:ablation} demonstrates that under varied LoRA $n$ ranks, PSEC consistently outperforms the MLP variant across various LoRA ranks, demonstrating the superior robustness of LoRA modules. Among the different rank settings, we observe that {$n=8$} gives the best results and is therefore chosen as the default choice for the experiments. We hypothesize that using a rank greater than 8 degenerates because the training data is quite limited (e.g., only 10 demonstrations).


% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.48\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{DMC/walk_perf3vs.pdf}
%         \caption{Performance Comparison for Walk}
%         \label{fig:walk}
%     \end{subfigure}
%     % \hfill
%     % \begin{subfigure}[b]{0.48\textwidth}
%     %     \centering
%     %     \includegraphics[width=\textwidth]{DMC/run_perf3vs.pdf}
%     %     \caption{Performance Comparison for Run}
%     %     \label{fig:run}
%     % \end{subfigure}
%     % \caption{\ljx{Ablations on the lora rank}}
%     \label{fig:ablation}
% \end{figure}

\vspace{-6pt}
\section{Conclusion}
\vspace{-6pt}
% \ljx{till here}
We propose PSEC, a framework that handles different skills as plug-and-play LoRA modules within an expandable skill library. This flexible approach enables the agents to reuse prior knowledge for efficient new skill acquisition and to progressively evolve in response to new challenges like humans. By exploiting the interpolation property of LoRA, we propose a context-aware compositional network that adaptively activates and blends different skills directly in the parameter space by merging the corresponding LoRA modules. This parameter-level composition enables the exploitation of more shared and complementary information across different skills, allowing for optimal compositions that collaboratively generate complex behaviors in dynamical environments. 
% Being versatile
PSEC demonstrates exceptional effectiveness across diverse practical applications, such as multi-objective composition, continual policy shift and dynamic shift settings, 
% PSEC demonstrates superior effectiveness to empower
making it highly versatile for
real-world scenarios where knowledge reuse and monotonic policy improvements are crucial. One limitation is the pretrained policy $\pi_0$ may encompass diverse distributions to ensure good LoRA tuning. However, this can be mitigated by utilizing the broad out-of-domain dataset to enhance distribution coverage. More discussions on limitations and future works can be found in Appendix~\ref{sec:limit_future_appendix}.

\section*{Acknowledgement}
This work is supported by National Key Research and Development Program of China under Grant (2022YFB2502904), National Natural Science Foundation of China under Grant 62403483, Grant U24A20279, and Grant U21A20518, and funding from Wuxi Research Institute of Applied Technologies, Tsinghua University under Grant 20242001120.
% It is more flexible to solve the task with different constraints, even meeting new constraints for this task.
% which is typically formulated as a Constrained Markov Decision Process~\citep{altman2021constrained} and aims to find a policy to maximize the expected cumulative rewards while satisfying the low-cost constraint. 


% In this section, we conduct experiments on the DSRL benchmark to evaluate the performance of PSEC's decompose-compose idea. 

% \section{Headings: first level}
% \label{headings}

% First level headings are in small caps,
% flush left and in point size 12. One line space before the first level
% heading and 1/2~line space after the first level heading.

% \subsection{Headings: second level}

% Second level headings are in small caps,
% flush left and in point size 10. One line space before the second level
% heading and 1/2~line space after the second level heading.

% \subsubsection{Headings: third level}

% Third level headings are in small caps,
% flush left and in point size 10. One line space before the third level
% heading and 1/2~line space after the third level heading.

% \section{Citations, figures, tables, references}
% \label{others}

% These instructions apply to everyone, regardless of the formatter being used.

% \subsection{Citations within the text}

% Citations within the text should be based on the \texttt{natbib} package
% and include the authors' last names and year (with the ``et~al.'' construct
% for more than two authors). When the authors or the publication are
% included in the sentence, the citation should not be in parenthesis using \verb|\citet{}| (as
% in ``See \citet{Hinton06} for more information.''). Otherwise, the citation
% should be in parenthesis using \verb|\citep{}| (as in ``Deep learning shows promise to make progress
% towards AI~\citep{Bengio+chapter2007}.'').

% The corresponding references are to be listed in alphabetical order of
% authors, in the \textsc{References} section. As to the format of the
% references themselves, any style is acceptable as long as it is used
% consistently.

% \subsection{Footnotes}

% Indicate footnotes with a number\footnote{Sample of the first footnote} in the
% text. Place the footnotes at the bottom of the page on which they appear.
% Precede the footnote with a horizontal rule of 2~inches
% (12~picas).\footnote{Sample of the second footnote}

% \subsection{Figures}

% All artwork must be neat, clean, and legible. Lines should be dark
% enough for purposes of reproduction; art work should not be
% hand-drawn. The figure number and caption always appear after the
% figure. Place one line space before the figure caption, and one line
% space after the figure. The figure caption is lower case (except for
% first word and proper nouns); figures are numbered consecutively.

% Make sure the figure caption does not get separated from the figure.
% Leave sufficient space to avoid splitting the figure and figure caption.

% You may use color figures.
% However, it is best for the
% figure captions and the paper body to make sense if the paper is printed
% either in black/white or in color.
% \begin{figure}[h]
% \begin{center}
% %\framebox[4.0in]{$\;$}
% \fbox{\rule[-.5cm]{0cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
% \end{center}
% \caption{Sample figure caption.}
% \end{figure}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

% \section{Default Notation}

% In an attempt to encourage standardized notation, we have included the
% notation file from the textbook, \textit{Deep Learning}
% \cite{goodfellow2016deep} available at
% \url{https://github.com/goodfeli/dlbook_notation/}.  Use of this style
% is not required and can be disabled by commenting out
% \texttt{math\_commands.tex}.


% \centerline{\bf Numbers and Arrays}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1in}p{3.25in}}
% $\displaystyle a$ & A scalar (integer or real)\\
% $\displaystyle \va$ & A vector\\
% $\displaystyle \mA$ & A matrix\\
% $\displaystyle \tA$ & A tensor\\
% $\displaystyle \mI_n$ & Identity matrix with $n$ rows and $n$ columns\\
% $\displaystyle \mI$ & Identity matrix with dimensionality implied by context\\
% $\displaystyle \ve^{(i)}$ & Standard basis vector $[0,\dots,0,1,0,\dots,0]$ with a 1 at position $i$\\
% $\displaystyle \text{diag}(\va)$ & A square, diagonal matrix with diagonal entries given by $\va$\\
% $\displaystyle \ra$ & A scalar random variable\\
% $\displaystyle \rva$ & A vector-valued random variable\\
% $\displaystyle \rmA$ & A matrix-valued random variable\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Sets and Graphs}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \sA$ & A set\\
% $\displaystyle \R$ & The set of real numbers \\
% $\displaystyle \{0, 1\}$ & The set containing 0 and 1 \\
% $\displaystyle \{0, 1, \dots, n \}$ & The set of all integers between $0$ and $n$\\
% $\displaystyle [a, b]$ & The real interval including $a$ and $b$\\
% $\displaystyle (a, b]$ & The real interval excluding $a$ but including $b$\\
% $\displaystyle \sA \backslash \sB$ & Set subtraction, i.e., the set containing the elements of $\sA$ that are not in $\sB$\\
% $\displaystyle \gG$ & A graph\\
% $\displaystyle \parents_\gG(\ervx_i)$ & The parents of $\ervx_i$ in $\gG$
% \end{tabular}
% \vspace{0.25cm}


% \centerline{\bf Indexing}
% \bgroup
% \def\arraystretch{1.5}

% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle \eva_i$ & Element $i$ of vector $\va$, with indexing starting at 1 \\
% $\displaystyle \eva_{-i}$ & All elements of vector $\va$ except for element $i$ \\
% $\displaystyle \emA_{i,j}$ & Element $i, j$ of matrix $\mA$ \\
% $\displaystyle \mA_{i, :}$ & Row $i$ of matrix $\mA$ \\
% $\displaystyle \mA_{:, i}$ & Column $i$ of matrix $\mA$ \\
% $\displaystyle \etA_{i, j, k}$ & Element $(i, j, k)$ of a 3-D tensor $\tA$\\
% $\displaystyle \tA_{:, :, i}$ & 2-D slice of a 3-D tensor\\
% $\displaystyle \erva_i$ & Element $i$ of the random vector $\rva$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}


% \centerline{\bf Calculus}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% % NOTE: the [2ex] on the next line adds extra height to that row of the table.
% % Without that command, the fraction on the first line is too tall and collides
% % with the fraction on the second line.
% $\displaystyle\frac{d y} {d x}$ & Derivative of $y$ with respect to $x$\\ [2ex]
% $\displaystyle \frac{\partial y} {\partial x} $ & Partial derivative of $y$ with respect to $x$ \\
% $\displaystyle \nabla_\vx y $ & Gradient of $y$ with respect to $\vx$ \\
% $\displaystyle \nabla_\mX y $ & Matrix derivatives of $y$ with respect to $\mX$ \\
% $\displaystyle \nabla_\tX y $ & Tensor containing derivatives of $y$ with respect to $\tX$ \\
% $\displaystyle \frac{\partial f}{\partial \vx} $ & Jacobian matrix $\mJ \in \R^{m\times n}$ of $f: \R^n \rightarrow \R^m$\\
% $\displaystyle \nabla_\vx^2 f(\vx)\text{ or }\mH( f)(\vx)$ & The Hessian matrix of $f$ at input point $\vx$\\
% $\displaystyle \int f(\vx) d\vx $ & Definite integral over the entire domain of $\vx$ \\
% $\displaystyle \int_\sS f(\vx) d\vx$ & Definite integral with respect to $\vx$ over the set $\sS$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Probability and Information Theory}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle P(\ra)$ & A probability distribution over a discrete variable\\
% $\displaystyle p(\ra)$ & A probability distribution over a continuous variable, or over
% a variable whose type has not been specified\\
% $\displaystyle \ra \sim P$ & Random variable $\ra$ has distribution $P$\\% so thing on left of \sim should always be a random variable, with name beginning with \r
% $\displaystyle  \E_{\rx\sim P} [ f(x) ]\text{ or } \E f(x)$ & Expectation of $f(x)$ with respect to $P(\rx)$ \\
% $\displaystyle \Var(f(x)) $ &  Variance of $f(x)$ under $P(\rx)$ \\
% $\displaystyle \Cov(f(x),g(x)) $ & Covariance of $f(x)$ and $g(x)$ under $P(\rx)$\\
% $\displaystyle H(\rx) $ & Shannon entropy of the random variable $\rx$\\
% $\displaystyle \KL ( P \Vert Q ) $ & Kullback-Leibler divergence of P and Q \\
% $\displaystyle \mathcal{N} ( \vx ; \vmu , \mSigma)$ & Gaussian distribution %
% over $\vx$ with mean $\vmu$ and covariance $\mSigma$ \\
% \end{tabular}
% \egroup
% \vspace{0.25cm}

% \centerline{\bf Functions}
% \bgroup
% \def\arraystretch{1.5}
% \begin{tabular}{p{1.25in}p{3.25in}}
% $\displaystyle f: \sA \rightarrow \sB$ & The function $f$ with domain $\sA$ and range $\sB$\\
% $\displaystyle f \circ g $ & Composition of the functions $f$ and $g$ \\
%   $\displaystyle f(\vx ; \vtheta) $ & A function of $\vx$ parametrized by $\vtheta$.
%   (Sometimes we write $f(\vx)$ and omit the argument $\vtheta$ to lighten notation) \\
% $\displaystyle \log x$ & Natural logarithm of $x$ \\
% $\displaystyle \sigma(x)$ & Logistic sigmoid, $\displaystyle \frac{1} {1 + \exp(-x)}$ \\
% $\displaystyle \zeta(x)$ & Softplus, $\log(1 + \exp(x))$ \\
% $\displaystyle || \vx ||_p $ & $\normlp$ norm of $\vx$ \\
% $\displaystyle || \vx || $ & $\normltwo$ norm of $\vx$ \\
% $\displaystyle x^+$ & Positive part of $x$, i.e., $\max(0,x)$\\
% $\displaystyle \1_\mathrm{condition}$ & is 1 if the condition is true, 0 otherwise\\
% \end{tabular}
% \egroup
% \vspace{0.25cm}



% \section{Final instructions}
% Do not change any aspects of the formatting parameters in the style files.
% In particular, do not modify the width or length of the rectangle the text
% should fit into, and do not change font sizes (except perhaps in the
% \textsc{References} section; see below). Please note that pages should be
% numbered.

% \section{Preparing PostScript or PDF files}

% Please prepare PostScript or PDF files with paper size ``US Letter'', and
% not, for example, ``A4''. The -t
% letter option on dvips will produce US Letter files.

% Consider directly generating PDF files using \verb+pdflatex+
% (especially if you are a MiKTeX user).
% PDF figures must be substituted for EPS figures, however.

% Otherwise, please generate your PostScript and PDF files with the following commands:
% \begin{verbatim}
% dvips mypaper.dvi -t letter -Ppdf -G0 -o mypaper.ps
% ps2pdf mypaper.ps mypaper.pdf
% \end{verbatim}

% \subsection{Margins in LaTeX}

% Most of the margin problems come from figures positioned by hand using
% \verb+\special+ or other commands. We suggest using the command
% \verb+\includegraphics+
% from the graphicx package. Always specify the figure width as a multiple of
% the line width as in the example below using .eps graphics
% \begin{verbatim}
%    \usepackage[dvips]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.eps}
% \end{verbatim}
% or % Apr 2009 addition
% \begin{verbatim}
%    \usepackage[pdftex]{graphicx} ...
%    \includegraphics[width=0.8\linewidth]{myfile.pdf}
% \end{verbatim}
% for .pdf graphics.
% See section~4.4 in the graphics bundle documentation (\url{http://www.ctan.org/tex-archive/macros/latex/required/graphics/grfguide.ps})

% A number of width problems arise when LaTeX cannot properly hyphenate a
% line. Please give LaTeX hyphenation hints using the \verb+\-+ command.

% \subsubsection*{Author Contributions}
% If you'd like to, you may include  a section for author contributions as is done
% in many journals. This is optional and at the discretion of the authors.

% \subsubsection*{Acknowledgments}
% Use unnumbered third level headings for the acknowledgments. All
% acknowledgments, including those to funding agencies, go at the end of the paper.

% \newpage
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\newpage
\appendix
\section{Limitations and Future works}
\label{sec:limit_future_appendix}
In this section, we provide detailed discussions about the limitations and their potential solutions.
\begin{itemize}
    \item \textbf{Assumption on the expressiveness of the pretrain policy}. The main limitation of PSEC is the assumption that the pre-trained $\pi_0$ covers a diverse distribution, which allows for efficient fine-tuning using small add-on LoRA modules. If this assumption does not hold, learning new skills through parameter-efficient fine-tuning may prove challenging, as significantly more parameters might be required to acquire new skills.

    \textit{Potential solutions}: Note that this assumption is mild in relevant papers that utilize LoRA to learn new skills~\citep{hu2021lora, liu2023tail}. To tackle this problem, one straightforward solution is to increase the value of LoRA ranks to increase the learning capabilities of the newly introduced modules. Another simple solution is to leverage the cheap and abundant out-of-domain data to enhance the distribution coverage of the pretrained $\pi_0$ to enable efficient LoRA adaptations. 

    \item \textbf{Redundant skill expansion}. In this paper, PSEC includes policies for all tasks in the skill library across its lifelong time. Although we adopt LoRA to reduce computational burden and memory usage, maintaining an extensive library of skill primitives may still lead to substantial computational costs.

    \textit{Potential solutions}: 
    Note that not all skills should be incorporated into the skill library, particularly those that are redundant and can be synthesized from other primitives. An interesting direction for future research is to develop an evaluation metric to assess the interconnections between different skills, such as the skill diversity~\citep{pertsch2021accelerating, eysenbachdiversity}, to only include essential, non-composable atomic primitives. Such a strategy could significantly reduce the management costs associated with maintaining the skill library.
    % Note that not all skills should be added into the skill library, especially for those redundant one that can be composed by other primitives. Therefore, one interesting future work is to explore one evaluation metric to assess the connections between different skills and only add the atomic primitives that cannot be composed by others. This operation might significantly reduce the management cost of the skill library.

    \item \textbf{Hyperparameter-tuning}: Another limitation is PSEC introduces another LoRA modules to learn new skills, which can introduce additional hyperparameters required to be tuned. 

    \textit{Potential solutions}: This limitation is widely existed in relevant works that try to reuse prior knowledge to learn new skills~\citep{liu2023tail, clark2024directly, wang2024sparse, peng2019mcp, barreto2018transfer}, since almost all papers require additional parameters or regularization to adapt to the new skills. In this paper, we have ablated the robustness of PSEC against varied LoRA ranks, and demonstrate consistent superiority over the naive MLP modules in Figure~\ref{fig:ablation}, highlighting the robustness of PSEC for hyperparameter tuning.

    \item \textbf{Simple context-aware compositional modular}: We employ a simple context-aware modular $\alpha(s;\theta)$ to dynamically combine different primitives. This operation is simple and may not fully leverage the shared structure across skills for the target task.

    \textit{Potential Solutions}: However, in our paper, we have demonstrated the superior advantages of this simple context-aware modular, as shown in Figure~\ref{fig:context_aware_compare}. One interesting future direction is to adopt a more advanced model architecture, training objective, or more flexible gating approach to optimize the modular.
\end{itemize}


\section{Discussions on more related works}
\label{sec:related_work_appendix}
\textbf{Tabula Rasa}. 
Tabula rasa learning is one popular paradigm for diverse existing decision-making applications, such as robotics and games~\citep{silver2017mastering, andrychowicz2020learning, berner2019dota, xudong2024goal, vinyals2019grandmaster}. It directly learns policies from scratch without the assistance of any prior knowledge. However, it suffers from notable drawbacks related to poor sample efficiency and constraints on the complexity of skills an agent can acquire~\citep{agarwal2022reincarnating}.

\textbf{Finetune-based Methods}. Some finetune-based methods aim to accelerate policy learning by leveraging prior knowledge. This knowledge may come from pretrained policy or offline data~\citep{A2PR,wang2024expressive,dai2024mamba,xudong2024iterative,cheng2024scaling}, such as 
Offline-to-online RL~\citep{nair2020awac, lee2022offline, agarwal2022reincarnating,cheng2024rime} and transfer RL~\citep{barreto2018transfer, li2019hierarchical, LANKBS}. Some methods maintain a policy library that contains pretrained policies and adaptively selects one policy from this set to assist policy training~\citep{kim2024unsupervised, wang2024train, barreto2018transfer}. However, they are generally restricted to single-task scenarios where all policies serve the same task~\citep{zhang2023policy}, or only sequentially activate one policy in the pretrained sets, which greatly limits the expressiveness of the pretrained primitives~\citep{li2019hierarchical}. Our method, on the contrary, can both leverage multi-task knowledge to fulfill the new task, and can simultaneously activate all skills to compose more complex behaviors.
% \textbf{Multi-task RL and IL}. Some multi-task RL and IL methods also attempt to leverage the shared features across different tasks to enhance the overall performance~\citep{wang2024sparse, yang2020multi, sun2022paco, sun2023efficient}. However, they primarily focus on avoiding conflicts between tasks and often overlook the benefits of reusing prior knowledge to tackle new tasks. 

\textbf{MoE in decision making}. The recent SDP~\citep{wang2024sparse} is particularly relevant to our work. Specifically, SDP employs Mixture of Experts (MoE)~\citep{shazeer2016outrageously} to encode skills as flexible combinations of forward path gated by distinct routers, allowing for efficient adaptation to new tasks by fine-tuning newly introduced expert tokens and task-specific routers. However, SDP necessitates that the pretrained policy \(\pi_0\) be modeled with MoE layers, which imposes additional requirements on the model architecture. In contrast, our approach does not impose any constraints on the structure of the pretrained network and allows for the direct incorporation of new skills as plug-and-play LoRA modules. Moreover, when we identify a skill that is underperforming, we can easily modify the skill library by simply removing its plug-and-play LoRA modules. In contrast, using MoE limits this flexibility in managing different skills, making it challenging to mitigate the side effects caused by suboptimal skills. Therefore, PSEC offers a more flexible approach to managing the skill library, making it more feasible to scale up and incorporate a larger number of skills.

\textbf{LoRA in decision making}. Other relevant works such as TAIL~\citep{liu2023tail}, LoRA-DT~\citep{huang2024solving} and L2M~\citep{schmied2024learning} also employ LoRA to encode skills. However, they solely investigate the parameter-isolation property of LoRA to prevent catastrophic forgetting, while overlooking the potential to merge different LoRA modules to interpolate new skills. Moreover, TAIL only studies the IL domain, L2M and LoRA-DT only study the RL domain, while PSEC both explore the effectiveness in RL and IL settings.

\textbf{LoRA for composition in other domains}. \citep{ponti2023combining, clark2024directly, huang2023lorahub, zhong2024multi, prabhakar2024lora} use LoRA for multi-task learning but using a fixed combination of LoRA modules, focusing on static settings like language model or image generation, thus limiting its expressiveness of the pretrained LoRA modules and flexibility of composition. In contrast, PSEC combines different LoRA via a context-aware modular, maximizing the expressiveness of pretrained skills to flexibly compose new skills, which is crucial for decision making since the real-time adjustment is required to handle the dynamical problems as shown in Figure~\ref{fig:meta-context}.

% \textcolor{black}{\textbf{Other relevant works}. The concept of employing modularized networks to encode different skills has been proposed in previous studies~\citep{happel1994design, sharkey1996combining, auda1998modular, auda1999modular, sodhani2022introduction, andreas2016neural, alet2018modular, ponti2023combining, clark2024directly, huang2023lorahub, zhong2024multi, prabhakar2024lora}. This modularized design enables efficient adaptation to new tasks by exploiting the shareable knowledge stored in different modules. In addition, it naturally facilitates continual evolvement by absorbing new skills in new modules in a parameter-isolation manner~\citep{sodhani2022introduction}. However, all these previous works fail to answer one crucial question in decision making: \textit{How to efficiently compose different skills to tackle dynamical demands?} Specifically, all previous works typically resort to fixed combinations of different modules, thus limiting the flexibility to handle decision-making scenarios where real-time composition adjustment is required to satisfy the dynamical demands. Some recent works take the dynamical demands into consideration by adaptively composing different skills~\citep{peng2019mcp, qureshi2020composing}. These methods, however, fail to effectively exploit the shared structure across different skills by naively composing different components in the original action space. PSEC, however, systematically investigates the advantages of skill compositions in parameter space over the noise space and action space, offering clear guidance for future research to expand and compose skills in parameter spaces rather than noise/action spaces.} 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.99\linewidth]{figs/related_work.jpg}
    \caption{\small \textcolor{black}{Illustrative comparisons between PSEC and other modularized multitask learning frameworks when deployed to continual learning settings.}}
    \label{fig:related_works}
\end{figure}

\textcolor{black}{\textbf{Modularized skills for multitask learning.} Multitask learning methods attempt to leverage the complementary benefits and commonalities across different tasks to enhance the cross-task generalization and capabilities~\citep{wang2024sparse, yang2020multi, sun2022paco, sun2023efficient, ruder2017overview,ma2024reward,ma2024highly}. To achieve effective skill sharing, two primitive paradigms are introduced, including \textit{Hard Parameter Sharing} and \textit{Soft Parameter Sharing}~\citep{ruder2017overview}, as shown in Figure~\ref{fig:related_works}. All these methods demonstrate a \textit{modularized structure}, where separate parameters are required to solve different tasks. Not only enjoying the benefits of multitask learning, this modularized design allows for efficient adaptation to new tasks by exploiting the shareable knowledge stored in different modules~\citep{happel1994design, sharkey1996combining, auda1998modular, auda1999modular, sodhani2022introduction, andreas2016neural, alet2018modular, ponti2023combining, clark2024directly, huang2023lorahub, zhong2024multi, prabhakar2024lora}.}

\textcolor{black}{Hard parameter sharing approaches~\citep{caruana1993multitask, sun2022paco, sun2023efficient, baxter1997bayesian, leon2021nutshell} aim to learn a shared feature that is strong and generalizable enough to capture the commonalities across all different tasks. This is achieved by developing a multi-head style structure, where different heads solve different tasks and all heads share some common layers~\citep{sun2022paco, sun2023efficient, leon2021nutshell, bakker2003task}. In this structure, zero-shot generalization to new tasks becomes possible if the shared layers can capture some generic features, following the spirits of meta learning~\citep{finn2017model, gordon2018metalearning, naik1992meta, LANKBS}. PSEC can be regarded as one specific type of hard parameter sharing method since different LoRA modules exploit a shared $\pi_0$. However, note that each LoRA module in PSEC is sequentially and independently optimized, thus making it easier to capture the task-specific features and avoid the potential gradient conflicts across different skills~\citep{yu2020gradient, liu2021conflict}. Previous methods, however, may introduce some gradient conflicts across different tasks that impede policy learning~\citep{sun2022paco, yang2020multi, caruana1997multitask}, or suffer from collapsing to an entropic state and fail to encode task-specific features~\citep{ponti2023combining}.}

\textcolor{black}{Soft parameter sharing approaches~\citep{yang2020multi, wang2024sparse, liu2024continual, duong2015low, yang2016trace, ruder2017overview} are similar to the hard ones, with the differences primarily in the shared features. Instead of directly employing shared layers~\citep{bakker2003task, caruana1993multitask, sun2022paco, leon2021nutshell}, soft parameter sharing approaches adopt regularizations to enforce a ``shared'' feature across tasks, such as minimizing the L2 distance or cosine similarity across the features for different tasks~\citep{duong2015low, yang2016trace, ruder2017overview}, adopting flexible structures like MoE layers~\citep{shazeer2016outrageously, yuksel2012twenty, wang2024sparse}, soft modular~\citep{yang2020multi}, or resorting moving average across different features~\citep{liu2024continual, lawson2024merging}. These methods enjoy more flexibility than hard parameter sharing but may suffer from potential instability caused by improper regularizations and outlier tasks. For instance, ~\citet{liu2024continual, lawson2024merging} may undergo performance degradation without appropriate averaging weights if they are trying to combine a suboptimal skill learned on limited data.}
% \textcolor{black}{More critically, both hard parameter sharing and soft parameter sharing methods may be affected by outlier tasks, since the shared features fail to encode the generic features that are applicable to these outliers. PSEC, however, encodes all task-specific features in the specific layers while remaininging 
% }

% \textcolor{black}{However, they primarily focus on avoiding conflicts between tasks and often overlook the benefits of reusing prior knowledge to tackle new tasks.}


\textcolor{black}{\textbf{Modularized skills for continual learning and compositions.} More critically, the modularized design naturally facilitates continual evolvement by absorbing new skills in new modules in a parameter-isolation manner~\citep{sodhani2022introduction}. This is one key advantage of modularized skills over traditional continual learning approaches since methods like EWC~\citep{kirkpatrick2017overcoming}, Rehearsal~\citep{rolnick2019experience}, Functional Regularization~\citep{pan2020continual} often exhibit some catastrophic forgetting. The modularization method, however, can address this problem fundamentally by learning new parameters without disrupting pretrained ones. Along this line, numerous works also adopt modularized structure in a hard or soft manner as we discussed earlier~\citep{ring1994continual, pape2011modular, huang2023lorahub, andreas2016neural, alet2018modular, clark2024directly, zhong2024multi, prabhakar2024lora, liu2024continual} like PSEC. However, PSEC differs fundamentally in three key axes, including \textit{how to obtain different modules}, \textit{how to compose modules}, and \textit{where to compose modules}.}

\begin{itemize}
    \item \textcolor{black}{\textbf{How to obtain different modules?} Many previous methods typically assume a fixed set of modules during pretraining and jointly train all modules at once following a multitask learning paradigm~\citep{ring1994continual,  pape2011modular, schwarz2018progress, ponti2023combining, alet2018modular}. Although this joint training approach enjoys the potential to exploit more shared features across tasks. The learned modules may fail to capture task-specific features, becoming general-purpose features and collapsing to highly entropic status, if the data distribution is very diverse and many outlier tasks exist~\citep{ruder2017overview, ponti2023combining}. In contrast, PSEC independently trains each LoRA by exploiting a shared, frozen, and general-purpose $\pi_0$, avoiding lots of conflicts across different tasks and avoiding the risks of collapsing~\citep{yu2020gradient, sun2022paco}. We conduct empirical evaluations in our rebuttal to demonstrate this.}

    \item \textcolor{black}{\textbf{How to compose modules?} PSEC can iteratively expand its skill library to include more skills and then combine them to form complex ones, which is one common advantage of all modularized approaches. So, previous works can also iteratively expand their modules to encode new skills and then compose the pretrained ones to tackle new tasks, such as~\citep{ring1994continual, morrow1997manipulation, pape2011modular, ponti2023combining, alet2018modular, huang2023lorahub, lecun2006tutorial, liu2022compositional, du2023reduce}. However, most previous works typically resort to a simple fixed combination of different modules, such as manually tuned weights~\citep{liu2022compositional, du2023reduce}, thus significantly limiting the flexibility to handle decision-making scenarios where real-time composition adjustment is required to satisfy the dynamic demands. For instance, $2^n=C_n^0+C_n^1+C_n^2+...+C_n^n$ skills could be composed of n different (non-redundant) skills by using binary compositional weight (0 for deactivate and 1 for activate). So, naively adopting a fixed combination of different skills can be very suboptimal. In contrast, PSEC introduces a context-aware composition to dynamically combine different skills, greatly enhancing the expressiveness of the skill libraries by interpolating or extrapolating across different skills.}

    \item \textcolor{black}{\textbf{Where to compose modules?} Another key problem that should be investigated is where to compose different modules. Directly in the original output space (noise space~\citep{ren2024diffusion, zhang2023composing} or action space~\citep{peng2019mcp, qureshi2020composing, lecun2006tutorial} or the parameter space~\citep{huang2023lorahub, prabhakar2024lora, pape2011modular, zhong2024multi}. PSEC systematically investigates the advantages of skill compositions in parameter space over the noise space and action space, offering clear guidance for future research to expand and compose skills in parameter spaces rather than noise/action spaces. 
    % Note that one recent work offers an opposite conclusion that composing in the parameter space is suboptimal~\citep{zhong2024multi}. However, we hypothesize this is because their experiments are primarily conducted with manually tuned compositional weights, which may fail to unleash the potential of composition in parameter space like PSEC. 
    Also, intuitively, Figure~\ref{fig:related_works} shows that PSEC holds the potential to exploit more complementary features or commonalities across tasks than naive hard parameter sharing or soft parameter sharing. Specifically, PSEC can fully leverage information across tasks to facilitate new task learning by employing the compositional network to combine all available parameters. Hard/Soft parameter sharing, however, must rely on a well-performed shared feature produced by the shared layers while discarding all other heads~\citep{liu2024continual, lawson2024merging}.}
\end{itemize}

% \textcolor{black}{However, all these previous works fail to answer one crucial question in decision making: \textit{How to efficiently compose different skills to tackle dynamical demands?} Specifically, all previous works typically resort to fixed combinations of different modules, thus limiting the flexibility to handle decision-making scenarios where real-time composition adjustment is required to satisfy the dynamical demands. Some recent works take the dynamical demands into consideration by adaptively composing different skills~\citep{peng2019mcp, qureshi2020composing}. These methods, however, fail to effectively exploit the shared structure across different skills by naively composing different components in the original action space. PSEC, however, systematically investigates the advantages of skill compositions in parameter space over the noise space and action space, offering clear guidance for future research to expand and compose skills in parameter spaces rather than noise/action spaces.}


Some works use logical options for skill composition~\citep{araki2021logical} but require significant human effort for skill management, limiting scalability. Additionally, ~\citet{araki2021logical} focuses on efficient pretraining, not on fast adaptation/continual improvement. In contrast, PSEC targets the later setups and minimizes human effort by incorporating new skills as LoRA modules, which are then combined through auto-learned compositional networks.

% In this section, we provide discussions on some relevant multi-task reinforcement learning and imitation learning methods. Some multi-task reinforcement learning and imitation learning work try to utilize the shared parameters across different tasks to enhance the overall performance~\citep{wang2024sparse, yang2020multi, sun2022paco, sun2023efficient}. However, they primitively focus on how to avoid the conflicts across different tasks and overlook the benefits of reusing prior knowledge from different pretrained skills to efficiently solve a new task. Among these works, the most recent SDP~\cite{wang2024sparse} is one of the most relevant works to our paper. Specifically, SDP utilizes MoE~\citep{shazeer2016outrageously} to encode skills as different gated functions, and can be efficiently adapted to new task by finetuning newly introduced expert tokens and gated functions. However, SDP requires the pretrained policy $\pi_0$ being modeled with MoE layers, which introduces additional requirement on the model architecture. In contrast, we do not impose any assumption on the pretrained network structure and can incorporate new skills directly as plug-and-play modules.
% \ljx{Sparse Diffusion Policy, Soft-Module, and other multi-task papers. @Bear, DDL 0927}



\section{Experimental Setups}
\label{sec:exp_setup_appendix}
\subsection{Multi-objective Composition}
\label{subsec:exp_mmobjective_appendix}
% For many real-world scenarios, a complex task can be separated into two basic tasks that are simpler to solve. Then we can compose these two simple policies to finish the complex task. 
% In this setting, we aim to evaluate the compositionality of our context-aware composition method in parameter space over other composition methods in Figure~\ref{fig:para_score_action}. Specifically, the safe offline reinforcement learning domain considers two objectives, safety and return. 
\textbf{Training details of PSEC}. In this setting, we have four networks required to train: the behavior policy $\pi_0$, the safety policy $\pi_1$ that minimizes the cost,  the reward policy $\pi_2$ that maximizes the return, and the context-aware modular $\alpha(s;\theta)\in\mathbb{R}^2$. For each task, we first pretrain $\pi_0$ parameterized by $W_0$ as behavior policy by minimizing the following objective on the full DSRL dataset $\mathcal{D}$~\citep{liu2023datasets} to ensure a diverse pretrained distribution coverage:
\begin{equation}
\mathcal{L}_{\pi_0}(W_0)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}}\left[\left\|{\epsilon}-{\epsilon}_{W_0}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right].
\label{equ:pi_0}
\end{equation}
Then, we equip the agent with the same dataset $\mathcal{D}$ but provide feasible label $h$ and reward labels $r$, forming the dataset $\mathcal{D}^h=\{(s,a,h,s')\}$ and $\mathcal{D}^r\{(s,a,r,s')\}$. Then we train $\pi_1$ and $\pi_2$ based on these datasets by optimizing their newly introduced LoRA modules $\Delta W_1$ and $\Delta W_2$ via minimizing the following objectives in Eq.~(\ref{equ:pi_1_dsrl}-\ref{equ:pi_2_dsrl}):

\begin{equation}
\mathcal{L}_{\pi_1}(\Delta W_1)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}^h}\left[w^h(s,a)\left\|{\epsilon}-{\epsilon}_{ W_1}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
\label{equ:pi_1_dsrl}
\end{equation}

\begin{equation}
\mathcal{L}_{\pi_2}(\Delta W_2)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}^r}\left[w^r(s,a)\left\|{\epsilon}-{\epsilon}_{ W_2}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
\label{equ:pi_2_dsrl}
\end{equation}

where the weights of LoRA augmented layer are $W_1=W_0 + 16 \Delta W_1$ and $W_2=W_0 + 16 \Delta W_2$ as defined in Eq.~(\ref{equ:lora}). $w^h(s,a):=\exp(-A_h^*(s,a))$ and $w^r(s,a):=\exp(A_r^*(s,a))$ are the weighting function derived from the optimal feasible value function $A_h^*(s,a)=Q_h^*(s,a)-V_h^*(s)$ and reward value function $A_r^*(s,a)=Q_r^*(s,a)-V_r^*(s,a)$, optimized via expectile regression following~\citep{kostrikov2021offline, zheng2024safe}, where $Q_h^*(s,a)$ and $V_h^*(s)$ can be obtained via minimizing Eq.~(\ref{eq:expectile_v}-\ref{eq:qh_loss}), $Q_r^*(s,a)$ and $V_r^*(s)$ can be obtained via minimizing Eq.~(\ref{eq:vr_loss}-\ref{eq:qr_loss}):
\begin{align}
\mathcal{L}_{V_h} &= \mathbb{E}_{(s,a)\sim \mathcal{D}^h}\left[L^{\tau}_{\rm rev}\left(Q_h(s,a)-V_h(s)\right) \right] ,\label{eq:expectile_v}\\
\mathcal{L}_{Q_h} &= \mathbb{E}_{(s,a,s',h)\sim \mathcal{D}^h}\left[\left(\left((1-\gamma)h(s)+\gamma\max \{h(s), V_h(s')\} \right) - Q_h(s,a)\right)^2 \right],\label{eq:qh_loss} \\
% \end{align}
% \begin{align}
\label{eq:vr_loss}
\mathcal{L}_{V_r} & = \mathbb{E}_{(s,a)\sim \mathcal{D}^r}\left[L^{\tau}\left(Q_r(s,a)-V_r(s)\right) \right], \\
\label{eq:qr_loss}
\mathcal{L}_{Q_r} & = \mathbb{E}_{(s,a,s',r)\sim \mathcal{D}^r}\left[\left(r+ \gamma V_r(s') - Q_r(s,a)\right)^2 \right].
\end{align}
where $L^\tau(u)=\left|\tau-\mathbb{I}(u<0)\right|u^2$ and $L^{\tau}_{\rm rev}(u)=|\tau - \mathbb{I}(u>0)|u^2$ with $\tau\in(0.5, 1)$. By doing so, $\pi_1$ and $\pi_2$ become one safety policy that avoids unsafe outcomes and one reward policy that tries to maximize the cumulative returns, respectively. 




Then, we train our context-aware modular network $\alpha(s;\theta)$ to combine $\pi_{0,1,2}$ to collaboratively tackle the safe offline RL problem. We filter the Top-30 trajectories with the highest rewards and costs below 5 from the dataset $\mathcal{D}$ to form a small near-expert dataset $\mathcal{D}^*$ that obtains a good balance among distributional shift, reward maximization and safety constraint. Then, we train $\alpha(s;\theta)$ by minimizing the following imitation learning loss based on the $\mathcal{D}^*$:

\begin{equation}
\mathcal{L}(\theta)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}^*}\left[\left\|{\epsilon}-{\epsilon}_{W}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
\label{equ:pi_theta_dsrl}
\end{equation}

where $W=W_0+\sum_{i=1}^2 \alpha_i(s;\theta)\Delta W_i$ as defined in Eq.~(\ref{equ:lora_adaptive_compose}).



We train $\pi_0$ for 1M gradient steps with a batch size of 2048 to ensure a good performance of $\pi_0$. Then, we only train $\pi_1$ and $\pi_2$ for 50K gradient steps, for the efficiency of LoRA modules. For $\alpha(s;\theta)$, we only train it for 1K gradient steps since all decomposed policies including $\pi_{0,1,2}$ are ready to be composed, which can significantly reduce the computational burden leveraging these pretrained policies. Summarized hyperparameters can be found in Table~\ref{tab:hyperparameters_mulob}.

\textbf{Baselines}. For FISOR~\citep{zheng2024safe}, CDT~\citep{liu2023constrained}, COptiDICE~\citep{lee2022coptidice}, CPQ~\citep{xu2022constraints} and BC, we adopt the results from FISOR~\citep{zheng2024safe}. For NSEC and ASEC results, we only change the compositional stages, and meanwhile keep all other training details the same to ensure a fair comparison.  Specifically, the context-aware modular for NSEC is trained via the following reparameterization method instead of the one in Eq.~(\ref{equ:pi_2_dsrl}):
\begin{equation}
    \epsilon_{\rm NSEC} = \epsilon_0 + \sum_{i=1}^2 \alpha_i(s;\theta) \epsilon_i,
    \label{equ:noise_compose}
\end{equation}
where $\epsilon_{0,1,2}$  is generated from networks with layers of $W_0$, $W_1=W_0+16\Delta W_1$ and $W_2=W_0+16\Delta W_2$, respectively. We can see that the composition in Eq.~(\ref{equ:noise_compose}) between skills happens in the noise space, and thus we denote it as NSEC (noise skill expansion and composition).

For ASEC, we directly compose the generated actions of different policies:
\begin{equation}
    a_{\rm ASEC} = a_0 + \sum_{i=1}^2 \alpha_i(s;\theta) a_i
    \label{equ:action_compose}
\end{equation}

where $a_{0,1,2}$ are the actions generated from the denoising process in Eq.~(\ref{equ:reverse}) using the predicted noise $\epsilon_{0,1,2}$ generated by networks with layers of $W_0$, $W_1=W_0+16\Delta W_1$ and $W_2=W_0+16\Delta W_2$, respectively. The composition happens in action space, and thus we denote it as ASEC (action skill expansion and composition).

% For our method PSEC, we first train the decomposed policies, behavior clone policy, safety policy, and return policy. The behavior clone policy is the pre-trained model. The safety policy and the return policy are represented by LoRA~\citep{hu2021lora} model. 
% Then we compose both policies together to solve the task in safe offline RL.  
% The decomposed policy is trained for 50k steps with 5 random seeds, which is only 5 percent of the previous offline RL methods training steps. During composition, we adopt a few filtered near-expert demonstrations that jointly consider the trilogy objective to train the context-aware modular, but are too limited to imitate good policies. The context-aware modular is trained for only 1k steps with 5 random seeds. Our method is less expensive to train, which is training efficient. In order to mimic safety-critical scenarios, the cost limit is set to 5.

% The experiments on DSRL benchmark are shown in Table~\ref{tab:main-result}. The experimental outcomes showcase the superiority of PSEC, outperforming other baselines and achieving state-of-the-art performance. The previous method also requires complex manual parameter tuning, but PSEC ensures that some mandatory parameters are the same while not adjusting any hyperparameters. The hyperparameters table is shown in Table~\ref{tab:hyperparameters_mulob}.

% To evaluate the flexibility of PSEC, we conducted experiments on Metadrive tasks to evaluate whether PSEC dynamically adapts to the use of different policies in different scenarios, as shown in Figure~\ref{fig:meta-context}. The results show that PSEC can adaptively adjust the contribution of decomposed policies to adapt to the different scenarios. Interestingly, we find that the dynamic adjustment of the policies is regular. When the car meets the obstacles, PSEC will enlarge the weight of the safety policy to ensure safe. During the process of turning right, PSEC will keep the larger weight of the safety policy and smaller weight of the return policy to ensure the safety of the car. When the car is on a straight road with fewer cars, PSEC will give about the equal weight to the safety policy and the return policy, which can better trade-off two policies based on real-time scenario dynamics.


% \begin{table*}[h]
%     \caption{Performance of different methods across varying trajectory lengths. The results correspond to scores mean and standard deviations across multiple trials. PSEC combines walker-stand and walker-walk to finish the walker-walk task.}
%     \label{tab:walktrajs}
%     % \vskip 0.15in
%     \centering
%     \small
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{cccccc}
%     \hline
%     \toprule
%     \multicolumn{1}{c}{\bf Trajectories}  & \multicolumn{1}{c}{\bf Train from scratch} & \multicolumn{1}{c}{\bf NSEC w/o CA} & \multicolumn{1}{c}{\bf NSEC} & \multicolumn{1}{c}{\bf PSEC w/o CA} & \multicolumn{1}{c}{\bf PSEC} \\
%     \midrule
%     10 & $58.9 \pm 36$ & $190.6 \pm 80$ & $320.9 \pm 150$ & $539.5 \pm 98$ & $\bm{688 \pm 17}$ \\
%     30 & $406 \pm 146$ & $548 \pm 79$ & $749 \pm 120$ & $814.6 \pm 113$ & $\bm{861.8 \pm 33}$ \\
%     50 & $848.9 \pm 107$ & $786.1 \pm 155$ & $906.6 \pm 58$ & $931.5 \pm 28$ & $\bm{935.3 \pm 25}$ \\
%     100 & $911.4 \pm 81$ & $921.9 \pm 46$ & $\bm{974.9 \pm 7}$ & $967.0 \pm 4$ & $\bm{973.5 \pm 2}$ \\
%     \bottomrule
%     \hline
%     \end{tabular}
%     \vspace{-5pt}
% \end{table*}



\subsection{Continual Policy Shift}
\label{subsec:exp_conpolicy_shift_appendix}
To evaluate PSEC's ability to continually evolving its capabilities when tackling new challenges, we conduct experiments on DeepMind Control Suite (DMC)~\citep{tassa2018deepmind}, where a walker agent is progressively required to stand, walk, and run, as shown in Figure~\ref{fig:SWR}. We use three expert datasets including walker-stand $\mathcal{D}_e^{\mathcal{T}_0}$, walker-walk $\mathcal{D}_e^{\mathcal{T}_1}$, and walker-run $\mathcal{D}_e^{\mathcal{T}_2}$, released by~\citet{bai2024pessimistic} for the policy learning. Specifically, $\mathcal{D}_e^{\mathcal{T}_0}$, $\mathcal{D}_e^{\mathcal{T}_1}$ and $\mathcal{D}_e^{\mathcal{T}_2}$ contains 1000, 10 and 10 trajectories, respectively. $\mathcal{D}_e^{\mathcal{T}_1}$ and $\mathcal{D}_e^{\mathcal{T}_2}$ contain only a handful of data because we aim to test if the agent can leverage the knowledge from the standing skill to efficiently adapt to new tasks. We first pretrain $\pi_0$ on the large $\mathcal{D}_e^{\mathcal{T}_0}$ to obtain the basic standing policy via minimizing the following behavior cloning loss:

\begin{equation}
\mathcal{L}_{\pi_{0}}(W_{0})=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_e^{\mathcal{T}_0}}\left[\left\|{\epsilon}-{\epsilon}_{W_{0}}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right].
\label{equ:pi_stand}
\end{equation}

\textbf{Stand$\rightarrow$ Walk (S$\rightarrow$ W) task}. Then, we can integrate the walking skill $\pi_1$ into the skill library $\Pi$ by optimizing the following objective:

\begin{equation}
\mathcal{L}_{\pi_{1}}(\Delta W_{1})=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_e^{\mathcal{T}_1}}\left[\left\|{\epsilon}-{\epsilon}_{W_{1}}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
\label{equ:pi_walk}
\end{equation}

where $W_1=W_0+16\Delta W_1$. Then, we train a context-aware modular $\alpha^{\rm walk}(s;\theta_1)\in\mathbb{R}$ to combine $\pi_0$ and $\pi_1$ to jointly tackle the walking task:

\begin{equation}
\mathcal{L}(\theta_1)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_e^{\mathcal{T}_1}}\left[\left\|{\epsilon}-{\epsilon}_{W_{\rm walk}}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
\label{equ:pi_walk_compose}
\end{equation}
where $W_{\rm walk}=W_0+\alpha^{\rm walk}(s;\theta_1)\Delta W_1$. In this setting, we hope the final policy parameterized by $W_{\rm walk}$ can outperform the naive policy that is trained from scratch on the small data $\mathcal{D}_e^{\mathcal{T}_1}$ to demonstrate the significance of utilizing the prior knowledge in $\pi_0$ for efficient task adaptation.

\textbf{Stand$\rightarrow$Run (S$\rightarrow$R) task}. Here, the adaptation for the running policy $\pi_2$ is similar.  We can replace $W_1$ in Eq.~(\ref{equ:pi_walk}) with $W_2=W_0+16\Delta W_2$ and $\mathcal{D}_e^{\mathcal{T}_1}$ as $\mathcal{D}_e^{\mathcal{T}_2}$ to train $\pi_2$ parameterized by $\Delta W_2$. Additionally, we replace ${W_{\rm walk}}$ in Eq.~(\ref{equ:pi_walk_compose}) as ${W_{\rm run}}=W_0+ \alpha^{\rm run}(s;\theta_2) \Delta W_2$ and $\mathcal{D}_e^{\mathcal{T}_1}$ as $\mathcal{D}_e^{\mathcal{T}_2}$ to train $\alpha^{\rm run}(s;\theta_2)$ to combine $\pi_{0}$ and $\pi_{2}$ to generate the running skill.

% \ljx{revised till here}
\textbf{Stand+Walk$\rightarrow$Run (S+W$\rightarrow$R) task}. After obtaining $\pi_0$, $\pi_1$, and $\pi_2$, the composition for the running skill becomes very simple. 
% We can replace $W_1$ in Eq.~(\ref{equ:pi_walk}) with $W_2=W_0+16\Delta W_2$ to train $\pi_2$ parameterized by $\Delta W_2$, 
We can replace ${W_{\rm walk}}$ in Eq.~(\ref{equ:pi_walk_compose}) as ${W}=W_0+\sum_{i=1}^2 \alpha_i(s;\theta) \Delta W_i$ to train $\alpha(s;\theta)\in\mathbb{R}^2$ to combine $\pi_{0,1,2}$ to generate the running skill. In this setup, we aim to prove that utilizing the library that contains $\pi_{0,1,2}$ (S+W$\rightarrow$R) can outperform $\pi_{0,2}$ (S$\rightarrow$R) to show the learning capability of PSEC can gradually grow after incorporating more skill primitives. 

% In this setting, we have four networks required to train: the stand policy $\pi_{stand}$, the walk policy $\pi_{walk}$, the run policy $\pi_{run}$, and the context-aware modular $\alpha_i(s;\theta), i=1,2,3$. Firstly, we pretrain $\pi_{stand}$ as pretrained model for 1 million steps on the full walk-stand dataset $\mathcal{D}_{stand}$, minimizing the following objective to ensure a strong performance for the basic policy:

% \begin{equation}
% \mathcal{L}_{\pi_{stand}}(W_{stand})=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_{stand}}\left[\left\|{\epsilon}-{\epsilon}_{W_{stand}}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right].
% \label{equ:pi_stand}
% \end{equation}


% First, we use the behavior clone~(BC) method to train the stand policy $\pi_s$ for 1 million steps on the Walker-stand dataset with 1000 trajectories, as the pretrained model in LoRA. The training objective is the following equation: 


% Then we train $\pi_{walk}$ with the walker-walk dataset with only 10 trajectories by optimizing the newly introduced LoRA modules $\Delta W_{walk}$ by minimizing the following objectives in Eq.~(\ref{equ:pi_walk}):

% \begin{equation}
% \mathcal{L}_{\pi_{walk}}(\Delta W_{walk})=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_{walk}}\left[\left\|{\epsilon}-{\epsilon}_{W_{walk}}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
% \label{equ:pi_walk}
% \end{equation}

% where the weights of LoRA augmented layer are $W_{walk}=W_{stand} + 16\Delta W_{walk}$. In this way, the walk policy $\pi_{walk}$ combines the knowledge of $\pi_{stand}$. Then, we train the context-aware modular $\alpha_i(s;\theta_{walk}), i=1$ to combine $\pi_{stand}, \pi_{walk}$ to collaboratively tackle the walker-walk task. We use only 10 trajectories from walk-walk dataset to train $\alpha_i^{walk}(s;\theta), i=1,2$ by minimizing the following imitation learning loss based on the $\mathcal{D}_{walk}$:

% \begin{equation}
% \mathcal{L}(\theta_{walk})=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}^*}\left[\left\|{\epsilon}-{\epsilon}_{W_{walk}}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
% \label{equ:pi_2_dsrl}
% \end{equation}

% where $W_{walk} = W_{stand} + \alpha_i(s;\theta_{walk}) \Delta W_{walk}$ as defined in Eq.~(\ref{equ:lora_adaptive_compose}).

% After traininig $\pi_0, \pi_1$ with $W_0, \Delta W_1$ to solve the walk-stand and walk-walk tasks, we 
% need train $\pi_2$ for the walk-run tasks under the continual learning setting. Then, we train $\pi_2$ with the walk-run dataset with only 10 trajectories by optimizing the newly introduced LoRA modules $\Delta W_2$ by minimizing the following objectives in Eq.~(\ref{equ:pi_run}):

% \begin{equation}
% \mathcal{L}_{\pi_{2}}(\Delta W_{2})=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_e^{\mathcal{T}_2}}\left[\left\|{\epsilon}-{\epsilon}_{W_{2}}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
% \label{equ:pi_run}
% \end{equation}
% where $W_2=W_0 + 16 \Delta W_2$. Then, we train a context-aware modular $\alpha_i^{\rm run}(s;\theta), i=1,2$ with only 10 trajectories from the walker-run dataset to leverage $\pi_0, \pi_1, \pi_2$ for the running task:

% \begin{equation}
% \mathcal{L}(\theta_2)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_e^{\mathcal{T}_2}}\left[\left\|{\epsilon}-{\epsilon}_{W_{\rm run}}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right].
% \label{equ:pi_stand}
% \end{equation}
% where $W_{\rm run}=W_0+\sum_{i=0}^{2} \alpha_i^{\rm run}(s;\theta)\Delta W_i$ as defined in Eq.~(\ref{equ:lora_adaptive_compose}). 

We train $\pi_0$ for 1M gradient steps with a batch size of 1024 to ensure a good performance of $\pi_0$. Then, we only train $\pi_1$ and $\pi_2$ for 10K gradient steps with 10 trajectories thanks to the efficiency of LoRA. For $\alpha^{\rm walk}(s; \theta), \alpha^{\rm run}(s; \theta), \alpha(s;\theta)$, we only train them for 1K gradient steps since the decomposed policies including $\pi_{0,1,2}$ in the skill library are ready to be composed, which can significantly reduce the computational burden leveraging these pretrained policies. The summarized hyperparameters can be found in Table~\ref{tab:hyperparameters_contipolicy}.

% In the future, if the agent meets a new task, it can leverage all of the policies from the skill library.  Meanwhile, the skill library can be constantly enriched and refined. Otherwise, we evaluate the performance of PSEC with different trajectory quantities, such as 10, 30, 50, and 100 trajectories as shown in Figure~\ref{fig:dmc_comparison}(a). The results show that PSEC using LoRA to train the policy can achieve better performance, especially with a small amount of data and training steps. PSEC can leverage the previous knowledge to adapt to new tasks efficiently. 

\textbf{Baselines.} 
We compare PSEC with other composition methods NSEC and ASEC, the Scratch method, and the variant PSEC~(MLP). NSEC and ASEC train the context-aware modular represented by Eq.~(\ref{equ:noise_compose}) and Eq.~(\ref{equ:action_compose}), respectively. Scratch method means training a policy from scratch by IDQL~\citep{hansen2023idql}, since we build our model based on the IDQL method. PSEC~(MLP) replaces the LoRA matrices with the MLP network in PSEC.

\textbf{Experimental setups for Figure~\ref{fig:dmc_comparison}}. For Figure~\ref{fig:dmc_comparison}(a), we evaluate the sample efficiency of PSEC framework. Specifically, we evaluate on the \textbf{S$\rightarrow$ W} task with different data quantities of the \textbf{W} dataset $\mathcal{D}^{\mathcal{T}_1}_e$, including 10, 30, 50, and 100 trajectories, trained with 10K, 30K, 50K, and 100K training steps, respectively.
We compare PSEC with other baselines to demonstrate the sample efficiency of parameter-level composition over other composition methods.
% Then, we use the same trajectories to train the context-aware modular with only 1K gradient steps for the walker-walk task. We compare PSEC with other baseline methods under the same trajectories and training steps. The performance is illustrated in Figure~\ref{fig:dmc_comparison}(a). 

For Figure~\ref{fig:dmc_comparison}(b), we visualize the training curves of PSEC, PSEC~(MLP) and Scratch for the \textbf{S$\rightarrow$W} task trained solely on Eq.~(\ref{equ:pi_walk}) without the composition in Eq.~(\ref{equ:pi_walk_compose}) to demonstrate the efficiency of LoRA modules over the naive MLPs and the efficiency to leverage pretrain policies. In this setting, $\mathcal{D}_e^{\mathcal{T}_1}$ contains
% We leverage the pre-trained standing skill $\pi_0$ to train the walking skill $\pi_1$ with 
10 trajectories and we train each method for 10K training steps. 

For Figure~\ref{fig:dmc_comparison}(c), w/o CA represents the compositional weight $\alpha$ is tuned by humans, rather than auto-generated by our context-aware modular $\alpha_\theta$. We compare PSEC, NSEC, ASEC with their corresponding  w/o CA variants to further demonstrate the importance of dynamical compositions.

% The PSEC w/o CA represents PSEC without the context-aware modular, which is the direct composition of the $\pi_0$ and $\pi_1$. NSEC w/o CA represents composing the $\pi_0$ and $\pi_1$ with fixed weights in the score-level. ASEC w/o CA represents composing the $\pi_0$ and $\pi_1$ with fixed weights in the action-level. In order to validate the effectiveness of the context-aware modular, we compare the performance of PSEC and PSEC without the context-aware modular with NSEC and ASEC. We train the context-aware modular with 10 trajectory data for only 1K training steps. 

% In this way, we compose the stand policy and the walk policy to be the final walk policy by training the context-aware modular for only 1k training steps, which can adjust the policy integration at the parameter-level based on the different states. 
% Otherwise, we evaluate the performance of PSEC with different trajectories, such as 10, 30, 50, and 100 trajectories as shown in Figure~\ref{fig:dmc_comparison}(a). The results show that PSEC using LoRA to train the policy can achieve better performance, especially with a small amount of data and training steps. PSEC can leverage the previous knowledge to adapt to new tasks efficiently. 

% Then, we only use 10 trajectories of run task to train the run LoRA module for 10k training steps. Similarly, we train the context-aware modular to compose the stand policy, walk LoRA module and run LoRA module to finish the learning of the run policy. In this way, the run policy will leverage the previously learned knowledge to achieve a better run.  The results of the experiments are shown in Table~\ref{tab:policy_shift}. The results show that PSEC leveraging previous knowledge can achieve better performance. Meanwhile, we conduct experiments on walk task with 10 trajectories to evaluate the training efficiency of PSEC. The training curve is shown in Figure~\ref{fig:dmc_comparison}(b).
% The results of experiments illustrate that PSEC can obtain high scores under the same training steps. PSEC can always outperform other baseline methods, even at only 10k training steps. We evaluate the efficiency of the context-aware modular, as shown in Figure~\ref{fig:dmc_comparison}(c).
% The context-aware modular plays an important role in PSEC. 
% The run task is more difficult than the walk task.
We conduct similar experiments on the \textbf{S$\rightarrow$R} task and 
% to evaluate the performance of composing the pretrained stand model and the run LoRA module.
the results are presented in Figure~\ref{fig:lora-run}. Note that the running skill is more difficult. PSEC shows marked superiority on this challenging setting.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\linewidth]{DMC/ablation_walk.pdf}
    \vspace{-5pt}
     \caption{\small Results in the policy shift setting. Each value is averaged over 10 episodes and 5 seeds.}
    \label{fig:ablation_walk}
    \vspace{-10pt}
\end{figure}


\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\linewidth]{DMC/SWR.pdf}
    \vspace{-5pt}
     \caption{\small Continual evolution on DeepMind Control Suite for Continual policy shift.}
    \label{fig:SWR}
    \vspace{-10pt}
\end{figure}


% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\linewidth]{DMC/SWR.pdf}
%     \vspace{-5pt}
%      \caption{\small Continual evolution on DeepMind Control Suite for Continual policy shift.}
%     \label{fig:SWR}
%     \vspace{-10pt}
% \end{figure}


% \begin{figure}[h]
%     \centering
%     \begin{subfigure}[b]{0.45\textwidth}
%         \centering
%         \includegraphics[width=\textwidth]{DMC/ablation_walk.pdf}
%         \caption{\small Sample efficiency.}
%         \label{fig:run}
%     \end{subfigure}
%     % \hfill
%     \begin{subfigure}[b]{0.49\textwidth}
%         \centering
%         \raisebox{19mm}{ % 调整右边图形的垂直位置
%             \includegraphics[width=\textwidth]{DMC/SWR.pdf}
%         }
%         \caption{\small Training efficiency.}
%         \label{fig:run_training_efficiency}
%     \end{subfigure}
%     % \vspace{-15pt}
%     \caption{\small Comparisons on sample and training efficiency and the effectiveness of context-aware modular. S, R denote stand, run, respectively. Each value is averaged over 10 episodes and 5 seeds.}
%     \label{fig:lora-run}
% \end{figure}

% \begin{table*}[t]
%     \caption{Performance of different methods across varying trajectory lengths. The results correspond to the mean and standard deviations across multiple trials. PSEC combines walker-stand and walker-run to finish the walker-run task.}
%     \label{tab:runtrajs}
%     % \vskip 0.15in
%     \centering
%     \small
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{cccccc}
%     \hline
%     \toprule
%     \multicolumn{1}{c}{\bf Trajectories}  & \multicolumn{1}{c}{\bf Train from scratch} & \multicolumn{1}{c}{\bf NSEC w/o CA} & \multicolumn{1}{c}{\bf NSEC} & \multicolumn{1}{c}{\bf PSEC w/o CA} & \multicolumn{1}{c}{\bf PSEC} \\
%     \midrule
%     10 & $25.5 \pm 4$ & $30.3 \pm 10$ & $38.5 \pm 6$ & $167.3 \pm 1$ & $\bm{221.9 \pm 9}$ \\
%     30 & $36.5 \pm 8$ & $56 \pm 8$ & $69 \pm 5$ & $334.5 \pm 6$ & $\bm{347.3 \pm 23}$ \\
%     50 & $105.3 \pm 13$ & $152.9 \pm 20$ & $142.1 \pm 40$ & $363.4 \pm 53$ & $\bm{391.6 \pm 19}$ \\
%     100 & $298 \pm 50$ & $319.3 \pm 70$ & $346.4 \pm 60$ & $424 \pm 14$ & $\bm{501 \pm 23}$ \\
%     \bottomrule
%     \hline
%     \end{tabular}
%     % \vspace{-10pt}
% \end{table*}



\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DMC/main_run.pdf}
        \caption{\small Sample efficiency.}
        \label{fig:run}
    \end{subfigure}
    % \hfill
    \begin{subfigure}[b]{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DMC/lora-run1.pdf}
        \caption{\small Training efficiency.}
        \label{fig:run_training_efficiency}
    \end{subfigure}
    % \hfill
    \hspace{0.1cm}
    \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DMC/run_context-aware-ablation.pdf}
        \vspace{-6pt}
        \caption{\small Context-aware efficiency.}
        \label{fig:run}
    \end{subfigure}
    % \vspace{-15pt}
    \caption{\small Comparisons on sample and training efficiency and the effectiveness of context-aware modular. S, R denote stand, run, respectively. Each value is averaged over 10 episodes and 5 seeds.}
    \label{fig:lora-run}
\end{figure}




% PSEC outperforms more significantly on more challenging tasks, both in terms of sample efficiency and training efficiency. 

% \begin{table*}[t]
%     \caption{Performance of different methods across varying trajectory lengths. The results correspond to the mean and standard deviations of scores across multiple trials.}
%     \label{walk-run-result}
%     % \vskip 0.15in
%     \centering
%     \small
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{lccccc}
%     \hline
%     \toprule
%     \multirow{2}{*}{\bf Trajectories} & \multirow{2}{*}{\bf Train walk} & \multirow{2}{*}{\bf Train run} & \multicolumn{2}{c}{\bf PSEC (stand+walk)} & \multirow{2}{*}{\bf PSEC (stand+run+walk) } \\
%     \cmidrule(lr){4-5} 
%      & {\bf from scratch}& {\bf from scratch} & {\bf w/o CA $\rightarrow$ walk} & {\bf $\rightarrow$ walk} & {\bf$\rightarrow$ walk} \\
%     \midrule
%     10 & $58.9 \pm 36$ & $25.5 \pm 4$ & $539.5 \pm 98$ & $688 \pm 17$ & $\bm{781.4 \pm 93}$ \\
%     30 & $406 \pm 146$ & $36.5 \pm 8$ & $814.6 \pm 113$ & $861.8 \pm 33$ & $\bm{868.3 \pm 22}$ \\
%     \bottomrule
%     \hline
%     \end{tabular}
% \end{table*}

% \begin{table*}[t]
%     \caption{Performance of different running methods across varying trajectory lengths. The results correspond to the mean and standard deviations of scores across multiple trials.}
%     \label{run-result}
%     % \vskip 0.15in
%     \centering
%     \small
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{lccccc}
%     \hline
%     \toprule
%     \multirow{2}{*}{\bf Trajectories} & \multirow{2}{*}{\bf Train walk} & \multirow{2}{*}{\bf Train run} & \multicolumn{2}{c}{\bf PSEC (stand+run)} & \multirow{2}{*}{\bf PSEC (stand+walk+run) } \\
%     \cmidrule(lr){4-5} 
%      & {\bf from scratch} & {\bf from scratch} & {\bf w/o CA $\rightarrow$ run} & {\bf $\rightarrow$ run} & {\bf$\rightarrow$ run} \\
%     \midrule
%     10 & $58.9 \pm 36$ & $25.5 \pm 4$ & $167.3 \pm 1$ & $221.9 \pm 9$ & $\bm{247.1 \pm 24}$ \\
%     30 & $406 \pm 146$ & $36.5 \pm 8$ & $334.5 \pm 6$ & $347.3 \pm 23$ & $\bm{350.6 \pm 16}$ \\
%     % liuleliule
%     \bottomrule
%     \hline
%     \end{tabular}
% \end{table*}



% \newpage
\subsection{Dynamic Shift}
\label{subsec:exp_dynamic_shift_appendix}
To further validate the versatility of PSEC, we conduct experiments in a practical and common setting: dynamic shift.
We conduct experiments on the D4RL benchmark, where we modify the dynamics and morphology of locomotive robots to reflect the dynamics changes. Our goal is to leverage the policies based on the source datasets $\mathcal{D}_o^{\mathcal{P}_0}$ and a small amount of the target datasets $\mathcal{D}_o^{\mathcal{P}_1}$ to adapt to the target task quickly. Specifically, the datasets $\mathcal{D}_o^{\mathcal{P}_0}$ contain 20K transitions with 3 types of dynamic modifications on $\mathcal{P}_0$: \textit{1) Friction}: the friction coefficient of the robot is modified; \textit{2) Gravity}: the gravity acceleration in the simulation environment is changed. \textit{3) Thigh}: the thigh is enlarged to double its original size to produce a morphology gap on the embodiment. The target datasets $\mathcal{D}_o^{\mathcal{P}_1}$ are sampled from the D4RL benchmark with un-modified dynamics $\mathcal{P}_1$, including 6 types: halcheetah-medium-v2, halfcheetah-medium-replay-v2, halfcheetah-medium-expert-v2, walker2d-medium-v2, walker2d-medium-replay-v2, walker2d-medium-expert-v2, as shown in Figure~\ref{fig:source-target}. Each dataset type of $\mathcal{D}_o^{\mathcal{P}_1}$ contains solely 10K transitions, which are too limited to train good policies directly on the target dynamics $\mathcal{P}_1$ from scratch.


We first pretrain $\pi_0$ with dataset $\mathcal{D}_o^{\mathcal{P}_0}$ for 20k training steps by behavior cloning via minimizing the following objectives:

\begin{equation}
\mathcal{L}_{\pi_0}(W_0)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_o^{\mathcal{P}_0}}\left[\left\|{\epsilon}-{\epsilon}_{W_0}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right].
\label{equ:pi_d4rl}
\end{equation}

Then, we try to use the limited 
% We have only 10k transitions from the original D4RL datasets with dynamics 
$\mathcal{P}_1$ to adapt $\pi_0$ to the target domain
% , where these datasets are not enough to train a better policy
. PSEC uses LoRA to train a new policy $\pi_1$ with the pretrained source policy $\pi_0$ by minimizing the following objectives:

\begin{equation}
\mathcal{L}_{\pi_1}(\Delta W_1)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_o^{\mathcal{P}_1}}\left[w^r(s,a)\left\|{\epsilon}-{\epsilon}_{ W_1}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
\label{equ:pi_1_d4rl}
\end{equation}

where $W_1 = W_0 + 16\Delta W_1$. Finally, PSEC uses the context-aware modular $\alpha(s;\theta)$ to integrate policy $\pi_0, \pi_1$ using the target dataset $\mathcal{D}_o^{\mathcal{P}_1}$ to transfer to the target dynamics $\mathcal{P}_1$. The context-aware modular $\alpha(s;\theta)$ is trained for only 1k training steps by minimizing the following objectives:

\begin{equation}
\mathcal{L}(\theta)=\mathbb{E}_{t\sim\mathcal{U},{\epsilon}\sim\mathcal{N}(0,I),({s},{a})\sim\mathcal{D}_o^{\mathcal{P}_1}}\left[\left\|{\epsilon}-{\epsilon}_{W}\left(\sqrt{\bar{\rho}_t}{a}+\sqrt{1-\bar{\rho}_t}{\epsilon},t,{s}\right)\right\|^2\right],
\label{equ:pi_2_d4rl}
\end{equation}

where $W=W_0+ \alpha(s;\theta)\Delta W_1$ as defined in Eq.~(\ref{equ:lora_adaptive_compose}). 

We train $\pi_0$ for 1M gradient steps with a batch size of 1024 to ensure a good performance of $\pi_0$. Then, we only train $\pi_1$ for 20k gradient steps, for the efficiency of LoRA modules. For $\alpha(s;\theta)$, we only train it for 1K gradients steps since all decomposed policies including $\pi_{0,1}$ are ready to be composed, which can efficiently adapt to the target domain leveraging the pretrained source policies. Summarized hyperparameters can be found in Table~\ref{tab:hyperparameters_dyshift}.

\textbf{Baselines.} We compare PSEC with other methods in dynamic shift settings, including behavioral cloning (BC), offline RL approaches like CQL~\citep{kumar2020conservative}, IQL~\citep{kostrikov2021offline}, and model-based methods such as MOPO~\citep{yu2020mopo}. Additionally, we evaluate more generalizable offline RL methods, specifically DOGE~\citep{li2023when} and TSRL~\citep{cheng2023look}, which have demonstrated superiority in small sample regimes. The baseline results for comparison are sourced from the TSRL paper~\citep{cheng2023look}, which reports state-of-the-art performance in these regimes. Furthermore, we assess policies trained on combinations of the offline datasets $\mathcal{D}_o^{\mathcal{P}_0}$ and $\mathcal{D}_o^{\mathcal{P}_1}$ under various dynamics settings, referred to as Joint train~(Gravity), Joint train~(Friction), and Joint train~(Thigh). These combinations involve training with one source dataset under dynamic shifts (e.g., changes in gravity, friction, or thigh size) and target datasets such as halfcheetah-medium-v2, halfcheetah-medium-replay-v2, halfcheetah-medium-expert-v2, walker2d-medium-v2, walker2d-medium-replay-v2, and walker2d-medium-expert-v2. In order to maintain fairness, the joint train method is trained in the same way as PSEC is trained on the source datasets. The results and training curves of PSEC across these settings are presented in Table~\ref{tab:performance_transposed} and Figure~\ref{fig_dmc_results}, respectively. These comparisons showcase the effectiveness of PSEC under dynamic shifts and small sample conditions.
% We compare PSEC with other methods on the dynamic shift setting, including BC, offline RL methods like CQL~\citep{kumar2020conservative}, IQL~\citep{kostrikov2021offline}, MOPO~\citep{yu2020mopo}. In addition, we evaluate some generalizable offline RL methods that are superior in the small sample regimes, including DOGE~\citep{li2023when} and TSRL~\citep{cheng2023look}. The baseline results are directly sourced from the TSRL papers~\citep{cheng2023look} with the state-of-the-art performance in small sample regimes. Meanwhile, we evaluate the policy train on the combination of  $\mathcal{D}_o^{\mathcal{P}_0}$ and $\mathcal{D}_o^{\mathcal{P}_1}$, referred to as Joint train~(Gravity), Joint train~(Friction), Joint train~(Thigh), which represents training one of the source datasets with Gravity, Friction, Thigh dynamics changing in combination with one of the target datasets, halfcheetah-medium-v2, halfcheetah-medium-reaply-v2, halfcheetah-medium-expert-v2, walker2d-medium-v2, walker2d-medium-replay-v2, walker2d-medium-expert-v2. The results and the training curves of PSEC are illustrated in Table~\ref{tab:performance_transposed} and Figure~\ref{fig_dmc_results}, respectively.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.93\linewidth]{DMC/soure_target.pdf}
    \vspace{-5pt}
     \caption{\small The illustration of the source and target domains for the dynamic shift setting.}
    \label{fig:source-target}
    \vspace{-10pt}
\end{figure}


\subsection{T-SNE Experimental Setups for Figure~\ref{fig:tsne}}
\label{subsec:tsne}
To provide empirical support of the advantages of parameter-level composition over other levels of composition, we visualize the t-SNE~\citep{van2008visualizing} projection of data samples in different spaces. Specifically, for each dataset $\mathcal{D}^{\mathcal{T}_0}_e$, $\mathcal{D}^{\mathcal{T}_1}_e$, $\mathcal{D}^{\mathcal{T}_2}_e$ in the continual policy shift setting in Section~\ref{subsec:exp_conpolicy_shift_appendix}, we randomly sample 512 data samples $(s,a)$, which forms three types of data that encode the standing, walking and running skill, respectively. In the action space, we directly utilize t-SNE projection to map these sampled data into a 2-dimentional space in Figure~\ref{fig:tsne} (c). For the noise space, we add 1 step of noise on the sampled actions following the forward diffusion process in Eq.~(\ref{equ:forward_diffusion}) and get the tuple $(s, a_1)$ for different skills. Then, we generate the noise based on this noisy tuples and visualize their t-SNE projections in Figure~\ref{fig:tsne} (b). In parameter-space, we feed the noisy tuples $(s,a_1)$ into the trained networks and get the output features of the middle LoRA augmented layers. Then, we project these features using t-SNE in Figure~\ref{fig:tsne} (a).

\section{More Experimental Results}
\label{sec:moreexp_appendix}
\begin{figure}[h]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{DMC/fixed_com.pdf}
        \caption{Fixed composition on \textbf{S$\rightarrow$R} task}
        \label{fig:fixed_weight}
    \end{subfigure}
    % \hspace{1cm}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering\includegraphics[width=\textwidth]{DMC/adaptive_com.pdf}
        \caption{PSEC on \textbf{S$\rightarrow$R} task}
        \label{fig:adaptive_weight}
    \end{subfigure}
    \caption{Outputs weights of the context-aware modular on DeepMind Control}
    \label{fig:ws_trend}
\end{figure}
\subsection{The Effectiveness of the Context-aware Modular}

\textbf{Context-aware modular for the continual policy shift.} To further explore the effectiveness of the context-aware module, we employ it to analyze the trajectories generated by policies composed using fixed compositional weights. Specifically, for the \textbf{S$\rightarrow$R} task in Section~\ref{subsec:exp_conpolicy_shift_appendix}, the fixed composition method denote $W_{\rm run}=W_0+\alpha \Delta W_2$, which uses a fixed $\alpha=16$ to compose $\pi_0$ and $\pi_2$. Figure~\ref{fig:ws_trend}~(a) shows that naively using fixed compositional weights might accidentally stuck in some local suboptimal behavior such as standing still or falling down. We can clearly observe that our context-aware modular provides corresponding responses to correct these undesired behaviors. Therefore, it is necessary to adjust the weights of different strategies to fit the current states. Figure~\ref{fig:ws_trend}~(b) presents the trajectories generated by PSEC. It clearly demonstrates that by utilizing the context-aware modular, the agent can make subtle adjustments between skills and stably run across the entire episodes.

\subsection{The Parameter Efficiency of PSEC}

\textbf{Parameter efficiency.} 
% To evaluate the parameter efficiency of PSEC, we compare the parameter count of PSEC and the performance on different tasks with Scratch and PSEC~(MLP). The parameter of PSEC contains the LoRA parameter and the context-aware parameter on the walker-walk task or walker-run task. This is why the parameter count of PSEC~(MLP) where replaces the LoRA weights of PSEC with MLP is more than the parameter count of training from scratch method. The results of the parameter counts are shown in Figure~\ref{fig:model-para}. Meanwhile, the performance of the different methods on the DeepMind Control Suite~(DMC)\citep{tassa2018deepmind} task is shown in Figure~\ref{fig:training_efficiency} and Figure~\ref{fig:run_training_efficiency}. The parameter count of PSEC is 7.58\% of the parameter count of the Scratch method. However, the performance of PSEC outperforms the Scratch method and the PSEC~(MLP) method. It demonstrates that PSEC own the strong parameter efficiency.
To evaluate the parameter efficiency of PSEC, we compare its parameter count and performance on various tasks against both the Scratch method and PSEC~(MLP). The parameter count for PSEC includes the LoRA parameters and context-aware parameters specific to the walker-walk or walker-run tasks. The Scratch method represents training the policy from scratch with standard MLP.
PSEC~(MLP), which substitutes the LoRA weights with a standard MLP and retains the context-aware modular, has a higher parameter count than the Scratch method. The parameter counts are illustrated in Figure~\ref{fig:model-para}. In terms of performance, the results from the DeepMind Control Suite (DMC) tasks, as shown in Figures~\ref{fig:dmc_comparison}~(b) and \ref{fig:lora-run}~(b), indicate that PSEC achieves significantly better performance despite having only 7.58\% of the parameters used in the Scratch method. This performance advantage over both the Scratch method and PSEC~(MLP) demonstrates that PSEC possesses strong parameter efficiency, effectively leveraging a smaller number of parameters for superior task performance. In this way, PSEC can leverage and expand upon its existing knowledge base in novel
situations to enhance learning efficiency and adaptability.

\begin{figure}[h]
    \centering
    % \hspace{1cm}
    \begin{subfigure}[b]{0.49\textwidth}
        \centering\includegraphics[width=\textwidth]{DMC/model-params-no-grid.pdf}
        % \caption{PSEC on \textbf{S$\rightarrow$R} task}
        \label{fig:model-para}
    \end{subfigure}
    \caption{\small Comparison of Model Parameters: The parameter count for PSEC is approximately 7.58\% of Scratch, demonstrating a significantly smaller model size while maintaining strong parameter efficiency, effectively leveraging a smaller number of parameters for superior task performance..}
    \label{fig:model-para}
\end{figure}



% \vspace*{\fill}
% \newpage
% \clearpage
\section{More experimental details}\label{More_details}
\subsection{\textcolor{black}{Description of Tasks}}
% 1.What are the tasks?

We conduct experiments on 9 MetaDrive tasks and 8 Bullet-Safety-Gym tasks in the DSRL benchmark~\citep{liu2023datasets}. The visualization of the environments is shown in Figure~\ref{fig:task_descri}.
The tasks aim to learn policy from different level datasets such that the policy satisfies a safety constraint (normalized cost $<$ 1) and achieves higher rewards.

\textbf{MetaDrive.} \textcolor{black}{It leverages the Panda3D game engine to simulate realistic driving scenarios. The tasks are categorized as \texttt{\{Road\}\{Vehicle\}}, where “Road” encompasses three levels of difficulty for self-driving cars: easy, medium, and hard, while “Vehicle” represents four levels of surrounding traffic density: sparse, mean, and dense. In MetaDrive’s autonomous driving tasks, costs are incurred from three safety-critical scenarios: (i) collision, (ii) out of road, and (iii) over-speed.}

\textbf{Bullet-Safety-Gym.} \textcolor{black}{The environments are built on the PyBullet physics simulator. They feature four types of agents: Ball, Car, Drone, and Ant, alongside two task types: Circle and Run. Tasks are designated as \texttt{\{Agent\}\{Task\}}, combining the agent and the corresponding task type.}

\begin{figure}[h]
    \centering
        \centering
        \includegraphics[width=0.95\textwidth]{metadrive/meta-task-fig.pdf}
        % \label{fig:task_descri}
        \vspace{-5pt}
    \caption{\small \textcolor{black}{Visualization of the simulation environments and representative tasks of MetaDrive and Bullet-Safety-Gym. The figure is credited to \cite{liu2023datasets}.}}
    \label{fig:task_descri}
    \vspace{-5pt}
\end{figure}

\subsection{\textcolor{black}{Illustration of the Recorded Data}}

% 2.What do the recorded data look in the offline benchmarks?

\textcolor{black}{To get a more intuitive look at the recorded data, we calculate the total reward and total cost for each trajectory in the datasets. These values are then plotted on a two-dimensional plane, where the x-axis corresponds to the total cost and the y-axis to the total reward. The results are shown in Figure~\ref{fig_dataset_vis} in the Appendix~\ref{More_details} of the paper. The plot highlights the dataset’s diversity, particularly in how it captures a range of trajectory behaviors. The reward frontiers relative to cost illuminate the task’s complexity, as the shape of these frontiers can significantly influence the challenges faced by offline learners. Trajectories offering high rewards but incurring high costs pose an alluring yet risky opportunity, often testing the balance between optimizing performance and maintaining safety constraints. This duality underscores the importance of robust algorithms that can navigate the trade-off effectively.}


\subsection{\textcolor{black}{Advantage of the Benchmark}}
% 3. Why would they provide a characteristic examples to highlight the pros and cons of PSEC?
\textcolor{black}{By generating diverse datasets across many environments with systematically varied complexities, the DSRL benchmark creates a rich and representative evaluation suite. This diversity ensures that our method is tested under a wide range of conditions, capturing different task structures, safety constraints, and levels of stochasticity. Meanwhile, the DSRL benchmark includes multiple objectives, making it well-suited for testing the flexibility and efficiency of our method in handling new tasks. Providing diverse datasets across varying difficulty levels and incorporating multiple optimization goals enables a comprehensive evaluation of our method’s adaptability and performance across a broad spectrum of scenarios.}



% \clearpage
\section{\textcolor{black}{More experiments on Meta-World}}
\textcolor{black}{To evaluate the effectiveness of PSEC on more complex experiments, we conduct experiments on Meta-World benchmark~\citep{yu2020meta}, which consists of 50 diverse tasks for robotic manipulation, such as grasping, manipulating objects, opening/closing a window, pushing buttons, locking/unlocking a door, and throwing a basketball. We compare PSEC with the strong baseline L2M~\citep{schmied2024learning}. Next, we will elaborate on the three experiment settings in our paper.}

\begin{figure}[h]
    \centering
        \centering
        \includegraphics[width=0.99\textwidth]{figs/metaworld-task-fig.pdf}
        % \label{fig:task_descri}
        \vspace{-10pt}
    \caption{\small \textcolor{black}{Visualization of the simulation environments and representative tasks of Meta-World.}}
    \label{fig:mw-task_descri}
    % \vspace{-10pt}
\end{figure}


\subsection{\textcolor{black}{Continual Learning Setting}}
\textcolor{black}{Following Continual world~\citep{wolczyk2021continual} and L2M~\citep{schmied2024learning}, we split the 50 tasks into 40 pre-training tasks and 10 fine-tuning unseen tasks (CW10). The training datasets are the same as the datasets collected by L2M. We train 10K steps per task in CW10, which is only 10\% training steps of L2M, with a batch size of 1024. After every 10K update steps, we switch to the next task in the sequence. 
% After every 5K update steps, we train the context-aware modular with 1K steps to compose the different skills.
Then we evaluate it on all tasks in the task sequence. The results are shown in Table~\ref{tab:success_rate} and Table~\ref{tab:psec_tasks}. We compare the performance of PSEC with L2M and other strong baselines. Thanks to the efficiency of skill composition in parameter space, PSEC can substantially outperform all L2M variants in a large margin, demonstrating that PSEC can achieve better performance on complex tasks.}


\begin{figure*}[ht]
\vskip 0.2in
\centering
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-easysparse-v0.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-easymean-v0.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-easydense-v0.pdf}
    \end{minipage}%
}%

% \vspace{-.2in}
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-mediumsparse-v0.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-mediummean-v0.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-mediumdense-v0.pdf}
    \end{minipage}%
}%


% \vspace{-.2in}
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-hardsparse-v0.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-hardmean-v0.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/OfflineMetadrive-harddense-v0.pdf}
    \end{minipage}%
}%

\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/SafetyAntRun.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/SafetyCarRun.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/SafetyBallRun.pdf}
    \end{minipage}%
}%

% \vspace{-.2in}
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/SafetyDroneRun.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/SafetyAntCircle.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/SafetyCarCircle.pdf}
    \end{minipage}%
}%

% \vspace{-.2in}
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/SafetyBallCircle.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{metadrive/SafetyDroneCircle.pdf}
    \end{minipage}%
}%

\centering
\caption{\textcolor{black}{Illustration of the cost-reward plot for datasets from MetaDrive and Bullet-Safety-Gym.}}
\vskip -0.2in
\label{fig_dataset_vis}
\end{figure*}



\begin{table}[h]
\centering
\begin{minipage}[t]{0.45\textwidth} % 左侧表格，占页面宽度的45%
\centering
\caption{\textcolor{black}{Success rates of different methods.}}
\label{tab:success_rate}
\vspace{0.6cm}
\setlength{\tabcolsep}{15pt}
\centering
\begin{tabular}{l|c}
% \begin{tabular}{@{}lc@{}}
\toprule
\textbf{Methods}    & \textbf{Success Rate} \\ \midrule
L2M                & 0.65                  \\
L2M-oracle         & 0.77                  \\
L2P-Pv2            & 0.40                  \\
L2P-PreT           & 0.34                  \\
L2P-PT             & 0.23                  \\
EWC                & 0.17                  \\
L2                 & 0.10                  \\
PSEC (Ours)            & \textbf{0.87}         \\ \bottomrule
\end{tabular}
\end{minipage}%
\hfill
\begin{minipage}[t]{0.50\textwidth} % 右侧表格，占页面宽度的45%
\centering
\caption{\textcolor{black}{Performance of PSEC on different tasks.}}
\label{tab:psec_tasks}
\setlength{\tabcolsep}{15pt}
\centering
\begin{tabular}{l|c}
% \begin{tabular}{@{}lc@{}}
\toprule
\textbf{Tasks}             & \textbf{PSEC} \\ \midrule
peg-unplug-side-v2         & 0.87          \\
window-close-v2            & 0.88          \\
shelf-place-v2             & 0.85          \\
push-v2                    & 0.89          \\
handle-press-side-v2       & 0.95          \\
stick-pull-v2              & 0.74          \\
push-back-v2               & 0.85          \\
faucet-close-v2            & 0.92          \\
push-wall-v2               & 0.86          \\
hammer-v2                  & 0.91          \\ \midrule
\textbf{Mean}              & \textbf{0.87} \\ \bottomrule
\end{tabular}
\end{minipage}
\end{table}


\begin{figure*}[h]
\vspace{-10pt}
\centering
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{DMC/reward_kde_curves_Stand.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{DMC/reward_kde_curves_Walk.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{DMC/reward_kde_curves_Run.pdf}
    \end{minipage}%
}%

\centering
\caption{\small \textcolor{black}{We evaluate the final running policies of PSEC, NSEC and ASEC with the “stand,” “walk,” and “run” rewards with 10 episodes and 3 random seeds. Then we plot the reward distribution by kernel density estimation (KDE). Each curve represents the probability density of rewards obtained for a specific reward. The results show that PSEC achieves high rewards across all tasks, whereas NSEC and ASEC cannot,  demonstrating that the PSEC's running skill retains behaviors from walking and standing and suggesting superior skill sharing of PSEC compared to NSEC and ASEC.}}
\vspace{-10pt}
\label{fig_reward_distri}
\end{figure*}
\subsection{\textcolor{black}{Unseen Tasks Setting}}
\textcolor{black}{To further evaluate the efficiency of PSEC on more challenging tasks, we pretrain on fewer (18) tasks and evaluate it on more (12) unseen tasks than the first setting. Firstly, we pretrain and finetune 18 tasks to obtain 18 LoRA modules. The performance on the 18 pretrained tasks is reported in Table~\ref{tab:task_performance}. We compare the performance of PSEC with Scratch, ASEC and NSEC methods. The results show that PSEC can achieve enhanced skill learning even when the pretrained model is combined with one LoRA for each task if the skill is composed in parameter space.} % \subsection{\textcolor{black}{Unseen tasks Setting}}
\textcolor{black}{Then, we evaluate PSEC with the obtained 18 LoRA modules on the unseen tasks. For the unseen tasks, we conduct two types of experiments: few-shot setting and zero-shot setting. }

\textcolor{black}{\textbf{Few-shot.}} \textcolor{black}{We perform few-shot learning by training the context-aware modular for 1k steps using only 10\% of the total available data for unseen tasks. This setup simulates scenarios with limited data on new tasks. The results, summarized in Table~\ref{tab:task_performance_fewshot}, demonstrate that PSEC achieves a high success rate on unseen tasks. This indicates that PSEC can effectively adapt to new tasks, showcasing its capability for rapid transfer learning and efficient adaptation in data-scarce environments.}

\textcolor{black}{\textbf{Zero-shot.}} \textcolor{black}{No data from the unseen tasks is used to train the context-aware modular. Instead, the modular is trained for 2k steps using datasets from 18 pre-trained tasks. It is then evaluated directly on 12 unseen tasks, utilizing 4 seeds and 10 episodes per task. The results are shown in Table~\ref{tab:task_performance_zero_shot}. Interestingly, even without access to unseen task data during training, PSEC demonstrates strong performance on several tasks. Notably, PSEC substantially outperforms NSEC and ASEC on this zero-shot transfer setting, highlighting the advantages of skill compositions in parameter spaces over noise and action spaces. Overall, the results demonstrate PSEC’s ability to effectively utilize knowledge from previously learned skills to achieve strong zero-shot transfer.} 
% This outcome highlights PSEC’s ability to effectively utilize knowledge from previously learned skills through its context-aware module. The results show that PSEC has some potential in zero-shot scenarios, enabling better performance in zero-shot scenarios}





\begin{table}[ht]
\centering
\caption{\textcolor{black}{Performance comparison on 18 pretrained tasks.}}
\label{tab:task_performance}
\setlength{\tabcolsep}{15pt}
\centering
\begin{tabular}{lccc|c}
% \begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Tasks}                  & \textbf{Scratch} & \textbf{ASEC} & \textbf{NSEC} & \textbf{PSEC} \\ \midrule
peg-insert-side-v2              & 0.50             & 0.87          & 0.88          & \textbf{0.90}          \\
peg-unplug-side-v2              & 0.35             & 0.61          & 0.78          & \textbf{0.86}          \\
button-press-topdown-v2         & 0.71             & 0.88          & 0.88          & \textbf{0.89}          \\
push-back-v2                    & 0.26             & 0.61          & 0.76          & \textbf{0.88}          \\
window-close-v2                 & 0.65             & 0.84          & 0.84          & \textbf{0.88}          \\
door-open-v2                    & 0.74             & 0.85          & \textbf{0.86}          & \textbf{0.86}          \\
handle-press-v2                 & 0.67             & 0.96          & \textbf{0.97}          & \textbf{0.97}          \\
plate-slide-side-v2             & 0.27             & 0.23          & 0.53          & \textbf{0.74}          \\
handle-pull-side-v2             & 0.76             & 0.94          & 0.94          & \textbf{0.95}          \\
window-open-v2                  & 0.87             & 0.75          & 0.88          & \textbf{0.89}          \\
door-close-v2                   & 0.90             & 0.89          & 0.89          & \textbf{0.91}          \\
reach-v2                        & 0.89             & \textbf{0.95}          & \textbf{0.95}          & \textbf{0.95}          \\
push-v2                         & 0.15             & 0.58          & 0.81          & \textbf{0.92}          \\
stick-push-v2                   & 0.44             & 0.54          & 0.17          & \textbf{0.79}          \\
drawer-close-v2                 & \textbf{0.97}             & \textbf{0.97}          & \textbf{0.97}          & \textbf{0.97}          \\
plate-slide-back-v2             & 0.90             & 0.94          & 0.94          & \textbf{0.95}          \\
coffee-button-v2                & 0.91             & 0.94          & 0.94          & \textbf{0.95}          \\
hand-insert-v2                  & 0.30             & 0.68          & 0.63          & \textbf{0.89}          \\ \midrule
\textbf{Mean}                   &  0.62                &  0.78             &  0.81             &  \textbf{0.90}             \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{\textcolor{black}{Few-shot performance comparison on 12 unseen tasks.}}
\label{tab:task_performance_fewshot}
\setlength{\tabcolsep}{15pt}
\centering
\begin{tabular}{lcc|c}
\toprule
\textbf{Tasks}                  & \textbf{ASEC} & \textbf{NSEC} & \textbf{PSEC} \\ \midrule
plate-slide-v2                   & 0.14          & 0.66          & \textbf{0.89}          \\
handle-press-side-v2             & 0.73          & 0.65          & \textbf{0.92}          \\
button-press-wall-v2             & 0.09          & 0.03          & \textbf{0.72}         \\
button-press-topdown-wall-v2     & 0.87          & 0.88          & \textbf{0.89}          \\
push-wall-v2                     & 0.57          & 0.68          & \textbf{0.88}          \\
reach-wall-v2                    & 0.41          & 0.36          & \textbf{0.90}          \\
faucet-close-v2                  & 0.41          & 0.49          & \textbf{0.90}          \\
button-press-v2                  & 0.02          & 0.14          & \textbf{0.23}          \\
plate-slide-back-side-v2         & 0.17          & 0.19          & \textbf{0.92}          \\
handle-pull-v2                   & 0.15          & 0.21          & \textbf{0.93}          \\
faucet-open-v2                   & 0.14          & 0.16          & \textbf{0.89}          \\
stick-pull-v2                    & 0.00          & 0.00          & \textbf{0.32}          \\ \bottomrule
\end{tabular}
\end{table}



% \begin{table}[ht]
% \centering
% \caption{Success rates of different methods.}
% \label{tab:success_rate}
% \begin{tabular}{@{}lc@{}}
% \toprule
% \textbf{Methods}    & \textbf{Success Rate} \\ \midrule
% L2M                & 0.65                  \\
% L2M-oracle         & 0.77                  \\
% L2P-Pv2            & 0.40                  \\
% L2P-PreT           & 0.34                  \\
% L2P-PT             & 0.23                  \\
% EWC                & 0.17                  \\
% L2                 & 0.10                  \\
% PSEC               & \textbf{0.85}         \\ \bottomrule
% \end{tabular}
% \end{table}

% \begin{table}[ht]
% \centering
% \caption{Performance of PSEC on different tasks.}
% \label{tab:psec_tasks}
% \begin{tabular}{@{}lc@{}}
% \toprule
% \textbf{Tasks}             & \textbf{PSEC} \\ \midrule
% peg-unplug-side-v2         & 0.87          \\
% window-close-v2            & 0.88          \\
% shelf-place-v2             & 0.85          \\
% push-v2                    & 0.89          \\
% handle-press-side-v2       & 0.95          \\
% stick-pull-v2              & 0.74          \\
% push-back-v2               & 0.85          \\
% faucet-close-v2            & 0.92          \\
% push-wall-v2               & 0.86          \\
% hammer-v2                  & 0.91          \\ \midrule
% \textbf{Mean}              & \textbf{0.85} \\ \bottomrule
% \end{tabular}
% \end{table}



\section{\textcolor{black}{More visualization of advantages of PSEC over NSEC and ASEC}}

\textcolor{black}{To test whether the newly learned skills effectively utilize the shared knowledge of previous skills, we evaluate the running policy obtained through context-aware modular combined with standing and walking skills on three rewards: stand, walk, and run. If the running skill can still get a relatively high stand or walk reward, this represents the final combined running skill retaining these previous skills. We compare PSEC with other composition methods ASEC and NSEC. For each method, we
rollout 10K steps and record the three rewards. The summarized rewards can be found in Figure~\ref{fig_reward_distri}. 
% save the trajectory of the run policy as it was evaluated on each task, and each task evaluated 10 episodes, each of length 1000, for a total of 10k transitions on each task. 
% Then we add the visualization of the reward distribution on stand, walk and run tasks. 
% Specifically, we summarize the run/walk/stand reward distributions for the trajectories rollouted by PSEC, NSEC, and ASEC.  
% as shown in Figure~\ref{fig_reward_distri}.
% This figure can inspect whether PSEC’s final combined running skill retains behaviors from its component skills including standing and walking. 
The results show that PSEC achieves high rewards across all tasks, whereas NSEC and ASEC cannot,  demonstrating that the PSEC's running skill retains behaviors from walking and standing and suggesting superior skill sharing of PSEC compared to NSEC and ASEC.}
% \begin{figure*}[h]
% \vskip 0.2in
% \centering
% \subfloat{
%     \begin{minipage}[t]{0.33\linewidth}
%         \centering
%         \includegraphics[width=1.7in]{DMC/reward_kde_curves_Stand.pdf}
%     \end{minipage}%
% }%
% \subfloat{
%     \begin{minipage}[t]{0.33\linewidth}
%         \centering
%         \includegraphics[width=1.7in]{DMC/reward_kde_curves_Walk.pdf}
%     \end{minipage}%
% }%
% \subfloat{
%     \begin{minipage}[t]{0.33\linewidth}
%         \centering
%         \includegraphics[width=1.7in]{DMC/reward_kde_curves_Run.pdf}
%     \end{minipage}%
% }%

% \centering
% \caption{\small \textcolor{black}{We evaluate the final running policies of PSEC, NSEC and ASEC with the “stand,” “walk,” and “run” rewards with 10 episodes and 3 random seeds. Then we plot the reward distribution by kernel density estimation (KDE). Each curve represents the probability density of rewards obtained for a specific reward. The results show that PSEC achieves high rewards across all tasks, whereas NSEC and ASEC cannot,  demonstrating that the PSEC's running skill retains behaviors from walking and standing and suggesting superior skill sharing of PSEC compared to NSEC and ASEC.}}
% \vskip -0.2in
% \label{fig_reward_distri}
% \end{figure*}
\begin{table}[ht]
% \vspace{-20pt}
\centering
\caption{\textcolor{black}{Zero-shot performance comparison on 12 unseen tasks.}}
% \vspace{-10pt}
\label{tab:task_performance_zero_shot}
\setlength{\tabcolsep}{15pt}
\centering
\begin{tabular}{lcc|c}
\toprule
\textbf{Tasks}                  & \textbf{ASEC} & \textbf{NSEC} & \textbf{PSEC} \\ \midrule
plate-slide-v2                   & 0.03          & 0.00          & \textbf{0.15}          \\
handle-press-side-v2             & 0.50          & 0.60          & \textbf{0.62}          \\
button-press-wall-v2             & 0.00          & 0.00          & \textbf{0.40}          \\
button-press-topdown-wall-v2     & 0.85          & 0.87          & \textbf{0.89}          \\
push-wall-v2                     & 0.53          & 0.53          & \textbf{0.71}          \\
reach-wall-v2                    & 0.34          & 0.05          & \textbf{0.90}          \\
faucet-close-v2                  & 0.00          & 0.00          & \textbf{0.16}          \\
button-press-v2                  & 0.00          & 0.00          & \textbf{0.15}          \\
plate-slide-back-side-v2         & 0.00          & 0.00          & 0.00          \\
handle-pull-v2                   & 0.00          & 0.00          & 0.00          \\
faucet-open-v2                   & 0.00          & 0.00          & \textbf{0.77}          \\
stick-pull-v2                    & 0.00          & 0.00          & 0.00          \\ \bottomrule
\end{tabular}
% \vspace{-20pt}
\end{table}

\begin{table*}[ht]
% \vspace{-20pt}
    \renewcommand{\arraystretch}{1.3} 
    \caption{Results in the dynamics shift setting over 10 episodes and 5 seeds. -m, -mr and -me refer to $\mathcal{D}_o^{\mathcal{P}_1}$ sampling from medium, medium-replay and medium-expert V2 data in D4RL~\citep{fu2020d4rl}, respectively. }
    % \vspace{-10pt}
    \label{tab:performance_transposed}
    \centering
    \small
    \setlength{\tabcolsep}{1.5pt}
    \begin{tabular}{lcccccc}
    \hline
    \toprule
    Metric & Halfcheetah-m & Halfcheetah-mr & Halfcheetah-me & Walker2d-m & Walker2d-mr & Walker2d-me \\
    \midrule
    % Size & 10k & 10k & 10k & 10k & 10k & 10k \\
    BC & 26.4 $\pm$ 7.3 & 14.3 $\pm$ 7.8 & 19.1 $\pm$ 9.4 & 15.8 $\pm$ 14.1 & 1.4 $\pm$ 1.9 & 21.7 $\pm$ 8.2 \\
    MOPO & -1.1 $\pm$ 4.1 & 11.7 $\pm$ 5.2 & -1.1 $\pm$ 1.4 & 3.1 $\pm$ 4.7 & 3.3 $\pm$ 2.7 & 0.1 $\pm$ 0.3 \\
    CQL & 35.4 $\pm$ 3.8 & 8.1 $\pm$ 9.4 & 26.5 $\pm$ 10.8 & 18.8 $\pm$ 18.8 & 8.5 $\pm$ 2.19 & 19.1 $\pm$ 14.4 \\
    IQL & 29.9 $\pm$ 0.2 & 22.7 $\pm$ 6.4 & 10.5 $\pm$ 8.8 & 22.5 $\pm$ 3.8 & 10.7 $\pm$ 11.9 & 26.5 $\pm$ 8.6 \\
    DOGE & \textbf{42.6 $\pm$ 3.4} & 23.4 $\pm$ 3.6 & 26.7 $\pm$ 6.6 & 45.1 $\pm$ 10.2 & 13.5 $\pm$ 8.4 & 35.3 $\pm$ 4.1 \\
    TSRL & 38.4 $\pm$ 3.1 & 28.1 $\pm$ 3.5 & 39.9 $\pm$ 21.1 & 49.7 $\pm$ 10.6 & 26.0 $\pm$ 11.3 & 46.4 $\pm$ 13.2 \\
    Joint train(Gravity) & 2.0 $\pm$ 1.4 & 6.8 $\pm$ 3.9 & 6.8 $\pm$ 5.4 & 39.4 $\pm$ 3.4 & 15.7 $\pm$ 7.7 & 33.5 $\pm$ 10.5 \\
    Joint train(Friction) & 15.8 $\pm$ 1.0 & 14.9 $\pm$ 1.2 & 16.5 $\pm$ 1.1 & 8.3 $\pm$ 1.1 & 7.6 $\pm$ 0.8 & 7.4 $\pm$ 0.5 \\
    Joint train(Thigh) & 9.5 $\pm$ 5.3 & 9.8 $\pm$ 8.5 & 6.4 $\pm$ 1.3 & 50.6 $\pm$ 8.8 & 6.3 $\pm$ 3.0 & 54.9 $\pm$ 14.8 \\
    \midrule
    \textbf{Dynamic shift} \\
    \midrule
    PSEC(Gravity) & 40.8 $\pm$ 0.9 & 29.2 $\pm$ 1.1 & 42.4 $\pm$ 1.0 & 57.2 $\pm$ 4.5 & \textbf{26.8 $\pm$ 5.2} & 71.8 $\pm$ 8.0 \\
    % \midrule
    % \textbf{Body mass shift} \\
    % \midrule
    PSEC(Friction) & 40.1 $\pm$ 1.2 & 31.1 $\pm$ 1.3 & 42.1 $\pm$ 1.0 & 61.7 $\pm$ 7.5 & 20.9 $\pm$ 4.6 & \textbf{75.0 $\pm$ 12.1} \\
    \midrule
    \textbf{Body shift} \\
    \midrule
    PSEC(Thigh) & \textbf{41.4 $\pm$ 0.3} & \textbf{32.3 $\pm$ 1.4} & \textbf{43.9 $\pm$ 2.5} & \textbf{64.96 $\pm$ 4.5} & 25.5 $\pm$ 4.5 & 71.4 $\pm$ 14.3 \\
    \bottomrule
    \hline
    \end{tabular}
    % \vspace{-20pt}
\end{table*}
% \newpage


% the case of a single-policy method or a fixed-composition method and compare it to PSEC. Specifically, the context-aware modular is used in a single-policy method or a fixed-composition method only as a detector to tell us how we should adjust the weights of the different policies at this point, but in practice these two approaches do not use the context-aware modular, which is one of the reasons for their failure. The results are shown in Figure~\ref{fig:ws_trend}. The results show that in some cases it is necessary to adjust the weights of the different strategies to fit the current states.


% \section{Appendix}
% You may include other additional sections here.




% \begin{figure}[t]
%     \centering
%     \begin{subfigure}[b]{0.33\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{dynamics/halfcheetah-medium (Friction).pdf}
%         % \caption{Only run on Walker-run task}
%         \label{fig:adaptive_weight}
%     \end{subfigure}
%     \hspace{1cm}
%     \begin{subfigure}[b]{0.33\linewidth}
%         \centering
%         \includegraphics[width=\textwidth]{dynamics/halfcheetah-medium (Gravity).pdf}
%         % \caption{Fixed stand and run on Walker-run task}
%         \label{fig:adaptive_weight}
%     \end{subfigure}
%     \hspace{1cm}
%     \begin{subfigure}[b]{0.33\linewidth}
%         \centering\includegraphics[width=\textwidth]{dynamics/halfcheetah-medium (Thigh Size).pdf}
%         % \caption{PSEC on Walker-run task}
%         \label{fig:adaptive_weight}
%     \end{subfigure}
%     \caption{Illustrative figure for context-aware trend on DeepMind Control}
%     \label{fig:ws_trend}
% \end{figure}
\begin{figure*}[h]
\vskip 0.2in
\centering
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium_F.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium_G.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium_T.pdf}
    \end{minipage}%
}%

\vspace{-.2in}
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium-replay_F.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium-replay_G.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium-replay_T.pdf}
    \end{minipage}%
}%

\vspace{-.2in}
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium-expert_F.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium-expert_G.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/halfcheetah-medium-expert_T.pdf}
    \end{minipage}%
}%

\vspace{-.2in}
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium_F.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium_G.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium_T.pdf}
    \end{minipage}%
}%

\vspace{-.2in}
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium-replay_F.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium-replay_G.pdf}
    \end{minipage}%
}%
\subfloat[]{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium-replay_T.pdf}
    \end{minipage}%
}%

\vspace{-.2in}
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium-expert_F.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium-expert_G.pdf}
    \end{minipage}%
}%
\subfloat{
    \begin{minipage}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=1.7in]{dynamics/walker2d-medium-expert_T.pdf}
    \end{minipage}%
}%

\centering
\caption{Results of performance conducted on dynamic shift and body shift tasks. The lines and shaded areas indicate the averages and standard deviations calculated over 5 random seeds.}
\vskip -0.2in
\label{fig_dmc_results}
\end{figure*}


\begin{table}[ht]

\caption{Hyperparameters for multi-objective composition tasks.}
\vskip 0.10in
\label{tab:hyperparameters_mulob}
\small
% \renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{15pt}
\centering
\begin{tabular}{c|l|c}
\hline
\toprule
& \textbf{Hyper-parameters} & \textbf{Value} \\
\midrule
\multirow{8}{*}{\shortstack{shared \\ hyperparameters}}
& Normalized state & True \\
& Target update rate & 1e-3 \\
& Expectile $\tau$  & 0.9 \\
& {Discount $\gamma$} & 0.99 \\
& Actor learning rate &  3e-4 \\
& {Critic learning rate} & 3e-4 \\
& Number of added Gaussian noise $T$ & 5\\
% & &1e-4 for AntMaze \\

\hline
\multirow{5}{*}{\textbf{$\pi_0$}} 
& hidden dim & 256  \\
& hidden layers & 2  \\
& activation function & ReLU \\
& Mini-batch size & 2048 \\
& Optimizer & Adam~\citep{kingma2014adam} \\
& Training steps & 1e6 \\

\hline
\multirow{11}{*}{\textbf{$\pi_1$}} 
& $Q_r^*(s,a)$ hidden dim & 256\\
& $Q_r^*(s,a)$ hidden layers & 2  \\
& $Q_r^*(s,a)$ activation function & ReLU \\
& $V_r^*(s)$ hidden dim & 256  \\
& $V_r^*(s)$ hidden layers & 2  \\
& $V_r^*(s)$ activation function & ReLU \\
& Actor hidden dim & 256  \\
& Actor hidden layers & 2  \\
& Actor Activation function & ReLU \\
& Mini-batch size & 2048 \\
& Optimizer & Adam \\
& Training steps & 5e4 \\

\hline
\multirow{11}{*}{\textbf{$\pi_2$}} 
& $Q_h^*(s,a)$ hidden dim & 256\\
& $Q_h^*(s,a)$ hidden layers & 2  \\
& $Q_h^*(s,a)$ activation function & ReLU \\
& $V_h^*(s)$ hidden dim & 256  \\
& $V_h^*(s)$ hidden layers & 2  \\
& $V_h^*(s)$ Activation function & ReLU \\
& Actor hidden dim & 256  \\
& Actor hidden layers & 2  \\
& Actor Activation function & ReLU \\
& Mini-batch size & 2048 \\
& Optimizer & Adam \\
& Training steps & 5e4 \\

\hline
\multirow{5}{*}{\textbf{$\alpha(s;\theta)$}} 
& hidden dim & 256  \\
& hidden layers & 2  \\
& activation function & ReLU \\
& Mini-batch size & 2048 \\
& Optimizer & Adam \\
& Training steps & 1e3 \\

\hline
\multirow{1}{*}{LoRA} 
& rank $n$ & 8, 16 \\
% & {$\alpha$ for Mujoco} & 40.0, 2.5\\ 
% & {$\epsilon_A$ } & 0\\ 
% & &\{2.5, 7.5, 20.0\} for AntMaze \\
		\bottomrule
\hline
\end{tabular}

\end{table}


\begin{table}[h]

\caption{Hyperparameters for continual policy shift.}
\vskip 0.10in
\label{tab:hyperparameters_contipolicy}
\small
% \renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{15pt}
\centering
\begin{tabular}{c|l|c}
\hline
\toprule
& \textbf{Hyper-parameters} & \textbf{Value} \\
\midrule
\multirow{8}{*}{\shortstack{shared \\ hyperparameters}}
& Normalized state & True \\
& Target update rate & 1e-3 \\
& Expectile $\tau$  & 0.9 \\
& {Discount $\gamma$} & 0.99 \\
& Actor learning rate &  3e-4 \\
& {Critic learning rate} & 3e-4 \\
& Number of added Gaussian noise $T$ & 5\\
% & &1e-4 for AntMaze \\

\hline
\multirow{5}{*}{\textbf{$\pi_0$}} 
& hidden dim & 256  \\
& hidden layers & 2  \\
& activation function & ReLU \\
& Mini-batch size & 1024 \\
& Optimizer & Adam \\
& Training steps & 1e6 \\

\hline
\multirow{5}{*}{\textbf{$\pi_1$}} 
& hidden dim & 256  \\
& hidden layers & 2  \\
& Activation function & ReLU \\
& Mini-batch size & 1024 \\
& Optimizer & Adam \\
& Training steps & 1e4 \\

\hline
\multirow{5}{*}{\textbf{$\pi_2$}} 
& Actor hidden dim & 256  \\
& Actor hidden layers & 2  \\
& Actor Activation function & ReLU \\
& Mini-batch size & 1024 \\
& Optimizer & Adam\\
& Training steps & 1e4 \\

\hline
\multirow{5}{*}{\textbf{$\alpha(s;\theta)$}} 
& hidden dim & 256  \\
& hidden layers & 2  \\
& activation function & ReLU \\
& Mini-batch size & 1024 \\
& Optimizer & Adam \\
& Training steps & 1e3 \\

\hline
\multirow{1}{*}{LoRA} 
& rank $n$ & 8 \\
% & {$\alpha$ for Mujoco} & 40.0, 2.5\\ 
% & {$\epsilon_A$ } & 0\\ 
% & &\{2.5, 7.5, 20.0\} for AntMaze \\
		\bottomrule
\hline
\end{tabular}

\end{table}

\begin{table}[t]

\caption{Hyperparameters for dynamic shift.}
\vskip 0.10in
\label{tab:hyperparameters_dyshift}
\small
% \renewcommand{\arraystretch}{1.5}
\setlength{\tabcolsep}{15pt}
\centering
\begin{tabular}{c|l|c}
\hline
\toprule
& \textbf{Hyper-parameters} & \textbf{Value} \\
\midrule
\multirow{8}{*}{\shortstack{shared \\ hyperparameters}}
& Normalized state & True \\
& Target update rate & 1e-3 \\
& Expectile $\tau$  & 0.9 \\
& {Discount $\gamma$} & 0.99 \\
& Actor learning rate &  3e-4 \\
& {Critic learning rate} & 3e-4 \\
& Number of added Gaussian noise $T$ & 5\\
% & &1e-4 for AntMaze \\

\hline
\multirow{5}{*}{\textbf{$\pi_0$}} 
& hidden dim & 256  \\
& hidden layers & 2  \\
& activation function & ReLU \\
& Mini-batch size & 1024 \\
& Optimizer & Adam \\
& Training steps & 1e6 \\

\hline
\multirow{11}{*}{\textbf{$\pi_1$}} 
& $Q_r^*(s,a)$ hidden dim & 256\\
& $Q_r^*(s,a)$ hidden layers & 2  \\
& $Q_r^*(s,a)$ activation function & ReLU \\
& $V_r^*(s)$ hidden dim & 256  \\
& $V_r^*(s)$ hidden layers & 2  \\
& $V_r^*(s)$ activation function & ReLU \\
& Actor hidden dim & 256  \\
& Actor hidden layers & 2  \\
& Actor Activation function & ReLU \\
& Mini-batch size & 1024 \\
& Optimizer & Adam \\
& Training steps & 2e4 \\

\hline
\multirow{5}{*}{\textbf{$\alpha(s;\theta)$}} 
& hidden dim & 256  \\
& hidden layers & 2  \\
& activation function & ReLU \\
& Mini-batch size & 1024 \\
& Optimizer & Adam \\
& Training steps & 1e3 \\

\hline
\multirow{1}{*}{LoRA} 
& rank $n$ & 8 \\
% & {$\alpha$ for Mujoco} & 40.0, 2.5\\ 
% & {$\epsilon_A$ } & 0\\ 
% & &\{2.5, 7.5, 20.0\} for AntMaze \\
		\bottomrule
\hline
\end{tabular}

\end{table}




% \begin{table*}[t]
%     \caption{Performance of different walking and running methods across varying trajectory lengths. The results correspond to the mean and standard deviations of scores across multiple trials.}
%     \label{walk-run-result}
%     \vskip 0.15in
%     \centering
%     \small
%     \setlength{\tabcolsep}{4pt}
%     \begin{tabular}{lccccc}
%     \hline
%     \toprule
%     \multirow{2}{*}{\bf Trajectories} & \multirow{2}{*}{\bf Direct walk} & \multirow{2}{*}{\bf Direct run} & \multicolumn{2}{c}{\bf PSEC (stand+walk)} & \multirow{2}{*}{\shortstack[c]{\bf PSEC\\(stand+run+walk) $\rightarrow$ walk}} \\
%     \cmidrule(lr){4-5} 
%      & & & \shortstack[c]{\bf w/o CA\\$\rightarrow$ walk} & \shortstack[c]{\bf $\rightarrow$ walk} & \\
%     \midrule
%     10000 & $58.9 \pm 36$ & $25.5 \pm 4$ & $539.5 \pm 98$ & $688 \pm 17$ & $\bm{781.4 \pm 93}$ \\
%     30000 & $406 \pm 146$ & $36.5 \pm 8$ & $814.6 \pm 113$ & $861.8 \pm 33$ & $\bm{868.3 \pm 22}$ \\
%     \bottomrule
%     \hline
%     \end{tabular}
% \end{table*}

% \usepackage{multirow}  % 需要在导言区加入这个宏包






% \begin{figure*}[ht]
% \vskip 0.2in
% \centering
% \subfigure{
% 		\begin{minipage}[t]{0.33\linewidth}
% 			\centering
% 			\includegraphics[width=2.2in]{DMC/fixed_trend_walker_run.pdf}
% 		\end{minipage}%
% 	}%
% \subfigure{
% 		\begin{minipage}[t]{0.33\linewidth}
% 			\centering
% 			\includegraphics[width=2.2in]{IDMC/fixed_trend_walker_run.pdf}
% 		\end{minipage}%
% 	}%
% \subfigure{
% 		\begin{minipage}[t]{0.33\linewidth}
% 			\centering
% 			\includegraphics[width=2.2in]{DMC/fixed_trend_walker_run.pdf}
% 		\end{minipage}%
% 	}%
% \vspace{-.1in}

% % \subfigure{
% % 		\begin{minipage}[t]{0.33\linewidth}
% % 			\centering
% % 			\includegraphics[width=2.2in]{datasets/hopper-medium-v2.pdf}
% % 		\end{minipage}%
% % 	}%
% % \subfigure{
% % 		\begin{minipage}[t]{0.33\linewidth}
% % 			\centering
% % 			\includegraphics[width=2.2in]{datasets/hopper-medium-replay-v2.pdf}
% % 		\end{minipage}%
% % 	}%
% % \subfigure{
% % 		\begin{minipage}[t]{0.33\linewidth}
% % 			\centering
% % 			\includegraphics[width=2.2in]{datasets/hopper-medium-expert-v2.pdf}
% % 		\end{minipage}%
% % 	}%
% %  \vspace{-.1in}

% % \subfigure{
% % 		\begin{minipage}[t]{0.33\linewidth}
% % 			\centering
% % 			\includegraphics[width=2.2in]{datasets/walker2d-medium-v2.pdf}
% % 		\end{minipage}%
% % }%
% % \subfigure{
% % 		\begin{minipage}[t]{0.33\linewidth}
% % 			\centering
% % 			\includegraphics[width=2.2in]{datasets/walker2d-medium-replay-v2.pdf}
% % 		\end{minipage}%
% % 	}%
% % \subfigure{
% % 		\begin{minipage}[t]{0.33\linewidth}
% % 			\centering
% % 			\includegraphics[width=2.2in]{datasets/walker2d-medium-expert-v2.pdf}
% % 		\end{minipage}%
% % 	}%
% \centering
% \vspace{-.1in}
% \caption{Results of performance comparisons conducted on nine original tasks in the D4RL dataset. The lines and shaded areas indicate the averages and standard deviations calculated over 5 random seeds, respectively.}
% \vskip -0.1in
% % 	\vspace{-0.5cm} 
% \label{fig_dmc_results}
% \end{figure*}

\end{document}
