\section{Related works}
\vspace{-7pt}
\label{rel_work}
% \ljx{revised till here}
% \textbf{Tabula Rasa}. 
% Tabula rasa learning is one popular paradigm for diverse existing decision-making applications, such as robotics and games~\citep{silver2017mastering, andrychowicz2020learning, berner2019dota, vinyals2019grandmaster}. It directly learns policies from scratch without the assistance of any prior knowledge. However, it suffers from notable drawbacks related to poor sample efficiency and constraints on the complexity of skills an agent can acquire~\citep{agarwal2022reincarnating}.

\textbf{Compositional Policies}.
% To tackle the problem of tabula rasa paradigm, 
Some previous methods try to leverage prior knowledge relying on pretrained primitive policies. More specifically, these methods used compositional networks in a hierarchical structure to adaptively compose primitives to form complex behaviors~\citep{peng2019mcp, qureshi2020composing, pertsch2021accelerating, merel2018neural, merel2020catch}. However, their expressiveness is limited by the expressiveness of simple Gaussian primitives. 
% The field of generative models for decision-making has seen significant progress in recent years. 
Recently, due to the strong expressiveness of the diffusion models and its inherent connection with Energy-Based Models~\citep{lecun2006tutorial}, many compositional policies have been approached by diffusion model. Diffusion models learn the gradient fields of an implicit energy function, which can be combined at inference time to generalize to new complex distribution readily~\citep{janner2022planning, wang2024poco, du2024position, liu2022compositional, luo2024potential}. However, these approaches rely on independently trained policies with fixed combination weights, which lack the flexibility to adapt to complex scenarios. Moreover, most previous methods can only combine skills after the policy distribution generation of each skill. Therefore, they fail to fully utilize the shared features of different skills to achieve optimal compositions. 
% Moreover, all previous methods assume a predefined policies libraries in advance and ignore policy expanding combined with existing policies to solve the unseen tasks, which is significantly important for the autonomous agent to self-evolution in given .
% Unlike these methods, w
We systematically investigate the advantages of skill composition 
% approach can adaptively compose skills 
within the parameter space, and compose skills in a context-aware manner with each skill modeled as a diffusion model. This ensures both flexibility and expressiveness in composing complex behaviors.
% \ljx  {revised till here}

\textbf{Continual Learning for Decision Making}.
Current continual learning methods for decision making, including continual reinforcement learning~(RL) and imitation learning (IL), primarily focus on mitigating catastrophic forgetting of prior knowledge when learning new tasks. They can be roughly classified into three categories: structure-based~\citep{smith2023continual, wang2024sparse},  regularization-based~\citep{kessler2020unclear}, and rehearsal-based methods~\citep{liu2024continual, peng2023ideal}. Different from previous continual RL and IL approaches, our study focuses on leveraging existing skills to facilitate efficient new task learning and enables the extension of skill sets. In addition, it naturally solves the catastrophic forgetting challenge due to the parameter isolation induced by the LoRA module~\citep{liu2023tail}, directly bypassing the key challenges of existing continual learning methods. 
% Therefore, our study can be considered as orthogonal to traditional continue learning methods.


% \textbf{Finetune-based Methods}. Some finetune-based methods aim to accelerate policy learning by leveraging prior knowledge. This knowledge may come from pretrained policy or offline data, such as 
% Offline-to-online RL~\citep{nair2020awac, lee2022offline, agarwal2022reincarnating} and transfer RL~\citep{barreto2018transfer, li2019hierarchical}. Some methods maintain a policy library that contains pretrained policies and adaptively selects one policy from this set to assist policy training~\citep{kim2024unsupervised, wang2024train, barreto2018transfer}. However, they are generally restricted to single-task scenarios where all policies serve the same task~\citep{zhang2023policy}, or only sequentially activate one policy in the pretrained sets, which greatly limits the expressiveness of the pretrained primitives~\citep{li2019hierarchical}. Our method, on the contrary, can both leverage multi-task knowledge to fulfill the new task, and can simultaneously activate all skills to compose more complex behaviors.

% \paragraph{Diffusion Composition methods} The field of generative models for decision-making has seen significant progress in recent years. 
% Recently, due to the strong expressiveness of the diffusion model for multimodality, there are many composition methods with the diffusion model. 

% \paragraph{Gaussian Composition methods}

% \paragraph{Continual Composition methods}
\vspace{-8pt}