We conducted a user study via Qualtrics to evaluate human trust in plans generated by the planners discussed above.~\footnote{This study was approved by IRB\#****. Study details are included in \Cref{appendix:user_study_details}.} footnote{This study was approved by IRB\#7035 and University of Texas at Austin IRB \#6873. Study details are included in \Cref{appendix:user_study_details}.} 

\subsection{Participants}
We recruited 30 participants through Prolific~\cite{palan2018prolific} (fluent English speakers over the age of 18). After informed consent, a reCAPTCHA test was administered as a bot check. To encourage engagement and accurate responses, participants were offered \emph{bonus payments} based on their evaluation accuracy, defined as correctly accepting correct plans and rejecting incorrect ones. 

The participants (80\% male, 17\% female, and 3\% preferred not to say) had an average age of $34.00$ (SD=$10.11$). Regarding prior experience with large language models (LLMs), $80\%$ of participants reported having used LLMs before, while $20\%$ had not. When asked about the frequency of using LLMs specifically for planning tasks, $33\%$ indicated that they use them frequently, $43\%$ occasionally, and $23\%$ never.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figures/procedure.pdf}
    \caption{User study procedure.}
    \label{fig:procedure}
\end{figure}

\subsection{Procedure}
% The study lasted approximately 20 minutes per participant. 
Participants began with a \emph{demo session} to familiarize themselves with the Gripper problem task and the study interface. 

The main part of the study comprised \emph{four sessions}, each corresponding to a different AI planner: \emph{Planner A (PDDL)}, \emph{Planner B (LLM)}, \emph{Planner C (LLM+Expl)}, and \emph{Planner D (LLM+Refine)}. The four sessions were presented in a randomized order to counterbalance any ordering effects. 

Each session contained \emph{two tasks}, each involving a new Gripper problem (unique initial and goal conditions) with similar difficulty (similar number of plan steps for equal number of rooms and balls). The order of tasks within each session was also randomized.
In each task, participants were first presented with a \emph{plan} generated by the planner and asked to rate their {trust} in the planner (\textbf{trust before}). Participants were then shown an \emph{intervention}, which varied depending on the session planner:
\begin{itemize}
    \item For \textbf{PDDL} and \textbf{LLM}, the intervention provided only the \textit{consequence of the plan}, e.g., \textit{``This plan is correct/wrong!''}.
    \item For \textbf{LLM+Expl}, the intervention included both the consequence of the plan and an \textit{explanation} of the outcome, e.g., \textit{``This plan is wrong because the robot misses the steps of moving ball4 from room4 to room1.''}
    \item For \textbf{LLM+Refine}, participants were first asked to \textit{choose between two lines} of the plan as a starting point for refinement. A \textit{revised plan} was then generated beginning from the selected line.
\end{itemize}
After the intervention, participants were again asked to rate their {trust} in the planner (\textbf{trust after}). 
Additionally, participants were asked to decide whether to \textbf{accept or reject} the plan before the intervention for PDDL and LLM planners but after for the other two. This allows evaluation of plan correctness prior to consequences for PDDL and LLM, while focusing on responses to interventions for the others.

At the end of the study, participants were informed of their \textit{evaluation accuracy} as the total number of correctly evaluated tasks out of 8 total tasks (2 tasks per session, 4 sessions in total).
The procedure is detailed in \Cref{fig:procedure}.

\subsection{Independent Variables}
We employed a within-subject design where each participant completes four sessions, each involving one of four planners. The PDDL solver provides 100\% correct plans, while the other three planners deliver 50\% correct plans. 
We set this accuracy to ensure non-perfect but meaningful performance across two tasks per session, approximating the observed accuracy in practice~\cite{zuo2024planetarium,hao2024planning}.
The LLM+Expl planner explains why the plans are correct or not, and the LLM+Refine planner allows participants to refine the plans.
The independent variables include correctness (comparing PDDL with LLM), explanation (comparing LLM with LLM+Expl), and refinement (comparing LLM with LLM+Refine).

\subsection{Dependent Measures}
For each session, participants evaluated two tasks. We measured user performance on \emph{evaluation accuracy} by the number of correctly evaluated tasks ($0$, $1$, or $2$). 
Participants also rated their \emph{trust} in the planner on a 7-point Likert scale (1 = strongly disagree, 7 = strongly agree) both \emph{before} and \emph{after} the intervention.
We also measured participants' \emph{propensity to trust} at the end of each session using a six-item scale~\cite{merritt2013trust} to assess their general tendency to trust AI planners. The response options were on a 5-point Likert-type scale ranging from 1 (strongly disagree) to 5 (strongly agree). In our survey, we adapted the scale by replacing the term ``machine" with ``AI planner" to reflect the context of our study better (see \Cref{appendix:propensity_to_trust_scale} for the exact scale we used). 

\subsection{Hypotheses}
We formulated the following hypotheses to examine the effects of correctness, explanations, and refinement on user performance (plan evaluation accuracy) and trust:
\begin{itemize}
    \item \textbf{H1:} Planners that are more correct increase evaluation accuracy.
    \item \textbf{H2:} Planners that provide explanations increase evaluation accuracy.
    \item \textbf{H3:} Planners that allow for plan refinement increase evaluation accuracy. 
    \item \textbf{H4:} Planners that are more correct improve user trust.
    \item \textbf{H5:} Planners that provide explanations improve user trust.
    \item \textbf{H6:} Planners that allow for plan refinement improve user trust. 
\end{itemize}