We evaluate factors influencing user trust in planners by comparing a language-model-based planner, denoted as an \emph{LLM Planner} (GPT-4o~\cite{achiam2023gpt}), with a traditional graph-search-based planner, denoted as a \emph{PDDL Solver} (Fast Downwards~\cite{fastdownward}). Unlike the PDDL Solver which relies on graph search algorithms, the LLM Planner can reason through the planning problem, explain its proposed solution, and iteratively refine the solution based on external feedback.

\subsection{Planning Problem}
A planning problem consists of a \emph{planning domain} (aspects of a problem that remains consistent, i.e. objects, predicates, actions) and a \emph{problem description} (particular instance of a planning task, i.e. initial state, goal state), expressed in PDDL. We present an example of the gripper problem in \Cref{app: grippers}.

We select the \emph{gripper} planning problems from the International Planning Competition~\cite{IPC} for plan generation and evaluation. In a gripper planning problem, a robot moves balls between a set of rooms using two grippers. The objective is to create a \emph{plan}---a sequence of actions---for the robot to move the balls to the defined target rooms. We present a few running examples of the gripper problem in \Cref{app: llm-planner} (\Cref{fig: correctness}).

\begin{figure}[t]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/correct.jpeg}
    \caption{Examples where LLM Planner correctly generates a plan for the gripper planning problem.}
    \Description{Planning Problem Correctness}
    \label{fig: correct}
\end{figure}

\subsection{PDDL Solver}
The PDDL Solver takes the planning domain and the problem description as inputs and then generates a plan (a sequence of actions with specific input parameters) described in PDDL. 
Next, we convert the generated plan into natural language for user studies following the procedure in~\cite{seipp-et-al-zenodo2022} and display it to users. We present an example in Figure \ref{fig: correct}.
The planner either generates a \emph{correct} plan defined as the shortest path between the initial and goal states or returns a signal indicating that no solution exists for the given problem.

\subsection{LLM Planner}
The LLM Planner addresses planning problems by querying a large language model, using a structured prompt format. The planner then retrieves a natural language plan from the language model. To ensure the output adheres to the desired format, we include a few in-context examples within the prompts. We present an example of this in Appendix \ref{app: llm-planner}.
Unlike the PDDL Solver, the LLM Planner may generate \emph{incorrect} plans that violate the problem specifications (e.g., preconditions of actions) or fail to achieve the goal, as language models may struggle with large state spaces compared to classical planners.

\paragraph{LLM Planner with Explanation (LLM+Expl)}
% Since LLMs exhibit explanation capabilities, we manually provide a natural language explanation to replicate it. 
To examine the influence of explanation on user trust, we create a natural language explanation of each generated plan. The trust improvement by adding explanations will motivate training an LLM to explain its plan.
This explanation includes an assessment of the planâ€™s correctness, identification of any violations of action preconditions, and an analysis of inconsistencies between the final state achieved and the intended goal state.
If a plan is correct, the explanation is ``the plan successfully satisfies the goal conditions.'' 
If a plan is incorrect, we identify the underlying cause as either a violation of action preconditions or a failure to achieve the goal state. In cases involving precondition violations, we specify the action responsible for the issue. 

For example, consider the action ``robot moves from room 1 to room 2,'' but the robot is initially located in room 3. This scenario constitutes a violation of the precondition for the ``move'' action. In the latter case, we describe the differences between the final state achieved and the intended goal state, e.g., ``fail to move ball 2 to room 2.''
This function enables the user to better understand why actions are chosen and their effect on the overall plan.
We present examples of explanations in \Cref{app: llm-planner} (\Cref{fig: explain}).

\paragraph{LLM Planner with Refinement (LLM+Refine)}
Refining an LLM-generated plan is also possible. So, we offer a prompting mechanism for the LLM Planner to refine the generated plan according to the user feedback. We present a sample user interface on the left of \Cref{fig: refine} in \Cref{app: llm-planner}. The mechanism works as follows:
First, request the user to indicate the step number where refinement should begin.
Second, send the planning domain, problem description, and the original plan to the language model. Next, query the model to rewrite the subsequent steps starting from the user-specified step number.
Finally, replace the original plan with the newly refined plan and display it to the user.
This mechanism enables the user to focus on a subset of steps, facilitating a deeper interpretation of those actions. However, the correctness of the refined plan is still not guaranteed.
