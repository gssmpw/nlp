% introduce PDDL domains
% why Gripper env as testing context
% motivation: comparing classical vs LLM planners
% - classical: PDDL solver fast-downward
% - LLM: gpt-4o
% explanation and refinement are two distinguishing features of LLM planners
% - how we demonstrate explanation and refinement in the study
We evaluate user trust in two planners over a set of planning problems and study the potential factors influencing user trust in the planners. In particular, we compare a language-model-based planner, denoted as an \emph{LLM Planner}, with a traditional graph-search-based planner, denoted as a \emph{PDDL Solver}. The PDDL Solver uses Fast Downwards \cite{fastdownward} as its underlying model, processing planning problems described in PDDL to generate an optimal solution. In comparison, the LLM Planner employs GPT-4o to interpret the planning problem and extract a solution generated by the language model. Unlike the PDDL Solver, the LLM Planner can reason through the planning problem, explain its proposed solution, and iteratively refine the solution based on external feedback. This study investigates how the correctness of solutions, the quality of explanations, and the refinement process influence user trust.

\subsection{Planning Problem}
% \begin{wrapfigure}{r}{0.4\textwidth}
% % \begin{figure}[t]
%     \centering
%     \includegraphics[width=\linewidth]{figures/problem-example.pdf}
%     \caption{A running example of a planning problem in our study.}
%     \Description{Planning Problem Example}
%     \label{fig: problem-example}
% % \end{figure}
% \end{wrapfigure}

We describe each planning problem in the \emph{Planning Domain Definition Language (PDDL)} and propose two planners to generate plans that solve the problem. We select the \emph{gripper} planning problems from the International Planning Competition \cite{IPC} for plan generation and evaluation. In a gripper planning problem, a robot moves balls between a set of rooms using two grippers. The objective is to create a plan for the robot to move the balls to the target rooms we defined. We present a few running examples of the gripper problem in Figure \ref{fig: correctness}.

A planning problem consists of a \emph{planning domain} and a \emph{problem description}, expressed in PDDL. 

\paragraph{Planning Domain}
A planning domain refers to the universal aspects of a problem that remains consistent across different instances of the problem. In particular, it defines the types of objects, predicates, and actions that exist in the planning problem. We present an example of the gripper problem in Appendix \ref{app: grippers}.

\paragraph{Problem Description} A problem description specifies the particular instance of a planning task within a given domain. It includes the planning domain to which it pertains, a set of objects, the initial state of these objects, and the goal state to be achieved.

\paragraph{Plan}
A plan is a sequence of actions with specific input parameters. Recall that an action corresponds to a state transition. If a plan (a sequence of actions) transits from the initial state to the goal state defined by a problem, then we consider the plan to be \emph{correct}. If a plan does not transit to the goal state or there exists an action violating its precondition, then the plan is \emph{wrong}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/correct.jpeg}
    \caption{Examples where LLM Planner correctly generates a plan for the gripper planning problem.}
    \Description{Planning Problem Correctness}
    \label{fig: correct}
\end{figure}

\subsection{PDDL Solver}
The PDDL Solver takes the planning domain and the problem description as inputs and then generates a plan described in PDDL. 
% It generates a plan in the following format:
% \vspace{4pt}
% \begin{lstlisting}[language=completion]
% (move robot1 room1 room3)
% (pick robot1 ball2 room3 rgripper1)
% (move robot1 room3 room2) ......
% \end{lstlisting}
Next, we convert the generated plan into natural language for user studies following the procedure in \cite{seipp-et-al-zenodo2022} and display it to users. We present an example in Figure \ref{fig: correct}.

The PDDL Solver applies a graph search algorithm to find a path (i.e., a list of transitions) from the initial state to the goal state. It either generates a \emph{correct} plan---defined as the shortest path between the initial and goal states---or returns a signal indicating that no solution exists for the given problem.

\subsection{LLM Planner}

The LLM Planner addresses planning problems by querying a large language model. In particular, it transmits the planning domain and problem description to the language model using a structured prompt format. The planner then retrieves a natural language plan from the language model. We use GPT-4o as the language model for the planner. To ensure the output adheres to the desired format, we include a few in-context examples within the prompts.

A language model solves a planning problem by interpreting the domain and problem descriptions, simulating state transitions, and generating a sequence of actions to achieve the goal. While effective for reasoning and plan generation, language models may struggle with large state spaces. Unlike the PDDL Solver, the LLM Planner may generate \emph{incorrect} plans that violate the problem specifications (e.g., preconditions of actions) or fail to achieve the goal.

\subsection{Explanation and Refinement}
Alongside the generated plans, we offer detailed explanations of all the plans and revisions of any incorrect plans. This study examines how these explanations and refinements influence human trust in the two planners.

\paragraph{LLM Planner with Explanation (LLM+Expl)}
For each generated plan, we manually provide a natural language explanation. This explanation includes an assessment of the plan’s correctness, identification of any violations of action preconditions, and an analysis of inconsistencies between the final state achieved and the intended goal state. We present examples of explanations in Figure \ref{fig: explain} in Appendix.

In particular, if a plan is correct, the explanation is simply ``the plan successfully satisfies the goal conditions.'' 
If a plan is incorrect, we identify the underlying cause as either a violation of action preconditions or a failure to achieve the goal state. In cases involving precondition violations, we specify the action responsible for the issue. For example, consider the action ``robot moves from room 1 to room 2,'' but the robot is initially located in room 3. This scenario constitutes a violation of the precondition for the ``move'' action. In the latter case, we describe the differences between the final state achieved and the intended goal state, e.g., ``fail to move ball 2 to room 2.''

% \begin{wrapfigure}{r}{0.5\textwidth}
%     \centering
%     \includegraphics[width=0.98\linewidth]{figures/refine.jpeg}
%     \includegraphics[width=0.98\linewidth]{figures/refine-correct.jpeg}
%     \includegraphics[width=0.98\linewidth]{figures/refine-wrong.jpeg}
%     \caption{Plan refinement by the LLM Planner. The top row presents two choices of plan refinement (where the refinement starts). The second and third row shows the refinement outcomes of the two choices, where the second row shows a correctly refined plan and the third row shows an incorrect plan.}
%     \Description{Refinement}
%     \label{fig: refine}
% \end{wrapfigure}

\paragraph{LLM Planner with Refinement (LLM+Refine)}
Note that a plan generated by the LLM Planner could be incorrect. Therefore, we offer a prompting mechanism for the LLM Planner to refine the generated plan according to the user feedback. The mechanism works as follows:

1. Request the user to indicate the step number of the first action in the plan that is incorrect, such as the step where an action’s precondition is violated. We present a sample user interface on the left of Figure \ref{fig: refine} in Appendix.

2. Send the planning domain, problem description, and the original plan to the language model. Then, query the model to rewrite the subsequent steps starting from the user-specified step number. We present a sample input prompt in Figure \ref{fig: refine-prompt} in the Appendix.

3. Replace the original plan with the newly refined plan and display it to the user.

This mechanism allows users to interact with the language model to refine the plan. It enables the language model to focus on a subset of steps, facilitating a deeper interpretation of the incorrect component. However, the correctness of the refined plan is not guaranteed. Figure \ref{fig: refine} in the Appendix shows an example of a correctly refined plan and an incorrectly refined plan.
