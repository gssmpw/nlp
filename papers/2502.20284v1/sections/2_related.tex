%======================================================
\subsection{LLM Planning}
When applied to planning tasks, LLMs are typically provided with a natural language description of a planning problem, with the goal of generating a corresponding plan. Several works have demonstrated the potential of using LLMs for decision making processes~\cite{singh2023progprompt,ren2023robots}. However, One of the main findings is that LLMs have limited ability to generate and validate plans on their own, even for simple planning tasks~\cite{kambhampati2024llms,valmeekam2022large,silver2022pddl,valmeekam2023planning}.

A number of studies adopt Planning Domain Definition Language (PDDL) as a testbed for assessing LLMs' ability to generate and refine plans in structured environments~\cite{silver2022pddl, silver2024generalized, liu2023llm+, zhou2024isr}. Our study also uses a PDDL domain called Grippers to examine human trust in LLM-generated plans.
Unlike classical PDDL planners~\cite{mcdermott20001998, ghallab2004automated, fastdownward}, LLMs bring additional features, such as the ability to provide natural language explanations and refine generated plans, which have the potential to enhance user trust in the planner. Research has shown that clear, personalized explanations enhance user trust~\cite{kunkel2019let}, while refinement frameworks like ART (Ask, Refine, and Trust)~\cite{sebo2019don} may help LLMs identify and address errors in their outputs.

%======================================================
\subsection{Trust on Automation}
This paper investigates human trust in large language models for planning, building on the concept of trust in automation as the willingness to rely on automated systems~\cite{lee2004trust}.
Studies have found that human trust changes over time during the interaction with automation, affected by various factors such as the automation's reliability, predictability, and transparency~\cite{hancock2011meta,schaefer2016meta}. 
Research also shows that stated accuracy can significantly affect user trust in a system~\cite{yin2019understanding}. To avoid bias from preconceptions about accuracy, this study does not disclose the planners' accuracy or details, instead referring to them generically (e.g., as Planner A or B), focusing on trust based on observed performance during interaction.
Various approaches exist for measuring trust, with user questionnaires being a common method to assess subjective belief of trust~\cite{martelaro2016tell,xu2015optimo}. For example, the study in~\cite{choi2015investigating} asked questions about users' trust in automated vehicles on a 7-point Likert scale. In addition, broader measures like the Propensity to Trust scale~\cite{merritt2013trust} can be used to provide insights into general attitudes toward AI planners.
