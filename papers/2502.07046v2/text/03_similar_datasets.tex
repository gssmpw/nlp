\section{Similar Datasets}\label{sec:similar_datasets}

%\DANIEL{We need to reduce this section on the already known datsets}
%%%%% DATASETS IN FOR SOFTWARE TASKS 

Significant efforts have produced datasets for evaluating \llms in SE tasks, including DeepFix for program repair \cite{gupta_deepfix_2017}, CodeContest and CoNaLa for program synthesis \cite{li_competition-level_2022, yin2018mining}, and \textit{CodeSearchNet} for code retrieval \cite{husain2019codesearchnet}. Expansions like \textit{CodeXGLUE} \cite{lu_codexglue_2021}, xCodeEval \cite{khan_xcodeeval_2023} target broader tasks, while benchmarks such as \textit{HumanEval} and SecurityEval focus on functional correctness and vulnerabilities \cite{chen_evaluating_2021, secEval}. Despite these efforts, existing datasets often suffer from contamination\cite{jain_livecodebench_2024,yadav_pythonsaga_2024}, with overlaps between training and evaluation data, and benchmarks are prone to memorization by models\cite{ramos2024largelanguagemodelsmemorizing}, limiting their effectiveness in assessing true generalization.
%Significant efforts have been made to collect data for training and evaluating \llms in SE tasks. Examples include DeepFix \cite{gupta_deepfix_2017}, which focuses on program repair, and CodeContest \cite{li_competition-level_2022} and CoNaLa \cite{yin2018mining}, both designed for program synthesis. Husain \etal introduced \textit{CodeSearchNet}, a widely recognized dataset aimed at automating code retrieval \cite{husain2019codesearchnet}. Building on this foundation, Microsoft researchers expanded \textit{CodeSearchNet} by integrating 12 SE-related datasets targeting tasks such as clone detection, code refinement, and translation, resulting in the creation of the \textit{CodeXGLUE} benchmark \cite{lu_codexglue_2021}. Inspired by \textit{CodeXGLUE}, several task-specific benchmarks have emerged, including xCodeEval \cite{khan_xcodeeval_2023}, a multilingual multitask benchmark, and Galeras \cite{rodriguezcardenas2023benchmarking}, which evaluates \llms using snippet features through causal inference. Similarly, \textit{HumanEval} was developed to validate the functional correctness of generated code \cite{chen_evaluating_2021}, followed by HumanEval-X \cite{zheng_codegeex_2023}, which focuses on code generation and translation. In the context of security, SecurityEval \cite{secEval} assesses how models such as InCoder \cite{fried_incoder_2023} and Copilot \cite{noauthor_github_nodate} might introduce vulnerabilities during source code generation.

Recent research work has explored the dynamic generation of prompts and testbeds, for instance, \textit{EvoPrompt} is a framework for automatic discrete prompt optimization that connects \llms with Evolutionary Algorithms\cite{guo_connecting_2024}. \textit{Evol-instruct} is a systematic approach to generate instruction-response pairs by iteratively improving prompts and responses through model self-enhancement\cite{xu_wizardlm_2023}. \textit{LiveCodeBench} is a benchmark for evaluating \llms designed to generate code\cite{jain_livecodebench_2024}. Unlike \snipgen, LiveCodeBench addresses issues of data contamination by using continuously updated problems from online coding competitions. 

%\DANIEL{We need to add recent papers on benchmarks and testbed contamination}

%%%% Datasets for instruction Tuning
%\textit{\textbf{Instruction Tuning.}} More closely aligned with \snipgen, some datasets include prompt definitions designed to instruct \llms to perform specific tasks, an approach frequently adopted for \textit{instruction-tuning}. For instance, WizardCoder \cite{luo_wizardcoder_2023} was trained on a dataset derived from CodeAlpaca \cite{codealpaca} using the \textit{Evol-Instruct} method \cite{xu_wizardlm_2023}. 
 
%Considerable research efforts have been directed towards data collection for training and testing \llms. Initially, Husain et al. introduced \textit{CodeSearchNet} to automate code retrieval \cite{husain2019codesearchnet}. Microsoft researchers later expanded \textit{CodeSearchNet}, adding 12 SE-related datasets for various downstream tasks such as clone detection, refinement, and translation, resulting in \textit{CodeXGLUE} \cite{lu_codexglue_2021}. Secondly, Wainakh et al. introduced \textit{IdBench} to assess generated identifiers by measuring similarity distances of semantic representations \cite{wainakh_evaluating_2019}. Chen et al. introduced \textit{HumanEval} to validate the functional correctness of generated code \cite{chen_evaluating_2021} with specific cases on SE tasks. Cassano et al. expanded later \textit{HumanEval} to create \textit{MultiPL-E} for code translation \cite{cassano_multipl-e_2022}. Rodriguez \etal, \cite{rodriguezcardenas2023benchmarking} introduce a benchmark using snippet features to evaluate \llms via causal inference. Most of these datasets are focused and provide code snippets examples and descriptions, vulnerabilities datasets are created aside. The lack of datasets in \llms is demonstrated by Siddiq \etal \cite{secEval} that introduced SecurityEval, a benchmark assessing how InCoder \cite{fried_incoder_2023} and Copilot \cite{noauthor_github_nodate} might introduce vulnerabilities in source code generation. Similarly, Pearce \etal \cite{pearce_asleep_2022} have created a dataset featuring 89 CWE-based scenarios.ng, providing context, and using the prompt template, \snipgen can automatically generate a set of prompts for the specific SE task. 

%Researchers and practitioners can use the generated set of prompts for evaluating the \llms model enabling the analysis for interpretation or accuracy benchmark. \snipgen does not change any canonical path for the training and testing process (see, Fig. \ref{fig:overview} section \circled{B}). Indeed \snipgen supports the evaluation and tries to avoid the bias by generating mutated inputs and selecting unseeing snippets.