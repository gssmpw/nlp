\section{Introduction}\label{sec:introduction}

%%% What is a llm
Large Language Models (\llms) have demonstrated significant success across diverse software engineering (SE) tasks, including code auto-completion \cite{austin2021program, Hendrycks2021apps, chen_generation_2021,White.MSR.2015,Ciniselli.TSE}, code summarization \cite{leclair_ensemble_2021,Moran.SANER.2022}, code review \cite{Tufano.ICSE.2021, Tufano.ICSE.2022}, code translation \cite{Nguyen:ICSE15},  clone detection \cite{White.ASE2016,Tufano.MSR.2018}, and program repair \cite{Tufano2019LearningBugFixes,zhou_devign_nodate,SANER.2019,Tufano.ICSE19.Changes,ASE.2018,Zimin.Sequencer,CanWeFix}. \llms are neural models trained on huge datasets including complete \github repositories. Common testbeds for evaluating LLMs for code such as \texttt{HumanEval}, \texttt{MBPP} and \texttt{CodeXGlue} are no longer sufficient \cite{jain_livecodebench_2024}. In addition, as benchmarks and testbeds are released, new \llms probably already seen those testbeds. Therefore the testbeds are prone to be outdated as soon as a new LLM is released.

%\llms are a type of transformers that work by decoding information from prompts and generating new outputs using encoded representations (\ie Decoder-Based models such as GPT-4). While \llms are widely adopted beyond the machine learning community due to their effectiveness, using them effectively presents distinct challenges.

%\DANIEL{Connect here the idea of using chain-of thoughts for evaluating llms}

%%% introduce prompt engineering
\llms perform complex tasks by relying on statistical knowledge acquired from data distributions, a phenomenon described by Wei \etal as \textit{emerging capabilities} \cite{wei_emergent_2022}.  Given the limited understanding of the nature of this phenomenon, we can formulate an important question: \textit{under what conditions \llms produce the desired output?} Prompt engineering addresses this question by harnessing these capabilities, guiding \llms to make more accurate predictions. Furthermore, given that \llms can extract rules from the provided context (\ie in-context learning), prompt engineering is a natural and intuitive way for people to use \llms.

%% benefits of prompt engineering. 

Recent studies have demonstrated that \llms exhibit improvements in accuracy for downstream tasks when prompts are enhanced and augmented \cite{Zhou2022LargeLM, White2023ChatGPTPP}. Moreover, new methods for crafting better prompts are being explored. For example, Beurer-Kellner \etal \cite{promptingIsProgramming} introduce the idea of Language Model Programming (LMP) which combines text-based prompting with scripting. Furthermore, Wei \etal \cite{wei_chain--thought_2023} shows that the incorporation of Chain-of-Thought (CoT) significantly improves the ability of \llms to perform complex reasoning.

%%% Problem introduction

%One challenge lies in understanding the internals of newer generation language models and using vendor-specific libraries and implementations as they operate on tokens. Constraining the decoding procedure to a set of legal words or phrases can be difficult. Many prompting techniques require manual interaction or task-specific interfaces, limiting the generality of implementations. Additionally, \llmc produces one sub-word token at a time, necessitating multiple calls to complete a sequence. This, coupled with the growing computational cost and latency as the prefix and response length increase, makes practical inference demanding and expensive. Pay-to-use APIs, like OpenAI's GPT-3, incur high usage costs per query due to these factors, affecting their practical feasibility.

Understanding the internal mechanisms of \llms presents a significant challenge. Current datasets and benchmarks often lack the curated data necessary for thorough performance analyses. Therefore, there is a critical need for consistent data points to effectively evaluate the performance of \llms across various SE tasks. We argue that well-designed testbeds and prompts are the key to accurately assessing \llms understanding of complex information, such as task-related semantics.

%\DANIEL{Metion snipgen has been used in galeras and vulnerability dataset generation (code smells for ast decomposition)}

%%%Solution
%\DANIEL{Introduce the tool and methodology}
To bridge the gap between existing datasets and benchmarks, we developed \snipgen. \snipgen is a framework to collect source code snippets from \github. Each snippet is automatically augmented with prompts tailored for various software tasks. Practitioners and researchers can query and generate new prompts according to the SE task and experiment with different configurations for evaluating \llms for code.  Our goal is to provide resources that can more accurately assess the performance of \llms and aid in the construction of more detailed benchmarks.

%%%Contributions
The contributions of this paper are listed as follows: 1) A Framework for mining software repositories and crafting data points augmented with prompts for specific SE downstream tasks. 2) a generated testbed comprising Python snippets with calculated features from the AST, natural language, and vulnerabilities analysis~\cite{zenodo_14279563}. 3) Prompt-generated dataset with mutated snippets crafted for Code Completion, Commit generation, and Code summarization. 4) source code and complementary material used in this research are published in an open-source repository\cite{snipgen}.