\section{Experience Report}\label{sec:experience}

In this section, we describe our experience of using \snipgen for collecting a testbed and generating a set of prompts. We also briefly describe three use cases illustrating how \snipgen was successfully used to evaluate \llms for code.

\subsection{\snipgen Testbed Generation}

The experience with \snipgen begins by mining repositories from \github, as detailed in \secref{sec:methodology}. We focused on the most popular Python repositories, applying the following query filters: \textit{language:Python fork:false size:$>=30000$ pushed:$>$2021-12-31 stars: $> 2000$.} The query gathers the most popular repositories in Python. We selected the top 200 repositories including \textit{keras, numpy, pandas, sentry, etc.} We extracted the new snippets reported on commits between 2022 and 2023 from selected repositories. Then we used the \textit{data curation} to remove duplicates and \textit{feature extraction} to generate and extract the associated features. We configured a $0.7$ similarity threshold~\cite{Allamanis19, wang_neural_2019} to de-duplicate snippets using HuggingFace tokenizer BPE. \snipgen saves the raw data and their features into a JSON and a database.  We randomly validated $960$ out of $\approx 227K$  data points to confirm the extracted features and the meaningfulness of the \textit{Docstring} and linked code.

We sampled until $5k$ data points from the \textit{RawData} testbed to construct six testbeds, each tailored for a specific SE task as described at \secref{sec:se_tasks}. To create \randomCut, we selected data points with more than $10$ tokens or $100$ characters, and subsequently, each data point was randomly truncated after the method signature. For \summarizationGen and \commitGen, we filtered \textit{RawDataDocstring} data points with more than 10 words or 50 characters. Table.~\ref{tab:dedupe} provides information about the SE task associated with each curated testbed, the percentage of detected duplicates, the final size, and the generated number of prompts.


\input{tabs/dedupe}
\subsection{Successful Use Cases}\label{sec:cases}
\textit{\textbf{Galeras}}\cite{galeras}: Galeras is a benchmark for measuring the causal effect of SE prompts for code completion. Galeras configures a set of treatments to assess the influence of potential confounders on the outcomes of ChatGPT (\ie GPT-4). The selected confounders are: \textit{prompt\_size} (from prompts), \textit{n\_whitespaces} (from documentation), \textit{token\_counts}, and \textit{nloc} (from code\_features). This use case of \snipgen demonstrates that prompt engineering strategies (such as those listed in \tabref{tab:templates} - processing prompt) have distinct causal effects on the performance of ChatGPT.


\textit{\textbf{SyntaxEval}}~\cite{syntax_capabilities}: In this use case \textit{Syn taxEval} evaluates the ability of Masked Language Models (\ie Encoder-based Transformers) to predict tokens associated with specific types in the AST representation (\ie syntactic features). \textit{SyntaxEval} used \snipgen to construct a \textbf{\textit{code completion}} testbed with approximately $50K$ Python snippets. \textit{SyntaxEval} aims to account for potential confounders such as \textit{ast\_data} and \textit{code\_features} (illustrated in \figref{fig:diagram}), the analysis revealed no evidence that the evaluated syntactic features influenced the accuracy of the selected models' predictions.

\textit{\textbf{ASTxplainer}}~\cite{astexplainer}: \textit{ASTxplainer} is an explainability method designed to assess how effectively a \llm (\eg decoder-based transformers) predicts syntactic structures. \textit{ASTxplainer} aggregates next-token prediction values through syntactic decomposition, quantified as AsC-Eval values to evaluate the effectiveness. \textit{ASTxplainer} findings reveal that the ability to predict syntactic structures strongly depends on the \llm's parameter size and fine-tuning strategy. Furthermore, causal analysis controlling for confounding variables (e.g., \textit{ast\_data} and \textit{code\_features}) shows that AsC-Eval values at the snippet level negatively impact the cross-entropy loss of the evaluated \llms.
%by aggregating next-token prediction values through syntactic decomposition (\ie AsC-Eval values).  \textit{ASTxplainer} findings include that predicting syntactic structures highly depends on the LLMsâ€™ parameter size and fine-tuning strategy. Additionally, after conducting causal analysis to control for confounding variables (\ie \textit{ast\_data} and \textit{code\_features}), the cross-entropy loss of the selected LLMs is negatively impacted by the AsC-Eval values at the snippet granularity.
