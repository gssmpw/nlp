\section{The \snipgen Framework}\label{sec:methodology}

\snipgen is a framework to extract snippets at method granularity from \github. \snipgen follow steps for curating the extracted raw data and take features from the data such as the number of identifiers, vocabulary, tokens, etc. Features derived from their AST representations and further complementary data. Our dataset can potentially improve the quality of the predictions in downstream tasks by augmenting the prompts, thereby enabling \llms to perform more effectively.


\begin{figure}[ht]
		\centering
		\includegraphics[width=0.48\textwidth]{img/2_methodology/Model3.pdf}
		\caption{\snipgen Data collection and prompt generation }
        %\vspace{-0.5cm}
        \label{fig:collection}
\end{figure}

Fig. \ref{fig:collection} depicts the process followed by \snipgen to generate a testbed and the \snipgen architecture. The \snipgen architecture comprises components to collect, curate, store, extract, and generate a SE-oriented testbed. The process begins with \circled{1}, the Data Collection phase, where source code snippets are extracted with \textit{pydriller} library~\cite{pydriller} from selected repositories in \github given a tie window. A set of snippets representing a Python method is extracted from each commit. This is followed by \circled{2}, a Pre-processing step, where the -Data Curation- \snipgen component stores the raw data in a \textit{MySQL} database. Once the data is formatted and saved in the storage, \snipgen looks for exact match snippets and removes duplicates. The -Feature extractor- component parses the code into the AST representation using tree-sitter~\cite{tree_sitter} and computes associated features (\ie the number of AST levels, AST nodes, comments, function name).

The data validation at step \circled{3} is a manual evaluation where the authors confirm the dimension values and the meaningfulness of the \textit{Docstring} and linked code, the two authors first selected the docstring with more than 20 words and evaluate the description against the code snippet. The description must depict the steps or intention of the snippet.

The testbed generation step \circled{4}, filters the raw data, evaluates the Jaccard similarity, and identifies vulnerable code. The raw filtering depends on the SE task, for example for code completion \snipgen filters the valid code with more than two lines of code. \snipgen uses CodeQL~\cite{codeql_overview} for vulnerability detection and appends the vulnerability location on the snippet. Finally, step \circled{5} uses the selected snippets from \circled{4} and applies the prompt template to the aimed SE task generating a final prompt. 
\snipgen enables the model evaluation and benchmarking as used in \cite{galeras, astexplainer, syntax_capabilities}.
The following subsections include a detailed description of the features of each data point.

\subsection{Data Point Feature Structure}
\begin{figure}[h]
		\centering
		\includegraphics[width=0.48\textwidth]{img/2_methodology/Diagram.pdf}
		\caption{\snipgen data schema. The snippet represents the core commit collected with documentation.  Linked tables contain calculated features. }
       %\vspace{-0.5cm}
        \label{fig:diagram}
\end{figure}
\snipgen can collect a set of Python methods that serve as evaluative data points. Each data point has associated features at seven dimensions as observed at \figref{fig:diagram}. These seven dimensions describe the static feature from the snippet. We aim to link code fragments with their properties. The first dimension corresponds to snippets' identification,  which includes the \commitID (\ie commit hash), \textit{repository} name, \textit{path}, \textit{file\_name}, \funName, \commitMessage. The second dimension is related to the associated documentation \docstring. The \docstring extended to complementary natural language features such as \textit{n\_words, vocab\_size, language,} and \whitespaces. The third dimension corresponds to the snippet's syntactic information, which includes the actual code base, \ASTerrors, \ASTlevels, \ASTnodes,\textit{n\_words}, \textit{vocab\_size}, \tokenCount, and \whitespaces. The fourth dimension corresponds to canonical software metrics, which include \nloc, \complexity, \identifiers. The fifth dimension depicts the span position for vulnerabilities detected from the \textit{code} snippet. The sixth dimension is associated with the snippet mutation when the code is randomly cut one line after the signature, therefore \snipgen identifies the signature as the original \textit{snippetID} and cut code. Finally, the seventh dimension comprises the linked features to the generated prompt. \snipgen labels the prompt to the SE task and the prompt configuration.

\subsection{Software Engineering Tesbed Task}
\label{sec:se_tasks}

Data curation, pre-processing, and data validation produce a testbed oriented to evaluate a model. For instance, a \textit{RandomCut} and \textit{WithDocString} testbeds might evaluate the model at SE tasks, such as \textit{\textbf{code completion}}—generating code to fill in missing parts of a function. \textit{WithDocString} testbed selects the snippets with valid documentation and code so the \llm input compresses both a description and code.
\textit{FromCommit} testbed is focused on selecting meaningful commit messages and linked source code so that to evaluate either \textit{\textbf{commit generation}}—producing commit messages based on code changes or \textit{\textbf{code generation}} producing the complete snippet from the description.  \textit{FromDocString} testbed select only meaningful code descriptions (\ie only \docstring) to generate the code snippet also configuring a code generation case.
\snipgen can be used to evaluate \textit{\textbf{code summarization}}—creating natural language descriptions of the functionality implemented in the provided source code. If we select the original code from the \textit{WithDocString} testbed and the ones at the top of docstring length then we can use the testbed for summarization.



\subsection{Prompt templates}
\label{sec:prompt_templates}

\input{tabs/prompt_templates}


The effectiveness of \llms in code generation is greatly influenced by prompt design. At this point \snipgen only produces a set of data points that can be organized as an input for an autoregressive \llm since the tesbed contains the input and the expected output. \snipgen combines prompt templates and gathered data points to build the final prompt input. The structure, keywords, and context of a prompt play a crucial role in shaping results and analyses. Prompts can be configured as a single-step or multi-step, with the latter allowing iterative refinement based on the model's initial response. Chau \etal \cite{liu_improving_2023} explore such multi-step configurations. Table \ref{tab:templates} lists eight prompt templates, practitioners can modify the template according to the evaluation task. From the proposed list, $P1-P5$ supports single-step SE tasks, while $P6-P8$ enables multi-step processing by combining prompts to refine outputs. For instance, \snipgen can combine $P1+P8$, $P3+P6$, or $P3+P8$ for code completion.


%The tool defines prompting templates used for each SE task, as detailed in \tabref{tab:templates}.

For \textit{\textbf{code completion}}, \snipgen defines three prompts. $P1$ asks the model to complete a method from a randomly selected cut position (\randomCut) in the specified programming language (\textit{language}). $P2$ extends $P1$ by including additional details, such as the method's \textit{signature} and \textit{docstring}. $P3$, in contrast, provides only an NL description extracted from the method's \textit{docstring}. In commit generation, $P4$ instructs the model to create an NL description of the changes made to transform the \randomCut version of a method into its complete code (\textit{code}). $P5$ is designed to ask the model to generate the commit message from the mutated code and the actual code. Lastly, in \textit{\textbf{code summarization}}, \textit{P6} provides only the code, which the model uses to generate a corresponding summary.



%For instance, \galeras uses \textit{CLD3} library for \textit{language} detection with a threshold of 0.9 to define sentence language such as English, Chinese, Spanish, etc.

%  

%Finally, we computed the number of lines of code (\nloc), and the cyclomatic \complexity to bring more information about the internal interaction and the \identifiers for the \textit{code}.



%The effectiveness of a \llms for code generation could be heavily influenced by the choice of prompt. The way we formulate the questions and actions, and introduce the context impacts the results as well as the analysis over the \llms. Therefore, the necessary keywords and structure for the prompt constitute the key aspect of building a functional prompt. Additionally, prompts could be configured to be multi-step or single-step. In other words, we can interact with the \llm and then provide more information or restrictions according to the answer in a second prompt. Chau \etal \cite{liu_improving_2023} describes the combination of prompts to be used at multi-step configuration. 

%Table \ref{tab:templates} lists a set of seven prompt templates that \snipgen can generate. We have five templates to support SE tasks using single-step prompt configuration and three templates for processing the prompt by combining them with the SE Tasks. The idea of the processing prompt is to guide and fine-tune the answer generated with the first interaction. For instance, \snipgen we can combine $P1+P7$, $P3+P6$, $P3+P7$ for code completion. 
%\DANIEL{Describe better the aim for each prompt, this took base from the CoT paper}



\subsection{\snipgen Prompt Generation and Use}


\begin{figure}[ht]
		\centering
		\includegraphics[width=0.5\textwidth]{img/2_methodology/SnipGen-Overview2.pdf}
		\caption{\snipgen Dataset and use. \textit{A} describes the \snipgen data collection and steps until prompt generation. \textit{B} describes the canonical path for training and evaluate \llms}
        %\vspace{-0.5cm}
        \label{fig:overview}
\end{figure}

The \snipgen framework is designed to select a SE task and evaluate a \llm using the testbed with a given context with a designed prompt. \figref{fig:overview} depicts the options a practitioner has to evaluate a \llm. The database supports a query to filter the snippets according to the SE task, for instance for code completion we can sample the snippets with linked \docstring with more than 10 words, this provides the task context (see, Fig. \ref{fig:overview} section \circled{A}). The prompt generation might contain a mutated snippet such as \randomCut to perform the required task. For example, for code completion, we will need a partial code snippet that must be auto-completed by the \llm therefore we need to cut the original snippet smartly. \snipgen can use the \randomCut method to split the code beyond the method signature. Practitioners can still evaluate the model using \textit{canonical} datasets and metrics to compare against the new collected \snipgen testbed.
