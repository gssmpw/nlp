\section{Experience Report}\label{sec:experience}

\DANIEL{This section should be completely rewritten and simplified, we just need to talk about the generated dataset and how we used it at galeras and astXplainer https://arxiv.org/pdf/2308.03873}
\subsection{Collecting Code Samples}
\input{tabs/dedupe}

As represented in the first step of the pipeline (refer to Fig.~\ref{fig:collection}-\circled{1}),  we filtered the most popular Python Github repositories using the following query: $language:Python$, $fork:false$, $size:>=30,000$,  $pushed:>2021-12-31$, $stars:>1,000$.

Our dataset aims to avoid data snooping, a problem where training samples are wrongly used to test statistical hypotheses. To do this, we focused on collecting code changes only up to January 2022. This is because we hypothesize \chatgpt and most newer \llms have not been trained on commits made after January 2, 2022, according to \cite{openai2023gpt4}. We carefully selected new methods from each commit, resulting in about 338K data points. For each of these data points, we extracted the comments or any associated documentation that describes its functionality.

In the second step (Fig.~\ref{fig:collection}-\circled{2}), we engineered and pre-processed code and documentation-related and security weaknesses from collected data points. We parsed the AST variables for our data points by employing the Tree-Sitter library. Therefore, each data point encompasses the \textit{code} and related features from the AST analysis. The AST features numerically describe the \ASTerrors, \ASTlevels, and \ASTnodes. The AST also describes the structural shape for the \textit{code} introducing the \textit{n\_words}, \textit{vocab\_size}, \tokenCount, and \whitespaces. The vocabulary size measures the count of unique words without repetition. \tokenCount is determined by using a universal tokenizer. We also calculated the number of variables that the \textit{code} is using and it is reflected as \identifiers. As for determining the security weaknesses, we use codeQL scanning suite \cite{noauthor_codeql_nodate}, focussing on the top `25'  Common Weakness Enumeration (CWE) \cite{noauthor_cwe_nodate}. CodeQL identifies vulnerability issues by scanning for predetermined rules defined with its query language. We report CodeQL results with the span positions of the identified vulnerability in the code. 

To guarantee efficient data management, we stored raw and preprocessed data points in a relational database once the previous features were engineered and extracted. Next, we removed duplicated samples using a distinct query reducing the testbed size to $\approx 227K$ data points for code. Of these reduced data points, $\approx 77K$ contains a valid \docstring. A \docstring is valid when its text is larger than 3 words. We use \textit{CLD3} library to detect the \textit{language} for each \docstring with a threshold of 0.9 to define sentence language such as English, Chinese, Spanish, etc.
Once these features were computed, the raw data was stored in a relational database, enabling efficient management and summarization of many data points.


In the third step \ref{fig:collection}-\circled{3}), we manually verified $960$ out of $\approx 227K$ data points. These verified data points were randomly selected from \textit{RawData} and \textit{RawDataDocstring} for a manual evaluation. The remaining data points underwent automatic validation. Our validation process ensures that the date of each pushed commit falls within the range of dates stated in the original query. Additionally, we confirmed that the methods associated with each commit were indeed updated within the same date range. Furthermore, we assessed the meaningfulness of the \docstring and \commitMessage by scrutinizing the consistency of the natural language descriptions with the actual code implementation, resulting in the removal of $\approx1.9\%$ from \textit{RawDataDocstring} and obtaining $\approx57K$ datapoints. Finally, \complexity was validated using the Codalyze plugin in Visual Studio Code.

In the final step (Fig.\ref{fig:collection}-\circled{4}), we sampled until $5k$ data points from the \textit{RawData} testbed to construct six additional testbeds, each tailored for a specific SE task. \snipgen encompasses \randomCut, \withDocstring, and \fromDocstring for \textit{code completion}; \commitGen for \textit{code generation}; \summarizationGen for \textit{code summarization}; and \textit{VulnerabilitySpan} for vulnerability detection. Details about these supplementary testbeds are outlined in Tab.~\ref{tab:dedupe}.

To create \randomCut, we selected data points with more than $10$ tokens or $100$ characters, and subsequently, each data point was randomly truncated after the method signature. For \summarizationGen and \commitGen, we filtered \textit{RawDataDocstring} data points with more than 10 words or 50 characters. After establishing the five testbeds, we employed Jaccard similarity on preprocessed data points with the BPE HuggingFace tokenizer to eliminate duplicate snippets. Given the absence of de-duplication between training and test sets (i.e., no multiset threshold), we configured a $0.7$ similarity threshold for our testbeds \cite{Allamanis19, wang_neural_2019}. Table.~\ref{tab:dedupe} provides information about the SE task associated with each curated testbed, the percentage of detected duplicates, final size and generated number of prompts.




 %Our methodology depicts data extraction based on experiment-specific parameters. Researchers are afforded the flexibility to choose datasets that align most seamlessly with their objectives. The pivotal role of \snipgen comes into play as it adeptly collects and extracts features from the chosen data. Each snippet is systematically labeled according to its relevance to a specific code generation sub-task in software engineering (SE). The presence of a \docstring within a snippet is leveraged by \snipgen to incorporate textual context seamlessly. Subsequently, a prompt template is strategically selected based on the SE sub-task, and a prompt is dynamically generated. This prompt serves as an invaluable tool for evaluating Language Model for Code (LLMc) but can also be efficiently trained and tested on canonical datasets, ensuring robustness and adaptability in the ever-evolving landscape of code generation research.



