\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
%\usepackage{csquotes}
%\usepackage [autostyle, english = american]{csquotes}

%\usepackage{minted}
%\usepackage{verbatim}
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
%\usepackage{pdflscape}
\usepackage{threeparttable}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\input{utils/macros}


\begin{document}
\bstctlcite{IEEEexample:BSTcontrol}
\title{
%SnipGen: A Comprehensive Repository Mining Tool for Evaluating LLMs for Code via Prompt Engineering
SnipGen: A Mining Repository Framework for Evaluating LLMs for Code 
}

\author{\IEEEauthorblockN{
Daniel Rodriguez-Cardenas, Alejandro Velasco, and
Denys Poshyvanyk}
\IEEEauthorblockA{Department of Computer Science,
William \& Mary\\
Williamsburg, VA\\
Email: dhrodriguezcar, svelascodimate, dposhyvanyk\{@wm.edu\}}}

%\author{\IEEEauthorblockN{Anonymous Author(s)}}

\maketitle

\begin{abstract}
%
Large Language Models (\llms), such as transformer-based neural networks trained on billions of parameters, have become increasingly prevalent in software engineering (SE). These models, trained on extensive datasets that include code repositories, exhibit remarkable capabilities for SE tasks. However, evaluating their effectiveness poses significant challenges, primarily due to the potential overlap between the datasets used for training and those employed for evaluation. To address this issue, we introduce \snipgen, a comprehensive repository mining framework designed to leverage prompt engineering across various downstream tasks for code generation. \snipgen aims to mitigate data contamination by generating robust testbeds and crafting tailored data points to assist researchers and practitioners in evaluating \llms for code-related tasks. In our exploratory study, \snipgen mined approximately $227K$ data points from $338K$ recent code changes in GitHub commits, focusing on method-level granularity. \snipgen features a collection of prompt templates that can be combined to create a Chain-of-Thought-like sequence of prompts, enabling a nuanced assessment of \llms' code generation quality. By providing the mining tool, the methodology, and the dataset, \snipgen empowers researchers and practitioners to rigorously evaluate and interpret \llms' performance in software engineering contexts.

%Large Language Models (\llms), like transformer-based neural networks trained on billions of parameters, have been increasingly adopted in software engineering. Such complex neural networks are trained on vast datasets containing both natural and programming languages. However, evaluating their effectiveness in performing specific tasks is challenging because of their size and complexity. To accurately assess these models, and the extent of their emerging capabilities, it's crucial to use the right inputs and diverse examples while avoiding bias. To address this challenge, we created \snipgen, a novel dataset that leverages prompt engineering across different downstream tasks for code generation. \snipgen provides crafted data points to assist researchers and practitioners in the evaluation of \llms in different scenarios. We used a semi-automatic approach to collect about $227K$ data points from $338K$ recent changes in code bases on \github, focusing on method-level granularity. Additionally, to guarantee the quality of our data, we manually validated the collected samples. Furthermore, \snipgen provides a set of templates, that can be combined to produce practical prompts for assessing the quality of \llms at different tasks. By providing both, the dataset and the methodology to build it, we aim to empower researchers and practitioners in evaluating and interpreting \llms. 


%Using a semi-automatic approach, we gathered $\approx 227K$ data points at method level granularity, extracted from $\approx 338K$ changes in recent code bases on \github. We selected the most relevant snippets based on a set of features that we calculated. From the collected samples, $\approx 57K$ of them have related documentation or comments. Furthermore, to ensure the data quality we manually validated the collected samples. We suggest a series of templates that, when combined, produce a set of practical prompts for achieving and assessing \llms. Through the provision of both this dataset and prompt generation, our goal is to empower researchers in the evaluation and interpretation of these models.
\end{abstract}

\begin{IEEEkeywords}
Deep learning, code generation, datasets, large language models, evaluation
\end{IEEEkeywords}

\input{text/01.intro}
\input{text/02.methodology}
\input{text/03.experience_report}
\input{text/03_similar_datasets}

\input{text/04.limitation}


%\input{text/05.conclusions}

%%\input{text/06_acknowledgment}

\bibliographystyle{IEEEtran}
\bibliography{utils/citations_bib}

\end{document}
