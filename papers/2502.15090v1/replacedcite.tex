\section{Related work}
\paragraph{Representation alignment}

Studies on the kinds of representations used by humans and machines have been of interest to many fields ____. Studies on \textit{representation alignment} ____ look specifically at the extent to which the internal representations of humans and neural networks converge on a similar structure. 
Across vision and text domains, models show notable alignment with human similarity judgments ---typically used as a window into human representational structures. ____ report significant alignment between human similarity judgments and representations of object classification networks, while ____ report similar alignment with GPT-3â€™s ____ embeddings. However, ____ finds that GPT-3's concept alignment is highly sensitive to prompt phrasing and
____ show that alignment in BERT ____ is very context-dependent.  Investigating general factors that can cause mis-alignment, %
____ conclude that the training dataset and objective function impact alignment, but model scale and architecture have no significant effect. Of note, alignment and performance are not inherently tied: mis-aligned models can exhibit significant capabilities ____. %


 



\paragraph{Activation steering} refers to a class of methods that intervene on a generative model's activations to perform targeted updates for controllable generation ____. ____ propose a method to identify sets of neurons in pre-trained transformer models that are responsible for detecting inputs in a specific style ____ or about a specific concept ____. Intervening on the expert neuron activations, %
successfully guides text generation into the desired direction. In a similar spirit, ____ use a contrastive prompt (one positive and one negative) to induce sentiment shift and detoxification, while
____ steer multilingual models to produce more target language tokens in open-ended generation. Finally, ____ introduce a unified approach to steer activations in LLMs and diffusion models based on optimal transport theory.